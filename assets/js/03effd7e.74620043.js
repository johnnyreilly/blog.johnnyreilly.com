"use strict";(self.webpackChunkjohnnyreilly_com=self.webpackChunkjohnnyreilly_com||[]).push([[40467],{12138:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/images/title-image-337c58e5e55f92f59a1d1db49366ec04.png"},19779:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/images/title-image-337c58e5e55f92f59a1d1db49366ec04.png"},28453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>s});var r=t(96540);const o={},i=r.createContext(o);function a(e){const n=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),r.createElement(i.Provider,{value:n},e.children)}},43245:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>a,metadata:()=>r,toc:()=>c});var r=t(96983),o=t(74848),i=t(28453);const a={slug:"using-kernel-memory-to-chunk-documents-into-azure-ai-search",title:"Using Kernel Memory to Chunk Documents into Azure AI Search",authors:"johnnyreilly",image:"./title-image.png",tags:["azure","c#","asp.net","ai"],description:"To build RAG (Retrieval Augmented Generation) experiences, where LLMs can query documents, you need a strategy to chunk those documents. Kernel Memory supports this.",hide_table_of_contents:!1},s=void 0,l={image:t(19779).A,authorsImageUrls:[void 0]},c=[];function u(e){const n={a:"a",img:"img",p:"p",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(n.p,{children:["I've recently been working on building retrieval augmented generation (RAG) experiences into applications; building systems where large language models (LLMs) can query documents. To achieve this, we first need a strategy to chunk those documents and make them LLM-friendly. ",(0,o.jsx)(n.a,{href:"https://github.com/microsoft/kernel-memory",children:"Kernel Memory"}),", a sister project of ",(0,o.jsx)(n.a,{href:"https://github.com/microsoft/semantic-kernel",children:"Semantic Kernel"})," supports this."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"title image reading &quot;Using Kernel Memory to Chunk Documents into Azure AI Search&quot; with the Azure Open AI / Azure AI Search logos",src:t(12138).A+"",width:"800",height:"450",loading:"lazy"})})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(u,{...e})}):u(e)}},96983:e=>{e.exports=JSON.parse('{"permalink":"/using-kernel-memory-to-chunk-documents-into-azure-ai-search","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2024-04-21-using-kernel-memory-to-chunk-documents-into-azure-ai-search/index.md","source":"@site/blog/2024-04-21-using-kernel-memory-to-chunk-documents-into-azure-ai-search/index.md","title":"Using Kernel Memory to Chunk Documents into Azure AI Search","description":"To build RAG (Retrieval Augmented Generation) experiences, where LLMs can query documents, you need a strategy to chunk those documents. Kernel Memory supports this.","date":"2024-04-21T00:00:00.000Z","tags":[{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."},{"inline":false,"label":"C#","permalink":"/tags/csharp","description":"The C# programming language."},{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."},{"inline":false,"label":"AI","permalink":"/tags/ai","description":"All things AI - Artificial Intelligence, Large Language Models and the like."}],"readingTime":16.47,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile-2025.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"using-kernel-memory-to-chunk-documents-into-azure-ai-search","title":"Using Kernel Memory to Chunk Documents into Azure AI Search","authors":"johnnyreilly","image":"./title-image.png","tags":["azure","c#","asp.net","ai"],"description":"To build RAG (Retrieval Augmented Generation) experiences, where LLMs can query documents, you need a strategy to chunk those documents. Kernel Memory supports this.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Large Language Models, Open API, View Models and the Backend for Frontend Pattern","permalink":"/large-language-models-view-models-backend-for-frontend"},"nextItem":{"title":"Overview of webpack, a JavaScript bundler","permalink":"/webpack-overview"}}')}}]);