"use strict";(self.webpackChunkjohnnyreilly_com=self.webpackChunkjohnnyreilly_com||[]).push([["39046"],{78708:function(e){e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"nodejs-azure-appinsights-fastify","metadata":{"permalink":"/nodejs-azure-appinsights-fastify","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2025-02-17-nodejs-azure-appinsights-fastify/index.md","source":"@site/blog/2025-02-17-nodejs-azure-appinsights-fastify/index.md","title":"Node.js, Azure Application Insights, and Fastify","description":"Learn how to set up a Node.js with Azure Application Insights and Fastify.","date":"2025-02-17T00:00:00.000Z","tags":[{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."},{"inline":false,"label":"Node.js","permalink":"/tags/node-js","description":"The Node.js runtime."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":4.585,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"nodejs-azure-appinsights-fastify","title":"Node.js, Azure Application Insights, and Fastify","authors":"johnnyreilly","tags":["azure","node.js","typescript"],"image":"./title-image.png","hide_table_of_contents":false,"description":"Learn how to set up a Node.js with Azure Application Insights and Fastify."},"unlisted":false,"nextItem":{"title":"Get Service Connections with the Azure DevOps API (REST and TypeScript)","permalink":"/get-service-connections-with-azure-devops-api"}},"content":"If you deploy a Node.js application to Azure, you might want to use Azure Application Insights to monitor it. This post shows you how to set up a Node.js application with Azure Application Insights. It also includes a Fastify plugin to automatically track requests. (Given the out of the box mechanism for tracking requests does not work with Fastify.)\\n\\n![title image reading \\"Node.js, Azure Application Insights, and Fastify\\" with the relevant logos](title-image.png)\\n\\nThis is one of those posts that gathers together information I found doing research and puts it in one place.\\n\\n\x3c!--truncate--\x3e\\n\\n## Setting up Azure Application Insights with Node.js\\n\\nSetting up Azure Application Insights with Node.js is straightforward. You need to install the [`applicationinsights`](https://github.com/microsoft/ApplicationInsights-node.js) package which integrates Node.js and Azure Application Insights:\\n\\n```sh\\nnpm install applicationinsights --save\\n```\\n\\nThen configure it with your connection string. You want to do this as early as possible when your application starts, so if there are issues, you know as soon as possible. Here\'s how you can do this:\\n\\n```ts\\nimport appInsights from \'applicationinsights\';\\n\\nlet client: appInsights.TelemetryClient | undefined;\\n\\nif (process.env.APPLICATIONINSIGHTS_CONNECTION_STRING) {\\n  // https://github.com/microsoft/applicationinsights-node.js?tab=readme-ov-file#configuration\\n  appInsights\\n    .setup(process.env.APPLICATIONINSIGHTS_CONNECTION_STRING)\\n    .setAutoCollectRequests(true)\\n    .setAutoCollectPerformance(true, true)\\n    .setAutoCollectExceptions(true)\\n    .setAutoCollectDependencies(true)\\n    .setAutoCollectConsole(true, true) // this will enable console logging\\n    .setAutoCollectPreAggregatedMetrics(true)\\n    .setSendLiveMetrics(false)\\n    .setInternalLogging(false, true)\\n    .enableWebInstrumentation(false)\\n    .start();\\n\\n  client = appInsights.defaultClient;\\n}\\n```\\n\\nThe above code does two things of interest:\\n\\n1. It sets up Azure Application Insights with the connection string you provide. For Node.js applications, the connection string tends to be stored in the environment variable `APPLICATIONINSIGHTS_CONNECTION_STRING` and I see no reason to deviate from that.\\n2. It configures the telemetry client to collect various types of telemetry data. You can see the full list of options [here](https://github.com/microsoft/applicationinsights-node.js?tab=readme-ov-file#configuration). In the example above, all the defaults are used with one exception. That exception is `setAutoCollectConsole(true, true)` which enables console logging. This is in response to this part of the docs:\\n\\n> Note that by default `setAutoCollectConsole` is configured to _exclude_ calls to `console.log`\\n> (and other `console` methods). By default, only calls to supported third-party loggers\\n> (e.g. `winston`, `bunyan`) will be collected. You can change this behavior to _include_ calls\\n> to `console` methods by using `setAutoCollectConsole(true, true)`.\\n\\nI\'m not using a third-party logger in this example, so I want to include calls to `console` methods.\\n\\n## Setting up Fastify with Azure Application Insights\\n\\nIf you\'re not a [Fastify](https://fastify.dev/) user you can stop here. But if you are a Fastify user, you may have discovered that `setAutoCollectRequests` appears not to be be working. There\'s a [GitHub issue to track this](https://github.com/microsoft/ApplicationInsights-node.js/issues/627), but no discernible sign that it\'s going to be fixed soon. So, I\'ve created a Fastify plugin to track requests manually based upon [@stefanpeer\'s comment](https://github.com/microsoft/ApplicationInsights-node.js/issues/627#issuecomment-2194527018):\\n\\n```ts\\nimport { FastifyInstance, FastifyPluginOptions } from \'fastify\';\\nimport fp from \'fastify-plugin\';\\nimport appInsights from \'applicationinsights\';\\n\\ndeclare module \'fastify\' {\\n  export interface FastifyRequest {\\n    // here we augment FastifyRequest interface as advised here: https://fastify.dev/docs/latest/Reference/Hooks/#using-hooks-to-inject-custom-properties\\n    app: { start: number };\\n  }\\n}\\n\\n// based on https://github.com/microsoft/ApplicationInsights-node.js/issues/627#issuecomment-2194527018\\nexport const appInsightsPlugin = fp(\\n  async (fastify: FastifyInstance, options: FastifyPluginOptions) => {\\n    if (!options.client) {\\n      console.log(\'App Insights not configured\');\\n      return;\\n    }\\n\\n    const client: appInsights.TelemetryClient = options.client;\\n    const urlsToIgnore = options.urlsToIgnore || [];\\n\\n    fastify.addHook(\\n      \'onRequest\',\\n      async function (this: FastifyInstance, request, _reply) {\\n        // store the start time of the request\\n        const start = Date.now();\\n        request.app = { start };\\n      },\\n    );\\n\\n    fastify.addHook(\\n      \'onResponse\',\\n      async function (this: FastifyInstance, request, reply) {\\n        if (urlsToIgnore.includes(request.raw.url)) return;\\n\\n        const duration = Date.now() - request.app.start;\\n        client.trackRequest({\\n          name: request.raw.method + \' \' + request.raw.url,\\n          url: request.raw.url,\\n          duration: duration,\\n          resultCode: reply.statusCode.toString(),\\n          success: reply.statusCode < 400,\\n          properties: {\\n            method: request.raw.method,\\n            url: request.raw.url,\\n          },\\n          measurements: {\\n            duration: duration,\\n          },\\n        });\\n      },\\n    );\\n  },\\n);\\n```\\n\\nThe above code creates a Fastify plugin that tracks requests manually. It does this by adding two hooks to Fastify:\\n\\n- `onRequest` - This hook stores the start time of the request.\\n- `onResponse` - This hook calculates the duration of the request and sends it to Azure Application Insights.\\n\\nIt also includes an `urlsToIgnore` option. This is an array of URLs that you don\'t want to track. For example, you might not want to track requests to the root of your application. You can pass this option when you register the plugin.\\n\\nTo make the code play nicely with TypeScript, we augment the `FastifyRequest` interface to include a `start` property. This is where we store the start time of the request that we supply to the `onResponse` hook and read from the `onRequest` hook to calculate the duration of the request.\\n\\nTo consume the plugin in a Fastify application, you can do something like this:\\n\\n```ts\\nimport appInsights from \'applicationinsights\';\\nimport Fastify, { FastifyInstance } from \'fastify\';\\n\\nimport { appInsightsPlugin } from \'./appInsightsPlugin.js\';\\n\\nlet client: appInsights.TelemetryClient | undefined;\\n\\nif (process.env.APPLICATIONINSIGHTS_CONNECTION_STRING) {\\n  // https://github.com/microsoft/applicationinsights-node.js?tab=readme-ov-file#configuration\\n  appInsights\\n    .setup(process.env.APPLICATIONINSIGHTS_CONNECTION_STRING)\\n    .setAutoCollectRequests(true)\\n    .setAutoCollectPerformance(true, true)\\n    .setAutoCollectExceptions(true)\\n    .setAutoCollectDependencies(true)\\n    .setAutoCollectConsole(true, true) // this will enable console logging\\n    .setAutoCollectPreAggregatedMetrics(true)\\n    .setSendLiveMetrics(false)\\n    .setInternalLogging(false, true)\\n    .enableWebInstrumentation(false)\\n    .start();\\n\\n  client = appInsights.defaultClient;\\n}\\n\\nexport const fastify: FastifyInstance = Fastify({\\n  logger: true,\\n});\\n\\nfastify.register(appInsightsPlugin, { client, urlsToIgnore: [\'/\'] });\\n```\\n\\nThis both sets up Azure Application Insights and registers the Fastify plugin. The `urlsToIgnore` option is set to `[\'/\']` which means that requests to the root of the application will not be tracked.\\n\\nWith this in place you\'ll see traffic in Azure Application Insights:\\n\\n![Screenshot of requests showing up in Application Insights](screenshot-app-insights-requests.png)\\n\\n## Conclusion\\n\\nUsing Azure Application Insights with Node.js is straightforward. However, if you\'re using Fastify, you\'ll need to track requests manually. This post shows you how to do that with a Fastify plugin. I hope you find it useful!"},{"id":"get-service-connections-with-azure-devops-api","metadata":{"permalink":"/get-service-connections-with-azure-devops-api","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2025-01-25-get-service-connections-with-azure-devops-api/index.md","source":"@site/blog/2025-01-25-get-service-connections-with-azure-devops-api/index.md","title":"Get Service Connections with the Azure DevOps API (REST and TypeScript)","description":"Learn how to get service connections with the Azure DevOps REST API using both curl and TypeScript.","date":"2025-01-25T00:00:00.000Z","tags":[{"inline":false,"label":"Azure Pipelines","permalink":"/tags/azure-pipelines","description":"The Azure Pipelines CI / CD service."},{"inline":false,"label":"Azure DevOps","permalink":"/tags/azure-devops","description":"The Azure DevOps suite of tools."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":3.755,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"get-service-connections-with-azure-devops-api","title":"Get Service Connections with the Azure DevOps API (REST and TypeScript)","authors":"johnnyreilly","tags":["azure pipelines","azure devops","typescript"],"image":"./title-image.png","hide_table_of_contents":false,"description":"Learn how to get service connections with the Azure DevOps REST API using both curl and TypeScript."},"unlisted":false,"prevItem":{"title":"Node.js, Azure Application Insights, and Fastify","permalink":"/nodejs-azure-appinsights-fastify"},"nextItem":{"title":"Slash command your deployment with GitHub Actions","permalink":"/slash-command-your-deployment-with-github-actions"}},"content":"If you work with Azure Pipelines, you\'ll likely have come upon the need to create service connections. These are the connections to external services that your pipelines need to run. You can interrogate these connections using the Azure DevOps REST API. This post goes through how to do this; both using curl and using TypeScript.\\n\\n![title image reading \\"Get Service Connections with the Azure DevOps API\\" with the Azure DevOps logo](title-image.png)\\n\\nI\'m writing this post because when I attempted to use the [Azure DevOps Client for Node.js](https://github.com/microsoft/azure-devops-node-api) package to acquire them I found it lacking, and [not for the first time](../2021-05-08-create-pipeline-with-azure-devops-api/index.md). I am going to allow myself a little moan here; ever since Microsoft acquired GitHub, the Azure DevOps ecosystem feels like it has had insufficient investment.\\n\\nHowever, as is often the case, there is a way. The Azure DevOps REST API is there for us, and with a little `fetch` we can get the job done.\\n\\n\x3c!--truncate--\x3e\\n\\n## Service connections vs service endpoints\\n\\nBefore we get into the code, let\'s clarify the terminology. In Azure DevOps, service connections are the connections to external services that your pipelines need to run. However, the Azure DevOps REST API refers to these as \\"service endpoints\\".\\n\\n![Screenshot of service connections in the Azure DevOps UI](screenshot-azure-devops-service-connections.webp)\\n\\nSo when you\'re looking the screenshot above and you see \\"service connections\\", remember that in the API they\'re referred to as \\"service endpoints\\". If there is an actual distinction between \\"service connections\\" and \\"service endpoints\\" I\'m not aware of it. If you know, please do let me know!\\n\\n## Curling service connections\\n\\nOnce you know that service connections are referred to as \\"service endpoints\\" in the Azure DevOps REST API, you can use the [documentation](https://learn.microsoft.com/en-us/rest/api/azure/devops/serviceendpoint/endpoints/get-service-endpoints?view=azure-devops-rest-7.2&tabs=HTTP) to get them. Here\'s a curl to get you started:\\n\\n```bash\\ncurl  --user \'\':\'PERSONAL_ACCESS_TOKEN\' --header \\"Content-Type: application/json\\" --header \\"Accept:application/json\\" https://dev.azure.com/{organization}/{project}/_apis/serviceendpoint/endpoints?api-version=7.1\\n```\\n\\n## What if I want to use TypeScript?\\n\\nNow that we can see there\'s a way to get service connections with curl, let\'s look at how we can do this with TypeScript. Effectively we\'ll want to `fetch` (instead of using curl) and statically type the response with some interfaces. Here\'s a function that will retrieve service connections:\\n\\n```ts\\nexport async function getAzureDevOpsServiceConnections({\\n  personalAccessToken,\\n  organization,\\n  projectName,\\n}: {\\n  /** requires the vso.serviceendpoint scope which grants the ability to read service endpoints / service connections */\\n  personalAccessToken: string;\\n  /** eg \\"johnnyreilly\\" */\\n  organization: string;\\n  /** eg \\"blog-demos\\" */\\n  projectName: string;\\n}): Promise<ServiceConnection[]> {\\n  // https://learn.microsoft.com/en-us/rest/api/azure/devops/serviceendpoint/endpoints/get-service-endpoints?view=azure-devops-rest-7.1&tabs=HTTP\\n  const url = `https://dev.azure.com/${organization}/${projectName}/_apis/serviceendpoint/endpoints?api-version=7.1`;\\n\\n  const response = await fetch(url, {\\n    method: \'GET\',\\n    headers: {\\n      Accept: \'application/json\',\\n      \'Content-Type\': \'application/json\',\\n      Authorization: `Basic ${Buffer.from(`PAT:${personalAccessToken}`).toString(\'base64\')}`,\\n      \'X-TFS-FedAuthRedirect\': \'Suppress\',\\n    },\\n  });\\n\\n  if (!response.ok) {\\n    throw new Error(`HTTP error! status: ${response.status.toString()}`);\\n  }\\n\\n  const data = (await response.json()) as Wrapper;\\n\\n  return data.value;\\n}\\n\\ninterface Wrapper {\\n  count: number;\\n  value: ServiceConnection[];\\n}\\n\\nexport interface ServiceConnection {\\n  authorization: Authorization;\\n  createdBy: CreatedBy;\\n  data: Record<string, null | string>;\\n  description: string;\\n  id: string;\\n  isOutdated: boolean;\\n  isReady: boolean;\\n  isShared: boolean;\\n  modificationDate?: string;\\n  modifiedBy?: ModifiedBy;\\n  name: string;\\n  owner: string;\\n  serviceEndpointProjectReferences: ServiceEndpointProjectReference[];\\n  serviceManagementReference: null;\\n  type: string;\\n  url: string;\\n}\\n\\nexport interface CreatedBy {\\n  _links: Links;\\n  descriptor: string;\\n  displayName: string;\\n  id: string;\\n  imageUrl: string;\\n  uniqueName: string;\\n  url: string;\\n}\\n\\nexport interface Links {\\n  avatar: Avatar;\\n}\\n\\nexport interface Avatar {\\n  href: string;\\n}\\n\\nexport interface Authorization {\\n  parameters: Record<string, null | string>;\\n  scheme: string;\\n}\\n\\nexport interface ServiceEndpointProjectReference {\\n  description: string;\\n  name: string;\\n  projectReference: ProjectReference;\\n}\\n\\nexport interface ProjectReference {\\n  id: string;\\n  name: string;\\n}\\n\\nexport interface ModifiedBy {\\n  displayName: null | string;\\n  id: string;\\n}\\n```\\n\\nThe function `getAzureDevOpsServiceConnections` is the one you\'ll want to call. It takes the following inputs:\\n\\n- `personalAccessToken`: the personal access token you\'ve created in Azure DevOps with the `vso.serviceendpoint` scope. You could equally use the [`System.AccessToken`](https://learn.microsoft.com/en-us/azure/devops/pipelines/build/variables?view=azure-devops&tabs=yaml#systemaccesstoken) that\'s available in your pipeline if that was appropriate.\\n- `organization`: the name of your Azure DevOps organization\\n- `projectName`: the name of the project you\'re interested in\\n\\nThe function returns an array of `ServiceConnection` objects. You\'ll note that the API actually returns a `Wrapper` object that contains a `count` and an array of `ServiceConnection` objects. This isn\'t actually useful for my purposes, so I\'ve just returned the array of `ServiceConnection` objects.\\n\\n## Conclusion\\n\\nHonestly the hardest part about writing this post was being sure that \\"service connections\\" and \\"service endpoints\\" were the same thing. Truly naming things is hard.\\n\\nHopefully this post has helped you get the service connections you need. And as I mentioned earlier, if you know an actual distinction between \\"service connections\\" and \\"service endpoints\\" please do let me know!"},{"id":"slash-command-your-deployment-with-github-actions","metadata":{"permalink":"/slash-command-your-deployment-with-github-actions","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2025-01-02-slash-command-your-deployment-with-github-actions/index.md","source":"@site/blog/2025-01-02-slash-command-your-deployment-with-github-actions/index.md","title":"Slash command your deployment with GitHub Actions","description":"Slash commands are a great way to interact with your GitHub issues. In this post, we look at how to implement a `/deploy` slash command to deploy an Azure Container Apps service with GitHub Actions.","date":"2025-01-02T00:00:00.000Z","tags":[{"inline":false,"label":"GitHub Actions","permalink":"/tags/github-actions","description":"The GitHub Actions CI / CD service."},{"inline":false,"label":"Azure Container Apps","permalink":"/tags/azure-container-apps","description":"The Azure Container Apps service. Effectively a managed Kubernetes service."}],"readingTime":11.31,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"slash-command-your-deployment-with-github-actions","title":"Slash command your deployment with GitHub Actions","authors":"johnnyreilly","tags":["github actions","azure container apps"],"image":"./title-image.png","hide_table_of_contents":false,"description":"Slash commands are a great way to interact with your GitHub issues. In this post, we look at how to implement a `/deploy` slash command to deploy an Azure Container Apps service with GitHub Actions."},"unlisted":false,"prevItem":{"title":"Get Service Connections with the Azure DevOps API (REST and TypeScript)","permalink":"/get-service-connections-with-azure-devops-api"},"nextItem":{"title":"Smuggling .gitignore, .npmrc and friends in npm packages","permalink":"/smuggling-gitignore-npmrc-in-npm-packages"}},"content":"In the world of computing, slash commands have a proud and noble history. They are a way to interact with a system by typing a command into a chat or terminal, usually with a `/` preceding the command; hence the name \\"slash commands\\". [GitHub has its own slash commands](https://docs.github.com/en/issues/tracking-your-work-with-issues/using-issues/about-slash-commands) that you can use in issues and pull requests to add code blocks and tables etc. The slash commands are, in truth, quite limited.\\n\\nHowever, through clever use of the GitHub Actions platform, it\'s possible to build something quite powerful which is \\"slash-command-shaped\\". In this post, we\'ll look at how to implement a `/deploy` slash command which, when invoked in a pull request, will deploy an Azure Container App with GitHub Actions.\\n\\n![title image reading \\"Slash command your deployment with GitHub Actions\\" with the GitHub Actions logo](title-image.png)\\n\\nThe technique we\'ll use is covering a deployment usecase, as we\'ll see, it could be adapted to many other scenarios.\\n\\n\x3c!--truncate--\x3e\\n\\n## First a bit about nuns\\n\\nI have an aunt that is a Poor Clare nun, and I\'ve been [over-engineering her convent\'s website for years](../2015-02-11-the-convent-with-continuous-delivery/index.md). Most of the time the site moulders away, but every now and then I get a flurry of requests for minor changes. Once I\'ve made the changes, they go live thanks to the magic of continuous deployment. But there\'s only ever been a single environment; production or \\"main\\".\\n\\nSometimes I\'d like to eyeball a change before I\'ve shipped it. Not always, sometimes. A particular case where this is useful, is when [Renovate](https://www.mend.io/renovate/) has submitted a dependency upgrade PR, and I\'d like to see the impact without having to install and run it locally somewhere. Because, unless I instead hit \\"merge\\" with crossed fingers, that\'s what I\'ll need to do. (I have done this and it doesn\'t always end well.)\\n\\nSo I decided it was time that the \\"Convent with Continuous Delivery\u2122\uFE0F\\" had a staging environment. And I decided that I\'d like to be able to deploy to it by entering the slash command `/deploy` in a pull request comment. Like this:\\n\\n![screenshot of pull request comments](screenshot-pull-request-comments.webp)\\n\\nAs we can see, I entered `/deploy` in a comment. In response, a GitHub Actions workflow then kicked off and deployed the staging environment. How did I do this? Let\'s find out.\\n\\n## The GitHub Actions workflow\\n\\nThe secret sauce that makes implementing slash commands in GitHub Actions possible is the [`issue_comment` event](https://docs.github.com/en/actions/writing-workflows/choosing-when-your-workflow-runs/events-that-trigger-workflows#issue_comment). This event is triggered when an issue or pull request comment is created, edited, or deleted. We\'re interested in the situation where a pull request comment is created, and it contains the `/deploy` command.\\n\\nBased upon the [example here](https://docs.github.com/en/actions/writing-workflows/choosing-when-your-workflow-runs/events-that-trigger-workflows#issue_comment-on-issues-only-or-pull-requests-only) it\'s possible to create a workflow that is triggered by the `issue_comment` event, but only when the comment is on a pull request, and that comment contains the text `/deploy`.\\n\\nHere\'s the workflow:\\n\\n```yaml\\non:\\n  issue_comment:\\n    types: [created]\\n\\njobs:\\n  run-for-pr-comment-with-deploy-command:\\n    # check if the comment comes from a pull request and contains the command `/deploy`\\n    if: github.event.issue.pull_request && contains(github.event.comment.body, \'/deploy\')\\n    # ...\\n```\\n\\nThe `if` statement is the key to this workflow. It checks if the comment comes from a pull request and contains the command `/deploy`. If both conditions are met, the workflow continues. We\'re in business!\\n\\n## Avoiding duplication with a reusable workflow\\n\\nI already have a GitHub Actions workflow that deploys the main environment. I don\'t want to duplicate this logic in the new workflow. Instead, I want to reuse the existing workflow and just pass in a different environment name. This is where [reusable workflows](https://docs.github.com/en/actions/learn-github-actions/reusing-workflows) come in.\\n\\nI think of these as functions that can be called from other workflows. They have inputs and outputs, and can be parameterised.\\n\\nI migrated the deployment logic to a reusable workflow called `util-build-and-deploy.yaml`. I pondered the best way to share this information with you, and I\'ve finally opted to include the entire workflow here. It\'s a bit long, but I think it\'s the best way to show you how it all fits together:\\n\\n```yaml\\nname: Build and deploy\\n\\non:\\n  workflow_call:\\n    inputs:\\n      deploy:\\n        required: true\\n        type: boolean\\n      branchName:\\n        required: true\\n        type: string\\n\\n    outputs:\\n      containerAppUrl:\\n        description: \'The URL of the deployed container app\'\\n        value: ${{ jobs.deploy.outputs.containerAppUrl }}\\n\\nenv:\\n  RESOURCE_GROUP: rg-my-convent\\n  REGISTRY: ghcr.io\\n\\njobs:\\n  build:\\n    runs-on: ubuntu-latest\\n    permissions:\\n      contents: read\\n      packages: write\\n    outputs:\\n      image-name: ${{ steps.vars.outputs.image_name }}\\n      sha-short: ${{ steps.vars.outputs.sha_short }}\\n      built-at: ${{ steps.vars.outputs.built_at }}\\n    steps:\\n      - name: Checkout repository\\n        uses: actions/checkout@v4\\n        with:\\n          ref: ${{ inputs.branchName }}\\n\\n      - name: Set sha-short and image-name environment variables\\n        id: vars\\n        run: |\\n          image_name=$(echo \\"${{ env.REGISTRY }}/${{ github.repository }}/node-service\\" | tr \'[:upper:]\' \'[:lower:]\')\\n          echo \\"image_name=$image_name\\" >> $GITHUB_OUTPUT\\n          sha_short=$(echo \\"$(git rev-parse --short HEAD)\\" | tr \'[:upper:]\' \'[:lower:]\')\\n          echo \\"sha_short=$sha_short\\" >> $GITHUB_OUTPUT\\n          echo \\"built_at=$(date +\'%Y-%m-%dT%H:%M:%S\')\\" >> $GITHUB_OUTPUT\\n\\n      # Login against a Docker registry\\n      # https://github.com/docker/login-action\\n      - name: Log into registry ${{ env.REGISTRY }}\\n        uses: docker/login-action@v3\\n        with:\\n          registry: ${{ env.REGISTRY }}\\n          username: ${{ github.actor }}\\n          password: ${{ secrets.GITHUB_TOKEN }}\\n\\n      # Extract metadata (tags, labels) for Docker\\n      # https://github.com/docker/metadata-action\\n      - name: Extract Docker metadata\\n        id: meta\\n        uses: docker/metadata-action@v5\\n        with:\\n          images: ${{ steps.vars.outputs.image_name }}\\n          context: git # so it uses the git branch that is checked out\\n          tags: |\\n            type=semver,pattern={{version}}\\n            type=semver,pattern={{major}}.{{minor}}\\n            type=semver,pattern={{major}}\\n            type=ref,event=branch\\n            type=ref,event=pr\\n            type=sha\\n\\n      # Build and push Docker image with Buildx (don\'t push if deploy is false)\\n      # https://github.com/docker/build-push-action\\n      - name: Build and push Docker image\\n        uses: docker/build-push-action@v6\\n        with:\\n          context: ./\\n          push: ${{ inputs.deploy }}\\n          tags: ${{ steps.meta.outputs.tags }}\\n          labels: ${{ steps.meta.outputs.labels }}\\n          build-args: |\\n            VITE_BRANCH_NAME=${{ inputs.branchName }}\\n            VITE_GIT_SHA=${{ steps.vars.outputs.sha_short }}\\n            VITE_BUILT_AT=${{ steps.vars.outputs.built_at }}\\n\\n  deploy:\\n    runs-on: ubuntu-latest\\n    if: inputs.deploy == true\\n    needs: build\\n    outputs:\\n      containerAppUrl: ${{ steps.deploy.outputs.CONTAINER_APP_URL }}\\n    permissions:\\n      id-token: write\\n      contents: read\\n      packages: write\\n    steps:\\n      - name: Checkout repository\\n        uses: actions/checkout@v4\\n        with:\\n          ref: ${{ inputs.branchName }}\\n\\n      - name: Azure login\\n        uses: azure/login@v2\\n        with:\\n          client-id: ${{ secrets.AZURE_CLIENT_ID }}\\n          tenant-id: ${{ secrets.AZURE_TENANT_ID }}\\n          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}\\n\\n      - name: Deploy to Azure\\n        id: deploy\\n        uses: azure/CLI@v2\\n        with:\\n          inlineScript: |\\n            RESOURCE_GROUP=\\"${{ env.RESOURCE_GROUP }}\\"\\n            BUILT_AT=\\"${{ needs.build.outputs.built-at }}\\"\\n            BRANCH_NAME=\\"${{ inputs.branchName }}\\"\\n            SHA_SHORT=\\"${{ needs.build.outputs.sha-short }}\\"\\n            REF_SHA=\\"${{ inputs.branchName }}.${{ needs.build.outputs.sha-short }}\\"\\n            DEPLOYMENT_NAME=\\"${REF_SHA////-}\\"\\n            echo \\"DEPLOYMENT_NAME=$DEPLOYMENT_NAME\\"\\n\\n            webServiceImage=\\"${{ needs.build.outputs.image-name }}:sha-$SHA_SHORT\\"\\n            echo \\"webServiceImage=$webServiceImage\\"\\n\\n            if [ \\"$BRANCH_NAME\\" == \\"main\\" ]; then\\n              webServiceContainerAppName=\\"main-web\\"\\n            else\\n              webServiceContainerAppName=\\"preview-web\\"\\n            fi\\n            echo \\"webServiceContainerAppName=$webServiceContainerAppName\\"\\n\\n            TAGS=\'{\\"owner\\":\\"johnnyreilly\\", \\"email\\":\\"johnny_reilly@hotmail.com\\"}\'\\n            az deployment group create \\\\\\n              --resource-group $RESOURCE_GROUP \\\\\\n              --name \\"$DEPLOYMENT_NAME\\" \\\\\\n              --template-file ./infra/main.bicep \\\\\\n              --parameters \\\\\\n                  webServiceImage=\\"$webServiceImage\\" \\\\\\n                  containerRegistry=${{ env.REGISTRY }} \\\\\\n                  containerRegistryUsername=${{ github.actor }} \\\\\\n                  containerRegistryPassword=${{ secrets.PACKAGES_TOKEN }} \\\\\\n                  branchName=\\"$BRANCH_NAME\\" \\\\\\n                  gitSha=\\"$SHA_SHORT\\" \\\\\\n                  builtAt=\\"$BUILT_AT\\" \\\\\\n                  workspaceName=\'shared-log-analytics\' \\\\\\n                  appInsightsName=\'shared-app-insights\' \\\\\\n                  managedEnvironmentName=\'shared-env\' \\\\\\n                  webServiceContainerAppName=\\"$webServiceContainerAppName\\" \\\\\\n                  tags=\\"$TAGS\\" \\\\\\n                  APPSETTINGS_API_KEY=\\"${{ secrets.APPSETTINGS_API_KEY }}\\" \\\\\\n                  APPSETTINGS_DOMAIN=\\"${{ vars.APPSETTINGS_DOMAIN }}\\" \\\\\\n                  APPSETTINGS_PRAYER_REQUEST_FROM_EMAIL=\\"${{ vars.APPSETTINGS_PRAYER_REQUEST_FROM_EMAIL }}\\" \\\\\\n                  APPSETTINGS_PRAYER_REQUEST_RECIPIENT_EMAIL=\\"${{ vars.APPSETTINGS_PRAYER_REQUEST_RECIPIENT_EMAIL }}\\"\\n\\n            CONTAINER_APP_URL=$(az containerapp show \\\\\\n              --resource-group \\"$RESOURCE_GROUP\\" \\\\\\n              --name \\"$webServiceContainerAppName\\" \\\\\\n              --query properties.configuration.ingress.fqdn \\\\\\n              --output tsv)\\n\\n            echo \\"CONTAINER_APP_URL=$CONTAINER_APP_URL\\"\\n            echo \\"CONTAINER_APP_URL=$CONTAINER_APP_URL\\" >> $GITHUB_OUTPUT\\n```\\n\\nLet\'s talk through what this workflow does:\\n\\n- It\'s triggered by a `workflow_call` event, which is how reusable workflows are triggered.\\n- It has two jobs: `build` and `deploy` and the `deploy` job is only run if the `deploy` input is `true`. (This allows us to call the workflow with `deploy: false` to only build the image)\\n- The `build` job checks out the code, sets some environment variables, logs into the Docker registry, extracts metadata for Docker, builds and pushes the Docker image.\\n- The `deploy` job checks out the code, logs into Azure, and deploys the container app to Azure Container Apps. It then outputs the URL of the deployed container app in order that we can display it to the user.\\n\\nNow most of this workflow is the same as the one I was originally using to deploy to the main environment. The key difference is it is now parameterised with the `branchName` input. This is important for two reasons:\\n\\n1. It allows us to deploy to different environments based on the branch name. In our case we\'ll deploy to the `preview-web` container app if the branch name is not `main`. Otherwise we\'ll deploy to `main-web`.\\n2. (and this is more subtle) We need to checkout the relevant branch of the repository in our workflow, so that we\'re building and deploying the correct thing. So you\'ll see us use the `branchName` input in the `actions/checkout` steps and you\'ll see us use `context: git` in the `docker/metadata-action` step.\\n\\nYou might be thinking at this point, \\"fine - but I don\'t have a containerised application and I don\'t have an Azure Container Apps service to deploy to\\". That\'s great! You can adapt this workflow to build any type of app you would like and deploy to any type of service. The crucial part is that you must build and deploy the code for the correct branch. This is why we pass the `branchName` input to the workflow.\\n\\nAnd now some bad news: the `issue_comment` event **doesn\'t** know the branch that the pull request is for. We\'re going to need to build _another_ reusable workflow that we will use to determine the branch name of the pull request.\\n\\n## Getting the branch name of the pull request\\n\\nNow we\'re old hands at creating reusable workflows, we\'re going to create another one that will determine the branch name of the pull request. We\'ll call this workflow `util-get-pr-branch-name.yaml`:\\n\\n```yaml\\nname: Get PR branch name\\n\\non:\\n  workflow_call:\\n    inputs:\\n      pullRequestNumber:\\n        required: true\\n        type: number\\n\\n    outputs:\\n      branchName:\\n        description: \'The source branch name for the pull request\'\\n        value: ${{ jobs.get-pr-branch-name.outputs.branchName }}\\n\\njobs:\\n  get-pr-branch-name:\\n    runs-on: ubuntu-latest\\n    outputs:\\n      branchName: ${{ steps.get-pr-branch-name.outputs.branchName }}\\n    steps:\\n      - id: get-pr-branch-name\\n        run: |\\n          branchName=$(gh pr view ${{ inputs.pullRequestNumber }} --json \\"headRefName\\" --jq \\".headRefName\\" --repo ${{ github.repository }})\\n          echo \\"branchName=$branchName\\" >> $GITHUB_OUTPUT\\n        env:\\n          GH_TOKEN: ${{ github.token }}\\n```\\n\\nThis is fairly self explanatory. The workflow takes a `pullRequestNumber` input and outputs the `branchName` of the pull request. It uses the `gh` CLI to get the branch name of the pull request using the `headRefName` property of a pull request. (Incidentally, the `env: GH_TOKEN: ${{ github.token }}` line is important as it allows the workflow to authenticate with GitHub.)\\n\\n## Putting it all together\\n\\nNow we have our two reusable workflows, we can put them together in a workflow that is triggered by the `issue_comment` event. This workflow will call the `util-get-pr-branch-name.yaml` workflow to get the branch name of the pull request, and then call the `util-build-and-deploy.yaml` workflow to build and deploy the code for that branch. Here\'s the `pull-request-commands.yaml` workflow:\\n\\n```yaml\\nname: Pull request commands\\n\\non:\\n  issue_comment:\\n    types: [created]\\n\\njobs:\\n  get-pr-branch-name:\\n    uses: ./.github/workflows/util-get-pr-branch-name.yaml\\n    with:\\n      pullRequestNumber: ${{ github.event.issue.number }}\\n\\n  pre-deploy:\\n    # check if the comment comes from a pull request and contains the command `/deploy`\\n    if: github.event.issue.pull_request && contains(github.event.comment.body, \'/deploy\')\\n    runs-on: ubuntu-latest\\n    steps:\\n      - run: |\\n          gh issue comment ${{ github.event.issue.number }} --body \\"Preview environment [deploying](https://github.com/johnnyreilly/poorclaresarundel-aca/actions/runs/${{ github.run_id }})...\\" --repo ${{ github.repository }}\\n        env:\\n          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\\n\\n  deploy:\\n    # check if the comment comes from a pull request and contains the command `/deploy`\\n    if: github.event.issue.pull_request && contains(github.event.comment.body, \'/deploy\')\\n    needs: [get-pr-branch-name, pre-deploy]\\n    uses: ./.github/workflows/util-build-and-deploy.yaml\\n    with:\\n      deploy: true\\n      branchName: ${{ needs.get-pr-branch-name.outputs.branchName }}\\n    secrets: inherit\\n\\n  post-deploy:\\n    runs-on: ubuntu-latest\\n    needs: deploy\\n    steps:\\n      - run: |\\n          gh issue comment ${{ github.event.issue.number }} --body \\"Preview environment deployed: https://${{ needs.deploy.outputs.containerAppUrl }}\\" --repo ${{ github.repository }}\\n        env:\\n          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\\n```\\n\\nWhat you\'re hopefully gleaning from the above is that we have 4 jobs in this workflow:\\n\\n- `get-pr-branch-name` - this job calls the `util-get-pr-branch-name.yaml` workflow to get the branch name of the pull request. Note that we pass the `github.event.issue.number` as the `pullRequestNumber` input.\\n- `pre-deploy` - this job runs immediately to post a comment in the pull request to let the user know that the preview environment is being deployed. (Again using the GitHub CLI.) The comment gives the user feedback that the command has been received and is being actioned. Based upon my experience, this response will show up in the pull request 5-10 seconds after the `/deploy` command is entered. Not as fast as I\'d like, but reasonable. For bonus points, I\'ve chosen to include a link to the GitHub Actions run that is deploying the preview environment. This is useful as it allows the user to see the progress of the deployment.\\n- `deploy` - this job calls the `util-build-and-deploy.yaml` workflow to build and deploy the code for the branch. Note that we pass the `branchName` input to the workflow using the `get-pr-branch-name.outputs.branchName` output. Note also that we\'re passing `deploy: true` to the workflow to ensure that the code is deployed, and that we\'re inheriting the secrets from the parent workflow. This is important as it allows the child workflow to access the secrets it needs to deploy the code.\\n- `post-deploy` - this job posts a comment in the pull request to let the user know that the preview environment has been deployed and where they can find it.\\n\\nOr maybe I should have said it better as a screenshot:\\n\\n![screenshot of pull request comments](screenshot-pull-request-comments.webp)\\n\\nYup! That\'s the same screenshot as before. I\'m just showing it again to remind you that this is what we\'ve built.\\n\\n## Writing other slash commands\\n\\nWe\'ve written a slash command for deployment in this post, but you could write a slash command for anything you like. The key is to use the `issue_comment` event to trigger the workflow, and to check the comment body for the command you\'re interested in. You could pass more information in the comment body than just the slash command. For example, you could pass the name of the environment you want to deploy to, or the version of the app you want to deploy. You could even pass multiple commands in a single comment. The world is your oyster!\\n\\nYou can then call other workflows to do the heavy lifting for you, remembering to pass in any inputs that are needed.\\n\\nIf you would like to see the repo where this was implemented, [look here](https://github.com/johnnyreilly/poorclaresarundel-aca/tree/f052dd2f5d55bcec8547624e928bbf90432f3872)."},{"id":"smuggling-gitignore-npmrc-in-npm-packages","metadata":{"permalink":"/smuggling-gitignore-npmrc-in-npm-packages","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2024-12-22-smuggling-gitignore-npmrc-in-npm-packages/index.md","source":"@site/blog/2024-12-22-smuggling-gitignore-npmrc-in-npm-packages/index.md","title":"Smuggling .gitignore, .npmrc and friends in npm packages","description":"The npm publish command will not just package up .gitignore and .npmrc files. This post shows how to use zipping and unzipping with postinstall and prepare scripts to include these files into your npm package.","date":"2024-12-22T00:00:00.000Z","tags":[{"inline":false,"label":"Node.js","permalink":"/tags/node-js","description":"The Node.js runtime."}],"readingTime":4.64,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"smuggling-gitignore-npmrc-in-npm-packages","title":"Smuggling .gitignore, .npmrc and friends in npm packages","authors":"johnnyreilly","tags":["node.js"],"image":"./title-image.png","hide_table_of_contents":false,"description":"The npm publish command will not just package up .gitignore and .npmrc files. This post shows how to use zipping and unzipping with postinstall and prepare scripts to include these files into your npm package."},"unlisted":false,"prevItem":{"title":"Slash command your deployment with GitHub Actions","permalink":"/slash-command-your-deployment-with-github-actions"},"nextItem":{"title":"npx and Azure Artifacts: the secret CLI delivery mechanism","permalink":"/npx-and-azure-artifacts-the-secret-cli-delivery-mechanism"}},"content":"I recently needed to include a number of `.gitignore` and `.npmrc` files in an npm package. I was surprised to find that the `npm publish` command strips these out of the published package by default. As a consequence, This broke my package, and so I needed to find a way to get round this shortcoming.\\n\\nI ended up using zipping and unzipping with `postinstall` and `prepare` scripts to include these files into my npm package.\\n\\n![title image reading \\"Smuggling dotfiles in npm packages\\" with the Node.js and npm logos](title-image.png)\\n\\nThis post shows how to use zipping and unzipping with `postinstall` and `prepare` scripts to include these files into your npm package.\\n\\n\x3c!--truncate--\x3e\\n\\n## A little backstory\\n\\nI\'m currently beavering away on a \\"create-\\\\*-app\\" tool that generates new projects from a number of available templates. That tool takes the form of a CLI tool built with TypeScript, published as a package to an npm registry and consumed with `npx`. Significantly, the templates that ship with the CLI take the form of a `templates` folder in the package, and the folders in those templates include `.npmrc` and `.gitignore` files; which are key to the functionality of the templates.\\n\\nWhen publishing my npm package, I discovered that the `.npmrc` and `.gitignore` files in subfolders were being stripped from the package. After a little research, I happened upon this [GitHub issue about npm](https://github.com/npm/npm/issues/3763) which describes some of the behaviour I was seeing. After a touch more digging, I came to understand that this behaviour is a result of npm treating the `.gitignore` and `.npmrc` files as configuration files rather than part of the package\'s intended content.\\n\\nHowever, given these files are essential to the templates\' functionality, I needed to find a way to include them in the package.\\n\\nI mused with explicitly including the specific files in the `files` section of the `package.json` file, but this would have been a maintenance headache. I wanted a more automated solution. Given that I have a single \\"special\\" folder called `templates` that contains all the templates, I pondered whether I could zip the folder on publish and unzip it on install. This would allow me to include the `.gitignore` and `.npmrc` files in the templates and have them copied into the new project when the template was used. And if there was another other curious behaviour around publishing, this solution should cover that too.\\n\\n## `prepare` and `postinstall` scripts\\n\\nI achieved this by using `prepare` and `postinstall` scripts in the `package.json` file.\\n\\nThe `prepare` and `postinstall` scripts are two of the lifecycle scripts that npm runs when installing a package. The `prepare` script runs before the package is packaged and published, and the `postinstall` script runs after the package is installed. I opted to use these scripts to zip and unzip the `templates` folder in my package.\\n\\nI performed the actual zipping and unzipping with some Node.js scripts. We\'ll look into the implementation of these scripts in a moment, but first please note the scripts we added to the `package.json` file:\\n\\n```json\\n  \\"scripts\\": {\\n    \\"postinstall\\": \\"node ./scripts/postinstall.js\\",\\n    \\"prepare\\": \\"node ./scripts/prepare.js\\"\\n  },\\n```\\n\\nThese scripts contain the paths to the Node.js scripts that perform the zipping and unzipping. The `postinstall` script runs after the package is installed, and the `prepare` script runs before the package is packaged and published.\\n\\nWhen it comes to zipping and unzipping, I used the [`adm-zip`](https://github.com/cthackers/adm-zip) package. This package provides a simple API for zipping and unzipping files and folders.\\n\\n## `prepare.js`\\n\\nWe\'ll first look at the `prepare.js` script. This script zips the `templates` folder in the package into a `templates.zip` file. The script then writes the zip file to the package\'s root directory.\\n\\n```js\\nimport AdmZip from \'adm-zip\';\\nimport fs from \'node:fs\';\\nimport { fileURLToPath } from \'node:url\';\\n\\nfunction packTemplates() {\\n  console.log(\'prepare running - packing templates\');\\n\\n  const templatesZipPath = fileURLToPath(\\n    new URL(\'../templates.zip\', import.meta.url),\\n  );\\n  const templatesDir = fileURLToPath(new URL(\'../templates\', import.meta.url));\\n\\n  const zip = new AdmZip();\\n\\n  console.log(`removing existing ${templatesZipPath}`);\\n  fs.rmSync(templatesZipPath, {\\n    force: true,\\n  });\\n\\n  console.log(`adding ${templatesDir} to zip file`);\\n  zip.addLocalFolder(templatesDir);\\n\\n  console.log(`writing zip to ${templatesZipPath}`);\\n  zip.writeZip(templatesZipPath);\\n}\\n\\npackTemplates();\\n```\\n\\nIt also removes any existing `templates.zip` file in the package\'s root directory before creating a new one. This is to ensure that the zip file is always up to date.\\n\\n## `postinstall.js`\\n\\nNow we\'ll look at the `postinstall.js` script. This script unzips the `templates.zip` file in the package into a `templates` folder. The script then writes the unzipped folder to the package\'s root directory.\\n\\n```js\\nimport AdmZip from \'adm-zip\';\\nimport fs from \'node:fs\';\\nimport { fileURLToPath } from \'node:url\';\\n\\nfunction extractTemplates() {\\n  console.log(\'postinstall running - extracting templates\');\\n  const templatesZipPath = fileURLToPath(\\n    new URL(\'../templates.zip\', import.meta.url),\\n  );\\n  const templatesDir = fileURLToPath(new URL(\'../templates\', import.meta.url));\\n\\n  let templatesExistsAlready = true;\\n  try {\\n    fs.accessSync(templatesDir);\\n  } catch {\\n    templatesExistsAlready = false;\\n  }\\n\\n  if (templatesExistsAlready) {\\n    console.log(\'templates already extracted\');\\n    return;\\n  }\\n\\n  console.log(`extracting from ${templatesZipPath} to ${templatesDir}`);\\n\\n  const extractZip = new AdmZip(templatesZipPath);\\n  extractZip.extractAllTo(templatesDir, /* overwrite */ false);\\n\\n  console.log(\'templates extracted\');\\n}\\n\\nextractTemplates();\\n```\\n\\nYou\'ll notice that the script checks whether the `templates` folder already exists before unzipping the `templates.zip` file. This is to ensure that the folder is only unzipped once.\\n\\n## Conclusion\\n\\nSo here we have a method for including `.gitignore` and `.npmrc` files in an npm package. By using zipping and unzipping with `postinstall` and `prepare` scripts, we can include these files in the package and have them copied into the new project when the package is installed.\\n\\nMy example is a `templates` folder - yours could be anything. And likewise if you have other files that are being stripped from your package, you could use this method to include them too."},{"id":"npx-and-azure-artifacts-the-secret-cli-delivery-mechanism","metadata":{"permalink":"/npx-and-azure-artifacts-the-secret-cli-delivery-mechanism","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2024-12-08-npx-and-azure-artifacts-the-secret-cli-delivery-mechanism/index.md","source":"@site/blog/2024-12-08-npx-and-azure-artifacts-the-secret-cli-delivery-mechanism/index.md","title":"npx and Azure Artifacts: the secret CLI delivery mechanism","description":"By combining npx and Azure Artifacts, you can deliver your command line application to consumers in a way that is easy to use and secure.","date":"2024-12-08T00:00:00.000Z","tags":[{"inline":false,"label":"Azure DevOps","permalink":"/tags/azure-devops","description":"The Azure DevOps suite of tools."},{"inline":false,"label":"Node.js","permalink":"/tags/node-js","description":"The Node.js runtime."}],"readingTime":6.685,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"npx-and-azure-artifacts-the-secret-cli-delivery-mechanism","title":"npx and Azure Artifacts: the secret CLI delivery mechanism","authors":"johnnyreilly","tags":["azure devops","node.js"],"image":"./title-image.png","hide_table_of_contents":false,"description":"By combining npx and Azure Artifacts, you can deliver your command line application to consumers in a way that is easy to use and secure."},"unlisted":false,"prevItem":{"title":"Smuggling .gitignore, .npmrc and friends in npm packages","permalink":"/smuggling-gitignore-npmrc-in-npm-packages"},"nextItem":{"title":"Azure Artifacts: Publish a private npm package with Azure DevOps","permalink":"/azure-artifacts-publish-private-npm-package-with-azure-devops"}},"content":"The [`npx` command](https://docs.npmjs.com/cli/v8/commands/npx) is a powerful tool for running CLI tools shipped as npm packages, without having to install them globally. `npx` is typically used to run packages on the public npm registry. However, if you have a private npm feed, you can also use `npx` to run packages available on that feed.\\n\\nAzure Artifacts is a feature of Azure DevOps that supports publishing npm packages to a feed for consumption. (You might want to read [this guide on publishing npm packages to Azure Artifacts](../2024-12-01-azure-artifacts-publish-private-npm-package-with-azure-devops/index.md).) By combining `npx` and Azure Artifacts, you can deliver your CLI tool to consumers in a way that\'s easy to use and secure.\\n\\n![title image reading \\"Azure Artifacts: Publish a private npm package with Azure DevOps\\" with an Azure DevOps and npm logos](title-image.png)\\n\\nThis post shows how to use `npx` and Azure Artifacts to deliver your private CLI tool to consumers.\\n\\n\x3c!--truncate--\x3e\\n\\n## Why is combining `npx` with private npm feeds useful?\\n\\nIf you\'ve ever found a need to deliver a private CLI tool to consumers, you\'ll know that it can be a challenge.\\n\\nI work for a large organization and we need to share internal tools with our colleagues. The problem is, that it\'s hard to get people to install tools. Either you need to provide detailed instructions on how to acquire and install the tool, or you need to work out some kind of internal distribution mechanism. You also have to think about how to update the tool. It\'s not simple.\\n\\nBy combining `npx` and Azure Artifacts it becomes much simpler. You can publish your CLI tool to a private npm feed and then consumers can run it with a single command. They don\'t need to install anything up front (apart from Node.js which they likely already have), and they don\'t need to worry about updates.\\n\\nA typical usecase is the one I\'ve mentioned; sharing tools internally in an organisation. But, broader than that, if you want to deliver a private CLI tool to consumers, this is a great way to do it.\\n\\nWe\'re going to look at how we\'d achieve this with Azure Artifacts as the host of the npm package. But, you could use any private npm feed that you have access to.\\n\\n## Publishing a package to Azure Artifacts\\n\\nBefore you can use `npx` to run your CLI tool, you need to publish it to a private npm feed. Here is a guide on [how to publish a private npm package with Azure Artifacts](../2024-12-01-azure-artifacts-publish-private-npm-package-with-azure-devops/index.md). In that example we published a package to a feed called `npmrc-script-organization` in the `johnnyreilly` organization of Azure DevOps / Azure Artifacts.\\n\\nFor the sake of this post, we\'ll say that our package is a CLI tool with the name `@johnnyreilly/my-cli-tool`.\\n\\nRemember, an npm package which houses a CLI tool is merely an npm package with a [`bin` entry in the `package.json`](https://docs.npmjs.com/cli/v10/configuring-npm/package-json#bin). This post is not about how to create a CLI tool, but rather how to deliver one to private consumers. If you would like to see an example of what a CLI tool package looks like, you can check out the [`azdo-npm-auth` package on GitHub](https://github.com/johnnyreilly/azdo-npm-auth). (In fact, we\'ll use `azdo-npm-auth` later in this post - it\'s an example of a CLI tool published to the **public** npm registry.)\\n\\nThe question now is, how we can run the (private) `@johnnyreilly/my-cli-tool` package with `npx`?\\n\\n## The `registry` config setting of `npm` / `npx`\\n\\nThe secret sauce of running a CLI tool from a private npm feed with `npx` is the [`registry` config setting of `npm` / `npx`](https://docs.npmjs.com/cli/v8/using-npm/config#registry). The `registry` option allows you to specify the URL of the npm feed that you want to use.\\n\\nFor our case, we grabbed the registry URL from the Azure DevOps UI by clicking on the \\"Connect to Feed\\" button in the Azure Artifacts section:\\n\\n![Screenshot of \\"connect to feed\\" in Azure DevOps](screenshot-connect-to-feed.webp)\\n\\nWhen we selected `npm`, ADO displayed instructions for setting up an `.npmrc` file for private consumption:\\n\\n![Screenshot of the instructions for setting up the `.npmrc` file](screenshot-npmrc.png)\\n\\nWe don\'t need to set up an `.npmrc` file to run the CLI tool with `npx`, but we do need to grab the registry URL, which we can see in the example `.npmrc` file above. In our case, the URL is `https://pkgs.dev.azure.com/johnnyreilly/_packaging/npmrc-script-organization/npm/registry/`. This is the URL of the registry (private npm feed) that we want to use.\\n\\n## Running the CLI tool with `npx`\\n\\nEquipped with the registry URL, we can now run our CLI tool with `npx`:\\n\\n```shell\\nnpx -y --registry https://pkgs.dev.azure.com/johnnyreilly/_packaging/npmrc-script-organization/npm/registry/ @johnnyreilly/my-cli-tool\\n```\\n\\nThis command will download the `@johnnyreilly/my-cli-tool` package from the private npm feed and run it. The `--registry` option tells `npx` to use the specified registry URL to download the package and the `-y` option tells `npx` to answer \\"yes\\" to the installation prompt.\\n\\nIf you need to pass arguments to the CLI tool, you can simply add them to the end of the command as you would with any CLI tool: (I\'ll put this over multiple lines for readability, but you can run it as a single line)\\n\\n```shell\\nnpx -y \\\\\\n  --registry https://pkgs.dev.azure.com/johnnyreilly/_packaging/npmrc-script-organization/npm/registry/ \\\\\\n  @johnnyreilly/my-cli-tool --arg1 hello\\n```\\n\\nThere is another way to specify the registry URL, which is to use the `npm_config_registry` environment variable. This approach is more verbose and is not cross platform (it won\'t work on Windows). But, if you prefer this approach, you can use this style of command:\\n\\n```shell\\nnpm_config_registry=https://pkgs.dev.azure.com/johnnyreilly/_packaging/npmrc-script-organization/npm/registry/ npx -y @johnnyreilly/my-cli-tool\\n```\\n\\n## What about authentication?\\n\\nIf you encounter an error like this:\\n\\n```shell\\nnpm error code E401\\nnpm error Unable to authenticate, your authentication token seems to be invalid.\\nnpm error To correct this please try logging in again with:\\nnpm error npm login\\n```\\n\\nThen npm is telling you to authenticate with the private npm feed / registry. This is because the feed is private and requires authentication. This is a good thing; it means that your package is secure; just as you\'d hoped.\\n\\nYou may have your own way of authenticating with the feed. If so, great! Do that now and skip the next section.\\n\\n## Using `azdo-npm-auth` to authenticate with Azure Artifacts\\n\\nOn the other hand, if you\'re using Azure Artifacts ([and your Azure DevOps organisation is connected with your Azure account / Microsoft Entra ID](https://learn.microsoft.com/en-us/azure/devops/organizations/accounts/connect-organization-to-azure-ad?view=azure-devops)), you can use [`azdo-npm-auth`](https://github.com/johnnyreilly/azdo-npm-auth) to solve your authentication needs. You can run `azdo-npm-auth` like this:\\n\\n```shell\\nnpx -y azdo-npm-auth --registry https://pkgs.dev.azure.com/johnnyreilly/_packaging/npmrc-script-organization/npm/registry/\\n```\\n\\nThe above command will acquire a PAT (Personal Access Token) from Azure DevOps and use it to create a user `.npmrc` file, which will be used by `npx` to authenticate with the feed subsequently.\\n\\nIf you encounter a `npm error code E401` as you run the `azdo-npm-auth` command, it\'s possible that you have a local `.npmrc` file that is tripping `npx` up. You can get around that by explicitly passing the `--registry` of the public npm feed / registry to `npx`:\\n\\n```shell\\nnpx -y --registry https://registry.npmjs.org azdo-npm-auth --registry https://pkgs.dev.azure.com/johnnyreilly/_packaging/npmrc-script-organization/npm/registry/\\n```\\n\\nThat\'s right; we\'re passing the public npm registry to `npx`\'s `--registry` and we\'re passing our private npm feed / registry to `azdo-npm-auth`\'s `--registry`. This gets around the `npm error code E401` issue.\\n\\n## Running the original command again\\n\\nWhichever way you authenticated, you should now be ready. You can now run the original command again; it should work this time. For example:\\n\\n```shell\\nnpx -y --registry https://pkgs.dev.azure.com/johnnyreilly/_packaging/npmrc-script-organization/npm/registry/ @johnnyreilly/my-cli-tool\\n```\\n\\nAnd that\'s it! You\'ve successfully run your CLI tool from a private npm feed with `npx`.\\n\\n## Conclusion\\n\\nIn this post we\'ve used Azure Artifacts as the host of the npm package, but you could use any npm feed that you have access to. The key is to use the `registry` option of `npm` / `npx` to specify the URL of the npm feed.\\n\\nBy combining `npx` and private npm feeds, you can deliver your CLI tool to consumers in a way that\'s easy to use and secure. Consumers can run your tool with a single command, without having to install anything up front. This is a powerful way to share private CLI tools."},{"id":"azure-artifacts-publish-private-npm-package-with-azure-devops","metadata":{"permalink":"/azure-artifacts-publish-private-npm-package-with-azure-devops","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2024-12-01-azure-artifacts-publish-private-npm-package-with-azure-devops/index.md","source":"@site/blog/2024-12-01-azure-artifacts-publish-private-npm-package-with-azure-devops/index.md","title":"Azure Artifacts: Publish a private npm package with Azure DevOps","description":"Azure DevOps has a feature called Azure Artifacts that supports publishing npm packages to a feed for consumption. This post shows how to publish a private npm package with Azure DevOps.","date":"2024-12-01T00:00:00.000Z","tags":[{"inline":false,"label":"Azure DevOps","permalink":"/tags/azure-devops","description":"The Azure DevOps suite of tools."},{"inline":false,"label":"Node.js","permalink":"/tags/node-js","description":"The Node.js runtime."}],"readingTime":2.775,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-artifacts-publish-private-npm-package-with-azure-devops","title":"Azure Artifacts: Publish a private npm package with Azure DevOps","authors":"johnnyreilly","tags":["azure devops","node.js"],"image":"./title-image.png","hide_table_of_contents":false,"description":"Azure DevOps has a feature called Azure Artifacts that supports publishing npm packages to a feed for consumption. This post shows how to publish a private npm package with Azure DevOps."},"unlisted":false,"prevItem":{"title":"npx and Azure Artifacts: the secret CLI delivery mechanism","permalink":"/npx-and-azure-artifacts-the-secret-cli-delivery-mechanism"},"nextItem":{"title":"Introducing azdo-npm-auth (Azure DevOps npm auth)","permalink":"/introducing-azdo-npm-auth"}},"content":"Azure DevOps has a feature called Azure Artifacts that supports publishing npm packages to a feed for consumption. Publishing a private npm package with Azure DevOps is a common scenario for teams that want to share code across projects or organizations. This post shows how to publish a private npm package with Azure DevOps.\\n\\n![title image reading \\"Azure Artifacts: Publish a private npm package with Azure DevOps\\" with an Azure DevOps and npm logos](title-image.png)\\n\\nPublishing a private npm package with Azure DevOps is fairly straightforward, but surprisingly documentation is a little sparse.\\n\\n\x3c!--truncate--\x3e\\n\\n## What feeds are available in Azure Artifacts?\\n\\nIf you don\'t already have a feed to publish your npm package to, you can create one in Azure DevOps [by following these instructions](https://learn.microsoft.com/en-us/azure/devops/artifacts/concepts/feeds?view=azure-devops).\\n\\nIf you\'re trying to find out what feeds are available in Azure Artifacts, you can find them in the Azure DevOps UI. Go to the Artifacts section in Azure DevOps and you\'ll see a list of feeds. The URL for the feed will be in the format `https://dev.azure.com/[ORGANIZATION]/_artifacts/feed`.\\n\\nThere you\'ll see a dropdown with the feeds you have access to:\\n\\n![screenshot of the feeds in Azure DevOps](screenshot-npm-feeds-in-azure-artifacts.webp)\\n\\nYou\'ll see from the screenshot that I have access to a feed called `npmrc-script-organization`. Let\'s use that feed to publish a private npm package.\\n\\n## Setting up the `.npmrc` file\\n\\nSo that you can publish to a private feed, you need to set up an `.npmrc` file in your project. This file will contain the URL of the feed you want to publish to, and your credentials. To set up the `.npmrc` file, you can click on the \\"Connect to Feed\\" button in the Azure DevOps UI:\\n\\n![Screenshot of \\"connect to feed\\" in Azure DevOps](screenshot-connect-to-feed.webp)\\n\\nThen select `npm` and you\'ll see the instructions for setting up the `.npmrc` file:\\n\\n![Screenshot of the instructions for setting up the `.npmrc` file](screenshot-npmrc.png)\\n\\n## Publishing with Azure Pipelines\\n\\nNow we\'re ready to publish our npm package with Azure DevOps. Here\'s an example of an Azure Pipelines YAML file that publishes a private npm package:\\n\\n```yml\\ntrigger:\\n  batch: true\\n\\npool:\\n  vmImage: ubuntu-latest\\n\\nvariables:\\n  isMainBranch: ${{ eq(variables[\'Build.SourceBranch\'], \'refs/heads/main\') }}\\n\\nstages:\\n  - stage: Build_Package_Publish\\n    displayName: Build package and publish\\n    jobs:\\n      - job:\\n        steps:\\n          - task: NodeTool@0\\n            inputs:\\n              versionSpec: 20\\n            displayName: Install Node.js\\n\\n          - task: npmAuthenticate@0\\n            inputs:\\n              workingFile: $(System.DefaultWorkingDirectory)/.npmrc\\n\\n          - bash: npm install\\n            displayName: \'npm install\'\\n\\n          - bash: npm run build\\n            displayName: \'npm build\'\\n\\n          - task: Npm@1\\n            displayName: Publish Package\\n            inputs:\\n              command: \'publish\'\\n              publishRegistry: \'useFeed\'\\n              publishFeed: \'npmrc-script-organization\'\\n```\\n\\nLet\'s break down the steps in this YAML file:\\n\\n- We\'re installing Node.js and authenticating with the `.npmrc` file.\\n- We\'re running `npm install` and `npm run build`. These are standard steps for building a Node.js project; yours might vary; what\'s important is that you are able to get your built package set up.\\n- Finally, we use the `Npm@1` task to publish the package. We specify the `publishRegistry` as `useFeed` and the `publishFeed` as `npmrc-script-organization`. This is the feed we\'re publishing to.\\n\\n## Conclusion\\n\\nIn this post, we\'ve seen how to publish a private npm package with Azure DevOps. We\'ve set up the `.npmrc` file, and we\'ve used an Azure Pipelines YAML file to publish the package. This is a common scenario for teams that want to share code across projects or organizations. I hope this post has been helpful to you!"},{"id":"introducing-azdo-npm-auth","metadata":{"permalink":"/introducing-azdo-npm-auth","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2024-11-09-introducing-azdo-npm-auth/index.md","source":"@site/blog/2024-11-09-introducing-azdo-npm-auth/index.md","title":"Introducing azdo-npm-auth (Azure DevOps npm auth)","description":"Azure DevOps npm auth eases setting up local authentication to Azure DevOps npm feeds, particularly for non Windows users.","date":"2024-11-09T00:00:00.000Z","tags":[{"inline":false,"label":"Azure DevOps","permalink":"/tags/azure-devops","description":"The Azure DevOps suite of tools."},{"inline":false,"label":"Node.js","permalink":"/tags/node-js","description":"The Node.js runtime."}],"readingTime":5.23,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"introducing-azdo-npm-auth","title":"Introducing azdo-npm-auth (Azure DevOps npm auth)","authors":"johnnyreilly","tags":["azure devops","node.js"],"image":"./title-image.png","hide_table_of_contents":false,"description":"Azure DevOps npm auth eases setting up local authentication to Azure DevOps npm feeds, particularly for non Windows users."},"unlisted":false,"prevItem":{"title":"Azure Artifacts: Publish a private npm package with Azure DevOps","permalink":"/azure-artifacts-publish-private-npm-package-with-azure-devops"},"nextItem":{"title":"Azure DevOps API: Set User Story column with the Azure DevOps Client for Node.js","permalink":"/azure-devops-set-user-story-column-api"}},"content":"Azure DevOps has a feature called Azure Artifacts that supports publishing npm packages to a feed for consumption. Typically those npm packages are intended to be consumed by a restricted audience. To install a package published to a private feed you need to configure authentication, and for non Windows users this is a convoluted process.\\n\\n![title image reading \\"Introducing Azure DevOps npm auth\\" with an Azure DevOps and npm logos](title-image.png)\\n\\n[`azdo-npm-auth`](https://github.com/johnnyreilly/azdo-npm-auth) exists to ease the setting up of local authentication to Azure DevOps npm feeds, particularly for non Windows users.\\n\\n\x3c!--truncate--\x3e\\n\\n## What problem are we solving?\\n\\nConsider the onboarding process for a Windows user for consuming an Azure Artifact npm feed:\\n\\n![screenshot of the onboarding process for Windows users](screenshot-onboarding-with-windows.png)\\n\\nNow consider the onboarding process for a non Windows user:\\n\\n![screenshot of the onboarding process for non Windows users](screenshot-onboarding-with-other.png)\\n\\nAs we can see, there is a significant difference in the onboarding experience between operating systems. Windows users can use a tool named [`vsts-npm-auth`](https://www.npmjs.com/package/vsts-npm-auth) which automates onboarding. Non Windows users have a longer road to follow. The instructions walk through manually creating an `.npmrc` file in a users home directory which contains information including a base 64 encoded Azure DevOps Personal Access Token with the Packaging read and write scopes. It is tedious to do.\\n\\n`azdo-npm-auth` aims to automate the toil, and make the onboarding experience for non Windows users as simple as it is for Windows users.\\n\\nThere is an official package named [`ado-npm-auth`](https://github.com/microsoft/ado-npm-auth). However, [due to issues I experienced in using the `ado-npm-auth` package](https://github.com/microsoft/ado-npm-auth/issues/50), I found myself creating `azdo-npm-auth`. By the way, the package was briefly named `ado-npm-auth-lite`; I renamed it as I felt `azdo-npm-auth` was a better name.\\n\\n## When do I need to run `azdo-npm-auth`?\\n\\nShould you encounter the following message when you try to `npm i`:\\n\\n```shell\\nnpm error code E401\\nnpm error Unable to authenticate, your authentication token seems to be invalid.\\nnpm error To correct this please try logging in again with:\\nnpm error npm login\\n```\\n\\nOR\\n\\n```shell\\nnpm error code E401\\nnpm error Incorrect or missing password.\\nnpm error If you were trying to login, change your password, create an\\nnpm error authentication token or enable two-factor authentication then\\nnpm error that means you likely typed your password in incorrectly.\\nnpm error Please try again, or recover your password at:\\nnpm error   https://www.npmjs.com/forgot\\nnpm error\\nnpm error If you were doing some other operation then your saved credentials are\\nnpm error probably out of date. To correct this please try logging in again with:\\nnpm error   npm login\\n```\\n\\nThat means either:\\n\\n- You have no user `.npmrc` file **OR**\\n- The token in your user `.npmrc` file is out of date\\n\\nIn either case, running `azdo-npm-auth` should resolve the issue. But the way you run it is important. To get `azdo-npm-auth` to create the necessary user `.npmrc` file for local development, run the following command:\\n\\n```shell\\nnpx -y --registry https://registry.npmjs.org azdo-npm-auth\\n```\\n\\nIt is possible to use environment variables to control the `registry` setting as well; consider the following (non-Windows compatible) example:\\n\\n```shell\\nnpm_config_registry=https://registry.npmjs.org npx azdo-npm-auth\\n```\\n\\nYou might be wondering what the `--registry https://registry.npmjs.org` part is for. It is a way to ensure that the `npx` command uses the **public** npm registry to install `azdo-npm-auth`. Without this, you might encounter a `npm error code E401` error like those above.\\n\\n## Configuration\\n\\n`azdo-npm-auth` requires that a project `.npmrc` file exists in order that it can acquire the information to run.\\n\\nThere is an optional `config` parameter which allows selection of a specific project `.npmrc` file. If the `config` parameter is not supplied, `azdo-npm-auth` will default to use the `.npmrc` in the current project directory.\\n\\nShould you not have one of these files already, there will be information in your Azure DevOps Artifacts section for connecting to the npm feed around creating a project `.npmrc` file. The required file should look something like this:\\n\\n```shell\\nregistry=https://pkgs.dev.azure.com/johnnyreilly/_packaging/npmrc-script-organization/npm/registry/\\n\\nalways-auth=true\\n```\\n\\n## Authenticating to Azure\\n\\nIf you would like `azdo-npm-auth` to acquire a token on your behalf, then it requires that your [Azure DevOps organisation is connected with your Azure account / Microsoft Entra ID](https://learn.microsoft.com/en-us/azure/devops/organizations/accounts/connect-organization-to-azure-ad?view=azure-devops). Then, assuming you are authenticated with Azure, it can acquire an Azure DevOps Personal Access Token on your behalf. To authenticate, run `az login`. [If you need to install the Azure CLI, follow these instructions](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli). It is not necessary to run `az login` if you are already authenticated with Azure.\\n\\nIf you would like to acquire a PAT token manually, there is a `--pat` option for that very circumstance.\\n\\n## Integration with `package.json`\\n\\n### Using a `preinstall` script\\n\\nA great way to integrate `azdo-npm-auth` is by using it in a `preinstall` script in your `package.json`:\\n\\n```json\\n\\"scripts\\": {\\n  \\"preinstall\\": \\"npx --yes azdo-npm-auth --config ./subdirectory-with-another-package-json/.npmrc\\"\\n},\\n```\\n\\nThe `--yes` flag above simply skips having npm challenge the user as to whether to download the package.\\n\\nHowever, as you\'re probably noticing, this requires having multiple `package.json`s and only having the `.npmrc` file in the nested one. Assuming that works for you, brilliant. It may not - no worries. We\'ll talk about that in a second.\\n\\nWith the above `preinstall` script in place, when the user performs `npm i` or similar, before attempting to install, the relevant user `.npmrc` file will be put in place so that installation just works\u2122\uFE0F. This is a **great** developer experience.\\n\\n### Using an `auth` script\\n\\nIf the complexity of nested `package.json`s doesn\'t work for you, we generally advise setting up a script like the one below:\\n\\n```json\\n\\"scripts\\": {\\n  \\"auth\\": \\"npx -y --registry https://registry.npmjs.org azdo-npm-auth\\"\\n},\\n```\\n\\nAnd running `npm run auth` when a `npm error code E401` is encountered. (Your script doesn\'t have to be called `auth` necessarily - if you like you could call it `fix-code-e401`, or something else entirely.)\\n\\n## What about CI?\\n\\nYou might be worried about `azdo-npm-auth` trying to create user `.npmrc` files when running CI builds. Happily this does not happen; it detects whether it is running in a CI environment and does **not** create a user `.npmrc` file in that case.\\n\\n## Summary\\n\\nIf you\'re a Mac or a Linux user, hopefully `azdo-npm-auth` can significantly ease the friction experienced doing local development with Azure DevOps npm feeds. You can see the [project code on GitHub here](https://github.com/johnnyreilly/azdo-npm-auth)."},{"id":"azure-devops-set-user-story-column-api","metadata":{"permalink":"/azure-devops-set-user-story-column-api","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2024-11-01-azure-devops-set-user-story-column-api/index.md","source":"@site/blog/2024-11-01-azure-devops-set-user-story-column-api/index.md","title":"Azure DevOps API: Set User Story column with the Azure DevOps Client for Node.js","description":"It is possible to set the column of a User Story in Azure DevOps with the Azure DevOps Client for Node.js, but the mechanism is surprising.","date":"2024-11-01T00:00:00.000Z","tags":[{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."},{"inline":false,"label":"Azure DevOps","permalink":"/tags/azure-devops","description":"The Azure DevOps suite of tools."},{"inline":false,"label":"Node.js","permalink":"/tags/node-js","description":"The Node.js runtime."}],"readingTime":4.925,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-devops-set-user-story-column-api","title":"Azure DevOps API: Set User Story column with the Azure DevOps Client for Node.js","authors":"johnnyreilly","tags":["typescript","azure devops","node.js"],"image":"./title-image.png","hide_table_of_contents":false,"description":"It is possible to set the column of a User Story in Azure DevOps with the Azure DevOps Client for Node.js, but the mechanism is surprising."},"unlisted":false,"prevItem":{"title":"Introducing azdo-npm-auth (Azure DevOps npm auth)","permalink":"/introducing-azdo-npm-auth"},"nextItem":{"title":"module ws does not provide an export named WebSocketServer","permalink":"/module-ws-does-not-provide-an-export-named-websocketserver"}},"content":"When I attempted to set the column of a User Story in Azure DevOps using the Azure DevOps Client for Node.js, I was surprised to find that the field `System.BoardColumn` was read-only and I bumped into the error:\\n\\n> TF401326: Invalid field status \'ReadOnly\' for field \'System.BoardColumn\'.\\n\\n![title image reading \\"Azure DevOps API: Set User Story column with the Azure DevOps Client for Node.js\\" with an Azure DevOps logo](title-image.png)\\n\\nThis post explains how to set the column of a User Story in Azure DevOps using the Azure DevOps Client for Node.js and it\'s based in part on a [Stack Overflow question](https://stackoverflow.com/questions/56165538/how-to-modify-boardcolumn-field-of-a-work-item-using-rest-api).\\n\\n\x3c!--truncate--\x3e\\n\\nThe [Azure DevOps Client for Node.js](https://github.com/microsoft/azure-devops-node-api) is a great way to interact with the Azure DevOps API if you\'re building with TypeScript. The library provides an API and the types. In this post, we\'ll use the client to set the column of a User Story in Azure DevOps rather than directly working with the API.\\n\\n![A screenshot of a user story in Azure DevOps](screenshot-azure-devops-column.webp)\\n\\n## Getting the WorkItemTrackingApi\\n\\nConsider the following code:\\n\\n```ts\\nimport * as nodeApi from \'azure-devops-node-api\';\\nimport {\\n  type WorkItem,\\n  WorkItemExpand,\\n} from \'azure-devops-node-api/interfaces/WorkItemTrackingInterfaces.js\';\\nimport type { IWorkItemTrackingApi } from \'azure-devops-node-api/WorkItemTrackingApi.js\';\\n\\n// ...\\n\\nconst authHandler = pat // If running locally you will use a Personal Access Token\\n  ? nodeApi.getPersonalAccessTokenHandler(\\n      pat,\\n      /** allowCrossOriginAuthentication */ true,\\n    )\\n  : // If running in Azure DevOps you will use the System.AccessToken\\n    nodeApi.getHandlerFromToken(\\n      sat,\\n      /** allowCrossOriginAuthentication */ true,\\n    );\\n\\nconst webApi = new nodeApi.WebApi(\\n  \'https://dev.azure.com/johnnyreilly/\',\\n  authHandler,\\n);\\nconst workItemTrackingApi = await webApi.getWorkItemTrackingApi();\\n```\\n\\nThis code sets up the Azure DevOps Client for Node.js and gets the `WorkItemTrackingApi`. The `WorkItemTrackingApi` is the API we\'ll use to set the column of a User Story. You\'ll need to replace `pat` and `sat` with your Personal Access Token and System Access Token, respectively (depending on where this code executes). And the organisation you use will likely be different from `johnnyreilly`.\\n\\n## Getting the User Story\\n\\nWe can now load our user story, given its ID:\\n\\n```ts\\nconst userStory = await workItemTrackingApi.getWorkItem(\\n  userStoryId,\\n  /** fields */ undefined,\\n  /** asOf */ undefined,\\n  /** expand */ WorkItemExpand.All,\\n);\\n```\\n\\nThis code loads the User Story with the ID `userStoryId`. We\'re using the `WorkItemExpand.All` option to ensure we get all the fields of the User Story.When you look at the User Story, you\'ll see that the `System.BoardColumn` field is there. This is the field we want to set, but we can\'t do it directly. If we try we\'ll encounter the notorious error:\\n\\n> TF401326: Invalid field status \'ReadOnly\' for field \'System.BoardColumn\'.\\n\\nHowever, if you look closely at the User Story you\'ll see two very similar fields:\\n\\n```json\\n\\"System.BoardColumn\\": \\"Blocked\\",\\n\\n// ...\\n\\n\\"WEF_1D7E8E9B92454212B8A5E6DFBCED0D17_Kanban.Column\\": \\"Blocked\\",\\n```\\n\\nThe `System.BoardColumn` field is read-only, but the `WEF_1D7E8E9B92454212B8A5E6DFBCED0D17_Kanban.Column` field is not. This is the field we can set to change the column of the User Story. And that will, in turn, set the `System.BoardColumn` field. Why this is the case is a mystery to me, but it works.\\n\\n## What columns can we set?\\n\\nThis may be more than what you need, but as someone who has bumped into the error:\\n\\n> TF401320: Rule Error for field State. Error code: Required, HasValues, LimitedToValues, SetByRule, InvalidEmpty.\\n\\nYou might want to know what columns you can set, in order that you can avoid this error happening. Or at least you might want to know what columns you can set so that you can provide a helpful error message to the user. Here\'s how you can get the columns for a board:\\n\\n```ts\\nconst workApi = await webApi.getWorkApi();\\n\\nconst columns = await workApi.getBoardColumns(\\n  {\\n    project: \'my-project\',\\n    team: \'my-team\',\\n  },\\n  \'Stories\',\\n);\\n\\nconst validColumns = columns\\n  .map((column) => column.name ?? \'\')\\n  .filter((name) => name);\\n```\\n\\nThis is optional, but I\'ve found it useful.\\n\\n## Setting the Column\\n\\nWith all that taken care of, we can now set the column of the User Story with this code:\\n\\n```ts\\nasync function updateUserStoryColumn({\\n  userStory,\\n  workItemTrackingApi,\\n  columnName,\\n  validColumns,\\n  whatIf,\\n}: {\\n  userStory: WorkItem;\\n  workItemTrackingApi: IWorkItemTrackingApi;\\n  columnName: string;\\n  validColumns: string[];\\n  whatIf: boolean;\\n}) {\\n  const isAValidColumn = validColumns.includes(columnName);\\n  if (!isAValidColumn) {\\n    throw new Error(\\n      `We cannot move a User Story to the column \\"${columnName}\\"; these are the columns that a User Story can be moved to: ${validColumns.map((column) => `\\"${column}\\"`).join(\', \')}`,\\n    );\\n  }\\n\\n  if (!userStory.fields) {\\n    throw new Error(\'No fields found\');\\n  }\\n\\n  // We are looking for a field like this:\\n  // \'WEF_1D7E8E9B92454212B8A5E6DFBCED0D17_Kanban.Column\': \'In-Progress\',\\n\\n  const wefField = [...Object.entries(userStory?.fields)].find(\\n    ([fieldName, _value]) => fieldName.includes(\'_Kanban.Column\'),\\n  );\\n\\n  if (!wefField) {\\n    throw new Error(\'No WEF field found\');\\n  }\\n  const fieldToUpdate = wefField[0];\\n\\n  // Define the update\\n  const patchDocument = [\\n    {\\n      op: \'add\', // surprisingly, this is the operation for updating a field - see https://learn.microsoft.com/en-us/rest/api/azure/devops/wit/work-items/update?view=azure-devops-rest-7.1&tabs=HTTP#update-a-field\\n      path: `/fields/${fieldToUpdate}`,\\n      value: columnName,\\n    },\\n    // COMMENTED OUT AS WILL NEVER WORK - LEFT FOR ILLUSTRATION\\n    // {\\n    //     op: \\"add\\",\\n    //     path: `/fields/System.BoardColumn`,\\n    //     value: columnName,\\n    // },\\n  ];\\n\\n  try {\\n    if (whatIf) {\\n      console.log(\\n        `Would update User Story ${userStory.id} to column ${columnName}`,\\n        patchDocument,\\n      );\\n      return;\\n    }\\n\\n    // Update the work item\\n    const updatedWorkItem = await workItemTrackingApi.updateWorkItem(\\n      /** customHeaders */ undefined,\\n      /** document */ patchDocument,\\n      /** id */ userStory.id!,\\n    );\\n    console.log(`User Story ${updatedWorkItem.id} moved to ${columnName}`);\\n  } catch (err) {\\n    console.error(\\n      `Error moving User Story ${userStory.id} to ${columnName}`,\\n      err,\\n    );\\n  }\\n}\\n\\nawait updateUserStoryColumn({\\n  userStory,\\n  workItemTrackingApi,\\n  columnName: \'Prioritised\',\\n  validColumns,\\n  whatIf: false,\\n});\\n```\\n\\nThe code above:\\n\\n- Checks that the column we want to move the User Story to is a valid column.\\n- Finds the curiously named `WEF_1D7E8E9B92454212B8A5E6DFBCED0D17_Kanban.Column` style field we need to update, purely by looking for a field that contains `_Kanban.Column`.\\n- Defines the update using the [mechanism documented here](https://learn.microsoft.com/en-us/rest/api/azure/devops/wit/work-items/update?view=azure-devops-rest-7.1&tabs=HTTP#update-a-field).\\n- Updates the User Story.\\n\\nYou can also see there\'s a `whatIf` option. If you set this to `true`, the code will log what it would do without actually doing it. This is useful for testing.\\n\\nAnd this mechanism works - the User Story is moved to the column you specify as desired."},{"id":"module-ws-does-not-provide-an-export-named-websocketserver","metadata":{"permalink":"/module-ws-does-not-provide-an-export-named-websocketserver","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2024-10-15-module-ws-does-not-provide-an-export-named-websocketserver/index.md","source":"@site/blog/2024-10-15-module-ws-does-not-provide-an-export-named-websocketserver/index.md","title":"module ws does not provide an export named WebSocketServer","description":"Resolve the ws npm issue: SyntaxError: The requested module ws does not provide an export named WebSocketServer","date":"2024-10-15T00:00:00.000Z","tags":[],"readingTime":0.665,"hasTruncateMarker":false,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"module-ws-does-not-provide-an-export-named-websocketserver","title":"module ws does not provide an export named WebSocketServer","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false,"description":"Resolve the ws npm issue: SyntaxError: The requested module ws does not provide an export named WebSocketServer"},"unlisted":false,"prevItem":{"title":"Azure DevOps API: Set User Story column with the Azure DevOps Client for Node.js","permalink":"/azure-devops-set-user-story-column-api"},"nextItem":{"title":"Static Typing for MUI React Data Grid Columns","permalink":"/static-typing-for-mui-react-data-grid-columns"}},"content":"I use Playwright for testing and mock Web Socket calls with the [ws](https://github.com/websockets/ws) package. I recently did an `npm upgrade` and found myself hitting this error message when I tried to run tests: \\n\\n```\\nSyntaxError: The requested module \'ws\' does not provide an export named \'WebSocketServer\'\\n```\\n\\nIt was caused by the following code:\\n\\n```ts\\nimport { WebSocketServer } from \\"ws\\"; // this goes bang!\\n\\n// ...\\n\\nconst mockWsServer = new WebSocketServer({ port: 5000 });\\n```\\n\\nThe fix was surprisingly simple to implement but hard to search for.  That\'s why I\'m writing this.\\n\\n## Resolving \\"The requested module \'ws\' does not provide\\"...\\n\\nThis fix is as simple switching the code to:\\n\\n```ts\\nimport ws from \\"ws\\";\\n\\n// ...\\n\\nconst mockWsServer = new ws.Server({ port: 5000 });\\n```\\n\\nAnd that should resolve the issue."},{"id":"static-typing-for-mui-react-data-grid-columns","metadata":{"permalink":"/static-typing-for-mui-react-data-grid-columns","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2024-10-07-static-typing-for-mui-react-data-grid-columns/index.md","source":"@site/blog/2024-10-07-static-typing-for-mui-react-data-grid-columns/index.md","title":"Static Typing for MUI React Data Grid Columns","description":"The MUI React Data Grid can be used with static typing to ensure the columns you pass to the component are correct. This post will show you how to do that.","date":"2024-10-07T00:00:00.000Z","tags":[{"inline":false,"label":"React","permalink":"/tags/react","description":"The React library."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."},{"inline":false,"label":"MUI","permalink":"/tags/mui","description":"The MUI / Material UI component library."}],"readingTime":5.32,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"static-typing-for-mui-react-data-grid-columns","title":"Static Typing for MUI React Data Grid Columns","authors":"johnnyreilly","tags":["react","typescript","mui"],"image":"./title-image.png","hide_table_of_contents":false,"description":"The MUI React Data Grid can be used with static typing to ensure the columns you pass to the component are correct. This post will show you how to do that."},"unlisted":false,"prevItem":{"title":"module ws does not provide an export named WebSocketServer","permalink":"/module-ws-does-not-provide-an-export-named-websocketserver"},"nextItem":{"title":"typescript-eslint with JSDoc JavaScript","permalink":"/typescript-eslint-with-jsdoc-js"}},"content":"The [MUI X Data Grid](https://mui.com/x/react-data-grid/) is a really handy component for rendering tabular data in React applications. But one thing that is not immediately obvious is how to use TypeScript to ensure that the columns you pass to the component are correct. This post will show you how to do that.\\n\\n![title image reading \\"Static Typing for MUI React Data Grid Columns\\" with a TypeScript logo and MUI logos](title-image.png)\\n\\nWhy does it matter? Well look at this screenshot of the Data Grid with incorrect column names:\\n\\n![screenshot of a grid with incorrect columns](screenshot-incorrect-columns.png)\\n\\n\x3c!--truncate--\x3e\\n\\nInterestingly, the `User` column is blank. Given the code, we\'d probably expect to see a users name there.\\n\\nLet\'s take look at the code for that screenshot:\\n\\n```tsx\\nimport * as React from \'react\';\\nimport Box from \'@mui/material/Box\';\\nimport { DataGrid } from \'@mui/x-data-grid\';\\n\\nexport default function BasicColumnsGrid() {\\n  const rows = [\\n    {\\n      id: 1,\\n      username: \'@MUI\',\\n      age: 20,\\n    },\\n  ];\\n\\n  const columns = [\\n    { field: \'user-name\', headerName: \'User\' },\\n    { field: \'age\', headerName: \'Age\' },\\n  ];\\n\\n  return (\\n    <Box sx={{ height: 250, width: \'100%\' }}>\\n      <DataGrid columns={columns} rows={rows} />\\n    </Box>\\n  );\\n}\\n```\\n\\nThe issue is that the `field` property in the column definition is incorrect. It should be `username` not `user-name`. We know that, but the TypeScript compiler doesn\'t. And the Data Grid doesn\'t appear to know that either; there\'s no error in the console surfacing an issue.\\n\\n## Using TypeScript to extract type information from the rows\\n\\nIt\'s possible to use TypeScript to ensure that the columns you pass to the Data Grid are valid. What we want, is TypeScript to say: \\"Hey, you\'ve passed the wrong column name to the Data Grid\\" (in it\'s own inimitable way). We can do that.\\n\\nWhat we want to do, is use TypeScript to analyse the `rows` array and extract type information. We can do that like so:\\n\\n```diff\\n  const rows = [\\n    {\\n      id: 1,\\n      username: \'@MUI\',\\n      age: 20,\\n    },\\n  ];\\n\\n+  type ValidRow = (typeof rows)[number];\\n+  type ValidField = keyof ValidRow;\\n+  type ColumnWithValidField = { field: ValidField };\\n```\\n\\nThe `ValidRow` type is the type of an element in the `rows` array:\\n\\n```ts\\ntype ValidRow = {\\n  id: number;\\n  username: string;\\n  age: number;\\n};\\n```\\n\\nThe `ValidField` type is derived from the `ValidRow` type; it is the keys of the `ValidRow` type. So, the `ValidField` type is:\\n\\n```ts\\ntype ValidField = \'id\' | \'username\' | \'age\';\\n```\\n\\nFinally, we can create a type that represents a column with a valid field in the form of the `ColumnWithValidField` type:\\n\\n```ts\\ntype ColumnWithValidField = {\\n  field: \'id\' | \'username\' | \'age\';\\n};\\n```\\n\\nThe type above says explicitly that the `field` property of a column must be one of the keys of the `ValidRow` type. This is the type information we require to ensure that the columns we pass to the Data Grid are correct.\\n\\n## Applying the type information to the columns\\n\\nNow we have this type information, we can then use that information to type the `columns` array. We can do that like so:\\n\\n```diff\\n  const columns = [\\n    { field: \'username\', headerName: \'User\' },\\n    { field: \'age\', headerName: \'Age\' },\\n-  ];\\n+  ] satisfies GridColDef<ValidRow>[] & ColumnWithValidField[];\\n```\\n\\nWhereas previously the `columns` array was not explicitly typed. Now it is with the `satisfies` operator. (For an excellent explanation of `satifies` read [Matt Pocock\'s post](https://www.totaltypescript.com/clarifying-the-satisfies-operator).)\\n\\nWe are saying that `columns` is an array of `GridColDef<ValidRow>` **and** that the `field` property of each element in the array is definitely one of the provided fields in the `rows` data. We need both of these conditions to be true:\\n\\n- Using `GridColDef<ValidRow>` ensures that the general columns schema matches what the Data Grid component needs.\\n- Using `ColumnWithValidField` ensures that the `field` property of each column is correct; based upon the `rows` field.\\n\\nLet\'s validate this approach works, trying to use our buggy input with this new approach:\\n\\n![screenshot of a grid with incorrect columns and TypeScript surfacing the issue](screenshot-incorrect-columns-with-helpful-error.png)\\n\\nWe can now see an error from the TypeScript compiler in VS Code: `Type \'\\"user-name\\"\' is not assignable to type \'\\"id\\" | \\"username\\" | \\"age\\"\'. Did you mean \'\\"username\\"\'?` It\'s even an actionable error, suggesting the correct field name!\\n\\n## Putting it all together\\n\\nHere\'s the full code (with the error corrected):\\n\\n```tsx\\nimport * as React from \'react\';\\nimport Box from \'@mui/material/Box\';\\nimport { DataGrid, GridColDef } from \'@mui/x-data-grid\';\\n\\nexport default function BasicColumnsGrid() {\\n  const rows = [\\n    {\\n      id: 1,\\n      username: \'@MUI\',\\n      age: 20,\\n    },\\n  ];\\n\\n  type ValidRow = (typeof rows)[number];\\n  type ValidField = keyof ValidRow;\\n  type ColumnWithValidField = { field: ValidField };\\n\\n  const columns = [\\n    { field: \'username\', headerName: \'User\' },\\n    { field: \'age\', headerName: \'Age\' },\\n  ] satisfies GridColDef<ValidRow>[] & ColumnWithValidField[];\\n\\n  return (\\n    <Box sx={{ height: 250, width: \'100%\' }}>\\n      <DataGrid columns={columns} rows={rows} />\\n    </Box>\\n  );\\n}\\n```\\n\\nWith this approach, you can be confident that the columns you pass to the Data Grid are correct. This is a great way to ensure that your code is correct and that you are using the Data Grid component as intended.\\n\\n## The importance of memoizing columns\\n\\nThe [MUI docs say](https://mui.com/x/react-data-grid/column-definition/):\\n\\n> The `columns` prop should keep the same reference between two renders. The columns are designed to be definitions, to never change once the component is mounted. Otherwise, you take the risk of losing elements like column width or order. You can create the array outside the render function or memoize it.\\n\\nMy own experience has been that I noticed no ill effects on my own use cases by **not** memoizing. When I asked the question I was advised this was still important [when you use a big number of columns and rows](https://github.com/mui/mui-x/issues/14862). To apply that to the example we\'ve been working with, it would look like this:\\n\\n```tsx\\nimport * as React from \'react\';\\nimport Box from \'@mui/material/Box\';\\nimport { DataGrid, GridColDef } from \'@mui/x-data-grid\';\\n\\nexport default function BasicColumnsGrid() {\\n  const rows = [\\n    {\\n      id: 1,\\n      username: \'@MUI\',\\n      age: 20,\\n    },\\n  ];\\n\\n  type ValidRow = (typeof rows)[number];\\n  type ValidField = keyof ValidRow;\\n  type ColumnWithValidField = { field: ValidField };\\n\\n  const columns = React.useMemo(() => [\\n    { field: \'username\', headerName: \'User\' },\\n    { field: \'age\', headerName: \'Age\' },\\n  ] satisfies GridColDef<ValidRow>[] & ColumnWithValidField[], []);\\n\\n  return (\\n    <Box sx={{ height: 250, width: \'100%\' }}>\\n      <DataGrid columns={columns} rows={rows} />\\n    </Box>\\n  );\\n}\\n```"},{"id":"typescript-eslint-with-jsdoc-js","metadata":{"permalink":"/typescript-eslint-with-jsdoc-js","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2024-09-02-typescript-eslint-with-jsdoc-js/index.md","source":"@site/blog/2024-09-02-typescript-eslint-with-jsdoc-js/index.md","title":"typescript-eslint with JSDoc JavaScript","description":"You can use typescript-eslint with JSDoc JavaScript to get the benefits of linting powered by type information in a JavaScript codebase; this post shows you how.","date":"2024-09-02T00:00:00.000Z","tags":[{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."},{"inline":false,"label":"JSDoc","permalink":"/tags/jsdoc","description":"Type safety through JSDoc annotations."}],"readingTime":11.605,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"typescript-eslint-with-jsdoc-js","title":"typescript-eslint with JSDoc JavaScript","authors":"johnnyreilly","tags":["javascript","typescript","jsdoc"],"image":"./title-image.png","hide_table_of_contents":false,"description":"You can use typescript-eslint with JSDoc JavaScript to get the benefits of linting powered by type information in a JavaScript codebase; this post shows you how."},"unlisted":false,"prevItem":{"title":"Static Typing for MUI React Data Grid Columns","permalink":"/static-typing-for-mui-react-data-grid-columns"},"nextItem":{"title":"Using AZD for faster incremental Azure Static Web App deployments in GitHub Actions","permalink":"/using-azd-for-faster-incremental-azure-static-web-app-deployments-in-github-actions"}},"content":"It\'s possible to statically type check a JavaScript codebase with TypeScript with JSDoc annotations. Going a little further, and using `typescript-eslint` to lint your codebase with the benefits of type information can improve your code quality even more. This post will show you how to set this up and talk about some of the gotchas.\\n\\n![title image reading \\"typescript-eslint with JSDoc JavaScript\\" with a typescript-eslint logo and TypeScript logo](title-image.png)\\n\\nWe\'ll also talk a little about how to use TypeScript in combination with JSDoc annotations in a JavaScript codebase.\\n\\n\x3c!--truncate--\x3e\\n\\n## Background\\n\\nI recently worked on a project where we had a five year old React JavaScript codebase. There wasn\'t time to convert the codebase to TypeScript, but we wanted to see if we could get some of the benefits of TypeScript by using JSDoc annotations. The codebase also didn\'t have linting in place. So we thought we\'d see if we could use `typescript-eslint` to lint our codebase. This was a little tricky to set up, so I thought I\'d write a post to help others who might be in the same situation.\\n\\n## Setting up TypeScript\\n\\nFirst things first, you\'ll need to set up TypeScript in your project. You can do this by running:\\n\\n```bash\\nnpm install --save-dev typescript\\n```\\n\\nWe now have TypeScript installed in our project. We\'ll need to create a `tsconfig.json` file to configure TypeScript to work with our JavaScript codebase:\\n\\n```json\\n{\\n  \\"compilerOptions\\": {\\n    /* Visit https://aka.ms/tsconfig to read more about this file */\\n\\n    /* Language and Environment */\\n    \\"lib\\": [\\n      \\"DOM\\",\\n      \\"DOM.Iterable\\",\\n      \\"ESNext\\"\\n    ] /* Specify a set of bundled library declaration files that describe the target runtime environment. */,\\n    \\"jsx\\": \\"preserve\\" /* Specify what JSX code is generated. */,\\n    \\"target\\": \\"ESNext\\", /* Set the JavaScript language version for emitted JavaScript and include compatible library declarations. */\\n\\n    /* Modules */\\n    \\"module\\": \\"preserve\\" /* Specify what module code is generated. */,\\n    \\"resolveJsonModule\\": true /* Enable importing .json files. */,\\n\\n    /* JavaScript Support */\\n    \\"checkJs\\": true /* Enable error reporting in type-checked JavaScript files. Implicitly sets `allowJs: true` */,\\n    // \\"maxNodeModuleJsDepth\\": 1,                        /* Specify the maximum folder depth used for checking JavaScript files from \'node_modules\'. Only applicable with \'allowJs\'. */\\n\\n    /* Emit */\\n    \\"noEmit\\": true /* Disable emitting files from a compilation. */,\\n\\n    /* Type Checking */\\n    \\"strict\\": true /* Enable all strict type-checking options. */,\\n\\n    /* Completeness */\\n    \\"skipLibCheck\\": true /* Skip type checking all .d.ts files. */\\n  },\\n  \\"include\\": [\\"src\\", \\"eslint.config.mjs\\"]\\n}\\n```\\n\\nThe main things to draw from the above `tsconfig.json` are:\\n\\n- it enables type checking of JavaScript files with JSDoc annotations with the `allowJs` / `checkJs` options\\n- it tells the TypeScript compiler to expect a modern browser environment with the `lib` option\\n- it tells the TypeScript compiler to expect React JSX with the `jsx` option\\n- it tells the TypeScript compiler not to emit any files with the `noEmit` option (as we\'re only using TypeScript for type checking - we will not be transpiling TypeScript into JavaScript)\\n- it tells the TypeScript compiler to run in strict mode with the `strict` option - I\'m going all in on type checking; you may want to be more selective\\n\\nFinally we need to add a script to our `package.json` to typecheck our codebase with the TypeScript compiler:\\n\\n```json\\n  \\"scripts\\": {\\n    \\"typecheck\\": \\"tsc --noEmit\\"\\n  }\\n```\\n\\nYou could omit the `--noEmit` flag given we\'ve set `noEmit` to `true` in our `tsconfig.json`, but I like to be explicit.\\n\\nWe can now run `npm run typecheck` to type check our JavaScript codebase with TypeScript. This will check all the JavaScript files in our `src` directory. At this point, you\'ll almost certainly see a lot of errors. Without some JSDoc annotations, TypeScript will struggle to infer some types for your codebase. Particularly parameters of functions.\\n\\nYour mission now is to add JSDoc annotations to your codebase to help TypeScript out. This is a bit of a grind, but it\'s worth it. You\'ll get a lot of the benefits of TypeScript without having to convert your codebase to TypeScript. Should you reach the point where all the errors are dealt with, you can include the `typecheck` script in your CI pipeline to ensure that new code is type checked / existing code doesn\'t regress.\\n\\n## JSDoc and the Type Annotations ECMAScript proposal\\n\\nIf you\'re looking for references on how to write JSDoc annotations, the [TypeScript handbook](https://www.typescriptlang.org/docs/handbook/jsdoc-supported-types.html) is a great place to start. It\'s also worth reading Gil Tayar\'s post on [JSDoc typings](https://gils-blog.tayar.org/posts/jsdoc-typings-all-the-benefits-none-of-the-drawbacks/).\\n\\n[Gil](https://gils-blog.tayar.org/about/), it\'s worth noting, is one of the people working on the [Type Annotations ECMAScript proposal](https://github.com/tc39/proposal-type-annotations) for adding support for type annotations to JavaScript. I\'ve written [a post about that proposal](../2022-04-16-type-annotations-proposal-strong-types-weakly-held/index.md) which you might find useful.\\n\\nThe fact that the proposal exists in the first place, points to how much the community would rather write TypeScript rather than JavaScript. At the time I write this, Bun, Deno and even Node.js now support running TypeScript directly; without a transpilation stage. And should the Type Annotations proposal one day reach Stage 4, then hopefully we can all move to that in browsers as well.\\n\\nThis is subjective, but it feels like a fairly common view that writing JSDoc is less fun than writing TypeScript.\\n\\n## Playing JSDoc on easy mode (JSDoc `.js` + `.ts`)\\n\\nBut in the situation I\'m in now, I have to write JSDoc. But I can combine writing JSDoc with also writing TypeScript, **when the code in question is type only**. Confused? Let me explain.\\n\\nIn our codebase we have a `logger.js` file which contains the following JavaScript function:\\n\\n```\\nfunction createLogger(\\n    sessionId,\\n    authToken,\\n    durableSessionId\\n) {\\n    return {\\n        error: (message, context, exception) => void logMessage(\'error\', message, {sessionId, durableSessionId, ...context}, exception, authToken),\\n        warn: (message, context, exception) => void logMessage(\'warn\', message, {sessionId, durableSessionId, ...context}, exception, authToken),\\n        info: (message, context, exception) => void logMessage(\'info\', message, {sessionId, durableSessionId, ...context}, exception, authToken),\\n        debug: (message, context, exception) => void logMessage(\'debug\', message, {sessionId, durableSessionId, ...context}, exception, authToken)\\n    }\\n}\\n```\\n\\nThis function is a factory function that returns a logger object. The logger object has four methods: `error`, `warn`, `info`, and `debug`. Each method logs a message at a different level. The `logMessage` function is a private function that does the actual logging (and it logs asynchronously hence the `void`).\\n\\nNow to provide that level of information in JSDoc, we\'d write something like this:\\n\\n```js\\n/**\\n * @typedef {function(string, object=, unknown=): void} LogMethod\\n */\\n\\n/**\\n * @typedef {Object} Logger\\n * @property {LogMethod} warn\\n * @property {LogMethod} error\\n * @property {LogMethod} info\\n * @property {LogMethod} debug\\n */\\n\\n/**\\n * @typedef {function(string, string|null, string|null): Logger} LoggerFactory\\n */\\n\\n/**\\n * @typedef {\\"debug\\" | \\"info\\" | \\"warn\\" | \\"error\\"} LogLevel\\n */\\n```\\n\\nFor my money, this is not that readable. I would much rather write it in TypeScript:\\n\\n```ts\\ntype LogMethod = (\\n  message: string,\\n  context?: object,\\n  exception?: unknown,\\n) => void;\\n\\nexport interface Logger {\\n  warn: LogMethod;\\n  error: LogMethod;\\n  info: LogMethod;\\n  debug: LogMethod;\\n}\\n\\nexport type LoggerFactory = (\\n  sessionId: string,\\n  authToken: string | null,\\n  durableSessionId?: string | null,\\n) => Logger;\\n\\nexport type LogLevel = \'debug\' | \'info\' | \'warn\' | \'error\';\\n```\\n\\nThis is pure TypeScript code. The tremendous news is that we can write type-only TypeScript and we can consume it in a JSDoc JavaScript file **because there is no runtime code in here**. It\'s purely type information. We\'ll write this in a `.ts` file and then we can `import` it into in our JS files.\\n\\nSo let\'s imagine we have a `loggerTypes.ts` file with the above TypeScript code in it. We can then import the `LoggerFactory` into our `logger.js` file like this:\\n\\n```diff\\n+/** @type {import(\'./loggerTypes\').LoggerFactory} */\\nfunction createLogger(\\n    sessionId,\\n    authToken,\\n    durableSessionId\\n) {\\n    return {\\n        error: (message, context, exception) => void logMessage(\'error\', message, {sessionId, durableSessionId, ...context}, exception, authToken),\\n        warn: (message, context, exception) => void logMessage(\'warn\', message, {sessionId, durableSessionId, ...context}, exception, authToken),\\n        info: (message, context, exception) => void logMessage(\'info\', message, {sessionId, durableSessionId, ...context}, exception, authToken),\\n        debug: (message, context, exception) => void logMessage(\'debug\', message, {sessionId, durableSessionId, ...context}, exception, authToken)\\n    }\\n}\\n```\\n\\nAnd with this single line addition to our `logger.js` file, we\'ve now got type information for our `createLogger` function. This is what it looks like in VS Code once you\'ve added that JSDoc annotation:\\n\\n![Screenshot of type information in VS Code](screenshot-jsdoc-in-vscode.png)\\n\\nSo here we\'ve had one line of JSDoc and we\'ve got all the type information we need **using TypeScript**. Our runtime code is still JavaScript, but our type information is TypeScript. This is a great way to get the benefits of TypeScript without having to write all your type information in JSDoc.\\n\\n## A note on `.ts` file naming\\n\\nI was intentional around the naming of the `.ts` file. I\'ve called it `loggerTypes.ts` because I want to make it clear that this file is all about types. It\'s not a (runtime) `.js` file, it\'s a `.ts` file - relevant for compile time. The `Types` suffix is deliberate too; it\'s not `logger.ts` entirely as planned.\\n\\nThe TypeScript compiler doesn\'t understand a world in which there is a `logger.js` file which imports types from a `logger.ts` file. Expect difficulty if you should try to use that approach. Particularly in a world in which ESM has lead to all imports being from `.js` files even when they are actually `.ts` source files.\\n\\nWhat\'s more, you would not typically see code like that in a TypeScript (or JavaScript) codebase. It would invite head scratching from anyone who came across it. So having `Types` in the name means that the compiler is not confused, and hopefully neither are your colleagues. You are unambiguously importing a different file. (By the way, having a 1-1 relationship between `.ts` and `.js` files as I\'m doing isn\'t mandatory, but I\'ve found it to be a useful pattern.)\\n\\nI did also consider the idea that the files I wrote should be `.d.ts` files rather than `.ts` files. After a [healthy discussion on Twitter](https://twitter.com/robpalmer2/status/1829856562422124734) I was directed by [Rob Palmer](https://twitter.com/robpalmer2) to what\'s probably the definitive answer on this from [Andrew Branch](https://github.com/andrewbranch) of the TypeScript team:\\n\\nAs ever in life, I find I cannot improve on [Andrew\'s guidance](https://github.com/microsoft/TypeScript/issues/52593#issuecomment-1419505081) and so I\'ll quote his views on the use of `.d.ts` files for storing hand-written utility types in full:\\n\\n> I would say it\u2019s discouraged or even harmful (though it won\u2019t be \u201Cdeprecated\u201D). At best, it\u2019s a technique with serious pitfalls that can be leveraged by people who understand them enough to set up additional tooling and safeguards to make it viable. Because `.d.ts` files only occur \u201Cnaturally\u201D as a pair with a `.js` file, together as outputs of a `.ts` file, a `.d.ts` file always implies the existence of a `.js` file. So the potential harm is readily apparent: if you hand-author `only-types.d.ts` and then write `import {} from \\"./only-types.js\\"`, this resolves and is legal in all settings, but in `verbatimModuleSyntax`, the import will be preserved and crash at runtime. While TypeScript has type-only imports and exports, it lacks the analogous concept of a type-only module, one which exists for type information purposes but is known to not exist at runtime, though I\u2019ve casually suggested multiple times that such a concept could be useful.\\n>\\n> But the main reason I personally avoid this is just because it doesn\u2019t copy into `outDir`. For my purposes, I\u2019d rather just eat the cost of the empty JS file (which also protects you from crashes should you accidentally import it at runtime).\\n\\nThanks also to [Remco Haszing](https://remcohaszing.nl/) for sharing that [even local `.d.ts` files should not be validated if you use `skipLibCheck`](https://twitter.com/remcohaszing/status/1829808165459804330). (Oddly, I didn\'t see that behaviour in my own testing, but I wouldn\'t be surprised if I was doing something quirky.)\\n\\n## Setting up `typescript-eslint`\\n\\nBut you didn\'t come here to just type check your codebase, you want to lint it too! Let\'s set up [`typescript-eslint`](https://typescript-eslint.io/) to lint our codebase with the benefits of type information. We\'re going to need to install a few more packages:\\n\\n```bash\\nnpm install --save-dev eslint @eslint/js @types/eslint__js typescript typescript-eslint eslint-plugin-react globals\\n```\\n\\nWe\'ll also need to create an `eslint.config.mjs` file to configure `eslint` to work with TypeScript (the eagle eyed amongst you will have noticed that we included this file in our `tsconfig.json` earlier). Here\'s what that file looks like:\\n\\n```js\\n/* eslint-disable @typescript-eslint/no-unsafe-member-access */\\nimport eslint from \'@eslint/js\';\\nimport globals from \'globals\';\\nimport tseslint from \'typescript-eslint\';\\n//@ts-expect-error no type definitions for eslint-plugin-react\\nimport pluginReact from \'eslint-plugin-react\';\\n\\nexport default [\\n  { files: [\'**/*.{js,mjs,cjs,ts,jsx,tsx}\'] },\\n  { languageOptions: { globals: globals.browser } },\\n  eslint.configs.recommended,\\n  ...tseslint.configs.recommendedTypeChecked, // yes we are using type checked\\n  {\\n    languageOptions: {\\n      parserOptions: {\\n        project: true,\\n        projectService: true,\\n        tsconfigRootDir: import.meta.dirname,\\n      },\\n    },\\n  },\\n  pluginReact.configs.flat.recommended,\\n  {\\n    settings: {\\n      react: {\\n        version: \'detect\',\\n      },\\n    },\\n  },\\n  {\\n    rules: {\\n      // Not compatible with JSDoc according to @bradzacher and https://github.com/typescript-eslint/typescript-eslint/issues/8955#issuecomment-2097518639\\n      \'@typescript-eslint/explicit-function-return-type\': \'off\',\\n      \'@typescript-eslint/explicit-module-boundary-types\': \'off\',\\n      \'@typescript-eslint/parameter-properties\': \'off\',\\n      \'@typescript-eslint/typedef\': \'off\',\\n    },\\n  },\\n];\\n```\\n\\nA common misconception is that you cannot use [`typescript-eslint`s linting with type information](https://typescript-eslint.io/getting-started/typed-linting) in JSDoc. You can. And here we are. However, there are some rules that are not compatible with JSDoc. We\'re turning those off in the `rules` section of the above `eslint.config.mjs` file.\\n\\nThe four rules we\'re turning off are rules that are not compatible with JSDoc according to [this thread on GitHub](https://github.com/typescript-eslint/typescript-eslint/issues/8955). In general, this is because the syntax required to satisfy the rule is not compatible with JS / JSDoc.\\n\\n## Conclusion\\n\\nIn this post we\'ve set up a JavaScript project to be type checked with JSDoc and the TypeScript compiler. We\'ve also set up `typescript-eslint` to lint the codebase, including using type information. Hopefully this will help increase the type safety of JavaScript projects of your own.\\n\\nI should say, it\'s been a learning exercise writing this post, and as I continue working on enriching the type safety of this particular codebase I\'m likely to learn more. I\'ll be updating this post as I do.\\n\\n[To read more on TypeScript vs JavaScript with JSDoc, you may want to read this post.](../2021-11-22-typescript-vs-jsdoc-javascript/index.md)"},{"id":"using-azd-for-faster-incremental-azure-static-web-app-deployments-in-github-actions","metadata":{"permalink":"/using-azd-for-faster-incremental-azure-static-web-app-deployments-in-github-actions","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2024-08-27-using-azd-for-faster-incremental-azure-static-web-app-deployments-in-github-actions/index.md","source":"@site/blog/2024-08-27-using-azd-for-faster-incremental-azure-static-web-app-deployments-in-github-actions/index.md","title":"Using AZD for faster incremental Azure Static Web App deployments in GitHub Actions","description":"Learn how to speed up deployments of Azure Static Web Apps in GitHub Actions using the AZD command.","date":"2024-08-27T00:00:00.000Z","tags":[{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."},{"inline":false,"label":"Bicep","permalink":"/tags/bicep","description":"The Bicep language for Azure Resource Manager templates."},{"inline":false,"label":"GitHub Actions","permalink":"/tags/github-actions","description":"The GitHub Actions CI / CD service."},{"inline":false,"label":"Azure Static Web Apps","permalink":"/tags/azure-static-web-apps","description":"The Azure Static Web Apps service."}],"readingTime":10.345,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"using-azd-for-faster-incremental-azure-static-web-app-deployments-in-github-actions","title":"Using AZD for faster incremental Azure Static Web App deployments in GitHub Actions","authors":"johnnyreilly","tags":["azure","bicep","github actions","azure static web apps"],"image":"./title-image.png","hide_table_of_contents":false,"description":"Learn how to speed up deployments of Azure Static Web Apps in GitHub Actions using the AZD command."},"unlisted":false,"prevItem":{"title":"typescript-eslint with JSDoc JavaScript","permalink":"/typescript-eslint-with-jsdoc-js"},"nextItem":{"title":"Using AZD for faster incremental Azure Container App deployments in Azure DevOps","permalink":"/using-azd-for-faster-incremental-azure-container-app-deployments-in-azure-devops"}},"content":"This post is a follow on from the post [Using AZD for faster incremental Azure Container App deployments in Azure DevOps](../2024-07-15-using-azd-for-faster-incremental-azure-container-app-deployments-in-azure-devops/index.md). In that post, we looked at how to speed up deployments of Azure Container Apps in Azure DevOps using the [Azure Developer CLI (`azd`)](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/). In this post, we\'re going to look at how to speed up deployments of Azure Static Web Apps in GitHub Actions using `azd`.\\n\\n![title image reading \\"Using AZD for faster incremental Azure Static Web App deployments in GitHub Actions\\" with the Azure Static Web Apps logo](title-image.png)\\n\\nThere\'s going to be some overlap between the last post and this one. I don\'t want to force you to read them both, so I\'ll duplicate some of the content from the previous post here. But I\'ll also add some new content that\'s specific to deploying Azure Static Web Apps in GitHub Actions with `azd`.\\n\\n\x3c!--truncate--\x3e\\n\\n## Faster deployments from `azd` 1.4 and beyond\\n\\nThe `azd` v1.4.0 release contained a significant feature: `azd provision` is now faster when there are no infrastructure changes.\\n\\nTo quote a trimmed down version of the [announcement](https://devblogs.microsoft.com/azure-sdk/azure-developer-cli-azd-october-2023-release/#azd-provision-is-now-faster-when-there-are-no-infrastructure-changes):\\n\\n> If you\u2019ve been using the Azure Developer CLI for a while, you may have noticed that sometimes `azd provision` takes a long time to complete when it may not need to. The wait time was because, prior to version 1.4.0, `azd provision` would always reprovision regardless of whether the underlying Infrastructure as Code had changed... As of today\u2019s 1.4.0 release, `azd provision` now checks the most recent deployment upstream on Azure to see if the state is the same as what\u2019s represented in the Infrastructure as Code that\u2019s been used to provision. If the state is the same, the provision is skipped... with this new experience, you should also notice improved performance when running `azd up` in a CI/CD pipeline as provisioning will be automatically skipped when there are no changes.\\n\\nThis can help us speed up deployments of Azure Static Web Apps in GitHub Actions.\\n\\n## Background\\n\\nYou\'re reading this post on my blog, which, at the time of writing, runs using Azure Static Web Apps. And has done for years. Every time I push a change, a deployment pipeline runs that deploys the changes to Azure. There\'s two distinct parts to the deployment pipeline:\\n\\n1. deploying infrastructure (the Azure resources that the blog relies upon such as the Azure Static Web App, a Cosmos DB etc)\\n2. building and deploying the application code (the blog itself)\\n\\nIt takes around **3 minutes** to deploy the infrastructure. And this is happening every time we update the site. But most of the time, there are no changes to be made to the infrastructure of the site; just the content. So it\'s a waste of time. I want to speed this up and `azd` can help me do that.\\n\\nSpecifically, I want to switch my usage of `az deployment group create` to `azd provision` because `azd provision` is faster when there are no infrastructure changes. We will drop the infrastructure deployment job time from **3 minutes** to **20 seconds** when there are no infrastructure changes.\\n\\nNow when I started trying to see if doing faster deployments of Static Web Apps was possible with `azd`, I couldn\'t discover any documentation. So I\'ve found myself writing the documentation I wish had existed. Please forgive me any mistakes I make along the way.\\n\\nTo be clear on scope, my intention here is only to speed up how we handle the deployment of the infrastructure. I don\'t want to deploy infrastructure if there are no changes; so I\'ll use `azd` to accomplish this goal. I\'m not going all in on `azd` for the deployment of the application code as well. For now, we\'ll focus solely on the infrastructure piece. Maybe we\'ll come back to the application code in a future post.\\n\\nFrom here on out, we\'ll go through the changes we need to make to our project to replace `az deployment group create` with `azd provision` for faster incremental Azure Static Web App deployments in GitHub Actions.\\n\\n## Hello `azure.yml`\\n\\nTo make use of `azd`, we\'ll requires an `azure.yml` file in our project. This file is going to contain the configuration for our `azd` project. Here\'s what it looks like:\\n\\n```yaml\\n# yaml-language-server: $schema=https://raw.githubusercontent.com/Azure/azure-dev/main/schemas/v1.0/azure.yaml.json\\n\\nname: my-static-web-app\\nmetadata:\\n  template: azd-init@1.9.6\\nservices:\\n  web:\\n    host: staticwebapp\\n    resourceName: ${STATIC_WEB_APP_NAME}\\n    project: ./blog-website\\n    language: js\\n```\\n\\nThe particular things to note in this file are:\\n\\n- we have one service - `web` - this is the service that represents our Static Web App\\n- our host is `staticwebapp` - this means we\'re deploying a Static Web App\\n- we provide the resource name of our static web app name in the `STATIC_WEB_APP_NAME` environment variable - this allows `azd` to identify it. (The `resourceName` parameter supports environment variable substitution and will plug in the name of the resource when it is used.)\\n- we provide the path to the project that contains the code for our Static Web App in the `project` parameter and specify it is `js` code in the `language` parameter. Neither of these parameters are used by `azd` during provisioning, but they are required.\\n\\n## Bicep modifications\\n\\nThe feature we want to consume from `azd` is the ability to avoid provisioning infrastructure when there are no changes. To do this, we need to make some modifications to our Bicep files in order that `azd` can determine whether there are changes or not.\\n\\n### Using resource group scoped deployments with azd\\n\\nWe\'re going to start off with a minor tweak to our `main.bicep` file; the entry point to our Bicep deployments.\\n\\n```bicep\\ntargetScope = \'resourceGroup\'\\n```\\n\\nThe change above allows us to use `azd` deployments targeted at existing resource groups. The default mode of operation for `azd` deployments is deploying a resource group to a subscription. We are seeking to [deploy to an existing resource group](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/resource-group-scoped-deployments).\\n\\nNow, strictly speaking, this isn\'t necessary for speeding up deployments with `azd`. But if you\'re not one for creating a resource group per deployment (as I am not), then this is a good idea. This kind of deployment requires less permissions and aligns with the principle of least privilege.\\n\\nWe\'ll need to opt into using this feature with `azd` later on when we update our workflow as at present resource group scoped deployments are considered \\"alpha\\".\\n\\n### New parameters in `main.bicep`\\n\\nWe\'re going to add an `envName` parameter that will be used to populate `azd-env-name` tags on resources:\\n\\n```bicep\\n@description(\'Environment eg dev, prod\')\\nparam envName string\\n```\\n\\n### Tagging resources with the azd tags\\n\\nNow that we\'ve passed the `envName` parameter to our `main.bicep` file, we\'re going to use it to tag our resources with the environment name. This allows `azd` to determine the environment of a given resource. We already have a `tags` object, let\'s add the value of the `envName` parameter to it with the special property name of `azd-env-name`:\\n\\n```bicep\\nvar combinedTags = union(tags, { \'azd-env-name\': envName })\\n```\\n\\nWe\'ll make use of the `combinedTags` object as we tag our resources instead of the `tags` object. We will also add an extra tag to the static web app resource to identify it as our `web` service:\\n\\n```bicep\\nvar tagsForStaticWebApp = union({\\n  // ...\\n  \'azd-service-name\': \'web\' // note the \\"web\\" matches the service name in azure.yml\\n}, combinedTags)\\n\\nresource staticWebApp \'Microsoft.Web/staticSites@2022-09-01\' = {\\n  name: staticWebAppName\\n  location: location\\n  tags: tagsForStaticWebApp\\n  // ...\\n}\\n```\\n\\n### Migrating to `main.bicepparam`\\n\\nPrior to using `azd`, we were using a `main.bicep` file to deploy our infrastructure and we provided parameters to this file via our GitHub Actions workflow. We\'re going to make a change to our pipeline to use a [`main.bicepparam`](https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/parameter-files?tabs=Bicep) file instead.\\n\\nThe `main.bicepparam` file is going to contain the parameters that we were previously providing directly to our `main.bicep` file. It\'s going to pick these up from environment variables that we\'ll declare and from environment variables provided by `azd`; such as the one to drive environment name. So there\'s a little more indirection in our parameter passing now. It used to be:\\n\\n_GitHub Actions -> `main.bicep`_\\n\\nNow it will be:\\n\\n_GitHub Actions -> `main.bicepparam` -> `main.bicep`_\\n\\nConsider the following (cut down) `main.bicepparam` file:\\n\\n```bicep\\nusing \'./main.bicep\'\\n\\nparam envName = readEnvironmentVariable(\'AZURE_ENV_NAME\', \'\')\\nparam location = readEnvironmentVariable(\'AZURE_LOCATION\', \'\')\\nparam branch = readEnvironmentVariable(\'REPOSITORY_BRANCH\', \'\')\\nparam staticWebAppName = readEnvironmentVariable(\'STATIC_WEB_APP_NAME\', \'\')\\n\\nparam tags = {\\n  owner: readEnvironmentVariable(\'TAGS_OWNER\', \'\')\\n  emain: readEnvironmentVariable(\'TAGS_EMAIL\', \'\')\\n}\\n```\\n\\nThis should pick up the values we need from environment values provided both by us and `azd`. Later we\'ll update the GitHub Actions workflow to ensure these are provided.\\n\\n## Updating our GitHub Actions workflow to use `azd`\\n\\nWe need to install and configure `azd` in our GitHub Actions workflow:\\n\\n```yml\\n- name: Install azd \uD83D\uDD27\\n  uses: Azure/setup-azd@v1.0.0\\n\\n- name: Set `azd` config options \uD83D\uDD27\\n  run: |\\n    azd config set auth.useAzCliAuth \\"true\\"\\n    azd config set alpha.resourceGroupDeployments on\\n```\\n\\nAs well as installing `azd`, we\'re setting two configuration options. The `auth.useAzCliAuth` option tells `azd` to use the Azure CLI for authentication - we already have the [Azure Login Action](https://github.com/marketplace/actions/azure-login) in our workflow, authenticating our pipeline so it can use the Azure CLI. With the `useAzCliAuth` option set, `azd` can make use of that existing authentication rather than needing us to authenticate it independently. The `alpha.resourceGroupDeployments` option enables resource group scoped deployments because we\'re using resource group scoped deployments in our Bicep files as we discussed earlier.\\n\\nNow we have `azd` in place and authenticated, we\'re ready to swap out `az deployment group create` for `azd provision`. We\'re going to remove the following job from our workflow:\\n\\n```yml\\n- name: Infra - provision \uD83D\uDD27\\n  uses: azure/CLI@v2\\n  with:\\n    inlineScript: |\\n      az deployment group create \\\\\\n        --resource-group ${{ env.RESOURCE_GROUP }} \\\\\\n        --name \\"${{ steps.deployment_name.outputs.DEPLOYMENT_NAME }}\\" \\\\\\n        --template-file ./infra/main.bicep \\\\\\n        --parameters \\\\\\n            branch=\'main\' \\\\\\n            location=\'${{ env.LOCATION }}\' \\\\\\n            staticWebAppName=\'${{ env.STATICWEBAPPNAME }}\' \\\\\\n            tags=\'${{ env.TAGS }}\' \\\\\\n            rootCustomDomainName=\'${{ env.ROOTCUSTOMDOMAINNAME }}\' \\\\\\n            blogCustomDomainName=\'${{ env.BLOGCUSTOMDOMAINNAME }}\'\\n```\\n\\nAnd in it\'s place we\'ll add the following:\\n\\n```yml\\n- name: Infra - provision \uD83D\uDD27\\n  run: azd provision --no-prompt\\n  env:\\n    # See https://learn.microsoft.com/en-gb/azure/developer/azure-developer-cli/configure-devops-pipeline?tabs=azdo\\n    AZURE_LOCATION: ${{ env.AZURE_LOCATION }}\\n    AZURE_SUBSCRIPTION_ID: ${{ vars.AZURE_SUBSCRIPTION_ID }}\\n    AZURE_ENV_NAME: prod # I only have one environment - you might have more\\n    # https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/manage-environment-variables#user-provided-environment-variables\\n    AZURE_RESOURCE_GROUP: ${{ env.AZURE_RESOURCE_GROUP }}\\n\\n    # Define the additional variables or secrets that are required only for provision - see main.bicepparam\\n    REPOSITORY_BRANCH: main # we only provision for the main branch\\n    STATIC_WEB_APP_NAME: ${{ env.STATIC_WEB_APP_NAME }}\\n\\n    TAGS_OWNER: ${{ env.TAGS_OWNER }}\\n    TAGS_EMAIL: ${{ env.TAGS_EMAIL }}\\n\\n    ROOT_CUSTOM_DOMAIN_NAME: ${{ env.ROOT_CUSTOM_DOMAIN_NAME }}\\n    BLOG_CUSTOM_DOMAIN_NAME: ${{ env.BLOG_CUSTOM_DOMAIN_NAME }}\\n```\\n\\nThe above amounts simply to a `azd provision --no-prompt` command, but it works because we are first authenticated and because we supply a number of environment variables to the job.\\n\\nYou\'ll see that we\'re populating environment variables that will be picked up by our `main.bicepparam` file. These were the same variables that were being passed explicitly to our `main.bicep` file when we were using `az deployment group create`. Now it will be `azd` that will be responsible for passing these values to our `main.bicep` file, using `main.bicepparam` as the connective tissue.\\n\\nWhen `azd provision` runs, it will look at the existing infrastructure and determine whether there are changes to be made. If there are no changes, then the deployment will be skipped. This is the magic of `azd`.\\n\\n## What does it look like when it works?\\n\\nWell, once the initial workflow has run (to tag the resources accordingly), a subsequent no-infra-change will look like this:\\n\\n![screenshot of azd detecting no changes and so not provisioning](screenshot-of-azd-detecting-no-changes.webp)\\n\\nThe `Skipped: Didn\'t find new changes` message is a sign that we\'re now no longer deploying in full each time. Only when we need to. This is us dropping infrastructure deployment job time from **3 minutes** to **20 seconds** when there are no infrastructure changes.\\n\\n## Conclusion\\n\\nIt is actually fairly straightforward to get the benefits of faster deployments with `azd` for Static Web Apps. In fact it\'s even more straightforward than with Container Apps, because you can choose to continue with your own preferred method of app code deployment. You\'re not obliged to use `azd deploy` as well as `azd provision`. So even if you might want to make the switch later, you can choose to do it gradually.\\n\\nIf you\'re interested in the PR that implemented this for my blog [you can find it here](https://github.com/johnnyreilly/blog.johnnyreilly.com/pull/913/files) - though I should warn you that I did some general refactoring in there as well, so please ignore tweaks to blog content etc."},{"id":"using-azd-for-faster-incremental-azure-container-app-deployments-in-azure-devops","metadata":{"permalink":"/using-azd-for-faster-incremental-azure-container-app-deployments-in-azure-devops","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2024-07-15-using-azd-for-faster-incremental-azure-container-app-deployments-in-azure-devops/index.md","source":"@site/blog/2024-07-15-using-azd-for-faster-incremental-azure-container-app-deployments-in-azure-devops/index.md","title":"Using AZD for faster incremental Azure Container App deployments in Azure DevOps","description":"Learn how to speed up deployments of Azure Container Apps in GitHub Actions using the AZD command.","date":"2024-07-15T00:00:00.000Z","tags":[{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."},{"inline":false,"label":"Bicep","permalink":"/tags/bicep","description":"The Bicep language for Azure Resource Manager templates."},{"inline":false,"label":"Azure Container Apps","permalink":"/tags/azure-container-apps","description":"The Azure Container Apps service. Effectively a managed Kubernetes service."},{"inline":false,"label":"Azure DevOps","permalink":"/tags/azure-devops","description":"The Azure DevOps suite of tools."},{"inline":false,"label":"Azure Pipelines","permalink":"/tags/azure-pipelines","description":"The Azure Pipelines CI / CD service."}],"readingTime":14.45,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"using-azd-for-faster-incremental-azure-container-app-deployments-in-azure-devops","title":"Using AZD for faster incremental Azure Container App deployments in Azure DevOps","authors":"johnnyreilly","tags":["azure","bicep","azure container apps","azure devops","azure pipelines"],"image":"./title-image.png","hide_table_of_contents":false,"description":"Learn how to speed up deployments of Azure Container Apps in GitHub Actions using the AZD command."},"unlisted":false,"prevItem":{"title":"Using AZD for faster incremental Azure Static Web App deployments in GitHub Actions","permalink":"/using-azd-for-faster-incremental-azure-static-web-app-deployments-in-github-actions"},"nextItem":{"title":"MUI React Tree View: pass data to TreeItem","permalink":"/mui-react-tree-view-pass-data-to-treeitem"}},"content":"When deploying Azure Container Apps from Azure DevOps, you can use the `azd` command to speed up deployments that do not affect infrastructure. Given that when you\'re deploying, it\'s far more common to be making a code and / or content change and not an infrastructure one, this can be a significant time saver.\\n\\n![title image reading \\"Using AZD for faster incremental Azure Container App deployments in Azure DevOps\\" with the Azure Container Apps logo](title-image.png)\\n\\nIf you\'re looking for information on how to use `azd` to speed up deployments of Azure Static Web Apps in GitHub Actions, then you might want to read [this post](../2024-08-27-using-azd-for-faster-incremental-azure-static-web-app-deployments-in-github-actions/index.md).\\n\\n\x3c!--truncate--\x3e\\n\\n## Faster deployments from `azd` 1.4 and beyond\\n\\nThe `azd` v1.4.0 release contained a significant feature: `azd provision` is now faster when there are no infrastructure changes.\\n\\nTo quote a trimmed down version of the [announcement](https://devblogs.microsoft.com/azure-sdk/azure-developer-cli-azd-october-2023-release/#azd-provision-is-now-faster-when-there-are-no-infrastructure-changes):\\n\\n> If you\u2019ve been using the Azure Developer CLI for a while, you may have noticed that sometimes `azd provision` takes a long time to complete when it may not need to. The wait time was because, prior to version 1.4.0, `azd provision` would always reprovision regardless of whether the underlying Infrastructure as Code had changed... As of today\u2019s 1.4.0 release, `azd provision` now checks the most recent deployment upstream on Azure to see if the state is the same as what\u2019s represented in the Infrastructure as Code that\u2019s been used to provision. If the state is the same, the provision is skipped... with this new experience, you should also notice improved performance when running `azd up` in a CI/CD pipeline as provisioning will be automatically skipped when there are no changes.\\n\\nI want this. We\'re going to unpack how to use this feature in the context of an Azure DevOps pipeline with Azure Container Apps. It turns out that there\'s a little more to it than just running `azd provision` and hoping for the best. In fact, there\'s gotchas aplenty - but it\'s totally achievable.\\n\\n## What about Bicep `what-if`?\\n\\nYou might be thinking at this point: \\"What about Bicep `what-if`?\\" It\'s a good question. `what-if` is a feature of the Azure CLI that allows you to see what changes would be made if you were to deploy a Bicep file. Unfortunately, my own experience of using `what-if` has been that it\'s quite unreliable. It will detect changes where there are none, and it will fail to detect changes where there are some. I\'d love to use it, but I can\'t trust it. If you\'d like to watch a more in depth discussion of the issue, [this video is a good place to start](https://www.youtube.com/watch?v=jlkwH-fP--M).\\n\\nThere appear to be some known issues with `what-if` that you can follow the progress of here:\\n\\n- https://github.com/Azure/arm-template-whatif/issues/83\\n- https://github.com/Azure/arm-template-whatif/issues/157\\n\\nGiven that `what-if` is not reliable, we\'re going to use `azd` to speed up our deployments.\\n\\n## Embracing `azd` in an existing Azure DevOps pipeline\\n\\nI\'m going to start with a pre-existing Azure Pipeline that deploys an Azure Container App. It uses the classic [`AzureResourceManagerTemplateDeployment@3` ARM template deployment v3 task](https://learn.microsoft.com/en-us/azure/devops/pipelines/tasks/reference/azure-resource-manager-template-deployment-v3?view=azure-pipelines) to deploy our infrastructure in the form of a `main.bicep` (and it\'s submodules) file.\\n\\nThis existing pipeline and infrastructure as code payload is in a good state. But it\'s slow. Every time the pipeline runs, the bicep section takes **8 minutes**. We\'re going to make it faster. Spoiler: we\'re going to get it down to **1 minute**.\\n\\n## Hello `azure.yml`\\n\\nOur project has no integration with `azd`. But we need `azd` to take advantage of the new `azd provision` feature. We\'re going to add a new file to our project: `azure.yml`. This file is going to contain the configuration for our `azd` project.\\n\\n```yaml\\n# yaml-language-server: $schema=https://raw.githubusercontent.com/Azure/azure-dev/main/schemas/v1.0/azure.yaml.json\\n\\nname: my-container-app\\nmetadata:\\n  template: azd-init@1.9.4\\nservices:\\n  app:\\n    image: myregistry.azurecr.io/${CONTAINER_IMAGE_NAME}:${APP_VERSION_TAG}\\n    host: containerapp\\n    resourceName: ${CONTAINER_APP_NAME}\\n```\\n\\nThe yaml above describes a container app service called `app` that uses an image from an Azure Container Registry. The `APP_VERSION_TAG` is a variable that we\'ll need to provide in our Azure DevOps pipeline. It\'s worth noticing the link at the top to the schema for the `azure.yml` file: https://raw.githubusercontent.com/Azure/azure-dev/main/schemas/v1.0/azure.yaml.json - much of the work around figuring out how to use `azd` was achieved by looking at the schema for the `azure.yml` file.\\n\\nOne thing we learned, as we looked at the schema, was that many parameters support environment variable substitution at runtime:\\n\\n![screenshot of schema file](screenshot-azure-yml-schema.png)\\n\\nThe screenshot above is taken from the Docker section of the configuration where environment variable substitution is widely supported. Originally the `image` parameter [did not support substitution](https://github.com/Azure/azure-dev/issues/4124). It does as of [v1.10.0](https://github.com/Azure/azure-dev/releases/tag/azure-dev-cli_1.10.0).\\n\\nYou\'ll notice that we pass `resourceName: ${CONTAINER_APP_NAME}`. You\'ll see later that we supply the `CONTAINER_APP_NAME` and this will be consumed by the `azd deploy` stage.\\n\\nIncidentally, we\'re using an approach whereby the image is built and pushed to the registry independently of `azd`. You could equally use `azd` to build and push the image. But we\'re not doing that here.\\n\\n## Bicep modifications\\n\\nI mentioned that we\'re adding a level of `azd` support to an existing pipeline. As part of that, we need to make modifications to our existing Bicep modules.\\n\\n### Using resource group scoped deployments with azd\\n\\nWe\'re going to start off with a minor tweak to our `main.bicep` file; the entry point to our Bicep deployments.\\n\\n```bicep\\ntargetScope = \'resourceGroup\'\\n```\\n\\nThe change above allows us to use `azd` deployments targeted at existing resource groups. The default mode of operation for `azd` deployments is deploying a resource group to a subscription. We are seeking to [deploy to an existing resource group](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/resource-group-scoped-deployments).\\n\\nNow, strictly speaking, this isn\'t necessary for speeding up deployments with `azd`. But if you\'re not one for creating a resource group per deployment (as I am not), then this is a good idea. This kind of deployment requires less permissions and may well better align with your organisation\'s security posture.\\n\\nWe\'ll need to opt into using this feature with `azd` later on in the pipeline; at present resource group scoped deployments are considered \\"beta\\".\\n\\n### The \\"does your service exist?\\" parameter\\n\\nWe\'re going to add a \\"magic\\" parameter to our `main.bicep` file. This parameter is going to be used to determine whether the container app we\'re deploying already exists. This is important because if the container app already exists, we will reuse the existing deployed container image during the `azd provision` stage. If it does not, then we\'ll deploy a new container image.\\n\\n```bicep\\n@description(\'Specifies if the container app exists - azd will provide this\')\\nparam containerAppExists bool = false\\n```\\n\\nWe\'ll look later at where this value comes from, but for now, we\'re just adding it to our `main.bicep` file. How do we use this parameter? In the module where we deploy our container app, we\'re going to make a couple of changes:\\n\\n```bicep\\nmodule fetchLatestImage \'../modules/fetch-container-image.bicep\' = {\\n  name: \'${name}-fetch-image\'\\n  params: {\\n    exists: containerAppExists\\n    name: name\\n  }\\n}\\n\\nresource app \'Microsoft.App/containerApps@2023-05-01\' = {\\n  name: name\\n  location: location\\n  tags: union(tags, {\'azd-service-name\':  \'web\' }) // note the \\"web\\" matches the service name in azure.yml\\n  // ...\\n  properties: {\\n    // ...\\n    template: {\\n      containers: [\\n        {\\n          image: fetchLatestImage.outputs.?containers[?0].?image ?? \'mcr.microsoft.com/azuredocs/containerapps-helloworld:latest\'\\n          // ...\\n        }\\n      ]\\n      // ...\\n    }\\n  }\\n}\\n```\\n\\nYou can see we tag the container app with the service name from the `azure.yml` file (`web`). This is important because it allows `azd` to determine whether the container app already exists and so power the `containerAppExists` parameter we added to our `main.bicep` file.\\n\\nWe\'re using the `containerAppExists` parameter to determine whether we should fetch the currently deployed image from the existing container app. If the container app exists, we\'ll use the existing image. If it does not, we\'ll use a default image. We\'d typically only expect to use the default image when we\'re deploying the container app to an environment for the first time. (You might be wondering how the new version of the application gets deployed; given that we\'re not using the new container image. It turns out that `azd deploy` is the command responsible for deploying the new image; we\'ll get to that later.)\\n\\nYou\'ll have noticed that we\'re using a new module called `fetch-container-image.bicep`. This module is responsible for attempting to fetch the existing image from the currently deployed container app:\\n\\n```bicep\\nparam exists bool\\nparam name string\\n\\nresource existingApp \'Microsoft.App/containerApps@2023-05-01\' existing = if (exists) {\\n  name: name\\n}\\n\\noutput containers array = exists ? existingApp.properties.template.containers : []\\n```\\n\\nThis is based on what files are generated when you perform an `azd init`, but has been customised for the specific version of the `Microsoft.App/containerApps` resource we\'re using.\\n\\n### Tagging resources with the environment name\\n\\nAnother addition we\'re going to make to our `main.bicep` file is to tag our resources with the environment name. This allows `azd` to determine the environment of a given resource. It\'s achieved by using the our already existing `envName` parameter and adding it to the tags of our resources:\\n\\n```bicep\\n@description(\'Environment eg dev, prod\')\\nparam envName string\\n\\n// ...\\n\\nvar combinedTags = union(tags, { \'azd-env-name\': envName })\\n```\\n\\n### Parameters in `main.bicep` must be immutable per environment\\n\\nIt\'s gotcha time! One thing we learned the hard way is that parameters in `main.bicep` must be **immutable per environment**. This means that you can\'t change the value of a parameter in a `main.bicep` file between deployments to an environment. This is because `azd` uses the `main.bicep` file to determine whether a deployment is incremental or not. If the parameters change, then `azd` will assume that the deployment requires infrastructure changes, and will reprovision the resources.\\n\\nWhat\'s more, as things stand, `azd` only has the ability to **fully** reprovision your resources. If your app consists of a database and a container app, and you only want to deploy a new version of the container app, you\'re out of luck. `azd` will deploy the database again too. This is a limitation of `azd` at the time of writing.\\n\\nI\'ve [raised a GitHub issue](https://github.com/Azure/azure-dev/issues/4123) in the hope that this feature might one day land. Perhaps it\'s super hard - quite possibly.\\n\\n## Welcome `main.bicepparam`\\n\\nPrior to using `azd`, we were using a `main.bicep` file to deploy our infrastructure and we provided parameters to this file via our Azure DevOps pipeline. We\'re going to make a change to our pipeline to use a [`main.bicepparam`](https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/parameter-files?tabs=Bicep) file instead.\\n\\nThe `main.bicepparam` file is going to contain the parameters that we were previously providing to our `main.bicep` file. It\'s going to pick these up from environment variables that we\'ll declare. We\'re also going to add our new `containerAppExists` parameter to this file, which will also collect its value from an environment variable. But it won\'t be us that provides that value; it will be `azd`.\\n\\nConsider the following (cut down) `main.bicepparam` file:\\n\\n```bicep\\nusing \'./main.bicep\'\\n\\nparam envName = readEnvironmentVariable(\'AZURE_ENV_NAME\', \'\')\\nparam location = readEnvironmentVariable(\'AZURE_LOCATION\', \'\')\\nparam serviceConnectionPrincipalId = readEnvironmentVariable(\'AZURE_PRINCIPAL_ID\', \'\')\\n\\nparam tags = {\\n  branch: readEnvironmentVariable(\'TAGS_BRANCH\', \'\')\\n  repo: readEnvironmentVariable(\'TAGS_REPO\', \'\')\\n}\\n\\n// ...\\n\\n// azd will provide the following parameters\\nparam containerAppExists = bool(readEnvironmentVariable(\'SERVICE_APP_RESOURCE_EXISTS\', \'false\'))\\n```\\n\\nThe `containerAppExists` parameter is determined by the `SERVICE_APP_RESOURCE_EXISTS` environment variable to provide this value. What\'s happening here is that we\'re picking up on a convention that `azd` uses to provide confirmation that a service already exists. `SERVICE_[SERVICENAME]_RESOURCE_EXISTS` is the pattern that `azd` uses to provide this information; where `[SERVICENAME]` is the name of the service as defined in the `azure.yml` file. In our case, it\'s `app` (or rather `APP`).\\n\\nIf you\'re curious about how this actually works [you can read the source code here](https://github.com/Azure/azure-dev/blob/837d4e8592c53375c7d9aa6df8b134c23cdeb487/cli/azd/pkg/project/service_target_containerapp.go#L174-L190) in the `azd` project. Here\'s the relevant code snippet:\\n\\n```go\\nfunc (at *containerAppTarget) addPreProvisionChecks(ctx context.Context, serviceConfig *ServiceConfig) error {\\n  // Attempt to retrieve the target resource for the current service\\n  // This allows the resource deployment to detect whether or not to pull existing container image during\\n  // provision operation to avoid resetting the container app back to a default image\\n  return serviceConfig.Project.AddHandler(\\"preprovision\\", func(ctx context.Context, args ProjectLifecycleEventArgs) error {\\n    exists := false\\n\\n    // Check if the target resource already exists\\n    targetResource, err := at.resourceManager.GetTargetResource(ctx, at.env.GetSubscriptionId(), serviceConfig)\\n    if targetResource != nil && err == nil {\\n      exists = true\\n    }\\n\\n    at.env.SetServiceProperty(serviceConfig.Name, \\"RESOURCE_EXISTS\\", strconv.FormatBool(exists))\\n    return at.envManager.Save(ctx, at.env)\\n  })\\n}\\n```\\n\\nNow that we have our `main.bicepparam` file, we\'re ready to migrate to our pipeline to use `azd`.\\n\\nWell, one extra bit first.\\n\\n## Workaround for `SERVICE_APP_RESOURCE_EXISTS` not being supplied\\n\\nAt the time of writing, there is an issue that means that the `SERVICE_APP_RESOURCE_EXISTS` environment variable is not being set by `azd`. [This is a known issue and is being worked on](https://github.com/Azure/azure-dev/issues/4402).\\n\\nIn the meantime, we have a workaround. We\'re going to set the `SERVICE_APP_RESOURCE_EXISTS` environment variable in our pipeline with a little bash and Azure CLI magic. We\'re going to set our manually created `SERVICE_APP_RESOURCE_EXISTS` environment variable to `true` if we detect a service already exists and `false` if not.\\n\\nYou\'ll note this as the `Check container app exists` step in the modifications to our pipeline below. This is a workaround and will be removed when the issue is resolved in `azd` itself.\\n\\n## Azure DevOps pipeline modifications\\n\\nThere\'s no two ways about it; our Azure DevOps pipeline modifications are pretty extensive. Where we were previously using the `AzureResourceManagerTemplateDeployment@3` task to deploy our Bicep files, we\'re now going to use the `azd` command to deploy our infrastructure and our container app.\\n\\nHere\'s a cut down version of our pipeline replacing the single `AzureResourceManagerTemplateDeployment@3` task with a series of tasks that use the `azd` command:\\n\\n```yaml\\n- task: setup-azd@0\\n  displayName: Install azd\\n\\n# If you can\'t use above task in your organization, you can remove it and uncomment below task to install azd\\n# - task: Bash@3\\n#   displayName: Install azd\\n#   inputs:\\n#     targetType: \\"inline\\"\\n#     script: |\\n#       curl -fsSL https://aka.ms/install-azd.sh | bash\\n\\n- pwsh: |\\n    azd config set auth.useAzCliAuth \\"true\\"\\n  displayName: Configure `azd` config options.\\n\\n- task: AzureCLI@2\\n  displayName: Login to ACR\\n  inputs:\\n    azureSubscription: $(registryServiceConnection)\\n    scriptType: bash\\n    scriptLocation: inlineScript\\n    inlineScript: |\\n      az acr login -n myregistry\\n\\n- task: AzureCLI@2\\n  displayName: Check container app exists # see https://github.com/Azure/azure-dev/issues/4593 for context on why this exists\\n  inputs:\\n    azureSubscription: ${{ variables.serviceConnection }}\\n    scriptType: bash\\n    scriptLocation: inlineScript\\n    inlineScript: |\\n      containerAppName=\\"$(containerAppName)\\"\\n      resourceGroupName=\\"$(resourceGroupName)\\"\\n\\n      if [ -z \\"$containerAppName\\" ] || [ -z \\"$resourceGroupName\\" ]; then\\n          echo \\"SERVICE_APP_RESOURCE_EXISTS: false\\"\\n          echo \\"##vso[task.setvariable variable=SERVICE_APP_RESOURCE_EXISTS]false\\"\\n      else\\n          if az containerapp show --name \\"$containerAppName\\" --resource-group \\"$resourceGroupName\\" > /dev/null 2>&1; then\\n              echo \\"SERVICE_APP_RESOURCE_EXISTS: true\\"\\n              echo \\"##vso[task.setvariable variable=SERVICE_APP_RESOURCE_EXISTS]true\\"\\n          else\\n              echo \\"SERVICE_APP_RESOURCE_EXISTS: false\\"\\n              echo \\"##vso[task.setvariable variable=SERVICE_APP_RESOURCE_EXISTS]false\\"\\n          fi\\n      fi\\n\\n- task: AzureCLI@2\\n  displayName: Provision Infra\\n  inputs:\\n    azureSubscription: $(serviceConnection)\\n    scriptType: bash\\n    scriptLocation: inlineScript\\n    inlineScript: |\\n      azd provision --no-prompt\\n  env:\\n    # See https://learn.microsoft.com/en-gb/azure/developer/azure-developer-cli/configure-devops-pipeline?tabs=azdo\\n    AZURE_LOCATION: $(location)\\n    AZURE_SUBSCRIPTION_ID: $(subscriptionId)\\n    AZURE_ENV_NAME: ${{parameters.env}}\\n    # https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/manage-environment-variables#user-provided-environment-variables\\n    AZURE_RESOURCE_GROUP: $(resourceGroupName)\\n    AZURE_PRINCIPAL_ID: $(serviceConnectionPrincipalId)\\n    # Define the additional variables or secrets that are required only for provision\\n    TAGS_BRANCH: $(Build.SourceBranch)\\n    TAGS_REPO: $(repo)\\n    CONTAINER_APP_NAME: $(myContainerAppName) # this is used to substitute the CONTAINER_APP_NAME value in the azure.yaml file\\n    CONTAINER_IMAGE_NAME: $(myContainerImageName) # this is used to substitute the CONTAINER_IMAGE_NAME value in the azure.yaml file\\n    APP_VERSION_TAG: $(containerImageTag) # the tag of the built image\\n    # ...\\n\\n- task: AzureCLI@2\\n  displayName: Deploy Application\\n  retryCountOnTaskFailure: 2\\n  inputs:\\n    azureSubscription: $(serviceConnection)\\n    scriptType: bash\\n    scriptLocation: inlineScript\\n    inlineScript: |\\n      azd deploy --no-prompt\\n  env:\\n    # See https://learn.microsoft.com/en-gb/azure/developer/azure-developer-cli/configure-devops-pipeline?tabs=azdo\\n    AZURE_LOCATION: $(location)\\n    AZURE_SUBSCRIPTION_ID: $(subscriptionId)\\n    AZURE_ENV_NAME: ${{parameters.env}}\\n    # https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/manage-environment-variables#user-provided-environment-variables\\n    AZURE_RESOURCE_GROUP: $(resourceGroupName)\\n    # Define the additional variables or secrets that are required only for deploy\\n    CONTAINER_APP_NAME: $(myContainerAppName) # this is used to substitute the CONTAINER_APP_NAME value in the azure.yaml file\\n    CONTAINER_IMAGE_NAME: $(myContainerImageName) # this is used to substitute the CONTAINER_IMAGE_NAME value in the azure.yaml file\\n    APP_VERSION_TAG: $(containerImageTag) # the tag of the built image\\n    # ...\\n```\\n\\nWhat\'s happening here? We\'ll take it step by step:\\n\\n- We\'re installing `azd` using the `setup-azd@0` task.\\n- We\'re configuring `azd` to use the Azure CLI for authentication and to enable resource group scoped deployments.\\n- We\'re logging into our Azure Container Registry. (If you\'re not building your image independently of `azd`, then you may not need this step.)\\n- We\'re provisioning our infrastructure using `azd provision --no-prompt`. Note that we\'re providing a number of environment variables to `azd` which will be detected in our `main.bicepparam` file.\\n- We\'re deploying our application using `azd deploy --no-prompt`. As we do that, we pass the `CONTAINER_APP_NAME` environment variable which will substitute into the `azure.yaml` file\\n\\nYou\'ll note that as we use `azd`, we make heavy use of environment variables. These environment variables will be picked up in the `main.bicepparam` file and passed through to the `main.bicep`. And of course there\'s the runtime `SERVICE_[SERVICENAME]_RESOURCE_EXISTS` parameter which `azd` will provide. Much of what you see here is inspired by [this documentation](https://learn.microsoft.com/en-gb/azure/developer/azure-developer-cli/configure-devops-pipeline?tabs=azdo).\\n\\n## What does it look like when it works?\\n\\nThat is the question! Like this:\\n\\n![screenshot of azd detecting no changes and so not provisioning](screenshot-of-azd-detecting-no-changes.avif)\\n\\nThe magic sentence in the above screenshot is: `SUCCESS: There are no changes to provision for your application.` This is what we\'re looking for. This is what makes our deployments faster.\\n\\n## Conclusion\\n\\nSo we\'ve done it, we\'ve speeded up our subsequent deployments by using `azd` in our Azure DevOps pipeline to avoid unnecessary infrastructure provisioning when there are no changes. This is a significant time saver. However, as we\'ve also seen, this is very easy to get wrong and quite hard to get right! Hopefully this will help you implement `azd` in your Azure DevOps pipelines.\\n\\nI couldn\'t have written this without [Marcel Michau](https://twitter.com/MarcelMichau) who did much of the heavy lifting on this project. I am the Boswell to his Johnson. Or something like that."},{"id":"mui-react-tree-view-pass-data-to-treeitem","metadata":{"permalink":"/mui-react-tree-view-pass-data-to-treeitem","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2024-07-01-mui-react-tree-view-pass-data-to-treeitem/index.md","source":"@site/blog/2024-07-01-mui-react-tree-view-pass-data-to-treeitem/index.md","title":"MUI React Tree View: pass data to TreeItem","description":"Learn how to pass arbitrary data to individual nodes in the MUI treeview component so individual nodes can be customised; for instance implementing a loader.","date":"2024-07-01T00:00:00.000Z","tags":[{"inline":false,"label":"React","permalink":"/tags/react","description":"The React library."},{"inline":false,"label":"MUI","permalink":"/tags/mui","description":"The MUI / Material UI component library."}],"readingTime":5.975,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"mui-react-tree-view-pass-data-to-treeitem","title":"MUI React Tree View: pass data to TreeItem","authors":"johnnyreilly","tags":["react","mui"],"image":"./title-image.png","hide_table_of_contents":false,"description":"Learn how to pass arbitrary data to individual nodes in the MUI treeview component so individual nodes can be customised; for instance implementing a loader."},"unlisted":false,"prevItem":{"title":"Using AZD for faster incremental Azure Container App deployments in Azure DevOps","permalink":"/using-azd-for-faster-incremental-azure-container-app-deployments-in-azure-devops"},"nextItem":{"title":"Web Workers, Comlink, Vite and TanStack Query","permalink":"/web-workers-comlink-vite-tanstack-query"}},"content":"I\'m a big fan of the Material-UI (MUI) library [treeview component](https://mui.com/x/react-tree-view/). I recently needed to do some customisation of the nodes in the treeview component I was rendering. The application I was working on acquired data for each node in our treeview component asynchronously. I wanted to show a loader for each node that was still loading.\\n\\nAchieving this required passing data to individual nodes in the treeview component. This appears to not be officially supported by the MUI treeview component. However, it is possible and this post will show you how to do it.\\n\\nThe comment on [this GitHub issue](https://github.com/mui/material-ui/issues/33175#issuecomment-1469725522) suggests that this will be directly supported in MUI v6. Until that time, you\'ll have to slightly hack the component to achieve this.\\n\\nI\'ve written previously about [how to check children and uncheck parents in the MUI treeview component](../2024-05-25-mui-react-tree-view-check-children-uncheck-parents/index.md). This post builds on that one. But, you need not be using multiselect / checkboxes etc to use the approach in this post. It applies generally to usage of the treeview component.\\n\\n![title image reading \\"MUI React Tree View: pass data to TreeItem\\" with the MUI logo](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## The behaviour we want to implement\\n\\nIn the application I\'m working on, we load data for each node in our treeview that is selected. While a nodes data is loading, we want to show a loader so users know that work is being done. We want to achieve something like this:\\n\\n![a treeview with a loading spinner](recording-loader.gif)\\n\\nThe labels in the above GIF aren\'t relevant to this post. The key thing to note is that the treeview is showing a loader for nodes that are still loading. Strictly speaking, the loader is a spinner from the MUI library. The [`CircularProgress`](https://mui.com/material-ui/react-progress/#circular) component to be precise. However, the loader could be anything you like.\\n\\nThe question is: how do we pass data to individual nodes in the treeview component so we can customise them?\\n\\n## The code\\n\\nWe can achieve this by using the `slots` and `slotProps` props on the `RichTreeView` component. (If you\'re using the `SimpleTreeView` that should work the same.) The `slots` prop allows us to customise the rendering of the treeview components. The `slotProps` prop allows us to pass data to the customised components. \\"Slots\\" is a MUI approach to customising components. You can read more about it [here](https://mui.com/base-ui/getting-started/usage/#slots).\\n\\nThe code below demonstrates how to pass data to individual nodes in the treeview component:\\n\\n```tsx\\nfunction OurComponent() {\\n  // ...\\n  return (\\n    <RichTreeView\\n      defaultExpandedItems={initialExpandedItems}\\n      multiSelect={true}\\n      checkboxSelection={true}\\n      selectedItems={selectedIds}\\n      onSelectedItemsChange={handleSelectedItemsChange}\\n      items={treeViewNodes}\\n      // note that we\'re using the TreeItemWithLoading component here\\n      slots={{ item: TreeItemWithLoading }}\\n      slotProps={{\\n        item: {\\n          //@ts-expect-error this works but MUI doesn\'t officially support this prior to v6 - see https://github.com/mui/material-ui/issues/33175#issuecomment-1469725522\\n          \'data-still-to-load-ids\': data.stillToLoad,\\n        },\\n      }}\\n    />\\n  );\\n}\\n```\\n\\nThe two key parts of the code above are the `slots` and `slotProps` props. In `slots`, we\'re passing a `TreeItemWithLoading` component to customise the rendering - we\'ll implement that component in moment. We\'re also passing a `data-still-to-load-ids` prop to the `TreeItemWithLoading` component. This is the data we want to pass to the individual nodes in the treeview component. In our case, it\'s a list of node ids that we\'re still loading the data for. You could pass any data you like here.\\n\\nThe thing that\'s probably (hopefully) setting off alarm bells in your head is the `//@ts-expect-error` comment. This is in place because MUI doesn\'t officially support passing data to individual nodes in the treeview component, and consequently TypeScript shouts about it. However, passing arbitrary data does work. It will be received by the rendering component and so can be used. The comment is there to acknowledge that this is a hack and to get TypeScript to stop complaining about it.\\n\\nNow let\'s look at the `TreeItemWithLoading` component:\\n\\n```tsx\\nconst TreeItemWithLoading = forwardRef(function TreeItemWithLoadingInternal(\\n  props: TreeItem2Props,\\n  ref: React.Ref<HTMLLIElement>,\\n) {\\n  return (\\n    <TreeItem2\\n      ref={ref}\\n      {...props}\\n      slots={{\\n        label: TreeItemLabelWithLoading,\\n      }}\\n      slotProps={{\\n        label: {\\n          //@ts-expect-error this works but MUI doesn\'t officially support this prior to v6 - see https://github.com/mui/material-ui/issues/33175#issuecomment-1469725522\\n          \'data-is-loading\': props[\'data-still-to-load-ids\'].includes(\\n            props.itemId,\\n          ),\\n        },\\n      }}\\n    />\\n  );\\n});\\n```\\n\\nThe `TreeItemWithLoading` component is a wrapper around the `TreeItem2` component. You\'ll note in our code we\'re using the [`TreeItem2` component](https://mui.com/x/react-tree-view/#tree-item-components). I won\'t explicitly discuss this, but it\'s worth noting that the `TreeItem2` component is intended to replace the `TreeItem` component in a future version of MUI; the `TreeItem2` component is more flexible and allows for more customisation.\\n\\nAnyway, the `TreeItemWithLoading` component is where the magic happens. For every node in the treeview, this component will be rendered. All it does is render a `TreeItem2` component, but with some customisation. Again using the `slots` / `slotProps` properties, it customises the underlying label component that is rendered. It replaces the default label component (`TreeItem2Label`) with a custom one that shows a loader if the node is still loading, that component is named `TreeItemLabelWithLoading`. We\'ll implement it in a moment.\\n\\nThe `TreeItemWithLoading` component is also responsible for passing the `data-is-loading` prop to the `TreeItemLabelWithLoading` component. You\'ll note that we\'re just passing a `boolean` this time which we construct by comparing the id of the node to the list of ids that are still loading.\\n\\nFinally, let\'s look at the `TreeItemLabelWithLoading` component:\\n\\n```tsx\\nfunction TreeItemLabelWithLoading(props: {\\n  children: string;\\n  className: string;\\n  \'data-is-loading\': boolean;\\n}) {\\n  const { children, \'data-is-loading\': isLoading, ...other } = props;\\n  return (\\n    <TreeItem2Label {...other}>\\n      {children}\\n      {isLoading && <CircularProgress size=\\"1em\\" sx={{ marginLeft: 0.5 }} />}\\n    </TreeItem2Label>\\n  );\\n}\\n```\\n\\nThis is just a wrapper of the `TreeItem2Label` component that would have been rendered by default. The only difference being: if the `data-is-loading` value passed is truthy, a loader (`CircularProgress`) component is rendered next to the label to indicate the loading state.\\n\\nConsider the image below:\\n\\n![screenshot of the treeview](screenshot-with-loader.webp)\\n\\nNotice that there are three nodes in this picture. Two without a loader, one with. This is the result of the logic in our `TreeItemLabelWithLoading` component.\\n\\n## Putting it all together\\n\\nNow we\'ve walked through how it works, let\'s put all the code together in one place:\\n\\n```tsx\\nfunction OurComponent() {\\n  // ...\\n  return (\\n    <RichTreeView\\n      defaultExpandedItems={initialExpandedItems}\\n      multiSelect={true}\\n      checkboxSelection={true}\\n      selectedItems={selectedIds}\\n      onSelectedItemsChange={handleSelectedItemsChange}\\n      items={treeViewNodes}\\n      // note that we\'re using the TreeItemWithLoading component here\\n      slots={{ item: TreeItemWithLoading }}\\n      slotProps={{\\n        item: {\\n          //@ts-expect-error this works but MUI doesn\'t officially support this prior to v6 - see https://github.com/mui/material-ui/issues/33175#issuecomment-1469725522\\n          \'data-still-to-load-ids\': data.stillToLoad,\\n        },\\n      }}\\n    />\\n  );\\n}\\n\\nfunction TreeItemLabelWithLoading(props: {\\n  children: string;\\n  className: string;\\n  \'data-is-loading\': boolean;\\n}) {\\n  const { children, \'data-is-loading\': isLoading, ...other } = props;\\n  return (\\n    <TreeItem2Label {...other}>\\n      {children}\\n      {isLoading && <CircularProgress size=\\"1em\\" sx={{ marginLeft: 0.5 }} />}\\n    </TreeItem2Label>\\n  );\\n}\\n\\nconst TreeItemWithLoading = forwardRef(function TreeItemWithLoadingInternal(\\n  props: TreeItem2Props,\\n  ref: React.Ref<HTMLLIElement>,\\n) {\\n  return (\\n    <TreeItem2\\n      ref={ref}\\n      {...props}\\n      slots={{\\n        label: TreeItemLabelWithLoading,\\n      }}\\n      slotProps={{\\n        label: {\\n          //@ts-expect-error this works but MUI doesn\'t officially support this prior to v6 - see https://github.com/mui/material-ui/issues/33175#issuecomment-1469725522\\n          \'data-is-loading\': props[\'data-still-to-load-ids\'].includes(\\n            props.itemId,\\n          ),\\n        },\\n      }}\\n    />\\n  );\\n});\\n```\\n\\nThe above will allow you to pass data to the components rendering the elements that make up your treeview, and allow you to customise accordingly.\\n\\nI\'m not completely certain this is \\"the way\\" that is advised. But it is a way that works. I\'m going to follow up with the MUI team to ensure this is good advice."},{"id":"web-workers-comlink-vite-tanstack-query","metadata":{"permalink":"/web-workers-comlink-vite-tanstack-query","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2024-06-23-web-workers-comlink-vite-tanstack-query/index.md","source":"@site/blog/2024-06-23-web-workers-comlink-vite-tanstack-query/index.md","title":"Web Workers, Comlink, Vite and TanStack Query","description":"Web Workers are a great way to offload work from the main thread. Comlink is a delightful way to communicate with Web Workers. TanStack Query is an awesome way to bring them together.","date":"2024-06-23T00:00:00.000Z","tags":[{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."},{"inline":false,"label":"React","permalink":"/tags/react","description":"The React library."}],"readingTime":5.735,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"web-workers-comlink-vite-tanstack-query","title":"Web Workers, Comlink, Vite and TanStack Query","authors":"johnnyreilly","tags":["typescript","react"],"image":"./title-image.png","description":"Web Workers are a great way to offload work from the main thread. Comlink is a delightful way to communicate with Web Workers. TanStack Query is an awesome way to bring them together.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"MUI React Tree View: pass data to TreeItem","permalink":"/mui-react-tree-view-pass-data-to-treeitem"},"nextItem":{"title":"Static Web Apps CLI: improve performance with Vite server proxy","permalink":"/static-web-apps-cli-improve-performance-with-vite-server-proxy"}},"content":"I\'ve written previously about combining [Web Workers and Comlink](../2020-02-21-web-workers-comlink-typescript-and-react/index.md). I recently found myself needing to use Web Workers again. As I picked them up this time I found myself making some different choices, now I was working in a codebase that used Vite to build. I ended up using [TanStack Query](https://github.com/tanstack/query) in combination with my [Web Workers](https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Using_web_workers) and [Comlink](https://github.com/GoogleChromeLabs/comlink). It was a really good fit and this post will show you how to use Web Workers with Comlink and TanStack Query.\\n\\n![title image reading \\"Web Workers, Comlink, Vite and TanStack Query\\" with the Static Web Apps CLI and Vite logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## A use case for Web Workers\\n\\nWeb Workers are a great way to offload work from the main thread. This can be particularly useful if you have a long running task that you don\'t want to block the UI thread. In my case, I was working on a feature that required a lot of computation. I wanted to keep the UI responsive whilst the numbers got crunched.\\n\\nImagine the following function in our `calculations.ts` file:\\n\\n```typescript\\nexport function expensiveCalculation(\\n  data: BigLumpOfData,\\n  from: Date,\\n  to: Date,\\n): number {\\n  // SUPER EXPENSIVE COMPUTATION CODE HERE...\\n  return importantNumber;\\n}\\n```\\n\\nThis function is doing some very expensive computation. We don\'t want to block the main thread with it. We want to use a Web Worker to offload this work. But the naked Web Worker API is a bit of a pain to work with. Instead then, we can use Comlink to make it easier.\\n\\n## Vite Comlink setup with `vite-plugin-comlink`\\n\\nAs I\'ve mentioned, we\'re working with Vite in this codebase. To get Comlink working with Vite, we can use the dedicated plugin named [`vite-plugin-comlink`](https://github.com/mathe42/vite-plugin-comlink). It is a wrapper around Comlink that simplifies using it with Vite.\\n\\nTo get started, we need to install the plugin and Comlink:\\n\\n```bash\\nnpm i --save-dev vite-plugin-comlink\\nnpm i --save comlink\\n```\\n\\nWe need the plugin at build time and Comlink at runtime. We can then add the plugin to our Vite config:\\n\\n```typescript\\nimport react from \'@vitejs/plugin-react\';\\nimport { defineConfig } from \'vite\';\\nimport { comlink } from \'vite-plugin-comlink\';\\n\\n// https://vitejs.dev/config/\\nexport default defineConfig({\\n  plugins: [comlink(), react()],\\n  worker: {\\n    plugins: () => [comlink()],\\n  },\\n});\\n```\\n\\nNote that we\'re adding the `comlink` plugin to both the main Vite config and the worker config. This is because we want to use Comlink in both places. The order of the plugins is important. We want to make sure that `comlink` is added right at the start.\\n\\nThis is a TypeScript project, so we update our `vite-env.d.ts` file to include the Comlink types:\\n\\n```typescript\\n/// <reference types=\\"vite/client\\" />\\n/// <reference types=\\"vite-plugin-comlink/client\\" />\\n```\\n\\n## Migrating our expensive calculations to Comlink\\n\\nNow we have Comlink set up, we can move our expensive calculations to a Web Worker. This is very easily achieved; we just need to create a variable that points to our Web Worker file:\\n\\n```typescript\\nconst calculationsWorker = new ComlinkWorker<\\n  typeof import(\'./calculations.js\')\\n>(new URL(\'./calculations.js\', import.meta.url), {\\n  name: \'calculationsComlink\',\\n  type: \'module\',\\n});\\n```\\n\\nThere\'s not much code above, but it\'s doing a lot. We\'re creating a new Web Worker with Comlink using the `ComlinkWorker`. This is an affordance provided by `vite-plugin-comlink` and it creates a Web Worker with Comlink. We\'re pointing it at our `calculations.js` file (`.js` as this is an ESM import representing the `calculations.ts` file). We\'re also giving the Web Worker a name, `calculationsComlink` - this will be handy when debugging. Finally, we\'re telling the Web Worker that it\'s a module. So we can use ESM imports in our Web Worker. Actually, we\'re not do that right now, but we could.\\n\\nNote also that the types will be inferred from the `calculations.js` file thanks to the `typeof import(\'./calculations.js\')`. This is a really nice feature of TypeScript.\\n\\nThe API of the `calculationsWorker` is the same as the `calculations.ts` file, with one subtle difference. All sync functions will move to being `Promise` based. So the API of the `calculationsWorker` is\\n\\n```typescript\\nexpensiveCalculation: (data: BigLumpOfData, from: Date, to: Date) =>\\n  Promise<number>;\\n```\\n\\nWhich is pretty much the same as the original function in `calculations.ts`:\\n\\n```typescript\\nexpensiveCalculation: (data: BigLumpOfData, from: Date, to: Date) => number;\\n```\\n\\nThe only difference is that the return type is now a `Promise<number>` rather than a `number`. If your function was `async` / `Promise`-based originally, then the API remains identical.\\n\\nIncidentally; this is terrific. The journey from a standard codebase to a Web Worker enabled codebase is really smooth.\\n\\n## Using TanStack Query to interact with the Comlink Web Worker\\n\\nThe final piece of the puzzle is to use TanStack Query (AKA React Query) to interact with our Web Worker. TanStack Query is a fantastic library for managing data in applications. It\'s a great fit for working with our Web Worker because it\'s designed to work with async data. We can use TanStack Query to manage interactions with our Web Worker.\\n\\nTo install it we run:\\n\\n```bash\\nnpm i @tanstack/react-query\\n```\\n\\nWe can then use it in our component like so:\\n\\n```typescript\\nconst expensiveCalculationResult = useQuery({\\n  queryKey: [\'expensiveCalculation\', data, from, to],\\n  queryFn: () =>\\n    calculationsWorker.expensiveCalculation(data, new Date(from), new Date(to)),\\n  staleTime: Infinity,\\n});\\n```\\n\\nThis is a pretty standard use of TanStack Query. We\'re using the `useQuery` hook to fetch data. We\'re passing in a `queryKey` that represents the data we\'re fetching. We\'re passing in a `queryFn` that fetches the data. In this case, it\'s calling our Web Worker. We\'re also setting `staleTime` to `Infinity` so that the data never goes stale. This is an optimisation, because we\'re not going to be refetching the data unless the dependencies change. (If you\'d like to understand more about `staleTime` read [TkDodo\'s excellent post](https://tkdodo.eu/blog/practical-react-query#the-defaults-explained)).\\n\\nThis is a really nice way to interact with our Web Worker. We\'re using TanStack Query to manage the data fetching and Comlink to interact with the Web Worker. It\'s a really nice fit.\\n\\n## Putting it all together\\n\\nHere\'s the full component that uses the Web Worker, Comlink and TanStack Query:\\n\\n```tsx\\nimport { useQuery } from \'@tanstack/react-query\';\\n\\nconst calculationsWorker = new ComlinkWorker<\\n  typeof import(\'./calculations.js\')\\n>(new URL(\'./calculations.js\', import.meta.url), {\\n  name: \'calculationsComlink\',\\n  type: \'module\',\\n});\\n\\nexport function WorkerDemo({\\n  data,\\n  from,\\n  to,\\n}: {\\n  data: BigLumpOfData;\\n  from: string;\\n  to: string;\\n}) {\\n  const expensiveCalculationResult = useQuery({\\n    queryKey: [\'expensiveCalculation\', data, from, to],\\n    queryFn: () =>\\n      calculationsWorker.expensiveCalculation(\\n        data,\\n        new Date(from),\\n        new Date(to),\\n      ),\\n    staleTime: Infinity,\\n  });\\n\\n  return (\\n    <div>\\n      <h1>Web Workers, Comlink and Tanstack Query in action!</h1>\\n\\n      {expensiveCalculationResult.data ? (\\n        <p>Calculation result: {expensiveCalculationResult.data}</p>\\n      ) : expensiveCalculationResult.isPending ? (\\n        <p>Calculating...</p>\\n      ) : expensiveCalculationResult.error ? (\\n        <p>Error: {expensiveCalculationResult.error}</p>\\n      ) : (\\n        <p>...</p>\\n      )}\\n    </div>\\n  );\\n}\\n```\\n\\nThis is a really nice way to use Web Workers with TanStack Query. We\'re using Comlink to interact with the Web Worker and TanStack Query to manage the data fetching. It\'s a really nice fit. I hope this helps you to use Web Workers in your Vite projects. Enjoy!"},{"id":"static-web-apps-cli-improve-performance-with-vite-server-proxy","metadata":{"permalink":"/static-web-apps-cli-improve-performance-with-vite-server-proxy","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2024-06-18-static-web-apps-cli-improve-performance-with-vite-server-proxy/index.md","source":"@site/blog/2024-06-18-static-web-apps-cli-improve-performance-with-vite-server-proxy/index.md","title":"Static Web Apps CLI: improve performance with Vite server proxy","description":"The Static Web Apps CLI has a slow proxy server. This post shows you how to improve performance by using Vite server proxy instead.","date":"2024-06-18T00:00:00.000Z","tags":[{"inline":false,"label":"Azure Static Web Apps","permalink":"/tags/azure-static-web-apps","description":"The Azure Static Web Apps service."},{"inline":false,"label":"Node.js","permalink":"/tags/node-js","description":"The Node.js runtime."}],"readingTime":5.63,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"static-web-apps-cli-improve-performance-with-vite-server-proxy","title":"Static Web Apps CLI: improve performance with Vite server proxy","authors":"johnnyreilly","tags":["azure static web apps","node.js"],"image":"./title-image.png","description":"The Static Web Apps CLI has a slow proxy server. This post shows you how to improve performance by using Vite server proxy instead.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Web Workers, Comlink, Vite and TanStack Query","permalink":"/web-workers-comlink-vite-tanstack-query"},"nextItem":{"title":"Dual Publishing ESM and CJS Modules with tsup and Are the Types Wrong?","permalink":"/dual-publishing-esm-cjs-modules-with-tsup-and-are-the-types-wrong"}},"content":"I often use the Azure Static Web Apps CLI for local development. It\'s not only handy when building Azure Static Web Apps, but also when building other types of web app, which also rely upon both a frontend server and some kind of API server. The Azure Static Web Apps CLI is particularly handy if you want to spoof authentication / authorization as well.\\n\\nChanges in the behaviour of Node.js in version 17 caused issues with the Static Web Apps CLI. You can read a [previous post which discussed this](../2023-05-20-static-web-apps-cli-node-18-could-not-connect-to-api/index.md). However, whilst the issue was fixed in version 1.1.4 of the Static Web Apps CLI, it caused significant performance regressions in the CLIs dev server functionality.\\n\\nThis post shows you how to improve your developer experience by using Vite server proxy instead.\\n\\n![title image reading \\"Static Web Apps CLI: improve performance with Vite server proxy\\" with the Static Web Apps CLI and Vite logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## What the Static Web Apps CLI does\\n\\nThe Static Web Apps CLI is a great tool for local development. It spins up a local server which proxies requests to your frontend / backend servers. This is a great way to handle local development without having to work around CORS issues. There\'s also an extra bonus in the box, in that you can spoof authentication / authorization as well.\\n\\nSo to be clear, for the purposes of local development, the Static Web Apps CLI is a dev server that does three important things:\\n\\n1. Proxies requests across to your front end dev server (in my case this is Vite, and this will become relevant later)\\n2. Proxies requests across to your backend server\\n3. Provides an authentication / authorization mechanism\\n\\n## The issue with the Static Web Apps CLI\\n\\nAll of this is great, but the performance of the proxy server has been poor since version 1.1.4 of the Static Web Apps CLI. So whilst the issue with Node.js 18 was fixed, the performance of the proxy server was impacted. Consider a comment from [this GitHub issue](https://github.com/Azure/static-web-apps-cli/issues/663#issuecomment-1646061953):\\n\\n> It works for me as well, the main problem is that the performance of the site is terrible when compared not running through swa cli. Pages take long time to load and resources (for example > 1s to load a font)\\n\\nRegrettably, performance has not improved significantly since that time. [You can read more about the performance issues in this issue](https://github.com/Azure/static-web-apps-cli/issues/736).\\n\\n## Proxying with Vite\\n\\nSo what can we do about this? Well, for my local development I\'m using Vite as my frontend server. Vite has a server proxy feature which is very performant.\\n\\n[Sam \\"Betty\\" McKoy](https://github.com/bzbetty) suggested that we could use the Vite server proxy instead of the Static Web Apps CLI proxy server [here](https://github.com/Azure/static-web-apps-cli/issues/736#issuecomment-2143373208). This is a great idea, and it\'s very easy to do.\\n\\nMy `package.json` file in the frontend app has the following scripts:\\n\\n```json\\n    \\"scripts\\": {\\n        \\"dev\\": \\"vite\\",\\n        \\"start\\": \\"swa start http://localhost:5173 --run \\\\\\"npm run dev\\\\\\" --api-location ./api\\"\\n    }\\n```\\n\\nRight now, once I\'ve run `npm start` the Static Web Apps CLI will start up the frontend server and the backend server. I can then browse to `http://localhost:4280` and see my app running. Front end requests will be proxied to `http://localhost:5173` and backend requests will be proxied to my API (in my case it\'s an Azure Function served on `http://localhost:7071`) - but it could be somewhere else, and [that\'s configurable](https://azure.github.io/static-web-apps-cli/docs/cli/swa-start/#start-api-server-manually).\\n\\nTo get the Vite server proxy working, I need to change the `vite.config.ts` to add the following `server.proxy` configuration:\\n\\n```ts\\nimport { defineConfig } from \'vite\';\\n\\n// https://vitejs.dev/config/\\nexport default defineConfig({\\n  // ...\\n  server: {\\n    proxy: {\\n      \'/api\': {\\n        target: \'http://127.0.0.1:4280\',\\n        changeOrigin: true,\\n        autoRewrite: true,\\n      },\\n      \'/.auth\': {\\n        target: \'http://127.0.0.1:4280\',\\n        changeOrigin: true,\\n        autoRewrite: true,\\n      },\\n    },\\n  },\\n});\\n```\\n\\nWhat does this do? Well, for requests that go to Vite (on `http://localhost:5173`), thanks to this change, requests with the prefix `/api` and `/.auth` are now proxied across back to the Static Web App CLI server at `http://localhost:4280`.\\n\\nYou\'ll note a few options being set:\\n\\n- [`changeOrigin`](https://vitejs.dev/config/server-options.html#server-proxy) - will change both host and origin headers to match the target\\n- [`autoRewrite`](https://github.com/http-party/node-http-proxy#options) - rewrites the location host/port on (201/301/302/307/308) redirects based on requested host/port.\\n\\nYou\'ll also note we\'re using a `target` of `http://127.0.0.1:4280` rather than `http://localhost:4280`. This is because of the changes in Node.js 17 and above which mean that IPv6 is the default DNS instead of IPv4.\\n\\nIf you were to try using a `target` of `http://localhost:4280` in the configuration you may see an error like below due to the IPv6 DNS issue:\\n\\n```bash\\n[run] 1:38:29 PM [vite] http proxy error: /.auth/login/aad\\n[run] Error: connect ECONNREFUSED ::1:4280\\n[run]     at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1555:16)\\n```\\n\\nIf you stick with `http://127.0.0.1:4280` and you should be fine. I discovered the workaround from [this GitHub issue](https://github.com/vitejs/vite/discussions/7620#discussioncomment-5689650).\\n\\n## Comparing performance\\n\\nWith the Vite `server.proxy` configuration in place, our app is now usable on 2 ports. We can browse to both `http://localhost:5173` and `http://localhost:4280` - both ports will show our app running. Going to `http://localhost:5173` is using Vite directly for static assets and proxying back to `http://localhost:4280` for API / auth. Going to `http://localhost:4280` is using the Static Web Apps CLI to proxy to Vite for static assets, uses itself for auth and proxies any API requests.\\n\\nI\'m going to take a medium sized app and run it with both configurations. I\'ll then use the Chrome DevTools to compare the performance of the two configurations. It\'ll be a basic test, but it should give us an idea of the performance difference.\\n\\nThe Static Web App CLI (`http://localhost:4280`) takes just over **10 seconds** to load the app.\\n\\n![screenshot of devtools showing 10 seconds finish time](devtools-performance-static-web-app-cli.png)\\n\\nThe Vite server proxy (`http://localhost:5173`) takes just around **1.5 seconds** to load the app.\\n\\n![screenshot of devtools showing 10 seconds finish time](devtools-performance-vite-server-proxy.png)\\n\\nThis is a significant improvement in performance. The Vite server proxy approach is nearly **10x faster** than the Static Web Apps CLI proxy server.\\n\\nIt\'s worth noting that we\'re still using the Static Web Apps CLI for authentication / authorization and for hitting the backend server. However, given that static assets are by far the most common request, this change will make a big difference to your local development experience.\\n\\n## Summary\\n\\nThe Static Web Apps CLI is a great tool for local development. However, the performance of the proxy server has been poor since version 1.1.4. This post has shown you how to improve performance by using the Vite server proxy instead. The Vite server proxy is much faster than the Static Web Apps CLI proxy server. This is a simple change to make, and it will make a big difference to your local development experience. Give it a try and see how much faster your app loads!"},{"id":"dual-publishing-esm-cjs-modules-with-tsup-and-are-the-types-wrong","metadata":{"permalink":"/dual-publishing-esm-cjs-modules-with-tsup-and-are-the-types-wrong","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2024-06-15-dual-publishing-esm-cjs-modules-with-tsup-are-the-types-wrong/index.md","source":"@site/blog/2024-06-15-dual-publishing-esm-cjs-modules-with-tsup-are-the-types-wrong/index.md","title":"Dual Publishing ESM and CJS Modules with tsup and Are the Types Wrong?","description":"Learn how to publish a package that supports both ECMAScript modules (ESM) and CommonJS modules (CJS) using tsup and Are the Types Wrong?","date":"2024-06-15T00:00:00.000Z","tags":[{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":6.115,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"dual-publishing-esm-cjs-modules-with-tsup-and-are-the-types-wrong","title":"Dual Publishing ESM and CJS Modules with tsup and Are the Types Wrong?","authors":"johnnyreilly","tags":["typescript"],"image":"./title-image.png","hide_table_of_contents":false,"description":"Learn how to publish a package that supports both ECMAScript modules (ESM) and CommonJS modules (CJS) using tsup and Are the Types Wrong?"},"unlisted":false,"prevItem":{"title":"Static Web Apps CLI: improve performance with Vite server proxy","permalink":"/static-web-apps-cli-improve-performance-with-vite-server-proxy"},"nextItem":{"title":"MUI React Tree View: check children, uncheck parents","permalink":"/mui-react-tree-view-check-children-uncheck-parents"}},"content":"If you need to publish a package that supports both ECMAScript modules (ESM) and CommonJS modules (CJS), you can use [`tsup`](https://github.com/egoist/tsup) to do so. This post will show you how to do that and how to ensure that the types are correct using the tool [`Are the Types Wrong?`](https://github.com/arethetypeswrong/arethetypeswrong.github.io).\\n\\n![title image reading \\"Dual Publishing ESM and CJS Modules with tsup and Are the Types Wrong?\\" with the Are the Types Wrong logo](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Package support for ESM and CJS with tsup\\n\\nI recently needed to build a package that supported both ESM and CJS modules. After attempting to implement this myself, and frankly aging ten years, I decided to see if there was a tool out there that could help me. Happily, there are a number of tools that can help with this and I found that the excellent `tsup` was the easiest to use.\\n\\n`tsup` is a zero-config TypeScript bundler with a focus on speed and simplicity. It\'s a great tool for building and bundling your TypeScript code into ESM and CJS modules and is powered by [`esbuild`](https://github.com/evanw/esbuild).\\n\\nTo get started, you can install `tsup` to your project:\\n\\n```bash\\nnpm install tsup --save-dev\\n```\\n\\nThen you can add a script to your `package.json` to build your package:\\n\\n```json\\n{\\n  \\"scripts\\": {\\n    \\"build\\": \\"tsup src/index.ts --format cjs,esm --dts --clean --sourcemap\\"\\n  }\\n}\\n```\\n\\nIn this example, we\'re building the `src/index.ts` file into both CJS and ESM modules, generating type definitions, cleaning up the output directory prior to build, and generating sourcemaps. You can, if you like, use a `tsup.config.ts` file to configure `tsup` but I found the above to be sufficient for my needs.\\n\\nWhen you run `npm run build`, you\'ll see that `tsup` has created a `dist` directory with the following structure:\\n\\n```bash\\ndist/\\n  index.cjs\\n  index.cjs.map\\n  index.d.cts\\n  index.js\\n  index.js.map\\n  index.d.ts\\n```\\n\\nThe `index.js` and `index.js.map` files are the ESM modules and the `index.cjs` and `index.cjs.map` files are the CJS modules. The `index.d.ts` and `index.d.cts` files contain the type definitions.\\n\\nThis is great news! You now have a package that supports both ESM and CJS modules. But now we need to get our `package.json` file set up correctly.\\n\\n## Linting your package.json file with Are the Types Wrong?\\n\\nWhen you\'re publishing a package, it\'s important to ensure that your `package.json` file is set up correctly. It is the manifest that other packages consume. If it is not set up correctly, things won\'t work at runtime and you\'ll have an unpleasant engineering experience in your IDE as well. Getting it right includes ensuring that the `exports` field is set up correctly to support both ESM and CJS modules. This can be a bit tricky to get right, so it\'s a good idea to use a tool to help you.\\n\\nI made the following additions to my `package.json` file to support both ESM and CJS modules:\\n\\n```json\\n{\\n  \\"//\\": \\"This is the correct way to set up a package with a `src/index.ts` root file that supports both ESM and CJS modules.\\",\\n  \\"type\\": \\"module\\",\\n  \\"main\\": \\"./dist/index.cjs\\",\\n  \\"module\\": \\"./dist/index.js\\",\\n  \\"types\\": \\"./dist/index.d.ts\\",\\n  \\"exports\\": {\\n    \\"import\\": {\\n      \\"types\\": \\"./dist/index.d.ts\\",\\n      \\"import\\": \\"./dist/index.js\\"\\n    },\\n    \\"require\\": {\\n      \\"types\\": \\"./dist/index.d.cts\\",\\n      \\"require\\": \\"./dist/index.cjs\\"\\n    }\\n  }\\n}\\n```\\n\\nLet\'s break this down:\\n\\n- The `type` field is set to `module` to indicate that the package supports ESM modules.\\n- The `main` field is set to `./dist/index.cjs` to indicate the entry point for CJS modules.\\n- The `module` field is set to `./dist/index.js` to indicate the entry point for ESM modules.\\n- The `types` field is set to `./dist/index.d.ts` to indicate the type definitions file for ESM modules.\\n\\nWe then set up the `exports` field to support both ESM and CJS modules. The `import` field is set up to support ESM modules and the `require` field is set up to support CJS modules. In each, the `types` field is set to the relevant type definitions file - significantly, the `types` comes first.\\n\\nHere I\'ve jumped straight to the good part here. I\'ve given you the correct way to set up the `exports` field for a package that supports both ESM and CJS modules. But how do you know if your `package.json` file is set up correctly?\\n\\nWell, in my case I used the tool [`Are the Types Wrong`](https://arethetypeswrong.github.io/) to lint my `package.json` file. `Are the Types Wrong` is a tool that checks your `package.json` file to ensure that it\'s set up correctly for publishing a package that supports both ESM and CJS modules. You\'re possibly aware of the [website](https://arethetypeswrong.github.io/) but did you know that there\'s a CLI tool that you can use to lint your `package.json` file?\\n\\nThere is.\\n\\nIf you would like to read the full documentation on the CLI tool, you can find it [here](https://github.com/arethetypeswrong/arethetypeswrong.github.io/blob/main/packages/cli/README.md). However, if you\'re like me and in a hurry, you can just use it like this:\\n\\n- cd into your project directory\\n- run `npx --yes @arethetypeswrong/cli --pack .`\\n- et voila, your `package.json` file will be linted for type issues\\n\\nTo see what this looks like, here\'s the output from running `npx @arethetypeswrong/cli` in the project directory of a package with the `package.json` file _not_ set up correctly:\\n\\n```bash\\napi v1.0.0\\n\\nBuild tools:\\n- typescript@^5.2.2\\n\\n\u274C Import resolved to JavaScript files, but no type declarations were found. https://github.com/arethetypeswrong/arethetypeswrong.github.io/blob/main/docs/problems/UntypedResolution.md\\n\\n\\n\u250C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n\u2502                   \u2502 \\"api\\"       \u2502\\n\u251C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\\n\u2502 node10            \u2502 \u274C No types \u2502\\n\u251C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\\n\u2502 node16 (from CJS) \u2502 \u274C No types \u2502\\n\u251C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\\n\u2502 node16 (from ESM) \u2502 \u274C No types \u2502\\n\u251C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\\n\u2502 bundler           \u2502 \u274C No types \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n```\\n\\nBy contrast, here\'s how it looks when you use the correct `package.json` file:\\n\\n```bash\\nshared v1.0.0\\n\\nBuild tools:\\n- typescript@^5.4.5\\n- tsup@^8.1.0\\n\\n No problems found \uD83C\uDF1F\\n\\n\\n\u250C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n\u2502                   \u2502 \\"api\\"    \u2502\\n\u251C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\\n\u2502 node10            \u2502 \uD83D\uDFE2       \u2502\\n\u251C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\\n\u2502 node16 (from CJS) \u2502 \uD83D\uDFE2 (CJS) \u2502\\n\u251C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\\n\u2502 node16 (from ESM) \u2502 \uD83D\uDFE2 (ESM) \u2502\\n\u251C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\\n\u2502 bundler           \u2502 \uD83D\uDFE2       \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n```\\n\\nIsn\'t that great? You can now be confident that your `package.json` file is set up correctly for publishing a package that supports both ESM and CJS modules, both from a runtime code and a types perspective. So the full `package.json` might look something like this:\\n\\n```json\\n{\\n  \\"name\\": \\"our-package\\",\\n  \\"version\\": \\"1.0.0\\",\\n  \\"description\\": \\"\\",\\n  \\"scripts\\": {\\n    \\"build\\": \\"tsup src/index.ts --format cjs,esm --dts --clean --sourcemap\\"\\n  },\\n  \\"author\\": \\"\\",\\n  \\"license\\": \\"ISC\\",\\n  \\"dependencies\\": {},\\n  \\"devDependencies\\": {\\n    \\"tsup\\": \\"^8.1.0\\",\\n    \\"typescript\\": \\"^5.4.5\\"\\n  },\\n  \\"type\\": \\"module\\",\\n  \\"main\\": \\"./dist/index.cjs\\",\\n  \\"module\\": \\"./dist/index.js\\",\\n  \\"types\\": \\"./dist/index.d.ts\\",\\n  \\"exports\\": {\\n    \\"import\\": {\\n      \\"types\\": \\"./dist/index.d.ts\\",\\n      \\"import\\": \\"./dist/index.js\\"\\n    },\\n    \\"require\\": {\\n      \\"types\\": \\"./dist/index.d.cts\\",\\n      \\"require\\": \\"./dist/index.cjs\\"\\n    }\\n  }\\n}\\n```\\n\\n## Conclusion\\n\\nIn this post, we\'ve seen how to publish a package that supports both ESM and CJS modules using `tsup`. We\'ve also seen how to ensure that the types are correct using the tool `Are the Types Wrong`.\\n\\nIt\'s probably worth saying that `Are the Types Wrong` doesn\'t check everything, and that `tsup`s method of dual compilation makes some unchecked assumptions too. Using these tools isn\'t a silver bullet, given how complicated the ESM/CJS compatibility story is, but they do their best to help.\\n\\nThanks to [Andrew Branch](https://github.com/andrewbranch) for putting together `Are the Types Wrong` and to the folk that work on `tsup` for making it easy to build packages that support both ESM and CJS modules. Thanks also to Andrew for his help in reviewing this post.\\n\\nRemember, friends don\'t let friends publish packages with incorrect types!"},{"id":"mui-react-tree-view-check-children-uncheck-parents","metadata":{"permalink":"/mui-react-tree-view-check-children-uncheck-parents","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2024-05-25-mui-react-tree-view-check-children-uncheck-parents/index.md","source":"@site/blog/2024-05-25-mui-react-tree-view-check-children-uncheck-parents/index.md","title":"MUI React Tree View: check children, uncheck parents","description":"Learn how to use the MUI treeview component with behaviour that selects child nodes when parents are select and deselects parent nodes when children are deselected.","date":"2024-05-25T00:00:00.000Z","tags":[{"inline":false,"label":"React","permalink":"/tags/react","description":"The React library."},{"inline":false,"label":"MUI","permalink":"/tags/mui","description":"The MUI / Material UI component library."}],"readingTime":4.98,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"mui-react-tree-view-check-children-uncheck-parents","title":"MUI React Tree View: check children, uncheck parents","authors":"johnnyreilly","tags":["react","mui"],"image":"./title-image.png","hide_table_of_contents":false,"description":"Learn how to use the MUI treeview component with behaviour that selects child nodes when parents are select and deselects parent nodes when children are deselected."},"unlisted":false,"prevItem":{"title":"Dual Publishing ESM and CJS Modules with tsup and Are the Types Wrong?","permalink":"/dual-publishing-esm-cjs-modules-with-tsup-and-are-the-types-wrong"},"nextItem":{"title":"Serialising ASP.NET method calls for later execution","permalink":"/serialising-aspnet-method-calls-for-later-execution"}},"content":"Every now and then, I need to use a treeview component in a React application. The Material-UI (MUI) library provides a [treeview component](https://mui.com/x/react-tree-view/) that is very useful. However, some of the default behaviours of the component differ from that which you typically find in a treeview component. I\'m speaking, of course, about node selection. I\'m used to a treeview component that, when a parent node is selected, auto selects the child nodes underneath. And by turn, when nodes are deselected, the parent nodes get deselected.\\n\\nThis post documents how to implement this behaviour with the MUI treeview component.\\n\\nSince initially writing this, I\'ve learned that it is likely that the kind of behaviour I\'m hand-rolling here, will natively land in the component. So, all being well, what follows should become unnecessary! To track native support [watch this GitHub issue](https://github.com/mui/mui-x/issues/12883).\\n\\n![title image reading \\"MUI React Tree View: check children, uncheck parents\\" with the MUI logo](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## The default behaviour and the desired behaviour\\n\\nBy default, the MUI treeview component checks and unchecks nodes individually. Each node is an island; when a node is selected or deselected, it has no bearing on any other nodes.\\n\\nWhat I\'d rather is some relationship between parent and child nodes during selection / deselection. The behaviour I\'d like has the following characteristics:\\n\\n- when a parent node is selected, all child nodes are selected\\n- when a parent node is deselected, all child nodes are deselected\\n- when a child node is deselected, the parent node is deselected, and any children are deselected also\\n- finally, if all children are selected, the parent node should be selected.\\n\\nThis is the behaviour that I\'m used to in a treeview component. In action it looks like this:\\n\\n![animated gif demonstrating new behaviour](treeview-demo.gif)\\n\\n## The code\\n\\nWe can implement this behaviour by tracking the selected nodes and then determining which nodes should be selected based on the current selection. The code below demonstrates how to do this:\\n\\n```tsx\\nimport * as React from \'react\';\\nimport Box from \'@mui/material/Box\';\\nimport { RichTreeView } from \'@mui/x-tree-view/RichTreeView\';\\nimport { TreeViewBaseItem } from \'@mui/x-tree-view/models\';\\n\\nconst MUI_X_PRODUCTS: TreeViewBaseItem[] = [\\n  {\\n    id: \'grid\',\\n    label: \'Data Grid\',\\n    children: [\\n      { id: \'grid-community\', label: \'@mui/x-data-grid\' },\\n      { id: \'grid-pro\', label: \'@mui/x-data-grid-pro\' },\\n      { id: \'grid-premium\', label: \'@mui/x-data-grid-premium\' },\\n    ],\\n  },\\n  {\\n    id: \'pickers\',\\n    label: \'Date and Time Pickers\',\\n    children: [\\n      { id: \'pickers-community\', label: \'@mui/x-date-pickers\' },\\n      { id: \'pickers-pro\', label: \'@mui/x-date-pickers-pro\' },\\n    ],\\n  },\\n  {\\n    id: \'charts\',\\n    label: \'Charts\',\\n    children: [{ id: \'charts-community\', label: \'@mui/x-charts\' }],\\n  },\\n  {\\n    id: \'tree-view\',\\n    label: \'Tree View\',\\n    children: [{ id: \'tree-view-community\', label: \'@mui/x-tree-view\' }],\\n  },\\n];\\n\\nfunction getParentNode(\\n  items: TreeViewBaseItem[],\\n  id: string,\\n): TreeViewBaseItem | undefined {\\n  for (const item of items) {\\n    if (item.children) {\\n      if (item.children.some((child) => child.id === id)) {\\n        // The current item is the parent of the supplied id\\n        return item;\\n      } else {\\n        // Recursively call the function for the children of the current item\\n        const parentNode = getParentNode(item.children, id);\\n        if (parentNode) {\\n          return parentNode;\\n        }\\n      }\\n    }\\n  }\\n\\n  // No parent found\\n  return undefined;\\n}\\n\\nfunction getAllParentIds(items: TreeViewBaseItem[], id: string) {\\n  const parentIds: string[] = [];\\n  let parent = getParentNode(items, id);\\n  while (parent) {\\n    parentIds.push(parent.id);\\n    parent = getParentNode(items, parent.id);\\n  }\\n  return parentIds;\\n}\\n\\nfunction getSelectedIdsAndChildrenIds(\\n  items: TreeViewBaseItem[],\\n  selectedIds: string[],\\n) {\\n  const selectedIdIncludingChildrenIds = new Set([...selectedIds]);\\n\\n  for (const item of items) {\\n    if (selectedIds.includes(item.id)) {\\n      // Add the current item\'s id to the result array\\n      selectedIdIncludingChildrenIds.add(item.id);\\n\\n      // Recursively call the function for the children of the current item\\n      if (item.children) {\\n        const childrenIds = item.children.map((child) => child.id);\\n        const childrenSelectedIds = getSelectedIdsAndChildrenIds(\\n          item.children,\\n          childrenIds,\\n        );\\n        childrenSelectedIds.forEach((selectedId) =>\\n          selectedIdIncludingChildrenIds.add(selectedId),\\n        );\\n      }\\n    } else if (item.children) {\\n      // walk the children to see if selections lay in there also\\n      const childrenSelectedIds = getSelectedIdsAndChildrenIds(\\n        item.children,\\n        selectedIds,\\n      );\\n      childrenSelectedIds.forEach((selectedId) =>\\n        selectedIdIncludingChildrenIds.add(selectedId),\\n      );\\n    }\\n  }\\n\\n  return [...Array.from(selectedIdIncludingChildrenIds)];\\n}\\n\\nfunction determineIdsToSet(\\n  items: TreeViewBaseItem[],\\n  newIds: string[],\\n  currentIds: string[],\\n) {\\n  const isDeselectingNode = currentIds.length > newIds.length;\\n  if (isDeselectingNode) {\\n    const removed = currentIds.filter((id) => !newIds.includes(id))[0];\\n\\n    const parentIdsToRemove = getAllParentIds(items, removed);\\n\\n    const childIdsToRemove = getSelectedIdsAndChildrenIds(items, [removed]);\\n\\n    const newIdsWithParentsAndChildrenRemoved = newIds.filter(\\n      (id) => !parentIdsToRemove.includes(id) && !childIdsToRemove.includes(id),\\n    );\\n\\n    return newIdsWithParentsAndChildrenRemoved;\\n  }\\n\\n  const added = newIds.filter((id) => !currentIds.includes(id))[0];\\n  const idsToSet = getSelectedIdsAndChildrenIds(items, newIds);\\n  let parent = getParentNode(items, added);\\n  while (parent) {\\n    const childIds = parent.children?.map((node) => node.id) ?? [];\\n    const allChildrenSelected = childIds.every((id) => idsToSet.includes(id));\\n    if (allChildrenSelected) {\\n      idsToSet.push(parent.id);\\n      parent = getParentNode(items, parent.id);\\n    } else {\\n      break;\\n    }\\n  }\\n  return idsToSet;\\n}\\n\\nexport default function CheckboxSelection() {\\n  const [selectedIds, setSelectedIds] = React.useState<string[]>([]);\\n\\n  const handleSelectedItemsChange = (\\n    _event: React.SyntheticEvent,\\n    ids: string[],\\n  ) => {\\n    setSelectedIds(determineIdsToSet(MUI_X_PRODUCTS, ids, selectedIds));\\n  };\\n\\n  return (\\n    <Box sx={{ height: 264, flexGrow: 1, maxWidth: 400 }}>\\n      <RichTreeView\\n        defaultExpandedItems={MUI_X_PRODUCTS.map((x) => x.id)}\\n        multiSelect={true}\\n        checkboxSelection={true}\\n        selectedItems={selectedIds}\\n        onSelectedItemsChange={handleSelectedItemsChange}\\n        items={MUI_X_PRODUCTS}\\n      />\\n    </Box>\\n  );\\n}\\n```\\n\\nYou\'ll see above that we\'re using the `RichTreeView` component from `@mui/x-tree-view`. We\'re setting it to `multiSelect` and `checkboxSelection` to allow for multiple selections and to show checkboxes next to each node. We\'re also tracking the selected nodes in the `selectedIds` state variable.\\n\\nThe `handleSelectedItemsChange` function is called whenever the selection changes. It determines which nodes should be selected based on the current selection and the new selection. The `determineIdsToSet` function is responsible for this logic.\\n\\nThe first thing it does is determine if a node is being deselected or selected, by comparing the length of the current selection with the new selection. If a node is being deselected, it finds the parent and child nodes of the deselected node and removes them from the new selection. If a node is being selected, it selects all the children of that node. It also finds the parent node and checks if all child nodes are selected. If they are, it selects the parent node.\\n\\n## Conclusion\\n\\nThe code above demonstrates how to implement a treeview component with parent-child node selection behaviour. This behaviour is, in my opinion, more intuitive and user-friendly than the default behaviour of the MUI treeview component.\\n\\nYou can also see this code in action [on StackBlitz](https://stackblitz.com/edit/mui-react-tree-view-check-children-uncheck-parents?file=Demo.tsx)."},{"id":"serialising-aspnet-method-calls-for-later-execution","metadata":{"permalink":"/serialising-aspnet-method-calls-for-later-execution","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2024-05-12-serialising-aspnet-method-calls-for-later-execution/index.md","source":"@site/blog/2024-05-12-serialising-aspnet-method-calls-for-later-execution/index.md","title":"Serialising ASP.NET method calls for later execution","description":"How can we take a method call, serialise it, perhaps store it in a database, and then later rehydrate and execute?","date":"2024-05-12T00:00:00.000Z","tags":[{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."},{"inline":false,"label":"C#","permalink":"/tags/csharp","description":"The C# programming language."}],"readingTime":5.72,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"serialising-aspnet-method-calls-for-later-execution","title":"Serialising ASP.NET method calls for later execution","authors":"johnnyreilly","image":"./title-image.png","tags":["azure","c#"],"description":"How can we take a method call, serialise it, perhaps store it in a database, and then later rehydrate and execute?","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"MUI React Tree View: check children, uncheck parents","permalink":"/mui-react-tree-view-check-children-uncheck-parents"},"nextItem":{"title":"Large Language Models, Open API, View Models and the Backend for Frontend Pattern","permalink":"/large-language-models-view-models-backend-for-frontend"}},"content":"Let\'s start with \\"why\\". Imagine you have an operation that you\'d like to perform, but before that operation is performed, some other things need to take place first. Maybe it needs to be approved by someone, maybe you need an explicit record of what method is to be executed.\\n\\nNow you could build a mechanism to manually cater for each scenario that triggered a method call. But that\'s a lot of boilerplate code for each implementation, and given we might want to cater for many scenarios, it wouldn\'t scale particularly well as an approach.\\n\\nSo how can we take a method call, serialise it, perhaps store it in a database, and then later rehydrate and execute?\\n\\n![title image reading \\"Serialising ASP.NET method calls for later execution\\" with the C# logo](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## What does serialising our method call require?\\n\\nTo serialise a method call, what do we need to store? Three things:\\n\\n1. The type of object that contains the method we want to subsequently invoke\\n2. The method on that object\\n3. The parameter values that will be passed to the method when it is called\\n\\nPretty simple, right? It\'s worth highlighting that there is an underlying assumption for this approach:\\n\\n**The method call does not depend on the object being in a specific state for the operation to succeed.**\\n\\nSo if, before calling a method on that object, you need to call another method called `OpenConnection` (for example) then this approach would not work. Likewise if subsequent cleanup is required after a method is called, this approach would not work.\\n\\nThe analogy may not be entirely accurate, but think of each method call as needing to be an atomic operation and you\'re probably heading in the right direction.\\n\\n## How do we serialise our method call?\\n\\nFirst of all, we need a data structure to store the information we need. We could use a `record` like this:\\n\\n```cs\\npublic record MethodCall(string ServiceName, string MethodName, object[] Parameters);\\n```\\n\\nThis record will store the name of the service, the name of the method, and the parameters that will be passed to the method when it is called.\\n\\nNext we need an example service that we can call. For instance:\\n\\n```cs\\npublic interface IOurService\\n{\\n    Task<bool> DoAThing(string name, decimal amount, bool isApproved);\\n}\\n```\\n\\nAn implementation of this service would be registered with the DI container when the application starts up. We don\'t need to know anything about the implementation of the service, just that it exists and that we can call methods on it.\\n\\nIf we consider a call to this method, it might look like this:\\n\\n```cs\\nIOurService.DoAThing(\\"the name\\", 100m, true);\\n```\\n\\nThe above can be represented as a `MethodCall` like this:\\n\\n```cs\\nMethodCall methodCall = new (\\n    ServiceName: typeof(IOurService).FullName ?? throw new InvalidOperationException(\\"Service name cannot be null\\"),\\n    MethodName: nameof(IOurService.DoAThing),\\n    Parameters: [ \\"the name\\", 100m, true]\\n);\\n```\\n\\nI\'m not going to do so in this post, but the `MethodCall` could be stored in a database. This is powerful because it means that we can store the method call, and then later rehydrate it and execute it.\\n\\n## How do we deserialise our method call and execute it?\\n\\nNow that we\'ve looked at how to serialise a method call, let\'s look at how we can deserialise and execute it. We need a class that can take a `MethodCall` and execute it. Herewith the `MethodCallInvoker` class that does just that:\\n\\n```cs\\npublic class MethodCallInvoker(\\n    IServiceProvider serviceProvider,\\n    MethodCall operation\\n)\\n{\\n    public async Task<object?> InvokeAsync()\\n    {\\n        Type? serviceType = Type.GetType(operation.ServiceName ?? throw new InvalidOperationException(\\"Service name cannot be null\\"));\\n        object? service = serviceProvider.GetService(serviceType ?? throw new InvalidOperationException(\\"Service type cannot be null\\"));\\n        MethodInfo? serviceMethod = serviceType.GetMethod(operation.MethodName) ?? throw new InvalidOperationException(\\"Method info cannot be null\\");\\n\\n        List<object> parameters = [];\\n        ParameterInfo[] requiredParameters = serviceMethod.GetParameters();\\n        for (int i = 0; i < requiredParameters.Length; i++)\\n        {\\n            ParameterInfo requiredParameter = requiredParameters[i];\\n            object? suppliedParameter = operation.Parameters[i];\\n\\n            bool suppliedValueIsOfCorrectType = requiredParameter.ParameterType == suppliedParameter.GetType();\\n\\n            if (suppliedValueIsOfCorrectType)\\n                parameters.Add(operation.Parameters[i]);\\n            else\\n                // Convert.ChangeType is used to convert the supplied parameter to the required type eg from double to decimal\\n                parameters.Add(Convert.ChangeType(suppliedParameter, requiredParameter.ParameterType, CultureInfo.InvariantCulture));\\n        }\\n\\n        Task? task = (Task?)serviceMethod.Invoke(service, [..parameters]) ?? throw new InvalidOperationException($\\"Method {operation.MethodName} did not return a task\\");\\n\\n        await task;\\n\\n        object? result = null;\\n        if (task.GetType().IsGenericType && task.GetType().GetGenericTypeDefinition() == typeof(Task<>))\\n        {\\n            // Get the result using reflection\\n            PropertyInfo? resultProperty = task.GetType().GetProperty(\\"Result\\");\\n            result = resultProperty?.GetValue(task);\\n        }\\n\\n        return result;\\n    }\\n}\\n```\\n\\nThe `MethodCallInvoker` class takes an `IServiceProvider` and a `MethodCall` in its constructor. Remember that the `IServiceProvider` can be used to get a service that has been registered with the DI container. By giving the `MethodCallInvoker` the `IServiceProvider`, we can get the service that we need to call the method on. The `InvokeAsync` method uses reflection to get the service, and the method that needs to be called.\\n\\nWe then do some more reflection gymnastics to ensure that the parameters that are passed to the method are of the correct type. When it deserialises the parameters, the converter will make a best guess on the types of the parameters. If a parameter is not of the correct type, it uses `Convert.ChangeType` to convert the parameter to the correct type. The canonical example of this is converting a `double` to a `decimal`.\\n\\nWith all this done, the `MethodCallInvoker` is ready to call the method. Because it\'s likely that the method being invoked will be an `async` method, we expect them to return a `Task`. It\'s possible there may be a value returned as well, and if there is we unwrap it from the `Task` and return it.\\n\\n## How do we use the `MethodCallInvoker`?\\n\\nLet\'s do an end to end demonstration of how to serialise a method call, deserialise it and execute it. Here\'s how you can do it:\\n\\n```cs\\nMethodCall methodCall = new (\\n    ServiceName: typeof(IOurService).FullName ?? throw new InvalidOperationException(\\"Service name cannot be null\\"),\\n    MethodName: nameof(IOurService.DoAThing),\\n    Parameters: [ \\"the name\\", 100m, true]\\n);\\nstring json = Newtonsoft.Json.JsonConvert.SerializeObject(methodCall);\\nMethodCall deserialized = Newtonsoft.Json.JsonConvert.DeserializeObject<MethodCall>(json) ?? throw new Exception(\\"Problem deserializing\\");\\nobject? result = await new MethodCallInvoker(_serviceProvider, deserialized).InvokeAsync();\\n```\\n\\nThe above code serialises the `MethodCall` to a JSON string, deserialises it back to a `MethodCall`, and then uses the `MethodCallInvoker` to execute the method.\\n\\nWhy are we using `Newtonsoft.Json` for our serialisation / deserialisation in this example? We don\'t have to, but let\'s say we\'re persisting this method call to a Cosmos DB, Cosmos uses JSON.NET for JSON handling. So this somewhat simulates what would happen during a potential persistence to a Cosmos container / subsequent loading from a Cosmos container. Otherwise I\'d likely use `System.Text.Json`.\\n\\n## Conclusion\\n\\nIn this post, we\'ve looked at how we can serialise a method call (which could be stored in a database), and then later rehydrate and execute it. We\'ve seen how we can use reflection to get the service and method that we need to call, and how we can convert the parameters to the correct type."},{"id":"large-language-models-view-models-backend-for-frontend","metadata":{"permalink":"/large-language-models-view-models-backend-for-frontend","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2024-05-03-large-language-models-view-models-backend-for-frontend/index.md","source":"@site/blog/2024-05-03-large-language-models-view-models-backend-for-frontend/index.md","title":"Large Language Models, Open API, View Models and the Backend for Frontend Pattern","description":"To integrate LLMs with APIs, there is a need for the LLM equivalent of view models and the backend for frontend pattern. This discusses it in the context of Semantic Kernel.","date":"2024-05-03T00:00:00.000Z","tags":[{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."},{"inline":false,"label":"C#","permalink":"/tags/csharp","description":"The C# programming language."},{"inline":false,"label":"AI","permalink":"/tags/ai","description":"All things AI - Artificial Intelligence, Large Language Models and the like."}],"readingTime":7.38,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"large-language-models-view-models-backend-for-frontend","title":"Large Language Models, Open API, View Models and the Backend for Frontend Pattern","authors":"johnnyreilly","image":"./title-image.png","tags":["azure","c#","ai"],"description":"To integrate LLMs with APIs, there is a need for the LLM equivalent of view models and the backend for frontend pattern. This discusses it in the context of Semantic Kernel.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Serialising ASP.NET method calls for later execution","permalink":"/serialising-aspnet-method-calls-for-later-execution"},"nextItem":{"title":"Using Kernel Memory to Chunk Documents into Azure AI Search","permalink":"/using-kernel-memory-to-chunk-documents-into-azure-ai-search"}},"content":"Of late, I\'ve been involved in work to integrate APIs into LLM interactions, using [Semantic Kernel](https://github.com/microsoft/semantic-kernel). This post is something of a brain dump on the topic. Given how fast this space is moving, I expect what is written here to be out of date, possibly even _before_ I hit publish. But nevertheless, I hope it\'s useful.\\n\\n![title image reading \\"Large Language Models, Open API, View Models and the Backend for Frontend Pattern\\" with the Azure Open AI / Swagger logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Swagger / Open API and Semantic Kernel\\n\\nAPIs are awesome. Imagine LLMs could interact with APIs to allow us to chat directly to our data. This is what [function calling](https://platform.openai.com/docs/guides/function-calling) provides. It allows us to take some kind of API and integrate it with our LLM. This is a powerful concept, but it\'s not without its challenges.\\n\\nAPIs are often documented in Swagger / Open API. This is a great way to document APIs, but it\'s not always the best way to interact with them from an LLM point of view. We\'ll go into more detail on the problems it can present in a moment, but first let\'s look at how we can use Semantic Kernel to integrate with APIs.\\n\\nIt\'s completely possible to plug an LLM into an Open API / Swagger spec described API using Semantic Kernel. Here\'s an example of how we might do that from the [Semantic Kernel GitHub repository](https://github.com/microsoft/semantic-kernel/blob/9a4450622021ce003234863bcf4def9613ae1153/dotnet/samples/Concepts/Plugins/CreatePluginFromOpenApiSpec_Jira.cs#L69-L77):\\n\\n```cs\\nvar apiPluginRawFileURL = new Uri(\\"https://raw.githubusercontent.com/microsoft/PowerPlatformConnectors/dev/certified-connectors/JIRA/apiDefinition.swagger.json\\");\\njiraFunctions = await kernel.ImportPluginFromOpenApiAsync(\\n    \\"jiraPlugin\\",\\n    apiPluginRawFileURL,\\n    new OpenApiFunctionExecutionParameters(\\n        httpClient, tokenProvider.AuthenticateRequestAsync,\\n        serverUrlOverride: new Uri(serverUrl)\\n    )\\n);\\n```\\n\\nThe code above is creating a Jira plugin from an Open API spec. Brilliant! We didn\'t have to do any work; Semantic Kernel has done the heavy lifting for us. It\'s created a plugin that we can use to interact with Jira. Are you ready for the but?\\n\\n## The problem with Swagger / Open API and LLMs\\n\\nThe example above illustrates the simplicity of integrating. But what it doesn\'t reveal is the unfortunate reality that **LLMs are not great at ignoring information**. They will mention information we explicitly tell them not to. Just to spite us.\\n\\nLet\'s take the Jira plugin as an example. When using direct Swagger / Open API integration I have found myself writing prompts like this:\\n\\n> Please tell me about stories that are assigned to me. Please never refer to the stories by their ids - use titles instead.\\n\\nOnly to find that in the responses the LLM will _still_ refer to the stories by their ids.\\n\\nIt\'s a bit like having a child who you\'ve told not to do something, only to find they\'ve done it anyway. The LLM may even cheekily say something like \\"I know you told me not too, but I included the id for reference\\". The scallywag.\\n\\nOr perhaps, given the variety of endpoints that are available in an API, the LLM will call one that we didn\'t want it to. Or perhaps our Swagger / Open API spec is poorly documented, and the LLM doesn\'t think it has an endpoint it can call.\\n\\n## View models and the BFFs to the rescue\\n\\nA useful framing for this problem is remembering when ORMs started to automate access to databases. We could take our ORM, and host it in a web service and, hey presto, our database was now accessible over HTTP. So let\'s take our React app (or whatever) and have it talk directly to our database.\\n\\nExcept, of course, that\'s a terrible idea. We don\'t want our front end talking to our database. There\'s a number of reasons why:\\n\\n- Too much information going backwards and forward between client and server (perhaps including information we\'d never like clients to see).\\n- Security; why are we exposing our database to updates directly from the internet? Is that wise?\\n\\nYou get the picture. We tend not to integrate our databases directly with our front ends with good reason.\\n\\nA common approach to tackle these issues is employing the [back end for front ends (BFF) pattern](https://learn.microsoft.com/en-us/azure/architecture/patterns/backends-for-frontends); having something that sits between our front end and our database. One of the things the BFF does is to provide a view of the data that is appropriate for the client. So for example, exposing a [view model](https://en.wikipedia.org/wiki/View_model) in the back end to serve the front end. It\'s a way to ensure that only the necessary information is exposed to the client.\\n\\nWe can take this idea and apply it to building integrations with APIs and LLMs. So rather than plugging a Swagger / Open API spec into Semantic Kernel, instead build a custom plugin that manages access to our API, and have it expose view models for providing data to LLMs.\\n\\nThat way we can ensure that only the necessary information is given to the LLM, and our answers do not contain data we would rather not see.\\n\\nSo rather than giving an LLM a data structure like this:\\n\\n```json\\n{\\n  \\"stories\\": [\\n    {\\n      \\"id\\": 1,\\n      \\"title\\": \\"Story 1\\",\\n      \\"description\\": \\"This is the first story\\"\\n      //... MANY MORE FIELDS\\n    },\\n    {\\n      \\"id\\": 2,\\n      \\"title\\": \\"Story 2\\",\\n      \\"description\\": \\"This is the second story\\"\\n      //... MANY MORE FIELDS\\n    }\\n  ]\\n}\\n```\\n\\nWe give it the trimmed down equivalent:\\n\\n```json\\n{\\n  \\"stories\\": [\\n    {\\n      \\"title\\": \\"Story 1\\",\\n      \\"description\\": \\"This is the first story\\"\\n    },\\n    {\\n      \\"title\\": \\"Story 2\\",\\n      \\"description\\": \\"This is the second story\\"\\n    }\\n  ]\\n}\\n```\\n\\nThis has the combined benefit of reducing our token usage / cost (as we\'re sending less data to the LLM) and reducing the risk of exposing data we\'d rather not.\\n\\nIt also has the advantage of allowing us to steer the LLM towards the functions we want it to call. If we only expose the functions we want the LLM to call, then we can ensure that it doesn\'t call functions we\'d rather it didn\'t.\\n\\n## \\"But integrating with APIs is a lot of work!\\"\\n\\nA common, and quite reasonable, complaint is that integrating with an API involves a lot of work. We have to write some code to interact with the API, and then we have to write the types that we\'ll use to pass data around. Fortunately there are tools like NSwag that use the Swagger / Open API spec to [automate creating a client with types to manage API interaction](../2021-03-06-generate-typescript-and-csharp-clients-with-nswag/index.md). If we\'re autogenerating our API clients, then the work of integrating an LLM with an API is significantly reduced.\\n\\nWith Semantic Kernel it effectively is reduced to creating a plugin; and that\'s quite simple to do. [There is guidance on how to create a plugin here](https://learn.microsoft.com/en-us/semantic-kernel/agents/plugins/using-the-kernelfunction-decorator?tabs=Csharp). So to create a BFF plugin for an API, we\'d need to create that plugin, exposing the functions we want to be called. Those functions will internally call into the APIs using the auto-generated Swagger clients and then map that to the view models which we want to expose to the LLM. Imagine something like this:\\n\\n```cs\\npublic record JiraStory(\\n    string Title,\\n    string Description\\n);\\n\\n[KernelFunction]\\n[Description(\\"Provides stories for a given user\\")]\\n[return: Description(\\"Jira user stories for the given user\\")]\\npublic async Task<JiraStory[]> GetUsersJiraStories(\\n  Kernel kernel,\\n\\n  [Description(\\"Email of user to filter by\\")]\\n  string userEmail\\n)\\n{\\n    var stories = await _jiraClient.GetStories(userEmail);\\n\\n    return stories\\n      .Select(story => new JiraStory(\\n        title: story.Title,\\n        description: story.Description\\n      ))\\n      .ToArray();\\n}\\n```\\n\\nThe code above exposes a well defined function to the LLM, which will return the stories for a given user. The function internally calls into the Jira API, and then maps the large amount of data returned from the API to a much slimmer view model that is appropriate for the LLM. As we can see, this was very little work indeed!\\n\\n## Conclusion\\n\\nThe integrated support for consuming Open API / Swagger specs is definitely going to improve over time, both in Semantic Kernel and in the wider ecosystem. However, it\'s possible that there is fundamental issue that needs to be solved, and that BFFs for LLMs may solve it. It\'s a way to ensure that only the necessary information is exposed to LLMs, and that the answers they give are appropriate for the context in which they are being used.\\n\\nI\'m not aware of a specific name for this pattern as yet. My colleague, Ryan suggested \\"Frontend for Language Models\\" (FLM) which is less of a mouthful than \\"Backend for Frontends for Language Models\\". Naming things is hard.\\n\\nAnother colleague (Rick), suggested that perhaps the BFF for LLMs could be built directly into APIs. So rather than having to implement a custom plugin that manages the interaction with API, we could still perhaps use the Swagger / Open API approach and avoid the custom plugin implementation. This is an interesting idea.\\n\\nMany thanks to [David Rosevear](https://github.com/drosevear), [George Karsas](https://www.linkedin.com/in/george-karsas), [Rick Roch\xe9](https://www.rickroche.com/), [Ryan Cook](https://github.com/RyanMatCook) and [Ali Somer](https://uk.linkedin.com/in/alisomer) whose thoughts, ideas and experimentation have fed into the thinking in this post."},{"id":"using-kernel-memory-to-chunk-documents-into-azure-ai-search","metadata":{"permalink":"/using-kernel-memory-to-chunk-documents-into-azure-ai-search","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2024-04-21-using-kernel-memory-to-chunk-documents-into-azure-ai-search/index.md","source":"@site/blog/2024-04-21-using-kernel-memory-to-chunk-documents-into-azure-ai-search/index.md","title":"Using Kernel Memory to Chunk Documents into Azure AI Search","description":"To build RAG (Retrieval Augmented Generation) experiences, where LLMs can query documents, you need a strategy to chunk those documents. Kernel Memory supports this.","date":"2024-04-21T00:00:00.000Z","tags":[{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."},{"inline":false,"label":"C#","permalink":"/tags/csharp","description":"The C# programming language."},{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."},{"inline":false,"label":"AI","permalink":"/tags/ai","description":"All things AI - Artificial Intelligence, Large Language Models and the like."}],"readingTime":16.155,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"using-kernel-memory-to-chunk-documents-into-azure-ai-search","title":"Using Kernel Memory to Chunk Documents into Azure AI Search","authors":"johnnyreilly","image":"./title-image.png","tags":["azure","c#","asp.net","ai"],"description":"To build RAG (Retrieval Augmented Generation) experiences, where LLMs can query documents, you need a strategy to chunk those documents. Kernel Memory supports this.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Large Language Models, Open API, View Models and the Backend for Frontend Pattern","permalink":"/large-language-models-view-models-backend-for-frontend"},"nextItem":{"title":"Overview of webpack, a JavaScript bundler","permalink":"/webpack-overview"}},"content":"I\'ve recently been working on building retrieval augmented generation (RAG) experiences into applications; building systems where large language models (LLMs) can query documents. To achieve this, we first need a strategy to chunk those documents and make them LLM-friendly. [Kernel Memory](https://github.com/microsoft/kernel-memory), a sister project of [Semantic Kernel](https://github.com/microsoft/semantic-kernel) supports this.\\n\\n![title image reading \\"Using Kernel Memory to Chunk Documents into Azure AI Search\\" with the Azure Open AI / Azure AI Search logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\nIf you haven\'t heard of Kernel Memory before, it\'s a library that, amongst other things, provides a way to chunk documents into smaller pieces. To quote the [Kernel Memory GitHub repository](https://github.com/microsoft/kernel-memory?tab=readme-ov-file#kernel-memory-km-and-semantic-memory-sm):\\n\\n> Kernel Memory (KM) is a service built on the feedback received and lessons learned from developing Semantic Kernel (SK) and Semantic Memory (SM). It provides several features that would otherwise have to be developed manually, such as storing files, extracting text from files, providing a framework to secure users\' data, etc. The KM codebase is entirely in .NET, which eliminates the need to write and maintain features in multiple languages. As a service, KM can be used from any language, tool, or platform, e.g. browser extensions and ChatGPT assistants.\\n\\nIn this post, I\'ll show you how to use Kernel Memory to chunk documents in the background of an ASP.NET application.\\n\\n## Kernel Memory: Serverless vs Service\\n\\nThere\'s two ways that we can run Kernel Memory: \\"Serverless\\" and \\"Service\\".\\n\\nRunning the full service is more powerful, but effectively requires running a separate application. We would then need to integrate our main app with that. Given that I\'m building with ASP.NET, I\'ll be using the serverless approach, which allows us to run Kernel Memory within the context of a single application (which will contain our main app code as well). We can then manage our integrations with Kernel Memory as simple method calls.\\n\\nThis is simpler and more cost-effective, but it does have some limitations. The service approach offers more features; including persistent retry logic. The documentation states that if we want to scale then we\'ll likely want to consider the service approach. But my own experience has been that serverless works very well for small to medium-sized applications.\\n\\nPerhaps surprisingly, using serverless we can still have the experience of running Kernel Memory as a **non-blocking** separate service within the context of our ASP.NET application. This is achieved by running Kernel Memory as a [hosted service](https://learn.microsoft.com/en-us/aspnet/core/fundamentals/host/hosted-services?view=aspnetcore-8.0) - this is the standard ASP.NET mechanism for running background tasks. That\'s what we\'re going to use.\\n\\nThere\'s four parts to bring this to life:\\n\\n1. Our Kernel Memory serverless instance - this is where the integration between Kernel Memory, Azure Open AI, Azure AI Search and the actual chunking takes place\\n2. A queue which we\'ll use to provide documents for chunking with Kernel Memory\\n3. Our hosted service which will bring together the queue and the Kernel Memory integration to manage our background document processing\\n4. An endpoint in our ASP.NET application to add documents to the queue\\n\\n## 1. Setting up Kernel Memory serverless\\n\\nThere\'s a number of dependencies that we need to add to our project to get Kernel Memory working. These are:\\n\\n```bash\\ndotnet add Azure.AI.OpenAI\\ndotnet add Azure.AI.FormRecognizer # if we want to use Document Intelligence - not mandatory\\ndotnet add Azure.Identity\\ndotnet add Azure.Storage.Blobs\\ndotnet add Microsoft.KernelMemory.Core\\ndotnet add Microsoft.SemanticKernel\\n```\\n\\nWith this in place we\'ll start to integrate with Kernel Memory. We will first construct ourselves an `IKernelMemory` like so:\\n\\n```cs\\n_memory = new KernelMemoryBuilder()\\n    .WithAzureOpenAITextEmbeddingGeneration(new AzureOpenAIConfig\\n    {\\n        APIType = AzureOpenAIConfig.APITypes.EmbeddingGeneration,\\n        Auth = AzureOpenAIConfig.AuthTypes.AzureIdentity,\\n        Endpoint = \\"https://cog-ourapp-dev.openai.azure.com/\\",\\n        Deployment = \\"OpenAi-text-embedding-ada2\\" // text-embedding-ada-002\\n    })\\n    .WithAzureOpenAITextGeneration(new AzureOpenAIConfig\\n    {\\n        APIType = AzureOpenAIConfig.APITypes.ChatCompletion,\\n        Auth = AzureOpenAIConfig.AuthTypes.AzureIdentity,\\n        Endpoint = \\"https://cog-ourapp-dev.openai.azure.com/\\",\\n        Deployment = \\"OpenAi-gpt-35-turbo-16k\\" // gpt-3.5-turbo-16k\\n    })\\n    .WithAzureAISearchMemoryDb(new AzureAISearchConfig\\n    {\\n        Auth = AzureAISearchConfig.AuthTypes.AzureIdentity,\\n        Endpoint = \\"https://srch-ourapp-dev.search.windows.net\\",\\n    })\\n    // Only necessary if you want to add Document Intelligence\\n    .WithAzureAIDocIntel(new AzureAIDocIntelConfig\\n    {\\n        Auth = AzureAIDocIntelConfig.AuthTypes.AzureIdentity,\\n        Endpoint = \\"https://cog-ourapp-dev.cognitiveservices.azure.com/\\",\\n    })\\n    .Build();\\n```\\n\\nWhat we\'re doing here, is creating an `IKernelMemory` instance and making it aware of all our deployed Azure resources. Going through how to deploy those is out of the scope of this post, but it\'s probably worth highlighting that we\'re using `AzureIdentity` for auth as it\'s particularly secure, if you would like to use other options, you certainly can.\\n\\nIt\'s probably worth highlighting that we\'re using the `text-embedding-ada-002` model for text embedding and the `gpt-3.5-turbo-16k` model for text generation. These are the models that I\'ve found to be most effective for my use cases. Of these, the text embedding model is the most important - it\'s the one that will be used to chunk documents.\\n\\nYou\'ll also note we\'re using Azure AI Document Intelligence; this is optional and just tackles a few more document chunking scenarios. It\'s not mandatory.\\n\\n### Chunking with Kernel Memory serverless\\n\\nWith our `IKernelMemory` ready to go, we now need a way to chunk documents. Deep down, this is achieved by acquiring the document we want to chunk from blob storage and passing it to `_memory.ImportDocumentAsync` with the name of the index we want to process into. You can see examples of this usage in the [Kernel Memory docs](https://microsoft.github.io/kernel-memory/serverless). You can also see how it works in the [Kernel Memory repository itself](https://github.com/microsoft/kernel-memory/blob/9112757f4fe25edd7bfbf10222621f11422bd3b5/service/Core/MemoryServerless.cs#L89).\\n\\nHowever, it\'s often helpful to have a number of other things in place to manage:\\n\\n1. Applying tags to documents (this gives us more power when querying later)\\n2. Creating acceptable names / ids for the Azure AI Search Service\\n3. Handling rate limiting - more on that in a moment\\n\\nTo that end, I tend to end up implementing a `Process` method that looks something like this:\\n\\n```cs\\npublic async Task Process(string index, string documentUrl)\\n{\\n    TokenCredential credential = _env.IsDevelopment() ? new AzureCliCredential() : new DefaultAzureCredential(new DefaultAzureCredentialOptions\\n    {\\n        ManagedIdentityClientId = \\"[Managed Identity ClientId Here]\\",\\n    });\\n\\n    BlobServiceClient azureBlobServiceClient = new (\\n        new Uri(\\"https://stourappdev.blob.core.windows.net\\"), credential\\n    );\\n    var containerClient = azureBlobServiceClient.GetBlobContainerClient(index);\\n\\n    // eg DocumentUrl: https://stourappdev.blob.core.windows.net/my-index/A%20Booklet.pdf\\n    string fileName = System.Web.HttpUtility.UrlDecode(documentToProcess.DocumentUrl.Split(\'/\').Last());\\n    var blobClient = containerClient.GetBlobClient(fileName);\\n\\n    MemoryStream documentContent = new();\\n    var response = await blobClient.DownloadToAsync(documentContent);\\n\\n    // example documentUrl:\\n    // Chunking, getting embeddings for and storing for documentUrl consisting of 103469 characters in https://stourappdev.blob.core.windows.net/my-index/A%20Booklet.pdf\\n    _logger.LogInformation($\\"Chunking, getting embeddings for and storing for {{{nameof(documentUrl)}}} consisting of {{count}} characters in {{{nameof(index)}}}\\", documentUrl, documentContent.Length, index);\\n\\n    string documentId = MakeDocumentId(documentUrl);\\n\\n    TagCollection tags = new()\\n    {\\n        { \\"DocumentUrl\\", documentUrl },\\n        { \\"FileName\\", fileName },\\n    };\\n\\n    var stopwatch = new Stopwatch();\\n    stopwatch.Start();\\n\\n    int? waitForSeconds = null;\\n    bool done = false;\\n    int attempt = 0;\\n    do\\n    {\\n        attempt++;\\n\\n        if (attempt == 4) // if we\'ve tried 3 times, give up\\n            throw new Exception($\\"Failed to store document {documentUrl} after {attempt - 1} attempts and {stopwatch.Elapsed.TotalSeconds} seconds\\");\\n\\n        try\\n        {\\n            if (waitForSeconds != null)\\n            {\\n                _logger.LogInformation($\\"Waiting {{{nameof(waitForSeconds)}}} seconds\\", waitForSeconds.Value);\\n                await Task.Delay(TimeSpan.FromSeconds(waitForSeconds.Value));\\n                waitForSeconds = null;\\n            }\\n\\n            _logger.LogInformation(\\"Importing documentId {documentId} attempt {attempt}\\", documentId, attempt);\\n            await _memory.ImportDocumentAsync(content: documentContent, fileName: fileName, documentId: documentId, index: index, tags: tags);\\n\\n            done = true;\\n        }\\n        catch (Microsoft.SemanticKernel.HttpOperationException e) when (e.InnerException is Azure.RequestFailedException azureRequestFailedException)\\n        {\\n            waitForSeconds = HandleRequestFailed(azureRequestFailedException);\\n        }\\n        catch (Azure.RequestFailedException azureRequestFailedException)\\n        {\\n            waitForSeconds = HandleRequestFailed(azureRequestFailedException);\\n        }\\n        catch (Exception ex)\\n        {\\n            throw new Exception($\\"Error storing document {documentUrl} on attempt {attempt} after {stopwatch.Elapsed.TotalSeconds} seconds\\", ex);\\n        }\\n    } while (!done);\\n\\n    stopwatch.Stop();\\n\\n    _logger.LogInformation($\\"Processed {{{nameof(documentId)}}} into {{{nameof(index)}}} index in {{{nameof(stopwatch.Elapsed.TotalSeconds)}}} seconds\\", documentId, index, stopwatch.Elapsed.TotalSeconds);\\n}\\n\\nint? HandleRequestFailed(Azure.RequestFailedException azureRequestFailedException)\\n{\\n    int? waitForSeconds;\\n    var response = azureRequestFailedException.GetRawResponse();\\n    var retryAfterSeconds = response?.Headers.FirstOrDefault(h => h.Name == \\"x-ratelimit-reset-requests\\").Value\\n        ?? response?.Headers.FirstOrDefault(h => h.Name == \\"Retry-After\\").Value;\\n\\n    //x-ratelimit-reset-requests: 4,x-ms-client-request-id: XXXX,apim-request-id: 69569dd5-2e4a-4bfa-9b52-f3eb08481a83,Strict-Transport-Security: max-age=31536000; includeSubDomains; preload,X-Content-Type-Options: nosniff,policy-id: DeploymentRatelimit-Call,x-ms-region: West Europe,Date: Fri, 01 Dec 2023 13:29:12 GMT,Content-Length: 85,Content-Type: application/json\\n    waitForSeconds = retryAfterSeconds != null ? int.Parse(retryAfterSeconds) : null;\\n\\n    if (waitForSeconds == null)\\n    {\\n        const string pattern = @\\"Try again in (\\\\d+) seconds\\";\\n        var match = Regex.Match(azureRequestFailedException.Message, pattern);\\n        // wait for 60 seconds if a specific value is not provided\\n        waitForSeconds = match.Success && int.TryParse(match.Groups[1].Value, out int seconds) ? seconds + 1 : 60;\\n    }\\n\\n    if (azureRequestFailedException.Status == (int)System.Net.HttpStatusCode.TooManyRequests)\\n    {\\n        // var headers = response?.Headers.Select(h => $\\"{h.Name}: {h.Value}\\").ToList() ?? new List<string>();\\n        // _logger.LogWarning(azureRequestFailedException, $\\"429 - too many requests, will wait {{{nameof(waitForSeconds)}}} seconds - HEADERS: {{{nameof(headers)}}}\\", waitForSeconds, string.Join(\\",\\", headers));\\n        _logger.LogWarning(azureRequestFailedException, $\\"429 - too many requests, will wait {{{nameof(waitForSeconds)}}} seconds\\", waitForSeconds);\\n    }\\n    else\\n    {\\n        var headers = response?.Headers.Select(h => $\\"{h.Name}: {h.Value}\\").ToList() ?? [];\\n        _logger.LogError(azureRequestFailedException, $\\"Azure.RequestFailedException - {{{nameof(azureRequestFailedException.Status)}}} status code, will wait {{{nameof(waitForSeconds)}}} seconds - HEADERS: {{{nameof(headers)}}}\\", azureRequestFailedException.Status, waitForSeconds, string.Join(\\",\\", headers));\\n    }\\n\\n    return waitForSeconds;\\n}\\n\\n\\n/// <summary>\\n/// Make a documentId from a fileName; remove all characters except A-Z, a-z, 0-9, ., _, -\\n/// </summary>\\nstatic string MakeDocumentId(string fileName) => Regex.Replace(fileName, \\"[^A-Za-z0-9._-]\\", \\"\\"); // eg \\"A Booklet.pdf\\"\\n```\\n\\nMuch of the code above concerns rate limiting / 429s. It\'s not uncommon when chunking to be hit by 429s - \\"Too many requests\\". Chunking documents requires use of Azure Open AI resources, and the level of access we have is typically restricted and controlled via quotas. There\'s an element of this that we can avoid by controlling the quota available on our Azure Open AI deployments ([you can read more about this here](../2023-08-17-azure-open-ai-capacity-quota-bicep/index.md)), and we can implement a certain amount of retry logic also.\\n\\nThe code above tries to handle a number of re-attempts as wisely as it can, and using the information that Azure APIs surface around when re-attempting is allowed. Interestingly you\'ll see a variety of strategies employed here around retry times, as the way information is surfaced to support this keeps changing! We can likely have less code in future when a final standard is committed to.\\n\\n### Bringing it together\\n\\nWe\'re going to put this all together in a single class called `RagGestionService`.\\n\\nYou might be puzzled by the name \\"RagGestion\\" - this is a term my good friend [George Karsas](https://medium.com/@georgekarsas) coined to describe the process of preparing documents for Retrieval Augmented Generation. It\'s a great term, and I\'ve adopted it!\\n\\nThe `RagGestionService` will look like this:\\n\\n```cs\\nusing Azure.Storage.Blobs;\\nusing Azure.Core;\\nusing Azure.Identity;\\n\\nusing Microsoft.KernelMemory;\\n\\nusing OurApp.Model;\\n\\nusing System.Diagnostics;\\nusing System.Text.RegularExpressions;\\nusing System.Text.Json;\\n\\nnamespace OurApp.Services;\\n\\npublic interface IRagGestionService\\n{\\n    Task Process(string indexName, string documentUrl);\\n}\\n\\npublic class RagGestionService : IRagGestionService\\n{\\n    private readonly IKernelMemory _memory;\\n    private readonly ILogger<RagGestionService> _logger;\\n\\n    public RagGestionService(\\n        ILogger<RagGestionService> logger\\n    )\\n    {\\n        _logger = logger;\\n\\n        _memory = new KernelMemoryBuilder()\\n            .WithAzureOpenAITextEmbeddingGeneration(new AzureOpenAIConfig\\n            {\\n                APIType = AzureOpenAIConfig.APITypes.EmbeddingGeneration,\\n                Auth = AzureOpenAIConfig.AuthTypes.AzureIdentity,\\n                Endpoint = \\"https://cog-ourapp-dev.openai.azure.com/\\",\\n                Deployment = \\"OpenAi-text-embedding-ada2\\"\\n            })\\n            .WithAzureOpenAITextGeneration(new AzureOpenAIConfig\\n            {\\n                APIType = AzureOpenAIConfig.APITypes.ChatCompletion,\\n                Auth = AzureOpenAIConfig.AuthTypes.AzureIdentity,\\n                Endpoint = \\"https://cog-ourapp-dev.openai.azure.com/\\",\\n                Deployment = \\"OpenAi-gpt-35-turbo-16k\\"\\n            })\\n            .WithAzureAISearchMemoryDb(new AzureAISearchConfig\\n            {\\n                Auth = AzureAISearchConfig.AuthTypes.AzureIdentity,\\n                Endpoint = \\"https://srch-ourapp-dev.search.windows.net\\",\\n            })\\n            // Only necessary if we want to add Document Intelligence\\n            .WithAzureAIDocIntel(new AzureAIDocIntelConfig\\n            {\\n                Auth = AzureAIDocIntelConfig.AuthTypes.AzureIdentity,\\n                Endpoint = \\"https://cog-ourapp-dev.cognitiveservices.azure.com/\\",\\n            })\\n            .Build();\\n    }\\n\\n    public async Task Process(string index, string documentUrl)\\n    {\\n        TokenCredential credential = _env.IsDevelopment() ? new AzureCliCredential() : new DefaultAzureCredential(new DefaultAzureCredentialOptions\\n        {\\n            ManagedIdentityClientId = \\"[Managed Identity ClientId Here]\\",\\n        });\\n\\n        BlobServiceClient azureBlobServiceClient = new (\\n            new Uri(\\"https://stourappdev.blob.core.windows.net\\"), credential\\n        );\\n        var containerClient = azureBlobServiceClient.GetBlobContainerClient(index);\\n\\n        // eg DocumentUrl: https://stourappdev.blob.core.windows.net/my-index/A%20Booklet.pdf\\n        string fileName = System.Web.HttpUtility.UrlDecode(documentToProcess.DocumentUrl.Split(\'/\').Last());\\n        var blobClient = containerClient.GetBlobClient(fileName);\\n\\n        MemoryStream documentContent = new();\\n        var response = await blobClient.DownloadToAsync(documentContent);\\n\\n        // example documentUrl:\\n        // Chunking, getting embeddings for and storing for documentUrl consisting of 103469 characters in https://stourappdev.blob.core.windows.net/my-index/A%20Booklet.pdf\\n        _logger.LogInformation($\\"Chunking, getting embeddings for and storing for {{{nameof(documentUrl)}}} consisting of {{count}} characters in {{{nameof(index)}}}\\", documentUrl, documentContent.Length, index);\\n\\n        string documentId = MakeDocumentId(documentUrl);\\n\\n        TagCollection tags = new()\\n        {\\n            { \\"DocumentUrl\\", documentUrl },\\n            { \\"FileName\\", fileName },\\n        };\\n\\n        var stopwatch = new Stopwatch();\\n        stopwatch.Start();\\n\\n        int? waitForSeconds = null;\\n        bool done = false;\\n        int attempt = 0;\\n        do\\n        {\\n            attempt++;\\n\\n            if (attempt == 4) // if we\'ve tried 3 times, give up\\n                throw new Exception($\\"Failed to store document {documentUrl} after {attempt - 1} attempts and {stopwatch.Elapsed.TotalSeconds} seconds\\");\\n\\n            try\\n            {\\n                if (waitForSeconds != null)\\n                {\\n                    _logger.LogInformation($\\"Waiting {{{nameof(waitForSeconds)}}} seconds\\", waitForSeconds.Value);\\n                    await Task.Delay(TimeSpan.FromSeconds(waitForSeconds.Value));\\n                    waitForSeconds = null;\\n                }\\n\\n                _logger.LogInformation(\\"Importing documentId {documentId} attempt {attempt}\\", documentId, attempt);\\n                await _memory.ImportDocumentAsync(content: documentContent, fileName: fileName, documentId: documentId, index: index, tags: tags);\\n\\n                done = true;\\n            }\\n            catch (Microsoft.SemanticKernel.HttpOperationException e) when (e.InnerException is Azure.RequestFailedException azureRequestFailedException)\\n            {\\n                waitForSeconds = HandleRequestFailed(azureRequestFailedException);\\n            }\\n            catch (Azure.RequestFailedException azureRequestFailedException)\\n            {\\n                waitForSeconds = HandleRequestFailed(azureRequestFailedException);\\n            }\\n            catch (Exception ex)\\n            {\\n                throw new Exception($\\"Error storing document {documentUrl} on attempt {attempt} after {stopwatch.Elapsed.TotalSeconds} seconds\\", ex);\\n            }\\n        } while (!done);\\n\\n        stopwatch.Stop();\\n\\n        _logger.LogInformation($\\"Processed {{{nameof(documentId)}}} into {{{nameof(index)}}} index in {{{nameof(stopwatch.Elapsed.TotalSeconds)}}} seconds\\", documentId, index, stopwatch.Elapsed.TotalSeconds);\\n    }\\n\\n    int? HandleRequestFailed(Azure.RequestFailedException azureRequestFailedException)\\n    {\\n        int? waitForSeconds;\\n        var response = azureRequestFailedException.GetRawResponse();\\n        var retryAfterSeconds = response?.Headers.FirstOrDefault(h => h.Name == \\"x-ratelimit-reset-requests\\").Value\\n            ?? response?.Headers.FirstOrDefault(h => h.Name == \\"Retry-After\\").Value;\\n\\n        //x-ratelimit-reset-requests: 4,x-ms-client-request-id: XXXX,apim-request-id: 69569dd5-2e4a-4bfa-9b52-f3eb08481a83,Strict-Transport-Security: max-age=31536000; includeSubDomains; preload,X-Content-Type-Options: nosniff,policy-id: DeploymentRatelimit-Call,x-ms-region: West Europe,Date: Fri, 01 Dec 2023 13:29:12 GMT,Content-Length: 85,Content-Type: application/json\\n        waitForSeconds = retryAfterSeconds != null ? int.Parse(retryAfterSeconds) : null;\\n\\n        if (waitForSeconds == null)\\n        {\\n            const string pattern = @\\"Try again in (\\\\d+) seconds\\";\\n            var match = Regex.Match(azureRequestFailedException.Message, pattern);\\n            // wait for 60 seconds if a specific value is not provided\\n            waitForSeconds = match.Success && int.TryParse(match.Groups[1].Value, out int seconds) ? seconds + 1 : 60;\\n        }\\n\\n        if (azureRequestFailedException.Status == (int)System.Net.HttpStatusCode.TooManyRequests)\\n        {\\n            // var headers = response?.Headers.Select(h => $\\"{h.Name}: {h.Value}\\").ToList() ?? new List<string>();\\n            // _logger.LogWarning(azureRequestFailedException, $\\"429 - too many requests, will wait {{{nameof(waitForSeconds)}}} seconds - HEADERS: {{{nameof(headers)}}}\\", waitForSeconds, string.Join(\\",\\", headers));\\n            _logger.LogWarning(azureRequestFailedException, $\\"429 - too many requests, will wait {{{nameof(waitForSeconds)}}} seconds\\", waitForSeconds);\\n        }\\n        else\\n        {\\n            var headers = response?.Headers.Select(h => $\\"{h.Name}: {h.Value}\\").ToList() ?? [];\\n            _logger.LogError(azureRequestFailedException, $\\"Azure.RequestFailedException - {{{nameof(azureRequestFailedException.Status)}}} status code, will wait {{{nameof(waitForSeconds)}}} seconds - HEADERS: {{{nameof(headers)}}}\\", azureRequestFailedException.Status, waitForSeconds, string.Join(\\",\\", headers));\\n        }\\n\\n        return waitForSeconds;\\n    }\\n\\n\\n    /// <summary>\\n    /// Make a documentId from a fileName; remove all characters except A-Z, a-z, 0-9, ., _, -\\n    /// </summary>\\n    static string MakeDocumentId(string fileName) => Regex.Replace(fileName, \\"[^A-Za-z0-9._-]\\", \\"\\"); // eg \\"A Booklet.pdf\\"\\n}\\n```\\n\\nBy the way, I don\'t advise hard-coding the Azure resources as I have here, but rather passing them in as configuration. Incidentally, we could also use dependency injection to inject a prepared `IKernelMemory` instance into the service, but again, I\'m keeping it simple here for clarity.\\n\\n## 2. Our document processor queue\\n\\nIn order that we have a way to provide documents for chunking, we need a queue. This is a simple queue that we can add documents to, and then process them in the background. We\'re going to use a `ConcurrentQueue` for this, with a little wrapper around it so we can encapsulate the queue for sharing between our UI and our background task, and also to do some logging.\\n\\n```cs\\nusing System.Collections.Concurrent;\\n\\nusing OurApp.Model;\\n\\nnamespace OurApp.Services.Implementations;\\n\\npublic record DocumentToProcess(\\n    string DocumentUrl,\\n    string IndexName\\n);\\n\\npublic interface IDocumentProcessorQueue\\n{\\n    DocumentToProcess? DequeueDocumentUri();\\n    void EnqueueDocumentUri(DocumentToProcess documentToProcess);\\n}\\n\\npublic class DocumentProcessorQueue : IDocumentProcessorQueue\\n{\\n    readonly ConcurrentQueue<DocumentToProcess> _documentUrlQueue;\\n    readonly ILogger<DocumentProcessorQueue> _logger;\\n\\n    public DocumentProcessorQueue(ILogger<DocumentProcessorQueue> logger)\\n    {\\n        _documentUrlQueue = new();\\n        _logger = logger;\\n    }\\n\\n    public void EnqueueDocumentUri(DocumentToProcess documentToProcess)\\n    {\\n        _logger.LogInformation($\\"Adding document for background processing onto {{{nameof(documentToProcess.IndexName)}}} index later: {{{nameof(documentToProcess.DocumentUrl)}}} ({{{nameof(_documentUrlQueue.Count)}}} items on queue)\\", documentToProcess.IndexName, documentToProcess.DocumentUrl, _documentUrlQueue.Count);\\n        _documentUrlQueue.Enqueue(documentToProcess);\\n    }\\n\\n    public DocumentToProcess? DequeueDocumentUri()\\n    {\\n        if (_documentUrlQueue.TryDequeue(out var documentToProcess))\\n        {\\n            _logger.LogInformation($\\"Document picked up for background processing onto {{{nameof(documentToProcess.IndexName)}}} index: {{{nameof(documentToProcess.DocumentUrl)}}} ({{{nameof(_documentUrlQueue.Count)}}} items remain on queue)\\", documentToProcess.IndexName, documentToProcess.DocumentUrl, _documentUrlQueue.Count);\\n            return documentToProcess;\\n        }\\n\\n        return null;\\n    }\\n}\\n```\\n\\nThe `EnqueueDocumentUri` method above will be called from the context of our UI - from an ASP.NET controller. This will be invoked when someone uploads a file and will also be responsible for adding the file to a BlobService for storage prior to processing.\\n\\nBy contrast, the `DequeueDocumentUri` method will be called from the context of our background service; it will call this method to pick up a file for processing.\\n\\n## 3. Our background service\\n\\nNext, we need a background service to bring together our `DocumentProcessorQueue` and our `RagGestionService`. This is a standard ASP.NET hosted service. It will look like this:\\n\\n```cs\\nusing OurApp.Model;\\nusing OurApp.Services;\\n\\nnamespace OurApp.BackgroundServices;\\n\\npublic class DocumentProcessorBackgroundService : BackgroundService\\n{\\n    private readonly ILogger<DocumentProcessorBackgroundService> _logger;\\n    private readonly IServiceProvider _services;\\n\\n    public DocumentProcessorBackgroundService(IServiceProvider services, ILogger<DocumentProcessorBackgroundService> logger)\\n    {\\n        _services = services;\\n        _logger = logger;\\n    }\\n\\n    protected override async Task ExecuteAsync(CancellationToken stoppingToken)\\n    {\\n        try\\n        {\\n            _logger.LogInformation(\\"Starting RagGestion\\");\\n\\n            var ragGestionService = _services.GetRequiredService<IRagGestionService>();\\n            var documentProcessorQueue = _services.GetRequiredService<IDocumentProcessorQueue>();\\n\\n            await PerformRagGestion(env, chunkerService, documentProcessorQueue, stoppingToken);\\n        }\\n        catch (Exception e)\\n        {\\n            _logger.LogError(e, $\\"Error processing document\\");\\n        }\\n    }\\n\\n    async Task PerformRagGestion(IRagGestionService ragGestionService, IDocumentProcessorQueue documentProcessorQueue, CancellationToken stoppingToken)\\n    {\\n        while (!stoppingToken.IsCancellationRequested)\\n        {\\n            await Task.Delay(TimeSpan.FromSeconds(5), stoppingToken);\\n\\n            DocumentToProcess? documentToProcess = documentProcessorQueue.DequeueDocumentUri();\\n            if (documentToProcess == null)\\n            {\\n                _logger.LogDebug(\\"No documents to process\\");\\n                continue;\\n            }\\n\\n            try\\n            {\\n                var watch = System.Diagnostics.Stopwatch.StartNew();\\n\\n                _logger.LogInformation($\\"Processing document: {{{nameof(documentToProcess.DocumentUrl)}}}\\", documentToProcess.DocumentUrl);\\n\\n                await ragGestionService.Process(\\n                    indexName: documentToProcess.IndexName,\\n                    documentUrl: documentToProcess.DocumentUrl\\n                );\\n\\n                watch.Stop();\\n\\n                _logger.LogInformation(\\n                    $\\"Chunked and stored {{{nameof(documentToProcess.DocumentUrl)}}} into Azure AI Search {{{nameof(documentToProcess.IndexName)}}} index in {{{nameof(watch.Elapsed.Seconds)}}} seconds\\",\\n                    documentToProcess.DocumentUrl, documentToProcess.IndexName, watch.Elapsed.TotalSeconds);\\n            }\\n            catch (Exception e)\\n            {\\n                _logger.LogError(e, $\\"Error processing document: {{{nameof(documentToProcess)}}}\\", documentToProcess);\\n            }\\n        }\\n    }\\n}\\n```\\n\\nThis service will run in the background of the ASP.NET application, will pick up documents from the queue (if there are any) and pass them to the `RagGestionService` for processing. It will trigger every 5 seconds, running for the lifetime of the application.\\n\\nYou\'ll see we\'re doing some timing here - this is because it\'s useful to know how long the process takes. If we\'re processing a lot of documents, we\'ll want to know how long it\'s taking to process each one.\\n\\n## 4. Adding documents to the queue\\n\\nTo add documents to the queue, we\'ll need to create an endpoint in our ASP.NET application. This endpoint will accept files and add them to the queue. Here\'s an example of how we might do that:\\n\\n```cs\\nusing Microsoft.AspNetCore.Mvc;\\n\\nusing Azure.Storage.Blobs;\\nusing Azure.Identity;\\nusing Azure.Core;\\n\\nnamespace OurApp.Controllers;\\n\\npublic record UploadedFile(\\n    string FileName,\\n    bool Succeeded,\\n    long Size,\\n    string DocumentUrl\\n);\\n\\n[ApiController]\\npublic class UploadController : ControllerBase\\n{\\n    private readonly IDocumentProcessorQueue _documentProcessorQueue;\\n    private readonly ILogger<UploadController> _log;\\n    private readonly IHostEnvironment _env;\\n\\n    public UploadController(\\n        ILogger<UploadController> log,\\n        IHostEnvironment env,\\n        IDocumentProcessorQueue documentProcessorQueue\\n    )\\n    {\\n        _log = log;\\n        _env = env;\\n        _documentProcessorQueue = documentProcessorQueue;\\n    }\\n\\n    [RequestSizeLimit(104857600)] // For files of up to 100 MB - perhaps larger than you\'d want to upload in a single go\\n    [HttpPost($\\"api/{nameof(UploadFiles)}\\")]\\n    [ProducesResponseType(StatusCodes.Status200OK, Type = typeof(List<UploadedFile>))]\\n    [ProducesResponseType(StatusCodes.Status403Forbidden, Type = typeof(string))]\\n    [ProducesResponseType(StatusCodes.Status404NotFound, Type = typeof(string))]\\n    public async Task<ActionResult<List<UploadedFile>>> UploadFiles(\\n        [FromQuery] string indexName,\\n        List<IFormFile> files\\n    )\\n    {\\n        var processedFiles = new List<UploadedFile>();\\n\\n        try\\n        {\\n            foreach (var formFile in files)\\n            {\\n                try\\n                {\\n                    TokenCredential credential = _env.IsDevelopment() ? new AzureCliCredential() : new DefaultAzureCredential(new DefaultAzureCredentialOptions\\n                    {\\n                        ManagedIdentityClientId = \\"[Managed Identity ClientId Here]\\",\\n                    });\\n\\n                    BlobServiceClient azureBlobServiceClient = new(\\n                        new Uri(\\"https://stourappdev.blob.core.windows.net\\"), credential\\n                    );\\n                    var containerClient = azureBlobServiceClient.GetBlobContainerClient(indexName);\\n                    if (!await containerClient.ExistsAsync())\\n                        await containerClient.CreateIfNotExistsAsync();\\n\\n                    var blobClient = containerClient.GetBlobClient(formFile.FileName);\\n                    StreamReader streamReader = new(formFile.OpenReadStream());\\n                    var uploaded = await blobClient.UploadAsync(streamReader.BaseStream, overwrite: true);\\n\\n                    var uploadedFile = new UploadedFile(\\n                        FileName: formFile.FileName,\\n                        Succeeded: true,\\n                        Size: formFile.Length,\\n                        DocumentUrl: blobClient.Uri.AbsoluteUri\\n                    );\\n                    processedFiles.Add(uploadedFile);\\n\\n                    _documentProcessorQueue.EnqueueDocumentUri(\\n                        new DocumentToProcess(\\n                            DocumentUrl: uploadedFile.DocumentUrl,\\n                            IndexName: indexName\\n                        )\\n                    );\\n                }\\n                catch (Exception ex)\\n                {\\n                    processedFiles.Add(new UploadedFile(\\n                        FileName: formFile.FileName,\\n                        Succeeded: false,\\n                        Size: formFile.Length,\\n                        DocumentUrl: string.Empty\\n                    ));\\n\\n                    _log.LogError(ex, \\"Failed to upload {file}\\", formFile.FileName);\\n                }\\n            }\\n\\n            return Ok(processedFiles);\\n        }\\n        catch (Exception ex)\\n        {\\n            _log.LogError(ex, \\"Problem uploading files\\");\\n            return BadRequest(\\"Problem uploading files\\");\\n        }\\n    }\\n}\\n```\\n\\nAs we can see, this endpoint:\\n\\n1. Accepts files from a POST request with an index name in the querystring\\n2. Uploads them to Blob Storage (matching the container name to the index they will be processed into in future)\\n3. Adds them to the queue with `_documentProcessorQueue.EnqueueDocumentUri`. This will then be picked up by the background service and processed.\\n\\n## Registering our services\\n\\nFinally, we\'ll need to register our services in the `Program.cs` file. We\'ll want to add the following:\\n\\n```cs\\nbuilder.Services\\n    .AddSingleton<IRagGestionService, RagGestionService>()\\n    .AddSingleton<IDocumentProcessorQueue, DocumentProcessorQueue>()\\n\\n    .AddHostedService<DocumentProcessorBackgroundService>()\\n;\\n```\\n\\nWith this in place we have an application that can upload documents and chunk them in the background.\\n\\n## Conclusion\\n\\nAnd that\'s it! This is an ASP.NET application that can chunk documents (or RagGest \uD83D\uDE09) in the background using Kernel Memory running in serverless mode. I haven\'t yet had the need to upgrade to the full Kernel Memory service. Perhaps the day will come, but the mileage we can get with this approach is considerable.\\n\\nMany thanks to [David Rosevear](https://github.com/drosevear) and [George Karsas](https://www.linkedin.com/in/george-karsas) for their help working on this mechanism. And George for \\"RagGestion\\" - I love it!"},{"id":"webpack-overview","metadata":{"permalink":"/webpack-overview","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2024-04-06-webpack-overview/index.md","source":"@site/blog/2024-04-06-webpack-overview/index.md","title":"Overview of webpack, a JavaScript bundler","description":"webpack is a JavaScript bundler that helps you bundle your code into a single file. It\'s a great tool for optimizing your code and improving performance.","date":"2024-04-06T00:00:00.000Z","tags":[{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."}],"readingTime":25.57,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"webpack-overview","title":"Overview of webpack, a JavaScript bundler","authors":["johnnyreilly"],"tags":["webpack"],"image":"./title-image.png","description":"webpack is a JavaScript bundler that helps you bundle your code into a single file. It\'s a great tool for optimizing your code and improving performance.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Using Kernel Memory to Chunk Documents into Azure AI Search","permalink":"/using-kernel-memory-to-chunk-documents-into-azure-ai-search"},"nextItem":{"title":"Azure Cosmos DB: container items and generics","permalink":"/azure-cosmosdb-container-item-generics"}},"content":"If you\'re a JavaScript developer, you\'ve probably heard of webpack. It\'s a JavaScript bundler that helps you bundle your code into a single file. It\'s a great tool for optimizing your code and improving performance. This article will give you an overview of webpack, its history and how it works.\\n\\nIt\'ll be a little different than your typical \\"what is webpack?\\" article, in that I write this as the maintainer of [`ts-loader`](https://github.com/TypeStrong/ts-loader), a loader used for integrating TypeScript with webpack. I\'ve worked in the webpack ecosystem for some years now and I\'ll share some of my experiences with you. I\'ll go through a little history around bundling, and try to understand why webpack came to be such a popular choice.\\n\\n![title image reading \\"Overview of webpack, a JavaScript bundler\\" with the webpack logo](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## A short history of web development\\n\\nTo answer the question \\"what is webpack?\\", we need to understand what a bundler is. To grasp that, we first need a little history lesson.\\n\\n### The early days of JavaScript\\n\\nIf you started web development after 2016, you might not realise that bundling is a relatively new concept.\\n\\nLet\'s roll back the clock to the late 2000\'s. This was when JavaScript development started to go mainstream. Around that time, jQuery was becoming popular and the web was a very different place. We didn\'t have the same tools we have today. We didn\'t have npm, we didn\'t have webpack, we didn\'t have React, we didn\'t have TypeScript. We didn\'t even have ES2015. We were still writing JavaScript in ES5.\\n\\nSo what did building a front end application look like back then? Well, I was a web developer, and I can tell you that it was a lot of work. To make a simple app you would have to:\\n\\n- Go to the websites of libraries you wanted to use, usually jQuery and jQuery UI.\\n- Download the library files you needed (both `jquery-1.4.4.js` and the minified `jquery-1.4.4.min.js` because there weren\'t minification tools back then)\\n- Include the library files in your HTML file using `script` tags, and significantly **before** other JavaScript files that would depend upon jQuery.\\n- For bonus points, you would also download the jQuery UI CSS files and include them in your HTML file.\\n- For extra bonus points, you would figure out a way to serve up non-minified versions of your JavaScript files in development, and minified versions in production.\\n\\nWhat I\'m hoping you can see from this is that it was a lot of work. And it was very error prone. If you forgot to include a library, or included it in the wrong order, or included the wrong version, or forgot to minify your files, or forgot to include the CSS files, your app would break. And it would be very difficult to debug.\\n\\n### Rise of the task runners\\n\\nAs web development became more popular, people started to realise that there was a lot of repetition in the process. So they started to automate it. Around 2012 we started to see the birth of the task runner. There were two main task runners that became popular: [Grunt](https://gruntjs.com/) and [Gulp](https://gulpjs.com/).\\n\\nThese task runners allowed you to automate the process of combining and minifying JavaScript and CSS files, and including them in your HTML file. They also allowed you to automate other tasks, like running tests, linting your code, and deploying your app. They did improve the web development experience, but they didn\'t solve all the problems.\\n\\nIt was still very easy to make mistakes. You could still forget to include a library, or include it in the wrong order, or get a path wrong, or forget to minify your files, or forget to include the CSS files. And it was still very difficult to debug.\\n\\nBut it was **so much better** than what we had before. So it became very popular.\\n\\n### The rise of the module bundler\\n\\nAround 2014, a new tool started to become popular: the module bundler. But what is a module bundler? Well, it\'s a tool that allows you to write your code in modules, and then bundle those modules into a single file. It also allows you to use other tools, like TypeScript, and CSS preprocessors like Sass and Less.\\n\\nThat\'s a lot of words, let\'s unpack them a little. For some time, the defacto way of acquiring JavaScript libraries has been through npm. npm is a package manager for JavaScript. However, it\'s worth remembering its history. npm started out as the package manager for Node.js. It was originally used to house packages that were used to build Node.js applications. It was never intended to be used for front end development. In fact, for a while there was an alternative front end package manager called [Bower](https://bower.io/).\\n\\nThe thing is, there\'s a lot of commonality between Node.js and front end development. Both use JavaScript. You\'re unlikely to need to run a web server in the browser. However, whether running in a browser or on a server, you might want to order an array with lodash, or make use of TypeScript, or perform validation with Zod. So it makes sense to use the same package manager for both.\\n\\nThe first tool that tackled this was [Browserify](http://browserify.org/). As the name suggests, it was a tool that allowed you to use Node.js style modules in the browser. It did this by taking your code, and recursively walking through it, finding all the `require` calls, and bundling them into a single file.\\n\\nBy doing this, it performed two useful functions:\\n\\n1. It opened up the ecosystem of Node.js packages to front end developers.\\n2. It allowed you to write your code in modules, which made it easier to reason about.\\n\\nBoth of these are tremendously significant. The first one is obvious; there\'s a rich ecosystem of modules which can be used to speed up the task of web development.\\n\\nThe value of modularity is less obvious, but it\'s very important. It\'s worth remembering that JavaScript didn\'t have modules until ES2015. But npm had its own module standard called CommonJS. Given that Browserify and webpack were both created before ES2015, they both used CommonJS modules in the context of the browser. This was a huge improvement over the previous way of doing things, which was to include a bunch of script tags in your HTML file, and writing all your code in a giant global object. The reason it\'s so wildly different is because the dependencies in your codebase moved from being **implicit** to being **explicit**. Instead of having to remember to include a bunch of script tags in your HTML file, you could just `require` the modules you needed. This made it much easier to reason about your codebase. What\'s more, you had a a `package.json` file that listed all your dependencies, so you could see at a glance what your dependencies were.\\n\\n## What is webpack?\\n\\nNow we understand a little of the history, we come to webpack. By the way, it\'s definitely not \\"Webpack\\" or \\"WebPack\\". It\'s [\\"webpack\\"](https://webpack.js.org/branding/). The person initially behind webpack is [Tobias Koppers](https://github.com/sokra); an engineer from Germany. Many, many people have contributed to the project since then, but Tobias is the person who has done the most work on it.\\n\\nI mentioned that I was a web developer whilst the web was evolving its developer tooling. In my case I was a longtime user of Gulp, and then Browserify. I moved to webpack in 2015. I can\'t remember exactly why I moved, but I think it was because I wanted to use TypeScript, and webpack had better TypeScript support than Browserify (more on this later). I also think I was attracted to webpack because it was a more holistic solution than Browserify. It had a plugin system, and it had loaders. I\'ll talk about those in a moment.\\n\\nFirst and foremost, it\'s worth saying that webpack is a module bundler. It takes your code, and recursively walks through it, finding all the `require` or `import` calls, building up a dependency graph, performing preprocessing tasks and producing runnable output, in the form of HTML, CSS and JavaScript. It also allows you to use other tools, like TypeScript, and CSS preprocessors like Sass and Less.\\n\\nOne of the most surprising things about webpack has been both its popularity, and how it has lasted. The web development world is famous for having the attention span of a distracted toddler. Tools replace tools, libraries replace libraries, and frameworks replace frameworks. But webpack has been around for a long time, and it\'s still the most popular bundler. At the time of writing it still has **110 million downloads a month**. That\'s a lot! Why is that?\\n\\nI think there are a few reasons.\\n\\nFirstly, because of the richness of the ecosystem and the flexibility of the tool, it\'s possible to solve pretty much all web development problems with webpack. There are newer, shinier, faster tools (and as we\'ll see later, webpack is starting to be displaced by some of these) but as a reliable tool that can solve all your problems, webpack is hard to beat.\\n\\nThat doesn\'t mean it\'s the easiest tool to work with on all occasions. The internet is awash with people bitterly complaining about the scars they bear from configuring webpack. It\'s true that webpack can be difficult to configure. But it\'s also true that webpack is a very powerful tool. Once you have it working, you generally don\'t have to touch it again.\\n\\nA second reason why webpack is so popular, is that it has become a \\"primitive\\". What I mean by that, is that it has become a library that other libraries depend upon. If you use Docusaurus, you\'re also using webpack as the underlying build tool. Many projects have a need of a build tool and have picked webpack to be that. This has led to a huge ecosystem of plugins and loaders. It\'s also led to a plethora of tutorials and blog posts. If you have a problem, it\'s likely that someone else has had the same problem and has written a blog post about it.\\n\\nBy way of example, a [blog post I wrote in 2016 about the webpack `DefinePlugin`](https://johnnyreilly.com/using-webpacks-defineplugin-with-typescript) still ranks highly in Google for \\"use webpack defineplugin with typescript\\" and is (to my surprise) one of my most popular blog posts. Here\'s a screenshot of it in the Google search results:\\n\\n![screenshot of the blog post in Google search results](screenshot-google-search-results-webpack-defineplugin.webp)\\n\\nThis speaks to the level of popularity around all things webpack.\\n\\n## Getting started\\n\\nThis article is intended to be an overview of webpack. The documentation, as you might expect from such a big project, is comprehensive and can be found here: https://webpack.js.org\\n\\nWhilst we won\'t go through every scenario and use case of webpack, we want to give you a sense of what working with webpack looks like. For the purposes of this article, let\'s get started with a simple example. We\'ll create a simple \\"Hello, webpack\\" app. And we\'ll enrich it as we go through the piece.\\n\\n## Creating a simple app\\n\\nFirst, let\'s make a folder, create a `package.json` file and install the webpack dependencies we need:\\n\\n```bash\\nmkdir hello-webpack\\ncd hello-webpack\\nnpm init -y\\nnpm install webpack webpack-cli webpack-dev-server html-webpack-plugin --save-dev\\n```\\n\\nThe dependencies we\'re installing are:\\n\\n- webpack\\n- [`webpack-cli`](https://github.com/webpack/webpack-cli) - a command line interface for webpack\\n- [`webpack-dev-server`](https://webpack.js.org/configuration/dev-server/) - a development server that allows you to serve up your app in a browser\\n- [`html-webpack-plugin`](https://github.com/jantimon/html-webpack-plugin/) - a plugin that allows you to generate an HTML file that includes your bundled JavaScript file(s)\\n\\n### Configuration with `webpack.config.js`\\n\\nWhilst it is possible to use webpack without configuring it, it\'s more typical to have a configuration file. This file is often called `webpack.config.js`; where a single configuration file is being used. It\'s also comon to may have more than one configuration file; perhaps one for development and one for production. We\'ll create a single `webpack.config.js` to use with our example app:\\n\\n```javascript\\nconst path = require(\'path\');\\nconst HtmlWebpackPlugin = require(\'html-webpack-plugin\');\\n\\nmodule.exports = {\\n  mode: \'development\', // mode can be \\"development\\", \\"production\\" or \\"none\\" https://webpack.js.org/configuration/mode/\\n  entry: \'./src/index.js\', // the entry point of our app https://webpack.js.org/concepts/entry-points/\\n  devtool: \'inline-source-map\', // the type of sourcemap to generate for debugging https://webpack.js.org/configuration/devtool/\\n  plugins: [\\n    new HtmlWebpackPlugin(), // a plugin to generate an HTML file https://github.com/jantimon/html-webpack-plugin\\n  ],\\n  output: {\\n    // where to put the bundled output https://webpack.js.org/concepts/output/\\n    filename: \'[name].[contenthash].js\',\\n    path: path.resolve(__dirname, \'dist\'),\\n    clean: true,\\n  },\\n};\\n```\\n\\nThis may seem a little overwhelming, so we\'ll through this configuration file property by property in a moment.\\n\\nThe one thing that you might be puzzled by, is the absence of a `module` section to cover loaders. This is because webpack supports processing JavaScript by default. We\'ll add a `module` section later when we want to process other types of files.\\n\\n#### `mode`\\n\\nThis is the mode that webpack will run in, and it essentially tells webpack to provide helpful defaults around how builds are performed. It can be `development`, `production` or `none`. We\'re using `development` because we\'re developing locally. If we were building for production, we\'d use `production`.\\n\\nRead more about modes here: https://webpack.js.org/configuration/mode/\\n\\nIncidentally, we can override this on the command line with the `--mode` flag. And we will.\\n\\n#### `entry`\\n\\nThis is the entry point of our app. It\'s the file that webpack will start with. In this case, it\'s `src/index.js`. It is possible to have multiple entry points, but we\'ll keep it simple for now.\\n\\nRead more about entry points here: https://webpack.js.org/concepts/entry-points/\\n\\n#### `devtool`\\n\\nThis is the type of sourcemap that webpack will generate. We\'re using `inline-source-map` because we\'re developing locally and we\'d like to be able to debug our source code in the browser. If we were building for production, we might make a different choice.\\n\\nRead more about sourcemaps here: https://webpack.js.org/configuration/devtool/ - there are many different types of sourcemap, and they all have different tradeoffs.\\n\\n#### `plugins`\\n\\nThis is a list of plugins that we want to use. We\'re using the `HtmlWebpackPlugin` to generate an HTML file that includes our bundled JavaScript file(s).\\n\\nRead more about plugins here: https://webpack.js.org/concepts/plugins/ - we\'ll talk more about plugins later.\\n\\n#### `output`\\n\\nThis is where we want webpack to put the bundled output. We\'re using `dist` as the folder name. We\'re also using a `[name].[contenthash].js` naming convention for our bundled JavaScript file. This means that webpack will generate a file called `main.[contenthash].js` in the `dist` folder. The `[contenthash]` part is a hash of the contents of the file. This is useful because it means that if the contents of the file change, the hash will change, and the filename will change. This helps because it means that we can cache the file for a long time, and if the contents change, the filename will change and the browser will download the new file.\\n\\nWe\'re also providing the [`clean: true`](https://webpack.js.org/guides/output-management/#cleaning-up-the-dist-folder) option which deletes the contents of our `dist` folder on each build.\\n\\nRead more about output here: https://webpack.js.org/concepts/output/\\n\\n### Creating a simple app\\n\\nWhat we need now, is some code to bundle. Let\'s create a `src` folder, and a `src/index.js` file; our first JavaScript file:\\n\\n```javascript\\nfunction app() {\\n  const element = document.createElement(\'div\');\\n\\n  element.innerHTML = \'Hello, webpack\';\\n\\n  return element;\\n}\\n\\ndocument.body.appendChild(app());\\n```\\n\\n### Local development with `webpack-dev-server`\\n\\nNow we\'re going to add two scripts to our `package.json` file. One to build our app, and one to serve it up in a browser whilst we\'re developing it:\\n\\n```json\\n  \\"scripts\\": {\\n    \\"build\\": \\"webpack build --mode production\\",\\n    \\"start\\": \\"webpack serve --open\\"\\n  },\\n```\\n\\nWith this in place, we can develop locally with `npm start`. This will serve up our app in a browser at `http://localhost:8080/` using `webpack-dev-server`:\\n\\n```bash\\n> hello-webpack@1.0.0 start\\n> webpack serve --open\\n\\n<i> [webpack-dev-server] Project is running at:\\n<i> [webpack-dev-server] Loopback: http://localhost:8080/\\n<i> [webpack-dev-server] On Your Network (IPv4): http://172.30.170.28:8080/\\n<i> [webpack-dev-server] On Your Network (IPv6): http://[fe80::1]:8080/\\n<i> [webpack-dev-server] Content not from webpack is served from \'/Users/jreilly/code/github.com/hello-webpack/public\' directory\\n<i> [webpack-dev-middleware] wait until bundle finished: /\\nasset main.4d4379bc3adfa037dc27.js 621 KiB [emitted] [immutable] (name: main)\\nasset index.html 252 bytes [emitted]\\nruntime modules 27.3 KiB 12 modules\\nmodules by path ./node_modules/ 178 KiB\\n  modules by path ./node_modules/webpack-dev-server/client/ 71.8 KiB 16 modules\\n  modules by path ./node_modules/webpack/hot/*.js 5.3 KiB\\n    ./node_modules/webpack/hot/dev-server.js 1.94 KiB [built] [code generated]\\n    ./node_modules/webpack/hot/log.js 1.86 KiB [built] [code generated]\\n    + 2 modules\\n  modules by path ./node_modules/html-entities/lib/*.js 81.8 KiB\\n    ./node_modules/html-entities/lib/index.js 7.91 KiB [built] [code generated]\\n    ./node_modules/html-entities/lib/named-references.js 73 KiB [built] [code generated]\\n    ./node_modules/html-entities/lib/numeric-unicode-map.js 339 bytes [built] [code generated]\\n    ./node_modules/html-entities/lib/surrogate-pairs.js 537 bytes [built] [code generated]\\n  ./node_modules/ansi-html-community/index.js 4.16 KiB [built] [code generated]\\n  ./node_modules/events/events.js 14.5 KiB [built] [code generated]\\n./src/index.js 163 bytes [built] [code generated]\\nwebpack 5.89.0 compiled successfully in 861 ms\\n```\\n\\nIf we open the browser at `http://localhost:8080`, we\'ll see our \\"Hello, webpack\\" message.\\n\\n### Building for production\\n\\nWe can build our app for production with `npm run build`:\\n\\n```bash\\nnpm run build\\n\\n> hello-webpack@1.0.0 build\\n> webpack build --mode production\\n\\nasset main.82d3f64b186c8eec8e7c.js 862 bytes [emitted] [immutable] [minimized] (name: main)\\nasset index.html 235 bytes [emitted]\\n./src/index.js 163 bytes [built] [code generated]\\nwebpack 5.89.0 compiled successfully in 516 ms\\n```\\n\\nThis has created a `dist` folder, and a `dist/index.html` file. Alongside that, it\'s created a `dist/main.82d3f64b186c8eec8e7c.js` file. If you open the `index.html` file in a browser, you\'ll see your \\"Hello, webpack\\" message.\\n\\nAt this point we have a simple app built with webpack. It\'s not doing much, but it\'s a start. And it\'ll give us a chance to talk about some concepts. Let\'s add some more features.\\n\\n## Integrating with plugins and loaders\\n\\nIf you want to do anything more than the most basic of apps, you\'ll need to use plugins and loaders. Let\'s add some more features to our app, and we\'ll use plugins and loaders to do it.\\n\\n### Loaders\\n\\nThe first thing we\'ll do is look at loaders. Loaders allow webpack to process other types of files (for example, TypeScript) and convert them into valid modules that can be consumed by your application and added to the dependency graph. An example of a loader is [`ts-loader`](https://github.com/TypeStrong/ts-loader) which allows you to use TypeScript with webpack.\\n\\nI should not brush past this, I\'m the primary maintainer of `ts-loader` and I\'m very proud of it. It gets around 30 million downloads a month at the time of writing. That suggests that roughly a quarter of webpacks users are also `ts-loader` users. `ts-loader` is a great loader, and I\'m very happy to have worked on it since 2016. There\'s actually a story behind how I came to work on it, [you can read it here](https://johnnyreilly.com/but-you-cant-die-i-love-you-ts-loader).\\n\\nLet\'s install `ts-loader` and TypeScript, and create a `tsconfig.json` file:\\n\\n```bash\\nnpm install typescript ts-loader --save-dev\\nnpx tsc --init\\n```\\n\\nNow we need to configure webpack to use `ts-loader`. We do this by updating our entry point to be a TypeScript file and adding a `module` section to our `webpack.config.js` file:\\n\\n```diff\\nconst path = require(\'path\');\\nconst HtmlWebpackPlugin = require(\'html-webpack-plugin\');\\n\\nmodule.exports = {\\n  mode: \'development\', // mode can be \\"development\\", \\"production\\" or \\"none\\" https://webpack.js.org/configuration/mode/\\n-  entry: \'./src/index.js\', // the entry point of our app https://webpack.js.org/concepts/entry-points/\\n+  entry: \'./src/index.ts\', // the entry point of our app https://webpack.js.org/concepts/entry-points/\\n  devtool: \'inline-source-map\', // the type of sourcemap to generate for debugging https://webpack.js.org/configuration/devtool/\\n  plugins: [\\n    new HtmlWebpackPlugin(), // a plugin to generate an HTML file https://github.com/jantimon/html-webpack-plugin\\n  ],\\n+  module: {\\n+    rules: [\\n+      {\\n+        test: /\\\\.([cm]?ts|tsx)$/,\\n+        loader: \'ts-loader\',\\n+      },\\n+    ],\\n+  },\\n  output: {\\n    // where to put the bundled output https://webpack.js.org/concepts/output/\\n    filename: \'[name].[contenthash].js\',\\n    path: path.resolve(__dirname, \'dist\'),\\n    clean: true,\\n  },\\n};\\n```\\n\\nThe `test` property is a regular expression that matches the files we want to process. In this case, we\'re matching `.ts`, `.tsx`, `.cts` and `.cts` files. The `loader` property is the name of the loader we want to use; `ts-loader`.\\n\\nLet\'s rename our `src/index.js` file to `src/index.ts` and change the code to use TypeScript:\\n\\n```typescript\\nfunction app(): HTMLDivElement {\\n  // the only TypeScript change we made is to add a return type\\n  const element = document.createElement(\'div\');\\n\\n  element.innerHTML = \'Hello, webpack\';\\n\\n  return element;\\n}\\n\\ndocument.body.appendChild(app());\\n```\\n\\nAnd just like that, we can use TypeScript in our app!\\n\\nWhat is `ts-loader` actually doing? Well, it\'s taking our TypeScript code, and converting it into JavaScript. For each TypeScript file, `ts-loader` is invoked. It takes the TypeScript code, and passes it to the TypeScript compiler. The TypeScript compiler converts the TypeScript code into JavaScript. `ts-loader` then takes the JavaScript code and passes it to webpack. webpack then takes the JavaScript code and bundles it.\\n\\nThis is what all loaders do; they take a file, process it, and pass it to webpack. There are many loaders available, and you can even write your own. They aren\'t restricted to languages that compile to JavaScript. You can find a list of loaders here: https://webpack.js.org/loaders/\\n\\n### Plugins\\n\\nWe\'ve covered loaders, now we\'ll cover plugins. Plugins allow you to do all kinds of things with webpack. The definition in the webpack documentation is delightfully broad:\\n\\n> Plugins are the backbone of webpack. ... They serve the purpose of doing anything else that a loader cannot do.\\n\\nThis is helpful when you remind yourself that a loader takes a file, processes it, and passes the output of that processing to webpack. It is single-file-oriented, if you like. A plugin is what you use when you want to do something that isn\'t single-file-oriented.\\n\\nSo maybe it\'s easier to give you some examples. We already have one plugin in our app, the `HtmlWebpackPlugin`. This plugin generates an HTML file that includes our bundled JavaScript file(s).\\n\\n#### `DefinePlugin`\\n\\nLet\'s add another plugin to our app. We\'ll add the [`DefinePlugin`](https://webpack.js.org/plugins/define-plugin/). This plugin allows you to define global constants that can be used in your code. Let\'s add it to our `webpack.config.js` file:\\n\\n```diff\\nconst path = require(\\"path\\");\\n+const webpack = require(\\"webpack\\");\\nconst HtmlWebpackPlugin = require(\\"html-webpack-plugin\\");\\n\\nmodule.exports = {\\n  mode: \\"development\\",\\n  entry: \'./src/index.ts\', // the entry point of our app https://webpack.js.org/concepts/entry-points/\\n  devtool: \\"inline-source-map\\",\\n  plugins: [\\n      new HtmlWebpackPlugin(),\\n+      new webpack.DefinePlugin({\\n+        \'MODE\': JSON.stringify(\\"PRODUCTION\\"),\\n+      })\\n  ],\\n  module: {\\n    rules: [\\n      {\\n        test: /\\\\.([cm]?ts|tsx)$/,\\n        loader: \'ts-loader\',\\n      },\\n    ],\\n  },\\n  output: {\\n    filename: \\"[name].[contenthash].js\\",\\n    path: path.resolve(__dirname, \\"dist\\"),\\n    clean: true,\\n  },\\n};\\n```\\n\\nWith the change above, we\'ve added the `DefinePlugin` to our list of plugins. We\'ve also defined a global constant called `MODE` that will be available in our code. We\'ve set the value of `MODE` to be the value of the `NODE_ENV` environment variable. We\'ll use this in our code in a moment.\\n\\nLet\'s update our `src/index.ts` file to use the `MODE` constant, and tell TypeScript about it:\\n\\n```diff\\n+declare const MODE: string;\\n\\nfunction app(): HTMLDivElement {\\n  const element = document.createElement(\\"div\\");\\n\\n-  element.innerHTML = \'Hello, webpack\';\\n+  element.innerHTML = `Hello, webpack, we are in ${MODE} mode.`;\\n\\n  return element;\\n}\\n\\ndocument.body.appendChild(app());\\n```\\n\\nNow if we build our app, we\'ll see that the `MODE` constant is available in our code, and we can use it. At runtime it will be replaced with the value we defined in our `webpack.config.js` file and our app will say:\\n\\n> Hello, webpack, we are in PRODUCTION mode.\\n\\n#### `fork-ts-checker-webpack-plugin`\\n\\nBefore we move on, let\'s add one more plugin to our app. We\'re going to add the [`fork-ts-checker-webpack-plugin`](https://github.com/TypeStrong/fork-ts-checker-webpack-plugin). This plugin allows you to run the TypeScript compiler in a separate process, and relieve `ts-loader` of the responsibility of handling type checking. This is useful because it means that webpack can run in parallel with the TypeScript compiler. This can significantly speed up your build times. I have worked on this plugin, as it has a sibling relationship with `ts-loader`. It\'s quite common to use both together; `ts-loader` to compile your code, and `fork-ts-checker-webpack-plugin` to type check it.\\n\\nLet\'s install `fork-ts-checker-webpack-plugin`:\\n\\n```bash\\nnpm install fork-ts-checker-webpack-plugin --save-dev\\n```\\n\\nAnd let\'s configure it in our `webpack.config.js` file:\\n\\n```diff\\nconst path = require(\\"path\\");\\nconst webpack = require(\\"webpack\\");\\nconst HtmlWebpackPlugin = require(\\"html-webpack-plugin\\");\\nconst ForkTsCheckerWebpackPlugin = require(\'fork-ts-checker-webpack-plugin\');\\n\\nmodule.exports = {\\n  mode: \\"development\\",\\n  entry: \'./src/index.ts\', // the entry point of our app https://webpack.js.org/concepts/entry-points/\\n  devtool: \\"inline-source-map\\",\\n  plugins: [\\n      new HtmlWebpackPlugin(),\\n      new webpack.DefinePlugin({\\n        \'MODE\': JSON.stringify(\'PRODUCTION\'),\\n      }),\\n+      new ForkTsCheckerWebpackPlugin()\\n  ],\\n  module: {\\n    rules: [\\n      {\\n        test: /\\\\.([cm]?ts|tsx)$/,\\n        loader: \'ts-loader\',\\n+        // we only need to explicitly specify transpileOnly option if you use ts-loader < 9.3.0\\n+        options: {\\n+          transpileOnly: true\\n+        }\\n      },\\n    ],\\n  },\\n  output: {\\n    filename: \\"[name].[contenthash].js\\",\\n    path: path.resolve(__dirname, \\"dist\\"),\\n    clean: true,\\n  },\\n};\\n```\\n\\nWe can see here that we\'re providing some configuration to `ts-loader`; setting it to `transpileOnly: true`. This means that `ts-loader` will only transpile our code, and not type check it. We\'re also adding the `ForkTsCheckerWebpackPlugin` to our list of plugins. This will run the TypeScript compiler in a separate process and perform type checking there.\\n\\nIt\'s worth noting that because of [this excellent PR](https://github.com/TypeStrong/ts-loader/pull/1451) by the primary maintainer [Piotr Ole\u015B](https://github.com/piotr-oles), we don\'t need to explicitly specify `transpileOnly: true` anymore. It\'s the default behaviour of `ts-loader` when the `fork-ts-checker-webpack-plugin` is detected. I\'ve left it in the example above to illustrate what configuring a loader looks like.\\n\\nTo test that this is working, let\'s add a type error to our `src/index.ts` file:\\n\\n```diff\\ndeclare const MODE: string;\\n\\nfunction app(): HTMLDivElement {\\n  const element = document.createElement(\\"div\\");\\n\\n  element.innerHTML = `Hello, webpack, we are in ${MODE} mode.`;\\n\\n-  return element;\\n+  return elemen;\\n}\\n\\ndocument.body.appendChild(app());\\n```\\n\\nAnd let\'s build our app:\\n\\n```bash\\nnpm run build\\n\\n> hello-webpack@1.0.0 build\\n> webpack build --mode production\\n\\nassets by status 1.09 KiB [cached] 2 assets\\n./src/index.ts 204 bytes [built] [code generated]\\n\\nERROR in ./src/index.ts:8:10\\nTS2552: Cannot find name \'elemen\'. Did you mean \'element\'?\\n     6 |   element.innerHTML = `Hello, webpack, we are in ${MODE} mode.`;\\n     7 |\\n  >  8 |   return elemen;\\n       |          ^^^^^^\\n     9 | }\\n    10 |\\n    11 | document.body.appendChild(app());\\n\\nwebpack 5.89.0 compiled with 1 error in 1710 ms\\n```\\n\\nWe can see that the TypeScript compiler has picked up our type error. If we remove the `fork-ts-checker-webpack-plugin` from our list of plugins, we\'ll see that the type error is no longer picked up:\\n\\n```bash\\nnpm run build\\n\\n> hello-webpack@1.0.0 build\\n> webpack build --mode production\\n\\nasset main.a1d11e49d0129cad93aa.js 885 bytes [emitted] [immutable] [minimized] (name: main)\\nasset index.html 235 bytes [emitted] [compared for emit]\\n./src/index.ts 204 bytes [built] [code generated]\\nwebpack 5.89.0 compiled successfully in 772 ms\\n```\\n\\nSo we can see that the `fork-ts-checker-webpack-plugin` is working.\\n\\n### Using plugins and loaders - some resources\\n\\nWe\'ve seen a number of examples of plugins. Almost all customisation of webpack is done through plugins and loaders. If you want to do something with webpack, it\'s likely that there\'s a plugin or loader that will help you do it. If you want to learn more about plugins and loaders, I recommend the following resources:\\n\\n- [How to detect dead code in a frontend project](https://blog.logrocket.com/how-detect-dead-code-frontend-project/#using-webpack-for-dead-code-detection)\\n- [Tree shaking JSON files with webpack](https://blog.logrocket.com/tree-shaking-json-files-webpack/)\\n- [Tree shaking and code splitting in webpack](https://blog.logrocket.com/tree-shaking-and-code-splitting-in-webpack/)\\n- [Building micro-frontends with webpack\u2019s Module Federation](https://blog.logrocket.com/building-micro-frontends-webpacks-module-federation/)\\n- [Improve your webpack build with the DLL plugin](https://blog.logrocket.com/speed-up-your-webpack-build-with-the-dll-plugin/)\\n- [Slimming down your bundle size](https://blog.logrocket.com/slimming-down-your-bundle-size/)\\n- [Parsing raw text inputs in web applications using ANTLR](https://blog.logrocket.com/parsing-raw-text-inputs-in-web-applications-using-antlr/)\\n- [An in-depth guide to performance optimization with webpack](https://blog.logrocket.com/guide-performance-optimization-webpack/)\\n\\n## webpack and the competition\\n\\nThere has been a lot of competition in the bundler space. For a long time, webpack has been the most popular bundler. But it\'s not the only game in town, and never has been. It\'s beyond the scope of this article to do a full comparison of webpack and its competitors, but there\'s some excellent articles out there that do just that:\\n\\n- [Snowpack vs. webpack: A build tool comparison](https://blog.logrocket.com/snowpack-vs-webpack-build-tool-comparison/)\\n- [Migrating to SWC: A brief overview](https://blog.logrocket.com/migrating-swc-webpack-babel-overview/)\\n- [webpack or esbuild: Why not both?](https://blog.logrocket.com/webpack-or-esbuild-why-not-both/)\\n- [Switching to Parcel from webpack](https://blog.logrocket.com/switching-to-parcel-from-webpack/)\\n- [Why you should migrate to Rspack from webpack](https://blog.logrocket.com/migrate-rspack-webpack/)\\n- [Introducing Turbopack: A Rust-based successor to webpack](https://blog.logrocket.com/introducing-turbopack-rust-based-successor-webpack/)\\n- [Benchmarking bundlers 2020: Rollup vs. Parcel vs. webpack](https://blog.logrocket.com/benchmarking-bundlers-2020-rollup-parcel-webpack/)\\n\\nFor the longest time, webpack has been the most popular bundler. Apparently incapable of being dislodged from that position. However, it looks like that might be changing. If we look at the npm download stats for webpack for the last five years, we can see that, for the first time, its popularity is starting to decrease. It\'s still the most popular bundler, but it\'s starting to decrease in popularity and competitors are starting to increase. This chart shows the npm download stats for webpack, esbuild, swc and vite over the last five years:\\n\\n![screenshot of a chart comparing webpack, esbuild, swc and vite usage over the years 2018-2023](screenshot-stats-webpack-vite-esbuild-swc.webp)\\n\\n[Vite](https://vitejs.dev/) is a bundler that came out of the Vue ecosystem. It\'s a very fast bundler that uses esbuild under the hood. [esbuild](https://esbuild.github.io/) is a bundler that came out of the Go ecosystem. [swc](https://github.com/swc-project/swc) is a super-fast TypeScript / JavaScript compiler written in Rust. All of these compete with webpack in some way.\\n\\nThe thing to note about all these competitors is that they are all faster than webpack. There\'s a reason for that. webpack is written in JavaScript. For most of the history of bundlers, JavaScript was what bundlers were implemented in. But the next generation of tools are written in other languages. esbuild is written in Go. swc is written in Rust. These languages have allowed massively improved performance. In terms of speed, webpack cannot compete with these tools. It\'s worth noting that even the creator of webpack, Tobias Koppers is now working on a [Rust-based successor to webpack named Turbopack](https://turbo.build/pack).\\n\\nNext generation tools keep appearing. [Bun](https://blog.logrocket.com/bun-adoption-guide/) is an alternative JavaScript runtime, implemented in Zig. It also ships with its own built in [Bun bundler](https://bun.sh/docs/bundler) which is reportedly even faster than esbuild and Rspack! We\'re likely to see even more of these tools in the future.\\n\\nSpeed is a very attractive proposition, and as we can see, we\'re starting to see the community move away from webpack. It\'s not a mass exodus, when people are starting new projects now, they\'re more than likely to use one of the newer tools. When I\'ve started a new project over the last year I\'ve tended to use Vite. I\'ve not used webpack for a new project for a long time. I\'m not alone in this.\\n\\nTo be clear: webpack is not going anywhere. But it\'s fair to say that it is starting to be displaced by some of the newer tools. This trend is only going to continue.\\n\\n## Conclusion\\n\\nIn this article we\'ve looked at what webpack is, and why it\'s so popular. We\'ve looked at its history, and how it came to be the most popular bundler. We\'ve examined how to get started with webpack, some of the high level concepts such as plugins and loaders. We\'ve also considered some of the competition, and how webpack is starting to be displaced by some of the newer tools.\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/webpack-adoption-guide/)\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.logrocket.com/webpack-adoption-guide/\\" />\\n</head>"},{"id":"azure-cosmosdb-container-item-generics","metadata":{"permalink":"/azure-cosmosdb-container-item-generics","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2024-04-01-cosmosdb-container-item-generics/index.md","source":"@site/blog/2024-04-01-cosmosdb-container-item-generics/index.md","title":"Azure Cosmos DB: container items and generics","description":"Learn how to use generics to store and retrieve different types of object in an Azure Cosmos DB Container. And how to deserialize the data property into a C# object of a specific type.","date":"2024-04-01T00:00:00.000Z","tags":[{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."},{"inline":false,"label":"C#","permalink":"/tags/csharp","description":"The C# programming language."}],"readingTime":6.11,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-cosmosdb-container-item-generics","title":"Azure Cosmos DB: container items and generics","authors":"johnnyreilly","tags":["azure","c#"],"image":"./title-image.png","hide_table_of_contents":false,"description":"Learn how to use generics to store and retrieve different types of object in an Azure Cosmos DB Container. And how to deserialize the data property into a C# object of a specific type."},"unlisted":false,"prevItem":{"title":"Overview of webpack, a JavaScript bundler","permalink":"/webpack-overview"},"nextItem":{"title":"Text-first MUI Tabs","permalink":"/text-first-mui-tabs"}},"content":"Cosmos DB is a great database for storing objects. But what if you want to store subtly different types of object in the same container? This post demonstrates how you can use generics to store and retrieve different types of object in an Azure Cosmos DB Container using C#.\\n\\n![title image reading \\"Azure Cosmos DB: container items and generics\\" with the Cosmos DB logo](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## The problem\\n\\nThe situation I have in mind isn\'t entirely different types of object. Rather, it\'s a standard type of object with a single property that can be of different types. Consider the following record:\\n\\n```json\\n{\\n  \\"id\\": \\"overview\\",\\n  \\"itemName\\": \\"vw-beetle\\",\\n  \\"type\\": \\"car\\",\\n  \\"data\\": {\\n    \\"Wheels\\": 4,\\n    \\"Colour\\": \\"blue\\"\\n  },\\n  \\"createdAt\\": \\"2024-03-28T10:55:57.860484+00:00\\",\\n  \\"createdBy\\": \\"john.reilly\\",\\n  \\"updatedAt\\": \\"2024-03-28T14:31:37.9882095+00:00\\",\\n  \\"updatedBy\\": \\"john.reilly\\",\\n  \\"_rid\\": \\"NisFAIjrg3wFAAAAAAAAAA==\\",\\n  \\"_self\\": \\"dbs/NisFAA==/colls/NisFAIjrg3w=/docs/NisFAIjrg3wFAAAAAAAAAA==/\\",\\n  \\"_etag\\": \\"\\\\\\"bd005ad6-0000-0c00-0000-66057f4a0000\\\\\\"\\",\\n  \\"_attachments\\": \\"attachments/\\",\\n  \\"_ts\\": 1711636298\\n}\\n```\\n\\nThe `data` property is a JSON object that can be of any shape. In this case, it\'s a car with four wheels and a blue colour. But it could just as easily be a house with a number of rooms and a garden. Or a person with a name and an age. Or a book with a title and an author. You get the idea.\\n\\nHow can we store and retrieve these objects in a Cosmos DB container with C#?\\n\\n## A generic solution\\n\\nThe answer is to use generics. Here\'s the `MyItem` record that we\'re using in the above code:\\n\\n```cs title=\\"MyItem.cs\\"\\nnamespace ContainerApp.Model.Database;\\n\\n#pragma warning disable IDE1006\\n\\npublic record MyItem(\\n    string id,\\n    /// <summary>\\n    /// This is the partition key\\n    /// </summary>\\n    string itemName,\\n    string type,\\n    DateTimeOffset createdAt,\\n    string createdBy,\\n    DateTimeOffset updatedAt,\\n    string updatedBy\\n) : MyItem<object>(id, creditReviewPackName, itemName, type, null, createdAt, createdBy, updatedAt, updatedBy);\\n\\npublic record MyItem<TData>(\\n    string id,\\n    /// <summary>\\n    /// This is the partition key\\n    /// </summary>\\n    string itemName,\\n    string type,\\n    TData? data,\\n    DateTimeOffset createdAt,\\n    string createdBy,\\n    DateTimeOffset updatedAt,\\n    string updatedBy\\n);\\n\\n#pragma warning restore IDE1006\\n```\\n\\nThe `MyItem` record is a generic record with a single type parameter `TData`. The first record is a convenience record that uses `object` as the type parameter. This is the record that we\'ll use when we\'re writing a record that does not have a `data` property, or when we\'re reading a record and we don\'t initially care about the `data` property.\\n\\nThe `type` field represents the type of `data`. This is a string that can be used to distinguish between different types of object. In the example above, the type is \\"car\\". In other examples, the type might be \\"house\\", \\"person\\", or \\"book\\". We can use this in future to filter the data by type and to deserialize the `data` property into the correct C# type; for instance `Car`. We just need to know how the `type` string maps to a particular C# type.\\n\\n## Writing to and reading from the Cosmos DB container\\n\\nIn the `DatabaseMyItemService` class, we have methods to write to and read from the Cosmos DB container:\\n\\n```cs title=\\"DatabaseMyItemService.cs\\"\\nusing System.Net;\\n\\nusing Microsoft.Azure.Cosmos;\\n\\nusing ContainerApp.Model.Database;\\nusing ContainerApp.Utilities;\\n\\nnamespace ContainerApp.Services;\\n\\npublic class DatabaseMyItemService : IDatabaseMyItemService\\n{\\n    public const string DatabaseName = \\"my-database\\";\\n\\n    public const string ContainerNameMyItems = \\"my-items\\";\\n\\n    private readonly CosmosClient _client;\\n    private readonly ILogger<DatabaseMyItemService> _logger;\\n\\n    public DatabaseMyItemService(ILogger<DatabaseMyItemService> logger, AppSettings appSettings)\\n    {\\n        _client = new CosmosClient(connectionString: appSettings.CosmosConnectionString);\\n        _logger = logger;\\n    }\\n\\n    public async Task<MyItem<TData>?> UpsertItem<TData>(MyItem<TData> myItem)\\n    {\\n        try\\n        {\\n            _logger.LogInformation($\\"Upserting {nameof(MyItem)} with {nameof(myItem.itemName)}: {{{nameof(myItem.itemName)}}}\\", myItem.itemName);\\n\\n            var container = _client\\n                .GetDatabase(DatabaseName)\\n                .GetContainer(ContainerNameMyItems);\\n\\n            MyItem<TData>? savedItem = await container.UpsertItemAsync(myItem, new PartitionKey(myItem.itemName));\\n\\n            return savedItem;\\n        }\\n        catch (CosmosException ex)\\n        {\\n            _logger.LogError(ex, $\\"Problem upserting {nameof(MyItem)} with {nameof(myItem.itemName)}: {{{nameof(myItem.itemName)}}}\\", myItem.itemName);\\n            throw new Exception($\\"Problem upserting {nameof(MyItem)} with {nameof(myItem.itemName)}: {myItem.itemName}\\", ex);\\n        }\\n    }\\n\\n    public async Task<MyItem<TData>?> GetItem<TData>(string itemName)\\n    {\\n        try\\n        {\\n            _logger.LogInformation($\\"Looking up {nameof(MyItem)} with {nameof(itemName)}: {{{nameof(itemName)}}}\\", itemName);\\n\\n            var container = _client\\n                .GetDatabase(DatabaseName)\\n                .GetContainer(ContainerNameMyItems);\\n\\n            // In this simplified example we\'re intentionally using id as partition key - https://stackoverflow.com/questions/54636852/implications-of-using-id-for-the-partition-key-in-cosmosdb\\n            MyItem<TData>? myItem = await container.ReadItemAsync<MyItem<TData>>(itemName, new PartitionKey(itemName));\\n\\n            return myItem;\\n        }\\n        catch (CosmosException ex) when (ex.StatusCode == HttpStatusCode.NotFound)\\n        {\\n            return null;\\n        }\\n    }\\n\\n    public async Task<List<MyItem>> GetItems(string itemName)\\n    {\\n        try\\n        {\\n            _logger.LogInformation($\\"Looking up {nameof(MyItem)}s by {nameof(itemName)}: {{{nameof(itemName)}}}\\", itemName);\\n\\n            var container = _client\\n                .GetDatabase(DatabaseName)\\n                .GetContainer(ContainerNameMyItems);\\n\\n            List<MyItem> myItems = await container.GetItemQueryIterator<MyItem>(\\n                new QueryDefinition(\\n                    \\"SELECT * FROM c WHERE c.itemName = @itemName\\"\\n                ).WithParameter(\\"@itemName\\", itemName)\\n            ).ReadAllToListAsync();\\n\\n            return myItems;\\n        }\\n        catch (CosmosException ex)\\n        {\\n            _logger.LogError(ex, $\\"Problem getting {nameof(MyItem)}s by {nameof(itemName)}: {{{nameof(itemName)}}}\\", itemName);\\n            throw new Exception($\\"Problem getting {nameof(MyItem)}s by {nameof(itemName)}: {itemName}\\", ex);\\n        }\\n    }\\n}\\n```\\n\\nYou\'ll note that the `UpsertItem` and `GetItem` methods are generic methods that take and return a `MyItem<TData>` record respectively. The `GetItems` method is not generic because it returns a list of `MyItem` records, which are the non-generic records; where `data` is of type `object?`.\\n\\nImagine, you might use the `GetItems` method to get all the items. If you wanted to load a particular item, in a strongly typed fashion, you might subsequently use the `GetItem` method to load a single item with a particular type, like so:\\n\\n```cs\\nvar myCar = await _databaseMyItemService.GetItem<Car>(\\"the-car\\");\\n```\\n\\n## Deserializing the `data` property with `JSON.NET`\\n\\nIf you want to avoid requerying the database to get the object in strongly typed form, you\'ll need to convert the `data` property into a C# object of a specific type. If you\'ve retrieved the non-generic `MyItem` from Cosmos, as far as C# is concerned, the `data` property is just an `object?` at this point. Well, that\'s not quite true. It\'s actually a `JObject` from the `Newtonsoft.Json` library. (This is because the Cosmos DB SDK uses `Newtonsoft.Json` internally.)\\n\\nYou can use `JObject.ToObject<T>()` to convert the `data` property into a C# object of a specific type. Here\'s an example of how you might do this:\\n\\n```cs\\nvar data = item.data is not Newtonsoft.Json.Linq.JObject dataJObject\\n    ? null\\n    : dataJObject.ToObject<Car>();\\n```\\n\\n## Deserializing the `data` property with `System.Text.Json`\\n\\nYou may well find yourself wanting to send a list of items to the front end. However, because the default serializer of ASP.NET is `System.Text.Json.JsonSerializer` you\'ll need a different approach to deal with the `JObject`, as you can\'t send a `JObject` to the front end. You need to deserialize it into a format that can be sent to the front end.\\n\\nIt\'s quite typical to have a method that converts a domain model to a view model; something like this:\\n\\n```cs\\npublic record MyItemViewModel(\\n    string ItemName,\\n    string Type,\\n    object? Data,\\n    DateTimeOffset CreatedAt,\\n    string CreatedBy,\\n    DateTimeOffset UpdatedAt,\\n    string UpdatedBy\\n);\\n```\\n\\nHere\'s an example of how you might convert our domain model to our view model. It includes a mechanism that uses `System.Text.Json.JsonSerializer` to deserialize the `data` property into an `object?` that **can** be sent to the front end:\\n\\n```cs\\npublic static MyItemViewModel ItemToItemViewModel(MyItem item)\\n{\\n    var data = creditReviewPackItem.data switch\\n    {\\n        Newtonsoft.Json.Linq.JObject dataJObject => System.Text.Json.JsonSerializer.Deserialize<object>(dataJObject.ToString()),\\n        Newtonsoft.Json.Linq.JArray dataJArray => System.Text.Json.JsonSerializer.Deserialize<object>(dataJArray.ToString()),\\n        _ => null\\n    };\\n\\n    return new(\\n        ItemName: item.itemName,\\n        Type: item.type,\\n        Data: data,\\n        CreatedAt: item.createdAt,\\n        CreatedBy: item.createdBy,\\n        UpdatedAt: item.updatedAt,\\n        UpdatedBy: item.updatedBy\\n    );\\n}\\n```\\n\\n## Conclusion\\n\\nIn this post, we\'ve seen how you can use generics to store and retrieve different types of object in an Azure Cosmos DB Container using C#. We\'ve seen how you can use a generic record to store objects with a single property that can be of different types. We\'ve also seen how you can use `Newtonsoft.Json` to deserialize the `data` property into a C# object of a specific type. And we\'ve seen how you can use `System.Text.Json` to deserialize the `data` property into an `object?` that can be sent to the front end."},{"id":"text-first-mui-tabs","metadata":{"permalink":"/text-first-mui-tabs","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2024-03-20-text-first-mui-tabs/index.md","source":"@site/blog/2024-03-20-text-first-mui-tabs/index.md","title":"Text-first MUI Tabs","description":"Learn how to use the MUI tabs component in a text first way that remains strongly typed.","date":"2024-03-20T00:00:00.000Z","tags":[{"inline":false,"label":"React","permalink":"/tags/react","description":"The React library."},{"inline":false,"label":"MUI","permalink":"/tags/mui","description":"The MUI / Material UI component library."}],"readingTime":6.635,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"text-first-mui-tabs","title":"Text-first MUI Tabs","authors":"johnnyreilly","tags":["react","mui"],"image":"./title-image.png","hide_table_of_contents":false,"description":"Learn how to use the MUI tabs component in a text first way that remains strongly typed."},"unlisted":false,"prevItem":{"title":"Azure Cosmos DB: container items and generics","permalink":"/azure-cosmosdb-container-item-generics"},"nextItem":{"title":"Generate a Word document in ASP.NET","permalink":"/generate-word-doc-in-asp-net"}},"content":"I love the Material-UI (MUI) library for React. Hand on heart, I\'m not very good at making UIs that are attractive. So I always grab for something to paper over the cracks. MUI is awesome for that.\\n\\nOne of the components that I use frequently is the [tabs component](https://mui.com/material-ui/react-tabs/). However, I\'ve found that it can be a little tricky to use in a \\"text-first\\" way, that also remains strongly typed. This post documents how to do just that!\\n\\n![title image reading \\"Text-first MUI Tabs\\" with the MUI logo](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Official example\\n\\nWhat does the tabs component look like? Well, here\'s a screenshot of it in action:\\n\\n![screenshot of the tabs component](screenshot-mui-tabs.png)\\n\\nIt\'s very useful if you\'d like your users to be able to switch between different views easily. The official MUI documentation provides an example of how to use the tabs component:\\n\\n```tsx\\nimport * as React from \'react\';\\nimport Tabs from \'@mui/material/Tabs\';\\nimport Tab from \'@mui/material/Tab\';\\nimport Typography from \'@mui/material/Typography\';\\nimport Box from \'@mui/material/Box\';\\n\\ninterface TabPanelProps {\\n  children?: React.ReactNode;\\n  index: number;\\n  value: number;\\n}\\n\\nfunction CustomTabPanel(props: TabPanelProps) {\\n  const { children, value, index, ...other } = props;\\n\\n  return (\\n    <div\\n      role=\\"tabpanel\\"\\n      hidden={value !== index}\\n      id={`simple-tabpanel-${index}`}\\n      aria-labelledby={`simple-tab-${index}`}\\n      {...other}\\n    >\\n      {value === index && (\\n        <Box sx={{ p: 3 }}>\\n          <Typography>{children}</Typography>\\n        </Box>\\n      )}\\n    </div>\\n  );\\n}\\n\\nfunction a11yProps(index: number) {\\n  return {\\n    id: `simple-tab-${index}`,\\n    \'aria-controls\': `simple-tabpanel-${index}`,\\n  };\\n}\\n\\nexport default function BasicTabs() {\\n  const [value, setValue] = React.useState(0);\\n\\n  const handleChange = (event: React.SyntheticEvent, newValue: number) => {\\n    setValue(newValue);\\n  };\\n\\n  return (\\n    <Box sx={{ width: \'100%\' }}>\\n      <Box sx={{ borderBottom: 1, borderColor: \'divider\' }}>\\n        <Tabs\\n          value={value}\\n          onChange={handleChange}\\n          aria-label=\\"basic tabs example\\"\\n        >\\n          <Tab label=\\"Item One\\" {...a11yProps(0)} />\\n          <Tab label=\\"Item Two\\" {...a11yProps(1)} />\\n          <Tab label=\\"Item Three\\" {...a11yProps(2)} />\\n        </Tabs>\\n      </Box>\\n      <CustomTabPanel value={value} index={0}>\\n        Item One\\n      </CustomTabPanel>\\n      <CustomTabPanel value={value} index={1}>\\n        Item Two\\n      </CustomTabPanel>\\n      <CustomTabPanel value={value} index={2}>\\n        Item Three\\n      </CustomTabPanel>\\n    </Box>\\n  );\\n}\\n```\\n\\nThis example is great, but (personally) I find it a little hard to read. There\'s a direct relationship between the tabs and the tab panels, but it\'s not immediately obvious. When you see the `0` passed to `a11yProps` and the `0` passed to `CustomTabPanel`, it\'s not clear that they\'re related. And if the `a11yProps` function call was not present, it would be even less clear.\\n\\nI\'d like to see the tabs and tab panels presented together in a more text-first way, that makes the relationship between tab and tab panel more apparent.\\n\\n## Text-first tabs\\n\\nThe code I\'d like to see would look something like this:\\n\\n```tsx\\n<Box sx={{ width: \'100%\' }}>\\n  <Box sx={{ borderBottom: 1, borderColor: \'divider\' }}>\\n    <Tabs\\n      value={selectedTab}\\n      onChange={handleChange}\\n      aria-label=\\"basic tabs example\\"\\n    >\\n      <Tab {...customTabProps(\'Item One\')} />\\n      <Tab {...customTabProps(\'Item Two\')} />\\n      <Tab {...customTabProps(\'Item Three\')} />\\n    </Tabs>\\n  </Box>\\n  <CustomTabPanel selectedTab={selectedTab} tab=\\"Item One\\">\\n    Item One\\n  </CustomTabPanel>\\n  <CustomTabPanel selectedTab={selectedTab} tab=\\"Item Two\\">\\n    Item Two\\n  </CustomTabPanel>\\n  <CustomTabPanel selectedTab={selectedTab} tab=\\"Item Three\\">\\n    Item Three\\n  </CustomTabPanel>\\n</Box>\\n```\\n\\nIn this code snippet, the tabs and tab panels have more of a linkage, in a text-first way. It\'s hopefully clear that the \\"Item One\\" `Tab` and the \\"Item One\\" `CustomTabPanel` are related.\\n\\nIn the code above, the `customTabProps` function is used to generate the props for the tabs (it\'s an evolution of the `a11yProps` function that handles accessibility props as well as all others). Meanwhile, the `CustomTabPanel` component is used to render the tab panels. The `selectedTab` state is used to keep track of the selected tab.\\n\\nHow does this work? And is it strongly typed? Let\'s find out.\\n\\n## Strongly typed tabs\\n\\nYes, it\'s strongly typed! We achieve this by defining a mapping of tab text to tab index:\\n\\n```ts\\nconst tabs = {\\n  \'Item One\': 0,\\n  \'Item Two\': 1,\\n  \'Item Three\': 2,\\n} as const;\\n\\ntype TabText = keyof typeof tabs;\\n\\ntype TabIndex = (typeof tabs)[TabText];\\n```\\n\\nSo \\"Item One\\" is `0`, \\"Item Two\\" is `1`, and \\"Item Three\\" is `2`.\\n\\nWe then do some TypeScript magic to strongly type this. We use `as const` to tell TypeScript this is an immutable object. With that done we can then extract the keys and values from the object and use them to create the derived types `TabText` and `TabIndex`.\\n\\n`TabText` is the keys of the `tabs` object and `TabIndex` is the values. So `TabText` is `\\"Item One\\" | \\"Item Two\\" | \\"Item Three\\"` and `TabIndex` is `0 | 1 | 2`. If we should subsequently amend the `tabs` object in our code, TypeScript will ensure that the `TabText` and `TabIndex` types are updated accordingly.\\n\\nWe can then use these types in our components:\\n\\n```tsx\\nfunction customTabProps(tab: TabText) {\\n  const index = tabs[tab];\\n  return {\\n    id: `simple-tab-tab-${index}`,\\n    \'aria-controls\': `simple-tab-tabpanel-${index}`,\\n    label: tab,\\n  };\\n}\\n\\ninterface CustomTabPanelProps {\\n  children?: React.ReactNode;\\n  tab: TabText;\\n  selectedTab: TabIndex;\\n}\\n\\nfunction CustomTabPanel(props: CustomTabPanelProps) {\\n  const { children, selectedTab, tab, ...other } = props;\\n  const index = tabs[tab];\\n\\n  return (\\n    <div\\n      role=\\"tabpanel\\"\\n      hidden={selectedTab !== index}\\n      id={`simple-tab-tabpanel-${index}`}\\n      aria-labelledby={`simple-tab-tab-${index}`}\\n      {...other}\\n    >\\n      {selectedTab === index && (\\n        <Box sx={{ p: 3 }}>\\n          <Typography>{children}</Typography>\\n        </Box>\\n      )}\\n    </div>\\n  );\\n}\\n```\\n\\nThen our final example code looks like this:\\n\\n```tsx\\nexport default function BasicTabs() {\\n  const [selectedTab, setSelectedTab] = React.useState<TabIndex>(\\n    tabs[\'Item One\'],\\n  );\\n  const handleChange = (event: React.SyntheticEvent, newValue: TabIndex) => {\\n    setSelectedTab(newValue);\\n  };\\n\\n  return (\\n    <Box sx={{ width: \'100%\' }}>\\n      <Box sx={{ borderBottom: 1, borderColor: \'divider\' }}>\\n        <Tabs\\n          value={selectedTab}\\n          onChange={handleChange}\\n          aria-label=\\"basic tabs example\\"\\n        >\\n          <Tab {...customTabProps(\'Item One\')} />\\n          <Tab {...customTabProps(\'Item Two\')} />\\n          <Tab {...customTabProps(\'Item Three\')} />\\n        </Tabs>\\n      </Box>\\n      <CustomTabPanel selectedTab={selectedTab} tab=\\"Item One\\">\\n        Item One\\n      </CustomTabPanel>\\n      <CustomTabPanel selectedTab={selectedTab} tab=\\"Item Two\\">\\n        Item Two\\n      </CustomTabPanel>\\n      <CustomTabPanel selectedTab={selectedTab} tab=\\"Item Three\\">\\n        Item Three\\n      </CustomTabPanel>\\n    </Box>\\n  );\\n}\\n```\\n\\nNote how we use our `TabIndex` types to strongly type the `selectedTab` state and the `handleChange` function. And also how the `TabText` type is used to strongly type the `tab` prop in the `CustomTabPanel` component and the `tab` argument in the `customTabProps` function. With this in place, we cannot provide invalid tab text to the `customTabProps` function or the `CustomTabPanel` component. TypeScript would fight us every step of the way if we tried.\\n\\n## Conclusion\\n\\nSo now we have a strongly typed, text-first way to use the MUI tabs component. We\'ve used TypeScript to ensure that our tabs and tab panels are related in a way that is clear and easy to understand. This approach makes our code more maintainable and easier to work with. The full code is below:\\n\\n```tsx\\nimport * as React from \'react\';\\nimport Tabs from \'@mui/material/Tabs\';\\nimport Tab from \'@mui/material/Tab\';\\nimport Typography from \'@mui/material/Typography\';\\nimport Box from \'@mui/material/Box\';\\n\\nconst tabs = {\\n  \'Item One\': 0,\\n  \'Item Two\': 1,\\n  \'Item Three\': 2,\\n} as const;\\n\\ntype TabText = keyof typeof tabs;\\n\\ntype TabIndex = (typeof tabs)[TabText];\\n\\nfunction customTabProps(tab: TabText) {\\n  const index = tabs[tab];\\n  return {\\n    id: `simple-tab-tab-${index}`,\\n    \'aria-controls\': `simple-tab-tabpanel-${index}`,\\n    label: tab,\\n  };\\n}\\n\\ninterface CustomTabPanelProps {\\n  children?: React.ReactNode;\\n  tab: TabText;\\n  selectedTab: TabIndex;\\n}\\n\\nfunction CustomTabPanel(props: CustomTabPanelProps) {\\n  const { children, selectedTab, tab, ...other } = props;\\n  const index = tabs[tab];\\n\\n  return (\\n    <div\\n      role=\\"tabpanel\\"\\n      hidden={selectedTab !== index}\\n      id={`simple-tab-tabpanel-${index}`}\\n      aria-labelledby={`simple-tab-tab-${index}`}\\n      {...other}\\n    >\\n      {selectedTab === index && (\\n        <Box sx={{ p: 3 }}>\\n          <Typography>{children}</Typography>\\n        </Box>\\n      )}\\n    </div>\\n  );\\n}\\n\\nexport default function BasicTabs() {\\n  const [selectedTab, setSelectedTab] = React.useState<TabIndex>(\\n    tabs[\'Item One\'],\\n  );\\n  const handleChange = (event: React.SyntheticEvent, newValue: TabIndex) => {\\n    setSelectedTab(newValue);\\n  };\\n\\n  return (\\n    <Box sx={{ width: \'100%\' }}>\\n      <Box sx={{ borderBottom: 1, borderColor: \'divider\' }}>\\n        <Tabs\\n          value={selectedTab}\\n          onChange={handleChange}\\n          aria-label=\\"basic tabs example\\"\\n        >\\n          <Tab {...customTabProps(\'Item One\')} />\\n          <Tab {...customTabProps(\'Item Two\')} />\\n          <Tab {...customTabProps(\'Item Three\')} />\\n        </Tabs>\\n      </Box>\\n      <CustomTabPanel selectedTab={selectedTab} tab=\\"Item One\\">\\n        Item One\\n      </CustomTabPanel>\\n      <CustomTabPanel selectedTab={selectedTab} tab=\\"Item Two\\">\\n        Item Two\\n      </CustomTabPanel>\\n      <CustomTabPanel selectedTab={selectedTab} tab=\\"Item Three\\">\\n        Item Three\\n      </CustomTabPanel>\\n    </Box>\\n  );\\n}\\n```\\n\\nIt\'s certainly more complicated than the official example (and this may well be why the official example is the way it is), but it matches my preferences.\\n\\nAs an aside, I\'d like the code even more if I had the following instead of using `Tab` with `customTabProps`:\\n\\n```tsx\\n<CustomTab tab=\\"Item One\\" />\\n```\\n\\nI avoided that in this post because it would have made the example more complicated. But I think it would be a nice improvement."},{"id":"generate-word-doc-in-asp-net","metadata":{"permalink":"/generate-word-doc-in-asp-net","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2024-03-19-generate-word-doc-in-asp-net/index.md","source":"@site/blog/2024-03-19-generate-word-doc-in-asp-net/index.md","title":"Generate a Word document in ASP.NET","description":"Learn how to generate a Word document using the Open XML library in ASP.NET.","date":"2024-03-19T00:00:00.000Z","tags":[{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":1.965,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"generate-word-doc-in-asp-net","title":"Generate a Word document in ASP.NET","authors":"johnnyreilly","tags":["asp.net"],"image":"./title-image.png","hide_table_of_contents":false,"description":"Learn how to generate a Word document using the Open XML library in ASP.NET."},"unlisted":false,"prevItem":{"title":"Text-first MUI Tabs","permalink":"/text-first-mui-tabs"},"nextItem":{"title":"Configure Azure connection strings and keys in Azure Bicep","permalink":"/configure-azure-connection-strings-keys-in-azure-bicep"}},"content":"Generating a Word document in the context of an ASP.NET controller is quite simple to do. However, it took me a little experimentation to work out just what was required. This post documents (pun **very** much intended) what we need to do.\\n\\n![title image reading \\"Generate a Word document in ASP.NET\\" with the Word and ASP.NET logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Open XML\\n\\nTo generate a Word document in .NET, the most straightforward way is to use the [Open XML library](https://github.com/dotnet/Open-XML-SDK). We can install the library using the following command:\\n\\n```sh\\ndotnet add package DocumentFormat.OpenXml\\n```\\n\\n## Generating a Word document in an ASP.NET controller\\n\\nWith the Open XML library installed, we can create a new Word document in the context of an ASP.NET controller. The following code demonstrates how to do this:\\n\\n```cs\\nusing Microsoft.AspNetCore.Mvc;\\nusing DocumentFormat.OpenXml;\\nusing DocumentFormat.OpenXml.Packaging;\\nusing DocumentFormat.OpenXml.Wordprocessing;\\n\\nnamespace MyApp.Controllers;\\n\\n[ApiController]\\npublic class WordDocumentController() : ControllerBase\\n{\\n    [HttpGet(\\"api/generate-word-document\\")]\\n    public IActionResult GetWordDocument()\\n    {\\n        // Create a new Word document\\n        using var stream = new MemoryStream();\\n        using var document = WordprocessingDocument.Create(stream, WordprocessingDocumentType.Document);\\n\\n        var mainPart = document.AddMainDocumentPart();\\n        mainPart.Document = new Document();\\n\\n        // Add content to the document\\n        var body = mainPart.Document.AppendChild(new Body());\\n        var paragraph = body.AppendChild(new Paragraph());\\n        var run = paragraph.AppendChild(new Run());\\n        run.AppendChild(new Text(\\"Hello, World!\\"));\\n\\n        // Save the document to a memory stream\\n        document.Save();\\n        var byteArray = stream.ToArray();\\n\\n        // Return the document as a file\\n        return File(byteArray, \\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\\", \\"document.docx\\");\\n    }\\n}\\n```\\n\\nIn this example, the `GetWordDocument` method creates a new Word document and adds the text \\"Hello, World!\\" to it. If we navigate to the `/api/generate-word-document` endpoint, we will receive a Word document with the text \\"Hello, World!\\" in it.\\n\\nThe document is then saved to a memory stream and returned as a file. The `File` method is used to return the document as a file with the MIME type `application/vnd.openxmlformats-officedocument.wordprocessingml.document` (which basically is the server saying \\"Hey! This is a Word document!\\").\\n\\n## Conclusion\\n\\nGenerating a Word document in an ASP.NET controller is quite simple to do using the Open XML library. We can create a new Word document, add content to it, and return it as a file using the `File` method.\\n\\nTo learn more about how to add content to a Word document using the Open XML library, it\'s worth reading the [Open XML SDK documentation](https://learn.microsoft.com/en-us/office/open-xml/word/overview).\\n\\nI hope this post helps you to generate Word documents in your ASP.NET applications!"},{"id":"configure-azure-connection-strings-keys-in-azure-bicep","metadata":{"permalink":"/configure-azure-connection-strings-keys-in-azure-bicep","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2024-03-10-configure-azure-connection-strings-keys-in-azure-bicep/index.md","source":"@site/blog/2024-03-10-configure-azure-connection-strings-keys-in-azure-bicep/index.md","title":"Configure Azure connection strings and keys in Azure Bicep","description":"Learn how to configure Azure resources like Azure Static Web Apps and Azure Container Apps with connection strings and access keys in Azure with Bicep.","date":"2024-03-10T00:00:00.000Z","tags":[{"inline":false,"label":"Bicep","permalink":"/tags/bicep","description":"The Bicep language for Azure Resource Manager templates."},{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."},{"inline":false,"label":"Azure Container Apps","permalink":"/tags/azure-container-apps","description":"The Azure Container Apps service. Effectively a managed Kubernetes service."},{"inline":false,"label":"Azure Static Web Apps","permalink":"/tags/azure-static-web-apps","description":"The Azure Static Web Apps service."}],"readingTime":4.64,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"configure-azure-connection-strings-keys-in-azure-bicep","title":"Configure Azure connection strings and keys in Azure Bicep","authors":"johnnyreilly","tags":["bicep","azure","azure container apps","azure static web apps"],"image":"./title-image.png","hide_table_of_contents":false,"description":"Learn how to configure Azure resources like Azure Static Web Apps and Azure Container Apps with connection strings and access keys in Azure with Bicep."},"unlisted":false,"prevItem":{"title":"Generate a Word document in ASP.NET","permalink":"/generate-word-doc-in-asp-net"},"nextItem":{"title":"Multiline strings in .env files","permalink":"/multiline-strings-dot-env-files"}},"content":"Imagine you\'re deploying a solution to Azure. It\'ll feature some resources like a database or a storage account. How do can you configure your application with access to these resources? One approach would be using Managed Identity. Another approach is configuring the connection strings and access keys in our application\'s configuration store as the Bicep templates are deployed. This is a common approach when working with Azure Functions, Azure Static Web Apps, Azure Container Apps and similar.\\n\\n![title image reading \\"Configure Azure connection strings and keys in Azure Bicep\\" with the Bicep and Azure logos](title-image.png)\\n\\nA wonderful aspect of this approach is that no human need ever get to see the connection strings / access keys. They\'ll be discovered and consumed by Azure during a deployment, and known to your application at runtime, but untrustworthy humans need never get to see them. This is secure, and therefore _good_.\\n\\n\x3c!--truncate--\x3e\\n\\n## Configure an Azure Static Web App with a connection string and an access key\\n\\nThe blog you are reading this on is hosted on Azure Static Web Apps and deployed with Bicep. It also has an Azure Cosmos DB database and an Application Insights instance. The Azure Static Web App has access to the database via its access key and has access to the Application Insights instance through a connection string. The key and connection string are supplied to the configuration of the SWA during deployment.\\n\\nLet\'s look at the Bicep configuration that deploys a database. Here\'s a snippet of the Bicep template:\\n\\n```bicep\\nresource databaseAccount \'Microsoft.DocumentDB/databaseAccounts@2023-04-15\' = {\\n  name: cosmosDbAccountName\\n  kind: \'GlobalDocumentDB\'\\n  location: location\\n  tags: tags\\n  properties: {\\n    consistencyPolicy: { defaultConsistencyLevel: \'Session\' }\\n    locations: locations\\n    enableAutomaticFailover: true\\n    databaseAccountOfferType: \'Standard\'\\n    publicNetworkAccess: \'Enabled\'\\n    ipRules: [for ipAddress in ipAddresses: {\\n      ipAddressOrRange: ipAddress\\n    }]\\n    backupPolicy: { type: \'Periodic\', periodicModeProperties: { backupIntervalInMinutes: 240, backupRetentionIntervalInHours: 720 }}\\n  }\\n}\\n```\\n\\nHere\'s a snippet of the Bicep template that deploys the Application Insights instance:\\n\\n```bicep\\nresource appInsights \'Microsoft.Insights/components@2020-02-02\' = {\\n  name: appInsightsName\\n  location: location\\n  kind: \'other\'\\n  properties: {\\n    Application_Type: \'web\'\\n    Flow_Type: \'Bluefield\'\\n    WorkspaceResourceId: workspace.id\\n    RetentionInDays: 90\\n    IngestionMode: \'LogAnalytics\'\\n    publicNetworkAccessForIngestion: \'Enabled\'\\n    publicNetworkAccessForQuery: \'Enabled\'\\n  }\\n}\\n```\\n\\nGiven that both of these resources are deployed, we can reference them subsequently and acquire connection strings / access keys.\\n\\nSo when we\'re getting ready to deploy the Azure Static Web App, we are able reference both the database and the Application Insights instance. Here\'s a snippet of the Bicep template that acquires the references:\\n\\n```bicep\\nresource databaseAccount \'Microsoft.DocumentDB/databaseAccounts@2023-04-15\' existing = {\\n  name: cosmosDbAccountName\\n}\\n\\nresource appInsightsResource \'Microsoft.Insights/components@2020-02-02\' existing = {\\n  name: appInsightsName\\n}\\n```\\n\\nWith those references in hand, we can now configure the Azure Static Web App with the connection string and access key. Here\'s a snippet of the Bicep template that configures the Azure Static Web App with the connection string and access key:\\n\\n```bicep\\n// deploy the Azure Static Web App\\nresource staticWebApp \'Microsoft.Web/staticSites@2022-09-01\' = {\\n  name: staticWebAppName\\n  location: location\\n  tags: tagsWithHiddenLinks\\n  sku: {\\n    name: \'Free\'\\n    tier: \'Free\'\\n  }\\n  properties: {\\n    repositoryUrl: \'https://github.com/johnnyreilly/blog.johnnyreilly.com\'\\n    repositoryToken: repositoryToken\\n    branch: branch\\n    provider: \'GitHub\'\\n    stagingEnvironmentPolicy: \'Enabled\'\\n    allowConfigFileUpdates: true\\n    buildProperties:{\\n      skipGithubActionWorkflowGeneration: true\\n    }\\n  }\\n}\\n\\n// configure the Azure Static Web App with the connection string and access key\\nresource staticWebAppAppSettings \'Microsoft.Web/staticSites/config@2022-09-01\' = {\\n  name: \'appsettings\'\\n  kind: \'config\'\\n  parent: staticWebApp\\n  properties: {\\n    APPINSIGHTS_INSTRUMENTATIONKEY: appInsightsResource.properties.InstrumentationKey\\n    APPLICATIONINSIGHTS_CONNECTION_STRING: appInsightsResource.properties.ConnectionString // <-- connection string\\n    COSMOS_ENDPOINT: databaseAccount.properties.documentEndpoint\\n    COSMOS_KEY: databaseAccount.listKeys().primaryMasterKey // <-- access key\\n  }\\n}\\n```\\n\\nI\'ve slightly tweaked the code to make it more readable, if you\'d like to see the full configuration of the Azure Static Web App in the source of my blog, you can find it [here](https://github.com/johnnyreilly/blog.johnnyreilly.com/blob/df2382e31dab82604e98d91f83967b8b559eb507/infra/static-web-app.bicep#L47C1-L57C2).\\n\\nYou can see the effect of this configuration in the Azure Portal. Here\'s a screenshot of the configured environment variables of the Azure Static Web App:\\n\\n![Screenshot of the Azure Static Web App in the Azure Portal](screenshot-azure-portal-environment-variables.png)\\n\\n## Configure an Azure Container App with a connection string and an access key\\n\\nWhat\'s hopefully apparent from the previous section is that in the end this amounts to injecting a string to the appropriate place in the configuration of the resource. This is true for Azure Container Apps as well. Here\'s a snippet of the Bicep template that configures an Azure Container App with a connection string and access key:\\n\\n```bicep\\nvar appInsightsInstrumentationRef = \'app-insights-instrumentation-key\'\\nvar appInsightsConnectionStringRef = \'app-insights-connection-string\'\\nvar cosmosKeyRef = \'cosmos-key\'\\n\\nresource webServiceContainerApp \'Microsoft.App/containerApps@2023-05-01\' = {\\n  name: webServiceContainerAppName\\n  tags: tags\\n  location: location\\n  properties: {\\n    // ...\\n    configuration: {\\n      secrets: [\\n        {\\n          name: appInsightsInstrumentationRef\\n          value: appInsightsResource.properties.InstrumentationKey\\n        }\\n        {\\n          name: appInsightsConnectionStringRef\\n          value: appInsightsResource.properties.ConnectionString // <-- connection string\\n        }\\n        {\\n          name: cosmosKeyRef\\n          value: databaseAccount.listKeys().primaryMasterKey // <-- access key\\n        }\\n        // ...\\n      ]\\n      // ...\\n    }\\n    template: {\\n      containers: [\\n        {\\n          // ...\\n          env: [\\n            {\\n              name: \'APPINSIGHTS_INSTRUMENTATIONKEY\'\\n              secretRef: appInsightsInstrumentationRef\\n            }\\n            {\\n              name: \'APPLICATIONINSIGHTS_CONNECTION_STRING\'\\n              secretRef: appInsightsConnectionStringRef\\n            }\\n            {\\n              name: \'COSMOS_ENDPOINT\'\\n              value: databaseAccount.properties.documentEndpoint\\n            }\\n            {\\n              name: \'COSMOS_KEY\'\\n              secretRef: cosmosKeyRef\\n            }\\n            // ...\\n          ]\\n        }\\n      ]\\n      // ...\\n    }\\n  }\\n}\\n```\\n\\nThe mechanism is slightly different, as befits the different service being used, but the principle is the same. We\'re injecting the connection string and access key into the configuration of the resource.\\n\\n## Conclusion\\n\\nIn this post we\'ve demonstrated how to deploy resources, acquire reference to them and safely configure Azure Static Web Apps and Azure Container Apps such that they can access the resources.\\n\\nThe pattern we\'ve used here is generally applicable in the Azure world. The same technique can be used to configure Azure Functions, Azure KeyVault, and many other Azure resources. The key is to understand the configuration of the resource you\'re working with and to understand how to inject the relevant secrets into that configuration."},{"id":"multiline-strings-dot-env-files","metadata":{"permalink":"/multiline-strings-dot-env-files","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2024-03-09-multiline-strings-dot-env-files/index.md","source":"@site/blog/2024-03-09-multiline-strings-dot-env-files/index.md","title":"Multiline strings in .env files","description":"Learn how to use multiline strings in .env files.","date":"2024-03-09T00:00:00.000Z","tags":[{"inline":false,"label":"Node.js","permalink":"/tags/node-js","description":"The Node.js runtime."}],"readingTime":0.835,"hasTruncateMarker":false,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"multiline-strings-dot-env-files","title":"Multiline strings in .env files","authors":"johnnyreilly","tags":["node.js"],"hide_table_of_contents":false,"description":"Learn how to use multiline strings in .env files."},"unlisted":false,"prevItem":{"title":"Configure Azure connection strings and keys in Azure Bicep","permalink":"/configure-azure-connection-strings-keys-in-azure-bicep"},"nextItem":{"title":"ESLint no-unused-vars: _ ignore prefix","permalink":"/typescript-eslint-no-unused-vars"}},"content":"I love using `.env` files to configure my applications. They\'re a great way to keep configuration in one place and to keep it out of the codebase. They\'re also a great way to keep secrets out of the codebase.\\n\\nBut what if you need to use a multiline string in a `.env` file? How do you do that? You just do it:\\n\\n```env\\nSINGLE_LINE=\\"you know what...\\"\\nMULTI_LINE=\\"you know what you did\\nLAST SUMMER\\"\\n```\\n\\nThat\'s right, you just use a newline character. It\'s that simple. Oddly, searching for that on the internet didn\'t yield the answer I was looking for. So I\'m writing it down here for posterity.\\n\\nWith your `.env` file in place, you can then consume it in your application using a package like [`dotenv`](https://www.npmjs.com/package/dotenv). Or if you\'d like to use a bash script to consume the `.env` file, you can do it like this:\\n\\n```bash\\n#!/usr/bin/env bash\\nset -a\\nsource test.env\\nset +a\\n\\nnpm run start # or whatever you need to do\\n```"},{"id":"typescript-eslint-no-unused-vars","metadata":{"permalink":"/typescript-eslint-no-unused-vars","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2024-02-18-typescript-eslint-no-unused-vars/index.md","source":"@site/blog/2024-02-18-typescript-eslint-no-unused-vars/index.md","title":"ESLint no-unused-vars: _ ignore prefix","description":"ESLints no-unused-vars is more flexible than TypeScript noUnusedLocals and noUnusedParameters. Here is how to make respect the TypeScript default of ignoring variables prefixed with _","date":"2024-02-18T00:00:00.000Z","tags":[{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."},{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."},{"inline":false,"label":"ESLint","permalink":"/tags/eslint","description":"The ESLint linter."}],"readingTime":5.225,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"typescript-eslint-no-unused-vars","title":"ESLint no-unused-vars: _ ignore prefix","authors":"johnnyreilly","tags":["typescript","javascript","eslint"],"image":"./title-image.png","description":"ESLints no-unused-vars is more flexible than TypeScript noUnusedLocals and noUnusedParameters. Here is how to make respect the TypeScript default of ignoring variables prefixed with _","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Multiline strings in .env files","permalink":"/multiline-strings-dot-env-files"},"nextItem":{"title":"Using Bun in Azure Pipelines","permalink":"/using-bun-in-azure-pipelines"}},"content":"I\'m a longtime user of TypeScripts [`noUnusedLocals`](https://www.typescriptlang.org/tsconfig#noUnusedLocals) and [`noUnusedParameters`](https://www.typescriptlang.org/tsconfig#noUnusedParameters) settings. I like to avoid leaving unused variables in my code; these compiler options help me do that.\\n\\nI use ESLint alongside TypeScript. The [`no-unused-vars`](https://eslint.org/docs/latest/rules/no-unused-vars) rule provides similar functionality to TypeScripts `noUnusedLocals` and `noUnusedParameters` settings, but has more power and more flexibility. For instance, `no-unused-vars` can catch unused error variables; TypeScript\'s `noUnusedLocals` and `noUnusedParameters` cannot.\\n\\nOne thing that I missed when switching to the ESLint option is that, with `noUnusedLocals` and `noUnusedParameters`, you can simply ignore unused variables by prefixing a variable with the `_` character. That\'s right, sometimes I want to declare a variable that I know I\'m not going to use, and I want to do that without getting shouted at by the linter.\\n\\nIt turns out you can get ESLint to respect the TypeScript default of ignoring variables prefixed with `_`; [it\'s just not the default configuration for `no-unused-vars`](https://github.com/typescript-eslint/typescript-eslint/issues/8464#issuecomment-1943325441). But with a little configuration we can have it. This post is a quick guide to how to implement that configuration.\\n\\n![title image reading \\"From TypeScript noUnusedLocals and noUnusedParameters to ESLint no-unused-vars (with `_` prefix)\\" with the ESLint and TypeScript logo](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## When would you want to ignore unused variables?\\n\\nThere are various scenarios when I want to ignore unused variables. Here are a few:\\n\\n- I\'m writing a function but I\'m not using all of the parameters yet. I plan to use them later, but I want to declare them now so I don\'t forget about them.\\n- An ignored variable can be a form of documentation. It can be a way to say \\"I know this is here, but I\'m not using it intentionally\\".\\n\\nNot everyone will agree with these reasons, but they work for me in certain situations.\\n\\nJust to offer the counterpoint, let me quote [Brad Zacher](https://github.com/bradzacher) who works on TypeScript ESLint:\\n\\n> On the one hand it is nice to have a short-hand to ignore things.\\n>\\n> On the other hand it is terrible having a short-hand to ignore things - it\'s a single character that\'s easy to miss in code review - so it\'s easy to sneak into a commit.\\n>\\n> For example I recently reviewed a PR where someone innocently did something like\\n>\\n> ```javascript\\n> import { promisify } from \'node:util\';\\n> import { exec as _exec } from \'node:child_process\';\\n>\\n> const exec = promisify(_exec);\\n> ```\\n>\\n> And they didn\'t realise that doing this would define a variable that would never get flagged if it\'s unused! Really bad!\\n\\nBrad has a valid point, but let\'s say you\'ve decided to `--ignore-pattern \'brad\'`, and want to make use of the `_` prefix anyway. (Sorry Brad!) Here\'s how you can do it.\\n\\n## The TypeScript settings\\n\\nI mentioned that I like to use the TypeScript `noUnusedLocals` and `noUnusedParameters` settings. Here\'s how they would be configured in a `tsconfig.json`:\\n\\n```json\\n{\\n  \\"compilerOptions\\": {\\n    \\"noUnusedLocals\\": true,\\n    \\"noUnusedParameters\\": true\\n  }\\n}\\n```\\n\\nGiven we\'re moving to ESLint so we\'ll explicitly turn these off in our `tsconfig.json` so we can use ESLint to do the same job:\\n\\n```json\\n{\\n  \\"compilerOptions\\": {\\n    \\"noUnusedLocals\\": false,\\n    \\"noUnusedParameters\\": false\\n  }\\n}\\n```\\n\\n## The ESLint settings\\n\\nWith those off in TypeScript, we can now configure ESLint to respect the `_` prefix. Here\'s how you can do that in your `.eslintrc.json`:\\n\\n```json\\n{\\n  \\"rules\\": {\\n    \\"@typescript-eslint/no-unused-vars\\": [\\n      \\"error\\",\\n      {\\n        \\"args\\": \\"all\\",\\n        \\"argsIgnorePattern\\": \\"^_\\",\\n        \\"caughtErrors\\": \\"all\\",\\n        \\"caughtErrorsIgnorePattern\\": \\"^_\\",\\n        \\"destructuredArrayIgnorePattern\\": \\"^_\\",\\n        \\"varsIgnorePattern\\": \\"^_\\",\\n        \\"ignoreRestSiblings\\": true\\n      }\\n    ]\\n  }\\n}\\n```\\n\\nThe `argsIgnorePattern`, `caughtErrorsIgnorePattern`, `destructuredArrayIgnorePattern`, and `varsIgnorePattern` settings are the ones that respect the `_` prefix. You have to set them all to `^_` to make it work. `^_` is a regular expression that matches any string that starts with an underscore. So if you actually had a different convention for ignoring variables, you could change this to match your convention.\\n\\nIncidentally, you have to explicitly set `args` to `\\"all\\"` and `caughtErrors` to `\\"all\\"` to make the `argsIgnorePattern`/`caughtErrorsIgnorePattern` settings work. If you don\'t, the settings are ignored.\\n\\nThere\'s an `ignoreRestSiblings` setting specified above that we\'ll get to in a minute. First of all, let\'s see how the linting we\'ve activated works in practice. Here\'s some code that demonstrates the settings in action:\\n\\n```ts\\nexport function demoTheProblems(\\n  unusedAndReportedArg: boolean,\\n  _unusedButIgnoredArg: boolean, // argsIgnorePattern\\n  someArray: string[],\\n) {\\n  try {\\n    const unusedAndReportedVar = true;\\n    const _unusedAndButIgnoredVar = false; // varsIgnorePattern\\n\\n    const [\\n      unusedAndReportedDestructuredArray,\\n      _unusedButIgnoredDestructuredArray, // destructuredArrayIgnorePattern\\n    ] = someArray;\\n    // caughtErrors\\n  } catch (unusedAndReportedErr) {\\n    // ...\\n  }\\n  try {\\n    // caughtErrorsIgnorePattern\\n  } catch (_unusedButIgnoredErr) {\\n    // ...\\n  }\\n}\\n```\\n\\nIn this code, the `unusedAndReportedArg`, `unusedAndReportedVar`, `unusedAndReportedDestructuredArray`, and `unusedAndReportedErr` variables are all reported as unused. ESLint considers them errors and shouts about them.\\n\\nBy contrast, the `_unusedButIgnoredArg`, `_unusedAndButIgnoredVar`, `_unusedButIgnoredDestructuredArray`, and `_unusedButIgnoredErr` variables are all ignored, because they are prefixed with an underscore. ESLint notices them but lets them past.\\n\\nIf we run ESLint on this code, we get the following output:\\n\\n```bash\\n   2:3   error  \'unusedAndReportedArg\' is defined but never used. Allowed unused args must match /^_/u                                                             @typescript-eslint/no-unused-vars\\n   7:11  error  \'unusedAndReportedVar\' is assigned a value but never used. Allowed unused vars must match /^_/u                                                    @typescript-eslint/no-unused-vars\\n  11:7   error  \'unusedAndReportedDestructuredArray\' is assigned a value but never used. Allowed unused elements of array destructuring patterns must match /^_/u  @typescript-eslint/no-unused-vars\\n  15:12  error  \'unusedAndReportedErr\' is defined but never used. Allowed unused args must match /^_/u                                                             @typescript-eslint/no-unused-vars\\n```\\n\\nPerfect! This is exactly what we wanted. You can see this in action in the [playground](https://typescript-eslint.io/play/#ts=5.3.3&fileType=.tsx&code=KYDwDg9gTgLgBAMwK4DsDGMCWEVwCbAC2EAKgBbAAKUEARgDZEDOAFALABQccqSTweAIIo8AJWCRYAwVADmALji0IERgEMUAGk7cA%2Br354AQkhgBJWSmjS5i5auAbNcAPQu4auUwtWoVNTAwwFAoOnBMEITAMlBqAJ6KTDBQmCiyANoAupwAlHAA3mHJcQVh3Gg4STwofNIi4pJBeABqnnAAvHDJSMAA3GVwFShV%2BjWGwsamPtYtbZ0IavT8va7uAG6e3pbWlAFBIZwDQ1XpA9wGdWIS0E0AIsBJUEgYSH5CULFx2lzcv6O1k3M2ze90ezxgrxsn1W%2BAe3Rebxi8Wmfl2gWCoR%2B3EyHXCkWiH3i-Q4YQAvoMAmgyHAWBchPVrlI8ABRD55fIwtBqJCyMgwVk0KBMAZuOAAOglZKKUBKhR%2B5K5MCpNP%2BhhMQN8AgF7M53N5-I%2B0C2mrR%2B0xvxhErFUo4pM4QA&eslintrc=N4KABGBEBOCuA2BTAzpAXGUEKQAIBcBPABxQGNoBLY-AWhXkoDt8B6Jge1tidmUQAmtAG4BDaKgwBtcNhyJo0DtEgAaWXKxzskcQHNJUUfHhqN23dAMBJPZ2iIACqPz4FTdFAB6AfTPadMlFYPQALfABRRWVDXRN-AKggkPCopQlbeycXN2gPDEhfBIDIARR8ODJ8WAcBAEFFUUJM5WzXd09Cv3VEqDEMu1bndrzOop7EyEpBhwAlcoBlSgAjRiYDTwrYRHMIAF9zAF1ZA72gA&tsconfig=N4KABGBEDGD2C2AHAlgGwKYCcDyiAuysAdgM6QBcYoEEkRsAqkQK4noAmAMrNAIaplKAM35sANOBp1GLNuwAKvTL3jo8WQWBED0kgL4g9QA&tokens=false).\\n\\n### The `ignoreRestSiblings` setting\\n\\nThe [`ignoreRestSiblings`](https://eslint.org/docs/latest/rules/no-unused-vars#ignorerestsiblings) setting is also useful. You may find the need to use the rest operator in a destructuring assignment to omit properties from an object and hold onto the rest. Here\'s an example:\\n\\n```ts\\nconst { formattedDate, date, ...totals } = payload;\\n```\\n\\nIn this case I don\'t want to use `formattedDate` or `date` but I do want to use `totals`. I can use the `ignoreRestSiblings` setting to ignore the unused variables without even needing a `_` prefix or similar. So I do.\\n\\n## Conclusion\\n\\nI hope this post has been helpful. I\'ve shown you how to configure ESLint to respect the TypeScript default of ignoring variables prefixed with `_`.\\n\\nMany thanks to Brad Zacher for his input on this post. You can read our discussion on the TypeScript ESLint GitHub repo [here](https://github.com/typescript-eslint/typescript-eslint/issues/8464)."},{"id":"using-bun-in-azure-pipelines","metadata":{"permalink":"/using-bun-in-azure-pipelines","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2024-02-04-using-bun-in-azure-pipelines/index.md","source":"@site/blog/2024-02-04-using-bun-in-azure-pipelines/index.md","title":"Using Bun in Azure Pipelines","description":"Bun is a fast JavaScript runtime which can be used to speed up the TypeScript / JavaScript you have. This post shows you how to use it in Azure Pipelines.","date":"2024-02-04T00:00:00.000Z","tags":[{"inline":false,"label":"Bun","permalink":"/tags/bun","description":"The Bun JavaScript runtime."},{"inline":false,"label":"Azure Pipelines","permalink":"/tags/azure-pipelines","description":"The Azure Pipelines CI / CD service."},{"inline":false,"label":"Azure DevOps","permalink":"/tags/azure-devops","description":"The Azure DevOps suite of tools."}],"readingTime":1.68,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"using-bun-in-azure-pipelines","title":"Using Bun in Azure Pipelines","authors":"johnnyreilly","tags":["bun","azure pipelines","azure devops"],"image":"./title-image.png","description":"Bun is a fast JavaScript runtime which can be used to speed up the TypeScript / JavaScript you have. This post shows you how to use it in Azure Pipelines.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"ESLint no-unused-vars: _ ignore prefix","permalink":"/typescript-eslint-no-unused-vars"},"nextItem":{"title":"Bicep lint with Azure Pipelines and GitHub Actions","permalink":"/bicep-lint-azure-pipelines-github-actions"}},"content":"I\'m a keen user of [Bun](https://bun.sh/). Bun is a fast TypeScript / JavaScript runtime which can be used to speed up the TypeScript / JavaScript you have. It\'s a drop-in replacement for Node.js, and it\'s compatible with the vast majority of the Node.js ecosystem. (There are still rough edges that have issues.) In this post we\'ll look at how to use it in Azure Pipelines.\\n\\n![title image reading \\"Using Bun in Azure Pipelines\\" with the Bun and Azure Pipelines logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## How to install Bun\\n\\nI use Azure Pipelines for much of my day to day work. However, my OSS work is (unsurprisingly) all GitHub oriented. Using Bun in GitHub Actions is straightforward; you just make use of the [Setup Bun GitHub Action](https://github.com/marketplace/actions/setup-bun) to install Bun and you\'re off to the races. There isn\'t an equivalent for Azure Pipelines **but that doesn\'t matter**.\\n\\nIt turns out there\'s a great variety of [ways to install Bun](https://bun.sh/docs/installation). However the simplest of the lot is to install it via npm, like so:\\n\\n```bash\\nnpm install -g bun\\n```\\n\\nThis installs the `bun` command globally from the [`bun` package](https://www.npmjs.com/package/bun). You can then use it to run your TypeScript / JavaScript. This is the approach we\'ll use in Azure Pipelines.\\n\\n## Using Bun in Azure Pipelines\\n\\nSince there\'s already a [dedicated Azure Pipelines task for npm](https://learn.microsoft.com/en-us/azure/devops/pipelines/tasks/reference/npm-v1?view=azure-pipelines), we can use that to install Bun. Here\'s an example of how to do that:\\n\\n```yml\\n- task: Npm@1\\n  displayName: setup bun\\n  inputs:\\n    command: \'custom\'\\n    customCommand: \'install -g bun\'\\n    verbose: true\\n```\\n\\nNow we\'re able to use Bun in our Azure Pipelines. Here\'s an example of how to use it to install dependencies and run a build:\\n\\n```yaml\\n- bash: bun install\\n  displayName: \'install\'\\n\\n- bash: bun run build\\n  displayName: \'build\'\\n```\\n\\n## Summary\\n\\nWe\'re able to use Bun in Azure Pipelines by installing it via npm and then using it as we would Node.js. This is a great way to speed up your TypeScript / JavaScript builds. I hope you find it useful!"},{"id":"bicep-lint-azure-pipelines-github-actions","metadata":{"permalink":"/bicep-lint-azure-pipelines-github-actions","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2024-01-30-bicep-lint-azure-pipelines-github-actions/index.md","source":"@site/blog/2024-01-30-bicep-lint-azure-pipelines-github-actions/index.md","title":"Bicep lint with Azure Pipelines and GitHub Actions","description":"Bicep lint allows you to lint bicep files to ensure they conform to best practices. In this post we\'ll look at how to run bicep lint in Azure Pipelines and GitHub Actions.","date":"2024-01-30T00:00:00.000Z","tags":[{"inline":false,"label":"Bicep","permalink":"/tags/bicep","description":"The Bicep language for Azure Resource Manager templates."},{"inline":false,"label":"GitHub Actions","permalink":"/tags/github-actions","description":"The GitHub Actions CI / CD service."},{"inline":false,"label":"Azure Pipelines","permalink":"/tags/azure-pipelines","description":"The Azure Pipelines CI / CD service."},{"inline":false,"label":"Azure DevOps","permalink":"/tags/azure-devops","description":"The Azure DevOps suite of tools."}],"readingTime":9.39,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"bicep-lint-azure-pipelines-github-actions","title":"Bicep lint with Azure Pipelines and GitHub Actions","authors":"johnnyreilly","tags":["bicep","github actions","azure pipelines","azure devops"],"image":"./title-image.png","description":"Bicep lint allows you to lint bicep files to ensure they conform to best practices. In this post we\'ll look at how to run bicep lint in Azure Pipelines and GitHub Actions.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Using Bun in Azure Pipelines","permalink":"/using-bun-in-azure-pipelines"},"nextItem":{"title":"Schemar: a GitHub Action to validate structured data","permalink":"/schemar-github-action-to-validate-structured-data"}},"content":"Bicep has had linting [since version 0.4.1](https://github.com/Azure/bicep/releases/tag/v0.4.1). It\'s a great way to ensure that your bicep files conform to best practices. Interestingly, when the linting feature first shipped, there wasn\'t an explicit lint command as part of the CLI. Instead, you had to run `bicep build` and it would run the linter as part of the build process. This was a little confusing as it was not obvious that the linter was running.\\n\\nAs of [version 0.21.1](https://github.com/Azure/bicep/releases/tag/v0.21.1) there is a dedicated [`bicep lint`](https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/linter) command. This is a nice step forwards; it allows you to explicitly lint your your code, rather than have it happen as a side effect of build. And it is useful if you want to run the linter as part of a CI/CD pipeline. What\'s more the `bicep lint` command is now available in the Azure CLI as well. You can run [`az bicep lint`](https://docs.microsoft.com/en-us/cli/azure/bicep?view=azure-cli-latest#az-bicep-lint) to lint your bicep files.\\n\\nIn this post we\'ll look at how to run lint Bicep in Azure Pipelines and GitHub Actions, and surface the output in the UI.\\n\\n![title image reading \\"Bicep lint with Azure Pipelines and GitHub Actions\\" with the Bicep logo](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## The general approach\\n\\nThe general approach is the same for both Azure Pipelines and GitHub Actions. One way or another, we\'ll run the Bicep `lint` command to lint our Bicep files and capture the output. As yet, there is no option to export the results of the lint command as a file. This may come, and [there is a discussion about it](https://github.com/Azure/bicep/issues/11960). However, there is a way to achieve our goal, which came out in discussion with [Anthony Martin](https://github.com/anthony-c-martin) of the Bicep team. We can write the output of the `lint` command to a file like so:\\n\\n```\\nbicep lint main.bicep --diagnostics-format sarif > lint.sarif\\n```\\n\\nThis will write the output of the `lint` command to a file called `lint.sarif`. This is a [SARIF](https://sarifweb.azurewebsites.net/) file. SARIF stands for Static Analysis Results Interchange Format. It\'s a standard for representing the results of static analysis tools. It\'s a JSON file, easy to parse and has integrations with GitHub Actions / Azure Pipelines. An example SARIF output is below:\\n\\n```json\\n{\\n  \\"$schema\\": \\"https://schemastore.azurewebsites.net/schemas/json/sarif-2.1.0-rtm.6.json\\",\\n  \\"version\\": \\"2.1.0\\",\\n  \\"runs\\": [\\n    {\\n      \\"tool\\": {\\n        \\"driver\\": {\\n          \\"name\\": \\"bicep\\"\\n        }\\n      },\\n      \\"results\\": [\\n        {\\n          \\"ruleId\\": \\"no-unused-vars\\",\\n          \\"message\\": {\\n            \\"text\\": \\"Variable \\\\\\"unusedVar\\\\\\" is declared but never used. [https://aka.ms/bicep/linter/no-unused-vars]\\"\\n          },\\n          \\"locations\\": [\\n            {\\n              \\"physicalLocation\\": {\\n                \\"artifactLocation\\": {\\n                  \\"uri\\": \\"file:///home/runner/work/blog.johnnyreilly.com/blog.johnnyreilly.com/./infra/main.bicep\\"\\n                },\\n                \\"region\\": {\\n                  \\"startLine\\": 19,\\n                  \\"charOffset\\": 5\\n                }\\n              }\\n            }\\n          ]\\n        }\\n      ],\\n      \\"columnKind\\": \\"utf16CodeUnits\\"\\n    }\\n  ]\\n}\\n```\\n\\nIn the example above we directly used the `bicep lint` command. An alternative approach is to use the Azure CLI like so:\\n\\n```\\naz bicep lint --file main.bicep --diagnostics-format sarif > lint.sarif\\n```\\n\\nThis is a little more verbose, but it does mean that you don\'t need to install the Bicep CLI on your build agent. You can use the Azure CLI instead; and this is already a first class citizen of Azure Pipelines and GitHub Actions, with dedicated support. (That said, there is a slight issue with this approach at the time of writing, which we\'ll come to later.)\\n\\nHowever we generate it, we can take the SARIF file and use it to surface the results of the linting process in Azure Pipelines and GitHub Actions.\\n\\n## Linting Bicep in GitHub Actions with the Azure CLI\\n\\nWe\'ll start off by looking at how to lint Bicep in GitHub Actions with the Azure CLI. But before we do that, we need something to lint. Within your `main.bicep` file you should have something like this:\\n\\n```bicep\\nvar unusedVar = 1 // unused variable\\n```\\n\\nAs it suggests, this is an unused variable. We\'re just using this to demonstrate the linting process. And alongside that, we want a [`bicepconfig.json`](https://aka.ms/bicep/config) file that looks like this:\\n\\n```json\\n{\\n  \\"analyzers\\": {\\n    \\"core\\": {\\n      \\"rules\\": {\\n        \\"no-unused-vars\\": {\\n          \\"level\\": \\"warning\\"\\n        }\\n      }\\n    }\\n  }\\n}\\n```\\n\\nThis explicitly enables the [`no-unused-vars`](https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/linter-rule-no-unused-variables) rule, and sets it to `warning` level. This means that the linter will warn us about unused variables, but it won\'t fail the build. We could set it to `error` level, and then the build would fail if there were any unused variables. We\'ll come back to this later.\\n\\nIn a GitHub workflow in your repository you should have steps like these:\\n\\n```yaml\\n- name: Lint Bicep\\n  uses: azure/CLI@v2\\n  with:\\n    inlineScript: |\\n      az bicep install\\n      az bicep lint --file ./infra/main.bicep --diagnostics-format sarif > bicep.sarif\\n\\n- name: Upload SARIF\\n  if: (success() || failure())\\n  uses: github/codeql-action/upload-sarif@v3\\n  with:\\n    category: bicep\\n    sarif_file: bicep.sarif\\n```\\n\\nThe above:\\n\\n- Installs Bicep to the Azure CLI\\n- Runs the `lint` command and writes the results to a file called `bicep.sarif`\\n- Uploads the SARIF file to GitHub\\n\\nThe `upload-sarif` action is provided by GitHub. [It allows surfacing the results of static analysis tools in GitHub](https://docs.github.com/en/code-security/code-scanning/integrating-with-code-scanning/uploading-a-sarif-file-to-github). Doing this will show the results of the linting process in the GitHub UI, and it will also show them in the GitHub Security / Code Scanning UI, like so:\\n\\n![screenshot of the no-unused-vars rule in GitHub UI](screenshot-github-actions-no-unused-vars.webp)\\n\\n## Linting Bicep in GitHub Actions with the Bicep CLI\\n\\nWe can also lint Bicep in GitHub Actions with the Bicep CLI. At the time of writing, there\'s a reason you might want to favour this approach over the Azure CLI approach. I won\'t drill into it in depth, but at present if `az bicep lint` fails / returns a non-0 exit code then no output is produced. We could make this happen by updating the `bicepconfig.json` to dial up the `no-unused-vars` rule to `error` level, like so:\\n\\n```json\\n{\\n  \\"analyzers\\": {\\n    \\"core\\": {\\n      \\"rules\\": {\\n        \\"no-unused-vars\\": {\\n          \\"level\\": \\"error\\"\\n        }\\n      }\\n    }\\n  }\\n}\\n```\\n\\nNow an unused variable will cause the build to fail. But the output of the `lint` command will not be surfaced in the GitHub UI. This is because the Azure CLI is not surfacing the output of the `lint` command when it fails.\\n\\nThe issue with the Azure CLI will hopefully be remedied; [you can track that here](https://github.com/Azure/azure-cli/issues/28259). For now you can use the Bicep CLI directly, where this isn\'t an issue. We\'ll do that now.\\n\\nI\'m basing this approach on Anthony Martin\'s example of [Bicep linting with GitHub Actions](https://github.com/anthony-c-martin/bicep-on-k8s/blob/e6dfa61fe7eae6fd6b148670f940041f3e294b20/.github/workflows/ci.yml#L36-L50):\\n\\n```yml\\n- name: Setup Bicep\\n  uses: anthony-c-martin/setup-bicep@v0.3\\n\\n- name: Lint Bicep\\n  run: |\\n    bicep lint ./infra/main.bicep --diagnostics-format sarif > bicep.sarif\\n\\n- name: Upload SARIF\\n  if: always()\\n  uses: github/codeql-action/upload-sarif@v3\\n  with:\\n    category: bicep\\n    sarif_file: bicep.sarif\\n```\\n\\nThe above does the same as the Azure CLI approach, but it uses the Bicep CLI directly. The [`setup-bicep` action](https://github.com/marketplace/actions/setup-bicep) is provided by Anthony Martin and installs the Bicep CLI on your build agent.\\n\\nAs I say, right now you may want to favour this approach over the Azure CLI approach to cover both surfacing warnings and errors. But hopefully that will change soon.\\n\\n## Linting Bicep in Azure Pipelines with the Azure CLI\\n\\nWe\'ve now seen how to lint Bicep in GitHub Actions with both the Azure CLI and the Bicep CLI. We can do the same in Azure Pipelines; but only the Azure CLI approach (as I\'m not sure if there\'s a `setup-bicep` equivalent for Azure Pipelines).\\n\\nHow you want to surface the results in Azure Pipelines is up to you. You could surface into the \\"Scans\\" portion of Azure Pipelines UI or into \\"Tests\\". I\'ll show you how to do both.\\n\\n### Surface the results in Scans\\n\\nTo surface the results in the scans part of Azure Pipelines you need to publish the SARIF file as a build artifact. You can do that like so:\\n\\n```yml\\njobs:\\n  - job: LintInfra\\n    displayName: Lint Infra\\n    dependsOn: []\\n    pool:\\n      vmImage: \'ubuntu-latest\'\\n    steps:\\n      - task: AzureCLI@2\\n        displayName: Lint main.bicep\\n        inputs:\\n          azureSubscription: service-connection-with-access-to-registry # you may not need this\\n          scriptType: bash\\n          scriptLocation: inlineScript\\n          inlineScript: |\\n            az bicep install\\n            az bicep lint --file infra/main.bicep --diagnostics-format sarif > $(System.DefaultWorkingDirectory)/bicep.sarif\\n\\n      - task: PublishBuildArtifacts@1\\n        condition: always()\\n        inputs:\\n          pathToPublish: $(System.DefaultWorkingDirectory)/bicep.sarif\\n          artifactName: CodeAnalysisLogs # required to show up in the scans tab\\n```\\n\\nThe above is essentially the same as the GitHub Actions example, but it uses the Azure CLI instead of the Bicep CLI. The `PublishBuildArtifacts` task is provided by Azure Pipelines. It allows you to publish build artifacts, which will show up in the Scans part of Azure Pipelines. You can see the results of the linting process in Scans, like so:\\n\\n![screenshot of the no-unused-vars rule in Azure Pipelines scans](screenshot-azure-pipelines-scans-no-unused-vars.webp)\\n\\nYou\'ll notice that the path above has a `file:///home/vsts/work/1/s` prefix before the bicep path report of `infra/main.bicep`. This is unfortunate and breaks \\"clickability\\". You cannot click on this and be taken to the file. It\'s possible to remedy this behaviour by doing a little find and replace magic on the SARIF file. You don\'t need to do this, but it does add to the developer experience.\\n\\nBelow is the same portion of the Azure Pipelines yaml file but with some additional bash that will use `sed` to replace all instances of the `file:///home/vsts/work/1/s` prefix with an empty string:\\n\\n```yml\\njobs:\\n  - job: LintInfra\\n    displayName: Lint Infra\\n    dependsOn: []\\n    pool:\\n      vmImage: \'ubuntu-latest\'\\n    steps:\\n      - task: AzureCLI@2\\n        displayName: Lint main.bicep\\n        inputs:\\n          azureSubscription: service-connection-with-access-to-registry # you may not need this\\n          scriptType: bash\\n          scriptLocation: inlineScript\\n          inlineScript: |\\n            az bicep install\\n            az bicep lint --file infra/main.bicep --diagnostics-format sarif > $(System.DefaultWorkingDirectory)/bicep.sarif\\n\\n            STRING_TO_REPLACE=\'file://$(Build.SourcesDirectory)/\'\\n            echo \\"##[group]Bicep linting results before $STRING_TO_REPLACE replace:\\"\\n            cat $(System.DefaultWorkingDirectory)/bicep.sarif\\n            echo \\"##[endgroup]\\"\\n\\n            sed -i \\"s|$STRING_TO_REPLACE||g\\" $(System.DefaultWorkingDirectory)/bicep.sarif\\n\\n            echo \\"##[group]Bicep linting results after $STRING_TO_REPLACE replace:\\"\\n            cat $(System.DefaultWorkingDirectory)/bicep.sarif\\n            echo \\"##[endgroup]\\"\\n\\n      - task: PublishBuildArtifacts@1\\n        condition: always()\\n        inputs:\\n          pathToPublish: $(System.DefaultWorkingDirectory)/bicep.sarif\\n          artifactName: CodeAnalysisLogs # required to show up in the scans tab\\n```\\n\\nAnd hey presto! Clickability restored.\\n\\n### Surface the results in Tests\\n\\nYou can also surface the results in the tests part of Azure Pipelines. To do that we\'re going to borrow a [suggestion from Anthony Martin](https://github.com/Azure/bicep/issues/11960#issuecomment-1737226501) and use the [`sarif-junit`](https://www.npmjs.com/package/sarif-junit) package. This package allows us to convert a SARIF file to a JUnit file. JUnit is a standard for representing test results and can be used with the `PublishTestResults` task.\\n\\n```yml\\njobs:\\n  - job: LintInfra\\n    displayName: Lint Infra\\n    dependsOn: []\\n    pool:\\n      vmImage: \'ubuntu-latest\'\\n    steps:\\n      - task: NodeTool@0\\n        displayName: \'Install Node.js\'\\n        inputs:\\n          versionSpec: \'18.x\'\\n\\n      - task: AzureCLI@2\\n        displayName: Lint main.bicep\\n        inputs:\\n          azureSubscription: service-connection-with-access-to-registry # you may not need this\\n          scriptType: bash\\n          scriptLocation: inlineScript\\n          inlineScript: |\\n            az bicep install\\n            az bicep lint --file infra/main.bicep --diagnostics-format sarif > $(System.DefaultWorkingDirectory)/bicep.sarif\\n            npx -y sarif-junit -i $(System.DefaultWorkingDirectory)/bicep.sarif -o $(System.DefaultWorkingDirectory)/bicep.xml\\n\\n      - task: PublishTestResults@2\\n        condition: always()\\n        inputs:\\n          testResultsFormat: \'JUnit\'\\n          testResultsFiles: \'$(System.DefaultWorkingDirectory)/bicep.xml\'\\n```\\n\\nSo the above is the same approach again but requires Node.js to be installed, and the results end up in the Tests part of Azure Pipelines. You can see the results of the linting process in Tests, like so:\\n\\n![screenshot of the no-unused-vars rule in Azure Pipelines tests](screenshot-azure-pipelines-tests-no-unused-vars.webp)\\n\\n## Summary\\n\\nThat\'s it! We\'ve seen how to lint Bicep in Azure Pipelines and GitHub Actions. We\'ve seen how to surface the results in the scans and tests parts of Azure Pipelines and in GitHub. We\'ve seen how to do it with the Azure CLI and the Bicep CLI. And we\'ve seen how to do it with warnings and errors. Hopefully this will help you to ensure that your Bicep files conform to best practices.\\n\\nMany thanks to Anthony Martin for his help, which laid the groundwork for much of what we explored in this post."},{"id":"schemar-github-action-to-validate-structured-data","metadata":{"permalink":"/schemar-github-action-to-validate-structured-data","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2024-01-02-schemar-github-action-to-validate-structured-data/index.md","source":"@site/blog/2024-01-02-schemar-github-action-to-validate-structured-data/index.md","title":"Schemar: a GitHub Action to validate structured data","description":"This post demonstrates how to use Schemar to validate structured data using a GitHub Action.","date":"2024-01-02T00:00:00.000Z","tags":[{"inline":false,"label":"SEO","permalink":"/tags/seo","description":"Search Engine Optimization."}],"readingTime":6.295,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"schemar-github-action-to-validate-structured-data","title":"Schemar: a GitHub Action to validate structured data","authors":"johnnyreilly","tags":["seo"],"image":"./title-image.png","description":"This post demonstrates how to use Schemar to validate structured data using a GitHub Action.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Bicep lint with Azure Pipelines and GitHub Actions","permalink":"/bicep-lint-azure-pipelines-github-actions"},"nextItem":{"title":"Snapshot log tests in .NET","permalink":"/snapshot-log-tests-dotnet"}},"content":"Of late, I\'ve found myself getting more and more into structured data. Structured data is a way of adding machine-readable information to web pages. To entertain myself, I liken it to static typing for websites. I\'ve written about structured data before, but in this post I want to focus on how to validate structured data.\\n\\nSpecifically, how can we validate structured data in the context of a GitHub workflow? I\'ve created a GitHub Action called [Schemar](https://github.com/marketplace/actions/schemar-ci-action) that facilitates just that. In this post we\'ll see how to use it.\\n\\n![title image reading \\"Schemar: a GitHub Action to validate structured data\\" with the GitHub Action logo](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\nIf you\'d like to read more about structured data, you might like to read these posts:\\n\\n- [Structured data, SEO and React](../2021-10-15-structured-data-seo-and-react/index.md)\\n- [How we fixed my SEO](../2023-11-28-how-we-fixed-my-seo/index.md)\\n- [Docusaurus blogs: adding breadcrumb structured data](../2023-02-05-docusaurus-blogs-adding-breadcrumb-structured-data/index.md)\\n\\n## What is Schemar?\\n\\nSchemar is a GitHub Action that validates structured data. It\'s a wrapper around the [Schema Markup Validator](https://validator.schema.org/) tool.\\n\\nIf you haven\'t heard of Schema.orgs validator; it originally started at Google as the Structured Data Testing Tool but was [repurposed and gifted to the community](https://developers.google.com/search/blog/2020/12/structured-data-testing-tool-update).\\n\\nThat tool is a website; Schemar is a wrapper around the tool that makes it easy to validate structured data in the context of a GitHub workflow. Let\'s imagine it\'s very important to you that your structured data is both present and valid. You could use Schemar to validate your structured data as part of your CI/CD pipeline.\\n\\nImagine Schemar to be the structured data equivalent of the [`lighthouse-ci-action`](https://github.com/treosh/lighthouse-ci-action) GitHub Action.\\n\\n## Using Schemar\\n\\nI\'m going to take my blog (that\'s what you\'re reading right now BTW) and use Schemar to validate the structured data on it. I already have a GitHub Action that builds and deploys my blog to a staging environment in Azure Static Web Apps and [validates it with Lighthouse](../2022-03-20-lighthouse-meet-github-actions/index.md). So I\'m going to add Schemar to that.\\n\\nBut before we do that, let\'s look at simple usage of Schemar. If you were to add a `.github/workflows/schemar.yml` file to your repo with the following contents:\\n\\n```yaml\\njobs:\\n  release:\\n    runs-on: ubuntu-latest\\n    steps:\\n      - uses: johnnyreilly/schemar@v0.1.1\\n        with:\\n          urls: https://johnnyreilly.com\\n\\nname: Validate structured data\\n\\non:\\n  pull_request: ~\\n  push:\\n    branches:\\n      - main\\n```\\n\\nThen you\'d have a GitHub workflow that would validate the structured data on `https://johnnyreilly.com` and fail if it wasn\'t valid.\\n\\nThe `urls` input of Schemar is a list of URLs to validate. In this case, we\'re just validating only one. The results look like this:\\n\\n> Validating https://johnnyreilly.com for structured data...\\n>\\n> https://johnnyreilly.com has structured data of these types:\\n>\\n> - Organization / Brand\\n> - WebSite\\n> - Blog\\n>\\n> For more details see https://validator.schema.org/#url=https%3A%2F%2Fjohnnyreilly.com\\n\\nWe can see that the home page of my blog has structured data of the types `Organization / Brand`, `WebSite` and `Blog`. And we can even click into the Schema Markup Validator to see the details.\\n\\nIf at some point I were to omit or break the structured data on my blog, then Schemar would fail the build. This is a great way to ensure that your structured data is always present and valid.\\n\\nWe\'re going to see what usage looks like in a minute, as we dive into a more sophisticated example.\\n\\n## Surfacing Schemar results in your pull requests\\n\\nNow that we\'ve seen a basic example, let\'s see what it looks like to use Schemar in a more sophisticated way. We\'re going to add Schemar to run against my blogs pull request previews, in the same way we\'re already running Lighthouse against them.\\n\\n### Adding Schemar to the GitHub Action\\n\\nI won\'t reiterate the whole GitHub workflow that spins up a preview environment here, but I\'ll show the key parts. You can see the whole thing in the [`build-and-deploy-static-web-app.yml` of the blog repo](https://github.com/johnnyreilly/blog.johnnyreilly.com/blob/main/.github/workflows/build-and-deploy-static-web-app.yml). You\'ll note I\'m using Azure Static Web Apps to host my blog - but any web platform will do.\\n\\nHere is the key part of the GitHub workflow:\\n\\n```yaml\\nstructured_data_report_job:\\n  name: Structured data report \uD83D\uDCDD\\n  needs: build_and_deploy_swa_job\\n  if: github.event_name == \'pull_request\' && github.event.action != \'closed\'\\n  runs-on: ubuntu-latest\\n  steps:\\n    - uses: actions/checkout@v4\\n\\n    - name: Wait for preview ${{ needs.build_and_deploy_swa_job.outputs.preview-url }} \u231A\\n      id: static_web_app_wait_for_preview\\n      uses: nev7n/wait_for_response@v1\\n      with:\\n        url: \'${{ needs.build_and_deploy_swa_job.outputs.preview-url }}\'\\n        responseCode: 200\\n        timeout: 600000\\n        interval: 1000\\n\\n    - name: Audit URLs for structured data \uD83E\uDDD0\\n      id: structured_data_audit\\n      uses: johnnyreilly/schemar@v0.1.1\\n      with:\\n        urls: |\\n          ${{ needs.build_and_deploy_swa_job.outputs.preview-url }}\\n          ${{ needs.build_and_deploy_swa_job.outputs.preview-url }}/about\\n          ${{ needs.build_and_deploy_swa_job.outputs.preview-url }}/blog\\n          ${{ needs.build_and_deploy_swa_job.outputs.preview-url }}/definitely-typed-the-movie\\n\\n    - name: Format structured data results\\n      id: format_structured_data_results\\n      if: always()\\n      uses: actions/github-script@v7\\n      with:\\n        script: |\\n          const structuredDataCommentMaker = (await import(\'${{ github.workspace }}/.github/workflows/structuredDataCommentMaker.mjs\')).default;\\n          const results = ${{ steps.structured_data_audit.outputs.results }};\\n          core.setOutput(\\"comment\\", structuredDataCommentMaker(\'${{ needs.build_and_deploy_swa_job.outputs.preview-url }}\', results));\\n\\n    - name: Add structured data results as comment \u270D\uFE0F\\n      id: structured_data_comment_to_pr\\n      if: always()\\n      uses: marocchino/sticky-pull-request-comment@v2\\n      with:\\n        number: ${{ github.event.pull_request.number }}\\n        header: structured_data\\n        message: ${{ steps.format_structured_data_results.outputs.comment }}\\n```\\n\\nAlong with the following `structuredDataCommentMaker.mjs` script:\\n\\n```js title=\\"structuredDataCommentMaker.mjs\\"\\n// @ts-check\\n\\n/**\\n * @typedef {Object} Result\\n * @prop {string} url\\n * @prop {ProcessedValidationResult} processedValidationResult\\n */\\n\\n/**\\n * @typedef {Object} ProcessedValidationResult\\n * @prop {boolean} success\\n * @prop {string} resultText\\n */\\n\\n/**\\n * @param {string} baseUrl\\n * @param {Result[]} results\\n */\\nfunction createStructuredDataReport(baseUrl, results) {\\n  const comment = `### \uD83D\uDCDD Structured data report\\n\\n${results\\n  .map((result) => {\\n    const shortUrl = result.url.replace(baseUrl, \'\') || \'/\';\\n    return `#### ${\\n      result.processedValidationResult.success ? \'\uD83D\uDFE2\' : \'\uD83D\uDD34\'\\n    } [${shortUrl}](${result.url}) \\n${result.processedValidationResult.resultText}`;\\n  })\\n  .join(\'\\\\n\')}\\n`;\\n  return comment;\\n}\\n\\nexport default createStructuredDataReport;\\n```\\n\\nLet\'s break this down:\\n\\n- We\'re using the [`nev7n/wait_for_response`](https://github.com/nev7n/wait_for_response) GitHub Action to wait for the preview to be available. This is because the preview URL is not available immediately after the preview is created.\\n- We\'re running Schemar against four URLs in our pull request preview. These pages should have structured data; and if any fail then it\'s likely a sign that something has gone wrong with my sites structured data story.\\n- We then take the output of the Schemar run and format it into a comment that we can add to the pull request - to do that we use the `structuredDataCommentMaker.mjs` script.\\n- Finally, we add the comment to the pull request using the [`marocchino/sticky-pull-request-comment`](https://github.com/marocchino/sticky-pull-request-comment) GitHub Action.\\n\\n### Testing it out\\n\\nLet\'s see what this looks like in action. I\'ve created a pull request that breaks the structured data from my blog. This is what the pull request looks like:\\n\\n```diff\\n-\'@type\': \'Person\',\\n+\'@type\': \'Blarg\', // let\'s break the schema!\\n```\\n\\nThe question is, what does the pull request look like after the GitHub Action has run? Here\'s the answer:\\n\\n![screenshot of the GitHub Action failing](screenshot-failed-github-action.png)\\n\\nIt failed! And it put a comment on the PR that looks like this:\\n\\n![screenshot of the GitHub Action comment on the PR](screenshot-pull-request-failed-comment.png)\\n\\nLet\'s unbreak the structured data and see what happens:\\n\\n```diff\\n-\'@type\': \'Blarg\', // let\'s break the schema!\\n+\'@type\': \'Person\',\\n```\\n\\n![screenshot of the GitHub Action failing](screenshot-succeeded-github-action.png)\\n\\nIt succeeded! And it put a comment on the PR that looks like this:\\n\\n![screenshot of the GitHub Action comment on the PR](screenshot-pull-request-succeeded-comment.png)\\n\\nThis is great! It means that I can be confident that my structured data is always present and valid. And if it isn\'t, then I\'ll know about it. I can even click through to the Schema Markup Validator to see the details.\\n\\n## Conclusion\\n\\nMy hope is that Schemar can be used to increase the quality of structured data on the web. I\'m using it to increase the quality of structured data on my blog. I hope you\'ll find it useful too.\\n\\nI\'ve also [shared this with the good folk of Schema.org](https://github.com/schemaorg/schemaorg/discussions/3432) in the hope they\'ll find it useful too. The [source code of Schemar can be found here](https://github.com/johnnyreilly/schemar)."},{"id":"snapshot-log-tests-dotnet","metadata":{"permalink":"/snapshot-log-tests-dotnet","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-12-20-snapshot-log-tests-dotnet/index.md","source":"@site/blog/2023-12-20-snapshot-log-tests-dotnet/index.md","title":"Snapshot log tests in .NET","description":"This post demonstrates how to write high quality and low effort log assertions using snapshot testing.","date":"2023-12-20T00:00:00.000Z","tags":[{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."},{"inline":false,"label":"C#","permalink":"/tags/csharp","description":"The C# programming language."},{"inline":false,"label":"Automated Testing","permalink":"/tags/automated-testing","description":"How to perform the automation of tests."}],"readingTime":7.125,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"snapshot-log-tests-dotnet","title":"Snapshot log tests in .NET","authors":"johnnyreilly","tags":["asp.net","c#","automated testing"],"image":"./title-image.png","description":"This post demonstrates how to write high quality and low effort log assertions using snapshot testing.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Schemar: a GitHub Action to validate structured data","permalink":"/schemar-github-action-to-validate-structured-data"},"nextItem":{"title":"Overview of Bun, a JavaScript runtime","permalink":"/bun-overview"}},"content":"Writing tests is important. The easier it is to write tests, the more likely they\'ll be written. I\'ve long loved snapshot testing for this reason. Snapshot testing takes away the need to manually write verification code in your tests. Instead, you write tests that compare the output of a call to your method with JSON serialised output you\'ve generated on a previous occasion. This approach takes less time to write, less time to maintain, and the solid readability of JSON makes it more likely you\'ll pick up on bugs. It\'s so much easier to scan JSON than it is a list of assertions.\\n\\nLoving snapshot testing as I do, I want to show you how to write high quality and low effort log assertions using snapshot testing. The behaviour of logging code is really important; it\'s this that we tend to rely upon when debugging production issues. But how do you test logging code? Well, you could write a bunch of assertions that check how your logger is used. But that\'s a lot of work, it\'s not super readable and it\'s not fun. (Always remember: if it\'s not fun, you\'re doing it wrong.)\\n\\nInstead, we\'ll achieve this using snapshot testing.\\n\\n![title image reading \\"Snapshot log tests in .NET\\" with the .NET logo](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\nI\'ve written previously about [manually implementing snapshot testing with .NET](../2018-11-17-snapshot-testing-for-c/index.md). That was great, but I subsequently moved to use the excellent [Snapshooter](https://github.com/SwissLife-OSS/snapshooter) instead. In this post we\'ll use that. I\'m using XUnit as my test framework; but Snapshooter also supports MSTest and NUnit if they are your preference.\\n\\n## How to test `ILogger`?\\n\\nBefore we get into the details of how to test logging code, let\'s first consider how we might test a method that uses `ILogger`. Here\'s a simple example:\\n\\n```cs\\nusing Microsoft.Extensions.Logging;\\n\\nnamespace MyApp.Tests.Services;\\n\\npublic class GreetingService(ILogger<GreetingService> log)\\n{\\n    private readonly ILogger<GreetingService> _log = log;\\n\\n    public string GetGreeting(string name)\\n    {\\n        _log.LogInformation($\\"Greeting {{{nameof(name)}}}\\", name);\\n\\n        return $\\"Hello, {name}!\\";\\n    }\\n}\\n```\\n\\nIf we look at the class above, we can see it has a dependency on `ILogger<GreetingService>`. This is a common pattern in .NET applications. The `ILogger` interface is used to write log messages. I wouldn\'t be surprised if it\'s the most commonly used interface in .NET applications.\\n\\nIf we execute the `GetGreeting` method above, we\'ll both get a greeting returned and a log message will be written. In order that we can test this method, we need to be able to verify that the log message was written correctly. We\'re going to do that by making use of a fake logger.\\n\\n## `FakeLogger`\\n\\nAs of .NET 8, there is a `FakeLogger` that ships as part of .NET. You can read more about that here: https://devblogs.microsoft.com/dotnet/fake-it-til-you-make-it-to-production/#logging-fake\\n\\nWe\'re going to make use of the official `FakeLogger` in this post. However, I\'m mindful that not everyone is on .NET 8 yet. So, I\'m going to show you how to implement a fake logger yourself. So if you\'re on .NET 6 / .NET 7 then you can use this.\\n\\nWhichever fake logger we use, it is a simple implementation of `ILogger<T>`. It\'s a fake because it doesn\'t actually write anything to a log. Instead, it records the log messages it\'s asked to write in a list of `FakeLogRecord`. We can then use this list to verify that the log messages were written correctly.\\n\\n### `FakeLogger` for .NET 8+\\n\\nIf you\'re working with .NET 8 or later, you can use the `FakeLogger` that ships with .NET. To add this to your project, add the [`Microsoft.Extensions.Logging.Testing`](https://www.nuget.org/packages/Microsoft.Extensions.Logging.Testing) and [`Microsoft.Extensions.TimeProvider.Testing`](https://www.nuget.org/packages/Microsoft.Extensions.TimeProvider.Testing) packages to your test project:\\n\\n```bash\\ndotnet add package Microsoft.Extensions.Diagnostics.Testing\\ndotnet add package Microsoft.Extensions.TimeProvider.Testing\\n```\\n\\n#### `FakeLogger` for earlier .NET versions\\n\\nIf you\'re working with an earlier version of .NET, you can implement a fake logger yourself:\\n\\n```cs\\nusing Microsoft.Extensions.Logging;\\n\\nnamespace MyApp.Tests.TestUtilities;\\n\\npublic record FakeLogRecord(LogLevel Level, string Message, Exception? Exception);\\n\\npublic class FakeLogger<T> : ILogger<T>\\n{\\n    public IReadOnlyList<FakeLogRecord> GetSnapshot() => _records;\\n    readonly List<FakeLogRecord> _records = [];\\n\\n    public IDisposable? BeginScope<TState>(TState state) where TState : notnull => NullScope.Instance;\\n\\n    public bool IsEnabled(LogLevel logLevel) => true;\\n\\n    public void Log<TState>(LogLevel logLevel, EventId eventId, TState state, Exception? exception, Func<TState, Exception?, string> formatter) =>\\n        _records.Add(new FakeLogRecord(logLevel, formatter(state, exception), exception));\\n\\n    /// <summary>\\n    /// Reference: https://github.com/aspnet/Logging/blob/master/src/Microsoft.Extensions.Logging.Abstractions/Internal/NullScope.cs\\n    /// </summary>\\n    sealed class NullScope : IDisposable\\n    {\\n        public static NullScope Instance { get; } = new NullScope();\\n\\n        private NullScope()\\n        {\\n        }\\n\\n        public void Dispose()\\n        {\\n        }\\n    }\\n}\\n```\\n\\nThis implementation is inspired by both the .NET 8 `FakeLogger` implementation and [David Nguyen\'s post](https://pnguyen.io/posts/verify-ilogger-call-in-dotnet-core/). It\'s not identical to the official implementation, but it\'s close enough for our purposes.\\n\\n## Testing `ILogger` with Snapshooter\\n\\nNow that we have a fake logger, we can use it to test the logging caused by calling our `GetGreeting` method.\\n\\n### Testing `ILogger` with Snapshooter for .NET 8+\\n\\nHere\'s how we might do that with our .NET 8 `FakeLogger`:\\n\\n```cs\\nusing Microsoft.Extensions.Logging.Testing;\\nusing Microsoft.Extensions.Options;\\nusing Microsoft.Extensions.Time.Testing;\\n\\nusing Snapshooter.Xunit;\\n\\nnamespace MyApp.Tests.Services;\\n\\npublic class GreetingServiceTests\\n{\\n    [Fact]\\n    public void GetGreeting_greets_and_logs()\\n    {\\n        // Arrange\\n        var fixedTimeLogCollector = new FakeLogCollector(Options.Create(new FakeLogCollectorOptions {\\n            TimeProvider = new FakeTimeProvider()\\n        }));\\n        var log = new FakeLogger<GreetingService>(fixedTimeLogCollector);\\n\\n        var greetingService = new GreetingService(log);\\n\\n        // Act\\n        var greeting = greetingService.GetGreeting(\\"John\\");\\n\\n        // Assert\\n        Snapshot.Match(new { log = log.Collector.GetSnapshot(), greeting });\\n    }\\n}\\n```\\n\\nHere, we create an instance of `FakeLogger<GreetingService>`. That `FakeLogger` is instantiated with an instance of `FakeLogCollector`. We\'re doing this because we want to stub out the time provider. If we don\'t do this, the log messages will contain the current time. That would will make our snapshot tests brittle. By stubbing out the time provider, we can ensure that the log messages will always contain the same time. This will make our snapshot tests more robust.\\n\\nIf you\'d like to learn more about stubbing out the time provider, I recommend you read [Andrew Lock\'s post on the subject](https://andrewlock.net/exploring-the-dotnet-8-preview-avoiding-flaky-tests-with-timeprovider-and-itimer/#testing-with-the-microsoft-extensions-timeprovider-testing-library).\\n\\nOur `FakeLogger` is passed into the `GreetingService` constructor, ready to be used. We call `GetGreeting`, and then we use Snapshooter to verify that the log messages were written correctly and that the greeting generated is what we expect. The log messages are acquired by calling the `GetSnapshot` method of the `FakeLogCollector`.\\n\\nWhen the test is first run, a `GreetingServiceTests.GetGreeting_greets_and_logs.snap` snapshot is created. This snapshot contains the serialised `log` and `greeting` objects. Subsequent runs of the test will compare the current output with the snapshot. If the output matches the snapshot, the test passes. If it doesn\'t, the test fails. Here is what the contents of the snapshot should look like:\\n\\n```json\\n{\\n  \\"log\\": [\\n    {\\n      \\"Level\\": \\"Information\\",\\n      \\"Id\\": {\\n        \\"Id\\": 0,\\n        \\"Name\\": null\\n      },\\n      \\"State\\": [\\n        {\\n          \\"Key\\": \\"name\\",\\n          \\"Value\\": \\"John\\"\\n        },\\n        {\\n          \\"Key\\": \\"{OriginalFormat}\\",\\n          \\"Value\\": \\"Greeting {name}\\"\\n        }\\n      ],\\n      \\"StructuredState\\": [\\n        {\\n          \\"Key\\": \\"name\\",\\n          \\"Value\\": \\"John\\"\\n        },\\n        {\\n          \\"Key\\": \\"{OriginalFormat}\\",\\n          \\"Value\\": \\"Greeting {name}\\"\\n        }\\n      ],\\n      \\"Exception\\": null,\\n      \\"Message\\": \\"Greeting John\\",\\n      \\"Scopes\\": [],\\n      \\"Category\\": \\"MyApp.Tests.Services.GreetingService\\",\\n      \\"LevelEnabled\\": true,\\n      \\"Timestamp\\": \\"2000-01-01T00:00:00+00:00\\"\\n    }\\n  ],\\n  \\"greeting\\": \\"Hello, John!\\"\\n}\\n```\\n\\nAnd that\'s it, we\'re done! We\'ve tested our logging code with minimal effort; the only assertion we wrote was `Snapshot.Match`. If we change behaviour of the `GetGreeting` method, the test will fail. To remedy we can then update the snapshot and we\'re good to go.\\n\\n### Testing `ILogger` with Snapshooter for earlier .NET versions\\n\\nIf we were using our own `FakeLogger` implementation, we\'d almost the same thing:\\n\\n```cs\\nusing Snapshooter.Xunit;\\n\\nnamespace MyApp.Tests.Services;\\n\\npublic class GreetingServiceTests\\n{\\n    [Fact]\\n    public void GetGreeting_greets_and_logs()\\n    {\\n        // Arrange\\n        var log = new FakeLogger<GreetingService>();\\n\\n        var greetingService = new GreetingService(log);\\n\\n        // Act\\n        var greeting = greetingService.GetGreeting(\\"John\\");\\n\\n        // Assert\\n        Snapshot.Match(new { log = log.GetSnapshot(), greeting });\\n    }\\n}\\n```\\n\\nYou\'ll notice that actually less code is involved this time. That\'s because we don\'t need to stub out the time provider. Our simple implementation of `FakeLogger` doesn\'t bother with time. So we can just call `GetSnapshot` on the `FakeLogger` instance.\\n\\nThe snapshot generated by this test will look like this:\\n\\n```json\\n{\\n  \\"log\\": [\\n    {\\n      \\"Level\\": \\"Information\\",\\n      \\"Message\\": \\"Greeting John\\",\\n      \\"Exception\\": null\\n    }\\n  ],\\n  \\"greeting\\": \\"Hello, John!\\"\\n}\\n```\\n\\nThere\'s a lot less information in this snapshot. That\'s because our simple implementation of `FakeLogger` doesn\'t bother with all of the things the official `FakeLogger` does. But for what we\'re doing here, it\'s enough.\\n\\n## Conclusion\\n\\nIn this post we\'ve seen how to use Snapshooter to test logging code.\\n\\nThis approach is easy to implement, easy to maintain and easy to read. Significantly: it involves very little work on our part. If you\'re not already using snapshot testing, I hope this post has inspired you to give it a try. If you are already using snapshot testing, I hope this post has inspired you to use it to test your logging code."},{"id":"bun-overview","metadata":{"permalink":"/bun-overview","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-12-15-bun-overview/index.md","source":"@site/blog/2023-12-15-bun-overview/index.md","title":"Overview of Bun, a JavaScript runtime","description":"Bun is a new, fast, TypeScript-first, npm compatible-first JavaScript runtime. This is a walkthrough of it!","date":"2023-12-15T00:00:00.000Z","tags":[{"inline":false,"label":"Bun","permalink":"/tags/bun","description":"The Bun JavaScript runtime."}],"readingTime":13.61,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null},{"name":"Megan Lee","title":"Content Marketing Manager","url":"https://www.linkedin.com/in/leemeganj/","image_url":"https://media.licdn.com/dms/image/C4D03AQFSGZ5WM0U3MQ/profile-displayphoto-shrink_800_800/0/1516959703212?e=1705536000&v=beta&t=3JcqFyOGoIFse9XyBD3KZCPb3Z4hEPgbj-8dgcDfxWQ","imageURL":"https://media.licdn.com/dms/image/C4D03AQFSGZ5WM0U3MQ/profile-displayphoto-shrink_800_800/0/1516959703212?e=1705536000&v=beta&t=3JcqFyOGoIFse9XyBD3KZCPb3Z4hEPgbj-8dgcDfxWQ","socials":{},"key":null,"page":null}],"frontMatter":{"slug":"bun-overview","title":"Overview of Bun, a JavaScript runtime","authors":["johnnyreilly",{"name":"Megan Lee","title":"Content Marketing Manager","url":"https://www.linkedin.com/in/leemeganj/","image_url":"https://media.licdn.com/dms/image/C4D03AQFSGZ5WM0U3MQ/profile-displayphoto-shrink_800_800/0/1516959703212?e=1705536000&v=beta&t=3JcqFyOGoIFse9XyBD3KZCPb3Z4hEPgbj-8dgcDfxWQ","imageURL":"https://media.licdn.com/dms/image/C4D03AQFSGZ5WM0U3MQ/profile-displayphoto-shrink_800_800/0/1516959703212?e=1705536000&v=beta&t=3JcqFyOGoIFse9XyBD3KZCPb3Z4hEPgbj-8dgcDfxWQ"}],"tags":["bun"],"image":"./title-image.png","description":"Bun is a new, fast, TypeScript-first, npm compatible-first JavaScript runtime. This is a walkthrough of it!","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Snapshot log tests in .NET","permalink":"/snapshot-log-tests-dotnet"},"nextItem":{"title":"How we fixed my SEO","permalink":"/how-we-fixed-my-seo"}},"content":"Like Node.js and Deno, Bun is a JavaScript runtime that provides a faster development experience while you\u2019re building frontend applications. It\u2019s gaining ground as a competitor to these widely used runtime environments \u2014 and for good reason.\\n\\nIn this evaluation guide, we\u2019ll explore the features that make Bun an excellent choice for developing fast, performant, error-free frontend apps. By the end of this article, you\u2019ll have a clear understanding of when and why you should use Bun in your projects.\\n\\n![title image reading \\"Bun overview: whats cooking\\" with the Bun logo](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## What is Bun?\\n\\nBun was created by Jarred Sumner with the intention that, if you currently use Node.js, you should easily be able to swap it out and replace it with Bun instead.\\n\\nIn other words, you should be able to quickly take advantage of Bun\u2019s awesome features without having to deal with a steep learning curve. Generally, you can use the same frameworks, libraries, and conventions you\u2019re used to, often without issue. Keep in mind there may be exceptions to this.\\n\\nThe Bun team released v1.0 on 8 September 2023. However, Bun was already widely known and used long before that. Jarred Sumner even [appeared on PodRocket](https://open.spotify.com/episode/512NHtxknFNeipHSZHIRXP) on 5 August 2022 to discuss Bun, more than a year before the release of v1.0.\\n\\nPart of the reason for Bun\u2019s rise in popularity even before its stable release is its speed and ease of use.\\n\\nWhile Node.js and Deno both use V8 \u2014 the Chrome JavaScript engine \u2014 Bun uses Apple\u2019s JavaScriptCore. This choice of JavaScript engine supports the [Bun team\u2019s self-described goal](https://bun.sh/blog/bun-v1.0) of \u201Celiminating slowness and complexity without throwing away everything that\'s great about JavaScript.\u201D\\n\\nAdditionally, Bun is written using Zig, a systems programming language that allows you to write extremely performant software \u2014 exactly what Bun intends to enable you to do. Interestingly, while Bun has now hit 1.0, Zig, which has been around longer, has not!\\n\\nIn contrast, Node.js is written in C++, and Deno is written in Rust. Zig makes it possible to write code that is both fast and safe, contributing to Bun\u2019s speed and performance.\\n\\n## Why use Bun?\\n\\nJavaScript is a great language, and the npm ecosystem is vast. These combined have powered large numbers of systems and development experiences for years. Given that this is the case, why would you want to use Bun in particular? What are the benefits that make it such a great choice?\\n\\nIf I were to explain why I\'ve been using Bun, it\'s because it is like using Node.js, but simply a better version of it. Bun is the Node.js I want to use. Let\'s go through some of the reasons why:\\n\\n- **Performance/speed:** Bun is fast. Really fast. That speed really makes a difference in terms of your efficiency when developing software\\n- **TypeScript support:** Bun has great TypeScript support. It\'s easy to use TypeScript with Bun. You don\'t need to do anything special to get it to work \u2014 it just works\\n- **Ease of use / DX:** Bun is a joy to use. Because it targets Node.js compatibility, there isn\'t much to learn if you were using Node.js before; you just use Bun\\n- **Community & ecosystem:** If you reach out to the Bun community with an issue, you\'re likely to get a response very quickly \u2014 it has a great community and ecosystem, including a very responsive and helpful core team\\n- **Documentation:** The documentation for Bun is excellent. It\'s clear, concise, and easy to follow. It\'s also very comprehensive. If you\'re looking for something, you\'ll likely find it in the docs\\n\\nBun also provides a built-in bundler, although it\u2019s still in beta and may not be widely used. However, it\u2019s fast, works as a bundler should, and was created by the Bun team, so it\'s likely to be supported on an ongoing basis.\\n\\nKeep in mind that \u2014 like any tool \u2014 Bun isn\u2019t perfect. Let\u2019s take a look at some things you should keep in mind before or while using Bun.\\n\\n### Considerations while using Bun\\n\\nWhilst Bun is a fantastic tool with tremendous goals, it\u2019s worth thinking about some of the friction and imperfections around Bun:\\n\\n- **Limited continuous integration implementations:** If you want to use Bun in GitHub Actions, an integration exists right now. This will download the desired version of Bun to the running machine, install it, and allow you to use Bun in the context of GitHub Actions. However, a similar standard mechanism for downloading and installing Bun may not yet exist if we want to use a different CI technology, like Azure Pipelines\\n- **Binary `bun.lockb` lock file:** One of Bun\u2019s more unusual aspects is the lock file it employs. Lock files are well established in the Node.js ecosystem, allowing deterministic installs of packages \u2014 like `yarn.lock` for Yarn or `package-lock.json` for npm. Bun has trodden a slightly different path with a binary lock file, `bun.lockb`. Using a binary lock file reduces the file size and improves efficiency in areas like parsing, processing, and version control. However, it also makes [working out the difference between an old and a new lock file quite involved](https://bun.sh/docs/install/lockfile). There\u2019s some friction here both in terms of seeing the diff as well as plugging into third-party tools that manage dependency upgrades\\n- **Not everything works:** While many workflows work just as you would hope, some don\u2019t. For instance, if you want to build a Docusaurus website, [right now you can\u2019t](https://github.com/oven-sh/bun/issues/3426). This is a reflection of how young Bun is at present; as time passes, more and more workflows will be supported. But depending upon the nature of the work you do, you may want to hold off a little while, until more use cases are unblocked\\n\\nIt\u2019s worth noting that these are not particularly significant hindrances, and many are not necessarily long-term issues either. As time passes, they\u2019ll likely become even less problematic. However, as of now, they are worth taking into account before you go all-in on Bun.\\n\\n## Key Bun features to know\\n\\nWe\u2019ve touched on why Bun was created and looked at some high-level reasons why you should consider using it. Now, it\u2019s time to dive deeper into what exactly Bun offers. Let\'s take a look at some of its features.\\n\\n### Node.js compatibility and npm support\u200B\\n\\nThe aim is that Bun will entirely support Node.js APIs. Consequently, the majority of npm packages \u2014 which were originally written for the browser and for Node.js \u2014 should just work with Bun.\\n\\nBun also includes a package manager, `bun install`, which is compatible with npm as well as faster than npm, Yarn, and pnpm by some margin! You can use `bun install` to install packages from npm or from a Bun registry.\\n\\nNotably, you still use `package.json` to manage your dependencies in Bun. This makes migrating from Node.js to Bun very easy and reduces the learning curve. If you just want to dip your feet in the water of Bun, you could just use it for installs and speed up your GitHub Actions!\\n\\n### Speed!\\n\\n\u200B\\nBun extends the JavaScriptCore engine \u2014 the engine that hails from the Safari browser \u2014 but with incredible performance, thanks to its Zig implementation. It\'s fast. Really fast. It\'s not only way faster than Node.js but also faster than Deno in most benchmarks.\\n\\nYou\'ll hear about speed in almost everything that Bun does. Speed is a feature. Bun runs fast. Bun starts fast. Bun installs fast. Bun hot reloads fast. Bun bundles fast. Bun runs tests fast. Bun is fast.\\n\\nYou\u2019re likely now tired of reading the word \u201Cfast\u201D \u2014 but hopefully, you get the point!\\n\\n### TypeScript, TSX, and JSX support\u200B\\n\\nIf you use `.ts` and `.tsx` files, Bun can make your life much easier. It can execute these files in the same way Node.js can execute JavaScript. No need to set up a build step or add ts-node \u2014 it just works.\\n\\nOne exciting aspect of this feature is that you can write TypeScript directly and execute it in a Node.js style. If you had been relying upon a build step or on JSDoc for strong typing in JavaScript, now you can just use TypeScript!\\n\\nSimilarly, when it comes to React, Bun transpiles it into JavaScript internally \u2014 you don\u2019t have to worry about it.\\n\\n### Hot reloading and watch mode\u200B\\n\\nBun has great support for watch mode and hot reloading. These features help you automatically monitor your source files for changes and then update your running application accordingly.\\n\\nYou\u2019re likely used to hot reloading taking some time; it\u2019s usually not instant. But with Bun, it pretty much is. How, you ask? I\u2019ll paraphrase the docs:\\n\\nDetects changes to files using OS native filesystem watcher APIs\\nCan scale to larger projects thanks to optimization techniques like setting a higher limit for file descriptors, statically allocating file path buffers, reusing file descriptors whenever possible, and more\\n\\nIn the Bun docs, you can also find a demonstration from the Bun team showing the performance of watch mode:\\n\\n![incredibly fast hot reloading in VS Code](vs-code-hot-reloading.gif)\\n\\nThat is fast.\\n\\n### Solves pain points of ES modules and CommonJS\u200B\\n\\nAs part of the JavaScript world, you\u2019re likely very aware that the long-standing but nonstandard CommonJS modules are starting to be displaced by standard ES modules.\\n\\nTypeScript enforces its own set of rules around import extensions that aren\'t compatible with ESM. Different build tools support path re-mapping via disparate, non-compatible mechanisms. Node.js, for instance, has many \u201Cgotchas\u201D around ESM resolution, making it difficult to use in practice.\\n\\nBun aims to provide a consistent and predictable module resolution system that just works. It does this very well indeed. Code that Node.js might struggle with, Bun handles with ease.\\n\\nThis is a great example of Bun taking the pain out of using JavaScript. It\u2019s a great example of how Bun saves time and increases happiness.\\n\\n### Support for web standard APIs\u200B\\n\\nSome web APIs aren\'t relevant in the context of a server-first runtime like Bun \u2014 for instance, the DOM API or History API. But many APIs are, and Bun embraces them. It doesn\'t reinvent the wheel; it leans into the ecosystem.\\n\\nA great example is the Fetch API, an API that has been around for a long time in the browser but is relatively new to Node.js. Bun supports Fetch out of the box, along with providing partial or complete support for many other Web APIs.\\n\\n### Testing\u200B\\n\\nAnother built-in Bun feature is its ultra-fast test runner, which is compatible with the Jest testing framework. The Bun runtime itself executes the tests, contributing to a smooth DX and optimal performance.\\n\\nBun\u2019s testing capabilities support features like the following:\\n\\n- **TypeScript support:** Write strongly typed tests in TypeScript without needing to transpile your code. This out-of-the-box support can also help you catch type-related errors early. No more adding ts-jest and ts-node into your project to get to the same goal!\\n- **JSX support:** Test your UI components as you build them \u2014 no additional configuration needed\\n- **Lifecycle hooks:** Set up conditions before tests run, then clean up after them, making your tests more reliable and easier to maintain\\n- **Snapshot testing:** Snapshot tests are a fantastic way to cut down on the code you have to write for a test and get good coverage of the behavior of portions of your application. If you loved this testing feature with Yarn, Bun provides it too!\\n- **UI & DOM testing:** If you want to test your DOM without the need for a browser, Bun has you covered\\n- **Watch mode with `--watch`:** You may prefer to have your tests running in the background and getting that feedback as files are updated. Bun tests give you just what you need with the speed you\u2019ve likely come to expect\\n\\nThese comprehensive testing features are a compelling alternative to what\u2019s out there already. But perhaps more significantly, if you have tests you\u2019re already writing in another test framework \u2014 which likely describes the majority of cases \u2014 then you can still use them with Bun.\\n\\n### Debugging\u200B\\n\\nAll platforms benefit from the ability to debug. Presently, Bun supports debugging in two ways:\\n\\n- [The web debugger](https://bun.sh/guides/runtime/web-debugger)\\n- [The VS Code debugger](https://bun.sh/guides/runtime/vscode-debugger)\\n\\nOf these two, the recommended approach at the time of this writing is to use the web debugger. The VS Code debugger is still in beta and has some bugs.\\n\\n### Additional internal Bun APIs\u200B\\n\\n[Bun implements a set of native APIs](https://bun.sh/docs/runtime/bun-apis) on the Bun global object and through a number of built-in modules. You can use these APIs, but they generally amount to being aliases for the Node.js API equivalents.\\n\\nIf you\u2019re concerned about vendor lock-in, or if you want your code to be more typical to readers, you may want to use the Node.js APIs over Bun\u2019s.\\n\\n## Use cases for Bun\u200B\\n\\nSo where would you use Bun? What are some of its ideal use cases?\\n\\nThere are many, but here are some of the use cases that I\'ve used Bun for:\\n\\n- **Installing npm packages:** Because Bun is compatible with npm, you can use it to install npm packages faster than with Node.js. This gives you back the time you would otherwise waste sitting and looking at your console to install locally or waiting for your CI to install packages\\n- **Running scripts:** I\'ve long used JavaScript and TypeScript to write command line scripts. Bun is a great way to run these, as it executes them much faster\\n- **Local development:** Being fast and easy to use makes Bun great for local development. In case I haven\u2019t said it enough, let me say it again: Bun is fast!\\n\\nYou can explore many of the other use cases for Bun in Bun\'s guides.\\n\\n## Bun vs. Node.js and Deno\u200B\\n\\nBun is a JavaScript runtime. So is Node.js, and so is Deno. So what\'s the difference between them? Why would you use Bun over Node.js or Deno?\\n\\nIt\'s possible to look at Bun as \\"Node.js - but better!\\". By this I mean it supports the same APIs as Node.js, but it\'s faster and often easier to use.\\n\\nFor example, you can write TypeScript with Node.js, but you will need to do some work to get it to work. With Bun, you can just write TypeScript. The speed of Bun is also a big selling point. Bun is much faster than Node.js \u2014 not just a little bit faster, but a lot faster.\\n\\nDeno is a very similar project to Bun: a fast JavaScript runtime that supports TypeScript. However, though Deno itself is very fast, Bun is faster in many benchmarks than Deno.\\n\\nAdditionally, Deno is a great project, but [at launch, it intentionally didn\'t support npm](https://deno.com/blog/v1). This lack of npm compatibility increased friction in the process of migrating a Node.js app to Deno.\\n\\nTimes have changed; Deno now supports npm, and that friction is lessened. But Bun has supported npm from the start and always intended to.\\n\\nHere\u2019s a comparison table you can use as a quick resource for comparing Node.js, Deno, and Bun:\\n\\n|                             | Node.js                            | Deno                                                                                                                                 | Bun                                             |\\n| --------------------------- | ---------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------- |\\n| JavaScript support          | \u2705                                 | \u2705                                                                                                                                   | \u2705                                              |\\n| TypeScript support          | \u274C                                 | \u2705                                                                                                                                   | \u2705                                              |\\n| JSX / TSX support           | \u274C                                 | \u2705                                                                                                                                   | \u2705                                              |\\n| Speed                       | Reasonable                         | Faster than Node.js                                                                                                                  | Faster than Node.js and Deno by many benchmarks |\\n| Node.js API compatible      | \u2705                                 | \u2705 \u2014 [permissions will need to be granted at runtime](https://docs.deno.com/runtime/manual/node/migrate#runtime-permissions-in-deno) | \u2705                                              |\\n| Module support - CommonJS   | \u2705                                 | \u274C                                                                                                                                   | \u2705                                              |\\n| Module support - ECMAScript | \u2705 \u2014 although not without friction | \u2705                                                                                                                                   | \u2705                                              |\\n\\n## Conclusion\u200B\\n\\nIf you love writing JavaScript and TypeScript, then you\'ll love Bun. It\'s fast, easy to pick up, and a joy to use. Bun is a great project and I\'m excited to see where it goes next.\\n\\nDo take a look at the Bun website to see its documentation and developer guides. I hope this resource will be helpful as you consider a JavaScript runtime to use in your next project.\\n\\n[This post was originally published on LogRocket](https://blog.logrocket.com/bun-adoption-guide/) and edited by [Megan Lee](https://www.linkedin.com/in/leemeganj/).\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.logrocket.com/bun-adoption-guide/\\" />\\n</head>"},{"id":"how-we-fixed-my-seo","metadata":{"permalink":"/how-we-fixed-my-seo","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-11-28-how-we-fixed-my-seo/index.md","source":"@site/blog/2023-11-28-how-we-fixed-my-seo/index.md","title":"How we fixed my SEO","description":"In October 2022 traffic to my site tanked. Growtika collaborated with me to fix it. This is what we did. Read it if you\'re trying to improve your SEO.","date":"2023-11-28T00:00:00.000Z","tags":[{"inline":false,"label":"SEO","permalink":"/tags/seo","description":"Search Engine Optimization."},{"inline":false,"label":"Docusaurus","permalink":"/tags/docusaurus","description":"The Docusaurus static site generator."},{"inline":false,"label":"Azure Static Web Apps","permalink":"/tags/azure-static-web-apps","description":"The Azure Static Web Apps service."}],"readingTime":32.095,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null},{"name":"Growtika","title":"A dedicated SEO and growth marketing firm for dev-focused, cybersecurity, fintech and deep tech startups","url":"https://growtika.com/","image_url":"./growtika-logo.webp","imageURL":"./growtika-logo.webp","socials":{},"key":null,"page":null}],"frontMatter":{"slug":"how-we-fixed-my-seo","title":"How we fixed my SEO","authors":["johnnyreilly",{"name":"Growtika","title":"A dedicated SEO and growth marketing firm for dev-focused, cybersecurity, fintech and deep tech startups","url":"https://growtika.com/","image_url":"./growtika-logo.webp","imageURL":"./growtika-logo.webp"}],"tags":["seo","docusaurus","azure static web apps"],"image":"./title-image.png","description":"In October 2022 traffic to my site tanked. Growtika collaborated with me to fix it. This is what we did. Read it if you\'re trying to improve your SEO.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Overview of Bun, a JavaScript runtime","permalink":"/bun-overview"},"nextItem":{"title":"Graph API: getting users Active Directory group names and ids with the C# SDK","permalink":"/graph-api-ad-users-group-name-ids-csharp-sdk"}},"content":"We might also call this:\\n\\n## I ruined my SEO and a stranger from Hacker News help me fix it\\n\\nThis is a follow up to my [\\"How I ruined my SEO\\"](../2023-01-15-how-i-ruined-my-seo/index.md) post. That was about how my site stopped ranking in Google\'s search results around October 2022. This post is about how [Growtika](https://growtika.com/) and I worked together to fix it.\\n\\nAs we\'ll see, the art of SEO (Search Engine Optimisation) is a mysterious one. We made a number of changes that we believe helped. All told, my site spent about a year out in the cold - barely surfacing in search results. But in October 2023 it started ranking again. And it\'s been ranking ever since.\\n\\nI put that down to the assistance rendered by Growtika. What was the nature of that assistance? I\'ll tell you. This post is a biggie; so buckle up!\\n\\n![title image reading \\"How we fixed my SEO\\" with images of graphs trending upwards in the background](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Growtika steps up!\\n\\nI wrote [\\"How I ruined my SEO\\"](../2023-01-15-how-i-ruined-my-seo/index.md) almost as self therapy. I was frustrated that my site\'s traffic had dropped. I knew it didn\'t really matter; my motivation for writing my blog is, in large part, about creating a long term memory for myself. But I was still frustrated. I write things that I know people find useful, and so it was suboptimal that my posts were no longer being found.\\n\\nI should include myself in that. When I\'m trying to remember how to do something (and I know I once knew how to do it) I\'ll often Google it. Hoping to see a blog post I once wrote that answers my question. But, no more. My own site was no longer being found by me. I was missing me. Vanity.\\n\\nI shared the post on Hacker News, not really expecting much to happen. But it ranked, and in amongst the conversation that followed, [someone named Growtika offered to help](https://news.ycombinator.com/item?id=34389421#34390189).\\n\\n![screenshot of the Hacker News conversation with Growtika](screenshot-hacker-news-growtika.webp)\\n\\nI hadn\'t heard of Growtika before; SEO is not my world. But it turned out that Growtika specialise in helping organisations with that. Out of the goodness of their hearts, they offered to assist me. Never one to look a gift horse in the mouth, I leapt at the offer.\\n\\n## The mysterious SEO feedback loop\\n\\nI spent some time with Growtika talking through my site. They made some suggestions around getting my site to align with best practices. They also schooled me on some of the basics of SEO. I was very much a novice in this area, and so I was grateful for the education.\\n\\nHere\'s the thing: SEO is a mystery. Or at least, it\'s not fully understood. Like Coke haven\'t published their recipe, Google doesn\'t publish its (ever evolving) algorithm. They do publish [SEO guidelines](https://developers.google.com/search/docs/fundamentals/seo-starter-guide), but they are just that: guidelines. And so, whilst there are best practices, there is no guarantee that following them will result in success.\\n\\nWhat\'s more, the feedback loop for changes is **long**. It\'s not like fixing a program with a bug, where you tweak the code, run the tests and see if it\'s fixed. It\'s more like making a change to a program, and then waiting weeks or months to see if it\'s fixed. And if it\'s not, you have to wait again to see if the next change you make fixes it.\\n\\nCause and effect are just not as obvious as you might like, when it comes to SEO. So whilst I\'m going to share what we did, I can\'t say for sure what actually lead to the improvement in my site\'s SEO. I\'m confident that they were all good things to do. But I cannot be certain which of them made the difference.\\n\\nAs an aside, Growtika think that it\'s pretty absurd that developers who write high quality technical articles (and for the sake of this point, let\'s say mine fit into this category sometimes), need to run the gauntlet of SEO best practices to get ranked. It really shouldn\'t be necessary. With one of the recent Google algorithm updates; [the helpful content algorithm update](https://developers.google.com/search/docs/appearance/helpful-content-system), it feels like Google are starting to understand that it and prioritize it in their search engine. But there\'s still a long way to go.\\n\\n## The changes we made\\n\\nOver the time we worked together, Growtika made a number of suggestions. Changes we might make that could improve my SEO. I\'m going to go through the suggestions over the rest of the post. I\'ll also share some of the rationale as I go along.\\n\\n### E-E-A-T all you can!\\n\\nThere\'s a concept used by Google for ranking known as Experience, Expertise, Authoritativeness, and Trust (E-E-A-T). It\'s about how much Google trusts the content on your site. When evaluating an author\'s profile for a blog post, it\'s worth considering the following E-E-A-T aspects:\\n\\n- **Experience**: What is the author\'s practical experience in the topic area?\\n- **Expertise**: Does the author have specialized knowledge or educational background in the subject?\\n- **Authoritativeness**: How is the author recognized by peers or industry experts? Are there publications or professional achievements highlighting their authority?\\n- **Trustworthiness**: Can the reader rely on the author for accurate and ethical information? Consider their track record and any endorsements by credible sources.\\n\\nI didn\'t have much that addressed these points on my site.\\n\\n#### 1. Author profile\\n\\nOn each blog post I had a profile picture. But it wasn\'t being all it could be; it looked like this:\\n\\n![picture of the profile image of this blog\'s author](screenshot-profile-picture-before.webp)\\n\\nIt\'s my face and the text _\\"John Reilly\\"_ which linked through to my Twitter (now X) profile page. Nice enough but not really demonstrating my expertise and authority on topics. I updated it to look like this:\\n\\n![picture of the profile image of this blog\'s author with a byline](screenshot-profile-picture-after.webp)\\n\\nAlongside my picture and name I added a byline to demonstrate my expertise and authority on topics: _\\"OSS Engineer - TypeScript, Azure, React, Node.js, .NET\\"_. Alongside that, I switched the link to the about page on my site instead of Twitter.\\n\\n#### 2. About page\\n\\nSince the author profile at the top of each post didn\'t answer all the E-E-A-T points, enriching my about page was the way forward. I updated the [about](/about) page to include a richer bio and a list of places where my site has been featured:\\n\\n![screenshot of the \\"where has this blog featured section\\"](screenshot-where-has-this-blog-featured.webp)\\n\\nThis was to demonstrate my expertise and authority on topics. We even snuck in some structured data - more on that later!\\n\\n### Duplicate content\\n\\nMy site is built using [Docusaurus](https://docusaurus.io/). Now I love Docusaurus, but it\'s not perfect. One of the problems with it is that it generates a number of pages that are not helpful for SEO as they **duplicate content**. (This is a bad thing for SEO.) Consider this report:\\n\\n![screenshot of duplicate content report](screenshot-duplicate-content.webp)\\n\\nThe report suggests there was a good amount of duplicate content on my site. This was because Docusaurus generates \\"pagination\\" pages which allow you to navigate click by click through the whole history of a blog.\\n\\n![screenshot of the Docusaurus pagination mechanism](screenshot-docusaurus-pagination.webp)\\n\\nAlso Docusaurus creates \\"tag\\" (or category) pages that reproduce blog posts under tags that have been used to categorise the posts:\\n\\n![screenshot of the Docusaurus tags mechanism](screenshot-docusaurus-tags.webp)\\n\\nIn both cases, these pages duplicate content - which lead to the above report. Rather frustratingly, the pages also feature `canonical` link tags which rather suggest that they are the canonical source of the content:\\n\\n![screenshot of a canonical tag which reads: link rel=\\"canonical\\" href=\\"https://johnnyreilly.com/page/2\\"](screenshot-not-helpful-canonical.png)\\n\\nIn my case, some of these pagination or tags pages were being prioritised over the original blog posts. This felt quite strange from a readers point of view. We took a number of steps to address this.\\n\\n#### 1. Remove or `noindex` unnecessary pages\\n\\nI wanted to remove or `noindex` the pagination and tags pages to provide a clear signal to search engines about which pages were the most important. I couldn\'t remove the pages without breaking the navigation on my site, so I chose instead to `noindex` them. My site is hosted on [Azure Static Web Apps](../2023-02-01-migrating-from-github-pages-to-azure-static-web-apps/index.md) and so I was able to achieve this fairly easily by adding the following to my [`staticwebapp.config.json`](https://learn.microsoft.com/en-us/azure/static-web-apps/configuration) file:\\n\\n```json title=\\"staticwebapp.config.json\\"\\n{\\n  \\"route\\": \\"/page/*\\",\\n  \\"headers\\": {\\n    \\"X-Robots-Tag\\": \\"noindex\\"\\n  }\\n},\\n{\\n  \\"route\\": \\"/tags/*\\",\\n  \\"headers\\": {\\n    \\"X-Robots-Tag\\": \\"noindex\\"\\n  }\\n},\\n```\\n\\nThis meant that the pagination and tags pages (which were served up under URLs beginning `/page/` and `/tags/` respectively) were still available, but search engines were encouraged [not to index them by the `X-Robots-Tag: noindex` header](https://developers.google.com/search/docs/crawling-indexing/robots-meta-tag#xrobotstag) that these pages now served.\\n\\nI might see if I could land a change like this in Docusaurus itself. I think it would be helpful for others. The mechanism would need to be slightly different, as Docusaurus doesn\'t have control over headers your site serves. But I think it would be possible to add a `noindex` meta tag to the pagination and tags pages HTML as is [suggested here](https://developers.google.com/search/docs/crawling-indexing/block-indexing#implementing-noindex):\\n\\n```html\\n<meta name=\\"robots\\" content=\\"noindex\\" />\\n```\\n\\n#### 2. Docusaurus truncate\\n\\nAs I\'ve mentioned, we have pagination and tags pages that duplicate content. It\'s possible to make this slightly better by truncating the content on the pagination and tags pages. It amounts to adding a `truncate` marker early into the content of each blog post.\\n\\nWith this in place, the pagination and tags pages don\'t duplicate the content of the blog posts in full, but instead just feature a snippet of the content. There\'s documentation on how to do this in [Docusaurus here](https://docusaurus.io/docs/blog#blog-list).\\n\\nI did this for every page on my blog. Given I have quite a lot of posts, doing it manually would have been tedious. So I scripted the insertion of a truncate marker after the first paragraph of each post, and it was done in a jiffy.\\n\\n### Tags review\\n\\nI also performed something of a tag rationalisation. I had a lot of tags, and many of them were not used on more than one blog post. Also, many of them were not the greatest of tags, as this slightly embarrassing screenshot demonstrates:\\n\\n![screenshot of the tags before my rationalisation](screenshot-tags-before.png)\\n\\nAs is probably apparent, I\'d not really thought about tags much. I\'d just added them as I\'d written blog posts. I\'d tagged first, asked questions later. I removed a lot of the (rather pointless) tags I had and also added a tags to blog posts that were missing them. This removed the \\"noise\\", so search engines understand the content of my blog posts, and readers also. Less is more.\\n\\n![screenshot of the tags after my rationalisation](screenshot-tags-after.png)\\n\\nMuch better!\\n\\n### `sitemap.xml` and `robots.txt`\\n\\nAlongside `noindex`ing the pagination and tags pages, we took a look at my `sitemap.xml` - this is automatically generated by Docusaurus. I removed the pagination and tags pages from the `sitemap.xml` as it\'s a little confusing to `noindex` a page and then include it in the `sitemap.xml`.\\n\\nFurther to that, I write posts for other websites sometimes and cross post it on my own blog, with a canonical pointing to the original post. Having these posts in the `sitemap.xml` wasn\'t quite right as they are not the canonical source of the content. I removed them.\\n\\nI\'ve a number of post processing steps that run in my build step of my site, and I included this filtering in it. In the end it amounted to [filtering an XML file; which is pretty straightforward - I wrote about it to demonstrate](../2022-11-22-xml-read-and-write-with-node-js/index.md).\\n\\nAs well as filtering my `sitemap.xml`, I went a little further and added `lastmod` timestamps to the `sitemap.xml` based on the git commit date of the markdown file that the blog post was generated from. This was to help search engines understand how recent the content on my site is. [I wrote about how I did this](../2022-11-25-adding-lastmod-to-sitemap-git-commit-date/index.md). Google have subsequently announced that they use [`lastmod` as a signal for scheduling re-crawling URLs](https://developers.google.com/search/blog/2023/06/sitemaps-lastmod-ping#the-lastmod-element) and so this turns out to have been a helpful change to make.\\n\\nAlongside this, I added a `robots.txt` to my site. These are files that search engines use to understand the structure of a site and what they should and should not index. I didn\'t previously have one and the one I added was pretty rudimentary:\\n\\n```text title=\\"robots.txt\\"\\nUser-agent: *\\nAllow: /\\n\\nSitemap: https://johnnyreilly.com/sitemap.xml\\n```\\n\\nI don\'t know how much this helped, but it certainly didn\'t hurt.\\n\\n### Internal linking / footer links\\n\\nWe next looked at our internal linking strategy. This is about how we link to other pages on our blog from within our blog posts. The idea is that we should link to other pages on our blog that are relevant to the topic of the blog post. This helps search engines understand the structure of our blog and the relationships between the pages.\\n\\nThis was something that I did a little, but I didn\'t really think about. I\'m now much more intentional around internal linking. I\'m very much an editor of my content, and as I\'m editing my posts / writing new posts I\'ll take a look at whether I\'m linking to other relevant posts on my blog and whether perhaps I should be.\\n\\nYou\'ll possibly have noticed a good number of internal links in this post! I\'m careful about how I do this - I have internal links where they are relevant and where I think it adds value. I don\'t have internal links for the sake of it. Whilst I want to improve my SEO, the main readers of my blog are humans, and I want to make sure that I\'m not making the experience worse for them.\\n\\nAlongside upping my internal linking game, Growtika suggested that the footer of my site was a prime place to add links to notable posts on my site, and also to provide an indication of topics that this site seeks to cover.\\n\\nA picture is worth a thousand words, so here\'s what the footer of my site used to look like:\\n\\n![screenshot of the site with very few links or much at all in the footer](screenshot-footer-before.webp)\\n\\nAnd here is what it looks like now:\\n\\n![screenshot of the site with a good number of links in the footer](screenshot-footer-after.png)\\n\\nAs you can see, the difference is significant. With the new enhanced footer I can call out particular articles around themes that I cover, I can highlight popular articles, and I can also emphasise articles that I think are particularly important, or recently updated. This is both about helping search engines understand what I consider to be important in my site, it\'s also helpful for humans that might scroll that far down. And goshdarnit, I think it looks rather fine too!\\n\\n### Pages crawl depth\\n\\nYou likely know that a primary way that search engines find content on your site is by following links. They have [crawlers](https://developers.google.com/search/docs/crawling-indexing/overview-google-crawlers) that do this. The more links that a crawler has to follow to get to a page, the more difficult it is for the crawler to find that page. This is known as the pages crawl depth.\\n\\nMy initial site structure was not great. I had a number of pages that were 4+ clicks away from the home page:\\n\\n![screenshot of a pages depth report](screenshot-pages-depth.webp)\\n\\nA primary reason for my pages crawl depth this was the pagination and tags pages I mentioned earlier. We originally displayed a single full length (not truncated) blog post per page, and so the pagination pages were many. Likewise, we had a lot of tags, and so the tags pages were many. This meant that the pages crawl depth was high. You want to keep the pages crawl depth as low as possible. Less is more.\\n\\nWe increased the number of blog posts displayed per page from **1** to **20** which dramatically reduced the amount of work the crawlers had to do. So instead of having few hundred pagination pages we reduced it to 16. Much better.\\n\\n### Remove date from urls\\n\\nIt used to be the case that the URLs for my blog posts always featured the date of publication. This was a hangover from when I used to use Blogger as my blogging platform. I\'d [migrated from Blogger to Docusaurus](../2021-03-15-definitive-guide-to-migrating-from-blogger-to-docusaurus/index.md), and I\'d kept the date in the URL. It so happens that Docusaurus has a similar behaviour too.\\n\\nGrowtika suggested that I remove the date from the URL. This was to make the URLs shorter and more readable. Search engines also have a preference both for shorter URLs and for pages that are recent, rather than pages that are old. So removing the date from the URL would help with both of those things. Or at least it would stop search engines that looked for the date in the URL from thinking that older content was irrelevant. (And with our `lastmod` timestamps in the `sitemap.xml` we were already helping search engines understand how recent the content on my site was.)\\n\\nI must admit, I didn\'t really want to make this change. I rather liked having the date in the URL. But, in Growtika we trust. I did it.\\n\\nWhere you used to go to:\\n\\nhttps://johnnyreilly.com/2019/10/08/definitely-typed-movie\\n\\nYou now go to:\\n\\nhttps://johnnyreilly.com/definitely-typed-the-movie\\n\\nAnd of course, we made sure a redirect mechanism was in place to ensure that the old URLs still worked. More on that later - you can test the redirect if you like!\\n\\n![screenshot of 301 redirect from the old url to the dateless one](screenshot-301-redirect.webp)\\n\\nTo implement this we used the [slug feature of Docusaurus](https://docusaurus.io/docs/api/plugins/@docusaurus/plugin-content-blog#slug). If you want to see the mega PR that implemented this on nearly 300 blog posts [it\'s here](https://github.com/johnnyreilly/blog.johnnyreilly.com/pull/423/files). You won\'t be surprised to learn I scripted this change - life\'s too short to do boring things by hand.\\n\\n### Blog archive renamed to blog\\n\\nAs I\'ve said, Docusaurus is _great_ but it historically has had some defaults that hurt SEO from a blogging perspective. One of these I identified when I was first planning to migrate from Blogger to Docusaurus. Docusaurus didn\'t ship with a blog archive. This is a page that allows you to browse through your historic blog posts. A place where you can see all that you\'ve written and when. I find this very helpful. It\'s also helpful for search engines to understand the structure of your site.\\n\\nI hand-rolled [my own blog archive for Docusaurus](../2021-05-01-blog-archive-for-docusaurus/index.md) before I migrated. It looked like this:\\n\\n![screenshot of the original blog archive functionality](../2021-05-01-blog-archive-for-docusaurus/docusaurus-blog-archive-small.webp)\\n\\nMy implementation was later made part of Docusaurus itself by [Gabriel Csapo](https://github.com/gabrielcsapo) in [this PR](https://github.com/facebook/docusaurus/pull/5428). So now by default, all Docusaurus sites have a blog archive that lives at `/archive` in the blog. This is great news for Docusaurus users!\\n\\nIn one if the more speculative changes we made, we changed the URL of the blog archive from `/archive` to `/blog` (and the associated navbar label).\\nIt was a wild guess (and it may not have made any difference) but the thinking was that it might affect the CTA (call to action) of people who see my site on Google. If they saw old date in the URL and \\"archive\\" in the breadcrumbs, maybe they\'d think the site is \\"not relevant for the search I have now\\"?\\n\\nSo our tweaked blog archive page now looked like this:\\n\\n![screenshot of the blog archive functionality as it looks now](screenshot-blog-archive-now.png)\\n\\nWe also added a `301` redirect from `/archive` to `/blog` to ensure that any links to the old URL would still work.\\n\\n### Structured data\\n\\nOne of the most intriguing strategies we followed was to build on the structured data support in my site. Structured data is a way of providing metadata about a page in a machine readable format. It\'s a way of providing a clear signal to search engines about the content of a page; it makes their lives easier.\\n\\nAs it turned out, I already had some structured data support in my site; [I\'d written about how to add it previously](../2021-10-15-structured-data-seo-and-react/index.md). But we were to go further!\\n\\n#### 1. FAQs with Structured Data\\n\\nOne of the experiments we ran was to add FAQs to a post, and with that, the equivalent FAQ Structured Data. The intent being to see if this would help with the SEO for that post. So, because I\'m super meta, I wrote a [post about how to do that](../2023-04-08-docusaurus-structured-data-faqs-mdx/index.md) which included FAQs and the equivalent structured data.\\n\\nI also added FAQ structured data to another post and Growtika resubmitted it to Google for indexing. Then two things happened. Firstly, the page was indexed:\\n\\n![screenshot showing the page featuring in search results](screenshot-faqs-structured-data-indexed.webp)\\n\\nAnd then the page started featuring FAQs in the search results:\\n\\n![screenshot showing the page featuring in search results and showing FAQs as well](screenshot-faqs-structured-data.webp)\\n\\nI\'ve included the reactions at the bottom of each screenshot above - we were quite excited!\\n\\n#### 2. Site wide structured data\\n\\nBeyond adding individual structured data to each page and post, I added site wide structured data. This would proclaim from the rooftops about the nature of my site.\\n\\nSo I decided to add site wide structured data of the following types: (there are many types of structured data which you can read about at https://schema.org/ and in [this Google document on the topic](https://developers.google.com/search/docs/appearance/structured-data/search-gallery))\\n\\n- Website\\n- Organisation / Brand\\n- Person\\n\\nYou can see how the structured data is implemented in [this PR](https://github.com/johnnyreilly/blog.johnnyreilly.com/pull/676). We used the [`headTags` API in Docusaurus](https://docusaurus.io/docs/api/docusaurus-config#headTags) to add site wide JSON-LD structured data. Funnily enough, [I contributed the `headTags` API to Docusaurus](https://github.com/facebook/docusaurus/pull/8151) long before I thought I\'d end up using it for this!\\n\\nIn this change we are _heavily_ inspired by the full structured data graph work [Yoast have done](https://yoast.com/rich-results-schema-structured-data-story/). With site wide structured data in place, every page that search engines index on my site will have structured data that describes the site as a whole.\\n\\n#### 3. Breadcrumbs with Structured Data\\n\\nFinally I added breadcrumbs to my blog posts. Breadcrumbs are a way of indicating to search engines where a page sits in the hierarchy of a site. [I wrote about how I did this](../2023-02-05-docusaurus-blogs-adding-breadcrumb-structured-data/index.md). It\'s worth noting that the approach outlined in that post I\'ve subsequently simplified. Originally I added a breadcrumb for the page structure and also one per tag on the post. I\'ve since removed the tag breadcrumbs as they were not adding much value. Less is more.\\n\\n### Do backlinks better!\\n\\nI mentioned in [\\"How I ruined my SEO\\"](../2023-01-15-how-i-ruined-my-seo/index.md) that I had a number of backlinks to my site. I also mentioned that I had broken a number of the backlinks by my carelessness. I planned to fix the broken backlinks and also do a better job of backlinks in general.\\n\\nI\'d already implemented support for dynamic redirects on my site. What this meant was, if someone linked through to a non-existent page on my site, rather than having just a 404 Not Found, I could do some fairly sophisticated work to redirect them to the correct URL. Thus protecting (and unbreaking previously broken) backlinks. By the way, using Azure Static Web Apps as my hosting mechanism really helped me out here as the dynamic redirect mechanism I had was super powerful - I wasn\'t limited to regexes. If you want see how I did that [have a read of this](../2022-12-22-azure-static-web-apps-dynamic-redirects-azure-functions/index.md).\\n\\nWhat I had was good, but I could do better. I did the following:\\n\\n- exhaustively fix all my broken backlinks; getting them all to redirect to the correct place. This meant logging broken backlinks and repairing them over time. Tedious, but worth it.\\n- add a redirect from my old site domain to my new one (blog.johnnyreilly.com -> johnnyreilly.com)\\n- redirect **only once**. I had a number of redirects that were chained together. I had a 301 leading to 301 leading to 301 (yes!) and only then leading to a 200. Redirecting only once would be better. This would ensure that search engines didn\'t have to follow a chain of redirects to get to the content they were looking for. Search engines don\'t like that; you lose \\"link juice\\" the more redirects there are. Also, multi redirects make my website work harder than it needs to.\\n\\nAgain, less is more. With these changes made, I had a much better backlink story.\\n\\n### Improving site performance\\n\\nAnother aspect which factors into SEO is performance. Google has a [Core Web Vitals](https://web.dev/articles/vitals) measurement which is about evaluating the performance of websites. It covers how fast a website loads, how responsive it is / how quickly it becomes interactive.\\n\\nThe thing that was hurting my site\'s performance was images. The images on my site were generally not optimised at all. They were also not lazy loaded. This meant that the images were slowing down the loading of my site, and this reflected in my [Lighthouse](https://developer.chrome.com/docs/lighthouse/overview/) scores.\\n\\nI took a number of actions to improve the site image performance.\\n\\n#### 1. Improved performance using TinyPNG\'s image optimisation API\\n\\nThe first, and most obvious, was to optimise the images on my site. There\'s many ways you can do this; I chose to use [TinyPNG\'s API](https://tinypng.com/developers). I wrote about [how I did this](../2023-01-22-image-optimisation-tinypng-api/index.md). Ultimately I wrote a script that optimised all the images on my site, and allowed me to run it on demand for the images of a particular post.\\n\\nThis shrunk the file size of images on my site served significantly, and improved the performance. Once again, less is more.\\n\\nI also [added Lighthouse to my site\'s build step](../2022-03-20-lighthouse-meet-github-actions/index.md), so I could get some performance measurements surfaced into my pull requests. This made it easy to catch potential regressions, where I might accidentally add unoptimised images to my site.\\n\\n#### 2. Improved performance using Cloudinary\'s on demand image transformation CDN\\n\\nHaving tackled the low hanging fruit of images not being optimal in the first place, I then went further. Cloudinary offer a [CDN that can transform images on demand](https://cloudinary.com/documentation/image_transformations). This means that you can serve the same image in different sizes, formats and qualities depending on the device that is requesting it. This is a great way to improve performance.\\n\\nI was able to plug the Cloudinary CDN into my site using by building a the [`rehype-cloudinary-docusaurus` plugin](https://github.com/johnnyreilly/rehype-cloudinary-docusaurus) which can be used to integrate Cloudinary into Docusaurus. You can [read more about how it works here](../2022-12-26-docusaurus-image-cloudinary-rehype-plugin/index.md).\\n\\nNow when my site serves an image, it serves the optimal image for the device that is requesting it. This improves the performance of my site. (And you can do this too if you\'re using Docusaurus!)\\n\\nIn fact I went a little further and scripted the patching of my [Open Graph images](../2021-12-12-open-graph-sharing-previews-guide/index.md) to make use of Cloudinary too. This meant that the images that were shared on social media were also optimised for the device that was requesting them. I don\'t think this helped with SEO, but I\'d noticed that large and / or slow loading Open Graph images aren\'t always used by platforms that support the Open Graph protocol. With the Cloudinary image transformation CDN in place, this became much less of an issue.\\n\\nIncidentally, Cloudinary got wind of this change and invited me onto their [DevJams live stream to talk about it](https://youtube.com/watch?v=G4WTEEwI6Qs). I was very flattered to be asked and it was a lot of fun!\\n\\n#### 3. Improved performance using `fetchpriority=\\"high\\"` and `loading=\\"lazy\\"`\\n\\nSo far we\'d handled the performance of images on my site by optimising them and serving them in an optimal way. But there was more we could do. We could also make sure that the images on my site were loaded in an optimal way.\\n\\nWe did this by adding `fetchpriority=\\"high\\"` to the first image on each blog post; the \\"title\\" image if you will. This is a hint to the browser that the image is important and should be loaded as soon as possible. We also added `loading=\\"lazy\\"` to all the other images on a given blog post. This is a hint to the browser that those images (the \\"below the fold\\" images) are not as important, and can be loaded later if and when they are required.\\n\\nThe effect of these two changes combined, is that when a browser lands on a blog post it loads the first image as soon as possible, and then it doesn\'t immediately load the rest images; it focuses on giving the user a usable page. The upshot of this is that the Largest Contentful Paint (LCP) is loaded as soon as possible and the browser remains more responsive. The remaining images may be loaded... Or they may not - it depends on whether people scroll down to them. This translates into improved perceived performance / user experience.\\n\\nAnd again: less is more. [I\'ve written about how using `fetchpriority=\\"high\\"` and `loading=\\"lazy\\"` was implemented in depth here](../2023-01-18-docusaurus-improve-core-web-vitals-fetchpriority/index.md).\\n\\n### Be careful publishing your content on other sites\\n\\nOne of the ideas I\'d had as I attempted to fix my SEO was to publish my content on other sites. I\'d seen other people do this, and I thought it might be a good idea. So I set up a mechanism that published my blog posts to [dev.to](https://dev.to/johnnyreilly) with the canonical pointing back to my site. I was so pleased with my idea I even wrote about [how I did it](../2022-12-11-publishing-docusaurus-to-devto-with-devto-api/index.md).\\n\\nPublishing content on other sites isn\'t inherently negative. However, in my case, it created confusion. I\'d hoped that publishing my content on dev.to would help my SEO. It didn\'t. My content on dev.to ranked higher than the original content on my own website (most times my site didn\'t rank at all).\\n\\nGrowtika were keen to \\"cancel the noise\\", which would improve their understanding of my ranking situation. Since dev.to was ranking instead of my site, it was difficult to analyze how long it took for articles to rank on my site. Stopping the submission of content to external sites would help clarify the situation.\\n\\nI turned the mechanism off. This helped them determine the time it took for my site to achieve a ranking.\\n\\n### Get your site featured in relevant places\\n\\nWhilst actually publishing your content on other sites with a canonical turns out not to be the best idea, getting your site featured in relevant places is a good idea. This is about getting your site linked to from other sites. This is a signal to search engines that your site is relevant and important.\\n\\nI already had a number of links to my site from other sites. For instance, I\'d regularly show up in [Azure Weekly](https://azureweekly.info/), [The Morning Brew](https://blog.cwa.me.uk/) and a number of other sites. Many of these use my RSS feed.\\n\\nI also submitted my site to a number of other places. For instance, I submitted my site to [daily.dev](https://daily.dev/). A good rule of thumb here for picking places tended to be \\"where do you go to read about the topics you write about?\\". But crucially I didn\'t place my content on others sites, I just linked to my site from other sites.\\n\\nAs a side note, I rather wish that RSS still thrived. I support it - people still use it to read my blog. But it\'s not as popular as it once was.\\n\\n### Add meta description to blog posts\\n\\nThe final tweak we\'re going to cover, is adding meta descriptions to my posts. This is a short description of the content of the blog post that is included in the HTML of the page:\\n\\n```html\\n<meta\\n  name=\\"description\\"\\n  content=\\"In October 2022 traffic to my site tanked. Growtika collaborated with me to fix it. This is what we did. Read it if you\'re trying to improve your SEO.\\"\\n/>\\n```\\n\\nIt\'s not visible to humans, but it is visible to search engines. And it\'s kind of visible to humans at one remove, in that it is often used as the description of the page in search results.\\n\\nWe followed these meta description guidelines:\\n\\n- Keep it between 150-155 characters\\n- Use an active voice and make it actionable / include a call to action\\n- Use a focused keyphrase\\n- Make sure it matches the content of the page\\n- Make it unique.\\n\\nI\'d previously not included meta descriptions on my blog posts. I found myself with the daunting task of writing meta descriptions for nearly 300 blog posts. I decided to script it. I wrote a script that would generate a meta description for each blog post based on the content of the blog post, powered by Azure Open AI. I then ran the script and added the meta descriptions to my blog posts. I wrote about [how I did this here](../2023-09-25-azure-open-ai-generate-article-metadata-with-typescript/index.md).\\n\\nThis left me with meta descriptions for all my blog posts. It was also rather fun to use AI for something that was not GPT or copilot related!\\n\\n## Drop off and recovery in numbers and graphs\\n\\nAs I mentioned earlier, my SEO has now recovered. I\'m ranking again in search results. I\'m not ranking quite as highly as I was before; I think my site is possibly still in the throes of recovery. But without a doubt, it\'s definitely trending in the right direction. I want to show you some graphs and numbers to demonstrate this.\\n\\n### Ahrefs\\n\\nHere\'s a graph from Ahrefs showing my site\'s performance over the last two years:\\n\\n![gif from ahrefs showing a massive drop off then recovery](screenshot-ahrefs-two-years.gif)\\n\\nNotably, you can see the drop off happened around the time of a Google algorithm update. Likewise, it\'s really important to highlight that the traffic **increased** around the time Google made _another_ algorithm update. It\'s impossible to know if the traffic would still have gone up if we had not made our changes. Perhaps it would, maybe to a lesser extent. We just don\'t know. It does demonstrate the power of Google\'s algorithm updates though. The graph alone tells a story, a phenomenal drop off in traffic followed by a recovery.\\n\\n### Google Search Console\\n\\nA couple of simpler graphs from Google Search Console tell a similar story. Here\'s a graph showing my site\'s performance over time:\\n\\n![screenshot from Google Search Console showing performance over time](screenshot-of-performance.webp)\\n\\nObserve the massive drop off in October 2022. And then the recovery in October / November 2023. It\'s a similar story to the ahrefs graph above. Further to that, here is the traffic for 28 days in February 2023 vs 28 days in October / November 2023:\\n\\n![screenshot from Google Search Console showing few results in February 2023 vs lots of results in October / November 2023](screenshot-google-search-console.webp)\\n\\nIf you\'re reading on a mobile device you may not even realise that there are two lines on the graph; the February 2023 line is so low and flat as to be almost invisible.\\n\\nThe difference is stark. I couldn\'t get arrested from a search perspective in February 2023; showing up a mere **5,000** times in search results. That sounds like a big number, but in search terms it\'s really not. But by October / November 2023 I was back in the game, showing up **274,000** times in search results. That\'s a **55x** increase!\\n\\nAn interesting aside, that I\'ve excluded from the graph, is that my total clicks increased from **1,160** to **5,470** when comparing Februray to October / November. This is a mere factor of **4**. What does that mean? It means when my site was showing up very rarely in search results, it enjoyed a very healthy click through rate. People clicked on my search results when they saw them!\\n\\n### Bing webmaster tools\\n\\nThis post has mostly been driven by discussing how Google stopped ranking my site in their search results. What I haven\'t really mentioned is that other search engines **did not** stop ranking my site. I was still showing up in Bing and DuckDuckGo\'s search results. Here\'s a graph from Bing webmaster tools (Bing\'s equivalent to Google Search Console) showing my site\'s performance over the last six months:\\n\\n![screenshot from Bing webmaster tools](screenshot-bing-webmaster-tools-graph.webp)\\n\\nThere\'s two things to take from the above graph:\\n\\n1. My site consistently ranked (and ranks) in Bing search results. There was no drop off in Bing as there was in Google.\\n2. The numbers from Bing are much lower than the numbers from Google. So whilst I was still ranking in Bing, it wasn\'t making up for the loss of traffic from Google. This is probably a reflection of the fact that Google is the dominant search engine.\\n\\n### Google Analytics\\n\\nYou might be curious as to what the actual impact of that is on my sites traffic. It\'s fairly significant; around 80% of my sites traffic comes from search engines. So when my Google SEO tanked, my traffic was significantly (but not terminally) impacted. Consider the graph below from Google Analytics:\\n\\n![screenshot of Google Analytics graph showing higher traffic now as compared to earlier](screenshot-google-analytics.png)\\n\\nIt covers a similar time frame to the Google Search Console and ahrefs graphs above. It shows that now, on a weekday (weekends are quieter) I get around **350** unique visitors to my site. Whereas for the comparison period it was more like **100** unique users per day. That\'s roughly a **3.5x%** increase in traffic; which is not to be sniffed at.\\n\\n## Conclusion\\n\\nThis is not a post I\'d expected to write. SEO used to be something I didn\'t think about much. But it turns out that a way to get my attention, is taking away my traffic! I\'m actually rather grateful that all this happened as it got me to thinking and learning about SEO in a way that I quite enjoyed.\\n\\nAs I say, whilst I can\'t be certain which of the changes we made made the difference, I\'m confident that my site now is better than my site a year ago. It loads faster, it\'s more performant, it\'s more structured, it\'s better linked, it\'s better optimised. It\'s better. It looks the part too. I\'m really quite proud of it.\\n\\nI\'m also phenomenally grateful to Growtika for their help. I should say that a few others offered pointers and suggestions which I was thankful for. But it was Growtika who stuck by my side for the long haul. For nearly a year they worked with me; and for a long time saw no improvements in my sites traffic at all. They didn\'t give up. They were patient with me, and they were generous with their time and expertise. I\'m very grateful to them for all their help.\\n\\nIf you\'re looking for help with SEO, I\'d recommend you check out [Growtika](https://growtika.com/). They\'re fantastic folk!"},{"id":"graph-api-ad-users-group-name-ids-csharp-sdk","metadata":{"permalink":"/graph-api-ad-users-group-name-ids-csharp-sdk","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-11-23-graph-api-ad-users-group-name-ids-csharp-sdk/index.md","source":"@site/blog/2023-11-23-graph-api-ad-users-group-name-ids-csharp-sdk/index.md","title":"Graph API: getting users Active Directory group names and ids with the C# SDK","description":"Learn how to get the Azure Active Directory group names and ids from the Graph API using the C# SDK.","date":"2023-11-23T00:00:00.000Z","tags":[{"inline":false,"label":"Authentication and Authorization","permalink":"/tags/auth","description":"Matters related to identity and permissioning"},{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."},{"inline":false,"label":"C#","permalink":"/tags/csharp","description":"The C# programming language."},{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":7.665,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"graph-api-ad-users-group-name-ids-csharp-sdk","title":"Graph API: getting users Active Directory group names and ids with the C# SDK","authors":"johnnyreilly","image":"./title-image.png","tags":["auth","azure","c#","asp.net"],"description":"Learn how to get the Azure Active Directory group names and ids from the Graph API using the C# SDK.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"How we fixed my SEO","permalink":"/how-we-fixed-my-seo"},"nextItem":{"title":"Migrating to v4 Azure Functions Node.js with TypeScript","permalink":"/migrating-azure-functions-node-js-v4-typescript"}},"content":"The Graph API is a great way to get information about users in Azure Active Directory. I recently needed to get the names and ids of the Active Directory groups that a user was a member of. Here\'s how to do it with the C# SDK.\\n\\nI\'m writing this post as, whilst it ends up being a relatively small amount of code and configuration required, if you don\'t know what that is, you can end up somewhat stuck. This should hopefully unstick you.\\n\\n![title image reading \\"Graph API: getting users AD group names and ids with the C# SDK\\" with the Azure Graph and C# logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Azure AD app registration API permissions\\n\\nTo query the Graph API, we\'ll need:\\n\\n- an Azure AD app registration\\n- a client id and client secret for that app registration\\n- the tenant id for the Azure AD tenant that the app registration is in\\n- `GroupMember.Read.All` and `User.Read.All` API application permissions\\n\\nOf the above, it\'s the API permissions that I want to draw your attention to. Ultimately, you\'ll need to get your Azure AD admin to grant consent to these permissions for your app registration so you have the necessary permissions to query the Graph API:\\n\\n![screenshot of the Azure Portal showing the API permissions we require](screenshot-azure-app-registration-api-permissions.png)\\n\\nHow did I know that I needed these permissions? Great question! I found the answer in the [\\"directoryObject: getMemberGroups / group memberships for a user\\" documentation](https://learn.microsoft.com/en-us/graph/api/directoryobject-getmembergroups?view=graph-rest-1.0&tabs=http#group-memberships-for-a-user).\\n\\nThe Application `User.Read.All` and `GroupMember.Read.All` permissions are the least privileged permissions that allow you to query the Graph API for the information we need. If you\'re interested in the other permissions available, check out the [Microsoft Graph permissions reference](https://learn.microsoft.com/en-us/graph/permissions-reference).\\n\\n## Querying the Graph API\\n\\nNow we\'ve configured our app registration, we can query the Graph API. I\'m going to show you how to do this with the C# SDK. If you\'re using a different language, you\'ll need to find the equivalent SDK for your language.\\n\\n### Install the SDK\\n\\nThe first thing we need to do is install the SDK. I\'m using the [Microsoft.Graph](https://www.nuget.org/packages/Microsoft.Graph/) package. At the time of writing, the latest version is 5.35.0.\\n\\nOnce you have added it, you\'ll have an entry in your `.csproj` along these lines:\\n\\n```xml title=\\"MyApp.csproj\\"\\n<PackageReference Include=\\"Microsoft.Graph\\" Version=\\"5.35.0\\" />\\n```\\n\\nIf you\'re interested in the underlying project, you can find it as [`msgraph-sdk-dotnet` on GitHub](https://github.com/microsoftgraph/msgraph-sdk-dotnet).\\n\\n### Create a GraphServiceClient\\n\\nNext, we need to create a `GraphServiceClient`. This is the object that we\'ll use to query the Graph API. For that we\'ll need a credential which we\'ll construct using the `tenantId`, `clientId` and `clientSecret` that we got from our app registration:\\n\\n```cs\\nusing Azure.Identity;\\nusing Microsoft.Graph;\\n\\n// ...\\n\\n// The client credentials flow requires that you request the\\n// /.default scope, and pre-configure your permissions on the\\n// app registration in Azure. An administrator must grant consent\\n// to those permissions beforehand.\\nvar scopes = new[] { \\"https://graph.microsoft.com/.default\\" };\\n\\n// using Azure.Identity;\\nvar options = new ClientSecretCredentialOptions\\n{\\n    AuthorityHost = AzureAuthorityHosts.AzurePublicCloud,\\n};\\n\\n// https://learn.microsoft.com/en-us/graph/sdks/choose-authentication-providers?tabs=csharp#using-a-client-secret\\n// https://learn.microsoft.com/dotnet/api/azure.identity.clientsecretcredential\\nvar clientSecretCredential = new ClientSecretCredential(\\n    tenantId, clientId, clientSecret, options);\\n\\n_graphClient = new GraphServiceClient(clientSecretCredential, scopes);\\n```\\n\\nThe above code is based on the [\\"client credentials provider / using a client secret\\" documentation](https://learn.microsoft.com/en-us/graph/sdks/choose-authentication-providers?tabs=csharp#using-a-client-secret). It\'s possible that you might want to use a different authentication provider. If so, check out the [\\"choose authentication providers\\" documentation](https://learn.microsoft.com/en-us/graph/sdks/choose-authentication-providers?tabs=csharp). Because we\'re querying for application permissions, we need to use the client credentials provider. We could, if we wanted to, use the client certificate approach instead. I\'m not going to cover that here.\\n\\n### Query the Graph API\\n\\nNow we come to the fun part. We\'re going to query the Graph API for the groups that a user is a member of. We\'ll need to pass in the `usernameOrId` of the user that we\'re interested in. I\'m using the user\'s email address as the `usernameOrId` in my application. You might want to use the user\'s id instead. It\'s up to you.\\n\\n```cs\\nList<GroupIdDisplayName> groupIds = new();\\n\\nMicrosoft.Graph.Models.GroupCollectionResponse? response = await _graphClient.Users[usernameOrId].MemberOf.GraphGroup.GetAsync(requestConfiguration =>\\n{\\n    requestConfiguration.QueryParameters.Select = [\\"id\\", \\"displayName\\"];\\n    requestConfiguration.QueryParameters.Top = 100;\\n});\\n\\nvar pageIterator = PageIterator<\\n    Microsoft.Graph.Models.Group,\\n    Microsoft.Graph.Models.GroupCollectionResponse?\\n>.CreatePageIterator(_graphClient, response, (group) =>\\n{\\n    groupIds.Add(new GroupIdDisplayName(Id: group.Id, DisplayName: group.DisplayName));\\n    return true;\\n});\\n\\nawait pageIterator.IterateAsync();\\n```\\n\\nLet\'s unpack the code above. It does the following:\\n\\n1. Creates an empty list of `GroupIdDisplayName` objects. This is the type that I\'m using to store the group ids and display names. You can use whatever type you want.\\n2. It indexes into the `Users` collection on the `GraphServiceClient` using the `usernameOrId` that we passed in. From there we make use of `MemberOf.GraphGroup` and call `GetAsync` on that, passing in a request configuration method. This is where we specify the `Select` and `Top` query parameters. We\'re using `Select` to specify only the properties that we want to return - by default lots more data would come back than we require. We\'re using `Top` to specify the maximum number of results that we want to return. I\'m using 100 as that\'s the maximum that the Graph API will allow. We\'ll need to use a `PageIterator` to get all of the results. More on that in a moment.\\n3. Because we want to get all of the results, we need to use a `PageIterator`. We create one using the `CreatePageIterator` method on the `PageIterator` class. We pass in the `GraphServiceClient` that we created earlier, the `response` that we got from the `GetAsync` call and a delegate that will be called for each result. In our delegate, we add the `GroupIdDisplayName` to our list and return `true` to indicate that we want to continue iterating. If we wanted to stop iterating, we\'d return `false`. Finally we call `IterateAsync` on the `PageIterator` to get all of the results.\\n\\nThe `PageIterator` is somewhat confusing, in my opinion. I\'m used to paging through results, but this way of doing it did puzzle me. If you\'re interested in learning more about the `PageIterator`, check out the [\\"paging\\" documentation](https://learn.microsoft.com/en-us/graph/sdks/paging?tabs=csharp). I also found [this documentation helpful](https://github.com/microsoftgraph/msgraph-sdk-dotnet/blob/dev/docs/upgrade-to-v5.md#pageiterator).\\n\\nInitially the `PageIterator` was throwing an exception when I used it. [I found the answer to that on StackOverflow](https://stackoverflow.com/questions/75860298/graphserviceclient-pageitarator-failes-with-the-parsable-does-not-contain-a-col). It turns out that the types you specify for the `PageIterator` need to be correct for the request above; and if not you may be impaced by type mismatches at runtime. The entries that are returned from the API may be wider or simply different to what you require. I\'ve the correct types in my code now - but I mention it just in case.\\n\\n## Putting it all together\\n\\nUltimately I wrote a `GroupService` that I could use to get the groups for a user. That service looks like this:\\n\\n```cs title=\\"GroupService.cs\\"\\nusing Azure.Identity;\\nusing Microsoft.Graph;\\n\\nnamespace ZebraGptContainerApp.Services.Implementations;\\n\\npublic record GroupIdDisplayName(\\n    string? Id,\\n    string? DisplayName\\n);\\n\\npublic interface IGroupService\\n{\\n    Task<List<GroupIdDisplayName>> GetGroups(string usernameOrId);\\n}\\n\\npublic class GroupService : IGroupService\\n{\\n    private readonly GraphServiceClient _graphClient;\\n    private readonly ILogger<GroupService> _logger;\\n\\n    // new this up in program.cs\\n    public GroupService(ILogger<GroupService> logger, string tenantId, string clientId, string clientSecret)\\n    {\\n        _logger = logger;\\n\\n        // The client credentials flow requires that you request the\\n        // /.default scope, and pre-configure your permissions on the\\n        // app registration in Azure. An administrator must grant consent\\n        // to those permissions beforehand.\\n        var scopes = new[] { \\"https://graph.microsoft.com/.default\\" };\\n\\n        // using Azure.Identity;\\n        var options = new ClientSecretCredentialOptions\\n        {\\n            AuthorityHost = AzureAuthorityHosts.AzurePublicCloud,\\n        };\\n\\n        // https://learn.microsoft.com/en-us/graph/sdks/choose-authentication-providers?tabs=csharp#using-a-client-secret\\n        // https://learn.microsoft.com/dotnet/api/azure.identity.clientsecretcredential\\n        var clientSecretCredential = new ClientSecretCredential(\\n            tenantId, clientId, clientSecret, options);\\n\\n        _graphClient = new GraphServiceClient(clientSecretCredential, scopes);\\n    }\\n\\n    public async Task<List<GroupIdDisplayName>> GetGroups(string usernameOrId)\\n    {\\n        // https://stackoverflow.com/questions/75860298/graphserviceclient-pageitarator-failes-with-the-parsable-does-not-contain-a-col\\n        try\\n        {\\n            List<GroupIdDisplayName> groupIds = new();\\n\\n            Microsoft.Graph.Models.GroupCollectionResponse? response = await _graphClient.Users[usernameOrId].MemberOf.GraphGroup.GetAsync(requestConfiguration =>\\n            {\\n                requestConfiguration.QueryParameters.Select = [\\"id\\", \\"displayName\\"];\\n                requestConfiguration.QueryParameters.Top = 100;\\n            });\\n\\n            var pageIterator = PageIterator<\\n                Microsoft.Graph.Models.Group,\\n                Microsoft.Graph.Models.GroupCollectionResponse?\\n            >.CreatePageIterator(_graphClient, response, (group) =>\\n            {\\n                groupIds.Add(new GroupIdDisplayName(Id: group.Id, DisplayName: group.DisplayName));\\n                return true;\\n            });\\n\\n            await pageIterator.IterateAsync();\\n\\n            return groupIds;\\n        }\\n        catch (Exception ex)\\n        {\\n            _logger.LogError(ex, \\"Problem getting groups with usernameOrId {usernameOrId}\\", usernameOrId);\\n            throw new Exception($\\"Problem getting groups with usernameOrId {usernameOrId}\\", ex);\\n        }\\n    }\\n}\\n```\\n\\nIt is constructed in `Program.cs`:\\n\\n```cs title=\\"Program.cs\\"\\n// ...\\n\\nbuilder.Services.AddSingleton<IGroupService>(serviceProvider =>\\n    new GroupService(\\n        logger: serviceProvider.GetRequiredService<ILogger<GroupService>>(),\\n        tenantId: builder.Configuration.GetValue<string>(\\"TenantId\\")!,\\n        clientId: builder.Configuration.GetValue<string>(\\"ClientId\\")!,\\n        clientSecret: builder.Configuration.GetValue<string>(\\"ClientSecret\\")!\\n    )\\n);\\n\\n// ...\\n```\\n\\nThen I can use it in my controller:\\n\\n```cs title=\\"GroupsController.cs\\"\\nusing Microsoft.AspNetCore.Mvc;\\n\\nnamespace MyApp.Controllers;\\n\\n[ApiController]\\npublic class GroupsController : ControllerBase\\n{\\n    readonly IGroupService _groupService;\\n    readonly ILogger<MeController> _log;\\n\\n    public GroupsController(IGroupService groupService, ILogger<MeController> log)\\n    {\\n        _groupService = groupService;\\n        _log = log;\\n    }\\n\\n    [HttpGet(\\"api/groups\\")]\\n    public async Task<IActionResult> GetGroups()\\n    {\\n        string? email = User?.Identity?.Name; // email in my context\\n        try\\n        {\\n            if (string.IsNullOrEmpty(email))\\n                return Unauthorized();\\n\\n            var groups = await _groupService.GetGroups(email);\\n            return Ok(groups);\\n        }\\n        catch (Exception ex)\\n        {\\n            _log.LogError(ex, \\"Problem getting groups for {email}\\", email);\\n            return BadRequest($\\"Problem getting groups for {email}\\");\\n        }\\n    }\\n}\\n```\\n\\nWhen you browse to `/api/groups`, you\'ll get a JSON response with the group ids and display names:\\n\\n![screenshot of the JSON returned from the controller displaying ids and displayNames](screenshot-group-ids-and-display-names.png)\\n\\nAnd if you\'re anything like me, you\'ll discover that you are in _way_ more groups than you thought you were.\\n\\n## Conclusion\\n\\nThis post has demonstrated both the configuration and the code required to query the Graph API for the groups that a user is a member of. Hopefully it also provides something of a reference for acquiring different types of Graph API permissions and using the C# SDK to query the Graph API."},{"id":"migrating-azure-functions-node-js-v4-typescript","metadata":{"permalink":"/migrating-azure-functions-node-js-v4-typescript","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-10-24-migrating-azure-functions-node-js-v4-typescript/index.md","source":"@site/blog/2023-10-24-migrating-azure-functions-node-js-v4-typescript/index.md","title":"Migrating to v4 Azure Functions Node.js with TypeScript","description":"Learn how to migrate a TypeScript Azure Functions app to the v4 Node.js programming model.","date":"2023-10-24T00:00:00.000Z","tags":[{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."},{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."},{"inline":false,"label":"Azure Functions","permalink":"/tags/azure-functions","description":"The Azure Functions service."},{"inline":false,"label":"Azure Static Web Apps","permalink":"/tags/azure-static-web-apps","description":"The Azure Static Web Apps service."}],"readingTime":8.26,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"migrating-azure-functions-node-js-v4-typescript","title":"Migrating to v4 Azure Functions Node.js with TypeScript","authors":"johnnyreilly","image":"./title-image.png","tags":["typescript","azure","azure functions","azure static web apps"],"description":"Learn how to migrate a TypeScript Azure Functions app to the v4 Node.js programming model.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Graph API: getting users Active Directory group names and ids with the C# SDK","permalink":"/graph-api-ad-users-group-name-ids-csharp-sdk"},"nextItem":{"title":"Bicep: Link Azure Application Insights to Static Web Apps","permalink":"/bicep-link-azure-application-insights-to-static-web-apps"}},"content":"There\'s a new programming model available for Node.js Azure Functions known as v4. There\'s documentation out there for [how to migrate JavaScript Azure Functions from v3 to v4](https://learn.microsoft.com/en-us/azure/azure-functions/functions-node-upgrade-v4?tabs=v4), but at the time of writing, TypeScript wasn\'t covered.\\n\\nThis post fills in the gaps for a TypeScript Azure Function. It\'s probably worth mentioning that [my blog](https://johnnyreilly.com) is an [Azure Static Web App](https://learn.microsoft.com/en-us/azure/static-web-apps/overview) with a TypeScript Node.js Azure Functions back end. So, this post is based on my experience migrating my blog to v4.\\n\\n![title image reading \\"Link Azure Application Insights to Static Web Apps with Bicep\\" with the Bicep and Azure Static Web App logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\nI\'m going to walk through the migration of my blog from v3 to v4. This takes place in [this pull request](https://github.com/johnnyreilly/blog.johnnyreilly.com/pull/728/files). I\'ll probably cover some of the ground of the offical JavaScript upgrade docs, but I\'ll also cover some of the TypeScript specific stuff.\\n\\nThere will be two main parts to this post:\\n\\n1. Changes to make to `package.json`\\n2. Migrating a Function\\n\\nThe second part will be the bulk of the post, but the first part is important too.\\n\\n## 1. Changes to make to the `package.json`\\n\\nSo, starting with the first part, there are three changes to make to the `package.json`:\\n\\n```diff\\n  \\"dependencies\\": {\\n+    \\"@azure/functions\\": \\"^4.0.1\\",\\n  },\\n  \\"devDependencies\\": {\\n-    \\"@azure/functions\\": \\"^3.5.0\\",\\n  },\\n+  \\"main\\": \\"dist/src/functions/*/index.js\\"\\n```\\n\\n1. Update the `@azure/functions` dependency to `^4.0.1` (or later)\\n2. The `@azure/functions` dev dependency becomes a regular dependency - this is because we\'ll be using the package at runtime now - previously we just used it to get the types at build time\\n3. Add a `main` property to the `package.json` with a glob that matches the functions in your project; in my case `dist/src/functions/*/index.js` - which will be our output from the TypeScript build\\n\\nAs I took care of **3.**, I found myself changing the folder structure of my functions. Actually, this isn\'t mandatory, but it was tricky for me to come up with a glob for my current structure. So I moved things around - you may not need to. All that matters is that your glob matches the output of your build.\\n\\n## 2. Migrating a Function\\n\\nIn order that we can understand what migration looks like, we must first take a look at the v3 version of a function. Here\'s the `fallback` function from my blog:\\n\\n```ts\\nimport type { AzureFunction, Context, HttpRequest } from \'@azure/functions\';\\n\\nimport { redirect } from \'./redirect\';\\nimport { saveToDatabase } from \'./saveToDatabase\';\\n\\nconst httpTrigger: AzureFunction = async function (\\n  context: Context,\\n  req: HttpRequest,\\n): Promise<void> {\\n  try {\\n    const originalUrl = req.headers[\'x-ms-original-url\'];\\n\\n    const { status, location } = redirect(originalUrl, context.log);\\n\\n    await saveToDatabase(originalUrl, { status, location }, context.log);\\n\\n    context.res = {\\n      status,\\n      headers: {\\n        location,\\n      },\\n    };\\n  } catch (error) {\\n    context.log.error(\\n      \'Problem with fallback\',\\n      error,\\n      req.headers[\'x-ms-original-url\'],\\n    );\\n  }\\n};\\n\\nexport default httpTrigger;\\n```\\n\\nThe above is the code I use to power [dynamic redirects in my Azure Static Web App with the Azure Function back-end](../2022-12-22-azure-static-web-apps-dynamic-redirects-azure-functions/index.md). It\'s a TypeScript Azure Function that takes a request, redirects to a new location and saves metadata about the redirect to a database.\\n\\nLooking at the code now, I rather think I should have called the function `redirect` rather than `fallback`. I\'ll leave it as is for now, but I\'ll probably change it in the future.\\n\\nWhat the `fallback` function does isn\'t significant for this post, but the structure is. Now let\'s look at the migrated version:\\n\\n```ts\\nimport type {\\n  HttpRequest,\\n  HttpResponseInit,\\n  InvocationContext,\\n} from \'@azure/functions\';\\nimport { app } from \'@azure/functions\';\\n\\nimport { redirect } from \'./redirect\';\\nimport { saveToDatabase } from \'./saveToDatabase\';\\n\\nexport async function fallback(\\n  request: HttpRequest,\\n  context: InvocationContext,\\n): Promise<HttpResponseInit> {\\n  try {\\n    const originalUrl = request.headers.get(\'x-ms-original-url\') || \'\';\\n\\n    const { status, location } = redirect(originalUrl, context);\\n\\n    await saveToDatabase(originalUrl, { status, location }, context);\\n\\n    return {\\n      status,\\n      headers: {\\n        location,\\n      },\\n    };\\n  } catch (error) {\\n    context.error(\\n      \'Problem with fallback\',\\n      error,\\n      request.headers.get(\'x-ms-original-url\'),\\n    );\\n    return {\\n      status: 500,\\n      body: \'something went wrong\',\\n    };\\n  }\\n}\\n\\napp.http(\'fallback\', {\\n  methods: [\'GET\'],\\n  handler: fallback,\\n});\\n```\\n\\nAs we can see, the logic looks pretty much the same. But a lot has changed. What\'s different? We\'ll go through the changes one by one.\\n\\n### `import`s used\\n\\nStarting at the top, the `import`s we use are different:\\n\\n```diff\\n-import type { AzureFunction, Context, HttpRequest } from \'@azure/functions\';\\n+import type {\\n+  HttpRequest,\\n+  HttpResponseInit,\\n+  InvocationContext,\\n+} from \'@azure/functions\';\\n+import { app } from \'@azure/functions\';\\n```\\n\\nWe\'re no longer just importing types, we\'re importing the `app` function from `@azure/functions` also. The types that are being imported are different too. We\'re no longer importing `AzureFunction, Context, HttpRequest` - instead we\'re importing `HttpRequest, HttpResponseInit,  InvocationContext`.\\n\\n### Hello `app`, goodbye `function.json`\\n\\nAs we saw, we\'re making use of the `app` function from `@azure/functions`. This is a new function that we use to register our Azure Functions. We no longer use `function.json`. Instead we use `app`. In the case of my `fallback` Azure Functions, we register it like this:\\n\\n```ts\\napp.http(\'fallback\', {\\n  methods: [\'GET\'],\\n  handler: fallback,\\n});\\n```\\n\\nWe\'re registering an HTTP trigger called `fallback` that responds to `GET` requests. The `handler` is the function that will be called when the trigger is invoked. There\'s more options available, but this is the minimum we need to register our function.\\n\\nThis minimal TypeScript/JavaScript replaces the more verbose `function.json` that used to sit alongside:\\n\\n```json\\n{\\n  \\"bindings\\": [\\n    {\\n      \\"authLevel\\": \\"anonymous\\",\\n      \\"type\\": \\"httpTrigger\\",\\n      \\"direction\\": \\"in\\",\\n      \\"name\\": \\"req\\",\\n      \\"methods\\": [\\"get\\", \\"post\\"]\\n    },\\n    {\\n      \\"type\\": \\"http\\",\\n      \\"direction\\": \\"out\\",\\n      \\"name\\": \\"res\\"\\n    }\\n  ],\\n  \\"scriptFile\\": \\"../dist/fallback/index.js\\"\\n}\\n```\\n\\nAll of this is gone, replaced by the `app` function usage. There\'s one part of the `function.json` that isn\'t covered by the `app` function, and that\'s the `scriptFile` property. This is covered by the `main` property we added to the `package.json` earlier.\\n\\nThe rest of the `function.json` is covered by the `app.http` call. Much terser.\\n\\n### Signature and types of our `function`\\n\\nThe signature of our function has changed too:\\n\\n```diff\\n-const httpTrigger: AzureFunction = async function (\\n-  context: Context,\\n-  req: HttpRequest\\n-): Promise<void> {\\n+export async function fallback(\\n+  request: HttpRequest,\\n+  context: InvocationContext,\\n+): Promise<HttpResponseInit> {\\n```\\n\\nYou\'ll see here that we\'re using a function declaration rather than a function expression. Our new function takes our new types, the subtly different `HttpRequest` and `InvocationContext`, which are similar to, but different from, the previous `Context` and `HttpRequest` types. The order of these parameters has changed also.\\n\\nThe return type of the function is now `Promise<HttpResponseInit>` rather than `Promise<void>`. What this means is, we\'re going to return values from our function, which we didn\'t do previously. Let\'s look at the implications of this.\\n\\n### From `context.res` to `Promise<HttpResponseInit>`\\n\\nWith a v3 function, we\'d set the `context.res` property to return values from our function. With a v4 function, we return values from our function directly. What does this look like?\\n\\n```diff\\n-    context.res = {\\n-      status,\\n-      headers: {\\n-        location,\\n-      },\\n-    };\\n+    return {\\n+      status,\\n+      headers: {\\n+        location,\\n+      },\\n+    };\\n```\\n\\nI rather like this change. My reasoning is that, in the event that there is subsequent code that would otherwise run after `context.res` was set, we no longer need to remember to subsequently `return` to prevent that running. (And yes, I have made that mistake on multiple occasions.) All we need do is return the value we want to return from our function.\\n\\n### `body -> jsonBody`\\n\\nAnother difference is that we no longer set the `body` property of the `context.res`. Instead we return an object with a `jsonBody` property, assuming we\'re returning JSON from our API. (And that\'s the most common case, right?)\\n\\nThis wasn\'t illustrated in the `fallback` function above, but here\'s an example of migrating a function that returns JavaScript object literal named `redirectSummary`:\\n\\n```diff\\n-    context.res = {\\n-      status: 200,\\n-      body: redirectSummary,\\n-    };\\n+    return {\\n+      status: 200,\\n+      jsonBody: redirectSummary,\\n+    };\\n```\\n\\nThe `body` property still exists, but it\'s for returning strings and other things, rather than JSON. If you\'re returning JSON, the easiest approach is to use the `jsonBody` property.\\n\\n### Runtime APIs\\n\\nFinally, the APIs offered by the `request` and `context` objects are different. I shan\'t go into detail here as it\'s [well covered in the official documentation](https://learn.microsoft.com/en-us/azure/azure-functions/functions-node-upgrade-v4?tabs=v4#review-your-usage-of-http-types). But I will show you one of the changes I made to my `fallback` function:\\n\\n```diff\\n-    const originalUrl = req.headers[\'x-ms-original-url\'];\\n+    const originalUrl = request.headers.get(\'x-ms-original-url\') || \'\';\\n```\\n\\nNot too significant a tweak, but there\'s a number of slight changes like this to make. (Related to this, the logging API on the `context` object is also different - but not significantly.)\\n\\n## 3. Running locally\\n\\nIf you\'re a fan of running locally then you may need to make this tweak to your `host.json`:\\n\\n```diff\\n    \\"extensionBundle\\": {\\n        \\"id\\": \\"Microsoft.Azure.Functions.ExtensionBundle\\",\\n-        \\"version\\": \\"[3.*, 4.0.0)\\"\\n+        \\"version\\": \\"[4.*, 5.0.0)\\"\\n    }\\n```\\n\\nA complete `host.json` might look like this:\\n\\n```json\\n{\\n    \\"version\\": \\"2.0\\",\\n    \\"logging\\": {\\n        \\"applicationInsights\\": {\\n            \\"samplingSettings\\": {\\n                \\"isEnabled\\": true,\\n                \\"excludedTypes\\": \\"Request\\"\\n            }\\n        }\\n    },\\n    \\"extensionBundle\\": {\\n        \\"id\\": \\"Microsoft.Azure.Functions.ExtensionBundle\\",\\n        \\"version\\": \\"[4.*, 5.0.0)\\"\\n    }\\n}\\n```\\n\\nAs discussed with [Eric Jizba](https://github.com/ejizba) on [this GitHub issue](https://github.com/Azure/azure-functions-core-tools/issues/3508#issuecomment-1894356158), updating the `host.json` may not actually be necessary.  For me it seemed to be the thing that turned a not working setup into a working setup; but it\'s possible I was mistaken. Certainly if I revert the change now I\'m still able to run locally.  What I\'m saying is: your mileage may vary.\\n\\n## Conclusion\\n\\nMigrating an Azure Function from v3 to v4 with TypeScript is a little more involved than I\'d expected. But I do like that this moves us to a code style that feels more \\"Node-y\\". ~~The official documentation is good, but it\'s not complete right now.~~ There\'s now a decent upgrade guide available: https://learn.microsoft.com/en-us/azure/azure-functions/functions-node-upgrade-v4?tabs=v4\\n\\nHopefully this post will help you migrate your TypeScript Azure Functions to v4."},{"id":"bicep-link-azure-application-insights-to-static-web-apps","metadata":{"permalink":"/bicep-link-azure-application-insights-to-static-web-apps","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-10-18-bicep-link-azure-application-insights-to-static-web-apps/index.md","source":"@site/blog/2023-10-18-bicep-link-azure-application-insights-to-static-web-apps/index.md","title":"Bicep: Link Azure Application Insights to Static Web Apps","description":"Learn how to link Azure Application Insights to an Azure Static Web App using Bicep.","date":"2023-10-18T00:00:00.000Z","tags":[{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."},{"inline":false,"label":"Bicep","permalink":"/tags/bicep","description":"The Bicep language for Azure Resource Manager templates."},{"inline":false,"label":"Azure Static Web Apps","permalink":"/tags/azure-static-web-apps","description":"The Azure Static Web Apps service."}],"readingTime":2.65,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"bicep-link-azure-application-insights-to-static-web-apps","title":"Bicep: Link Azure Application Insights to Static Web Apps","authors":"johnnyreilly","image":"./title-image.png","tags":["azure","bicep","azure static web apps"],"description":"Learn how to link Azure Application Insights to an Azure Static Web App using Bicep.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Migrating to v4 Azure Functions Node.js with TypeScript","permalink":"/migrating-azure-functions-node-js-v4-typescript"},"nextItem":{"title":"Docusaurus 3: how to migrate rehype plugins","permalink":"/docusaurus-3-how-to-migrate-rehype-plugins"}},"content":"If you\'re looking into a Production issue with your Azure Static Web App, you\'ll want to be able to get to your logs as fast as possible. You can do this by linking your Static Web App to an Azure Application Insights instance. If you\'ve used the Azure Portal to create your Static Web App, the setup phase will likely have done this for you already. But if you\'re using Bicep to create your Static Web App, you\'ll need to do this yourself.\\n\\nThis post will show you how to do that using Bicep.\\n\\n![title image reading \\"Link Azure Application Insights to Static Web Apps with Bicep\\" with the Bicep and Azure Static Web App logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## A linked Azure Application Insights instance\\n\\nWhat we want to achieve can be summmed up by this screenshot:\\n\\n![screenshot of the Azure Portal displaying the App Insights tab of a Static Web App, with a linked App Insights in view](screenshot-azure-portal-open-in-application-insights.png)\\n\\nInside the Azure Portal, inside our Static Web App, we want to see the App Insights tab and we want to see a linked App Insights instance. We do. But how?\\n\\n## Bicep linking\\n\\nThe Bicep code to achieve this is pretty simple:\\n\\n```bicep title=\\"static-web-app.bicep\\"\\nvar tagsWithHiddenLinks = union({\\n  \'hidden-link: /app-insights-resource-id\': appInsightsId\\n  \'hidden-link: /app-insights-instrumentation-key\': appInsightsInstrumentationKey\\n  \'hidden-link: /app-insights-conn-string\': appInsightsConnectionString\\n}, tags)\\n\\nresource staticWebApp \'Microsoft.Web/staticSites@2022-09-01\' = {\\n  name: staticWebAppName\\n  location: location\\n  tags: tagsWithHiddenLinks // <--- here\\n  sku: {\\n    name: \'Free\'\\n    tier: \'Free\'\\n  }\\n  properties: {\\n    repositoryUrl: \'https://github.com/johnnyreilly/blog.johnnyreilly.com\'\\n    repositoryToken: repositoryToken\\n    branch: branch\\n    provider: \'GitHub\'\\n    stagingEnvironmentPolicy: \'Enabled\'\\n    allowConfigFileUpdates: true\\n    buildProperties:{\\n      skipGithubActionWorkflowGeneration: true\\n    }\\n  }\\n}\\n```\\n\\nConsider the code above; it\'s a fairly standard Bicep resource declaration for a Static Web App. The only difference is the `tags` property. We\'re using the `union` function to add three additional tags to the `tags` property that has been passed into the `static-web-app.bicep` module. These tags are the `hidden-link` tags that link the Static Web App to the App Insights instance.\\n\\nLuke Murray has a great post on [`hidden-` tags in Azure](https://luke.geek.nz/azure/hidden-tags-in-azure/) that I recommend you read if you want to know more about them. Essentially, hidden tags are tags that don\'t show up in the Azure Portal and have some metadata purpose. `hidden-link` tags are a subset of those that are used to link resources together.\\n\\n## Where would you get the values for the `hidden-link` tags?\\n\\nIn the case of the `hidden-link` tags we\'re using here, we need to know the `id`, `InstrumentationKey` and `ConnectionString` of the App Insights instance we want to link to. We can get these values from the App Insights instance itself. Because of the way Bicep works, it\'s necessary to get those in a parent module to the Static Web App module. Here\'s an example:\\n\\n```bicep\\nresource appInsightsResource \'Microsoft.Insights/components@2020-02-02\' existing = {\\n  name: appInsightsName\\n}\\n\\nmodule staticWebApp \'./static-web-app.bicep\' = {\\n  name: \'${deployment().name}-staticWebApp\'\\n  params: {\\n    // ...\\n    appInsightsId: appInsightsResource.id\\n    appInsightsConnectionString: appInsightsResource.properties.ConnectionString\\n    appInsightsInstrumentationKey: appInsightsResource.properties.InstrumentationKey\\n    // ...\\n  }\\n}\\n```\\n\\n## Summary\\n\\nWith this is place we have the linking we need to get to our logs quickly as we navigate around inside the Azure Portal. And we have it in place in a way that\'s repeatable and consistent. I hope you find this useful."},{"id":"docusaurus-3-how-to-migrate-rehype-plugins","metadata":{"permalink":"/docusaurus-3-how-to-migrate-rehype-plugins","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-10-09-docusaurus-3-how-to-migrate-rehype-plugins/index.md","source":"@site/blog/2023-10-09-docusaurus-3-how-to-migrate-rehype-plugins/index.md","title":"Docusaurus 3: how to migrate rehype plugins","description":"Learn how to migrate rehype plugins to Docusaurus 3.","date":"2023-10-09T00:00:00.000Z","tags":[{"inline":false,"label":"Docusaurus","permalink":"/tags/docusaurus","description":"The Docusaurus static site generator."}],"readingTime":12.425,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"docusaurus-3-how-to-migrate-rehype-plugins","title":"Docusaurus 3: how to migrate rehype plugins","authors":"johnnyreilly","image":"./title-image.png","tags":["docusaurus"],"description":"Learn how to migrate rehype plugins to Docusaurus 3.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Bicep: Link Azure Application Insights to Static Web Apps","permalink":"/bicep-link-azure-application-insights-to-static-web-apps"},"nextItem":{"title":"Azure Open AI: generate article metadata with TypeScript","permalink":"/azure-open-ai-generate-article-metadata-with-typescript"}},"content":"Docusaurus v3 is on the way. One of the big changes that is coming with Docusaurus 3 is MDX 3. My blog has been built with Docusaurus 2 and I have a number of rehype plugins that I use to improve the experience of the blog. These include:\\n\\n- [a plugin to improve Core Web Vitals with fetchpriority / lazy loading](../2023-01-18-docusaurus-improve-core-web-vitals-fetchpriority/index.md)\\n- [a plugin to serving Docusaurus images with Cloudinary](../2022-12-26-docusaurus-image-cloudinary-rehype-plugin/index.md)\\n\\nI wanted to migrate these plugins to Docusaurus 3. This post is about how I did that - and if you\'ve got a rehype plugin it could probably provide some guidance on the changes you\'d need to make.\\n\\n![title image reading \\"Migrating rehype plugins to Docusaurus 3\\" with the Docusaurus logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## What needs to change?\\n\\nThe Docusaurus team put out a blog post on preparing for the Docusaurus 3 migration. [Part of that post mentions MDX plugins](https://docusaurus.io/blog/preparing-your-site-for-docusaurus-v3#mdx-plugins):\\n\\n> All the official packages (Unified, Remark, Rehype...) in the MDX ecosystem are now **ES Modules** only and do not support CommonJS anymore.\\n\\nThis affects how you write your plugins. It also has a bearing on how you import your plugins, given that the Docusaurus configuration file itself is still CommonJS. The post adds:\\n\\n> If you created custom Remark or Rehype plugins, you may need to refactor those, or eventually rewrite them completely, due to how the new AST is structured.\\n\\nThis turned out to be the case for me. I had to rewrite my plugins completely. I\'ll go through each of them in turn.\\n\\n## Migrating the `fetchpriority` plugin\\n\\nThe `fetchpriority` plugin is a rehype plugin that I wrote to improve the Core Web Vitals of my blog. It does this by making the first image on a page eager loaded with `fetchpriority=\\"high\\"` and lazy loading all other images. The Docusaurus 2 / MDX 1 code looked like this:\\n\\n```js title=\\"image-fetch-priority-rehype-plugin.js\\"\\n// @ts-check\\nconst visit = require(\'unist-util-visit\');\\n\\n/**\\n * Create a rehype plugin that will make the first image eager loaded with fetchpriority=\\"high\\" and lazy load all other images\\n * @returns rehype plugin that will make the first image eager loaded with fetchpriority=\\"high\\" and lazy load all other images\\n */\\nfunction imageFetchPriorityRehypePlugin() {\\n  /** @type {Map<string, string>} */ const files = new Map();\\n\\n  /** @type {import(\'unified\').Transformer} */\\n  return (tree, vfile) => {\\n    visit(tree, [\'element\', \'jsx\'], (node) => {\\n      if (node.type === \'element\' && node[\'tagName\'] === \'img\') {\\n        // handles nodes like this:\\n        // {\\n        //   type: \'element\',\\n        //   tagName: \'img\',\\n        //   properties: {\\n        //     src: \'https://some.website.com/cat.gif\',\\n        //     alt: null\\n        //   },\\n        //   ...\\n        // }\\n\\n        const key = `img|${vfile.history[0]}`;\\n        const imageAlreadyProcessed = files.get(key);\\n        const fetchpriorityThisImage =\\n          !imageAlreadyProcessed ||\\n          imageAlreadyProcessed === node[\'properties\'][\'src\'];\\n\\n        if (!imageAlreadyProcessed) {\\n          files.set(key, node[\'properties\'][\'src\']);\\n        }\\n\\n        if (fetchpriorityThisImage) {\\n          node[\'properties\'].fetchpriority = \'high\';\\n          node[\'properties\'].loading = \'eager\';\\n        } else {\\n          node[\'properties\'].loading = \'lazy\';\\n        }\\n      } else if (node.type === \'jsx\' && node[\'value\']?.includes(\'<img \')) {\\n        // handles nodes like this:\\n\\n        // {\\n        //   type: \'jsx\',\\n        //   value: \'<img src={require(\\"!/workspaces/blog.johnnyreilly.com/blog-website/node_modules/url-loader/dist/cjs.js?limit=10000&name=assets/images/[name]-[hash].[ext]&fallback=/workspaces/blog.johnnyreilly.com/blog-website/node_modules/file-loader/dist/cjs.js!./bower-with-the-long-paths.png\\").default} width=\\"640\\" height=\\"497\\" />\'\\n        // }\\n\\n        // if (!vfile.history[0].includes(\'blog/2023-01-15\')) return;\\n\\n        const key = `jsx|${vfile.history[0]}`;\\n        const imageAlreadyProcessed = files.get(key);\\n        const fetchpriorityThisImage =\\n          !imageAlreadyProcessed || imageAlreadyProcessed === node[\'value\'];\\n\\n        if (!imageAlreadyProcessed) {\\n          files.set(key, node[\'value\']);\\n        }\\n\\n        if (fetchpriorityThisImage) {\\n          node[\'value\'] = node[\'value\'].replace(\\n            /<img /g,\\n            \'<img loading=\\"eager\\" fetchpriority=\\"high\\" \',\\n          );\\n        } else {\\n          node[\'value\'] = node[\'value\'].replace(\\n            /<img /g,\\n            \'<img loading=\\"lazy\\" \',\\n          );\\n        }\\n      }\\n    });\\n  };\\n}\\n\\nmodule.exports = imageFetchPriorityRehypePlugin;\\n```\\n\\nThe new plugin looks like this:\\n\\n```js title=\\"image-fetch-priority-rehype-plugin.mjs\\"\\n// @ts-check\\nimport { visit } from \'unist-util-visit\';\\n\\n/**\\n * Create a rehype plugin that will make the first image eager loaded with fetchpriority=\\"high\\" and lazy load all other images\\n * @returns rehype plugin that will make the first image eager loaded with fetchpriority=\\"high\\" and lazy load all other images\\n */\\nexport default function imageFetchPriorityRehypePlugin() {\\n  /** @type {Map<string, string>} */ const files = new Map();\\n\\n  /** @type {import(\'unified\').Transformer} */\\n  return (tree, vfile) => {\\n    visit(tree, [\'mdxJsxTextElement\'], (node) => {\\n      if (node.type === \'mdxJsxTextElement\' && node[\'name\'] === \'img\') {\\n        // handles nodes like this:\\n        // {\\n        //   type: \'mdxJsxTextElement\',\\n        //   name: \'img\',\\n        //   attributes: [\\n        //     {\\n        //       type: \'mdxJsxAttribute\',\\n        //       name: \'alt\',\\n        //       value: \'title image reading &quot;Azure Container Apps, Bicep, managed certificates and custom domains&quot; with the Azure Container App logos\'\\n        //     },\\n        //     {\\n        //       type: \'mdxJsxAttribute\',\\n        //       name: \'src\',\\n        //       value: {\\n        //         type: \'mdxJsxAttributeValueExpression\',\\n        //         value: \'require(\\"!/home/john/code/github/blog.johnnyreilly.com/blog-website/node_modules/url-loader/dist/cjs.js?limit=10000&name=assets/images/[name]-[contenthash].[ext]&fallback=/home/john/code/github/blog.johnnyreilly.com/blog-website/node_modules/file-loader/dist/cjs.js!./screenshot-azure-portal-bring-your-own-certificates.webp\\").default\',\\n        //         data: [Object]\\n        //       }\\n        //     },\\n        //     { type: \'mdxJsxAttribute\', name: \'width\', value: \'800\' },\\n        //     { type: \'mdxJsxAttribute\', name: \'height\', value: \'450\' }\\n        //   ],\\n        //   children: []\\n        // }\\n\\n        const srcIndex = node[\'attributes\'].findIndex(\\n          (attr) => attr.name === \'src\',\\n        );\\n        const requireString = node[\'attributes\'][srcIndex].value.value;\\n\\n        const key = `jsx|${vfile.history[0]}`;\\n        const imageAlreadyProcessed = files.get(key);\\n        const fetchpriorityThisImage =\\n          !imageAlreadyProcessed || imageAlreadyProcessed === requireString;\\n\\n        if (!imageAlreadyProcessed) {\\n          files.set(key, requireString);\\n        }\\n\\n        // expect to be -1\\n        const loadingIndex = node[\'attributes\'].findIndex(\\n          (attr) => attr.name === \'loading\',\\n        );\\n\\n        if (fetchpriorityThisImage) {\\n          // expect to be -1\\n          const fetchpriorityIndex = node[\'attributes\'].findIndex(\\n            (attr) => attr.name === \'fetchpriority\',\\n          );\\n          if (loadingIndex > -1) {\\n            node[\'attributes\'][loadingIndex].value = \'eager\';\\n          } else {\\n            node[\'attributes\'].push({\\n              type: \'mdxJsxAttribute\',\\n              name: \'loading\',\\n              value: \'eager\',\\n            });\\n          }\\n\\n          if (fetchpriorityIndex > -1) {\\n            node[\'attributes\'][fetchpriorityIndex].value = \'high\';\\n          } else {\\n            node[\'attributes\'].push({\\n              type: \'mdxJsxAttribute\',\\n              name: \'fetchpriority\',\\n              value: \'high\',\\n            });\\n          }\\n        } else {\\n          if (loadingIndex > -1) {\\n            node[\'attributes\'][loadingIndex].value = \'lazy\';\\n          } else {\\n            node[\'attributes\'].push({\\n              type: \'mdxJsxAttribute\',\\n              name: \'loading\',\\n              value: \'lazy\',\\n            });\\n          }\\n        }\\n      }\\n    });\\n  };\\n}\\n```\\n\\nWhat\'s different? Well, a number of things; let\'s go through them.\\n\\n### CommonJS to ES Module\\n\\nYou\'ll note the old plugin has the name `image-fetch-priority-rehype-plugin.js` and the new plugin has the name `image-fetch-priority-rehype-plugin.mjs`. This is because the new plugin is an ES Module and the old plugin is CommonJS.\\n\\nFurther to that, the old plugin used `module.exports = imageFetchPriorityRehypePlugin` to expose functionality and the new plugin uses `export default imageFetchPriorityRehypePlugin`.\\n\\n### Different AST\\n\\nThe abstract syntax tree (AST) is different. MDX 1 and MDX 3 make different ASTs and we must migrate to the new one. Interestingly, it seems to be slightly simpler in some ways. MDX 1 surfaced both `element` / `img` nodes and `jsx` nodes. By contrast, MDX 3 appears to surface just `mdxJsxTextElement` which are similar to MDX 1\'s `jsx` nodes, but come with their own AST representation of expression based attributes in the `data` property.\\n\\nThe logic of the new plugin is similar to the old plugin, but the code is different to cater for the different AST.\\n\\nAnd that\'s it - we have a new `fetchpriority` plugin that works with Docusaurus 3 and MDX 3!\\n\\n## Migrating the `cloudinary` plugin\\n\\nFirstly, let\'s remind ourselves what the `cloudinary` plugin does. It takes an image URL and transforms it into a Cloudinary URL. So like this:\\n\\n```diff\\n-https://my.website.com/cat.gif\\n+https://res.cloudinary.com/demo/image/fetch/https://my.website.com/cat.gif\\n```\\n\\nAnd at runtime, Cloudinary\'s [Fetch mechanism](https://cloudinary.com/documentation/fetch_remote_images#fetch_and_deliver_remote_files) will handle transforming the image into a format that is optimised for the browser that is requesting it.\\n\\nIt turns out that the `fetchpriority` plugin is a much more straightforward migration than the `cloudinary` plugin. And the reason for that is related to the aforementioned AST changes. Let\'s start with the old plugin:\\n\\n```js title=\\"cloudinary-rehype-plugin.js\\"\\n//@ts-check\\nconst visit = require(\'unist-util-visit\');\\n\\n/**\\n * Create a rehype plugin that will replace image URLs with Cloudinary URLs\\n * @param {*} options cloudName your Cloudinary\u2019s cloud name eg demo, baseUrl the base URL of your website eg https://johnnyreilly.com - should not include a trailing slash, will likely be the same as the config.url in your docusaurus.config.js\\n * @returns rehype plugin that will replace image URLs with Cloudinary URLs\\n */\\nfunction imageCloudinaryRehypePlugin(\\n  /** @type {{ cloudName: string; baseUrl: string }} */ options,\\n) {\\n  const { cloudName, baseUrl } = options;\\n  const srcRegex = / src={(.*)}/;\\n\\n  /** @type {import(\'unified\').Plugin<[], import(\'hast\').Root>} */\\n  return (tree) => {\\n    visit(tree, [\'element\', \'jsx\'], (node) => {\\n      if (node.type === \'element\' && node[\'tagName\'] === \'img\') {\\n        // handles nodes like this:\\n\\n        // {\\n        //   type: \'element\',\\n        //   tagName: \'img\',\\n        //   properties: {\\n        //     src: \'https://some.website.com/cat.gif\',\\n        //     alt: null\\n        //   },\\n        //   ...\\n        // }\\n\\n        const url = node[\'properties\'].src;\\n\\n        node[\\n          \'properties\'\\n        ].src = `https://res.cloudinary.com/${cloudName}/image/fetch/${url}`;\\n      } else if (node.type === \'jsx\' && node[\'value\']?.includes(\'<img \')) {\\n        // handles nodes like this:\\n\\n        // {\\n        //   type: \'jsx\',\\n        //   value: \'<img src={require(\\"!/workspaces/blog.johnnyreilly.com/blog-website/node_modules/url-loader/dist/cjs.js?limit=10000&name=assets/images/[name]-[hash].[ext]&fallback=/workspaces/blog.johnnyreilly.com/blog-website/node_modules/file-loader/dist/cjs.js!./bower-with-the-long-paths.png\\").default} width=\\"640\\" height=\\"497\\" />\'\\n        // }\\n\\n        const match = node[\'value\'].match(srcRegex);\\n        if (match) {\\n          const urlOrRequire = match[1];\\n          node[\'value\'] = node[\'value\'].replace(\\n            srcRegex,\\n            ` src={${`\\\\`https://res.cloudinary.com/${cloudName}/image/fetch/${baseUrl}\\\\$\\\\{${urlOrRequire}\\\\}\\\\``}}`,\\n          );\\n        }\\n      }\\n    });\\n  };\\n}\\n\\nmodule.exports = imageCloudinaryRehypePlugin;\\n```\\n\\nThe old plugin had two kinds of nodes it had to deal with, `element` and `jsx`. The new plugin will have to deal with just one kind of node, `mdxJsxTextElement`. (Just the same as with the `fetchpriority` plugin.)\\n\\nNow you may have noticed that the JSX node in the old plugin has a slightly more complex `src` attribute:\\n\\n```jsx\\n<img src={require(\\"!/workspaces/blog.johnnyreilly.com/blog-website/node_modules/url-loader/dist/cjs.js?limit=10000&name=assets/images/[name]-[hash].[ext]&fallback=/workspaces/blog.johnnyreilly.com/blog-website/node_modules/file-loader/dist/cjs.js!./bower-with-the-long-paths.png\\").default} width=\\"640\\" height=\\"497\\" />`\\n```\\n\\nThat `src` attribute is a JavaScript expression. It\'s not a string. It\'s a JavaScript expression that will be evaluated later by webpack, and will return the path to the image in the final (webpack-based) Docusaurus build.\\n\\nSo transformation into a Cloudinary URL for JSX nodes is a little tougher. In the MDX 1 plugin, we needed to wrap the `require` expression in backticks and prefix it with `https://res.cloudinary.com/${cloudName}/image/fetch/${baseUrl}` where `${baseUrl}` is the base URL of our website. We also need to prefix the expression with a `$` to indicate that it\'s a JavaScript expression. Tough to read but it works.\\n\\nRereading that paragraph, I realise it\'s hard to understand. Perhaps easier to see it in action. Here\'s what we want our plugin to do to the JSX node above:\\n\\n```diff\\n-require(\\"!/home/john/code/github/blog.johnnyreilly.com/blog-website/node_modules/url-loader/dist/cjs.js?limit=10000&name=assets/images/[name]-[contenthash].[ext]&fallback=/home/john/code/github/blog.johnnyreilly.com/blog-website/node_modules/file-loader/dist/cjs.js!./screenshot-azure-portal-bring-your-own-certificates.webp\\").default\\n+`https://res.cloudinary.com/demo/image/fetch/f_auto,q_auto,w_auto,dpr_auto/https://johnnyreilly.com${require(\\"!/home/john/code/github/blog.johnnyreilly.com/blog-website/node_modules/url-loader/dist/cjs.js?limit=10000&name=assets/images/[name]-[contenthash].[ext]&fallback=/home/john/code/github/blog.johnnyreilly.com/blog-website/node_modules/file-loader/dist/cjs.js!./screenshot-azure-portal-bring-your-own-certificates.webp\\").default}`\\n```\\n\\nIt turns out it\'s even tougher doing this with MDX 3 as compared to MDX 1. This is because MDX 3\'s AST includes all kinds of metadata around the `mdxJsxAttributeValueExpression`:\\n\\n```js\\n{\\n  type: \'mdxJsxAttribute\',\\n  name: \'src\',\\n  value: {\\n    type: \'mdxJsxAttributeValueExpression\',\\n    value: \'require(\\"!/home/john/code/github/blog.johnnyreilly.com/blog-website/node_modules/url-loader/dist/cjs.js?limit=10000&name=assets/images/[name]-[contenthash].[ext]&fallback=/home/john/code/github/blog.johnnyreilly.com/blog-website/node_modules/file-loader/dist/cjs.js!./screenshot-azure-portal-bring-your-own-certificates.webp\\").default\',\\n    data: [Object] // <--- There\'s a lot of metadata in here!\\n  }\\n},\\n```\\n\\nThe `data` object above is a full on AST representation of the `require` expression. And to make a plugin that works with MDX 3, we need to use that AST representation to build up the new `src` attribute. This involves some string manipulation and some AST traversal. It\'s not pretty but it works.\\n\\nHere\'s the new plugin:\\n\\n```js title=\\"cloudinary-rehype-plugin.mjs\\"\\n//@ts-check\\nimport { visit } from \'unist-util-visit\';\\nimport * as acorn from \'acorn\';\\nimport { mdxJsx } from \'micromark-extension-mdx-jsx\';\\nimport { fromMarkdown } from \'mdast-util-from-markdown\';\\nimport { mdxJsxFromMarkdown, mdxJsxToMarkdown } from \'mdast-util-mdx-jsx\';\\nimport { toMarkdown } from \'mdast-util-to-markdown\';\\n\\n/**\\n * @typedef {object} Params a label and an href\\n * @property {string} cloudName your Cloudinary\u2019s cloud name eg demo\\n * @property {string} baseUrl the base URL of your website eg https://johnnyreilly.com - should not include a trailing slash, will likely be the same as the config.url in your docusaurus.config.js\\n */\\n\\n/**\\n * Create a rehype plugin that will replace image URLs with Cloudinary URLs\\n * @param {Params} params\\n * @returns rehype plugin that will replace image URLs with Cloudinary URLs\\n */\\nexport default function imageCloudinaryRehypePlugin({ cloudName, baseUrl }) {\\n  const imageCloudinaryRehypeVisitor = imageCloudinaryRehypeVisitor({\\n    cloudName,\\n    baseUrl,\\n  });\\n  return (tree) => {\\n    // eslint-disable-next-line @typescript-eslint/no-unsafe-call\\n    visit(tree, [\'mdxJsxTextElement\'], imageCloudinaryRehypeVisitor);\\n  };\\n}\\n/**\\n * Create a rehype visitor that will replace image URLs with Cloudinary URLs - exposed for testing purposes\\n * @param {Params} params\\n * @returns rehype plugin that will replace image URLs with Cloudinary URLs\\n */\\nexport function imageCloudinaryRehypeVisitor({ cloudName, baseUrl }) {\\n  const srcRegex = / src=\\\\{(.*)\\\\}/;\\n  return function imageCloudinaryRehypeVisitor(node) {\\n    const imgWithAttributes = node;\\n    if (\\n      imgWithAttributes.type === \'mdxJsxTextElement\' &&\\n      imgWithAttributes.name === \'img\'\\n    ) {\\n      // handles nodes like this:\\n      // {\\n      //   type: \'mdxJsxTextElement\',\\n      //   name: \'img\',\\n      //   attributes: [\\n      //     {\\n      //       type: \'mdxJsxAttribute\',\\n      //       name: \'alt\',\\n      //       value: \'title image reading &quot;Azure Container Apps, Bicep, managed certificates and custom domains&quot; with the Azure Container App logos\'\\n      //     },\\n      //     {\\n      //       type: \'mdxJsxAttribute\',\\n      //       name: \'src\',\\n      //       value: {\\n      //         type: \'mdxJsxAttributeValueExpression\',\\n      //         value: \'require(\\"!/home/john/code/github/blog.johnnyreilly.com/blog-website/node_modules/url-loader/dist/cjs.js?limit=10000&name=assets/images/[name]-[contenthash].[ext]&fallback=/home/john/code/github/blog.johnnyreilly.com/blog-website/node_modules/file-loader/dist/cjs.js!./screenshot-azure-portal-bring-your-own-certificates.webp\\").default\',\\n      //         data: [Object]\\n      //       }\\n      //     },\\n      //     { type: \'mdxJsxAttribute\', name: \'width\', value: \'800\' },\\n      //     { type: \'mdxJsxAttribute\', name: \'height\', value: \'450\' }\\n      //   ],\\n      //   children: []\\n      // }\\n      const srcIndex = imgWithAttributes.attributes.findIndex(\\n        (attr) => attr.name === \'src\',\\n      );\\n      const requireAttribute = imgWithAttributes.attributes[srcIndex].value;\\n      if (typeof requireAttribute !== \'string\') {\\n        const asMarkdown = toMarkdown(imgWithAttributes, {\\n          extensions: [mdxJsxToMarkdown()],\\n        });\\n\\n        // <img\\n        //    alt=\\"screenshot of typescript playground saying &#39;ComponentThatReturnsANumber&#39; cannot be used as a JSX component. Its return type &#39;number&#39; is not a valid JSX element.(2786)\\"\\n        //    src={require(\\"!/home/john/code/github/blog.johnnyreilly.com/blog-website/node_modules/url-loader/dist/cjs.js?limit=10000&name=assets/images/[name]-[contenthash].[ext]&fallback=/home/john/code/github/blog.johnnyreilly.com/blog-website/node_modules/file-loader/dist/cjs.js!./screenshot-typescript-playground.png\\").default}\\n        //    width=\\"690\\" height=\\"298\\" />\\n\\n        const match = asMarkdown.match(srcRegex);\\n        if (match) {\\n          const urlOrRequire = match[1];\\n          const cloudinaryRequireString = `\\\\`https://res.cloudinary.com/${cloudName}/image/fetch/f_auto,q_auto,w_auto,dpr_auto/${baseUrl}\\\\$\\\\{${urlOrRequire}\\\\}\\\\``;\\n\\n          const newMarkdown = asMarkdown.replace(\\n            srcRegex,\\n            ` src={${cloudinaryRequireString}}`,\\n          );\\n\\n          // <img\\n          //    alt=\\"screenshot of typescript playground saying &#39;ComponentThatReturnsANumber&#39; cannot be used as a JSX component. Its return type &#39;number&#39; is not a valid JSX element.(2786)\\"\\n          //    src={`https://res.cloudinary.com/priou/image/fetch/f_auto,q_auto,w_auto,dpr_auto/https://johnnyreilly.com${require(\\"!/home/john/code/github/blog.johnnyreilly.com/blog-website/node_modules/url-loader/dist/cjs.js?limit=10000&name=assets/images/[name]-[contenthash].[ext]&fallback=/home/john/code/github/blog.johnnyreilly.com/blog-website/node_modules/file-loader/dist/cjs.js!./screenshot-typescript-playground.png\\").default}`}\\n          //    width=\\"690\\" height=\\"298\\" />\\n          const tree = fromMarkdown(newMarkdown, {\\n            extensions: [mdxJsx({ acorn, addResult: true })],\\n            mdastExtensions: [mdxJsxFromMarkdown()],\\n          });\\n\\n          const newSrcAttributeIndex = tree.children[0][\'attributes\'].findIndex(\\n            (attr) => attr.name === \'src\',\\n          );\\n\\n          if (newSrcAttributeIndex !== -1) {\\n            imgWithAttributes.attributes[srcIndex] =\\n              tree.children[0][\'attributes\'][newSrcAttributeIndex];\\n          }\\n        }\\n      }\\n    }\\n  };\\n}\\n```\\n\\nMuch is happening here. Let\'s go through it.\\n\\n### CommonJS to ES Module\\n\\nThis amounts to the same changes as the `fetchpriority` plugin. The old plugin has the name `cloudinary-rehype-plugin.js` and the new plugin has the name `cloudinary-rehype-plugin.mjs`. This is because the new plugin is an ES Module and the old plugin is CommonJS. Related to this, the old plugin used `module.exports = imageCloudinaryRehypePlugin` to expose functionality and the new plugin uses `export default imageCloudinaryRehypePlugin`.\\n\\n### Different AST\\n\\nWe\'re dealing with a different AST and just need to tackle the `mdxJsxTextElement` which are similar to MDX 1\'s `jsx` nodes, but come with their own AST representation of expression based attributes in the `data` property.\\n\\nThe hardest part of this (and it is hard / confusing) is dealing with the `require` expression in the `src` attribute. What we do is:\\n\\n1. Convert the `mdxJsxTextElement` to back to markdown - this is the full `img` element in its AST form\\n2. Use a regex to find the `require` expression in the `src` attribute of the markdown\\n3. Transform the `require` expression to a Cloudinary URL using the same mechanism as with the MDX 1 plugin\\n4. Convert the markdown back to an `mdxJsxTextElement` using a technique adapted from [`mdast-util-mdx-jsx`](https://github.com/syntax-tree/mdast-util-mdx-jsx#use)\\n5. Replace the `src` attribute with the new `src` attribute including the updated `require` expression AST in the `mdxJsxAttributeValueExpression` attributes `data` property.\\n\\nIf you were to compare the MDX 1 plugin with the MDX 3 plugin, 2 and 3 from the above points are the same. Points 1, 4 and 5 are new.\\n\\nWith this in place we have a new plugin that works with Docusaurus 3 and MDX 3!\\n\\n## `rehype-cloudinary-docusaurus@2`\\n\\nYou may recall that I published an npm package named [`rehype-cloudinary-docusaurus`](https://www.npmjs.com/package/rehype-cloudinary-docusaurus) which packages up the plugin to make it easy for people to use. I\'ve updated that package to use the new plugin and it is available now. You can see the [pull request here](https://github.com/johnnyreilly/rehype-cloudinary-docusaurus/pull/9). The new version is `3.0.0`."},{"id":"azure-open-ai-generate-article-metadata-with-typescript","metadata":{"permalink":"/azure-open-ai-generate-article-metadata-with-typescript","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-09-25-azure-open-ai-generate-article-metadata-with-typescript/index.md","source":"@site/blog/2023-09-25-azure-open-ai-generate-article-metadata-with-typescript/index.md","title":"Azure Open AI: generate article metadata with TypeScript","description":"Use the TypeScript Azure Open AI SDK to generate article metadata.","date":"2023-09-25T00:00:00.000Z","tags":[{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."},{"inline":false,"label":"AI","permalink":"/tags/ai","description":"All things AI - Artificial Intelligence, Large Language Models and the like."}],"readingTime":9.715,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-open-ai-generate-article-metadata-with-typescript","title":"Azure Open AI: generate article metadata with TypeScript","authors":"johnnyreilly","image":"./title-image.png","tags":["azure","typescript","ai"],"description":"Use the TypeScript Azure Open AI SDK to generate article metadata.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Docusaurus 3: how to migrate rehype plugins","permalink":"/docusaurus-3-how-to-migrate-rehype-plugins"},"nextItem":{"title":"TypeScript: The Movie","permalink":"/typescript-documentary"}},"content":"This post grew out of my desire to improve the metadata for my blog posts. I have been blogging for more than ten years, and the majority of my posts lack descriptions. A description is meta tag that sits in a page and describes the contents of the page. This is what this posts description meta tag looks like in HTML:\\n\\n```html\\n<meta\\n  name=\\"description\\"\\n  content=\\"Use the TypeScript Azure Open AI SDK to generate article metadata.\\"\\n/>\\n```\\n\\nDescriptions are important for search engine optimisation (SEO) and for accessibility. You can [read up more on the topic here](https://developers.google.com/search/docs/appearance/snippet). I wanted to have descriptions for _all_ my blog posts. But writing around 230 descriptions for my existing posts was not something I wanted to do manually. I wanted to automate it.\\n\\n![title image reading \\"Azure Open AI: generate article metadata with TypeScript\\" with the Azure Open AI / TypeScript logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## TypeScript Azure Open AI SDK\\n\\nI\'ve been using Azure Open AI for a while now, and I\'ve been using the [TypeScript SDK in the `@azure/openai` package](https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/openai/openai/README.md) to interact with it. What I wanted to do, was to use the SDK to generate descriptions for my blog posts based on the content. Since my blog is powered by Docusaurus and each post is a Markdown file, I had easy access to a individual files that could be summarised.\\n\\n## What I wanted to do\\n\\nThe plan was, to build a script to do the following:\\n\\n- read all of my blog posts without descriptions\\n- for each one, generate a description using Azure Open AI\\n- write the description to the Markdown file\\n\\nI wanted to use [Bun](https://bun.sh/) for this as it supports TypeScript by default. Using Node.js would equally be possible; but it wouldn\'t have been so easy to use TypeScript.\\n\\n## Reading the blog posts\\n\\nI started off by creating a new Bun project:\\n\\n```bash\\nmkdir open-ai-description\\ncd open-ai-description\\nbun init\\n```\\n\\nThen adding the various packages we needed, including the Azure Open AI one:\\n\\n```bash\\nbun add @azure/openai @azure/identity\\nbun add --dev bun-types typescript\\n```\\n\\nI then created an `index.ts` file and added the following code:\\n\\n```typescript\\nimport fs from \'fs\';\\nimport path from \'path\';\\nimport { produceSummary } from \'./summarizer\';\\n\\nconst docusaurusDirectory = \'../blog-website\';\\n\\nasync function getBlogDirsOrderedDescending() {\\n  const rootBlogPath = path.resolve(docusaurusDirectory, \'blog\');\\n  const blogDirs = (await fs.promises.readdir(rootBlogPath))\\n    .filter((file) => fs.statSync(path.join(rootBlogPath, file)).isDirectory())\\n    .map((file) => path.join(rootBlogPath, file));\\n\\n  blogDirs.sort().reverse();\\n\\n  return blogDirs;\\n}\\n\\ninterface BlogPost {\\n  path: string;\\n  content: string;\\n}\\n\\ninterface BlogPostWithDescription extends BlogPost {\\n  description: string;\\n}\\n\\nasync function generatePostsWithDescription() {\\n  const blogDirs = await getBlogDirsOrderedDescending();\\n\\n  const postsWithoutDescription: BlogPost[] = [];\\n\\n  for (const blogDir of blogDirs) {\\n    console.log(`Processing ${blogDir}`);\\n\\n    const indexMdPath = path.join(blogDir, \'index.md\');\\n    const blogPostContent = await fs.promises.readFile(indexMdPath, \'utf-8\');\\n\\n    const frontMatter = blogPostContent.split(\'---\')[1];\\n    const hasDescription = frontMatter.includes(\'\\\\ndescription: \');\\n    if (!hasDescription) {\\n      postsWithoutDescription.push({\\n        path: indexMdPath,\\n        content: blogPostContent,\\n      });\\n    }\\n  }\\n\\n  console.log(\\n    `Found ${postsWithoutDescription.length} posts without description`,\\n  );\\n\\n  const postsWithDescription: BlogPostWithDescription[] = [];\\n\\n  for (const post of postsWithoutDescription) {\\n    const [, frontmatter, article] = post.content.split(\'---\');\\n\\n    console.log(\\n      `** Generating description for ${post.path\\n        .replace(\'/index.md\', \'\')\\n        .split(\'/\')\\n        .pop()}`,\\n    );\\n    const description = await produceSummary(article);\\n\\n    if (description) {\\n      postsWithDescription.push({ ...post, description });\\n      console.log(`** description: ${description}`);\\n\\n      await fs.promises.writeFile(\\n        post.path,\\n        `---${frontmatter}description: \'${description.replaceAll(\\"\'\\", \'\')}\'\\n---${article}`,\\n      );\\n    } else {\\n      console.log(`** no description generated`);\\n    }\\n  }\\n\\n  return postsWithDescription;\\n}\\n\\nasync function main() {\\n  const startedAt = new Date();\\n\\n  const postsWithDescription: BlogPostWithDescription[] =\\n    await generatePostsWithDescription();\\n\\n  const finishedAt = new Date();\\n  const duration = (finishedAt.getTime() - startedAt.getTime()) / 1000;\\n  console.log(\\n    `${postsWithDescription.length} summaries generated in ${duration} seconds`,\\n  );\\n}\\n\\nawait main();\\n```\\n\\nThe code above does the following:\\n\\n- reads all of the blog posts in my blog; they\'re all directories in the `blog` directory with an `index.md` underneath so it\'s pretty easy\\n- for each post, it checks to see if there is a description in the front matter ([front matter is a metadata section at the top of the Markdown file](https://docusaurus.io/docs/markdown-features#front-matter))\\n- if there is no description, it adds the post to a list of posts without descriptions\\n- it then loops through the posts without descriptions and generates a description for each one using the `produceSummary` function - more on that in a moment\\n\\n## Generating the descriptions\\n\\nSo far, so text wrangling. Let\'s look at the `produceSummary` function in the `summarizer.ts` file:\\n\\n```typescript\\nimport { OpenAIClient } from \'@azure/openai\';\\nimport { AzureCliCredential } from \'@azure/identity\';\\n\\n// Make sure you have az login\'d and have the correct subscription selected\\n// debug with:\\n// bun --inspect-brk open-ai-description/index.ts\\n// or:\\n// cd open-ai-description\\n// bun --inspect-brk index.ts\\n\\ninterface OpenAiClientAndDeploymentName {\\n  openAIClient: OpenAIClient;\\n  deploymentName: string;\\n}\\n\\nlet openAiClientAndDeploymentName: OpenAiClientAndDeploymentName | undefined;\\n\\nfunction getClientAndDeploymentName(): OpenAiClientAndDeploymentName {\\n  if (openAiClientAndDeploymentName) {\\n    return openAiClientAndDeploymentName;\\n  }\\n\\n  // You will need to set these environment variables or edit the following values\\n  const endpoint = process.env[\'ENDPOINT\'];\\n\\n  if (!endpoint) {\\n    throw new Error(\\n      \'Missing ENDPOINT environment variable with a value like https://<resource name>.openai.azure.com\',\\n    );\\n  }\\n\\n  // This will use the Azure CLI\'s currently logged in user;\\n  // make sure you\'ve done `az login` and have the correct subscription selected\\n  const credential = new AzureCliCredential();\\n  const openAIClient = new OpenAIClient(endpoint, credential);\\n  const deploymentName = \'OpenAi-gpt-35-turbo\';\\n\\n  openAiClientAndDeploymentName = { openAIClient, deploymentName };\\n\\n  return openAiClientAndDeploymentName;\\n}\\n\\nexport async function produceSummary(article: string): Promise<string> {\\n  const { openAIClient, deploymentName } = getClientAndDeploymentName();\\n  const minChars = 120;\\n  const maxChars = 156;\\n\\n  const messages = [\\n    {\\n      role: \'system\',\\n      content: `You are a summarizer. You will be given the text of an article and will produce a summary / meta description which summarizes the article. The summary / meta descriptions you produce must be between ${minChars} and ${maxChars} characters long. If they are longer or shorter than that they cannot be used. Avoid using the \\\\`\'\\\\` character as it is not supported by the blog website - you may use the \\\\`\u2019\\\\` character instead. Do not use the expression \\"the author\\" or \\"the writer\\" in your summary.`,\\n    },\\n    {\\n      role: \'user\',\\n      content: `Here is an article to summarize:\\n\\n${article}`,\\n    },\\n  ];\\n\\n  let attempts = 0;\\n  const maxAttempts = 10;\\n  let summary = \'\';\\n  while (attempts++ < maxAttempts) {\\n    console.log(`Attempt ${attempts} of ${maxAttempts}`);\\n\\n    // This will use the Azure CLI\'s currently logged in user;\\n    // make sure you\'ve done `az login` and have the correct subscription selected\\n    const result = await openAIClient.getChatCompletions(\\n      deploymentName,\\n      messages,\\n      {\\n        temperature: 0.9,\\n      },\\n    );\\n\\n    for (const choice of result.choices) {\\n      summary = choice.message?.content || \'\';\\n\\n      if (summary.length >= minChars && summary.length <= maxChars) {\\n        return summary;\\n      }\\n      console.log(`Summary was ${summary.length} characters long; no good`);\\n    }\\n\\n    messages.push(\\n      {\\n        role: \'assistant\',\\n        content: summary,\\n      },\\n      {\\n        role: \'user\',\\n        content:\\n          summary.length < minChars\\n            ? `Too short; try again please - we require a summary that is between ${minChars} and ${maxChars} characters long.`\\n            : `Too long; try again please - we require a summary that is between ${minChars} and ${maxChars} characters long.`,\\n      },\\n    );\\n\\n    console.log(messages);\\n\\n    await sleep(500);\\n  }\\n\\n  console.log(`Failed to produce a summary in ${maxAttempts} attempts`);\\n  return \'\';\\n}\\n\\nfunction sleep(ms: number) {\\n  return new Promise((resolve) => setTimeout(resolve, ms));\\n}\\n```\\n\\nThere\'s a lot going on here, so let\'s break it down.\\n\\n### Authentication\\n\\nOftentimes the fiddliest part of using Azure Open AI is authentication. In this case, I\'m using the Azure CLI credential. So to run this you need to authenticate with the Azure CLI with `az login`. (Remember to make sure you have the correct subscription selected. You can check this by running `az account show` and checking that the `isDefault` property is set to `true`.)\\n\\nOnce you\'re logged in, this code will use the currently logged in user to authenticate with Azure Open AI.\\n\\n```typescript\\nconst credential = new AzureCliCredential();\\nconst openAIClient = new OpenAIClient(endpoint, credential);\\nconst deploymentName = \'OpenAi-gpt-35-turbo\';\\n```\\n\\nYou need to set the `endpoint` variable to the endpoint of your Azure Open AI resource. You can find this in the Azure Portal by going to your Azure Open AI resource. Look for something like `https://<resource name>.openai.azure.com`. You\'ll also need to get the name of your deployment. In my case this is `OpenAi-gpt-35-turbo`.\\n\\n### Producing the summary\\n\\nOnce you\'ve authenticated and got the client you can start to summarise. The first thing to do is provide a system message to prime the model with context on what we\'re trying to do. As part of writing a good description, there\'s a sweet spot to hit in terms of length; too short and it\'s not useful, too long and it gets truncated. So we\'re going to aim for between 120 and 156 characters. We\'re also going to encourage the AI to avoid certain wording constructs and also avoid using the `\'` character as it upsets the front matter.\\n\\nOnce primed, we hand over the blog content to the AI and ask it to produce a summary.\\n\\n```typescript\\nconst minChars = 120;\\nconst maxChars = 156;\\n\\nconst messages = [\\n  {\\n    role: \'system\',\\n    content: `You are a summarizer. You will be given the text of an article and will produce a summary / meta description which summarizes the article. The summary / meta descriptions you produce must be between ${minChars} and ${maxChars} characters long. If they are longer or shorter than that they cannot be used. Avoid using the \\\\`\'\\\\` character as it is not supported by the blog website - you may use the \\\\`\u2019\\\\` character instead. Do not use the expression \\"the author\\" or \\"the writer\\" in your summary.`,\\n  },\\n  {\\n    role: \'user\',\\n    content: `Here is an article to summarize:\\n\\n${article}`,\\n  },\\n];\\n```\\n\\nThe way we interact with the Azure Open AI is with the `getChatCompletions` method, which is effectively a strongly typed wrapper for the [chat-completions endpoint in Azure](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#chat-completions).\\n\\n```typescript\\nlet attempts = 0;\\nconst maxAttempts = 10;\\nlet summary = \'\';\\nwhile (attempts++ < maxAttempts) {\\n  console.log(`Attempt ${attempts} of ${maxAttempts}`);\\n\\n  // This will use the Azure CLI\'s currently logged in user;\\n  // make sure you\'ve done `az login` and have the correct subscription selected\\n  const result = await openAIClient.getChatCompletions(\\n    deploymentName,\\n    messages,\\n    {\\n      temperature: 0.9, // the closer to 1, the more creative the AI will be\\n    },\\n  );\\n\\n  for (const choice of result.choices) {\\n    summary = choice.message?.content || \'\';\\n\\n    if (summary.length >= minChars && summary.length <= maxChars) {\\n      return summary;\\n    }\\n    console.log(`Summary was ${summary.length} characters long; no good`);\\n  }\\n\\n  messages.push(\\n    {\\n      role: \'assistant\',\\n      content: summary,\\n    },\\n    {\\n      role: \'user\',\\n      content:\\n        summary.length < minChars\\n          ? `Too short; try again please - we require a summary that is between ${minChars} and ${maxChars} characters long.`\\n          : `Too long; try again please - we require a summary that is between ${minChars} and ${maxChars} characters long.`,\\n    },\\n  );\\n\\n  console.log(messages);\\n}\\n\\nconsole.log(`Failed to produce a summary in ${maxAttempts} attempts`);\\nreturn \'\';\\n```\\n\\nWhat\'s quite interesting, is that you really can\'t rely on the AI do what you ask it to do. It _may_ create a description of an appropriate length. It may not. So we need to check what it gives us, and if it doesn\'t satisfy our needs, then we ask it to try again. We\'ll give it a maximum of 10 attempts per post, as surprisingly, every now and then it struggles to meet the brief and infinite loops are to be avoided.\\n\\n## Running the script\\n\\nWhen the script ran (after I\'d `az login`-ed) it produced descriptions for all my blog posts. I reviewed each summary and tweaked them where necessary. If I really didn\'t like a description I\'d delete and run the script again. In the end I had descriptions for all my blog posts that I was pretty happy with. If you take a look at [this giant PR](https://github.com/johnnyreilly/blog.johnnyreilly.com/pull/656) you can see them all landing.\\n\\nHopefully this post provides a useful example of how to use the TypeScript Azure Open AI SDK to generate article metadata. You can see the raw code [here](https://github.com/johnnyreilly/blog.johnnyreilly.com/tree/main/open-ai-description)."},{"id":"typescript-documentary","metadata":{"permalink":"/typescript-documentary","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-09-20-typescript-documentary/index.md","source":"@site/blog/2023-09-20-typescript-documentary/index.md","title":"TypeScript: The Movie","description":"A documentary has been made about TypeScript and I feature in the story of how it came to be what it is today.","date":"2023-09-20T00:00:00.000Z","tags":[{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":0.495,"hasTruncateMarker":false,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"typescript-documentary","title":"TypeScript: The Movie","authors":"johnnyreilly","image":"./typescript-the-movie.webp","tags":["typescript"],"description":"A documentary has been made about TypeScript and I feature in the story of how it came to be what it is today.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Azure Open AI: generate article metadata with TypeScript","permalink":"/azure-open-ai-generate-article-metadata-with-typescript"},"nextItem":{"title":"Azure Open AI: handling capacity and quota limits with Bicep","permalink":"/azure-open-ai-capacity-quota-bicep"}},"content":"I am excited to announce that a documentary has been made about TypeScript! It premiered on YouTube at 5pm British Summertime September 21st 2023.\\n\\n<iframe src=\\"https://www.youtube.com/embed/U6s2pdxebSo?si=7X3eRhmSXLUnlGr5\\" title=\\"TypeScript Origins documentary\\" frameBorder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" allowFullScreen></iframe>\\n\\nI had the good fortune to be involved in the making of the documentary. In part this was thanks to my work on Definitely Typed. Another reason was my work on recording [the history of DefinitelyTyped](../2019-10-08-definitely-typed-the-movie/index.md).\\n\\nYou can [see it on YouTube here](https://youtu.be/U6s2pdxebSo?si=Bw6scRpzmmSw5J2j) or hit play on the embedded video above. Thanks to the [Keyboard Stories](https://www.keyboard-stories.com/) team for making this happen!"},{"id":"azure-open-ai-capacity-quota-bicep","metadata":{"permalink":"/azure-open-ai-capacity-quota-bicep","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-08-17-azure-open-ai-capacity-quota-bicep/index.md","source":"@site/blog/2023-08-17-azure-open-ai-capacity-quota-bicep/index.md","title":"Azure Open AI: handling capacity and quota limits with Bicep","description":"This post details how to control the capacity of an Azure Open AI deployment with Bicep so that you don\'t exceed your quota.","date":"2023-08-17T00:00:00.000Z","tags":[{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."},{"inline":false,"label":"Bicep","permalink":"/tags/bicep","description":"The Bicep language for Azure Resource Manager templates."},{"inline":false,"label":"AI","permalink":"/tags/ai","description":"All things AI - Artificial Intelligence, Large Language Models and the like."}],"readingTime":3.445,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-open-ai-capacity-quota-bicep","title":"Azure Open AI: handling capacity and quota limits with Bicep","authors":"johnnyreilly","image":"./title-image.png","tags":["azure","bicep","ai"],"description":"This post details how to control the capacity of an Azure Open AI deployment with Bicep so that you don\'t exceed your quota.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"TypeScript: The Movie","permalink":"/typescript-documentary"},"nextItem":{"title":"Azure Pipelines meet Vitest","permalink":"/azure-pipelines-meet-vitest"}},"content":"We\'re currently in the gold rush period of AI. The world cannot get enough. A consequence of this, is that rationing is in force. It\'s like the end of the second world war, but with GPUs. This is a good thing, because it means that we can\'t just spin up as many resources as we like. It\'s a bad thing, for the exact same reason.\\n\\nIf you\'re making use of Azure\'s Open AI resources for your AI needs, you\'ll be aware that there are [limits known as \\"quotas\\"](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/quota?tabs=bicep) in place. If you\'re looking to control how many resources you\'re using, you\'ll want to be able to control the capacity of your deployments. This is possible with Bicep.\\n\\nThis post grew out of a [GitHub issue](https://github.com/Azure/bicep-types-az/issues/1660#issuecomment-1643484703) around the topic where people were bumping on the message `the capacity should be null for standard deployment` as they attempted to deploy. At the time that issue was raised, there was very little documentation on how to handle this. Since then, things have improved, but I thought it would be useful to have a post on the topic.\\n\\n![title image reading \\"Azure Open AI: handling capacity and quota limits with Bicep\\" with the Azure Open AI / Bicep logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Viewing capacity and quota limits in the Azure Open AI Studio\\n\\nIf you take a look at the [Azure Open AI Studio](https://oai.azure.com/) you\'ll notice a \\"Quotas\\" section:\\n\\n![screenshot of azure open ai studio with quotas highlighted](./screenshot-azure-ai-studio.webp)\\n\\nYou\'ll see above that we\'ve got two deployments of GPT-35-Turbo in our subscription. Both of these contribute towards an overall limit of 360K TPM. If we try and deploy resources and have an overall capacity total that exceeds that, our deployment will fail.\\n\\nThat being the case, we need to be able to control the capacity of our deployments. This is possible with Bicep.\\n\\n## Controlling capacity and quota limits with Bicep\\n\\nConsider the following `account-deployments.bicep`:\\n\\n```bicep\\n@description(\'Name of the Cognitive Services resource\')\\nparam cognitiveServicesName string\\n\\n@description(\'Name of the deployment resource.\')\\nparam deploymentName string\\n\\n@description(\'Deployment model format.\')\\nparam format string\\n\\n@description(\'Deployment model name.\')\\nparam name string\\n\\n@description(\'Deployment model version.\')\\nparam version string = \'1\'\\n\\n@description(\'The name of RAI policy.\')\\nparam raiPolicyName string = \'Default\'\\n\\n@allowed([\\n  \'NoAutoUpgrade\'\\n  \'OnceCurrentVersionExpired\'\\n  \'OnceNewDefaultVersionAvailable\'\\n])\\n@description(\'Deployment model version upgrade option. see https://learn.microsoft.com/en-us/azure/templates/microsoft.cognitiveservices/2023-05-01/accounts/deployments?pivots=deployment-language-bicep#deploymentproperties\')\\nparam versionUpgradeOption string = \'OnceNewDefaultVersionAvailable\'\\n\\n@description(\'\'\'Deployments SKU see: https://learn.microsoft.com/en-us/azure/templates/microsoft.cognitiveservices/2023-05-01/accounts/deployments?pivots=deployment-language-bicep#sku\\neg:\\n\\nsku: {\\n  name: \'Standard\'\\n  capacity: 10\\n}\\n\\n\'\'\')\\nparam sku object\\n\\n// https://learn.microsoft.com/en-us/azure/templates/microsoft.cognitiveservices/2023-05-01/accounts?pivots=deployment-language-bicep\\nresource cog \'Microsoft.CognitiveServices/accounts@2023-05-01\' existing = {\\n  name: cognitiveServicesName\\n}\\n\\n// https://learn.microsoft.com/en-us/azure/templates/microsoft.cognitiveservices/2023-05-01/accounts/deployments?pivots=deployment-language-bicep\\nresource deployment \'Microsoft.CognitiveServices/accounts/deployments@2023-05-01\' = {\\n  name: deploymentName\\n  parent: cog\\n  sku: sku\\n  properties: {\\n    model: {\\n      format: format\\n      name: name\\n      version: version\\n    }\\n    raiPolicyName: raiPolicyName\\n    versionUpgradeOption: versionUpgradeOption\\n  }\\n}\\n\\noutput deploymentName string = deployment.name\\noutput deploymentResourceId string = deployment.id\\n```\\n\\nWe can use this to deploy.... deployments (naming here is definitely confusing) to Azure like so:\\n\\n```bicep\\nvar cognitiveServicesDeployments = [\\n  {\\n    name: \'OpenAi-gpt-35-turbo\'\\n    shortName: \'gpt35t\'\\n    model: {\\n      format: \'OpenAI\'\\n      name: \'gpt-35-turbo\'\\n      version: \'0301\'\\n    }\\n    sku: {\\n      name: \'Standard\'\\n      capacity: repositoryBranch == \'refs/heads/main\' ? 100 : 10 // capacity in thousands of TPM\\n    }\\n  }\\n]\\n\\n// Model Deployment - one at a time as parallel deployments are not supported\\n@batchSize(1)\\nmodule openAiAccountsDeployments35Turbo \'account-deployments.bicep\' = [for deployment in cognitiveServicesDeployments: {\\n  name: \'${deployment.shortName}-cog-accounts-deployments\'\\n  params: {\\n    cognitiveServicesName: openAi.outputs.cognitiveServicesName\\n    deploymentName: deployment.name\\n    format: deployment.model.format\\n    name: deployment.model.name\\n    version: deployment.model.version\\n    sku: deployment.sku\\n  }\\n}]\\n```\\n\\nWe\'re currently only deploying a single account deployment in our array, but we do it this way as it\'s not unusual to deploy multiple deployments together. Notice the `sku` portion above:\\n\\n```bicep\\n    sku: {\\n      name: \'Standard\'\\n      capacity: repositoryBranch == \'refs/heads/main\' ? 100 : 10\\n    }\\n```\\n\\nHere we provision a larger `capacity` for our feature branch deployments than our `main` branch deployments. This demonstrates our own usage, whereby we deploy a smaller capacity for our feature branches so that we can test things out, but then deploy a larger capacity for our main branch deployments.\\n\\nSignificantly, we\'re controlling the capacity of our deployments. The way in which you choose to decide on the capacity of your deployments is up to you, but the above demonstrates how you can do it with Bicep and stick within your quota limits."},{"id":"azure-pipelines-meet-vitest","metadata":{"permalink":"/azure-pipelines-meet-vitest","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-08-05-azure-pipelines-meet-vitest/index.md","source":"@site/blog/2023-08-05-azure-pipelines-meet-vitest/index.md","title":"Azure Pipelines meet Vitest","description":"This post details how to integrate the test runner Vitest with Azure Pipelines.","date":"2023-08-05T00:00:00.000Z","tags":[{"inline":false,"label":"Azure Pipelines","permalink":"/tags/azure-pipelines","description":"The Azure Pipelines CI / CD service."},{"inline":false,"label":"Automated Testing","permalink":"/tags/automated-testing","description":"How to perform the automation of tests."}],"readingTime":2.665,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-pipelines-meet-vitest","title":"Azure Pipelines meet Vitest","authors":"johnnyreilly","image":"./title-image.png","tags":["azure pipelines","automated testing"],"description":"This post details how to integrate the test runner Vitest with Azure Pipelines.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Azure Open AI: handling capacity and quota limits with Bicep","permalink":"/azure-open-ai-capacity-quota-bicep"},"nextItem":{"title":"Azure Container Apps, Bicep, bring your own certificates and custom domains","permalink":"/azure-container-apps-bicep-bring-your-own-certificates-custom-domains"}},"content":"This post explains how to integrate the tremendous test runner [Vitest](https://vitest.dev/) with the continuous integration platform [Azure Pipelines](https://azure.microsoft.com/en-gb/products/devops/pipelines/). If you read [the post on integrating with Jest](../2020-12-30-azure-pipelines-meet-jest/index.md), you\'ll recognise a lot of common ground with this. Once again we want:\\n\\n1. Tests run as part of our pipeline\\n2. A failing test fails the build\\n3. Test results reported in Azure Pipelines UI\\n\\n![title image reading \\"Azure Pipelines meet Vitest\\" with the Pipelines and Vitest logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\nThis post assumes we have a Vitest project set up and an Azure Pipeline in place. Let\'s get started.\\n\\n## Tests run as part of our pipeline\\n\\nFirst of all, lets get the tests running. We\'ll crack open our `azure-pipelines.yml` file and, in the appropriate place add the following:\\n\\n```yml\\n- bash: npm run test:ci\\n  displayName: \'npm test\'\\n  workingDirectory: src/client-app\\n```\\n\\nThe above will, when run, trigger a `npm run test:ci` in the `src/client-app` folder of the project (it\'s here where the app lives). What does `test:ci` do? Well, it\'s a script in the `package.json` that looks like this:\\n\\n```json\\n\\"test\\": \\"vitest\\",\\n\\"test:ci\\": \\"vitest run --reporter=default --reporter=junit --outputFile=reports/junit.xml\\",\\n```\\n\\nYou\'ll note above we\'ve got 2 scripts; `test` and `test:ci`. The former is the default script that Vitest will run when you run `npm test`. The latter is the script that we\'ll use in our pipeline. The difference between the two is that the `test:ci` script will:\\n\\n1. Doesn\'t run in watch mode\\n2. Fail the build if any tests fail\\n3. Produce a JUnit XML report which details test results. This is the format that Azure Pipelines can use to ingest test results.\\n\\nThe test results are written to `reports/junit.xml` which is a path relative to the `src/client-app` folder. Because you may test this locally, it\'s probably worth adding the `reports` folder to your `.gitignore` file to avoid it accidentally being committed.\\n\\n## Report test results in Azure Pipelines UI\\n\\nOur tests are running, but we\'re not seeing the results in the Azure Pipelines UI. For that we need the [`PublishTestResults` task](https://learn.microsoft.com/en-us/azure/devops/pipelines/tasks/reference/publish-test-results-v2).\\n\\nWe need to add a new step to our `azure-pipelines.yml` file _after_ our `npm run test:ci` step:\\n\\n```yml\\n- task: PublishTestResults@2\\n  displayName: \'supply npm test results to pipelines\'\\n  condition: succeededOrFailed() # because otherwise we won\'t know what tests failed\\n  inputs:\\n    testResultsFiles: \'src/client-app/reports/junit.xml\'\\n```\\n\\nThis will read the test results from our `src/client-app/reports/junit.xml` file and pump them into Pipelines. Do note that we\'re _always_ running this step; so if the previous step failed (as it would in the case of a failing test) we still pump out the details of what that failure was.\\n\\nAnd that\'s it! Azure Pipelines and Vitest integrated.\\n\\n![screenshot of test results published to Azure Pipelines](test-results.webp)\\n\\n## Putting it all together\\n\\nThe complete `azure-pipelines.yml` additions look like this:\\n\\n```yml\\n- bash: npm run test:ci\\n  displayName: \'npm test\'\\n  workingDirectory: src/client-app\\n\\n- task: PublishTestResults@2\\n  displayName: \'supply npm test results to pipelines\'\\n  condition: succeededOrFailed() # because otherwise we won\'t know what tests failed\\n  inputs:\\n    testResultsFiles: \'src/client-app/reports/junit.xml\'\\n```\\n\\nPlease note, there\'s nothing special about the `reports/junit.xml` file. You can change the name of the file and/or the location of the file. Just make sure you update the `testResultsFiles` value in the `PublishTestResults` task to match."},{"id":"azure-container-apps-bicep-bring-your-own-certificates-custom-domains","metadata":{"permalink":"/azure-container-apps-bicep-bring-your-own-certificates-custom-domains","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-07-20-azure-container-apps-bicep-bring-your-own-certificates-custom-domains/index.md","source":"@site/blog/2023-07-20-azure-container-apps-bicep-bring-your-own-certificates-custom-domains/index.md","title":"Azure Container Apps, Bicep, bring your own certificates and custom domains","description":"Azure Container Apps supports custom domains with \\"bring your own certificates\\" and this post demonstrates how to do it with Bicep.","date":"2023-07-20T00:00:00.000Z","tags":[{"inline":false,"label":"Azure Container Apps","permalink":"/tags/azure-container-apps","description":"The Azure Container Apps service. Effectively a managed Kubernetes service."},{"inline":false,"label":"Bicep","permalink":"/tags/bicep","description":"The Bicep language for Azure Resource Manager templates."}],"readingTime":3.665,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-container-apps-bicep-bring-your-own-certificates-custom-domains","title":"Azure Container Apps, Bicep, bring your own certificates and custom domains","authors":"johnnyreilly","tags":["azure container apps","bicep"],"image":"./title-image.png","description":"Azure Container Apps supports custom domains with \\"bring your own certificates\\" and this post demonstrates how to do it with Bicep.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Azure Pipelines meet Vitest","permalink":"/azure-pipelines-meet-vitest"},"nextItem":{"title":"TypeScript 5.1: declaring JSX element types","permalink":"/typescript-5-1-declaring-jsx-element-types"}},"content":"Azure Container Apps supports custom domains via certificates. If you\'re looking to make use of the managed certificates in Azure Container Apps using Bicep, then you might want to take a look at [this post on the topic](../2023-06-18-azure-container-apps-bicep-managed-certificates-custom-domains/index.md).\\n\\nThis post will instead look at how we can use the \\"bring your own certificates\\" approach in Azure Container Apps using Bicep. Well, as much as that is possible; there appear to be limitations in what can be achieved with Bicep at the time of writing.\\n\\n![title image reading \\"Azure Container Apps, Bicep, bring your own certificates and custom domains\\" with the Azure Container App logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\nThis post assumes you already have an Azure Container App in place and you want to bind a custom domain to it. If you don\'t have an Azure Container App in place, then you might want to take a look at [this post on the topic](../2021-12-27-azure-container-apps-build-and-deploy-with-bicep-and-github-actions/index.md).\\n\\n## Make a certificate\\n\\nSo the first thing we need to do is make a certificate. This is pretty simple, and in my case amounts to the following command:\\n\\n```shell\\nsudo openssl req -x509 -newkey rsa:4096 -sha256 -days 3650 -nodes \\\\\\n  -keyout poorclaresarundel.org.key -out poorclaresarundel.org.crt -subj \\"/CN=poorclaresarundel.org\\" \\\\\\n  -addext \\"subjectAltName=DNS:poorclaresarundel.org,DNS:www.poorclaresarundel.org,IP:8.8.8.8\\"\\nsudo chmod +r poorclaresarundel.org.key\\ncat poorclaresarundel.org.crt poorclaresarundel.org.key > poorclaresarundel.org.pem\\n```\\n\\nThis makes a certificate with a 10 year expiry date. You\'ll note the `8.8.8.8` listed as the IP address above. You should replace that with the static IP address of your Container Apps Environment. You can find that in the Azure Portal. It\'s listed on the \\"Overview\\" tab of your Azure Container App. It\'s the \\"Static IP\\" value:\\n\\n![screenshot of the Azure Portal with the static IP address highlighted](screenshot-azure-portal-static-ip-address.webp)\\n\\nYou\'ll also note the `poorclaresarundel.org` listed as the domain name above. You should replace that with your domain name. You\'ll need to do that in two places; in the `openssl` command and in the `subjectAltName` value. More on the significance of that particular domain name later.\\n\\n## Container apps environment and certificates\\n\\nNow, we\'ll crack open our Azure Container App\'s environment in the Azure Portal and navigate to the \\"Certificates\\" tab. Then we need to click on the \\"Bring your own certificates (.pfx)\\" tab, and we are presented with a screen like this:\\n\\n![Screenshot of Azure Portal on the certificates screen](screenshot-azure-portal-bring-your-own-certificates.webp)\\n\\nNote the \\"Add certificate\\" button. Use that to upload your certificate - in my case that\'s the `poorclaresarundel.org.pem` file. You\'ll need to provide the password for the certificate too. Oh and you\'ll be asked for a friendly name. We\'ll remember the friendly name you use - we\'ll need it later.\\n\\nThis is a real world example; my aunt\'s website. My aunt is Poor Clare nun and, for years, I\'ve done an average job of maintaining her [convent\'s website](https://www.poorclaresarundel.org/). If I was her, I\'d be wishing her nephew was a designer rather than an engineer. Or maybe an engineer with more of a sense what looks good. But here we are - she\'s a nun and so consequently much too nice to say that. Anyway, I digress. The point is, I\'ve got a certificate for her website and I\'m going to use it here.\\n\\n![screenshot of uploaded certificate in the Azure Portal](screenshot-azure-portal-bring-your-own-certificates-uploaded.webp)\\n\\n## Bicep for the custom domain\\n\\nI haven\'t managed to work out how one would handle the certificate upload in Bicep (and I rather suspect it is not supported). However, I have worked out how to handle the certificate in the Azure Container App once it\'s been uploaded with regards to custom domains. Find the `Microsoft.App/containerApps` resource in your Bicep and add a `customDomains` property to it. It should look something like this:\\n\\n```bicep\\nresource environment \'Microsoft.App/managedEnvironments@2022-10-01\' = {\\n  name: environmentName\\n  // ...\\n}\\n\\nresource webServiceContainerApp \'Microsoft.App/containerApps@2022-10-01\' = {\\n  name: webServiceContainerAppName\\n  tags: tags\\n  location: location\\n  properties: {\\n    // ...\\n    configuration: {\\n      // ...\\n      ingress: {\\n        // ...\\n        customDomains: [\\n          {\\n              name: \'www.poorclaresarundel.org\'\\n              // note the friendly name of \\"poorclaresarundel.org\\" forms the last segment of the id below\\n              certificateId: \'${environment.id}/certificates/poorclaresarundel.org\'\\n              bindingType: \'SniEnabled\'\\n          }\\n        ]\\n      }\\n    }\\n    // ...\\n  }\\n}\\n```\\n\\n## Conclusion\\n\\nWith the above post you should be able to deploy an Azure Container App with a custom domain and provide your own certificate, which is then referenced using Bicep. If you\'d like to see the full Bicep file, then you can find it [here](https://github.com/johnnyreilly/poorclaresarundel-aca/blob/main/infra/main.bicep).\\n\\n## Attributions\\n\\n<a href=\\"https://www.flaticon.com/free-icons/certificate\\" title=\\"certificate icons\\">Certificate icon in title image created by Freepik - Flaticon</a>"},{"id":"typescript-5-1-declaring-jsx-element-types","metadata":{"permalink":"/typescript-5-1-declaring-jsx-element-types","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-07-09-typescript-5-1-declaring-jsx-element-types/index.md","source":"@site/blog/2023-07-09-typescript-5-1-declaring-jsx-element-types/index.md","title":"TypeScript 5.1: declaring JSX element types","description":"With TypeScript 5.1, it becomes possible for libraries to control what types are used for JSX elements. This post looks at why this matters.","date":"2023-07-09T00:00:00.000Z","tags":[{"inline":false,"label":"React","permalink":"/tags/react","description":"The React library."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":5.45,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"typescript-5-1-declaring-jsx-element-types","title":"TypeScript 5.1: declaring JSX element types","authors":"johnnyreilly","tags":["react","typescript"],"image":"./title-image.png","description":"With TypeScript 5.1, it becomes possible for libraries to control what types are used for JSX elements. This post looks at why this matters.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Azure Container Apps, Bicep, bring your own certificates and custom domains","permalink":"/azure-container-apps-bicep-bring-your-own-certificates-custom-domains"},"nextItem":{"title":"Azure Container Apps, Bicep, managed certificates and custom domains","permalink":"/azure-container-apps-bicep-managed-certificates-custom-domains"}},"content":"A new feature arrives with TypeScript 5.1, [it is described as \\"Decoupled Type-Checking Between JSX Elements and JSX Tag Types\\"](https://devblogs.microsoft.com/typescript/announcing-typescript-5-1-beta/#decoupled-type-checking-between-jsx-elements-and-jsx-tag-types).\\n\\nIt\'s all about handing control of JSX type definitions to libraries. With this feature, libraries can control what types are used for JSX elements. Why does this matter? Great question! Until version 5.1, TypeScript did an imperfect job of representing what is possible with JSX. This feature allows libraries to do a better job of that, and we\'ll look into it in this post.\\n\\nIt\'s probably worth saying, that this is a complicated feature. If you don\'t understand it (and as the author of this post I\'ll confess that I had to work quite hard to understand it), **that is okay**. This is a low level feature that is only likely to be used by library / type definition authors. It\'s a primitive that will unlock possibilites for people writing JSX - but it\'s something that people will mainly feel the benefit of, without directly doing anything themselves, or necessarily noticing that things have changed for the better.\\n\\n![title image reading \\"TypeScript 5.1: declaring JSX element types\\" with the TypeScript logo](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## What\'s the problem?\\n\\nTypeScript creates a type system which sits on top of JavaScript, and provides static typing capabilities. As the language has grown more sophisticated, it has been able to get closer and closer to representing the full range of possibilities that JavaScript offers. As an example of this evolution, if you remember the early days of TypeScript, you\'ll remember a time before union types. Back then, you had to use `any` to represent a value that could be one of a number of types. That imperfect representation of JavaScript was solved with union types:\\n\\n```diff\\n-function printStringOrNumber(stringOrNumber: any) {\\n+function printStringOrNumber(stringOrNumber: string | number) {\\n    console.log(stringOrNumber);\\n}\\n```\\n\\nThe problem we\'re looking at here is in the same vein. But it specifically applies to JSX; which is widely used in libraries like React. With JSX support in TypeScript (up to and including 5.0), it was not possible to accurately represent all JSX possibilities. This is because the type of a JSX element returned from a function component was always `JSX.Element | null`. This is a type that is defined in the TypeScript compiler, and is not something that can be changed by a library author.\\n\\nHow does this play out? Well, let\'s take a look at a simple example. Let\'s say we have a function component that returns a number. We might write something like this:\\n\\n```tsx\\nfunction ComponentThatReturnsANumber() {\\n  return 42;\\n}\\n\\n<ComponentThatReturnsANumber />;\\n```\\n\\nThe above is legitimate JSX, but it is not legitimate TypeScript. The TypeScript compiler will complain:\\n\\n![screenshot of typescript playground saying \'ComponentThatReturnsANumber\' cannot be used as a JSX component. Its return type \'number\' is not a valid JSX element.(2786)](screenshot-typescript-playground.png)\\n\\n[You can see this in the TypeScript Playground](https://www.typescriptlang.org/play?#code/JYWwDg9gTgLgBAJQKYEMDG8BmUIjgIilQ3wG4AoczAVwDsNgJa4BhXSWpWmAFQAsUMZDGpRaAZwCCAOWogARkigAKAJRwA3uThwiIsXAAsAJgoBfSgB424Jl14ChSfRJlzFUOAHoAfOSA)\\n\\nThis errors because function components that return anything but `JSX.Element | null` are not allowed as element types in React according to TypeScript. However, in React, function components **can** return a `ReactNode`. That type includes `number | string | Iterable<ReactNode> | undefined` ([and will likely include `Promise<ReactNode>` in the future](https://github.com/reactjs/rfcs/pull/229)).\\n\\nAs an aside, a return value of `number` would be perfectly fine in class components - the restrictions are different there. I spoke to Sebastian about this and he said:\\n\\n> An interesting note is that before function components we did have full control. Due to `ElementClass`, class components already could return `ReactNode` at the type level. It was just function components that were missing full control (or any other component types Suspense or Profiler).\\n\\nSo this is the problem: is not possible to represent in TypeScript today what is actually possible in React (or other JSX libraries). Furthermore, what\'s returned from JSX may change over time, and TypeScript needs to be able to represent that.\\n\\n## The arrival of `JSX.ElementType`\\n\\nSebastian Silbermann [opened a pull request to TypeScript](https://github.com/microsoft/TypeScript/pull/51328). It had the title \\"RFC: Consult new JSX.ElementType for valid JSX element types\\". In that PR Sebastian explained the issue we\'ve just looked at above, and proposed a solution. The solution was to introduce a new type, `JSX.ElementType`.\\n\\nTo illustrate the what `JSX.ElementType` actually is as compared to a JSX element, consider this illustration:\\n\\n```\\n// <Component />\\n//  ^^^^^^^^^    JSX element type\\n// ^^^^^^^^^^^^^ JSX element\\n```\\n\\nThe significance of `JSX.ElementType` is that it is used to represent the type of a JSX element and **allow library authors to control what types are used for JSX elements**. This control was not available before.\\n\\nThe TypeScript pull request was merged, and so Sebastian (who helps maintain the React type definitions) exercised the new powers in [this pull request to the DefinitelyTyped repository for the React type definitions](https://github.com/DefinitelyTyped/DefinitelyTyped/pull/65135). At the time of writing, this pull request is still open, but once merged and shipped the React community will feel the benefits.\\n\\nThe changes are subtle; You can see in this pull request that `ReactElement | null` is generally replaced with `ReactNode`:\\n\\n```diff\\n     type JSXElementConstructor<P> =\\n-        | ((props: P) => ReactElement<any, any> | null)\\n+        | ((props: P) => ReactNode)\\n         | (new (props: P) => Component<any, any>);\\n```\\n\\nRemember how we mentioned earlier on that function components couldn\'t return numbers? Let\'s look at the updated tests in the PR:\\n\\n```diff\\n    const ReturnNumber = () => 0xeac1;\\n+   const FCNumber: React.FC = ReturnNumber;\\n    class RenderNumber extends React.Component {\\n        render() {\\n          return 0xeac1;\\n        }\\n    }\\n```\\n\\nWith this change, React components that return numbers are now valid JSX elements. This is because `JSX.ElementType` is now `ReactNode`, which includes numbers. Essentially this represents that new things are possible as a consequence of this change. The library and type definition author now has more control over what is possible in JSX.\\n\\nTo quote Sebastian again:\\n\\n> Now we have control over any potential component type.\\n\\nLet\'s look again at our component that produces a number again:\\n\\n```tsx\\nfunction ComponentThatReturnsANumber() {\\n  return 42;\\n}\\n\\n<ComponentThatReturnsANumber />;\\n```\\n\\nWith Sebastian\'s changes, this becomes valid TypeScript. And as React, and other JSX libraries evolve, TypeScript compatibility can also.\\n\\n## Summary\\n\\nThe TL;DR of this post is \\"TypeScript will better allow for the modelling of JSX in TypeScript 5.1\\". I\'m indebted to [Sebastian Silbermann](https://github.com/eps1lon) and [Daniel Rosenwasser](https://github.com/DanielRosenwasser) for their explanations of this feature. Thanks in particular to Sebastian for implementing this feature and for reviewing this post.\\n\\nI hope this post helps you understand the feature a little better.\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/declaring-jsx-types-typescript-5-1/)\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.logrocket.com/declaring-jsx-types-typescript-5-1/\\" />\\n</head>"},{"id":"azure-container-apps-bicep-managed-certificates-custom-domains","metadata":{"permalink":"/azure-container-apps-bicep-managed-certificates-custom-domains","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-06-18-azure-container-apps-bicep-managed-certificates-custom-domains/index.md","source":"@site/blog/2023-06-18-azure-container-apps-bicep-managed-certificates-custom-domains/index.md","title":"Azure Container Apps, Bicep, managed certificates and custom domains","description":"Azure Container Apps support managed certificates and custom domains. However, deploying them with Bicep is not straightforward. This post explains how to do it.","date":"2023-06-18T00:00:00.000Z","tags":[{"inline":false,"label":"Azure Container Apps","permalink":"/tags/azure-container-apps","description":"The Azure Container Apps service. Effectively a managed Kubernetes service."},{"inline":false,"label":"Bicep","permalink":"/tags/bicep","description":"The Bicep language for Azure Resource Manager templates."}],"readingTime":5.95,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-container-apps-bicep-managed-certificates-custom-domains","title":"Azure Container Apps, Bicep, managed certificates and custom domains","authors":"johnnyreilly","tags":["azure container apps","bicep"],"image":"./title-image.png","description":"Azure Container Apps support managed certificates and custom domains. However, deploying them with Bicep is not straightforward. This post explains how to do it.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"TypeScript 5.1: declaring JSX element types","permalink":"/typescript-5-1-declaring-jsx-element-types"},"nextItem":{"title":"Azure Container Apps, Easy Auth and .NET authentication","permalink":"/azure-container-apps-easy-auth-and-dotnet-authentication"}},"content":"Azure Container Apps support managed certificates and custom domains. However, deploying them with Bicep is not straightforward - although it is possible. It seems likely there\'s a bug in the implementation in Azure, but I\'m not sure. Either way, it\'s possible to deploy managed certificates and custom domains using Bicep. You just need to know how.\\n\\nIf, instead, you\'re looking to make use of the \\"bring your own certificates\\" approach in Azure Container Apps using Bicep, then you might want to take a look at [this post on the topic](../2023-07-20-azure-container-apps-bicep-bring-your-own-certificates-custom-domains/index.md).\\n\\nI\'ve facetiously subtitled this post \\"a three pipe(line) problem\\" because it took three Azure Pipelines to get it working. This is not Azure Pipelines specific though, it\'s just that I was using Azure Pipelines to deploy the Bicep. Really, this applies to any way of deploying Bicep. GitHub Actions, Azure CLI or whatever.\\n\\nIf you\'re here because you\'ve encountered the dread message:\\n\\n> `Creating managed certificate requires hostname \'....\' added as a custom hostname to a container app in environment \'caenv-appname-dev\'`\\n\\nThen you\'re in the right place. I\'m going to explain how to get past that error message and get your custom domain working with your Azure Container App whilst still using Bicep. It\'s going to get ugly. But it will work.\\n\\n![title image reading \\"Azure Container Apps, Bicep, managed certificates and custom domains\\" with the Azure Container App logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## A three pipe(line) problem\\n\\nI spent much of the last week attempting to attach a custom domain to an Azure Container App. I was using Bicep to deploy the infrastructure and I was using Azure DevOps to deploy the Bicep.\\n\\nThere wasn\'t any documentation I could find about this, and so I decided to try and work it out for myself. I was able to get it working, but it was a bit of a journey. I\'m going to share the steps I took here in the hope that it helps someone else.\\n\\nI titled this section \\"A three pipe(line) problem\\" because it turned out it required three Azure Pipeline runs to deploy a custom domain with a managed certificate. That, and the fact that it seemed a great way to get a Sherlock Holmes pun into the mix. I feel justified; there was no small amount of detective work involved.\\n\\n## Reverse engineering the Bicep from the Azure Portal\\n\\nI knew that to get a custom domain working with an Azure Container App I would need to do two things:\\n\\n1. Create a managed certificate on my managed environment\\n2. Create a custom domain on my container app\\n\\nSo I fired up the Azure Portal and did those two things. Then I went to the export template option and downloaded the ARM template. I was hoping to see how the Azure Portal did it. Since my eyes bleed a little when I attempt to read ARM templates, I [decompiled the ARM template into the Bicep equivalent](https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/decompile?tabs=azure-cli).\\n\\nIt actually seemed relatively simple. First there was a [`Microsoft.App/managedEnvironments/managedCertificates`](https://learn.microsoft.com/en-us/azure/templates/microsoft.app/2022-11-01-preview/managedenvironments/managedcertificates?pivots=deployment-language-bicep) resource which created the managed certificate:\\n\\n```bicep\\nresource managedEnvironmentManagedCertificate \'Microsoft.App/managedEnvironments/managedCertificates@2022-11-01-preview\' = {\\n  parent: managedEnvironment\\n  name: \'${managedEnvironment.name}-certificate\'\\n  location: location\\n  tags: tags\\n  properties: {\\n    subjectName: customDomainName\\n    domainControlValidation: \'CNAME\'\\n  }\\n}\\n```\\n\\nThen there was the addition of a [`customDomains`](https://learn.microsoft.com/en-us/azure/templates/microsoft.app/containerapps?pivots=deployment-language-bicep#customdomain) property to the `Microsoft.App/containerApps` resource which referenced the managed certificate:\\n\\n```bicep\\nresource containerApp \'Microsoft.App/containerApps@2022-11-01-preview\' = {\\n  //...\\n  properties: {\\n    configuration: {\\n      //...\\n      ingress: {\\n        //...\\n        customDomains: [\\n          {\\n            name: managedEnvironmentManagedCertificate.properties.subjectName\\n            certificateId: managedEnvironmentManagedCertificate.id\\n            bindingType: \'SniEnabled\'\\n          }\\n        ]\\n        //...\\n      }\\n      //...\\n    }\\n    //...\\n  }\\n  //...\\n}\\n```\\n\\nIt looked simple enough. I added those two resources to my Bicep file and ran the pipeline. It failed. I got this error:\\n\\n> `Creating managed certificate requires hostname \'....\' added as a custom hostname to a container app in environment \'caenv-appname-dev\'`\\n\\nGoogling that error message didn\'t lead me to [this issue](https://github.com/microsoft/azure-container-apps/issues/607) on the Azure Container Apps GitHub repo. Other people were having similar issues. I was able to gather enough clues from that issue to get me to a working approach. I may be the first person in the world to have got this far... Wouldn\'t that be special?\\n\\n## The three pipe(line) solution\\n\\nSo whilst the original approach I came up with looked like it should work, it did not. What succeeded was an approach where I ran one pipeline to deploy some Bicep, a second pipeline to deploy some tweaked Bicep, and a third pipeline to deploy some more tweaked Bicep. Then profit.\\n\\nHerewith the details of the three pipelines / three Bicep deployments:\\n\\n### Bicep template 1: Create a disabled custom domain\\n\\nOur first Bicep template is going to create a custom domain on our container app. However, we\'re going to set the `bindingType` property to `Disabled`.\\n\\n```bicep\\nresource containerApp \'Microsoft.App/containerApps@2022-11-01-preview\' = {\\n  //...\\n  properties: {\\n    configuration: {\\n      //...\\n      ingress: {\\n        //...\\n        customDomains: [\\n          {\\n            name: customDomainName\\n            bindingType: \'Disabled\'\\n          }\\n        ]\\n        //...\\n      }\\n      //...\\n    }\\n    //...\\n  }\\n  //...\\n}\\n```\\n\\nDeploy the above template and you\'ll have a custom domain on your container app, but it won\'t be active. This will be graduated to an active custom domain in the third Bicep template.\\n\\n### Bicep template 2: Create the managed certificate\\n\\nNow we\'re going to create the managed certificate by adding the following resource to our Bicep template:\\n\\n```bicep\\nresource managedEnvironmentManagedCertificate \'Microsoft.App/managedEnvironments/managedCertificates@2022-11-01-preview\' = {\\n  parent: managedEnvironment\\n  name: \'${managedEnvironment.name}-certificate\'\\n  location: location\\n  tags: tags\\n  properties: {\\n    subjectName: customDomainName\\n    domainControlValidation: \'CNAME\'\\n  }\\n}\\n```\\n\\nAt present there\'s no relation between the managed certificate and the custom domain. We\'ll fix that in the third Bicep template. Deploy this template and you\'ll have a managed certificate. However, the deployment will likely fail with an following error of the following form:\\n\\n```json\\n{\\n  \\"status\\": \\"Failed\\",\\n  \\"error\\": {\\n    \\"code\\": \\"AuthorizationFailed\\",\\n    \\"message\\": \\"The client \'GUID-GOES-HERE\' with object id \'GUID-GOES-HERE\' does not have authorization to perform action \'Microsoft.App/locations/managedCertificateOperationStatuses/read\' over scope \'/subscriptions/SUBSCRIPTION-ID-GOES-HERE/providers/Microsoft.App/locations/West Europe/managedCertificateOperationStatuses/GUID-GOES-HERE\' or the scope is invalid. If access was recently granted, please refresh your credentials.\\"\\n  }\\n}\\n```\\n\\nDon\'t sweat it. You should have a managed certificate. That\'s what we need.\\n\\n### Bicep template 3: Create an active custom domain\\n\\nReady for the big finish? We\'re going to take our Bicep template back to what we originally tried:\\n\\n```bicep\\nresource managedEnvironmentManagedCertificate \'Microsoft.App/managedEnvironments/managedCertificates@2022-11-01-preview\' = {\\n  parent: managedEnvironment\\n  name: \'${managedEnvironment.name}-certificate\'\\n  location: location\\n  tags: tags\\n  properties: {\\n    subjectName: customDomainName\\n    domainControlValidation: \'CNAME\'\\n  }\\n}\\n\\nresource containerApp \'Microsoft.App/containerApps@2022-11-01-preview\' = {\\n  //...\\n  properties: {\\n    configuration: {\\n      //...\\n      ingress: {\\n        //...\\n        customDomains: [\\n          {\\n            name: managedEnvironmentManagedCertificate.properties.subjectName\\n            certificateId: managedEnvironmentManagedCertificate.id\\n            bindingType: \'SniEnabled\'\\n          }\\n        ]\\n        //...\\n      }\\n      //...\\n    }\\n    //...\\n  }\\n  //...\\n}\\n```\\n\\nNote that the `customDomains` property of the `Microsoft.App/containerApps` resource now references the managed certificate and is `SniEnabled`. Deploy this template and you\'ll have a working managed certificate and a working custom domain on your container app. Huzzah!\\n\\n## Conclusion\\n\\nI write this post purely to help others who may be struggling with this. I\'m assuming this is some kind of bug, and I\'m hoping the Azure Container Apps team will fix it soon. I\'ve raised a specific issue on the Azure Container Apps GitHub repo for this problem. You can find it [here](https://github.com/microsoft/azure-container-apps/issues/796).\\n\\n## Attributions\\n\\n<a href=\\"https://www.flaticon.com/free-icons/bad-habit\\" title=\\"bad habit icons\\">Pipe icon in title image created by Freepik - Flaticon</a>"},{"id":"azure-container-apps-easy-auth-and-dotnet-authentication","metadata":{"permalink":"/azure-container-apps-easy-auth-and-dotnet-authentication","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-06-11-azure-container-apps-easy-auth-and-dotnet-authentication/index.md","source":"@site/blog/2023-06-11-azure-container-apps-easy-auth-and-dotnet-authentication/index.md","title":"Azure Container Apps, Easy Auth and .NET authentication","description":"Azure Container Apps support Easy Auth. However, .NET applications run in ACAs do not recognise Easy Auth authentication. This post explains the issue and solves it.","date":"2023-06-11T00:00:00.000Z","tags":[{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."},{"inline":false,"label":"Azure Container Apps","permalink":"/tags/azure-container-apps","description":"The Azure Container Apps service. Effectively a managed Kubernetes service."},{"inline":false,"label":"Easy Auth","permalink":"/tags/easy-auth","description":"The Azure Easy Auth feature used for authentication and authorization."},{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."},{"inline":false,"label":"Authentication and Authorization","permalink":"/tags/auth","description":"Matters related to identity and permissioning"}],"readingTime":7.17,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-container-apps-easy-auth-and-dotnet-authentication","title":"Azure Container Apps, Easy Auth and .NET authentication","authors":"johnnyreilly","tags":["azure","azure container apps","easy auth","asp.net","auth"],"image":"./title-image.png","description":"Azure Container Apps support Easy Auth. However, .NET applications run in ACAs do not recognise Easy Auth authentication. This post explains the issue and solves it.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Azure Container Apps, Bicep, managed certificates and custom domains","permalink":"/azure-container-apps-bicep-managed-certificates-custom-domains"},"nextItem":{"title":"Private Bicep registry authentication with AzureResourceManagerTemplateDeployment@3","permalink":"/private-bicep-registry-authentication-azureresourcemanagertemplatedeployment"}},"content":"Easy Auth is a great way to authenticate your users. However, when used in the context of Azure Container Apps, .NET applications do not, by default, recognise that Easy Auth is in place. You might be authenticated but .NET will still act as if you aren\'t. `builder.Services.AddAuthentication()` and `app.UseAuthentication()` doesn\'t change that. This post explains the issue and solves it through the implementation of an `AuthenticationHandler`.\\n\\n![title image reading \\"Azure Container Apps, Easy Auth and .NET authentication\\" with the Azure Container App logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\nIf you\'re looking for information about Easy Auth and roles with .NET and Azure App Service, you can find it here:\\n\\n- [Azure App Service, Easy Auth and Roles with .NET](../2021-01-14-azure-easy-auth-and-roles-with-dotnet-and-core/index.md)\\n- [Azure App Service, Easy Auth and Roles with .NET and Microsoft.Identity.Web](../2021-01-17-azure-easy-auth-and-roles-with-net-and-microsoft-identity-web/index.md)\\n\\n## `User.Identity.IsAuthenticated == false`\\n\\nWhen I\'m building an application, I want to focus on the problem I\'m solving. I don\'t want to think about how to implement my own authentication system. Rather, I lean on an auth provider for that, and if I\'m working in the Azure ecosystem, that often means Easy Auth, usually with Azure AD.\\n\\nI recently started building a .NET application using Easy Auth and deploying to Azure Container Apps. One thing that surprised me when I tested it out was that, whilst I was being authenticated, my app didn\'t seem to be aware of it. When I inspected the `User.Identity.IsAuthenticated` property in my application, it was `false`. The reason why lies [in the documentation](https://learn.microsoft.com/en-us/azure/container-apps/authentication#access-user-claims-in-application-code):\\n\\n> For all language frameworks, Container Apps makes the claims in the incoming token available to your application code. The claims are injected into the request headers, which are present whether from an authenticated end user or a client application. External requests aren\'t allowed to set these headers, so they\'re present only if set by Container Apps. Some example headers include:\\n>\\n> `X-MS-CLIENT-PRINCIPAL-NAME`\\n>\\n> `X-MS-CLIENT-PRINCIPAL-ID`\\n>\\n> _Code that is written in any language or framework can get the information that it needs from these headers._\\n\\nThe emphasis above is mine. What it\'s saying here is this: **you need to implement this yourself**.\\n\\n## Easy Auth Azure Container App headers\\n\\nSure enough, when I inspected the headers in my application, I could see these:\\n\\n![screenshot of easy auth headers including X-MS-CLIENT-PRINCIPAL-NAME`, `X-MS-CLIENT-PRINCIPAL-ID`, `X-MS-CLIENT-PRINCIPAL-IDP` and `X-MS-CLIENT-PRINCIPAL`](screenshot-easy-auth-headers.webp)\\n\\nThe `X-MS-CLIENT-PRINCIPAL` header is particularly interesting. From the appearance, you might assume it\'s a [JWT](https://jwt.io/). It\'s not. It\'s actually a base 64 encoded JSON string that represents the signed in user and their claims. It\'s actually super easy to decode in your browser devtools:\\n\\n```js\\nJSON.parse(atob(xMsClientPrincipal));\\n```\\n\\nIf you decode it, you\'ll see something like this:\\n\\n![a screenshot of the decoded object](screenshot-decoded-x-ms-client-principal-header.png)\\n\\nGiven that this information is present, what we can do is tell .NET about it.\\n\\nBut before we do that, let\'s pause to talk about a current limitation with Easy Auth on Azure Container Apps; JWT support.\\n\\n## Lack of JWT / Token support\\n\\nThe problem is, there\'s not JWT token that we can make use of in the headers of an Azure Container App. This is supported in [App Service which has a token store](https://learn.microsoft.com/en-us/azure/app-service/overview-authentication-authorization#token-store) and supports the [following headers](https://learn.microsoft.com/en-us/azure/app-service/configure-authentication-oauth-tokens#retrieve-tokens-in-app-code):\\n\\n```\\nX-MS-TOKEN-AAD-ID-TOKEN\\nX-MS-TOKEN-AAD-ACCESS-TOKEN\\nX-MS-TOKEN-AAD-EXPIRES-ON\\nX-MS-TOKEN-AAD-REFRESH-TOKEN\\n```\\n\\nIf there was a populated `X-MS-TOKEN-AAD-ACCESS-TOKEN` then it would unlock all manner of possibilities. Let\'s say I want to make use of the Graph API on behalf of my logged in user. I cannot.\\n\\nReading [the docs](https://learn.microsoft.com/en-us/graph/sdks/choose-authentication-providers?tabs=csharp#on-behalf-of-provider) I believe I should be trying to use the \\"on behalf of\\" approach:\\n\\n```cs\\nvar scopes = new[] { \\"https://graph.microsoft.com/.default\\" };\\n\\n// Multi-tenant apps can use \\"common\\",\\n// single-tenant apps must use the tenant ID from the Azure portal\\nvar tenantId = \\"common\\";\\n\\n// Values from app registration\\nvar clientId = \\"YOUR_CLIENT_ID\\";\\nvar clientSecret = \\"YOUR_CLIENT_SECRET\\";\\n\\n// using Azure.Identity;\\nvar options = new OnBehalfOfCredentialOptions\\n{\\n    AuthorityHost = AzureAuthorityHosts.AzurePublicCloud,\\n};\\n\\n// This is the incoming token to exchange using on-behalf-of flow\\nvar oboToken = \\"JWT_TOKEN_TO_EXCHANGE\\";\\n\\nvar onBehalfOfCredential = new OnBehalfOfCredential(\\n    tenantId, clientId, clientSecret, oboToken, options);\\n\\nvar graphClient = new GraphServiceClient(onBehalfOfCredential, scopes);\\n```\\n\\nBut because we have no token to exchange, we can\'t make use of delegated permissions in the Graph API. This is a current limitation of using Easy Auth and Azure Container Apps. Keep an eye on [this GitHub issue if this is interesting to you](https://github.com/microsoft/azure-container-apps/issues/479).\\n\\n## Implementing `AddAzureContainerAppsEasyAuth()`\\n\\nWe\'re going to implement an `AuthenticationHandler` that takes the information from the `X-MS-CLIENT-PRINCIPAL` header and uses it to create a `ClaimsPrincipal`:\\n\\n```cs title=\\"AzureContainerAppsEasyAuth.cs\\"\\nusing Microsoft.AspNetCore.Authentication;\\nusing Microsoft.Extensions.Logging;\\nusing Microsoft.Extensions.Options;\\nusing System.Security.Claims;\\nusing System.Text.Encodings.Web;\\nusing System.Text.Json;\\nusing System.Text.Json.Serialization;\\n\\n/// <summary>\\n/// Support for EasyAuth authentication in Azure Container Apps\\n/// </summary>\\nnamespace Azure.ContainerApps.EasyAuth;\\n\\npublic static class EasyAuthAuthenticationBuilderExtensions\\n{\\n    public const string EASYAUTHSCHEMENAME = \\"EasyAuth\\";\\n\\n    public static AuthenticationBuilder AddAzureContainerAppsEasyAuth(\\n        this AuthenticationBuilder builder,\\n        Action<EasyAuthAuthenticationOptions>? configure = null)\\n    {\\n        if (configure == null) configure = o => { };\\n\\n        return builder.AddScheme<EasyAuthAuthenticationOptions, EasyAuthAuthenticationHandler>(\\n            EASYAUTHSCHEMENAME,\\n            EASYAUTHSCHEMENAME,\\n            configure);\\n    }\\n}\\n\\npublic class EasyAuthAuthenticationOptions : AuthenticationSchemeOptions\\n{\\n    public EasyAuthAuthenticationOptions()\\n    {\\n        Events = new object();\\n    }\\n}\\n\\npublic class EasyAuthAuthenticationHandler : AuthenticationHandler<EasyAuthAuthenticationOptions>\\n{\\n    public EasyAuthAuthenticationHandler(\\n        IOptionsMonitor<EasyAuthAuthenticationOptions> options,\\n        ILoggerFactory logger,\\n        UrlEncoder encoder,\\n        ISystemClock clock)\\n        : base(options, logger, encoder, clock)\\n    {\\n    }\\n\\n    protected override async Task<AuthenticateResult> HandleAuthenticateAsync()\\n    {\\n        try\\n        {\\n            var easyAuthProvider = Context.Request.Headers[\\"X-MS-CLIENT-PRINCIPAL-IDP\\"].FirstOrDefault() ?? \\"aad\\";\\n            var msClientPrincipalEncoded = Context.Request.Headers[\\"X-MS-CLIENT-PRINCIPAL\\"].FirstOrDefault();\\n            if (string.IsNullOrWhiteSpace(msClientPrincipalEncoded))\\n                return AuthenticateResult.NoResult();\\n\\n            var decodedBytes = Convert.FromBase64String(msClientPrincipalEncoded);\\n            using var memoryStream = new MemoryStream(decodedBytes);\\n            var clientPrincipal = await JsonSerializer.DeserializeAsync<MsClientPrincipal>(memoryStream);\\n\\n            if (clientPrincipal == null || !clientPrincipal.Claims.Any())\\n                return AuthenticateResult.NoResult();\\n\\n            var claims = clientPrincipal.Claims.Select(claim => new Claim(claim.Type, claim.Value));\\n\\n            // remap \\"roles\\" claims from easy auth to the more standard ClaimTypes.Role / \\"http://schemas.microsoft.com/ws/2008/06/identity/claims/role\\"\\n            var easyAuthRoleClaims = claims.Where(claim => claim.Type == \\"roles\\");\\n            var claimsAndRoles = claims.Concat(easyAuthRoleClaims.Select(role => new Claim(ClaimTypes.Role, role.Value)));\\n\\n            var principal = new ClaimsPrincipal();\\n            principal.AddIdentity(new ClaimsIdentity(claimsAndRoles, clientPrincipal.AuthenticationType, clientPrincipal.NameType, ClaimTypes.Role));\\n\\n            var ticket = new AuthenticationTicket(principal, easyAuthProvider);\\n            var success = AuthenticateResult.Success(ticket);\\n            Context.User = principal;\\n\\n            return success;\\n        }\\n        catch (Exception ex)\\n        {\\n            return AuthenticateResult.Fail(ex);\\n        }\\n    }\\n}\\n\\npublic class MsClientPrincipal\\n{\\n    [JsonPropertyName(\\"auth_typ\\")]\\n    public string? AuthenticationType { get; set; }\\n    [JsonPropertyName(\\"claims\\")]\\n    public IEnumerable<UserClaim> Claims { get; set; } = Array.Empty<UserClaim>();\\n    [JsonPropertyName(\\"name_typ\\")]\\n    public string? NameType { get; set; }\\n    [JsonPropertyName(\\"role_typ\\")]\\n    public string? RoleType { get; set; }\\n}\\n\\npublic class UserClaim\\n{\\n    [JsonPropertyName(\\"typ\\")]\\n    public string Type { get; set; } = string.Empty;\\n    [JsonPropertyName(\\"val\\")]\\n    public string Value { get; set; } = string.Empty;\\n}\\n```\\n\\nThere\'s a few things to note from the above:\\n\\n- `EasyAuthAuthenticationHandler` is an `AuthenticationHandler` that takes the information from the `X-MS-CLIENT-PRINCIPAL` header and uses it to create a `ClaimsPrincipal`.\\n- The `MsClientPrincipal` class is a representation of the decoded `X-MS-CLIENT-PRINCIPAL` header.\\n- `AddAzureContainerAppsEasyAuth` is an extension method for the `AuthenticationBuilder` object - this allows users to make use of the handler in their application.\\n- With Easy Auth, role claims arrive in the custom `\\"roles\\"` claim. This is somewhat non-standard and so we remap `\\"roles\\"` claims to be `ClaimTypes.Role` / `\\"http://schemas.microsoft.com/ws/2008/06/identity/claims/role\\"` claims as well. This should ensure that anything built with the expectation of that type of claim behaves in the way you\'d expect.\\n\\n## Using `AddAzureContainerAppsEasyAuth()`\\n\\nNow we\'ve written our handler, we can use it in our application. We do this by calling `AddAzureContainerAppsEasyAuth()` in our `Program.cs`:\\n\\n```cs title=\\"Program.cs\\"\\n//...\\n\\nbuilder.Services\\n    .AddAuthentication(EasyAuthAuthenticationBuilderExtensions.EASYAUTHSCHEMENAME)\\n    .AddAzureContainerAppsEasyAuth(); // <-- here\\nbuilder.Services.AddAuthorization();\\n\\n//...\\n\\nvar app = builder.Build();\\n\\n//...\\n\\napp.UseAuthentication();\\napp.UseAuthorization();\\n\\n//...\\n```\\n\\nNow when we run our application, we\'ll see that `User.Identity.IsAuthenticated` is `true` when we\'re authenticated in Azure Container Apps!\\n\\n## Easy Auth differs Azure App Service, Azure Container Apps, Azure Static Web Apps and Azure Functions\\n\\nOne thing that became very clear to me as I worked on this, is that Easy Auth is implemented differently in Azure App Service, Azure Container Apps, Azure Static Web Apps and Azure Functions. Whilst the authentication appears to be the same, the headers are different across the services. So the code above will work in Azure Container Apps; for other Azure services I can\'t vouch for it.\\n\\nThe code in this post is very similar to that in [`MaximeRouiller.Azure.AppService.EasyAuth`](https://github.com/MaximRouiller/MaximeRouiller.Azure.AppService.EasyAuth). But it\'s not quite the same, as that library depends upon a `WEBSITE_AUTH_ENABLED` environment variable, which isn\'t present in Azure Container Apps.\\n\\nLikewise, the `Microsoft.Identity.Web` package [supports Easy Auth, but for Azure App Service](https://github.com/AzureAD/microsoft-identity-web/wiki/1.2.0#integration-with-azure-app-services-authentication-of-web-apps-running-with-microsoftidentityweb). If you [take a look at the code](https://github.com/AzureAD/microsoft-identity-web/blob/c7f146e93180deece63f879453e98a872cdda658/src/Microsoft.Identity.Web/AppServicesAuth/AppServicesAuthenticationInformation.cs#L38), you\'ll see it is powered by environment variables and headers, which aren\'t present in Azure Container Apps.\\n\\nSo whilst it would be tremendous if this was built into .NET, or in a NuGet package somewhere. I\'m not aware of one at the time of writing, so I made this. Perhaps this should become a NuGet package? Let me know if you think so!\\n\\nI\'ve also raised a [feature request in the `Microsoft.Identity.Web` repo to support Azure Container Apps](https://github.com/AzureAD/microsoft-identity-web/issues/2274). If you\'d like to see this, please upvote it!"},{"id":"private-bicep-registry-authentication-azureresourcemanagertemplatedeployment","metadata":{"permalink":"/private-bicep-registry-authentication-azureresourcemanagertemplatedeployment","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-06-02-private-bicep-registry-authentication-azureresourcemanagertemplatedeployment/index.md","source":"@site/blog/2023-06-02-private-bicep-registry-authentication-azureresourcemanagertemplatedeployment/index.md","title":"Private Bicep registry authentication with AzureResourceManagerTemplateDeployment@3","description":"You can deploy Bicep to Azure with the dedicated Azure DevOps task; however authentication to private Bicep registries is not supported.  This post shares a workaround.","date":"2023-06-02T00:00:00.000Z","tags":[{"inline":false,"label":"Bicep","permalink":"/tags/bicep","description":"The Bicep language for Azure Resource Manager templates."},{"inline":false,"label":"Azure DevOps","permalink":"/tags/azure-devops","description":"The Azure DevOps suite of tools."}],"readingTime":2.69,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"private-bicep-registry-authentication-azureresourcemanagertemplatedeployment","title":"Private Bicep registry authentication with AzureResourceManagerTemplateDeployment@3","authors":"johnnyreilly","tags":["bicep","azure devops"],"image":"./title-image.png","description":"You can deploy Bicep to Azure with the dedicated Azure DevOps task; however authentication to private Bicep registries is not supported.  This post shares a workaround.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Azure Container Apps, Easy Auth and .NET authentication","permalink":"/azure-container-apps-easy-auth-and-dotnet-authentication"},"nextItem":{"title":"Static Web Apps CLI and Node.js 18: could not connect to API","permalink":"/static-web-apps-cli-node-18-could-not-connect-to-api"}},"content":"If you deploy Bicep templates to Azure in Azure DevOps, you\'ll likely use the dedicated Azure DevOps task; the catchily named [`AzureResourceManagerTemplateDeployment@3`](https://learn.microsoft.com/en-us/azure/devops/pipelines/tasks/reference/azure-resource-manager-template-deployment-v3?view=azure-pipelines). This task has had support for deploying Bicep since early 2022. But whilst vanilla Bicep is supported, there\'s a use case which isn\'t supported; private Bicep registries.\\n\\n![title image reading \\"Private Bicep registry authentication with AzureResourceManagerTemplateDeployment@3\\" with the Bicep, Azure and Azure DevOps logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## \\"Unable to restore the module... Status: 401 (Unauthorized)\\"\\n\\n[Private Bicep registries](https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/private-module-registry?tabs=azure-powershell) are a great way to share Bicep modules across your organisation. We use them in the organisation that I\'m part of; it\'s a good a way to help us move faster and to share common security baselines.\\n\\nAlas it turns out that the `AzureResourceManagerTemplateDeployment@3` task doesn\'t presently play well with private Bicep registries. This is because it\'s necessary to authenticate to a private registry before you can consume modules. And that\'s not supported by the `AzureResourceManagerTemplateDeployment@3` task. What does that mean? Well take a look at the following Azure Pipeline:\\n\\n```yml\\n- task: AzureResourceManagerTemplateDeployment@3\\n  name: DeployInfra\\n  displayName: Deploy infra\\n  retryCountOnTaskFailure: 3\\n  inputs:\\n    deploymentScope: Resource Group\\n    azureResourceManagerConnection: ${{ variables.serviceConnection }}\\n    subscriptionId: $(subscriptionId)\\n    action: Create Or Update Resource Group\\n    resourceGroupName: $(resourceGroupName)\\n    location: $(location)\\n    templateLocation: Linked artifact\\n    csmFile: \'infra/main.bicep\'\\n```\\n\\nYou\'ll note that it passes a Bicep file to the `csmFile` property. This is the file that will be deployed. But what if that file references modules from a private registry? Well, you\'ll see an error like this:\\n\\n![screenshot of the failing pipeline including the text \'Error BCP192: Unable to restore the module with reference \\"br:icebox.azurecr.io/bicep/ice/providers/document-db/database-accounts:v1.3\\": Service request failed.\'](screenshot-authentication-failure.png)\\n\\nAs you can see from the `Status: 401 (Unauthorized)`, we have an authentication problem; the task doesn\'t know how to authenticate to the private registry.\\n\\n## Workaround\\n\\nYou might think, \\"oh well that\'s it then, I can\'t use private Bicep registries with the `AzureResourceManagerTemplateDeployment@3` task\\". But you\'d be wrong. There\'s a workaround. Essentially, when the `AzureResourceManagerTemplateDeployment@3` task runs, it attempts to restore the modules it needs as a first step to compiling the Bicep in to ARM. But it only does this if it needs to. If we perform the restore manually first, then the task won\'t need to do it again. That\'s the trick.\\n\\nBefore the `AzureResourceManagerTemplateDeployment@3` task runs, we can run a task to restore the modules. We can use the `AzureCLI@2` task to do this. Here\'s an example:\\n\\n```yml\\n- task: AzureCLI@2\\n  displayName: Bicep restore\\n  inputs:\\n    azureSubscription: service-connection-with-access-to-registry\\n    scriptType: bash\\n    scriptLocation: inlineScript\\n    inlineScript: |\\n      bicep restore infra/main.bicep\\n```\\n\\nWhere `service-connection-with-access-to-registry` is an Azure Resource Manager service connection using service principal authentication which has access to the private Bicep registry.\\n\\n![screenshot of service connection](screenshot-service-connection.webp)\\n\\nSo if the above task runs _prior_ to the `AzureResourceManagerTemplateDeployment@3` task, then the modules will be restored and the `AzureResourceManagerTemplateDeployment@3` task will be able to compile the Bicep in to ARM and deploy it to Azure. This solves the problem; albeit at the cost of an extra task in the pipeline.\\n\\n## In the box, in the future?\\n\\nIt would be tremendous if authentication to private Bicep registries was supported by the `AzureResourceManagerTemplateDeployment@3` task. I\'ve raised a feature request for this [here](https://github.com/microsoft/azure-pipelines-tasks/issues/18426). If you\'d like to see this too, please do add your voice to the issue."},{"id":"static-web-apps-cli-node-18-could-not-connect-to-api","metadata":{"permalink":"/static-web-apps-cli-node-18-could-not-connect-to-api","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-05-20-static-web-apps-cli-node-18-could-not-connect-to-api/index.md","source":"@site/blog/2023-05-20-static-web-apps-cli-node-18-could-not-connect-to-api/index.md","title":"Static Web Apps CLI and Node.js 18: could not connect to API","description":"With Node.js 18, the Static Web Apps CLI fails to connect to the API - there is a way to fix this.","date":"2023-05-20T00:00:00.000Z","tags":[{"inline":false,"label":"Azure Static Web Apps","permalink":"/tags/azure-static-web-apps","description":"The Azure Static Web Apps service."},{"inline":false,"label":"Node.js","permalink":"/tags/node-js","description":"The Node.js runtime."}],"readingTime":2.805,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"static-web-apps-cli-node-18-could-not-connect-to-api","title":"Static Web Apps CLI and Node.js 18: could not connect to API","authors":"johnnyreilly","tags":["azure static web apps","node.js"],"image":"./title-image.png","description":"With Node.js 18, the Static Web Apps CLI fails to connect to the API - there is a way to fix this.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Private Bicep registry authentication with AzureResourceManagerTemplateDeployment@3","permalink":"/private-bicep-registry-authentication-azureresourcemanagertemplatedeployment"},"nextItem":{"title":"TypeScript 5: importsNotUsedAsValues replaced by ESLint consistent-type-imports","permalink":"/typescript-5-importsnotusedasvalues-error-eslint-consistent-type-imports"}},"content":"I make use of Azure Static Web Apps a lot. I recently upgraded to Node.js 18 and found that the Static Web Apps CLI no longer worked when trying to run locally; the API would not connect when running `swa start`:\\n\\n`[swa] \u274C Could not connect to \\"http://localhost:7071/\\". Is the server up and running?`\\n\\nThis post shares a workaround. This works for v1.1.3 or earlier of the Static Web Apps CLI. If you\'re using v1.1.4 or later, you should not need this workaround. But in that case [you might find this post helpful on improving performance with 1.1.4 or later](../2024-06-18-static-web-apps-cli-improve-performance-with-vite-server-proxy/index.md).\\n\\n![title image reading \\"Static Web Apps CLI and Node.js 18: could not connect to API\\" with the Static Web Apps CLI and Node.js logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\nWith Node.js 17 onwards there were changes in the behaviour of Node.js concerning DNS names. Although it\'s not obvious, the [changes happened here](https://github.com/nodejs/node/pull/39987) and the result of this was that IPv6 became the default DNS instead of IPv4. You can read more about this [on this GitHub issue](https://github.com/nodejs/node/issues/40537).\\n\\n## How this affects the Static Web Apps CLI\\n\\nMy own setup is a Vite front end and a Function App back end. I have a `package.json` in the folder of the front end app with the following scripts:\\n\\n```json\\n\\"dev\\": \\"vite\\",\\n\\"start\\": \\"swa start http://localhost:5173 --run \\\\\\"npm run dev\\\\\\" --api-location ../FunctionApp\\"\\n```\\n\\nI could see both front end and back end starting up in the console, but inevitably the SWA CLI would report:\\n\\n`[swa] \u274C Could not connect to \\"http://localhost:7071/\\". Is the server up and running?`\\n\\nI experienced this when moving from Node.js 16 to Node.js 18. A dependency of the Static Web Apps CLI; the [wait-on](https://github.com/jeffbski/wait-on) library which waits for endpoints to become available, was impacted by the new behavior. [With Node.js 18 this is broken](https://github.com/jeffbski/wait-on/issues/137).\\n\\nA fix to the overall issue was released in [v1.1.4 of the Static Web Apps CLI](https://github.com/Azure/static-web-apps-cli/releases/tag/v1.1.4). Unfortunately, it caused performance issues with the proxy server. [This post shows you how to work around this issue](../2024-06-18-static-web-apps-cli-improve-performance-with-vite-server-proxy/index.md). If you\'d like to work around the issue with v1.1.3 or earlier, read on.\\n\\n## The workaround for v1.1.3 or earlier\\n\\nVarious workarounds are suggested in [this GitHub issue](https://github.com/Azure/static-web-apps-cli/issues/663). I shared my own there, and I\'m sharing it here too. (Mostly for me, I\'ll lay money I need this again and again.)\\n\\nIn the root of my project I installed [concurrently](https://www.npmjs.com/package/concurrently):\\n\\n```\\nnpm i concurrently\\n```\\n\\nThen, again in the root of my project I added the following scripts:\\n\\n```json\\n\\"debug\\": \\"concurrently -n \\\\\\"staticwebapp,functionapp\\\\\\" -c \\\\\\"bgBlue.bold,bgMagenta.bold\\\\\\" \\\\\\"npm run debug:staticwebapp\\\\\\" \\\\\\"npm run debug:functionapp\\\\\\"\\",\\n\\"debug:staticwebapp\\": \\"cd src/StaticWebApp && npm run debug\\",\\n\\"debug:functionapp\\": \\"cd src/FunctionApp && func start\\",\\n```\\n\\nWhat\'s happening here is that I\'m running the Static Web Apps CLI and the Function App CLI in separate processes, and running them concurrently when we run `npm run debug`. You\'ll note that the `debug:staticwebapp` script is running another `debug` script with the Static Web Apps CLI in the `src/StaticWebApp` folder:\\n\\n```json\\n\\"debug\\": \\"swa start http://localhost:5173 --run \\\\\\"npm run dev\\\\\\" --api-location http://127.0.0.1:7071\\",\\n```\\n\\nThe `--api-location` flag is pointing to the endpoint the Function App is surfaced at. This is the key to the workaround.\\n\\n## Summary\\n\\nThis takes us back to having a setup that will work with Node.js 18. Hopefully this is only needed for a short while, but it\'s good to have a workaround in the meantime."},{"id":"typescript-5-importsnotusedasvalues-error-eslint-consistent-type-imports","metadata":{"permalink":"/typescript-5-importsnotusedasvalues-error-eslint-consistent-type-imports","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-05-09-typescript-5-importsnotusedasvalues-error-eslint-consistent-type-imports/index.md","source":"@site/blog/2023-05-09-typescript-5-importsnotusedasvalues-error-eslint-consistent-type-imports/index.md","title":"TypeScript 5: importsNotUsedAsValues replaced by ESLint consistent-type-imports","description":"TypeScript deprecated tsconfig.json option \\"importsNotUsedAsValues\\": \\"error\\" in 5. You can make type imports explicit with CommonJS if you use ESLint consistent-type-imports.","date":"2023-05-09T00:00:00.000Z","tags":[{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."},{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."},{"inline":false,"label":"ESLint","permalink":"/tags/eslint","description":"The ESLint linter."}],"readingTime":6.14,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"typescript-5-importsnotusedasvalues-error-eslint-consistent-type-imports","title":"TypeScript 5: importsNotUsedAsValues replaced by ESLint consistent-type-imports","authors":"johnnyreilly","tags":["typescript","javascript","eslint"],"image":"./title-image.png","description":"TypeScript deprecated tsconfig.json option \\"importsNotUsedAsValues\\": \\"error\\" in 5. You can make type imports explicit with CommonJS if you use ESLint consistent-type-imports.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Static Web Apps CLI and Node.js 18: could not connect to API","permalink":"/static-web-apps-cli-node-18-could-not-connect-to-api"},"nextItem":{"title":"Migrating Azure Functions from JSDoc JavaScript to TypeScript","permalink":"/migrating-azure-functions-from-jsdoc-javascript-to-typescript"}},"content":"I really like type imports that are unambiguous. For this reason, I\'ve made use of the `\\"importsNotUsedAsValues\\": \\"error\\"` option in `tsconfig.json` for a while now. This option has been deprecated in TypeScript 5.0.0, and will be removed in TypeScript 5.5.0. This post will look at what you can do instead.\\n\\n![title image reading \\"TypeScript 5: `importsNotUsedAsValues` replaced by ESLint `consistent-type-imports`\\" with the ESLint and TypeScript logo](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## What `\\"importsNotUsedAsValues\\": \\"error\\"` provided\\n\\nPrior to TypeScript 5.0, if you wanted to make your type imports explicit, you could use the `\\"importsNotUsedAsValues\\": \\"error\\"` option in `tsconfig.json`. This would mean that you would need to use `import type` for type imports, and `import` for value imports. Consider the following:\\n\\n```ts\\nimport { ResourceGraphClient } from \'@azure/arm-resourcegraph\';\\n```\\n\\nIn my code I was only using `ResourceGraphClient` as a type, so I would need to change it to:\\n\\n```ts\\nimport type { ResourceGraphClient } from \'@azure/arm-resourcegraph\';\\n```\\n\\n_or_\\n\\n```ts\\nimport { type ResourceGraphClient } from \'@azure/arm-resourcegraph\';\\n```\\n\\nAnd if I rebelled, the TypeScript compiler would complain:\\n\\n> `This import is never used as a value and must use \'import type\' because \'importsNotUsedAsValues\' is set to \'error\'.ts(1371)`\\n\\n![screenshot of VS Code displaying the error message](screenshot-importsnotusedasvalues-error.png)\\n\\n## TypeScript 5 deprecates `importsNotUsedAsValues`\\n\\nHowever, when I upgraded to TypeScript 5, I started seeing the following error:\\n\\n> `Option \'importsNotUsedAsValues\' is deprecated and will stop functioning in TypeScript 5.5. Specify compilerOption \'\\"ignoreDeprecations\\": \\"5.0\\"\' to silence this error. Use \'verbatimModuleSyntax\' instead.`\\n\\n![screenshot of VS Code displaying the error message](screenshot-importsnotusedasvalues-deprecated.png)\\n\\nThe error was the result of [this pull request](https://github.com/microsoft/TypeScript/pull/52203). The message made me think I just needed to migrate to `verbatimModuleSyntax`, like so:\\n\\n```diff title=\\"tsconfig.json\\"\\n-    \\"importsNotUsedAsValues\\": \\"error\\",\\n+    \\"verbatimModuleSyntax\\": true,\\n```\\n\\nHowever, when I did so, my terminal became a sea of errors:\\n\\n```bash\\nsrc/telemetry.ts:7:13 - error TS1286: ESM syntax is not allowed in a CommonJS module when \'verbatimModuleSyntax\' is enabled.\\n\\nimport * as task from \'./task.json\';\\n            ~~~~\\n\\nsrc/telemetry.ts:9:1 - error TS1287: A top-level \'export\' modifier cannot be used on value declarations in a CommonJS module when \'verbatimModuleSyntax\' is enabled.\\n\\nexport async function sendTelemetry({\\n```\\n\\nIt turns out that in `verbatimModuleSyntax`, you can\'t write ESM syntax in files that will emit as CommonJS - which is exactly what my codebase is doing. [Andrew Branch](https://github.com/andrewbranch), who is part of the TypeScript team, sent me an explanation from a draft of some new TypeScript docs:\\n\\n> In TypeScript 5.0, a new compiler option called `verbatimModuleSyntax` was introduced to help TypeScript authors know exactly how their `import` and `export` statements will be emitted. When enabled, the flag requires imports and exports in input files to be written in the form that will undergo the least amount of transformation before emit. So if a file will be emitted as ESM, imports and exports must be written in ESM syntax; if a file will be emitted as CJS, it must be written in the CommonJS-inspired TypeScript syntax (`import fs = require(\\"fs\\")` and `export = {}`). This setting is particularly recommended for Node projects that use mostly ESM, but have a select few CJS files. It is not recommended for projects that currently target CJS, but may want to target ESM in the future.\\n\\nIt further turns out that `importsNotUsedAsValues` was never intended to be used in in the way that I did; effectively as a linting mechanism. Andrew [said this on the topic](https://github.com/microsoft/TypeScript/pull/52203#issuecomment-1476574601):\\n\\n> `importsNotUsedAsValues` was made to serve the opposite purpose you (and basically everyone) were using it for. By default, TypeScript elides unneeded import statements from JS emit even without marking them as `type` imports. `importsNotUsedAsValues` was created as a way to escape that behavior, not (primarily) to make it more explicit. `verbatimModuleSyntax` allows you to escape the elision behavior, and takes the explicitness of what your imports mean a step further by making you write CJS-style imports when emitting to CJS. So in my book, all the important cases that `importsNotUsedAsValues` (and `preserveValueImports`) covered, plus more, are covered by `verbatimModuleSyntax`, which is way more explainable. It\u2019s mostly a matter of reducing complexity for the sake of explanation.\\n\\nUntil the world has finished migrating to ES Modules (which will be a while), I\'m going to need to stick with CommonJS as my emit target, whilst still planning to write ES Module imports in my code. But I really like being explicit about my imports. So what can I do?\\n\\n## ESLint and `@typescript-eslint/consistent-type-imports` to the rescue\\n\\nI mentioned that I was using `importsNotUsedAsValues` essentially as a linting mechanism. And it transpires that the answer to my need lives in ESLint itself. There\'s a rule named [`@typescript-eslint/consistent-type-imports`](https://typescript-eslint.io/rules/consistent-type-imports/) which tackles exactly this. If you\'re using [ESLint](https://eslint.org/) and [typescript-eslint](https://typescript-eslint.io/), you can add this rule to your `.eslintrc.js`:\\n\\n```js title=\\"eslintrc.js\\"\\nmodule.exports = {\\n  // ...\\n  rules: {\\n    // ...\\n    \'@typescript-eslint/consistent-type-imports\': \'error\', // the replacement of \\"importsNotUsedAsValues\\": \\"error\\"\\n  },\\n};\\n```\\n\\nOr if you prefer to have the type imports inline, you can use:\\n\\n```js title=\\"eslintrc.js\\"\\nmodule.exports = {\\n  // ...\\n  rules: {\\n    // ...\\n    \'@typescript-eslint/consistent-type-imports\': [\\n      \'error\',\\n      {\\n        fixStyle: \'inline-type-imports\',\\n      },\\n    ], // the replacement of \\"importsNotUsedAsValues\\": \\"error\\"\\n  },\\n};\\n```\\n\\nWith this in place, we\'re back to where we were before; just with a different engine:\\n\\n> All imports in the declaration are only used as types. Use `import type`.eslint@typescript-eslint/consistent-type-imports\\n\\n![screenshot of VS Code displaying the error message](screenshot-consistent-type-imports-error.png)\\n\\nAnd we even have the ability to auto-fix the errors as well now. Thanks `typescript-eslint`!\\n\\n## `no-import-type-side-effects`\\n\\nWe are not quite done. There\'s another typescript-eslint rule that we can use to help us. [`no-import-type-side-effects`](https://typescript-eslint.io/rules/no-import-type-side-effects/) is a rule that will warn you if you have any side effects in your type imports. What does that mean? Well, consider the following code:\\n\\n```ts\\nimport { type A, type B } from \'mod\';\\n\\n// is transpiled to\\nimport {} from \'mod\';\\n\\n// which is the same as\\nimport \'mod\';\\n```\\n\\nYou may not want a runtime import at all. You can do that by using a **top-level** `type` qualifier for imports when it only imports specifiers with an inline `type` qualifier:\\n\\n```ts\\nimport type { A, B } from \'mod\';\\n\\n// is transpiled to.... nothing! Hence no side effects\\n```\\n\\nSo if side effects is something you\'re concerned about, consider this rule as well. Note that whether `import { type A } from \'mod\'` transpiles to a side-effect import or gets completely removed depends on your `tsc` options, or what transpiler you\u2019re using. But `import type` statements _always_ get removed.\\n\\n## Make VS Code prefer type imports\\n\\nOne of the delightful features of TypeScript in VS Code is TypeScript generated auto-imports. Thanks in large part to the work of [Andrew Branch](https://github.com/andrewbranch) on the TypeScript team, the editor will often generate the `import` that you need when you\'re coding. However, it will generally create value imports; _not_ type imports. So it might auto add this:\\n\\n```ts\\nimport { ResourceGraphClient } from \'@azure/arm-resourcegraph\';\\n```\\n\\nNot this:\\n\\n```ts\\nimport type { ResourceGraphClient } from \'@azure/arm-resourcegraph\';\\n```\\n\\nAs of TypeScript 5.3, this is now an editor-specific option. In Visual Studio Code, it can be enabled in the settings UI under \\"TypeScript \u203A Preferences: Prefer Type Only Auto Imports\\", or as the JSON configuration option `typescript.preferences.preferTypeOnlyAutoImports`. You can read about this in the [TypeScript 5.3 release notes](https://devblogs.microsoft.com/typescript/announcing-typescript-5-3/#settings-to-prefer-type-auto-imports).\\n\\nTurn it on - it\'ll make you happy!\\n\\n## Summary\\n\\nThanks to Andrew Branch for reviewing this post, and massively improving it! Any mistakes are mine, not his."},{"id":"migrating-azure-functions-from-jsdoc-javascript-to-typescript","metadata":{"permalink":"/migrating-azure-functions-from-jsdoc-javascript-to-typescript","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-05-08-migrating-azure-functions-from-jsdoc-javascript-to-typescript/index.md","source":"@site/blog/2023-05-08-migrating-azure-functions-from-jsdoc-javascript-to-typescript/index.md","title":"Migrating Azure Functions from JSDoc JavaScript to TypeScript","description":"Azure Functions can be written in JavaScript or TypeScript. This post will demonstrate how to migrate an Azure Function from JSDoc JavaScript to TypeScript.","date":"2023-05-08T00:00:00.000Z","tags":[{"inline":false,"label":"Azure Functions","permalink":"/tags/azure-functions","description":"The Azure Functions service."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."},{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."}],"readingTime":7.475,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"migrating-azure-functions-from-jsdoc-javascript-to-typescript","title":"Migrating Azure Functions from JSDoc JavaScript to TypeScript","authors":"johnnyreilly","tags":["azure functions","typescript","javascript"],"image":"./title-image.png","description":"Azure Functions can be written in JavaScript or TypeScript. This post will demonstrate how to migrate an Azure Function from JSDoc JavaScript to TypeScript.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"TypeScript 5: importsNotUsedAsValues replaced by ESLint consistent-type-imports","permalink":"/typescript-5-importsnotusedasvalues-error-eslint-consistent-type-imports"},"nextItem":{"title":"Teams Direct Message API with Power Automate","permalink":"/ms-teams-direct-message-api"}},"content":"I wrote previously about how to implement a [dynamic redirect mechanism for Azure Static Web Apps using Azure Functions](../2022-12-22-azure-static-web-apps-dynamic-redirects-azure-functions/index.md). I implemented this using JSDoc JavaScript. I\'ve since migrated this to TypeScript and I thought it would be interesting to share the process.\\n\\n![title image reading \\"Migrating Azure Functions from JSDoc JavaScript to TypeScript\\" with the JS and TS logos and Azure Functions logo](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Why migrate from JSDoc JavaScript to TypeScript?\\n\\nAs regular readers will know, I\'m both a big fan of TypeScript and a big fan of JSDoc JavaScript. I think both are great. So why migrate from JSDoc JavaScript to TypeScript? For me it\'s mostly about the developer experience; JSDoc is more verbose, and the wider ecosystem does a lot more TypeScript than it does JSDoc JavaScript. So when you get beyond a simple application, for me at least, TypeScript is a better choice.\\n\\n[My blog](https://johnnyreilly.com/) is an [Azure Static Web App](https://docs.microsoft.com/en-us/azure/static-web-apps/overview) with an Azure Functions back end. I\'ve been using JSDoc JavaScript for my Azure Functions. This post will take the back end and migrate it to TypeScript. There\'s various affordances that I have in place already that I don\'t want to lose along the way:\\n\\n- I can debug in VS Code\\n- I deploy using GitHub Actions\\n- I have automated tests in place using Jest\\n\\nAll of these affordances are available to me with TypeScript, and I want to keep them. Let\'s begin migrating. Incidentally, the code for this migration [lies in this PR](https://github.com/johnnyreilly/blog.johnnyreilly.com/pull/558).\\n\\n## Migrating the `tsconfig.json`\\n\\nOur `tsconfig.json` manages the way the TypeScript compiler interacts with our code. We\'ll start by migrating this:\\n\\n```diff title=\\"tsconfig.json\\"\\n{\\n  \\"compilerOptions\\": {\\n-    \\"noEmit\\": true,\\n+    \\"noEmit\\": false,\\n+    \\"outDir\\": \\"dist\\",\\n+    \\"rootDir\\": \\".\\",\\n+    \\"sourceMap\\": true,\\n\\n+    \\"allowJs\\": false,\\n+    \\"checkJs\\": false,\\n\\n+    \\"moduleResolution\\": \\"node\\",\\n  }\\n}\\n```\\n\\nLet\'s look at the changes we\'ve made:\\n\\n- We\'re now emitting files from our compilation thanks to `noEmit` being set to `false`. We will no longer be actually running the code we write, but we will run the JavaScript we emit.\\n- We\'re specifying an `outDir` of `dist` - this is where our compiled JavaScript will be emitted.\\n- We\'re specifying a `rootDir` of `.` - this is the root of our TypeScript source files.\\n- We\'re creating source maps for our emitted JavaScript files - this will help us debug our original source code. (Even though we\'re not running it.)\\n- We\'re no longer allowing JavaScript files to be part of our program thanks to `allowJs` being set to `false`. (And we\'re not checking them either thanks to `checkJs` being set to `false` - I suspect this is implicitly set to `false` to `allowJs` being `false` - just to be clear I\'ve specified it.)\\n- We\'re specifying a `moduleResolution` of `node` - this is how TypeScript will look up a file from a given module specifier.\\n\\n## Migrating the `package.json`\\n\\nTo migrate our `package.json` we\'ll need to add some dependencies and tweak our scripts:\\n\\n```diff title=\\"package.json\\"\\n  \\"scripts\\": {\\n    \\"build\\": \\"tsc\\",\\n+    \\"watch\\": \\"tsc -w\\",\\n+    \\"prestart\\": \\"npm run build\\",\\n    \\"start\\": \\"func start\\",\\n    \\"test\\": \\"jest\\"\\n  },\\n  \\"devDependencies\\": {\\n    \\"@azure/functions\\": \\"^3.5.0\\",\\n+    \\"@types/jest\\": \\"^29.2.5\\",\\n+    \\"@types/node\\": \\"^18.x\\",\\n    \\"jest\\": \\"^29.3.1\\",\\n+    \\"ts-jest\\": \\"^29.1.0\\",\\n    \\"typescript\\": \\"^5.0.0\\"\\n  }\\n}\\n```\\n\\nWe\'re adding two scripts:\\n\\n- `watch` - this will watch our TypeScript files and recompile them when they change - we don\'t need this really; but it can be useful depending on your preferred workflow.\\n- `prestart` - this will run before `start` and will ensure our TypeScript files are compiled, and we have up to date JavaScript to run.\\n\\nIn our `devDependencies` we\'re adding dependencies for Jest and Node.js. We\'re also adding `ts-jest` which will allow us to run Jest tests written in TypeScript.\\n\\n## Migrating `.vscode/settings.json` and `.vscode/tasks.json`\\n\\nI mentioned debugging earlier - to get that in place we need to work on the `settings.json` and `tasks.json` files in the `.vscode` folder. First the `settings.json`:\\n\\n```diff title=\\"settings.json\\"\\n-  \\"azureFunctions.projectLanguage\\": \\"JavaScript\\",\\n+  \\"azureFunctions.projectLanguage\\": \\"TypeScript\\",\\n```\\n\\nThe above is pretty self explanatory. We\'re changing the language of our Azure Functions project from JavaScript to TypeScript. Now the `tasks.json`:\\n\\n```diff title=\\"tasks.json\\"\\n{\\n  \\"version\\": \\"2.0.0\\",\\n  \\"tasks\\": [\\n+    {\\n+      \\"type\\": \\"shell\\",\\n+      \\"label\\": \\"npm build (functions)\\",\\n+      \\"command\\": \\"npm run build\\",\\n+      \\"dependsOn\\": \\"npm install (functions)\\",\\n+      \\"problemMatcher\\": \\"$tsc\\",\\n+      \\"options\\": {\\n+        \\"cwd\\": \\"${workspaceFolder}/blog-website/api\\"\\n+      }\\n+    },\\n  ]\\n}\\n```\\n\\nWe\'ve got a new task in place here that runs our `npm run build` script. This will compile our TypeScript files to JavaScript. We\'ve also got a `cwd` set to `blog-website/api` - this is the folder where our Azure Functions live - your own will likely be different. Alongside this new script, there\'s some tweaks to existing tasks to make them depend on our new build task:\\n\\n```diff title=\\"tasks.json\\"\\n{\\n  \\"version\\": \\"2.0.0\\",\\n  \\"tasks\\": [\\n    {\\n      \\"type\\": \\"func\\",\\n      \\"label\\": \\"func: host start\\",\\n      \\"command\\": \\"host start\\",\\n      \\"problemMatcher\\": \\"$func-node-watch\\",\\n      \\"isBackground\\": true,\\n+      \\"dependsOn\\": \\"npm build (functions)\\",\\n+      \\"options\\": {\\n+        \\"cwd\\": \\"${workspaceFolder}/blog-website/api\\"\\n+      }\\n    },\\n    // ...\\n    {\\n      \\"type\\": \\"shell\\",\\n      \\"label\\": \\"npm prune (functions)\\",\\n      \\"command\\": \\"npm prune --production\\",\\n+      \\"dependsOn\\": \\"npm build (functions)\\",\\n      \\"problemMatcher\\": [],\\n      \\"options\\": {\\n        \\"cwd\\": \\"${workspaceFolder}/blog-website/api\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\n## Migrate our Azure Functions\\n\\nThis isn\'t going to be an exhaustive guide of migrating from JSDoc JavaScript to TypeScript. I\'m going to focus on the things that I found most relevant to Azure Functions. Let\'s start with the `fallback/function.json` file that powers our dynamic redirects and lives at `/api/fallback`:\\n\\n```diff title=\\"fallback/function.json\\"\\n{\\n  \\"bindings\\": [\\n    {\\n      \\"authLevel\\": \\"anonymous\\",\\n      \\"type\\": \\"httpTrigger\\",\\n      \\"direction\\": \\"in\\",\\n      \\"name\\": \\"req\\",\\n      \\"methods\\": [\\"get\\", \\"post\\"]\\n    },\\n    {\\n      \\"type\\": \\"http\\",\\n      \\"direction\\": \\"out\\",\\n      \\"name\\": \\"res\\"\\n    }\\n  ],\\n+  \\"scriptFile\\": \\"../dist/fallback/index.js\\"\\n}\\n```\\n\\nThere\'s one addition here, to repoint the `scriptFile` to the compiled JavaScript.\\n\\nNow let\'s look at the code for the function. Originally a JavaScript file named `index.js`; it must be renamed to `index.ts`. The original code looked like this:\\n\\n```js title=\\"fallback/index.js\\"\\nconst redirect = require(\'./redirect\');\\nconst saveToDatabase = require(\'./saveToDatabase\');\\n\\n/**\\n *\\n * @param { import(\\"@azure/functions\\").Context } context\\n * @param { import(\\"@azure/functions\\").HttpRequest } req\\n */\\nasync function fallback(context, req) {\\n  //...\\n}\\n\\nmodule.exports = fallback;\\n```\\n\\nAfter renaming to `index.ts` and adding some TypeScript types, it looks like this:\\n\\n```ts title=\\"fallback/index.ts\\"\\nimport type { AzureFunction, Context, HttpRequest } from \'@azure/functions\';\\n\\nimport { redirect } from \'./redirect\';\\nimport { saveToDatabase } from \'./saveToDatabase\';\\n\\nconst httpTrigger: AzureFunction = async function (\\n  context: Context,\\n  req: HttpRequest,\\n): Promise<void> {\\n  //...\\n};\\n\\nexport default httpTrigger;\\n```\\n\\nAs we can see, the type importing becomes much more succinct. We\'re also exporting the function as `default` rather than `module.exports`. This is because we\'re using ES Modules rather than CommonJS modules for authoring. We can also see that we\'re using `import` rather than `require` to import our functions. Whilst CommonJS was more straightforward to use, the syntax for ES Modules feels much nicer to use; at least to me. (It\'s worth noting that we\'re not using ES Modules in our compiled JavaScript - we\'re still using CommonJS there; but we don\'t need to think about that when we\'re writing our code.)\\n\\nI won\'t walk through migrating the other files, but the process is the same. Rename the file to `.ts`, add TypeScript types, and use `import` rather than `require`.\\n\\n## Migrating our tests\\n\\nWe\'re pretty much on the home stretch now; we just need to migrate our tests. Let\'s start with the `jest.config.js`:\\n\\n```diff title=\\"jest.config.js\\"\\n+  preset: \'ts-jest\',\\n+  testPathIgnorePatterns: [\'<rootDir>/node_modules/\', \'<rootDir>/dist/\'],\\n```\\n\\nWe\'re adding a `preset` of `ts-jest` which will allow us to run TypeScript tests. If you recall we added the `ts-jest` dependency earlier; it was for this.\\n\\nWe\'re also adding `dist` to our `testPathIgnorePatterns` - this is because we don\'t want to run our tests against our compiled JavaScript - we\'d end up running the tests twice without this.\\n\\nLet\'s turn our attention to the tests directly. Again we do the classic rename from `.js` to `.ts`, and our imports become terser and more ES Module-y:\\n\\n```diff title=\\"redirect.test.ts\\"\\n-/**\\n- * @typedef { import(\\"@azure/functions\\").Logger } Logger\\n- */\\n+import type { Logger } from \'@azure/functions\';\\n-const { describe, expect, test } = require(\'@jest/globals\');\\n+import { describe, expect, test } from \'@jest/globals\';\\n-const redirect = require(\'./redirect\');\\n+import { redirect } from \'./redirect\';\\n```\\n\\nBeautiful. Finally we\'ve got some tweaks to the code of the tests themselves. Firstly, declaring our mock becomes much easier:\\n\\n```diff title=\\"redirect.test.ts\\"\\n-/** @type {jest.Mock<Logger>} */ const mockLogger = jest.fn();\\n+const mockLogger: jest.Mock<Logger> = jest.fn();\\n```\\n\\nSecondly, casting our mock to the type we want is much more straightforward:\\n\\n```diff title=\\"redirect.test.ts\\"\\n-/** @type {any} */ (mockLogger)\\n+mockLogger as unknown as Logger\\n```\\n\\nI have no real how to `as` cast twice in JSDoc - and now I don\'t need to.\\n\\n## Conclusion\\n\\nWe now have a TypeScript Azure Functions codebase, with all of the debugging / testing / deployment affordances we had before. We didn\'t have to do anything around deployment, because it just works\u2122\uFE0F. I hope this post has been useful to you!"},{"id":"ms-teams-direct-message-api","metadata":{"permalink":"/ms-teams-direct-message-api","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-05-04-ms-teams-direct-message-api/index.md","source":"@site/blog/2023-05-04-ms-teams-direct-message-api/index.md","title":"Teams Direct Message API with Power Automate","description":"Teams does not have a public API for sending direct messages to people rather than channels. This post describes a workaround using Power Automate and the Teams Notification API.","date":"2023-05-04T00:00:00.000Z","tags":[],"readingTime":9.725,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null},{"name":"Chris Tacey-Green","title":"Engineer, Architect, Human","url":"https://github.com/ctaceygreen","image_url":"https://avatars.githubusercontent.com/u/11404995?v=4","imageURL":"https://avatars.githubusercontent.com/u/11404995?v=4","socials":{},"key":null,"page":null}],"frontMatter":{"slug":"ms-teams-direct-message-api","title":"Teams Direct Message API with Power Automate","authors":["johnnyreilly",{"name":"Chris Tacey-Green","title":"Engineer, Architect, Human","url":"https://github.com/ctaceygreen","image_url":"https://avatars.githubusercontent.com/u/11404995?v=4","imageURL":"https://avatars.githubusercontent.com/u/11404995?v=4"}],"tags":[],"image":"./title-image.png","description":"Teams does not have a public API for sending direct messages to people rather than channels. This post describes a workaround using Power Automate and the Teams Notification API.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Migrating Azure Functions from JSDoc JavaScript to TypeScript","permalink":"/migrating-azure-functions-from-jsdoc-javascript-to-typescript"},"nextItem":{"title":"Docusaurus: Structured Data FAQs with MDX","permalink":"/docusaurus-structured-data-faqs-mdx"}},"content":"I\'ve written previously about [sending Teams notifications using a webhook](../2019-12-18-teams-notification-webhooks/index.md), and it\'s a technique I\'ve used a lot. But I\'ve always wanted to be able to send a direct message to a user, and that\'s not possible with the webhook approach. I work with a marvellous fellow named Chris Tacey-Green, and he\'s figured out a way to do this using Power Automate and the Teams Notification API. I\'m going to describe how he did it here, with a little help from him.\\n\\nIt\'s probably worth saying, both Chris and I work for Investec, and we\'re going to share some of the things we\'ve learned about using Teams and Power Automate in the hope that it\'s useful to others. But we\'re not speaking on behalf of Investec, and we\'re not suggesting that this is the best way to do things. This is likely in the \\"do things that do not scale\\" category. Significantly though, it works!\\n\\nYou will see some screenshots of our internal Teams environment, but we\'ve tried to keep them to a minimum, and we\'re not going to share any sensitive information.\\n\\n![title image reading \\"Teams Direct Message API with Power Automate\\" with a Teams logo](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## What are we trying to do?\\n\\nTeams is a great way to keep people informed of what\'s going on. But sometimes you want to send a message to a specific person. You can @mention them in a channel, but that\'s not the same as a direct message. And you can send them an email, but that\'s not the same as a direct message either. What we want is a way to send a direct message to a user in Teams, via an API. Because we want to automate. Tragically, sending DMs in Teams via an API is not supported at the time of writing (or if it is - please let us know!)\\n\\nAll is not lost.\\n\\n## Power Automate\\n\\nPower Automate is a tool that allows you to automate tasks. It\'s a low-code/no-code tool that allows you to create workflows that can be triggered by events. It\'s a great tool for automating tasks. And it\'s a great tool for sending direct messages in Teams. Or so it turns out.\\n\\nYou see, it\'s possible to send a notification to a user in Teams using the Teams Notification API. And it\'s possible to trigger a Power Automate workflow using the \\"when a new channel message is added\\" trigger. So if we can get a notification to the Teams Notification API, we can trigger a Power Automate workflow. And if we can trigger a Power Automate workflow, we can send a direct message to a user in Teams.\\n\\nThis post will do two things:\\n\\n1. Describe how to set up a Power Automate workflow that sends a direct message to a user in Teams\\n2. Describe how to trigger that workflow using the Teams Notification API and Adaptive Card messages\\n\\n## Setting up the Power Automate workflow\\n\\nFirst things first, we need to create a Power Automate workflow that sends a direct message to a user in Teams. This is pretty straightforward, but when it came to doing this, I knew _nothing_.\\n\\nBefore we start this, I should warn you there\'s going to be lots of screenshots. As far as I\'m aware, there\'s not a code-first way to create Power Automate flows, so point and click is the only game in town.\\n\\nThe first thing to do, is fire up Teams and install the Power Automate app:\\n\\n![screenshot of installing the Power Automate app](screenshot-power-automate-app.webp)\\n\\nThis allows us to access the Power Automate app. There we can create a new workflow (from blank) with the \\"when a new channel message is added\\" trigger:\\n\\n![screenshot of creating the Power Automate flow with the when a new channel message is added trigger](screenshot-power-automate-when-a-new-channel-message-is-added.webp)\\n\\nWe then need to select the team and channel that we want to trigger the workflow:\\n\\n![screenshot of selecting the team and channel](screenshot-power-automate-team-and-channel.webp)\\n\\nYou\'ll note in the screenshot above, we\'ve got a dedicated channel for this workflow. This is because we don\'t want to disturb channels people are already using with the messages we will write to this channel. To all intents and purposes, this channel could actually be invisible to users - we just need it to exist to be our carrier pidgeon.\\n\\nWith the trigger in place, we need to create the action. We\'re going to use the \\"apply to each\\" control, which will run for every message that comes through. We want to use the \\"Message mentions\\" output, which will give us the user that was mentioned in the message. We don\'t want to use the similarly named \\"Message mentions item\\":\\n\\n![screenshot of selecting the output from previous steps](screenshot-power-automate-select-output-previous-steps.webp)\\n\\nThe message mention gives us our user, we\'ll then use the \\"Get user profile (V2)\\" operation so we can look up that user up.\\n\\nIt\'s at this point, that we start to do something slightly more complicated in the Power Automate UI. We\'re going to need to supply an id to the \\"Get user profile (V2)\\" operation. We\'re going to use an expression to determine this id. The expression we\'re going to use is:\\n\\n```\\ntriggerOutputs()?[\'body/mentions\'][0].mentioned.user.id\\n```\\n\\nThis expression is operating on JSON similar to the following:\\n\\n```json\\n{\\n  \\"@odata.type\\": \\"#microsoft.graph.chatMessage\\",\\n  \\"etag\\": \\"1683099494281\\",\\n  \\"messageType\\": \\"message\\",\\n  \\"createdDateTime\\": \\"2023-05-03T07:38:14.281Z\\",\\n  \\"lastModifiedDateTime\\": \\"2023-05-03T07:38:14.281Z\\",\\n  // ...\\n  \\"mentions\\": [\\n    {\\n      \\"id\\": 0,\\n      \\"mentionText\\": \\"John Reilly\\",\\n      \\"mentioned\\": {\\n        \\"application\\": null,\\n        \\"device\\": null,\\n        \\"conversation\\": null,\\n        \\"tag\\": null,\\n        \\"user\\": {\\n          \\"@odata.type\\": \\"#microsoft.graph.teamworkUserIdentity\\",\\n          \\"id\\": \\"my-id-it-is-a-guid\\",\\n          \\"displayName\\": \\"John Reilly\\",\\n          \\"userIdentityType\\": \\"aadUser\\"\\n        }\\n      }\\n    }\\n  ]\\n  // ...\\n}\\n```\\n\\nSo that expression is just some JavaScript that gets us to the value we need; the id of the first mentioned user. We provide that in the \\"Expression\\" field and then click the \\"OK\\" button:\\n\\n![screenshot of entering that into PA](screenshot-power-automate-expression1.webp)\\n\\nNow it\'s time for our final action - sending on the message with the \\"post card in chat or channel\\" operation:\\n\\n![screenshot of adding a post card entry](screenshot-power-automate-post-card.webp)\\n\\nWe\'ll post as the Flow bot, post in a chat with the Flow bot and make our recipient \\"Mail\\" (which behind the scenes is the expression `outputs(\'Get_user_profile_(V2)\')?[\'body/mail\']`).\\n\\n![screenshot of adding the recipient mail](screenshot-power-automate-recipient-mail.webp)\\n\\nFinally, it\'s once more expression time, as we use this value for our Adaptive Card:\\n\\n```\\ntriggerOutputs()?[\'body/attachments\'][0][\'content\']\\n```\\n\\n![screenshot of entering expression](screenshot-power-automate-expression2.webp)\\n\\nHopefully, what you\'ve realised is that we\'re just taking the Adaptive Card from the message that triggered the workflow and passing it on to the recipient. We\'re intentionally doing as little as possible in the Power Automate workflow, as it\'s the trickiest part of our solution to work with. (Debugging Power Automate workflows is possible, but it\'s not the most fun you\'ll ever have.)\\n\\nWe now have our complete workflow, and it looks like this:\\n\\n![screenshot of the Power Automate workflow](screenshot-power-automate-workflow.webp)\\n\\n## Triggering the Power Automate workflow\\n\\nWhilst we have a workflow, we don\'t have anything to trigger it yet. To do that, we\'ll need to send a message to the channel we created earlier. We can do this using the Teams Notification API. And it needs to be a special kind of message; it needs to be an Adaptive Card, which includes a mention of the person we want to direct message.\\n\\nMy post on [sending Teams notifications using a webhook](../2019-12-18-teams-notification-webhooks/index.md) describes how to send a message to a channel using the Teams Notification API. We\'re going to use the same technique, but we\'re going to send an [Adaptive Card](https://github.com/Microsoft/AdaptiveCards/) instead of a plain message. I won\'t repeat the details of creating a webhook connector here, instead let\'s just focus on what we need to send to the API.\\n\\nUltimately, it\'s just a [POST request to the webhook connector](../2023-03-09-node-18-axios-and-unsafe-legacy-renegotiation-disabled/index.md), with a JSON body that looks like this:\\n\\n```json\\n{\\n  \\"type\\": \\"message\\",\\n  \\"attachments\\": [\\n    {\\n      \\"contentType\\": \\"application/vnd.microsoft.card.adaptive\\",\\n      \\"contentUrl\\": null,\\n      \\"content\\": {\\n        \\"type\\": \\"AdaptiveCard\\",\\n        \\"version\\": \\"1.0\\",\\n        \\"body\\": [\\n          {\\n            \\"type\\": \\"TextBlock\\",\\n            \\"size\\": \\"medium\\",\\n            \\"text\\": \\"Hey <at>John Reilly</at>!\\\\n\\\\nYou have 102 unresolved secret findings! Click on the button below to view them.\\",\\n            \\"wrap\\": true\\n          }\\n        ],\\n        \\"$schema\\": \\"http://adaptivecards.io/schemas/adaptive-card.json\\",\\n        \\"msteams\\": {\\n          \\"entities\\": [\\n            {\\n              \\"type\\": \\"mention\\",\\n              \\"text\\": \\"<at>John Reilly</at>\\",\\n              \\"mentioned\\": {\\n                \\"id\\": \\"John.Reilly@investec.co.uk\\",\\n                \\"name\\": \\"John Reilly\\"\\n              }\\n            }\\n          ]\\n        },\\n        \\"actions\\": [\\n          {\\n            \\"type\\": \\"Action.OpenUrl\\",\\n            \\"title\\": \\"View findings\\",\\n            \\"url\\": \\"https://some.url.com/\\",\\n            \\"role\\": \\"button\\"\\n          }\\n        ]\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nThe part that\'s relevant to us is the `msteams` property. This is where we specify the user we want to mention. We do this by specifying the `id` and `name` properties. The `id` property is the email address of the user we want to mention. The `name` property is the display name of the user we want to mention.\\n\\nThis is the secret sauce that allows our Power Automate flow to work. It reads these values and uses them to send a message to the user we want to mention. Alongside that, the `msteams.entities[].text` property must be in your message body. That\'s what Teams uses to link the mention to the part of the message that\'s \\"doing the mentioning\\" (thanks Chris for pointing this out).\\n\\n## Testing it out\\n\\nSo far, so screenshots and code. Does it work? Let\'s find out.\\n\\nWhen we run our tool for triggering the Teams Notification API, we get a message in our channel:\\n\\n![screenshot of the adaptive card in the shared teams channel](screenshot-adaptive-card-in-channel.webp)\\n\\nNote that it has the @mention of the user: me. Now that this message in the relevant channel exists, our Power Automate workflow will be triggered. I\'ve seen it take between 2 and 10 minutes for the trigger to fire. When it does, the flow runs and we get a direct message from the Flow bot:\\n\\n![screenshot of the adaptive card in a teams direct chat](screenshot-teams-direct-message.webp)\\n\\nAnd this is our handrolled direct message to a user in Microsoft Teams. It\'s the same message we sent to the channel, just forwarded on by the Flow bot. Brilliant. The eagle eyed amongst you will note the ugly `<at id=\\"0\\">John Reilly</at>`; we\'re losing something in our forwarding. This could be remedied if we made our Power Automate flow a little more complex, but as mentioned, we\'re trying to keep our flow as simple as possible. The `<at id=\\"0\\">` is a small price to pay for the simplicity of our flow.\\n\\n## User blocked the conversation with the bot\\n\\nAs you look at your Power Automate Flow runs, you can sometimes spot failures along these lines:\\n\\n![screenshot of a failed run of the power automate flow](screenshot-power-automate-flow-failure.webp)\\n\\nIf you look to the right on that screenshot you can see the error message:\\n\\n```json\\nRequest to the Bot framework failed with error:\\n\\n{\\n  \\"error\\": {\\n    \\"code\\":\\"ConversationBlockedByUser\\",\\n    \\"message\\":\\"User blocked the conversation with the bot.\\"\\n  }\\n}\\n```\\n\\nThese kinds of failures appear to be a consequence of someone having blocked the Power Automate Flow bot. If you dig into the inputs (\\"Click to download\\" in the screenshot) you can discover the user who blocked the bot and have a conversation with them about it. Unblocking seems to be fairly straightforward; you just need to right-click / ctrl-click on the Power Automate app in Teams and select \\"Unblock\\":\\n\\n![screenshot of unblocking the bot](screenshot-unblock-the-bot.webp)\\n\\nIn our experience, this is a rare occurrence, but it\'s worth being aware of.\\n\\n## Conclusion\\n\\nTogether we\'ve built a mechanism for sending a direct message to a user in Microsoft Teams. We\'ve used the Teams Notification API to send a message to a channel, and we\'ve used Power Automate to pick up that message and send it on to the user we want to mention.\\n\\nThanks so much to Chris for coming up with this novel way of sending a direct message to a user in Microsoft Teams. I hope you\'ve found this post useful."},{"id":"docusaurus-structured-data-faqs-mdx","metadata":{"permalink":"/docusaurus-structured-data-faqs-mdx","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-04-08-docusaurus-structured-data-faqs-mdx/index.md","source":"@site/blog/2023-04-08-docusaurus-structured-data-faqs-mdx/index.md","title":"Docusaurus: Structured Data FAQs with MDX","description":"This demos how to make an MDX component that renders FAQs into a page, and the same information as Structured Data. It also shows how to use it with Docusaurus.","date":"2023-04-08T00:00:00.000Z","tags":[{"inline":false,"label":"Docusaurus","permalink":"/tags/docusaurus","description":"The Docusaurus static site generator."},{"inline":false,"label":"SEO","permalink":"/tags/seo","description":"Search Engine Optimization."}],"readingTime":6.185,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"docusaurus-structured-data-faqs-mdx","title":"Docusaurus: Structured Data FAQs with MDX","authors":"johnnyreilly","tags":["docusaurus","seo"],"image":"./title-image.png","description":"This demos how to make an MDX component that renders FAQs into a page, and the same information as Structured Data. It also shows how to use it with Docusaurus.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Teams Direct Message API with Power Automate","permalink":"/ms-teams-direct-message-api"},"nextItem":{"title":"Bicep user defined types and Bash single item arrays","permalink":"/bicep-user-defined-types-and-bash-single-item-arrays"}},"content":"import FAQStructuredData from \\"../../src/theme/MDXComponents/FAQStructuredData\\";\\n\\nexport const faqs = [\\n{\\nquestion: \\"How do I use the FAQ Structured Data component?\\",\\nanswer:\\n\\"Simply create an import statement for the component and then use it in your MDX file. You\'ll need to pass in an array of FAQs. This array can be inline, you can declare it as a variable, or you can import it from another file.\\",\\n},\\n{\\nquestion: \\"How do I use the FAQ Structured Data component in a blog post?\\",\\nanswer:\\n\\"The usage is the same as in a regular MDX file. But the import statement will sit after the frontmatter of the blog post.\\",\\n},\\n{\\nquestion:\\n\\"Can I use the FAQ Structured Data component in a regular MD file?\\",\\nanswer: \\"Yes! It just works\u2122\uFE0F.\\",\\n},\\n];\\n\\nI\'ve written previously about [using Structured Data with React](../2021-10-15-structured-data-seo-and-react/index.md). This post goes a little further and talks about how to use Structured Data with Docusaurus and MDX. More specifically it looks at how to create a component that both renders FAQs into a page, and the same information as Structured Data.\\n\\n![title image reading \\"Docusaurus: Structured Data FAQs with MDX\\" with a Docusaurus logo](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## FAQs and Structured Data\\n\\nI\'ve been working with [Growtika](https://growtika.com/) to repair my SEO after [shredding it somehow last year](../2023-01-15-how-i-ruined-my-seo/index.md). One of the experiments we ran was to add [FAQs to a post](../2023-02-01-migrating-from-github-pages-to-azure-static-web-apps/index.md), and with that, the equivalent FAQ Structured Data. The intent being to see if this would help with the SEO for that post.\\n\\nMy blog is written in [MDX](https://mdxjs.com/), and hosted on [Docusaurus](https://docusaurus.io/). I wanted to see if I could create an MDX component that would render the FAQs into a page, and the same information as Structured Data. The [Docusaurus docs suggested this was feasible](https://docusaurus.io/docs/markdown-features/react), and I wanted to see if I could make it work.\\n\\nAnd it turns out that other people are interested in this too; there\'s a feature request on [Docusaurus\'s Canny](https://docusaurus.io/feature-requests/p/creation-of-structured-faq) for exactly this.\\n\\nSo I created a component that could be used to render FAQs into a page as markdown, and the same information as Structured Data. I thought it would be useful to share that component with the world. Hello world, herewith the component:\\n\\n## The FAQStructuredData MDX component\\n\\nI created a directory called `FAQStructuredData` in the ``src/theme/MDXComponents` directory. This directory is where I keep my custom MDX components. I then created an `index.js` file in that directory. This is the file that contains the component:\\n\\n```jsx title=\\"src/theme/MDXComponents/FAQStructuredData/index.js\\"\\n/**\\n * @typedef { import(\'./types\').FAQStructuredDataProps } FAQStructuredDataProps\\n * @typedef { import(\'./types\').FAQStructuredData } FAQStructuredData\\n */\\n\\nimport React from \'react\';\\n\\n/**\\n * A component that renders a FAQ structured data and markdown entries\\n *\\n * @see https://developers.google.com/search/docs/appearance/structured-data/faqpage\\n * @param {FAQStructuredDataProps} props\\n * @returns\\n */\\nexport default function FAQStructuredData(props) {\\n  /** @type {FAQStructuredData} */ const faqStructuredData = {\\n    \'@context\': \'https://schema.org\',\\n    \'@type\': \'FAQPage\',\\n    mainEntity: props.faqs.map((faq) => ({\\n      \'@type\': \'Question\',\\n      name: faq.question,\\n      acceptedAnswer: {\\n        \'@type\': \'Answer\',\\n        text: faq.answer,\\n      },\\n    })),\\n  };\\n  return (\\n    <>\\n      <script\\n        type=\\"application/ld+json\\"\\n        dangerouslySetInnerHTML={{ __html: JSON.stringify(faqStructuredData) }}\\n      />\\n\\n      <h2>FAQs</h2>\\n      {faqStructuredData.mainEntity.map((faq) => (\\n        <React.Fragment key={faq.name}>\\n          <h3>{faq.name}</h3>\\n          {faq.acceptedAnswer.text}\\n        </React.Fragment>\\n      ))}\\n    </>\\n  );\\n}\\n```\\n\\n```ts title=\\"src/theme/MDXComponents/FAQStructuredData/types.d.ts\\"\\nexport interface FAQ {\\n  question: string;\\n  answer: string;\\n}\\n\\nexport interface FAQStructuredDataProps {\\n  faqs: FAQ[];\\n}\\n\\nexport interface FAQStructuredData {\\n  \'@context\': string;\\n  \'@type\': string;\\n  mainEntity: FAQQuestionStructuredData[];\\n}\\n\\nexport interface FAQQuestionStructuredData {\\n  \'@type\': \'Question\';\\n  name: string;\\n  acceptedAnswer: {\\n    \'@type\': \'Answer\';\\n    text: string;\\n  };\\n}\\n```\\n\\nSome things to note about this component:\\n\\n- The code is written in JavaScript, but it\'s using [TypeScript types via JSDoc](../2021-11-22-typescript-vs-jsdoc-javascript/index.md). I don\'t believe you can write MDX components in TypeScript (please someone let me know if it turns out this is possible). But static typing is useful, and still possible thanks to JSDoc.\\n- On that, we have a `types.d.ts` file that contains the types for the component. Using TypeScript directly is still possible alongside JSDoc, as long as there is no runtime code in the file, and a definition file (which the `types.d.ts` file is), has no runtime code. We can simply use it to store types that we import into the component.\\n- The component expects an `faqs` prop. This is an array of FAQs. Each FAQ is an object with a `question` and `answer` property. The component then renders the FAQs as markdown, and the same information as JSON-LD Structured Data. We\'re using the Google guidelines for [FAQ Structured Data](https://developers.google.com/search/docs/appearance/structured-data/faqpage#examples).\\n- The component renders a `h2` tag titled \\"FAQs\\". Under that, each FAQ is rendered with a `h3` tag and the answer sits directly below it.\\n\\n## Importing our MDX component\\n\\nNow the `FAQStructuredData` component is created, we can use it in our MDX (or straight MD) files. We can import the component and create an array called `faqs` like so:\\n\\n```mdx\\n---\\nslug: docusaurus-structured-data-faqs-mdx\\ntitle: \'Docusaurus: Structured Data FAQs with MDX\'\\nauthors: johnnyreilly\\ntags: [Docusaurus, Structured Data]\\nimage: ./title-image.png\\ndescription: \'This demos how to make an MDX component that renders FAQs into a page, and the same information as Structured Data. It also shows how to use it with Docusaurus.\'\\nhide_table_of_contents: false\\n---\\n\\nimport FAQStructuredData from \'../../src/theme/MDXComponents/FAQStructuredData\';\\n\\nexport const faqs = [\\n  {\\n    question: \'How do I use the FAQ Structured Data component?\',\\n    answer:\\n      \\"Simply create an import statement for the component and then use it in your MDX file. You\'ll need to pass in an array of FAQs. This array can be inline, you can declare it as a variable, or you can import it from another file.\\",\\n  },\\n  {\\n    question: \'How do I use the FAQ Structured Data component in a blog post?\',\\n    answer:\\n      \'The usage is the same as in a regular MDX file. But the import statement will sit after the frontmatter of the blog post.\',\\n  },\\n  {\\n    question:\\n      \'Can I use the FAQ Structured Data component in a regular MD file?\',\\n    answer: \'Yes! It just works\u2122\uFE0F.\',\\n  },\\n];\\n\\n;\\n```\\n\\nNote we\'re doing this in a blog post and the import statement is after the frontmatter. Then we can use the component like so:\\n\\n```mdx\\n<FAQStructuredData faqs={faqs} />\\n```\\n\\nWe\'ll do that, right here, right now:\\n\\n<FAQStructuredData faqs={faqs} />\\n\\n## Testing the Structured Data\\n\\nYou can see, we have FAQs rendered in the body of our blog post. If we put the URL of the post into the [Google Structured Data Testing Tool](https://search.google.com/test/rich-results?url=https%3A%2F%2Fjohnnyreilly.com%2Fdocusaurus-structured-data-faqs-mdx), we can see that the Structured Data is being rendered correctly:\\n\\n![Screenshot of rich results test showing FAQs are detected](screenshot-rich-results-test.webp)\\n\\nWe can even go one better, shortly after I posted this article, I did a search in Google for \\"how do you have Docusaurus with Structured Data FAQs with MDX\\" and I got this:\\n\\n![A screenshot of the Google search window with the search \'how do you have Docusaurus with Structured Data FAQs with MDX\' and the FAQs showing up as a featured snippet](screenshot-featured-snippets-faqs.webp)\\n\\nThat\'s our FAQs being surfaced as a [featured snippet](https://support.google.com/websearch/answer/9351707?hl=en-GB&visit_id=638180439903372599-4066254776&p=featured_snippets&rd=1#zippy=%2Cwhy-featured-snippets-may-be-removed). Nice!\\n\\n## Conclusion\\n\\nWe\'ve now got a reusable FAQs component that renders the FAQs as markdown, and the same information as Structured Data. We can use it in our MDX files, and we can use it in our blog posts. We can also use it in regular MD files. Yay! I\'ve only used this in the context of Docusaurus, but I suspect it can be used in other contexts too.\\n\\nI\'d rather like it if this was built into Docusaurus, and if it could read directly from the Markdown files. But this is a good start. I hope you find it useful."},{"id":"bicep-user-defined-types-and-bash-single-item-arrays","metadata":{"permalink":"/bicep-user-defined-types-and-bash-single-item-arrays","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-04-05-bicep-user-defined-types-and-bash-single-item-arrays/index.md","source":"@site/blog/2023-04-05-bicep-user-defined-types-and-bash-single-item-arrays/index.md","title":"Bicep user defined types and Bash single item arrays","description":"The error \\"Expected a value of type \\\\\'Array\\\\\', but received a value of type \\\\\'String\\\\\'\\", presents when wrestling with the AZ CLI, Bash single item arrays and Bicep.","date":"2023-04-05T00:00:00.000Z","tags":[{"inline":false,"label":"Bicep","permalink":"/tags/bicep","description":"The Bicep language for Azure Resource Manager templates."}],"readingTime":4.025,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"bicep-user-defined-types-and-bash-single-item-arrays","title":"Bicep user defined types and Bash single item arrays","authors":"johnnyreilly","tags":["bicep"],"image":"./title-image.png","description":"The error \\"Expected a value of type \\\\\'Array\\\\\', but received a value of type \\\\\'String\\\\\'\\", presents when wrestling with the AZ CLI, Bash single item arrays and Bicep.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Docusaurus: Structured Data FAQs with MDX","permalink":"/docusaurus-structured-data-faqs-mdx"},"nextItem":{"title":"Playwright, GitHub Actions and Azure Static Web Apps staging environments","permalink":"/playwright-github-actions-and-azure-static-web-apps-staging-environments"}},"content":"When sending a single item array to a Bicep template you may get an error like this:\\n\\n```bash\\nERROR: InvalidTemplate - Deployment template validation failed: \'Template parameter \'allowedIPAddresses\' was provided an invalid value. Expected a value of type \'Array\', but received a value of type \'String\'.\\n```\\n\\nThis is down to the fact that Bash arrays when used with the Azure CLI can be a little surprising. If we initialise a single item array then it\'s not an array. It\'s a string. This is a bit of a pain when you\'re trying to pass a single item array to a Bicep template. It\'s possible to work around this with JSON and Bicep user defined types. Let\'s see how.\\n\\n![title image reading \\"Bicep user defined types and Bash single item arrays\\" with a Bicep logo](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Expected a value of type \'Array\', but received a value of type \'String\'\\n\\nI had a Bicep template that took a parameter of type `array`:\\n\\n```bicep\\nparam allowedIPAddresses array\\n```\\n\\nI was invoking this template using the Azure CLI, in a Bash script. (Technically using GitHub Actions; but that\'s somewhat by the by.) I wanted to pass a single item array to the template. I did this:\\n\\n```bash\\naz deployment group create \\\\\\n  --resource-group testgroup \\\\\\n  --template-file <path-to-template> \\\\\\n  --parameters allowedIPAddresses=\'(\\"8.8.8.8\\")\'\\n```\\n\\nSurprisingly, this resulted in the error:\\n\\n```bash\\nERROR: InvalidTemplate - Deployment template validation failed: \'Template parameter \'allowedIPAddresses\' was provided an invalid value. Expected a value of type \'Array\', but received a value of type \'String\'.\\n```\\n\\nDespite following the [documentation for passing arrays](https://learn.microsoft.com/en-us/azure/azure-resource-manager/templates/deploy-cli#inline-parameters), passing a single item array to the template did not work.\\n\\nI\'m not the only person who has wrestled with this. There\'s a [GitHub issue](https://github.com/Azure/bicep/issues/5936) on the Bicep repo that discusses this. The issue is that Bash arrays when used with the Azure CLI can be a little surprising. If I initialise a single item array then it\'s not an array. It\'s a string. This is a bit of a pain when you\'re trying to pass a single item array to a Bicep template.\\n\\n## Workaround: JSON and Bicep user defined types\\n\\nThere are ways to make the array syntax work, but they\'re not very intuitive. I wanted to avoid this; I put a premium on understanding my code and make choices to optimise for that. The solution I came up with was to use JSON and Bicep user defined types.\\n\\nPassing JSON to the Azure CLI is pretty easy. You just need to wrap the JSON in single quotes. I could do this:\\n\\n```bash\\nanArrayInJSON=\'{\\"allowedIPAddresses\\":[\\"8.8.8.8\\"]}\'\\naz deployment group create \\\\\\n    --name showJSON  \\\\\\n    --resource-group myResourceGroup \\\\\\n    --template-file $templateFile \\\\\\n    --parameters anArrayInJSON=\\"$anArrayInJSON\\"\\n```\\n\\nThe syntax is very simple and, as we can see, it\'s possible to have properties which are arrays. This is great. I can pass a JSON object to the Azure CLI and it\'ll be parsed correctly. So I can do this:\\n\\n```bicep\\nparam anArrayInJSON object\\n\\n//...\\n\\nvar allowedIPAddresses = anArrayInJSON.allowedIPAddresses\\n```\\n\\nThis works, but I miss the type safety of Bicep. I want to be able to say that `allowedIPAddresses` is an array. And if I can go further, I\'d like to say it\'s a `string` array also. I can do this with a [Bicep user defined type](https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/user-defined-data-types). It\'s worth noting that user defined types are a new feature in Bicep and you\'ll need to use the latest version of Bicep to use them and opt in by putting this option in your `bicepconfig.json` file:\\n\\n```json\\n{\\n  \\"experimentalFeaturesEnabled\\": {\\n    \\"userDefinedTypes\\": true\\n  }\\n}\\n```\\n\\nWith that in place we can redefine `anArrayInJSON` as a user defined type:\\n\\n```bicep\\nparam anArrayInJSON {\\n  allowedIPAddresses: string[]\\n}\\n```\\n\\nThis is a little more verbose, but it\'s a lot more explicit. We\'re saying that `anArrayInJSON` is an object with a property called `allowedIPAddresses` which is an array of strings. This is great. We can now use `anArrayInJSON.allowedIPAddresses` in our template and we\'ll get type safety. We\'ll also get helpful error messages if we pass the wrong type of data to the template:\\n\\n```bash\\nERROR: InvalidTemplate - Deployment template validation failed: \'Template parameter \'anArrayInJSON.allowedIPAddresses\' was provided an invalid value. Expected a value of type \'Array\', but received a value of type \'Null\'. Please see https://aka.ms/arm-create-parameter-file for usage details.\'.\\n```\\n\\n## Conclusion\\n\\nInterestingly, I\'d say that I\'m unlikely to ever use a Bicep parameter of type `array` again, precisely for the reason that I\'ve outlined here. So none of this:\\n\\n```bicep\\nparam anArray array\\n```\\n\\nAnd none of its user defined type equivalent:\\n\\n```bicep\\nparam anArray string[]\\n```\\n\\nI\'ll probably use the approach I\'ve outlined here instead. I\'ll pass a JSON object to the template and then use a user defined type to define the properties of that object. This is a little more verbose, but it\'s a lot more explicit. I think that\'s a good trade-off."},{"id":"playwright-github-actions-and-azure-static-web-apps-staging-environments","metadata":{"permalink":"/playwright-github-actions-and-azure-static-web-apps-staging-environments","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-03-20-playwright-github-actions-and-azure-static-web-apps-staging-environments/index.md","source":"@site/blog/2023-03-20-playwright-github-actions-and-azure-static-web-apps-staging-environments/index.md","title":"Playwright, GitHub Actions and Azure Static Web Apps staging environments","description":"Azure Static Web Apps staging environments allow you to test changes before they go live. This shows how to use Playwright against staging environments.","date":"2023-03-20T00:00:00.000Z","tags":[{"inline":false,"label":"Azure Static Web Apps","permalink":"/tags/azure-static-web-apps","description":"The Azure Static Web Apps service."},{"inline":false,"label":"GitHub Actions","permalink":"/tags/github-actions","description":"The GitHub Actions CI / CD service."}],"readingTime":9.005,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"playwright-github-actions-and-azure-static-web-apps-staging-environments","title":"Playwright, GitHub Actions and Azure Static Web Apps staging environments","authors":"johnnyreilly","tags":["azure static web apps","github actions"],"image":"./title-image.png","description":"Azure Static Web Apps staging environments allow you to test changes before they go live. This shows how to use Playwright against staging environments.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Bicep user defined types and Bash single item arrays","permalink":"/bicep-user-defined-types-and-bash-single-item-arrays"},"nextItem":{"title":"Migrating from ts-node to Bun","permalink":"/migrating-from-ts-node-to-bun"}},"content":"Azure Static Web Apps staging environments allow you to test changes before they go live. This post shows how to use Playwright against staging environments with GitHub Actions. It\'s a follow up to my previous post on [using Lighthouse with Azure Static Web Apps staging environments](../2022-03-20-lighthouse-meet-github-actions/index.md).\\n\\n![title image reading \\"Playwright, GitHub Actions and Azure Static Web Apps staging environments\\" with product logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Playwright, GitHub Actions and Azure Static Web Apps\\n\\nWhat\'s the problem we\'re trying to solve? Let\'s do our best Simon Sinek impression and start with \\"Why?\\". The \\"Why?\\" is that we want only to ship changes that haven\'t broken our application.\\n\\nNow let\'s move onto \\"How?\\" The way we guard against breaking production is by running automated tests on all changes. Playwright is a tool that allows us to do that. We get a fully fledged staging environment available to us on all pull requests. We want to run Playwright integration tests against our staging environment. We want to do this as part of our CI/CD pipeline; in our GitHub Actions workflow.\\n\\nI\'m going to write about this in the context of my blog. My blog is open source and [you can find the code here](https://github.com/johnnyreilly/blog.johnnyreilly.com). I\'m going to present a simplified solution in this post, but you can find the full solution on GitHub.\\n\\n## Adding Playwright to the project\\n\\nTo add Playwright to my blog I followed the [instructions on the Playwright website](https://playwright.dev/docs/intro). Essentially I ran the following command:\\n\\n```bash\\nnpm init playwright@latest\\n```\\n\\nBy and large I accepted the defaults. However, I deleted the GitHub Actions workflow that was created, in favour of my own which we\'ll get to soon. I\'d created my tests in a `blog-website-tests` directory. This sits alongside the `blog-website` directory which contains the code for my blog.\\n\\nI made one tweak to the `playwright.config.ts` file that was created. I added the following line:\\n\\n```ts\\n//...\\n\\n/**\\n * See https://playwright.dev/docs/test-configuration.\\n */\\nexport default defineConfig({\\n  //...\\n\\n  use: {\\n    //...\\n\\n    // WE WILL SET THIS IN THE GITHUB ACTIONS WORKFLOW\\n    baseURL: process.env.PLAYWRIGHT_TEST_BASE_URL || \'http://localhost:3000\',\\n  },\\n\\n  //...\\n});\\n```\\n\\nWhat\'s going on here? I\'m setting the `baseURL` to be the value of the `PLAYWRIGHT_TEST_BASE_URL` environment variable. If that\'s not set then I\'m defaulting to `http://localhost:3000`, which is where my blog is served when running locally. I\'ll explain why I\'m doing this in a moment.\\n\\n## A test using `baseURL`\\n\\nThe [`baseURL`](https://playwright.dev/docs/api/class-testoptions#test-options-base-url) can be used in a Playwright test to determine where to run the tests. That\'s exactly what I\'m doing in the following test file named `the.spec.ts`:\\n\\n```ts\\nimport { test, expect } from \'@playwright/test\';\\n\\ntest(\'page should have title and a root navigation link\', async ({\\n  page,\\n  baseURL,\\n}) => {\\n  await page.goto(baseURL!);\\n  const title = await page.title();\\n  expect(title).toBe(\'johnnyreilly\');\\n\\n  const navTitle = page.getByRole(\'link\', {\\n    name: \'Profile picture of John Reilly John Reilly \u2764\uFE0F\uD83C\uDF3B\',\\n  });\\n  await expect(navTitle).toBeVisible();\\n});\\n\\ntest(\'can navigate to about page\', async ({ page, baseURL }) => {\\n  await page.goto(baseURL!);\\n  await page.getByRole(\'link\', { name: \'About\', exact: true }).click();\\n\\n  const navTitle = page.getByRole(\'heading\', {\\n    name: \\"Hi! I\'m John Reilly - welcome! \u2764\uFE0F\uD83C\uDF3B\\",\\n  });\\n  await expect(navTitle).toBeVisible();\\n});\\n```\\n\\nThe `baseURL` is used in the `page.goto` call. This means that the tests will run against the URL that I specify. In the case of the GitHub Actions workflow, I\'ll specify the URL of the staging environment. These are simple tests that check that the title of the page is correct and that I can navigate to the about page. Consider them smoke tests.\\n\\n## Creating a GitHub Actions workflow\\n\\nWe now have a test that we can run against a URL. We need to create a GitHub Actions workflow that will run the test against the staging environment. I\'ve created a workflow file named `build-and-deploy-static-web-app.yml` in the `.github/workflows` directory. It looks like this:\\n\\n```yml\\nname: Static Web App - Build and Deploy \uD83C\uDFD7\uFE0F\\n\\non:\\n  push:\\n    branches:\\n      - main\\n  pull_request:\\n    types: [opened, synchronize, reopened, closed]\\n    branches:\\n      - main\\n  workflow_dispatch:\\n\\npermissions:\\n  id-token: write # Require write permission to Fetch an OIDC token.\\n  contents: write\\n  pull-requests: write\\n\\nenv:\\n  LOCATION: westeurope\\n  STATICWEBAPPNAME: blog.johnnyreilly.com\\n\\njobs:\\n  build_and_deploy_swa_job:\\n    if: github.event_name == \'push\' || (github.event_name == \'pull_request\' && github.event.action != \'closed\')\\n    runs-on: ubuntu-latest\\n    name: Site build and deploy \uD83C\uDFD7\uFE0F\\n    steps:\\n      - name: Checkout \uD83D\uDCE5\\n        uses: actions/checkout@v3\\n\\n      # Auth between GitHub and Azure is handled by https://github.com/jongio/github-azure-oidc\\n      # https://github.com/Azure/login#sample-workflow-that-uses-azure-login-action-using-oidc-to-run-az-cli-linux\\n      # other login options are possible too\\n      - name: AZ CLI login \uD83D\uDD11\\n        uses: azure/login@v2\\n        with:\\n          client-id: ${{ secrets.AZURE_CLIENT_ID }}\\n          tenant-id: ${{ secrets.AZURE_TENANT_ID }}\\n          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}\\n\\n      - name: Get preview URL \uD83D\uDCDD\\n        id: static_web_app_preview_url\\n        uses: azure/CLI@v2\\n        with:\\n          inlineScript: |\\n            DEFAULTHOSTNAME=$(az staticwebapp show -n \'${{ env.STATICWEBAPPNAME }}\' | jq -r \'.defaultHostname\')\\n\\n            PREVIEW_URL=\\"https://${DEFAULTHOSTNAME/.[1-9]./-${{github.event.pull_request.number }}.${{ env.LOCATION }}.1.}\\"\\n\\n            echo \\"PREVIEW_URL=$PREVIEW_URL\\" >> $GITHUB_OUTPUT\\n\\n      - name: Setup Node.js \uD83D\uDD27\\n        uses: actions/setup-node@v3\\n        with:\\n          node-version: \'18\'\\n          cache: \'yarn\'\\n\\n      - name: Install and build site \uD83D\uDD27\\n        run: |\\n          cd blog-website\\n          yarn install --frozen-lockfile\\n          yarn run build\\n          cp staticwebapp.config.json build/staticwebapp.config.json\\n\\n      - name: Get API key \uD83D\uDD11\\n        id: static_web_app_apikey\\n        uses: azure/CLI@v2\\n        with:\\n          inlineScript: |\\n            APIKEY=$(az staticwebapp secrets list --name \'${{ env.STATICWEBAPPNAME }}\' | jq -r \'.properties.apiKey\')\\n            echo \\"APIKEY=$APIKEY\\" >> $GITHUB_OUTPUT\\n\\n      - name: Deploy site \uD83D\uDE80\\n        id: static_web_app_build_and_deploy\\n        uses: Azure/static-web-apps-deploy@v1\\n        with:\\n          azure_static_web_apps_api_token: ${{ steps.static_web_app_apikey.outputs.APIKEY }}\\n          repo_token: ${{ secrets.GITHUB_TOKEN }} # Used for Github integrations (i.e. PR comments)\\n          action: \'upload\'\\n          skip_app_build: true\\n          app_location: \'/blog-website/build\' # App source code path\\n          api_location: \'/blog-website/api\' # Api source code path - optional\\n\\n    outputs:\\n      preview-url: ${{steps.static_web_app_preview_url.outputs.PREVIEW_URL}}\\n\\n  integration_tests_job:\\n    name: Integration tests \uD83D\uDCA1\uD83C\uDFE0\\n    needs: build_and_deploy_swa_job\\n    if: github.event_name == \'pull_request\' && github.event.action != \'closed\'\\n    runs-on: ubuntu-latest\\n    steps:\\n      - uses: actions/checkout@v3\\n\\n      - name: Wait for preview ${{ needs.build_and_deploy_swa_job.outputs.preview-url }} \u231A\\n        id: static_web_app_wait_for_preview\\n        uses: nev7n/wait_for_response@v1\\n        with:\\n          url: \'${{ needs.build_and_deploy_swa_job.outputs.preview-url }}\'\\n          responseCode: 200\\n          timeout: 600000\\n          interval: 1000\\n\\n      - uses: actions/setup-node@v3\\n        with:\\n          node-version: 18\\n\\n      - name: Install dependencies\\n        run: npm ci\\n        working-directory: ./blog-website-tests\\n\\n      - name: Install Playwright Browsers\\n        run: npx playwright install --with-deps\\n        working-directory: ./blog-website-tests\\n\\n      - name: Run Playwright tests\\n        env:\\n          PLAYWRIGHT_TEST_BASE_URL: \'${{ needs.build_and_deploy_swa_job.outputs.preview-url }}\'\\n        run: npx playwright test\\n        working-directory: ./blog-website-tests\\n\\n      - uses: actions/upload-artifact@v3\\n        if: always()\\n        with:\\n          name: playwright-report\\n          path: blog-website-tests/playwright-report/\\n          retention-days: 30\\n\\n  close_pull_request_job:\\n    if: github.event_name == \'pull_request\' && github.event.action == \'closed\'\\n    runs-on: ubuntu-latest\\n    name: Cleanup staging \uD83D\uDCA5\\n    steps:\\n      - name: AZ CLI login \uD83D\uDD11\\n        uses: azure/login@v2\\n        with:\\n          client-id: ${{ secrets.AZURE_CLIENT_ID }}\\n          tenant-id: ${{ secrets.AZURE_TENANT_ID }}\\n          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}\\n\\n      - name: Get API key \uD83D\uDD11\\n        id: apikey\\n        uses: azure/CLI@v2\\n        with:\\n          inlineScript: |\\n            APIKEY=$(az staticwebapp secrets list --name \'${{ env.STATICWEBAPPNAME }}\' | jq -r \'.properties.apiKey\')\\n            echo \\"APIKEY=$APIKEY\\" >> $GITHUB_OUTPUT\\n\\n      - name: Destroy staging environment \uD83D\uDCA5\\n        id: closepullrequest\\n        uses: Azure/static-web-apps-deploy@v1\\n        with:\\n          azure_static_web_apps_api_token: ${{ steps.apikey.outputs.APIKEY }}\\n          action: \'close\'\\n```\\n\\nAs I said earlier, this has been chopped down from the full version in my repo. It contains specific variables from my own project, but you can see the general structure of the workflow.\\n\\nLet\'s look at what happens above; there are 3 jobs:\\n\\n1. Site build and deploy \uD83C\uDFD7\uFE0F - This is the main job that builds the site and deploys it to the Static Web App.\\n2. Integration tests \uD83D\uDCA1\uD83C\uDFE0 - This job runs the Playwright tests against the preview URL of our Static Web App.\\n3. Cleanup staging \uD83D\uDCA5 - This job runs when a pull request is closed, and destroys the staging environment.\\n\\nLet\'s dig into 1 and 2 a bit more. We\'ll ignore 3 as it\'s pretty self explanatory.\\n\\n### Site build and deploy \uD83C\uDFD7\uFE0F\\n\\nThis job is the main job that builds the site and deploys it to the Static Web App. It\'s a bit long, but it\'s not too complicated. Let\'s break it down:\\n\\n1. Checkout \uD83D\uDCE5 - This is the first step, and it checks out the code from GitHub.\\n2. AZ CLI login \uD83D\uDD11 - This step logs into Azure using the `azure/login` action. This is required to run the `az` CLI commands.\\n3. Get preview URL \uD83D\uDCDD - This step constructs the preview URL of the Static Web App using the `defaultHostname`, the location of deployment, the pull request number and the partition id.\\n\\n   The partition id is the `1` in the URL. It matches whichever partition id that exists for the domain. Right now, if you create a new SWA, you will not get a `1` since SWA is now on partition `2`. When that partition gets filled, it will move on to partition `3`. Ultimately, you just need to find out what the partition id for your SWA is, then you can hardcode it into your workflow.\\n\\n   The complete preview URL is required to run the Playwright tests against the preview URL.\\n\\n4. Setup Node.js \uD83D\uDD27 - This step sets up Node.js, which is required to build the site.\\n5. Install and build site \uD83D\uDD27 - This step installs the dependencies and builds the site - we build our SWA ourselves; you can generally just leave this to the `Azure/static-web-apps-deploy@v1` task. We don\'t because [we have some post processing to do that requires Bun](../2023-03-18-migrating-from-ts-node-to-bun/index.md).\\n6. Get API key \uD83D\uDD11 - This step gets the API key for the Static Web App. This is required to deploy the site.\\n7. Deploy site \uD83D\uDE80 - This step deploys the site to the Static Web App.\\n\\n### Integration tests \uD83D\uDCA1\uD83C\uDFE0\\n\\nOur tests job depends upon the previous job; specifically the preview URL of our Static Web App. You can\'t run Playwright tests if you\'ve nothing to run them against! Again, let\'s dig into it:\\n\\n1. Checkout \uD83D\uDCE5 - This is the first step, and it checks out the code from GitHub.\\n2. Wait for preview ... \u231A - This step waits for the preview URL to be available. This is required because the Static Web App takes a few minutes to deploy, and we don\'t want to run the tests until it\'s deployed.\\n3. Setup Node.js \uD83D\uDD27 - This step sets up Node.js, which is required to run the tests.\\n4. Install dependencies - This step installs the dependencies for the tests.\\n5. Install Playwright Browsers - This step installs the browsers that Playwright will use to run the tests.\\n6. Run Playwright tests - This step runs the Playwright tests.\\n7. Upload test report - This step uploads the test report as an artifact. This is useful if you want to see the test report after the tests have run.\\n\\n## How does it look?\\n\\nWhen we put all this together and push it up to GitHub, we see that tests run as part of the pull request:\\n\\n![Screenshot of the GitHub Action with passing tests](screenshot-github-action.webp)\\n\\nThis screenshot is taken directly from my own blog, and so includes things like Lighthouse that are excluded from this post. But what you can see is that tests are indeed running; and we can see the test report as an artifact:\\n\\n![Screenshot of the test report](screenshot-playwright-test-results.png)\\n\\n## Conclusion\\n\\nSo there you have it; a simple way to run Playwright tests against your Static Web App. I hope you found this useful, and if you have any questions, please feel free to reach out to me."},{"id":"migrating-from-ts-node-to-bun","metadata":{"permalink":"/migrating-from-ts-node-to-bun","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-03-18-migrating-from-ts-node-to-bun/index.md","source":"@site/blog/2023-03-18-migrating-from-ts-node-to-bun/index.md","title":"Migrating from ts-node to Bun","description":"Migrating from ts-node to Bun is surprisingly easy - this post ports a console app from ts-node to Bun and compares performance.","date":"2023-03-18T00:00:00.000Z","tags":[{"inline":false,"label":"Node.js","permalink":"/tags/node-js","description":"The Node.js runtime."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":9.245,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"migrating-from-ts-node-to-bun","title":"Migrating from ts-node to Bun","authors":"johnnyreilly","tags":["node.js","typescript"],"image":"./title-image.png","description":"Migrating from ts-node to Bun is surprisingly easy - this post ports a console app from ts-node to Bun and compares performance.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Playwright, GitHub Actions and Azure Static Web Apps staging environments","permalink":"/playwright-github-actions-and-azure-static-web-apps-staging-environments"},"nextItem":{"title":"Node.js 18, Axios and unsafe legacy renegotiation disabled","permalink":"/node-18-axios-and-unsafe-legacy-renegotiation-disabled"}},"content":"I\'ve wanted to take a look at some of the alternative JavaScript runtimes for a while. The thing that has held me back is npm compatibility. I want to be able to run my code in a runtime that isn\'t Node.js and still be able to use npm packages. I\'ve been using [ts-node](https://typestrong.org/ts-node/) for a long time now; it\'s what I reach for when I\'m building any kind of console app. In this post I want to port a console app from ts-node to [Bun](https://bun.sh/) and see how easy it is.\\n\\n![title image reading \\"From ts-node to Bun\\"](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## The ts-node app\\n\\nI have a [technical blog](https://johnnyreilly.com/) which is built on Docusaurus. When the Docusaurus build completes, a post processing script runs to do things like:\\n\\n- update the `sitemap.xml` to include the `lastmod` date based on [git commit date](https://johnnyreilly.com/docusaurus-createfeeditems-api-git-commit-date), and truncate the number of entries in the file\\n- patch the html files to use Cloudinary as an image CDN for open graph images\\n\\nThese scripts are implemented as a simple ts-node console app. For historical reasons it\'s called `trim-xml` (it originally just truncated the `sitemap.xml` file). It\'s not a particularly good name but I\'m not going to change it now. As the blog is open source, you can see the [code of `trim-xml` here](https://github.com/johnnyreilly/blog.johnnyreilly.com/tree/main/trim-xml].\\n\\nWhat we\'re interested in, is porting this app from ts-node to Bun. The app has a few dependencies; so npm compatibility is important to us. Let\'s see how it goes.\\n\\n## Installing Bun\\n\\nI installed Bun on my Ubuntu machine using the following command:\\n\\n```bash\\ncurl -fsSL https://bun.sh/install | bash\\n```\\n\\nWhich resulted in the following output:\\n\\n```bash\\nbun was installed successfully to ~/.bun/bin/bun\\n\\nAdded \\"~/.bun/bin\\" to $PATH in \\"~/.zshrc\\"\\n\\nTo get started, run:\\n\\n exec /usr/bin/zsh\\n  bun --help\\n```\\n\\nI was a little weirded out by the inconsistent indentation in the output but I\'m sure that\'s just a formatting issue. (I submitted a [PR to fix this](https://github.com/oven-sh/bun/pull/2175).) When I ran the suggested commands it looked like bun was happy and healthy.\\n\\n## Porting the install from yarn to bun\\n\\nWith bun in place I was ready to port the app. I opened up the (as I say, badly named) `trim-xml` directory and triggered installation of the dependencies using `bun install`:\\n\\n```bash\\ncd trim-xml\\nbun install\\n```\\n\\nOutput looked like this:\\n\\n```bash\\nbun install v0.5.7 (5929daee)\\n + @types/node@18.14.1\\n + fast-xml-parser@4.1.2\\n + simple-git@3.16.1\\n + typescript@4.9.5\\n\\n 5 packages installed [2.34s]\\n```\\n\\nAs well, a new `bun.lockb` file had appeared in the directory alongside the `package.json`. Although I can\'t find any documentation on it, I\'m guessing that this is the Bun equivalent of `package-lock.json` or `yarn.lock`. It\'s a binary file, so you can\'t read it. I did find this [project which allows you read bun.lockb files](https://github.com/JacksonKearl/bun-lockb) which looks like a useful way to solve that problem.\\n\\nTo avoid confusion, I also deleted the `yarn.lock` file. Yay - I\'ve installed things! And pretty fast! What next?\\n\\n## From `@types/node` to `bun/types`\\n\\nAs I looked at the output for the install I realised that the `@types/node` package had been installed. The `@types/node` package is a package that contains TypeScript definitions for the Node.js runtime. Given we\'re moving to using Bun, it seemed likely that I didn\'t need these. But I likely did need something that represented the Bun runtime types. (Which incidentally, I would imagine to be pretty similar to the Node.js runtime types.)\\n\\nI had a quick look at the Bun documentation and found the [`bun/types`](https://oven-sh.github.io/bun-types/) package. I added it to my project, whilst removing `@types/node` and `ts-node`:\\n\\n```bash\\nbun remove @types/node\\nbun remove ts-node\\nbun add bun-types\\n```\\n\\nOutput looked like this:\\n\\n```bash\\nbun remove v0.5.7 (5929daee)\\n - @types/node\\n\\n 1 packages removed [3.00ms]\\nbun remove v0.5.7 (5929daee)\\n - ts-node\\n\\n 1 packages removed [843.00ms]\\nbun add v0.5.7 (5929daee)\\n\\n installed bun-types@0.5.7\\n\\n\\n 1 packages installed [1.97s]\\n```\\n\\nThe [docs also say](https://oven-sh.github.io/bun-types/#usage):\\n\\n> Add this to your `tsconfig.json` or `jsconfig.json`:\\n>\\n> ```json\\n> {\\n>   \\"compilerOptions\\": {\\n>     \\"lib\\": [\\"ESNext\\"],\\n>     \\"module\\": \\"esnext\\",\\n>     \\"target\\": \\"esnext\\",\\n>     // \\"bun-types\\" is the important part\\n>     \\"types\\": [\\"bun-types\\"]\\n>   }\\n> }\\n> ```\\n\\nI aligned my existing `tsconfig.json` with the above. For my console app this meant the following changes:\\n\\n```diff\\n  {\\n    \\"compilerOptions\\": {\\n-      \\"target\\": \\"ES2022\\",\\n+      \\"target\\": \\"esnext\\",\\n-      // \\"lib\\": [],\\n+      \\"lib\\": [\\"ESNext\\"],\\n-      \\"module\\": \\"NodeNext\\",\\n+      \\"module\\": \\"esnext\\",\\n-      // \\"types\\": [],\\n+      \\"types\\": [\\"bun-types\\"],\\n    },\\n  }\\n```\\n\\n## `moduleResolution` with Bun\\n\\nI\'d imagined that at this point I\'d be able to run the app, but when I navigated around in VS Code I saw that I had a bunch of errors. I was getting errors like this:\\n\\n![screenshot of VS Code saying \\"Cannot find module \'fast-xml-parser\'. Did you mean to set the \'moduleResolution\' option to \'node\', or to add aliases to the \'paths\' option?ts(2792)\\"](screenshot-cannot-find-module.png)\\n\\nThe error message was suggesting I needed to explicitly state that I wanted to use the Node.js module resolution algorithm. Whilst we\'re using Bun, we\'re porting a Node app - so this made sense. So I made one more change to the `tsconfig.json` to satisy this:\\n\\n```diff\\n  {\\n    \\"compilerOptions\\": {\\n-      // \\"moduleResolution\\": \\"node\\",\\n+      \\"moduleResolution\\": \\"Bundler\\",\\n    },\\n  }\\n```\\n\\nWith that in place, the module resolution errors were... resolved. (Sorry.)\\n\\n## File APIs with Bun\\n\\nHowever, I was still getting errors. This time they were about the [`fs.promises` API](https://nodejs.org/api/fs.html#promises-api). I was getting errors like this:\\n\\n![screenshot of errors in VS Code reporting the absence of the fs.promises API](screenshot-file-apis.png)\\n\\nIt looked like the version of bun I was using didn\'t support that API. As I dug through my code I realised that I was using the `fs.promises` API in a few places. I was using it in the following ways:\\n\\n- `await fs.promises.readdir`\\n- `await fs.promises.readFile`\\n- `await fs.promises.writeFile`\\n\\nFor `fs.promises.readFile` and `fs.promises.writeFile` I was able to replace them with the Bun equivalents [`Bun.file(path).text()`](https://bun.sh/docs/api/file-io#reading-files) and [`Bun.write(path, content)`](https://bun.sh/docs/api/file-io#writing-files) respectively:\\n\\n```diff\\n- `await fs.promises.readFile`\\n+ `await Bun.file(path).text()`\\n- `await fs.promises.writeFile(path, content)`\\n+ `await Bun.write(path, content)`\\n```\\n\\nThere appeared to be no Bun equivalent for `fs.promises.readdir`, so I used the [sync Node.js API](https://nodejs.org/api/fs.html#fsreaddirsyncpath-options):\\n\\n```diff\\n- `await fs.promises.readdir`\\n+ `fs.readdirSync(path)`\\n```\\n\\nWe now had code without any errors. (At least in VS Code as far as TypeScript was concerned. I had yet to run the app to see if it worked.)\\n\\n### Clarification on `fs.promises`\\n\\nI was tweeting about my findings as I wrote this, and [Jarred Sumner (who works on Bun) was kind enough to share](https://twitter.com/jarredsumner/status/1629818921904902145) that the `fs.promises` API is implemented but the types aren\'t as yet.\\n\\n![Screenshot of exchange on Twitter with Jarred responding \\"it sort of exists, but looks like the types are out of date. I say sort of because, actually everything async is sync for node:fs and it just wraps in a Promise. If you use fs createReadStream / fs.createWriteStream or Bun.file(path).stream() it\u2019ll be concurrent / async\\"](screenshot-tweet-fs-promises-exists.png)\\n\\n## Running the app\\n\\nI now needed to do one more thing:\\n\\n```diff\\n-    \\"start\\": \\"ts-node index.ts\\"\\n+    \\"start\\": \\"bun index.ts\\"\\n```\\n\\nThat\'s right; update the `start` script in `package.json` to use `bun` instead of `ts-node`. And now I was able to run the app with `bun start`:\\n\\n```bash\\nLoading /home/john/code/github/blog.johnnyreilly.com/blog-website/build/sitemap.xml\\nReducing 526 urls to 512 urls\\n```\\n\\nThe first positive thing about what I saw, was that we appeared to have running code. Yay! The program also appeared to be executing instantaneously, which seemed surprising. I was expecting Bun to be faster, but this seemed too fast.\\n\\nAlso, we seemed to be lacking many of the log messages I\'d expect. I was expecting to see about 1000 log messages. Something wasn\'t right.\\n\\n## Top level `await` and Bun\\n\\nThe issue was that my `main` function was asynchronous. However, because support for top level `await` wasn\'t available in Node.js when I originally wrote the code, I\'d called the `main` function synchronously. Fortunately Node didn\'t complain about that, and the program behaved in the way required.\\n\\nHowever Bun looked like it was respecting the fact that `main` was asynchronous. That\'s why it was apparently executing so quickly; it wasn\'t waiting for the `main` method to complete before terminating.\\n\\nTo be honest, Bun\'s behaviour here is just right; the code as is didn\'t suggest that it was interested in waiting for the `main` function to complete. But it turns out that waiting is exactly the desired behaviour. To bring things right, we could use top level `await`. So I made the following change to my `index.ts` file:\\n\\n```diff\\n- main();\\n+ await main();\\n```\\n\\nAnd now I was getting the expected log messages; and the program appeared to be working as expected.\\n\\n## GitHub Actions and Bun\\n\\nI was now able to run the app locally. But I wanted to run it in GitHub Actions. I just needed to add the `setup-bun` action to my workflow, so bun was available in the GitHub Actions environment:\\n\\n```yaml\\n- name: Setup bun \uD83D\uDD27\\n  uses: oven-sh/setup-bun@v1\\n  with:\\n    bun-version: latest\\n```\\n\\n## Performance comparison; Bun vs ts-node\\n\\nI was expecting Bun to be faster than ts-node. Let\'s take a run of our app in GitHub Actions with ts-node and compare it to a run of our app with Bun:\\n\\n### ts-node\\n\\n```bash\\nPost processing finished in 17.09 seconds\\nDone in 19.52s.\\n```\\n\\n### Bun\\n\\n```bash\\nPost processing finished in 12.367 seconds\\nDone in 12.72s.\\n```\\n\\nI haven\'t done any formal benchmarking, but it looks like Bun is about 50% faster than ts-node for this usecase. That\'s pretty good. It\'s also worth expanding on how this breaks down.\\n\\nYou\'ll notice in the logs above there\'s two log entries:\\n\\n1. The \\"Post processing\\" reflects the time taken to run the `main` function.\\n2. The \\"Done\\" reflects the time taken to run the `bun` command end to end.\\n\\nWhat can we learn from this? First of all, running code in ts-node takes 17 seconds, compared to 12 seconds with Bun. **So Bun is performing about 40% faster at running code.**\\n\\nThe end to end is 19 seconds with ts-node, compared to 14 seconds with Bun. **So Bun is performing about 50% faster end to end.** There\'s two parts to this; the time taken to compile the code and the time taken to start up. We\'re doing type checking with ts-node; which if deactivated would make a difference.\\n\\nHowever, when you look at the difference between the end to end runtime and code runtime with Bun, it\'s a mere 0.353 seconds. ts-node clocks in at 2.43 seconds for the same. So ts-node is about 6.5 times slower at starting up. That\'s a pretty big difference; it\'s unlikely that all of this is TypeScript compilation; Node.js is fundamentally slower at getting going than Bun is.\\n\\n## Conclusion\\n\\nMoving from ts-node to Bun was a pretty easy process. I was able to do it in a few hours. I was able to run the app locally and in GitHub Actions. And I was able to run the app in less time.\\n\\nThis all makes me feel very positive about Bun. I\'m looking forward to using it more in the future.\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/migrating-typescript-app-node-js-bun/)\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.logrocket.com/migrating-typescript-app-node-js-bun/\\" />\\n</head>"},{"id":"node-18-axios-and-unsafe-legacy-renegotiation-disabled","metadata":{"permalink":"/node-18-axios-and-unsafe-legacy-renegotiation-disabled","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-03-09-node-18-axios-and-unsafe-legacy-renegotiation-disabled/index.md","source":"@site/blog/2023-03-09-node-18-axios-and-unsafe-legacy-renegotiation-disabled/index.md","title":"Node.js 18, Axios and unsafe legacy renegotiation disabled","description":"With Node.js 18, unsafe TLS legacy renegotiation was disabled. Some APIs still need it and SSL inspection can downgrade TLS. This post shows an Axios workaround.","date":"2023-03-09T00:00:00.000Z","tags":[{"inline":false,"label":"Node.js","permalink":"/tags/node-js","description":"The Node.js runtime."}],"readingTime":2.855,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"node-18-axios-and-unsafe-legacy-renegotiation-disabled","title":"Node.js 18, Axios and unsafe legacy renegotiation disabled","authors":"johnnyreilly","tags":["node.js"],"image":"./title-image.png","description":"With Node.js 18, unsafe TLS legacy renegotiation was disabled. Some APIs still need it and SSL inspection can downgrade TLS. This post shows an Axios workaround.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Migrating from ts-node to Bun","permalink":"/migrating-from-ts-node-to-bun"},"nextItem":{"title":"In defence of pull requests","permalink":"/in-defence-of-pull-requests"}},"content":"Node.js 18 doesn\'t allow legacy TLS renegotion by default. But some APIs still need it. Also, corporate network traffic network is often subject to SSL inspection and that can manifest as a downgrade in TLS negotiation. [Palo Alto Networks SSL Inbound Inspection is an example of an SSL inspector that can downgrade TLS](https://docs.paloaltonetworks.com/pan-os/9-1/pan-os-admin/decryption/configure-ssl-inbound-inspection).\\n\\nThis post shows how to support work around this issue with Axios.\\n\\n![title image reading \\"Node.js 18, Axios and unsafe legacy renegotiation disabled\\" and Axios / Node.js logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## The error\\n\\nIf you have code that uses Node.js and Axios, you may have encountered this error when you upgraded to Node.js 18:\\n\\n```bash\\nEPROTO B8150000:error:0A000152:SSL routines:final_renegotiate:unsafe legacy renegotiation disabled\\n```\\n\\nor if you\'re using the Azure SDK for JavaScript, you may have seen this:\\n\\n```bash\\nRestError: write EPROTO 40736C4DF87F0000:error:0A000152:SSL routines:final_renegotiate:unsafe legacy renegotiation disabled:../deps/openssl/openssl/ssl/statem/extensions.c:922:\\n```\\n\\n## Why does this happen?\\n\\nThe source of this error is Node.js 18 disabling unsafe legacy TLS renegotiation. The motivation for this is noble; it\'s to mitigate [CVE-2009-3555](https://cve.mitre.org/cgi-bin/cvename.cgi?name=cve-2009-3555).\\n\\nAlas, there are APIs that still use legacy TLS negotiation. And SSL inspection can make APIs that actually do support modern TLS appear as though they do not. I first encountered this issue when working with the [Teams webhook API](../2019-12-18-teams-notification-webhooks/index.md), and for a while incorrectly thought that the fault was with the API. It was not, it lay with Palo Alto Networks SSL Inbound Inspection.\\n\\nI subsequently encountered the self same issue with the [Azure SDK for JavaScript](https://github.com/Azure/azure-sdk-for-js) and [in discussion with the team the SSL inspection was identified as a likely cause](https://github.com/Azure/azure-sdk-for-js/issues/26310). We were further able to confirm that SSL inspection was the cause by working with our network team to disable SSL inspection for the API in question. This resolved the issue.\\n\\n## Working around the issue\\n\\nBut what say you can\'t disable SSL inspection? Or what if you\'re using an API that doesn\'t support modern TLS negotiation? Well, you can work around the issue by allowing legacy TLS renegotiation.\\n\\n[I found details on how to do this using Axios on Stack Overflow](https://stackoverflow.com/questions/74324019/allow-legacy-renegotiation-for-nodejs/74600467#74600467). I kept needing to come back to it again and again, so I wrote this up to make the solution easier for me to find.\\n\\nSo if you are facing this issue, here\'s how to work around it with Axios.\\n\\n```ts\\nimport crypto from \'crypto\';\\nimport https from \'https\';\\n\\n/**\\n * Handle this problem with Node 18\\n * write EPROTO B8150000:error:0A000152:SSL routines:final_renegotiate:unsafe legacy renegotiation disabled\\n * see https://stackoverflow.com/questions/74324019/allow-legacy-renegotiation-for-nodejs/74600467#74600467\\n **/\\nconst allowLegacyRenegotiationforNodeJsOptions = {\\n  httpsAgent: new https.Agent({\\n    // for self signed you could also add\\n    // rejectUnauthorized: false,\\n    // allow legacy server\\n    secureOptions: crypto.constants.SSL_OP_LEGACY_SERVER_CONNECT,\\n  }),\\n};\\n\\nfunction makeRequest(url: string, data: object) {\\n  return axios({\\n    ...allowLegacyRenegotiationforNodeJsOptions,\\n    url,\\n    headers: {\\n      Accept: \'application/json\',\\n      \'Content-Type\': \'application/json\',\\n    },\\n    method: \'POST\',\\n    data: { some: \'data\' },\\n  });\\n}\\n```\\n\\nThere\'s not much going on here; we\'re just telling Axios to use an https agent that allows legacy TLS renegotiation. No more than that. With this approach, you can make Axios requests to APIs that use legacy TLS renegotiation. I\'d love to be able to do this with the Fetch API, but I haven\'t found a way to do that yet.\\n\\n## Summary\\n\\nNode.js 18 disables unsafe legacy TLS renegotiation by default. This can cause issues with APIs that still use legacy TLS renegotiation. It can also cause issues if your requests are subject to SSL inspection.\\n\\nThis post demonstrates how to work around the issue with Axios."},{"id":"in-defence-of-pull-requests","metadata":{"permalink":"/in-defence-of-pull-requests","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-02-11-in-defence-of-pull-requests/index.md","source":"@site/blog/2023-02-11-in-defence-of-pull-requests/index.md","title":"In defence of pull requests","description":"Some people feel that pull requests are a barrier to contribution. I disagree.","date":"2023-02-11T00:00:00.000Z","tags":[],"readingTime":3.42,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"in-defence-of-pull-requests","title":"In defence of pull requests","authors":"johnnyreilly","tags":[],"image":"./title-image.png","description":"Some people feel that pull requests are a barrier to contribution. I disagree.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Node.js 18, Axios and unsafe legacy renegotiation disabled","permalink":"/node-18-axios-and-unsafe-legacy-renegotiation-disabled"},"nextItem":{"title":"Docusaurus blogs: adding breadcrumb structured data","permalink":"/docusaurus-blogs-adding-breadcrumb-structured-data"}},"content":"Not everyone values pull requests. I really do, and this post explains why.\\n\\n![title image reading \\"In defence of pull requests\\"](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n[Graeme Lockley](https://twitter.com/lockersmyboy) recently shared [this tweet](https://twitter.com/nhumrich/status/1623435760379768832) with me:\\n\\n![screenshot of tweet saying \\"Code reviews and pull requests were invented for open source projects where you want to gatekeep changes from people you don\'t know and don\'t trust to change the code safely\\"](screenshot-tweet-code-reviews-and-pull-requests.webp)\\n\\nI don\'t feel the same way; and ended up writing a long screed back to Graeme as to why. I thought I\'d share it here too, in only slightly refined format:\\n\\n> I\'ve seen this idea floating around. There is something to be said for low friction contribution for people that you trust. For that reason I definitely apply more scrutiny to PRs from people that I know / trust less as compared to people I know / trust more. However, to add a little more nuance. Here we go!\\n\\n## Pull Requests provide a moment for contemplation\\n\\nA moment to take stock of what\'s been built, and whether we\'d be happy with it landing that way. Because I\'m an equal opportunities kinda guy, I apply that to myself. When I raise a PR, before I let others know it\'s ready for review, I will tend to do a first review myself. It\'s amazing the different perspective you can have as the consumer of a PR as compared to a producer. I find I change things often before sharing with others as a consequence.\\n\\n## Pull Requests provide a chance for communication\\n\\nEngineers are not obligated to communicate about what they do. And famously many of us aren\'t very good at it either. You become good at things that you practice at. PRs provide an opportunity to express in clear language, the aim of a change and why it is implemented in a certain way. That allows the engineer to practice repeatedly the act of communication, which will make them a more useful engineer to those around them.\\n\\nVery much related to this, PRs are a teaching opportunity. It\'s a way to level up the next generation of engineers that are learning from you. What we do is more than the code we write, it\'s the culture we create.\\n\\n## Pull Requests provide an opportunity for collaboration\\n\\nThis may shock you, but I don\'t always get things perfect. My ideas and implementations are often \\"good starts\\", but which are wildly improved through collaboration with others. PRs provide a way to collaborate on a change. I value them specifically for that reason.\\n\\nUnfortunately the prompting tweet is talking about PRs being used on OSS projects; and the nature of work I do that _isn\'t_ OSS means I can\'t evidence it. However, I can point you to a [PR I raised on the Docusaurus repo](https://github.com/facebook/docusaurus/pull/8378#discussion_r1044277801) where I was collaborating with the marvellous [S\xe9bastien Lorber](https://github.com/slorber) on a change. I\'d say it\'s a good example of how PRs can be used to collaborate on a change; it\'s definitely how I want to roll regardless of the project I\'m working on.\\n\\n![screenshot of the linked PR demonstrating collaboration https://github.com/facebook/docusaurus/pull/8378#discussion_r1044277801](screenshot-collaborating-on-github.png)\\n\\n## Where does automated testing fit in?\\n\\nFinally, automated testing. If you value automated testing, you must ask yourself the question: where does it fit into the contribution picture? Running automated tests against contributions is a good way to test the value those contributions provide. If you don\'t run them prior to contribution, then when do you run them? And how do you evidence the results?\\n\\n## Conclusion\\n\\nLow friction contribution is a good goal. In the case of very simple pull requests, automating from top to bottom with minimal need for human interaction is a great idea. In fact if you\'d like to see an example of this in the wild, it\'s worth taking a look at the automation the TypeScript team, and in particular [Orta Therox](https://orta.io), applied to the [Definitely Typed](https://github.com/DefinitelyTyped/DefinitelyTyped) repo.\\n\\nBut, safe to say, I think there\'s a great deal more nuance to the topic than implied by the raw tweet. Pull requests are to be cherished, not spurned. Yay pull requests!"},{"id":"docusaurus-blogs-adding-breadcrumb-structured-data","metadata":{"permalink":"/docusaurus-blogs-adding-breadcrumb-structured-data","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-02-05-docusaurus-blogs-adding-breadcrumb-structured-data/index.md","source":"@site/blog/2023-02-05-docusaurus-blogs-adding-breadcrumb-structured-data/index.md","title":"Docusaurus blogs: adding breadcrumb structured data","description":"Docusaurus blogs can add breadcrumb structured data to their blog posts. This post shows how to add it using the JSON-LD format.","date":"2023-02-05T00:00:00.000Z","tags":[{"inline":false,"label":"Docusaurus","permalink":"/tags/docusaurus","description":"The Docusaurus static site generator."},{"inline":false,"label":"SEO","permalink":"/tags/seo","description":"Search Engine Optimization."}],"readingTime":5.715,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"docusaurus-blogs-adding-breadcrumb-structured-data","title":"Docusaurus blogs: adding breadcrumb structured data","authors":"johnnyreilly","tags":["docusaurus","seo"],"image":"./title-image.png","description":"Docusaurus blogs can add breadcrumb structured data to their blog posts. This post shows how to add it using the JSON-LD format.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"In defence of pull requests","permalink":"/in-defence-of-pull-requests"},"nextItem":{"title":"Migrating from GitHub Pages to Azure Static Web Apps","permalink":"/migrating-from-github-pages-to-azure-static-web-apps"}},"content":"By default, Docusaurus blogs don\'t add breadcrumb structured data to their blog posts. It\'s not hard to make it happen though; this post shows how to add it using the JSON-LD format.\\n\\n![title image reading \\"Docusaurus blogs: adding breadcrumb structured data\\" with the Docusaurus logo](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## What are breadcrumbs?\\n\\nTake a look at this:\\n\\n![screenshot of Google search results with a highlighted breadcrumb](./screenshot-google-search-results-breadcrumbs.webp)\\n\\nWhat you\'re looking at is a [blog post of mine](../2021-07-14-directory-build-props-c-sharp-9-for-all/index.md) showing up in Google search results. Significantly, it has a breadcrumb which I\'ve highlighted. It indicates that the blog post sits under the blogs \\"Archive\\" page, which in turn sits under the home page of the site.\\n\\nThis breadcrumb was driven by structured data that my blog surfaces. Structured data is a form of metadata that is intended to be easily machine readable; and consequently helpful to search engines like Google. Now, what is a breadcrumb to Google?\\n\\n> Google Search uses breadcrumb markup in the body of a web page to categorize the information from the page in search results.\\n\\n[You can read more on breadcrumbs in the Google documentation](https://developers.google.com/search/docs/appearance/structured-data/breadcrumb). This post is about how to add breadcrumbs to your Docusaurus blog posts, to help Google categorise your blog posts.\\n\\nIt\'s worth noting that what we\'re going to do here is add a JSON-LD structured data breadcrumb to the blog post. There\'s going to be no physical breadcrumb on the page itself. It could be nice to add a physical breadcrumb, but that\'s not what we\'re going to do here as it would not be a trivial addition. (As an aside, Docusaurus does use physical breadcrumbs in its documentation pages; which surface structured data.)\\n\\nDocusaurus already has structured data support for blog posts; [in fact I had a hand in that](https://github.com/facebook/docusaurus/pull/5322). I like me some structured data \uD83D\uDE09. The existing structured data is article / `BlogPosting` metadata. We\'re going to enrich the structured data for blog posts by adding a `BreadcrumbList` as well.\\n\\nIncidentally, if you\'d like to learn more about React, JSON-LD and structured data, I\'ve [written about it, and done a short talk on the topic](../2021-10-15-structured-data-seo-and-react/index.md).\\n\\n## Adding a breadcrumb to a blog post\\n\\nWith all that preamble out of the way, let\'s get to the good stuff. We\'re going to add a breadcrumb to a blog post. To do that, we need to adjust two components in Docusaurus; the `BlogArchivePage` and the `BlogPostPage`. We\'re going to do this by swizzling. Let\'s crack open the terminal and get started:\\n\\n```bash\\nnpm run swizzle @docusaurus/theme-classic BlogArchivePage -- --wrap --danger\\nnpm run swizzle @docusaurus/theme-classic BlogPostPage -- --wrap --danger\\n```\\n\\nThis will create two files in the `src/theme/` directory:\\n\\n- `src/theme/BlogArchivePage/index.js`\\n- `src/theme/BlogPostPage/index.js`\\n\\nYou\'ll note from the command that we\'ve used the `--wrap` flag. This is because we want to wrap the existing component. If we didn\'t use the `--wrap` flag, we\'d be replacing the existing component. We\'re wrapping rather than replacing as it will make maintenance easier as Docusaurus evolves.\\n\\n## Adding a breadcrumb to the blog archive page\\n\\nWe\'re now going to replace the generated `BlogArchivePage` component with the following:\\n\\n```jsx\\nimport React from \'react\';\\nimport BlogArchivePage from \'@theme-original/BlogArchivePage\';\\nimport useDocusaurusContext from \'@docusaurus/useDocusaurusContext\';\\n\\nexport default function BlogArchivePageWrapper(props) {\\n  const { siteConfig } = useDocusaurusContext();\\n\\n  // https://developers.google.com/search/docs/appearance/structured-data/breadcrumb#json-ld\\n  const breadcrumbStructuredData = {\\n    \'@context\': \'https://schema.org\',\\n    \'@type\': \'BreadcrumbList\',\\n    name: \'Archive breadcrumb\',\\n    itemListElement: [\\n      {\\n        \'@type\': \'ListItem\',\\n        position: 1,\\n        name: \'Home\',\\n        item: siteConfig.url,\\n      },\\n      {\\n        \'@type\': \'ListItem\',\\n        position: 2,\\n        name: \'Archive\',\\n      },\\n    ],\\n  };\\n\\n  return (\\n    <>\\n      <script\\n        type=\\"application/ld+json\\"\\n        dangerouslySetInnerHTML={{\\n          __html: JSON.stringify(breadcrumbStructuredData),\\n        }}\\n      />\\n      <BlogArchivePage {...props} />\\n    </>\\n  );\\n}\\n```\\n\\nHere we\'re constructing a JSON-LD structured data object that represents a breadcrumb. We\'re then adding it to the page as a script tag with the `type` of `application/ld+json`. And we\'re rendering the wrapped `BlogArchivePage` component. This is so that we can add the structured data breadcrumb to the page without having to duplicate the existing code.\\n\\nThere\'s two entries in the `itemListElement` array. The first is the home page of the site. The second is the archive page itself. We\'re not going to add a link to the archive page as it\'s the current page.\\n\\n## Adding a breadcrumb to the blog post page\\n\\nOkay, one down - one to go. We\'re now going to replace the generated `BlogPostPage` component with the following:\\n\\n```jsx\\nimport React from \'react\';\\nimport BlogPostPage from \'@theme-original/BlogPostPage\';\\nimport useDocusaurusContext from \'@docusaurus/useDocusaurusContext\';\\n\\nexport default function BlogPostPageWrapper(props) {\\n  const { siteConfig } = useDocusaurusContext();\\n\\n  /** @type {import(\'@docusaurus/plugin-content-blog\').BlogPostMetadata} */ const blogMetaData =\\n    props.content.metadata;\\n\\n  // https://developers.google.com/search/docs/appearance/structured-data/breadcrumb#json-ld\\n  const archiveBreadcrumbStructuredData = {\\n    \'@context\': \'https://schema.org\',\\n    \'@type\': \'BreadcrumbList\',\\n    name: \'Archive breadcrumb\',\\n    itemListElement: [\\n      {\\n        \'@type\': \'ListItem\',\\n        position: 1,\\n        name: \'Home\',\\n        item: siteConfig.url,\\n      },\\n      {\\n        \'@type\': \'ListItem\',\\n        position: 2,\\n        name: \'Archive\',\\n        item: `${siteConfig.url}/archive`,\\n      },\\n      {\\n        \'@type\': \'ListItem\',\\n        position: 3,\\n        name: blogMetaData.title,\\n      },\\n    ],\\n  };\\n\\n  const tagsBreadcrumbStructuredData = blogMetaData.tags.map((tag) => ({\\n    \'@context\': \'https://schema.org\',\\n    \'@type\': \'BreadcrumbList\',\\n    name: `Tags ${tag.label} breadcrumb`,\\n    itemListElement: [\\n      {\\n        \'@type\': \'ListItem\',\\n        position: 1,\\n        name: \'Home\',\\n        item: siteConfig.url,\\n      },\\n      {\\n        \'@type\': \'ListItem\',\\n        position: 2,\\n        name: \'Tags\',\\n        item: `${siteConfig.url}/tags`,\\n      },\\n      {\\n        \'@type\': \'ListItem\',\\n        position: 3,\\n        name: tag.label,\\n        item: `${siteConfig.url}${tag.permalink}`,\\n      },\\n      {\\n        \'@type\': \'ListItem\',\\n        position: 4,\\n        name: blogMetaData.title,\\n      },\\n    ],\\n  }));\\n\\n  const breadcrumbStructuredData = [\\n    archiveBreadcrumbStructuredData,\\n    ...tagsBreadcrumbStructuredData,\\n  ];\\n\\n  return (\\n    <>\\n      <script\\n        type=\\"application/ld+json\\"\\n        dangerouslySetInnerHTML={{\\n          __html: JSON.stringify(breadcrumbStructuredData),\\n        }}\\n      />\\n      <BlogPostPage {...props} />\\n    </>\\n  );\\n}\\n```\\n\\nAgain, we\'re constructing a JSON-LD structured data object that represents a breadcrumb. But this time we\'re going to add multiple breadcrumbs to the page. The first is the archive breadcrumb. The other breadcrumbs are generated for each tag.\\n\\nI\'m somewhat on the fence as to whether it\'s useful to have a breadcrumb for each tag. [In fact, originally I didn\'t have it when I first added support](https://github.com/johnnyreilly/blog.johnnyreilly.com/pull/416). But I\'ve added it in as it\'s not a lot of work and it\'s not a lot of code. I\'m not sure if it\'s useful or not. [I\'ve added it now](https://github.com/johnnyreilly/blog.johnnyreilly.com/commit/e69633ca6cc6cae98cd405580e9659594ac92f8a); I\'m going to leave it in in place for a bit and see how it goes.\\n\\n## Using the Rich Results test to validate the breadcrumbs\\n\\nOnce we\'ve shipped the changes we can test them using the [Google Rich Results Test](https://search.google.com/test/rich-results). The screenshot below was taken after I\'d deployed the changes and [the test was run](https://search.google.com/test/rich-results?url=https%3A%2F%2Fjohnnyreilly.com%2Fdirectory-build-props-c-sharp-9-for-all).\\n\\n![screenshot of the Rich Results Test featuring article and breadcrumbs](./screenshot-google-rich-results-test-breadcrumbs.webp)\\n\\n![screenshot of the Rich Results Test featuring the specific 4 breadcrumbs](./screenshot-google-rich-results-test-breadcrumbs-breakdown.png)\\n\\nWe can also check the breadcrumbs in the [Google Search Console](https://search.google.com/search-console/r/breadcrumbs):\\n\\n![Screenshot of the Google search console](./screenshot-google-search-console-breadcrumbs.webp)\\n\\nSo that\'s it, now we have breadcrumbs on the blog posts.\\n\\n## Conclusion\\n\\nThis is a useful addition to the blog. I\'d like it more if it was a physical breadcrumb as well; not just an \\"invisible\\" one. [I\'ve opened an issue with Docusaurus to see if that\'s possible](https://docusaurus.io/feature-requests/p/add-breadcrumb-for-blog-posts). I would imagine, if that does get added, it would likely be a single breadcrumb rather than multiple ones. But let me not preempt; let\'s see what comes of it."},{"id":"migrating-from-github-pages-to-azure-static-web-apps","metadata":{"permalink":"/migrating-from-github-pages-to-azure-static-web-apps","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-02-01-migrating-from-github-pages-to-azure-static-web-apps/index.md","source":"@site/blog/2023-02-01-migrating-from-github-pages-to-azure-static-web-apps/index.md","title":"Migrating from GitHub Pages to Azure Static Web Apps","description":"How to migrate from GitHub Pages to Azure Static Web Apps, using Bicep and GitHub Actions to deploy.","date":"2023-02-01T00:00:00.000Z","tags":[{"inline":false,"label":"Azure Static Web Apps","permalink":"/tags/azure-static-web-apps","description":"The Azure Static Web Apps service."},{"inline":false,"label":"Bicep","permalink":"/tags/bicep","description":"The Bicep language for Azure Resource Manager templates."},{"inline":false,"label":"GitHub Actions","permalink":"/tags/github-actions","description":"The GitHub Actions CI / CD service."}],"readingTime":8.725,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"migrating-from-github-pages-to-azure-static-web-apps","title":"Migrating from GitHub Pages to Azure Static Web Apps","authors":"johnnyreilly","tags":["azure static web apps","bicep","github actions"],"image":"./title-image.png","description":"How to migrate from GitHub Pages to Azure Static Web Apps, using Bicep and GitHub Actions to deploy.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Docusaurus blogs: adding breadcrumb structured data","permalink":"/docusaurus-blogs-adding-breadcrumb-structured-data"},"nextItem":{"title":"Docusaurus blogs: using the createFeedItems API with git commit date","permalink":"/docusaurus-createfeeditems-api-git-commit-date"}},"content":"import FAQStructuredData from \'../../src/theme/MDXComponents/FAQStructuredData\';\\n\\nexport const faqs = [\\n{\\nquestion:\\n\'Why should I migrate from GitHub Pages to Azure Static Web Apps?\',\\nanswer:\\n\\"If you like the idea of using a single platform for hosting your static website and deploying previews, then Azure Static Web Apps is a great option. It\'s also free to use!\\",\\n},\\n{\\nquestion: \'What is Bicep and how can it be used for migration?\',\\nanswer:\\n\\"Bicep is a Domain Specific Language (DSL) for deploying Azure resources. It\'s a great way to define your infrastructure as code. You can use Bicep to deploy your Azure Static Web App. You can also deploy an Azure Static Web App using the Azure Portal or the Azure CLI.\\",\\n},\\n{\\nquestion:\\n\'How do I enable staging environments / deployment previews in Azure Static Web Apps?\',\\nanswer:\\n\\"You don\'t need to do anything special to enable staging environments. They\'re enabled by default.\\",\\n},\\n];\\n\\nYou can use Bicep and GitHub Actions to build and deploy to a static website on Azure Static Web Apps. This post demonstrates how.\\n\\n![title image reading \\"Migrating from GitHub Pages to Azure Static Web Apps\\" with GitHub and Azure Static Web Apps logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Why migrate?\\n\\nThis blog has been hosted on GitHub Pages for some time. It also makes use of Netlify for deployment previews. These are both great, but it\'s always niggled that there\'s two mechanisms in play; each separately configured. It\'s time to simplify.\\n\\n[Azure Static Web Apps](https://azure.microsoft.com/en-us/services/app-service/static/) supports both hosting static websites and deployment previews (known as \\"staging environments\\"). So we\'re going to migrate across to use Static Web Apps in place of both of GitHub Pages and Netlify. I\'m choosing to use Bicep to do this as I tend towards using infrastructure as code. If you wanted to roll with a more \\"point and click\\" approach in the Azure Portal, you could do that too. Simply ignore the Bicep related portions of the post.\\n\\n## Bicep\\n\\nThe first thing we\'re going to need is a Bicep template to deploy our SWA. In our GitHub repo we\'re going to add a `infra` folder, and in there we\'ll create a `main.bicep` file:\\n\\n```bicep\\nparam location string\\nparam branch string\\nparam name string\\nparam tags object\\n@secure()\\nparam repositoryToken string\\nparam customDomainName string\\n\\nresource staticWebApp \'Microsoft.Web/staticSites@2021-02-01\' = {\\n  name: name\\n  location: location\\n  tags: tags\\n  sku: {\\n    name: \'Free\'\\n    tier: \'Free\'\\n  }\\n  properties: {\\n    repositoryUrl: \'https://github.com/johnnyreilly/blog.johnnyreilly.com\'\\n    repositoryToken: repositoryToken\\n    branch: branch\\n    provider: \'GitHub\'\\n    stagingEnvironmentPolicy: \'Enabled\'\\n    allowConfigFileUpdates: true\\n    buildProperties:{\\n      skipGithubActionWorkflowGeneration: true\\n    }\\n  }\\n}\\n\\n// resource customDomain \'Microsoft.Web/staticSites/customDomains@2021-02-01\' = {\\n//   parent: staticWebApp\\n//   name: customDomainName\\n//   properties: {}\\n// }\\n\\noutput staticWebAppDefaultHostName string = staticWebApp.properties.defaultHostname // eg gentle-bush-0db02ce03.azurestaticapps.net\\noutput staticWebAppId string = staticWebApp.id\\noutput staticWebAppName string = staticWebApp.name\\n```\\n\\nMost of the Bicep template above is self-explanatory. There\'s a few things to highlight:\\n\\n- We\'re using the \\"Free\\" SKU which means we don\'t have to pay to run our website.\\n- We need to provide a `repositoryToken` - this is a little surprising as you\'ll see later in the template that we supply the `skipGithubActionWorkflowGeneration: true` which means we\'re _not_ requiring our SWA to interact with GitHub on our behalf - but it seems that there\'s a requirement for a GitHub token anyway. We\'ll roll with it.\\n- We\'re enabling deployment previews / staging environments with `stagingEnvironmentPolicy: \'Enabled\'`\\n- The `branch` is always set to `main` - we have to let Azure know this so it knows which branch is the primary branch and hence which other ones will have staging environments.\\n- It also includes a section for the custom domain which is commented out - we\'ll uncomment that later once we\'ve set up our custom domain / DNS.\\n\\n## Setting up a resource group\\n\\nWith our Bicep in place, we\'re going to need a resource group to send it to. We\'re going to create ourselves a resource group in West Europe:\\n\\n```shell\\naz group create -g rg-blog-johnnyreilly-com -l westeurope\\n```\\n\\n## Secrets for GitHub Actions\\n\\nWe\'re aiming to set up a GitHub Action to handle our deployment which depends upon some secrets.\\n\\n### `AZURE_CREDENTIALS` - GitHub logging into Azure\\n\\nFirst a `AZURE_CREDENTIALS` secret that facilitates GitHub logging into Azure. We\'ll use the Azure CLI to create this:\\n\\n```shell\\naz ad sp create-for-rbac --name \\"myApp\\" --role contributor \\\\\\n    --scopes /subscriptions/{subscription-id}/resourceGroups/{resource-group} \\\\\\n    --sdk-auth\\n```\\n\\nRemember to replace the `{subscription-id}` with your subscription id and `{resource-group}` with the name of your resource group (`rg-blog-johnnyreilly-com` if you\'re following along). This command will pump out a lump of JSON that looks something like this:\\n\\n```json\\n{\\n  \\"clientId\\": \\"a-client-id\\",\\n  \\"clientSecret\\": \\"a-client-secret\\",\\n  \\"subscriptionId\\": \\"a-subscription-id\\",\\n  \\"tenantId\\": \\"a-tenant-id\\",\\n  \\"activeDirectoryEndpointUrl\\": \\"https://login.microsoftonline.com\\",\\n  \\"resourceManagerEndpointUrl\\": \\"https://management.azure.com/\\",\\n  \\"activeDirectoryGraphResourceId\\": \\"https://graph.windows.net/\\",\\n  \\"sqlManagementEndpointUrl\\": \\"https://management.core.windows.net:8443/\\",\\n  \\"galleryEndpointUrl\\": \\"https://gallery.azure.com/\\",\\n  \\"managementEndpointUrl\\": \\"https://management.core.windows.net/\\"\\n}\\n```\\n\\nTake this and save it as the `AZURE_CREDENTIALS` secret in GitHub.\\n\\n### `WORKFLOW_TOKEN` - Azure accessing the GitHub container registry\\n\\nWe also need a secret for updating workflows from Azure. Azure Static Web Apps can update your workflow - they need access to do this when we\'re deploying. To facilitate this we\'ll set up a `WORKFLOW_TOKEN` secret. This is a GitHub personal access token with the `workflow` scope. [Follow the instructions here to create the token.](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token)\\n\\nIronically, we\'re not planning to use this functionality, but the validation for the Bicep template will fail if it isn\'t supplied.\\n\\n## Deploying with GitHub Actions\\n\\nWith our secrets configured, we\'re now well placed to update our GitHub Action. We\'ll tweak the content of `.github/workflows/build-and-deploy.yaml` file in our repository to the following:\\n\\n```yaml\\nname: Build and Deploy\\n\\non:\\n  push:\\n    branches:\\n      - main\\n  pull_request:\\n    types: [opened, synchronize, reopened, closed]\\n    branches:\\n      - main\\n\\nenv:\\n  RESOURCE_GROUP: rg-blog-johnnyreilly-com\\n  LOCATION: westeurope\\n  STATICWEBAPPNAME: blog.johnnyreilly.com\\n  TAGS: \'{\\"owner\\":\\"johnnyreilly\\", \\"email\\":\\"johnny_reilly@hotmail.com\\"}\'\\n\\njobs:\\n  build_and_deploy_swa_job:\\n    if: github.event_name == \'push\' || (github.event_name == \'pull_request\' && github.event.action != \'closed\')\\n    runs-on: ubuntu-latest\\n    name: Build and deploy\\n    steps:\\n      - uses: actions/checkout@v2\\n        with:\\n          submodules: true\\n\\n      - name: Azure Login\\n        uses: azure/login@v1\\n        with:\\n          creds: ${{ secrets.AZURE_CREDENTIALS }}\\n\\n      - name: Set Deployment Name\\n        id: deployment_name\\n        run: |\\n          REF_SHA=\'${{ github.ref }}.${{ github.sha }}\'\\n          DEPLOYMENT_NAME=\\"${REF_SHA////-}\\"\\n          echo \\"DEPLOYMENT_NAME=$DEPLOYMENT_NAME\\" >> $GITHUB_OUTPUT\\n\\n      - name: Static Web App - change details\\n        id: static_web_app_what_if\\n        if: github.event_name == \'pull_request\'\\n        uses: azure/CLI@v2\\n        with:\\n          inlineScript: |\\n            az deployment group what-if \\\\\\n              --resource-group ${{ env.RESOURCE_GROUP }} \\\\\\n              --name \\"${{ steps.deployment_name.outputs.DEPLOYMENT_NAME }}\\" \\\\\\n              --template-file ./infra/main.bicep \\\\\\n              --parameters \\\\\\n                  branch=\'main\' \\\\\\n                  location=\'${{ env.LOCATION }}\' \\\\\\n                  name=\'${{ env.STATICWEBAPPNAME }}\' \\\\\\n                  tags=\'${{ env.TAGS }}\' \\\\\\n                  repositoryToken=\'${{ secrets.WORKFLOW_TOKEN }}\' \\\\\\n                  customDomainName=\'${{ env.STATICWEBAPPNAME }}\'\\n\\n      - name: Static Web App - deploy infra\\n        id: static_web_app_deploy\\n        if: github.event_name != \'pull_request\'\\n        uses: azure/CLI@v2\\n        with:\\n          inlineScript: |\\n            az deployment group create \\\\\\n              --resource-group ${{ env.RESOURCE_GROUP }} \\\\\\n              --name \\"${{ steps.deployment_name.outputs.DEPLOYMENT_NAME }}\\" \\\\\\n              --template-file ./infra/main.bicep \\\\\\n              --parameters \\\\\\n                  branch=\'main\' \\\\\\n                  location=\'${{ env.LOCATION }}\' \\\\\\n                  name=\'${{ env.STATICWEBAPPNAME }}\' \\\\\\n                  tags=\'${{ env.TAGS }}\' \\\\\\n                  repositoryToken=\'${{ secrets.WORKFLOW_TOKEN }}\' \\\\\\n                  customDomainName=\'${{ env.STATICWEBAPPNAME }}\'\\n\\n      - name: Static Web App - get API key for deployment\\n        id: static_web_app_apikey\\n        uses: azure/CLI@v2\\n        with:\\n          inlineScript: |\\n            APIKEY=$(az staticwebapp secrets list --name \'${{ env.STATICWEBAPPNAME }}\' | jq -r \'.properties.apiKey\')\\n            echo \\"APIKEY=$APIKEY\\" >> $GITHUB_OUTPUT\\n\\n      - name: Static Web App - build and deploy\\n        id: static_web_app_build_and_deploy\\n        uses: Azure/static-web-apps-deploy@v1\\n        with:\\n          azure_static_web_apps_api_token: ${{ steps.static_web_app_apikey.outputs.APIKEY }}\\n          repo_token: ${{ secrets.GITHUB_TOKEN }} # Used for Github integrations (i.e. PR comments)\\n          action: \'upload\'\\n          ###### Repository/Build Configurations - These values can be configured to match your app requirements. ######\\n          # For more information regarding Static Web App workflow configurations, please visit: https://aka.ms/swaworkflowconfig\\n          app_location: \'/blog-website\' # App source code path\\n          api_location: \'\' # Api source code path - optional\\n          output_location: \'build\' # Built app content directory - optional\\n          ###### End of Repository/Build Configurations ######\\n\\n      - name: Static Web App - get preview URL\\n        id: static_web_app_preview_url\\n        uses: azure/CLI@v2\\n        with:\\n          inlineScript: |\\n            DEFAULTHOSTNAME=$(az staticwebapp show -n \'${{ env.STATICWEBAPPNAME }}\' | jq -r \'.defaultHostname\')\\n            echo $DEFAULTHOSTNAME\\n\\n            PREVIEW_URL=\\"https://${DEFAULTHOSTNAME/.[1-9]./-${{github.event.pull_request.number }}.${{ env.LOCATION }}.1.}\\"\\n            echo $PREVIEW_URL\\n\\n            echo \\"PREVIEW_URL=$PREVIEW_URL\\" >> $GITHUB_OUTPUT\\n\\n    outputs:\\n      preview-url: ${{steps.static_web_app_preview_url.outputs.PREVIEW_URL}}\\n\\n  close_pull_request_job:\\n    if: github.event_name == \'pull_request\' && github.event.action == \'closed\'\\n    runs-on: ubuntu-latest\\n    name: Cleanup Pull Request staging environment\\n    steps:\\n      - name: Azure Login\\n        uses: azure/login@v1\\n        with:\\n          creds: ${{ secrets.AZURE_CREDENTIALS }}\\n\\n      - name: Get API key for deployment\\n        id: apikey\\n        uses: azure/CLI@v2\\n        with:\\n          inlineScript: |\\n            APIKEY=$(az staticwebapp secrets list --name \'${{ env.STATICWEBAPPNAME }}\' | jq -r \'.properties.apiKey\')\\n            echo \\"APIKEY=$APIKEY\\" >> $GITHUB_OUTPUT\\n\\n      - name: Close Pull Request\\n        id: closepullrequest\\n        uses: Azure/static-web-apps-deploy@v1\\n        with:\\n          azure_static_web_apps_api_token: ${{ steps.apikey.outputs.APIKEY }}\\n          action: \'close\'\\n```\\n\\nThe above workflow does the following:\\n\\n- For main branch deployments it releases our static web app making use of Bicep. For pull requests it tells us if there\'s any changes that the current PR would make to our SWA as a consequence.\\n- It acquires an API Key from Azure which can then be used to perform a deployment.\\n- It deploys [using the dedicated GitHub Action for SWAs](https://github.com/Azure/static-web-apps-deploy)\\n- It calculates the preview URL for a given pull request (it isn\'t used as yet, but could be)\\n- When a pull request is closed it triggers the GitHub Action to clean up the preview environment.\\n\\n## DNS and custom domains\\n\\nOnce our GitHub Action has run for the first time on the main branch, we\'ll be deploying to Azure Static Web Apps.\\n\\nOnce we\'ve started deploying there, we want to get our custom domain set up to point to it. To do this, we\'re going to fire up the [Azure Portal](https://portal.azure.com) and go to add a custom domain:\\n\\n![screenshot of the Azure Portal Add Custom Domain screen](./custom-domain.png)\\n\\nWe\'re going to add a TXT record for my blog. Azure generates a code for us:\\n\\n![screenshot of the Azure Portal Add Custom Domain screen](./custom-domain-code.png)\\n\\nWe need to take that code and go a register it with our DNS provider. In my case that\'s Cloudflare, so we can go there and add it:\\n\\n![screenshot of Cloudflare](./cloudflare-dns.png)\\n\\nAfter a while (I think about twenty minutes in my case), this lead to the domain name being validated:\\n\\n![screenshot of the Azure Portal Add Custom Domain screen with domain validated](./custom-domain-code-validated.png)\\n\\nNow that we have a custom domain set up in Azure, we want to uncomment the `resource customDomain` portion of the Bicep template now as well:\\n\\n```bicep\\nresource customDomain \'Microsoft.Web/staticSites/customDomains@2021-02-01\' = {\\n  parent: staticWebApp\\n  name: customDomainName\\n  properties: {}\\n}\\n```\\n\\nThis will mean that subsequent deployments to Azure do _not_ wipe out our newly configured domain name.\\n\\nWe\'re now ready to start pointing our DNS to the Static Web Apps instance. We jump back across to Cloudflare and we amend the CNAME record that currently points to johnnyreilly.github.io, and switch it to point to the auto-generated domain in Azure:\\n\\n![screenshot of Cloudflare with the CNAME record set](./cloudflare-dns-cname.png)\\n\\nAnd just like that, we\'re hosted on Static Web Apps!\\n\\n<FAQStructuredData faqs={faqs} />"},{"id":"docusaurus-createfeeditems-api-git-commit-date","metadata":{"permalink":"/docusaurus-createfeeditems-api-git-commit-date","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-01-28-docusaurus-createfeeditems-api-git-commit-date/index.md","source":"@site/blog/2023-01-28-docusaurus-createfeeditems-api-git-commit-date/index.md","title":"Docusaurus blogs: using the createFeedItems API with git commit date","description":"The Docusaurus createFeedItems API can be used to tweak RSS feeds for your blog. This post shows how to use it with the git commit date.","date":"2023-01-28T00:00:00.000Z","tags":[{"inline":false,"label":"Docusaurus","permalink":"/tags/docusaurus","description":"The Docusaurus static site generator."}],"readingTime":5.745,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"docusaurus-createfeeditems-api-git-commit-date","title":"Docusaurus blogs: using the createFeedItems API with git commit date","authors":"johnnyreilly","tags":["docusaurus"],"image":"./title-image.png","description":"The Docusaurus createFeedItems API can be used to tweak RSS feeds for your blog. This post shows how to use it with the git commit date.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Migrating from GitHub Pages to Azure Static Web Apps","permalink":"/migrating-from-github-pages-to-azure-static-web-apps"},"nextItem":{"title":"Image Optimisation with the TinyPNG API","permalink":"/image-optimisation-tinypng-api"}},"content":"A new API landed in Docusaurus 2.3.0 - it\'s called `createFeedItems`. It\'s a great API that allows you to tweak the Atom / RSS / JSON feeds for your blog. This post shows how to use it with the git commit date.\\n\\nThis post builds upon a technique we\'ve previously used to drive the `lastmod` properties of our sitemap. [You can read about driving `lastmod` from git commit here](../2022-11-25-adding-lastmod-to-sitemap-git-commit-date/index.md).\\n\\n\\n![title image reading \\"Docusaurus: using the createFeedItems API with git commit date\\" with the Docusaurus logo](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Updated 27/11/2023 - Docusaurus 3\\n\\nThis post was originally written targetting Docusaurus 2. Now Docusaurus 3 is landing, this post has been updated to cater for Docusaurus 3 usage.\\n\\n## `createFeedItems` API\\n\\n[I worked on the createFeedItems API for Docusaurus](https://github.com/facebook/docusaurus/pull/8378). When the [feature was announced](https://twitter.com/docusaurus/status/1619019412610191379), there were a number of suggested use cases:\\n\\n[![screenshot of a tweet describing things you could do with the createFeedItems API](./screenshot-tweet-createfeeditems.webp)](https://twitter.com/docusaurus/status/1619019412610191379)\\n\\nAs someone who worked on the API, you naturally might imagine that I\'d have some ideas for how to use it. I do!\\n\\nThere\'s two particular use cases that I\'ve been thinking about:\\n\\n1. Trimming the number of feed items\\n2. Using the latest git commit date for the feed item date\\n\\nThe reason I want to trim the number of feed items is because I have written a lot of blog posts. I learned that some RSS readers were choking on the size of my feed and rendering it unusable. So I thought a decent approach would be to trim the number of feed items to a more manageable number.\\n\\nThe second use case is a lot more fun! I want to use the git commit date for the feed item date. Docusaurus uses the date of post itself to drive this by default. You can see this by looking at the [Docusaurus atom feed](https://docusaurus.io/blog/atom.xml):\\n\\n![screenshot of Docusaurus atom feed](./screenshot-docusaurus-atom-feed.webp)\\n\\nThat\'s not a bad default. However, I tend to go back and edit my posts, particularly in the recent weeks after publishing. I don\'t want the date of the feed item to be the date of the post. I want it to be the date of the most recent commit. That way, if I go back and edit a post, the feed item date will be updated.\\n\\nWe\'re going to implement both of these.\\n\\n## `createFeedItems` API usage\\n\\nThe `createFeedItems` API is a function that takes a list of feed items and returns a list of feed items. I find looking at code easier than reading about code so let\'s look at [the example code from the docs](https://docusaurus.io/docs/blog#feed):\\n\\n```js\\n/** @type {import(\'@docusaurus/types\').Config} */\\nconst config = {\\n  // ...\\n  presets: [\\n    [\\n      \'@docusaurus/preset-classic\',\\n      {\\n        blog: {\\n          feedOptions: {\\n            type: \'all\',\\n            copyright: `Copyright \xa9 ${new Date().getFullYear()} Facebook, Inc.`,\\n            createFeedItems: async (params) => {\\n              const { blogPosts, defaultCreateFeedItems, ...rest } = params;\\n              return defaultCreateFeedItems({\\n                // keep only the 10 most recent blog posts in the feed\\n                blogPosts: blogPosts.filter((item, index) => index < 10),\\n                ...rest,\\n              });\\n            },\\n          },\\n        },\\n      },\\n    ],\\n  ],\\n};\\n```\\n\\nAs we can see - this is a function, which receives a single parameter. That parameter is an object with a number of properties. The most important of these is `blogPosts`. This is a list of blog posts. We can filter this list and return a new list. We can also call `defaultCreateFeedItems` to get the default behaviour. We can then tweak the result of that call.\\n\\nImportantly it\'s an `async` function. This means that we can do async work in it. We\'re going to use that when we get the git commit date.\\n\\n## Our implementation\\n\\nNow we know how to use the API, let\'s implement it to handle our use cases. To get the git commit date, we\'re going to use a package called [`simple-git`](https://github.com/steveukx/git-js). We\'ll add this as a dependency of our Docusaurus project:\\n\\n```bash\\nyarn add simple-git\\n```\\n\\nWe\'re going to create a new file to sit alongside our `docusaurus.config.js` file. We\'ll call it `createFeedItems.mjs` (as it\'s an ESM file):\\n\\n```js\\n//@ts-check\\nimport path from \'path\';\\nimport { simpleGit } from \'simple-git\';\\n\\n/** @type {import(\'@docusaurus/plugin-content-blog\').CreateFeedItemsFn} */\\nexport async function createFeedItems(params) {\\n  const { blogPosts, defaultCreateFeedItems, ...rest } = params;\\n\\n  const feedItems = await defaultCreateFeedItems({\\n    blogPosts,\\n    ...rest,\\n  });\\n\\n  for (const feedItem of feedItems) {\\n    // blogPost.metadata.permalink: \'/2023/01/22/image-optimisation-tinypng-api\',\\n    // feedItem.link: \'https://johnnyreilly.com/2023/01/22/image-optimisation-tinypng-api\',\\n    const relatedBlogEntry = blogPosts.find((blogPost) =>\\n      feedItem.link.endsWith(blogPost.metadata.permalink),\\n    );\\n    if (!relatedBlogEntry) {\\n      console.log(\'blogFilePath not found\', feedItem.link);\\n      throw new Error(`blogFilePath not found ${feedItem.link}`);\\n    }\\n\\n    // source: \'@site/blog/2023-01-22-image-optimisation-tinypng-api/index.md\',\\n    const gitLatestCommitString = await getGitLatestCommitDateFromFilePath(\\n      relatedBlogEntry.metadata.source.replace(\'@site/\', \'blog-website/\'),\\n    );\\n    const gitLatestCommitDate = gitLatestCommitString\\n      ? new Date(gitLatestCommitString)\\n      : undefined;\\n    if (gitLatestCommitDate) {\\n      feedItem.date = gitLatestCommitDate;\\n    }\\n  }\\n\\n  // keep only the 20 most recently updated blog posts in the feed\\n  const latest20FeedItems = Array.from(feedItems)\\n    .sort((a, b) => b.date.getDate() - a.date.getDate())\\n    .slice(0, 20);\\n\\n  return latest20FeedItems;\\n}\\n\\n/**\\n * Given a file path, return the last commit date\\n * @param {string} filePath\\n * @returns\\n */\\nasync function getGitLatestCommitDateFromFilePath(filePath) {\\n  const git = getSimpleGit();\\n\\n  const log = await git.log({\\n    file: filePath,\\n  });\\n\\n  const latestCommitDate = log.latest?.date;\\n\\n  return latestCommitDate;\\n}\\n\\n/** @type {import(\'simple-git\').SimpleGit | undefined} */\\nlet git;\\n\\n/**\\n * get a simple git instance\\n * @returns SimpleGit\\n */\\nfunction getSimpleGit() {\\n  if (!git) {\\n    const baseDir = path.resolve(process.cwd(), \'..\');\\n\\n    /** @type {Partial<import(\'simple-git\').SimpleGitOptions>} */\\n    const options = {\\n      baseDir,\\n      binary: \'git\',\\n      maxConcurrentProcesses: 6,\\n      trimmed: false,\\n    };\\n\\n    git = simpleGit(options);\\n  }\\n\\n  return git;\\n}\\n```\\n\\nWhat\'s happening here? Well, the `createFeedItems` function is taking the blog posts that come in and then calling `defaultCreateFeedItems` to get the default behaviour. We then iterate over the feed items and for each one we find the related blog post. We then use `simple-git` to get the last commit date for the blog post. We then set the feed item\'s date to the last commit date. We then sort the feed items by date and take the first 20. We then return those 20 feed items.\\n\\nIt\'s as simple as that. There\'s a few bits in there which are specific to my blog (like the `blog-website` directory) but you can see how you can tweak this to suit your needs.\\n\\nWith this implemented, we\'ll reference this in our `docusaurus.config.js` file:\\n\\n```js\\n//@ts-check\\nimport { createFeedItems } from \'./createFeedItems.mjs\';\\n\\n/** @type {import(\'@docusaurus/types\').Config} */\\nconst config = {\\n  // ...\\n  presets: [\\n    [\\n      \'@docusaurus/preset-classic\',\\n      /** @type {import(\'@docusaurus/preset-classic\').Options} */\\n      ({\\n        // ...\\n        feedOptions: {\\n          // ...\\n          createFeedItems,\\n          // ...\\n        },\\n        // ...\\n      }),\\n    ],\\n  ],\\n  // ...\\n};\\n\\nexport default config;\\n```\\n\\nAnd we\'re done! We can now run `yarn build` and see the results:\\n\\n![screenshot of Docusaurus atom feed for johnnyreilly](./screenshot-johnnyreilly-atom-feed.webp)\\n\\nLook for yourself at [johnnyreilly.com/atom.xml](https://johnnyreilly.com/atom.xml) or [johnnyreilly.com/rss.xml](https://johnnyreilly.com/rss.xml).\\n\\n## Conclusion\\n\\nHere we\'ve learned how to use the `createFeedItems` API to customise the feed items that are generated. We\'ve also seen how to use `simple-git` to get the last commit date for a file. We\'ve then used that to set the date of the feed item to the last commit date."},{"id":"image-optimisation-tinypng-api","metadata":{"permalink":"/image-optimisation-tinypng-api","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-01-22-image-optimisation-tinypng-api/index.md","source":"@site/blog/2023-01-22-image-optimisation-tinypng-api/index.md","title":"Image Optimisation with the TinyPNG API","description":"Image optimisation can be automated with the TinyPNG API. This post demonstrates how to do that.","date":"2023-01-22T00:00:00.000Z","tags":[],"readingTime":6.395,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"image-optimisation-tinypng-api","title":"Image Optimisation with the TinyPNG API","authors":"johnnyreilly","tags":[],"image":"./title-image.webp","description":"Image optimisation can be automated with the TinyPNG API. This post demonstrates how to do that.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Docusaurus blogs: using the createFeedItems API with git commit date","permalink":"/docusaurus-createfeeditems-api-git-commit-date"},"nextItem":{"title":"Docusaurus: improving Core Web Vitals with fetchpriority","permalink":"/docusaurus-improve-core-web-vitals-fetchpriority"}},"content":"Image optimisation can be automated with the TinyPNG API. This post demonstrates how to do that.\\n\\n![title image reading \\"Image Optimisation with the TinyPNG API\\" with TinyPNG and Lighthouse logos](title-image.webp)\\n\\n\x3c!--truncate--\x3e\\n\\n## Images and optimisation\\n\\nImages are a big part of the web. They\'re also a big part of the web\'s payload. If we\'re not careful, we can end up with a site that\'s slow to load and expensive to host. I run [Lighthouse](https://developer.chrome.com/docs/lighthouse/overview/) on my blog and I\'m always looking for ways to improve the performance of the site. One of the things that Lighthouse flags is image optimisation.\\n\\nIt\'s a good idea to optimise our images; to make sure they\'re not unhelpfully large. We can do this manually using tools like [TinyPNG](https://tinypng.com/) or [Squoosh](https://squoosh.app/). However, it\'s also possible to automate this process. In this post, I\'ll show you how to do that using the TinyPNG API.\\n\\n## The TinyPNG API\\n\\nThe [TinyPNG API](https://tinypng.com/developers) is a paid service. We can get a free API key which allows us to optimise 500 images per month. If we need to optimise more than that, we\'ll need to pay for a subscription. I rarely find I optimise more than 500 images per month so I\'m happy with the free plan.\\n\\nIt\'s worth noting that the name \\"TinyPNG\\" is a bit of a misnomer. The API supports a number of image formats including PNG, JPEG and WebP. It\'s not just for PNGs. In fact we\'ll be using the WebP format in this post.\\n\\nYou can just use the API directly. However, I prefer to use a client library. We\'ll be using [the Node.js](https://tinypng.com/developers/reference/nodejs) library.\\n\\n## Making a command line tool\\n\\nWe\'re going to initialise a simple Node.js console application called \\"tinify\\" using [TypeScript](https://www.typescriptlang.org/) and [`ts-node`](https://typestrong.org/ts-node/):\\n\\n```bash\\nmkdir tinify\\ncd tinify\\nnpm init -y\\nnpm install @types/node tinify ts-node typescript\\nnpx tsc --init\\n```\\n\\nYou\'ll note that we\'re using the `tinify` npm package [which is developed here](https://github.com/tinify/tinify-nodejs). Handily this package ships with TypeScript definitions, so we don\'t need to install a separate types package.\\n\\nIn our `package.json` file we\'ll add a `start` script to run our application:\\n\\n```json\\n  \\"scripts\\": {\\n    \\"start\\": \\"ts-node index.ts\\"\\n  },\\n```\\n\\nIn our `tsconfig.json` file we\'ll also up the `target` to a new ECMAScript emit version to allow us to use some newer language features. We don\'t need this for TinyPNG, but it\'s nice to use the newer features:\\n\\n```json\\n{\\n  \\"compilerOptions\\": {\\n    \\"target\\": \\"es2021\\"\\n  }\\n}\\n```\\n\\nNow we can create our `index.ts` file:\\n\\n```ts\\nimport fs from \'fs\';\\nimport path from \'path\';\\nimport tinify from \'tinify\';\\n\\nfunction setUpTinify() {\\n  if (!process.env.TINIFY_KEY) {\\n    console.log(\\n      \'Run with: TINIFY_KEY=$YOUR_API_KEY IMAGE_DIR=$YOUR_IMAGE_DIRECTORY yarn start\',\\n    );\\n    process.exit(1);\\n  }\\n\\n  tinify.key = process.env.TINIFY_KEY;\\n}\\n\\nfunction getImageFilesFromDirectory(dir: string) {\\n  return fs\\n    .readdirSync(dir)\\n    .filter(\\n      (file) =>\\n        file.endsWith(\'.jpg\') ||\\n        file.endsWith(\'.jpeg\') ||\\n        file.endsWith(\'.webp\') ||\\n        file.endsWith(\'.png\'),\\n    )\\n    .map((file) => path.resolve(dir, file))\\n    .filter((file) => fs.statSync(file).size > 0);\\n}\\n\\nasync function processImageFiles(imageFiles: string[]) {\\n  let processed = 0;\\n  let totalOriginalSizeKb = 0n;\\n  let totalNewSizeKb = 0n;\\n  let failed: string[] = [];\\n\\n  for (const imageFilePath of imageFiles) {\\n    try {\\n      console.log(`\\n\uD83D\uDDBC\uFE0F  Processing ${imageFilePath}\\n`);\\n      const originalImageFilePrefix = imageFilePath.substring(\\n        0,\\n        imageFilePath.lastIndexOf(\'.\'),\\n      );\\n\\n      const originalStats = await fs.promises.stat(imageFilePath, {\\n        bigint: true,\\n      });\\n      const originalSizeKb = originalStats.size / 1024n;\\n\\n      const source = tinify.fromFile(imageFilePath);\\n      const converted = source.convert({ type: [\'image/webp\', \'image/png\'] });\\n      const convertedExtension = await converted.result().extension();\\n      const newImageFilePath = `${originalImageFilePrefix}.${convertedExtension}`;\\n      await converted.toFile(newImageFilePath);\\n\\n      const newStats = await fs.promises.stat(newImageFilePath, {\\n        bigint: true,\\n      });\\n      const newSizeKb = newStats.size / 1024n;\\n\\n      const imageFileName = path.basename(imageFilePath);\\n      const newImageFileName = path.basename(newImageFilePath);\\n\\n      totalOriginalSizeKb += originalSizeKb;\\n      totalNewSizeKb += newSizeKb;\\n\\n      console.log(`- \uD83D\uDD34 ${originalSizeKb}kb - ${imageFileName}\\n- \uD83D\uDFE2 ${newSizeKb}kb - ${newImageFileName}\\n- \uD83D\uDD3D ${calculatePercentageReduction({ originalSizeKb, newSizeKb }).toFixed(\\n        2,\\n      )}% reduction\\n\\n\u2705 Processed! (${++processed} of ${imageFiles.length})\\n\\n----------------------`);\\n    } catch (e) {\\n      console.log(`\\\\n\u274C Failed to process ${imageFilePath}`);\\n      failed.push(imageFilePath);\\n    }\\n  }\\n\\n  console.log(`\\n************************************************\\n* Total savings for ${imageFiles.length} images \\n- \uD83D\uDD34 ${totalOriginalSizeKb}kb\\n- \uD83D\uDFE2 ${totalNewSizeKb}kb\\n- \uD83D\uDD3D ${calculatePercentageReduction({\\n    originalSizeKb: totalOriginalSizeKb,\\n    newSizeKb: totalNewSizeKb,\\n  }).toFixed(2)}% reduction\\n************************************************\\n`);\\n\\n  if (failed.length > 0) console.log(\'Failed to process\', failed);\\n}\\n\\nfunction calculatePercentageReduction({\\n  originalSizeKb,\\n  newSizeKb,\\n}: {\\n  originalSizeKb: bigint;\\n  newSizeKb: bigint;\\n}) {\\n  return (\\n    ((Number(originalSizeKb) - Number(newSizeKb)) / Number(originalSizeKb)) *\\n    100\\n  );\\n}\\n\\nasync function run() {\\n  setUpTinify();\\n\\n  let directory = process.env.IMAGE_DIR;\\n\\n  if (!directory) {\\n    console.log(\'No directory specified!\');\\n    process.exit(1);\\n  }\\n\\n  const imageFiles = getImageFilesFromDirectory(directory);\\n  console.log(`Found ${imageFiles.length} image files in ${directory}`);\\n  await processImageFiles(imageFiles);\\n}\\n\\n// do it!\\nrun();\\n```\\n\\nThere\'s a number of things happening here. Let me walk it through; each time we run:\\n\\n1. We\'re checking that we have a TinyPNG API key and an image directory specified. If not, we\'ll exit with an error message.\\n2. We\'re getting a list of image files from the specified directory. We look for files with the extensions `.jpg`, `.jpeg`, `.webp` and `.png` (those formats supported by TinyPNG). We also filter out any files that are empty.\\n3. We\'re looping through the image files and processing them one by one. We\'re using the `tinify` package to shrink the image; and we say we\'ll accept either `webp` or `png` as our target format. Tinify will decide which is the most optimal format upon each request and render accordingly. Finally we\'re saving the new files to the same directory as the original file. We\'re also calculating the percentage reduction in file size.\\n\\nIf we wanted to look just at the code that does the actual conversion, it\'s this:\\n\\n```ts\\nconst source = tinify.fromFile(imageFilePath);\\nconst converted = source.convert({ type: [\'image/webp\', \'image/png\'] });\\nconst convertedExtension = await converted.result().extension();\\nconst newImageFilePath = `${originalImageFilePrefix}.${convertedExtension}`;\\nawait converted.toFile(newImageFilePath);\\n```\\n\\n## Using the tool\\n\\nWith our tool written, we now need to test it out. I\'ve a directory of images that I want to compress: `~/code/github/open-graph-sharing-previews/images-to-shrink`\\n\\n![screenshot of image files before optimisation](screenshot-files-before-optimisation.png)\\n\\nNow let\'s run our tool against that directory and see what happens:\\n\\n```bash\\nTINIFY_KEY=YOUR_API_KEY_GOES_HERE IMAGE_DIR=~/code/github/open-graph-sharing-previews/images-to-shrink yarn start\\n\\nyarn run v1.22.18\\n$ ts-node index.ts\\nFound 6 image files in /home/john/code/github/open-graph-sharing-previews/images-to-shrink\\n\\n\uD83D\uDDBC\uFE0F  Processing /home/john/code/github/open-graph-sharing-previews/images-to-shrink/screenshot-of-demo-with-devtools-open.png\\n\\n- \uD83D\uDD34 253kb - screenshot-of-demo-with-devtools-open.png\\n- \uD83D\uDFE2 83kb - screenshot-of-demo-with-devtools-open.png\\n- \uD83D\uDD3D 67.19% reduction\\n\\n\u2705 Processed! (1 of 6)\\n\\n----------------------\\n\\n\uD83D\uDDBC\uFE0F  Processing /home/john/code/github/open-graph-sharing-previews/images-to-shrink/screenshot-of-email-demonstrating-sharing-with-a-non-cropped-image.png\\n\\n- \uD83D\uDD34 158kb - screenshot-of-email-demonstrating-sharing-with-a-non-cropped-image.png\\n- \uD83D\uDFE2 50kb - screenshot-of-email-demonstrating-sharing-with-a-non-cropped-image.png\\n- \uD83D\uDD3D 68.35% reduction\\n\\n\u2705 Processed! (2 of 6)\\n\\n----------------------\\n\\n\uD83D\uDDBC\uFE0F  Processing /home/john/code/github/open-graph-sharing-previews/images-to-shrink/screenshot-of-tweet-demonstrating-sharing-with-a-cropped-image.png\\n\\n- \uD83D\uDD34 391kb - screenshot-of-tweet-demonstrating-sharing-with-a-cropped-image.png\\n- \uD83D\uDFE2 64kb - screenshot-of-tweet-demonstrating-sharing-with-a-cropped-image.webp\\n- \uD83D\uDD3D 83.63% reduction\\n\\n\u2705 Processed! (3 of 6)\\n\\n----------------------\\n\\n\uD83D\uDDBC\uFE0F  Processing /home/john/code/github/open-graph-sharing-previews/images-to-shrink/screenshot-of-tweet-demonstrating-sharing.png\\n\\n- \uD83D\uDD34 407kb - screenshot-of-tweet-demonstrating-sharing.png\\n- \uD83D\uDFE2 78kb - screenshot-of-tweet-demonstrating-sharing.webp\\n- \uD83D\uDD3D 80.84% reduction\\n\\n\u2705 Processed! (4 of 6)\\n\\n----------------------\\n\\n\uD83D\uDDBC\uFE0F  Processing /home/john/code/github/open-graph-sharing-previews/images-to-shrink/screenshot-of-twitter-validator.png\\n\\n- \uD83D\uDD34 162kb - screenshot-of-twitter-validator.png\\n- \uD83D\uDFE2 49kb - screenshot-of-twitter-validator.webp\\n- \uD83D\uDD3D 69.75% reduction\\n\\n\u2705 Processed! (5 of 6)\\n\\n----------------------\\n\\n\uD83D\uDDBC\uFE0F  Processing /home/john/code/github/open-graph-sharing-previews/images-to-shrink/title-image.png\\n\\n- \uD83D\uDD34 308kb - title-image.png\\n- \uD83D\uDFE2 49kb - title-image.webp\\n- \uD83D\uDD3D 84.09% reduction\\n\\n\u2705 Processed! (6 of 6)\\n\\n----------------------\\n\\n************************************************\\n* Total savings for 6 images\\n- \uD83D\uDD34 1679kb\\n- \uD83D\uDFE2 373kb\\n- \uD83D\uDD3D 77.78% reduction\\n************************************************\\n\\nDone in 25.23s.\\n```\\n\\nIsn\'t that impressive? We\'ve reduced the file size of all of these images by an average amount of 77.78%! That\'s a huge saving.\\n\\nIf we look a little closer, we\'ll see that on two occasions the format has remained as a PNG file and the size has shrunk. In four cases, the format has changed to a WebP file. When we look at our directory again, we\'ll see that the files have been updated, and some new WebP files have been created:\\n\\n![screenshot of image files after optimisation](screenshot-files-after-optimisation.png)\\n\\n## Conclusion\\n\\nWe\'ve seen how we can use the TinyPNG API to optimise our images. We\'ve also built a tool that uses the TinyPNG API to optimise the images in a given directory.\\n\\nIt\'s all automated. We can now run this script whenever we want to optimise the images in any directory!\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/automate-image-optimization-tinypng-api/)\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.logrocket.com/automate-image-optimization-tinypng-api/\\" />\\n</head>"},{"id":"docusaurus-improve-core-web-vitals-fetchpriority","metadata":{"permalink":"/docusaurus-improve-core-web-vitals-fetchpriority","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-01-18-docusaurus-improve-core-web-vitals-fetchpriority/index.md","source":"@site/blog/2023-01-18-docusaurus-improve-core-web-vitals-fetchpriority/index.md","title":"Docusaurus: improving Core Web Vitals with fetchpriority","description":"By using `fetchpriority` on your Largest Contentful Paint you can improve your Core Web Vitals. This post implements that with Docusaurus.","date":"2023-01-18T00:00:00.000Z","tags":[{"inline":false,"label":"Docusaurus","permalink":"/tags/docusaurus","description":"The Docusaurus static site generator."}],"readingTime":4.875,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"docusaurus-improve-core-web-vitals-fetchpriority","title":"Docusaurus: improving Core Web Vitals with fetchpriority","authors":"johnnyreilly","tags":["docusaurus"],"image":"./title-image.png","description":"By using `fetchpriority` on your Largest Contentful Paint you can improve your Core Web Vitals. This post implements that with Docusaurus.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Image Optimisation with the TinyPNG API","permalink":"/image-optimisation-tinypng-api"},"nextItem":{"title":"How I ruined my SEO","permalink":"/how-i-ruined-my-seo"}},"content":"By using `fetchpriority` on your Largest Contentful Paint you can improve your Core Web Vitals. This post implements that with Docusaurus v2. There is a follow on post that details [migrating this plugin to Docusaurus v3](../2023-10-09-docusaurus-3-how-to-migrate-rehype-plugins/index.md).\\n\\n![title image reading \\"Docusaurus: improving Core Web Vitals with fetchpriority\\"](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Avoiding lazy loading on the Largest Contentful Paint\\n\\nAt the weekend [I wrote a post documenting how I believe I ruined the SEO on my blog](../2023-01-15-how-i-ruined-my-seo/index.md). That post ended up [trending on Hacker News](https://news.ycombinator.com/item?id=34389421). People made suggestions around things I could do that could improve things. One post in particular caught my eye from [Growtika](https://growtika.com) saying:\\n\\n> Page speed: It\'s one of the most important ranking factor. You don\'t have to get 100 score, but passing the core web vitals score and having higher score on mobile is recommended.\\n>\\n> https://pagespeed.web.dev/report?url=https%3A%2F%2Fjohnnyreilly.com%2F&form_factor=mobile\\n>\\n> A cool trick to improve the result fast is by removing the lazy load effect from the LCP:\\n\\n![screenshot of web test results that reads largest contentful paint image was lazily loaded ](screenshot-largest-contentful-paint-image-lazy-loaded.webp)\\n\\nAnother person chimed in with:\\n\\n> Indeed. Even better, making it high priority instead of normal: https://addyosmani.com/blog/fetch-priority/\\n\\n## `fetchpriority`\\n\\nI hadn\'t heard of `fetchpriority` before this, but the linked article by [Addy Osmani](https://addyosmani.com) carried this tip:\\n\\n> Add `fetchpriority=\\"high\\"` to your Largest Contentful Paint (LCP) image to get it to load sooner. Priority Hints sped up Etsy\u2019s LCP by 4% with some sites seeing an improvement of up to 20-30% in their lab tests. In many cases, fetchpriority should lead to a nice boost for LCP.\\n\\nI was keen to try this out. Somewhat interestingly, I was the person responsible for [originally contributing lazy loading to Docusaurus](https://github.com/facebook/docusaurus/pull/6598). For what it\'s worth, lazy loading is a _good thing_ to do. It\'s just that in this case, it was causing the LCP to be lazy loaded. I wanted to change that.\\n\\n## Swizzling the image component\\n\\nSince my initial contribution, the [implementation had been tweaked to allow user control via Swizzling](https://github.com/facebook/docusaurus/pull/6990). By the way, [swizzling is a great feature of Docusaurus](https://docusaurus.io/docs/swizzling). It allows you to override the default implementation of a component. In this case, I wanted to override the `Img` component and opt out of lazy loading. I did this by running the following command:\\n\\n```bash\\nyarn swizzle @docusaurus/theme-classic MDXComponents/Img -- --eject\\n```\\n\\nThis created a file at `src/theme/MDXComponents/Img.js`. I then made the following change:\\n\\n```diff\\nimport React from \'react\';\\nimport clsx from \'clsx\';\\nimport styles from \'./styles.module.css\';\\nfunction transformImgClassName(className) {\\n  return clsx(className, styles.img);\\n}\\nexport default function MDXImg(props) {\\n  return (\\n    // eslint-disable-next-line jsx-a11y/alt-text\\n    <img\\n-      loading=\\"lazy\\"\\n      {...props}\\n      className={transformImgClassName(props.className)}\\n    />\\n  );\\n}\\n```\\n\\nGetting rid of the `loading=\\"lazy\\"` attribute was all I needed to do. This gets us to the point where none of our images are lazy loaded anymore. Stage 1 complete!\\n\\n## Adding `fetchpriority=\\"high\\"` to the LCP with a custom plugin\\n\\nThe next thing to do was to write a small Rehype plugin to add `fetchpriority=\\"high\\"` to the LCP. I did this by creating a new JavaScript file called `image-fetchpriority-rehype-plugin.js`:\\n\\n```js\\n// @ts-check\\nconst visit = require(\'unist-util-visit\');\\n\\n/**\\n * Create a rehype plugin that will make the first image eager loaded with fetchpriority=\\"high\\" and lazy load all other images\\n * @returns rehype plugin that will make the first image eager loaded with fetchpriority=\\"high\\" and lazy load all other images\\n */\\nfunction imageFetchPriorityRehypePluginFactory() {\\n  /** @type {Map<string, string>} */ const files = new Map();\\n\\n  /** @type {import(\'unified\').Transformer} */\\n  return (tree, vfile) => {\\n    visit(tree, [\'element\', \'jsx\'], (node) => {\\n      if (node.type === \'element\' && node[\'tagName\'] === \'img\') {\\n        // handles nodes like this:\\n        // {\\n        //   type: \'element\',\\n        //   tagName: \'img\',\\n        //   properties: {\\n        //     src: \'https://some.website.com/cat.gif\',\\n        //     alt: null\\n        //   },\\n        //   ...\\n        // }\\n\\n        const key = `img|${vfile.history[0]}`;\\n        const imageAlreadyProcessed = files.get(key);\\n        const fetchpriorityThisImage =\\n          !imageAlreadyProcessed ||\\n          imageAlreadyProcessed === node[\'properties\'][\'src\'];\\n\\n        if (!imageAlreadyProcessed) {\\n          files.set(key, node[\'properties\'][\'src\']);\\n        }\\n\\n        if (fetchpriorityThisImage) {\\n          node[\'properties\'].fetchpriority = \'high\';\\n          node[\'properties\'].loading = \'eager\';\\n        } else {\\n          node[\'properties\'].loading = \'lazy\';\\n        }\\n      } else if (node.type === \'jsx\' && node[\'value\']?.includes(\'<img \')) {\\n        // handles nodes like this:\\n\\n        // {\\n        //   type: \'jsx\',\\n        //   value: \'<img src={require(\\"!/workspaces/blog.johnnyreilly.com/blog-website/node_modules/url-loader/dist/cjs.js?limit=10000&name=assets/images/[name]-[hash].[ext]&fallback=/workspaces/blog.johnnyreilly.com/blog-website/node_modules/file-loader/dist/cjs.js!./bower-with-the-long-paths.png\\").default} width=\\"640\\" height=\\"497\\" />\'\\n        // }\\n\\n        // if (!vfile.history[0].includes(\'blog/2023-01-15\')) return;\\n\\n        const key = `jsx|${vfile.history[0]}`;\\n        const imageAlreadyProcessed = files.get(key);\\n        const fetchpriorityThisImage =\\n          !imageAlreadyProcessed || imageAlreadyProcessed === node[\'value\'];\\n\\n        if (!imageAlreadyProcessed) {\\n          files.set(key, node[\'value\']);\\n        }\\n\\n        if (fetchpriorityThisImage) {\\n          node[\'value\'] = node[\'value\'].replace(\\n            /<img /g,\\n            \'<img loading=\\"eager\\" fetchpriority=\\"high\\" \',\\n          );\\n        } else {\\n          node[\'value\'] = node[\'value\'].replace(\\n            /<img /g,\\n            \'<img loading=\\"lazy\\" \',\\n          );\\n        }\\n      }\\n    });\\n  };\\n}\\n\\nmodule.exports = imageFetchPriorityRehypePluginFactory;\\n```\\n\\nThe above plugin runs over the AST of the MDX file and adds `fetchpriority=\\"high\\"` to the first image. It also adds `loading=\\"eager\\"` to the first image and `loading=\\"lazy\\"` to all other images.\\n\\nInterestingly, when I was writing it I discovered that the visitor is invoked multiple times for the same elements. I\'m not quite sure why, but the logic in the plugin uses a `Map` to keep track of which images have already been processed. TL;DR it works!\\n\\nI then added the plugin to the `docusaurus.config.js` file:\\n\\n```js\\n//@ts-check\\nconst imageFetchPriorityRehypePlugin = require(\'./image-fetchpriority-rehype-plugin\');\\n\\n/** @type {import(\'@docusaurus/types\').Config} */\\nconst config = {\\n  // ...\\n  presets: [\\n    [\\n      \'@docusaurus/preset-classic\',\\n      /** @type {import(\'@docusaurus/preset-classic\').Options} */\\n      ({\\n        // ...\\n        blog: {\\n          // ...\\n          rehypePlugins: [imageFetchPriorityRehypePlugin],\\n          // ...\\n        },\\n        // ...\\n      }),\\n    ],\\n  ],\\n  // ...\\n};\\n\\nmodule.exports = config;\\n```\\n\\n## What does it look like when applied?\\n\\nNow we have this in place, if we run the same test with [pagespeed](https://pagespeed.web.dev/) we have different results:\\n\\n![screenshot showing fetchpriority=\\"high\\" has been applied to LCP image](screenshot-largest-contentful-paint-image-fetchpriority.webp)\\n\\nWe\'re now _not_ lazy loading the image and we\'re also making it a high priority fetch. Great news!\\n\\nI\'d like for this to be the default behaviour for Docusaurus. I\'m not sure if it\'s possible to do this in a way that\'s straightforward. [I\'ve raised an issue on the Docusaurus repo to see if it\'s possible](https://github.com/facebook/docusaurus/issues/8552)."},{"id":"how-i-ruined-my-seo","metadata":{"permalink":"/how-i-ruined-my-seo","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-01-15-how-i-ruined-my-seo/index.md","source":"@site/blog/2023-01-15-how-i-ruined-my-seo/index.md","title":"How I ruined my SEO","description":"In October 2022 traffic to my blog dropped like a stone. What happened?","date":"2023-01-15T00:00:00.000Z","tags":[{"inline":false,"label":"SEO","permalink":"/tags/seo","description":"Search Engine Optimization."}],"readingTime":9.065,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"how-i-ruined-my-seo","title":"How I ruined my SEO","authors":"johnnyreilly","tags":["seo"],"image":"./title-image.png","description":"In October 2022 traffic to my blog dropped like a stone. What happened?","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Docusaurus: improving Core Web Vitals with fetchpriority","permalink":"/docusaurus-improve-core-web-vitals-fetchpriority"},"nextItem":{"title":"Azure Pipelines - Node.js 16 and custom pipelines task extensions","permalink":"/azure-pipelines-custom-pipelines-task-extension-node-16"}},"content":"In October 2022 traffic to my blog dropped like a stone. What happened? Somehow I ruined my SEO. Don\'t be me. I\'ll tell you what I got up to and hopefully you can avoid doing the same.\\n\\n## Updated 20/11/2023: SEO fixed!\\n\\nThere\'s a follow up to this named [\\"How we fixed my SEO\\"](../2023-11-28-how-we-fixed-my-seo/index.md) that you may enjoy.\\n\\n![title image reading \\"How I ruined my SEO\\" with an image of a tire fire in the background](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## What I did on my holidays\\n\\nNaturally I blame all of this on a holiday to the British seaside. I was away for a week, and whilst I was away I did not have access to a laptop. This is intentional by the way; I spend too much time on computers one way or another. I force myself to disconnect on holidays. But whilst I didn\'t have the ability to program, I had the ability to ponder.\\n\\nI found myself going down a rabbit hole on SEO. I\'d never really thought about it previously, and I thought \\"what would happen if I made some tweaks?\\" My expectation was that I\'d slightly improve my SEO. Probably not by much, but I\'d learn something and it\'d be fun. What actually happened was that in October 2022 (after my fiddling), traffic from search engines more or less dried up. Not quite the plan.\\n\\n![screenshot of google analytics demonstrating traffic rapidly tailing off](screenshot-google-analytics.png)\\n\\nOdds are, the was probably because of my actions. I\'m not sure what I did wrong, but I\'m going to share what I did and maybe you can tell me where I pulled the pin out of the hand grenade.\\n\\nFrustratingly, the feedback loop on SEO is anything but tight. You make a change, and then weeks (or months) later you see the results. And by then you\'ve forgotten what you did. So I\'m going to try and document what I did and what I think I did wrong.\\n\\nIncidentally, I\'m hoping someone will read this and tell me what I did wrong. I did something. I assume I did something. Come with me and embrace your inner Sherlock. I\'m going to share evidence and maybe you can draw some conclusions.\\n\\nSo what did I get up to? In the time before my traffic fell off a cliff I did all kinds of things. Let\'s begin.\\n\\n## Upgraded to Docusaurus 2.2\\n\\nMy blog runs on Docusaurus. I upgraded from 2.1 to 2.2. I can\'t see why that would be an issue. I don\'t think it is.\\n\\n## Added fontaine\\n\\n[I started using fontaine on my blog](https://github.com/johnnyreilly/blog.johnnyreilly.com/pull/305). If you haven\'t tried it out, [you can find it here](https://github.com/danielroe/fontaine). It helps reduce Cumulative Layout Shift. The flash of unstyled content jank that you can see when you first land on a site, before fonts have loaded. I can\'t see why that would be an issue. It should improve my blogs Core Web Vitals and help stuff rank better, not worse. I think this is a red herring.\\n\\n## Google Analytics - sharing my g-tag with the Docusaurus docs\\n\\nHere\'s where I suspect we may have a candidate. I did a foolish thing. You may be aware that Google are sunsetting Google Analytics as was, in favour of Google Analytics 4. I was using Google Analytics to track my blog traffic and thought \\"oh well I best migrate then\\".\\n\\nMigration involved using in a new plugin for Docusaurus. However, the docs weren\'t great. I managed to work out how to get it working, and I thought I\'d help the community by [submitting a docs PR](https://github.com/facebook/docusaurus/pull/7252). Can you see where this is going?\\n\\n![screenshot showing me submitting my actual GA4 tag](screenshot-docusaurus-g-tag.png)\\n\\nYup. I managed to land my GA4 tag in the actual Docusaurus docs... I know, I know. I\'m a mug. You might be wondering how I found out. Well the real giveaway was that I\'ve never written any blogposts in Chinese.\\n\\n![Screenshot of search console insights with traffic from Chinese websites](screenshot-search-console-insights.webp)\\n\\nI started seeing unfamiliar entries in my search traffic. I couldn\'t work out what was going on. It didn\'t make sense. Then I remembered my PR and the terrible truth became apparent:\\n\\n> It is a truth universally acknowledged, that a developer in possession of a good keyboard, must **copy and paste**.\\n\\nNightmare. Other people were using my GA4 tag.\\n\\nI did try to roll this back; search GitHub for my tag and [submit PRs to remove it](https://github.com/facebook/docusaurus/pull/8313). But not every PR was merged. In the end I gave up and created a new GA4 property and started again. Out there right now, there are still websites sending my old GA4 tag traffic to Google. What a horlicks.\\n\\nI don\'t know if Google tracks for sites sharing analytics tags and deranks them as a consequence, but I suspect it\'s a possibility. Who knows? (Maybe you do? Tell me!)\\n\\n## Googles new spam update\\n\\nWhen I started to see traffic tail off, I started to look around for clues. It turns out there\'s a subculture of SEO tools out there. I\'m not sure how I missed them before. I found [ahrefs](https://ahrefs.com) and [semrush](https://semrush.com); others too. This graph from ahrefs caught my eye:\\n\\n![screenshot of ahrefs demonstrating traffic rapidly tailing off aligned with google spam update](screenshot-ahrefs-spam-update.webp)\\n\\nYou can see everything going South for me in October. What you can also see are Google search updates on the X axis. It turns out Google regularly update their search algorithm. [Interestingly, one of their updates coincides with my traffic tailing off](https://ahrefs.com/google-algorithm-updates#october-2022-spam-update-2022-10-19).\\n\\n> ### October 2022 Spam Update\\n>\\n> Spam updates target sites that don\'t follow the webmaster guidelines. Sites impacted by these updates may be seeking short term gains while ignoring best practices. This update was completed on October 21st.\\n>\\n> [Google Search spam updates and your site \u2197](https://developers.google.com/search/updates/spam-updates)\\n\\n\\"Bingo!\\" I thought. \\"This is it!\\" But as I dug through the details, I became doubtful. Nothing on my site looks spammy. In my opinion obviously. But try as I might, I couldn\'t see it any other way. My content isn\'t spammy. Unless I\'m missing something? Am I?\\n\\n## From PNG to WebP and back again\\n\\nMost of the images on my blog were PNGs. Lighthouse would regularly suggest migrating to a newer image format. I read around and the suggestion generally was that WebP was the way to go. So I did. But I think I made a bit of a mistake. As the images were converted, their filenames changed.\\n\\nBecause I didn\'t think it mattered, I didn\'t implement redirects. My view was \\"the blog posts have references to the new image names - that\'s likely all that matters\\". I\'d lay money that\'s a mistake; that I should have implemented redirects and the site is being penalised.\\n\\nAgain, do tell me if I\'m running with a false assumption here.\\n\\nOh the \\"and back again\\". I make use of [Open Graph sharing previews on my blog](../2021-12-12-open-graph-sharing-previews-guide/index.md) - so people using my links on social media get a nice preview of the content. [I learned from Steve Fenton that open graph doesn\'t always support WebP](https://www.stevefenton.co.uk/blog/2022/10/webp-opengraph-images/). Which sucks.\\n\\nSo I decided to revert my Open Graph images back to being PNGs; with entirely different names. Again I didn\'t implement redirects - no wonder Google loves me!\\n\\n## Backlinks / referring domains\\n\\nAs I did my deepdive into SEO, I learned that backlinks and referring domains are important. I had a lot of them; I\'ve been blogging for a long time. However, I suspect I had rather scorched the earth by failing to implement redirects. This chart from ahrefs shows the impact:\\n\\n![screenshot of an ahrefs graph showing a drop off in the number of referring domains around mid 2022](screenshot-referring-domains.webp)\\n\\nMy assumption here is that by failing to implement redirects, I\'ve lost a lot of backlinks. Previous 200s had transitioned to be 404s and Google had noticed.\\n\\n## RSS feeds\\n\\nI mentioned that I\'ve been blogging a long time. Consequently I have a lot of blog posts. I also have [Atom](https://johnnyreilly.com/atom.xml) / [RSS](https://johnnyreilly.com/rss.xml) feeds on my blog. I didn\'t realise that there are limits on the size of these feeds. It doesn\'t appear to be standardised; but when I took a look at my feeds in various feed readers, I found they were erroring due to the size of the feeds.\\n\\nI decided to start truncating the number of entries in my feeds. It\'s not so hard to do, just a post build step which [reads, amends and writes the XML](../2022-11-22-xml-read-and-write-with-node-js/index.md).\\n\\nWith this in place RSS readers seemed to be happier. And given a number of publications read my RSS feeds, it\'s likely that this will increase my backlinks over time.\\n\\nI also contributed a [PR to Docusaurus](https://github.com/facebook/docusaurus/pull/8378) that will allow everyone to configure and adjust the number of entries in their feeds directly through Docusaurus; as opposed to afterwards in a post build step.\\n\\n## Dynamic redirects - too little too late?\\n\\nAs I\'ve mentioned, I broke links by not implementing redirects. It might be closing the stable door after the horse has bolted, but I decided to go back and implement redirects. In December 2022 [I implemented dynamic redirects on my blog using Azure Static Web Apps and Azure Functions](../2022-12-22-azure-static-web-apps-dynamic-redirects-azure-functions/index.md).\\n\\nI implemented redirects for:\\n\\n- images\\n- blog posts (from my old Blogger URLs to my new Docusaurus URLs)\\n- RSS / Atom feeds (Blogger had both of these but at different endpoints)\\n- renamed blog posts (I renamed a number of blog posts over time to be more SEO friendly)\\n\\nI also decided to do some research. [I plugged Application Insights into my blog](../2023-01-01-application-insights-bicep-azure-static-web-apps/index.md) and started logging out when redirects were being hit. I also started logging out when 404s were being hit. I wanted to see if I was missing anything. I\'ve been checking the logs every day since, and adding new redirects as I go.\\n\\nWill this help over time? Answers on a postcard please. Or toot / tweet / email / DM me.\\n\\nAs an aside, looking at the logs in itself has been a lesson:\\n\\n![many redirects from Wordpress URLs](screenshot-application-insights-404.webp)\\n\\nSomeone on the internet is _always_ trying to hack you. And usually under the assumption you\'re running WordPress / PHP.\\n\\n## Help me Obi-Wan, you\'re my only hope\\n\\nAs you can see, I\'ve done a lot of tinkering. I\'m not quite sure what torched my SEO. It may be one thing, it may be a combination of things. I don\'t know if there\'s a road back.\\n\\nI\'m hoping someone will read this and tell me what I did wrong. I did something. Or at least I assume I\'m the cause. Maybe I\'m not? Maybe I\'m missing something entirely. If you know, please let me know. I really want to understand!\\n\\n## Discussion on Hacker News\\n\\nThis was disussed on Hacker News. [You can read the discussion here](https://news.ycombinator.com/item?id=34389421)."},{"id":"azure-pipelines-custom-pipelines-task-extension-node-16","metadata":{"permalink":"/azure-pipelines-custom-pipelines-task-extension-node-16","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-01-05-azure-pipelines-custom-pipelines-task-extension-node-16/index.md","source":"@site/blog/2023-01-05-azure-pipelines-custom-pipelines-task-extension-node-16/index.md","title":"Azure Pipelines - Node.js 16 and custom pipelines task extensions","description":"Support for Node.js 16 for Azure Pipelines custom pipelines task extensions has arrived. From a TypeScript perspective, this post documents how to migrate.","date":"2023-01-05T00:00:00.000Z","tags":[{"inline":false,"label":"Azure Pipelines","permalink":"/tags/azure-pipelines","description":"The Azure Pipelines CI / CD service."},{"inline":false,"label":"Node.js","permalink":"/tags/node-js","description":"The Node.js runtime."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":4.18,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-pipelines-custom-pipelines-task-extension-node-16","title":"Azure Pipelines - Node.js 16 and custom pipelines task extensions","authors":"johnnyreilly","tags":["azure pipelines","node.js","typescript"],"image":"./title-image.png","description":"Support for Node.js 16 for Azure Pipelines custom pipelines task extensions has arrived. From a TypeScript perspective, this post documents how to migrate.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"How I ruined my SEO","permalink":"/how-i-ruined-my-seo"},"nextItem":{"title":"Using Application Insights with Bicep to monitor Azure Static Web Apps and Azure Functions","permalink":"/application-insights-bicep-azure-static-web-apps"}},"content":"Support for Node.js 16 for Azure Pipelines custom pipelines task extensions has arrived. From a TypeScript perspective, this post documents how to migrate from a Node.js 10 custom task to one that runs on Node 16 using [`azure-pipelines-task-lib`](https://www.npmjs.com/package/azure-pipelines-task-lib).\\n\\n![title image reading \\"Azure Pipelines - Node.js 16 and custom pipelines task extensions\\" with Azure Pipelines, Node.js and TypeScript logos](title-image.png)\\n\\n## Updated 26th September 2024 - Node.js 20 support available\\n\\nIt\'s now possible to use Node.js 20 in tasks! See more details below:\\n\\n- https://aka.ms/node-runner-guidance\\n- [There is a guide on how to migrate tasks to Node.js 20 here](https://github.com/microsoft/azure-pipelines-tasks/blob/master/docs/migrateNode20.md)\\n\\nWhat\'s more we\'re going to start to see [warnings emitted in pipelines](https://learn.microsoft.com/en-us/azure/devops/release-notes/2024/pipelines/sprint-240-update#tasks-that-use-an-end-of-life-node-runner-version-to-execute-emit-warnings) when an EOL Node version is used.\\n\\n\x3c!--truncate--\x3e\\n\\n## The road to Node.js 16\\n\\nAzure Pipelines custom pipelines task extensions have been around for a while. They\'re a great way to extend the functionality of Azure Pipelines. They\'re written in TypeScript and run on Node.js. [You can learn how to write one here](https://learn.microsoft.com/en-us/azure/devops/extend/develop/add-build-task?view=azure-devops). However, until recently they were restricted to only be able to run on Node.js 6 or Node.js 10. This was a problem as [support for Node 6 ended in 2018 and Node 10 ended in 2020](https://endoflife.date/nodejs).\\n\\nA [GitHub issue was opened to track support for different Node versions with custom tasks](https://github.com/microsoft/azure-pipelines-agent/issues/3195), but it remained unresolved for a long time. [In October 2022 it was announced that Node.js 16 support was available](https://learn.microsoft.com/en-us/azure/devops/release-notes/2022/sprint-210-update#node-16-task-runner-in-pipeline-agent).\\n\\n## Migrating a task to Node.js 16\\n\\nThere\'s an official migration guide to help you migrate your task from Node.js 6 or Node.js 10 to Node.js 16. It\'s [available here](https://github.com/microsoft/azure-pipelines-tasks/blob/3ab93334eb3e5c1f3750403e3b6f976909ae45c3/docs/migrateNode16.md). It gave me a couple of pointers but I wanted to document the process in a bit more detail. Also, I wanted to show how you can start to get some benefits from being on Node.js 16 with TypeScript.\\n\\nThe version of the [`azure-pipelines-task-lib`](https://www.npmjs.com/package/azure-pipelines-task-lib) being used in the `package.json` should be incremented to `4.0.0` or higher. If you haven\'t already, it\'s worth updating the `@types/node` version to `16.0.0` or higher. This will give you access to the types of the Node 16 APIs.\\n\\nThe migration guide suggests updating the `task.json` to have a `Node16` property alongside the existing `Node10` one:\\n\\n```diff\\n\\"execution\\": {\\n  \\"Node10\\": {\\n    \\"target\\": \\"bash.js\\",\\n    \\"argumentFormat\\": \\"\\"\\n  },\\n+  \\"Node16\\": {\\n+    \\"target\\": \\"bash.js\\",\\n+    \\"argumentFormat\\": \\"\\"\\n+  }\\n}\\n```\\n\\nI\'m rather unclear as to the benefits of having a `Node10` and a `Node16` alongside each other; there\'s no useful reason to do so that I can come up with. I may be missing something.\\n\\nEither way, in my own case I wanted to take advantage of the Node 16 environment and so I removed the `Node10` property entirely. My `task.json` now looks like this:\\n\\n```json\\n  \\"execution\\": {\\n    \\"Node16\\": {\\n      \\"target\\": \\"index.js\\"\\n    }\\n  }\\n```\\n\\nThis was all I needed to do, to get to the point of having a Node 16 compatible task. But we want to go a little further.\\n\\n## Updating TypeScript to use Node 16\\n\\nNow we have Node 16, we can now start using some of the APIs available there if we\'d like, and we can stop transpiling to an older version of JavaScript. To do this we need to update our TypeScript configuration in our `tsconfig.json` file:\\n\\n```diff\\n-    \\"target\\": \\"es6\\" /* Specify ECMAScript target version: \'ES3\' (default), \'ES5\', \'ES2015\', \'ES2016\', \'ES2017\', \'ES2018\', \'ES2019\', \'ES2020\', or \'ESNEXT\'. */,\\n-    \\"lib\\": [] /* Specify library files to be included in the compilation. */,\\n+    \\"target\\": \\"es2021\\" /* Specify ECMAScript target version: \'ES3\' (default), \'ES5\', \'ES2015\', \'ES2016\', \'ES2017\', \'ES2018\', \'ES2019\', \'ES2020\', or \'ESNEXT\'. */,\\n+    \\"lib\\": [\\"ES2021\\"] /* Specify library files to be included in the compilation. */,\\n```\\n\\nHere we\'re just changing the emitted JavaScript to be more modern. We\'re also updating the `lib` property to include the `ES2021` library. This will give us access to the types of the Node 16 APIs.\\n\\n## How do we know we\'re using Node 16?\\n\\nGreat question! I was suspicious that the task was still running on Node 10. I wanted to know for sure. I ran a migrated task with system diagnostics enabled:\\n\\n![Screenshot of Azure Pipelines including the text \\"##[debug]Using node path: /home/vsts/agents/2.213.2/externals/node16/bin/node\\"](screenshot-azure-pipelines-node-16.png)\\n\\nAs we can see, we\'re using Node 16. This is great news!\\n\\n```bash\\n##[debug]Using node path: /home/vsts/agents/2.213.2/externals/node16/bin/node\\n```\\n\\n## Conclusion\\n\\nThat\'s it, we\'re now writing modern custom pipelines task extensions using Node.js 16 and TypeScript. [Microsoft have commented](https://learn.microsoft.com/en-us/azure/devops/release-notes/2022/sprint-210-update#node-16-task-runner-in-pipeline-agent) on the lack of alignment between Node task runners and the Node release cycle:\\n\\n> The original design of the Node task runner did not make Node version upgrades straightforward for task authors, and as a result has not kept up with the latest Node releases. We\'ve heard feedback from customers on this, and are now making a number of changes to enable Azure Pipelines agents to keep installed Node versions in sync with the Node release cadence and support lifecycle while minimizing impacts on task and pipeline authors.\\n\\nSo, by the sounds of it, the problem is being taken seriously and will be addressed."},{"id":"application-insights-bicep-azure-static-web-apps","metadata":{"permalink":"/application-insights-bicep-azure-static-web-apps","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-01-01-application-insights-bicep-azure-static-web-apps/index.md","source":"@site/blog/2023-01-01-application-insights-bicep-azure-static-web-apps/index.md","title":"Using Application Insights with Bicep to monitor Azure Static Web Apps and Azure Functions","description":"Application Insights are a great way to monitor Azure Static Web Apps and Azure Functions. But how do you deploy that using Bicep? Let\'s find out!","date":"2023-01-01T00:00:00.000Z","tags":[{"inline":false,"label":"Azure Static Web Apps","permalink":"/tags/azure-static-web-apps","description":"The Azure Static Web Apps service."},{"inline":false,"label":"Azure Functions","permalink":"/tags/azure-functions","description":"The Azure Functions service."},{"inline":false,"label":"Bicep","permalink":"/tags/bicep","description":"The Bicep language for Azure Resource Manager templates."},{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."}],"readingTime":5.445,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"application-insights-bicep-azure-static-web-apps","title":"Using Application Insights with Bicep to monitor Azure Static Web Apps and Azure Functions","authors":"johnnyreilly","tags":["azure static web apps","azure functions","bicep","azure"],"image":"./title-image.png","description":"Application Insights are a great way to monitor Azure Static Web Apps and Azure Functions. But how do you deploy that using Bicep? Let\'s find out!","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Azure Pipelines - Node.js 16 and custom pipelines task extensions","permalink":"/azure-pipelines-custom-pipelines-task-extension-node-16"},"nextItem":{"title":"Serving Docusaurus images with Cloudinary","permalink":"/docusaurus-image-cloudinary-rehype-plugin"}},"content":"Application Insights are a great way to monitor Azure Static Web Apps and Azure Functions. But how do you deploy that using Bicep? Let\'s find out!\\n\\n![title image reading \\"Using Application Insights with Bicep to monitor Azure Static Web Apps and Azure Functions\\" with the Bicep, Application Insights, Azure Static Web Apps and Azure Functions logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Updated 9 March 2023\\n\\nI\'ve updated this post to use the correct configuration approach for Static Web Apps with Azure Functions. Historically they used to be configured separately but that\'s no longer the case. You can see some discussion of this [on this GitHub issue](https://github.com/Azure/static-web-apps/issues/1089#issuecomment-1458710885).\\n\\nTo be super clear; the [`Microsoft.Web/staticSites/config@2022-03-01` `functionappsettings` is deprecated](https://learn.microsoft.com/en-us/azure/templates/microsoft.web/staticsites/config-functionappsettings?pivots=deployment-language-bicep). Don\'t use it. Use the `appsettings` resource alone instead.\\n\\n## Monitoring Azure Static Web Apps\\n\\nThis post should possibly win some kind of \\"least pithy blog title\\" award. But it\'s definitely descriptive. Let\'s get into it.\\n\\nI recently wrote [about using dynamic redirects in Azure Static Web Apps using the Azure Function they support](../2022-12-22-azure-static-web-apps-dynamic-redirects-azure-functions/index.md). I wanted to monitor the redirects that were being performed. I knew I could do this with Application Insights. But how do I deploy Application Insights using Bicep?\\n\\n[My blog](https://johnnyreilly.com) runs on Azure Static Web Apps which is deployed using Bicep. [I\'ve written about deploying Azure Static Web Apps with Bicep previously](../2023-02-01-migrating-from-github-pages-to-azure-static-web-apps/index.md). I wanted to add Application Insights to that deployment.\\n\\n## Deploying Application Insights with Bicep\\n\\nThe first thing we need to do is deploy the Application Insights workspace. This is a resource that is required for Application Insights to work. And then deploy an Application Insights resource that uses it. We can achieve that with the following `appInsights.bicep` Bicep module:\\n\\n```bicep\\nparam location string\\nparam tags object\\nparam workspaceName string = \'appInsightsWorkspace\'\\nparam appInsightsName string = \'appInsights\'\\n\\n// https://learn.microsoft.com/en-us/azure/templates/microsoft.operationalinsights/workspaces?pivots=deployment-language-bicep\\nresource workspace \'Microsoft.OperationalInsights/workspaces@2022-10-01\' = {\\n  name: workspaceName\\n  location: location\\n  tags: tags\\n  properties: {\\n    sku: {\\n      name: \'PerGB2018\'\\n    }\\n    retentionInDays: 30\\n    workspaceCapping: {}\\n  }\\n}\\n\\n// https://learn.microsoft.com/en-us/azure/templates/microsoft.insights/components?pivots=deployment-language-bicep\\nresource appInsights \'Microsoft.Insights/components@2020-02-02\' = {\\n  name: appInsightsName\\n  location: location\\n  kind: \'other\'\\n  properties: {\\n    Application_Type: \'web\'\\n    Flow_Type: \'Bluefield\'\\n    WorkspaceResourceId: workspace.id\\n    RetentionInDays: 90\\n    IngestionMode: \'LogAnalytics\'\\n    publicNetworkAccessForIngestion: \'Enabled\'\\n    publicNetworkAccessForQuery: \'Enabled\'\\n  }\\n}\\n\\noutput appInsightsId string = appInsights.id\\n```\\n\\n## Using the Application Insights module\\n\\nWe can now use the module in our `main.bicep` file:\\n\\n```bicep\\nparam location string\\nparam branch string\\nparam staticWebAppName string\\nparam tags object\\n@secure()\\nparam repositoryToken string\\nparam rootCustomDomainName string\\nparam blogCustomDomainName string\\nparam workspaceName string = \'blog-app-insights-workspace\'\\nparam appInsightsName string = \'blog-app-insights\'\\n\\nmodule appInsights \'./appInsights.bicep\' = {\\n  name: \'appInsights\'\\n  params: {\\n    location: location\\n    tags: tags\\n    workspaceName: workspaceName\\n    appInsightsName: appInsightsName\\n  }\\n}\\n\\nresource appInsightsResource \'Microsoft.Insights/components@2020-02-02\' existing = {\\n  name: appInsightsName\\n}\\n\\nmodule staticWebApp \'./staticWebApp.bicep\' = {\\n  name: \'staticWebApp\'\\n  params: {\\n    location: location\\n    branch: branch\\n    staticWebAppName: staticWebAppName\\n    tags: tags\\n    repositoryToken: repositoryToken\\n    rootCustomDomainName: rootCustomDomainName\\n    blogCustomDomainName: blogCustomDomainName\\n    appInsightsId: appInsights.outputs.appInsightsId\\n    appInsightsConnectionString: appInsightsResource.properties.ConnectionString\\n    appInsightsInstrumentationKey: appInsightsResource.properties.InstrumentationKey\\n  }\\n}\\n\\noutput staticWebAppDefaultHostName string = staticWebApp.outputs.staticWebAppDefaultHostName\\noutput staticWebAppId string = staticWebApp.outputs.staticWebAppId\\noutput staticWebAppName string = staticWebApp.outputs.staticWebAppName\\n```\\n\\nThere\'s a few things to note here:\\n\\n- We have two modules. One for the Application Insights workspace and one for the Azure Static Web App.\\n- The Static Web App module _depends_ on the outputs from the Application Insights module. This is because we need the `id`, `InstrumentationKey` and `ConnectionString` properties of the Application Insights resource.\\n\\n## Configuring the Azure Static Web App to use Application Insights\\n\\nAt this point we have something that deploys the Application Insights. The interesting part now is how we configure the Azure Static Web App to use Application Insights. We need to do that in the `staticWebApp.bicep` file:\\n\\n```bicep\\nparam location string\\nparam branch string\\nparam staticWebAppName string\\nparam tags object\\n@secure()\\nparam repositoryToken string\\nparam rootCustomDomainName string\\nparam blogCustomDomainName string\\nparam appInsightsId string\\n@secure()\\nparam appInsightsInstrumentationKey string\\n@secure()\\nparam appInsightsConnectionString string\\n\\nvar tagsWithHiddenLinks = union({\\n  \'hidden-link: /app-insights-resource-id\': appInsightsId\\n  \'hidden-link: /app-insights-instrumentation-key\': appInsightsInstrumentationKey\\n  \'hidden-link: /app-insights-conn-string\': appInsightsConnectionString\\n}, tags)\\n\\nresource staticWebApp \'Microsoft.Web/staticSites@2022-03-01\' = {\\n  name: staticWebAppName\\n  location: location\\n  tags: tagsWithHiddenLinks\\n  sku: {\\n    name: \'Free\'\\n    tier: \'Free\'\\n  }\\n  properties: {\\n    repositoryUrl: \'https://github.com/johnnyreilly/blog.johnnyreilly.com\'\\n    repositoryToken: repositoryToken\\n    branch: branch\\n    provider: \'GitHub\'\\n    stagingEnvironmentPolicy: \'Enabled\'\\n    allowConfigFileUpdates: true\\n    buildProperties:{\\n      skipGithubActionWorkflowGeneration: true\\n    }\\n  }\\n}\\n\\nresource staticWebAppAppSettings \'Microsoft.Web/staticSites/config@2022-03-01\' = {\\n  name: \'appsettings\'\\n  kind: \'string\'\\n  parent: staticWebApp\\n  properties: {\\n    APPINSIGHTS_INSTRUMENTATIONKEY: appInsightsInstrumentationKey\\n    APPLICATIONINSIGHTS_CONNECTION_STRING: appInsightsConnectionString\\n  }\\n}\\n\\nresource rootCustomDomain \'Microsoft.Web/staticSites/customDomains@2022-03-01\' = {\\n  parent: staticWebApp\\n  name: rootCustomDomainName\\n  properties: {}\\n}\\n\\nresource blogCustomDomain \'Microsoft.Web/staticSites/customDomains@2022-03-01\' = {\\n  parent: staticWebApp\\n  name: blogCustomDomainName\\n  properties: {}\\n}\\n\\noutput staticWebAppDefaultHostName string = staticWebApp.properties.defaultHostname\\noutput staticWebAppId string = staticWebApp.id\\noutput staticWebAppName string = staticWebApp.name\\n```\\n\\nThere\'s some code here you can ignore; the part related to custom domains for instance.\\n\\nBut there\'s two relevant things to note:\\n\\n1. Configuring the Azure Static Web App and Azure Function to use Application Insights\\n2. Connecting the Azure Static Web App to the Application Insights resource in the Azure Portal\\n\\n### 1. Configuring the Azure Static Web App and Azure Function to use Application Insights\\n\\nFirst of all, let\'s look at how we get data flowing from the Azure Static Web App and Azure Function to Application Insights:\\n\\n```bicep\\nresource staticWebAppAppSettings \'Microsoft.Web/staticSites/config@2022-03-01\' = {\\n  name: \'appsettings\'\\n  kind: \'string\'\\n  parent: staticWebApp\\n  properties: {\\n    APPINSIGHTS_INSTRUMENTATIONKEY: appInsightsInstrumentationKey\\n    APPLICATIONINSIGHTS_CONNECTION_STRING: appInsightsConnectionString\\n  }\\n}\\n```\\n\\nWe\'re setting the `APPINSIGHTS_INSTRUMENTATIONKEY` and `APPLICATIONINSIGHTS_CONNECTION_STRING` application settings on the Azure Static Web App and its associated Azure Function. These settings are what tells the Azure Static Web App and Azure Function to use Application Insights. Please note that the configuration above is _shared_ by the Azure Static Web App and Azure Function.\\n\\n### 2. Connecting the Azure Static Web App to the Application Insights resource in the Azure Portal\\n\\nThe other thing we need to do is to connect the Azure Static Web App to the Application Insights resource in the Azure Portal. What that means is that when you click on the Application Insights resource in the Azure Portal, you\'ll have a button which takes you from the Azure Static Web App in the portal to Application Insights resource:\\n\\n![screenshot of the Azure Portal Static Web App connected to the Application Insights resource](screenshot-azure-portal-application-insights-hidden-link.webp)\\n\\nThis is done by setting the `hidden-link` tags on the Azure Static Web App resource. Here\'s how we do that:\\n\\n```bicep\\nvar tagsWithHiddenLinks = union({\\n  \'hidden-link: /app-insights-resource-id\': appInsightsId\\n  \'hidden-link: /app-insights-instrumentation-key\': appInsightsInstrumentationKey\\n  \'hidden-link: /app-insights-conn-string\': appInsightsConnectionString\\n}, tags)\\n\\nresource staticWebApp \'Microsoft.Web/staticSites@2022-03-01\' = {\\n  name: staticWebAppName\\n  location: location\\n  tags: tagsWithHiddenLinks\\n  // ...\\n}\\n```\\n\\n## Conclusion\\n\\nWith this in place, we can now deploy our Azure Static Web App with an Application Insights resource using Bicep and have the Azure Static Web App connected to, and providing data to, the Application Insights resource. Monitoring awaits!\\n\\n![Screenshot of Application Insights in the Azure Portal](screenshot-application-insights.png)"},{"id":"docusaurus-image-cloudinary-rehype-plugin","metadata":{"permalink":"/docusaurus-image-cloudinary-rehype-plugin","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-12-26-docusaurus-image-cloudinary-rehype-plugin/index.md","source":"@site/blog/2022-12-26-docusaurus-image-cloudinary-rehype-plugin/index.md","title":"Serving Docusaurus images with Cloudinary","description":"Cloudinary offers an image CDN which can improve performance of your site. This post details how to get Docusaurus to use Cloudinary to serve optimised images.","date":"2022-12-26T00:00:00.000Z","tags":[{"inline":false,"label":"Docusaurus","permalink":"/tags/docusaurus","description":"The Docusaurus static site generator."}],"readingTime":9.61,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"docusaurus-image-cloudinary-rehype-plugin","title":"Serving Docusaurus images with Cloudinary","authors":"johnnyreilly","tags":["docusaurus"],"image":"./title-image.png","description":"Cloudinary offers an image CDN which can improve performance of your site. This post details how to get Docusaurus to use Cloudinary to serve optimised images.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Using Application Insights with Bicep to monitor Azure Static Web Apps and Azure Functions","permalink":"/application-insights-bicep-azure-static-web-apps"},"nextItem":{"title":"Azure Static Web Apps: dynamic redirects with Azure Functions","permalink":"/azure-static-web-apps-dynamic-redirects-azure-functions"}},"content":"Cloudinary offers an image CDN which can improve performance of your site. This post details how to get Docusaurus v2 to use Cloudinary to serve optimised images. There is a follow on post that details [migrating this plugin to Docusaurus v3](../2023-10-09-docusaurus-3-how-to-migrate-rehype-plugins/index.md).\\n\\n![title image reading \\"Serving Docusaurus images with Cloudinary\\" with the Docusaurus and Cloudinary logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Updated 13th January 2023 - `f_auto` / `q_auto` support\\n\\nI received a note from the marvellous [Rebeccca Peltz](https://github.com/rebeccapeltz) of Cloudinary, alerting me to the fact that Cloudinary supports using `f_auto` and `q_auto` for images fetched from URLs. To quote her:\\n\\n> `f_auto` causes Cloudinary to look at User Agent information in the request header and provides the best image format for the browser or device making the request. `q_auto` provides compression that makes the image smaller without creating pixelation.\\n>\\n> ...\\n>\\n> Here\u2019s what one of your URLs would look like with fetch and f_auto,q_auto\\n>\\n> `https://res.cloudinary.com/priou/image/fetch/f_auto,q_auto/https://johnnyreilly.com/assets/images/screenshot-image-from-cloudinary-cb313fdeb91761d777ed1732f7c054c9.webp`\\n\\nThis sounded nothing but advantageous and so it\'s now the default behaviour of the plugin, as of v1.2.0. [See the pull request here](https://github.com/johnnyreilly/rehype-cloudinary-docusaurus/pull/5). Thanks Rebecca!\\n\\n## What is Cloudinary?\\n\\nTo quote [Cloudinary\'s website](https://cloudinary.com/blog/delivering_all_your_websites_images_through_a_cdn):\\n\\n> Most leading blogs deliver their assets (images, JS, CSS, etc.) through state-of-the-art CDNs and utilize online resizing technologies. With faster, off-site access, they greatly improve their users\u2019 browsing experience, while reducing load on their servers.\\n>\\n> Using Cloudinary you can use these same technologies today, in your website or blog, without any hassle.\\n\\nConsumption of the CDN is very simple. You simply prefix the URL of the image you want to serve with the URL of the Cloudinary CDN. For example, if you want to serve the following image:\\n\\n`https://johnnyreilly.com/img/profile-64x64.jpg`\\n\\nyou can serve it from Cloudinary with the following URL:\\n\\n`https://res.cloudinary.com/demo/image/fetch/https://johnnyreilly.com/img/profile-64x64.jpg`.\\n\\nYou see? All we did was prefix `https://res.cloudinary.com/demo/image/fetch/` to the URL of the image we wanted to serve. That\'s it. When you visit the URL, you\'ll see the image served from Cloudinary. Behind the scenes, Cloudinary will fetch the image from the original source and serve it to you.\\n\\n:::note\\n\\nThe `demo` part of the URL is the name of the Cloudinary account. You can create your own account and use that instead.\\n\\n:::\\n\\n## Cloudinary account settings\\n\\nOnce you have created your account, you\'ll need to tweak the settings. There\'s two tweaks, one mandatory and one that\'s optional.\\n\\n### Disable restricted media types: Fetched URL\\n\\nFirst the mandatory one. We need to uncheck the `Disable restricted media types: Fetched URL` setting. The double negative shenanigans make this confusing; to read it another way we are \\"allowing fetching URLs\\". Much clearer! We need to do this is because we\'re fetching the image from a URL. If we didn\'t make the change, Cloudinary would refuse to serve the image. It wouldn\'t even try to fetch it.\\n\\n![screenshot of Cloudinary settings with the Disable restricted media types: Fetched URL unchecked](screenshot-cloudinary-restricted.webp)\\n\\n:::caution\\n\\nRemember to scroll down and hit the \\"Save\\" button. (Otherwise your changes won\'t be saved.)\\n\\n:::\\n\\n### Allowed fetch domains\\n\\nThe second setting is optional. If you want to restrict the domains from which you can fetch images, you can do so. You might want to do this if you want to prevent others from making use of your Cloudinary account and blowing your limits. I\'m not sure how likely that is, but it\'s a possibility.\\n\\n![screenshot of Cloudinary settings with the allowed fetch domains restricted to blog.johnnyreilly.com](screenshot-cloudinary-allowed-fetch-domains.webp)\\n\\nAbove I\'m restricting my account to only fetch images from my own site; `blog.johnnyreilly.com`. To my mind, it\'s the Cloudinary content security policy for fetching images.\\n\\n## Docusaurus Cloudinary rehype image plugin\\n\\nNow we have our Cloudinary account set up, we can use it with Docusaurus. To do so, we need to create a rehype plugin. This is a plugin for the [rehype](https://github.com/rehypejs/rehype/) HTML processor. It\'s a plugin that will transform the HTML image syntax into a Cloudinary URL.\\n\\nThe plugin takes the form of a JavaScript file we\'ll call `docusaurus-cloudinary-rehype-plugin.js`:\\n\\n```js\\n//@ts-check\\nconst visit = require(\'unist-util-visit\');\\n\\n/**\\n * Create a rehype plugin that will replace image URLs with Cloudinary URLs\\n * @param {*} options cloudName your Cloudinary\u2019s cloud name eg demo, baseUrl the base URL of your website eg https://johnnyreilly.com - should not include a trailing slash, will likely be the same as the config.url in your docusaurus.config.js\\n * @returns rehype plugin that will replace image URLs with Cloudinary URLs\\n */\\nfunction imageCloudinaryRehypePluginFactory(\\n  /** @type {{ cloudName: string; baseUrl: string }} */ options,\\n) {\\n  const { cloudName, baseUrl } = options;\\n  const srcRegex = / src={(.*)}/;\\n\\n  /** @type {import(\'unified\').Plugin<[], import(\'hast\').Root>} */\\n  return (tree) => {\\n    visit(tree, [\'element\', \'jsx\'], (node) => {\\n      if (node.type === \'element\' && node[\'tagName\'] === \'img\') {\\n        // handles nodes like this:\\n\\n        // {\\n        //   type: \'element\',\\n        //   tagName: \'img\',\\n        //   properties: {\\n        //     src: \'https://some.website.com/cat.gif\',\\n        //     alt: null\\n        //   },\\n        //   ...\\n        // }\\n\\n        const url = node[\'properties\'].src;\\n\\n        node[\\n          \'properties\'\\n        ].src = `https://res.cloudinary.com/${cloudName}/image/fetch/${url}`;\\n      } else if (node.type === \'jsx\' && node[\'value\']?.includes(\'<img \')) {\\n        // handles nodes like this:\\n\\n        // {\\n        //   type: \'jsx\',\\n        //   value: \'<img src={require(\\"!/workspaces/blog.johnnyreilly.com/blog-website/node_modules/url-loader/dist/cjs.js?limit=10000&name=assets/images/[name]-[hash].[ext]&fallback=/workspaces/blog.johnnyreilly.com/blog-website/node_modules/file-loader/dist/cjs.js!./bower-with-the-long-paths.png\\").default} width=\\"640\\" height=\\"497\\" />\'\\n        // }\\n\\n        const match = node[\'value\'].match(srcRegex);\\n        if (match) {\\n          const urlOrRequire = match[1];\\n          node[\'value\'] = node[\'value\'].replace(\\n            srcRegex,\\n            ` src={${`\\\\`https://res.cloudinary.com/${cloudName}/image/fetch/${baseUrl}\\\\$\\\\{${urlOrRequire}\\\\}\\\\``}}`,\\n          );\\n        }\\n      }\\n    });\\n  };\\n}\\n\\nmodule.exports = imageCloudinaryRehypePluginFactory;\\n```\\n\\nThis plugin is a factory function that takes two parameters: the name of your Cloudinary account and the base URL of your website. It returns a rehype plugin that will transform the HTML image syntax into a Cloudinary URL.\\n\\nIf you look at the code, you\'ll see that it handles two different types of image syntax; an `img` tag and a JSX image tag. The `img` tag is a very simple transform; it just prefixes the `src` attribute with `https://res.cloudinary.com/${cloudName}/image/fetch/` where `${cloudName}` is the name of your Cloudinary account; eg `demo`.\\n\\nThe JSX image tag is a little more complex. It\'s a little more complex because we have a complete JSX node which contains an `img` element. The `src` attribute is a JavaScript expression. It\'s not a string. It\'s a JavaScript expression that will be evaluated at runtime through some webpack goodness.\\n\\nThis means that we need to do a little more work to transform it into a Cloudinary URL. We need to wrap the expression in backticks and prefix it with `https://res.cloudinary.com/${cloudName}/image/fetch/${baseUrl}` where `${baseUrl}` is the base URL of your website. We also need to prefix the expression with a `$` to indicate that it\'s a JavaScript expression. Tough to read but it works.\\n\\n## Using the plugin\\n\\nNow we have our plugin, we can use it. We need to add it to our `docusaurus.config.js` file. We do this by adding it to the `rehypePlugins` array:\\n\\n```js\\n//@ts-check\\nconst docusaurusCloudinaryRehypePlugin = require(\'./docusaurus-cloudinary-rehype-plugin\');\\n\\nconst url = \'https://johnnyreilly.com\';\\n\\n/** @type {import(\'@docusaurus/types\').Config} */\\nconst config = {\\n  // ...\\n  presets: [\\n    [\\n      \'@docusaurus/preset-classic\',\\n      /** @type {import(\'@docusaurus/preset-classic\').Options} */\\n      ({\\n        // ...\\n        blog: {\\n          // ...\\n          rehypePlugins: [\\n            [\\n              docusaurusCloudinaryRehypePlugin,\\n              {\\n                cloudName: \'demo\',\\n                baseUrl: url,\\n              },\\n            ],\\n          ],\\n          // ...\\n        },\\n        // ...\\n      }),\\n    ],\\n  ],\\n  // ...\\n};\\n\\nmodule.exports = config;\\n```\\n\\nNote that we pass in the name of our Cloudinary account and the base URL of our website. We can now run our website and see the images being transformed into Cloudinary URLs:\\n\\n![Screenshot of image being served from the Cloudinary CDN](screenshot-image-from-cloudinary.webp)\\n\\nExcellent! We\'re now serving our images from the Cloudinary CDN.\\n\\n## Introducing `rehype-cloudinary-docusaurus`\\n\\nBut who wants to make a rehype plugin? I don\'t. I want to use a rehype plugin. So I created one. It\'s called [`rehype-cloudinary-docusaurus`](https://github.com/johnnyreilly/rehype-cloudinary-docusaurus) and you can find it on npm. It\'s a drop-in replacement for the plugin we created above. You can add it like this (use whichever package manager CLI tool you prefer):\\n\\n```bash\\nnpm i rehype-cloudinary-docusaurus\\n```\\n\\nAnd then usage is:\\n\\n```js\\n//@ts-check\\nconst docusaurusCloudinaryRehypePlugin = require(\'rehype-cloudinary-docusaurus\');\\n\\nconst url = \'https://johnnyreilly.com\';\\n\\n/** @type {import(\'@docusaurus/types\').Config} */\\nconst config = {\\n  // ...\\n  presets: [\\n    [\\n      \'@docusaurus/preset-classic\',\\n      /** @type {import(\'@docusaurus/preset-classic\').Options} */\\n      ({\\n        // ...\\n        blog: {\\n          // ...\\n          rehypePlugins: [\\n            [\\n              docusaurusCloudinaryRehypePlugin,\\n              {\\n                cloudName: \'demo\',\\n                baseUrl: url,\\n              },\\n            ],\\n          ],\\n          // ...\\n        },\\n        // ...\\n      }),\\n    ],\\n  ],\\n  // ...\\n};\\n\\nmodule.exports = config;\\n```\\n\\nYou will also need to disable the `url-loader` in your Docusaurus build which transforms images into base64 strings, as this will conflict with the plugin. [There isn\'t a first class way to do this in Docusaurus at present](https://github.com/facebook/docusaurus/pull/5498). However by setting the environment variable `WEBPACK_URL_LOADER_LIMIT` to `0` you can disable it. [You can see an implementation example in this pull request](https://github.com/johnnyreilly/blog.johnnyreilly.com/pull/397). It amounts to adding the [`cross-env`](https://github.com/kentcdodds/cross-env) package and then adding the following to your `package.json`:\\n\\n```json\\n    \\"build\\": \\"cross-env WEBPACK_URL_LOADER_LIMIT=0 docusaurus build\\",\\n```\\n\\n## What about pull request previews?\\n\\nWe\'ve done all the hard stuff, now let\'s do some finessing. We want to make sure that our pull request previews still work. My blog runs on Azure Static Web Apps and benefits from a [staging environments / pull request previews feature that lets you see a change before it is merged](../2022-02-08-azure-static-web-apps-a-netlify-alternative/index.md). It\'s useful not only for human intrigue, but for running [tools like Lighthouse against your site to catch issues](../2022-03-20-lighthouse-meet-github-actions/index.md).\\n\\nWe don\'t want to be serving images from the Cloudinary CDN when we\'re running a pull request preview. We could make it work, but it doesn\'t seem worth the candle. We can just serve the images from our website.\\n\\nHowever, to support that we need to have a mechanism to detect when we\'re running a pull request preview. We can do that by setting an environment variable in our Azure Static Web Apps configuration:\\n\\n```yml\\n- name: Install and build site \uD83D\uDD27\\n  run: |\\n    cd blog-website\\n    yarn install --frozen-lockfile\\n    USE_CLOUDINARY=${{ github.event_name != \'pull_request\' }} yarn run build\\n```\\n\\nThe above code sets an environment variable called `USE_CLOUDINARY` to `false` if the GitHub Action is running for a pull request, and `true` if not. [You\'ll note that I\'m building my website externally to the Azure Static Web Apps build process](../2022-12-18-azure-static-web-apps-build-app-externally/index.md). If I was building my website as part of the Azure Static Web Apps build process, I\'d use the [custom `app_build_command` feature](https://learn.microsoft.com/en-us/azure/static-web-apps/build-configuration?tabs=github-actions#custom-build-commands) to set the environment variable.\\n\\nWith our environment variable in place, we can conditionally add the plugin to our `rehypePlugins` array:\\n\\n```js\\n//@ts-check\\nconst docusaurusCloudinaryRehypePlugin = require(\'rehype-cloudinary-docusaurus\');\\n\\nconst USE_CLOUDINARY = process.env[\'USE_CLOUDINARY\'] === \'true\';\\n\\nconst url = \'https://johnnyreilly.com\';\\n\\n/** @type {import(\'@docusaurus/types\').Config} */\\nconst config = {\\n  // ...\\n  presets: [\\n    [\\n      \'@docusaurus/preset-classic\',\\n      /** @type {import(\'@docusaurus/preset-classic\').Options} */\\n      ({\\n        // ...\\n        blog: {\\n          // ...\\n          rehypePlugins: USE_CLOUDINARY\\n            ? [\\n                [\\n                  docusaurusCloudinaryRehypePlugin,\\n                  {\\n                    cloudName: \'demo\',\\n                    baseUrl: url,\\n                  },\\n                ],\\n              ]\\n            : [],\\n          // ...\\n        },\\n        // ...\\n      }),\\n    ],\\n  ],\\n\\n  // ...\\n};\\n\\nmodule.exports = config;\\n```\\n\\nWith that in place, images will be served from the Cloudinary CDN when we\'re running our website normally, but will be served from our website when we\'re running a pull request preview.\\n\\n## Core Web Vitals and preconnect\\n\\nFinally, it\'s worth adding an entry to the `headTags` of your `docusaurus.config.js` to ensure that your site preconnects to Cloudinary\'s CDN. This speeds up the time until images will be served to your users. That addition looks like this:\\n\\n```js\\n/** @type {import(\'@docusaurus/types\').Config} */\\nconst config = {\\n  // ...\\n  headTags: [\\n    // ...\\n\\n    // <link rel=\\"preconnect\\" href=\\"https://res.cloudinary.com\\" />\\n    {\\n      tagName: \'link\',\\n      attributes: {\\n        rel: \'preconnect\',\\n        href: \'https://res.cloudinary.com\',\\n      },\\n    },\\n\\n    // ...\\n  ],\\n  // ...\\n};\\n```\\n\\n## Conclusion\\n\\nWe\'ve seen how we can use a rehype plugin to transform HTML image syntax into Cloudinary URLs. We\'ve also seen how we can use an environment variable to conditionally add the plugin to our Docusaurus configuration."},{"id":"azure-static-web-apps-dynamic-redirects-azure-functions","metadata":{"permalink":"/azure-static-web-apps-dynamic-redirects-azure-functions","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-12-22-azure-static-web-apps-dynamic-redirects-azure-functions/index.md","source":"@site/blog/2022-12-22-azure-static-web-apps-dynamic-redirects-azure-functions/index.md","title":"Azure Static Web Apps: dynamic redirects with Azure Functions","description":"Azure Static Web Apps can perform URL redirects using the `routes` section in the `staticwebapp.config.json`. However it is limited. This post will demonstrate dynamic URL redirects with Azure Functions.","date":"2022-12-22T00:00:00.000Z","tags":[{"inline":false,"label":"Azure Static Web Apps","permalink":"/tags/azure-static-web-apps","description":"The Azure Static Web Apps service."},{"inline":false,"label":"Azure Functions","permalink":"/tags/azure-functions","description":"The Azure Functions service."},{"inline":false,"label":"GitHub Actions","permalink":"/tags/github-actions","description":"The GitHub Actions CI / CD service."}],"readingTime":4.78,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-static-web-apps-dynamic-redirects-azure-functions","title":"Azure Static Web Apps: dynamic redirects with Azure Functions","authors":"johnnyreilly","tags":["azure static web apps","azure functions","github actions"],"image":"./title-image.png","description":"Azure Static Web Apps can perform URL redirects using the `routes` section in the `staticwebapp.config.json`. However it is limited. This post will demonstrate dynamic URL redirects with Azure Functions.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Serving Docusaurus images with Cloudinary","permalink":"/docusaurus-image-cloudinary-rehype-plugin"},"nextItem":{"title":"Azure Static Web Apps: build app externally","permalink":"/azure-static-web-apps-build-app-externally"}},"content":"Azure Static Web Apps can perform URL redirects using the `routes` section in the `staticwebapp.config.json`. However it is limited. This post will demonstrate dynamic URL redirects with Azure Functions.\\n\\n![title image reading \\"Azure Static Web Apps: dynamic redirects with Azure Functions\\" with the Static Web Apps and Azure Functions logo](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## The limits of `routes` in `staticwebapp.config.json`\\n\\nI recently found myself fixing up some redirects for my blog, which runs on Azure Static Web Apps. I had quite a few redirects to implement and I ended up with [a very large `routes` section](https://learn.microsoft.com/en-us/azure/static-web-apps/configuration#routes). It was so large that I exceeded [the 20kb limit that affect Azure Static Web Apps](https://learn.microsoft.com/en-us/azure/static-web-apps/configuration#restrictions).\\n\\nI bemoaned this on Twitter and got some great advice from [Anthony Chu who works on Azure Static Web Apps](https://twitter.com/nthonyChu):\\n\\n[![Tweet that reads: Your best bet today might be to use a function that handles 404 response overrides. You can do the lookup with it and return a redirect if found. There might be a small cold start on those routes but for this case maybe it\u2019s okay.](screenshot-tweet-azure-function-redirect.webp)](https://twitter.com/nthonyChu/status/1605248878009208832)\\n\\nAnthony went on to [share details of an example implementation that Nuxt.js has implemented](https://twitter.com/nthonyChu/status/1605429770715402240). I took this as a challenge to implement something similar for my blog. Let\'s see how we got on.\\n\\n## Adding an Azure Function to our Azure Static Web App\\n\\nThe first thing we need to do is add an Azure Function to our Azure Static Web App. All Static Web Apps can be backed by an Azure Function App. [We can create a simple JavaScript HttpTrigger Azure Function following this guide](https://learn.microsoft.com/en-us/azure/static-web-apps/add-api?tabs=react#create-the-api). I used JavScript as it seemed like the simplest option. If we wanted a different language we could use one.\\n\\nWe\'re going to name the single function `fallback`, so it will be served up at `/api/fallback`. The code of the function is:\\n\\n```js\\n//@ts-check\\nconst { parseURL } = require(\'ufo\');\\nconst routes = require(\'./redirects\');\\n\\n/**\\n *\\n * @param { import(\\"@azure/functions\\").Context } context\\n * @param { import(\\"@azure/functions\\").HttpRequest } req\\n */\\nasync function fallback(context, req) {\\n  const originalUrl = req.headers[\'x-ms-original-url\'];\\n  if (originalUrl) {\\n    // This URL has been proxied as there was no static file matching it.\\n    context.log(`x-ms-original-url: ${originalUrl}`);\\n\\n    const parsedURL = parseURL(originalUrl);\\n\\n    const matchedRoute = routes.find((route) =>\\n      parsedURL.pathname.includes(route.route),\\n    );\\n\\n    if (matchedRoute) {\\n      context.log(`Redirecting ${originalUrl} to ${matchedRoute.redirect}`);\\n\\n      context.res = {\\n        status: matchedRoute.statusCode,\\n        headers: { location: matchedRoute.redirect },\\n      };\\n      return;\\n    }\\n  }\\n\\n  context.log(\\n    `No explicit redirect for ${originalUrl} so will redirect to 404`,\\n  );\\n\\n  context.res = {\\n    status: 302,\\n    headers: {\\n      location: originalUrl\\n        ? `/404?originalUrl=${encodeURIComponent(originalUrl)}`\\n        : \'/404\',\\n    },\\n  };\\n}\\n\\nmodule.exports = fallback;\\n```\\n\\nWhat\'s happening here? Well, we\'re using the `ufo` package to parse the URL which we grab from the `x-ms-original-url` header. We then look for a match in our `redirects.js`, which is a _big_ list of potential redirects.\\n\\nIf we find a match, we redirect based upon that match. Otherwise we redirect to the custom 404 screen in our app. And we include the original URL in the query string for visibility. (With this in place, any unhandled redirects should show up in Google Analytics etc.)\\n\\n## JSDoc types with `@azure/functions`\\n\\nYou\'ll notice that we\'re using JSDoc types in the above code and enabling type checking through use of [`// @ts-check`](https://www.typescriptlang.org/docs/handbook/intro-to-js-ts.html#ts-check). We\'re providing types to our function through the [`@azure/functions`](https://www.npmjs.com/package/@azure/functions) package which we\'ve added as a `devDependency`. You don\'t have to do this, but it\'s a nice way to get some type safety.\\n\\n## Consuming the Azure Function from our Azure Static Web App\\n\\nNow our Azure Function is in place, we need to configure our Azure Static Web App to use it. In our `staticwebapp.config.json` we\'ll make some changes:\\n\\n```json\\n  \\"navigationFallback\\": {\\n    \\"rewrite\\": \\"/api/fallback\\"\\n  },\\n  \\"platform\\": {\\n    \\"apiRuntime\\": \\"node:18\\"\\n  },\\n  \\"routes\\": [\\n    {\\n      \\"route\\": \\"/404\\",\\n      \\"statusCode\\": 404\\n    },\\n```\\n\\nHere we:\\n\\n- Point to our apps navigation fallback to our `fallback` function (`/api/fallback`) - this will be called whenever a URL is not matched by a static file.\\n- We declare an `apiRuntime` of Node.js 18 - this is the version of Node.js that our Azure Function is using.\\n- Whenever the `/404` route is hit in our app, we ensure the status code presented is 404.\\n- We remove the `route` redirects we had in place as these will now be handled by `/api/fallback` (this isn\'t shown in the above snippet)\\n\\n## Deploying our Azure Function\\n\\nWe need to deploy our Function, and we achieve that by tweaking the our GitHub Action that deploys our Static Web App. We add the following:\\n\\n```yml\\napi_location: \'/blog-website/api\'\\n```\\n\\nAnd now our Azure Function will be built and deployed alongside our blog.\\n\\n## Testing our Azure Function\\n\\nWe can demonstrate this works pretty easily. Let\'s take a super old blog post of mine, where I upgraded to TypeScript 0.9.5 (!!!) The route has changed since I originally posted back in 2014. If we go to https://johnnyreilly.com/2014/01/upgrading-to-typescript-095-personal.html (the old Blogger URL), we\'ll be redirected (301\'d to be specific - signalling a permanent move) to https://johnnyreilly.com/upgrading-to-typescript-095-personal - the new URL. This is demonstrated in the following screenshot - note the `location` header in the response:\\n\\n![screenshot of redirect in Chrome Devtools](screenshot-redirect-in-chrome-devtools.webp)\\n\\nThis particular redirect is driven by [an entry in our `redirects.js`](https://github.com/johnnyreilly/blog.johnnyreilly.com/blob/e21d3faf897505e860fc351260ab45ef6fa21d60/blog-website/api/fallback/redirects.js#L475-L479) file:\\n\\n```json\\n  {\\n    route: \'/2014/01/upgrading-to-typescript-095-personal.html\',\\n    redirect: \'/upgrading-to-typescript-095-personal\',\\n    statusCode: 301,\\n  },\\n```\\n\\n## Conclusion\\n\\nI\'d love it if there was a way to do this without an Azure Function. Imagine a `staticwebapp.config.js` that could be used to configure redirects. That would be awesome. But for now, this is a pretty good solution. Thanks to Anthony Chu for the inspiration and the example. (And thanks to the Nuxt.js team for the example too!)\\n\\n[If you\'d like to see what it looked like when this landed in this very blog, then look at this pull request](https://github.com/johnnyreilly/blog.johnnyreilly.com/pull/384)."},{"id":"azure-static-web-apps-build-app-externally","metadata":{"permalink":"/azure-static-web-apps-build-app-externally","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-12-18-azure-static-web-apps-build-app-externally/index.md","source":"@site/blog/2022-12-18-azure-static-web-apps-build-app-externally/index.md","title":"Azure Static Web Apps: build app externally","description":"Azure Static Web Apps can generally build themselves with Oryx. If you need finer grained control of your build, you can with `skip_app_build: true`.","date":"2022-12-18T00:00:00.000Z","tags":[{"inline":false,"label":"Azure Static Web Apps","permalink":"/tags/azure-static-web-apps","description":"The Azure Static Web Apps service."},{"inline":false,"label":"GitHub Actions","permalink":"/tags/github-actions","description":"The GitHub Actions CI / CD service."}],"readingTime":3.145,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-static-web-apps-build-app-externally","title":"Azure Static Web Apps: build app externally","authors":"johnnyreilly","tags":["azure static web apps","github actions"],"image":"./title-image.png","description":"Azure Static Web Apps can generally build themselves with Oryx. If you need finer grained control of your build, you can with `skip_app_build: true`.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Azure Static Web Apps: dynamic redirects with Azure Functions","permalink":"/azure-static-web-apps-dynamic-redirects-azure-functions"},"nextItem":{"title":"Publishing Docusaurus to dev.to with the dev.to API","permalink":"/publishing-docusaurus-to-devto-with-devto-api"}},"content":"Azure Static Web Apps can generally build themselves with Oryx. If you need finer grained control of your build, you can with `skip_app_build: true` and some GitHub Actions.\\n\\n![title image reading \\"Azure Static Web Apps: build app externally\\" with the Static Web Apps logo](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Build with Oryx\\n\\nI love Azure Static Web Apps. [My blog](https://johnnyreilly.com) is built with them. I\'ve written about them many times.\\n\\nOne of the things I like about Azure Static Web Apps is that they can build themselves. You can just push your code to GitHub and they\'ll build it using a tool called [Oryx](https://github.com/microsoft/Oryx). This is great for simple scenarios. Actually, it\'s good for medium to complex scenarios too. However, if you ever get to that \\"break glass\\" moment where you need to do something unusual with your build, you can.\\n\\nLet\'s start by looking at a simple Azure Static Web Apps configuration:\\n\\n```yaml\\n- name: Static Web App - get API key for deployment\\n  id: static_web_app_apikey\\n  uses: azure/CLI@v2\\n  with:\\n    inlineScript: |\\n      APIKEY=$(az staticwebapp secrets list --name \'${{ env.STATICWEBAPPNAME }}\' | jq -r \'.properties.apiKey\')\\n      echo \\"APIKEY=$APIKEY\\" >> $GITHUB_OUTPUT\\n\\n- name: Static Web App - build and deploy\\n  id: static_web_app_build_and_deploy\\n  uses: Azure/static-web-apps-deploy@v1\\n  with:\\n    azure_static_web_apps_api_token: ${{ steps.static_web_app_apikey.outputs.APIKEY }}\\n    repo_token: ${{ secrets.GITHUB_TOKEN }} # Used for Github integrations (i.e. PR comments)\\n    action: \'upload\'\\n    app_location: \'/blog-website\' # App source code path\\n    output_location: \'build\' # Built app content directory - optional\\n\\n    # For more information regarding Static Web App workflow configurations, please visit: https://aka.ms/swaworkflowconfig\\n    api_location: \'\' # Api source code path - optional\\n```\\n\\nAbove is an old version of what my blog used to build and deploy itself. With the yaml above, Oryx built the app and deployed it. [I wanted to add the last modified date to my blog posts.](../2022-11-25-adding-lastmod-to-sitemap-git-commit-date/index.md) It would have been fiddly to do this in Oryx.\\n\\n## Build externally\\n\\nSo, I decided to build the app externally and then deploy it. I did this by tweaking the yaml above to add some extra steps:\\n\\n```yaml\\n- name: Get API key \uD83D\uDD11\\n  id: static_web_app_apikey\\n  uses: azure/CLI@v2\\n  with:\\n    inlineScript: |\\n      APIKEY=$(az staticwebapp secrets list --name \'${{ env.STATICWEBAPPNAME }}\' | jq -r \'.properties.apiKey\')\\n      echo \\"APIKEY=$APIKEY\\" >> $GITHUB_OUTPUT\\n\\n- name: Setup Node.js \uD83D\uDD27\\n  uses: actions/setup-node@v3\\n  with:\\n    node-version: \'18\'\\n    cache: \'yarn\'\\n\\n- name: Install and build site \uD83D\uDD27\\n  run: |\\n    cd blog-website\\n    yarn install --frozen-lockfile\\n    yarn run build\\n    # copy staticwebapp.config.json to build folder\\n    cp staticwebapp.config.json build/staticwebapp.config.json\\n\\n- name: Deploy site \uD83D\uDE80\\n  id: static_web_app_build_and_deploy\\n  uses: Azure/static-web-apps-deploy@v1\\n  with:\\n    azure_static_web_apps_api_token: ${{ steps.static_web_app_apikey.outputs.APIKEY }}\\n    repo_token: ${{ secrets.GITHUB_TOKEN }} # Used for Github integrations (i.e. PR comments)\\n    action: \'upload\'\\n    skip_app_build: true\\n    app_location: \'/blog-website/build\' # App source code path\\n    # output_location: \'build\' # Built app content directory - optional\\n\\n    # For more information regarding Static Web App workflow configurations, please visit: https://aka.ms/swaworkflowconfig\\n    api_location: \'\' # Api source code path - optional\\n```\\n\\nWhat\'s changed? Well, I\'ve added a few steps:\\n\\n- Setup Node.js - essentially, this is just installing Node.js so we can build the app\\n- Install and build site - this is where we actually do install the dependencies and build the app\\n- Significantly (and [thanks to Vivek Jilla for this tip](https://github.com/Azure/static-web-apps/issues/1017#issuecomment-1356786140)), we copy the `staticwebapp.config.json` file to the build folder. This is important because it contains the routing information for the app. Without it, any rules you have in your `staticwebapp.config.json` file won\'t be applied.\\n- We set `skip_app_build: true` - this tells Azure Static Web Apps to skip the build step and point it at the `build` folder instead, where the built app (with `staticwebapp.config.json`) can be found.\\n\\nWith this in place I\'m now able to build the app externally and deploy it to Azure Static Web Apps. This is great for when you need to do something a little more complex than Oryx can handle."},{"id":"publishing-docusaurus-to-devto-with-devto-api","metadata":{"permalink":"/publishing-docusaurus-to-devto-with-devto-api","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-12-11-publishing-docusaurus-to-devto-with-devto-api/index.md","source":"@site/blog/2022-12-11-publishing-docusaurus-to-devto-with-devto-api/index.md","title":"Publishing Docusaurus to dev.to with the dev.to API","description":"The dev.to API provides a way to cross post your Docusaurus blogs to dev.to. This post describes how to do that with TypeScript, Node.js and the dev.to API.","date":"2022-12-11T00:00:00.000Z","tags":[{"inline":false,"label":"Docusaurus","permalink":"/tags/docusaurus","description":"The Docusaurus static site generator."},{"inline":false,"label":"GitHub Actions","permalink":"/tags/github-actions","description":"The GitHub Actions CI / CD service."}],"readingTime":9.18,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"publishing-docusaurus-to-devto-with-devto-api","title":"Publishing Docusaurus to dev.to with the dev.to API","authors":"johnnyreilly","tags":["docusaurus","github actions"],"image":"./title-image.png","description":"The dev.to API provides a way to cross post your Docusaurus blogs to dev.to. This post describes how to do that with TypeScript, Node.js and the dev.to API.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Azure Static Web Apps: build app externally","permalink":"/azure-static-web-apps-build-app-externally"},"nextItem":{"title":"Deep linking with Azure Static Web Apps and Easy Auth","permalink":"/azure-static-web-apps-easyauth-deeplink"}},"content":"The dev.to API provides a way to cross post your Docusaurus blogs to dev.to. This post describes how to do that with TypeScript, Node.js and the dev.to API.\\n\\n![title image reading \\"Deep linking with Azure Static Web Apps and Easy Auth\\" with Azure AD and Static Web App logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Why not use \\"Publishing to DEV Community \uD83D\uDC69\u200D\uD83D\uDCBB\uD83D\uDC68\u200D\uD83D\uDCBB from RSS\\"?\\n\\nIf you take a look at the [dev.to settings (under extensions)](https://dev.to/settings/extensions) you\'ll see that you can post to dev.to using an RSS feed:\\n\\n![Screenshot of the \\"Publishing to DEV Community \uD83D\uDC69\u200D\uD83D\uDCBB\uD83D\uDC68\u200D\uD83D\uDCBB from RSS\\" section of dev.to](screenshot-devto-publishing-rss.png)\\n\\nThis is great, but it has a number of downsides:\\n\\n- every post published to your blog will be published to dev.to - there\'s no fine grained control\\n- every post published arrives as \\"draft\\" - you have to manually push it \\"live\\".\\n- _most significantly_ - it handles code snippets poorly. Everything ends up as a single line of text. This is a real shame because code snippets are a key part of a blog post.\\n\\nSo after initially setting this up, I decided to look for a better way.\\n\\n## The dev.to API\\n\\n[It turns out that dev.to have an API.](https://developers.forem.com/api). The API is pretty well documented and it\'s pretty easy to use. The docs mention version 0 and version 1 of the API. Version 0 is officially deprecated, but version 1 appears to be incomplete - certainly the docs are. I ended up using version 0 for this post despite attempting to use version 1; I\'ll update this post when v1 gets there.\\n\\nThe only thing you need to do to use the API is [generate an API key](https://dev.to/settings/extensions):\\n\\n![Screenshot of the \\"DEV Community \uD83D\uDC69\u200D\uD83D\uDCBB\uD83D\uDC68\u200D\uD83D\uDCBB API Keys\\" screen](screenshot-devto-apikey.png)\\n\\n## TypeScript console app\\n\\nI\'m going to use a TypeScript console app to do the work. Let\'s scaffold up an example project alongside our Docusaurus site:\\n\\n```bash\\nmkdir from-docusaurus-to-devto\\ncd from-docusaurus-to-devto\\nnpx typescript --init\\nyarn init\\nyarn add @types/node ts-node typescript @docusaurus/utils\\n```\\n\\nAnd in the `package.json` file add a `start` script:\\n\\n```json\\n{\\n  \\"scripts\\": {\\n    \\"start\\": \\"ts-node index.ts\\"\\n  }\\n}\\n```\\n\\nFinally, create an empty `index.ts` file. We\'ll fill this in shortly.\\n\\n## TypeScript dev.to API client\\n\\nBefore we do that, we\'re going to need a dev.to API client. Let\'s create a `devtoApiClient.ts` file and add the following:\\n\\n```ts\\nexport interface User {\\n  name: string;\\n  username: string;\\n  twitter_username: string;\\n  github_username: string;\\n  user_id: number;\\n  website_url: string;\\n  profile_image: string;\\n  profile_image_90: string;\\n}\\n\\nexport interface ArticleToPublish {\\n  title: string;\\n  body_markdown: string;\\n  published: boolean;\\n  main_image: string | undefined;\\n  canonical_url: string;\\n  description?: string;\\n  tags: string[];\\n}\\n\\nexport interface Article {\\n  type_of: string;\\n  id: number;\\n  title: string;\\n  description: string;\\n  published: boolean;\\n  published_at: string;\\n  slug: string;\\n  path: string;\\n  url: string;\\n  comments_count: number;\\n  public_reactions_count: number;\\n  page_views_count: number;\\n  positive_reactions_count: number;\\n  cover_image: string | null;\\n  canonical_url: string;\\n  published_timestamp: string;\\n  tag_list: string[];\\n  user: User;\\n  body_markdown: string;\\n  body_html: string;\\n  reading_time_minutes: number;\\n}\\n\\nexport interface DevToApiClient {\\n  getArticles: () => Promise<Article[]>;\\n  createArticle: (article: ArticleToPublish) => Promise<void>;\\n  updateArticle: (id: number, article: ArticleToPublish) => Promise<void>;\\n}\\n\\nexport function devtoApiClientFactory(apiKey: string): DevToApiClient {\\n  const baseUrl = \'https://dev.to/api\';\\n\\n  return {\\n    getArticles: async () => {\\n      try {\\n        const articles: Article[] = [];\\n        let page = 1;\\n        const pageSize = 100;\\n        while (true) {\\n          const url = `${baseUrl}/articles/me/published?page=${page}&page_size=${pageSize}`;\\n          const res = await fetch(url, {\\n            headers: {\\n              \'api-key\': apiKey,\\n              accept: \'application/vnd.forem.api-v1+json\',\\n            },\\n          });\\n          if (!res.ok) {\\n            console.error(res);\\n            throw new Error(`Failed to get articles ${url}`);\\n          }\\n          const data = (await res.json()) as Article[];\\n          if (data.length === 0) break;\\n\\n          page += 1;\\n          articles.push(...data);\\n        }\\n        return articles;\\n      } catch (e) {\\n        console.error(\'Failed to get articles\', e);\\n        throw new Error(\'Failed to get articles\');\\n      }\\n    },\\n\\n    createArticle: async (article: ArticleToPublish) => {\\n      try {\\n        const url = `${baseUrl}/articles`;\\n        const res = await fetch(url, {\\n          headers: {\\n            \'api-key\': apiKey,\\n            \'Content-Type\': \'application/json\',\\n          },\\n          method: \'POST\',\\n          body: JSON.stringify({\\n            article,\\n          }),\\n        });\\n        if (!res.ok) {\\n          console.error(res);\\n          console.error(await res.json());\\n          throw new Error(`Failed to create article ${article.canonical_url}`);\\n        }\\n        const data = (await res.json()) as Article;\\n        const { body_html, body_markdown, ...rest } = data;\\n        console.log(`Created article ${article.canonical_url}`, rest);\\n      } catch (e) {\\n        console.error(\'Failed to create article\', e);\\n        throw new Error(\'Failed to create article\');\\n      }\\n    },\\n\\n    updateArticle: async (id: number, article: ArticleToPublish) => {\\n      try {\\n        const url = `${baseUrl}/articles/${id}`;\\n        const res = await fetch(url, {\\n          headers: {\\n            \'api-key\': apiKey,\\n            \'Content-Type\': \'application/json\',\\n          },\\n          method: \'PUT\',\\n          body: JSON.stringify({\\n            article,\\n          }),\\n        });\\n        if (!res.ok) {\\n          console.error(res);\\n          console.error(await res.json());\\n          throw new Error(`Failed to update article ${article.canonical_url}`);\\n        }\\n        const data = (await res.json()) as Article;\\n        const { body_html, body_markdown, ...rest } = data;\\n        console.log(`Updated article ${article.canonical_url}`, rest);\\n      } catch (e) {\\n        console.error(\'Failed to update article\', e);\\n        throw new Error(\'Failed to update article\');\\n      }\\n    },\\n  };\\n}\\n```\\n\\nThis is a simple API client that uses the [Fetch API](https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API) to make requests to the dev.to API. It\'s not a complete implementation of the API, but we only need a few article related endpoints to do the following:\\n\\n- Get all the articles that have been published to dev.to\\n- Create a new article\\n- Update an existing article\\n\\n## From blog post markdown to published blog posts\\n\\nNow we can use the API client in our `index.ts` file:\\n\\n```ts\\nimport fs from \'fs\';\\nimport path from \'path\';\\nimport { parseFrontMatter } from \'@docusaurus/utils\';\\nimport {\\n  Article,\\n  DevToApiClient,\\n  devtoApiClientFactory,\\n} from \'./devtoApiClient\';\\n\\nconst rootUrl = \'https://johnnyreilly.com\';\\nconst rootGitHubUrl =\\n  \'https://raw.githubusercontent.com/johnnyreilly/blog.johnnyreilly.com/main/blog-website/blog/\';\\nconst docusaurusBlogDirectory = \'../blog-website/blog\';\\n\\nconst markdownImageRexEx = /!\\\\[.*\\\\]\\\\((.*)\\\\)/g;\\nconst markdownRelativeBlogUrlRegex = /\\\\[.*\\\\]\\\\(\\\\.\\\\.\\\\/(.*)\\\\/index.md\\\\)/g;\\n\\nasync function getLastXBlogPostsToPublish({\\n  numberOfPosts,\\n}: {\\n  numberOfPosts: number;\\n}) {\\n  const blogPosts = await fs.promises.readdir(docusaurusBlogDirectory, {\\n    withFileTypes: true,\\n  });\\n  const blogPostDirectoryNames = blogPosts\\n    .slice(0)\\n    .reverse()\\n    .filter((post) => post.isDirectory())\\n    .map((post) => post.name)\\n    .slice(0, numberOfPosts);\\n\\n  return blogPostDirectoryNames;\\n}\\n\\nasync function publishBlogPostToDevTo({\\n  blogFilePathRelative,\\n  articlesByCanonicalUrl,\\n  devtoApiClient,\\n}: {\\n  blogFilePathRelative: string;\\n  articlesByCanonicalUrl: Map<string, Article>;\\n  devtoApiClient: DevToApiClient;\\n}) {\\n  const blogFilePath = path.join(\\n    docusaurusBlogDirectory,\\n    blogFilePathRelative,\\n    \'index.md\',\\n  );\\n  console.log(`Processing ${blogFilePath}`);\\n\\n  const blogFileContent = await fs.promises.readFile(blogFilePath, \'utf8\');\\n  const { frontMatter, content } = parseFrontMatter(blogFileContent);\\n\\n  const canonicalUrl = makeCanonicalUrl(\\n    blogFilePathRelative,\\n    frontMatter[\'slug\'] as string | undefined,\\n  );\\n  const contentWithCanonicalUrls = enrichMarkdownWithCanonicalUrls(content);\\n  const contentWithGitHubImages = enrichMarkdownWithImagesFromGitHub(\\n    contentWithCanonicalUrls,\\n    blogFilePathRelative,\\n  );\\n  const tags = frontMatter[\'tags\'] as string[];\\n  const title = frontMatter[\'title\'] as string;\\n  const published = true;\\n  const main_image = makeMainImage(frontMatter, blogFilePathRelative);\\n  const trimmedTags = tags.slice(0, 4).map((tag) => tag.replace(/\\\\W/g, \'\'));\\n\\n  const body_markdown = `---\\ntitle: ${title}\\npublished: ${published}\\ntags: ${trimmedTags.join(\',\')}\\ncanonical_url: ${canonicalUrl}\\n---\\n${contentWithGitHubImages}`;\\n\\n  const article = {\\n    title,\\n    body_markdown,\\n    published,\\n    main_image,\\n    canonical_url: canonicalUrl,\\n    tags: trimmedTags,\\n  };\\n\\n  console.log(`\\\\n---------------------------------------------------\\\\n\\\\n`);\\n  const existingArticle = articlesByCanonicalUrl.get(canonicalUrl);\\n\\n  if (existingArticle) {\\n    console.log(`Updating article ${canonicalUrl}`);\\n    await devtoApiClient.updateArticle(existingArticle.id, article);\\n  } else {\\n    console.log(`Creating article ${canonicalUrl}`);\\n    await devtoApiClient.createArticle(article);\\n  }\\n}\\n\\nfunction makeMainImage(\\n  frontMatter: { [key: string]: unknown },\\n  blogFilePathRelative: string,\\n) {\\n  const image =\\n    typeof frontMatter[\'image\'] === \'string\'\\n      ? (frontMatter[\'image\'] as string)\\n      : \'\';\\n  const main_image = image\\n    ? rootGitHubUrl +\\n      blogFilePathRelative +\\n      \'/\' +\\n      image.substring(image.indexOf(\'/\') + 1)\\n    : undefined;\\n  return main_image;\\n}\\n\\nfunction makeCanonicalUrl(\\n  blogFilePathRelative: string,\\n  frontMatterSlug?: string,\\n) {\\n  const parsedBlogFileName = `${rootUrl}/${blogFilePathRelative\\n    .substring(0, 10)\\n    .split(\'-\')\\n    .join(\'/\')}/${blogFilePathRelative.substring(11)}`;\\n\\n  const canonicalUrl = frontMatterSlug\\n    ? `${rootUrl}/${frontMatterSlug}`\\n    : parsedBlogFileName;\\n  return canonicalUrl;\\n}\\n\\nfunction enrichMarkdownWithImagesFromGitHub(\\n  content: string,\\n  blogFilePathRelative: string,\\n) {\\n  return Array.from(content.matchAll(markdownImageRexEx))\\n    .map((matches) => {\\n      const [completeMatch, url] = matches;\\n      const withGitHubUrl = completeMatch.replace(\\n        url,\\n        rootGitHubUrl + blogFilePathRelative + \'/\' + url,\\n      );\\n      console.log(`Replacing ${completeMatch} with ${withGitHubUrl}`);\\n      return { oldImage: completeMatch, newImage: withGitHubUrl };\\n    })\\n    .reduce(\\n      (contentInProgress, { oldImage, newImage }) =>\\n        contentInProgress.replace(oldImage, newImage),\\n      content,\\n    );\\n}\\n\\nfunction enrichMarkdownWithCanonicalUrls(content: string) {\\n  return Array.from(content.matchAll(markdownRelativeBlogUrlRegex))\\n    .map((matches) => {\\n      const [\\n        /* eg [I wanted to add the last modified date to my blog posts.](../2022-11-25-adding-lastmod-to-sitemap-git-commit-date/index.md) */\\n        completeMatch,\\n        /* eg 2022-11-25-adding-lastmod-to-sitemap-git-commit-date */\\n        relativeBlogPath,\\n      ] = matches;\\n\\n      const withCanonicalUrl = completeMatch.replace(\\n        `../${relativeBlogPath}/index.md`,\\n        makeCanonicalUrl(relativeBlogPath),\\n      );\\n      console.log(`Replacing ${completeMatch} with ${withCanonicalUrl}`);\\n      return { oldImage: completeMatch, newImage: withCanonicalUrl };\\n    })\\n    .reduce(\\n      (contentInProgress, { oldImage, newImage }) =>\\n        contentInProgress.replace(oldImage, newImage),\\n      content,\\n    );\\n}\\n\\nfunction makeDevtoApiClient() {\\n  const devToApiKey = process.env.DEVTO_APIKEY;\\n\\n  if (!devToApiKey) {\\n    console.log(\'No dev.to API key specified!\');\\n    process.exit(1);\\n  }\\n\\n  return devtoApiClientFactory(devToApiKey);\\n}\\n\\nconst sleep = async ({ seconds }: { seconds: number }) =>\\n  new Promise((resolve) => setTimeout(resolve, seconds * 1000));\\n\\nasync function run() {\\n  const devtoApiClient = makeDevtoApiClient();\\n  const articles = await devtoApiClient.getArticles();\\n  const articlesByCanonicalUrl = new Map<string, Article>(\\n    Array.from(articles).map((article) => [article.canonical_url, article]),\\n  );\\n  const blogPostsToPublish = await getLastXBlogPostsToPublish({\\n    numberOfPosts: 5,\\n  });\\n\\n  for (const blogFilePathRelative of blogPostsToPublish) {\\n    await publishBlogPostToDevTo({\\n      blogFilePathRelative,\\n      articlesByCanonicalUrl,\\n      devtoApiClient,\\n    });\\n\\n    console.log(\'Sleeping for 5 seconds because rate limiting...\');\\n    await sleep({ seconds: 5 });\\n  }\\n}\\n\\n// do it!\\nrun();\\n```\\n\\nThere\'s a lot happening here, let me summarise it:\\n\\n- Grab the last 5 blog posts from the Docusaurus blog directory; this is the number of posts I want to publish to dev.to on each run\\n- For each blog post, parse the front matter and the content\\n- Build up the article object to send to dev.to. We do a few tricks here to make the article look nice:\\n  - To make the URL we\'ll use the `slug` front matter if it exists, otherwise use the date and title\\n  - Enrich the images in the content with the GitHub URLs so we can use images from the blog post\\n  - Use the first 4 tags from the front matter - dev.to only allows 4 tags. Also we\'ll strip those tags of any non-word characters\\n  - Default to `published` immediately\\n- If the article already exists on dev.to, update it, otherwise create it\\n\\nBecause dev.to practise rate limiting on their API, I\'ve added a 5 second sleep between each article to ensure we don\'t get blocked. It\'s a little arbitrary, but it works well enough.\\n\\nDoes it work? Let\'s find out!\\n\\n![Screenshot of dev.to dashboard showing published posts](screenshot-devto-published-posts.png)\\n\\nIt works! I\'ve published 5 posts to dev.to from my blog. [I can now go to dev.to and see them.](https://dev.to/johnnyreilly)\\n\\n## Running the script from GitHub Actions\\n\\nNow that we have the script, we need to run it. I\'m going to use GitHub Actions to do this, but you could use any CI/CD tool you like.\\n\\nI add a new `deploy_to_devto_job` to my existing workflow and I set it to run on every push to the `main` branch. I don\'t want to publish to dev.to on every pull request; I want to publish once a blog post is published. So I add an `if` condition to the job to check that the event is not a pull request.\\n\\n```yaml\\ndeploy_to_devto_job:\\n  name: Publish to dev.to \uD83D\uDDDE\uFE0F\\n  needs: build_and_deploy_swa_job\\n  if: github.event_name != \'pull_request\'\\n  runs-on: ubuntu-latest\\n  steps:\\n    - uses: actions/checkout@v3\\n\\n    - name: Setup Node.js \uD83D\uDD27\\n      uses: actions/setup-node@v3\\n      with:\\n        node-version: \'18\'\\n        cache: \'yarn\'\\n\\n    - name: Publish to dev.to \uD83D\uDDDE\uFE0F\\n      run: |\\n        cd from-docusaurus-to-devto\\n        yarn install --frozen-lockfile\\n        DEVTO_APIKEY=${{ secrets.DEVTO_APIKEY }} yarn start\\n```\\n\\nIf you\'d like to use this you\'ll need to add a `DEVTO_APIKEY` secret to your repository secrets. You can get this from your dev.to account settings. Remember to keep it secret!\\n\\n## Conclusion\\n\\nThis is all a bit of an experiment to see what happens if I start to cross publish my blog posts to dev.to. I\'m not sure if I\'ll keep doing it, but I\'m going to trial it and see how it goes.\\n\\nYou can use this approach with your own blog site - you\'ll need to do a little path and URL fiddling, but everything else should be just as you need."},{"id":"azure-static-web-apps-easyauth-deeplink","metadata":{"permalink":"/azure-static-web-apps-easyauth-deeplink","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-12-04-azure-static-web-apps-easyauth-deeplink/index.md","source":"@site/blog/2022-12-04-azure-static-web-apps-easyauth-deeplink/index.md","title":"Deep linking with Azure Static Web Apps and Easy Auth","description":"Azure Static Web Apps does not support deep linking with authentication. This post describes how to work around this limitation.","date":"2022-12-04T00:00:00.000Z","tags":[{"inline":false,"label":"Azure Static Web Apps","permalink":"/tags/azure-static-web-apps","description":"The Azure Static Web Apps service."},{"inline":false,"label":"Authentication and Authorization","permalink":"/tags/auth","description":"Matters related to identity and permissioning"},{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."}],"readingTime":6.27,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-static-web-apps-easyauth-deeplink","title":"Deep linking with Azure Static Web Apps and Easy Auth","authors":"johnnyreilly","tags":["azure static web apps","auth","azure"],"image":"./title-image.png","description":"Azure Static Web Apps does not support deep linking with authentication. This post describes how to work around this limitation.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Publishing Docusaurus to dev.to with the dev.to API","permalink":"/publishing-docusaurus-to-devto-with-devto-api"},"nextItem":{"title":"Docusaurus: Using fontaine to reduce custom font cumulative layout shift","permalink":"/docusaurus-using-fontaine-to-reduce-custom-font-cumulative-layout-shift"}},"content":"Azure Static Web Apps doesn\'t support deep linking with authentication. The [post login redirect](https://learn.microsoft.com/en-us/azure/static-web-apps/authentication-authorization?tabs=invitations#post-login-redirect) parameter of `post_login_redirect_uri` does not support query string parameters. This post describes how to work around this limitation.\\n\\n![title image reading \\"Deep linking with Azure Static Web Apps and Easy Auth\\" with Azure AD and Static Web App logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Updated 1st March 2023\\n\\nI\'m happy to say that this blog post is no longer necessary; [the behavour is now built into Azure Static Web Apps](https://github.com/Azure/static-web-apps/issues/435#issuecomment-1353985870). Here is an example `staticwebapp.config.json` which supports deep linking using the [`.referrer`](https://learn.microsoft.com/en-us/azure/static-web-apps/authentication-authorization?tabs=invitations#set-up-post-sign-in-redirect) post sign-in redirect:\\n\\n```json\\n{\\n  \\"auth\\": {\\n    \\"identityProviders\\": {\\n      \\"azureActiveDirectory\\": {\\n        // ...\\n      }\\n    }\\n  },\\n  \\"navigationFallback\\": {\\n    \\"rewrite\\": \\"index.html\\"\\n  },\\n  \\"routes\\": [\\n    {\\n      \\"route\\": \\"/login\\",\\n      \\"rewrite\\": \\"/.auth/login/aad\\",\\n      \\"allowedRoles\\": [\\"anonymous\\", \\"authenticated\\"]\\n    },\\n    {\\n      \\"route\\": \\"/.auth/login/github\\",\\n      \\"statusCode\\": 404\\n    },\\n    {\\n      \\"route\\": \\"/.auth/login/twitter\\",\\n      \\"statusCode\\": 404\\n    },\\n    {\\n      \\"route\\": \\"/logout\\",\\n      \\"redirect\\": \\"/.auth/logout\\",\\n      \\"allowedRoles\\": [\\"anonymous\\", \\"authenticated\\"]\\n    },\\n    {\\n      \\"route\\": \\"/*\\",\\n      \\"allowedRoles\\": [\\"authenticated\\"]\\n    }\\n  ],\\n  \\"responseOverrides\\": {\\n    \\"401\\": {\\n      \\"redirect\\": \\"/.auth/login/aad?post_login_redirect_uri=.referrer\\",\\n      \\"statusCode\\": 302\\n    }\\n  }\\n  // ...\\n}\\n```\\n\\n## Deep linking\\n\\nImagine the situation: your colleague sends you `https://our-app.com/pages/important-page?someId=theId`. You click the link and you\'re presented with a login screen. You login and you\'re presented with a page, but not the one your colleague meant you to see. What do you do now? If you realise what\'s happened, you\'ll likely paste the URL into the address bar again so you end up where you hope to. But what if you don\'t realise what\'s happened? Answer: confusion and frustration.\\n\\nIf you\'re using Azure Static Web Apps, you\'re likely to have this problem. [Azure Static Web Apps doesn\'t support deep linking with authentication](https://github.com/Azure/static-web-apps/issues/435). When you get redirected you\'ll find you are (at best) missing the query parameters. If you take a look at the link here you\'ll see a suggested workaround. We\'re going to develop that idea in this post.\\n\\n## The workaround\\n\\nThe idea of the workaround is this:\\n\\n- at the start of the authentication process, store the URL you\'re trying to get to in local storage\\n- when the authentication process completes, redirect to the URL you stored in local storage\\n\\nThe post suggested a React specific approach. We\'d like something that is framework agnostic. So if you\'re running with Svelte, Vue, Angular or something else, you can use this approach too.\\n\\n## The implementation\\n\\nWe\'re going to need to make sure our [`staticwebapp.config.json`](https://learn.microsoft.com/en-us/azure/static-web-apps/configuration) is set up to support our goal:\\n\\n```json\\n{\\n  \\"auth\\": {\\n    \\"identityProviders\\": {\\n      \\"azureActiveDirectory\\": {\\n        \\"registration\\": {\\n          \\"openIdIssuer\\": \\"https://login.microsoftonline.com/AAD_TENANT_ID/v2.0\\",\\n          \\"clientIdSettingName\\": \\"AAD_CLIENT_ID\\",\\n          \\"clientSecretSettingName\\": \\"AAD_CLIENT_SECRET\\"\\n        }\\n      }\\n    }\\n  },\\n  \\"navigationFallback\\": {\\n    \\"rewrite\\": \\"index.html\\"\\n  },\\n  \\"routes\\": [\\n    {\\n      \\"route\\": \\"/login\\",\\n      \\"rewrite\\": \\"/.auth/login/aad\\",\\n      \\"allowedRoles\\": [\\"anonymous\\", \\"authenticated\\"]\\n    },\\n    {\\n      \\"route\\": \\"/.auth/login/github\\",\\n      \\"statusCode\\": 404\\n    },\\n    {\\n      \\"route\\": \\"/.auth/login/twitter\\",\\n      \\"statusCode\\": 404\\n    },\\n    {\\n      \\"route\\": \\"/logout\\",\\n      \\"redirect\\": \\"/.auth/logout\\",\\n      \\"allowedRoles\\": [\\"anonymous\\", \\"authenticated\\"]\\n    },\\n    {\\n      \\"route\\": \\"/*.json\\",\\n      \\"allowedRoles\\": [\\"authenticated\\"]\\n    }\\n  ],\\n  \\"responseOverrides\\": {\\n    \\"401\\": {\\n      \\"redirect\\": \\"/login\\",\\n      \\"statusCode\\": 302\\n    }\\n  },\\n  \\"globalHeaders\\": {\\n    \\"content-security-policy\\": \\"default-src https: \'unsafe-eval\' \'unsafe-inline\'; object-src \'none\'\\"\\n  },\\n  \\"mimeTypes\\": {\\n    \\".json\\": \\"text/json\\",\\n    \\".md\\": \\"text/markdown\\",\\n    \\".xml\\": \\"application/xml\\"\\n  }\\n}\\n```\\n\\nThere\'s a number of things to note here:\\n\\n- we\'re using Azure Active Directory as our identity provider (and disabling others) - the approach in this post will work with any identity provider; this is just the one I\'m using. Easy Auth supports [a number of identity providers](https://learn.microsoft.com/en-us/azure/app-service/overview-authentication-authorization#identity-providers)\\n- we\'re creating a `/login` route to redirect to the Azure AD login page - you don\'t have to do this, but it\'s a nice touch.\\n- we\'re protecting the `*.json` files with authentication - this is because our JSON files actually contain secure information. If we were using say an API instead, we\'d protect that with authentication instead. Crucially, access to HTML / JS / CSS is _not_ protected. This is important, because we need to be able to access our `index.html` file and associated JavaScript to store the URL we\'re trying to get to in local storage.\\n\\nWith this in place, we can implement our workaround. Let\'s create a file called `deeplink.ts`:\\n\\n```ts\\nconst deeplinkPathAndQueryKey = \'deeplink:pathAndQuery\';\\n\\n/**\\n * If authenticated, redirect to the path and query string stored in local storage.\\n * If not authenticated, store the current path and query string in local storage and redirect to the login page.\\n *\\n * @param loginUrl The URL to redirect to if the user is not authenticated\\n */\\nexport async function deeplink(loginUrl: string) {\\n  if (!loginUrl) {\\n    throw new Error(\'loginUrl is required\');\\n  }\\n\\n  const pathAndQuery = location.pathname + location.search;\\n  console.log(`deeplink: URL before: ${pathAndQuery}`);\\n\\n  const deeplinkPathAndQuery = localStorage.getItem(deeplinkPathAndQueryKey);\\n\\n  const isAuth = await isAuthenticated();\\n\\n  if (isAuth) {\\n    if (deeplinkPathAndQuery && pathAndQuery === \'/\') {\\n      console.log(`deeplink: Redirecting to ${deeplinkPathAndQuery}`);\\n      localStorage.removeItem(deeplinkPathAndQueryKey);\\n      history.replaceState(null, \'\', deeplinkPathAndQuery);\\n    }\\n  } else if (!deeplinkPathAndQuery) {\\n    if (pathAndQuery !== \'/\' && pathAndQuery !== loginUrl) {\\n      console.log(\\n        `deeplink: Storing redirect URL of ${pathAndQuery} and redirecting to ${loginUrl}`,\\n      );\\n      localStorage.setItem(deeplinkPathAndQueryKey, pathAndQuery);\\n      location.href = loginUrl;\\n    } else {\\n      console.log(`deeplink: Redirecting to ${loginUrl}`);\\n      location.href = loginUrl;\\n    }\\n  }\\n}\\n\\nasync function isAuthenticated() {\\n  try {\\n    const response = await fetch(\'/.auth/me\');\\n    const authMe = (await response.json()) as AuthMe;\\n    const isAuth = authMe.clientPrincipal !== null;\\n    return isAuth;\\n  } catch (error) {\\n    console.error(\'Failed to fetch /.auth/me\', error);\\n    return false;\\n  }\\n}\\n\\ninterface AuthMe {\\n  clientPrincipal: null | {\\n    claims: {\\n      typ: string;\\n      val: string;\\n    }[];\\n    identityProvider: string;\\n    userDetails: string;\\n    userId: string;\\n    userRoles: string[];\\n  };\\n}\\n```\\n\\nThe code above implements our workaround. It does the following:\\n\\n- it checks whether a user is authenticated by hitting the `/.auth/me` endpoint that is provided by the Easy Auth / Static Web Apps authentication system\\n- if users are not authenticated, it:\\n  - stores the path and query string in localStorage and\\n  - redirects them to the login page\\n- when they return post-authentication it retrieves the path and query string from localStorage and sets the URL to that\\n\\nWhat does usage look like? Well let\'s take the root of a simple React app:\\n\\n```tsx\\nimport { StrictMode } from \'react\';\\nimport { BrowserRouter } from \'react-router-dom\';\\nimport { createRoot } from \'react-dom/client\';\\nimport App from \'./App\';\\nimport { deeplink } from \'easyauth-deeplink\';\\n\\nfunction main() {\\n  const container = document.getElementById(\'root\');\\n  if (container) {\\n    const root = createRoot(container);\\n    root.render(\\n      <StrictMode>\\n        <BrowserRouter>\\n          <App />\\n        </BrowserRouter>\\n      </StrictMode>,\\n    );\\n  }\\n}\\n\\ndeeplink(\'/login\').then(main);\\n// or\\ndeeplink(\'/.auth/login/aad\').then(main);\\n// or\\ndeeplink(\'/.auth/login/github\').then(main);\\n// or\\ndeeplink(\'/.auth/login/twitter\').then(main);\\n// or\\ndeeplink(\'/.auth/login/google\').then(main);\\n// etc\\n```\\n\\nYou can see here that the first thing we do is call `deeplink` with the URL of the login page (you can see I\'ve provided a number of options). This will redirect the user to the login page if they\'re not authenticated, and will redirect them to the URL they were trying to access if they are authenticated. Once that\'s done, we render our app.\\n\\nYou should be able to apply this regardless of your framework. The important thing is that you call `deeplink` before you render your app.\\n\\n## Announcing `easyauth-deeplink`\\n\\nI\'ve created a package called [`easyauth-deeplink`](https://github.com/johnnyreilly/easyauth-deeplink) that implements the workaround above. You can install it with `npm install easyauth-deeplink` or `yarn add easyauth-deeplink`. It\'s a single file, so you can just copy and paste it into your project if you prefer.\\n\\n## Conclusion\\n\\nIt would be tremendous if this became a feature that was built into Azure Static Web Apps. Maybe one day it will be. In the meantime, I hope this workaround helps you.\\n\\nIt should be said that whilst we\'ve described usage in this post with Static Web Apps, the same approach should work with any Azure Service that has Easy Auth enabled; App Service / Function Apps etc. I\'ve not tried it, but I\'d be surprised if it didn\'t work."},{"id":"docusaurus-using-fontaine-to-reduce-custom-font-cumulative-layout-shift","metadata":{"permalink":"/docusaurus-using-fontaine-to-reduce-custom-font-cumulative-layout-shift","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-12-01-docusaurus-using-fontaine-to-reduce-custom-font-cumulative-layout-shift/index.md","source":"@site/blog/2022-12-01-docusaurus-using-fontaine-to-reduce-custom-font-cumulative-layout-shift/index.md","title":"Docusaurus: Using fontaine to reduce custom font cumulative layout shift","description":"Custom font usage can introduce cumulative layout shift (or \\"jank\\") to your website. This post shows how to use fontaine to reduce this with Docusaurus sites.","date":"2022-12-01T00:00:00.000Z","tags":[{"inline":false,"label":"Docusaurus","permalink":"/tags/docusaurus","description":"The Docusaurus static site generator."}],"readingTime":5.055,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"docusaurus-using-fontaine-to-reduce-custom-font-cumulative-layout-shift","title":"Docusaurus: Using fontaine to reduce custom font cumulative layout shift","authors":"johnnyreilly","tags":["docusaurus"],"image":"./title-image.png","description":"Custom font usage can introduce cumulative layout shift (or \\"jank\\") to your website. This post shows how to use fontaine to reduce this with Docusaurus sites.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Deep linking with Azure Static Web Apps and Easy Auth","permalink":"/azure-static-web-apps-easyauth-deeplink"},"nextItem":{"title":"Adding lastmod to sitemap based on git commits","permalink":"/adding-lastmod-to-sitemap-git-commit-date"}},"content":"Custom font usage can introduce cumulative layout shift (or \\"jank\\") to your website. This post shows how to use [fontaine](https://github.com/unjs/fontaine) to reduce this with Docusaurus sites.\\n\\n![title image reading \\"Docusaurus: Using fontaine to reduce custom font cumulative layout shift\\" in a ridiculous font with the Docusaurus logo and a screenshot of a preload link HTML element](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## What is cumulative layout shift?\\n\\nCumulative layout shift (CLS) is a metric that measures the instability of content on a web page. It\'s a [Core Web Vitals](https://web.dev/vitals/) metric.\\n\\nYou may well know it as \\"jank\\". It\'s jank that you see when a page loads and the text moves around. It\'s an irritation. There\'s a great description of it in [this post on the topic](https://web.dev/cls/); let me quote it here:\\n\\n> Have you ever been reading an article online when something suddenly changes on the page? Without warning, the text moves, and you\'ve lost your place. Or even worse: you\'re about to tap a link or a button, but in the instant before your finger lands\u2014BOOM\u2014the link moves, and you end up clicking something else!\\n\\nFor the rest of this post I\'ll generally to refer to CLS as jank, as it\'s a more relatable term.\\n\\n## What \\"jank\\" looks like\\n\\nMy blog uses a custom font called [Poppins](https://fonts.google.com/specimen/Poppins). Lovely though it is, using the font introduces jank to my site. It\'s particularly noticeable on mobile phones. Here\'s a gif of the jank in action:\\n\\n![A gif of the mobile view of my blog loading and shifting in layout as the custom font arrives](my-jank.gif)\\n\\nYou see how the text shifts around as the custom font arrives? On the first line we either see:\\n\\n- the fallback font rendering four words on one line: _\\"Bicep: Static Web Apps\\"_\\n\\n  OR\\n\\n- the custom font (Poppins) rendering three words on one line: _\\"Bicep: Static Web\\"_\\n\\nIt\'s very noticeable. You can actually put a number on it. The number is the CLS score which you can determine with [Lighthouse](https://developer.chrome.com/docs/lighthouse/overview/). The CLS score is the sum of the layout shifts that occur on the page. The higher the score, the more jank there is. Cumulative Layout Shift was logged as **0.019** for the page above. That\'s not great.\\n\\nI\'d taken steps to reduce the issues experienced, such as [font preloading](../2021-12-29-preload-fonts-with-docusaurus/index.md). But the issues still remained, particularly on mobile networks where speed of loading is decreased, and it takes a longer time for the custom font to load.\\n\\nI had rather given up on improving things further. But then....\\n\\n## fontaine\\n\\nOne evening I was vaguely browsing Twitter when I came across a tweet from [Daniel Roe](https://twitter.com/danielcroe) which [announced a new tool called fontaine](https://twitter.com/danielcroe/status/1581428654479138817):\\n\\n![screenshot of tweet saying: \\"\u26A1\uFE0F Announcing `fontaine`! It\'s a zero-runtime, cross-framework library that significantly & automatically reduces layout shift caused by custom fonts.\\"](screenshot-tweet-about-fontaine.webp)\\n\\nI was intrigued. I wanted to try it out. I wanted to see if it could reduce the jank on my blog.\\n\\n## Using fontaine with Docusaurus\\n\\nI added fontaine as a dependency to my blog:\\n\\n```bash\\nyarn add -D fontaine\\n```\\n\\nI then added cracked open my `docusaurus.config.js` file and wrote a small plugin to make use of fontaine:\\n\\n```js\\nconst fontaine = require(\'fontaine\');\\n\\n// ...\\n\\n/** @type {import(\'@docusaurus/types\').Config} */\\nconst config = {\\n  // ...\\n\\n  plugins: [\\n    // ...\\n    function fontainePlugin(_context, _options) {\\n      return {\\n        name: \'fontaine-plugin\',\\n        configureWebpack(_config, _isServer) {\\n          return {\\n            plugins: [\\n              fontaine.FontaineTransform.webpack({\\n                fallbacks: [\\n                  \'system-ui\',\\n                  \'-apple-system\',\\n                  \'BlinkMacSystemFont\',\\n                  \'Segoe UI\',\\n                  \'Roboto\',\\n                  \'Oxygen\',\\n                  \'Ubuntu\',\\n                  \'Cantarell\',\\n                  \'Open Sans\',\\n                  \'Helvetica Neue\',\\n                  \'sans-serif\',\\n                ],\\n                // You may need to resolve assets like `/fonts/Poppins-Bold.ttf` to a particular directory\\n                resolvePath: (id) => \'../fonts/\' + id,\\n              }),\\n            ],\\n          };\\n        },\\n      };\\n    },\\n    // ...\\n  ],\\n  // ...\\n};\\n```\\n\\nThis didn\'t initially seem to make any difference. I put it up as a [work-in-progress PR](https://github.com/johnnyreilly/blog.johnnyreilly.com/pull/305) and wrote up my findings as best I could. Daniel was kind enough to take a look. He uncovered two issues:\\n\\n- There was a bug in fontaine around how it handled CSS variables; [he implemented a fix](https://github.com/unjs/fontaine/commit/a708bb07ccc48f385c67ccc3b1eed280d8ee47fc)\\n- Docusaurus uses custom fonts through the mechanism of CSS variables. This indirection confuses fontaine as it can\'t read those variables. To accomodate this, we needed to update my CSS variable to add the override font family to the CSS variable:\\n\\n```diff\\n-  --ifm-font-family-base: \'Poppins\';\\n+  --ifm-font-family-base: \'Poppins\', \'Poppins override\';\\n```\\n\\nBehind the scenes, there is a \'Poppins override\' `@font-face` rule that has been created by fontaine. By manually adding this override font family to our CSS variable, we make our site use the fallback \'Poppins override\' `@font-face` rule with the correct font metrics that fontaine generates.\\n\\nIt\'s worth emphasising that for the typical user of fontaine, this is not something they need to do. It\'s only necessary for Docusaurus users because they use custom fonts through CSS variables. Using fontaine is very \\"plug and play\\" for most users.\\n\\nDaniel was kind enough to [raise a PR incorporating both the tweaks](https://github.com/johnnyreilly/blog.johnnyreilly.com/pull/307). When I merged that PR, I saw the following:\\n\\n![A gif of the mobile view of my blog loading and shifting in layout as the custom font arrives](my-jank-fixed.gif)\\n\\nLook at that! You can see the font loading, but there\'s no more jumping of words from one line to another. It\'s a huge improvement.\\n\\n## Conclusion\\n\\nIf you want to improve your CLS score, fontaine is a great tool. This post demonstrates using it with Docusaurus. But please note that this is a generally useful tool that you can use with Vite, Next.js and others. It\'s not specific to Docusaurus.\\n\\nPrior to using fontaine, my blogs Cumulative Layout Shift was logged as **0.019**. After incorporating it, the same score is logged as **0**. This is good news!\\n\\nI\'m very grateful to Daniel for his help in getting it working with my blog. He went above and beyond, so thank you Daniel!\\n\\nIn testament to what a great idea fontaine is built upon, in the time I\'ve been writing this post [`@next/font`](https://nextjs.org/blog/next-13#nextfont) has been announced, which is based upon a similar idea.\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/docusaurus-using-fontaine-reduce-cumulative-layout-shift/)\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.logrocket.com/docusaurus-using-fontaine-reduce-cumulative-layout-shift/\\" />\\n</head>"},{"id":"adding-lastmod-to-sitemap-git-commit-date","metadata":{"permalink":"/adding-lastmod-to-sitemap-git-commit-date","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-11-25-adding-lastmod-to-sitemap-git-commit-date/index.md","source":"@site/blog/2022-11-25-adding-lastmod-to-sitemap-git-commit-date/index.md","title":"Adding lastmod to sitemap based on git commits","description":"This post demonstrates enriching an XML sitemap with `lastmod` timestamps based on git commits.","date":"2022-11-25T00:00:00.000Z","tags":[{"inline":false,"label":"Node.js","permalink":"/tags/node-js","description":"The Node.js runtime."},{"inline":false,"label":"Docusaurus","permalink":"/tags/docusaurus","description":"The Docusaurus static site generator."}],"readingTime":4.555,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"adding-lastmod-to-sitemap-git-commit-date","title":"Adding lastmod to sitemap based on git commits","authors":"johnnyreilly","tags":["node.js","docusaurus"],"image":"./title-image.png","description":"This post demonstrates enriching an XML sitemap with `lastmod` timestamps based on git commits.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Docusaurus: Using fontaine to reduce custom font cumulative layout shift","permalink":"/docusaurus-using-fontaine-to-reduce-custom-font-cumulative-layout-shift"},"nextItem":{"title":"XML: read and write with Node.js","permalink":"/xml-read-and-write-with-node-js"}},"content":"This post demonstrates enriching an XML sitemap with `lastmod` timestamps based on git commits. The sitemap being enriched in this post was generated automatically by Docusaurus. The techniques used are predicated on the way Docusaurus works; in that it is file based. You could easily use this technique for another file based website solution; but you would need tweaks to target the relevant files you would use to drive your `lastmod`.\\n\\nIf you\'re interested in applying the same technique to your RSS / Atom / JSON feeds in Docusaurus, [you may find this post interesting](../2023-01-28-docusaurus-createfeeditems-api-git-commit-date/index.md).\\n\\n![title image reading \\"Adding lastmod to sitemap based on git commits\\" with XML and Docusaurus logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Updated 30/03/2024 - this is built into Docusaurus 3.2\\n\\nI\'m delighted to say that [Docusaurus 3.2 has this functionality built in](https://docusaurus.io/blog/releases/3.2#sitemap-lastmod). So you don\'t need this anymore!\\n\\n## Reading git log in Node.js\\n\\n[In the last post I showed how to manipulate XML in Node.js, and filter our sitemap](../2022-11-22-xml-read-and-write-with-node-js/index.md). In this post we\'ll build upon what we did last time, read the git log in Node.js and use that to power a `lastmod` property.\\n\\nThe `lastmod` property ([documented here](https://sitemaps.org/protocol.html#lastmoddef)) is a optional, and if supplied, should be date of last modification of a page in a W3C Datetime format. (This allows `YYYY-MM-DD`.)\\n\\nTo read the git log in Node.js we\'ll use the [simple-git](https://www.npmjs.com/package/simple-git) package. It\'s a great package that makes it easy to read the git log. Other stuff too - but that\'s what we care about today.\\n\\n```shell\\nyarn add simple-git\\n```\\n\\nTo work with `simple-git` we need to create a `Git` instance. We can do that like so:\\n\\n```ts\\nimport { simpleGit, SimpleGit, SimpleGitOptions } from \'simple-git\';\\n\\nfunction getSimpleGit(): SimpleGit {\\n  const baseDir = path.resolve(process.cwd(), \'..\');\\n\\n  const options: Partial<SimpleGitOptions> = {\\n    baseDir,\\n    binary: \'git\',\\n    maxConcurrentProcesses: 6,\\n    trimmed: false,\\n  };\\n\\n  const git = simpleGit(options);\\n\\n  return git;\\n}\\n```\\n\\n## From sitemap to git log\\n\\nIt\'s worth pausing to consider what our sitemap looks like:\\n\\n```xml\\n<?xml version=\\"1.0\\" encoding=\\"UTF-8\\"?><urlset xmlns=\\"http://www.sitemaps.org/schemas/sitemap/0.9\\" xmlns:news=\\"http://www.google.com/schemas/sitemap-news/0.9\\" xmlns:xhtml=\\"http://www.w3.org/1999/xhtml\\" xmlns:image=\\"http://www.google.com/schemas/sitemap-image/1.1\\" xmlns:video=\\"http://www.google.com/schemas/sitemap-video/1.1\\">\\n  <url>\\n    <loc>https://johnnyreilly.com/2012/01/07/standing-on-shoulders-of-giants</loc>\\n    <changefreq>weekly</changefreq>\\n    <priority>0.5</priority>\\n  </url>\\n  <url>\\n    <loc>https://johnnyreilly.com/2022/09/20/react-usesearchparamsstate</loc>\\n    <changefreq>weekly</changefreq>\\n    <priority>0.5</priority>\\n  </url>\\n  \x3c!-- ... --\x3e\\n</urlset>\\n```\\n\\nIf you look at the URL (`loc`) you can see that it\'s fairly easy to determine the path to the original markdown file. If we take the URL `https://johnnyreilly.com/2012/01/07/standing-on-shoulders-of-giants`, we can see that the path to the markdown file is `blog-website/blog/2012-01-07-standing-on-shoulders-of-giants/index.md`.\\n\\nAs long as we don\'t have a custom slug in play (and I rarely do), we have a reliable way to get from blog post URL (`loc`) to markdown file. With that we can use `simple-git` to get the git log for that file. We can then use that to populate the `lastmod` property.\\n\\n```ts\\nconst dateBlogUrlRegEx = /(\\\\d\\\\d\\\\d\\\\d\\\\/\\\\d\\\\d\\\\/\\\\d\\\\d)\\\\/(.+)/;\\n\\nasync function enrichUrlsWithLastmod(\\n  filteredUrls: SitemapUrl[],\\n): Promise<SitemapUrl[]> {\\n  const git = getSimpleGit();\\n\\n  const urls: SitemapUrl[] = [];\\n  for (const url of filteredUrls) {\\n    if (urls.includes(url)) {\\n      continue;\\n    }\\n\\n    try {\\n      // example url.loc: https://johnnyreilly.com/2012/01/07/standing-on-shoulders-of-giants\\n      const pathWithoutRootUrl = url.loc.replace(rootUrl + \'/\', \'\'); // eg 2012/01/07/standing-on-shoulders-of-giants\\n\\n      const match = pathWithoutRootUrl.match(dateBlogUrlRegEx);\\n\\n      if (!match || !match[1] || !match[2]) {\\n        urls.push(url);\\n        continue;\\n      }\\n\\n      const date = match[1].replaceAll(\'/\', \'-\'); // eg 2012-01-07\\n      const slug = match[2]; // eg standing-on-shoulders-of-giants\\n\\n      const file = `blog-website/blog/${date}-${slug}/index.md`;\\n      const log = await git.log({\\n        file,\\n      });\\n\\n      const lastmod = log.latest?.date.substring(0, 10);\\n      urls.push(lastmod ? { ...url, lastmod } : url);\\n      console.log(url.loc, lastmod);\\n    } catch (e) {\\n      console.log(\'file date not looked up\', url.loc, e);\\n      urls.push(url);\\n    }\\n  }\\n  return urls;\\n}\\n```\\n\\nAbove we\'re using a regular expression to extract the date and slug from the URL. We then use those to construct the path to the markdown file. We then use `simple-git` to get the git log for that file. We then use the latest commit date to populate the `lastmod` property, and push that onto the `urls` array.\\n\\nFinally we return the `urls` array and write that to the sitemap before we write it out:\\n\\n```ts\\nsitemap.urlset.url = await enrichUrlsWithLastmod(filteredUrls);\\n```\\n\\nOur new sitemap looks like this:\\n\\n```xml\\n<?xml version=\\"1.0\\" encoding=\\"UTF-8\\"?><urlset xmlns=\\"http://www.sitemaps.org/schemas/sitemap/0.9\\" xmlns:news=\\"http://www.google.com/schemas/sitemap-news/0.9\\" xmlns:xhtml=\\"http://www.w3.org/1999/xhtml\\" xmlns:image=\\"http://www.google.com/schemas/sitemap-image/1.1\\" xmlns:video=\\"http://www.google.com/schemas/sitemap-video/1.1\\">\\n  <url>\\n    <loc>https://johnnyreilly.com/2012/01/07/standing-on-shoulders-of-giants</loc>\\n    <changefreq>weekly</changefreq>\\n    <priority>0.5</priority>\\n    <lastmod>2021-12-19</lastmod>\\n  </url>\\n  <url>\\n    <loc>https://johnnyreilly.com/2012/01/14/jqgrid-its-just-far-better-grid</loc>\\n    <changefreq>weekly</changefreq>\\n    <priority>0.5</priority>\\n    <lastmod>2022-11-03</lastmod>\\n  </url>\\n  \x3c!-- ... --\x3e\\n</urlset>\\n```\\n\\nYou see the `lastmod` property has been populated for URLs based upon the most recent commit for that file. Yay!\\n\\n## GitHub Actions - `fetch_depth`\\n\\nYou might think we were done (I thought we were done), but we\'re not. We\'re not done because we\'re using GitHub Actions to build the site.\\n\\nWhen I tested this locally, it worked fine. However, when I pushed it to GitHub Actions, it surfaced a `latest.date` which wasn\'t populated with the value you\'d hope. The reason was that the `fetch_depth` was set to 1 (the default). This meant that the git log wasn\'t providing the information we\'d hope for. By changing the `fetch_depth` to 0 the situation is resolved.\\n\\n```yaml\\n- uses: actions/checkout@v3\\n  with:\\n    # Number of commits to fetch. 0 indicates all history for all branches and tags.\\n    # Default: 1\\n    fetch-depth: 0\\n```\\n\\n## Updated 12th November 2023: Google\'s view on `lastmod`, `changefreq` and `priority`\\n\\nGoogle have announced that they [use `lastmod` as a specific signal for triggering recrawling](https://developers.google.com/search/blog/2023/06/sitemaps-lastmod-ping#the-lastmod-element). It goes on to say that it doesn\'t use the `changefreq` or `priority` elements to trigger recrawling of URLs.\\n\\nSo if you want to have a sitemap that triggers reindexing well, having an accurate `lastmod` will help.\\n\\n## Conclusion\\n\\nThis post demonstrates how you can enrich a `lastmod`less sitemap to have one that is driven by git commit date. I hope it helps!"},{"id":"xml-read-and-write-with-node-js","metadata":{"permalink":"/xml-read-and-write-with-node-js","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-11-22-xml-read-and-write-with-node-js/index.md","source":"@site/blog/2022-11-22-xml-read-and-write-with-node-js/index.md","title":"XML: read and write with Node.js","description":"This post demonstrates reading and writing XML in Node.js using fast-xml-parser. We will use the Docusauruses XML sitemap as an example.","date":"2022-11-22T00:00:00.000Z","tags":[{"inline":false,"label":"Node.js","permalink":"/tags/node-js","description":"The Node.js runtime."},{"inline":false,"label":"Docusaurus","permalink":"/tags/docusaurus","description":"The Docusaurus static site generator."}],"readingTime":5.905,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"xml-read-and-write-with-node-js","title":"XML: read and write with Node.js","authors":"johnnyreilly","tags":["node.js","docusaurus"],"image":"./title-image.png","description":"This post demonstrates reading and writing XML in Node.js using fast-xml-parser. We will use the Docusauruses XML sitemap as an example.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Adding lastmod to sitemap based on git commits","permalink":"/adding-lastmod-to-sitemap-git-commit-date"},"nextItem":{"title":"Azure AD Claims with Static Web Apps and Azure Functions","permalink":"/azure-ad-claims-static-web-apps-azure-functions"}},"content":"This post demonstrates reading and writing XML in Node.js using `fast-xml-parser`. We\'ll use the Docusauruses XML sitemap as an example.\\n\\n![title image reading \\"XML: read and write with Node.js\\" with XML and Docusaurus logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Updated 03/05/2023\\n\\nThis post talks about manipulating the Docusaurus sitemap as an example of how to work on XML with Node.js.\\n\\nIt\'s worth noting that Docusaurus has offered a way to [configure the sitemap as of Docusaurus 3.3](https://docusaurus.io/blog/releases/3.3#createsitemapitems) which [I worked on](https://github.com/facebook/docusaurus/pull/10083).\\n\\nHowever, the techniques described here are still useful for working with XML in Node.js.\\n\\n## Docusaurus sitemap\\n\\nI was prompted to write this post by wanting to edit the sitemap on my Docusaurus blog. I wanted to remove the `/page/` and `/tag/` routes from the sitemap. They effectively serve as duplicate content and I don\'t want them to be indexed by search engines. (A little more is required to remove them from search engines - see the section at the end of the post.)\\n\\nI was able to find the sitemap in the `build` folder of my Docusaurus site. It\'s called `sitemap.xml` and it\'s in the root of the `build` folder. It looks like this:\\n\\n```xml\\n<?xml version=\\"1.0\\" encoding=\\"UTF-8\\"?><urlset xmlns=\\"http://www.sitemaps.org/schemas/sitemap/0.9\\" xmlns:news=\\"http://www.google.com/schemas/sitemap-news/0.9\\" xmlns:xhtml=\\"http://www.w3.org/1999/xhtml\\" xmlns:image=\\"http://www.google.com/schemas/sitemap-image/1.1\\" xmlns:video=\\"http://www.google.com/schemas/sitemap-video/1.1\\">\\n  <url>\\n    <loc>https://johnnyreilly.com/2012/01/07/standing-on-shoulders-of-giants</loc>\\n    <changefreq>weekly</changefreq>\\n    <priority>0.5</priority>\\n  </url>\\n  <url>\\n    <loc>https://johnnyreilly.com/2022/09/20/react-usesearchparamsstate</loc>\\n    <changefreq>weekly</changefreq>\\n    <priority>0.5</priority>\\n  </url>\\n  <url>\\n    <loc>https://johnnyreilly.com/page/10</loc>\\n    <changefreq>weekly</changefreq>\\n    <priority>0.5</priority>\\n  </url>\\n  <url>\\n    <loc>https://johnnyreilly.com/tags/ajax</loc>\\n    <changefreq>weekly</changefreq>\\n    <priority>0.5</priority>\\n  </url>\\n  \x3c!-- ... --\x3e\\n</urlset>\\n```\\n\\n## `fast-xml-parser`\\n\\nAfter experimenting with a few different XML parsers I settled on [`fast-xml-parser`](https://github.com/NaturalIntelligence/fast-xml-parser). It\'s fast, it\'s simple and it\'s well maintained. It also handles XML namespaces and attributes well. (This appears to be rare in XML parsers.)\\n\\nLet\'s scaffold up an example project alongside our Docusaurus site:\\n\\n```bash\\nmkdir trim-xml\\ncd trim-xml\\nnpx typescript --init\\nyarn init\\nyarn add @types/node fast-xml-parser ts-node typescript\\n```\\n\\nAnd in the `package.json` file add a `start` script:\\n\\n```json\\n{\\n  \\"scripts\\": {\\n    \\"start\\": \\"ts-node index.ts\\"\\n  }\\n}\\n```\\n\\nFinally, create an empty `index.ts` file.\\n\\n## Reading XML\\n\\nOur Docusaurus sitemap is in the `build` folder of our Docusaurus site. Let\'s read it in and parse it into a JavaScript object:\\n\\n```ts\\nimport { XMLParser, XMLBuilder } from \'fast-xml-parser\';\\nimport fs from \'fs\';\\nimport path from \'path\';\\n\\ninterface Sitemap {\\n  urlset: {\\n    url: { loc: string; changefreq: string; priority: number }[];\\n  };\\n}\\n\\nasync function trimXML() {\\n  const sitemapPath = path.resolve(\\n    \'..\',\\n    \'blog-website\',\\n    \'build\',\\n    \'sitemap.xml\',\\n  );\\n\\n  console.log(`Loading ${sitemapPath}`);\\n  const sitemapXml = await fs.promises.readFile(sitemapPath, \'utf8\');\\n\\n  const parser = new XMLParser({\\n    ignoreAttributes: false,\\n  });\\n  let sitemap: Sitemap = parser.parse(sitemapXml);\\n\\n  console.log(sitemap);\\n}\\n\\ntrimXML();\\n```\\n\\nWe\'re using the `XMLParser` class to parse the XML into a JavaScript object. We\'re also using the `ignoreAttributes` option to ensure that attributes are included in the parsed object. When we run this we get the following output:\\n\\n```bash\\nLoading /home/john/code/github/blog.johnnyreilly.com/blog-website/build/sitemap.xml\\n{\\n  \'?xml\': { \'@_version\': \'1.0\', \'@_encoding\': \'UTF-8\' },\\n  urlset: {\\n    url: [\\n      [Object], [Object], [Object], [Object], [Object], [Object],\\n      [Object], [Object], [Object], [Object], [Object], [Object],\\n      [Object], [Object], [Object], [Object], [Object], [Object],\\n      [Object], [Object], [Object], [Object], [Object], [Object],\\n      [Object], [Object], [Object], [Object], [Object], [Object],\\n      [Object], [Object], [Object], [Object], [Object], [Object],\\n      [Object], [Object], [Object], [Object], [Object], [Object],\\n      [Object], [Object], [Object], [Object], [Object], [Object],\\n      [Object], [Object], [Object], [Object], [Object], [Object],\\n      [Object], [Object], [Object], [Object], [Object], [Object],\\n      [Object], [Object], [Object], [Object], [Object], [Object],\\n      [Object], [Object], [Object], [Object], [Object], [Object],\\n      [Object], [Object], [Object], [Object], [Object], [Object],\\n      [Object], [Object], [Object], [Object], [Object], [Object],\\n      [Object], [Object], [Object], [Object], [Object], [Object],\\n      [Object], [Object], [Object], [Object], [Object], [Object],\\n      [Object], [Object], [Object], [Object],\\n      ... 1481 more items\\n    ],\\n    \'@_xmlns\': \'http://www.sitemaps.org/schemas/sitemap/0.9\',\\n    \'@_xmlns:news\': \'http://www.google.com/schemas/sitemap-news/0.9\',\\n    \'@_xmlns:xhtml\': \'http://www.w3.org/1999/xhtml\',\\n    \'@_xmlns:image\': \'http://www.google.com/schemas/sitemap-image/1.1\',\\n    \'@_xmlns:video\': \'http://www.google.com/schemas/sitemap-video/1.1\'\\n  }\\n}\\n```\\n\\nAs we can see, the `fast-xml-parser` library has parsed the XML into a JavaScript object. We can see that the `urlset` element has an array of `url` elements. Each `url` element has a `loc`, `changefreq` and `priority` element. We can also see that the `urlset` element has a number of attributes. This matches the XML we saw earlier and the interface we defined.\\n\\n## Filtering and writing XML\\n\\nNow that we have the XML parsed into a JavaScript object we can filter it just like we would any other JavaScript object. We have all the power of JavaScript at our fingertips!\\n\\nAs I mentioned earlier, I want to remove all the URLs that represent duplicate content. This includes \\"pagination\\" URLs. These are URLs that are used to navigate between pages of content. For example, the URL `https://johnnyreilly.com/page/10` is a pagination URL. I want to remove these URLs from the sitemap. I also want to get rid of the \\"tags\\" URLs. These are URLs that are used to navigate between posts that have a particular tag. For example, the URL `https://johnnyreilly.com/tags/ajax` is a tag URL. I want to remove these URLs from the sitemap too.\\n\\nThis is simplicity itself now we\'re in JavaScript land. We can use the `filter` method on the `url` array to remove the URLs we don\'t want:\\n\\n```ts\\nconst rootUrl = \'https://johnnyreilly.com\';\\nconst filteredUrls = sitemap.urlset.url.filter(\\n  (url) =>\\n    url.loc !== `${rootUrl}/tags` &&\\n    !url.loc.startsWith(rootUrl + \'/tags/\') &&\\n    !url.loc.startsWith(rootUrl + \'/page/\'),\\n);\\n```\\n\\nWe can then update the `url` array with the filtered URLs:\\n\\n```ts\\nsitemap.urlset.url = filteredUrls;\\n```\\n\\nFinally, we can write the XML back out to a file:\\n\\n```ts\\nconst builder = new XMLBuilder({\\n  ignoreAttributes: false,\\n});\\nconst xml = builder.buildObject(sitemap);\\n\\nconst outputPath = path.resolve(\'sitemap.xml\');\\nawait fs.promises.writeFile(outputPath, xml);\\n```\\n\\nNote again that we\'re using the `ignoreAttributes` option to ensure that attributes are included in the XML.\\n\\nLet\'s put it all together into a single file:\\n\\n```ts\\nimport { XMLParser, XMLBuilder } from \'fast-xml-parser\';\\nimport fs from \'fs\';\\nimport path from \'path\';\\n\\ninterface Sitemap {\\n  urlset: {\\n    url: { loc: string; changefreq: string; priority: number }[];\\n  };\\n}\\n\\nasync function trimXML() {\\n  const sitemapPath = path.resolve(\\n    \'..\',\\n    \'blog-website\',\\n    \'build\',\\n    \'sitemap.xml\',\\n  );\\n\\n  console.log(`Loading ${sitemapPath}`);\\n  const sitemapXml = await fs.promises.readFile(sitemapPath, \'utf8\');\\n\\n  const parser = new XMLParser({\\n    ignoreAttributes: false,\\n  });\\n  let sitemap: Sitemap = parser.parse(sitemapXml);\\n\\n  const rootUrl = \'https://johnnyreilly.com\';\\n  const filteredUrls = sitemap.urlset.url.filter(\\n    (url) =>\\n      url.loc !== `${rootUrl}/tags` &&\\n      !url.loc.startsWith(rootUrl + \'/tags/\') &&\\n      !url.loc.startsWith(rootUrl + \'/page/\'),\\n  );\\n\\n  console.log(\\n    `Reducing ${sitemap.urlset.url.length} urls to ${filteredUrls.length} urls`,\\n  );\\n\\n  sitemap.urlset.url = filteredUrls;\\n\\n  const builder = new XMLBuilder({ format: false, ignoreAttributes: false });\\n  const shorterSitemapXml = builder.build(sitemap);\\n\\n  console.log(`Saving ${sitemapPath}`);\\n  await fs.promises.writeFile(sitemapPath, shorterSitemapXml);\\n}\\n\\ntrimXML();\\n```\\n\\nWith that we\'re done. We can run the script and see the result:\\n\\n```bash\\nLoading /github/workspace/blog-website/build/sitemap.xml\\nReducing 1598 urls to 281 urls\\nSaving /github/workspace/blog-website/build/sitemap.xml\\n```\\n\\n## Conclusion\\n\\nIn this post we\'ve seen how to use the `fast-xml-parser` library to parse XML into a JavaScript object, operate upon that object and then write it back out to XML.\\n\\nIf you\'d to see how I\'m using this directly on my blog, it\'s probably worth looking at [this PR](https://github.com/johnnyreilly/blog.johnnyreilly.com/pull/344).\\n\\n## PS `noindex`\\n\\nThis is unrelated to XML processing, but I didn\'t want to miss this out. [Merely editing the sitemap isn\'t enough to remove them from search engines](https://developers.google.com/search/docs/crawling-indexing/remove-information). We\'re also going to serve a `noindex` response header for those routes by adjusting the [`staticwebapp.config.json` file of our Static Web App](https://learn.microsoft.com/en-us/azure/static-web-apps/configuration):\\n\\n```json\\n{\\n  // ...\\n  \\"routes\\": [\\n    // ...\\n    {\\n      \\"route\\": \\"/tags/*\\",\\n      \\"headers\\": {\\n        \\"X-Robots-Tag\\": \\"noindex\\"\\n      }\\n    },\\n    {\\n      \\"route\\": \\"/page/*\\",\\n      \\"headers\\": {\\n        \\"X-Robots-Tag\\": \\"noindex\\"\\n      }\\n    }\\n  ]\\n  // ...\\n}\\n```"},{"id":"azure-ad-claims-static-web-apps-azure-functions","metadata":{"permalink":"/azure-ad-claims-static-web-apps-azure-functions","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-11-17-azure-ad-claims-static-web-apps-azure-functions/index.md","source":"@site/blog/2022-11-17-azure-ad-claims-static-web-apps-azure-functions/index.md","title":"Azure AD Claims with Static Web Apps and Azure Functions","description":"Authorization with Azure Static Web Apps linked to Azure Functions has an issue. Azure AD app role claims are not supplied; this post will demo a workaround.","date":"2022-11-17T00:00:00.000Z","tags":[{"inline":false,"label":"Authentication and Authorization","permalink":"/tags/auth","description":"Matters related to identity and permissioning"},{"inline":false,"label":"Azure Functions","permalink":"/tags/azure-functions","description":"The Azure Functions service."},{"inline":false,"label":"Azure Static Web Apps","permalink":"/tags/azure-static-web-apps","description":"The Azure Static Web Apps service."},{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."}],"readingTime":11.74,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-ad-claims-static-web-apps-azure-functions","title":"Azure AD Claims with Static Web Apps and Azure Functions","authors":"johnnyreilly","tags":["auth","azure functions","azure static web apps","azure"],"image":"./title-image.png","description":"Authorization with Azure Static Web Apps linked to Azure Functions has an issue. Azure AD app role claims are not supplied; this post will demo a workaround.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"XML: read and write with Node.js","permalink":"/xml-read-and-write-with-node-js"},"nextItem":{"title":"Debugging Azure Functions in VS Code on Mac OS","permalink":"/debugging-azure-functions-vs-code-mac-os"}},"content":"Authorization in Azure Functions is impaired by an issue with Azure Static Web Apps linked to Azure Functions. Azure AD app role claims are not supplied to Azure Functions. This post will demonstrate a workaround.\\n\\n![title image reading \\"Azure AD Claims with Static Web Apps and Azure Functions\\" with Azure AD, Azure Functions and Static Web App logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Updated 28th November 2022\\n\\nAfter I posted this, [Thomas Gauvin](https://twitter.com/thomasgauvin) (Product manager for Static Web Apps) was kind enough to tweet this:\\n\\n[![screenshot of tweet from Thomas Gauvin saying \\"Thanks for writing this @johnny_reilly, I know this is a pain point with SWA auth at the moment. I\'m sure this article will help others in the meantime. We\'re working on correcting our docs + looking to add support for this in the future\\"](screenshot-twitter-thomas-gauvin-support-in-future.webp)](https://twitter.com/thomasgauvin/status/1596242773686079496)\\n\\nSo by the sounds of it, this blog post will not be required in the longer term, as support should to be added directly. Tremendous news!\\n\\n## Where\'s my claims?\\n\\nThere is a limitation that affects authorization when you have a linked backend paired with an Azure Static Web App. Let\'s take the case of having an Azure Function App as the linked backend. Essentially the Azure Function app _does not_ receive the claims that the Static Web App receives. [There\'s an issue tracking this on GitHub](https://github.com/Azure/static-web-apps/issues/988), and it seems that this is a general problem with Static Web Apps, Azure AD and linked backends.\\n\\nWe have a Static Web App, with an associated C# Function App (using the [Bring Your Own Functions](../2022-10-14-bicep-static-web-apps-linked-backends/index.md) AKA \\"linked backend\\" approach). Both the Static Web App and Function App are associated with the same Azure AD App Registration.\\n\\nWhen we\'re authenticated with Azure AD and go to the auth endpoint in our Static Web App: `/.auth/me` we see:\\n\\n```json\\n{\\n  \\"clientPrincipal\\": {\\n    \\"identityProvider\\": \\"aad\\",\\n    \\"userId\\": \\"d9178465-3847-4d98-9d23-b8b9e403b323\\",\\n    \\"userDetails\\": \\"johnny_reilly@hotmail.com\\",\\n    \\"userRoles\\": [\\"authenticated\\", \\"anonymous\\"],\\n    \\"claims\\": [\\n      // ...\\n      {\\n        \\"typ\\": \\"http://schemas.microsoft.com/identity/claims/objectidentifier\\",\\n        \\"val\\": \\"d9178465-3847-4d98-9d23-b8b9e403b323\\"\\n      },\\n      {\\n        \\"typ\\": \\"http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress\\",\\n        \\"val\\": \\"johnny_reilly@hotmail.com\\"\\n      },\\n      {\\n        \\"typ\\": \\"name\\",\\n        \\"val\\": \\"John Reilly\\"\\n      },\\n      {\\n        \\"typ\\": \\"roles\\",\\n        \\"val\\": \\"OurApp.Read\\"\\n      },\\n      // ...\\n      {\\n        \\"typ\\": \\"ver\\",\\n        \\"val\\": \\"2.0\\"\\n      }\\n    ]\\n  }\\n}\\n```\\n\\nNote the claims in there. These include custom claims that we\'ve configured against our Azure AD App Registration such as roles with `OurApp.Read`.\\n\\nSo we can access claims successfully in the Static Web App (the front end). However, the associated Function App does **not** have access to the claims.\\n\\nIt\'s possible to see this by implementing a function in our Azure Function App which surfaces roles:\\n\\n```cs\\n[FunctionName(\\"GetRoles\\")]\\npublic static async Task<IActionResult> Run(\\n    [HttpTrigger(AuthorizationLevel.Anonymous, \\"get\\", \\"post\\", Route = \\"GetRoles\\")] HttpRequest req\\n)\\n{\\n    var roles = req.HttpContext.User?.Claims.Select(c => new { c.Type, c.Value });\\n\\n    return new OkObjectResult(roles);\\n}\\n```\\n\\nWhen this `/api/GetRoles` endpoint is accessed we see this:\\n\\n```json\\n[\\n  {\\n    \\"Type\\": \\"http://schemas.xmlsoap.org/ws/2005/05/identity/claims/nameidentifier\\",\\n    \\"Value\\": \\"d9178465-3847-4d98-9d23-b8b9e403b323\\"\\n  },\\n  {\\n    \\"Type\\": \\"http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name\\",\\n    \\"Value\\": \\"johnny_reilly@hotmail.com\\"\\n  },\\n  {\\n    \\"Type\\": \\"http://schemas.microsoft.com/ws/2008/06/identity/claims/role\\",\\n    \\"Value\\": \\"authenticated\\"\\n  },\\n  {\\n    \\"Type\\": \\"http://schemas.microsoft.com/ws/2008/06/identity/claims/role\\",\\n    \\"Value\\": \\"anonymous\\"\\n  }\\n]\\n```\\n\\nAt first look, this seems great; we have claims! But when we look again we realise that we have far less claims than we might have hoped for. Crucially, our custom claims / app roles like `OurApp.Read` are missing.\\n\\n## Maybe they\'re hiding in `x-ms-client-principal`?\\n\\nIf we look directly at the `x-ms-client-principal` header, maybe we\'ll find what we need?\\n\\n```cs\\n[FunctionName(\\"GetRoles\\")]\\npublic static async Task<IActionResult> GetRoles(\\n    [HttpTrigger(AuthorizationLevel.Anonymous, \\"get\\", Route = \\"GetRoles\\")] HttpRequest req\\n)\\n{\\n    var header = req.Headers[\\"x-ms-client-principal\\"];\\n    var data = header.FirstOrDefault();\\n    if (data == null)\\n    {\\n        return new OkObjectResult(\\"nothing\\");\\n    }\\n\\n    var decoded = System.Convert.FromBase64String(data);\\n    var json = System.Text.ASCIIEncoding.ASCII.GetString(decoded);\\n\\n    return new OkObjectResult(json);\\n}\\n```\\n\\nAlas not. We have the user\'s email and some simple roles (\\"authenticated\\" and \\"anonymous\\"), but no sign of our custom claims / app roles:\\n\\n```json\\n{\\n  \\"identityProvider\\": \\"aad\\",\\n  \\"userId\\": \\"d9178465-3847-4d98-9d23-b8b9e403b323\\",\\n  \\"userDetails\\": \\"johnny_reilly@hotmail.com\\",\\n  \\"userRoles\\": [\\"authenticated\\", \\"anonymous\\"]\\n}\\n```\\n\\nThis is the problem: we want our Azure Function App to be able to make use of the same custom claims / app roles that we use for authorization in the Static Web App. How can we achieve this?\\n\\n## Microsoft Graph API\\n\\nThe answer lies with the Microsoft Graph API. We can interrogate it to get the app role assignments for the user. This will give us the same information that we have in the Static Web App. (Well to be strictly accurate, it will be a slightly different set of claims. But what matters is it will be the app role assignment claims that we want to use for authorization.)\\n\\nWe already have an Azure AD app registration. In order that we can interrogate the Microsoft Graph API, we\'ll need the following permissions:\\n\\n![Screenshot of the Azure AD app registration API permissions screen](screenshot-azure-portal-azure-ad-app-registration-api-permissions.png)\\n\\n- [User.Read](https://learn.microsoft.com/en-us/graph/permissions-reference#delegated-permissions-85) - to sign in\\n- [User.Read.All](https://learn.microsoft.com/en-us/graph/permissions-reference#application-permissions-81) - for acquiring the app role assignments against a user\\n- [Application.Read.All](https://learn.microsoft.com/en-us/graph/permissions-reference#application-permissions-4) - to get more information about the app role assignments - allowing us to translate the app role assignments into the claims that we want to use for authorization\\n\\nOf the above permissions, it\'s likely that you\'ll already have delegated `User.Read` in place; the other two you might need to add and ensure they\'re granted in Azure.\\n\\n## Interrogating the Microsoft Graph API\\n\\nNow we have an Azure AD App Registration with sufficient permissions, we\'ll need a `GraphClient` to interrogate the Microsoft Graph API. To get that we\'re going to build an `AuthenticatedGraphClientFactory`:\\n\\n```cs\\nusing System.Net.Http;\\nusing System.Net.Http.Headers;\\nusing System.Threading.Tasks;\\nusing Microsoft.Graph;\\nusing Microsoft.Identity.Client;\\n\\nnamespace MyApp.Auth\\n{\\n    public interface IAuthenticatedGraphClientFactory\\n    {\\n        (GraphServiceClient, string) GetAuthenticatedGraphClientAndClientId();\\n    }\\n\\n    public class AuthenticatedGraphClientFactory : IAuthenticatedGraphClientFactory\\n    {\\n        private GraphServiceClient? _graphServiceClient;\\n        private readonly string _clientId;\\n        private readonly string _clientSecret;\\n        private readonly string _tenantId;\\n\\n        public AuthenticatedGraphClientFactory(\\n            string clientId,\\n            string clientSecret,\\n            string tenantId\\n        )\\n        {\\n            _clientId = clientId;\\n            _clientSecret = clientSecret;\\n            _tenantId = tenantId;\\n        }\\n\\n        public (GraphServiceClient, string) GetAuthenticatedGraphClientAndClientId()\\n        {\\n            var authenticationProvider = CreateAuthenticationProvider();\\n\\n            _graphServiceClient = new GraphServiceClient(authenticationProvider);\\n\\n            return (_graphServiceClient, _clientId);\\n        }\\n\\n        private IAuthenticationProvider CreateAuthenticationProvider()\\n        {\\n            // this specific scope means that application will default to what is defined in the application registration rather than using dynamic scopes\\n            string[] scopes = new string[]\\n            {\\n                \\"https://graph.microsoft.com/.default\\"\\n            };\\n\\n            var confidentialClientApplication = ConfidentialClientApplicationBuilder.Create(_clientId)\\n                .WithAuthority($\\"https://login.microsoftonline.com/{_tenantId}/v2.0\\")\\n                .WithClientSecret(_clientSecret)\\n                .Build();\\n\\n            return new MsalAuthenticationProvider(confidentialClientApplication, scopes); ;\\n        }\\n    }\\n\\n    public class MsalAuthenticationProvider : IAuthenticationProvider\\n    {\\n        private readonly IConfidentialClientApplication _clientApplication;\\n        private readonly string[] _scopes;\\n\\n        public MsalAuthenticationProvider(IConfidentialClientApplication clientApplication, string[] scopes)\\n        {\\n            _clientApplication = clientApplication;\\n            _scopes = scopes;\\n        }\\n\\n        /// <summary>\\n        /// Update HttpRequestMessage with credentials\\n        /// </summary>\\n        public async Task AuthenticateRequestAsync(HttpRequestMessage request)\\n        {\\n            var token = await GetTokenAsync();\\n\\n            request.Headers.Authorization = new AuthenticationHeaderValue(\\"bearer\\", token);\\n        }\\n\\n        /// <summary>\\n        /// Acquire Token\\n        /// </summary>\\n        public async Task<string?> GetTokenAsync()\\n        {\\n            var authResult = await _clientApplication.AcquireTokenForClient(_scopes).ExecuteAsync();\\n\\n            return authResult.AccessToken;\\n        }\\n    }\\n}\\n```\\n\\nWhen we execute `GetAuthenticatedGraphClientAndClientId` we\'ll get back a `GraphServiceClient` that we can use to interrogate the Microsoft Graph API. We\'ll also get back the client ID of the Graph API App. We\'ll need this later. Note that the `AuthenticatedGraphClientFactory` requires the client ID, client secret and tenant ID of the Azure AD App Registration.\\n\\nNow we have the ability to interrogate the Microsoft Graph API, we can write a `PrincipalService.cs` class that will interrogate it and return the app role assignments for the user:\\n\\n```cs\\nusing System;\\nusing System.Collections.Generic;\\nusing System.Linq;\\nusing System.Security.Claims;\\nusing System.Text;\\nusing System.Text.Json;\\nusing System.Threading.Tasks;\\nusing Microsoft.AspNetCore.Http;\\nusing Microsoft.Extensions.Logging;\\nusing Microsoft.Graph;\\n\\nnamespace MyApp.Auth\\n{\\n    public interface IPrincipalService\\n    {\\n        Task<ClaimsPrincipal> GetPrincipal(HttpRequest req);\\n    }\\n\\n    public class PrincipalService : IPrincipalService\\n    {\\n        readonly ILogger<PrincipalService> _log;\\n        readonly IAuthenticatedGraphClientFactory _graphClientFactory;\\n\\n        public PrincipalService(\\n            IAuthenticatedGraphClientFactory graphClientFactory,\\n            ILogger<PrincipalService> log\\n        )\\n        {\\n            _graphClientFactory = graphClientFactory;\\n            _log = log;\\n        }\\n\\n        public async Task<ClaimsPrincipal> GetPrincipal(HttpRequest req)\\n        {\\n            try\\n            {\\n                MsClientPrincipal? principal = MakeMsClientPrincipal(req);\\n\\n                if (principal == null)\\n                    return new ClaimsPrincipal();\\n\\n                if (!principal.UserRoles?.Where(NotAnonymous).Any() ?? true)\\n                    return new ClaimsPrincipal();\\n\\n                ClaimsIdentity identity = await MakeClaimsIdentity(principal);\\n\\n                return new ClaimsPrincipal(identity);\\n            }\\n            catch (Exception e)\\n            {\\n                _log.LogError(e, \\"Error parsing claims principal\\");\\n                return new ClaimsPrincipal();\\n            }\\n        }\\n\\n        MsClientPrincipal? MakeMsClientPrincipal(HttpRequest req)\\n        {\\n            MsClientPrincipal? principal = null;\\n\\n            if (req.Headers.TryGetValue(\\"x-ms-client-principal\\", out var header))\\n            {\\n                var data = header.FirstOrDefault();\\n                if (data != null)\\n                {\\n                    var decoded = Convert.FromBase64String(data);\\n                    var json = Encoding.UTF8.GetString(decoded);\\n                    _log.LogInformation($\\"x-ms-client-principal: {json}\\");\\n                    principal = JsonSerializer.Deserialize<MsClientPrincipal>(json, new JsonSerializerOptions { PropertyNameCaseInsensitive = true });\\n                }\\n            }\\n\\n            return principal;\\n        }\\n\\n        async Task<ClaimsIdentity> MakeClaimsIdentity(MsClientPrincipal principal)\\n        {\\n            var identity = new ClaimsIdentity(principal.IdentityProvider);\\n\\n            identity.AddClaim(new Claim(ClaimTypes.NameIdentifier, principal.UserId!));\\n            identity.AddClaim(new Claim(ClaimTypes.Name, principal.UserDetails!));\\n\\n            if (principal.UserRoles != null)\\n                identity.AddClaims(principal.UserRoles\\n                    .Where(NotAnonymous)\\n                    .Select(userRole => new Claim(ClaimTypes.Role, userRole)));\\n\\n            var username = principal.UserDetails;\\n            if (username != null)\\n            {\\n                var userAppRoleAssignments = await GetUserAppRoleAssignments(username);\\n                identity.AddClaims(userAppRoleAssignments\\n                    .Select(userAppRoleAssignments => new Claim(ClaimTypes.Role, userAppRoleAssignments)));\\n            }\\n\\n            return identity;\\n        }\\n\\n        static bool NotAnonymous(string r) =>\\n            !string.Equals(r, \\"anonymous\\", StringComparison.CurrentCultureIgnoreCase);\\n\\n        async Task<string[]> GetUserAppRoleAssignments(string username)\\n        {\\n            try\\n            {\\n                var (graphClient, clientId) = _graphClientFactory.GetAuthenticatedGraphClientAndClientId();\\n                _log.LogInformation(\\"Getting AppRoleAssignments for {username}\\", username);\\n\\n                var userRoleAssignments = await graphClient.Users[username]\\n                    .AppRoleAssignments\\n                    .Request()\\n                    .GetAsync();\\n\\n                var roleIds = new List<string>();\\n                var pageIterator = PageIterator<AppRoleAssignment>\\n                    .CreatePageIterator(\\n                        graphClient,\\n                        userRoleAssignments,\\n                        // Callback executed for each item in the collection\\n                        (appRoleAssignment) =>\\n                        {\\n                            if (appRoleAssignment.AppRoleId.HasValue && appRoleAssignment.AppRoleId.Value != Guid.Empty)\\n                                roleIds.Add(appRoleAssignment.AppRoleId.Value.ToString());\\n\\n                            return true;\\n                        },\\n                        // Used to configure subsequent page requests\\n                        (baseRequest) =>\\n                        {\\n                            // Re-add the header to subsequent requests\\n                            baseRequest.Header(\\"Prefer\\", \\"outlook.body-content-type=\\\\\\"text\\\\\\"\\");\\n                            return baseRequest;\\n                        });\\n\\n                await pageIterator.IterateAsync();\\n\\n                var applications = await graphClient.Applications\\n                    .Request()\\n                    .Filter($\\"appId eq \'{clientId}\'\\") // we\'re only interested in the app that we\'re running as\\n                    .GetAsync();\\n\\n                var appRoleAssignments = applications\\n                    .FirstOrDefault()\\n                    ?.AppRoles\\n                    ?.Where(appRole => appRole.Id.HasValue && roleIds.Contains(appRole.Id!.Value.ToString()))\\n                    .Select(appRole => appRole.Value)\\n                    .ToArray();\\n\\n                return appRoleAssignments ?? Array.Empty<string>();\\n            }\\n            catch (Exception e)\\n            {\\n                _log.LogError(e, \\"Error getting AppRoleAssignments\\");\\n                return Array.Empty<string>();\\n            }\\n        }\\n\\n        class MsClientPrincipal\\n        {\\n            public string? IdentityProvider { get; set; }\\n            public string? UserId { get; set; }\\n            public string? UserDetails { get; set; }\\n            public IEnumerable<string>? UserRoles { get; set; }\\n        }\\n    }\\n}\\n```\\n\\nQuite a lot of code! Let\'s walk through what it does:\\n\\n1. It takes the `x-ms-client-principal` header and deserializes it into a `MsClientPrincipal` object - this is the cut down version of the `ClaimsPrincipal` object that we saw earlier:\\n\\n```json\\n{\\n  \\"identityProvider\\": \\"aad\\",\\n  \\"userId\\": \\"d9178465-3847-4d98-9d23-b8b9e403b323\\",\\n  \\"userDetails\\": \\"johnny_reilly@hotmail.com\\",\\n  \\"userRoles\\": [\\"authenticated\\", \\"anonymous\\"]\\n}\\n```\\n\\n2. It creates a new `ClaimsIdentity` using that information, but stripping out the `anonymous` role as it\'s superfluous.\\n\\n3. Using the `userDetails` (email address) from the `MsClientPrincipal` object, it gets the app role assignments for that user from the Graph API. (We needed `User.Read.All` to do this.)\\n\\n4. In a perfect world, we\'d be able to use the `AppRoleAssignments` property on the `User` object to get the app role assignments for a user, but unfortunately that doesn\'t come with the human readable name you\'d hope for; the `MyApp.Read`. So we have to interrogate the Graph API once more and use the `Application` that represents our Azure AD App Registration (we acquire this by filtering for an `appId` matching our `clientId`). Then we can get the human readable / `MyApp.Read` role assignment.\\n\\n5. It adds the app role assignments as role claims to the `ClaimsIdentity` object.\\n\\n6. It returns the `ClaimsIdentity` object wrapped in a `ClaimsPrincipal` object.\\n\\n## Using the `PrincipalService`\\n\\nIn order that we can make use of our `PrincipalService` we need to configure it and the `AuthenticatedGraphClientFactory` in our `Startup` class:\\n\\n```cs\\nservices.AddTransient<IAuthenticatedGraphClientFactory>(sp =>\\n    new AuthenticatedGraphClientFactory(\\n        // The parameters can be sourced from the Azure AD App Registration\\n        clientId,\\n        clientSecret,\\n        tenantId\\n    ));\\n\\nservices.AddTransient<IPrincipalService, PrincipalService>();\\n```\\n\\nWith that in place, we can now use the `IPrincipalService` in a function:\\n\\n```cs\\nusing System.Linq;\\nusing System.Threading.Tasks;\\nusing MyApp.Auth;\\nusing Microsoft.AspNetCore.Http;\\nusing Microsoft.AspNetCore.Mvc;\\nusing Microsoft.Azure.WebJobs;\\nusing Microsoft.Azure.WebJobs.Extensions.Http;\\n\\nnamespace MyApp.Functions\\n{\\n    public class GetClaimsPrincipalFunction\\n    {\\n        private readonly IPrincipalService _principalService;\\n\\n        public GetClaimsPrincipalFunction(\\n            IPrincipalService principalService\\n        )\\n        {\\n            _principalService = principalService;\\n        }\\n\\n        [FunctionName(nameof(GetPrincipal))]\\n        public async Task<IActionResult> GetPrincipal(\\n            [HttpTrigger(AuthorizationLevel.Anonymous, \\"get\\", Route = \\"get-principal\\")] HttpRequest request\\n        )\\n        {\\n            var principal = await _principalService.GetPrincipal(request);\\n            var identity = principal?.Identity;\\n            var data = new\\n            {\\n                Name = identity?.Name ?? \\"\\",\\n                AuthenticationType = identity?.AuthenticationType ?? \\"\\",\\n                Claims = principal?.Claims.Select(c => new { c.Type, c.Value }),\\n            };\\n\\n            return new OkObjectResult(data);\\n        }\\n\\n        [FunctionName(nameof(AmIInRole))]\\n        public async Task<IActionResult> AmIInRole(\\n            [HttpTrigger(AuthorizationLevel.Anonymous, \\"get\\", Route = \\"am-i-in-role\\")] HttpRequest request\\n        )\\n        {\\n            var role = request.Query[\\"role\\"].FirstOrDefault();\\n\\n            if (string.IsNullOrEmpty(role))\\n                return new BadRequestObjectResult(\\"role query parameter is required\\");\\n\\n            var principal = await _principalService.GetPrincipal(request);\\n\\n            var isInRole = principal?.IsInRole(role) == true;\\n            if (!isInRole)\\n                return new ObjectResult($\\"Forbidden for {role}\\")\\n                {\\n                    StatusCode = Status403Forbidden\\n                };\\n\\n            return new OkObjectResult($\\"Welcome {principal?.Identity?.Name} - you have role {role}!\\");\\n        }\\n    }\\n}\\n```\\n\\nThe above class has 2 functions:\\n\\n- `GetPrincipal` - returns the `ClaimsPrincipal` object as JSON\\n- `AmIInRole` - takes a `role` query parameter, tests if a user has that role and returns a 403 if they don\'t and a 200 with a welcome message if they do\\n\\n### `GetPrincipal` - what claims do we have?\\n\\nLet\'s try out the `GetPrincipal` function, when I go to the `/api/get-principal` endpoint I see this:\\n\\n```json\\n{\\n  \\"name\\": \\"johnny_reilly@hotmail.com\\",\\n  \\"authenticationType\\": \\"aad\\",\\n  \\"claims\\": [\\n    {\\n      \\"type\\": \\"http://schemas.xmlsoap.org/ws/2005/05/identity/claims/nameidentifier\\",\\n      \\"value\\": \\"d9178465-3847-4d98-9d23-b8b9e403b323\\"\\n    },\\n    {\\n      \\"type\\": \\"http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name\\",\\n      \\"value\\": \\"johnny_reilly@hotmail.com\\"\\n    },\\n    {\\n      \\"type\\": \\"http://schemas.microsoft.com/ws/2008/06/identity/claims/role\\",\\n      \\"value\\": \\"authenticated\\"\\n    },\\n    {\\n      \\"type\\": \\"http://schemas.microsoft.com/ws/2008/06/identity/claims/role\\",\\n      \\"value\\": \\"OurApp.Read\\"\\n    }\\n  ]\\n}\\n```\\n\\nThis isn\'t the _same_ information as the Static Web Apps principal, but it\'s close enough for our purposes. Crucially, we can see the AppRoleAssignment `OurApp.Read` that we assigned to our user in the Azure Portal. That is the key information that we need, and that we are missing by default.\\n\\nCrucially this is enough information for us to be able to apply authorization to our functions.\\n\\n### `AmIInRole` - test `IsInRole` functionality\\n\\nWe can demonstrate applying authorization by using the `AmIInRole` function. This internally uses the inbuilt [`IsInRole`](https://learn.microsoft.com/en-us/dotnet/api/system.security.claims.claimsprincipal.isinrole?view=net-6.0) functionality of the `ClaimsPrincipal` object, and returns an appropriate API result accordingly.\\n\\nIf I go to the `/api/am-i-in-role?role=OurApp.Read` endpoint I get a 200 status code and the message: `Welcome johnny_reilly@hotmail.com - you have role OurApp.Read!`. This makes sense, my user account has the `OurApp.Read` role.\\n\\nLet\'s test that we also deny access appropriately. There is an `OurApp.Write` role; my account does not have this. If I go to the `/api/am-i-in-role?role=OurApp.Write` endpoint I get a 403 status code and the message: `Forbidden for OurApp.Write`.\\n\\nIt works!\\n\\n## Conclusion\\n\\nWe\'ve demonstrated a way to acquire a `ClaimsPrincipal` object that contains the AppRoleAssignments for a user. This is enough information for us to be able to apply authorization to our functions.\\n\\nIt would be ideal if this wasn\'t required, and I\'m hoping that the Static Web Apps team will be able to provide a solution for this in the future. [Keep an eye on this GitHub issue.](https://github.com/Azure/static-web-apps/issues/988) In the meantime, this is a workable solution.\\n\\nThanks to [Warren Joubert](https://github.com/warrenandre) for his help with this post."},{"id":"debugging-azure-functions-vs-code-mac-os","metadata":{"permalink":"/debugging-azure-functions-vs-code-mac-os","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-11-11-debugging-azure-functions-vs-code-mac-os/index.md","source":"@site/blog/2022-11-11-debugging-azure-functions-vs-code-mac-os/index.md","title":"Debugging Azure Functions in VS Code on Mac OS","description":"The VS Code debugging mechanism for Azure Functions on Mac OS frequently breaks. This post documents an approach to get it working.","date":"2022-11-11T00:00:00.000Z","tags":[{"inline":false,"label":"Azure Functions","permalink":"/tags/azure-functions","description":"The Azure Functions service."},{"inline":false,"label":"VS Code","permalink":"/tags/vs-code","description":"The Visual Studio Code editor."}],"readingTime":2.08,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"debugging-azure-functions-vs-code-mac-os","title":"Debugging Azure Functions in VS Code on Mac OS","authors":"johnnyreilly","tags":["azure functions","vs code"],"image":"./title-image.png","description":"The VS Code debugging mechanism for Azure Functions on Mac OS frequently breaks. This post documents an approach to get it working.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Azure AD Claims with Static Web Apps and Azure Functions","permalink":"/azure-ad-claims-static-web-apps-azure-functions"},"nextItem":{"title":"Getting started with the Web Monetization API","permalink":"/web-monetization-api"}},"content":"VS Code\'s debugging mechanism for Azure Functions on Mac OS frequently breaks. This post documents an approach to get it working.\\n\\n![title image reading \\"Debugging Azure Functions in VS Code on Mac OS\\" with Docusaurus, SWC and webpack logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## The problem\\n\\nI frequently use a Mac to develop Azure Functions. I use VS Code as my editor.\\n\\nDebugging is can be very useful when you\'re developing; getting to understand what the computer can see at runtime is a superpower. Regrettably with Azure Functions, I often find that the debugger fails to attach. When this happens, I can\'t actually debug my Azure Functions.\\n\\nThis is a known issue. In fact, this blog post is me sharing a workaround that I\'ve needed again and again, but keep losing. Not my own work, the work of [Bas Stottelaar](https://github.com/basilfx). I share it as a public service announcement - and to remind myself how to do it! [The original issue (and workaround) is here](https://github.com/OmniSharp/omnisharp-vscode/issues/4903#issuecomment-993015843). Yay Bas!\\n\\nThere appears to be something wrong with the standard code signing of `vsdbg` and / or `vsdbg-ui`. The workaround is to sign the binaries yourself.\\n\\n## The workaround\\n\\nYou\'ll first need to generate a self signed certificate to be used for code signing. [There\'s a good resource on Stack Overflow covering this](https://stackoverflow.com/a/58363510/761388). You should only ever need to do this once. You can then use the same certificate every time you apply the workaround.\\n\\nIn fact it\'s probably worth emphasising that you\'ll likely need to apply this workaround again and again. It\'s not a permanent fix. The workaround script that you need to run is:\\n\\n```bash\\ncd ~/.vscode/extensions/ms-dotnettools.csharp-1.25.2-darwin-x64/.debugger/x86_64\\ncodesign --remove-signature vsdbg-ui && codesign --remove-signature vsdbg\\ncodesign -s my-codesign-cert vsdbg-ui && codesign -s my-codesign-cert vsdbg\\n```\\n\\nA thing to note about the above is the version in the path. You\'ll need to change that to match the version of the C# extension that you have installed. You can find the version in the VS Code extensions view:\\n\\n![Screenshot of C# extension in VS Code; in this case version 1.25.2](screenshot-csharp-extension-vs-code.webp)\\n\\nIn this case the version is `1.25.2`; as is reflected in the path above.\\n\\nOnce the script has been run, I\'ve found that restarting VS Code is a good idea. Regrettably the \\"I cannot debug my Azure Functions\\" issue is likely to reoccur in future. When it does, the workaround will need to be reapplied.\\n\\nIn the long term, I\'d love to see some debugging improvements for Azure Functions. Until that time, we have this."},{"id":"web-monetization-api","metadata":{"permalink":"/web-monetization-api","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-10-20-web-monetization-api/index.md","source":"@site/blog/2022-10-20-web-monetization-api/index.md","title":"Getting started with the Web Monetization API","description":"The Web Monetization API is a browser API streams payment from the browser to the website. This post walks through getting started adding it to a site.","date":"2022-10-20T00:00:00.000Z","tags":[],"readingTime":7.68,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"web-monetization-api","title":"Getting started with the Web Monetization API","authors":"johnnyreilly","tags":[],"image":"./title-image.png","description":"The Web Monetization API is a browser API streams payment from the browser to the website. This post walks through getting started adding it to a site.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Debugging Azure Functions in VS Code on Mac OS","permalink":"/debugging-azure-functions-vs-code-mac-os"},"nextItem":{"title":"Bicep: Static Web Apps and Linked Backends","permalink":"/bicep-static-web-apps-linked-backends"}},"content":"The Web Monetization API is a JavaScript browser API that allows the creation of a payment stream from the user agent to the website. This post walks through getting started adding it to a site.\\n\\n![title image reading \\"Web Monetization API - getting started\\" with the Web Monetization logo](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## The Web Monetization API\\n\\nOver the summer I attended the [HalfStack at the Beach](https://halfstackconf.com/newquay/) conference and heard a talk from [Alex Lakatos](https://twitter.com/avolakatos) on the Web Monetization API. I hadn\'t heard about this API previously; it turns out it is a new way to monetize a website. My own blog already featured a [Buy Me a Coffee](https://www.buymeacoffee.com/qUBm0Wh) link, which allows generous people to send me small amounts of money if they\'ve found something I\'ve written useful. The Web Monetization API appears to be that, but built into the browser and proposed as a W3C standard at the [Web Platform Incubator Community Group](https://discourse.wicg.io/t/proposal-web-monetization-a-new-revenue-model-for-the-web/3785).\\n\\nI was intrigued by the Web Monetization API. Alex was kind enough to share some links with me, and I decided to take it for a spin; to try out using it and to document the findings. This post is going to be exactly that. It\'s written from the perspective of someone who doesn\'t know the Web Monetization API save for what they\'ve heard in a talk. Over the course of this post I\'ll try to get to know it a little better, and try to integrate it into [my blog](https://johnnyreilly.com). As I do that I\'ll share what I\'m doing and how I found things; to try to provide a useful resource (and some feedback) on what adoption feels like.\\n\\nI\'ll start with the https://webmonetization.org/ site - in there I found a [quick start](https://webmonetization.org/docs/getting-started) which I decided to work through.\\n\\n## Wallet\\n\\nThe first thing to do, if you\'d like to adopt Web Monetization, is [set up a wallet](https://webmonetization.org/docs/getting-started#1-set-up-a-web-monetized-wallet). This allows you to receive money from people - it\'s a bank account essentially; one that supports integration with Web Monetization. There appeared to be two options for this:\\n\\n- [uphold](https://wallet.uphold.com/)\\n- [gatehub](https://gatehub.net/)\\n\\n[Right now, uphold offers a greater number of features](https://webmonetization.org/docs/ilp-wallets/#digital-wallets), so decided to create a wallet with them.\\n\\n## Uphold\\n\\nThe signup process was pretty straightforward. I got slightly confused was seeing this prompt:\\n\\n![screenshot reading \\"How will you use Uphold? ... Trade cryptocurrencies, Currency conversion, Deposit or withdraw cryptocurrencies, Transfers between users\\"](./screenshot-uphold-purpose.webp)\\n\\nI wasn\'t entirely sure what I needed. The Web Monetization API seemed most likely to be about transfers between users, so I went with that.\\n\\nWhen it asked this question:\\n\\n![screenshot reading \\"International payments/transfers Tell us where you\'ll be moving money. Select from the regions below.\\"](./screenshot-uphold-transfers.webp)\\n\\nI opted to accept all regions. After the usual signup process, I was able to see able to see my new (empty) account:\\n\\n![screenshot of the dashboard of uphold with a balance of \xa30](./screenshot-uphold-dashboard.webp)\\n\\n## Payment pointer\\n\\nThe next thing we needed to do was acquire our payment pointer. This was a little tricky to track down and eventually Alex showed me where to go. On the right hand side of the dashboard, there is an \\"anything to anything\\" section:\\n\\n![gif of the payment pointer found in uphold](./payment-pointer.gif)\\n\\nClicking on the \\"copy\\" button copies the payment pointer to the clipboard. I\'ll need this later. In my case that is: `$ilp.uphold.com/LwQQhXdpwxeJ`.\\n\\nYou might be looking at the payment pointer and thinking, \\"that looks kinda URL-y\\" ... And you\'d be be right! Because `$ilp.uphold.com/LwQQhXdpwxeJ` is equivalent to this URL: `https://ilp.uphold.com/LwQQhXdpwxeJ`. We just swap out the `$` for `https://`.\\n\\n## Monetization link tag\\n\\nThe next thing to do is to make a `link` tag using the payment pointer. This is the tag that will tell the browser that the page supports Web Monetization. That `link` tag should live in every page of our Web Monetized site. The tag looks like this:\\n\\n```html\\n<link rel=\\"monetization\\" href=\\"https://ilp.uphold.com/LwQQhXdpwxeJ\\" />\\n```\\n\\nAs you can see, the `href` attribute is the payment pointer we just acquired; in its \\"https\\" form.\\n\\n## Docusaurus link tag: updated 30/10/2022\\n\\nThe final step here would be adding this `link` tag to the pages served up by our site. In my case, I use Docusaurus to power my blog. To add an extra `link` tag with Docusaurus we need to add it to the [`docusaurus.config.js`](https://docusaurus.io/docs/next/seo#global-metadata) file.\\n\\nIf you\'re using [Docusaurus 2.2 or greater](https://docusaurus.io/blog/releases/2.2#config-headtags) you can use the new [`headTags` API](https://docusaurus.io/docs/api/docusaurus-config#headTags). Usage looks like this:\\n\\n```js\\nmodule.exports = {\\n  // ...\\n  headTags: [\\n    {\\n      tagName: \'link\',\\n      attributes: {\\n        rel: \'monetization\',\\n        href: \'https://ilp.uphold.com/LwQQhXdpwxeJ\',\\n      },\\n    },\\n    // This will become <link rel=\\"monetization\\" href=\\"https://ilp.uphold.com/LwQQhXdpwxeJ\\" /> in the generated HTML\\n  ],\\n  // ...\\n};\\n```\\n\\nIf you\'re using an older version of Docusaurus, you can the syntax for adding an extra `link` tag in the head comes in the form of a mini plugin:\\n\\n```js\\nmodule.exports = {\\n  // ...\\n  plugins: [\\n    // ...\\n    function extraHeadTagsPlugin(context, options) {\\n      return {\\n        name: \'extra-head-tags-plugin\',\\n        injectHtmlTags({ content }) {\\n          return {\\n            headTags: [\\n              {\\n                tagName: \'link\',\\n                attributes: {\\n                  rel: \'monetization\',\\n                  href: \'https://ilp.uphold.com/LwQQhXdpwxeJ\',\\n                },\\n                // This will become <link rel=\\"monetization\\" href=\\"https://ilp.uphold.com/LwQQhXdpwxeJ\\" /> in the generated HTML\\n              },\\n            ],\\n          };\\n        },\\n      };\\n    },\\n    // ...\\n  ],\\n};\\n```\\n\\nIt\'s also worth knowing that historically the Web Monetization API used a `meta` tag instead of a `link` tag - and that tag used the `$` prefix instead of `https://`. That tag looked like this:\\n\\n```html\\n<meta name=\\"monetization\\" content=\\"$ilp.uphold.com/LwQQhXdpwxeJ\\" />\\n```\\n\\nBut the `link` tag is the current standard, and that\'s what you should look to adopt.\\n\\n## Hello world Web Monetization API?\\n\\nWith this done, my site is web monetized! Or at least... I think it is... What does that mean? Well, I wasn\'t entirely sure. I reached out to Alex again, showed him my site and said \\"does this work?\\" He said:\\n\\n![screenshot of conversation with Alex on Twitter, him saying \\"Hey John. That\'s it! I just sent you a little tip on uphold, if you\'ve set that up correctly, you\'ll see it in your account\\"](screenshot-am-i-doing-it-right-alex.webp)\\n\\nAnd sure enough, I found Alex had indeed sent me the princely sum of 83 pence ($1) on Uphold... It had worked!\\n\\n## Coil\\n\\nIt turned out that Alex had used a browser extension called [Coil](https://coil.com/) to send me the money. It\'s a browser extension that allows you to send money to websites that support Web Monetization. It\'s a bit like a browser based Patreon or Buy Me a Coffee. But slightly different; [to quote their docs](https://help.coil.com/docs/general-info/intro-to-coil/index.html#how-is-coil-different-from-other-membership-services-like-patreon-and-flattr):\\n\\n> With services like Patreon, you select which creators to support, then pay each creator separately, depending on the membership plans they offer. Coil streams payments in real time to any web monetized sites you visit.\\n\\nSo people can explicitly tip a website using Coil, or they can just use Coil to browse the web and the website will get a small amount of money from Coil. For years I\'ve heard whispers of \\"micropayments are the missing piece of the web\\" - this seemed to be solving that problem and I was intrigued.\\n\\nI\'d set up an Uphold account so I could receive money from other people. Coil is like the flipside of that; it would let me send money to other people. You need that money to come from somewhere. It turned out that I could set up a Coil account using the Uphold account I\'d just created:\\n\\n![screenshot of entering my uphold payment pointer into my coil account](screenshot-setting-up-coil.png)\\n\\nSo that\'s what I did. I entered my payment pointer into Coil and now I can send money to other people\'s sites that support Web Monetization. But what does that look like? Well, I decided to try it out on my own site. I installed the [Coil browser extension](https://coil.com/) and then went to my site and gave it a whirl:\\n\\n![GIF of tipping myself $1 using Coil](tipping-with-coil.gif)\\n\\nI went to my blog and sure enough, I was able to send a tip to myself. When I flipped over to my Uphold account, I could see that the money was on its way!\\n\\n![screenshot of uphold including details of an incoming payment of $1 or 93 pence](screenshot-uphold-incoming-tip.webp)\\n\\nJust as Alex had been able to send me $1 on September 4th, I was able to send myself $1 on September 10th! (Incidentally, the shift in amount from 83 pence to 93 pence between transactions is purely due to the changing value exchange rate between GBP and USD. At present the Pound is decreasing in value against the Dollar, so the amount of money I received in GBP when I tipped myself $1 worked out to be more than when Alex did.)\\n\\n## Conclusion\\n\\nIn this post we have got to know the Web Monetization API, we\'ve used it to monetize our own site and we\'ve used it to tip ourselves. We\'ve also seen how Coil works and how it can be used to tip other people\'s sites. I\'m excited to see how this develops. It feels like a way to support people who are making things we care about on the web.\\n\\nThanks so much to [Alex Lakatos](https://twitter.com/avolakatos) for telling me about this in the first place and for answering all my questions!\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/getting-started-web-monetization-api/)\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.logrocket.com/getting-started-web-monetization-api/\\" />\\n</head>"},{"id":"bicep-static-web-apps-linked-backends","metadata":{"permalink":"/bicep-static-web-apps-linked-backends","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-10-14-bicep-static-web-apps-linked-backends/index.md","source":"@site/blog/2022-10-14-bicep-static-web-apps-linked-backends/index.md","title":"Bicep: Static Web Apps and Linked Backends","description":"Azure Static Web Apps can be linked to Azure Functions, Azure Container Apps etc to provide the linked backend for a site. This post provisions with Bicep.","date":"2022-10-14T00:00:00.000Z","tags":[{"inline":false,"label":"Bicep","permalink":"/tags/bicep","description":"The Bicep language for Azure Resource Manager templates."},{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."},{"inline":false,"label":"Azure Static Web Apps","permalink":"/tags/azure-static-web-apps","description":"The Azure Static Web Apps service."}],"readingTime":3.14,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"bicep-static-web-apps-linked-backends","title":"Bicep: Static Web Apps and Linked Backends","authors":"johnnyreilly","tags":["bicep","azure","azure static web apps"],"image":"./title-image.png","description":"Azure Static Web Apps can be linked to Azure Functions, Azure Container Apps etc to provide the linked backend for a site. This post provisions with Bicep.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Getting started with the Web Monetization API","permalink":"/web-monetization-api"},"nextItem":{"title":"TypeScript Unit Tests with Debug Support","permalink":"/typescript-unit-tests-with-debug-support"}},"content":"Azure Static Web Apps can be linked to Azure Functions, Azure Container Apps etc to provide the linked backend for a site. This post will demonstrate how to do this with Bicep.\\n\\n![title image reading \\"Bicep: Static Web Apps and Linked Backends\\" with Bicep and Static Web App logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nAzure Static Web Apps ship with their own slightly restricted Azure Functions backend; it does not have all of the triggers of the standard offering. If you should need that wider featureset, you can link to an existing Azure Functions instance instead. This is known as the \\"bring your own functions\\" approach and is [documented here](https://learn.microsoft.com/en-us/azure/static-web-apps/functions-bring-your-own). The back end doesn\'t have to be Azure Functions; it could be Azure Container Apps also. This post will demonstrate how to do this with Azure Functions and with Bicep.\\n\\n## The Function App Bicep\\n\\nYou\'re going to need to create an Azure Function in your Bicep template. We\'ll do this here with a Bicep module called `function.bicep`:\\n\\n```bicep\\nparam functionAppName string\\nparam location string\\nparam hostingPlanName string\\nparam storageAccountName string\\nparam tags object\\n\\nresource functionApp \'Microsoft.Web/sites@2022-03-01\' = {\\n  name: functionAppName\\n  kind: \'functionapp,linux\'\\n  location: location\\n  tags: tags\\n  properties: {\\n    siteConfig: {\\n      appSettings: [\\n        {\\n          name: \'FUNCTIONS_EXTENSION_VERSION\'\\n          value: \'~4\'\\n        }\\n        {\\n          name: \'FUNCTIONS_WORKER_RUNTIME\'\\n          value: \'node\'\\n        }\\n        {\\n          name: \'AzureWebJobsStorage\'\\n          value: \'DefaultEndpointsProtocol=https;AccountName=${storageAccount.name};EndpointSuffix=${environment().suffixes.storage};AccountKey=${storageAccount.listKeys().keys[0].value}\'\\n        }\\n        {\\n          name: \'WEBSITE_CONTENTAZUREFILECONNECTIONSTRING\'\\n          value: \'DefaultEndpointsProtocol=https;AccountName=${storageAccount.name};EndpointSuffix=${environment().suffixes.storage};AccountKey=${storageAccount.listKeys().keys[0].value}\'\\n        }\\n        {\\n          name: \'WEBSITE_CONTENTSHARE\'\\n          value: \'${toLower(functionAppName)}a6e3\'\\n        }\\n      ]\\n      cors: {\\n        allowedOrigins: [\\n          \'https://portal.azure.com\'\\n        ]\\n      }\\n      use32BitWorkerProcess: false\\n      ftpsState: \'FtpsOnly\'\\n      linuxFxVersion: \'Node|16\'\\n    }\\n    serverFarmId: serverFarm.id\\n    clientAffinityEnabled: false\\n    httpsOnly: true\\n  }\\n}\\n\\nresource serverFarm \'Microsoft.Web/serverfarms@2022-03-01\' = {\\n  name: hostingPlanName\\n  location: location\\n  kind: \'linux\'\\n  tags: {}\\n  properties: {\\n    reserved: true\\n  }\\n  sku: {\\n    tier: \'Dynamic\'\\n    name: \'Y1\'\\n  }\\n  dependsOn: []\\n}\\n\\nresource storageAccount \'Microsoft.Storage/storageAccounts@2022-05-01\' = {\\n  name: storageAccountName\\n  location: location\\n  tags: {}\\n  sku: {\\n    name: \'Standard_LRS\'\\n  }\\n  properties: {\\n    supportsHttpsTrafficOnly: true\\n    minimumTlsVersion: \'TLS1_2\'\\n  }\\n  kind: \'StorageV2\'\\n}\\n\\noutput functionAppResourceId string = functionApp.id\\n```\\n\\nIt also creates a storage account and a server farm to support the function app. You\'ll note it exports the resource name of the function app. We\'ll use this in the next step.\\n\\n## The Static Web App Bicep\\n\\nIn our main Bicep template we\'ll create a static web app and link it to the function app we created in the previous step. We\'ll do this with a Bicep module called `main.bicep`:\\n\\n```bicep\\nparam location string\\nparam branch string\\nparam staticWebAppName string\\nparam functionAppName string\\nparam hostingPlanName string\\nparam storageAccountName string\\nparam tags object\\n@secure()\\nparam repositoryToken string\\nparam customDomainName string\\n\\nmodule functionApp \'function.bicep\' = {\\n  name: \'functionApp\'\\n  params: {\\n    location: location\\n    tags: tags\\n    functionAppName: functionAppName\\n    hostingPlanName: hostingPlanName\\n    storageAccountName: storageAccountName\\n  }\\n}\\n\\nresource staticWebApp \'Microsoft.Web/staticSites@2021-02-01\' = {\\n  name: staticWebAppName\\n  location: location\\n  tags: tags\\n  sku: {\\n    // Free doesn\'t work with linked backends\\n    name: \'Standard\'\\n    tier: \'Standard\'\\n  }\\n  properties: {\\n    repositoryUrl: \'https://github.com/johnnyreilly/blog.johnnyreilly.com\'\\n    repositoryToken: repositoryToken\\n    branch: branch\\n    provider: \'GitHub\'\\n    stagingEnvironmentPolicy: \'Enabled\'\\n    allowConfigFileUpdates: true\\n    buildProperties:{\\n      skipGithubActionWorkflowGeneration: true\\n    }\\n  }\\n}\\n\\nresource customDomain \'Microsoft.Web/staticSites/customDomains@2021-02-01\' = {\\n  parent: staticWebApp\\n  name: customDomainName\\n  properties: {}\\n}\\n\\nresource staticWebAppBackend \'Microsoft.Web/staticSites/linkedBackends@2022-03-01\' = {\\n  name: \'${staticWebAppName}/backend\'\\n  properties: {\\n    backendResourceId: functionApp.outputs.functionAppResourceId\\n    region: location\\n  }\\n}\\n\\noutput staticWebAppDefaultHostName string = staticWebApp.properties.defaultHostname // eg gentle-bush-0db02ce03.azurestaticapps.net\\noutput staticWebAppId string = staticWebApp.id\\noutput staticWebAppName string = staticWebApp.name\\n```\\n\\nThe crucial bit above is this:\\n\\n```bicep\\nresource staticWebAppBackend \'Microsoft.Web/staticSites/linkedBackends@2022-03-01\' = {\\n  name: \'${staticWebAppName}/backend\'\\n  properties: {\\n    backendResourceId: functionApp.outputs.functionAppResourceId\\n    region: location\\n  }\\n}\\n```\\n\\nThis links the static web app to the function app we created in the previous step. We use the `functionApp.outputs.functionAppResourceId` to get the resource ID of the function app from our module.\\n\\n## The Deployment\\n\\nOnce this is deployed to Azure, if you click on the APIs section of the static web app you\'ll see the function app is now linked:\\n\\n![The function app is now linked to the static web app as demonstrated in the Azure Portal](screenshot-azure-portal-linked-backend.webp)"},{"id":"typescript-unit-tests-with-debug-support","metadata":{"permalink":"/typescript-unit-tests-with-debug-support","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-10-01-typescript-unit-tests-with-debug-support/index.md","source":"@site/blog/2022-10-01-typescript-unit-tests-with-debug-support/index.md","title":"TypeScript Unit Tests with Debug Support","description":"Unit tests are an important part of the development process. This post will outline how to write unit tests using TypeScript and how to debug them as well.","date":"2022-10-01T00:00:00.000Z","tags":[{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."},{"inline":false,"label":"Automated Testing","permalink":"/tags/automated-testing","description":"How to perform the automation of tests."}],"readingTime":6.34,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"typescript-unit-tests-with-debug-support","title":"TypeScript Unit Tests with Debug Support","authors":"johnnyreilly","tags":["typescript","automated testing"],"image":"./title-image.png","description":"Unit tests are an important part of the development process. This post will outline how to write unit tests using TypeScript and how to debug them as well.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Bicep: Static Web Apps and Linked Backends","permalink":"/bicep-static-web-apps-linked-backends"},"nextItem":{"title":"Faster Docusaurus builds with swc-loader","permalink":"/faster-docusaurus-build-swc-loader"}},"content":"<head>\\n    <link rel=\\"canonical\\" href=\\"https://meticulous.ai/blog/typescript-unit-tests-with-debugging/\\" />\\n</head>\\n\\n\x3c!--truncate--\x3e\\n\\nUnit tests are an important part of the development process. They are used to verify that the code is working as intended. This post will outline how to write unit tests using TypeScript and how to debug them as well.\\n\\n![title image reading \\"TypeScript Unit Tests with Debug Support\\" with TypeScript and Jest logos](title-image.png)\\n\\n## Unit Tests\\n\\nWhen we are writing unit tests to verify system behaviour, we have to make choices. We need to choose the test framework that we\'ll use to run our tests. In the JavaScript world we\'ll be choosing from options including Jest, Mocha, tape, Jasmine and others. There are numerous other testing tools like Cypress and Playwright which cover broader automated testing needs, but we\'re intentionally just thinking about unit tests right now and so we\'ll exclude those.\\n\\nOf the various choices available, Jest is (at time of writing) very much the most popular. Since we have do not have a particular reason for favouring one of the frameworks that isn\'t as popular as Jest, that\'s what we\'ll use.\\n\\nTests are a wonderful tool for asserting system behaviour. However, they can fail for mysterious reasons. When that happens, it can be helpful to see what the computer can see. It can be helpful to be able to debug your tests in the way you might hope to debug your other code.\\n\\nIn this post:\\n\\n1. We\'ll set up a TypeScript Node.js project, containing some code we\'d like to test.\\n2. We\'ll configure our project to work with Jest and we\'ll write a test.\\n3. We\'ll debug our unit test.\\n\\nLet\'s begin.\\n\\n## Setting up our TypeScript project\\n\\nFirst we\'ll create ourselves a new Node.js project:\\n\\n```shell\\nmkdir typescript-unit-tests-with-debug-support\\ncd typescript-unit-tests-with-debug-support\\nnpm init --yes\\n```\\n\\nAt this point we have an empty Node.js project. Let\'s add TypeScript to it as a dependency and initialise our TypeScript project:\\n\\n```shell\\nnpm install typescript\\nnpx -p typescript tsc --init\\n```\\n\\nWe now have a fully working TypeScript Node.js project and we\'re ready to start writing some code!\\n\\nThis is a post about demonstrating unit testing with TypeScript. So naturally we need something to test. We\'re going write a simple module called `greeter.ts` which has the following content:\\n\\n```ts\\nexport function makeGreeting(name: string): string {\\n  const lengthOfName = name.length;\\n  const greeting = `Well hello there ${name}, I see your name is ${lengthOfName} characters long!`;\\n  return greeting;\\n}\\n```\\n\\n`greeter.ts` is a TypeScript file that contains a single simple function. The `makeGreeting` function takes a string parameter and, over a number of lines, constructs a greeting string which the function returns. The nature of the greeting is inconsequential. However, remember later we want to be able to debug our test. We\'ve intentionally written a function featuring more than one line of code. We\'ve done this so we can demonstrate the benefits of debugging by showing the program state as it is in the process of executing.\\n\\n## Setting up the Jest project\\n\\nThe next step after setting up our TypeScript Node.js project, is adding tests, and the ability to run them, using Jest.\\n\\nFirst of all we\'re going to need to add Jest to our project and initially configure it:\\n\\n```shell\\nnpm install --save-dev jest\\nnpx jest --init\\n```\\n\\nAs part of the initialisation you should be prompted with a number of questions:\\n\\n```\\nnpx jest --init\\n\\nThe following questions will help Jest to create a suitable configuration for your project\\n\\n\u2714 Would you like to use Typescript for the configuration file? \u2026 no\\n\u2714 Choose the test environment that will be used for testing \u203A node\\n\u2714 Do you want Jest to add coverage reports? \u2026 no\\n\u2714 Which provider should be used to instrument code for coverage? \u203A v8\\n\u2714 Automatically clear mock calls, instances, contexts and results before every test? \u2026 no\\n```\\n\\nWe\'ll select all the defaults; including _not_ using TypeScript for the configuration file. We don\'t require a configuration file written in TypeScript to be able to write TypeScript tests. The initialisation will create a `jest.config.js` file which contains the configuration used to run our tests.\\n\\nNext, we\'ll update the `scripts` section of our `package.json` to invoke Jest:\\n\\n```json\\n  \\"scripts\\": {\\n    \\"test\\": \\"jest\\"\\n  },\\n```\\n\\nAt this point we\'re in a place where we can run tests written in JavaScript. But we want to run tests written in TypeScript. [Jest supports this scenario well](https://jestjs.io/docs/getting-started#using-typescript), using Babel. So we\'ll add the dependencies we need:\\n\\n```shell\\nnpm install --save-dev babel-jest @babel/core @babel/preset-env @babel/preset-typescript @types/jest\\n```\\n\\nWith all that done, let\'s see if we can write a test. We\'ll create a `greeter.test.ts` file to sit alongside `greeter.ts`:\\n\\n```ts\\nimport { makeGreeting } from \'./greeter\';\\n\\ntest(\'given a name produces the expected greeting\', () => {\\n  expect(makeGreeting(\'George\')).toBe(\\n    \'Well hello there George, I see your name is 6 characters long!\',\\n  );\\n});\\n```\\n\\nThis simple test, invokes the `makeGreeting` function in our `greeter.ts` file and asserts the return value is as expected. Let us see if we can run our test with `npm run test`:\\n\\n![screenshot of tests running and passing in the terminal](./screenshot-of-tests-passing.png)\\n\\nSuccess! We\'ve now created a TypeScript project, written a function, written a test for that function and we have the ability to run it.\\n\\n## Set up debugging support\\n\\nThe final thing we wanted to tackle was adding debug support. In times past, this was often quite tricky to configure. However, debugging has become much easier due to the excellent [`vscode-jest`](https://github.com/jest-community/vscode-jest) project, which is dedicated to making \\"testing more intuitive and fun\\". In fact, with this extension the experience is now very \\"plug and play\\" which is a great thing.\\n\\nInside VS Code, we will install the vscode-jest extension:\\n\\n![screenshot of the VS Code Jest extension](./screenshot-of-vscode-jest.png)\\n\\nOnce it\'s installed, we\'ll need to restart VS Code, and we may also need to enter the `Jest: Start All Runners` command in VS Codes power bar:\\n\\n![screenshot of the Jest: Start All Runners command in VS Code](./screenshot-jest-start-all-runners.png)\\n\\nOnce the Jest runners have started, we start to see the benefits that the VS Code Jest plugin offers. Where tests exist in our code, they are detected by the plugin and run. Depending upon whether tests are passing or failing we will be presented with a red cross or a green tick denoting failure or success directly alongside the code:\\n\\n![screenshot of the jest test explorer with a green tick next to a passing test](./screenshot-jest-test-explorer.png)\\n\\nUsing the test explorer, it\'s possible to run tests on demand. Even more excitingly, it\'s now possible to debug them too. If you examine the test explorer and right / command click on a given test, you\'ll be presented with the option to debug a test:\\n\\n![screenshot of the context menu in the Jest explorer featuring the words \\"Debug Test\\"](./screenshot-jest-test-explorer-debug-test.png)\\n\\nExcitingly this means exactly what we might hope. If we put breakpoints in our code, when the test runs we\'ll now hit them. We\'ll be able to debug and introspect each test that runs:\\n\\n![screenshot of a test being debugged](./screenshot-jest-debug-test.png)\\n\\nIf you look at the screenshot above you\'ll see we\'ve stopped on a breakpoint, we\'re able to examine the context of the program at the point that it has paused. We can step further on in our code, we can do all the useful things that debugging affords us. We have succeeded in debugging.\\n\\n## Conclusion\\n\\nIn this piece we\'ve taken a look at how to get up and running with a unit testable TypeScript project. Beyond that, we\'ve demonstrated how we can debug our TypeScript tests using the VS Code editor.\\n\\n[This post was originally published on Meticulous.](https://meticulous.ai/blog/typescript-unit-tests-with-debugging/)"},{"id":"faster-docusaurus-build-swc-loader","metadata":{"permalink":"/faster-docusaurus-build-swc-loader","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-09-29-faster-docusaurus-build-swc-loader/index.md","source":"@site/blog/2022-09-29-faster-docusaurus-build-swc-loader/index.md","title":"Faster Docusaurus builds with swc-loader","description":"This post demonstrates how to speed up your Docusaurus build by using SWC and the `swc-loader` for webpack.","date":"2022-09-29T00:00:00.000Z","tags":[{"inline":false,"label":"Docusaurus","permalink":"/tags/docusaurus","description":"The Docusaurus static site generator."},{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."}],"readingTime":2.215,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"faster-docusaurus-build-swc-loader","title":"Faster Docusaurus builds with swc-loader","authors":"johnnyreilly","tags":["docusaurus","webpack"],"image":"./title-image.png","description":"This post demonstrates how to speed up your Docusaurus build by using SWC and the `swc-loader` for webpack.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"TypeScript Unit Tests with Debug Support","permalink":"/typescript-unit-tests-with-debug-support"},"nextItem":{"title":"React: storing state in URL with URLSearchParams","permalink":"/react-usesearchparamsstate"}},"content":"This post demonstrates how to speed up your Docusaurus build by using SWC and the `swc-loader` for webpack.\\n\\n![title image reading \\"Faster Docusaurus builds with swc-loader\\" with Docusaurus, SWC and webpack logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## SWC\\n\\nAt present there\'s a number of projects which have been providing alternate transpilation mechanisms to transform TypeScript / modern JavaScript into JavaScript that will run widely supported browsers. Historically this has been handled by tools like the TypeScript compiler itself and Babel. Both of these tools are written in TypeScript / JavaScript. The new tools and projects which have been appearing often use languages like Go and Rust which offer the gift of performance gains. Shorter build times in other words.\\n\\nWe\'re going to make use of [SWC (Speedy Web Compiler)](https://swc.rs/) to speed up the Docusaurus build. To quote the SWC docs:\\n\\n> SWC can be used for both compilation and bundling. For compilation, it takes JavaScript / TypeScript files using modern JavaScript features and outputs valid code that is supported by all major browsers.\\n>\\n> \uD83C\uDFCE SWC is 20x faster than Babel on a single thread and 70x faster on four cores.\\n\\nWe like faster! Interestingly, the Docusaurus site itself is built with SWC and has been since 19th March 2022. You can see [Josh Cena](https://twitter.com/SidaChen63)\'s [PR implementing SWC for Docusaurus here](https://github.com/facebook/docusaurus/pull/6944).\\n\\nHowever, by default, Docusaurus is built using Babel. This post will demonstrate how to make the switch. In fact as part of the PR that implements this post, this blog (also platformed on Docusaurus) will migrate from Babel to SWC. [See the blog post PR here](https://github.com/johnnyreilly/blog.johnnyreilly.com/pull/288).\\n\\n## Goodbye `babel-loader`, hello `swc-loader`\\n\\nDocusaurus is bundled using webpack. As a consequence, we need a tool to bridge the gap between webpack and SWC. That tool is the [`swc-loader`](https://github.com/swc-project/swc-loader).\\n\\nBy default, the Docusaurus build uses Babel for its build. Let\'s add `swc-loader` and `@swc/core` to the project:\\n\\n```bash\\nyarn add @swc/core swc-loader\\n```\\n\\nWith those in place, we\'re now able to tweak our the webpack config in `docusaurus.config.js` to use `swc-loader` instead of `babel-loader`:\\n\\n```js\\nconst config = {\\n  // ....\\n\\n  webpack: {\\n    jsLoader: (isServer) => ({\\n      loader: require.resolve(\'swc-loader\'),\\n      options: {\\n        jsc: {\\n          parser: {\\n            syntax: \'typescript\',\\n            tsx: true,\\n          },\\n          target: \'es2017\',\\n        },\\n        module: {\\n          type: isServer ? \'commonjs\' : \'es6\',\\n        },\\n      },\\n    }),\\n  },\\n\\n  // ....\\n};\\n```\\n\\n## Build times\\n\\nWith this in place, we\'re done. We can now run `yarn build` and see the difference in build times. On GitHub actions (where I build my blog), the build time for the blog site went from around 6 minutes to around 4 minutes. It\'s somewhat variable, but there\'s a definite improvement, and every little helps!"},{"id":"react-usesearchparamsstate","metadata":{"permalink":"/react-usesearchparamsstate","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-09-20-react-usesearchparamsstate/index.md","source":"@site/blog/2022-09-20-react-usesearchparamsstate/index.md","title":"React: storing state in URL with URLSearchParams","description":"The React `useState` hook is a great way to persist state. This post demos a simple React hook that stores state in the URL querystring.","date":"2022-09-20T00:00:00.000Z","tags":[{"inline":false,"label":"React","permalink":"/tags/react","description":"The React library."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":6.68,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"react-usesearchparamsstate","title":"React: storing state in URL with URLSearchParams","authors":"johnnyreilly","tags":["react","typescript"],"description":"The React `useState` hook is a great way to persist state. This post demos a simple React hook that stores state in the URL querystring.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Faster Docusaurus builds with swc-loader","permalink":"/faster-docusaurus-build-swc-loader"},"nextItem":{"title":"Reverse engineering the Azure Application Insights Transactions URL","permalink":"/reverse-engineering-azure-app-insights-transactions-url"}},"content":"The React [`useState`](https://reactjs.org/docs/hooks-reference.html#usestate) hook is a great way to persist state inside the context of a component in React. This post demonstrates a simple React hook that stores state in the URL querystring, building on top of React Routers `useSearchParams` hook.\\n\\n\x3c!--truncate--\x3e\\n\\n## `useState`\\n\\nUsage of the `useState` hook looks like this:\\n\\n```ts\\nconst [greeting, setGreeting] = useState(\'hello world\');\\n\\n// ....\\n\\nsetGreeting(\'hello John\'); // will set greeting to \'hello John\'\\n```\\n\\nHowever, there is a disadvantage to using `useState`; that state is not persistent and not shareable. So if you want someone else to see what you can see in an application, you\'re reliant on them carrying out the same actions that got your application into its current state. Doing that can be time consuming and error prone. Wouldn\'t it be great if there was a simple way to share state?\\n\\n## A stateful URL\\n\\nAn effective way to share state between users, without needing a backend for persistence, is with the URL. A URL can contain the required state in the form of the route and the querystring / search parameters. The search parameters are particularly powerful as they are entirely generic and customisable.\\n\\nThanks to the [URLSearchParams API](https://developer.mozilla.org/en-US/docs/Web/API/URLSearchParams), it\'s possible to manipulate the querystring _without_ round-tripping to the server. This is a primitive upon which we can build; as long as the URL limit (around [2000 chars](https://stackoverflow.com/a/417184/761388)) is not exceeded, we\'re free to persist state in your URL. Consider:\\n\\nhttps://our-app.com?greeting=hi\\n\\nThe URL above is storing a single piece of state; the `greeting`. Consider:\\n\\nhttps://our-app.com?greeting=hi&name=john\\n\\nThe URL above is going further and storing multiple pieces of state; the `greeting` and `name`.\\n\\n## `useSearchParams`\\n\\nIf you\'re working with React, the [React Router](https://reactrouter.com/) project makes consuming state in the URL, particularly in the form of querystring or search parameters, straightforward. It achieves this with the [`useSearchParams`](https://reactrouter.com/docs/en/v6/hooks/use-search-params) hook:\\n\\n```ts\\nimport { useSearchParams } from \'react-router-dom\';\\n\\nconst [searchParams, setSearchParams] = useSearchParams();\\n\\nconst greeting = searchParams.get(\'greeting\');\\n\\n// ...\\n\\nsetSearchParams({ greeting: \'bonjour\' }); // will set URL like so https://our-app.com?greeting=bonjour - this value will feed through to anything driven by the URL\\n```\\n\\nThis is a great mechanism for persisting state both locally and in a shareable way.\\n\\nA significant benefit of this approach is that it doesn\'t require posting to the server. It\'s just using browser APIs like the URLSearchParams API. Changing a query string parameter happens entirely locally and instantaneously.\\n\\n## The `useSearchParamsState` hook\\n\\nWhat the `useSearchParams` hook doesn\'t do, is maintain other query string or search parameters.\\n\\nIf you are maintaining multiple pieces of state in your application, that will likely mean multiple query string or search parameters. What would be quite useful, is a hook which allows us the update state _without_ losing other state. Furthermore, it would be great if we didn\'t have to first acquire the `searchParams` object and then manipulate it. It\'s time for our `useSearchParamsState` hook:\\n\\n```ts\\nimport { useSearchParams } from \'react-router-dom\';\\n\\nexport function useSearchParamsState(\\n  searchParamName: string,\\n  defaultValue: string,\\n): readonly [\\n  searchParamsState: string,\\n  setSearchParamsState: (newState: string) => void,\\n] {\\n  const [searchParams, setSearchParams] = useSearchParams();\\n\\n  const acquiredSearchParam = searchParams.get(searchParamName);\\n  const searchParamsState = acquiredSearchParam ?? defaultValue;\\n\\n  const setSearchParamsState = (newState: string) => {\\n    const next = Object.assign(\\n      {},\\n      [...searchParams.entries()].reduce(\\n        (o, [key, value]) => ({ ...o, [key]: value }),\\n        {},\\n      ),\\n      { [searchParamName]: newState },\\n    );\\n    setSearchParams(next);\\n  };\\n  return [searchParamsState, setSearchParamsState];\\n}\\n```\\n\\nThe above hook can roughly be thought of as `useState<string>` but storing that state in the URL.\\n\\nLet\'s think about how it works. When initialised, the hook takes two parameters:\\n\\n- `searchParamName` - this is the name of the querystring parameter where state is persisted.\\n- `defaultValue` - if there is no value in the querystring, this is the fallback value\\n\\nThe hook then goes on to wrap the `useSearchParams` hook. It interrogates the `searchParams` for the supplied `searchParamName`, and if it isn\'t present, falls back to the `defaultValue`.\\n\\nThe `setSearchParamsState` method definition looks somewhat complicated but essentially all it does is get the contents of the existing search parameters, and applies the new state for the current property. It\'s probably worth pausing here a second to observe an opinion that\'s lurking in this implementation. It is actually valid to have multiple values for the same search parameter. Whilst this is possible, it\'s somewhat rare for this to be used. This implementation only allows for a single value for any given parameter, as that is quite useful behaviour.\\n\\nWith all this in place, we have a hook that can be used like so:\\n\\n```ts\\nconst [greeting, setGreeting] = useSearchParamsState(\'greeting\', \'hello\');\\n```\\n\\nThe above code returns back a `greeting` value which is derived from the `greeting` search parameter. It also returns a `setGreeting` function which allows setting the `greeting` value. This is the same API as `useState` and so should feel idiomatic to a user of React. Tremendous!\\n\\n## Performance - updated 18th December 2022\\n\\nAt this point you might be thinking \\"why don\u2019t we use the `useSearchParamsState` hook always?\\". The fact of the matter is, you could but there\u2019s a reason why you might not want to: performance. The `useSearchParamsState` hook is slower to use than the `useState` hook. Let\'s think about why.\\n\\nIf you\u2019re using the `useState` hook, then ultimately a variable is being updated inside the program that represents your application. This is internal state. However, for the `useSearchParamsState` hook the story is slightly different. The `useSearchParamsState` hook is built upon the `useSearchParams` hook in react-router, as we\u2019ve seen. [If you look at the implementation of that hook](https://github.com/remix-run/react-router/blob/590b7a25a454d998c83f4e5d6f00ad5a6217533b/packages/react-router-dom/index.tsx#L785), you can see that it relies on various browser APIs such as `location` and `History`.\\n\\nThe upshot of this is that the state for our `useSearchParamsState` hook is `external` to our application. It might not feel external because we haven\'t had to set up a database or an API or anything, but external it is. State lives in the browsers APIs, and with that comes a performance penalty. Every time we change state the following happens:\\n\\n- The `useSearchParams` hook in react-router will invoke the `History` API\\n- The browser will update the URL\\n- The instance of react-router running at the root of your application will detect changes in the `location.search` and will surface a new value for your application.\\n- The code in your application that depends upon this will react.\\n\\nThe above is slower than just invoking `useState` and relying upon a local variable. It\u2019s not overwhelmingly slower; generally I\u2019ve not had an issue because browsers are very fast these days. But it\u2019s worth bearing in mind, that if you\u2019re intending to write code that is as performant as possible, then this is probably a hook to avoid. Anything that involves an external API, even if it\u2019s an API that lives in the browser, will be slower than local variables. That said, I would expect there to be few applications to which this is a significant factor - but it\u2019s worth considering.\\n\\n## Persisting querystring across your site\\n\\nNow we have this exciting mechanism set up which allows us to store state in our URL and consequently easily share state by sending someone our URL.\\n\\nWhat would also be useful is a way to navigate around our site _without_ losing that state. Imagine I\'ve got a date range selected and stored in my URL. As I click around from screen to screen, I want to persist that. I don\'t want to have to reselect the date range on each screen.\\n\\nHow can we do this? Well, it turns out to be quite easy. All we need is the `useLocation` hook and the corresponding `location.search` property. That represents the querystring, hence every time we render a link we just include that like so:\\n\\n```ts\\nconst [location] = useLocation();\\n\\nreturn (<Link to={`/my-page${location.search}`}>Page</>)\\n```\\n\\nNow as we navigate around our site, that state will be maintained.\\n\\n## Conclusion\\n\\nIn this post we\'ve created a `useSearchParamsState` hook, which allows state to be persisted to URLs for sharing purposes.\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/use-state-url-persist-state-usesearchparams/)\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.logrocket.com/use-state-url-persist-state-usesearchparams/\\" />\\n</head>"},{"id":"reverse-engineering-azure-app-insights-transactions-url","metadata":{"permalink":"/reverse-engineering-azure-app-insights-transactions-url","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-09-03-reverse-engineering-azure-app-insights-transactions-url/index.md","source":"@site/blog/2022-09-03-reverse-engineering-azure-app-insights-transactions-url/index.md","title":"Reverse engineering the Azure Application Insights Transactions URL","description":"This post reverse engineers the Azure Application Insights Transactions URL, showing how to make a link href, using both TypeScript and C#.","date":"2022-09-03T00:00:00.000Z","tags":[{"inline":false,"label":"C#","permalink":"/tags/csharp","description":"The C# programming language."},{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":7.955,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"reverse-engineering-azure-app-insights-transactions-url","title":"Reverse engineering the Azure Application Insights Transactions URL","authors":"johnnyreilly","tags":["c#","azure","typescript"],"image":"./title-image.png","description":"This post reverse engineers the Azure Application Insights Transactions URL, showing how to make a link href, using both TypeScript and C#.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"React: storing state in URL with URLSearchParams","permalink":"/react-usesearchparamsstate"},"nextItem":{"title":"Swashbuckle and schemaId is already used","permalink":"/swashbuckle-schemaid-already-used"}},"content":"Logs matter. In Azure, logs generally live in Application Insights, in the Transaction Search section. This post reverse engineers the Azure Application Insights Transactions URL, and details how to construct a link to take you directly there, using both TypeScript and C#.\\n\\n![title image reading \\"Reverse engineering the Azure Application Insights Transactions URL\\" with a screenshot of the Transactions screen in the Azure Portal](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Bring me the logs!\\n\\nIf you\'ve ever supported a production system, you will know this to be true: logs matter. Logs help you understand what\'s gone wrong. (You\'re never looking at logs when something has gone right.) When it comes to Azure, logs tend to reside in Application Insights, specifically Transactions:\\n\\n![Screenshot of the Azure Application Insights Transaction Search screen](screenshot-application-insights-transaction-search.png)\\n\\nWhilst Transaction Search is very powerful, it can also be a little tough to find the things that you need. In a system I\'m working on now, we\'ve found ourselves building an application that allows us to provide support. We use it to bring together disparate pieces of information across our estate. As we use it, we\'re usually looking at a particular slice of time. If we don\'t find what we need in our application we\'ll find a need to dig into the logs for the same period.\\n\\nRather than manually logging into Azure, finding Application Insights, going to Transactions and entering the time period, what if we could just go there at the click of a link? We can.\\n\\nLook at the screenshot above, do you see the \\"Copy link\\" button? That button copies a URL to the clipboard which encapsulates the current search criteria. And it turns out we can reverse engineer it!\\n\\n## Breaking down the link\\n\\nFirst of all, let\'s take a look at the incredibly long URL that\'s copied to the clipboard:\\n\\n```text\\nhttps://portal.azure.com/#blade/AppInsightsExtension/BladeRedirect/BladeName/searchV1/ResourceId/%2Fsubscriptions%2F4e41a677-9a57-4a7c-9c4c-e71bae5d998e%2Fresourcegroups%2Frg-maas-shared-storage-dev-001%2Fproviders%2Fmicrosoft.insights%2Fcomponents%2Fappi-maas-shared-dev/BladeInputs/%7B%22tables%22%3A%5B%22traces%22%5D%2C%22timeContextWhereClause%22%3A%22%7C+where+timestamp+%3E+datetime(%5C%222022-05-03T10%3A04%3A33.267Z%5C%22)+and+timestamp+%3C+datetime(%5C%222022-05-03T10%3A34%3A33.267Z%5C%22)%22%2C%22filterWhereClause%22%3A%22%7C+where+severityLevel+in+(%5C%223%5C%22)%7C+where+*+has+%5C%22healthcheck%5C%22%7C+order+by+timestamp+desc%22%2C%22originalParams%22%3A%7B%22eventTypes%22%3A%5B%7B%22value%22%3A%22availabilityResult%22%2C%22tableName%22%3A%22availabilityResults%22%2C%22label%22%3A%22Availability%22%7D%2C%7B%22value%22%3A%22request%22%2C%22tableName%22%3A%22requests%22%2C%22label%22%3A%22Request%22%7D%2C%7B%22value%22%3A%22exception%22%2C%22tableName%22%3A%22exceptions%22%2C%22label%22%3A%22Exception%22%7D%2C%7B%22value%22%3A%22pageView%22%2C%22tableName%22%3A%22pageViews%22%2C%22label%22%3A%22Page+View%22%7D%2C%7B%22value%22%3A%22trace%22%2C%22tableName%22%3A%22traces%22%2C%22label%22%3A%22Trace%22%7D%2C%7B%22value%22%3A%22customEvent%22%2C%22tableName%22%3A%22customEvents%22%2C%22label%22%3A%22Custom+Event%22%7D%2C%7B%22value%22%3A%22dependency%22%2C%22tableName%22%3A%22dependencies%22%2C%22label%22%3A%22Dependency%22%7D%5D%2C%22timeContext%22%3A%7B%22durationMs%22%3A1800000%2C%22endTime%22%3A%222022-05-03T10%3A34%3A33.267Z%22%7D%2C%22filter%22%3A%5B%5D%2C%22searchPhrase%22%3A%7B%22originalPhrase%22%3A%22healthcheck%22%2C%22_tokens%22%3A%5B%7B%22conjunction%22%3A%22and%22%2C%22value%22%3A%22healthcheck%22%2C%22isNot%22%3Afalse%2C%22kql%22%3A%22+*+has+%5C%22healthcheck%5C%22%22%7D%5D%7D%2C%22sort%22%3A%22desc%22%7D%7D\\n```\\n\\nThere\'s 1860 characters in there. That\'s a lot - but still less than the [general limit of 2000 characters](https://stackoverflow.com/questions/417142/what-is-the-maximum-length-of-a-url-in-different-browsers). This mighty long URL can be broken down into four distinct parts. Let\'s break it down:\\n\\n### 1. Main Azure Portal routing\\n\\nFirstly this:\\n\\n```text\\nhttps://portal.azure.com/#blade/AppInsightsExtension/BladeRedirect/BladeName/searchV1/ResourceId/\\n```\\n\\nThis is a recognisable base URL and takes us to the relevant part of the Azure Portal.\\n\\n### 2. ResourceId\\n\\nNext we have a URL encoded ResourceId:\\n\\n```text\\n%2Fsubscriptions%2F4e41a677-9a57-4a7c-9c4c-e71bae5d998e%2Fresourcegroups%2Frg-maas-shared-storage-dev-001%2Fproviders%2Fmicrosoft.insights%2Fcomponents%2Fappi-maas-shared-dev\\n```\\n\\nIf we run it through `decodeURIComponent` you can see it in it\'s raw form:\\n\\n```js\\ndecodeURIComponent(\\n  \'%2Fsubscriptions%2F4e41a677-9a57-4a7c-9c4c-e71bae5d998e%2Fresourcegroups%2Frg-maas-shared-storage-dev-001%2Fproviders%2Fmicrosoft.insights%2Fcomponents%2Fappi-maas-shared-dev\',\\n);\\n\\n// creates: /subscriptions/4e41a677-9a57-4a7c-9c4c-e71bae5d998e/resourcegroups/rg-maas-shared-storage-dev-001/providers/microsoft.insights/components/appi-maas-shared-dev\'\\n```\\n\\nThis is the ResourceId of the Application Insights instance that we\'re looking at. This is the same as the one we saw in the URL when we were looking at the Application Insights instance in the Azure Portal, and it\'s the ResourceId that can be obtained by clicking on the \\"JSON View\\" link:\\n\\n![screenshot of the application insights overview screen with a JSON View icon on the right](screenshot-application-insights-overview.png)\\n\\n### 3. More Azure Portal routing\\n\\nThe next part of the URL is just some more Azure Portal routing:\\n\\n```text\\n/BladeInputs/\\n```\\n\\n### 4. The query\\n\\nFinally we have the (very long) query:\\n\\n```text\\n%7B%22tables%22%3A%5B%22traces%22%5D%2C%22timeContextWhereClause%22%3A%22%7C+where+timestamp+%3E+datetime(%5C%222022-05-03T10%3A04%3A33.267Z%5C%22)+and+timestamp+%3C+datetime(%5C%222022-05-03T10%3A34%3A33.267Z%5C%22)%22%2C%22filterWhereClause%22%3A%22%7C+where+severityLevel+in+(%5C%223%5C%22)%7C+where+_+has+%5C%22healthcheck%5C%22%7C+order+by+timestamp+desc%22%2C%22originalParams%22%3A%7B%22eventTypes%22%3A%5B%7B%22value%22%3A%22availabilityResult%22%2C%22tableName%22%3A%22availabilityResults%22%2C%22label%22%3A%22Availability%22%7D%2C%7B%22value%22%3A%22request%22%2C%22tableName%22%3A%22requests%22%2C%22label%22%3A%22Request%22%7D%2C%7B%22value%22%3A%22exception%22%2C%22tableName%22%3A%22exceptions%22%2C%22label%22%3A%22Exception%22%7D%2C%7B%22value%22%3A%22pageView%22%2C%22tableName%22%3A%22pageViews%22%2C%22label%22%3A%22Page+View%22%7D%2C%7B%22value%22%3A%22trace%22%2C%22tableName%22%3A%22traces%22%2C%22label%22%3A%22Trace%22%7D%2C%7B%22value%22%3A%22customEvent%22%2C%22tableName%22%3A%22customEvents%22%2C%22label%22%3A%22Custom+Event%22%7D%2C%7B%22value%22%3A%22dependency%22%2C%22tableName%22%3A%22dependencies%22%2C%22label%22%3A%22Dependency%22%7D%5D%2C%22timeContext%22%3A%7B%22durationMs%22%3A1800000%2C%22endTime%22%3A%222022-05-03T10%3A34%3A33.267Z%22%7D%2C%22filter%22%3A%5B%5D%2C%22searchPhrase%22%3A%7B%22originalPhrase%22%3A%22healthcheck%22%2C%22_tokens%22%3A%5B%7B%22conjunction%22%3A%22and%22%2C%22value%22%3A%22healthcheck%22%2C%22isNot%22%3Afalse%2C%22kql%22%3A%22+_+has+%5C%22healthcheck%5C%22%22%7D%5D%7D%2C%22sort%22%3A%22desc%22%7D%7D\\n```\\n\\nInitially this doesn\'t look like much. It\'s just a long string of characters. But if we run it through `decodeURIComponent` we can see that it\'s actually a JSON object:\\n\\n```js\\ndecodeURIComponent(\\n  \'%7B%22tables%22%3A%5B%22traces%22%5D%2C%22timeContextWhereClause%22%3A%22%7C+where+timestamp+%3E+datetime(%5C%222022-05-03T10%3A04%3A33.267Z%5C%22)+and+timestamp+%3C+datetime(%5C%222022-05-03T10%3A34%3A33.267Z%5C%22)%22%2C%22filterWhereClause%22%3A%22%7C+where+severityLevel+in+(%5C%223%5C%22)%7C+where+_+has+%5C%22healthcheck%5C%22%7C+order+by+timestamp+desc%22%2C%22originalParams%22%3A%7B%22eventTypes%22%3A%5B%7B%22value%22%3A%22availabilityResult%22%2C%22tableName%22%3A%22availabilityResults%22%2C%22label%22%3A%22Availability%22%7D%2C%7B%22value%22%3A%22request%22%2C%22tableName%22%3A%22requests%22%2C%22label%22%3A%22Request%22%7D%2C%7B%22value%22%3A%22exception%22%2C%22tableName%22%3A%22exceptions%22%2C%22label%22%3A%22Exception%22%7D%2C%7B%22value%22%3A%22pageView%22%2C%22tableName%22%3A%22pageViews%22%2C%22label%22%3A%22Page+View%22%7D%2C%7B%22value%22%3A%22trace%22%2C%22tableName%22%3A%22traces%22%2C%22label%22%3A%22Trace%22%7D%2C%7B%22value%22%3A%22customEvent%22%2C%22tableName%22%3A%22customEvents%22%2C%22label%22%3A%22Custom+Event%22%7D%2C%7B%22value%22%3A%22dependency%22%2C%22tableName%22%3A%22dependencies%22%2C%22label%22%3A%22Dependency%22%7D%5D%2C%22timeContext%22%3A%7B%22durationMs%22%3A1800000%2C%22endTime%22%3A%222022-05-03T10%3A34%3A33.267Z%22%7D%2C%22filter%22%3A%5B%5D%2C%22searchPhrase%22%3A%7B%22originalPhrase%22%3A%22healthcheck%22%2C%22_tokens%22%3A%5B%7B%22conjunction%22%3A%22and%22%2C%22value%22%3A%22healthcheck%22%2C%22isNot%22%3Afalse%2C%22kql%22%3A%22+_+has+%5C%22healthcheck%5C%22%22%7D%5D%7D%2C%22sort%22%3A%22desc%22%7D%7D\',\\n);\\n\\n// creates: \'{\\"tables\\":[\\"traces\\"],\\"timeContextWhereClause\\":\\"|+where+timestamp+>+datetime(\\\\\\\\\\"2022-05-03T10:04:33.267Z\\\\\\\\\\")+and+timestamp+<+datetime(\\\\\\\\\\"2022-05-03T10:34:33.267Z\\\\\\\\\\")\\",\\"filterWhereClause\\":\\"|+where+severityLevel+in+(\\\\\\\\\\"3\\\\\\\\\\")|+where+_+has+\\\\\\\\\\"healthcheck\\\\\\\\\\"|+order+by+timestamp+desc\\",\\"originalParams\\":{\\"eventTypes\\":[{\\"value\\":\\"availabilityResult\\",\\"tableName\\":\\"availabilityResults\\",\\"label\\":\\"Availability\\"},{\\"value\\":\\"request\\",\\"tableName\\":\\"requests\\",\\"label\\":\\"Request\\"},{\\"value\\":\\"exception\\",\\"tableName\\":\\"exceptions\\",\\"label\\":\\"Exception\\"},{\\"value\\":\\"pageView\\",\\"tableName\\":\\"pageViews\\",\\"label\\":\\"Page+View\\"},{\\"value\\":\\"trace\\",\\"tableName\\":\\"traces\\",\\"label\\":\\"Trace\\"},{\\"value\\":\\"customEvent\\",\\"tableName\\":\\"customEvents\\",\\"label\\":\\"Custom+Event\\"},{\\"value\\":\\"dependency\\",\\"tableName\\":\\"dependencies\\",\\"label\\":\\"Dependency\\"}],\\"timeContext\\":{\\"durationMs\\":1800000,\\"endTime\\":\\"2022-05-03T10:34:33.267Z\\"},\\"filter\\":[],\\"searchPhrase\\":{\\"originalPhrase\\":\\"healthcheck\\",\\"_tokens\\":[{\\"conjunction\\":\\"and\\",\\"value\\":\\"healthcheck\\",\\"isNot\\":false,\\"kql\\":\\"+_+has+\\\\\\\\\\"healthcheck\\\\\\\\\\"\\"}]},\\"sort\\":\\"desc\\"}}\'\\n```\\n\\nAnd if we parse that JSON object we get:\\n\\n```json\\n{\\n  \\"tables\\": [\\"traces\\"],\\n  \\"timeContextWhereClause\\": \\"|+where+timestamp+>+datetime(\\\\\\"2022-05-03T10:04:33.267Z\\\\\\")+and+timestamp+<+datetime(\\\\\\"2022-05-03T10:34:33.267Z\\\\\\")\\",\\n  \\"filterWhereClause\\": \\"|+where+severityLevel+in+(\\\\\\"3\\\\\\")|+where+_+has+\\\\\\"healthcheck\\\\\\"|+order+by+timestamp+desc\\",\\n  \\"originalParams\\": {\\n    \\"eventTypes\\": [\\n      {\\n        \\"value\\": \\"availabilityResult\\",\\n        \\"tableName\\": \\"availabilityResults\\",\\n        \\"label\\": \\"Availability\\"\\n      },\\n      {\\n        \\"value\\": \\"request\\",\\n        \\"tableName\\": \\"requests\\",\\n        \\"label\\": \\"Request\\"\\n      },\\n      {\\n        \\"value\\": \\"exception\\",\\n        \\"tableName\\": \\"exceptions\\",\\n        \\"label\\": \\"Exception\\"\\n      },\\n      {\\n        \\"value\\": \\"pageView\\",\\n        \\"tableName\\": \\"pageViews\\",\\n        \\"label\\": \\"Page+View\\"\\n      },\\n      {\\n        \\"value\\": \\"trace\\",\\n        \\"tableName\\": \\"traces\\",\\n        \\"label\\": \\"Trace\\"\\n      },\\n      {\\n        \\"value\\": \\"customEvent\\",\\n        \\"tableName\\": \\"customEvents\\",\\n        \\"label\\": \\"Custom+Event\\"\\n      },\\n      {\\n        \\"value\\": \\"dependency\\",\\n        \\"tableName\\": \\"dependencies\\",\\n        \\"label\\": \\"Dependency\\"\\n      }\\n    ],\\n    \\"timeContext\\": {\\n      \\"durationMs\\": 1800000,\\n      \\"endTime\\": \\"2022-05-03T10:34:33.267Z\\"\\n    },\\n    \\"filter\\": [],\\n    \\"searchPhrase\\": {\\n      \\"originalPhrase\\": \\"healthcheck\\",\\n      \\"_tokens\\": [\\n        {\\n          \\"conjunction\\": \\"and\\",\\n          \\"value\\": \\"healthcheck\\",\\n          \\"isNot\\": false,\\n          \\"kql\\": \\"+_+has+\\\\\\"healthcheck\\\\\\"\\"\\n        }\\n      ]\\n    },\\n    \\"sort\\": \\"desc\\"\\n  }\\n}\\n```\\n\\nWe can clearly see in the object above the aspects that contribute to our query. It\'s worth highlighting that when I generated the above query, I had the `traces` table selected and I was searching for the phrase \\"healthcheck\\". If I had selected `requests` instead, the `tables` array would have contained `requests` instead of `traces`. If I had been searching for a different phrase, the `searchPhrase` and `filterWhereClause` objects would have contained different values.\\n\\n## Reverse engineering a link\\n\\nNow that we understand what makes up a URL, we\'re safe to build our own mechanisms to generate a URL.\\n\\n## TypeScript URL builder\\n\\nWe\'ll start by creating the TypeScript version of the URL builder. We\'ll start by creating a new file called `urlBuilder.ts` and we\'ll add the following code:\\n\\n```ts\\nfunction makeAzureApplicationInsightsTransactionUrl({\\n  applicationInsightsId,\\n  endDate,\\n  startDate,\\n}: {\\n  applicationInsightsId: string;\\n  startDate: Date;\\n  endDate: Date;\\n}) {\\n  const endDateAsString = endDate.toISOString(); // eg 2022-05-03T14:22:51.180Z\\n  const startDateAsString = startDate.toISOString();\\n  const durationMs = endDate.getTime() - startDate.getTime();\\n  const logsQuery = {\\n    tables: [\\n      \'availabilityResults\',\\n      \'requests\',\\n      \'exceptions\',\\n      \'pageViews\',\\n      \'traces\',\\n      \'customEvents\',\\n      \'dependencies\',\\n    ],\\n    timeContextWhereClause: `| where timestamp > datetime(${startDateAsString}) and timestamp < datetime(\\"${endDateAsString}\\")`,\\n    filterWhereClause: \'| order by timestamp desc\',\\n    originalParams: {\\n      eventTypes: [\\n        {\\n          value: \'availabilityResult\',\\n          tableName: \'availabilityResults\',\\n          label: \'Availability\',\\n        },\\n        { value: \'request\', tableName: \'requests\', label: \'Request\' },\\n        {\\n          value: \'exception\',\\n          tableName: \'exceptions\',\\n          label: \'Exception\',\\n        },\\n        {\\n          value: \'pageView\',\\n          tableName: \'pageViews\',\\n          label: \'Page View\',\\n        },\\n        { value: \'trace\', tableName: \'traces\', label: \'Trace\' },\\n        {\\n          value: \'customEvent\',\\n          tableName: \'customEvents\',\\n          label: \'Custom Event\',\\n        },\\n        {\\n          value: \'dependency\',\\n          tableName: \'dependencies\',\\n          label: \'Dependency\',\\n        },\\n      ],\\n      timeContext: {\\n        durationMs: durationMs,\\n        endTime: endDateAsString,\\n      },\\n      filter: [],\\n      searchPhrase: {\\n        originalPhrase: \'\',\\n        _tokens: [],\\n      },\\n      sort: \'desc\',\\n    },\\n  };\\n\\n  const baseUrl = `https://portal.azure.com/#blade/AppInsightsExtension/BladeRedirect/BladeName/searchV1/ResourceId/`;\\n  const encodedApplicationInsightsId = encodeURIComponent(\\n    applicationInsightsId,\\n  );\\n  const moreRouting = `/BladeInputs/`;\\n  const encodedLogsQuery = encodeURIComponent(JSON.stringify(logsQuery));\\n  const logsUrl = `${baseUrl}${encodedApplicationInsightsId}${moreRouting}${encodedLogsQuery}`;\\n\\n  return logsUrl;\\n}\\n```\\n\\nThe above code is a function that takes in an object with the following properties:\\n\\n- `applicationInsightsId` - the ID of the Application Insights resource\\n- `startDate` - the start date of the time range\\n- `endDate` - the end date of the time range\\n\\nYou can see that it takes these inputs and uses them to build up a URL made up of the four sections we identified earlier.\\n\\nThe URL it generates is the URL that will open the Application Insights logs blade in the Azure portal with the time range selected. This code is not including any kind of search phrase, but it could easily be adjusted to cater for that.\\n\\n## C# URL builder\\n\\nWe can do the same thing in C#. It\'s a bit more verbose than the TypeScript version, but it\'s still pretty straightforward. We\'ll create a new file called `UrlBuilder.cs` and add the following code:\\n\\n```cs\\nusing System.Collections.Generic;\\nusing Newtonsoft.Json;\\n\\nnamespace AzureApplicationInsightsTransactionSearchUrl\\n{\\n  public static class UrlBuilder\\n  {\\n    /// <summary>\\n    /// eg 2022-05-03T14:22:51.180Z\\n    /// </summary>\\n    public static string ToAzureLogsString(this DateTime value) =>\\n        value.ToString(\\"yyyy-MM-ddTHH:mm:ss.fffK\\", CultureInfo.InvariantCulture);\\n\\n    public static string MakeAzureApplicationInsightsTransactionUrl(\\n      string applicationInsightsId,\\n      DateTime startDate,\\n      DateTime endDate\\n    )\\n    {\\n      var endDateAsString = endDate.ToAzureLogsString();\\n      var startDateAsString = startDate.ToAzureLogsString();\\n      var durationMs = Convert.ToInt32((endDate - startDate).TotalMilliseconds);\\n\\n      var logsQuery = new LogsQuery(\\n        Tables: new List<string> {\\n          \\"availabilityResults\\",\\n          \\"requests\\",\\n          \\"exceptions\\",\\n          \\"pageViews\\",\\n          \\"traces\\",\\n          \\"customEvents\\",\\n          \\"dependencies\\"\\n        },\\n        TimeContextWhereClause: $\\"| where timestamp > datetime(\\\\\\"{startDateAsString}\\\\\\") and timestamp < datetime(\\\\\\"{endDateAsString}\\\\\\")\\",\\n        FilterWhereClause: $\\"| order by timestamp desc\\",\\n        OriginalParams: new OriginalParams(\\n          EventTypes: new List<EventType>\\n          {\\n            new (\\n                Value: \\"availabilityResult\\",\\n                TableName: \\"availabilityResults\\",\\n                Label: \\"Availability\\"\\n            ),\\n            new (\\n                Value: \\"request\\",\\n                TableName: \\"requests\\",\\n                Label: \\"Request\\"\\n            ),\\n            new (\\n                Value: \\"exception\\",\\n                TableName: \\"exceptions\\",\\n                Label: \\"Exception\\"\\n            ),\\n            new (\\n                Value: \\"pageView\\",\\n                TableName: \\"pageViews\\",\\n                Label: \\"Page View\\"\\n            ),\\n            new (\\n                Value: \\"trace\\",\\n                TableName: \\"traces\\",\\n                Label: \\"Trace\\"\\n            ),\\n            new (\\n                Value: \\"customEvent\\",\\n                TableName: \\"customEvents\\",\\n                Label: \\"Custom Event\\"\\n            ),\\n            new (\\n                Value: \\"dependency\\",\\n                TableName: \\"dependencies\\",\\n                Label: \\"Dependency\\"\\n            ),\\n          },\\n          TimeContext: new TimeContext(\\n              DurationMs: durationMs,\\n              EndTime: endDateAsString\\n          ),\\n          Filter: new List<Filter>(),\\n          SearchPhrase: new SearchPhrase(\\n              OriginalPhrase: \\"\\",\\n              Tokens: new List<Token>()\\n          ),\\n          Sort: \\"desc\\"\\n        )\\n      );\\n\\n      var baseUrl = \\"https://portal.azure.com/#blade/AppInsightsExtension/BladeRedirect/BladeName/searchV1/ResourceId/\\";\\n      var encodedApplicationInsightsId = WebUtility.UrlEncode(applicationInsightsId);\\n      var moreRouting = \\"/BladeInputs/\\";\\n      var encodedLogsQuery = WebUtility.UrlEncode(JsonConvert.SerializeObject(logsQuery));\\n      var logsUrl = $\\"{baseUrl}{encodedApplicationInsightsId}{moreRouting}{encodedLogsQuery}\\";\\n\\n      return logsUrl;\\n    }\\n  }\\n\\n  public record EventType(\\n      [property: JsonProperty(\\"value\\")] string Value,\\n      [property: JsonProperty(\\"tableName\\")] string TableName,\\n      [property: JsonProperty(\\"label\\")] string Label\\n  );\\n\\n  public record TimeContext(\\n      [property: JsonProperty(\\"durationMs\\")] int DurationMs,\\n      [property: JsonProperty(\\"endTime\\")] string EndTime\\n  );\\n\\n  public record Dimension(\\n      [property: JsonProperty(\\"displayName\\")] string DisplayName,\\n      [property: JsonProperty(\\"tables\\")] IReadOnlyList<string> Tables,\\n      [property: JsonProperty(\\"name\\")] string Name,\\n      [property: JsonProperty(\\"draftKey\\")] string DraftKey\\n  );\\n\\n  public record Operator(\\n      [property: JsonProperty(\\"label\\")] string Label,\\n      [property: JsonProperty(\\"value\\")] string Value,\\n      [property: JsonProperty(\\"isSelected\\")] bool IsSelected\\n  );\\n\\n  public record Filter(\\n      [property: JsonProperty(\\"dimension\\")] Dimension Dimension,\\n      [property: JsonProperty(\\"values\\")] IReadOnlyList<string> Values,\\n      [property: JsonProperty(\\"operator\\")] Operator Operator\\n  );\\n\\n  public record Token(\\n      [property: JsonProperty(\\"conjunction\\")] string Conjunction,\\n      [property: JsonProperty(\\"value\\")] string Value,\\n      [property: JsonProperty(\\"isNot\\")] bool IsNot,\\n      [property: JsonProperty(\\"kql\\")] string Kql\\n  );\\n\\n  public record SearchPhrase(\\n      [property: JsonProperty(\\"originalPhrase\\")] string OriginalPhrase,\\n      [property: JsonProperty(\\"_tokens\\")] IReadOnlyList<Token> Tokens\\n  );\\n\\n  public record OriginalParams(\\n      [property: JsonProperty(\\"eventTypes\\")] IReadOnlyList<EventType> EventTypes,\\n      [property: JsonProperty(\\"timeContext\\")] TimeContext TimeContext,\\n      [property: JsonProperty(\\"filter\\")] IReadOnlyList<Filter> Filter,\\n      [property: JsonProperty(\\"searchPhrase\\")] SearchPhrase SearchPhrase,\\n      [property: JsonProperty(\\"sort\\")] string Sort\\n  );\\n\\n  public record LogsQuery(\\n      [property: JsonProperty(\\"tables\\")] IReadOnlyList<string> Tables,\\n      [property: JsonProperty(\\"timeContextWhereClause\\")] string TimeContextWhereClause,\\n      [property: JsonProperty(\\"filterWhereClause\\")] string FilterWhereClause,\\n      [property: JsonProperty(\\"originalParams\\")] OriginalParams OriginalParams\\n  );\\n}\\n```\\n\\nNote that most of the verbosity comes from the fact that we\'re using C# 9 record types to represent the JSON objects that we\'re serializing. If you\'re not familiar with C# 9 [record types](https://docs.microsoft.com/en-us/dotnet/csharp/whats-new/csharp-9#record-types). We\'re also using JSON.Net for our serialization, but you could use System.Text.Json if you wanted to. You would need to amend the `JsonProperty` attributes to be `JsonPropertyName` attributes instead.\\n\\n## Conclusion\\n\\nIn this post we\'ve understood what goes into the URL for Application Insights Transactions, and we\'ve seen how to generate that URL in TypeScript and C#. We\'ve also seen how to use the URL to search for transactions in Application Insights. I hope you found this post useful. Thanks for reading!"},{"id":"swashbuckle-schemaid-already-used","metadata":{"permalink":"/swashbuckle-schemaid-already-used","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-08-31-swashbuckle-schemaid-already-used/index.md","source":"@site/blog/2022-08-31-swashbuckle-schemaid-already-used/index.md","title":"Swashbuckle and schemaId is already used","description":"Swashbuckle can fail to generate a swagger / Open API document with the message \\"The same schemaId is already used...\\". This post offers a way forward.","date":"2022-08-31T00:00:00.000Z","tags":[{"inline":false,"label":"Swagger","permalink":"/tags/swagger","description":"The Swagger API documentation framework - now known as OpenAPI."}],"readingTime":2.255,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"swashbuckle-schemaid-already-used","title":"Swashbuckle and schemaId is already used","authors":"johnnyreilly","tags":["swagger"],"image":"./title-image.png","description":"Swashbuckle can fail to generate a swagger / Open API document with the message \\"The same schemaId is already used...\\". This post offers a way forward.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Reverse engineering the Azure Application Insights Transactions URL","permalink":"/reverse-engineering-azure-app-insights-transactions-url"},"nextItem":{"title":"Terry Pratchett and the Azure Static Web Apps","permalink":"/terry-pratchett-x-clacks-overhead-azure-static-webapps"}},"content":"Swashbuckle can fail to generate a swagger / Open API document with the message \\"The same schemaId is already used...\\". This post explains what that means, and offers a way to work around it.\\n\\n![title image reading \\"Swashbuckle and schemaId is already used\\" with the Azure Static Web Apps logo and a Terry Pratchett icon by Lisa Krymova from NounProject.com](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## \\"The same schemaId is already used...\\"\\n\\nWhen creating a swagger / Open API document with Swashbuckle, it\'s possible to encounter an error of this nature:\\n\\n```\\nSystem.InvalidOperationException: Can\'t use schemaId \\"$MyType\\" for type \\"$OneNamespace.MyType\\". The same schemaId is already used for type \\"$OtherNamespace.MyType\\"\\n```\\n\\n[Richard Morris](https://github.com/domaindrivendev) explains the reason for this [here](https://github.com/domaindrivendev/Swashbuckle.AspNetCore/issues/1607#issuecomment-788900097):\\n\\n> By default, SB uses the short (unqualified name) which has its benefits because it keeps the docs simpler but also it\u2019s downside if model names are duplicated in different namespaces.\\n\\nThe solution for this is using the `CustomSchemaIds` configuration option for Swashbuckle. This allows the customisation of type names, such that collisions are prevented. We need types to be unique strings. A simple way to tackle this is something like this:\\n\\n```cs\\nservices.AddSwaggerGen(options =>\\n{\\n    options.CustomSchemaIds(type => type.ToString());\\n});\\n```\\n\\nHowever, the types created using the above approach can be verbose. Wouldn\'t it be nice if we could essentially have the names we had before, but just handle duplicates with an incrementing number?\\n\\n## Nicer names with `SwashbuckleSchemaHelper`\\n\\nWe can do exactly this. What we\'ll do is put together a class called `SwashbuckleSchemaHelper`:\\n\\n```cs\\npublic class SwashbuckleSchemaHelper\\n{\\n    private readonly Dictionary<string, List<string>> _schemaNameRepetition = new();\\n\\n    // borrowed from https://github.com/domaindrivendev/Swashbuckle.AspNetCore/blob/95cb4d370e08e54eb04cf14e7e6388ca974a686e/src/Swashbuckle.AspNetCore.SwaggerGen/SchemaGenerator/SchemaGeneratorOptions.cs#L44\\n    private string DefaultSchemaIdSelector(Type modelType)\\n    {\\n        if (!modelType.IsConstructedGenericType) return modelType.Name.Replace(\\"[]\\", \\"Array\\");\\n\\n        var prefix = modelType.GetGenericArguments()\\n            .Select(genericArg => DefaultSchemaIdSelector(genericArg))\\n            .Aggregate((previous, current) => previous + current);\\n\\n        return prefix + modelType.Name.Split(\'`\').First();\\n    }\\n\\n    public string GetSchemaId(Type modelType)\\n    {\\n        string id = DefaultSchemaIdSelector(modelType);\\n\\n        if (!_schemaNameRepetition.ContainsKey(id))\\n            _schemaNameRepetition.Add(id, new List<string>());\\n\\n        var modelNameList = _schemaNameRepetition[id];\\n        var fullName = modelType.FullName ?? \\"\\";\\n        if (!string.IsNullOrEmpty(fullName) && !modelNameList.Contains(fullName))\\n            modelNameList.Add(fullName);\\n\\n        int index = modelNameList.IndexOf(fullName);\\n\\n        return $\\"{id}{(index >= 1 ? index.ToString() : \\"\\")}\\";\\n    }\\n}\\n```\\n\\nThe above class borrows the [`DefaultSchemaIdSelector`](https://github.com/domaindrivendev/Swashbuckle.AspNetCore/blob/95cb4d370e08e54eb04cf14e7e6388ca974a686e/src/Swashbuckle.AspNetCore.SwaggerGen/SchemaGenerator/SchemaGeneratorOptions.cs#L44) implementation from Swashbuckle itself. It creates the type name using that, and then uses a `Dictionary` to track the numbers of usages of it; suffixing a number where there are duplicates to indicate which duplicate is in play on this occasion. This number suffix is inspired [by an answer on Stack Overflow](https://stackoverflow.com/a/72677918/761388) and also by [Glenn Piper\'s comment here](https://github.com/domaindrivendev/Swashbuckle.AspNetCore/issues/1607#issuecomment-1258337736).\\n\\nUsage of this looks like this:\\n\\n```cs\\nservices.AddSwaggerGen(options =>\\n{\\n    var schemaHelper = new SwashbuckleSchemaHelper();\\n    options.CustomSchemaIds(type => schemaHelper.GetSchemaId(type));\\n});\\n```\\n\\nThe result of using this approach is that you\'ll start to generate multiple types: `MyType` and `MyType2`, and importantly a goodbye to the \\"The same schemaId is already used...\\" message."},{"id":"terry-pratchett-x-clacks-overhead-azure-static-webapps","metadata":{"permalink":"/terry-pratchett-x-clacks-overhead-azure-static-webapps","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-07-23-terry-pratchett-x-clacks-overhead-azure-static-webapps/index.md","source":"@site/blog/2022-07-23-terry-pratchett-x-clacks-overhead-azure-static-webapps/index.md","title":"Terry Pratchett and the Azure Static Web Apps","description":"Terry Pratchett has a HTTP header: X-Clacks-Overhead. This post shows how we can make Azure Static Web Apps join in.","date":"2022-07-23T00:00:00.000Z","tags":[{"inline":false,"label":"Azure Static Web Apps","permalink":"/tags/azure-static-web-apps","description":"The Azure Static Web Apps service."}],"readingTime":2.03,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"terry-pratchett-x-clacks-overhead-azure-static-webapps","title":"Terry Pratchett and the Azure Static Web Apps","authors":"johnnyreilly","tags":["azure static web apps"],"image":"./title-image.png","description":"Terry Pratchett has a HTTP header: X-Clacks-Overhead. This post shows how we can make Azure Static Web Apps join in.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Swashbuckle and schemaId is already used","permalink":"/swashbuckle-schemaid-already-used"},"nextItem":{"title":"Get Build Validations with the Azure DevOps API","permalink":"/azure-devops-api-build-validations"}},"content":"Terry Pratchett is remembered on the internet. A non-standardised HTTP header: `X-Clacks-Overhead` is broadcast by websites seeking to pay tribute to the great man. This post shows how we can make Azure Static Web Apps join in.\\n\\n![title image reading \\"Terry Pratchett and the Azure Static Web Apps\\" with the Azure Static Web Apps logo and a Terry Pratchett icon by Lisa Krymova from NounProject.com](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## What is `X-Clacks-Overhead`?\\n\\nThis is well documented in the [`X-Clacks-Overhead`](https://xclacksoverhead.org/) website. To quote the highlights:\\n\\n> X-Clacks-Overhead is a non-standardised HTTP header based upon the fictional work of the late, great, Sir Terry Pratchett...\\n>\\n> As a way to preserve the memory of Sir Terry Pratchett, the users of the [SubReddit for the Discworld series](https://www.reddit.com/r/discworld/) came up with [the idea behind the X-Clacks-Overhead HTTP Header](https://www.reddit.com/r/discworld/comments/2yt9j6/gnu_terry_pratchett/). It allows web authors to silently commemorate someone through the use of a non-invasive header that can be transmitted from server to server, or server to client without operational interference.\\n>\\n> You would only know the header is present if you analysed the transmission headers of your content requests on web sites serving the header.\\n\\nPut simply, participating websites will broadcast the `X-Clacks-Overhead: GNU Terry Pratchett` header when they are serving content to a user.\\n\\n## Azure Static Web Apps serving the `X-Clacks-Overhead` header\\n\\nNow we understand what we want to do, we can make an Azure Static Web Apps project do just that.\\n\\nWe\'re going to need an `staticwebappconfig.json` in root of our app, so we can configure our SWA. To add extra headers in, you use the [`globalHeaders`](https://docs.microsoft.com/en-us/azure/static-web-apps/configuration#global-headers) setting:\\n\\n```json\\n{\\n  \\"globalHeaders\\": {\\n    \\"X-Clacks-Overhead\\": \\"GNU Terry Pratchett\\"\\n  }\\n}\\n```\\n\\nAbove, we added the `X-Clacks-Overhead` header. When our app is deployed, it will automatically add this header to all requests:\\n\\n![screenshot of Chrome Devtools showing the `x-clacks-overhead` header on this blog](./screenshot-x-clacks-overhead-response-header.png)\\n\\nThe above screenshot shows this very blog broadcasting the `X-Clacks-Overhead` header. If you crack open devtools you can validate this for yourself. The pull request that added it in [can be found here](https://github.com/johnnyreilly/blog.johnnyreilly.com/pull/273).\\n\\n## Who else is out there?\\n\\nIt\'s great to know you\'re in good company. [This page](https://xclacksoverhead.org/listing/the-signal) tracks websites that are broadcasting the `X-Clacks-Overhead` header. You can see from the image below that it has found this website too!\\n\\n![screenshot of https://xclacksoverhead.org/listing/the-signal showing this blog being listed](./screenshot-x-clacks-overhead-listing.png)\\n\\nSo if you\'d like your Azure Static Web App to whisper Terry Pratchett\'s name under its breath; make it so!"},{"id":"azure-devops-api-build-validations","metadata":{"permalink":"/azure-devops-api-build-validations","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-07-10-azure-devops-api-build-validations/index.md","source":"@site/blog/2022-07-10-azure-devops-api-build-validations/index.md","title":"Get Build Validations with the Azure DevOps API","description":"Use the Azure DevOps API to acquire the build validations a project uses. This post shows you how using curl and the Node.js API.","date":"2022-07-10T00:00:00.000Z","tags":[{"inline":false,"label":"Azure Pipelines","permalink":"/tags/azure-pipelines","description":"The Azure Pipelines CI / CD service."},{"inline":false,"label":"Azure DevOps","permalink":"/tags/azure-devops","description":"The Azure DevOps suite of tools."}],"readingTime":3.435,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-devops-api-build-validations","title":"Get Build Validations with the Azure DevOps API","authors":"johnnyreilly","tags":["azure pipelines","azure devops"],"image":"./title-image.png","description":"Use the Azure DevOps API to acquire the build validations a project uses. This post shows you how using curl and the Node.js API.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Terry Pratchett and the Azure Static Web Apps","permalink":"/terry-pratchett-x-clacks-overhead-azure-static-webapps"},"nextItem":{"title":"Azure Static Web Apps: Failed to deploy the Azure Functions","permalink":"/static-web-apps-failed-to-deploy-the-azure-functions"}},"content":"Build validations are a great way to protect your branches in Azure DevOps. It\'s possible to use the Azure DevOps API to acquire the build validations a project uses. This post shows you how using curl and the Node.js API.\\n\\n![title image reading \\"Get Build Validations with the Azure DevOps API\\" with Azure Pipelines and Azure DevOps logo](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Build validations\\n\\nWe care about our `main` branch. Before changes can be made to it, we want them to meet some kind of quality benchmark. In Azure DevOps these come in the form of branch policies. We\'re interested in a particular type of these called [build validations](https://docs.microsoft.com/en-us/azure/devops/repos/git/branch-policies?view=azure-devops&tabs=browser#build-validation):\\n\\n> A build validation policy queues a new build when a new PR is created or changes are pushed to an existing PR that targets the branch. The build policy evaluates the build results to determine whether the PR can be completed.\\n\\nIf you want to examine whether a project uses build validations, you can do so using the Azure DevOps API. It\'s available under the [policy configurations endpoint](https://docs.microsoft.com/en-us/rest/api/azure/devops/policy/configurations/list?view=azure-devops-rest-7.1).\\n\\n## curl build validations\\n\\nTo acquire build validations from the API we\'ll need a personal access token. We can make one of those here: https://dev.azure.com/organisation-name/_usersSettings/tokens (where `organisation-name` is the name of our organisation).\\n\\nWith that in hand, let\'s acquire the branch policies with a [curl](https://curl.se/):\\n\\n```bash\\ncurl  --user \'\':\'PERSONAL_ACCESS_TOKEN\' --header \\"Content-Type: application/json\\" --header \\"Accept:application/json\\" https://dev.azure.com/{organisation}/{project}/_apis/policy/configurations?api-version=7.1-preview.1\\n```\\n\\nYou\'ll retrieve a JSON array that represents all the branch policies that are in play, _including_ build validations. Below is an example of a retrieved build validation:\\n\\n```json\\n[\\n  //...\\n  {\\n    \\"createdBy\\": {\\n      \\"displayName\\": \\"Project Collection Build Service (organisation-name)\\",\\n      \\"url\\": \\"https://spsprodweu1.vssps.visualstudio.com/A2426c3ce-09b0-4333-9218-58da7d53c564/_apis/Identities/aad5a5e5-baf9-40b3-ad30-0cb828c25629\\",\\n      \\"_links\\": {\\n        \\"avatar\\": {\\n          \\"href\\": \\"https://dev.azure.com/organisation-name/_apis/GraphProfile/MemberAvatars/svc.MjQyNmMzY2UtMDliMC00MzMzLTkyMTgtNThkYTdkNTNjNTY0OkJ1aWxkOmE1YTE3N2U2LTZhMzAtNGRiMi1iYzMxLTllOTE1M2U3Yjk0Nw\\"\\n        }\\n      },\\n      \\"id\\": \\"aad5a5e5-baf9-40b3-ad30-0cb828c25629\\",\\n      \\"uniqueName\\": \\"Build\\\\\\\\a5a177e6-6a30-4db2-bc31-9e9153e7b947\\",\\n      \\"imageUrl\\": \\"https://dev.azure.com/organisation-name/_api/_common/identityImage?id=aad5a5e5-baf9-40b3-ad30-0cb828c25629\\",\\n      \\"descriptor\\": \\"svc.MjQyNmMzY2UtMDliMC00MzMzLTkyMTgtNThkYTdkNTNjNTY0OkJ1aWxkOmE1YTE3N2U2LTZhMzAtNGRiMi1iYzMxLTllOTE1M2U3Yjk0Nw\\"\\n    },\\n    \\"createdDate\\": \\"2022-07-06T14:25:39.5585302Z\\",\\n    \\"isEnabled\\": true,\\n    \\"isBlocking\\": true,\\n    \\"isDeleted\\": false,\\n    \\"settings\\": {\\n      \\"buildDefinitionId\\": 1107,\\n      \\"queueOnSourceUpdateOnly\\": true,\\n      \\"manualQueueOnly\\": false,\\n      \\"displayName\\": \\"PR Validation\\",\\n      \\"validDuration\\": 720,\\n      \\"scope\\": [\\n        {\\n          \\"refName\\": \\"refs/heads/main\\",\\n          \\"matchKind\\": \\"Exact\\",\\n          \\"repositoryId\\": \\"dc4213a0-fc26-4afc-8b3e-4fd27eaaafa6\\"\\n        }\\n      ]\\n    },\\n    \\"isEnterpriseManaged\\": false,\\n    \\"_links\\": {\\n      \\"self\\": {\\n        \\"href\\": \\"https://dev.azure.com/organisation-name/ea861f16-ec9d-4256-a2a6-55dd7533af36/_apis/policy/configurations/1261\\"\\n      },\\n      \\"policyType\\": {\\n        \\"href\\": \\"https://dev.azure.com/organisation-name/ea861f16-ec9d-4256-a2a6-55dd7533af36/_apis/policy/types/0609b952-1397-4640-95ec-e00a01b2c241\\"\\n      }\\n    },\\n    \\"revision\\": 2,\\n    \\"id\\": 1261,\\n    \\"url\\": \\"https://dev.azure.com/organisation-name/ea861f16-ec9d-4256-a2a6-55dd7533af36/_apis/policy/configurations/1261\\",\\n    \\"type\\": {\\n      \\"id\\": \\"0609b952-1397-4640-95ec-e00a01b2c241\\",\\n      \\"url\\": \\"https://dev.azure.com/organisation-name/ea861f16-ec9d-4256-a2a6-55dd7533af36/_apis/policy/types/0609b952-1397-4640-95ec-e00a01b2c241\\",\\n      \\"displayName\\": \\"Build\\"\\n    }\\n  }\\n]\\n```\\n\\nNote the `type` property with the `displayName` of `\\"Build\\"`. That\'s how we identify build validations in amongst the other branch policies. If you\'d like to filter the output down to just build validations on the command line, you can by mixing your curl with a little [jq](https://stedolan.github.io/jq/):\\n\\n```bash\\ncurl  --user \'\':\'PERSONAL_ACCESS_TOKEN\' --header \\"Content-Type: application/json\\" --header \\"Accept:application/json\\" https://dev.azure.com/{organisation}/{project}/_apis/policy/configurations?api-version=7.1-preview.1 | jq -c \'.value[] | select(.type.displayName == \\"Build\\")\'\\n```\\n\\n## Node.js API\\n\\nIf you\'d like to achieve the same using TypeScript, you can. The [Azure DevOps Client for Node.js](https://github.com/microsoft/azure-devops-node-api) provides an API and (in large part) the types. Let\'s obtain the build validations for a project in Node.js using the client:\\n\\n```ts\\nimport * as nodeApi from \'azure-devops-node-api\';\\n\\ninterface BuildValidation {\\n  buildDefinitionId: number | undefined;\\n  name: string | undefined;\\n  repositoryId: string | undefined;\\n}\\n\\nasync function getProjectBuildValidations(\\n  projectId: string,\\n): Promise<BuildValidation[]> {\\n  const authHandler = nodeApi.getPersonalAccessTokenHandler(\\n    pat,\\n    /** allowCrossOriginAuthentication */ true,\\n  );\\n\\n  const webApi = new nodeApi.WebApi(orgUrl, authHandler);\\n  const policyApi = await webApi.getPolicyApi();\\n\\n  const configurations = await policyApi.getPolicyConfigurations(projectId);\\n\\n  const buildValidations = configurations\\n    .filter(\\n      // we only want the build validations\\n      (configuration) => configuration.type?.displayName === \'Build\',\\n    )\\n    .map((configuration) => ({\\n      // map down to a custom type\\n      buildDefinitionId: configuration.settings.buildDefinitionId,\\n      name: configuration.settings.displayName,\\n      repositoryId:\\n        configuration.settings.scope?.length > 0\\n          ? configuration.settings.scope[0].repositoryId\\n          : undefined,\\n    }));\\n\\n  return buildValidations;\\n}\\n```\\n\\nThe above `getProjectBuildValidations` function acquires build validations for a given project, and maps them into this custom type:\\n\\n```ts\\ninterface BuildValidation {\\n  /** the id of the azure pipeline which is triggered by the build validation */\\n  buildDefinitionId: number | undefined;\\n  /** the name of the build validation */\\n  name: string | undefined;\\n  /** if a build validation is tied to a repository (probable) this is the repository id */\\n  repositoryId: string | undefined;\\n}\\n```\\n\\nWhich leaves us with a delightfully strongly typed array of build validations!\\n\\n## Conclusion\\n\\nThis post has detailed what build validations are in Azure DevOps. It has also demonstrated how to query the Azure DevOps API for them using the command line and using Node.js."},{"id":"static-web-apps-failed-to-deploy-the-azure-functions","metadata":{"permalink":"/static-web-apps-failed-to-deploy-the-azure-functions","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-07-07-static-web-apps-failed-to-deploy-the-azure-functions/index.md","source":"@site/blog/2022-07-07-static-web-apps-failed-to-deploy-the-azure-functions/index.md","title":"Azure Static Web Apps: Failed to deploy the Azure Functions","description":"Azure Static Web Apps presently have an issue which blocks deployment of Azure Functions with the message \\"Failed to deploy the Azure Functions\\". What is it?","date":"2022-07-07T00:00:00.000Z","tags":[{"inline":false,"label":"Azure Static Web Apps","permalink":"/tags/azure-static-web-apps","description":"The Azure Static Web Apps service."},{"inline":false,"label":"Azure Functions","permalink":"/tags/azure-functions","description":"The Azure Functions service."}],"readingTime":3.15,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"static-web-apps-failed-to-deploy-the-azure-functions","title":"Azure Static Web Apps: Failed to deploy the Azure Functions","authors":"johnnyreilly","tags":["azure static web apps","azure functions"],"image":"./title-image.png","description":"Azure Static Web Apps presently have an issue which blocks deployment of Azure Functions with the message \\"Failed to deploy the Azure Functions\\". What is it?","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Get Build Validations with the Azure DevOps API","permalink":"/azure-devops-api-build-validations"},"nextItem":{"title":"Azure Container Apps: dapr pubsub","permalink":"/azure-container-apps-pubsub"}},"content":"Azure Static Web Apps presently have an issue which blocks deployment of Azure Functions with the message \\"Failed to deploy the Azure Functions\\". This happens when the resource is tagged with an `EnvironmentId` tag and is discussed in [this GitHub issue](https://github.com/Azure/static-web-apps/issues/723). There is a workaround which we will examine.\\n\\n![title image reading \\"Azure Static Web Apps: Failed to deploy the Azure Functions\\" with an Azure Functions logo](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Failed to deploy the Azure Functions\\n\\n[Azure Static Web Apps](https://azure.microsoft.com/en-us/services/app-service/static/) are a combination of static front end hosting and an optional serverless API back end. The front end portion of Azure Static Web Apps is very plug and play. However there can be complexities when it comes to adding an API back end. One issue is a failure to deploy in the context of an Azure Pipeline which presents like this:\\n\\n![screenshot of an Azure Pipeines run featuring the words \\"Failed to deploy the Azure Functions\\"](screenshot-of-azure-pipelines-failed-to-deploy-the-azure-functions.png)\\n\\nAbove you can see the [`AzureStaticWebApp@0`](https://github.com/microsoft/azure-pipelines-tasks/tree/master/Tasks/AzureStaticWebAppV0) dedicated Azure Pipelines task that is used to deploy Azure Static Web Apps. It fails with the message \\"Failed to deploy the Azure Functions\\". There is no actionable feedback in this output, which makes coming up with remedies difficult.\\n\\nAs I was looking into this with Emad Khalifah of Microsoft Support, Emad pointed me to [this GitHub issue](https://github.com/Azure/static-web-apps/issues/723). Ironically, this had been raised by my colleage [John McCormick](https://github.com/johnmccormick99) back in February 2022.\\n\\nThe issue he experienced occurred when an `EnvironmentId` tag tag is present on the Azure Static Web Apps resource like so:\\n\\n![screenshot of the Azure Portal with a tag of \\"EnvironmentId\\"](screenshot-of-azure-portal-with-environmentid.png)\\n\\nWe do this in our organisation, and looking at the comments on the issue, it appears there are others out there doing the same. Given the name of the tag, this is not terribly surprising.\\n\\nUnfortunately, this issue has not been resolved. Can we work around the issue? Yes.\\n\\n## Fiddling with tags\\n\\nIn our organisation we enforce the tags we use in Azure strictly. So we found ourself in the bind of both needing tags for our own internal processes, but needing to work around the issue.\\n\\nHowever, the excellent [Ryan Cook](https://github.com/ryanmatcook) came up with a workaround. It works like this:\\n\\n- use the Azure CLI to remove the `EnvironmentId` tag from the static web app resource\\n- Deploy the static web app\\n- use the Azure CLI to add the `EnvironmentId` tag back to the static web app resource\\n\\nIt\'s hacky. It can require multiple runs to work (I ascribe this to eventual consistency issues in Azure; but that\'s not based on evidence). However, it can be a way forward. I acknowledge this is suboptimal.\\n\\nShould anyone else bump on this issue, here is the (hacky) workaround:\\n\\n```yml\\n- task: AzureCLI@2\\n  displayName: \'Remove EnvironmentId tag from resource group\'\\n  inputs:\\n    azureSubscription: ${{ variables.serviceConnection }}\\n    scriptType: bash\\n    scriptLocation: inlineScript\\n    inlineScript: |\\n      az tag update --resource-id /subscriptions/$(subscriptionId)/resourcegroups/$(resourceGroup) --operation delete --tags EnvironmentId=$(environmentId)\\n\\n# you may want to introduce a delay here, without a delay this can be somewhat unreliable\\n\\n- task: AzureStaticWebApp@0\\n  name: DeployStaticWebApp\\n  displayName: Deploy Static Web App\\n  inputs:\\n    app_location: \'MyApp\'\\n    output_location: \'dist\'\\n    api_location: \'api\'\\n    azure_static_web_apps_api_token: $(apiKey)\\n\\n- task: AzureCLI@2\\n  displayName: \'Add EnvironmentId tag back to resource group\'\\n  inputs:\\n    azureSubscription: ${{ variables.serviceConnection }}\\n    scriptType: bash\\n    scriptLocation: inlineScript\\n    inlineScript: |\\n      az tag update --resource-id /subscriptions/$(subscriptionId)/resourcegroups/$(resourceGroup) --operation merge --tags EnvironmentId=$(environmentId)\\n```\\n\\n## Conclusion\\n\\nAlthough I haven\'t tested it, the nature of the failure appears to be general; and so would likely affect deployments using GitHub Actions also. So if you\'re a GitHub Actions user, I suspect this approach could be tweaked and applied there also.\\n\\nIt would be tremendous to see [this issue](https://github.com/Azure/static-web-apps/issues/723) fixed within SWAs directly. It\'s not great that this issue has been marked as closed without (as far as we can tell) fixing the underlying problem."},{"id":"azure-container-apps-pubsub","metadata":{"permalink":"/azure-container-apps-pubsub","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-06-21-azure-container-apps-pubsub/index.md","source":"@site/blog/2022-06-21-azure-container-apps-pubsub/index.md","title":"Azure Container Apps: dapr pubsub","description":"This post shows how to build and deploy two Azure Container Apps using Bicep and GitHub Actions which communicate using daprs pubsub building block.","date":"2022-06-21T00:00:00.000Z","tags":[{"inline":false,"label":"Azure Container Apps","permalink":"/tags/azure-container-apps","description":"The Azure Container Apps service. Effectively a managed Kubernetes service."},{"inline":false,"label":"Bicep","permalink":"/tags/bicep","description":"The Bicep language for Azure Resource Manager templates."}],"readingTime":20.725,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-container-apps-pubsub","title":"Azure Container Apps: dapr pubsub","authors":"johnnyreilly","tags":["azure container apps","bicep"],"image":"./title-image.png","description":"This post shows how to build and deploy two Azure Container Apps using Bicep and GitHub Actions which communicate using daprs pubsub building block.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Azure Static Web Apps: Failed to deploy the Azure Functions","permalink":"/static-web-apps-failed-to-deploy-the-azure-functions"},"nextItem":{"title":"TypeScript 4.7 and ECMAScript Module Support","permalink":"/typescript-4-7-and-ecmascript-module-support"}},"content":"This post shows how to build and deploy two Azure Container Apps using Bicep and GitHub Actions. These apps will communicate using [dapr](https://docs.dapr.io/)\'s [publish & subscribe (pubsub) building block](https://docs.dapr.io/developing-applications/building-blocks/pubsub/howto-publish-subscribe/).\\n\\n![title image reading \\"Azure Container Apps: dapr pubsub\\"  with the dapr, Bicep, Azure Container Apps and GitHub Actions logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\nThis post will build upon code written in a [previous post](../2022-01-22-azure-container-apps-dapr-bicep-github-actions-debug-devcontainer/index.md) which built and deployed a simple web application to Azure Container Apps using Bicep and GitHub Actions using the GitHub container registry. Behind the scenes, that app was made up of a .NET app and a Node.js app communicating via dapr\'s [service invocation building block](https://docs.dapr.io/developing-applications/building-blocks/service-invocation/howto-invoke-discover-services/).\\n\\nThere\'s a good chance you\'ve just googled \\"pubsub dapr azure container apps\\" and you don\'t want to read through all this. You just want the code. That\'s fine. The code for this blogpost is [here](https://github.com/johnnyreilly/dapr-devcontainer-debug-and-deploy/releases/tag/v2.0.0).\\n\\n## You got mail: service invocation\\n\\nRight now we have a:\\n\\n- Node.js web app and a\\n- .NET app\\n\\nThe web app, when called, uses dapr service invocation to acquire a weather forecast from a .NET app.\\n\\nWhat we want to investigate is dapr\'s pubsub building block. But pubsub doesn\'t really \\"fit\\" into our current app. Let\'s alter it. Instead of showing users a weather forecast when they browse to the site, we\'ll instead look for our users to provide an email address, and we\'ll mail them a weather forecast.\\n\\nThis kind of app could work both using dapr service invocation or using pubsub. We\'re going to implement using our current service invocation approach first. Once that works, we\'ll then pivot that into using dapr pubsub.\\n\\nThis isn\'t rocket surgery; this is playing around with dapr and Azure Container Apps and seeing how they all hang together.\\n\\n### .NET meet mailgun\\n\\nOur existing .NET app needs the ability to send email. For that we\'re going to reach for [mailgun](https://app.mailgun.com/app/dashboard), and we\'ll use [RestSharp](https://restsharp.dev/) to call it. Let\'s add RestSharp as a dependency:\\n\\n```shell\\ndotnet add package RestSharp\\n```\\n\\nWith that in place, let\'s turn to our `WeatherForecastController` and make it send an email.\\n\\n```cs\\nusing Config;\\n\\nusing Microsoft.AspNetCore.Mvc;\\nusing Microsoft.Extensions.Options;\\n\\nusing RestSharp;\\nusing RestSharp.Authenticators;\\n\\nnamespace WeatherService.Controllers;\\n\\n[ApiController]\\npublic class WeatherForecastController : ControllerBase\\n{\\n    private static readonly string[] Summaries = new[]\\n    {\\n        \\"Freezing\\", \\"Bracing\\", \\"Chilly\\", \\"Cool\\", \\"Mild\\", \\"Warm\\", \\"Balmy\\", \\"Hot\\", \\"Sweltering\\", \\"Scorching\\"\\n    };\\n\\n    private readonly ILogger<WeatherForecastController> _logger;\\n    private readonly MailConfig _options;\\n\\n    public WeatherForecastController(\\n        ILogger<WeatherForecastController> logger,\\n        IOptions<MailConfig> options\\n    )\\n    {\\n        _logger = logger;\\n        _options = options.Value;\\n    }\\n\\n    public record SendWeatherForecastBody(string? Email);\\n\\n    [HttpPost(\\"SendWeatherForecast\\")]\\n    public async Task<string> SendWeatherForecast(SendWeatherForecastBody body)\\n    {\\n        try\\n        {\\n            if (string.IsNullOrEmpty(body.Email)) throw new Exception(\\"Email required\\");\\n\\n            var weatherForecast = Enumerable.Range(1, 5).Select(index => new WeatherForecast\\n            {\\n                Date = DateTime.Now.AddDays(index),\\n                TemperatureC = Random.Shared.Next(-20, 55),\\n                Summary = Summaries[Random.Shared.Next(Summaries.Length)]\\n            })\\n            .ToArray();\\n\\n            var toEmailAddress = body.Email;\\n            var text = $@\\"The weather forecast is:\\n\\n{string.Join(\\"\\\\n\\", weatherForecast.Select(wf => $\\"On {wf.Date} the weather will be {wf.Summary}\\"))}\\n\\";\\n\\n            await SendSimpleMessage(\\n                toEmailAddress: toEmailAddress,\\n                text: text\\n            );\\n\\n            return $\\"We have mailed {toEmailAddress} with the following:\\\\n\\\\n{text}\\";\\n        }\\n        catch (Exception exc)\\n        {\\n            _logger.LogError(exc, $\\"Problem!\\");\\n\\n            return exc.Message;\\n        }\\n    }\\n\\n    async Task<RestResponse> SendSimpleMessage(string toEmailAddress, string text)\\n    {\\n        RestClient client = new(new RestClientOptions\\n        {\\n            BaseUrl = new Uri(\\"https://api.mailgun.net/v3\\")\\n        })\\n        {\\n            Authenticator =\\n            new HttpBasicAuthenticator(\\"api\\", _options.MailgunApiKey)\\n        };\\n        RestRequest request = new();\\n        request.AddParameter(\\"domain\\", \\"mg.priou.co.uk\\", ParameterType.UrlSegment);\\n        request.Resource = \\"{domain}/messages\\";\\n        request.AddParameter(\\"from\\", \\"John Reilly <johnny_reilly@hotmail.com>\\");\\n        request.AddParameter(\\"to\\", toEmailAddress);\\n        request.AddParameter(\\"subject\\", \\"Weather forecast\\");\\n        request.AddParameter(\\"text\\", text);\\n\\n        return await client.PostAsync(request);\\n    }\\n}\\n```\\n\\nIn our new and improved controller we:\\n\\n- Switch our `GET` endpoint to be a `POST` one instead, to reflect that we\'re going to take an action (sending an email) each time it\'s hit. (RESTful to the end)\\n- Rather than returning the weather forecast to our caller, we take the email address supplied and we send the weather forecast to it\\n\\nYou\'ll also notice we\'re passing a `IOptions<MailConfig>` to the constructor of our class, it\'s in this configuration object we store our Mailgun api key. So we\'re going to need to define a `MailConfig` class:\\n\\n```cs\\npublic class MailConfig\\n{\\n    public string MailgunApiKey { get; set; } = string.Empty;\\n}\\n```\\n\\nAnd we need to update our `Program.cs` so it recognises `MailConfig` and configures it:\\n\\n```cs\\nvar builder = WebApplication.CreateBuilder(args);\\n\\nbuilder.Services.Configure<MailConfig>(builder.Configuration.GetSection(\\"Mail\\"));\\n// ...\\n```\\n\\nThanks to the default setup of .NET, we\'ll now be able to populate this using `appsettings.json` files and environment variables. Since our API key is a secret we\'ll avoid putting it in source control, and instead populate an environment variable that .NET can read:\\n\\n```\\nMAIL__MAILGUNAPIKEY=key-goes-here\\n```\\n\\nThe `__` above is the convention that .NET follows for nesting with environment variables; this is equivalent to the following structure in an `appsettings.json` file:\\n\\n```json\\n{\\n  \\"Mail\\": {\\n    \\"MailgunApiKey\\": \\"api-key-goes-here\\"\\n  }\\n  // ...\\n}\\n```\\n\\n### Webservice gets a form\\n\\nNow that we\'ve tweaked our WeatherService, we need to tweak the web site that calls it. We\'ll do that by first adding some dependencies that allow our Koa web service to handle routing a little easier:\\n\\n```shell\\nnpm install @koa/router koa-body --save\\nnpm install @types/koa__router --save-dev\\n```\\n\\nThen we\'ll tweak our `index.ts` like so:\\n\\n```ts\\nimport Koa from \'koa\';\\nimport Router from \'@koa/router\';\\nimport koaBody from \'koa-body\';\\nimport axios from \'axios\';\\n\\n// How we connect to the dotnet service with dapr\\nconst daprSidecarBaseUrl = `http://localhost:${\\n  process.env.DAPR_HTTP_PORT || 3501\\n}`;\\n// app id header for service discovery\\nconst weatherServiceAppIdHeaders = {\\n  \'dapr-app-id\': process.env.WEATHER_SERVICE_NAME || \'dotnet-app\',\\n};\\n\\nconst app = new Koa();\\nconst router = new Router();\\n\\napp.use(async (ctx, next) => {\\n  try {\\n    await next();\\n    const status = ctx.status || 404;\\n    if (status === 404) ctx.throw(404);\\n  } catch (err: any) {\\n    ctx.status = err.status || 500;\\n    ctx.body = ctx.status === 404 ? \'not found alas\' : `hmmm: ${ctx.status}`;\\n  }\\n});\\n\\nconst formHtml = (header: string) => `<!DOCTYPE html>\\n<html>\\n<head>\\n<title>Email me!</title>\\n</head>\\n<body>\\n<form method=\\"post\\">\\n    <h1>${header}</h1>\\n    <label for=\\"email\\">Enter your email:</label>\\n    <input type=\\"email\\" id=\\"email\\" name=\\"email\\" required>\\n    <button type=\\"submit\\">Submit</button>\\n</form>\\n</body>\\n</html>\\n`;\\n\\nrouter.get(\'/\', async (ctx, next) => {\\n  ctx.body = formHtml(\\"We\'d like to email you\\");\\n});\\n\\nrouter.post(\'/\', koaBody(), async (ctx, next) => {\\n  try {\\n    if (ctx.request.body.email) {\\n      await axios.post(\\n        `${daprSidecarBaseUrl}/SendWeatherForecast`,\\n        {\\n          email: ctx.request.body.email,\\n        },\\n        {\\n          headers: weatherServiceAppIdHeaders,\\n        },\\n      );\\n\\n      ctx.body = formHtml(\'Message sent\');\\n    } else {\\n      ctx.body = formHtml(\'No email supplied\');\\n    }\\n  } catch (exc) {\\n    console.error(\'Problem calling weather service\', exc);\\n    ctx.body = \'Something went wrong!\';\\n  }\\n});\\n\\napp.use(router.routes()).use(router.allowedMethods());\\n\\nconst portNumber = 3000;\\napp.listen(portNumber);\\nconsole.log(`listening on port ${portNumber}`);\\n```\\n\\nThe above leaves us with a very simple form based web app that sends an email containing weather forecast:\\n\\n![A gif that demos entering an email address in the form, submitting it and seeing an email arrive with a weather forecast in](demo-send-email.gif)\\n\\n### Secrets in Bicep\\n\\nWhilst we can run locally, we want to be able to deploy this. So we need to update our Bicep template to receive a `MAIL__MAILGUNAPIKEY` parameter:\\n\\n```bicep\\nparam branchName string\\n\\nparam webServiceImage string\\nparam webServicePort int\\nparam webServiceIsExternalIngress bool\\n\\nparam weatherServiceImage string\\nparam weatherServicePort int\\nparam weatherServiceIsExternalIngress bool\\n\\nparam containerRegistry string\\nparam containerRegistryUsername string\\n@secure()\\nparam containerRegistryPassword string\\n\\nparam tags object\\n\\n@secure()\\nparam MAIL__MAILGUNAPIKEY string\\n\\nparam location string = resourceGroup().location\\nvar minReplicas = 1\\nvar maxReplicas = 1\\n\\nvar branch = toLower(last(split(branchName, \'/\')))\\n\\nvar environmentName = \'${branch}-env\'\\nvar workspaceName = \'${branch}-log-analytics\'\\nvar appInsightsName = \'${branch}-app-insights\'\\nvar webServiceContainerAppName = \'${branch}-web\'\\nvar weatherServiceContainerAppName = \'${branch}-weather\'\\n\\nvar containerRegistryPasswordRef = \'container-registry-password\'\\nvar mailgunApiKeyRef = \'mailgun-api-key\'\\n\\nresource workspace \'Microsoft.OperationalInsights/workspaces@2021-12-01-preview\' = {\\n  name: workspaceName\\n  location: location\\n  tags: tags\\n  properties: {\\n    sku: {\\n      name: \'PerGB2018\'\\n    }\\n    retentionInDays: 30\\n    workspaceCapping: {}\\n  }\\n}\\n\\nresource appInsights \'Microsoft.Insights/components@2020-02-02\' = {\\n  name: appInsightsName\\n  location: location\\n  tags: tags\\n  kind: \'web\'\\n  properties: {\\n    Application_Type: \'web\'\\n    Flow_Type: \'Bluefield\'\\n  }\\n}\\n\\nresource environment \'Microsoft.App/managedEnvironments@2022-01-01-preview\' = {\\n  name: environmentName\\n  location: location\\n  tags: tags\\n  properties: {\\n    daprAIInstrumentationKey: appInsights.properties.InstrumentationKey\\n    appLogsConfiguration: {\\n      destination: \'log-analytics\'\\n      logAnalyticsConfiguration: {\\n        customerId: workspace.properties.customerId\\n        sharedKey: listKeys(workspace.id, workspace.apiVersion).primarySharedKey\\n      }\\n    }\\n  }\\n}\\n\\nresource weatherServiceContainerApp \'Microsoft.App/containerApps@2022-01-01-preview\' = {\\n  name: weatherServiceContainerAppName\\n  tags: tags\\n  location: location\\n  properties: {\\n    kubeEnvironmentId: environment.id\\n    configuration: {\\n      secrets: [\\n        {\\n          name: containerRegistryPasswordRef\\n          value: containerRegistryPassword\\n        }\\n        {\\n          name: mailgunApiKeyRef\\n          value: MAIL__MAILGUNAPIKEY\\n        }\\n      ]\\n      registries: [\\n        {\\n          server: containerRegistry\\n          username: containerRegistryUsername\\n          passwordSecretRef: containerRegistryPasswordRef\\n        }\\n      ]\\n      ingress: {\\n        external: weatherServiceIsExternalIngress\\n        targetPort: weatherServicePort\\n      }\\n    }\\n    template: {\\n      containers: [\\n        {\\n          image: weatherServiceImage\\n          name: weatherServiceContainerAppName\\n          env: [\\n            {\\n              name: \'MAIL__MAILGUNAPIKEY\'\\n              secretRef: mailgunApiKeyRef\\n            }\\n          ]\\n        }\\n      ]\\n      scale: {\\n        minReplicas: minReplicas\\n        maxReplicas: maxReplicas\\n      }\\n      dapr: {\\n        enabled: true\\n        appPort: weatherServicePort\\n        appId: weatherServiceContainerAppName\\n      }\\n    }\\n  }\\n}\\n\\nresource webServiceContainerApp \'Microsoft.App/containerApps@2022-01-01-preview\' = {\\n  name: webServiceContainerAppName\\n  tags: tags\\n  location: location\\n  properties: {\\n    kubeEnvironmentId: environment.id\\n    configuration: {\\n      secrets: [\\n        {\\n          name: containerRegistryPasswordRef\\n          value: containerRegistryPassword\\n        }\\n      ]\\n      registries: [\\n        {\\n          server: containerRegistry\\n          username: containerRegistryUsername\\n          passwordSecretRef: containerRegistryPasswordRef\\n        }\\n      ]\\n      ingress: {\\n        external: webServiceIsExternalIngress\\n        targetPort: webServicePort\\n      }\\n    }\\n    template: {\\n      containers: [\\n        {\\n          image: webServiceImage\\n          name: webServiceContainerAppName\\n          env: [\\n            {\\n              name: \'WEATHER_SERVICE_NAME\'\\n              value: weatherServiceContainerAppName\\n            }\\n          ]\\n        }\\n      ]\\n      scale: {\\n        minReplicas: minReplicas\\n        maxReplicas: maxReplicas\\n      }\\n      dapr: {\\n        enabled: true\\n        appPort: webServicePort\\n        appId: webServiceContainerAppName\\n      }\\n    }\\n  }\\n}\\n\\noutput webServiceUrl string = webServiceContainerApp.properties.latestRevisionFqdn\\n```\\n\\nWe can see that we treat the `MAIL__MAILGUNAPIKEY` as a secret. It\'s passed in using the `@secure` decorator and it\'s configured as a secret inside the `weatherServiceContainerApp` Azure Container App.\\n\\nWe have a GitHub Action that handles our deployment. We\'ll need to introduce the `MAIL__MAILGUNAPIKEY` secret both to the deploy step of the `build-and-deploy.yaml`:\\n\\n```yaml\\ndeploy:\\n  runs-on: ubuntu-latest\\n  needs: [build]\\n  steps:\\n    - name: Checkout repository\\n      uses: actions/checkout@v2\\n\\n    - name: Azure Login\\n      uses: azure/login@v1\\n      with:\\n        creds: ${{ secrets.AZURE_CREDENTIALS }}\\n\\n    - name: Deploy bicep\\n      uses: azure/CLI@v2\\n      if: github.event_name != \'pull_request\'\\n      with:\\n        inlineScript: |\\n          REF_SHA=\'${{ github.ref }}.${{ github.sha }}\'\\n          DEPLOYMENT_NAME=\\"${REF_SHA////-}\\"\\n          echo \\"DEPLOYMENT_NAME=$DEPLOYMENT_NAME\\"\\n\\n          TAGS=\'{\\"owner\\":\\"johnnyreilly\\", \\"email\\":\\"johnny_reilly@hotmail.com\\"}\'\\n          az deployment group create \\\\\\n            --resource-group ${{ env.RESOURCE_GROUP }} \\\\\\n            --name \\"$DEPLOYMENT_NAME\\" \\\\\\n            --template-file ./infra/main.bicep \\\\\\n            --parameters \\\\\\n                branchName=\'${{ github.event.number == 0 && \'main\' ||  format(\'pr-{0}\', github.event.number) }}\' \\\\\\n                webServiceImage=\'${{ needs.build.outputs.image-node }}\' \\\\\\n                webServicePort=3000 \\\\\\n                webServiceIsExternalIngress=true \\\\\\n                weatherServiceImage=\'${{ needs.build.outputs.image-dotnet }}\' \\\\\\n                weatherServicePort=5000 \\\\\\n                weatherServiceIsExternalIngress=false \\\\\\n                containerRegistry=${{ env.REGISTRY }} \\\\\\n                containerRegistryUsername=${{ github.actor }} \\\\\\n                containerRegistryPassword=${{ secrets.PACKAGES_TOKEN }} \\\\\\n                tags=\\"$TAGS\\" \\\\\\n                MAIL__MAILGUNAPIKEY=\\"${{ secrets.MAIL__MAILGUNAPIKEY }}\\"\\n```\\n\\nAnd we\'ll need to create the associated secret as well:\\n\\n![Screenshot of the secrets in the GitHub website that we need to create including MAIL__MAILGUNAPIKEY](screenshot-github-secrets.png)\\n\\n## You got mail: pubsub!\\n\\nSo we\'re now at the point where we have a pubsub style app - but still implemented using the service invocation approach. It\'s time to start migrating to using dapr\'s pubsub capabilities. Now, caveat emptor, pivoting from service invocation involves a fair amount of code. I\'ll try and be as brief as I can as we make the switch. However there will be big ol\' lumps of code blocks as we do this. You may find it easier to just examine the finished code. I will in no way feel bad if that\'s the path you follow.\\n\\n### Publishing with dapr-client\\n\\nThe first thing we need, is for our Node.js app to publish using the dapr pubsub mechanism. The easiest way to do that is with the [dapr-client](https://docs.dapr.io/developing-applications/sdks/js/):\\n\\n```shell\\nnpm install dapr-client --save\\n```\\n\\nWe then switch out using axios to send our email command, to use `dapr-client` instead:\\n\\n```ts\\nimport Koa from \'koa\';\\nimport Router from \'@koa/router\';\\nimport koaBody from \'koa-body\';\\n\\nimport { DaprClient } from \'dapr-client\';\\n\\nconst daprHost = \'localhost\'; // Dapr Sidecar Host\\nconst daprPort = `${process.env.DAPR_HTTP_PORT || 3501}`; // Dapr Sidecar Port\\n\\nconst client = new DaprClient(daprHost, daprPort);\\n\\nconst app = new Koa();\\nconst router = new Router();\\n\\napp.use(async (ctx, next) => {\\n  try {\\n    await next();\\n    const status = ctx.status || 404;\\n    if (status === 404) ctx.throw(404);\\n  } catch (err: any) {\\n    ctx.status = err.status || 500;\\n    ctx.body = ctx.status === 404 ? \'not found alas\' : `hmmm: ${ctx.status}`;\\n  }\\n});\\n\\nconst formHtml = (header: string) => `<!DOCTYPE html>\\n<html>\\n<head>\\n<title>Email me!</title>\\n</head>\\n<body>\\n<form method=\\"post\\">\\n    <h1>${header}</h1>\\n    <label for=\\"email\\">Enter your email:</label>\\n    <input type=\\"email\\" id=\\"email\\" name=\\"email\\" required>\\n    <button type=\\"submit\\">Submit</button>\\n</form>\\n</body>\\n</html>\\n`;\\n\\nrouter.get(\'/\', async (ctx, next) => {\\n  ctx.body = formHtml(\\"We\'d like to email you\\");\\n});\\n\\nrouter.post(\'/\', koaBody(), async (ctx, next) => {\\n  try {\\n    if (ctx.request.body.email) {\\n      // Send a message\\n      const sent = await client.pubsub.publish(\\n        /* pubSubName */ \'weather-forecast-pub-sub\',\\n        /* topic */ \'weather-forecasts\',\\n        /* data */ {\\n          email: ctx.request.body.email,\\n        },\\n      );\\n\\n      ctx.body = formHtml(`Message sent: ${sent}`);\\n    } else {\\n      ctx.body = formHtml(\'No email supplied\');\\n    }\\n  } catch (exc) {\\n    console.error(\'Problem calling weather service\', exc);\\n    ctx.body = \'Something went wrong!\';\\n  }\\n});\\n\\napp.use(router.routes()).use(router.allowedMethods());\\n\\nconst portNumber = 3000;\\napp.listen(portNumber);\\nconsole.log(`listening on port ${portNumber}`);\\n```\\n\\nThe thing to note above is the `client.pubsub.publish`; our WebService will now be publishing using pubsub, instead of using axios and service invocation.\\n\\n### Subscribing\\n\\nOur WeatherService needs to be able to receive what is published. To make that happen, we\'ll make use of the following NuGet package in WeatherService:\\n\\n```shell\\ndotnet add package Dapr.AspNetCore --version 1.7.0\\n```\\n\\nOur `Program.cs` is adjusted to cater for this:\\n\\n```cs\\nusing Config;\\n\\nvar builder = WebApplication.CreateBuilder(args);\\n\\nbuilder.Services.Configure<MailConfig>(builder.Configuration.GetSection(\\"Mail\\"));\\n\\nbuilder.Services.AddControllers().AddDapr();\\n// Learn more about configuring Swagger/OpenAPI at https://aka.ms/aspnetcore/swashbuckle\\nbuilder.Services.AddEndpointsApiExplorer();\\nbuilder.Services.AddSwaggerGen();\\n\\nvar app = builder.Build();\\n\\n// Configure the HTTP request pipeline.\\nif (app.Environment.IsDevelopment())\\n{\\n    app.UseSwagger();\\n    app.UseSwaggerUI();\\n}\\n\\napp.UseAuthorization();\\n\\napp.UseCloudEvents();\\n\\napp.MapSubscribeHandler(); // This is the Dapr subscribe handler\\napp.MapControllers();\\n\\napp.Run();\\n```\\n\\nThe significant pieces above are:\\n\\n```cs\\nbuilder.Services.AddControllers().AddDapr();\\n\\n// ...\\n\\napp.UseCloudEvents();\\napp.MapSubscribeHandler(); // This is the Dapr subscribe handler\\n```\\n\\nWe\'ll also need to update our `WeatherForecastController.cs`:\\n\\n```cs\\nusing Config;\\n\\nusing Microsoft.AspNetCore.Mvc;\\nusing Microsoft.Extensions.Options;\\n\\nusing RestSharp;\\nusing RestSharp.Authenticators;\\n\\nusing Dapr;\\n\\nnamespace WeatherService.Controllers;\\n\\n[ApiController]\\npublic class WeatherForecastController : ControllerBase\\n{\\n    private static readonly string[] Summaries = new[]\\n    {\\n        \\"Freezing\\", \\"Bracing\\", \\"Chilly\\", \\"Cool\\", \\"Mild\\", \\"Warm\\", \\"Balmy\\", \\"Hot\\", \\"Sweltering\\", \\"Scorching\\"\\n    };\\n\\n    private readonly ILogger<WeatherForecastController> _logger;\\n    private readonly MailConfig _options;\\n\\n    public WeatherForecastController(\\n        ILogger<WeatherForecastController> logger,\\n        IOptions<MailConfig> options\\n    )\\n    {\\n        _logger = logger;\\n        _options = options.Value;\\n    }\\n\\n    public record SendWeatherForecastBody(string? Email);\\n\\n    [Topic(pubsubName: \\"weather-forecast-pub-sub\\", name: \\"weather-forecasts\\")]\\n    [HttpPost(\\"SendWeatherForecast\\")]\\n    public async Task<ActionResult<string>> SendWeatherForecast(SendWeatherForecastBody body)\\n    {\\n        try\\n        {\\n            if (string.IsNullOrEmpty(body.Email)) throw new Exception(\\"Email required\\");\\n\\n            var weatherForecast = Enumerable.Range(1, 5).Select(index => new WeatherForecast\\n            {\\n                Date = DateTime.Now.AddDays(index),\\n                TemperatureC = Random.Shared.Next(-20, 55),\\n                Summary = Summaries[Random.Shared.Next(Summaries.Length)]\\n            })\\n            .ToArray();\\n\\n            var toEmailAddress = body.Email;\\n            var text = $@\\"The weather forecast is:\\n\\n{string.Join(\\"\\\\n\\", weatherForecast.Select(wf => $\\"On {wf.Date} the weather will be {wf.Summary}\\"))}\\n\\";\\n\\n            await SendSimpleMessage(\\n                toEmailAddress: toEmailAddress,\\n                text: text\\n            );\\n\\n            return Ok($\\"We have mailed {toEmailAddress} with the following:\\\\n\\\\n{text})\\");\\n        }\\n        catch (Exception exc)\\n        {\\n            _logger.LogError(exc, $\\"Problem!\\");\\n\\n            return BadRequest(exc.Message);\\n        }\\n    }\\n\\n    async Task<RestResponse> SendSimpleMessage(string toEmailAddress, string text)\\n    {\\n        RestClient client = new(new RestClientOptions\\n        {\\n            BaseUrl = new Uri(\\"https://api.mailgun.net/v3\\")\\n        })\\n        {\\n            Authenticator =\\n            new HttpBasicAuthenticator(\\"api\\", _options.MailgunApiKey)\\n        };\\n        RestRequest request = new();\\n        request.AddParameter(\\"domain\\", \\"mg.priou.co.uk\\", ParameterType.UrlSegment);\\n        request.Resource = \\"{domain}/messages\\";\\n        request.AddParameter(\\"from\\", \\"John Reilly <johnny_reilly@hotmail.com>\\");\\n        request.AddParameter(\\"to\\", toEmailAddress);\\n        request.AddParameter(\\"subject\\", \\"Weather forecast\\");\\n        request.AddParameter(\\"text\\", text);\\n\\n        return await client.PostAsync(request);\\n    }\\n}\\n```\\n\\nReally the only new thing here is the `Topic` attribute on the `SendWeatherForecast` endpoint:\\n\\n```cs\\n[Topic(pubsubName: \\"weather-forecast-pub-sub\\", name: \\"weather-forecasts\\")]\\n```\\n\\nThis is used (as you might imagine) to route messages.\\n\\nThe real difference to call out in what we\'ve done so far, is that both our publisher (Node.js) and our subscriber (.NET) have become \\"dapr aware\\". Although there have been changes in our code to achieve this, they have not been extensive. Noisy, yes. But not big changes.\\n\\n### Components\\n\\nIn order to communicate via pubsub, dapr needs some [components](https://docs.dapr.io/concepts/components-concept/) in place. We\'ll create a folder in the root of our project named `components`, and in there create three files:\\n\\n#### `pubsub.yaml`\\n\\n```yml\\napiVersion: dapr.io/v1alpha1\\nkind: Component\\nmetadata:\\n  name: weather-forecast-pub-sub\\n  namespace: default\\nspec:\\n  type: pubsub.redis\\n  version: v1\\n  metadata:\\n    - name: redisHost\\n      value: localhost:6379\\n    - name: redisPassword\\n      value: \'\'\\nscopes:\\n  - node-app\\n  - dotnet-app\\n```\\n\\n#### `statestore.yaml`\\n\\n```yml\\napiVersion: dapr.io/v1alpha1\\nkind: Component\\nmetadata:\\n  name: statestore\\n  namespace: default\\nspec:\\n  type: state.redis\\n  version: v1\\n  metadata:\\n    - name: redisHost\\n      value: localhost:6379\\n    - name: redisPassword\\n      value: \'\'\\n    - name: actorStateStore\\n      value: \'true\'\\n```\\n\\n#### `subscription.yaml`\\n\\n```yml\\napiVersion: dapr.io/v1alpha1\\nkind: Subscription\\nmetadata:\\n  name: weather-forecast-pub-sub\\nspec:\\n  topic: weather-forecasts\\n  route: /SendWeatherForecast\\n  pubsubname: weather-forecast-pub-sub\\nscopes:\\n  - node-app\\n  - dotnet-app\\n```\\n\\nThese three files are fairly self-explanatory. It\'s worth drawing attention to the following though:\\n\\n1. We\'re going to use these components when running locally and so we\'ll use Redis for our persistence. When we deploy to Azure Container Apps we\'ll use something more Azure specific.\\n2. We\'re granting access in these components to our node-app (WebService) and our dotnet-app (WeatherService)\\n3. We\'re wiring up our subscription in `subscription.yaml`- it\'s this that will be used to route traffic from publishing to subscription.\\n\\nWith the above in place we\'re almost ready to be able to run this locally and debug using VS Code. The final tweak is to make our apps aware of the dapr components. This is achieved by adding `\\"componentsPath\\": \\"./components\\",` to the entries in our `tasks.json` file. In full it looks like this:\\n\\n```json\\n{\\n  // See https://go.microsoft.com/fwlink/?LinkId=733558\\n  // for the documentation about the tasks.json format\\n  \\"version\\": \\"2.0.0\\",\\n  \\"tasks\\": [\\n    {\\n      \\"label\\": \\"dotnet-build\\",\\n      \\"command\\": \\"dotnet\\",\\n      \\"type\\": \\"process\\",\\n      \\"args\\": [\\n        \\"build\\",\\n        \\"${workspaceFolder}/WeatherService/WeatherService.csproj\\",\\n        \\"/property:GenerateFullPaths=true\\",\\n        \\"/consoleloggerparameters:NoSummary\\"\\n      ],\\n      \\"problemMatcher\\": \\"$msCompile\\"\\n    },\\n    {\\n      \\"label\\": \\"daprd-debug-dotnet\\",\\n      \\"appId\\": \\"dotnet-app\\",\\n      \\"appPort\\": 5000,\\n      \\"httpPort\\": 3500,\\n      \\"grpcPort\\": 50000,\\n      \\"metricsPort\\": 9090,\\n      \\"componentsPath\\": \\"./components\\",\\n      \\"type\\": \\"daprd\\",\\n      \\"dependsOn\\": [\\"dotnet-build\\"]\\n    },\\n    {\\n      \\"label\\": \\"daprd-down-dotnet\\",\\n      \\"appId\\": \\"dotnet-app\\",\\n      \\"type\\": \\"daprd-down\\"\\n    },\\n\\n    {\\n      \\"label\\": \\"npm-install\\",\\n      \\"type\\": \\"shell\\",\\n      \\"command\\": \\"npm install\\",\\n      \\"options\\": {\\n        \\"cwd\\": \\"${workspaceFolder}/WebService\\"\\n      }\\n    },\\n    {\\n      \\"label\\": \\"webservice-build\\",\\n      \\"type\\": \\"typescript\\",\\n      \\"tsconfig\\": \\"WebService/tsconfig.json\\",\\n      \\"problemMatcher\\": [\\"$tsc\\"],\\n      \\"group\\": {\\n        \\"kind\\": \\"build\\",\\n        \\"isDefault\\": true\\n      },\\n      \\"dependsOn\\": [\\"npm-install\\"]\\n    },\\n    {\\n      \\"label\\": \\"daprd-debug-node\\",\\n      \\"appId\\": \\"node-app\\",\\n      \\"appPort\\": 3000,\\n      \\"httpPort\\": 3501,\\n      \\"grpcPort\\": 50001,\\n      \\"metricsPort\\": 9091,\\n      \\"componentsPath\\": \\"./components\\",\\n      \\"type\\": \\"daprd\\",\\n      \\"dependsOn\\": [\\"webservice-build\\"]\\n    },\\n    {\\n      \\"label\\": \\"daprd-down-node\\",\\n      \\"appId\\": \\"node-app\\",\\n      \\"type\\": \\"daprd-down\\"\\n    }\\n  ]\\n}\\n```\\n\\nWith this in place we\'re ready to run our apps locally using pubsub. We can publish from the WebService and receive in the WeatherService. This results in the expected email being sent, as we would hope.\\n\\n### Bicep\\n\\nThe missing piece is Azure. How do we deploy this to Azure Container Apps? Well, we have everything we need to do this, save for the Bicep. We need to augment the Bicep we already have to include our Azure Container Apps dapr components. The full template looks like this:\\n\\n```bicep\\nparam branchName string\\n\\nparam webServiceImage string\\nparam webServicePort int\\nparam webServiceIsExternalIngress bool\\n\\nparam weatherServiceImage string\\nparam weatherServicePort int\\nparam weatherServiceIsExternalIngress bool\\n\\nparam containerRegistry string\\nparam containerRegistryUsername string\\n@secure()\\nparam containerRegistryPassword string\\n\\nparam tags object\\n\\n@secure()\\nparam MAIL__MAILGUNAPIKEY string\\n\\nparam location string = resourceGroup().location\\n\\n@description(\'Storage Account type\')\\n@allowed([\\n  \'Premium_LRS\'\\n  \'Premium_ZRS\'\\n  \'Standard_GRS\'\\n  \'Standard_GZRS\'\\n  \'Standard_LRS\'\\n  \'Standard_RAGRS\'\\n  \'Standard_RAGZRS\'\\n  \'Standard_ZRS\'\\n])\\nparam storageAccountType string = \'Standard_LRS\'\\n\\n@description(\'The name of the Storage Account\')\\nparam storageAccountName string = \'store${uniqueString(resourceGroup().id)}\'\\n\\nparam serviceBusNamespace string = \'pubsub-namespace\'\\n\\nresource storageAccount \'Microsoft.Storage/storageAccounts@2021-06-01\' = {\\n  name: storageAccountName\\n  location: location\\n  sku: {\\n    name: storageAccountType\\n  }\\n  kind: \'StorageV2\'\\n  properties: {}\\n}\\n\\nresource serviceBus \'Microsoft.ServiceBus/namespaces@2021-06-01-preview\' = {\\n  name: serviceBusNamespace\\n  location: location\\n}\\n\\nresource servicebus_authrule \'Microsoft.ServiceBus/namespaces/AuthorizationRules@2021-06-01-preview\' existing = {\\n  name: \'RootManageSharedAccessKey\'\\n  parent: serviceBus\\n}\\n\\nresource topic \'Microsoft.ServiceBus/namespaces/topics@2021-06-01-preview\' = {\\n  name: \'weather-forecasts\'\\n  parent: serviceBus\\n}\\n\\nvar minReplicas = 1\\nvar maxReplicas = 1\\n\\nvar branch = toLower(last(split(branchName, \'/\')))\\n\\nvar environmentName = \'shared-env\'\\nvar workspaceName = \'${branch}-log-analytics\'\\nvar appInsightsName = \'${branch}-app-insights\'\\nvar webServiceContainerAppName = \'${branch}-web\'\\nvar weatherServiceContainerAppName = \'${branch}-weather\'\\n\\nvar containerRegistryPasswordRef = \'container-registry-password\'\\nvar mailgunApiKeyRef = \'mailgun-api-key\'\\n\\nresource workspace \'Microsoft.OperationalInsights/workspaces@2021-12-01-preview\' = {\\n  name: workspaceName\\n  location: location\\n  tags: tags\\n  properties: {\\n    sku: {\\n      name: \'PerGB2018\'\\n    }\\n    retentionInDays: 30\\n    workspaceCapping: {}\\n  }\\n}\\n\\nresource appInsights \'Microsoft.Insights/components@2020-02-02\' = {\\n  name: appInsightsName\\n  location: location\\n  tags: tags\\n  kind: \'web\'\\n  properties: {\\n    Application_Type: \'web\'\\n    Flow_Type: \'Bluefield\'\\n  }\\n}\\n\\nresource environment \'Microsoft.App/managedEnvironments@2022-01-01-preview\' = {\\n  name: environmentName\\n  location: location\\n  tags: tags\\n  properties: {\\n    daprAIInstrumentationKey: appInsights.properties.InstrumentationKey\\n    appLogsConfiguration: {\\n      destination: \'log-analytics\'\\n      logAnalyticsConfiguration: {\\n        customerId: workspace.properties.customerId\\n        sharedKey: listKeys(workspace.id, workspace.apiVersion).primarySharedKey\\n      }\\n    }\\n  }\\n  resource statestoreComponent \'daprComponents@2022-03-01\' = {\\n    name: \'statestore\'\\n    properties: {\\n      componentType: \'state.azure.blobstorage\'\\n      version: \'v1\'\\n      ignoreErrors: false\\n      initTimeout: \'5s\'\\n      secrets: [\\n        {\\n          name: \'storageaccountkey\'\\n          value: listKeys(resourceId(\'Microsoft.Storage/storageAccounts/\', storageAccount.name), storageAccount.apiVersion).keys[0].value\\n        }\\n      ]\\n      metadata: [\\n        {\\n          name: \'accountName\'\\n          value: storageAccount.name\\n        }\\n        {\\n          name: \'containerName\'\\n          value: \'storage_container_name\'\\n        }\\n        {\\n          name: \'accountKey\'\\n          secretRef: \'storageaccountkey\'\\n        }\\n      ]\\n      scopes: [\\n        weatherServiceContainerAppName\\n        webServiceContainerAppName\\n      ]\\n    }\\n  }\\n  resource pubsubComponent \'daprComponents@2022-03-01\' = {\\n    name: \'weather-forecast-pub-sub\'\\n    properties: {\\n      componentType: \'pubsub.azure.servicebus\'\\n      version: \'v1\'\\n      metadata: [\\n        {\\n          name: \'connectionString\'\\n          secretRef: \'sb-root-connectionstring\'\\n        }\\n      ]\\n      secrets: [\\n        {\\n          name: \'sb-root-connectionstring\'\\n          value: listKeys(\'${serviceBus.id}/AuthorizationRules/RootManageSharedAccessKey\', serviceBus.apiVersion).primaryConnectionString\\n        }\\n      ]\\n      scopes: [\\n        weatherServiceContainerAppName\\n        webServiceContainerAppName\\n      ]\\n    }\\n  }\\n}\\n\\nresource weatherServiceContainerApp \'Microsoft.App/containerApps@2022-01-01-preview\' = {\\n  name: weatherServiceContainerAppName\\n  tags: tags\\n  location: location\\n  properties: {\\n    managedEnvironmentId: environment.id\\n    configuration: {\\n      dapr: {\\n        enabled: true\\n        appPort: weatherServicePort\\n        appId: weatherServiceContainerAppName\\n      }\\n      secrets: [\\n        {\\n          name: containerRegistryPasswordRef\\n          value: containerRegistryPassword\\n        }\\n        {\\n          name: mailgunApiKeyRef\\n          value: MAIL__MAILGUNAPIKEY\\n        }\\n      ]\\n      registries: [\\n        {\\n          server: containerRegistry\\n          username: containerRegistryUsername\\n          passwordSecretRef: containerRegistryPasswordRef\\n        }\\n      ]\\n      ingress: {\\n        external: weatherServiceIsExternalIngress\\n        targetPort: weatherServicePort\\n      }\\n    }\\n    template: {\\n      containers: [\\n        {\\n          image: weatherServiceImage\\n          name: weatherServiceContainerAppName\\n          env: [\\n            {\\n              name: \'MAIL__MAILGUNAPIKEY\'\\n              secretRef: mailgunApiKeyRef\\n            }\\n          ]\\n        }\\n      ]\\n      scale: {\\n        minReplicas: minReplicas\\n        maxReplicas: maxReplicas\\n      }\\n    }\\n  }\\n}\\n\\nresource webServiceContainerApp \'Microsoft.App/containerApps@2022-01-01-preview\' = {\\n  name: webServiceContainerAppName\\n  tags: tags\\n  location: location\\n  properties: {\\n    managedEnvironmentId: environment.id\\n    configuration: {\\n      dapr: {\\n        enabled: true\\n        appPort: webServicePort\\n        appId: webServiceContainerAppName\\n      }\\n      secrets: [\\n        {\\n          name: containerRegistryPasswordRef\\n          value: containerRegistryPassword\\n        }\\n      ]\\n      registries: [\\n        {\\n          server: containerRegistry\\n          username: containerRegistryUsername\\n          passwordSecretRef: containerRegistryPasswordRef\\n        }\\n      ]\\n      ingress: {\\n        external: webServiceIsExternalIngress\\n        targetPort: webServicePort\\n      }\\n    }\\n    template: {\\n      containers: [\\n        {\\n          image: webServiceImage\\n          name: webServiceContainerAppName\\n          env: [\\n            {\\n              name: \'WEATHER_SERVICE_NAME\'\\n              value: weatherServiceContainerAppName\\n            }\\n          ]\\n        }\\n      ]\\n      scale: {\\n        minReplicas: minReplicas\\n        maxReplicas: maxReplicas\\n      }\\n    }\\n  }\\n}\\n\\noutput webServiceUrl string = webServiceContainerApp.properties.latestRevisionFqdn\\n```\\n\\nNow this is undeniably a big lump of Bicep. Let\'s drill into the significant differences:\\n\\n1. We\'re creating an Azure storage account.\\n2. We\'re creating an Azure Service Bus and a topic under it named `\'weather-forecasts\'`.\\n3. Underneath our managed environment, we\'re creating a statestore (using the storage account) which is the Azure equivalent of our `statestore.yml`, but using Azure storage.\\n4. Also underneath our managed environment, we\'re creating a pubsub (using the service bus) which is the Azure equivalent of our `pubsub.yml`, but using our Azure ServiceBus.\\n\\nIt\'s also worth noting that we always have an instance of the services running; `minReplicas: 1`. This is because when we dial it down to 0, the Weather Service will stop running. Probably there\'s a fancy KEDA trigger that prevents this; I haven\'t figured it out.\\n\\n### No declarative pubsub subscription support\\n\\nWhilst you might be thinking \\"we\'re home free now!\\" - it turns out we\'re not. Whilst we\'d created Azure equivalents of our `statestore.yml` and `pubsub.yml`, you\'ll note there didn\'t seem to be an equivalent of the `subscriptions` component in Bicep.\\n\\n[It turns out support for declarative pub/sub subscriptions is not yet available](https://docs.microsoft.com/en-us/azure/container-apps/dapr-overview?tabs=bicep1%2Cyaml#known-limitations):\\n\\n> Known limitations\\n> Declarative pub/sub subscriptions\\n\\nSo whilst we can take the code we have here and run locally, we cannot deploy it to Azure.\\n\\nHowever, whilst there\'s no declaritive support for subscriptions, there is programmatic support. It involves more of a pivot in how we put together our code. But since it\'s the only game in town, we\'ll give it a go.\\n\\n## You got mail: programmatic subscriptions!\\n\\nWe can get rid of our `subscriptions.yaml` file now - we\'re going programmatic instead of declarative.\\n\\nWe\'re going to replace our `WeatherForecastController.cs` with a `WeatherForecastEndpoints.cs` which contains very similar code, but uses the .NET 6 minimal API approach instead: (There appears to be a way to work with MVC but it\'s not clear how to use it, and it appears to be a more confusing approach than the .NET 6 minimal API approach.)\\n\\n```cs\\nusing Config;\\n\\nusing Microsoft.Extensions.Options;\\n\\nusing RestSharp;\\nusing RestSharp.Authenticators;\\n\\nusing Dapr;\\n\\nnamespace WeatherService.Endpoints;\\n\\npublic static class WeatherForecastEndpoints\\n{\\n    private static readonly string[] Summaries = new[]\\n    {\\n        \\"Freezing\\", \\"Bracing\\", \\"Chilly\\", \\"Cool\\", \\"Mild\\", \\"Warm\\", \\"Balmy\\", \\"Hot\\", \\"Sweltering\\", \\"Scorching\\"\\n    };\\n\\n    public record SendWeatherForecastBody(string? Email);\\n\\n    public static IEndpointRouteBuilder MapWeatherForecastEndpoints(this IEndpointRouteBuilder endpoints)\\n    {\\n        endpoints.MapPost(\\"/SendWeatherForecast\\",\\n            [Topic(\\"weather-forecast-pub-sub\\", \\"weather-forecasts\\")]\\n            async (\\n                SendWeatherForecastBody body,\\n                ILogger<SendWeatherForecastBody> logger,\\n                IOptions<MailConfig> options\\n            ) =>\\n            {\\n                try\\n                {\\n                    if (string.IsNullOrEmpty(body.Email)) throw new Exception(\\"Email required\\");\\n\\n                    var weatherForecast = Enumerable.Range(1, 5).Select(index => new WeatherForecast\\n                    {\\n                        Date = DateTime.Now.AddDays(index),\\n                        TemperatureC = Random.Shared.Next(-20, 55),\\n                        Summary = Summaries[Random.Shared.Next(Summaries.Length)]\\n                    })\\n                    .ToArray();\\n\\n                    var toEmailAddress = body.Email;\\n                    var text = $@\\"The weather forecast is:\\n\\n{string.Join(\\"\\\\n\\", weatherForecast.Select(wf => $\\"On {wf.Date} the weather will be {wf.Summary}\\"))}\\n\\";\\n\\n                    await SendSimpleMessage(\\n                        mailgunApiKey: options.Value.MailgunApiKey,\\n                        toEmailAddress: toEmailAddress,\\n                        text: text\\n                    );\\n\\n                    return Results.Ok($\\"We have mailed {toEmailAddress} with the following:\\\\n\\\\n{text})\\");\\n                }\\n                catch (Exception exc)\\n                {\\n                    logger.LogError(exc, $\\"Problem!\\");\\n\\n                    return Results.BadRequest(exc.Message);\\n                }\\n            });\\n\\n        return endpoints;\\n    }\\n\\n    static async Task<RestResponse> SendSimpleMessage(string mailgunApiKey, string toEmailAddress, string text)\\n    {\\n        RestClient client = new(new RestClientOptions\\n        {\\n            BaseUrl = new Uri(\\"https://api.mailgun.net/v3\\")\\n        })\\n        {\\n            Authenticator =\\n            new HttpBasicAuthenticator(\\"api\\", mailgunApiKey)\\n        };\\n        RestRequest request = new();\\n        request.AddParameter(\\"domain\\", \\"mg.priou.co.uk\\", ParameterType.UrlSegment);\\n        request.Resource = \\"{domain}/messages\\";\\n        request.AddParameter(\\"from\\", \\"John Reilly <johnny_reilly@hotmail.com>\\");\\n        request.AddParameter(\\"to\\", toEmailAddress);\\n        request.AddParameter(\\"subject\\", \\"Weather forecast\\");\\n        request.AddParameter(\\"text\\", text);\\n\\n        return await client.PostAsync(request);\\n    }\\n}\\n```\\n\\nThe significant thing to note above is the `[Topic(\\"weather-forecast-pub-sub\\", \\"weather-forecasts\\")]` that we\'re adding to our `MapPost` in the `MapWeatherForecastEndpoints` method. This is the equivalent of the `subscriptions` component that we wanted to create in Bicep but couldn\'t. This is our programmatic subscription.\\n\\nWe also need to tweak our `Program.cs` to cater for the new `WeatherForecastEndpoints` class:\\n\\n```cs\\nusing Config;\\nusing WeatherService.Endpoints;\\n\\nvar builder = WebApplication.CreateBuilder(args);\\n\\nbuilder.Services.Configure<MailConfig>(builder.Configuration.GetSection(\\"Mail\\"));\\n\\nbuilder.Services.AddControllers().AddDapr();\\n// Learn more about configuring Swagger/OpenAPI at https://aka.ms/aspnetcore/swashbuckle\\nbuilder.Services.AddEndpointsApiExplorer();\\nbuilder.Services.AddSwaggerGen();\\n\\nvar app = builder.Build();\\n\\n// Configure the HTTP request pipeline.\\nif (app.Environment.IsDevelopment())\\n{\\n    app.UseSwagger();\\n    app.UseSwaggerUI();\\n}\\n\\napp.UseAuthorization();\\n\\napp.UseCloudEvents();\\n\\napp.MapSubscribeHandler(); // This is the Dapr subscribe handler\\n\\napp.MapWeatherForecastEndpoints();\\n\\napp.Run();\\n```\\n\\nSo the `app.MapWeatherForecastEndpoints();` is what wires up our `WeatherForecastEndpoints` class.\\n\\nWith that in place, we\'re ready to deploy to Azure.\\n\\n![A gif that demos entering an email address in the form, submitting it and seeing an email arrive with a weather forecast in](demo-send-email-with-pubsub.gif)\\n\\nWe now have Azure Container Apps running in Azure, using the dapr pubsub component. Hopefully in future declarative subscribtions will be available also, but for now we can use the programmatic approach."},{"id":"typescript-4-7-and-ecmascript-module-support","metadata":{"permalink":"/typescript-4-7-and-ecmascript-module-support","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-06-07-typescript-4-7-and-ecmascript-module-support/index.md","source":"@site/blog/2022-06-07-typescript-4-7-and-ecmascript-module-support/index.md","title":"TypeScript 4.7 and ECMAScript Module Support","description":"As part of the TypeScript 4.7 release comes a major upgrade to ECMAScript Module Support for Node.js. This post takes a look at what that means.","date":"2022-06-07T00:00:00.000Z","tags":[{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."},{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."}],"readingTime":6.1,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"typescript-4-7-and-ecmascript-module-support","title":"TypeScript 4.7 and ECMAScript Module Support","authors":"johnnyreilly","tags":["typescript","javascript"],"image":"./title-image.png","description":"As part of the TypeScript 4.7 release comes a major upgrade to ECMAScript Module Support for Node.js. This post takes a look at what that means.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Azure Container Apps: dapr pubsub","permalink":"/azure-container-apps-pubsub"},"nextItem":{"title":"Azure Static Web Apps: Node.js 16 / 18 and Oryx","permalink":"/azure-static-web-apps-node-16-oryx"}},"content":"As part of the TypeScript 4.7 release comes a major upgrade to ECMAScript Module Support for Node.js. This post takes a look at what that means.\\n\\n![title image reading \\"Upgrading to React 18 with TypeScript\\" with the React, TypeScript and Definitely Typed logos`](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## A short history of ECMAScript modules\\n\\nWhen ES6 shipped back in 2015, with it came the concept of modules for JavaScript. Back then it was known as \\"ES6 modules\\". These days they are called ECMAScript modules.\\n\\nWhilst writing code using ECMAScript module semantics came quickly for front end, for the back end (which is generally Node.js) that has not the case. There\'s a number of reasons for this:\\n\\n1. There was already an established module system used in Node.js called [CommonJS](https://en.wikipedia.org/wiki/CommonJS)\\n2. Node.js itself did not initially offer support for ECMAScript modules; in large part because of the problems associated with being able to support CommonJS _as well_ as ECMAScript modules.\\n\\nHowever, with the release Node.js 14 support for ECMAScript modules (AKA \\"ESM\\") landed. If you\'re interested in the details of that module support then it\'s worth [reading this post on ECMAScript modules](https://blog.logrocket.com/es-modules-in-node-today/).\\n\\n## TypeScript support\\n\\nThe TypeScript team have been experimenting with ways to offer support for ECMAScript modules from a Node.js perspective, and with TypeScript 4.7 support is being released.\\n\\nIn this post we\'ll test drive that support by attempting to build a simple module in TypeScript using the new ECMAScript modules support. As we do this, we\'ll discuss what it looks like to author ECMAScript modules for Node.js in TypeScript.\\n\\nLet\'s go!\\n\\n## Making a module\\n\\nWe\'re going to make a module named `greeter` - let\'s initialise it:\\n\\n```shell\\nmkdir greeter\\ncd greeter\\nnpm init --yes\\n```\\n\\nWe now have a `package.json` that looks something like this:\\n\\n```json\\n{\\n  \\"name\\": \\"greeter\\",\\n  \\"version\\": \\"1.0.0\\",\\n  \\"description\\": \\"\\",\\n  \\"main\\": \\"index.js\\",\\n  \\"scripts\\": {\\n    \\"test\\": \\"echo \\\\\\"Error: no test specified\\\\\\" && exit 1\\"\\n  },\\n  \\"keywords\\": [],\\n  \\"author\\": \\"\\",\\n  \\"license\\": \\"ISC\\"\\n}\\n```\\n\\nNode.js supports a new setting in `package.json` called `type`. [This can be set to either \\"module\\" or \\"commonjs\\"](https://nodejs.org/api/packages.html#type). To quote the docs:\\n\\n> Files ending with `.js` are loaded as ES modules when the nearest parent package.json file contains a top-level field `\\"type\\"` with a value of `\\"module\\"`.\\n\\nWith that in mind, we\'ll add a `\\"type\\": \\"module\\"` to our `package.json`.\\n\\nWe\'re now ECMAScript module support compliant, let\'s start adding some TypeScript.\\n\\n## Adding TypeScript 4.7\\n\\nIn order that we can make use of TypeScript ECMAScript modules support we\'re going to install TypeScript 4.7 (currently in beta):\\n\\n```\\nnpm install typescript@4.7.0-beta --save\\n```\\n\\nWith this in place we\'ll initialise a TypeScript project:\\n\\n```\\nnpx tsc --init\\n```\\n\\nThis will create a `tsconfig.json` file which contains many options. We will tweak the `module` option to be `nodenext` to opt into ECMAScript module support:\\n\\n```json\\n{\\n  \\"compilerOptions\\": {\\n    // ...\\n    \\"module\\": \\"nodenext\\" /* Specify what module code is generated. */,\\n    \\"outDir\\": \\"./lib\\" /* Specify an output folder for all emitted files. */,\\n    \\"declaration\\": true /* Generate .d.ts files from TypeScript and JavaScript files in your project. */\\n\\n    // ...\\n  }\\n}\\n```\\n\\nWe\'ve also set the `outDir` option, such that compiled JavaScript will go into that directory, and the `declaration` option such that `.d.ts` files will be generated. We\'ll also update the `\\"scripts\\"` section of our `package.json` to include `build` and `start` scripts:\\n\\n```json\\n  \\"scripts\\": {\\n    \\"build\\": \\"tsc\\",\\n    \\"start\\": \\"node lib/index.js\\"\\n  },\\n```\\n\\n## Writing TypeScript ECMAScript modules\\n\\nWith all that set up, we\'re ready to write some TypeScript ECMAScript modules. First we\'ll write a `greetings.ts` module:\\n\\n```ts\\nexport function helloWorld(): string {\\n  return \'hello world!\';\\n}\\n```\\n\\nThere is nothing new or surprising about this; it\'s just a module exporting a single function named `helloWorld`. It becomes more interesting as we write our `index.ts` module:\\n\\n```ts\\nimport { helloWorld } from \'./greetings.js\';\\n\\nconst greeting = helloWorld();\\n\\nconsole.log(greeting);\\n```\\n\\nThe code above imports our `helloWorld` function and then executes it; writing the output to the console. Not particularly noteworthy. However, the way we import is. We are importing from `\'./greetings.js\'`. In the past we would have written:\\n\\n```ts\\nimport { helloWorld } from \'./greetings\';\\n```\\n\\nNow we write:\\n\\n```ts\\nimport { helloWorld } from \'./greetings.js\';\\n```\\n\\nThis can feel slightly odd and unnatural because we have no `greetings.js` in our codebase; only `greetings.ts`. The imports we\'re writing, reflect the code that will end up being executed; once our TypeScript has been compiled to JavaScript. In ES modules relative import paths need to use extensions.\\n\\nThe easiest way to demonstrate that this is legitimate, is to run the code:\\n\\n```shell\\nnpm run build && npm start\\n```\\n\\nWhich results in:\\n\\n```shell\\n> greeter@1.0.0 build\\n> tsc\\n\\n\\n> greeter@1.0.0 start\\n> node lib/index.js\\n\\nhello world!\\n```\\n\\nSo it works!\\n\\n## ECMAScript and CommonJS side by side\\n\\nPart of ECMAScript module support is the ability to specify the module type of a file based on the file suffix. If you use `.mjs`, you\'re explicitly saying a file is an ECMAScript module. If you use `.cjs`, you\'re explicitly saying a file is an CommonJS module. If you\'re authoring with TypeScript you\'d use `mts` and `cts` respectively and they\'d be transpiled to `mjs` and `cjs`.\\n\\nHappily Node.js allows ES modules to import CommonJS modules as if they were ES modules with a default export; which is good news for interop. Let\'s test that out by writing a `oldGreetings.cts` module:\\n\\n```ts\\nexport function helloOldWorld(): string {\\n  return \'hello old world!\';\\n}\\n```\\n\\nExactly the same syntax as before. We\'ll adjust our `index.ts` to consume this:\\n\\n```ts\\nimport { helloWorld } from \'./greetings.js\';\\nimport { helloOldWorld } from \'./oldGreetings.cjs\';\\n\\nconsole.log(helloWorld());\\nconsole.log(helloOldWorld());\\n```\\n\\nNote that we\'re importing from `\'./oldGreetings.cjs\'`. We\'ll see if it works:\\n\\n```shell\\nnpm run build && npm start\\n```\\n\\nWhich results in:\\n\\n```shell\\n> greeter@1.0.0 build\\n> tsc\\n\\n\\n> greeter@1.0.0 start\\n> node lib/index.js\\n\\nhello world!\\nhello old world!\\n```\\n\\nIt does work!\\n\\n## What files are emitted?\\n\\nBefore we close out, it might be interesting to look at what TypeScript is doing when we run our `npm run build`. It transpiles our TypeScript into JavaScript in our `lib` directory:\\n\\n![A screenshot of VS Code showing the files in the lib directory](screenshot-output-files.png)\\n\\nNote the `greetings.ts` file has resulted in `greetings.js` and a `greetings.d.ts` files. Whereas `oldGreetings.cts` has resulted in `oldGreetings.cjs` and a `oldGreetings.d.cts` files; reflecting the different module types represented.\\n\\nIt\'s also interesting to look at the difference in the emitted JavaScript. When you consider how similar the source files were. If you look at `greetings.js`:\\n\\n```js\\nexport function helloWorld() {\\n  return \'hello world!\';\\n}\\n```\\n\\nThis is the same code as `greetings.ts` but with types stripped. However, if we look at `oldGreetings.cjs` we see this:\\n\\n```js\\n\'use strict\';\\nObject.defineProperty(exports, \'__esModule\', { value: true });\\nexports.helloOldWorld = void 0;\\nfunction helloOldWorld() {\\n  return \'hello old world!\';\\n}\\nexports.helloOldWorld = helloOldWorld;\\n```\\n\\nIn the middle the same code as `oldGreetings.cts` but with types stripped, but around that boilerplate code that TypeScript is emitting for us to aid in interop.\\n\\n## Conclusion\\n\\nWe\'ve seen what TypeScript support for ECMAScript modules looks like, and how to set up a module to embrace it.\\n\\nIf you\'d like to read up further on the topic, the [TypeScript 4.7 beta release notes](https://devblogs.microsoft.com/typescript/announcing-typescript-4-7-beta/#esm-nodejs) are an excellent resource.\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/typescript-4-7-ecmascript-module-support/)\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.logrocket.com/typescript-4-7-ecmascript-module-support/\\" />\\n</head>"},{"id":"azure-static-web-apps-node-16-oryx","metadata":{"permalink":"/azure-static-web-apps-node-16-oryx","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-05-28-azure-static-web-apps-node-16-oryx/index.md","source":"@site/blog/2022-05-28-azure-static-web-apps-node-16-oryx/index.md","title":"Azure Static Web Apps: Node.js 16 / 18 and Oryx","description":"Azure Static Web Apps presently fixes to Node.js 14 when building. If you require a different version of Node to build, here is how.","date":"2022-05-28T00:00:00.000Z","tags":[{"inline":false,"label":"Azure Static Web Apps","permalink":"/tags/azure-static-web-apps","description":"The Azure Static Web Apps service."},{"inline":false,"label":"GitHub Actions","permalink":"/tags/github-actions","description":"The GitHub Actions CI / CD service."},{"inline":false,"label":"Docusaurus","permalink":"/tags/docusaurus","description":"The Docusaurus static site generator."},{"inline":false,"label":"Node.js","permalink":"/tags/node-js","description":"The Node.js runtime."},{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."}],"readingTime":2.065,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-static-web-apps-node-16-oryx","title":"Azure Static Web Apps: Node.js 16 / 18 and Oryx","authors":"johnnyreilly","tags":["azure static web apps","github actions","docusaurus","node.js","azure"],"image":"./title-image.png","description":"Azure Static Web Apps presently fixes to Node.js 14 when building. If you require a different version of Node to build, here is how.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"TypeScript 4.7 and ECMAScript Module Support","permalink":"/typescript-4-7-and-ecmascript-module-support"},"nextItem":{"title":"Azure Static Web Apps: named preview environments with Azure DevOps","permalink":"/static-web-apps-azure-devops-named-preview-environments"}},"content":"Azure Static Web Apps presently fixes to Node.js 14 when building. If you require a different version of Node to build, this can be a problem. This post outlines a workaround.\\n\\n![title image reading \\"Azure Static Web Apps: Node.js 16 and Oryx\\" with Azure and Node.js logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## The engine \\"node\\" is incompatible\\n\\nAs I was upgrading this blog to [Docusaurus v2.0.0-beta.21](https://github.com/facebook/docusaurus/releases/tag/v2.0.0-beta.21) I noticed this error in my build:\\n\\n```shell\\nerror @docusaurus/core@2.0.0-beta.21: The engine \\"node\\" is incompatible with this module. Expected version \\">=16.14\\". Got \\"14.19.1\\"\\nerror Found incompatible module.\\n\\n\\n---End of Oryx build logs---\\nOryx has failed to build the solution.\\n```\\n\\n[Oryx](https://github.com/microsoft/Oryx), which performs the build for Static Web Apps, is fixed to Node 14 for the default LTS version (for now, this will definitely change sometime in 2023). You can check for the constant `NodeLtsVersion` [here](https://github.com/microsoft/Oryx/blob/main/src/BuildScriptGenerator/Node/NodeConstants.cs) to check which version of Node Oryx is using as the `DEFAULT_NODE_VERSION`. To override this default, can either use an `engines` setting in `package.json`, or use an environment setting in the The GitHub Action.\\n\\n## Solution 1: `engines` to the rescue!\\n\\nYou can specify the node version you require in your `package.json` with the [`engines`](https://docs.npmjs.com/cli/v7/configuring-npm/package-json#engines) property. This means you can do something like this:\\n\\n```json\\n  \\"engines\\": {\\n    \\"node\\": \\">=16\\"\\n  }\\n```\\n\\nAnd have the version of Node.js you require installed by Oryx.\\n\\nThanks to [Cormac McCarthy](https://github.com/cormacpayne) for his [comment](https://github.com/Azure/static-web-apps/issues/694#issuecomment-1137492562) which lead me to try this approach out.\\n\\n[You can see the PR where I made this change for my blog here.](https://github.com/johnnyreilly/blog.johnnyreilly.com/pull/228)\\n\\n## Solution 2: Environment variables for the win!\\n\\nYou can change the version for the build step using an environnment variable. This is documented in the [Microsoft Docs](https://learn.microsoft.com/en-us/azure/developer/javascript/how-to/with-web-app/static-web-app-with-swa-cli/create-static-web-app)\\n\\nModify the workflow file, from the `./github/workflows` directory. Just add these last two lines:\\n\\n```yaml\\n     - name: Build And Deploy\\n        id: builddeploy\\n        uses: Azure/static-web-apps-deploy@v1\\n        with:\\n          azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN_SAMPLE }}\\n          repo_token: ${{ secrets.GITHUB_TOKEN }} # Used for Github integrations (i.e. PR comments)\\n          action: \'upload\'\\n          app_location: \'/\' # App source code path\\n          api_location: \'api\' # Api source code path - optional\\n          output_location: \'public\' # Built app content directory - optional\\n        env:  # Put a node version on the following line\\n          NODE_VERSION: 18.12.0\\n```\\n\\nYou can use a specific node version (18.12.0 or 16.18.0) or a major node version (18 or 16). The latter approach installs the latest minor version.\\n\\n**Note:** The Oryx image is updated quarterly. You can get a list of the supported node versions [here](https://github.com/microsoft/Oryx/blob/main/doc/supportedPlatformVersions.md).\\n\\nThanks to Eric C\xf4t\xe9 from [React Academy](https://reactAcademy.live) for the information."},{"id":"static-web-apps-azure-devops-named-preview-environments","metadata":{"permalink":"/static-web-apps-azure-devops-named-preview-environments","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-05-07-static-web-apps-azure-devops-named-preview-environments/index.md","source":"@site/blog/2022-05-07-static-web-apps-azure-devops-named-preview-environments/index.md","title":"Azure Static Web Apps: named preview environments with Azure DevOps","description":"Azure Static Web Apps have just released a new feature for Azure DevOps users called \\"named preview environments\\". Let us have a look","date":"2022-05-07T00:00:00.000Z","tags":[{"inline":false,"label":"Azure Static Web Apps","permalink":"/tags/azure-static-web-apps","description":"The Azure Static Web Apps service."},{"inline":false,"label":"Azure DevOps","permalink":"/tags/azure-devops","description":"The Azure DevOps suite of tools."}],"readingTime":5.135,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"static-web-apps-azure-devops-named-preview-environments","title":"Azure Static Web Apps: named preview environments with Azure DevOps","authors":"johnnyreilly","tags":["azure static web apps","azure devops"],"image":"./title-image.png","description":"Azure Static Web Apps have just released a new feature for Azure DevOps users called \\"named preview environments\\". Let us have a look","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Azure Static Web Apps: Node.js 16 / 18 and Oryx","permalink":"/azure-static-web-apps-node-16-oryx"},"nextItem":{"title":"Upgrading to React 18 with TypeScript","permalink":"/upgrading-to-react-18-typescript"}},"content":"Azure Static Web Apps have just released a new feature for Azure DevOps users called \\"named preview environments\\". They allow users to deploy changes to an environment, prior to merging.\\n\\n![title image reading \\"Azure Static Web App Deploy Previews with Azure DevOps\\" with a Azure, Bicep and Azure DevOps logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## What are named preview environments?\\n\\nThe [announcement](https://docs.microsoft.com/en-gb/azure/static-web-apps/named-environments?tabs=azure-devops) describes them like so:\\n\\n> You can configure your site to deploy every change to a named environment. This preview deployment is published at a stable URL that includes the environment name. For example, if the environment is named `release`, then the preview is available at a location like `<DEFAULT_HOST_NAME>-release.<LOCATION>.azurestaticapps.net`.\\n\\n[I\'d previously written about how to hand roll preview environments with Azure DevOps using Bicep](../2021-12-05-azure-static-web-app-deploy-previews-with-azure-devops/index.md). But now there\'s dedicated functionality that covers this, let\'s see if we can test it out.\\n\\n## Deploy Static Web App with Bicep\\n\\nWe\'ll start with an empty repo in Azure DevOps and we\'ll create the Bicep template for deploying a Static Web App to Azure:\\n\\n```bicep\\nparam appName string\\nparam repositoryUrl string\\nparam repositoryBranch string\\n\\nparam location string = resourceGroup().location\\nparam skuName string = \'Free\'\\nparam skuTier string = \'Free\'\\n\\nresource staticWebApp \'Microsoft.Web/staticSites@2021-03-01\' = {\\n  name: appName\\n  location: location\\n  sku: {\\n    name: skuName\\n    tier: skuTier\\n  }\\n  properties: {\\n    // The provider, repositoryUrl and branch fields are required for successive deployments to succeed\\n    // for more details see: https://github.com/Azure/static-web-apps/issues/516\\n    provider: \'DevOps\'\\n    repositoryUrl: repositoryUrl\\n    branch: repositoryBranch\\n    buildProperties: {\\n      skipGithubActionWorkflowGeneration: true\\n    }\\n  }\\n}\\n\\noutput staticWebAppDefaultHostName string = staticWebApp.properties.defaultHostname // eg gentle-bush-0db02ce03.azurestaticapps.net\\noutput staticWebAppId string = staticWebApp.id\\noutput staticWebAppName string = staticWebApp.name\\n```\\n\\nThe above deploys a Static Web App configured for Azure DevOps.\\n\\nWe are now outputting the `defaultHostname`, `id` and `name` of our newly provisioned SWA. Doing this allows us to do build things in the pipeline around our SWA should we choose to.\\n\\n## Azure Pipelines\\n\\nWe\'re going to need an Azure Pipeline for this. We\'ll create an `azure-pipelines.yml` file in the root of our repo:\\n\\n```yml\\ntrigger:\\n  - \'*\'\\n\\npool:\\n  vmImage: ubuntu-latest\\n\\nvariables:\\n  # subscriptionId is a variable defined on the pipeline itself\\n  - name: appName\\n    value: \'our-static-web-app\'\\n  - name: location\\n    value: \'westeurope\' #\xa0at time of writing static sites are available in limited locations such as westeurope\\n  - name: serviceConnection\\n    value: \'azure-resource-manager-rg-static-web-apps\' # Azure Resource Manager Service Connection created in Azure DevOps with permission against the rg-static-web-apps resource group in Azure\\n  - name: azureResourceGroup # this resource group lives in westeurope\\n    value: \'rg-static-web-apps\'\\n  - name: isMain\\n    value: $[eq(variables[\'Build.SourceBranch\'], \'refs/heads/main\')] # runtime expression\\n\\nsteps:\\n  - checkout: self\\n    submodules: true\\n\\n  - bash: az bicep build --file infra/static-web-app/main.bicep\\n    displayName: \'Compile Bicep to ARM\'\\n\\n  - task: AzureResourceManagerTemplateDeployment@3\\n    name: DeployStaticWebAppInfra\\n    displayName: Deploy Static Web App infra\\n    inputs:\\n      deploymentScope: Resource Group\\n      azureResourceManagerConnection: $(serviceConnection)\\n      subscriptionId: $(subscriptionId)\\n      action: Create Or Update Resource Group\\n      resourceGroupName: $(azureResourceGroup)\\n      location: $(location)\\n      templateLocation: Linked artifact\\n      csmFile: \'infra/static-web-app/main.json\' # created by bash script\\n      overrideParameters: >-\\n        -repositoryUrl $(Build.Repository.Uri)\\n        -repositoryBranch $(Build.SourceBranchName)\\n        -appName $(appName)\\n      deploymentMode: Incremental\\n      deploymentOutputs: deploymentOutputs\\n\\n  - task: PowerShell@2\\n    name: \'SetDeploymentOutputVariables\'\\n    displayName: \'Set Deployment Output Variables\'\\n    inputs:\\n      targetType: inline\\n      script: |\\n        $armOutputObj = \'$(deploymentOutputs)\' | ConvertFrom-Json\\n        $armOutputObj.PSObject.Properties | ForEach-Object {\\n          $keyname = $_.Name\\n          $value = $_.Value.value\\n\\n          # Creates a standard pipeline variable\\n          Write-Output \\"##vso[task.setvariable variable=$keyName;]$value\\"\\n\\n          # Display keys and values in pipeline\\n          Write-Output \\"output variable: $keyName $value\\"\\n        }\\n      pwsh: true\\n\\n  - task: AzureCLI@2\\n    displayName: \'Acquire API key for deployment\'\\n    inputs:\\n      azureSubscription: $(serviceConnection)\\n      scriptType: bash\\n      scriptLocation: inlineScript\\n      inlineScript: |\\n        APIKEY=$(az staticwebapp secrets list --name $(staticWebAppName) | jq -r \'.properties.apiKey\')\\n        echo \\"##vso[task.setvariable variable=apiKey;issecret=true]$APIKEY\\"\\n\\n  - task: AzureStaticWebApp@0\\n    name: DeployStaticWebApp\\n    displayName: Deploy Static Web App\\n    condition: and(succeeded(), eq(variables.isMain, \'true\'))\\n    inputs:\\n      app_location: \'static-web-app\'\\n      # api_location: \'api\'\\n      output_location: \'build\'\\n      azure_static_web_apps_api_token: $(apiKey)\\n\\n  - task: AzureStaticWebApp@0\\n    name: DeployStaticWebAppPreview\\n    displayName: Deploy Static Web App to named preview environment\\n    condition: and(succeeded(), ne(variables.isMain, \'true\'))\\n    inputs:\\n      app_location: \'static-web-app\'\\n      # api_location: \'api\'\\n      output_location: \'build\'\\n      azure_static_web_apps_api_token: $(apiKey)\\n      deployment_environment: \'pullrequest\'\\n```\\n\\nThere\'s two significant parts to the above pipeline. First the trigger, which ensures we run the pipeline on each change:\\n\\n```yml\\ntrigger:\\n  - \'*\' # this means we\'ll trigger on each change\\n```\\n\\nNext the two `AzureStaticWebApp@0` tasks:\\n\\n```yml\\n- task: AzureStaticWebApp@0\\n  name: DeployStaticWebApp\\n  displayName: Deploy Static Web App\\n  condition: and(succeeded(), eq(variables.isMain, \'true\'))\\n  inputs:\\n    app_location: \'static-web-app\'\\n    # api_location: \'api\'\\n    output_location: \'build\'\\n    azure_static_web_apps_api_token: $(apiKey)\\n\\n- task: AzureStaticWebApp@0\\n  name: DeployStaticWebAppPreview\\n  displayName: Deploy Static Web App to named preview environment\\n  condition: and(succeeded(), ne(variables.isMain, \'true\'))\\n  inputs:\\n    app_location: \'static-web-app\'\\n    # api_location: \'api\'\\n    output_location: \'build\'\\n    azure_static_web_apps_api_token: $(apiKey)\\n    deployment_environment: \'pullrequest\'\\n```\\n\\nDepending upon whether we\'re using the `main` branch or not, we either use or do not use the `deployment_environment` property. When it is not the `main` branch we supply the `deployment_environment` property with a value of `\'pullrequest\'`. This is the name of our preview environment; and the value will be used in the URL we end up with. In my own experiments it seems that using hyphens in the name can be problematic - so I would advise avoiding this.\\n\\n## Creating a site\\n\\nSo we can test this out, we need a static web app to deploy. We\'ll spin up a simple Docusaurus site:\\n\\n```\\nnpx create-docusaurus@latest static-web-app classic\\n```\\n\\nUpon the initial commit of our main branch we end up with a website, once the pipeline has run:\\n\\n![screenshot of Azure Pipelines, including the phrase \\"Visit your site at: https://zealous-beach-05119b203.1.azurestaticapps.net\\"](screenshot-of-azure-pipeline-main-deployment.png)\\n\\nNote the URL:\\n\\n![screenshot of Docusaurus site deployed to Azure Static Web Apps](screenshot-of-main-static-web-app.png)\\n\\n## Testing the preview\\n\\nNow our main site is deployed, let\'s test out the preview environment. We\'ll create a new branch:\\n\\n```shell\\ngit checkout -b test-preview\\n```\\n\\nAnd we\'ll update the `pages.index.js` file to include this message: \\"Hello from preview environment!\\". Once we commit and push our changes, we see the pipeline run:\\n\\n![screenshot of Azure Pipelines, including the phrase \\"Visit your site at: https://zealous-beach-05119b203-pullrequest.westeurope.1.azurestaticapps.net\\"](screenshot-of-azure-pipeline-preview-deployment.png)\\n\\nNote that this time we are deploying to our preview environment instead.\\n\\n![screenshot of Docusaurus site deployed to Azure Static Web Apps](screenshot-of-preview-static-web-app.png)\\n\\nAs we can see, this preview is showing our \\"Hello from preview environment!\\" changes as well; whilst the main environment is unchanged.\\n\\n![animated GIF demonstrating both environments with different content](both-environments.gif)\\n\\n## Conclusion\\n\\nAzure DevOps now has support for named preview environments for Azure Static Web Apps; a powerful addition to the product.\\n\\nYou can see further discussion of this feature on the [Azure/static-web-apps repo](https://github.com/Azure/static-web-apps/issues/510#issuecomment-1116307462)."},{"id":"upgrading-to-react-18-typescript","metadata":{"permalink":"/upgrading-to-react-18-typescript","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-05-01-upgrading-to-react-18-typescript/index.md","source":"@site/blog/2022-05-01-upgrading-to-react-18-typescript/index.md","title":"Upgrading to React 18 with TypeScript","description":"The upgrade of the React type definitions to support React 18 involved some significant breaking changes. This post the upgrade path.","date":"2022-05-01T00:00:00.000Z","tags":[{"inline":false,"label":"React","permalink":"/tags/react","description":"The React library."},{"inline":false,"label":"Definitely Typed","permalink":"/tags/definitely-typed","description":"The Definitely Typed project for TypeScript type definitions"},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":5.91,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"upgrading-to-react-18-typescript","title":"Upgrading to React 18 with TypeScript","authors":"johnnyreilly","tags":["react","definitely typed","typescript"],"image":"./title-image.png","description":"The upgrade of the React type definitions to support React 18 involved some significant breaking changes. This post the upgrade path.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Azure Static Web Apps: named preview environments with Azure DevOps","permalink":"/static-web-apps-azure-devops-named-preview-environments"},"nextItem":{"title":"Type annotations: strong types, weakly held","permalink":"/type-annotations-proposal-strong-types-weakly-held"}},"content":"The upgrade of the React type definitions to support React 18 involved some significant breaking changes. This post digs into that and examines what the upgrade path looks like.\\n\\n![title image reading \\"Upgrading to React 18 with TypeScript\\" with the React, TypeScript and Definitely Typed logos`](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## React 18 and Definitely Typed\\n\\nAfter a significant period of time in alpha / beta, [React 18 shipped on March 29th 2022](https://reactjs.org/blog/2022/03/29/react-v18.html). Since the first alpha was released, [support has been available in TypeScript](https://blog.logrocket.com/how-to-use-typescript-with-react-18-alpha/). This has been made possible through the type definitions at [Definitely Typed](https://github.com/DefinitelyTyped/DefinitelyTyped), the repository for high quality TypeScript type definitions. It\'s particularly down to the fine work of [Sebastian Silbermann](https://twitter.com/sebsilbermann) who has put a lot of work into the React 18 definitions.\\n\\nNow that React 18 has shipped, [the type definitions for React 18 were updated in Sebastian\'s pull request](https://github.com/DefinitelyTyped/DefinitelyTyped/pull/56210). Many projects have been, and will be, broken by this change. This post will look at what that breakage can look like and how to resolve it.\\n\\nBefore we do that, let\'s first consider the problem of Definitely Typed and [semantic versioning](http://semver.org/).\\n\\n## Definitely Typed and semantic versioning\\n\\nPeople are used to the idea of semantic versioning in the software they consume. They expect a major version bump to indicate breaking changes. This is exactly what React has just done by incrementing from v17 to v18.\\n\\n**Definitely Typed does not support semantic versioning.**\\n\\nThis is not out of spite. This is because DT intentionally publishes type definitions to npm, under the scope of `@types`. So, for example, the type definitions of React are published to [`@types/react`](https://www.npmjs.com/package/@types/react).\\n\\nIt\'s important to be aware that npm is built on top of semantic versioning. To make consumption of type definitions easier, the versioning of a type definition package will seek to emulate the versioning of the npm package it supports. So for [`react`](https://www.npmjs.com/package/react) `18.0.0`, the corresponding type definition would be [`@types/react`](https://www.npmjs.com/package/@types/react)\'s `18.0.0`.\\n\\nIf there\'s a breaking change to the `@types/react` type definition (or any other for that matter) then the new version published will not increment the major or minor version numbers. The increment will be applied to the patch number alone. This is done to maintain the simpler consumption model of types through npm.\\n\\n## React 18 - breaking type changes\\n\\nAll that said, for very widely used type definitions, it\'s not unusual to at least make an effort towards minimising breaking changes where that is possible.\\n\\nAs an aside, it\'s interesting to know that the Definitely Typed automation tooling splits type definitions into three categories: [\\"Well-liked by everyone\\", \\"Popular\\" and \\"Critical\\"](https://github.com/DefinitelyTyped/dt-mergebot/blob/5485345b210a4baf8e63376a930554bf2b7dd311/src/basic.ts#L11-L14). Thanks [Andrew Branch for sharing that](https://twitter.com/atcb/status/1438559981838626817)! React, being very widely used, is considered \\"Critical\\".\\n\\nWhen Sebastian submitted [a pull request to upgrade the TypeScript React type definitions](https://github.com/DefinitelyTyped/DefinitelyTyped/pull/56210), the opportunity was taken to make breaking changes. These were not all directly related to React 18. Many were fixing long standing issues with the React type definitions.\\n\\nSebastian\'s write up on the PR is excellent and I\'d encourage you to read it. Here is a summary of the breaking changes:\\n\\n1. Removal of implicit children\\n2. Remove `{}` from `ReactFragment` (related to 1.)\\n3. `this.context` becomes `unknown`\\n4. Using `noImplicitAny` now enforces a type is supplied with `useCallback`\\n5. Remove deprecated types to align with official React ones\\n\\nOf the above, the removal of implicit children is the most breaking of the changes and [Sebastian wrote a blog post to explain the rationale](https://solverfox.dev/writing/no-implicit-children). He was also good enough to write a [codemod to help](https://github.com/eps1lon/types-react-codemod).\\n\\nWith that in mind, let\'s go upgrade a codebase to React 18!\\n\\n## Upgrading\\n\\nTo demonstrate what upgrading looks like, I\'m going to upgrade my aunt\'s website. It\'s a fairly simple site, and the pull request for the upgrade [can be found here](https://github.com/johnnyreilly/poor-clares-arundel-koa/pull/69).\\n\\nThe first thing to do is upgrade React itself in the `package.json`:\\n\\n```diff\\n-    \\"react\\": \\"^17.0.0\\",\\n-    \\"react-dom\\": \\"^17.0.0\\",\\n+    \\"react\\": \\"^18.0.0\\",\\n+    \\"react-dom\\": \\"^18.0.0\\",\\n```\\n\\nNext we\'ll upgrade our type definitions:\\n\\n```diff\\n-    \\"@types/react\\": \\"^17.0.0\\",\\n-    \\"@types/react-dom\\": \\"^17.0.0\\",\\n+    \\"@types/react\\": \\"^18.0.0\\",\\n+    \\"@types/react-dom\\": \\"^18.0.0\\",\\n```\\n\\nWhen you install your dependencies, do check your lock file (`yarn.lock` / `package-lock.json` etc). It\'s important that you only have `@types/react` and `@types/react-dom` packages which are version 18+ listed.\\n\\nNow that your install has completed, we start to see the following error message:\\n\\n> Property \'children\' does not exist on type \'LoadingProps\'.ts(2339)\\n\\n... In the following code:\\n\\n```tsx\\ninterface LoadingProps {\\n  // you\'ll note there\'s no `children` prop here - this is what\'s prompting the error message\\n  noHeader?: boolean;\\n}\\n\\n// if props.noHeader is false then this component returns just the icon and a message\\n// if props.noHeader is true then this component returns the same but wrapped in an h1\\nconst Loading: React.FunctionComponent<LoadingProps> = (props) =>\\n  props.noHeader ? (\\n    <>\\n      <FontAwesomeIcon icon={faSnowflake} spin /> Loading {props.children} ...\\n    </>\\n  ) : (\\n    <h1 className=\\"loader\\">\\n      <FontAwesomeIcon icon={faSnowflake} spin /> Loading {props.children} ...\\n    </h1>\\n  );\\n```\\n\\n![screenshot of the above code snippet with \\"Property \'children\' does not exist on type \'LoadingProps\'.ts(2339)\\" displayed over the `props.children`](screenshot-no-children.png)\\n\\nWhat we\'re seeing here is the \\"removal of implicit children\\" in action. Before we did the upgrade, all `React.Component` and `React.FunctionComponent`s had a `children` property in place which allowed React users to use this without declaring it. This is no longer the case. If you have a component with `children` you have to explicitly declare them.\\n\\nIn my case, I could fix the issue by adding a `children` property directly:\\n\\n```tsx\\ninterface LoadingProps {\\n  noHeader?: boolean;\\n  children: string;\\n}\\n```\\n\\nBut why write code when you can get someone else to write it on your behalf?\\n\\nLet\'s make use of [Sebastian\'s codemod](https://github.com/eps1lon/types-react-codemod) instead. To do that we simply enter the following command:\\n\\n```shell\\nnpx types-react-codemod preset-18 ./src\\n```\\n\\nWhen it runs you should find yourself with a prompt which says something like this:\\n\\n```shell\\n? Pick transforms to apply (Press <space> to select, <a> to toggle all, <i> to invert selection, and <enter> to proceed)\\n\u276F\u25C9 context-any\\n \u25C9 deprecated-react-type\\n \u25C9 deprecated-sfc-element\\n \u25C9 deprecated-sfc\\n \u25C9 deprecated-stateless-component\\n \u25C9 implicit-children\\n \u25C9 useCallback-implicit-any\\n```\\n\\n![screenshot of the terminal prompt above](screenshot-codemod-in-action.png)\\n\\nI\'m going to select `a` and let the codemod run. For my own project, 37 files are updated. It\'s the same modification for all files. In each case, a components props are wrapped by `React.PropsWithChildren`. Let\'s look at what that looks like for our `Loading` component:\\n\\n```diff\\n-const Loading: React.FunctionComponent<LoadingProps> = (props) =>\\n+const Loading: React.FunctionComponent<React.PropsWithChildren<LoadingProps>> = (props) =>\\n```\\n\\n`PropsWithChildren` is very simple; it just adds `children` back, like so:\\n\\n```ts\\ntype PropsWithChildren<P> = P & { children?: ReactNode | undefined };\\n```\\n\\nThis resolves the compilation issues we were having earlier; no type issues are reported anymore.\\n\\n## Wrapping up\\n\\nWe now understand how the breaking type changes came to present with React 18, and we know how to upgrade our codebase using the handy codemod. Thanks [Sebastian Silbermann](https://twitter.com/sebsilbermann) for not only putting this work into getting the type definitions in the best state they could be, and making it easier for the community to upgrade.\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/upgrading-react-18-typescript/)\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.logrocket.com/upgrading-react-18-typescript/\\" />\\n</head>"},{"id":"type-annotations-proposal-strong-types-weakly-held","metadata":{"permalink":"/type-annotations-proposal-strong-types-weakly-held","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-04-16-type-annotations-proposal-strong-types-weakly-held/index.md","source":"@site/blog/2022-04-16-type-annotations-proposal-strong-types-weakly-held/index.md","title":"Type annotations: strong types, weakly held","description":"Type annotations is a proposal which would allow for the inclusion of types in JavaScript code. Here is a description of the proposal and some thoughts.","date":"2022-04-16T00:00:00.000Z","tags":[{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."},{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."},{"inline":false,"label":"JSDoc","permalink":"/tags/jsdoc","description":"Type safety through JSDoc annotations."}],"readingTime":8.645,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"type-annotations-proposal-strong-types-weakly-held","title":"Type annotations: strong types, weakly held","authors":"johnnyreilly","tags":["typescript","javascript","jsdoc"],"image":"./title-image.png","description":"Type annotations is a proposal which would allow for the inclusion of types in JavaScript code. Here is a description of the proposal and some thoughts.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Upgrading to React 18 with TypeScript","permalink":"/upgrading-to-react-18-typescript"},"nextItem":{"title":"ESLint your C# in VS Code with Roslyn Analyzers","permalink":"/eslint-your-csharp-in-vs-code-with-roslyn-analyzers"}},"content":"Recently, a new ECMAScript proposal called [\\"Type Annotations\\"](https://github.com/tc39/proposal-type-annotations) (originally named [\\"Types as Comments\\"](https://github.com/giltayar/proposal-types-as-comments)) was revealed. The purpose is to allow type annotations to be valid JavaScript syntax. Albeit syntax that is ignored by JavaScript engines. The proposal is being worked on by Gil Tayar, Daniel Rosenwasser, Romulo Cintra, Rob Palmer, and others. Many of these people are from TypeScript community - however this proposal intentionally does not exist to benefit TypeScript alone.\\n\\n\x3c!--truncate--\x3e\\n\\nIt\'s a contentious topic. As a regular (and longtime) TypeScript user, here\'s a description of the proposal and some thoughts.\\n\\n![title image reading \\"Type annotations: strong types, weakly held\\" with the JavaScript logo](title-image.png)\\n\\n## What is the proposal?\\n\\nTypes annotations is a proposal which would allow for the inclusion of types in JavaScript code. Consider the following piece of TypeScript:\\n\\n```ts\\nconst theAnswer: number = 42;\\n```\\n\\nAt present, this is not valid JavaScript. If you try and run the above in a JavaScript engine you\'ll get an error. Types are not part of JavaScript syntax.\\n\\n![screenshot of `const theAnswer: number = 42;` entered into the Chrome devtools and responding with an error that says `Uncaught SyntaxError: Missing initializer in const declaration`](screenshot-types-in-the-chrome-console.png)\\n\\nInterestingly, it\'s already possible to store types within JavaScript through a standard known as JSDoc. [I\'ve written about how TypeScript and JSDoc connect before.](https://blog.logrocket.com/typescript-vs-jsdoc-javascript/), essentially the thing to note is that JSDoc amounts to storing type declarations in the context of JavaScript comments.\\n\\nIt\'s already possible to write our code sample in valid JavaScript expressing the types within JSDoc. It looks like this:\\n\\n```ts\\n/** @type {number} */\\nconst theAnswer = 42;\\n```\\n\\nThis works, but it took two lines of code instead of one. The proposal allows for types to be directly expressed; not written as comments. So rather than writing the JSDoc equivalent, imagine if JavaScript was happy with the following instead:\\n\\n```ts\\nconst theAnswer: number = 42;\\n```\\n\\nThat\'s what the proposal amounts to.\\n\\n## What isn\'t it?\\n\\nNow that we understand what the proposal is, let\'s consider what it isn\'t.\\n\\nTypes annotations isn\'t an endorsement of a particular type system. Furthermore, it is not type checking in the browser or type checking in Node.js.\\n\\nLet\'s consider each of these. There\'s a number of languages which allow us to type check JavaScript. TypeScript, Flow, Hegel and others all play in this space. They are all similar, but different. They have different syntax and they do different things.\\n\\nWhat they have in common, is the space where types live in their syntax or grammar. The proposal essentially says \\"hey we might have different approaches to describing types, but we agree about where the types ought to live - let\'s standardise that\\".\\n\\nThis is why the original proposal name of \\"types as comments\\" is instructive; these types would be ignored by JavaScript runtimes. The fact they would be ignored is an indication that no existing type system would be \\"anointed\\" by this proposal.\\n\\nConsider the following:\\n\\n```ts\\nconst theAnswer: gibberish = 42;\\n```\\n\\nThis is neither TypeScript or Flow. Both would complain about the above. JavaScript, if this proposal were adopted, would be entirely untroubled.\\n\\nTo reiterate: the proposal is not an endorsement of any given type system and it follows that there is no runtime type checking being introduced to JavaScript.\\n\\n## Why do this at all?\\n\\nIt\'s worth taking a look at [Daniel Rosenwasser](https://devblogs.microsoft.com/typescript/a-proposal-for-type-syntax-in-javascript/)\'s post where he announces the proposal. Daniel is part of the TypeScript team and one of champions of this proposal, along with [Rob Palmer](https://twitter.com/robpalmer2) at Bloomberg and [Romulo Cintra](https://twitter.com/romulocintra) at Igalia.\\n\\nHe says:\\n\\n> Today, you can create a .js file in your editor and start sprinkling in types in the form of JSDoc comments.\\n>\\n> ```js\\n> /**\\n>  * @param a {number}\\n>  * @param b {number}\\n>  */\\n> function add(a, b) {\\n>   return a + b;\\n> }\\n> ```\\n>\\n> Because these are just comments, they don\u2019t change how your code runs at all \u2013 they\u2019re just a form of documentation, but TypeScript uses them to give you a better JavaScript editing experience ... This feature makes it incredibly convenient to get some of the TypeScript experience without a build step, and you can use it for small scripts, basic web pages, server code in Node.js, etc.\\n>\\n> Still, you\u2019ll notice that this is a little verbose \u2013 we love how lightweight the inner-loop is for writing JavaScript, but we\u2019re missing how convenient TypeScript makes it to just write types.\\n>\\n> _So what if we had both?_\\n>\\n> What if we could have something like TypeScript syntax which was totally ignored \u2013 sort of like comments \u2013 in JavaScript.\\n>\\n> ```ts\\n> function add(a: number, b: number) {\\n>   return a + b;\\n> }\\n> ```\\n\\nWhat I take from this, is that JavaScript with types annotations, would be a more developer friendly JSDoc.\\n\\n## \\"It\'s the JSDoc I always wanted!\\"\\n\\nThis idea really resonates with me. I\'m a longtime user of JSDoc. Let me articulate why I find it useful.\\n\\nWhat I wanted, way back before TypeScript existed, was JavaScript with static typing. TypeScript _mostly_ is that. At least in the way I choose to use it.\\n\\nI don\'t use `enum`s, `namespace`s, `decorator`s etc. This is significant as each of those features steps has an emit aspect; using one of these will require transpilation to create special JavaScript to represent a custom TypeScript implemented feature. All other TypeScript features are _erased_ by transpilation; there\'s no execution characteristics.\\n\\nSo by subsetting the features of TypeScript, we can choose to use only those features that do not have an emit aspect. By making that choice, it\'s possible to use just JavaScript, if we\'re willing to commit to using JSDoc syntax within JavaScript _instead_ of TypeScript. There\'s many in the community who are doing this on sizeable projects like [webpack](https://github.com/webpack/webpack) already. We don\'t lose type checking, we don\'t lose refactoring possibilities thanks to editors like VS Code.\\n\\nJSDoc is great, but it\'s undeniably more verbose than writing TypeScript. If types annotations was to be adopted, we\'d able to write TypeScript in our JavaScript files. We\'d be able to use TypeScript to type check that **if we wanted to**. But we wouldn\'t need to transpile our code prior to running. We could run our source code directly. Brilliant!\\n\\n## Controversy and Compromise\\n\\nUp until now, as we\'ve looked at the proposal, the story has been one of JavaScript becoming \\"types tolerant\\". And as a consequence, the syntax of Flow / TypeScript / Hegel et al would in future being considered valid JavaScript.\\n\\nThis paints a picture of JavaScript, a dynamic language, being changed to accomodate the sensibilities of those who favour static typing. If you should glance at the discussions on Hacker News and in the issues of the proposal it\'s clear there\'s a very vocal section of JavaScript developers who consider this proposal to be thoroughly unwanted.\\n\\nWhilst it\'s unlikely that the most fervent dynamic language advocate will change their mind, it\'s worth considering the nuance of this proposal. In actual fact, the proposal is a two way street; to comply with types becoming JavaScript native, languages like TypeScript would likely make changes to accomodate.\\n\\n## Generic invocations and TypeScript\\n\\nThere\'s a few cases which apply, the one that seems most significant is that of generic invocation. [To quote the proposal](https://github.com/giltayar/proposal-types-as-comments#generic-invocations):\\n\\n> One can explicitly specify the type arguments of a generic function invocation or generic class instantiation [in TypeScript](https://www.typescriptlang.org/docs/handbook/2/functions.html#specifying-type-arguments).\\n>\\n> ```ts\\n> // TypeScript\\n> add<number>(4, 5);\\n> new Point<bigint>(4n, 5n);\\n> ```\\n>\\n> The above syntax is already valid JavaScript that users may rely on, so we cannot use this syntax as-is.\\n\\nSo if this proposal was to land, writing today\'s style TypeScript in JavaScript would _not_ work in the case of generic invocations.\\n\\nIf we read on in the proposal it says;\\n\\n> We expect some form of new syntax that could be used to resolve this ambiguity.\\n> No specific solution is proposed at this point of time, but one example option is to use a syntactic prefix such as `::`\\n>\\n> ```ts\\n> // Types as Comments - example syntax solution\\n> add::<number>(4, 5)\\n> new Point::<bigint>(4n, 5n)\\n> ```\\n>\\n> These type arguments (`::<type>`) would be ignored by the JavaScript runtime.\\n> It would be reasonable for this non-ambiguous syntax to be adopted in TypeScript as well.\\n\\nThis last sentence is significant. Let\'s read it again:\\n\\n> It would be reasonable for this non-ambiguous syntax to be adopted in TypeScript as well\\n\\nWhilst not being an absolute commitment, this certainly suggests that TypeScript would be willing to change its own syntax to align with something that was standardised as typed JavaScript.\\n\\nSpeaking personally, I don\'t love the proposed new syntax; but I understand the rationale. Certainly a new generic invocation syntax is something I could come to terms with. It\'s good of the TypeScript team to be open to the idea of making changes to the language to align with the proposal. This is not zero cost to them. This demonstrates that to allow this proposal to land, there will be compromises on many sides. It\'s likely that Flow will be similarly affected also.\\n\\n## Conclusion\\n\\nWhen you see the various discussions on this topic online, it\'s clear there are many strong feelings. The proposal hasn\'t even reached stage 1 (of the potential 4 stages required for adoption). This may be a feature that doesn\'t make it. Or perhaps takes a long time to land on a mutually agreed design.\\n\\nSpeaking personally I\'m hopeful that this does end up being part of the language. Not only do I like running raw JS, I see the benefits of being able to onboard people from JavaScript to TypeScript by allowing types to live directly in JavaScript.\\n\\nIt\'s said that prediction is very difficult, especially if it\'s about the future. So it is hard to know for sure what the long term effects on the language and the ecosystem of this proposal might be. It would certainly lower the barrier to entry for using static typing with JavaScript, and as consequence, would likely lead to greater adoption and hence less bugs in userland. Time will tell.\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/types-as-comments-strong-types-weakly-held/)\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.logrocket.com/types-as-comments-strong-types-weakly-held/\\" />\\n</head>"},{"id":"eslint-your-csharp-in-vs-code-with-roslyn-analyzers","metadata":{"permalink":"/eslint-your-csharp-in-vs-code-with-roslyn-analyzers","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-04-06-eslint-your-csharp-in-vs-code-with-roslyn-analyzers/index.md","source":"@site/blog/2022-04-06-eslint-your-csharp-in-vs-code-with-roslyn-analyzers/index.md","title":"ESLint your C# in VS Code with Roslyn Analyzers","description":"ESLint provides linting for TypeScript and JavaScript in VS Code. A similar experience is available for C# in VS Code through Roslyn Analyzers.","date":"2022-04-06T00:00:00.000Z","tags":[{"inline":false,"label":"C#","permalink":"/tags/csharp","description":"The C# programming language."},{"inline":false,"label":"VS Code","permalink":"/tags/vs-code","description":"The Visual Studio Code editor."},{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."},{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":10.34,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"eslint-your-csharp-in-vs-code-with-roslyn-analyzers","title":"ESLint your C# in VS Code with Roslyn Analyzers","authors":"johnnyreilly","tags":["c#","vs code","javascript","asp.net"],"image":"./title-image.png","description":"ESLint provides linting for TypeScript and JavaScript in VS Code. A similar experience is available for C# in VS Code through Roslyn Analyzers.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Type annotations: strong types, weakly held","permalink":"/type-annotations-proposal-strong-types-weakly-held"},"nextItem":{"title":"Azure DevOps: consume a private artifact feed","permalink":"/azure-devops-consume-private-nuget-artifact-feed"}},"content":"ESLint provides a great linting experience for TypeScript and JavaScript in VS Code. The suggestions, fixes and ignore options make creating clean code a joy. A similar experience is available for C# in VS Code through Roslyn Analyzers - this post tells us more.\\n\\n![title image reading \\"ESLint your C# in VS Code with Roslyn Analyzers\\" with the C# and VS Code logos`](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Linting and C#\\n\\nJavaScript and TypeScript benefit from a tremendous tooling ecosystem which allows us to simply format and lint our codebases as we\'re editing. Similar tooling exists for C#. [Previously I wrote about using `dotnet-format` to have a Prettier-like experience for formatting our C#](../2020-12-22-prettier-your-csharp-with-dotnet-format-and-lint-staged/index.md). If that last post focussed on formatting C#; looking through the lens of [Prettier](https://prettier.io/), this post focusses on linting; looking through the lens of [ESLint](https://eslint.org/).\\n\\n## Roslyn Analyzers\\n\\nThere\'s often overlap between linting and formatting tooling; and so it goes with C# as well. Linting and formatting in the .NET space make use of the [Roslyn Analyzers](https://github.com/dotnet/roslyn-analyzers):\\n\\n> Roslyn analyzers analyze your code for style, quality and maintainability, design and other issues. The documentation for Roslyn Analyzers can be found at docs.microsoft.com/dotnet/fundamentals/code-analysis/overview.\\n\\nTo learn more about them, it\'s worth reading [the excellent piece on the topic](https://endjin.com/blog/2022/01/raising-coding-standard-dotnet-analyzers) by [Ian Griffiths](https://twitter.com/idg10).\\n\\n## \\"Analyse `this`\\"\\n\\nIn order that we can see what the linting experience is like in VS Code, we\'re going to need a project to work on. We have the .NET 6 SDK installed, so we\'ll create ourselves a project:\\n\\n```shell\\ndotnet new webapi -o AnalyseThis\\n```\\n\\nWe have the [C# extension](https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.csharp) installed already, but we\'re getting no feedback on the code. Maybe it\'s already beautiful?\\n\\nOr maybe not. We\'re going to need an `.editorconfig` file to control all the code style settings. You can create this directly using the `dotnet` CLI like so;\\n\\n```shell\\ndotnet new editorconfig\\n```\\n\\nOnce this runs, it creates a file with all of the settings in with their default values. Alongside that, we need to wake VS Code up to our brave new world by setting the following in our `settings.json`:\\n\\n```json\\n{\\n  \\"omnisharp.enableRoslynAnalyzers\\": true,\\n  \\"omnisharp.enableEditorConfigSupport\\": true\\n}\\n```\\n\\nOr alternatively, use the GUI in VS Code to set these settings directly:\\n\\n![screenshot of the VS Code settings screen](screenshot-vs-code-settings-enable.png)\\n\\nIt\'s then a good idea to turn OmniSharp off and on again, so it picks up these changes:\\n\\n![screenshot of the VS Code \\"restart OmniSharp\\"](screenshot-vs-code-restart-omnisharp.png)\\n\\nThen, excitingly, we start to see code analysis, or linting, messages in the problems pane of VS Code:\\n\\n![screenshot of a first linting message and the code to which it applies](screenshot-initial-problems.png)\\n\\nIt\'s possible to use the `dotnet-format` command to surface this information:\\n\\n```shell\\ndotnet format style -v detailed --severity info --verify-no-changes\\n  The dotnet runtime version is \'6.0.2\'.\\n  Formatting code files in workspace \'/workspaces/AnalyseThis.csproj\'.\\n    Determining projects to restore...\\n  All projects are up-to-date for restore.\\n  Project AnalyseThis is using configuration from \'/workspaces/.editorconfig\'.\\n  Project AnalyseThis is using configuration from \'/workspaces/obj/Debug/net6.0/AnalyseThis.GeneratedMSBuildEditorConfig.editorconfig\'.\\n  Project AnalyseThis is using configuration from \'/usr/share/dotnet/sdk/6.0.200/Sdks/Microsoft.NET.Sdk/analyzers/build/config/analysislevel_6_default.editorconfig\'.\\n  Running 45 analyzers on AnalyseThis.\\n/workspaces/Controllers/WeatherForecastController.cs(14,57): info IDE0052: Private member \'WeatherForecastController._logger\' can be removed as the value assigned to it is never read [/workspaces/AnalyseThis.csproj]\\n  Formatted code file \'/workspaces/Controllers/WeatherForecastController.cs\'.\\n  Formatted 1 of 6 files.\\n  Format complete in 7993ms.\\n```\\n\\nNote the `IDE0052: Private member \'WeatherForecastController._logger\' can be removed as the value assigned to it is never read` message above.\\n\\n## Now fail my build!\\n\\nThis is all very exciting - we\'ve a world of extra linting at our fingertips! But what\'s a touch disappointing, is that the above information isn\'t surfaced in my build. What if as a team we commit to a particular code style? If I can\'t enforce that in the build, it\'s likely not going to happen.\\n\\nSo what do I do? Well, the information is out there on how to do this, but it\'s easy to miss. [You can find the details here](https://docs.microsoft.com/en-us/dotnet/fundamentals/code-analysis/overview#enable-on-build). We update our `AnalyseThis.csproj` to include an `EnforceCodeStyleInBuild` setting like so:\\n\\n```xml\\n  <PropertyGroup>\\n    <TargetFramework>net6.0</TargetFramework>\\n    <Nullable>enable</Nullable>\\n    <ImplicitUsings>enable</ImplicitUsings>\\n\\n    <EnforceCodeStyleInBuild>true</EnforceCodeStyleInBuild>\\n  </PropertyGroup>\\n```\\n\\nWe\'re going to replace our exhaustive `.editorconfig` file with a much simpler one:\\n\\n```ini\\n# Remove the line below if you want to inherit .editorconfig settings from higher directories\\nroot = true\\n\\n[*.cs]\\n# Default severity for analyzer diagnostics with category \'Style\' (escalated to build warnings)\\ndotnet_analyzer_diagnostic.category-Style.severity = warning\\n```\\n\\nDo you see what we did here? We told our build to treat `Style` diagnostics (lints) as warnings. Once OmniSharp picks this up, more linting messages start to appear in the problems pane of VS Code:\\n\\n![screenshot of more linting messages](screenshot-extra-problems.png)\\n\\nAnd what\'s more, if we attempt to build, guess what?\\n\\n```shell\\ndotnet build\\nMicrosoft (R) Build Engine version 17.1.0+ae57d105c for .NET\\nCopyright (C) Microsoft Corporation. All rights reserved.\\n\\n  Determining projects to restore...\\n  All projects are up-to-date for restore.\\n/workspaces/AnalyseThis/Controllers/WeatherForecastController.cs(3,1): warning IDE0160: Convert to block scoped namespace [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/WeatherForecast.cs(1,1): warning IDE0160: Convert to block scoped namespace [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Program.cs(1,1): warning IDE0008: Use explicit type instead of \'var\' [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Program.cs(10,1): warning IDE0008: Use explicit type instead of \'var\' [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Program.cs(15,5): warning IDE0058: Expression value is never used [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Program.cs(16,5): warning IDE0058: Expression value is never used [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n  AnalyseThis -> /workspaces/AnalyseThis/bin/Debug/net6.0/AnalyseThis.dll\\n\\nBuild succeeded.\\n\\n/workspaces/AnalyseThis/Controllers/WeatherForecastController.cs(3,1): warning IDE0160: Convert to block scoped namespace [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/WeatherForecast.cs(1,1): warning IDE0160: Convert to block scoped namespace [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Program.cs(1,1): warning IDE0008: Use explicit type instead of \'var\' [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Program.cs(10,1): warning IDE0008: Use explicit type instead of \'var\' [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Program.cs(15,5): warning IDE0058: Expression value is never used [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Program.cs(16,5): warning IDE0058: Expression value is never used [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n    6 Warning(s)\\n    0 Error(s)\\n\\nTime Elapsed 00:00:06.53\\n```\\n\\nThat\'s right! The same messages from the problems pane are now surfaced in our build as warnings. And we can kick it up a notch too; let\'s make them errors:\\n\\n```ini\\n# Remove the line below if you want to inherit .editorconfig settings from higher directories\\nroot = true\\n\\n[*.cs]\\n# Default severity for analyzer diagnostics with category \'Style\' (escalated to build errors)\\ndotnet_analyzer_diagnostic.category-Style.severity = error\\n```\\n\\nOnce OmniSharp catches up we see our warnings transform into errors:\\n\\n![screenshot of a more linting messages](screenshot-extra-problems-as-errors.png)\\n\\nAnd if we build...\\n\\n```shell\\ndotnet build\\nMicrosoft (R) Build Engine version 17.1.0+ae57d105c for .NET\\nCopyright (C) Microsoft Corporation. All rights reserved.\\n\\n  Determining projects to restore...\\n  All projects are up-to-date for restore.\\n/workspaces/AnalyseThis/WeatherForecast.cs(1,1): error IDE0160: Convert to block scoped namespace [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Controllers/WeatherForecastController.cs(3,1): error IDE0160: Convert to block scoped namespace [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Program.cs(1,1): error IDE0008: Use explicit type instead of \'var\' [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Program.cs(10,1): error IDE0008: Use explicit type instead of \'var\' [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Program.cs(15,5): error IDE0058: Expression value is never used [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Program.cs(16,5): error IDE0058: Expression value is never used [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n\\nBuild FAILED.\\n\\n/workspaces/AnalyseThis/WeatherForecast.cs(1,1): error IDE0160: Convert to block scoped namespace [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Controllers/WeatherForecastController.cs(3,1): error IDE0160: Convert to block scoped namespace [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Program.cs(1,1): error IDE0008: Use explicit type instead of \'var\' [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Program.cs(10,1): error IDE0008: Use explicit type instead of \'var\' [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Program.cs(15,5): error IDE0058: Expression value is never used [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Program.cs(16,5): error IDE0058: Expression value is never used [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n    0 Warning(s)\\n    6 Error(s)\\n\\nTime Elapsed 00:00:04.22\\n```\\n\\nYes! Our style diagnostics are now failing the build. This is terrific!\\n\\n## Categories\\n\\nIt\'s worth pausing a second and considering the category upgrade we did here:\\n\\n```ini\\ndotnet_analyzer_diagnostic.category-Style.severity = error\\n```\\n\\nThere\'s a number of different categories that encapsulate groups of rules, [they\'re documented here](https://docs.microsoft.com/en-us/dotnet/fundamentals/code-analysis/categories). Taken from there you can see the wealth of different categories that exist:\\n\\n| Category                               | Description                                                                                                                                                                              | EditorConfig value                                              |\\n| -------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------- |\\n| Design rules                           | Design rules support adherence to the .NET Framework Design Guidelines.                                                                                                                  | `dotnet_analyzer_diagnostic.category-Design.severity`           |\\n| Documentation rules                    | Documentation rules support writing well-documented libraries through the correct use of XML documentation comments for externally visible APIs.                                         | `dotnet_analyzer_diagnostic.category-Documentation.severity`    |\\n| Globalization rules                    | Globalization rules support world-ready libraries and applications.                                                                                                                      | `dotnet_analyzer_diagnostic.category-Globalization.severity`    |\\n| Portability and interoperability rules | Portability rules support portability across different platforms. Interoperability rules support interaction with COM clients.                                                           | `dotnet_analyzer_diagnostic.category-Interoperability.severity` |\\n| Maintainability rules                  | Maintainability rules support library and application maintenance.                                                                                                                       | `dotnet_analyzer_diagnostic.category-Maintainability.severity`  |\\n| Naming rules                           | Naming rules support adherence to the naming conventions of the .NET design guidelines.                                                                                                  | `dotnet_analyzer_diagnostic.category-Naming.severity`           |\\n| Performance rules                      | Performance rules support high-performance libraries and applications.                                                                                                                   | `dotnet_analyzer_diagnostic.category-Performance.severity`      |\\n| SingleFile rules                       | Single-file rules support single-file applications.                                                                                                                                      | `dotnet_analyzer_diagnostic.category-SingleFile.severity`       |\\n| Reliability rules                      | Reliability rules support library and application reliability, such as correct memory and thread usage.                                                                                  | `dotnet_analyzer_diagnostic.category-Reliability.severity`      |\\n| Security rules                         | Security rules support safer libraries and applications. These rules help prevent security flaws in your program.                                                                        | `dotnet_analyzer_diagnostic.category-Security.severity`         |\\n| Style rules                            | Style rules support consistent code style in your codebase. These rules start with the \\"IDE\\" prefix.                                                                                     | `dotnet_analyzer_diagnostic.category-Style.severity`            |\\n| Usage rules                            | Usage rules support proper usage of .NET.                                                                                                                                                | `dotnet_analyzer_diagnostic.category-Usage.severity`            |\\n| N/A                                    | You can use this EditorConfig value to enable the following rules: IDE0051, IDE0064, IDE0076. While these rules start with \\"IDE\\", they are not technically part of the `Style` category. | `dotnet_analyzer_diagnostic.category-CodeQuality.severity`      |\\n\\nThe `IDE0052` information we saw when we used `dotnet format` earlier is technically part of the `CodeQuality` category. If we wanted to, we we could dial that up that category to an error like so:\\n\\n```ini\\n# Default severity for analyzer diagnostics with category \'CodeQuality\' (escalated to build errors)\\ndotnet_analyzer_diagnostic.category-CodeQuality.severity = error\\n```\\n\\n## Opt out of rules\\n\\nAs it turns out, I disagree with the complaints I\'m getting on the codebase right now, so I\'d like to dial those down to ignore. To do that globally, you simply put configuration in the `.editorconfig` to reflect that:\\n\\n```ini\\n# Remove the line below if you want to inherit .editorconfig settings from higher directories\\nroot = true\\n\\n[*.cs]\\n# Default severity for analyzer diagnostics with category \'Style\' (escalated to build warnings)\\ndotnet_analyzer_diagnostic.category-Style.severity = error\\n\\ndotnet_diagnostic.IDE0008.severity = none\\ndotnet_diagnostic.IDE0058.severity = none\\ndotnet_diagnostic.IDE0160.severity = none\\n```\\n\\nWhat we\'re doing here is saying \\"upgrade all [style rules](https://docs.microsoft.com/en-us/dotnet/fundamentals/code-analysis/style-rules/) to be errors, but `IDE0008`, `IDE0058` and `IDE0160` (which are style rules) - ignore those; don\'t tell me about them\\".\\n\\nNow I\'m not going to be bothered by those errors in future. Great.\\n\\n## Dial up information to warning\\n\\nIf we look again at our problems pane in VS Code, we can see there\'s an entry there. It\'s not an error, it\'s not a warning. It\'s information:\\n\\n![screenshot of a first linting message and the code to which it applies](screenshot-initial-problems.png)\\n\\nLet\'s say we want to take that and dial it up to be a warning, such that it surfaces in the build too. We can with a simple addition to our `.editorconfig`:\\n\\n```ini\\n# Remove the line below if you want to inherit .editorconfig settings from higher directories\\nroot = true\\n\\n[*.cs]\\n# Default severity for analyzer diagnostics with category \'Style\' (escalated to build warnings)\\ndotnet_analyzer_diagnostic.category-Style.severity = error\\n\\ndotnet_diagnostic.IDE0008.severity = none\\ndotnet_diagnostic.IDE0058.severity = none\\ndotnet_diagnostic.IDE0160.severity = none\\n\\n# Roslyn analzer surfaces this as information - we\'ll dial it up to a warning\\ndotnet_diagnostic.IDE0052.severity = warning\\n```\\n\\nOnce OmniSharp notices:\\n\\n![screenshot of our information now a warning](screenshot-information-as-warning.png)\\n\\nAnd if we run the build, there it is!\\n\\n```\\ndotnet build\\nMicrosoft (R) Build Engine version 17.1.0+ae57d105c for .NET\\nCopyright (C) Microsoft Corporation. All rights reserved.\\n\\n  Determining projects to restore...\\n  All projects are up-to-date for restore.\\n/workspaces/AnalyseThis/Controllers/WeatherForecastController.cs(14,57): warning IDE0052: Private member \'WeatherForecastController._logger\' can be removed as the value assigned to it is never read [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n  AnalyseThis -> /workspaces/AnalyseThis/bin/Debug/net6.0/AnalyseThis.dll\\n\\nBuild succeeded.\\n\\n/workspaces/AnalyseThis/Controllers/WeatherForecastController.cs(14,57): warning IDE0052: Private member \'WeatherForecastController._logger\' can be removed as the value assigned to it is never read [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n    1 Warning(s)\\n    0 Error(s)\\n\\nTime Elapsed 00:00:02.21\\n```\\n\\n## Deactivate linting partially\\n\\nLet\'s say we want to ignore that one warning. We\'d like the equivalent functionality to `// eslint-disable-next-line`. That doesn\'t exist alas. However, what does is the equivalent to this:\\n\\n```js\\n/* eslint-disable */\\n\\nalert(\'foo\');\\n\\n/* eslint-enable */\\n```\\n\\nIn our case what we\'d do is this:\\n\\n```cs\\n#pragma warning disable\\n    private readonly ILogger<WeatherForecastController> _logger;\\n#pragma warning restore\\n```\\n\\nOr to be more specific:\\n\\n```cs\\n#pragma warning disable IDE0052\\n    private readonly ILogger<WeatherForecastController> _logger;\\n#pragma warning restore IDE0052\\n```\\n\\nAnd now we can opt out of that rule in this specific place - whilst maintaining it more generally.\\n\\n## Conclusion\\n\\nThere\'s powerful linting tools in C#, hopefully this guide has made it easier for you to surface them, control them and apply them both to VS Code and to your build.\\n\\nThanks to [Joey Robichaud](https://twitter.com/JoeyRobichaud), [Tim Heuer](https://twitter.com/timheuer) and [Youssef Victor](https://twitter.com/YoussefV1313) for some excellent pointers that fed into the writing of this post. [You can see the help they provided here](https://github.com/dotnet/roslyn/issues/60620)."},{"id":"azure-devops-consume-private-nuget-artifact-feed","metadata":{"permalink":"/azure-devops-consume-private-nuget-artifact-feed","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-03-30-azure-devops-consume-private-nuget-artifact-feed/index.md","source":"@site/blog/2022-03-30-azure-devops-consume-private-nuget-artifact-feed/index.md","title":"Azure DevOps: consume a private artifact feed","description":"To build applications both locally and in an Azure Pipeline using Private Azure Artifact feeds with Azure DevOps, follow these steps.","date":"2022-03-30T00:00:00.000Z","tags":[{"inline":false,"label":"Azure DevOps","permalink":"/tags/azure-devops","description":"The Azure DevOps suite of tools."},{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":3.395,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-devops-consume-private-nuget-artifact-feed","title":"Azure DevOps: consume a private artifact feed","authors":"johnnyreilly","tags":["azure devops","asp.net"],"image":"./title-image.png","description":"To build applications both locally and in an Azure Pipeline using Private Azure Artifact feeds with Azure DevOps, follow these steps.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"ESLint your C# in VS Code with Roslyn Analyzers","permalink":"/eslint-your-csharp-in-vs-code-with-roslyn-analyzers"},"nextItem":{"title":"Lighthouse meet GitHub Actions","permalink":"/lighthouse-meet-github-actions"}},"content":"Private Azure Artifact feeds in in Azure DevOps can be used to serve NuGet packages. To build applications both locally and in an Azure Pipeline using those packages, there are a few steps to follow which this post will demonstrate.\\n\\n![title image reading \\"Azure DevOps: consume a private artifact feed\\" with the Azure DevOps and Azure Pipelines logos`](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Make a `nuget.config`\\n\\nTo consume a private feed, you\'ll likely want to create a `nuget.config` file in the root of your repo. Here you list the package sources you want to consume, typically the NuGet official package source _as well_ as your private feed. See the example below:\\n\\n```xml\\n<?xml version=\\"1.0\\" encoding=\\"utf-8\\"?>\\n  <configuration>\\n    <packageSources>\\n      <add key=\\"NuGet official package source\\" value=\\"https://api.nuget.org/v3/index.json\\" />\\n      <add key=\\"my-nuget-packages\\" value=\\"https://pkgs.dev.azure.com/my-org/_packaging/my-nuget-packages/nuget/v3/index.json\\" />\\n    </packageSources>\\n  </configuration>\\n```\\n\\n## Consuming a private feed locally with the Azure Artifacts Credential Provider\\n\\nWith our `nuget.config` in place, can we build locally? Yes, once we\'ve authenticated. If you\'re using Rider or Visual Studio, these may take care of this for you. However, if you\'re using VS Code you might need to do something else.\\n\\nIf you experience 401\'s when you run `dotnet restore` like so:\\n\\n```shell\\nerror : Unable to load the service index for source https://pkgs.dev.azure.com/my-org/_packaging/not-there/nuget/v3/index.json. [/dev.azure.com/project/repo/src/App.csproj]\\nerror : Response status code does not indicate success: 401\\n```\\n\\nThen it\'s probably a sign you need to install the [Azure Artifacts Credential Provider](https://github.com/Microsoft/artifacts-credprovider). With that you should be able to restore nuget packages. See instructions [here](https://github.com/Microsoft/artifacts-credprovider#setup).\\n\\nOn Linux and Mac this is as simple as running `sh -c \\"$(curl -fsSL https://aka.ms/install-artifacts-credprovider.sh)\\"` in your terminal.\\n\\nSubsequently, running `dotnet restore --interactive` should trigger an authentication flow in the terminal, and subject to successful authentication, restore packages from the private feed.\\n\\n## Consuming a private feed in Azure Pipelines\\n\\nYou will need to authenticate within your pipeline before you can acquire your private feed packages. This is as simple as this:\\n\\n```yml\\n- task: NuGetAuthenticate@0\\n```\\n\\nBefore building / publishing or running tests, you must first explicitly `dotnet restore` and provide the path to the `nuget.config`. You can do this with the dedicated [.NET Core CLI task](https://docs.microsoft.com/en-us/azure/devops/pipelines/tasks/build/dotnet-core-cli) task like so:\\n\\n```yml\\n- task: DotNetCoreCLI@2\\n  displayName: \'dotnet restore\'\\n  inputs:\\n    command: \'restore\'\\n    projects: \'src/App/App.csproj\'\\n    nugetConfigPath: \'../../nuget.config\'\\n    feedsToUse: config\\n```\\n\\n## The publish gotcha\\n\\nOn occasion, it can happen that Azure Pipelines doesn\'t seem to be happy running a publish task with private feeds. Consider, a task like this:\\n\\n```yml\\n- task: DotNetCoreCLI@2\\n  displayName: \'dotnet publish\'\\n  inputs:\\n    command: publish\\n    arguments: \'--configuration Release --output $(Build.ArtifactStagingDirectory)/${{parameters.artifactName}} /p:SourceRevisionId=$(Build.SourceVersion)\'\\n    zipAfterPublish: true\\n    publishWebProjects: false\\n    workingDirectory: src/App\\n```\\n\\nThis can result in non-actionable errors like this:\\n\\n> `##[error]Error: There was an error when attempting to execute the process \'/opt/hostedtoolcache/dotnet/dotnet\'. This may indicate the process failed to start. Error: spawn /opt/hostedtoolcache/dotnet/dotnet ENOENT`\\n\\nA workaround in this situation is to invoke .NET through a bash script directly like so:\\n\\n```yml\\n- bash: |\\n    cd src/App\\n    dotnet --list-sdks\\n    echo \\"\\"\\n    echo \\"**************\\"\\n    echo \\"dotnet restore --configfile ../../nuget.config\\"\\n    dotnet restore --configfile ../../nuget.config\\n    echo \\"\\"\\n    echo \\"**************\\"\\n    echo \\"dotnet build --configuration Release --no-restore\\"\\n    dotnet build --configuration Release\\n    echo \\"\\"\\n    echo \\"**************\\"\\n    echo \\"dotnet publish --configuration Release --no-restore --output $(Build.ArtifactStagingDirectory)/App /p:SourceRevisionId=$(Build.SourceVersion)\\"\\n    dotnet publish --configuration Release --no-restore --output $(Build.ArtifactStagingDirectory)/App /p:SourceRevisionId=$(Build.SourceVersion)\\n  displayName: \'dotnet publish\'\\n\\n- task: ArchiveFiles@2\\n  displayName: \'Create $(Build.ArtifactStagingDirectory)/App.zip\'\\n  inputs:\\n    rootFolderOrFile: \'$(Build.ArtifactStagingDirectory)/App\'\\n    includeRootFolder: false\\n    archiveFile: \'$(Build.ArtifactStagingDirectory)/App.zip\'\\n```\\n\\nAnd note that after publishing we use the [Archive Files task](https://docs.microsoft.com/en-us/azure/devops/pipelines/tasks/utility/archive-files) to zip up the output of our publishing.\\n\\nYou may be tempted to use the zip command line utility to make your zip. Do not do this. I did this. I learned, through no small amount of suffering, that there is a problem with this. Whilst you can make a zip this way that will be consumed happily by Mac and OSX, when it comes to being deployed to Azure (even if you\'re deploying to Linux within Azure) via zip deploy it will not work. I can\'t tell you why, just that it won\'t. So use the dedicated task.\\n\\n## Summing up\\n\\nAnd that\'s it; with these approaches in place you should be able to build applications consuming privage NuGet feeds with ease."},{"id":"lighthouse-meet-github-actions","metadata":{"permalink":"/lighthouse-meet-github-actions","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-03-20-lighthouse-meet-github-actions/index.md","source":"@site/blog/2022-03-20-lighthouse-meet-github-actions/index.md","title":"Lighthouse meet GitHub Actions","description":"This post illustrates how to integrate Lighthouse into a GitHub Actions workflow for an Azure Static Web App.","date":"2022-03-20T00:00:00.000Z","tags":[{"inline":false,"label":"Azure Static Web Apps","permalink":"/tags/azure-static-web-apps","description":"The Azure Static Web Apps service."},{"inline":false,"label":"GitHub Actions","permalink":"/tags/github-actions","description":"The GitHub Actions CI / CD service."},{"inline":false,"label":"Docusaurus","permalink":"/tags/docusaurus","description":"The Docusaurus static site generator."}],"readingTime":11.765,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"lighthouse-meet-github-actions","title":"Lighthouse meet GitHub Actions","authors":"johnnyreilly","tags":["azure static web apps","github actions","docusaurus"],"image":"./title-image.png","description":"This post illustrates how to integrate Lighthouse into a GitHub Actions workflow for an Azure Static Web App.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Azure DevOps: consume a private artifact feed","permalink":"/azure-devops-consume-private-nuget-artifact-feed"},"nextItem":{"title":"Swashbuckle & inheritance: Give. Me. The. Types","permalink":"/swashbuckle-inheritance-multiple-return-types"}},"content":"Lighthouse is a tremendous tool for auditing the performance and usability of websites. Rather than having to perform these audits manually, it\'s helpful to be able to plug it into your CI pipeline. This post illustrates how to integrate Lighthouse into a GitHub Actions workflow for an Azure Static Web App, and report findings directly in pull requests that are raised.\\n\\n![title image reading \\"Lighthouse meet GitHub Actions\\" with the Lighthouse logo and a screenshot of the results in a GitHub comment`](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## What we\'ll do\\n\\nThis post isn\'t a walkthrough of how to use Lighthouse effectively. There is already [great guidance out there on this topic](https://blog.logrocket.com/lighthouse-and-how-to-use-it-more-effectively/).\\n\\nInstead, we\'re going build a simple web application, in the context of a GitHub repo. We\'ll wire it up to deploy via GitHub Actions to Azure Static Web Apps. Static Web Apps is a free hosting option for static websites and it comes with [staging environments](https://docs.microsoft.com/en-us/azure/static-web-apps/review-publish-pull-requests) or deployment previews built in. This feature deploys a fully functional version of a site each time a pull request is raised, built upon the changes implemented in that pull request.\\n\\nThe staging environment is a perfect place to implement our Lighthouse checks. If a pull request impacts usability or performance, seeing those details in the context of our pull request is exactly where we\'d like to learn this. This kind of check gives us the opportunity to ensure we only merge when we\'re happy that the changes do not negatively impact our Lighthouse scores.\\n\\nIn this post we\'ll start from the point of an empty GitHub repo and build up from there.\\n\\n## Create our application\\n\\nInside the root of our repository we\'re going to create a [Docusaurus site](https://docusaurus.io/). Docusaurus is a good example of a static site, the kind of which is a natural fit for Jamstack. We could equally use something else like [Hugo](https://gohugo.io/) for instance.\\n\\nAt the command line we\'ll enter:\\n\\n```shell\\nnpx create-docusaurus@latest website classic\\n```\\n\\nAnd Docusaurus will create a new site in the `website` directory. Let\'s commit and push this and turn our attention to Azure.\\n\\n## Creating a Static Web App in Azure\\n\\nThere\'s a number of ways to create a Static Web App in Azure. It\'s possible to use [infrastructure as code with a language like Bicep](https://johnnyreilly.com/2021/08/15/bicep-azure-static-web-apps-azure-devops#bicep-template). But for this post let\'s use the [Azure Portal](https://portal.azure.com) instead. If you don\'t have an account already, you can set one up for free very quickly.\\n\\nOnce you\'ve logged in, click \\"Create a resource\\" and look up Static Web App:\\n\\n![Screenshot of the Azure Portal, \\"Create a resource\\" Azure Static Web Apps section](screenshot-azure-portal-create-a-resource.png)\\n\\nClick on \\"Create\\" and you\'ll be take to the creation dialog:\\n\\n![Screenshot of the Azure Portal, \\"Create a resource\\" Azure Static Web Apps dialog](screenshot-azure-portal-create-a-resource-dialog.png)\\n\\nYou\'ll need to create a resource group for your SWA to live in, give the app a name, the \\"Free\\" plan and a deployment source of GitHub.\\n\\nClick on the \\"Sign in with GitHub\\" button and authorize Azure to access your GitHub account for Static Web Apps.\\n\\n![Screenshot of the Azure Portal, \\"Create a resource\\" Azure Static Web Apps dialog - repository settings](screenshot-azure-portal-create-a-resource-dialog-repo.png)\\n\\nAt this point Azure will query GitHub on your behalf and look up the organisations and repositories you have access to. Select the repository that you\'d like to deploy to your Static Web App and select the branch you\'d like to deploy.\\n\\nYou also need to provide Azure with some build details that help it understand how your app is built. We\'ll provide a preset of \\"Custom\\". We\'ll set the \\"App location\\" (the root of our front end app) to be `\\"/website\\"` to tally up with the application we just created. We\'ll leave \\"Api location\\" blank and we\'ll set the output location to be `\\"build\\"` - this is the directory under `website` where Docusaurus will create our site.\\n\\nFinally click \\"Review + create\\" and then \\"Create\\".\\n\\nAzure will now:\\n\\n- Create an Azure Static Web app resource in Azure\\n- Update your repository to add a GitHub Actions workflow to deploy your static web app\\n- Kick off a first run of the GitHub Actions workflow to deploy your SWA.\\n\\nPretty amazing, right?\\n\\nWhen you look at the resource in Azure it will look something like this:\\n\\n![Screenshot of the Azure Portal, your Azure Static Web Apps resource](screenshot-azure-portal-static-web-app-resource.png)\\n\\nIf you click on the GitHub Action runs you\'ll be presented with your GitHub Action:\\n\\n![Screenshot of the GitHub Action](screenshot-github-action.png)\\n\\nAnd when that finishes running you\'ll be able to see your deployed Static Web App by clicking on the URL in the Azure Portal:\\n\\n![Screenshot of your Static Web App running in a browser](screenshot-static-web-app.png)\\n\\nWe now have:\\n\\n- a GitHub repo\\n- which contains a simple web application\\n- and a GitHub Actions workflow which:\\n  - deploys to an Azure Static Web App\\n  - spins up a staging environment for pull requests\\n\\n## Preparing to plug in Lighthouse\\n\\nWith this groundwork in place we\'re ready to add Lighthouse into the mix. If you look in the `/.github/workflows` folder of your repo, you\'ll find a workflow file with contents along these lines:\\n\\n```yml\\nname: Azure Static Web Apps CI/CD\\n\\non:\\n  push:\\n    branches:\\n      - main\\n  pull_request:\\n    types: [opened, synchronize, reopened, closed]\\n    branches:\\n      - main\\n\\njobs:\\n  build_and_deploy_job:\\n    if: github.event_name == \'push\' || (github.event_name == \'pull_request\' && github.event.action != \'closed\')\\n    runs-on: ubuntu-latest\\n    name: Build and Deploy Job\\n    steps:\\n      - uses: actions/checkout@v2\\n        with:\\n          submodules: true\\n\\n      - name: Build And Deploy\\n        id: builddeploy\\n        uses: Azure/static-web-apps-deploy@v1\\n        with:\\n          azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN_AGREEABLE_ROCK_039A51810 }}\\n          repo_token: ${{ secrets.GITHUB_TOKEN }} # Used for Github integrations (i.e. PR comments)\\n          action: \'upload\'\\n          ###### Repository/Build Configurations - These values can be configured to match your app requirements. ######\\n          # For more information regarding Static Web App workflow configurations, please visit: https://aka.ms/swaworkflowconfig\\n          app_location: \'/website\' # App source code path\\n          api_location: \'\' # Api source code path - optional\\n          output_location: \'build\' # Built app content directory - optional\\n          ###### End of Repository/Build Configurations ######\\n\\n  close_pull_request_job:\\n    if: github.event_name == \'pull_request\' && github.event.action == \'closed\'\\n    runs-on: ubuntu-latest\\n    name: Close Pull Request Job\\n    steps:\\n      - name: Close Pull Request\\n        id: closepullrequest\\n        uses: Azure/static-web-apps-deploy@v1\\n        with:\\n          azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN_AGREEABLE_ROCK_039A51810 }}\\n          action: \'close\'\\n```\\n\\nThis was created for us when we set up our SWA in Azure. We\'re now going to update the contents to add some Lighthouse jobs.\\n\\nBefore we do that, we need to acquire two things:\\n\\n1. the custom domain of our static web app\\n2. the location of the resource group where the SWA resides\\n\\nThese two pieces of information are required such that we can determine the URL of our staging environments.\\n\\n### Custom domain\\n\\nWe acquire the custom domain of our static web app in the \\"Custom Domains\\" screen of the Azure Portal:\\n\\n![screenshot of the custom domain screen in the Azure Portal](screenshot-azure-portal-static-web-app-custom-domain.png)\\n\\nThe custom domain is the auto-generated custom domain - it\'s highlighted in the screenshot above. For the SWA we\'re building here the custom domain is `agreeable-rock-039a51810.1.azurestaticapps.net`.\\n\\n### Location\\n\\nWe acquire the location by looking at the resource group in the Azure Portal. For the SWA we\'ve been building the location is \\"Central US\\". However, rather than the \\"display name\\" variant of the location, what we want is the \\"code\\" which will be used in the URL. You can see what this is by clicking on the \\"JSON view\\" in the Azure Portal:\\n\\n![screenshot of the resource group JSON view with location highlighted](screenshot-azure-portal-static-web-app-rg-location.png)\\n\\nAs the screenshot above demonstrates, the code we need is `centralus`.\\n\\n## Plugging in Lighthouse\\n\\nWe now have all we need to plug in Lighthouse. Let\'s create a branch:\\n\\n```shell\\ngit checkout -b lighthouse\\n```\\n\\nWe\'re going to add a new \\"Lighthouse report\\" job to our GitHub Actions workflow file:\\n\\n```yml\\nenv:\\n  RESOURCE_GROUP: rg-blog-johnnyreilly-com\\n  LOCATION: westeurope\\n  STATICWEBAPPNAME: blog.johnnyreilly.com\\n\\nlighthouse_report_job:\\n  name: Lighthouse report\\n  if: github.event_name == \'pull_request\' && github.event.action != \'closed\'\\n  runs-on: ubuntu-latest\\n  steps:\\n    - uses: actions/checkout@v2\\n\\n    # Auth between GitHub and Azure is handled by https://github.com/jongio/github-azure-oidc\\n    # https://github.com/Azure/login#sample-workflow-that-uses-azure-login-action-using-oidc-to-run-az-cli-linux\\n    # other login options are possible too\\n    - name: AZ CLI login \uD83D\uDD11\\n      uses: azure/login@v1\\n      with:\\n        client-id: ${{ secrets.AZURE_CLIENT_ID }}\\n        tenant-id: ${{ secrets.AZURE_TENANT_ID }}\\n        subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}\\n\\n    - name: Static Web App - get preview URL\\n      id: static_web_app_preview_url\\n      uses: azure/CLI@v2\\n      with:\\n        inlineScript: |\\n          DEFAULTHOSTNAME=$(az staticwebapp show -n \'${{ env.STATICWEBAPPNAME }}\' | jq -r \'.defaultHostname\')\\n\\n          PREVIEW_URL=\\"https://${DEFAULTHOSTNAME/.[1-9]./-${{github.event.pull_request.number }}.${{ env.LOCATION }}.1.}\\"\\n\\n          echo \\"PREVIEW_URL=$PREVIEW_URL\\" >> $GITHUB_OUTPUT\\n\\n    - name: Static Web App - wait for preview\\n      id: static_web_app_wait_for_preview\\n      uses: nev7n/wait_for_response@v1\\n      with:\\n        url: \'${{ steps.static_web_app_preview_url.outputs.PREVIEW_URL }}\'\\n        responseCode: 200\\n        timeout: 600000\\n        interval: 1000\\n\\n    - name: Audit URLs using Lighthouse\\n      id: lighthouse_audit\\n      uses: treosh/lighthouse-ci-action@v8\\n      with:\\n        urls: |\\n          ${{ steps.static_web_app_preview_url.outputs.PREVIEW_URL }}\\n        configPath: ./.github/workflows/lighthousesrc.json\\n        uploadArtifacts: true\\n        temporaryPublicStorage: true\\n        runs: 5\\n\\n    - name: Format lighthouse score\\n      id: format_lighthouse_score\\n      uses: actions/github-script@v5\\n      with:\\n        script: |\\n          const lighthouseCommentMaker = require(\'./.github/workflows/lighthouseCommentMaker.js\');\\n\\n          const lighthouseOutputs = {\\n            manifest: ${{ steps.lighthouse_audit.outputs.manifest }},\\n            links: ${{ steps.lighthouse_audit.outputs.links }}\\n          };\\n\\n          const comment = lighthouseCommentMaker({ lighthouseOutputs });\\n          core.setOutput(\\"comment\\", comment);\\n\\n    - name: Add Lighthouse stats as comment\\n      id: comment_to_pr\\n      uses: marocchino/sticky-pull-request-comment@v2.0.0\\n      with:\\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\\n        number: ${{ github.event.pull_request.number }}\\n        header: lighthouse\\n        message: ${{ steps.format_lighthouse_score.outputs.comment }}\\n```\\n\\nThere\'s a number of things happening in this workflow. Let\'s walk through them.\\n\\n### Static Web App - get preview URL\\n\\nHere we acquire the preview URL of our static web app using:\\n\\n- the default host name of the static web app\\n- the location\\n- the pull request number eg 123\\n\\nGiven a default host name of `agreeable-rock-039a51810`, a location of `centralus` and a pull request number of `123`, the preview url would be `agreeable-rock-039a51810-123.centralus.1.azurestaticapps.net`. Using a little bash magic we create an output variable named `PREVIEW_URL` containing that value. We\'ll re-use it later in the workflow.\\n\\n### Static Web App - wait for preview\\n\\nWe don\'t want to run our test until the static web app is up and running. To cater for this we\'re going to pull in the [`wait_for_response`](https://github.com/nev7n/wait_for_response) GitHub Action. This polls until a website returns a `200`, we\'re going to point it at our SWA.\\n\\n### Audit URLs using Lighthouse\\n\\nThe big moment has arrived! We\'re going to plug Lighthouse into our workflow using the [`lighthouse-ci-action`](https://github.com/treosh/lighthouse-ci-action) GitHub Action.\\n\\nWe provide a `configPath: ./.github/workflows/lighthousesrc.json` which points to file that configures our Lighthouse configuration. We\'ll create that file as well and populate it with this:\\n\\n```json\\n{\\n  \\"ci\\": {\\n    \\"collect\\": {\\n      \\"settings\\": {\\n        \\"configPath\\": \\"./.github/workflows/lighthouse-config.js\\"\\n      }\\n    }\\n  }\\n}\\n```\\n\\nThis in turn hands off the specific configuration to a `lighthouse-config.js` file that we also need to create:\\n\\n```js\\n// see https://github.com/GoogleChrome/lighthouse/blob/master/docs/configuration.md\\nmodule.exports = {\\n  extends: \'lighthouse:default\',\\n  settings: {\\n    // audits can be found here:\\n    // https://github.com/GoogleChrome/lighthouse/blob/eba2a4d19c5786dc37e993858ff4b663181f81e5/lighthouse-core/config/default-config.js#L174\\n    skipAudits: [\\n      \'canonical\', // for staging sites this will always be incorrect\\n      \'maskable-icon\',\\n      \'valid-source-maps\',\\n      \'unsized-images\',\\n      \'offline-start-url\',\\n    ],\\n  },\\n};\\n```\\n\\nThe configuration above can be amended based upon the various links in the comments. Generally it\'s a good idea to roll with the defaults; however skipping the `canonical` audit is sensible as it will reliably be incorrect for staging sites.\\n\\nAlong side the Lighthouse configuration, there\'s config for the GitHub Action itself:\\n\\n- `uploadArtifacts: true` - will save results as an action artifacts\\n- `temporaryPublicStorage: true` - will upload lighthouse report to the temporary storage\\n- `runs: 5` - will run Lighthouse 5 times to get more reliable performance results\\n\\n### Format lighthouse score\\n\\nWe\'ve run Lighthouse at this point. What we want to do next is take the results of the run and build up some text that we can add to our pull request as a comment.\\n\\nFor this we\'re going to use the [`github-script`](https://github.com/actions/github-script) GitHub Action, grab the outputs of the previous step and call out to a `lighthouseCommentMaker.js` file we\'re going to write to make the comment we\'d like to publish to our PR:\\n\\n```js\\n// @ts-check\\n\\n/**\\n * @typedef {Object} Summary\\n * @prop {number} performance\\n * @prop {number} accessibility\\n * @prop {number} best-practices\\n * @prop {number} seo\\n * @prop {number} pwa\\n */\\n\\n/**\\n * @typedef {Object} Manifest\\n * @prop {string} url\\n * @prop {boolean} isRepresentativeRun\\n * @prop {string} htmlPath\\n * @prop {string} jsonPath\\n * @prop {Summary} summary\\n */\\n\\n/**\\n * @typedef {Object} LighthouseOutputs\\n * @prop {Record<string, string>} links\\n * @prop {Manifest[]} manifest\\n */\\n\\nconst formatScore = (/** @type { number } */ score) => Math.round(score * 100);\\nconst emojiScore = (/** @type { number } */ score) =>\\n  score >= 0.9 ? \'\uD83D\uDFE2\' : score >= 0.5 ? \'\uD83D\uDFE0\' : \'\uD83D\uDD34\';\\n\\nconst scoreRow = (\\n  /** @type { string } */ label,\\n  /** @type { number } */ score,\\n) => `| ${emojiScore(score)} ${label} | ${formatScore(score)} |`;\\n\\n/**\\n * @param {LighthouseOutputs} lighthouseOutputs\\n */\\nfunction makeComment(lighthouseOutputs) {\\n  const { summary } = lighthouseOutputs.manifest[0];\\n  const [[testedUrl, reportUrl]] = Object.entries(lighthouseOutputs.links);\\n\\n  const comment = `## \u26A1\uFE0F\uD83C\uDFE0 Lighthouse report\\n\\nWe ran Lighthouse against the changes and produced this [report](${reportUrl}). Here\'s the summary:\\n\\n| Category | Score |\\n| -------- | ----- |\\n${scoreRow(\'Performance\', summary.performance)}\\n${scoreRow(\'Accessibility\', summary.accessibility)}\\n${scoreRow(\'Best practices\', summary[\'best-practices\'])}\\n${scoreRow(\'SEO\', summary.seo)}\\n${scoreRow(\'PWA\', summary.pwa)}\\n\\n*Lighthouse ran against [${testedUrl}](${testedUrl})*\\n`;\\n\\n  return comment;\\n}\\n\\nmodule.exports = ({ lighthouseOutputs }) => {\\n  return makeComment(lighthouseOutputs);\\n};\\n```\\n\\nThe above code takes the Lighthouse outputs and creates some Markdown to represent the results. It uses some nice emoji as well. Wonderfully, we\'re entirely free to customise this as much as we\'d like; it\'s just code! All that matters is that a string is pumped out at the end.\\n\\n### Add Lighthouse stats as comment\\n\\nFinally we\'re ready to add the comment to the PR. We\'ll do this using the [`sticky-pull-request-comment`](https://github.com/marocchino/sticky-pull-request-comment) GitHub Action. We pass in the comment we\'ve just made in the previous step, as well as some other parameters, and this will write the comment to the PR.\\n\\n## Putting it all together\\n\\nWhen we commit our changes and raise a pull request, we see our GitHub Action run, and then once it has we see a Lighthouse report being attached to our pull request:\\n\\n![screenshot of GitHub pull request showing the Lighthouse results as a comment](screenshot-lighthouse-github-comment.png)\\n\\nNote you can also click on a link in the comment to go directly to the full report.\\n\\n![screenshot of Lighthouse report](screenshot-lighthouse-report.png)\\n\\nNow, with each PR that is raised, any regressions in performance can be observed and resolved _before_ they make get in front of customer\'s eyes!\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/lighthouse-meets-github-actions-use-lighthouse-ci/)\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.logrocket.com/lighthouse-meets-github-actions-use-lighthouse-ci/\\" />\\n</head>"},{"id":"swashbuckle-inheritance-multiple-return-types","metadata":{"permalink":"/swashbuckle-inheritance-multiple-return-types","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-03-06-swashbuckle-inheritance-multiple-return-types/index.md","source":"@site/blog/2022-03-06-swashbuckle-inheritance-multiple-return-types/index.md","title":"Swashbuckle & inheritance: Give. Me. The. Types","description":"For API endpoints that return multiple types, you can use inheritance with Swashbuckle to get create a Swagger / Open API definition; here is how.","date":"2022-03-06T00:00:00.000Z","tags":[{"inline":false,"label":"Swagger","permalink":"/tags/swagger","description":"The Swagger API documentation framework - now known as OpenAPI."},{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":5.56,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"swashbuckle-inheritance-multiple-return-types","title":"Swashbuckle & inheritance: Give. Me. The. Types","authors":"johnnyreilly","tags":["swagger","asp.net"],"image":"./title-image.png","description":"For API endpoints that return multiple types, you can use inheritance with Swashbuckle to get create a Swagger / Open API definition; here is how.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Lighthouse meet GitHub Actions","permalink":"/lighthouse-meet-github-actions"},"nextItem":{"title":"Azure Static Web Apps - a Netlify alternative","permalink":"/azure-static-web-apps-a-netlify-alternative"}},"content":"For API endpoints that return multiple types, you can use inheritance with Swashbuckle to get create a Swagger / Open API definition featuring the variety of available types. Serving all these types is not the default behaviour. This post shows you how to opt in.\\n\\n![title image reading \\"Swashbuckle and inheritance: Give. Me. The. Types\\" with Sid Swashbuckle the Pirate and Open API logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Making a simple API\\n\\nThe first thing we\'re going to need is an API, which we\'ll build with the .NET 6 SDK:\\n\\n```bash\\ndotnet new webapi\\ndotnet add package Swashbuckle.AspNetCore\\n```\\n\\nWhen we run this with `dotnet run` we find Swashbuckle living at http://localhost:5000/swagger/index.html defining our web api that serves up a WeatherForecast:\\n\\n![screenshot of swagger UI including `WeatherForecast`](screenshot-initial-swagger-ui.png)\\n\\nIf we look at the `swagger.json` created at our `http://localhost:5000/swagger/v1/swagger.json` endpoint we see the following definition:\\n\\n```json\\n{\\n  \\"openapi\\": \\"3.0.1\\",\\n  \\"info\\": {\\n    \\"title\\": \\"SwashbuckleInheritance\\",\\n    \\"version\\": \\"1.0\\"\\n  },\\n  \\"paths\\": {\\n    \\"/WeatherForecast\\": {\\n      \\"get\\": {\\n        \\"tags\\": [\\"WeatherForecast\\"],\\n        \\"operationId\\": \\"GetWeatherForecast\\",\\n        \\"responses\\": {\\n          \\"200\\": {\\n            \\"description\\": \\"Success\\",\\n            \\"content\\": {\\n              \\"text/plain\\": {\\n                \\"schema\\": {\\n                  \\"type\\": \\"array\\",\\n                  \\"items\\": {\\n                    \\"$ref\\": \\"#/components/schemas/WeatherForecast\\"\\n                  }\\n                }\\n              },\\n              \\"application/json\\": {\\n                \\"schema\\": {\\n                  \\"type\\": \\"array\\",\\n                  \\"items\\": {\\n                    \\"$ref\\": \\"#/components/schemas/WeatherForecast\\"\\n                  }\\n                }\\n              },\\n              \\"text/json\\": {\\n                \\"schema\\": {\\n                  \\"type\\": \\"array\\",\\n                  \\"items\\": {\\n                    \\"$ref\\": \\"#/components/schemas/WeatherForecast\\"\\n                  }\\n                }\\n              }\\n            }\\n          }\\n        }\\n      }\\n    }\\n  },\\n  \\"components\\": {\\n    \\"schemas\\": {\\n      \\"WeatherForecast\\": {\\n        \\"type\\": \\"object\\",\\n        \\"properties\\": {\\n          \\"date\\": {\\n            \\"type\\": \\"string\\",\\n            \\"format\\": \\"date-time\\"\\n          },\\n          \\"temperatureC\\": {\\n            \\"type\\": \\"integer\\",\\n            \\"format\\": \\"int32\\"\\n          },\\n          \\"temperatureF\\": {\\n            \\"type\\": \\"integer\\",\\n            \\"format\\": \\"int32\\",\\n            \\"readOnly\\": true\\n          },\\n          \\"summary\\": {\\n            \\"type\\": \\"string\\",\\n            \\"nullable\\": true\\n          }\\n        },\\n        \\"additionalProperties\\": false\\n      }\\n    }\\n  }\\n}\\n```\\n\\nOnly a single return type is defined: `WeatherForecast`.\\n\\n## Multiple return types\\n\\nNow we\'ve got our simple API, let\'s evolve it to serve up multiple types. We\'re going to do this by updating our `WeatherForecast.cs` as follows:\\n\\n```cs\\npublic class WeatherForecast\\n{\\n    public DateTime Date { get; set; }\\n\\n    public int TemperatureC { get; set; }\\n\\n    public int TemperatureF => 32 + (int)(TemperatureC / 0.5556);\\n\\n    public string? Summary { get; set; }\\n}\\n\\npublic class WeatherForecastWithLocation : WeatherForecast\\n{\\n    public string? Location { get; set; }\\n}\\n```\\n\\nWe now have both a `WeatherForecast` and a `WeatherForecastWithLocation` that inherits from `WeatherForecast` and adds in a `Location` property.\\n\\nWe\'ll also update the `GetWeatherForecast` endpoint to surface both these types:\\n\\n```cs\\n[HttpGet(Name = \\"GetWeatherForecast\\")]\\npublic IEnumerable<WeatherForecast> Get() =>\\n    DateTime.Now.Minute < 30\\n        ? Enumerable.Range(1, 5).Select(index => new WeatherForecast\\n        {\\n            Date = DateTime.Now.AddDays(index),\\n            TemperatureC = Random.Shared.Next(-20, 55),\\n            Summary = Summaries[Random.Shared.Next(Summaries.Length)]\\n        })\\n        : Enumerable.Range(1, 5).Select(index => new WeatherForecastWithLocation\\n        {\\n            Date = DateTime.Now.AddDays(index),\\n            TemperatureC = Random.Shared.Next(-20, 55),\\n            Summary = Summaries[Random.Shared.Next(Summaries.Length)],\\n            Location = \\"London\\"\\n        })\\n        .ToArray();\\n```\\n\\nWe\'ve amended the endpoint to return `WeatherForecast`s for the first thirty minutes of each hour, and `WeatherForecastWithLocation`s for the second thirty minutes. This is plainly a contrived example, but it demonstrates what it looks like to have an API endpoint with multiple return types.\\n\\nIncidentally, the reason we\'re able to achieve this without the compiler shouting at us is because our endpoint is saying it returns a `WeatherForecast` and that is the base type of `WeatherForecastWithLocation` as well.\\n\\nTo prove that it works, we wait for half past the hour and enter:\\n\\n```bash\\ncurl -X \'GET\' \'http://localhost:5000/WeatherForecast\'\\n```\\n\\nWe see back JSON that includes the `Location` property. Huzzah!\\n\\n```json\\n[\\n  {\\n    \\"location\\": \\"London\\",\\n    \\"date\\": \\"2022-03-07T08:51:02.0932353+00:00\\",\\n    \\"temperatureC\\": -4,\\n    \\"temperatureF\\": 25,\\n    \\"summary\\": \\"Bracing\\"\\n  },\\n  {\\n    \\"location\\": \\"London\\",\\n    \\"date\\": \\"2022-03-08T08:51:02.0938418+00:00\\",\\n    \\"temperatureC\\": -5,\\n    \\"temperatureF\\": 24,\\n    \\"summary\\": \\"Balmy\\"\\n  },\\n  {\\n    \\"location\\": \\"London\\",\\n    \\"date\\": \\"2022-03-09T08:51:02.0938513+00:00\\",\\n    \\"temperatureC\\": 51,\\n    \\"temperatureF\\": 123,\\n    \\"summary\\": \\"Warm\\"\\n  },\\n  {\\n    \\"location\\": \\"London\\",\\n    \\"date\\": \\"2022-03-10T08:51:02.0938518+00:00\\",\\n    \\"temperatureC\\": 35,\\n    \\"temperatureF\\": 94,\\n    \\"summary\\": \\"Warm\\"\\n  },\\n  {\\n    \\"location\\": \\"London\\",\\n    \\"date\\": \\"2022-03-11T08:51:02.0938537+00:00\\",\\n    \\"temperatureC\\": 2,\\n    \\"temperatureF\\": 35,\\n    \\"summary\\": \\"Cool\\"\\n  }\\n]\\n```\\n\\nWhilst we\'ve got behaviour that handles multiple return types, what we don\'t have is Swagger / Open API that represents that. Despite our tweaks, our Swagger / Open API definition remains unchanged.\\n\\n## Serving up subtypes\\n\\nIn a perfect world, C# would have support for discriminated unions, and we\'d be using [`oneOf`](https://swagger.io/docs/specification/data-models/oneof-anyof-allof-not/) to represent the multiple types being surfaced. [The day may come where C# supports discriminated unions](https://github.com/dotnet/csharplang/issues/113), but until that time we\'ll be achieving this behaviour with inheritance. We do this by having an endpoint that surfaces up a base type, and all our possible return types must either subclass that base type, or be that base type.\\n\\nTo be clearer: we want our served up Swagger / Open API definition to serve up the definitions of our subclasses. It needs to shout about `WeatherForecastWithLocation` in the same way it shouts about `WeatherForecast`.\\n\\nIt turns out that this is eminently achievable with Swashbuckle, but you do need to know where to look. [Look here](https://github.com/domaindrivendev/Swashbuckle.AspNetCore#describing-discriminators).\\n\\nTo apply this tweak to our own `Program.cs` we simply update the `AddSwaggerGen` as follows:\\n\\n```cs\\nbuilder.Services.AddSwaggerGen(swaggerGenOptions =>\\n{\\n    swaggerGenOptions.UseAllOfForInheritance();\\n    swaggerGenOptions.UseOneOfForPolymorphism();\\n\\n    swaggerGenOptions.SelectSubTypesUsing(baseType =>\\n        typeof(Program).Assembly.GetTypes().Where(type => type.IsSubclassOf(baseType))\\n    );\\n});\\n```\\n\\nThere\'s three things we\'re doing here:\\n\\n1. With [`UseAllOfForInheritance`](https://github.com/domaindrivendev/Swashbuckle.AspNetCore#enabling-inheritance) we\'re enabling inheritance - this allows us to maintain the inheritance hierarchy in any generated client models.\\n2. With [`UseOneOfForPolymorphism`](https://github.com/domaindrivendev/Swashbuckle.AspNetCore#enabling-polymorphism) we\'re listing the possible subtypes for an action that accepts/returns base types.\\n3. With [`SelectSubTypesUsing`](https://github.com/domaindrivendev/Swashbuckle.AspNetCore#detecting-subtypes) we\'re pointing Swashbuckle at the type hierarchies it exposes in the generated Swagger.\\n\\nThen next time we `dotnet run` we see that we\'re serving up both `WeatherForecast` and `WeatherForecastWithLocation`:\\n\\n![screenshot of swagger UI including `WeatherForecast` and `WeatherForecastWithLocation`](screenshot-swagger-ui-with-location.png)\\n\\nWe can also see this directly in the `swagger.json` created at our `http://localhost:5000/swagger/v1/swagger.json` endpoint:\\n\\n```json\\n{\\n  \\"openapi\\": \\"3.0.1\\",\\n  \\"info\\": {\\n    \\"title\\": \\"SwashbuckleInheritance\\",\\n    \\"version\\": \\"1.0\\"\\n  },\\n  \\"paths\\": {\\n    \\"/WeatherForecast\\": {\\n      \\"get\\": {\\n        \\"tags\\": [\\"WeatherForecast\\"],\\n        \\"operationId\\": \\"GetWeatherForecast\\",\\n        \\"responses\\": {\\n          \\"200\\": {\\n            \\"description\\": \\"Success\\",\\n            \\"content\\": {\\n              \\"text/plain\\": {\\n                \\"schema\\": {\\n                  \\"type\\": \\"array\\",\\n                  \\"items\\": {\\n                    \\"oneOf\\": [\\n                      {\\n                        \\"$ref\\": \\"#/components/schemas/WeatherForecast\\"\\n                      },\\n                      {\\n                        \\"$ref\\": \\"#/components/schemas/WeatherForecastWithLocation\\"\\n                      }\\n                    ]\\n                  }\\n                }\\n              },\\n              \\"application/json\\": {\\n                \\"schema\\": {\\n                  \\"type\\": \\"array\\",\\n                  \\"items\\": {\\n                    \\"oneOf\\": [\\n                      {\\n                        \\"$ref\\": \\"#/components/schemas/WeatherForecast\\"\\n                      },\\n                      {\\n                        \\"$ref\\": \\"#/components/schemas/WeatherForecastWithLocation\\"\\n                      }\\n                    ]\\n                  }\\n                }\\n              },\\n              \\"text/json\\": {\\n                \\"schema\\": {\\n                  \\"type\\": \\"array\\",\\n                  \\"items\\": {\\n                    \\"oneOf\\": [\\n                      {\\n                        \\"$ref\\": \\"#/components/schemas/WeatherForecast\\"\\n                      },\\n                      {\\n                        \\"$ref\\": \\"#/components/schemas/WeatherForecastWithLocation\\"\\n                      }\\n                    ]\\n                  }\\n                }\\n              }\\n            }\\n          }\\n        }\\n      }\\n    }\\n  },\\n  \\"components\\": {\\n    \\"schemas\\": {\\n      \\"WeatherForecast\\": {\\n        \\"type\\": \\"object\\",\\n        \\"properties\\": {\\n          \\"date\\": {\\n            \\"type\\": \\"string\\",\\n            \\"format\\": \\"date-time\\"\\n          },\\n          \\"temperatureC\\": {\\n            \\"type\\": \\"integer\\",\\n            \\"format\\": \\"int32\\"\\n          },\\n          \\"temperatureF\\": {\\n            \\"type\\": \\"integer\\",\\n            \\"format\\": \\"int32\\",\\n            \\"readOnly\\": true\\n          },\\n          \\"summary\\": {\\n            \\"type\\": \\"string\\",\\n            \\"nullable\\": true\\n          }\\n        },\\n        \\"additionalProperties\\": false\\n      },\\n      \\"WeatherForecastWithLocation\\": {\\n        \\"type\\": \\"object\\",\\n        \\"allOf\\": [\\n          {\\n            \\"$ref\\": \\"#/components/schemas/WeatherForecast\\"\\n          }\\n        ],\\n        \\"properties\\": {\\n          \\"location\\": {\\n            \\"type\\": \\"string\\",\\n            \\"nullable\\": true\\n          }\\n        },\\n        \\"additionalProperties\\": false\\n      }\\n    }\\n  }\\n}\\n```\\n\\nThere\'s two things to note about the new definition:\\n\\n1. The `WeatherForecastWithLocation` type is included in the `schemas`\\n2. The return type has widened to include `WeatherForecastWithLocation` as well using `oneOf`\\n\\n   ```json\\n   \\"oneOf\\": [\\n       {\\n           \\"$ref\\": \\"#/components/schemas/WeatherForecast\\"\\n       },\\n       {\\n           \\"$ref\\": \\"#/components/schemas/WeatherForecastWithLocation\\"\\n       }\\n   ]\\n   ```\\n\\nSuccess!"},{"id":"azure-static-web-apps-a-netlify-alternative","metadata":{"permalink":"/azure-static-web-apps-a-netlify-alternative","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-02-08-azure-static-web-apps-a-netlify-alternative/index.md","source":"@site/blog/2022-02-08-azure-static-web-apps-a-netlify-alternative/index.md","title":"Azure Static Web Apps - a Netlify alternative","description":"Azure Static Web Apps are a new offering from Microsoft. This post looks at what they are and how they compare to Netlify.","date":"2022-02-08T00:00:00.000Z","tags":[{"inline":false,"label":"Azure Static Web Apps","permalink":"/tags/azure-static-web-apps","description":"The Azure Static Web Apps service."},{"inline":false,"label":"GitHub Actions","permalink":"/tags/github-actions","description":"The GitHub Actions CI / CD service."},{"inline":false,"label":"Docusaurus","permalink":"/tags/docusaurus","description":"The Docusaurus static site generator."}],"readingTime":7.685,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-static-web-apps-a-netlify-alternative","title":"Azure Static Web Apps - a Netlify alternative","authors":"johnnyreilly","tags":["azure static web apps","github actions","docusaurus"],"description":"Azure Static Web Apps are a new offering from Microsoft. This post looks at what they are and how they compare to Netlify.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Swashbuckle & inheritance: Give. Me. The. Types","permalink":"/swashbuckle-inheritance-multiple-return-types"},"nextItem":{"title":"Lazy loading images with Docusaurus","permalink":"/lazy-loading-images-with-docusaurus"}},"content":"Jamstack sites have taken the world by storm. There\'s currently fierce competition between offerings like [Netlify and Cloudflare](https://blog.logrocket.com/netlify-vs-cloudflare-pages/). A new player in this space is Azure Static Web Apps. This post will look at what working with SWAs is like and will demonstrate deploying one using GitHub Actions.\\n\\n\x3c!--truncate--\x3e\\n\\n## Jamstack and Azure Static Web Apps\\n\\n[Jamstack](https://en.m.wikipedia.org/wiki/Jamstack) stands for JavaScript, API and Markup In Jamstack websites, the application logic typically resides on the client side. Typically these clients are built as [single-page applications](https://en.m.wikipedia.org/wiki/Single-page_application) and often have HTML files statically generated for every possible path to support search engine optimization.\\n\\nAzure Static Web Apps were released for general use in [May 2021](https://azure.microsoft.com/en-us/updates/azure-static-web-apps-is-now-generally-available/) and offer features including:\\n\\n- Globally distributed content for production apps\\n- Auto-provisioned preview environments\\n- Custom domain configuration and free SSL certificates\\n- Built-in access to a variety of authentication providers\\n- Route-based authorization\\n- Custom routing\\n- Integration with serverless APIs powered by Azure Functions\\n- A custom Visual Studio Code developer extension\\n\\nSignificantly, [these features are available to use for free](https://azure.microsoft.com/en-gb/pricing/details/app-service/static/). With Netlify there is also a [free tier](https://www.netlify.com/pricing/), however it\'s quite easy to exceed the build limits of the free tier and land yourself with an unexpected bill. By combining Azure Static Web Apps with GitHub Actions we can build comparable experiences and save ourselves money!\\n\\nSo let\'s build ourselves a simple SWA and deploy it with GitHub Actions.\\n\\n## Create our application\\n\\nInside the root of our repository we\'re going to create a [Docusaurus site](https://docusaurus.io/). Docusaurus is a good example of a static site, the kind of which is a natural fit for Jamstack. We could equally use something else like [Hugo](https://gohugo.io/) for instance.\\n\\nAt the command line we\'ll enter:\\n\\n```shell\\nnpx create-docusaurus@latest website classic\\n```\\n\\nAnd Docusaurus will create a new site in the `website` directory. Let\'s commit and push this and turn our attention to Azure.\\n\\n## Creating a Static Web App in Azure\\n\\nThere\'s a number of ways to create a Static Web App in Azure. It\'s possible to use [infrastructure as code with a language like Bicep](https://johnnyreilly.com/2021/08/15/bicep-azure-static-web-apps-azure-devops#bicep-template). But for this post let\'s use the [Azure Portal](https://portal.azure.com) instead. If you don\'t have an account already, you can set one up for free very quickly.\\n\\nOnce you\'ve logged in, click \\"Create a resource\\" and look up Static Web App:\\n\\n![Screenshot of the Azure Portal, \\"Create a resource\\" Azure Static Web Apps section](screenshot-azure-portal-create-a-resource.png)\\n\\nClick on \\"Create\\" and you\'ll be take to the creation dialog:\\n\\n![Screenshot of the Azure Portal, \\"Create a resource\\" Azure Static Web Apps dialog](screenshot-azure-portal-create-a-resource-dialog.png)\\n\\nYou\'ll need to create a resource group for your SWA to live in, give the app a name, the \\"Free\\" plan and a deployment source of GitHub.\\n\\nClick on the \\"Sign in with GitHub\\" button and authorize Azure to access your GitHub account for Static Web Apps.\\n\\n![Screenshot of the Azure Portal, \\"Create a resource\\" Azure Static Web Apps dialog - repository settings](screenshot-azure-portal-create-a-resource-dialog-repo.png)\\n\\nAt this point Azure will query GitHub on your behalf and look up the organisations and repositories you have access to. Select the repository that you\'d like to deploy to your Static Web App and select the branch you\'d like to deploy.\\n\\nYou also need to provide Azure with some build details that help it understand how your app is built. We\'ll provide a preset of \\"Custom\\". We\'ll set the \\"App location\\" (the root of our front end app) to be `\\"/website\\"` to tally up with the application we just created. We\'ll leave \\"Api location\\" blank and we\'ll set the output location to be `\\"build\\"` - this is the directory under `website` where Docusaurus will create our site.\\n\\nFinally click \\"Review + create\\" and then \\"Create\\".\\n\\nAzure will now:\\n\\n- Create an Azure Static Web app resource in Azure\\n- Update your repository to add a GitHub Actions workflow to deploy your static web app\\n- Kick off a first run of the GitHub Actions workflow to deploy your SWA.\\n\\nPretty amazing, right?\\n\\nWhen you look at the resource in Azure it will look something like this:\\n\\n![Screenshot of the Azure Portal, your Azure Static Web Apps resource](screenshot-azure-portal-static-web-app-resource.png)\\n\\nIf you click on the GitHub Action runs you\'ll be presented with your GitHub Action:\\n\\n![Screenshot of the GitHub Action](screenshot-github-action.png)\\n\\nAnd when that finishes running you\'ll be able to see your deployed Static Web App by clicking on the URL in the Azure Portal:\\n\\n![Screenshot of your Static Web App running in a browser](screenshot-static-web-app.png)\\n\\nWe\'ve gone from having nothing, to having a brand new website in Azure, shipped via continous deployment in GitHub Actions in a matter of minutes. This is low friction and high value!\\n\\n## Authentication\\n\\nNow we\'ve done our initial deployment, let\'s take it a stage further and add authentication.\\n\\nOne of the awesome features of Static Web Apps is the fact that [authentication is available straight out of the box](https://docs.microsoft.com/en-us/azure/static-web-apps/authentication-authorization?tabs=invitations#login). We can pick from GitHub, Azure Active Directory and Twitter as identity providers. Let\'s roll with GitHub and amend our `website/src/pages/index.js` to support authentication:\\n\\n```jsx\\nimport React, { useState, useEffect } from \'react\';\\nimport clsx from \'clsx\';\\nimport Layout from \'@theme/Layout\';\\nimport useDocusaurusContext from \'@docusaurus/useDocusaurusContext\';\\nimport styles from \'./index.module.css\';\\n\\n/**\\n * @typedef {object} UserInfo\\n * @prop {\\"github\\"} identityProvider\\n * @prop {string} userId\\n * @prop {string} userDetails\\n * @prop {string[]} userRoles\\n */\\n\\n/**\\n * @return {UserInfo | null}\\n */\\nfunction useUserInfo() {\\n  const [userInfo, setUserInfo] = useState(null);\\n\\n  useEffect(() => {\\n    async function getUserInfo() {\\n      const response = await fetch(\'/.auth/me\');\\n      const payload = await response.json();\\n      const { clientPrincipal } = payload;\\n      return clientPrincipal;\\n    }\\n\\n    getUserInfo().then((ui) => setUserInfo(ui));\\n  }, []);\\n\\n  return userInfo;\\n}\\n\\nexport default function Home() {\\n  const { siteConfig } = useDocusaurusContext();\\n  const userInfo = useUserInfo();\\n\\n  return (\\n    <Layout\\n      title={`Hello from ${siteConfig.title}`}\\n      description=\\"Description will go into a meta tag in <head />\\"\\n    >\\n      <header className={clsx(\'hero hero--primary\', styles.heroBanner)}>\\n        <div className=\\"container\\">\\n          <h1 className=\\"hero__title\\">{siteConfig.title}</h1>\\n          <p className=\\"hero__subtitle\\">{siteConfig.tagline}</p>\\n          <div className={styles.buttons}>\\n            {userInfo ? (\\n              <p>Hello {userInfo.userDetails}</p>\\n            ) : (\\n              <a\\n                className=\\"button button--secondary button--lg\\"\\n                href=\\"/.auth/login/github\\"\\n              >\\n                Click here to login\\n              </a>\\n            )}\\n          </div>\\n        </div>\\n      </header>\\n    </Layout>\\n  );\\n}\\n```\\n\\nThe above code does the following:\\n\\n- Implements a React hook named `useUserInfo` which calls the `/.auth/me` endpoint of your SWA. This returns `null` when not authenticated, and the `UserInfo` when authenticated.\\n- For users who are not authenticated, display a link button which takes people to `/.auth/login/github`, thus triggering the GitHub authentication flow.\\n- For users who are authenticated, display the users `userDetails`; the GitHub username.\\n\\nLet\'s commit and push this and (when our build has finished running) browse to our Static Web App once again:\\n\\n![Screenshot of Static Web App now featuring a login button](screenshot-static-web-app-login.png)\\n\\nIf we click to login, we\'re taken through the GitHub authentication flow:\\n\\n![Screenshot of Static Web App now featuring a login button](screenshot-static-web-app-login-github.png)\\n\\nOnce you\'ve authorised and granted consent you\'ll be redirected to your app and see that you\'re logged in:\\n\\n![Screenshot of Static Web App showing I\'m logged in](screenshot-static-web-app-logged-in.png)\\n\\nIf we pop open the devtools of Chrome we\'ll see what comes back from the `/.auth/me` endpoint:\\n\\n![Screenshot of Chrome devtools displaying a JSON structure](screenshot-static-web-app-devtools-me.png)\\n\\n```json\\n{\\n  \\"clientPrincipal\\": {\\n    \\"identityProvider\\": \\"github\\",\\n    \\"userId\\": \\"1f5b4b7de7d445e29dd6188bcc7ee052\\",\\n    \\"userDetails\\": \\"johnnyreilly\\",\\n    \\"userRoles\\": [\\"anonymous\\", \\"authenticated\\"]\\n  }\\n}\\n```\\n\\nWe\'ve now implemented and demonstrated authentication with Azure Static Web Apps with very little effort on our behalf. This is tremendous!\\n\\n## Staging Environments\\n\\nFinally, let\'s look at a super cool feature that Static Web Apps provides by default. If you take a look at the Environments tab of your SWA you\'ll see this:\\n\\n![Screenshot of the Azure Portal, your Azure Static Web Apps resource - featuring the phrase \\"Open pull requests against the linked repository to create a staging environment.\\"](screenshot-azure-portal-static-web-app-resource-environments.png)\\n\\n> ## Staging\\n>\\n> Open pull requests against the linked repository to create a staging environment.\\n\\nLet\'s try that out! We\'ll create a new branch:\\n\\n```shell\\ngit checkout -b feat/show-me-staging\\n```\\n\\nIn our `index.js` we\'ll add an arbitrary piece of text:\\n\\n```jsx\\n<p>I\'m a staging environment!</p>\\n```\\n\\nThen we\'ll commit and push our branch to GitHub and create a pull request. This triggers our GitHub Action to run once again. But this time, rather than publishing over our existing Static Web App, it\'s going to spin up a brand new one with our changes in. Not only that, it\'s going to put a link for us in our GitHub pull request so we can browse straight to it:\\n\\n![Screenshot of the pull request in GitHub including a comment from the GitHub Actions bot which says: \\"Azure Static Web Apps: Your stage site is ready! Visit it here: https://ambitious-island-05069ea10-2.centralus.azurestaticapps.net\\"](screenshot-github-pull-request-deploy-preview.png)\\n\\nThis is the equivalent of Netlify Deploy Previews, implemented with Azure Static Web Apps and GitHub Actions. Given the allowances for GitHub Actions currently sit at [2,000 free minutes per month](https://docs.github.com/en/billing/managing-billing-for-github-actions/about-billing-for-github-actions) as compared with Netlify\'s [300 free minutes per month](https://www.netlify.com/pricing/), you\'re less likely to receive a bill for using Static Web Apps.\\n\\nThis staging environment will last only until the pull request is closed. At that point the environment is torn down by the GitHub Action.\\n\\n## Conclusion\\n\\nIn this post we\'ve deployed a website to a Static Web App using GitHub Actions and implemented authentication. We\'ve also demonstrated Azure\'s equivalent of Netlify\'s deploy previews; staging environments.\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/azure-static-web-apps-netlify-alternative/)\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.logrocket.com/azure-static-web-apps-netlify-alternative/\\" />\\n</head>"},{"id":"lazy-loading-images-with-docusaurus","metadata":{"permalink":"/lazy-loading-images-with-docusaurus","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-02-02-lazy-loading-images-with-docusaurus/index.md","source":"@site/blog/2022-02-02-lazy-loading-images-with-docusaurus/index.md","title":"Lazy loading images with Docusaurus","description":"Docusaurus websites can implement native lazy-loading of images, you can by writing a Rehype plugin.","date":"2022-02-02T00:00:00.000Z","tags":[{"inline":false,"label":"Docusaurus","permalink":"/tags/docusaurus","description":"The Docusaurus static site generator."}],"readingTime":2.955,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"lazy-loading-images-with-docusaurus","title":"Lazy loading images with Docusaurus","authors":"johnnyreilly","tags":["docusaurus"],"image":"./title-image.png","description":"Docusaurus websites can implement native lazy-loading of images, you can by writing a Rehype plugin.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Azure Static Web Apps - a Netlify alternative","permalink":"/azure-static-web-apps-a-netlify-alternative"},"nextItem":{"title":"Azure Container Apps: dapr, devcontainer, debug and deploy","permalink":"/azure-container-apps-dapr-bicep-github-actions-debug-devcontainer"}},"content":"If you\'d like to improve the performance of a Docusaurus website by implementing native lazy-loading of images, you can. This post shows you how you too can have `<img loading=\\"lazy\\" ` on your images by writing a Rehype plugin.\\n\\n![title image reading \\"Lazy loading images with Docusaurus\\" with a Docusaurus logo and an image that reads `<img loading=\\"lazy\\" `](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Updated 26/02/2022\\n\\nYou don\'t need this anymore. As of Docusaurus [v2.0.0-beta.16](https://github.com/facebook/docusaurus/releases/tag/v2.0.0-beta.16) Docusaurus lazy loads markdown images by default. You can see the commit where it was added [here](https://github.com/facebook/docusaurus/pull/6598). Isn\'t that wonderful?\\n\\n\u2705cumulative no of network requests for Docusaurus sites will go \uD83D\uDC47\\n\u2705perceived performance will go \u261D\uFE0F\\n\u2705hosting costs will go \uD83D\uDC47\\n\\n## Lazy loading images\\n\\nNative browser lazy loading for images is a relatively recent innovation. To read more on the topic, [do look at this post](https://web.dev/browser-level-image-lazy-loading/). The TL;DR is this though: by adding the `loading=\\"lazy\\"` attribute to an `img` element, modern browsers will delay loading the image until it is needed. This provides better performance to your users: when it comes to loading, less is more.\\n\\n## Docusaurus\\n\\nIf you\'re using Docusaurus then you\'re likely writing Markdown. I am. This blog is written using Markdown, and converted, using [MDX plugins](https://docusaurus.io/docs/next/markdown-features/plugins) into JSX. This handles images as well as we can [see here](https://github.com/facebook/docusaurus/blob/6ec0db4722cbf988fd5280a4442223637c2de8d7/packages/docusaurus-mdx-loader/src/remark/transformImage/index.ts#L79):\\n\\n```ts\\njsxNode.value = `<img ${alt}src={${src}}${title}${width}${height} />`;\\n```\\n\\nThe crucial thing to note about the above, is the lack of the `loading=\\"lazy\\"` attribute. Can we add that somehow? Yes we can!\\n\\n## Rehype plugin\\n\\nTo do this, we\'re going to write our own mini [rehype plugin](https://github.com/rehypejs) that will take the HTML being pumped out of Docusaurus and add the `loading=\\"lazy\\"` attribute.\\n\\nAlongside our `docusaurus.config.js` we\'re going to create a `image-lazy-remark-plugin.js` file:\\n\\n```js\\nconst visit = require(\'unist-util-visit\');\\n\\n/** @type {import(\'unified\').Plugin<[], import(\'hast\').Root>} */\\nfunction lazyLoadImagesPlugin() {\\n  return (tree) => {\\n    visit(tree, [\'element\', \'jsx\'], (node) => {\\n      if (node.type === \'element\' && node.tagName === \'img\') {\\n        // handles nodes like this:\\n\\n        // {\\n        //   type: \'element\',\\n        //   tagName: \'img\',\\n        //   properties: {\\n        //     src: \'https://some.website.com/cat.gif\',\\n        //     alt: null\\n        //   },\\n        //   ...\\n        // }\\n\\n        node.properties.loading = \'lazy\';\\n      } else if (node.type === \'jsx\' && node.value.includes(\'<img \')) {\\n        // handles nodes like this:\\n\\n        // {\\n        //   type: \'jsx\',\\n        //   value: \'<img src={require(\\"!/workspaces/blog.johnnyreilly.com/blog-website/node_modules/url-loader/dist/cjs.js?limit=10000&name=assets/images/[name]-[hash].[ext]&fallback=/workspaces/blog.johnnyreilly.com/blog-website/node_modules/file-loader/dist/cjs.js!./bower-with-the-long-paths.png\\").default} width=\\"640\\" height=\\"497\\" />\'\\n        // }\\n\\n        node.value = node.value.replace(/<img /g, \'<img loading=\\"lazy\\" \');\\n      }\\n    });\\n  };\\n}\\n\\nmodule.exports = lazyLoadImagesPlugin;\\n```\\n\\nAs the code above suggests, it looks for `img` elements, whether they be in HTML or JSX, and adds in the `loading=\\"lazy\\"` attribute.\\n\\nTo apply this to our blog, we simply tweak the `docusaurus.config.js` file to make use of our plugin:\\n\\n```js\\nconst imageLazyRemarkPlugin = require(\'./image-lazy-remark-plugin\');\\n\\n// ...\\n\\n/** @type {import(\'@docusaurus/types\').Config} */\\nconst config = {\\n  // ...\\n\\n  presets: [\\n    [\\n      \'@docusaurus/preset-classic\',\\n      /** @type {import(\'@docusaurus/preset-classic\').Options} */\\n      ({\\n        // ...\\n        blog: {\\n          // ...\\n          rehypePlugins: [imageLazyRemarkPlugin],\\n        },\\n        // ...\\n      }),\\n    ],\\n  ],\\n  // ...\\n};\\n```\\n\\n## What\'s the result?\\n\\nWith this in place, next time we run a build, we can see the attribute being applied to our image elements:\\n\\n![screenshot of an img element with the loading=\\"lazy\\" attribute set](screenshot-of-img-loading-lazy-element.png)\\n\\nConsequently, when we fire up devtools we can see that only the images onscreen are being loaded. In the example below we\'re _not_ seeing five other images being loaded because they\'re offscreen and haven\'t been scrolled to as yet:\\n\\n![screenshot of chrome devtools showing only two images being loaded - the ones that are on the screen](screenshot-of-chrome-devtools-showing-only-onscreen-images-loaded.png)\\n\\nAmazing! It works! It\'s possible that this could land directly in Docusaurus one day. [Go here to follow the discussion on this.](https://docusaurus.io/feature-requests/p/lazy-loading-images-in-blog-posts-by-default)"},{"id":"azure-container-apps-dapr-bicep-github-actions-debug-devcontainer","metadata":{"permalink":"/azure-container-apps-dapr-bicep-github-actions-debug-devcontainer","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2022-01-22-azure-container-apps-dapr-bicep-github-actions-debug-devcontainer/index.md","source":"@site/blog/2022-01-22-azure-container-apps-dapr-bicep-github-actions-debug-devcontainer/index.md","title":"Azure Container Apps: dapr, devcontainer, debug and deploy","description":"Build and deploy two Azure Container Apps using Bicep and GitHub Actions, communicate using dapr, build, run and debug in VS Code using a devcontainer.","date":"2022-01-22T00:00:00.000Z","tags":[{"inline":false,"label":"Bicep","permalink":"/tags/bicep","description":"The Bicep language for Azure Resource Manager templates."},{"inline":false,"label":"GitHub Actions","permalink":"/tags/github-actions","description":"The GitHub Actions CI / CD service."},{"inline":false,"label":"Azure Container Apps","permalink":"/tags/azure-container-apps","description":"The Azure Container Apps service. Effectively a managed Kubernetes service."}],"readingTime":21.67,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-container-apps-dapr-bicep-github-actions-debug-devcontainer","title":"Azure Container Apps: dapr, devcontainer, debug and deploy","authors":"johnnyreilly","tags":["bicep","github actions","azure container apps"],"image":"./title-image.png","description":"Build and deploy two Azure Container Apps using Bicep and GitHub Actions, communicate using dapr, build, run and debug in VS Code using a devcontainer.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Lazy loading images with Docusaurus","permalink":"/lazy-loading-images-with-docusaurus"},"nextItem":{"title":"Preload fonts with Docusaurus (updated 03/11/2022)","permalink":"/preload-fonts-with-docusaurus"}},"content":"This post shows how to build and deploy two Azure Container Apps using Bicep and GitHub Actions. These apps will communicate using [dapr](https://docs.dapr.io/), be built in [VS Code using a devcontainer](https://code.visualstudio.com/docs/remote/containers). It will be possible to debug in VS Code and run with `docker-compose`.\\n\\nThis follows on from the [previous post](../2021-12-27-azure-container-apps-build-and-deploy-with-bicep-and-github-actions/index.md) which built and deployed a simple web application to Azure Container Apps using Bicep and GitHub Actions using the GitHub container registry.\\n\\n![title image reading \\"Azure Container Apps dapr, devcontainer, debug and deploy\\"  with the dapr, Bicep, Azure Container Apps and GitHub Actions logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Updated 02/05/2022\\n\\nThis post has been updated to reflect the migration of Azure Container Apps from the Microsoft.Web namespace to the Microsoft.App namespace in March 2022. See: https://github.com/microsoft/azure-container-apps/issues/109\\n\\n## What we\'re going to build\\n\\nAs an engineer, I\'m productive when:\\n\\n- Integrating different services together is a turnkey experience and\\n- I\'m able to easily debug my code\\n\\nI\'ve found that using dapr and VS Code I\'m able to achieve both of these goals. I can build an application made up of multiple services, compose them together using dapr and deploy them to Azure Container Apps with relative ease.\\n\\nIn this post we\'re going to build an example of that from scratch, with a [koa/node.js](https://koajs.com/) (built with TypeScript) front end that will communicate with a [dotnet](https://dotnet.microsoft.com/en-us/) service via dapr.\\n\\nAll the work done in this post can be found in the [`dapr-devcontainer-debug-and-deploy`](https://github.com/johnnyreilly/dapr-devcontainer-debug-and-deploy/tree/v1.0.0) repo. As a note, if you\'re interested in this topic it\'s also worth looking at the [`Azure-Samples/container-apps-store-api-microservice`](https://github.com/Azure-Samples/container-apps-store-api-microservice) repo.\\n\\n## Setting up our devcontainer\\n\\nThe first thing we\'ll do is set up our devcontainer. We\'re going to use a tweaked version of the [docker-in-docker](https://github.com/microsoft/vscode-dev-containers/tree/main/containers/docker-in-docker) image from the [vscode-dev-containers](https://github.com/microsoft/vscode-dev-containers) repo.\\n\\nIn the root of our project we\'ll create a `.devcontainer` folder, and within that a `library-scripts` folder. There\'s a number of communal scripts from the [`vscode-dev-containers`](https://github.com/microsoft/vscode-dev-containers) repo which we\'re going to lift and shift into in our `library-scripts` folder:\\n\\n- [docker-in-docker-debian.sh](https://github.com/microsoft/vscode-dev-containers/blob/d93de4632781372d4b4da1699e27ae3a2404c96c/script-library/docker-in-docker-debian.sh) - for installing Docker in Docker\\n- [azcli-debian.sh](https://github.com/microsoft/vscode-dev-containers/blob/d93de4632781372d4b4da1699e27ae3a2404c96c/script-library/azcli-debian.sh) - for installing the Azure CLI\\n\\nIn the `.devcontainer` folder we want to create a `Dockerfile`:\\n\\n```docker\\n# [Choice] .NET version: 6.0, 5.0, 3.1, 2.1\\nARG VARIANT=3.1\\nFROM mcr.microsoft.com/vscode/devcontainers/dotnet:0-${VARIANT}\\nRUN su vscode -c \\"umask 0002 && dotnet tool install -g Microsoft.Tye --version \\\\\\"0.10.0-alpha.21420.1\\\\\\" 2>&1\\"\\n\\n# [Choice] Node.js version: none, lts/*, 16, 14, 12, 10\\nARG NODE_VERSION=\\"14\\"\\nRUN if [ \\"${NODE_VERSION}\\" != \\"none\\" ]; then su vscode -c \\"umask 0002 && . /usr/local/share/nvm/nvm.sh && nvm install ${NODE_VERSION} 2>&1\\"; fi\\n\\n# [Option] Install Azure CLI\\nARG INSTALL_AZURE_CLI=\\"false\\"\\nCOPY library-scripts/azcli-debian.sh /tmp/library-scripts/\\nRUN if [ \\"$INSTALL_AZURE_CLI\\" = \\"true\\" ]; then bash /tmp/library-scripts/azcli-debian.sh; fi \\\\\\n    && apt-get clean -y && rm -rf /var/lib/apt/lists/* /tmp/library-scripts \\\\\\n    && az bicep install\\n\\n# [Option] Enable non-root Docker access in container\\nARG ENABLE_NONROOT_DOCKER=\\"true\\"\\n# [Option] Use the OSS Moby CLI instead of the licensed Docker CLI\\nARG USE_MOBY=\\"true\\"\\n# [Option] Engine/CLI Version\\nARG DOCKER_VERSION=\\"latest\\"\\n\\n# Enable new \\"BUILDKIT\\" mode for Docker CLI\\nENV DOCKER_BUILDKIT=1\\n\\nARG USERNAME=vscode\\n\\n# Install needed packages and setup non-root user. Use a separate RUN statement to add your\\n# own dependencies. A user of \\"automatic\\" attempts to reuse an user ID if one already exists.\\nCOPY library-scripts/docker-in-docker-debian.sh /tmp/library-scripts/\\nRUN apt-get update \\\\\\n    && apt-get install python3-pip -y \\\\\\n# Use Docker script from script library to set things up\\n    && /bin/bash /tmp/library-scripts/docker-in-docker-debian.sh \\"${ENABLE_NONROOT_DOCKER}\\" \\"${USERNAME}\\" \\"${USE_MOBY}\\" \\"${DOCKER_VERSION}\\"\\n\\n# Install Dapr\\nRUN wget -q https://raw.githubusercontent.com/dapr/cli/master/install/install.sh -O - | /bin/bash \\\\\\n    # Clean up\\n    && apt-get autoremove -y && apt-get clean -y && rm -rf /var/lib/apt/lists/* /tmp/library-scripts/\\n\\n# Add daprd to the path for the VS Code Dapr extension.\\nENV PATH=\\"${PATH}:/home/${USERNAME}/.dapr/bin\\"\\n\\n# Install Tye\\nENV PATH=/home/${USERNAME}/.dotnet/tools:$PATH\\n\\nVOLUME [ \\"/var/lib/docker\\" ]\\n\\n# Setting the ENTRYPOINT to docker-init.sh will configure non-root access\\n# to the Docker socket. The script will also execute CMD as needed.\\nENTRYPOINT [ \\"/usr/local/share/docker-init.sh\\" ]\\nCMD [ \\"sleep\\", \\"infinity\\" ]\\n\\n# [Optional] Uncomment this section to install additional OS packages.\\n# RUN apt-get update && export DEBIAN_FRONTEND=noninteractive \\\\\\n#     && apt-get -y install --no-install-recommends <your-package-list-here>\\n```\\n\\nThe above is a loose riff on the [docker-in-docker Dockerfile](https://github.com/microsoft/vscode-dev-containers/blob/main/containers/docker-in-docker/.devcontainer/Dockerfile), lovingly mixed with the [Azure-Samples container-apps Dockerfile](https://github.com/Azure-Samples/container-apps-store-api-microservice/blob/main/.devcontainer/Dockerfile).\\n\\nIt installs the following:\\n\\n- Dot Net\\n- Node.js\\n- the Azure CLI\\n- Docker\\n- Bicep\\n- Dapr\\n\\nNow we have our `Dockerfile`, we need a `devcontainer.json` to go with it:\\n\\n```json\\n// For format details, see https://aka.ms/devcontainer.json. For config options, see the README at:\\n// https://github.com/microsoft/vscode-dev-containers/tree/v0.205.0/containers/dapr-dotnet\\n{\\n  \\"name\\": \\"dapr\\",\\n  \\"build\\": {\\n    \\"dockerfile\\": \\"Dockerfile\\",\\n    \\"args\\": {\\n      // Update \'VARIANT\' to pick a .NET Core version: 3.1, 5.0, 6.0\\n      \\"VARIANT\\": \\"6.0\\",\\n      // Options\\n      \\"NODE_VERSION\\": \\"lts/*\\",\\n      \\"INSTALL_AZURE_CLI\\": \\"true\\"\\n    }\\n  },\\n  \\"runArgs\\": [\\"--init\\", \\"--privileged\\"],\\n  \\"mounts\\": [\\"source=dind-var-lib-docker,target=/var/lib/docker,type=volume\\"],\\n  \\"overrideCommand\\": false,\\n\\n  // Use this environment variable if you need to bind mount your local source code into a new container.\\n  \\"remoteEnv\\": {\\n    \\"LOCAL_WORKSPACE_FOLDER\\": \\"${localWorkspaceFolder}\\",\\n    \\"PATH\\": \\"/home/vscode/.dapr/bin/:/home/vscode/.dotnet/tools:$PATH${containerEnv:PATH}\\"\\n  },\\n\\n  // Set *default* container specific settings.json values on container create.\\n  \\"settings\\": {},\\n\\n  // Add the IDs of extensions you want installed when the container is created.\\n  \\"extensions\\": [\\n    \\"ms-azuretools.vscode-dapr\\",\\n    \\"ms-azuretools.vscode-docker\\",\\n    \\"ms-dotnettools.csharp\\",\\n    \\"ms-vscode.azurecli\\",\\n    \\"ms-azuretools.vscode-bicep\\"\\n  ],\\n\\n  // Use \'forwardPorts\' to make a list of ports inside the container available locally.\\n  // \\"forwardPorts\\": [],\\n\\n  // Ensure Dapr is running on opening the container\\n  \\"postCreateCommand\\": \\"dapr uninstall --all && dapr init\\",\\n\\n  // Comment out connect as root instead. More info: https://aka.ms/vscode-remote/containers/non-root.\\n  \\"remoteUser\\": \\"vscode\\",\\n  \\"features\\": {\\n    \\"azure-cli\\": \\"latest\\"\\n  }\\n}\\n```\\n\\nThe above will:\\n\\n- install Node 16 / dotnet 6 and the latest Azure CLI\\n- install a number of VS Code extensions related to dapr / Docker / Bicep / Azure / C#\\n- install dapr when the container starts\\n\\nWe\'re ready! Reopen your repo in a container (it will take a while first time out) and you\'ll be ready to go.\\n\\n## Create a dotnet service\\n\\nNow we\'re going to create a dotnet service. The aim of this post is not to build a specific application, but rather to demonstrate how simple service to service communication is with dapr. So we\'ll use the web api template that ships with dotnet 6. That arrives with a fake weather API included, so we\'ll name our service accordingly:\\n\\n```shell\\ndotnet new webapi -o WeatherService\\n```\\n\\nInside the created `Program.cs`, find the following line and delete it:\\n\\n```cs\\napp.UseHttpsRedirection();\\n```\\n\\nHTTPS is important, however Azure Container Apps are going to tackle that for us.\\n\\n## Create a Node.js service (with Koa)\\n\\nCreating our dotnet service was very simple. We\'re now going to create a web app with Node.js and Koa that calls our dotnet service. This will be a little more complicated - but still surprisingly simple thanks to the great API choices of dapr.\\n\\nLet\'s make that service:\\n\\n```shell\\nmkdir WebService\\ncd WebService\\nnpm init -y\\nnpm install koa axios --save\\nnpm install @types/koa @types/node @types/axios typescript --save-dev\\n```\\n\\nWe\'re installing the following:\\n\\n- [koa](https://koajs.com/) - the web framework we\'re going to use\\n- [axios](https://axios-http.com/) - to make calls to our dotnet service via HTTP / dapr\\n- [TypeScript](https://www.typescriptlang.org/) and associated type definitions, so we can take advantage of static typing. Admittedly since we\'re building a minimal example this is not super beneficial; but TS makes me happy and I\'d certainly want static typing in place if going beyond a simple example. Start as you mean to go on.\\n\\nWe\'ll create a `tsconfig.json`:\\n\\n```json\\n{\\n  \\"compilerOptions\\": {\\n    \\"esModuleInterop\\": true,\\n    \\"module\\": \\"commonjs\\",\\n    \\"target\\": \\"es2017\\",\\n    \\"noImplicitAny\\": true,\\n    \\"outDir\\": \\"./dist\\",\\n    \\"strict\\": true,\\n    \\"sourceMap\\": true\\n  }\\n}\\n```\\n\\nWe\'ll update the `scripts` section of our `package.json` like so:\\n\\n```json\\n  \\"scripts\\": {\\n    \\"build\\": \\"tsc\\",\\n    \\"start\\": \\"node dist/index.js\\"\\n  },\\n```\\n\\nSo we can build and start our web app. Now let\'s write it!\\n\\nWe\'re going to create an `index.ts` file:\\n\\n```ts\\nimport Koa from \'koa\';\\nimport axios from \'axios\';\\n\\n// How we connect to the dotnet service with dapr\\nconst daprSidecarBaseUrl = `http://localhost:${\\n  process.env.DAPR_HTTP_PORT || 3501\\n}`;\\n// app id header for service discovery\\nconst weatherServiceAppIdHeaders = {\\n  \'dapr-app-id\': process.env.WEATHER_SERVICE_NAME || \'dotnet-app\',\\n};\\n\\nconst app = new Koa();\\n\\napp.use(async (ctx) => {\\n  try {\\n    const data = await axios.get<WeatherForecast[]>(\\n      `${daprSidecarBaseUrl}/weatherForecast`,\\n      {\\n        headers: weatherServiceAppIdHeaders,\\n      },\\n    );\\n\\n    ctx.body = `And the weather today will be ${data.data[0].summary}`;\\n  } catch (exc) {\\n    console.error(\'Problem calling weather service\', exc);\\n    ctx.body = \'Something went wrong!\';\\n  }\\n});\\n\\nconst portNumber = 3000;\\napp.listen(portNumber);\\nconsole.log(`listening on port ${portNumber}`);\\n\\ninterface WeatherForecast {\\n  date: string;\\n  temperatureC: number;\\n  temperatureF: number;\\n  summary: string;\\n}\\n```\\n\\nThe above code is fairly simple but is achieving quite a lot. It:\\n\\n- uses various environment variables to construct the URLs / headers which allow connecting to the dapr sidecar running alongside the app, and consequently to the weather service through the dapr sidecar running alongside the weather service. We\'re going to set up the environment variables which this code relies upon later.\\n- spins up a web server with koa on port 3000\\n- that web server, when sent an HTTP request, will call the `weatherForecast` endpoint of the dotnet app. It will grab what comes back, take the first entry in there and surface that up as the weather forecast.\\n- We\'re also defining a `WeatherForecast` interface to represent the type of the data that comes back from the dotnet service\\n\\nIt\'s worth dwelling for a moment on the simplicity that dapr is affording us here. We\'re able to make HTTP requests to our dotnet service just like they were any other service running locally. What\'s actually happening is illustrated by the diagram below:\\n\\n![a diagram showing traffic going from the web service to the weather service and back again via dapr](./dapr-sidecar.drawio.svg)\\n\\nWe\'re making HTTP requests from the web service, which look like they\'re going directly to the weather service. But in actual fact, they\'re being routed through dapr sidecars until they reach their destination. Why is this fantastic? Well there\'s two things we aren\'t having to think about here:\\n\\n- certificates\\n- inter-service authentication\\n\\nBoth of these can be complex and burn a large amount of engineering time. Because we\'re using dapr it\'s not a problem we have to solve. Isn\'t that great?\\n\\n## Debugging dapr in VS Code\\n\\nWe want to be able to debug this code. We can achieve that in VS Code by setting a [`launch.json`](https://code.visualstudio.com/docs/editor/debugging#_launchjson-attributes) and a [`tasks.json`](https://code.visualstudio.com/docs/editor/tasks) file.\\n\\nFirst of all we\'ll create a `launch.json` file in the `.vscode` folder of our repo:\\n\\n```json\\n{\\n  // Use IntelliSense to learn about possible attributes.\\n  // Hover to view descriptions of existing attributes.\\n  // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387\\n  \\"version\\": \\"0.2.0\\",\\n  \\"compounds\\": [\\n    {\\n      \\"name\\": \\"All Container Apps\\",\\n      \\"configurations\\": [\\"WeatherService\\", \\"WebService\\"],\\n      \\"presentation\\": {\\n        \\"hidden\\": false,\\n        \\"group\\": \\"Containers\\",\\n        \\"order\\": 1\\n      }\\n    }\\n  ],\\n  \\"configurations\\": [\\n    {\\n      \\"name\\": \\"WeatherService\\",\\n      \\"type\\": \\"coreclr\\",\\n      \\"request\\": \\"launch\\",\\n      \\"preLaunchTask\\": \\"daprd-debug-dotnet\\",\\n      \\"postDebugTask\\": \\"daprd-down-dotnet\\",\\n      \\"program\\": \\"${workspaceFolder}/WeatherService/bin/Debug/net6.0/WeatherService.dll\\",\\n      \\"args\\": [],\\n      \\"cwd\\": \\"${workspaceFolder}/WeatherService\\",\\n      \\"stopAtEntry\\": false,\\n      \\"env\\": {\\n        \\"DOTNET_ENVIRONMENT\\": \\"Development\\",\\n        \\"DOTNET_URLS\\": \\"http://localhost:5000\\",\\n        \\"DAPR_HTTP_PORT\\": \\"3500\\",\\n        \\"DAPR_GRPC_PORT\\": \\"50000\\",\\n        \\"DAPR_METRICS_PORT\\": \\"9090\\"\\n      }\\n    },\\n\\n    {\\n      \\"name\\": \\"WebService\\",\\n      \\"type\\": \\"node\\",\\n      \\"request\\": \\"launch\\",\\n      \\"preLaunchTask\\": \\"daprd-debug-node\\",\\n      \\"postDebugTask\\": \\"daprd-down-node\\",\\n      \\"program\\": \\"${workspaceFolder}/WebService/index.ts\\",\\n      \\"cwd\\": \\"${workspaceFolder}/WebService\\",\\n      \\"env\\": {\\n        \\"NODE_ENV\\": \\"development\\",\\n        \\"PORT\\": \\"3000\\",\\n        \\"DAPR_HTTP_PORT\\": \\"3501\\",\\n        \\"DAPR_GRPC_PORT\\": \\"50001\\",\\n        \\"DAPR_METRICS_PORT\\": \\"9091\\",\\n        \\"WEATHER_SERVICE_NAME\\": \\"dotnet-app\\"\\n      },\\n      \\"protocol\\": \\"inspector\\",\\n      \\"outFiles\\": [\\"${workspaceFolder}/WebService/dist/**/*.js\\"],\\n      \\"serverReadyAction\\": {\\n        \\"action\\": \\"openExternally\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nThe things to note about this are:\\n\\n- we create a Node.js (\\"WebService\\") and a dotnet (\\"WeatherService\\") configuration. These are referenced by the `All Container Apps` compound. Kicking off that will start both the Node.js and the dotnet apps.\\n- The Node.js app runs a `daprd-debug-node` task prior to launch and a `daprd-down-node` task when debugging completes. Comparable tasks are run by the dotnet container - we\'ll look at these in a moment.\\n- Various environment variables are configured, most of which control the behaviour of dapr. When we\'re debugging locally we\'ll be using some non-typical ports to accomodate multiple dapr sidecars being in play at the same time. Note also the `\\"WEATHER_SERVICE_NAME\\": \\"dotnet-app\\"` - it\'s this that allows the WebService to communicate with the WeatherService - `dotnet-app` is the `appId` used to identify a service with dapr. We\'ll see that as we configure our `tasks.json`.\\n\\nHere\'s the `tasks.json` we must make:\\n\\n```json\\n{\\n  // See https://go.microsoft.com/fwlink/?LinkId=733558\\n  // for the documentation about the tasks.json format\\n  \\"version\\": \\"2.0.0\\",\\n  \\"tasks\\": [\\n    {\\n      \\"label\\": \\"dotnet-build\\",\\n      \\"command\\": \\"dotnet\\",\\n      \\"type\\": \\"process\\",\\n      \\"args\\": [\\n        \\"build\\",\\n        \\"${workspaceFolder}/WeatherService/WeatherService.csproj\\",\\n        \\"/property:GenerateFullPaths=true\\",\\n        \\"/consoleloggerparameters:NoSummary\\"\\n      ],\\n      \\"problemMatcher\\": \\"$msCompile\\"\\n    },\\n    {\\n      \\"label\\": \\"daprd-debug-dotnet\\",\\n      \\"appId\\": \\"dotnet-app\\",\\n      \\"appPort\\": 5000,\\n      \\"httpPort\\": 3500,\\n      \\"grpcPort\\": 50000,\\n      \\"metricsPort\\": 9090,\\n      \\"type\\": \\"daprd\\",\\n      \\"dependsOn\\": [\\"dotnet-build\\"]\\n    },\\n    {\\n      \\"label\\": \\"daprd-down-dotnet\\",\\n      \\"appId\\": \\"dotnet-app\\",\\n      \\"type\\": \\"daprd-down\\"\\n    },\\n\\n    {\\n      \\"label\\": \\"npm-install\\",\\n      \\"type\\": \\"shell\\",\\n      \\"command\\": \\"npm install\\",\\n      \\"options\\": {\\n        \\"cwd\\": \\"${workspaceFolder}/WebService\\"\\n      }\\n    },\\n    {\\n      \\"label\\": \\"webservice-build\\",\\n      \\"type\\": \\"typescript\\",\\n      \\"tsconfig\\": \\"WebService/tsconfig.json\\",\\n      \\"problemMatcher\\": [\\"$tsc\\"],\\n      \\"group\\": {\\n        \\"kind\\": \\"build\\",\\n        \\"isDefault\\": true\\n      },\\n      \\"dependsOn\\": [\\"npm-install\\"]\\n    },\\n    {\\n      \\"label\\": \\"daprd-debug-node\\",\\n      \\"appId\\": \\"node-app\\",\\n      \\"appPort\\": 3000,\\n      \\"httpPort\\": 3501,\\n      \\"grpcPort\\": 50001,\\n      \\"metricsPort\\": 9091,\\n      \\"type\\": \\"daprd\\",\\n      \\"dependsOn\\": [\\"webservice-build\\"]\\n    },\\n    {\\n      \\"label\\": \\"daprd-down-node\\",\\n      \\"appId\\": \\"node-app\\",\\n      \\"type\\": \\"daprd-down\\"\\n    }\\n  ]\\n}\\n```\\n\\nThere\'s two sets of tasks here; one for the WeatherService and one for the WebService. You\'ll see some commonalities here. For each service there\'s a `daprd` task that depends upon the relevant service being built and passes the various ports for the dapr sidecar to run on that runs just before debugging kicks off. To go with that, there\'s a `daprd-down` task for each service that runs when debugging finishes and shuts down dapr.\\n\\nWe\'re now ready to debug our app. Let\'s hit F5.\\n\\n![screenshot of debugging the index.ts file in VS Code](./debugging.png)\\n\\nAnd if we look at our browser:\\n\\n![screenshot of browsing Firefox at http://localhost:3000 and seeing \\"And the weather today will be Freezing\\" in the output](./app-running.png)\\n\\nIt works! We\'re running a Node.js WebService which, when called, is communicating with our dotnet WeatherService and surfacing up the results. Brilliant!\\n\\n## Containerising our services with Docker\\n\\nBefore we can deploy each of our services, they need to be containerised.\\n\\nFirst let\'s add a `Dockerfile` to the `WeatherService` folder:\\n\\n```docker\\nFROM mcr.microsoft.com/dotnet/sdk:6.0 as build\\nWORKDIR /app\\nCOPY . .\\nRUN dotnet restore\\nRUN dotnet publish -o /app/publish\\n\\nFROM mcr.microsoft.com/dotnet/aspnet:6.0 as runtime\\nWORKDIR /app\\nCOPY --from=build /app/publish /app\\n\\nENV DOTNET_ENVIRONMENT=Production\\nENV ASPNETCORE_URLS=\'http://+:5000\'\\nEXPOSE 5000\\nENTRYPOINT [ \\"dotnet\\", \\"/app/WeatherService.dll\\" ]\\n```\\n\\nThen we\'ll add a `Dockerfile` to the `WebService` folder:\\n\\n```docker\\nFROM node:16 AS build\\nWORKDIR /app\\nCOPY package.json ./\\nCOPY package-lock.json ./\\nRUN npm install\\n\\nCOPY . .\\nRUN npm run build\\n\\nFROM node:16 AS runtime\\nWORKDIR /app\\nCOPY --from=build /app/dist /app\\nCOPY --from=build /app/package.json /app\\nCOPY --from=build /app/package-lock.json /app\\nRUN npm install\\n\\nENV NODE_ENV production\\nEXPOSE 3000\\nENTRYPOINT [ \\"node\\", \\"/app/index.js\\" ]\\n```\\n\\nLikely these `Dockerfile`s could be optimised further; but we\'re not focussed on that just now. What we have now are two simple `Dockerfile`s that will give us images we can run. Given that one depends on the other it makes sense to bring them together with a `docker-compose.yml` file which we\'ll place in the root of the repo:\\n\\n```yml\\nversion: \'3.4\'\\n\\nservices:\\n  weatherservice:\\n    image: ${REGISTRY:-weatherservice}:${TAG:-latest}\\n    build:\\n      context: ./WeatherService\\n      dockerfile: Dockerfile\\n    ports:\\n      - \'50000:50000\' # Dapr instances communicate over gRPC so we need to expose the gRPC port\\n    environment:\\n      DOTNET_ENVIRONMENT: \'Development\'\\n      ASPNETCORE_URLS: \'http://+:5000\'\\n      DAPR_HTTP_PORT: 3500\\n      DAPR_GRPC_PORT: 50000\\n      DAPR_METRICS_PORT: 9090\\n\\n  weatherservice-dapr:\\n    image: \'daprio/daprd:latest\'\\n    command:\\n      [\\n        \'./daprd\',\\n        \'-app-id\',\\n        \'dotnet-app\',\\n        \'-app-port\',\\n        \'5000\',\\n        \'-dapr-http-port\',\\n        \'3500\',\\n        \'-placement-host-address\',\\n        \'placement:50006\',\\n      ]\\n    network_mode: \'service:weatherservice\'\\n    depends_on:\\n      - weatherservice\\n\\n  webservice:\\n    image: ${REGISTRY:-webservice}:${TAG:-latest}\\n    ports:\\n      - \'3000:3000\' # The web front end port\\n      - \'50001:50001\' # Dapr instances communicate over gRPC so we need to expose the gRPC port\\n    build:\\n      context: ./WebService\\n      dockerfile: Dockerfile\\n    environment:\\n      NODE_ENV: \'development\'\\n      PORT: \'3000\'\\n      DAPR_HTTP_PORT: 3501\\n      DAPR_GRPC_PORT: 50001\\n      DAPR_METRICS_PORT: 9091\\n      WEATHER_SERVICE_NAME: \'dotnet-app\'\\n\\n  webservice-dapr:\\n    image: \'daprio/daprd:latest\'\\n    command: [\\n        \'./daprd\',\\n        \'-app-id\',\\n        \'node-app\',\\n        \'-app-port\',\\n        \'3000\',\\n        \'-dapr-http-port\',\\n        \'3501\',\\n        \'-placement-host-address\',\\n        \'placement:50006\', # Dapr\'s placement service can be reach via the docker DNS entry\\n      ]\\n    network_mode: \'service:webservice\'\\n    depends_on:\\n      - webservice\\n\\n  dapr-placement:\\n    image: \'daprio/dapr:latest\'\\n    command: [\'./placement\', \'-port\', \'50006\']\\n    ports:\\n      - \'50006:50006\'\\n```\\n\\nWith this in place we can run `docker-compose up` and bring up our application locally.\\n\\nAnd now we have docker images built, we can look at deploying them.\\n\\n## Deploying to Azure\\n\\nAt this point we have pretty much everything we need in terms of application code and the ability to build and debug it. Now we\'d like to deploy it to Azure.\\n\\nLet\'s begin with the Bicep required to deploy our Azure Container Apps.\\n\\nIn our repository we\'ll create an `infra` directory, into which we\'ll place a `main.bicep` file which will contain our Bicep template:\\n\\n```bicep\\nparam branchName string\\n\\nparam webServiceImage string\\nparam webServicePort int\\nparam webServiceIsExternalIngress bool\\n\\nparam weatherServiceImage string\\nparam weatherServicePort int\\nparam weatherServiceIsExternalIngress bool\\n\\nparam containerRegistry string\\nparam containerRegistryUsername string\\n@secure()\\nparam containerRegistryPassword string\\n\\nparam tags object\\n\\nparam location string = resourceGroup().location\\n\\nvar minReplicas = 0\\nvar maxReplicas = 1\\n\\nvar branch = toLower(last(split(branchName, \'/\')))\\n\\nvar environmentName = \'shared-env\'\\nvar workspaceName = \'${branch}-log-analytics\'\\nvar appInsightsName = \'${branch}-app-insights\'\\nvar webServiceContainerAppName = \'${branch}-web\'\\nvar weatherServiceContainerAppName = \'${branch}-weather\'\\n\\nvar containerRegistryPasswordRef = \'container-registry-password\'\\n\\nresource workspace \'Microsoft.OperationalInsights/workspaces@2021-12-01-preview\' = {\\n  name: workspaceName\\n  location: location\\n  tags: tags\\n  properties: {\\n    sku: {\\n      name: \'PerGB2018\'\\n    }\\n    retentionInDays: 30\\n    workspaceCapping: {}\\n  }\\n}\\n\\nresource appInsights \'Microsoft.Insights/components@2020-02-02\' = {\\n  name: appInsightsName\\n  location: location\\n  tags: tags\\n  kind: \'web\'\\n  properties: {\\n    Application_Type: \'web\'\\n    Flow_Type: \'Bluefield\'\\n  }\\n}\\n\\nresource environment \'Microsoft.App/managedEnvironments@2022-01-01-preview\' = {\\n  name: environmentName\\n  location: location\\n  tags: tags\\n  properties: {\\n    daprAIInstrumentationKey: appInsights.properties.InstrumentationKey\\n    appLogsConfiguration: {\\n      destination: \'log-analytics\'\\n      logAnalyticsConfiguration: {\\n        customerId: workspace.properties.customerId\\n        sharedKey: listKeys(workspace.id, workspace.apiVersion).primarySharedKey\\n      }\\n    }\\n  }\\n}\\n\\nresource weatherServiceContainerApp \'Microsoft.App/containerApps@2022-01-01-preview\' = {\\n  name: weatherServiceContainerAppName\\n  kind: \'containerapps\'\\n  tags: tags\\n  location: location\\n  properties: {\\n    managedEnvironmentId: environment.id\\n    configuration: {\\n      dapr: {\\n        enabled: true\\n        appPort: weatherServicePort\\n        appId: weatherServiceContainerAppName\\n      }\\n      secrets: [\\n        {\\n          name: containerRegistryPasswordRef\\n          value: containerRegistryPassword\\n        }\\n      ]\\n      registries: [\\n        {\\n          server: containerRegistry\\n          username: containerRegistryUsername\\n          passwordSecretRef: containerRegistryPasswordRef\\n        }\\n      ]\\n      ingress: {\\n        external: weatherServiceIsExternalIngress\\n        targetPort: weatherServicePort\\n      }\\n    }\\n    template: {\\n      containers: [\\n        {\\n          image: weatherServiceImage\\n          name: weatherServiceContainerAppName\\n          transport: \'auto\'\\n        }\\n      ]\\n      scale: {\\n        minReplicas: minReplicas\\n        maxReplicas: maxReplicas\\n      }\\n    }\\n  }\\n}\\n\\nresource webServiceContainerApp \'Microsoft.App/containerApps@2022-01-01-preview\' = {\\n  name: webServiceContainerAppName\\n  kind: \'containerapps\'\\n  tags: tags\\n  location: location\\n  properties: {\\n    managedEnvironmentId: environment.id\\n    configuration: {\\n      dapr: {\\n        enabled: true\\n        appPort: webServicePort\\n        appId: webServiceContainerAppName\\n      }\\n      secrets: [\\n        {\\n          name: containerRegistryPasswordRef\\n          value: containerRegistryPassword\\n        }\\n      ]\\n      registries: [\\n        {\\n          server: containerRegistry\\n          username: containerRegistryUsername\\n          passwordSecretRef: containerRegistryPasswordRef\\n        }\\n      ]\\n      ingress: {\\n        external: webServiceIsExternalIngress\\n        targetPort: webServicePort\\n      }\\n    }\\n    template: {\\n      containers: [\\n        {\\n          image: webServiceImage\\n          name: webServiceContainerAppName\\n          transport: \'auto\'\\n          env: [\\n            {\\n              name: \'WEATHER_SERVICE_NAME\'\\n              value: weatherServiceContainerAppName\\n            }\\n          ]\\n        }\\n      ]\\n      scale: {\\n        minReplicas: minReplicas\\n        maxReplicas: maxReplicas\\n      }\\n    }\\n  }\\n}\\n\\noutput webServiceUrl string = webServiceContainerApp.properties.latestRevisionFqdn\\n```\\n\\nThis will deploy two container apps - one for our `WebService` and one for our `WeatherService`. Alongside that we\'ve resources for logging and environments.\\n\\n## Setting up a resource group\\n\\nWith our Bicep in place, we\'re going to need a resource group to send it to. Right now, Azure Container Apps aren\'t available everywhere. So we\'re going to create ourselves a resource group in North Europe which does support ACAs:\\n\\n```shell\\naz group create -g rg-aca -l northeurope\\n```\\n\\n## Secrets for GitHub Actions\\n\\nWe\'re aiming to set up a GitHub Action to handle our deployment. This will depend upon a number of secrets:\\n\\n![Screenshot of the secrets in the GitHub website that we need to create](screenshot-github-secrets.png)\\n\\nWe\'ll need to create each of these secrets.\\n\\n### `AZURE_CREDENTIALS` - GitHub logging into Azure\\n\\nSo GitHub Actions can interact with Azure on our behalf, we need to provide it with some credentials. We\'ll use the Azure CLI to create these:\\n\\n```shell\\naz ad sp create-for-rbac --name \\"myApp\\" --role contributor \\\\\\n    --scopes /subscriptions/{subscription-id}/resourceGroups/{resource-group} \\\\\\n    --sdk-auth\\n```\\n\\nRemember to replace the `{subscription-id}` with your subscription id and `{resource-group}` with the name of your resource group (`rg-aca` if you\'re following along). This command will pump out a lump of JSON that looks something like this:\\n\\n```json\\n{\\n  \\"clientId\\": \\"a-client-id\\",\\n  \\"clientSecret\\": \\"a-client-secret\\",\\n  \\"subscriptionId\\": \\"a-subscription-id\\",\\n  \\"tenantId\\": \\"a-tenant-id\\",\\n  \\"activeDirectoryEndpointUrl\\": \\"https://login.microsoftonline.com\\",\\n  \\"resourceManagerEndpointUrl\\": \\"https://management.azure.com/\\",\\n  \\"activeDirectoryGraphResourceId\\": \\"https://graph.windows.net/\\",\\n  \\"sqlManagementEndpointUrl\\": \\"https://management.core.windows.net:8443/\\",\\n  \\"galleryEndpointUrl\\": \\"https://gallery.azure.com/\\",\\n  \\"managementEndpointUrl\\": \\"https://management.core.windows.net/\\"\\n}\\n```\\n\\nTake this and save it as the `AZURE_CREDENTIALS` secret in Azure.\\n\\n### `PACKAGES_TOKEN` - Azure accessing the GitHub container registry\\n\\nWe also need a secret for accessing packages from Azure. We\'re going to be publishing packages to the GitHub container registry. Azure is going to need to be able to access this when we\'re deploying. ACA deployment works by telling Azure where to look for an image and providing any necessary credentials to do the acquisition. To facilitate this we\'ll set up a `PACKAGES_TOKEN` secret. This is a GitHub personal access token with the `read:packages` scope. [Follow the instructions here to create the token.](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token)\\n\\n## Deploying with GitHub Actions\\n\\nWith our secrets configured, we\'re now well placed to write our GitHub Action. We\'ll create a `.github/workflows/build-and-deploy.yaml` file in our repository and populate it thusly:\\n\\n```yaml\\n# yaml-language-server: $schema=./build.yaml\\nname: Build and Deploy\\non:\\n  # Trigger the workflow on push or pull request,\\n  # but only for the main branch\\n  push:\\n    branches:\\n      - main\\n  pull_request:\\n    branches:\\n      - main\\n    # Publish semver tags as releases.\\n    tags: [\'v*.*.*\']\\n  workflow_dispatch:\\n\\nenv:\\n  RESOURCE_GROUP: rg-aca\\n  REGISTRY: ghcr.io\\n  IMAGE_NAME: ${{ github.repository }}\\n\\njobs:\\n  build:\\n    runs-on: ubuntu-latest\\n    strategy:\\n      matrix:\\n        services:\\n          [\\n            { \'imageName\': \'node-service\', \'directory\': \'./WebService\' },\\n            { \'imageName\': \'dotnet-service\', \'directory\': \'./WeatherService\' },\\n          ]\\n    permissions:\\n      contents: read\\n      packages: write\\n    outputs:\\n      image-node: ${{ steps.image-tag.outputs.image-node-service }}\\n      image-dotnet: ${{ steps.image-tag.outputs.image-dotnet-service }}\\n    steps:\\n      - name: Checkout repository\\n        uses: actions/checkout@v2\\n\\n      # Login against a Docker registry except on PR\\n      # https://github.com/docker/login-action\\n      - name: Log into registry ${{ env.REGISTRY }}\\n        if: github.event_name != \'pull_request\'\\n        uses: docker/login-action@v1\\n        with:\\n          registry: ${{ env.REGISTRY }}\\n          username: ${{ github.actor }}\\n          password: ${{ secrets.GITHUB_TOKEN }}\\n\\n      # Extract metadata (tags, labels) for Docker\\n      # https://github.com/docker/metadata-action\\n      - name: Extract Docker metadata\\n        id: meta\\n        uses: docker/metadata-action@v3\\n        with:\\n          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/${{ matrix.services.imageName }}\\n          tags: |\\n            type=semver,pattern={{version}}\\n            type=semver,pattern={{major}}.{{minor}}\\n            type=semver,pattern={{major}}\\n            type=ref,event=branch\\n            type=sha\\n\\n      # Build and push Docker image with Buildx (don\'t push on PR)\\n      # https://github.com/docker/build-push-action\\n      - name: Build and push Docker image\\n        uses: docker/build-push-action@v2\\n        with:\\n          context: ${{ matrix.services.directory }}\\n          push: ${{ github.event_name != \'pull_request\' }}\\n          tags: ${{ steps.meta.outputs.tags }}\\n          labels: ${{ steps.meta.outputs.labels }}\\n\\n      - name: Output image tag\\n        id: image-tag\\n        run: |\\n          name=$(echo \\"image-${{ matrix.services.imageName }}\\" | tr \'[:upper:]\' \'[:lower:]\')\\n          value=$(echo \\"${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/${{ matrix.services.imageName }}:sha-$(git rev-parse --short HEAD)\\" | tr \'[:upper:]\' \'[:lower:]\')\\n          echo \\"setting output: $name=$value\\"\\n          echo \\"$name=$value\\" >> $GITHUB_OUTPUT\\n\\n  deploy:\\n    runs-on: ubuntu-latest\\n    needs: [build]\\n    steps:\\n      - name: Checkout repository\\n        uses: actions/checkout@v2\\n\\n      - name: Azure Login\\n        uses: azure/login@v1\\n        with:\\n          creds: ${{ secrets.AZURE_CREDENTIALS }}\\n\\n      - name: Deploy bicep\\n        uses: azure/CLI@v2\\n        if: github.event_name != \'pull_request\'\\n        with:\\n          inlineScript: |\\n            REF_SHA=\'${{ github.ref }}.${{ github.sha }}\'\\n            DEPLOYMENT_NAME=\\"${REF_SHA////-}\\"\\n            echo \\"DEPLOYMENT_NAME=$DEPLOYMENT_NAME\\"\\n\\n            TAGS=\'{\\"owner\\":\\"johnnyreilly\\", \\"email\\":\\"johnny_reilly@hotmail.com\\"}\'\\n            az deployment group create \\\\\\n              --resource-group ${{ env.RESOURCE_GROUP }} \\\\\\n              --name \\"$DEPLOYMENT_NAME\\" \\\\\\n              --template-file ./infra/main.bicep \\\\\\n              --parameters \\\\\\n                  branchName=\'${{ github.event.number == 0 && \'main\' ||  format(\'pr-{0}\', github.event.number) }}\' \\\\\\n                  webServiceImage=\'${{ needs.build.outputs.image-node }}\' \\\\\\n                  webServicePort=3000 \\\\\\n                  webServiceIsExternalIngress=true \\\\\\n                  weatherServiceImage=\'${{ needs.build.outputs.image-dotnet }}\' \\\\\\n                  weatherServicePort=5000 \\\\\\n                  weatherServiceIsExternalIngress=false \\\\\\n                  containerRegistry=${{ env.REGISTRY }} \\\\\\n                  containerRegistryUsername=${{ github.actor }} \\\\\\n                  containerRegistryPassword=${{ secrets.PACKAGES_TOKEN }} \\\\\\n                  tags=\\"$TAGS\\"\\n```\\n\\nThere\'s a lot in this workflow. Let\'s dig into the `build` and `deploy` jobs to see what\'s happening.\\n\\n### `build` - building our image\\n\\nThe `build` job is all about building our container images and pushing then to the GitHub registry. It\'s heavily inspired by [Jeff Hollan](https://twitter.com/jeffhollan)\'s [Azure sample app GHA](https://github.com/Azure-Samples/container-apps-store-api-microservice). When we look at the `strategy` we can see a `matrix` of `services` consisting of two services; our node app and our dotnet app:\\n\\n```yaml\\nstrategy:\\n  matrix:\\n    services:\\n      [\\n        { \'imageName\': \'node-service\', \'directory\': \'./WebService\' },\\n        { \'imageName\': \'dotnet-service\', \'directory\': \'./WeatherService\' },\\n      ]\\n```\\n\\nThis is a matrix because a typical use case of an Azure Container Apps will be multi-container - just as this is. The `outputs` pumps out the details of our `image-node` and `image-dotnet` images to be used later:\\n\\n```yaml\\noutputs:\\n  image-node: ${{ steps.image-tag.outputs.image-node-service }}\\n  image-dotnet: ${{ steps.image-tag.outputs.image-dotnet-service }}\\n```\\n\\nWith that understanding in place, let\'s examine what each of the steps in the `build` job does\\n\\n- `Log into registry` - logs into the GitHub container registry\\n- `Extract Docker metadata` - acquire tags which will be used for versioning\\n- `Build and push Docker image` - build the docker image and if this is not a PR: tag, label and push it to the registry\\n- `Output image tag` - write out the image tag for usage in deployment\\n\\n### `deploy` - shipping our image to Azure\\n\\nThe `deploy` job runs the [`az deployment group create`](https://docs.microsoft.com/en-us/cli/azure/deployment/group?view=azure-cli-latest#az_deployment_group_create) command which performs a deployment of our `main.bicep` file.\\n\\n```yaml\\n- name: Deploy bicep\\n  uses: azure/CLI@v2\\n  if: github.event_name != \'pull_request\'\\n  with:\\n    inlineScript: |\\n      REF_SHA=\'${{ github.ref }}.${{ github.sha }}\'\\n      DEPLOYMENT_NAME=\\"${REF_SHA////-}\\"\\n      echo \\"DEPLOYMENT_NAME=$DEPLOYMENT_NAME\\"\\n\\n      TAGS=\'{\\"owner\\":\\"johnnyreilly\\", \\"email\\":\\"johnny_reilly@hotmail.com\\"}\'\\n      az deployment group create \\\\\\n        --resource-group ${{ env.RESOURCE_GROUP }} \\\\\\n        --name \\"$DEPLOYMENT_NAME\\" \\\\\\n        --template-file ./infra/main.bicep \\\\\\n        --parameters \\\\\\n            branchName=\'${{ github.event.number == 0 && \'main\' ||  format(\'pr-{0}\', github.event.number) }}\' \\\\\\n            webServiceImage=\'${{ needs.build.outputs.image-node }}\' \\\\\\n            webServicePort=3000 \\\\\\n            webServiceIsExternalIngress=true \\\\\\n            weatherServiceImage=\'${{ needs.build.outputs.image-dotnet }}\' \\\\\\n            weatherServicePort=5000 \\\\\\n            weatherServiceIsExternalIngress=false \\\\\\n            containerRegistry=${{ env.REGISTRY }} \\\\\\n            containerRegistryUsername=${{ github.actor }} \\\\\\n            containerRegistryPassword=${{ secrets.PACKAGES_TOKEN }} \\\\\\n            tags=\\"$TAGS\\"\\n```\\n\\nIn either case we pass the same set of parameters:\\n\\n```shell\\nbranchName=\'${{ github.event.number == 0 && \'main\' ||  format(\'pr-{0}\', github.event.number) }}\' \\\\\\nwebServiceImage=\'${{ needs.build.outputs.image-node }}\' \\\\\\nwebServicePort=3000 \\\\\\nwebServiceIsExternalIngress=true \\\\\\nweatherServiceImage=\'${{ needs.build.outputs.image-dotnet }}\' \\\\\\nweatherServicePort=5000 \\\\\\nweatherServiceIsExternalIngress=true \\\\\\ncontainerRegistry=${{ env.REGISTRY }} \\\\\\ncontainerRegistryUsername=${{ github.actor }} \\\\\\ncontainerRegistryPassword=${{ secrets.PACKAGES_TOKEN }} \\\\\\ntags=\\"$tags\\"\\n```\\n\\nThese are either:\\n\\n- secrets we set up earlier\\n- [special github variables](https://docs.github.com/en/actions/learn-github-actions/contexts)\\n- environment variables declared at the start of the script or\\n- outputs from the build step - this is where we acquire our node and dotnet images\\n\\n## Running it\\n\\nWhen the GitHub Action has been run you\'ll find that Azure Container Apps are now showing up inside the Azure Portal in your resource group, alongside the other resources:\\n\\n![screenshot of the Azure Container App\'s resource group in the Azure Portal](screenshot-azure-portal-resource-group.png)\\n\\nIf we take a look at our web ACA we\'ll see\\n\\n![screenshot of the web Azure Container App\'s in the Azure Portal](screenshot-azure-portal-container-app.png)\\n\\nAnd when we take a closer look at the container app, we find a URL we can navigate to:\\n\\n![screenshot of the Azure Container App in the Azure Portal revealing it\'s URL](screenshot-working-app.png)\\n\\nCongratulations! You\'ve built and deployed a simple web app to Azure Container Apps with Bicep and GitHub Actions and secrets.\\n\\n## `The subscription \'***\' cannot have more than 2 environments.`\\n\\nBefore signing off, it\'s probably worth sharing this gotcha. If you\'ve been playing with Azure Container Apps you may have already deployed an \\"environment\\" (`Microsoft.Web/kubeEnvironments`). It\'s fairly common to have a limit of one environment per subscription, which is what this message is saying. So either delete other environments, share the one you have or arrange to raise the limit on your subscription."},{"id":"preload-fonts-with-docusaurus","metadata":{"permalink":"/preload-fonts-with-docusaurus","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-12-29-preload-fonts-with-docusaurus/index.md","source":"@site/blog/2021-12-29-preload-fonts-with-docusaurus/index.md","title":"Preload fonts with Docusaurus (updated 03/11/2022)","description":"Improve website performance by preloading web fonts in Docusaurus using `webpack-font-preload-plugin` or `headTags` API, as described in this tutorial.","date":"2021-12-29T00:00:00.000Z","tags":[{"inline":false,"label":"Docusaurus","permalink":"/tags/docusaurus","description":"The Docusaurus static site generator."},{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."}],"readingTime":4.305,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"preload-fonts-with-docusaurus","title":"Preload fonts with Docusaurus (updated 03/11/2022)","authors":"johnnyreilly","tags":["docusaurus","webpack"],"image":"./title-image.png","hide_table_of_contents":false,"description":"Improve website performance by preloading web fonts in Docusaurus using `webpack-font-preload-plugin` or `headTags` API, as described in this tutorial."},"unlisted":false,"prevItem":{"title":"Azure Container Apps: dapr, devcontainer, debug and deploy","permalink":"/azure-container-apps-dapr-bicep-github-actions-debug-devcontainer"},"nextItem":{"title":"Query deployment outputs with the Azure CLI","permalink":"/azure-cli-show-query-output-properties"}},"content":"When we\'re using custom fonts in our websites, it\'s good practice to preload the fonts to minimise the [flash of unstyled text](https://css-tricks.com/fout-foit-foft/). This post shows how to achieve this with Docusaurus.\\n\\n\x3c!--truncate--\x3e\\n\\nIt does so by building a Docusaurus plugin which makes use of [Satyendra Singh](https://github.com/sn-satyendra)\'s excellent [`webpack-font-preload-plugin`](https://github.com/sn-satyendra/webpack-font-preload-plugin).\\n\\n**Updated 03/11/2022:** Subsequently this post demonstrates how to achieve font preloading directly, by using the the `headTags` API.\\n\\n![title image reading \\"Preload fonts with Docusaurus\\" in a ridiculous font with the Docusaurus logo and a screenshot of a preload link HTML element](title-image.png)\\n\\n## Preload web fonts with Docusaurus\\n\\nTo quote the docs of the `webpack-font-preload-plugin`:\\n\\n> The [preload](https://developer.mozilla.org/en-US/docs/Web/HTML/Preloading_content) value of the `<link>` element\'s `rel` attribute lets you declare fetch requests in the HTML\'s `<head>`, specifying resources that your page will need very soon, which you want to start loading early in the page lifecycle, before browsers\' main rendering machinery kicks in. This ensures they are available earlier and are less likely to block the page\'s render, improving performance.\\n>\\n> This plugin specifically targets fonts used with the application which are bundled using webpack. The plugin would add `<link>` tags in the begining of `<head>` of your html:\\n>\\n> ```html\\n> <link rel=\\"preload\\" href=\\"/font1.woff\\" as=\\"font\\" crossorigin />\\n> <link rel=\\"preload\\" href=\\"/font2.woff\\" as=\\"font\\" crossorigin />\\n> ```\\n\\nIf you want to learn more about preloading web fonts, it\'s also worth [reading this excellent article](https://web.dev/codelab-preload-web-fonts/).\\n\\nThe blog you\'re reading is built with [Docusaurus](https://docusaurus.io/). Our mission: for the HTML our Docusaurus build pumps out to feature preload `link` elements. Something like this:\\n\\n```html\\n<link\\n  rel=\\"preload\\"\\n  href=\\"/assets/fonts/Poppins-Regular-8081832fc5cfbf634aa664a9eff0350e.ttf\\"\\n  as=\\"font\\"\\n  crossorigin=\\"\\"\\n/>\\n```\\n\\nThis `link` element has the `rel=\\"preload\\"` attribute set, which triggers font preloading.\\n\\nBut the thing to take from the above text is that the filename features a hash in the name. This demonstrates that the font is being pumped through the Docusaurus build, which is powered by webpack. So we need some webpack whispering to get font preloading going.\\n\\n## Making a plugin\\n\\nWe\'re going to make a minimal [Docusaurus plugin](https://docusaurus.io/docs/using-plugins#creating-plugins) using `webpack-font-preload-plugin`. Let\'s add it to our project:\\n\\n```shell\\nyarn add webpack-font-preload-plugin\\n```\\n\\nNow in the `docusaurus.config.js` we can create our minimal plugin:\\n\\n```js\\nconst FontPreloadPlugin = require(\'webpack-font-preload-plugin\');\\n\\n//...\\n/** @type {import(\'@docusaurus/types\').Config} */\\nconst config = {\\n  //...\\n  plugins: [\\n    function preloadFontPlugin(_context, _options) {\\n      return {\\n        name: \'preload-font-plugin\',\\n        configureWebpack(_config, _isServer) {\\n          return {\\n            plugins: [new FontPreloadPlugin()],\\n          };\\n        },\\n      };\\n    },\\n    // ...\\n  ],\\n  //...\\n};\\n```\\n\\nIt\'s a super simple plugin, it does nothing more than `new` up an instance of the webpack plugin, inside the context of the `configureWebpack` method. That\'s all that\'s required.\\n\\nWith this in place we\'re now seeing the `<link rel=\\"preload\\" ... />` elements being included in the HTML pumped out of our Docusaurus build. This means we have font preloading working:\\n\\n![screenshot of the Chrome devtools featuring link rel=\\"preload\\" elements](screenshot-preload-devtools.png)\\n\\nHuzzah!\\n\\n## Using the `headTags` API\\n\\nIf you\'re using [Docusaurus 2.2 or greater](https://docusaurus.io/blog/releases/2.2#config-headtags) you can use the new [`headTags` API](https://docusaurus.io/docs/api/docusaurus-config#headTags) and bypass using an extra dependency entirely.\\n\\nTo make this work, we need to ensure that our fonts live in the `static` directory which is reliably addressable - not hashed by webpack. We can then use the `headTags` API to add the `link` elements to the `head` of the HTML:\\n\\n```js\\nmodule.exports = {\\n  // ...\\n  headTags: [\\n    // the entry below will make this tag: <link rel=\\"preload\\" href=\\"/fonts/Poppins-Regular.ttf\\" as=\\"font\\" type=\\"font/ttf\\" crossorigin=\\"anonymous\\">\\n    {\\n      tagName: \'link\',\\n      attributes: {\\n        rel: \'preload\',\\n        href: \'/fonts/Poppins-Regular.ttf\',\\n        as: \'font\',\\n        type: \'font/ttf\',\\n        crossorigin: \'anonymous\',\\n      },\\n    },\\n    // the entry below will make this tag: <link rel=\\"preload\\" href=\\"/fonts/Poppins-Bold.ttf\\" as=\\"font\\" type=\\"font/ttf\\" crossorigin=\\"anonymous\\">\\n    {\\n      tagName: \'link\',\\n      attributes: {\\n        rel: \'preload\',\\n        href: \'/fonts/Poppins-Bold.ttf\',\\n        as: \'font\',\\n        type: \'font/ttf\',\\n        crossorigin: \'anonymous\',\\n      },\\n    },\\n  ],\\n  // ...\\n};\\n```\\n\\nIn our `custom.css` we need to add the following:\\n\\n```css\\n@font-face {\\n  font-family: \'Poppins\';\\n  src: url(\'https://johnnyreilly.com/fonts/Poppins-Regular.ttf\');\\n  font-weight: 400;\\n  font-style: normal;\\n  font-display: swap;\\n}\\n\\n@font-face {\\n  font-family: \'Poppins\';\\n  src: url(\'https://johnnyreilly.com/fonts/Poppins-Bold.ttf\');\\n  font-weight: 700;\\n  font-style: normal;\\n  font-display: swap;\\n}\\n```\\n\\nNote that the urls are fully qualified to prevent webpack from trying to bundle them. Another bonus of using the `static` folder is that we can apply long term caching. I\'m using [Azure Static Web Apps](https://azure.microsoft.com/en-us/products/app-service/static/) to run my site and so I\'m achieving this with the following in `staticwebapp.config.json`:\\n\\n```json\\n{\\n  \\"trailingSlash\\": \\"auto\\",\\n  \\"routes\\": [\\n    {\\n      \\"route\\": \\"/img/*\\",\\n      \\"headers\\": {\\n        \\"cache-control\\": \\"must-revalidate, max-age=15770000\\"\\n      }\\n    },\\n    {\\n      \\"route\\": \\"/fonts/*\\",\\n      \\"headers\\": {\\n        \\"cache-control\\": \\"must-revalidate, max-age=15770000\\"\\n      }\\n    }\\n  ],\\n  \\"globalHeaders\\": {\\n    \\"content-security-policy\\": \\"default-src https: \'unsafe-eval\' \'unsafe-inline\'; object-src \'none\'; script-src \'self\' https://www.googleanalytics.com https://www.google-analytics.com https://www.googleoptimize.com https://www.googletagmanager.com \'unsafe-inline\'; img-src \'self\' data: https: https://johnnyreilly.com https://thankful-sky-0bfc7e803-320.westeurope.1.azurestaticapps.net https://www.google-analytics.com https://www.googletagmanager.com\\",\\n    \\"X-Clacks-Overhead\\": \\"GNU Terry Pratchett\\",\\n    \\"Access-Control-Allow-Origin\\": \\"*\\"\\n  }\\n}\\n```\\n\\nThings to note from the above:\\n\\n- `Access-Control-Allow-Origin` and `Vary` are in place to allow my staging sites to access the fonts from the production site. Without this, the fonts won\'t load in the staging site.\\n- The `img` and `fonts` directories sit under the `static` directory. For those directories we\'re going to use `cache-control` set to 6 months for the fonts and static images. They rarely change and so this is an appropriate strategy.\\n\\nThis blog post was migrated to the `headTags` API approach with the release of Docusaurus 2.2.0. [You can see the PR here](https://github.com/johnnyreilly/blog.johnnyreilly.com/pull/321)."},{"id":"azure-cli-show-query-output-properties","metadata":{"permalink":"/azure-cli-show-query-output-properties","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-12-28-azure-cli-show-query-output-properties/index.md","source":"@site/blog/2021-12-28-azure-cli-show-query-output-properties/index.md","title":"Query deployment outputs with the Azure CLI","description":"Discover how to query Azure deployment outputs using the Azure CLI, bash, and jq, and convert them to GitHub Action job outputs.","date":"2021-12-28T00:00:00.000Z","tags":[{"inline":false,"label":"GitHub Actions","permalink":"/tags/github-actions","description":"The GitHub Actions CI / CD service."},{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."}],"readingTime":2.58,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-cli-show-query-output-properties","title":"Query deployment outputs with the Azure CLI","authors":"johnnyreilly","tags":["github actions","azure"],"image":"./title-image.png","hide_table_of_contents":false,"description":"Discover how to query Azure deployment outputs using the Azure CLI, bash, and jq, and convert them to GitHub Action job outputs."},"unlisted":false,"prevItem":{"title":"Preload fonts with Docusaurus (updated 03/11/2022)","permalink":"/preload-fonts-with-docusaurus"},"nextItem":{"title":"Azure Container Apps: build and deploy with Bicep and GitHub Actions","permalink":"/azure-container-apps-build-and-deploy-with-bicep-and-github-actions"}},"content":"It\'s often desirable to query the outputs of deployments to Azure. This post demonstrates how to do this using the Azure CLI, bash and jq. It also shows how to generically convert deployment outputs to GitHub Action job outputs.\\n\\n![title image reading \\"Query deployment outputs with the Azure CLI\\" with the Azure logo and the Azure Cloud Shell in the background](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Deployment outputs\\n\\nWhen we deploy something to Azure, we frequently have outputs which we want to use. Let\'s consider the canonical case, whereby a website is created and we want to use the URL of where it has been deployed. We can see these values in the Azure Portal:\\n\\n![a screenshot of the Azure portal demostrating deployment outputs, there is a single output of \\"nodeUrl\\"](./screenshot-azure-portal-deployment-outputs.png)\\n\\nThe above deployment has a single output of `nodeUrl`. Rather than logging into the portal to acquire this value, how can we do so using the Azure CLI and bash?\\n\\n## Acquire all outputs\\n\\nThe way to acquire outputs from the Azure CLI is using the [`az group deployment show`](https://docs.microsoft.com/en-us/cli/azure/group/deployment?view=azure-cli-latest#az_group_deployment_show) command:\\n\\n```bash\\naz deployment group show \\\\\\n  -g <resource-group-name> \\\\\\n  -n <deployment-name> \\\\\\n  --query properties.outputs\\n```\\n\\nRunning the above will produce a piece of JSON that contains all our outputs. In our case, we have a single deployment output: `nodeUrl`. So our JSON looks like this:\\n\\n```json\\n{\\n  \\"nodeUrl\\": {\\n    \\"type\\": \\"String\\",\\n    \\"value\\": \\"some.url.northeurope.azurecontainerapps.io\\"\\n  }\\n}\\n```\\n\\n## Acquire an individual output\\n\\nTo acquire an individual output, you can provide a targeted `--query` to pull out the value you care about. However, there\'s a slight issue:\\n\\n```bash\\njohn@Azure:~$ NODE_URL=$(az deployment group show -g rg-aca -n our-deployment --query properties.outputs.nodeUrl.value)\\njohn@Azure:~$ echo $NODE_URL\\n\\"some.url.northeurope.azurecontainerapps.io\\"\\n```\\n\\nThe value we capture in the `NODE_URL` variable above is surrounded by quotes. These will probably get in the way when we\'re scripting something with this. Rather than purging them with bash, I tend to use [`jq`\'s `--raw-output / -r` option](https://stedolan.github.io/jq/manual/) to grab the raw string.\\n\\n```bash\\njohn@Azure:~$ NODE_URL=$(az deployment group show -g rg-aca -n our-deployment --query properties.outputs | jq -r \'.nodeUrl.value\')\\njohn@Azure:~$ echo $NODE_URL\\nsome.url.northeurope.azurecontainerapps.io\\n```\\n\\nPerfect!\\n\\nThere\'s another approach you could use which [Aleksandar Nikoli\u0107 shared](https://twitter.com/alexandair/status/1476554234543890437), which means jq needn\'t be used at all; using the `tsv` output formatter:\\n\\n```bash\\njohn@Azure:~$ NODE_URL=$(az deployment group show -g rg-aca -n our-deployment --query properties.outputs.nodeUrl.value -o tsv)\\njohn@Azure:~$ echo $NODE_URL\\nsome.url.northeurope.azurecontainerapps.io\\n```\\n\\n## Convert deployment outputs to GitHub Action job outputs\\n\\nBefore wrapping up, here\'s one more useful script, if you find yourself automating in the context of GitHub Actions. It\'s often useful to take the deployment outputs, and convert them into GHA job outputs that can be used in other jobs.\\n\\nWith JSON and [jq](https://stedolan.github.io/jq/) in hand, it\'s possible to expose these like so:\\n\\n```bash\\nDEPLOYMENT_OUTPUTS=$(az deployment group show \\\\\\n  --resource-group ${{ env.RESOURCE_GROUP }} \\\\\\n  --name $DEPLOYMENT_NAME \\\\\\n  --query properties.outputs)\\n\\necho \'convert deployment outputs to outputs\'\\necho $DEPLOYMENT_OUTPUTS | jq -c \'. | to_entries[] | [.key, .value.value]\' |\\n  while IFS=$\\"\\\\n\\" read -r c; do\\n    OUTPUT_NAME=$(echo \\"$c\\" | jq -r \'.[0]\')\\n    OUTPUT_VALUE=$(echo \\"$c\\" | jq -r \'.[1]\')\\n    echo \\"setting output $OUTPUT_NAME=$OUTPUT_VALUE\\"\\n    echo \\"$OUTPUT_NAME=$OUTPUT_VALUE\\" >> $GITHUB_OUTPUT\\n  done\\n```"},{"id":"azure-container-apps-build-and-deploy-with-bicep-and-github-actions","metadata":{"permalink":"/azure-container-apps-build-and-deploy-with-bicep-and-github-actions","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-12-27-azure-container-apps-build-and-deploy-with-bicep-and-github-actions/index.md","source":"@site/blog/2021-12-27-azure-container-apps-build-and-deploy-with-bicep-and-github-actions/index.md","title":"Azure Container Apps: build and deploy with Bicep and GitHub Actions","description":"Learn how to deploy a web app to Azure Container Apps using Bicep and GitHub Actions. This post covers the configuration and deployment of secrets.","date":"2021-12-27T00:00:00.000Z","tags":[{"inline":false,"label":"Bicep","permalink":"/tags/bicep","description":"The Bicep language for Azure Resource Manager templates."},{"inline":false,"label":"GitHub Actions","permalink":"/tags/github-actions","description":"The GitHub Actions CI / CD service."},{"inline":false,"label":"Azure Container Apps","permalink":"/tags/azure-container-apps","description":"The Azure Container Apps service. Effectively a managed Kubernetes service."}],"readingTime":13.565,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-container-apps-build-and-deploy-with-bicep-and-github-actions","title":"Azure Container Apps: build and deploy with Bicep and GitHub Actions","authors":"johnnyreilly","tags":["bicep","github actions","azure container apps"],"image":"./title-image.png","hide_table_of_contents":false,"description":"Learn how to deploy a web app to Azure Container Apps using Bicep and GitHub Actions. This post covers the configuration and deployment of secrets."},"unlisted":false,"prevItem":{"title":"Query deployment outputs with the Azure CLI","permalink":"/azure-cli-show-query-output-properties"},"nextItem":{"title":"Azure Container Apps, Bicep and GitHub Actions","permalink":"/azure-container-apps-bicep-and-github-actions"}},"content":"This post shows how to build and deploy a simple web application to Azure Container Apps using Bicep and GitHub Actions. This includes the configuration and deployment of secrets.\\n\\nThis post follows on from the [previous post](../2021-12-19-azure-container-apps-bicep-and-github-actions/index.md) which deployed infrastructure and a \\"hello world\\" container, this time introducing the building of an image and storing it in the [GitHub container registry](https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry) so it can be deployed.\\n\\nIf you\'d like to learn more about using dapr with Azure Container Apps then you might want to read [this post](../2022-01-22-azure-container-apps-dapr-bicep-github-actions-debug-devcontainer/index.md).\\n\\n![title image reading \\"Azure Container Apps: build and deploy with Bicep and GitHub Actions\\" with the Bicep, Azure Container Apps and GitHub Actions logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Updated 02/05/2022\\n\\nThis post has been updated to reflect the migration of Azure Container Apps from the Microsoft.Web namespace to the Microsoft.App namespace in March 2022. See: https://github.com/microsoft/azure-container-apps/issues/109\\n\\n## The containerised convent\\n\\nI learn the most about a technology when I\'m using it to build something. It so happens that I have an aunt that\'s a nun, and long ago she persuaded me to build her convent a website. I\'m a good nephew and I complied. Since that time I\'ve been merrily overengineering it for fun and non-profit.\\n\\nMy aunts website is a pretty vanilla node app. Significantly it is already containerised and runs on [Azure App Service Web App for Containers](https://azure.microsoft.com/en-gb/services/app-service/containers/). Given it lives in the context of a container, this makes it a great candidate for porting to Azure Container Apps.\\n\\nSo that\'s what we\'ll do in this post. But where I\'m building and deploying my aunt\'s container, you could equally be substituting your own; with some minimal changes.\\n\\n## Bicep\\n\\nLet\'s begin with the Bicep required to deploy our Azure Container App.\\n\\nIn our repository we\'ll create an `infra` directory, into which we\'ll place a `main.bicep` file which will contain our Bicep template:\\n\\n```bicep\\nparam nodeImage string\\nparam nodePort int\\nparam nodeIsExternalIngress bool\\n\\nparam containerRegistry string\\nparam containerRegistryUsername string\\n@secure()\\nparam containerRegistryPassword string\\n\\nparam tags object\\n\\n@secure()\\nparam APPSETTINGS_API_KEY string\\nparam APPSETTINGS_DOMAIN string\\nparam APPSETTINGS_FROM_EMAIL string\\nparam APPSETTINGS_RECIPIENT_EMAIL string\\n\\nvar location = resourceGroup().location\\nvar environmentName = \'env-${uniqueString(resourceGroup().id)}\'\\nvar minReplicas = 0\\n\\nvar nodeServiceAppName = \'node-app\'\\nvar workspaceName = \'${nodeServiceAppName}-log-analytics\'\\nvar appInsightsName = \'${nodeServiceAppName}-app-insights\'\\n\\nvar containerRegistryPasswordRef = \'container-registry-password\'\\nvar mailgunApiKeyRef = \'mailgun-api-key\'\\n\\nresource workspace \'Microsoft.OperationalInsights/workspaces@2021-12-01-preview\' = {\\n  name: workspaceName\\n  location: location\\n  tags: tags\\n  properties: {\\n    sku: {\\n      name: \'PerGB2018\'\\n    }\\n    retentionInDays: 30\\n    workspaceCapping: {}\\n  }\\n}\\n\\nresource appInsights \'Microsoft.Insights/components@2020-02-02\' = {\\n  name: appInsightsName\\n  location: location\\n  tags: tags\\n  kind: \'web\'\\n  properties: {\\n    Application_Type: \'web\'\\n    Flow_Type: \'Bluefield\'\\n  }\\n}\\n\\nresource environment \'Microsoft.App/managedEnvironments@2022-01-01-preview\' = {\\n  name: environmentName\\n  location: location\\n  tags: tags\\n  properties: {\\n    appLogsConfiguration: {\\n      destination: \'log-analytics\'\\n      logAnalyticsConfiguration: {\\n        customerId: workspace.properties.customerId\\n        sharedKey: listKeys(workspace.id, workspace.apiVersion).primarySharedKey\\n      }\\n    }\\n    containerAppsConfiguration: {\\n      daprAIInstrumentationKey: appInsights.properties.InstrumentationKey\\n    }\\n  }\\n}\\n\\nresource containerApp \'Microsoft.App/containerApps@2022-01-01-preview\' = {\\n  name: nodeServiceAppName\\n  kind: \'containerapps\'\\n  tags: tags\\n  location: location\\n  properties: {\\n    managedEnvironmentId: environment.id\\n    configuration: {\\n      secrets: [\\n        {\\n          name: containerRegistryPasswordRef\\n          value: containerRegistryPassword\\n        }\\n        {\\n          name: mailgunApiKeyRef\\n          value: APPSETTINGS_API_KEY\\n        }\\n      ]\\n      registries: [\\n        {\\n          server: containerRegistry\\n          username: containerRegistryUsername\\n          passwordSecretRef: containerRegistryPasswordRef\\n        }\\n      ]\\n      ingress: {\\n        \'external\': nodeIsExternalIngress\\n        \'targetPort\': nodePort\\n      }\\n    }\\n    template: {\\n      containers: [\\n        {\\n          image: nodeImage\\n          name: nodeServiceAppName\\n          transport: \'auto\'\\n          env: [\\n            {\\n              name: \'APPSETTINGS_API_KEY\'\\n              secretref: mailgunApiKeyRef\\n            }\\n            {\\n              name: \'APPSETTINGS_DOMAIN\'\\n              value: APPSETTINGS_DOMAIN\\n            }\\n            {\\n              name: \'APPSETTINGS_FROM_EMAIL\'\\n              value: APPSETTINGS_FROM_EMAIL\\n            }\\n            {\\n              name: \'APPSETTINGS_RECIPIENT_EMAIL\'\\n              value: APPSETTINGS_RECIPIENT_EMAIL\\n            }\\n          ]\\n        }\\n      ]\\n      scale: {\\n        minReplicas: minReplicas\\n      }\\n    }\\n  }\\n}\\n```\\n\\nLet\'s talk through this template. The environment, workspace and app insights resources are fairly self explanatory. The `containerApp` resource is where the action is. We\'ll drill into that resource and the parameters used to configure it.\\n\\n### The node container app\\n\\nWe\'re going to create a single container app for our node web application. This is configured with these parameters:\\n\\n```bicep\\nparam nodeImage string\\nparam nodePort int\\nparam nodeIsExternalIngress bool\\n```\\n\\nThe above parameters relate to the node application that represents the website. The `nodeImage` is the container image which should be deployed to a container app. The `nodePort` is the port from the app which should be exposed (`3000` in our case). `nodeIsExternalIngress` is [whether the container should be accessible on the internet](https://docs.microsoft.com/en-us/azure/container-apps/ingress?tabs=bash#configuration). (Always `true` incidentally.)\\n\\nWhen these parameters are applied to the `containerApp` resource, it looks like this:\\n\\n```bicep\\nvar nodeServiceAppName = \'node-app\'\\n\\nresource containerApp \'Microsoft.App/containerApps@2022-01-01-preview\' = {\\n  // ...\\n  properties: {\\n      // ...\\n      ingress: {\\n        \'external\': nodeIsExternalIngress\\n        \'targetPort\': nodePort\\n      }\\n    }\\n    template: {\\n      containers: [\\n        {\\n          image: nodeImage\\n          name: nodeServiceAppName\\n          // ...\\n        }\\n      ]\\n      // ...\\n    }\\n  }\\n}\\n```\\n\\n### Accessing the GitHub Container Registry\\n\\nGiven that we\'ve told Bicep to deploy an `image`, we\'re going to need to tell it what registry it can use to acquire that image. Our template takes these parameters:\\n\\n```bicep\\nparam containerRegistry string\\nparam containerRegistryUsername string\\n@secure()\\nparam containerRegistryPassword string\\n\\nparam tags object\\n```\\n\\nWith the exception of the `tags` object which is metadata to apply to resources, these parameters are related to the container registry where our images will be stored. GitHub\'s in our case. Remember, what we deploy to Azure Container Apps are container images. To get something running in an ACA, it first has to reside in a container registry. There\'s a multitude of container registries out there and we\'re using the one directly available in GitHub. As an alternative, we could use an [Azure Container Registry](https://azure.microsoft.com/en-us/services/container-registry/), or [Docker Hub](https://hub.docker.com/) - or something else entirely.\\n\\nDo note the [`@secure()`](https://docs.microsoft.com/en-us/azure/azure-resource-manager/bicep/parameters#secure-parameters) decorator. This marks the `containerRegistryPassword` parameter as secure. The value for a secure parameter isn\'t saved to the deployment history and isn\'t logged. Typically you\'ll want to mark secrets with the `@secure()` decorator for this very reason.\\n\\nWe use the parameters to configure the `registries` property of our container app. This tells the ACA where it can go to collect the image it needs. You can also see our first usage of secrets here. We declare the `containerRegistryPassword` as a secret which is stored against the ref `\'container-registry-password\'`; captured as the variable `containerRegistryPasswordRef`. That variable is then referenced in the `passwordSecretRef` property - thus telling ACA where it can find the password.\\n\\n```bicep\\nvar containerRegistryPasswordRef = \'container-registry-password\'\\n\\nresource containerApp \'Microsoft.App/containerApps@2022-01-01-preview\' = {\\n  // ...\\n  properties: {\\n    // ...\\n    configuration: {\\n      secrets: [\\n        {\\n          name: containerRegistryPasswordRef\\n          value: containerRegistryPassword\\n        }\\n        // ...\\n      ]\\n      registries: [\\n        {\\n          server: containerRegistry\\n          username: containerRegistryUsername\\n          passwordSecretRef: containerRegistryPasswordRef\\n        }\\n      ]\\n      // ...\\n    }\\n    // ...\\n  }\\n}\\n```\\n\\n### Secrets / Configuration\\n\\nThe final collection of parameters are unrelated to the infrastructure of deployment, rather they are the things required to configure our running application:\\n\\n```bicep\\n@secure()\\nparam APPSETTINGS_API_KEY string\\nparam APPSETTINGS_DOMAIN string\\nparam APPSETTINGS_FROM_EMAIL string\\nparam APPSETTINGS_RECIPIENT_EMAIL string\\n```\\n\\nAgain we\'ve got a secret marked with `@secure()` in the form of our `APPSETTINGS_API_KEY`. Just as we did with `containerRegistryPassword`, we declare `APPSETTINGS_API_KEY` to be a secret, which is stored against the ref `\'mailgun-api-key\'`; captured as the variable `mailgunApiKeyRef`.\\n\\nAll of our configuration is exposed to the running application through environment variables. By and large this is achieved through the mechanism of key / value pairs (well technically `name` / `value`) with a slight variation for secrets. Similar to the `passwordSecretRef` mechanism we used for the registry password, we use a `secretref` in place of `value` when passing a secret, and the value will be the ref that was set up in the `secrets` section; `mailgunApiKeyRef` in this case.\\n\\n```bicep\\nvar mailgunApiKeyRef = \'mailgun-api-key\'\\n\\nresource containerApp \'Microsoft.App/containerApps@2022-01-01-preview\' = {\\n  // ...\\n  properties: {\\n    // ...\\n    configuration: {\\n      secrets: [\\n        // ...\\n        {\\n          name: mailgunApiKeyRef\\n          value: APPSETTINGS_API_KEY\\n        }\\n      ]\\n      // ...\\n    }\\n    template: {\\n      containers: [\\n        {\\n          // ...\\n          env: [\\n            {\\n              name: \'APPSETTINGS_API_KEY\'\\n              secretref: mailgunApiKeyRef\\n            }\\n            {\\n              name: \'APPSETTINGS_DOMAIN\'\\n              value: APPSETTINGS_DOMAIN\\n            }\\n            {\\n              name: \'APPSETTINGS_FROM_EMAIL\'\\n              value: APPSETTINGS_FROM_EMAIL\\n            }\\n            {\\n              name: \'APPSETTINGS_RECIPIENT_EMAIL\'\\n              value: APPSETTINGS_RECIPIENT_EMAIL\\n            }\\n          ]\\n        }\\n      ]\\n      // ...\\n    }\\n  }\\n}\\n```\\n\\n## Setting up a resource group\\n\\nWith our Bicep in place, we\'re going to need a resource group to send it to. Right now, Azure Container Apps aren\'t available everywhere. So we\'re going to create ourselves a resource group in North Europe which does support ACAs:\\n\\n```shell\\naz group create -g rg-aca -l northeurope\\n```\\n\\n## Secrets for GitHub Actions\\n\\nWe\'re aiming to set up a GitHub Action to handle our deployment. This will depend upon a number of secrets:\\n\\n![Screenshot of the secrets in the GitHub website that we need to create](screenshot-github-secrets.png)\\n\\nWe\'ll need to create each of these secrets.\\n\\n### `AZURE_CREDENTIALS` - GitHub logging into Azure\\n\\nSo GitHub Actions can interact with Azure on our behalf, we need to provide it with some credentials. We\'ll use the Azure CLI to create these:\\n\\n```shell\\naz ad sp create-for-rbac --name \\"myApp\\" --role contributor \\\\\\n    --scopes /subscriptions/{subscription-id}/resourceGroups/{resource-group} \\\\\\n    --sdk-auth\\n```\\n\\nRemember to replace the `{subscription-id}` with your subscription id and `{resource-group}` with the name of your resource group (`rg-aca` if you\'re following along). This command will pump out a lump of JSON that looks something like this:\\n\\n```json\\n{\\n  \\"clientId\\": \\"a-client-id\\",\\n  \\"clientSecret\\": \\"a-client-secret\\",\\n  \\"subscriptionId\\": \\"a-subscription-id\\",\\n  \\"tenantId\\": \\"a-tenant-id\\",\\n  \\"activeDirectoryEndpointUrl\\": \\"https://login.microsoftonline.com\\",\\n  \\"resourceManagerEndpointUrl\\": \\"https://management.azure.com/\\",\\n  \\"activeDirectoryGraphResourceId\\": \\"https://graph.windows.net/\\",\\n  \\"sqlManagementEndpointUrl\\": \\"https://management.core.windows.net:8443/\\",\\n  \\"galleryEndpointUrl\\": \\"https://gallery.azure.com/\\",\\n  \\"managementEndpointUrl\\": \\"https://management.core.windows.net/\\"\\n}\\n```\\n\\nTake this and save it as the `AZURE_CREDENTIALS` secret in Azure.\\n\\n### `PACKAGES_TOKEN` - Azure accessing the GitHub container registry\\n\\nWe also need a secret for accessing packages from Azure. We\'re going to be publishing packages to the GitHub container registry. Azure is going to need to be able to access this when we\'re deploying. ACA deployment works by telling Azure where to look for an image and providing any necessary credentials to do the acquisition. To facilitate this we\'ll set up a `PACKAGES_TOKEN` secret. This is a GitHub personal access token with the `read:packages` scope. [Follow the instructions here to create the token.](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token)\\n\\n### Secrets for the app\\n\\nAlongside these infrastructure / deployment related secrets, we\'ll need ones to configure the app at runtime:\\n\\n- `APPSETTINGS_API_KEY` - an API key for Mailgun which will be used to send emails\\n- `APPSETTINGS_DOMAIN` - the domain for the email eg `mg.poorclaresarundel.org`\\n- `APPSETTINGS_FROM_EMAIL` - who automated emails should come from eg `noreply@mg.poorclaresarundel.org`\\n- `APPSETTINGS_RECIPIENT_EMAIL` - the email address emails should be sent to\\n\\nStrictly speaking, only the API key is a secret. But to simplify this post we\'ll configure all of these as secrets in GitHub.\\n\\n## Deploying with GitHub Actions\\n\\nWith our secrets configured, we\'re now well placed to write our GitHub Action. We\'ll create a `.github/workflows/deploy.yaml` file in our repository and populate it thusly:\\n\\n```yaml\\n# yaml-language-server: $schema=./build.yaml\\nname: Build and Deploy\\non:\\n  # Trigger the workflow on push or pull request,\\n  # but only for the main branch\\n  push:\\n    branches:\\n      - main\\n  pull_request:\\n    branches:\\n      - main\\n    # Publish semver tags as releases.\\n    tags: [\'v*.*.*\']\\n  workflow_dispatch:\\n\\nenv:\\n  RESOURCE_GROUP: rg-aca\\n  REGISTRY: ghcr.io\\n  IMAGE_NAME: ${{ github.repository }}\\n\\njobs:\\n  build:\\n    runs-on: ubuntu-latest\\n    strategy:\\n      matrix:\\n        services:\\n          [{ \'imageName\': \'node-service\', \'directory\': \'./node-service\' }]\\n    permissions:\\n      contents: read\\n      packages: write\\n    outputs:\\n      containerImage-node: ${{ steps.image-tag.outputs.image-node-service }}\\n    steps:\\n      - name: Checkout repository\\n        uses: actions/checkout@v2\\n\\n      # Login against a Docker registry except on PR\\n      # https://github.com/docker/login-action\\n      - name: Log into registry ${{ env.REGISTRY }}\\n        if: github.event_name != \'pull_request\'\\n        uses: docker/login-action@v1\\n        with:\\n          registry: ${{ env.REGISTRY }}\\n          username: ${{ github.actor }}\\n          password: ${{ secrets.GITHUB_TOKEN }}\\n\\n      # Extract metadata (tags, labels) for Docker\\n      # https://github.com/docker/metadata-action\\n      - name: Extract Docker metadata\\n        id: meta\\n        uses: docker/metadata-action@v3\\n        with:\\n          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/${{ matrix.services.imageName }}\\n          tags: |\\n            type=semver,pattern={{version}}\\n            type=semver,pattern={{major}}.{{minor}}\\n            type=semver,pattern={{major}}\\n            type=ref,event=branch\\n            type=sha\\n\\n      # Build and push Docker image with Buildx (don\'t push on PR)\\n      # https://github.com/docker/build-push-action\\n      - name: Build and push Docker image\\n        uses: docker/build-push-action@v2\\n        with:\\n          context: ${{ matrix.services.directory }}\\n          push: ${{ github.event_name != \'pull_request\' }}\\n          tags: ${{ steps.meta.outputs.tags }}\\n          labels: ${{ steps.meta.outputs.labels }}\\n\\n      - name: Output image tag\\n        id: image-tag\\n        run: |\\n          name=$(echo \\"image-${{ matrix.services.imageName }}\\" | tr \'[:upper:]\' \'[:lower:]\')\\n          value=$(echo \\"${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/${{ matrix.services.imageName }}:sha-$(git rev-parse --short HEAD)\\" | tr \'[:upper:]\' \'[:lower:]\')\\n          echo \\"setting output: $name=$value\\"\\n          echo \\"$name=$value\\" >> $GITHUB_OUTPUT\\n\\n  deploy:\\n    runs-on: ubuntu-latest\\n    needs: [build]\\n    steps:\\n      - name: Checkout repository\\n        uses: actions/checkout@v2\\n\\n      - name: Azure Login\\n        uses: azure/login@v1\\n        with:\\n          creds: ${{ secrets.AZURE_CREDENTIALS }}\\n\\n      - name: Deploy bicep\\n        uses: azure/CLI@v2\\n        if: github.event_name != \'pull_request\'\\n        with:\\n          inlineScript: |\\n            tags=\'{\\"owner\\":\\"johnnyreilly\\", \\"email\\":\\"johnny_reilly@hotmail.com\\"}\'\\n            az deployment group create \\\\\\n              --resource-group ${{ env.RESOURCE_GROUP }} \\\\\\n              --template-file ./infra/main.bicep \\\\\\n              --parameters \\\\\\n                  nodeImage=\'${{ needs.build.outputs.containerImage-node }}\' \\\\\\n                  nodePort=3000 \\\\\\n                  nodeIsExternalIngress=true \\\\\\n                  containerRegistry=${{ env.REGISTRY }} \\\\\\n                  containerRegistryUsername=${{ github.actor }} \\\\\\n                  containerRegistryPassword=${{ secrets.PACKAGES_TOKEN }} \\\\\\n                  tags=\\"$tags\\" \\\\\\n                  APPSETTINGS_API_KEY=\\"${{ secrets.APPSETTINGS_API_KEY }}\\" \\\\\\n                  APPSETTINGS_DOMAIN=\\"${{ secrets.APPSETTINGS_DOMAIN }}\\" \\\\\\n                  APPSETTINGS_FROM_EMAIL=\\"${{ secrets.APPSETTINGS_FROM_EMAIL }}\\" \\\\\\n                  APPSETTINGS_RECIPIENT_EMAIL=\\"${{ secrets.APPSETTINGS_RECIPIENT_EMAIL }}\\"\\n\\n      - name: What-if bicep\\n        uses: azure/CLI@v2\\n        if: github.event_name == \'pull_request\'\\n        with:\\n          inlineScript: |\\n            tags=\'{\\"owner\\":\\"johnnyreilly\\", \\"email\\":\\"johnny_reilly@hotmail.com\\"}\'\\n            az deployment group what-if \\\\\\n              --resource-group ${{ env.RESOURCE_GROUP }} \\\\\\n              --template-file ./infra/main.bicep \\\\\\n              --parameters \\\\\\n                  nodeImage=\'${{ needs.build.outputs.containerImage-node }}\' \\\\\\n                  nodePort=3000 \\\\\\n                  nodeIsExternalIngress=true \\\\\\n                  containerRegistry=${{ env.REGISTRY }} \\\\\\n                  containerRegistryUsername=${{ github.actor }} \\\\\\n                  containerRegistryPassword=${{ secrets.PACKAGES_TOKEN }} \\\\\\n                  tags=\\"$tags\\" \\\\\\n                  APPSETTINGS_API_KEY=\\"${{ secrets.APPSETTINGS_API_KEY }}\\" \\\\\\n                  APPSETTINGS_DOMAIN=\\"${{ secrets.APPSETTINGS_DOMAIN }}\\" \\\\\\n                  APPSETTINGS_FROM_EMAIL=\\"${{ secrets.APPSETTINGS_FROM_EMAIL }}\\" \\\\\\n                  APPSETTINGS_RECIPIENT_EMAIL=\\"${{ secrets.APPSETTINGS_RECIPIENT_EMAIL }}\\"\\n```\\n\\nThere\'s a lot in this workflow. Let\'s dig into the `build` and `deploy` jobs to see what\'s happening.\\n\\n### `build` - building our image\\n\\nThe `build` job is all about building our container images and pushing then to the GitHub registry. It\'s heavily inspired by [Jeff Hollan](https://twitter.com/jeffhollan)\'s [Azure sample app GHA](https://github.com/Azure-Samples/container-apps-store-api-microservice). When we look at the `strategy` we can see a `matrix` of `services` consisting of a single service; our node app:\\n\\n```yaml\\nstrategy:\\n  matrix:\\n    services: [{ \'imageName\': \'node-service\', \'directory\': \'./node-service\' }]\\n```\\n\\nThis is a matrix because a typical use case of an Azure Container App will be multi-container, so we\'re starting generic from the beginning. The `outputs` pumps out the details of our `containerImage-node` image to be used later:\\n\\n```yaml\\noutputs:\\n  containerImage-node: ${{ steps.image-tag.outputs.image-node-service }}\\n```\\n\\nWith that understanding in place, let\'s examine what each of the steps in the `build` job does\\n\\n- `Log into registry` - logs into the GitHub container registry\\n- `Extract Docker metadata` - acquire tags which will be used for versioning\\n- `Build and push Docker image` - build the docker image and if this is not a PR: tag, label and push it to the registry\\n- `Output image tag` - write out the image tag for usage in deployment\\n\\n### `deploy` - shipping our image to Azure\\n\\nThe `deploy` job does two possible things with our Bicep template; `main.bicep`.\\n\\nIn the case of a pull request, it runs the [`az deployment group what-if`](https://docs.microsoft.com/en-us/cli/azure/deployment/group?view=azure-cli-latest#az_deployment_group_what_if) - this allows us to see what the effect would be of applying a PR to our infrastructure.\\n\\n```yaml\\n- name: What-if bicep\\n  uses: azure/CLI@v2\\n  if: github.event_name == \'pull_request\'\\n  with:\\n    inlineScript: |\\n      tags=\'{\\"owner\\":\\"johnnyreilly\\", \\"email\\":\\"johnny_reilly@hotmail.com\\"}\'\\n      az deployment group what-if \\\\\\n        --resource-group ${{ env.RESOURCE_GROUP }} \\\\\\n        --template-file ./infra/main.bicep \\\\\\n        --parameters \\\\\\n            nodeImage=\'${{ needs.build.outputs.containerImage-node }}\' \\\\\\n            nodePort=3000 \\\\\\n            nodeIsExternalIngress=true \\\\\\n            containerRegistry=${{ env.REGISTRY }} \\\\\\n            containerRegistryUsername=${{ github.actor }} \\\\\\n            containerRegistryPassword=${{ secrets.PACKAGES_TOKEN }} \\\\\\n            tags=\\"$tags\\" \\\\\\n            APPSETTINGS_API_KEY=\\"${{ secrets.APPSETTINGS_API_KEY }}\\" \\\\\\n            APPSETTINGS_DOMAIN=\\"${{ secrets.APPSETTINGS_DOMAIN }}\\" \\\\\\n            APPSETTINGS_FROM_EMAIL=\\"${{ secrets.APPSETTINGS_FROM_EMAIL }}\\" \\\\\\n            APPSETTINGS_RECIPIENT_EMAIL=\\"${{ secrets.APPSETTINGS_RECIPIENT_EMAIL }}\\"\\n```\\n\\nWhen it\'s not a pull request, it runs the [`az deployment group create`](https://docs.microsoft.com/en-us/cli/azure/deployment/group?view=azure-cli-latest#az_deployment_group_create) command which performs a deployment of our `main.bicep` file.\\n\\n```yaml\\n- name: Deploy bicep\\n  uses: azure/CLI@v2\\n  if: github.event_name != \'pull_request\'\\n  with:\\n    inlineScript: |\\n      tags=\'{\\"owner\\":\\"johnnyreilly\\", \\"email\\":\\"johnny_reilly@hotmail.com\\"}\'\\n      az deployment group create \\\\\\n        --resource-group ${{ env.RESOURCE_GROUP }} \\\\\\n        --template-file ./infra/main.bicep \\\\\\n        --parameters \\\\\\n            nodeImage=\'${{ needs.build.outputs.containerImage-node }}\' \\\\\\n            nodePort=3000 \\\\\\n            nodeIsExternalIngress=true \\\\\\n            containerRegistry=${{ env.REGISTRY }} \\\\\\n            containerRegistryUsername=${{ github.actor }} \\\\\\n            containerRegistryPassword=${{ secrets.PACKAGES_TOKEN }} \\\\\\n            tags=\\"$tags\\" \\\\\\n            APPSETTINGS_API_KEY=\\"${{ secrets.APPSETTINGS_API_KEY }}\\" \\\\\\n            APPSETTINGS_DOMAIN=\\"${{ secrets.APPSETTINGS_DOMAIN }}\\" \\\\\\n            APPSETTINGS_FROM_EMAIL=\\"${{ secrets.APPSETTINGS_FROM_EMAIL }}\\" \\\\\\n            APPSETTINGS_RECIPIENT_EMAIL=\\"${{ secrets.APPSETTINGS_RECIPIENT_EMAIL }}\\"\\n```\\n\\nIn either case we pass the same set of parameters:\\n\\n```shell\\nnodeImage=\'${{ needs.build.outputs.containerImage-node }}\' \\\\\\nnodePort=3000 \\\\\\nnodeIsExternalIngress=true \\\\\\ncontainerRegistry=${{ env.REGISTRY }} \\\\\\ncontainerRegistryUsername=${{ github.actor }} \\\\\\ncontainerRegistryPassword=${{ secrets.PACKAGES_TOKEN }} \\\\\\ntags=\\"$tags\\" \\\\\\nAPPSETTINGS_API_KEY=\\"${{ secrets.APPSETTINGS_API_KEY }}\\" \\\\\\nAPPSETTINGS_DOMAIN=\\"${{ secrets.APPSETTINGS_DOMAIN }}\\" \\\\\\nAPPSETTINGS_FROM_EMAIL=\\"${{ secrets.APPSETTINGS_FROM_EMAIL }}\\" \\\\\\nAPPSETTINGS_RECIPIENT_EMAIL=\\"${{ secrets.APPSETTINGS_RECIPIENT_EMAIL }}\\"\\n```\\n\\nThese are either:\\n\\n- secrets we set up earlier\\n- environment variables declared at the start of the script or\\n- outputs from the build step - this is where we acquire our node image\\n\\n## Running it\\n\\nWhen the GitHub Action has been run you\'ll find that Azure Container App is now showing up inside the Azure Portal in your resource group, alongside the other resources:\\n\\n![screenshot of the Azure Container App\'s resource group in the Azure Portal](screenshot-azure-portal-container-app.png)\\n\\nAnd when we take a closer look at the container app, we find a URL we can navigate to:\\n\\n![screenshot of the Azure Container App in the Azure Portal revealing it\'s URL](screenshot-azure-portal-container-app-url.png)\\n\\nCongratulations! You\'ve built and deployed a simple web app to Azure Container Apps with Bicep and GitHub Actions and secrets."},{"id":"azure-container-apps-bicep-and-github-actions","metadata":{"permalink":"/azure-container-apps-bicep-and-github-actions","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-12-19-azure-container-apps-bicep-and-github-actions/index.md","source":"@site/blog/2021-12-19-azure-container-apps-bicep-and-github-actions/index.md","title":"Azure Container Apps, Bicep and GitHub Actions","description":"Learn how to deploy an Azure Container App to Azure with Bicep and GitHub Actions. A basic template is provided for deployment.","date":"2021-12-19T00:00:00.000Z","tags":[{"inline":false,"label":"Bicep","permalink":"/tags/bicep","description":"The Bicep language for Azure Resource Manager templates."},{"inline":false,"label":"GitHub Actions","permalink":"/tags/github-actions","description":"The GitHub Actions CI / CD service."},{"inline":false,"label":"Azure Container Apps","permalink":"/tags/azure-container-apps","description":"The Azure Container Apps service. Effectively a managed Kubernetes service."}],"readingTime":3.725,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-container-apps-bicep-and-github-actions","title":"Azure Container Apps, Bicep and GitHub Actions","authors":"johnnyreilly","tags":["bicep","github actions","azure container apps"],"image":"./title-image.png","hide_table_of_contents":false,"description":"Learn how to deploy an Azure Container App to Azure with Bicep and GitHub Actions. A basic template is provided for deployment."},"unlisted":false,"prevItem":{"title":"Azure Container Apps: build and deploy with Bicep and GitHub Actions","permalink":"/azure-container-apps-build-and-deploy-with-bicep-and-github-actions"},"nextItem":{"title":"Open Graph: a guide to sharable social media previews","permalink":"/open-graph-sharing-previews-guide"}},"content":"Azure Container Apps are an exciting way to deploy containers to Azure. This post shows how to deploy the infrastructure for an Azure Container App to Azure using Bicep and GitHub Actions. The [Azure Container App documentation](https://docs.microsoft.com/en-us/azure/container-apps/) features quickstarts for deploying your first container app using both the Azure Portal and the Azure CLI. These are great, but there\'s a gap if you prefer to deploy using Bicep and you\'d like to get your CI/CD setup right from the beginning. This post aims to fill that gap.\\n\\nIf you\'re interested in building your own containers as well, it\'s worth looking at [this follow up post](../2021-12-27-azure-container-apps-build-and-deploy-with-bicep-and-github-actions/index.md).\\n\\n![title image reading \\"Azure Container Apps, Bicep and GitHub Actions\\" with the Bicep, Azure Container Apps and GitHub Actions logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Updated 02/05/2022\\n\\nThis post has been updated to reflect the migration of Azure Container Apps from the Microsoft.Web namespace to the Microsoft.App namespace in March 2022. See: https://github.com/microsoft/azure-container-apps/issues/109\\n\\n## Bicep\\n\\nLet\'s begin with the Bicep required to deploy an Azure Container App.\\n\\nIn our new repository we\'ll create an `infra` directory, into which we\'ll place a `main.bicep` file which will contain our Bicep template.\\n\\nI\'ve pared this down to the simplest Bicep template that I can; it only requires a name parameter:\\n\\n```bicep\\nparam name string\\nparam secrets array = []\\n\\nvar location = resourceGroup().location\\nvar environmentName = \'Production\'\\nvar workspaceName = \'${name}-log-analytics\'\\n\\nresource workspace \'Microsoft.OperationalInsights/workspaces@2021-12-01-preview\' = {\\n  name: workspaceName\\n  location: location\\n  properties: {\\n    sku: {\\n      name: \'PerGB2018\'\\n    }\\n    retentionInDays: 30\\n    workspaceCapping: {}\\n  }\\n}\\n\\nresource environment \'Microsoft.App/managedEnvironments@2022-01-01-preview\' = {\\n  name: environmentName\\n  location: location\\n  properties: {\\n    appLogsConfiguration: {\\n      destination: \'log-analytics\'\\n      logAnalyticsConfiguration: {\\n        customerId: workspace.properties.customerId\\n        sharedKey: listKeys(workspace.id, workspace.apiVersion).primarySharedKey\\n      }\\n    }\\n  }\\n}\\n\\nresource containerApp \'Microsoft.App/containerApps@2022-01-01-preview\' = {\\n  name: name\\n  kind: \'containerapps\'\\n  location: location\\n  properties: {\\n    managedEnvironmentId: environment.id\\n    configuration: {\\n      secrets: secrets\\n      registries: []\\n      ingress: {\\n        \'external\':true\\n        \'targetPort\':80\\n      }\\n    }\\n    template: {\\n      containers: [\\n        {\\n          \'name\':\'simple-hello-world-container\'\\n          \'image\':\'mcr.microsoft.com/azuredocs/containerapps-helloworld:latest\'\\n          \'command\':[]\\n          \'resources\':{\\n            \'cpu\':\'.25\'\\n            \'memory\':\'.5Gi\'\\n          }\\n        }\\n      ]\\n    }\\n  }\\n}\\n```\\n\\nSome things to note from the template:\\n\\n- We\'re deploying three resources; a container app, a kube environment and an operational insights.\\n- Just like the official quickstarts we\'re going to use the `containerapps-helloworld` image.\\n\\n## Setting up a resource group\\n\\nIn order that you can deploy your Bicep, we\'re going to need a resource group to send it to. Right now, Azure Container Apps aren\'t available everywhere. So we\'re going to create ourselves a resource group in North Europe which does support ACAs:\\n\\n```shell\\naz group create -g rg-aca -l northeurope\\n```\\n\\n## Deploying with the Azure CLI\\n\\nWith this resource group in place, we could simply deploy using the Azure CLI like so:\\n\\n```shell\\naz deployment group create \\\\\\n  --resource-group rg-aca \\\\\\n  --template-file ./infra/main.bicep \\\\\\n  --parameters \\\\\\n    name=\'container-app\'\\n```\\n\\n## Deploying with GitHub Actions\\n\\nHowever, we\'re aiming to set up a GitHub Action to do this for us. We\'ll create a `.github/workflows/deploy.yaml` file in our repository:\\n\\n```yaml\\nname: Deploy\\non:\\n  push:\\n    branches: [main]\\n  workflow_dispatch:\\n\\nenv:\\n  RESOURCE_GROUP: rg-aca\\n\\njobs:\\n  deploy:\\n    runs-on: ubuntu-latest\\n    steps:\\n      - name: Checkout repository\\n        uses: actions/checkout@v2\\n\\n      - name: Azure Login\\n        uses: azure/login@v1\\n        with:\\n          creds: ${{ secrets.AZURE_CREDENTIALS }}\\n\\n      - name: Deploy bicep\\n        uses: azure/CLI@v2\\n        with:\\n          inlineScript: |\\n            az deployment group create \\\\\\n              --resource-group ${{ env.RESOURCE_GROUP }} \\\\\\n              --template-file ./infra/main.bicep \\\\\\n              --parameters \\\\\\n                name=\'container-app\'\\n```\\n\\nThe above GitHub action is very simple. It:\\n\\n1. Logs into Azure using some `AZURE_CREDENTIALS` we\'ll set up in a moment.\\n2. Invokes the Azure CLI to deploy our Bicep template.\\n\\nLet\'s create that `AZURE_CREDENTIALS` secret in GitHub:\\n\\n![Screenshot of `AZURE_CREDENTIALS` secret in the GitHub website that we need to create](screenshot-github-secrets.webp)\\n\\nWe\'ll use the Azure CLI once more:\\n\\n```shell\\naz ad sp create-for-rbac --name \\"myApp\\" --role contributor \\\\\\n    --scopes /subscriptions/{subscription-id}/resourceGroups/{resource-group} \\\\\\n    --sdk-auth\\n```\\n\\nRemember to replace the `{subscription-id}` with your subscription id and `{resource-group}` with the name of your resource group (`rg-aca` if you\'re following along). This command will pump out a lump of JSON that looks something like this:\\n\\n```json\\n{\\n  \\"clientId\\": \\"a-client-id\\",\\n  \\"clientSecret\\": \\"a-client-secret\\",\\n  \\"subscriptionId\\": \\"a-subscription-id\\",\\n  \\"tenantId\\": \\"a-tenant-id\\",\\n  \\"activeDirectoryEndpointUrl\\": \\"https://login.microsoftonline.com\\",\\n  \\"resourceManagerEndpointUrl\\": \\"https://management.azure.com/\\",\\n  \\"activeDirectoryGraphResourceId\\": \\"https://graph.windows.net/\\",\\n  \\"sqlManagementEndpointUrl\\": \\"https://management.core.windows.net:8443/\\",\\n  \\"galleryEndpointUrl\\": \\"https://gallery.azure.com/\\",\\n  \\"managementEndpointUrl\\": \\"https://management.core.windows.net/\\"\\n}\\n```\\n\\nTake this and save it as the `AZURE_CREDENTIALS` secret in Azure.\\n\\n## Running it\\n\\nWhen the GitHub Action has been run you\'ll find that Azure Container App is now showing up inside the Azure Portal:\\n\\n![screenshot of the Azure Container App in the Azure Portal](screenshot-azure-portal-container-app.png)\\n\\nYou\'ll see a URL is displayed, when you go that URL you\'ll find the hello world image is running!\\n\\n![screenshot of the running Azure Container App](screenshot-of-running-container-app.png)"},{"id":"open-graph-sharing-previews-guide","metadata":{"permalink":"/open-graph-sharing-previews-guide","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-12-12-open-graph-sharing-previews-guide/index.md","source":"@site/blog/2021-12-12-open-graph-sharing-previews-guide/index.md","title":"Open Graph: a guide to sharable social media previews","description":"Create sharable social media previews with Open Graph tags. Learn the required tags, testing tools, and platform rendering issues in this guide.","date":"2021-12-12T00:00:00.000Z","tags":[],"readingTime":7.76,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"open-graph-sharing-previews-guide","title":"Open Graph: a guide to sharable social media previews","authors":"johnnyreilly","tags":[],"image":"./title-image.png","hide_table_of_contents":false,"description":"Create sharable social media previews with Open Graph tags. Learn the required tags, testing tools, and platform rendering issues in this guide."},"unlisted":false,"prevItem":{"title":"Azure Container Apps, Bicep and GitHub Actions","permalink":"/azure-container-apps-bicep-and-github-actions"},"nextItem":{"title":"Azure Static Web App Deploy Previews with Azure DevOps","permalink":"/azure-static-web-app-deploy-previews-with-azure-devops"}},"content":"The Open Graph protocol has become the standard mechanism for sharing rich content on the web. This post looks at what implementing Open Graph tags for sharable previews (often called social media previews) looks like, the tools you can use and also examines the different platform rendering issue.\\n\\n![title image reading \\"Open Graph: a guide to sharable social media previews\\" with the open graph logo and screenshots of twitter shared cards](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Updated: 26 November 2022\\n\\nI\'ve updated this post to advise on image types to favour.\\n\\n## Open Graph protocol and sharing\\n\\nYou may have noticed, that when you share a URL, the platform on which you\'re sharing may display a kind of \\"preview\\" of the link. Here\'s an example of sharing a link to a blog on Twitter:\\n\\n[![screenshot of tweet demonstrating sharing](screenshot-of-tweet-demonstrating-sharing.webp)](https://twitter.com/johnny_reilly/status/1454092877722800131)\\n\\nSharing a link has automagically generated a preview \\"card\\" at the bottom of the tweet. It contains an image, it has the title of the blog and it has a description of the post as well.\\n\\nThis looks pretty fabulous and it gives the reader of that tweet some fairly rich information about what might be in that post. It potentially saves readers a click if it\'s obvious that the post isn\'t particularly interesting to them. Conversely, it makes it more likely that the reader will click if it does seem intriguing. Sharing previews are an asset.\\n\\nTwitter made this card using a combination of Open Graph metatags (and some custom tags) which my blog surfaces.\\n\\n## Open Graph meta tags\\n\\nThe [Open Graph protocol](https://ogp.me/) came out of Facebook and it describes itself thusly:\\n\\n> The Open Graph protocol enables any web page to become a rich object in a social graph. For instance, this is used on Facebook to allow any web page to have the same functionality as any other object on Facebook.\\n\\nWhat Open Graph is all about, is meta tags. Adding meta tags to an HTML page to explicitly define pieces of standardised information. Now there\'s many purposes for this, and we\'re interested in just one: sharing.\\n\\nNow that we understand what sharing previews give us, let\'s understand how they work. The [Open Graph](https://ogp.me/#metadata) website has a great walkthrough of the minimum requirement for Open Graph:\\n\\n> - `og:title` - The title of your object as it should appear within the graph, e.g., \\"The Rock\\".\\n> - `og:type` - The type of your object, e.g., \\"video.movie\\". Depending on the type you specify, other properties may also be required.\\n> - `og:image` - An image URL which should represent your object within the graph.\\n> - `og:url` - The canonical URL of your object that will be used as its permanent ID in the graph, e.g., \\"https://www.imdb.com/title/tt0117500/\\".\\n>\\n> As an example, the following is the Open Graph protocol markup for The Rock on IMDB:\\n>\\n> ```html\\n> <html prefix=\\"og: https://ogp.me/ns#\\">\\n>   <head>\\n>     <title>The Rock (1996)</title>\\n>     <meta property=\\"og:title\\" content=\\"The Rock\\" />\\n>     <meta property=\\"og:type\\" content=\\"video.movie\\" />\\n>     <meta property=\\"og:url\\" content=\\"https://www.imdb.com/title/tt0117500/\\" />\\n>     <meta\\n>       property=\\"og:image\\"\\n>       content=\\"https://ia.media-imdb.com/images/rock.jpg\\"\\n>     />\\n>     ...\\n>   </head>\\n>   ...\\n> </html>\\n> ```\\n\\nSharing previews have very similar, but crucially slightly different, requirements. Five tags are required to generate a sharable preview:\\n\\n- `og:title` - The title of your page\\n- `og:description` - A description of the content of that page\\n- `og:image` - An image URL which should appear in the social media share.\\n- `og:url` - The canonical URL of your web page.\\n- `twitter:card` - A [custom tag which is only required by Twitter](https://developer.twitter.com/en/docs/twitter-for-websites/cards/guides/getting-started#started) indicating the type of share, be it `\\"summary\\"`, `\\"summary_large_image\\"`, `\\"app\\"`, or `\\"player\\"`. Probably `\\"summary\\"` or `\\"summary_large_image\\"` for most use cases.\\n\\nIf we implement these, then our page will offer sharable previews.\\n\\nWith this understanding in place; we can take a look at what it would look like to add sharable previews to a website. We\'ll make ourselves a React website with:\\n\\n```\\nnpx react-static create\\n```\\n\\nWhen prompted, name the site `demo` and select the `blank` template.\\n\\nPlease note, nothing that we\'re doing here is React specific; it\'s applicable to all websites regardless of the technology they\'re built with; this is just a straightforward way to demo a website.\\n\\nWe\'re using [`react-static`](https://github.com/react-static/react-static) for this demo because it is a static site generator. This is significant because, as a general rule, many platforms that support sharing, do not crawl dynamically generated meta tags. By this we mean, tags generated by JavaScript at runtime. Rather, these tags must be baked into the HTML that is served up, so a static site generator like `react-static` fits the brief well as it takes care of this.\\n\\nWe\'re going to replace the `App.js` that is scaffolded out with our own `App.js`:\\n\\n```jsx\\nimport * as React from \'react\';\\nimport { Head } from \'react-static\';\\nimport \'./app.css\';\\n\\nfunction App() {\\n  const openGraphData = {\\n    title: \'Open Graph: a guide to sharing previews\',\\n    description:\\n      \'This page features the Open Graph protocol markup for sharing previews.\',\\n    url: \'https://johnnyreilly.github.io/open-graph-sharing-previews/\',\\n    image:\\n      \'https://upload.wikimedia.org/wikipedia/commons/7/72/Open_Graph_protocol_logo.png\',\\n  };\\n  return (\\n    <div className=\\"App\\">\\n      <Head>\\n        <meta property=\\"og:title\\" content={openGraphData.title} />\\n        <meta property=\\"og:description\\" content={openGraphData.description} />\\n        <meta property=\\"og:url\\" content={openGraphData.url} />\\n        <meta property=\\"og:image\\" content={openGraphData.image} />\\n        <meta name=\\"twitter:card\\" content=\\"summary\\" />\\n      </Head>\\n      <h1>{openGraphData.title}</h1>\\n      <img src={openGraphData.image} alt=\\"The Open Graph protocol logo\\" />\\n      <h2>Share it and see!</h2>\\n    </div>\\n  );\\n}\\n\\nexport default App;\\n```\\n\\nThe code above renders the required meta tags for sharing previews. When we build and deploy this we can see they show up like so:\\n\\n![screenshot of demo with devtools open illustrating the meta tags](screenshot-of-demo-with-devtools-open.png)\\n\\n## Tools for testing sharing\\n\\nNow we have a demo, it would be tremendous to be able to test it out. There\'s various official tools to test your URLs:\\n\\n- [Twitter](https://cards-dev.twitter.com/validator)\\n- [Facebook](https://developers.facebook.com/tools/debug/)\\n- [LinkedIn](https://www.linkedin.com/post-inspector/inspect/)\\n\\nThere\'s also a number of unoffical \\"aggregator\\" tools that attempt to render the appearance of your social previews across multiple platforms to save you going to each tool in turn:\\n\\n- https://www.opengraph.xyz/\\n- https://metatags.io/\\n- https://socialsharepreview.com/\\n\\nLet\'s test out the Twitter validator:\\n\\n![screenshot of testing out our site using the twitter validator](screenshot-of-twitter-validator.webp)\\n\\nTerrific! We have sharable previews enabled for the site we\'ve made.\\n\\n## Sharable preview rendering: not yet standard\\n\\nNow we have a sense of what sharing previews look like, what powers them and how to implement them. So far we\'ve looked just at Twitter for examples of sharing previews. However, support for Open Graph sharing previews is widespread. Examples of other places where you can use them include: Facebook, Polywork, Slack, Teams, Linked In, Outlook.com, Discord... The list is now very long indeed.\\n\\nHowever, each platform implements sharing previews according to their own standard. What does mean? Well, a link shared on Twitter will look different to one shared on Outlook.com. For example:\\n\\n![screenshot of an email being sent in outlook with a share preview card to the same blog showing the untruncated title](screenshot-of-email-demonstrating-sharing-with-a-non-cropped-image.png)\\n\\nAbove I\'m sharing a link to a blog post. The image is to the left, the title and description is to the right. Now let\'s look at the same link shared on Twitter:\\n\\n[![screenshot of a tweet where the image in the share preview card has been cropped making the title unreadable](screenshot-of-tweet-demonstrating-sharing-with-a-cropped-image.webp)](https://twitter.com/AzureWeekly/status/1436733027489652743)\\n\\nHere the image is above the title and the description. More distressingly, the image has been cropped which renders the title slightly unreadable.\\n\\nSo whilst the mechanism for sharing is roughly standardised, the rendering is not. It\'s not dissimilar to the web in the year 2000. Back then, a single piece of HTML could be rendered in many different ways, depending upon the browser. The same statement is true now for Open Graph sharing. Sharing can look very different depending upon the platform which is displaying the preview. The only way to avoid this at present is to thoroughly on all the platforms where we want to share links; ensuring the sharable previews look acceptable.\\n\\n## `og:image` type: PNG, JPEG or WebP? What\'s best?\\n\\nLet\'s think about the type of image we reference in the `og:image` tag for a moment. This is the image that will be displayed in the sharing preview:\\n\\n```html\\n<meta property=\\"og:image\\" content=\\"https://ia.media-imdb.com/images/rock.jpg\\" />\\n```\\n\\nWe can use any image format we like. However, there are some considerations to bear in mind. Whilst you might imagine that the image format is not important, it is. This is because not all platforms support all image formats. Whilst say Twitter and Facebook support PNG, JPEG and WebP, other platforms do not. For example, Teams does not support WebP. So if we want to share a link on Teams, we likely want to use a JPEG or PNG image.\\n\\nI should tell you that I learned this the hard way, deciding to use WebP for most of the images on my blog, including the Open Graph image. I was then surprised to find that the images were not displaying in Teams. I had to go back and change the Open Graph images back to PNG. Don\'t be me.\\n\\nIncidentally, the world could use a \\"caniuse\\" for Open Graph sharing previews. I\'d love to see one.\\n\\n## Conclusion\\n\\nIn this post we\'ve understood what sharable previews are, how to add them to a website, how to test them and some of the rough edges to be aware of.\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/open-graph-sharable-social-media-previews/)\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.logrocket.com/open-graph-sharable-social-media-previews/\\" />\\n</head>"},{"id":"azure-static-web-app-deploy-previews-with-azure-devops","metadata":{"permalink":"/azure-static-web-app-deploy-previews-with-azure-devops","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-12-05-azure-static-web-app-deploy-previews-with-azure-devops/index.md","source":"@site/blog/2021-12-05-azure-static-web-app-deploy-previews-with-azure-devops/index.md","title":"Azure Static Web App Deploy Previews with Azure DevOps","description":"This post describes a pull request deployment preview mechanism for Azure Static Web Apps inspired by the Netlify offering.","date":"2021-12-05T00:00:00.000Z","tags":[{"inline":false,"label":"Azure Static Web Apps","permalink":"/tags/azure-static-web-apps","description":"The Azure Static Web Apps service."},{"inline":false,"label":"Azure DevOps","permalink":"/tags/azure-devops","description":"The Azure DevOps suite of tools."}],"readingTime":10.825,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-static-web-app-deploy-previews-with-azure-devops","title":"Azure Static Web App Deploy Previews with Azure DevOps","authors":"johnnyreilly","tags":["azure static web apps","azure devops"],"image":"./title-image.png","hide_table_of_contents":false,"description":"This post describes a pull request deployment preview mechanism for Azure Static Web Apps inspired by the Netlify offering."},"unlisted":false,"prevItem":{"title":"Open Graph: a guide to sharable social media previews","permalink":"/open-graph-sharing-previews-guide"},"nextItem":{"title":"TypeScript vs JSDoc JavaScript","permalink":"/typescript-vs-jsdoc-javascript"}},"content":"I love [Netlify deploy previews](https://www.netlify.com/products/deploy-previews/). This post implements a pull request deployment preview mechanism for Azure Static Web Apps in the context of Azure DevOps which is very much inspired by the Netlify offering.\\n\\n![title image reading \\"Azure Static Web App Deploy Previews with Azure DevOps\\" with a Azure, Bicep and Azure DevOps logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\nHaving a build of your latest pull request which is deployed and clickable from the PR itself is a wonderful developer experience. It reduces friction for testing out changes by allowing you to see the impact from within the PR itself. No checking to see if an environment is free with the rest of the team, then manually running a pipeline and waiting whilst a deployment happens. No. It\'s all there without you having to lift a finger. I use Netlify deploy previews on my blog and have become accustomed to the delight that is this:\\n\\n![screenshot of a Netlify deploy preview on my latest blog post](screenshot-of-netlify-deploy-preview-in-pull-request.png)\\n\\nI love this and I wanted to implement the \\"browse the preview\\" mechanism in Azure DevOps as well, using Azure Static Web Apps. This blog post contains two things:\\n\\n1. A pull request deployment environment mechanism using Azure and Azure Pipelines with Bicep.\\n2. A mechanism for updating a pull request in Azure DevOps with a link to the deployment environment (the \\"browse the preview\\")\\n\\nIt\'s worth bearing in mind that there\'s a very similar feature to what we\'re going to build for **1.** in SWAs now called \\"staging environments\\" that is presently only available on GitHub and not Azure DevOps:\\n\\n[![screenshot of Anthony Chu at Microsoft saying \\"Unfortunately environments is not yet available for Azure DevOps.\\"](screenshot-of-staging-environments-not-available-yet.png)](https://docs.microsoft.com/en-us/answers/questions/574288/creating-environments-for-azure-static-web-app.html)\\n\\nIt\'s possible that in future the deployment environment aspect of this blog post may be rendered redundant by staging environments landing in Azure DevOps. However, the second part, which updates a PR in ADO with a link is probably generally useful. And it may be the case that the approach of provisioning an environment on demand and extracting a URL could be reworked to work with App Service and similar too.\\n\\nI wrote about using [SWAs with Azure DevOps earlier this year](../2021-08-15-bicep-azure-static-web-apps-azure-devops/index.md). This blog post will take the form of a [pull request on the code written in that post](https://dev.azure.com/johnnyreilly/azure-static-web-apps/_git/azure-static-web-apps/pullrequest/3). [The finished code for this blog post can be found here](https://dev.azure.com/johnnyreilly/azure-static-web-apps/_git/azure-static-web-apps?version=GThand-rolled-deploy-previews).\\n\\n## Getting `defaultHostName` from Static Web Apps\\n\\nThe first thing we\'re going to do is take the Bicep from that post and tweak it to the following:\\n\\n```bicep\\nparam appName string\\nparam repositoryUrl string\\nparam repositoryBranch string\\n\\nparam skuName string = \'Free\'\\nparam skuTier string = \'Free\'\\n\\nresource staticWebApp \'Microsoft.Web/staticSites@2021-02-01\' = {\\n  name: repositoryBranch == \'main\' ? appName : \'${appName}-${repositoryBranch}\'\\n  location: resourceGroup().location\\n  sku: {\\n    name: skuName\\n    tier: skuTier\\n  }\\n  properties: {\\n    // The provider, repositoryUrl and branch fields are required for successive deployments to succeed\\n    // for more details see: https://github.com/Azure/static-web-apps/issues/516\\n    provider: \'DevOps\'\\n    repositoryUrl: repositoryUrl\\n    branch: repositoryBranch\\n    buildProperties: {\\n      skipGithubActionWorkflowGeneration: true\\n    }\\n  }\\n}\\n\\noutput staticWebAppDefaultHostName string = staticWebApp.properties.defaultHostname // eg gentle-bush-0db02ce03.azurestaticapps.net\\noutput staticWebAppId string = staticWebApp.id\\noutput staticWebAppName string = staticWebApp.name\\n```\\n\\nThere\'s some changes in here. First of all we\'re using a newer version of the `staticSites` resource in Azure. You\'ll also see that we name the resource conditionally now. If we\'re on the `main` branch we name it as we did before with `appName`. But if we aren\'t then we suffix the `name` with the `repositoryBranch`. It\'s worth knowing that [there are restrictions and conventions for Azure resource naming](https://docs.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-best-practices/resource-abbreviations#compute-and-web). If you have a branch name that is just alphanumerics and hyphens you\'ll be fine.\\n\\nYou\'ll see the output of the Bicep file has changed. Previously we were outputting the `apiKey` that we used for deployment. This isn\'t the securest of approaches as, by having this as a deployment output, this data can be accessed by people who share access with your Azure portal. So we\'re going to use a different (and more secure) approach to acquire this in our pipeline later.\\n\\nMore significantly, we are now outputting the `staticWebAppDefaultHostName` of our newly provisioned SWA. This is the location where people will be able to view the deployment preview. Since we want to pump that into our pull request description, so people can click on the link, we are going to need this. We\'re also pumping out the `staticWebAppId` and `staticWebAppName`. We\'ll use the `staticWebAppName` to acquire the `apiKey` in our pipeline.\\n\\n## Azure Pipelines tweaks\\n\\nNow to the pipeline. After the deployment, our updated pipeline is going to acquire the `apiKey` for deployment like so:\\n\\n```yml\\n- task: AzureCLI@2\\n  displayName: \'Acquire API key for deployment\'\\n  inputs:\\n    azureSubscription: $(serviceConnection)\\n    scriptType: bash\\n    scriptLocation: inlineScript\\n    inlineScript: |\\n      APIKEY=$(az staticwebapp secrets list --name $(staticWebAppName) | jq -r \'.properties.apiKey\')\\n      echo \\"##vso[task.setvariable variable=apiKey;issecret=true]$APIKEY\\"\\n```\\n\\nThe above uses the [Azure CLI task](https://docs.microsoft.com/en-us/azure/devops/pipelines/tasks/deploy/azure-cli?view=azure-devops) to acquire the `apiKey`. It uses [jq](https://stedolan.github.io/jq/) to pull out the required property from the JSON and writes it as a secret variable in the pipeline to be used in the deployment.\\n\\nAt the end of the pipeline, if we\'re not on the `main` branch, the the pipeline is going to run a custom script that will update the PR with the preview URL:\\n\\n```yml\\n- task: Npm@1\\n  displayName: \'Pull request preview install\'\\n  condition: and(succeeded(), ne(variables.isMain, \'true\'))\\n  inputs:\\n    command: \'install\'\\n    workingDir: pull-request-preview\\n\\n- task: Npm@1\\n  displayName: \'Pull request preview\'\\n  condition: and(succeeded(), ne(variables.isMain, \'true\'))\\n  inputs:\\n    command: \'custom\'\\n    customCommand: \'run pull-request-preview -- --sat \\"$(System.AccessToken)\\" --project \\"$(System.TeamProject)\\" --repository \\"$(Build.Repository.Name)\\" --systemCollectionUri \\"$(System.CollectionUri)\\" --pullRequestId $(System.PullRequest.PullRequestId) --previewUrl \\"https://$(staticWebAppDefaultHostName)\\"\'\\n    workingDir: pull-request-preview\\n```\\n\\nWe haven\'t written that script yet; we will in a moment.\\n\\nThe complete `azure-piplines.yml` is below, and you\'ll notice we\'ve moved all variables save for the `subscriptionId` into the `azure-pipelines.yml` and we\'re using a `westeurope` location / resource group as at present `staticSites` is not available everywhere:\\n\\n```yml\\npool:\\n  vmImage: ubuntu-latest\\n\\nvariables:\\n  # subscriptionId is a variable defined on the pipeline itself\\n  - name: appName\\n    value: \'our-static-web-app\'\\n  - name: location\\n    value: \'westeurope\' # at time of writing static sites are available in limited locations such as westeurope\\n  - name: serviceConnection\\n    value: \'azureRMWestEurope\'\\n  - name: azureResourceGroup # this resource group lives in westeurope\\n    value: \'johnnyreilly\'\\n\\nsteps:\\n  - checkout: self\\n    submodules: true\\n\\n  - bash: az bicep build --file infra/static-web-app/main.bicep\\n    displayName: \'Compile Bicep to ARM\'\\n\\n  - task: AzureResourceManagerTemplateDeployment@3\\n    name: DeployStaticWebAppInfra\\n    displayName: Deploy Static Web App infra\\n    inputs:\\n      deploymentScope: Resource Group\\n      azureResourceManagerConnection: $(serviceConnection)\\n      subscriptionId: $(subscriptionId)\\n      action: Create Or Update Resource Group\\n      resourceGroupName: $(azureResourceGroup)\\n      location: $(location)\\n      templateLocation: Linked artifact\\n      csmFile: \'infra/static-web-app/main.json\' # created by bash script\\n      overrideParameters: >-\\n        -repositoryUrl $(Build.Repository.Uri)\\n        -repositoryBranch $(Build.SourceBranchName)\\n        -appName $(appName)\\n      deploymentMode: Incremental\\n      deploymentOutputs: deploymentOutputs\\n\\n  - task: PowerShell@2\\n    name: \'SetDeploymentOutputVariables\'\\n    displayName: \'Set Deployment Output Variables\'\\n    inputs:\\n      targetType: inline\\n      script: |\\n        $armOutputObj = \'$(deploymentOutputs)\' | ConvertFrom-Json\\n        $armOutputObj.PSObject.Properties | ForEach-Object {\\n          $keyname = $_.Name\\n          $value = $_.Value.value\\n\\n          # Creates a standard pipeline variable\\n          Write-Output \\"##vso[task.setvariable variable=$keyName;]$value\\"\\n\\n          # Display keys and values in pipeline\\n          Write-Output \\"output variable: $keyName $value\\"\\n        }\\n      pwsh: true\\n\\n  - task: AzureCLI@2\\n    displayName: \'Acquire API key for deployment\'\\n    inputs:\\n      azureSubscription: $(serviceConnection)\\n      scriptType: bash\\n      scriptLocation: inlineScript\\n      inlineScript: |\\n        APIKEY=$(az staticwebapp secrets list --name $(staticWebAppName) | jq -r \'.properties.apiKey\')\\n        echo \\"##vso[task.setvariable variable=apiKey;issecret=true]$APIKEY\\"\\n\\n  - task: AzureStaticWebApp@0\\n    name: DeployStaticWebApp\\n    displayName: Deploy Static Web App\\n    inputs:\\n      app_location: \'static-web-app\'\\n      # api_location: \'api\'\\n      output_location: \'build\'\\n      azure_static_web_apps_api_token: $(apiKey)\\n\\n  - task: Npm@1\\n    displayName: \'Pull request preview install\'\\n    condition: and(succeeded(), ne(variables.isMain, \'true\'))\\n    inputs:\\n      command: \'install\'\\n      workingDir: pull-request-preview\\n\\n  - task: Npm@1\\n    displayName: \'Pull request preview\'\\n    condition: and(succeeded(), ne(variables.isMain, \'true\'))\\n    inputs:\\n      command: \'custom\'\\n      customCommand: \'run pull-request-preview -- --sat \\"$(System.AccessToken)\\" --project \\"$(System.TeamProject)\\" --repository \\"$(Build.Repository.Name)\\" --systemCollectionUri \\"$(System.CollectionUri)\\" --pullRequestId $(System.PullRequest.PullRequestId) --previewUrl \\"https://$(staticWebAppDefaultHostName)\\"\'\\n      workingDir: pull-request-preview\\n```\\n\\n## Updating the PR with a preview URL\\n\\nWe want to be able to update our pull request with our deploy URL. To make that happen, we\'re going to whiz up a little node app using TypeScript, ts-node and [the azure-devops-node-api package](https://github.com/microsoft/azure-devops-node-api).\\n\\nLet\'s create our app:\\n\\n```bash\\nmkdir pull-request-preview\\ncd pull-request-preview\\nnpm init --yes\\nnpm install @types/node @types/yargs ts-node typescript azure-devops-node-api yargs --save\\n```\\n\\nWe\'ll update our newly created `package.json` file with a `pull-request-preview` script which will be the entry point.\\n\\n```json\\n  \\"scripts\\": {\\n    \\"pull-request-preview\\": \\"ts-node ./index.ts\\"\\n  },\\n```\\n\\nWe\'ll add a `tsconfig.json` file that looks like this:\\n\\n```json\\n{\\n  \\"compilerOptions\\": {\\n    \\"target\\": \\"ES2015\\",\\n    \\"module\\": \\"CommonJS\\",\\n    \\"strict\\": true,\\n    \\"esModuleInterop\\": true,\\n    \\"moduleResolution\\": \\"node\\"\\n  }\\n}\\n```\\n\\nFinally we\'ll add our script in a new `index.ts` file:\\n\\n```ts\\n#!/usr/bin/env node\\nimport yargs from \'yargs/yargs\';\\nimport * as nodeApi from \'azure-devops-node-api\';\\nimport { IGitApi } from \'azure-devops-node-api/GitApi\';\\nimport { PullRequestStatus } from \'azure-devops-node-api/interfaces/GitInterfaces\';\\n\\nconst parser = yargs(process.argv.slice(2)).options({\\n  pat: { type: \'string\', default: \'\' },\\n  sat: { type: \'string\', default: \'\' },\\n  systemCollectionUri: { type: \'string\', demandOption: true },\\n  project: { type: \'string\', demandOption: true },\\n  repository: { type: \'string\', demandOption: true },\\n  pullRequestId: { type: \'number\' },\\n  previewUrl: { type: \'string\', demandOption: true },\\n});\\n\\n(async () => {\\n  await run(await parser.argv);\\n})();\\n\\nasync function run({\\n  pat,\\n  sat,\\n  project,\\n  repository,\\n  systemCollectionUri,\\n  pullRequestId,\\n  previewUrl,\\n}: {\\n  pat: string;\\n  sat: string;\\n  systemCollectionUri: string;\\n  project: string;\\n  repository: string;\\n  pullRequestId: number | undefined;\\n  previewUrl: string;\\n}) {\\n  const config: Config = { project, repository };\\n  const gitApi = await getGitApi({ pat, sat, systemCollectionUri });\\n\\n  if (!pullRequestId)\\n    console.log(\\n      \'No pull request id supplied, so will look up latest active PR\',\\n    );\\n\\n  const pullRequestIdToUpdate =\\n    pullRequestId || (await getActivePullRequestId({ gitApi, config }));\\n  if (!pullRequestIdToUpdate) {\\n    console.log(\'No pull request found\');\\n    return;\\n  }\\n\\n  console.log(\\n    `Updating ${systemCollectionUri}/${project}/_git/${repository}/pullrequest/${pullRequestIdToUpdate} with a preview URL of ${previewUrl}`,\\n  );\\n\\n  const pullRequest = await getPullRequest({\\n    gitApi,\\n    config,\\n    pullRequestId: pullRequestIdToUpdate,\\n  });\\n\\n  await updatePullRequestDescription({\\n    gitApi,\\n    config,\\n    pullRequestId: pullRequestIdToUpdate,\\n    description: makePreviewDescriptionMarkdown(\\n      pullRequest.description!,\\n      previewUrl,\\n    ),\\n  });\\n\\n  console.log(\\n    `Updated pull request description a preview URL of ${previewUrl}`,\\n  );\\n}\\n\\ninterface Config {\\n  project: string;\\n  repository: string;\\n}\\n\\nasync function getGitApi({\\n  sat,\\n  pat,\\n  systemCollectionUri,\\n}: {\\n  pat: string;\\n  sat: string;\\n  systemCollectionUri: string;\\n}) {\\n  const authHandler = pat\\n    ? nodeApi.getPersonalAccessTokenHandler(\\n        pat,\\n        /** allowCrossOriginAuthentication */ true,\\n      )\\n    : nodeApi.getHandlerFromToken(\\n        sat,\\n        /** allowCrossOriginAuthentication */ true,\\n      );\\n\\n  const webApi = new nodeApi.WebApi(systemCollectionUri, authHandler);\\n  const gitApi = await webApi.getGitApi();\\n\\n  return gitApi;\\n}\\n\\nasync function getActivePullRequestId({\\n  gitApi,\\n  config,\\n}: {\\n  gitApi: IGitApi;\\n  config: Config;\\n}) {\\n  const topActivePullRequest = await gitApi.getPullRequests(\\n    config.repository, // repository.id!,\\n    { status: PullRequestStatus.Active },\\n    config.project,\\n    undefined,\\n    /** skip */ 0,\\n    /** top */ 1,\\n  );\\n\\n  return topActivePullRequest.length > 0\\n    ? topActivePullRequest[0].pullRequestId\\n    : undefined;\\n}\\n\\nasync function getPullRequest({\\n  gitApi,\\n  config,\\n  pullRequestId,\\n}: {\\n  gitApi: IGitApi;\\n  config: Config;\\n  pullRequestId: number;\\n}) {\\n  const pullRequest = await gitApi.getPullRequest(\\n    config.repository, // repository.id!,\\n    pullRequestId,\\n    config.project,\\n    undefined,\\n    /** skip */ 0,\\n    /** top */ 1,\\n    /** includeCommits */ false,\\n    /** includeWorkItemRefs */ false,\\n  );\\n  return pullRequest;\\n}\\n\\nasync function updatePullRequestDescription({\\n  gitApi,\\n  config,\\n  pullRequestId,\\n  description,\\n}: {\\n  gitApi: IGitApi;\\n  config: Config;\\n  pullRequestId: number;\\n  description: string;\\n}) {\\n  // To do an update with the API you must provide a new object with only the properties you are updating\\n  const updatePullRequest = {\\n    description,\\n  };\\n  await gitApi.updatePullRequest(\\n    updatePullRequest,\\n    config.repository,\\n    pullRequestId,\\n    config.project,\\n  );\\n}\\n\\nfunction makePreviewDescriptionMarkdown(desc: string, previewUrl: string) {\\n  const previewRegex = /(> -*\\\\n> # Preview:\\\\n.*\\\\n>.*\\\\n> -*\\\\n)/;\\n\\n  const makePreview = (previewUrl: string) => `> ---\\n> # Preview:\\n> ${previewUrl}\\n> \\n> ---\\n`;\\n\\n  const alreadyHasPreview = desc.match(previewRegex);\\n  return alreadyHasPreview\\n    ? desc.replace(previewRegex, makePreview(previewUrl))\\n    : makePreview(previewUrl) + desc;\\n}\\n```\\n\\nThe above code does two things:\\n\\n1. Looks up the pull request, using the details supplied from the pipeline. It\'s worth noting that the `System.PullRequest.PullRequestId` variable is [initialized only if the build ran because of a Git PR affected by a branch policy](https://docs.microsoft.com/en-us/azure/devops/pipelines/build/variables?view=azure-devops&tabs=yaml). If you don\'t have that set up, the script falls back to using the latest active pull request. This is generally useful when you\'re getting set up in the first place; you won\'t want to rely on this behaviour.\\n2. Updates the pull request description with a prefix piece of markdown that provides the link to the preview URL. This is our \\"browse the preview\\":\\n   ![screenshot of rendered markdown with the preview link](screenshot-of-deploy-preview-small.png)\\n\\nThis script could be refactored into a dedicated Azure Pipelines custom task.\\n\\n## Permissions\\n\\nThe first time you run this you may encounter a permissions error of the form:\\n\\n```\\nError: TF401027: You need the Git \'PullRequestContribute\' permission to perform this action.\\n```\\n\\nTo remedy this you need to give your build service the relevant permissions to update a pull request. You can do that by going to the security settings of your repo and setting \\"Contribute to pull requests\\" to \\"Allow\\" for your build service:\\n\\n![Screenshot of \\"Contribute to pull requests\\" permission in Azure DevOps Git security being set to \\"Allow\\" ](screenshot-of-git-repository-security-settings.webp)\\n\\n## Enjoy! (and keep Azure tidy)\\n\\nWhen the pipeline is now run you can see that a deployment preview link is now updated onto the PR description:\\n\\n![Screenshot of deployment preview on PR](screenshot-of-deploy-preview.webp)\\n\\nThis will happen whenever a PR is raised which is tremendous.\\n\\nA thing to remember, is that there\'s nothing in this post that tears down the temporary deployment after the pull request has been merged. It will hang around. We happen to be using free resources in this post, but if we weren\'t there would be cost implications. Either way, you\'ll want to clean up unused environments as a matter of course. And I\'d advise automating that.\\n\\nSo be tidy and cost aware with this approach."},{"id":"typescript-vs-jsdoc-javascript","metadata":{"permalink":"/typescript-vs-jsdoc-javascript","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-11-22-typescript-vs-jsdoc-javascript/index.md","source":"@site/blog/2021-11-22-typescript-vs-jsdoc-javascript/index.md","title":"TypeScript vs JSDoc JavaScript","description":"JSDoc annotations in JavaScript codebase add a new dynamic to the debate between JavaScript and TypeScript. It allows for type checking of JavaScript code.","date":"2021-11-22T00:00:00.000Z","tags":[{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."},{"inline":false,"label":"JSDoc","permalink":"/tags/jsdoc","description":"Type safety through JSDoc annotations."}],"readingTime":5.75,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"typescript-vs-jsdoc-javascript","title":"TypeScript vs JSDoc JavaScript","authors":"johnnyreilly","tags":["javascript","typescript","jsdoc"],"image":"./title-image.png","hide_table_of_contents":false,"description":"JSDoc annotations in JavaScript codebase add a new dynamic to the debate between JavaScript and TypeScript. It allows for type checking of JavaScript code."},"unlisted":false,"prevItem":{"title":"Azure Static Web App Deploy Previews with Azure DevOps","permalink":"/azure-static-web-app-deploy-previews-with-azure-devops"},"nextItem":{"title":"Azure standard availability tests with Bicep","permalink":"/azure-standard-tests-with-bicep"}},"content":"There\'s a debate to be had about whether using JavaScript or TypeScript leads to better outcomes when building a project. The introduction of using JSDoc annotations to type a JavaScript codebase introduces a new dynamic to this discussion. This post will investigate what that looks like, and come to an (opinionated) conclusion.\\n\\n![title image reading \\"JSDoc JavaScript vs TypeScript\\" with a JavaScript logo and TypeScript logo](title-image.png)\\n\\nIf you\'d like to learn more about setting up a codebase to be type checked with JSDoc and TypeScript, then [read this guide](../2024-09-02-typescript-eslint-with-jsdoc-js/index.md).\\n\\n\x3c!--truncate--\x3e\\n\\n## Updated 29th March 2023\\n\\nThis blog evolved to become a talk:\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/5MZoAcheyE4?start=240\\" title=\\"YouTube video player\\" frameBorder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowFullScreen></iframe>\\n\\nSlightly surreally, there\'s an [audiobook version of this post](https://www.youtube.com/watch?v=pj8SoTZbCTE) thanks to ThePrimeagen. Essentially he reads the blog post and says he didn\'t like it. But it made me laugh \uD83D\uDE09:\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/pj8SoTZbCTE\\" title=\\"YouTube video player\\" frameBorder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" allowFullScreen></iframe>\\n\\nIf you\'re looking for a good reference on using JSDoc with TypeScript then [read this guide](https://www.typescriptlang.org/docs/handbook/jsdoc-supported-types.html).\\n\\n## Background\\n\\nIf you\'d talked to me in 2018, I would have solidly recommended using TypeScript, and steering away from JavaScript. The rationale is simple: I\'m exceedingly convinced of the value that static typing provides in terms of productivity / avoiding bugs in production. I appreciate this can be a contentious issue, but that is my settled opinion on the subject. Other opinions are available.\\n\\nTypeScript has long had a good static typing story. JavaScript is dynamically typed and so historically has not. Thanks to TypeScript support for JSDoc, JavaScript can now be statically type checked.\\n\\n## What is JSDoc JavaScript?\\n\\nJSDoc itself actually dates way back to 1999. According to the [Wikipedia entry](https://en.wikipedia.org/wiki/JSDoc):\\n\\n> JSDoc is a markup language used to annotate JavaScript source code files. Using comments containing JSDoc, programmers can add documentation describing the application programming interface of the code they\'re creating.\\n\\nThe TypeScript team have taken JSDoc support and run with it. You can now use a [variant of JSDoc annotations](https://www.typescriptlang.org/docs/handbook/jsdoc-supported-types.html) to provide type information in JavaScript files.\\n\\nWhat does this look like? Well, to take a simple example, a TypeScript statement like so:\\n\\n```ts\\nlet myString: string;\\n```\\n\\nCould become the equivalent JavaScript statement with a JSDoc annotation:\\n\\n```ts\\n/** @type {string} */\\nlet myString;\\n```\\n\\nThis is type enhanced JavaScript which the TypeScript compiler can understand and type check.\\n\\n## Why use JSDoc JavaScript?\\n\\nWhy would you use JSDoc JavaScript instead of TypeScript? Well there\'s a number of possible use cases.\\n\\nPerhaps you\'re writing simple node scripts and you\'d like a little type safety to avoid mistakes. Or perhaps you want to dip your project\'s toe in the waters of static type checking but without fully committing. JSDoc allows for that. Or perhaps your team simply prefers not having a compile step.\\n\\nThat, in fact, was the rationale of the webpack team. A little bit of history: webpack has always been a JavaScript codebase. As the codebase grew and grew, there was often discussion about using static typing. However, having a compilation step wasn\'t desired.\\n\\nTypeScript had been quietly adding support for type checking JavaScript with the assistance of JSDoc for some time. Initial support arrived with the `--checkJs` compiler option in [TypeScript 2.3](https://www.typescriptlang.org/docs/handbook/release-notes/typescript-2-3.html#errors-in-js-files-with---checkjs).\\n\\nA community member by the name of [Mohsen Azimi](https://twitter.com/mohsen____) experimentally started out using this approach to type check the webpack codebase. [His PR](https://github.com/webpack/webpack/pull/6862) ended up being a test case that helped improve the type checking of JavaScript by TypeScript. TypeScript v2.9 shipped with a whole host of JSDoc improvements as a consequence of the webpack work. Being such a widely used project this also helped popularise the approach of using JSDoc to type check JavaScript codebases. It demonstrated that this approach could work on a significantly sized codebase.\\n\\nThese days, JSDoc type checking with TypeScript is extremely powerful. Whilst not quite on par with TypeScript (not all TypeScript syntax is supported in JSDoc) the gap in functionality is pretty small.\\n\\nIt\'s a completely legitimate choice to build a JavaScript codebase with all the benefits of static typing.\\n\\n## Why use TypeScript?\\n\\nSo if you were starting a project today, and you\'d decided you wanted to make use of static typing, how do you choose? TypeScript or JavaScript with JSDoc?\\n\\nWell, unless you\'ve a compelling need to avoid a compilation step, I\'m going to suggest that TypeScript may be the better choice for a number of reasons.\\n\\nFirstly, the tooling support for using TypeScript directly is better than that for JSDoc JavaScript. At the time of writing, things like refactoring tools etc in your editor work more effectively with TypeScript than with JSDoc JavaScript. (Although these are improving as time goes by.)\\n\\nSecondly, working with JSDoc is distinctly \\"noisier\\". It requires far more keystrokes to achieve the same level of type safety. Consider the following TypeScript:\\n\\n```ts\\nfunction stringsStringStrings(\\n  p1: string,\\n  p2?: string,\\n  p3?: string,\\n  p4 = \'test\',\\n): string {\\n  // ...\\n}\\n```\\n\\nAs compared to the equivalent JSDoc JavaScript:\\n\\n```ts\\n/**\\n * @param {string}  p1\\n * @param {string=} p2\\n * @param {string} [p3]\\n * @param {string} [p4=\\"test\\"]\\n * @return {string}\\n */\\nfunction stringsStringStrings(p1, p2, p3, p4) {\\n  // ...\\n}\\n```\\n\\nIt may be my own familiarity with TypeScript speaking, but I find that the TypeScript is easier to read and comprehend as compared to the JSDoc JavaScript alternative. The fact that all JSDoc annotations live in comments, rather than directly in syntax, makes it harder to follow. (It certainly doesn\'t help that many VS Code themes present comments in a very faint colour.)\\n\\nMy final reason for favouring TypeScript comes down to falling into the [\\"pit of success\\"](https://blog.codinghorror.com/falling-into-the-pit-of-success/). You\'re cutting _against_ the grain when it comes to static typing and JavaScript. You can have it, but you have to work that bit harder to ensure that you have statically typed code. On the other hand, you\'re cutting _with_ the grain when it comes to static typing and TypeScript. You have to work hard to opt out of static typing. The TypeScript defaults tend towards static typing, whilst the JavaScript defaults tend away.\\n\\nAs someone who very much favours static typing, you can imagine how this is compelling to me!\\n\\n## It\'s your choice!\\n\\nSo in a way, I don\'t feel super strongly whether people use JavaScript or TypeScript. But having static typing will likely be a benefit to new projects. Bottom line, I\'m keen that people fall into the \\"pit of success\\", so my recommendation for a new project would be TypeScript.\\n\\nI really like JSDoc myself, and will often use it on small projects. It\'s a fantastic addition to TypeScript\'s capabilities. For bigger projects, I\'ll likely go with TypeScript from the get go. But really, this is a choice - and either is great.\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/typescript-vs-jsdoc-javascript/)\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.logrocket.com/typescript-vs-jsdoc-javascript/\\" />\\n</head>"},{"id":"azure-standard-tests-with-bicep","metadata":{"permalink":"/azure-standard-tests-with-bicep","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-11-18-azure-standard-tests-with-bicep/index.md","source":"@site/blog/2021-11-18-azure-standard-tests-with-bicep/index.md","title":"Azure standard availability tests with Bicep","description":"Learn how to deploy Azure standard tests using Bicep! This post goes through the process and includes a complete code snippet.","date":"2021-11-18T00:00:00.000Z","tags":[{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."},{"inline":false,"label":"Bicep","permalink":"/tags/bicep","description":"The Bicep language for Azure Resource Manager templates."}],"readingTime":5.75,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-standard-tests-with-bicep","title":"Azure standard availability tests with Bicep","authors":"johnnyreilly","tags":["azure","bicep"],"image":"./title-image.png","hide_table_of_contents":false,"description":"Learn how to deploy Azure standard tests using Bicep! This post goes through the process and includes a complete code snippet."},"unlisted":false,"prevItem":{"title":"TypeScript vs JSDoc JavaScript","permalink":"/typescript-vs-jsdoc-javascript"},"nextItem":{"title":"NSwag generated C# client: Open API property name clashes and decimal types rather than double","permalink":"/nswag-generated-c-sharp-client-property-name-clash"}},"content":"Azure standard tests are a tremendous way to monitor the uptime of your services in Azure. Sometimes also called availability tests, web tests and ping tests, this post goes through how to deploy one using Bicep. It also looks at some of the gotchas that you may encounter as you\'re setting it up.\\n\\n![title image reading \\"Azure standard availability tests with Bicep\\" with a Bicep logo and Azure logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## What are standard tests?\\n\\nTo quote the [docs](https://docs.microsoft.com/en-us/azure/azure-monitor/app/availability-standard-tests):\\n\\n> Standard tests are a single request test that is similar to the URL ping test but more advanced. In addition to validating whether an endpoint is responding and measuring the performance, Standard tests also includes SSL certificate validity, proactive lifetime check, HTTP request verb (for example GET,HEAD,POST, etc.), custom headers, and custom data associated with your HTTP request.\\n\\nSo we can use these to:\\n\\n- send requests to a URL\\n- from a variety of geographic locations\\n- and determine if it is responding with a 200 status code\\n\\nThe URL may be one of our own service URLs, but it could be checking any kind of URL. It\'s web specific, not Azure specific.\\n\\n## Standard test Bicep\\n\\nNow we\'re going to write a Bicep module that provisions a standard test named `standard-test.bicep`:\\n\\n```bicep\\n@description(\'Tags that our resources need\')\\nparam tags object\\n\\n@description(\'The resource id of the app insights which the webtest will reference\')\\nparam appInsightsResourceId string\\n\\n@description(\'The name of the webtest to create\')\\nparam standardTestName string\\n\\n@description(\'URL to test\')\\nparam urlToTest string\\n\\n@description(\'Interval in seconds between test runs for this WebTest. Default value is 300.\')\\nparam frequency int = 300\\n\\n@description(\'Seconds until this WebTest will timeout and fail. Default value is 30.\')\\nparam timeout int = 30\\n\\n// useful reference:\\n// https://docs.microsoft.com/en-us/azure/azure-monitor/app/monitor-web-app-availability#azure\\n@allowed([\\n  \'emea-au-syd-edge\' // Australia\u202FEast\\n  \'latam-br-gru-edge\' // Brazil South\\n  \'us-fl-mia-edge\' // Central US\\n  \'apac-hk-hkn-azr\' // East Asia\\n  \'us-va-ash-azr\' // East US\\n  \'emea-ch-zrh-edge\' // France South (Formerly France Central)\\n  \'emea-fr-pra-edge\' // France Central\\n  \'apac-jp-kaw-edge\' // Japan East\\n  \'emea-gb-db3-azr\' // North Europe\\n  \'us-il-ch1-azr\' // North Central US\\n  \'us-tx-sn1-azr\' // South Central US\\n  \'apac-sg-sin-azr\' // Southeast Asia\\n  \'emea-se-sto-edge\' // UK West\\n  \'emea-nl-ams-azr\' // West Europe\\n  \'us-ca-sjc-azr\' // West US\\n  \'emea-ru-msa-edge\' // UK South\\n])\\n@description(\'The populations (locations) for the test\')\\nparam testPopulations array = [\\n  \'emea-se-sto-edge\' // UK West\\n  \'emea-ru-msa-edge\' // UK South\\n  \'emea-gb-db3-azr\' // North Europe\\n  \'us-va-ash-azr\' // East US\\n  \'apac-sg-sin-azr\' // Southeast Asia\\n]\\n\\nvar tagsWithHiddenLink = union({\\n  \'hidden-link:${appInsightsResourceId}\': \'Resource\'\\n}, tags)\\n\\nresource standardWebTest \'Microsoft.Insights/webtests@2018-05-01-preview\' = {\\n  name: standardTestName\\n  location: resourceGroup().location\\n  tags: tagsWithHiddenLink\\n  kind: \'ping\'\\n  properties: {\\n    SyntheticMonitorId: urlToTest\\n    Name: urlToTest\\n    Description: null\\n    Enabled: true\\n    Frequency: frequency\\n    Timeout: timeout\\n    Kind: \'standard\'\\n    RetryEnabled: true\\n    Locations: [for testPopulation in testPopulations: {\\n      Id: testPopulation\\n    }]\\n    Configuration: null\\n    Request: {\\n      RequestUrl: urlToTest\\n      Headers: null\\n      HttpVerb: \'GET\'\\n      RequestBody: null\\n      ParseDependentRequests: false\\n      FollowRedirects: null\\n    }\\n    ValidationRules: {\\n      ExpectedHttpStatusCode: 200\\n      IgnoreHttpsStatusCode: false\\n      ContentValidation: null\\n      SSLCheck: true\\n      SSLCertRemainingLifetimeCheck: 7\\n    }\\n  }\\n}\\n\\noutput standardWebTestName string = standardWebTest.name\\noutput standardWebTestId string = standardWebTest.id\\n```\\n\\n### Locations / populations\\n\\nYou\'ll note that a parameter to the Bicep module is `testPopulations`. These are the geographical places where requests will be sent from. You\'ll note we have a default value of five populations, but these could be any of the (presently) sixteen valid values. If you were wondering where those are sourced from, [here is the link to the Azure docs](https://docs.microsoft.com/en-us/azure/azure-monitor/app/availability-standard-tests#location-population-tags).\\n\\n### The `hidden-link` tag\\n\\nAnother significant call out should go to the `hidden-link` tag. The `hidden-link` tag is a mandatory tag that connects the test (known in Azure as a \\"webtest\\") to an app insights instance.\\n\\nIf you do not provide a `hidden-link` tag, or if you try to specify a resource group other than the app insights resource group, Azure will fail to deploy your test and you may find yourself presented with an error like this in the deployments section of the Azure Portal.\\n\\n> Resource should exist in the same resource group as the linked component\\n\\n![screenshot of the Azure Portal Deployments section saying \\"Resource should exist in the same resource group as the linked component\\"](screenshot-azure-portal-deployments-resource-should-exist-in-the-same-resource-group.webp)\\n\\nIn our module we set both the `hidden-link` tag as well as the tags that have been supplied via the `tags` parameter.\\n\\n### App insights and standard tests share a resource group\\n\\nAnother thing that can cause issues is the deployment of your app insights resource. It\'s not unusual to spin up Azure resources on demand, for a given branch of your source code. Those resources will be named in relation to the branch and will depend upon one another. I\'ve never managed to successfully create an app insights resource, and reference it from a standard test within the same Bicep file. It appears to be necessary to separate the two actions, such that Azure recognises the existence of the app insights resource when the standard test is deployed.\\n\\nIf you are working with long-lived app insights it won\'t be an issue for you, but if you aren\'t it\'s worth being aware of.\\n\\n## Using `standard-test.bicep`\\n\\nOur Bicep module can be invoked from another Bicep module named `ping-them.bicep` like so:\\n\\n```bicep\\n@description(\'Tags that our resources need\')\\nparam tags object\\n\\n@description(\'The name of the app insights\')\\nparam appInsightsName string\\n\\n@description(\'An object where the keys are the name of the web test and the values are the URL eg {\\"my-standard-test\\": \\"https://status.azure.com/en-gb/status\\"} \')\\nparam standardTests object\\n\\nvar appInsightsResourceId = resourceId(\'Microsoft.Insights/components\', appInsightsName)\\n\\nmodule standardTestsToCreate \'standard-test.bicep\' = [for standardTest in items(standardTests): {\\n  name: standardTest.key\\n  params: {\\n    tags: tags\\n    appInsightsResourceId: appInsightsResourceId\\n    standardTestName: standardTest.key\\n    urlToTest: standardTest.value\\n  }\\n}]\\n```\\n\\nAs you can see, this module itself takes a number of parameters, and will typically be invoked from some kind of continuous integration mechanism such as Azure Pipelines or GitHub Actions.\\n\\nThis module is written in the expectation that multiple URLs will need to be pinged, and so it has a parameter named `standardTests` which is effectively a dictionary of key-value pairs, where the key is the name of the standard test, and the value is the URL to test.\\n\\nThe module makes use of the [`items`](https://docs.microsoft.com/en-us/azure/azure-resource-manager/bicep/bicep-functions-array#items) array helper in Bicep to convert the object into an array that can be iterated over.\\n\\n## Azure Pipelines test\\n\\nWe\'re going to use Azure Pipelines to test this out. Here\'s an `azure-pipelines.yml` file:\\n\\n```yml\\ntrigger:\\n  - main\\n\\npool:\\n  vmImage: ubuntu-latest\\n\\nsteps:\\n  - checkout: self\\n    submodules: true\\n\\n  - bash: az bicep build --file ping-them.bicep\\n    displayName: \'Compile Bicep to ARM\'\\n\\n  - task: AzureResourceManagerTemplateDeployment@3\\n    name: DeploySharedWebTests\\n    displayName: Deploy Shared Web Tests\\n    inputs:\\n      deploymentScope: Resource Group\\n      azureResourceManagerConnection: ${{ variables.serviceConnection }}\\n      subscriptionId: $(subscriptionId)\\n      action: Create Or Update Resource Group\\n      resourceGroupName: $(resourceGroup)\\n      location: $(location)\\n      templateLocation: Linked artifact\\n      csmFile: \'ping-them.json\' # created by bash script\\n      overrideParameters: >-\\n        -tags {\\"owner\\": \\"@johnny_reilly\\", \\"branch\\": \\"$(Build.SourceBranchName)\\"}\\n        -appInsightsName $(appInsightsName)\\n        -standardTests {\\"my-standard-test\\": \\"https://status.azure.com/en-gb/status\\"}\\n      deploymentMode: Incremental\\n```\\n\\nWhen run, it invokes our `ping-them.bicep` module, passing two URLs to test.\\n\\nWhen executed, you end up with a delightful \\"availability test\\" (which is your standard test) in Azure:\\n\\n![screenshot of an Availability test in the Azure Portal](screenshot-azure-portal-availability.png)"},{"id":"nswag-generated-c-sharp-client-property-name-clash","metadata":{"permalink":"/nswag-generated-c-sharp-client-property-name-clash","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-10-31-nswag-generated-c-sharp-client-property-name-clash/index.md","source":"@site/blog/2021-10-31-nswag-generated-c-sharp-client-property-name-clash/index.md","title":"NSwag generated C# client: Open API property name clashes and decimal types rather than double","description":"Generate C# and TypeScript client libraries from OpenAPI / Swagger definitions using NSwag while overcoming language conflicts and numeric types.","date":"2021-10-31T00:00:00.000Z","tags":[{"inline":false,"label":"C#","permalink":"/tags/csharp","description":"The C# programming language."},{"inline":false,"label":"Swagger","permalink":"/tags/swagger","description":"The Swagger API documentation framework - now known as OpenAPI."}],"readingTime":10.58,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"nswag-generated-c-sharp-client-property-name-clash","title":"NSwag generated C# client: Open API property name clashes and decimal types rather than double","authors":"johnnyreilly","tags":["c#","swagger"],"image":"./title-image.png","hide_table_of_contents":false,"description":"Generate C# and TypeScript client libraries from OpenAPI / Swagger definitions using NSwag while overcoming language conflicts and numeric types."},"unlisted":false,"prevItem":{"title":"Azure standard availability tests with Bicep","permalink":"/azure-standard-tests-with-bicep"},"nextItem":{"title":"Docusaurus, meta tags and Google Discover","permalink":"/docusaurus-meta-tags-and-google-discover"}},"content":"NSwag is a great tool for generating client libraries in C# and TypeScript from Open API / Swagger definitions. You can face issues where Open API property names collide due to the nature of the C# language, and when you want to use `decimal` for your floating point numeric type over `double`. This post demonstrates how to get over both issues.\\n\\n![title image reading \\"NSwag generated C# client: Open API property name clashes and decimal types rather than double\\" with a C# logo and Open API logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Make a C# Client Generator\\n\\nLet\'s get a console app set up that will allow us to generate a C# client using an Open API file:\\n\\n```sh\\ndotnet new console -o NSwag\\ncd NSwag\\ndotnet add package NSwag.CodeGeneration.CSharp\\n```\\n\\nWe\'ll also add a `petstore-simple.json` file to our project which we\'ll borrow from https://github.com/OAI/OpenAPI-Specification/blob/main/examples/v2.0/json/petstore-simple.json (home of the Open API specification):\\n\\n```json\\n{\\n  \\"swagger\\": \\"2.0\\",\\n  \\"info\\": {\\n    \\"version\\": \\"1.0.0\\",\\n    \\"title\\": \\"Swagger Petstore\\",\\n    \\"description\\": \\"A sample API that uses a petstore as an example to demonstrate features in the swagger-2.0 specification\\",\\n    \\"termsOfService\\": \\"http://swagger.io/terms/\\",\\n    \\"contact\\": {\\n      \\"name\\": \\"Swagger API Team\\"\\n    },\\n    \\"license\\": {\\n      \\"name\\": \\"MIT\\"\\n    }\\n  },\\n  \\"host\\": \\"petstore.swagger.io\\",\\n  \\"basePath\\": \\"/api\\",\\n  \\"schemes\\": [\\"http\\"],\\n  \\"consumes\\": [\\"application/json\\"],\\n  \\"produces\\": [\\"application/json\\"],\\n  \\"paths\\": {\\n    \\"/pets\\": {\\n      \\"get\\": {\\n        \\"description\\": \\"Returns all pets from the system that the user has access to\\",\\n        \\"operationId\\": \\"findPets\\",\\n        \\"produces\\": [\\n          \\"application/json\\",\\n          \\"application/xml\\",\\n          \\"text/xml\\",\\n          \\"text/html\\"\\n        ],\\n        \\"parameters\\": [\\n          {\\n            \\"name\\": \\"tags\\",\\n            \\"in\\": \\"query\\",\\n            \\"description\\": \\"tags to filter by\\",\\n            \\"required\\": false,\\n            \\"type\\": \\"array\\",\\n            \\"items\\": {\\n              \\"type\\": \\"string\\"\\n            },\\n            \\"collectionFormat\\": \\"csv\\"\\n          },\\n          {\\n            \\"name\\": \\"limit\\",\\n            \\"in\\": \\"query\\",\\n            \\"description\\": \\"maximum number of results to return\\",\\n            \\"required\\": false,\\n            \\"type\\": \\"integer\\",\\n            \\"format\\": \\"int32\\"\\n          }\\n        ],\\n        \\"responses\\": {\\n          \\"200\\": {\\n            \\"description\\": \\"pet response\\",\\n            \\"schema\\": {\\n              \\"type\\": \\"array\\",\\n              \\"items\\": {\\n                \\"$ref\\": \\"#/definitions/Pet\\"\\n              }\\n            }\\n          },\\n          \\"default\\": {\\n            \\"description\\": \\"unexpected error\\",\\n            \\"schema\\": {\\n              \\"$ref\\": \\"#/definitions/ErrorModel\\"\\n            }\\n          }\\n        }\\n      },\\n      \\"post\\": {\\n        \\"description\\": \\"Creates a new pet in the store.  Duplicates are allowed\\",\\n        \\"operationId\\": \\"addPet\\",\\n        \\"produces\\": [\\"application/json\\"],\\n        \\"parameters\\": [\\n          {\\n            \\"name\\": \\"pet\\",\\n            \\"in\\": \\"body\\",\\n            \\"description\\": \\"Pet to add to the store\\",\\n            \\"required\\": true,\\n            \\"schema\\": {\\n              \\"$ref\\": \\"#/definitions/NewPet\\"\\n            }\\n          }\\n        ],\\n        \\"responses\\": {\\n          \\"200\\": {\\n            \\"description\\": \\"pet response\\",\\n            \\"schema\\": {\\n              \\"$ref\\": \\"#/definitions/Pet\\"\\n            }\\n          },\\n          \\"default\\": {\\n            \\"description\\": \\"unexpected error\\",\\n            \\"schema\\": {\\n              \\"$ref\\": \\"#/definitions/ErrorModel\\"\\n            }\\n          }\\n        }\\n      }\\n    },\\n    \\"/pets/{id}\\": {\\n      \\"get\\": {\\n        \\"description\\": \\"Returns a user based on a single ID, if the user does not have access to the pet\\",\\n        \\"operationId\\": \\"findPetById\\",\\n        \\"produces\\": [\\n          \\"application/json\\",\\n          \\"application/xml\\",\\n          \\"text/xml\\",\\n          \\"text/html\\"\\n        ],\\n        \\"parameters\\": [\\n          {\\n            \\"name\\": \\"id\\",\\n            \\"in\\": \\"path\\",\\n            \\"description\\": \\"ID of pet to fetch\\",\\n            \\"required\\": true,\\n            \\"type\\": \\"integer\\",\\n            \\"format\\": \\"int64\\"\\n          }\\n        ],\\n        \\"responses\\": {\\n          \\"200\\": {\\n            \\"description\\": \\"pet response\\",\\n            \\"schema\\": {\\n              \\"$ref\\": \\"#/definitions/Pet\\"\\n            }\\n          },\\n          \\"default\\": {\\n            \\"description\\": \\"unexpected error\\",\\n            \\"schema\\": {\\n              \\"$ref\\": \\"#/definitions/ErrorModel\\"\\n            }\\n          }\\n        }\\n      },\\n      \\"delete\\": {\\n        \\"description\\": \\"deletes a single pet based on the ID supplied\\",\\n        \\"operationId\\": \\"deletePet\\",\\n        \\"parameters\\": [\\n          {\\n            \\"name\\": \\"id\\",\\n            \\"in\\": \\"path\\",\\n            \\"description\\": \\"ID of pet to delete\\",\\n            \\"required\\": true,\\n            \\"type\\": \\"integer\\",\\n            \\"format\\": \\"int64\\"\\n          }\\n        ],\\n        \\"responses\\": {\\n          \\"204\\": {\\n            \\"description\\": \\"pet deleted\\"\\n          },\\n          \\"default\\": {\\n            \\"description\\": \\"unexpected error\\",\\n            \\"schema\\": {\\n              \\"$ref\\": \\"#/definitions/ErrorModel\\"\\n            }\\n          }\\n        }\\n      }\\n    }\\n  },\\n  \\"definitions\\": {\\n    \\"Pet\\": {\\n      \\"type\\": \\"object\\",\\n      \\"allOf\\": [\\n        {\\n          \\"$ref\\": \\"#/definitions/NewPet\\"\\n        },\\n        {\\n          \\"required\\": [\\"id\\"],\\n          \\"properties\\": {\\n            \\"id\\": {\\n              \\"type\\": \\"integer\\",\\n              \\"format\\": \\"int64\\"\\n            }\\n          }\\n        }\\n      ]\\n    },\\n    \\"NewPet\\": {\\n      \\"type\\": \\"object\\",\\n      \\"required\\": [\\"name\\"],\\n      \\"properties\\": {\\n        \\"name\\": {\\n          \\"type\\": \\"string\\"\\n        },\\n        \\"tag\\": {\\n          \\"type\\": \\"string\\"\\n        }\\n      }\\n    },\\n    \\"ErrorModel\\": {\\n      \\"type\\": \\"object\\",\\n      \\"required\\": [\\"code\\", \\"message\\"],\\n      \\"properties\\": {\\n        \\"code\\": {\\n          \\"type\\": \\"integer\\",\\n          \\"format\\": \\"int32\\"\\n        },\\n        \\"message\\": {\\n          \\"type\\": \\"string\\"\\n        }\\n      }\\n    }\\n  }\\n}\\n```\\n\\nWe\'ll tweak our `NSwag.csproj` file to ensure that the `json` file is included in our build output:\\n\\n```xml\\n<Project Sdk=\\"Microsoft.NET.Sdk\\">\\n  \x3c!-- ... ---\x3e\\n  <ItemGroup>\\n    <Content Include=\\"**\\\\*.json\\">\\n      <CopyToOutputDirectory>PreserveNewest</CopyToOutputDirectory>\\n    </Content>\\n  </ItemGroup>\\n</Project>\\n```\\n\\nThis will give us a console app with a reference to NSwag. Now we\'ll flesh out the `Program.cs` file thusly:\\n\\n```cs\\nusing System;\\nusing System.IO;\\nusing System.Reflection;\\nusing System.Threading.Tasks;\\nusing NJsonSchema;\\nusing NJsonSchema.Visitors;\\nusing NSwag.CodeGeneration.CSharp;\\n\\nnamespace NSwag {\\n    class Program {\\n        static async Task Main(string[] args) {\\n            Console.WriteLine(\\"Generating client...\\");\\n            await ClientGenerator.GenerateCSharpClient();\\n            Console.WriteLine(\\"Generated client.\\");\\n        }\\n    }\\n\\n    public static class ClientGenerator {\\n\\n        public async static Task GenerateCSharpClient() =>\\n            GenerateClient(\\n                // https://github.com/OAI/OpenAPI-Specification/blob/main/examples/v2.0/json/petstore-simple.json\\n                document: await GetDocumentFromFile(\\"petstore-simple.json\\"),\\n                generatedLocation: \\"GeneratedClient.cs\\",\\n                generateCode: (OpenApiDocument document) => {\\n                    var settings = new CSharpClientGeneratorSettings();\\n\\n                    var generator = new CSharpClientGenerator(document, settings);\\n                    var code = generator.GenerateFile();\\n                    return code;\\n                }\\n            );\\n\\n        private static void GenerateClient(OpenApiDocument document, string generatedLocation, Func<OpenApiDocument, string> generateCode) {\\n            var root = Path.GetDirectoryName(Assembly.GetEntryAssembly().Location);\\n            var location = Path.GetFullPath(Path.Join(root, @\\"../../../\\", generatedLocation));\\n\\n            Console.WriteLine($\\"Generating {location}...\\");\\n\\n            var code = generateCode(document);\\n\\n            System.IO.File.WriteAllText(location, code);\\n        }\\n\\n        private static async Task<OpenApiDocument> GetDocumentFromFile(string swaggerJsonFilePath) {\\n            var root = Path.GetDirectoryName(Assembly.GetEntryAssembly().Location);\\n            var swaggerJson = await File.ReadAllTextAsync(Path.GetFullPath(Path.Join(root, swaggerJsonFilePath)));\\n            var document = await OpenApiDocument.FromJsonAsync(swaggerJson);\\n\\n            return document;\\n        }\\n    }\\n}\\n```\\n\\nIf we perform a `dotnet run` we now pump out a `GeneratedClient.cs` file which is a C# client library for the pet store. Fabulous.\\n\\nSo far so dandy. We\'re taking an Open API `json` file and generating a C# client library from it.\\n\\n## When properties collide\\n\\nIt\'s time to break things. We\'re presently generating a `Pet` class that looks like this:\\n\\n```cs\\n[System.CodeDom.Compiler.GeneratedCode(\\"NJsonSchema\\", \\"10.5.2.0 (Newtonsoft.Json v13.0.0.0)\\")]\\npublic partial class Pet : NewPet\\n{\\n    [Newtonsoft.Json.JsonProperty(\\"id\\", Required = Newtonsoft.Json.Required.Always)]\\n    public long Id { get; set; }\\n}\\n```\\n\\nWe\'re going to take our `Pet` definition in the `petstore-simple.json` file, and add a new `@id` property alongside the `id` property:\\n\\n```json\\n\\"Pet\\": {\\n    \\"type\\": \\"object\\",\\n    \\"allOf\\": [\\n        {\\n            \\"$ref\\": \\"#/definitions/NewPet\\"\\n        },\\n        {\\n            \\"required\\": [\\n                \\"id\\"\\n            ],\\n            \\"properties\\": {\\n                \\"id\\": {\\n                    \\"type\\": \\"integer\\",\\n                    \\"format\\": \\"int64\\"\\n                },\\n                \\"@id\\": {\\n                    \\"type\\": \\"integer\\",\\n                    \\"format\\": \\"int64\\"\\n                }\\n            }\\n        }\\n    ]\\n},\\n```\\n\\nFor why? Whilst this may seem esoteric, this is a scenario that can present. It\'s not unknown to encounter properties which are identical, save for an `@` prefix. This is often the case for meta-properties.\\n\\nWhat do we get if we run our generator over that?\\n\\n```cs\\n[System.CodeDom.Compiler.GeneratedCode(\\"NJsonSchema\\", \\"10.5.2.0 (Newtonsoft.Json v13.0.0.0)\\")]\\npublic partial class Pet : NewPet\\n{\\n    [Newtonsoft.Json.JsonProperty(\\"id\\", Required = Newtonsoft.Json.Required.Always)]\\n    public long Id { get; set; }\\n\\n    [Newtonsoft.Json.JsonProperty(\\"@id\\", Required = Newtonsoft.Json.Required.Always)]\\n    public long Id { get; set; }\\n}\\n```\\n\\nWe get code that doesn\'t compile. You can\'t have two properties in a C# class with the same name. You also cannot have `@` as a character in a C# property or variable name. To quote the [docs](https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/tokens/verbatim):\\n\\n> The @ special character serves as a verbatim identifier.\\n\\nIt so happens that, by default, NSwag purges `@` characters from property names. If there isn\'t another property which is named the same save for an `@` prefix, this is a fine strategy. If there is, as for us now, you\'re toast.\\n\\nThere\'s a workaround. We\'ll create a new `HandleAtCSharpPropertyNameGenerator` class:\\n\\n```cs\\n/// <summary>\\n/// Replace characters which will not comply with C# syntax with something that will\\n/// </summary>\\npublic class HandleAtCSharpPropertyNameGenerator : NJsonSchema.CodeGeneration.IPropertyNameGenerator {\\n    /// <summary>Generates the property name.</summary>\\n    /// <param name=\\"property\\">The property.</param>\\n    /// <returns>The new name.</returns>\\n    public virtual string Generate(JsonSchemaProperty property) =>\\n        ConversionUtilities.ConvertToUpperCamelCase(property.Name\\n            .Replace(\\"\\\\\\"\\", string.Empty)\\n            .Replace(\\"@\\", \\"__\\") // make \\"@\\" => \\"__\\", so \\"@type\\" => \\"__type\\"\\n            .Replace(\\"?\\", string.Empty)\\n            .Replace(\\"$\\", string.Empty)\\n            .Replace(\\"[\\", string.Empty)\\n            .Replace(\\"]\\", string.Empty)\\n            .Replace(\\"(\\", \\"_\\")\\n            .Replace(\\")\\", string.Empty)\\n            .Replace(\\".\\", \\"-\\")\\n            .Replace(\\"=\\", \\"-\\")\\n            .Replace(\\"+\\", \\"plus\\"), true)\\n            .Replace(\\"*\\", \\"Star\\")\\n            .Replace(\\":\\", \\"_\\")\\n            .Replace(\\"-\\", \\"_\\")\\n            .Replace(\\"#\\", \\"_\\");\\n}\\n```\\n\\nThis is a replacement for the `CSharpPropertyNameGenerator` that NSwag ships with. Rather than purging the `@` character, it replaces usage with a double underscore: `__`.\\n\\nWe\'ll make use of our new `PropertyNameGenerator`:\\n\\n```cs\\npublic async static Task GenerateCSharpClient() =>\\n    GenerateClient(\\n        // https://github.com/OAI/OpenAPI-Specification/blob/main/examples/v2.0/json/petstore-simple.json\\n        document: await GetDocumentFromFile(\\"petstore-simple.json\\"),\\n        generatedLocation: \\"GeneratedClient.cs\\",\\n        generateCode: (OpenApiDocument document) => {\\n            var settings = new CSharpClientGeneratorSettings {\\n                CSharpGeneratorSettings = {\\n                    PropertyNameGenerator = new HandleAtCSharpPropertyNameGenerator() // @ shouldn\'t cause us problems\\n                }\\n            };\\n\\n            var generator = new CSharpClientGenerator(document, settings);\\n            var code = generator.GenerateFile();\\n            return code;\\n        }\\n    );\\n```\\n\\nWith this in place, when we `dotnet run` we create a class that looks like this:\\n\\n```cs\\n[System.CodeDom.Compiler.GeneratedCode(\\"NJsonSchema\\", \\"10.5.2.0 (Newtonsoft.Json v13.0.0.0)\\")]\\npublic partial class Pet : NewPet\\n{\\n    [Newtonsoft.Json.JsonProperty(\\"id\\", Required = Newtonsoft.Json.Required.Always)]\\n    public long Id { get; set; }\\n\\n    [Newtonsoft.Json.JsonProperty(\\"@id\\", Required = Newtonsoft.Json.Required.Always)]\\n    public long __id { get; set; }\\n}\\n```\\n\\nSo the newly generated property name is `__id` rather than the clashing `Id`. Rather wonderfully, this works. It resolves the issue we faced. We\'ve chosen to use `__` as our prefix - we could choose something else if that worked better for us.\\n\\nKnowing that this hook exists is super useful.\\n\\n## Use `decimal` not `double` for floating point numbers\\n\\nAnother common problem with generated C# clients is the number type used to represent floating point numbers. The default for C# is `double`.\\n\\nThis is a reasonable choice when you consider the [official format](https://swagger.io/docs/specification/data-models/data-types/#numbers) for highly precise floating point numbers is `double`:\\n\\n> OpenAPI has two numeric types, `number` and `integer`, where `number` includes both integer and floating-point numbers. An optional `format` keyword serves as a hint for the tools to use a specific numeric type:\\n>\\n> `float` - Floating-point numbers.\\n> `double` - Floating-point numbers with double precision.\\n\\nLet\'s tweak our pet definition to reflect this:\\n\\n```json\\n\\"Pet\\": {\\n    \\"type\\": \\"object\\",\\n    \\"allOf\\": [\\n        {\\n            \\"$ref\\": \\"#/definitions/NewPet\\"\\n        },\\n        {\\n            \\"required\\": [\\n                \\"id\\"\\n            ],\\n            \\"properties\\": {\\n                \\"id\\": {\\n                    \\"type\\": \\"number\\",\\n                    \\"format\\": \\"double\\"\\n                },\\n                \\"@id\\": {\\n                    \\"type\\": \\"number\\",\\n                    \\"format\\": \\"double\\"\\n                }\\n            }\\n        }\\n    ]\\n},\\n```\\n\\nWith this in place, when we `dotnet run` we create a class that looks like this:\\n\\n```cs\\n[System.CodeDom.Compiler.GeneratedCode(\\"NJsonSchema\\", \\"10.5.2.0 (Newtonsoft.Json v13.0.0.0)\\")]\\npublic partial class Pet : NewPet\\n{\\n    [Newtonsoft.Json.JsonProperty(\\"id\\", Required = Newtonsoft.Json.Required.Always)]\\n    public double Id { get; set; }\\n\\n    [Newtonsoft.Json.JsonProperty(\\"@id\\", Required = Newtonsoft.Json.Required.Always)]\\n    public double __id { get; set; }\\n}\\n```\\n\\nC# developers may well rather work with a [`decimal`](https://docs.microsoft.com/en-us/dotnet/api/system.decimal?view=net-5.0) type which can handle \\"financial calculations that require large numbers of significant integral and fractional digits and no round-off errors\\".\\n\\nThere is a way to switch from using `double` to `decimal` in your generated clients. I\'ve been using the approach for some years, and I suspect I first adapted it from [a comment on GitHub](https://github.com/RicoSuter/NSwag/issues/1814#issuecomment-448752684).\\n\\nIt uses the [visitor pattern](https://en.m.wikipedia.org/wiki/Visitor_pattern) and looks like this:\\n\\n```cs\\n/// <summary>\\n/// By default the C# decimal number type used is double; this makes it decimal\\n/// </summary>\\npublic class DoubleToDecimalVisitor : JsonSchemaVisitorBase {\\n    protected override JsonSchema VisitSchema(JsonSchema schema, string path, string typeNameHint) {\\n        if (schema.Type == JsonObjectType.Number)\\n            schema.Format = JsonFormatStrings.Decimal;\\n\\n        return schema;\\n    }\\n}\\n```\\n\\nThe code above, when invoked upon our `OpenApiDocument`, changes the format of all number types to be `decimal`. Which results in code along these lines:\\n\\n```cs\\n[System.CodeDom.Compiler.GeneratedCode(\\"NJsonSchema\\", \\"10.5.2.0 (Newtonsoft.Json v13.0.0.0)\\")]\\npublic partial class Pet : NewPet\\n{\\n    [Newtonsoft.Json.JsonProperty(\\"id\\", Required = Newtonsoft.Json.Required.Always)]\\n    public decimal Id { get; set; }\\n\\n    [Newtonsoft.Json.JsonProperty(\\"@id\\", Required = Newtonsoft.Json.Required.Always)]\\n    public decimal __id { get; set; }\\n}\\n```\\n\\nIf we take all the code, and put it together, we end up with this:\\n\\n```cs\\nusing System;\\nusing System.IO;\\nusing System.Reflection;\\nusing System.Threading.Tasks;\\nusing NJsonSchema;\\nusing NJsonSchema.Visitors;\\nusing NSwag.CodeGeneration.CSharp;\\n\\nnamespace NSwag {\\n    class Program {\\n        static async Task Main(string[] args) {\\n            Console.WriteLine(\\"Generating client...\\");\\n            await ClientGenerator.GenerateCSharpClient();\\n            Console.WriteLine(\\"Generated client.\\");\\n        }\\n    }\\n\\n    public static class ClientGenerator {\\n\\n        public async static Task GenerateCSharpClient() =>\\n            GenerateClient(\\n                // https://github.com/OAI/OpenAPI-Specification/blob/main/examples/v2.0/json/petstore-simple.json\\n                document: await GetDocumentFromFile(\\"petstore-simple.json\\"),\\n                generatedLocation: \\"GeneratedClient.cs\\",\\n                generateCode: (OpenApiDocument document) => {\\n                    new DoubleToDecimalVisitor().Visit(document); // we want decimals not doubles\\n\\n                    var settings = new CSharpClientGeneratorSettings {\\n                        CSharpGeneratorSettings = {\\n                            PropertyNameGenerator = new HandleAtCSharpPropertyNameGenerator() // @ shouldn\'t cause us problems\\n                        }\\n                    };\\n\\n                    var generator = new CSharpClientGenerator(document, settings);\\n                    var code = generator.GenerateFile();\\n                    return code;\\n                }\\n            );\\n\\n        private static void GenerateClient(OpenApiDocument document, string generatedLocation, Func<OpenApiDocument, string> generateCode) {\\n            var root = Path.GetDirectoryName(Assembly.GetEntryAssembly().Location);\\n            var location = Path.GetFullPath(Path.Join(root, @\\"../../../\\", generatedLocation));\\n\\n            Console.WriteLine($\\"Generating {location}...\\");\\n\\n            var code = generateCode(document);\\n\\n            System.IO.File.WriteAllText(location, code);\\n        }\\n\\n        private static async Task<OpenApiDocument> GetDocumentFromFile(string swaggerJsonFilePath) {\\n            var root = Path.GetDirectoryName(Assembly.GetEntryAssembly().Location);\\n            var swaggerJson = await File.ReadAllTextAsync(Path.GetFullPath(Path.Join(root, swaggerJsonFilePath)));\\n            var document = await OpenApiDocument.FromJsonAsync(swaggerJson);\\n\\n            return document;\\n        }\\n    }\\n\\n    /// <summary>\\n    /// By default the C# decimal number type used is double; this makes it decimal\\n    /// </summary>\\n    public class DoubleToDecimalVisitor : JsonSchemaVisitorBase {\\n        protected override JsonSchema VisitSchema(JsonSchema schema, string path, string typeNameHint) {\\n            if (schema.Type == JsonObjectType.Number)\\n                schema.Format = JsonFormatStrings.Decimal;\\n\\n            return schema;\\n        }\\n    }\\n\\n    /// <summary>\\n    /// Replace characters which will not comply with C# syntax with something that will\\n    /// </summary>\\n    public class HandleAtCSharpPropertyNameGenerator : NJsonSchema.CodeGeneration.IPropertyNameGenerator {\\n        /// <summary>Generates the property name.</summary>\\n        /// <param name=\\"property\\">The property.</param>\\n        /// <returns>The new name.</returns>\\n        public virtual string Generate(JsonSchemaProperty property) =>\\n            ConversionUtilities.ConvertToUpperCamelCase(property.Name\\n                .Replace(\\"\\\\\\"\\", string.Empty)\\n                .Replace(\\"@\\", \\"__\\") // make \\"@\\" => \\"__\\", so \\"@type\\" => \\"__type\\"\\n                .Replace(\\"?\\", string.Empty)\\n                .Replace(\\"$\\", string.Empty)\\n                .Replace(\\"[\\", string.Empty)\\n                .Replace(\\"]\\", string.Empty)\\n                .Replace(\\"(\\", \\"_\\")\\n                .Replace(\\")\\", string.Empty)\\n                .Replace(\\".\\", \\"-\\")\\n                .Replace(\\"=\\", \\"-\\")\\n                .Replace(\\"+\\", \\"plus\\"), true)\\n                .Replace(\\"*\\", \\"Star\\")\\n                .Replace(\\":\\", \\"_\\")\\n                .Replace(\\"-\\", \\"_\\")\\n                .Replace(\\"#\\", \\"_\\");\\n    }\\n}\\n```\\n\\n## Conclusion\\n\\nThis post takes the tremendous NSwag, and demonstrates a mechanism for using it to create C# clients from an Open API / Swagger documents which:\\n\\n- can handle property names with an `@` prefix which might collide with the same property without the prefix\\n- use `decimal` as the preferred number type for floating point numbers"},{"id":"docusaurus-meta-tags-and-google-discover","metadata":{"permalink":"/docusaurus-meta-tags-and-google-discover","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-10-18-docusaurus-meta-tags-and-google-discover/index.md","source":"@site/blog/2021-10-18-docusaurus-meta-tags-and-google-discover/index.md","title":"Docusaurus, meta tags and Google Discover","description":"Boost your websites appearance in Google Discover with high-quality images and `max-image-preview:large` meta tag setting in Docusaurus.","date":"2021-10-18T00:00:00.000Z","tags":[{"inline":false,"label":"Docusaurus","permalink":"/tags/docusaurus","description":"The Docusaurus static site generator."}],"readingTime":2.68,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"docusaurus-meta-tags-and-google-discover","title":"Docusaurus, meta tags and Google Discover","authors":"johnnyreilly","tags":["docusaurus"],"image":"./title-image.png","hide_table_of_contents":false,"description":"Boost your websites appearance in Google Discover with high-quality images and `max-image-preview:large` meta tag setting in Docusaurus."},"unlisted":false,"prevItem":{"title":"NSwag generated C# client: Open API property name clashes and decimal types rather than double","permalink":"/nswag-generated-c-sharp-client-property-name-clash"},"nextItem":{"title":"Structured data, SEO and React","permalink":"/structured-data-seo-and-react"}},"content":"Google Discover is a way that people can find your content. To make your content more attractive, Google encourage using high quality images which are enabled by setting the `max-image-preview:large` meta tag. This post shows you how to achieve that with Docusaurus.\\n\\n![title image reading \\"Docusaurus, meta tags and Google Discover\\" with a Docusaurus logo and the Google Discover phone photo taken from https://developers.google.com/search/docs/advanced/mobile/google-discover](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Google Discover\\n\\nI\'m an Android user. Google Discover will present articles to me in various places on my phone. [According to the docs](https://developers.google.com/search/docs/advanced/mobile/google-discover):\\n\\n> With Discover, you can get updates for your interests, like your favorite sports team or news site, without searching for them. You can choose the types of updates you want to see in Discover in the Google app or when you\u2019re browsing the web on your phone.\\n\\nIt turns out that my own content is showing up in Discover. I (ahem) discovered this by looking at the Google search console and noticing a \\"Discover\\" tab:\\n\\n![screenshot of the Google search console featuring a \\"discover\\" image](screenshot-of-discover-in-search-console.webp)\\n\\nAs I read up about Discover I noticed this:\\n\\n> To increase the likelihood of your content appearing in Discover, we recommend the following:\\n> ...\\n>\\n> - Include compelling, high-quality images in your content, especially large images that are more likely to generate visits from Discover. Large images need to be at least 1200 px wide and enabled by the `max-image-preview:large` setting...\\n\\nI was already trying to include images with my blog posts as described... But `max-image-preview:large` was news to me. [Reading up further](https://developers.google.com/search/docs/advanced/robots/robots_meta_tag#max-image-preview) revealed that the \\"setting\\" was simply a meta tag to be added to the HTML that looked like this:\\n\\n```html\\n<meta name=\\"robots\\" content=\\"max-image-preview:standard\\" />\\n```\\n\\nIncidentally, applying this setting will affect all forms of search results. So not just Discover, but Google web search, Google Images and Assistant as well. The result of having this meta tag will be that bigger images are displayed in search results, which should make the content more attractive.\\n\\n## Docusaurus let\'s get meta\\n\\nNow we understand what we want (an extra meta tag on all our pages), how do we apply this to Docusaurus?\\n\\nWell, it\'s remarkably simple. There\'s an optional [`metadata`](https://docusaurus.io/docs/api/themes/configuration#metadata) property in `docusaurus.config.js`. This property allows you to configure additional html metadata (and override existing ones). The property is an array of `Metadata`, each entry of which will be directly passed to the `<meta />` tag.\\n\\nSo in our case we\'d want to pass an object with `name: \'robots\'` and `content: \'max-image-preview:large\'` to render our desired meta tag. Which looks like this:\\n\\n```js\\n/** @type {import(\'@docusaurus/types\').DocusaurusConfig} */\\nmodule.exports = {\\n  //...\\n  themeConfig: {\\n    // <meta name=\\"robots\\" content=\\"max-image-preview:large\\" />\\n    metadata: [{ name: \'robots\', content: \'max-image-preview:large\' }],\\n    //...\\n  },\\n  //...\\n};\\n```\\n\\nWith that in place, we find our expected `meta` tag is now part of our rendered HTML:\\n\\n![screenshot of the meta robots max-image-preview:large tag taken from Chrome Devtools](screenshot-of-meta-tag.png)\\n\\n## Meta meta\\n\\nWe should now have a more Google Discover-friendly website which is tremendous!\\n\\nBefore signing off, here\'s a fun fact: the PR that published this blog post is the _same_ PR that added `max-image-preview:standard` to my blog. [Peep it here](https://github.com/johnnyreilly/blog.johnnyreilly.com/pull/114) - meta in so many ways \uD83D\uDE09"},{"id":"structured-data-seo-and-react","metadata":{"permalink":"/structured-data-seo-and-react","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-10-15-structured-data-seo-and-react/index.md","source":"@site/blog/2021-10-15-structured-data-seo-and-react/index.md","title":"Structured data, SEO and React","description":"Add structured data to your website to help search engines understand your content & get it in front of more people. Example shown in a React app.","date":"2021-10-15T00:00:00.000Z","tags":[{"inline":false,"label":"SEO","permalink":"/tags/seo","description":"Search Engine Optimization."},{"inline":false,"label":"React","permalink":"/tags/react","description":"The React library."}],"readingTime":7.405,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"structured-data-seo-and-react","title":"Structured data, SEO and React","authors":"johnnyreilly","tags":["seo","react"],"image":"./title-image.png","hide_table_of_contents":false,"description":"Add structured data to your website to help search engines understand your content & get it in front of more people. Example shown in a React app."},"unlisted":false,"prevItem":{"title":"Docusaurus, meta tags and Google Discover","permalink":"/docusaurus-meta-tags-and-google-discover"},"nextItem":{"title":"Permissioning Azure Pipelines with Bicep and Azure RBAC Role Assignments","permalink":"/permissioning-azure-pipelines-bicep-role-assignments"}},"content":"People being able to discover your website when they search is important. This post is about how you can add structured data to a site. Adding structured data will help search engines like Google understand your content, and get it in front of more eyeballs. We\'ll illustrate this by making a simple React app which incorporates structured data.\\n\\n![title image reading \\"Structured data, SEO and React\\" with a screenshot of the rich results tool in the background](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Updated 5th January 2023\\n\\nThis blog evolved to become a talk:\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/zi1CHB-eVck?start=282\\" title=\\"YouTube video player\\" frameBorder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowFullScreen></iframe>\\n\\nIf you\'d like to read about the related topic of [adding breadcrumb Structured Data to a Docusaurus app, I\'ve another post covering that](../2023-02-05-docusaurus-blogs-adding-breadcrumb-structured-data/index.md).\\n\\n## What is structured data?\\n\\nGoogle, DuckDuckGo and others are proficient at understanding the content of websites. However, scraping HTML is not a highly reliable way to categorise content. HTML is about presentation and it can have all manner of different structures. To make the life of search engines easier, there\'s a standardized format known as \\"structured data\\" which can be embedded within a page. That standardized format allows you to explicitly declare the type of content the page contains.\\n\\nSo let\'s say you\'ve written an article, you can reliably state in a language that Google understands \\"this page is an article, it has this title, this description and image and was published on this date\\". There are hundreds of types of structured data available, and you can read about all of them in depth at https://schema.org/ which is maintained by representatives of the search engine community.\\n\\nIt\'s worth knowing that whilst there are many types of structured data available to choose from, there are definitely more popular options and those that are more niche. So [Article](https://schema.org/Article) is likely to be used a great deal more than, perhaps, [MolecularEntity](https://schema.org/MolecularEntity).\\n\\nAs well as there being different types of structured data, there also a variety of formats which can be used to provide it; these include [JSON-LD](http://json-ld.org/), [Microdata](https://www.w3.org/TR/microdata/) and [RDFa](https://rdfa.info/). Google explicitly prefer JSON-LD and so that\'s what we\'ll focus on. JSON-LD is effectively a rending of a piece of JSON inside a `script` tag with the custom type of `application/ld+json`. For example:\\n\\n```html\\n<script type=\\"application/ld+json\\">\\n  {\\n    \\"@context\\": \\"https://schema.org/\\",\\n    \\"@type\\": \\"Recipe\\",\\n    \\"name\\": \\"Chocolate Brownie\\",\\n    \\"author\\": {\\n      \\"@type\\": \\"Person\\",\\n      \\"name\\": \\"John Reilly\\"\\n    },\\n    \\"datePublished\\": \\"2014-09-01\\",\\n    \\"description\\": \\"The most amazing chocolate brownie recipe\\",\\n    \\"prepTime\\": \\"PT60M\\"\\n  }\\n<\/script>\\n```\\n\\n## Structured data in action\\n\\nWhilst structured data is helpful for search engines in general, it can also make a difference to the way your content is rendered _inside_ search results. For instance, let\'s search for \\"best brownie recipe\\" in Google and see what shows up:\\n\\n![screenshot of google search results for \\"best brownie recipe\\" including a rich text results set at the top of the list showing recipes from various sources](screenshot-of-rich-text-results.webp)\\n\\nWhen you look at the screenshot above, you\'ll notice that at the top of the list (before the main search results) there\'s a carousel which shows various brownie recipe links, with dedicated pictures, titles and descriptions. Where did this come from? The answer, unsurprisingly, is structured data.\\n\\nIf we click on the first link, we\'re taken to the recipe in question. Looking at the HTML of that page we find a number of JSON-LD sections:\\n\\n![screenshot of JSON-LD sections in the BBC Good Food website](structured-data-in-action.png)\\n\\nIf we grab the content of one JSON-LD section and paste it into the devtools console, it becomes much easier to read:\\n\\n![screenshot of JSON-LD section transformed into a JavaScript Object Literal](single-structured-data-as-JSON.png)\\n\\nIf we look at the `@type` property we can see it\'s a `\\"Recipe\\"`. This means it\'s an example of the https://schema.org/Recipe schema. If we look further at the `headline` property, it reads `\\"Best ever chocolate brownies recipe\\"`. That matches up with headline that was displayed in the search results.\\n\\nNow we have a sense of what the various search engines are using as they categorise the page, and we understand exactly what is powering the carousel in the Google search results.\\n\\nIncidentally, there\'s a special name for this \\"carousel\\"; it is a \\"rich result\\". A rich result is a search result singled out for special treatment when it is displayed. Google provide a [Rich Results Test tool](https://search.google.com/test/rich-results) which allows you to validate if a site provides structured data which is eligible to be featured in rich results. We\'ll make use of this later.\\n\\n## Adding structured data to a website\\n\\nNow we\'ll make ourselves a React app and add structured data to it. In the console we\'ll execute the following command:\\n\\n```bash\\nnpx create-react-app my-app\\n```\\n\\nWe now have a simple React app which consists of a single page. Let\'s replace the content of the existing `App.js` file with this:\\n\\n```jsx\\n//@ts-check\\nimport \'./App.css\';\\n\\nfunction App() {\\n  // https://schema.org/Article\\n  const articleStructuredData = {\\n    \'@context\': \'https://schema.org\',\\n    \'@type\': \'Article\',\\n    headline: \'Structured data for you\',\\n    description: \'This is an article that demonstrates structured data.\',\\n    image: \'https://upload.wikimedia.org/wikipedia/commons/4/40/JSON-LD.svg\',\\n    datePublished: new Date(\'2021-09-04T09:25:01.340Z\').toISOString(),\\n    author: {\\n      \'@type\': \'Person\',\\n      name: \'John Reilly\',\\n      url: \'https://johnnyreilly.com/about\',\\n    },\\n  };\\n\\n  return (\\n    <div className=\\"App\\">\\n      <script\\n        type=\\"application/ld+json\\"\\n        dangerouslySetInnerHTML={{\\n          __html: JSON.stringify(articleStructuredData),\\n        }}\\n      />\\n\\n      <h1>{articleStructuredData.headline}</h1>\\n      <h3>\\n        by{\' \'}\\n        <a href={articleStructuredData.author.url}>\\n          {articleStructuredData.author.name}\\n        </a>{\' \'}\\n        on {articleStructuredData.datePublished}\\n      </h3>\\n\\n      <img\\n        style={{ width: \'5em\' }}\\n        alt=\\"https://json-ld.org/ - Website content released under a Creative Commons CC0 Public Domain Dedication except where an alternate is specified., CC0, via Wikimedia Commons\\"\\n        src={articleStructuredData.image}\\n      />\\n\\n      <p>{articleStructuredData.description}</p>\\n\\n      <p>Take a look at the source of this page and find the JSON-LD!</p>\\n    </div>\\n  );\\n}\\n\\nexport default App;\\n```\\n\\nIf we look at the code above, we can see we\'re creating a JavaScript object literal named `articleStructuredData` which contains the data of an `https://schema.org/Article`. Our `articleStructuredData` is then used to do two things:\\n\\n1. to contribute to the content of the page\\n2. to render a JSON-LD script element: `<script type=\\"application/ld+json\\">` which is populated by calling `JSON.stringify(articleStructuredData)`\\n\\n### A note on JSON-LD and `dangerouslySetInnerHTML`\\n\\nYou\'ll note we\'re using `dangerouslySetInnerHTML` to render the JSON-LD script element. That is because an issue arises if we instead inline it like so:\\n\\n```html\\n<script type=\\"application/ld+json\\">\\n  {JSON.stringify(articleStructuredData)}\\n<\/script>\\n```\\n\\nIf we do this, the `\\"` characters in the JSON would be escaped as `&quot;` and would look like something this:\\n\\n```html\\n<script type=\\"application/ld+json\\">\\n  {\\n    &quot;@context&quot;: &quot;https://schema.org&quot;,\\n    &quot;@type&quot;: &quot;Article&quot;,\\n    &quot;headline&quot;: &quot;Structured data for you&quot;,\\n    &quot;description&quot;: &quot;This is an article that demonstrates structured data.&quot;,\\n    &quot;image&quot;: &quot;https://upload.wikimedia.org/wikipedia/commons/4/40/JSON-LD.svg&quot;,\\n    &quot;datePublished&quot;: &quot;2020-02-11T06:42:03.706Z&quot;,\\n    &quot;author&quot;: {\\n      &quot;@type&quot;: &quot;Person&quot;,\\n      &quot;name&quot;: &quot;John Reilly&quot;,\\n      &quot;url&quot;: &quot;https://johnnyreilly.com/about&quot;,\\n    }\\n  }\\n<\/script>\\n```\\n\\nRather than:\\n\\n```html\\n<script type=\\"application/ld+json\\">\\n  {\\n    \\"@context\\": \\"https://schema.org\\",\\n    \\"@type\\": \\"Article\\",\\n    \\"headline\\": \\"Structured data for you\\",\\n    \\"description\\": \\"This is an article that demonstrates structured data.\\",\\n    \\"image\\": \\"https://upload.wikimedia.org/wikipedia/commons/4/40/JSON-LD.svg\\",\\n    \\"datePublished\\": \\"2020-02-11T06:42:03.706Z\\",\\n    \\"author\\": {\\n      \\"@type\\": \\"Person\\",\\n      \\"name\\": \\"John Reilly\\",\\n      \\"url\\": \\"https://johnnyreilly.com/about\\"\\n    }\\n  }\\n<\/script>\\n```\\n\\nThat `&quot;`s would make the JSON-LD invalid to some parsers. A great example of a parser troubled by this is the Google Search Console, which can trip up with a `Parsing error: Missing \'}\' or object member name.` error:\\n\\n![screenshot of the Google Search Console with the issue Parsing error: Missing } or object member name.](screenshot-google-search-console.webp)\\n\\nUsing `dangerouslySetInnerHTML` resolves issues like this. It\'s worth noting that the \\"HTML\\" we\'re actually rendering is JSON, and it\'s safe to render that because we are the creators of it.\\n\\n## Running the site\\n\\nWhen we run our site locally with `npm start` we see a simple article site that looks like this:\\n\\n![screenshot of article page](screenshot-of-article.png)\\n\\nNow let\'s see if it supports structured data in the way we hope.\\n\\n## Using the Rich Results Test\\n\\nIf we go to `https://search.google.com/test/rich-results` we find the Rich Results Test tool. There\'s two ways you can test; providing a URL or providing code. In our case we don\'t have a public facing URL and so we\'re going to use the HTML that React is rendering.\\n\\nIn devtools we\'ll use the \\"copy outerHTML\\" feature to grab the HTML, then we\'ll paste it into Rich Results:\\n\\n![screenshot of rich results tool in code view](screenshot-of-rich-results-tool.png)\\n\\nWe hit the \\"TEST CODE\\" button and we see results that look like this:\\n\\n![screenshot of the results of testing our site using the rich results tool](screenshot-of-rich-results-tool-test.webp)\\n\\nSo we\'ve been successful in building a website that renders structured data. More than that, we\'re doing it in a way that we know Google will recognise and can use to render rich results in search. That\'s a really useful way to drive traffic to our website.\\n\\nThis post has illustrated what it looks like to create an `Article`. Google has some [great resources](https://developers.google.com/search/docs/advanced/structured-data/search-gallery) on other types that it supports and prioritises for rich results which should help you build the structured data you need for your particular content.\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/react-structured-data-and-seo/)\\n\\nIf you found this post interesting, you may enjoy one where I went a little further and wrote [about adding FAQ Structured Data to a Docusaurus site using MDX](../2023-04-08-docusaurus-structured-data-faqs-mdx/index.md).\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.logrocket.com/react-structured-data-and-seo/\\" />\\n</head>"},{"id":"permissioning-azure-pipelines-bicep-role-assignments","metadata":{"permalink":"/permissioning-azure-pipelines-bicep-role-assignments","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-09-12-permissioning-azure-pipelines-bicep-role-assignments/index.md","source":"@site/blog/2021-09-12-permissioning-azure-pipelines-bicep-role-assignments/index.md","title":"Permissioning Azure Pipelines with Bicep and Azure RBAC Role Assignments","description":"Learn to permission Azure Pipelines to access resources through RBAC role assignments with Bicep. Includes examples and integration tests.","date":"2021-09-12T00:00:00.000Z","tags":[{"inline":false,"label":"Bicep","permalink":"/tags/bicep","description":"The Bicep language for Azure Resource Manager templates."},{"inline":false,"label":"Azure Pipelines","permalink":"/tags/azure-pipelines","description":"The Azure Pipelines CI / CD service."},{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."},{"inline":false,"label":"Azure DevOps","permalink":"/tags/azure-devops","description":"The Azure DevOps suite of tools."}],"readingTime":8.72,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"permissioning-azure-pipelines-bicep-role-assignments","title":"Permissioning Azure Pipelines with Bicep and Azure RBAC Role Assignments","authors":"johnnyreilly","tags":["bicep","azure pipelines","azure","azure devops"],"image":"./title-image.png","hide_table_of_contents":false,"description":"Learn to permission Azure Pipelines to access resources through RBAC role assignments with Bicep. Includes examples and integration tests."},"unlisted":false,"prevItem":{"title":"Structured data, SEO and React","permalink":"/structured-data-seo-and-react"},"nextItem":{"title":"Google APIs: authentication with TypeScript","permalink":"/google-apis-authentication-with-typescript"}},"content":"How can we deploy resources to Azure, and then run an integration test through them in the context of an Azure Pipeline? This post will show how to do this by permissioning our Azure Pipeline to access these resources using Azure RBAC role assignments. It will also demonstrate a dotnet test that runs in the context of the pipeline and makes use of those role assignments.\\n\\n![title image reading \\"Permissioning Azure Pipelines with Bicep and Role Assignments\\" and some Azure logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\nWe\'re following this approach as an alternative to [exporting connection strings](../2021-07-07-output-connection-strings-and-keys-from-azure-bicep/index.md), as these can be viewed in the Azure Portal; which may be an security issue if you have many people who are able to access the portal and view deployment outputs.\\n\\nWe\'re going to demonstrate this approach using Event Hubs. It\'s worth calling out that this is a generally useful approach which can be applied to any Azure resources that support Azure RBAC Role Assignments. So wherever in this post you read \\"Event Hubs\\", imagine substituting other Azure resources you\'re working with.\\n\\nThe post will do the following:\\n\\n- Add Event Hubs to our Azure subscription\\n- Permission our service connection / service principal\\n- Deploy to Azure with Bicep\\n- Write an integration test\\n- Write a pipeline to bring it all together\\n\\n## Add Event Hubs to your subscription\\n\\nFirst of all, we may need to add Event Hubs to our Azure subscription.\\n\\nWithout this in place, we may encounter errors of the type:\\n\\n> ##[error]MissingSubscriptionRegistration: The subscription is not registered to use namespace \'Microsoft.EventHub\'. See https://aka.ms/rps-not-found for how to register subscriptions.\\n\\nWe do this by going to \\"Resource Providers\\" in the [Azure Portal](https://portal.azure.com) and registering the resources you need. Lots are registered by default, but not all.\\n\\n![Screenshot of the Azure Portal, subscriptions -> resource providers section, showing that Event Hubs have been registered](screenshot-azure-portal-subscription-resource-providers.webp)\\n\\n## Permission our service connection / service principal\\n\\nIn order that we can run pipelines related to Azure, we mostly need to have an Azure Resource Manager service connection set up in Azure DevOps. Once that exists, we also need to give it a role assignment to allow it to create role assignments of its own when pipelines are running.\\n\\nWithout this in place, we may encounter errors of the type:\\n\\n> ##[error]The template deployment failed with error: \'Authorization failed for template resource `{GUID-THE-FIRST}` of type `Microsoft.Authorization/roleAssignments`. The client `{GUID-THE-SECOND}` with object id `{GUID-THE-SECOND}` does not have permission to perform action `Microsoft.Authorization/roleAssignments/write` at scope `/subscriptions/\\\\*\\\\*\\\\*/resourceGroups/johnnyreilly/providers/Microsoft.EventHub/namespaces/evhns-demo/providers/Microsoft.Authorization/roleAssignments/{GUID-THE-FIRST}`.\'.\\n\\nEssentially, we want to be able to run pipelines that say \\"hey Azure, we want to give permissions to our service connection\\". We are doing this _with_ the self same service connection, so (chicken and egg) we first need to give it permission to give those commands in future. This is a little confusing; but let\'s role with it. (Pun most definitely intended. \uD83D\uDE09)\\n\\nTo grant that permission / add that role assignment, we go to the service connection in Azure Devops:\\n\\n![Screenshot of the service connection in Azure DevOps](screenshot-azure-devops-service-connection.webp)\\n\\nWe can see there\'s two links here; first we\'ll click on \\"Manage Service Principal\\", which will take us to the service principal in the Azure Portal:\\n\\n![Screenshot of the service principal in the Azure Portal](screenshot-azure-portal-service-principal.png)\\n\\nTake note of the display name of the service principal; we\'ll need that as we click on the \\"Manage service connection roles\\" link, which will take us to the resource groups IAM page in the Azure Portal:\\n\\n![Screenshot of the resource groups IAM page in the Azure Portal](screenshot-azure-portal-service-principal-access-control.png)\\n\\nHere we can click on \\"Add role assignment\\", select \\"Owner\\":\\n\\n![Screenshot of the add role assignment IAM page in the Azure Portal](screenshot-azure-portal-add-role-assignment.png)\\n\\nThen when selecting members we should be able to look up the service principal to assign it:\\n\\n![Screenshot of the add role assignment select member IAM page in the Azure Portal](screenshot-azure-portal-add-role-assignment-member.png)\\n\\nWe now have a service connection which we should be able to use for granting permissions / role assignments, which is what we need.\\n\\n## Event Hub and Role Assignment with Bicep\\n\\nNext we want a Bicep file that will, when run, provision an Event Hub and a role assignment which will allow our Azure Pipeline (via its service connection) to interact with it.\\n\\n```bicep\\n@description(\'Name of the eventhub namespace\')\\nparam eventHubNamespaceName string\\n\\n@description(\'Name of the eventhub name\')\\nparam eventHubName string\\n\\n@description(\'The service principal\')\\nparam principalId string\\n\\n// Create an Event Hub namespace\\nresource eventHubNamespace \'Microsoft.EventHub/namespaces@2021-01-01-preview\' = {\\n  name: eventHubNamespaceName\\n  location: resourceGroup().location\\n  sku: {\\n    name: \'Standard\'\\n    tier: \'Standard\'\\n    capacity: 1\\n  }\\n  properties: {\\n    zoneRedundant: true\\n  }\\n}\\n\\n// Create an Event Hub inside the namespace\\nresource eventHub \'Microsoft.EventHub/namespaces/eventhubs@2021-01-01-preview\' = {\\n  parent: eventHubNamespace\\n  name: eventHubName\\n  properties: {\\n    messageRetentionInDays: 7\\n    partitionCount: 1\\n  }\\n}\\n\\n// give Azure Pipelines Service Principal permissions against the Event Hub\\n\\nvar roleDefinitionAzureEventHubsDataOwner = subscriptionResourceId(\'Microsoft.Authorization/roleDefinitions\', \'f526a384-b230-433a-b45c-95f59c4a2dec\')\\n\\nresource integrationTestEventHubReceiverNamespaceRoleAssignment \'Microsoft.Authorization/roleAssignments@2018-01-01-preview\' = {\\n  name: guid(principalId, eventHub.id, roleDefinitionAzureEventHubsDataOwner)\\n  scope: eventHubNamespace\\n  properties: {\\n    roleDefinitionId: roleDefinitionAzureEventHubsDataOwner\\n    principalId: principalId\\n  }\\n}\\n```\\n\\nDo note that our bicep template takes the service principal id as a parameter. We\'re going to supply this later from our Azure Pipeline.\\n\\n## Our test\\n\\nWe\'re now going to write a dotnet integration test which will make use of the infrastructure deployed by our Bicep template. Let\'s create a new test project:\\n\\n```\\nmkdir src\\ncd src\\ndotnet new xunit -o IntegrationTests\\ncd IntegrationTests\\ndotnet add package Azure.Identity\\ndotnet add package Azure.Messaging.EventHubs\\ndotnet add package FluentAssertions\\ndotnet add package Microsoft.Extensions.Configuration.EnvironmentVariables\\n```\\n\\nWe\'ll create a test file called `EventHubTest.cs` with these contents:\\n\\n```cs\\nusing System;\\nusing System.Collections.Generic;\\nusing System.Linq;\\nusing System.Text;\\nusing System.Threading;\\nusing System.Threading.Tasks;\\nusing Azure.Identity;\\nusing Azure.Messaging.EventHubs;\\nusing Azure.Messaging.EventHubs.Consumer;\\nusing Azure.Messaging.EventHubs.Producer;\\nusing FluentAssertions;\\nusing Microsoft.Extensions.Configuration;\\nusing Newtonsoft.Json;\\nusing Xunit;\\nusing Xunit.Abstractions;\\n\\nnamespace IntegrationTests\\n{\\n    public record EchoMessage(string Id, string Message, DateTime Timestamp);\\n\\n    public class EventHubTest\\n    {\\n        private readonly ITestOutputHelper _output;\\n\\n        public EventHubTest(ITestOutputHelper output)\\n        {\\n            _output = output;\\n        }\\n\\n        [Fact]\\n        public async Task Can_post_message_to_event_hub_and_read_it_back()\\n        {\\n            // ARRANGE\\n            var configuration = new ConfigurationBuilder()\\n                .AddEnvironmentVariables()\\n                .Build();\\n\\n            // populated by variables specified in the Azure Pipeline\\n            var eventhubNamespaceName = configuration[\\"EVENTHUBNAMESPACENAME\\"];\\n            eventhubNamespaceName.Should().NotBeNull();\\n            var eventhubName = configuration[\\"EVENTHUBNAME\\"];\\n            eventhubName.Should().NotBeNull();\\n            var tenantId = configuration[\\"TENANTID\\"];\\n            tenantId.Should().NotBeNull();\\n\\n            // populated as a consequence of the addSpnToEnvironment in the azure-pipelines.yml\\n            var servicePrincipalId = configuration[\\"SERVICEPRINCIPALID\\"];\\n            servicePrincipalId.Should().NotBeNull();\\n            var servicePrincipalKey = configuration[\\"SERVICEPRINCIPALKEY\\"];\\n            servicePrincipalKey.Should().NotBeNull();\\n\\n            var fullyQualifiedNamespace = $\\"{eventhubNamespaceName}.servicebus.windows.net\\";\\n\\n            var clientCredential = new ClientSecretCredential(tenantId, servicePrincipalId, servicePrincipalKey);\\n            var eventHubClient = new EventHubProducerClient(\\n                fullyQualifiedNamespace: fullyQualifiedNamespace,\\n                eventHubName: eventhubName,\\n                credential: clientCredential\\n            );\\n            var ourGuid = Guid.NewGuid().ToString();\\n            var now = DateTime.UtcNow;\\n            var sentEchoMessage = new EchoMessage(Id: ourGuid, Message: $\\"Test message\\", Timestamp: now);\\n            var sentEventData = new EventData(\\n                Encoding.UTF8.GetBytes(JsonConvert.SerializeObject(sentEchoMessage))\\n            );\\n\\n            // ACT\\n            await eventHubClient.SendAsync(new List<EventData> { sentEventData }, CancellationToken.None);\\n\\n            var eventHubConsumerClient = new EventHubConsumerClient(\\n                consumerGroup: EventHubConsumerClient.DefaultConsumerGroupName,\\n                fullyQualifiedNamespace: fullyQualifiedNamespace,\\n                eventHubName: eventhubName,\\n                credential: clientCredential\\n            );\\n\\n            List<PartitionEvent> partitionEvents = new();\\n            await foreach (var partitionEvent in eventHubConsumerClient.ReadEventsAsync(new ReadEventOptions\\n            {\\n                MaximumWaitTime = TimeSpan.FromSeconds(10)\\n            }))\\n            {\\n                if (partitionEvent.Data == null) break;\\n                _output.WriteLine(Encoding.UTF8.GetString(partitionEvent.Data.EventBody.ToArray()));\\n                partitionEvents.Add(partitionEvent);\\n            }\\n\\n            // ASSERT\\n            partitionEvents.Count.Should().BeGreaterOrEqualTo(1);\\n            var firstOne = partitionEvents.FirstOrDefault(evnt =>\\n              ExtractTypeFromEventBody<EchoMessage>(evnt, _output)?.Id == ourGuid\\n            );\\n            var receivedEchoMessage = ExtractTypeFromEventBody<EchoMessage>(firstOne, _output);\\n            receivedEchoMessage.Should().BeEquivalentTo(sentEchoMessage, because: \\"the event body should be the same one posted to the message queue\\");\\n        }\\n\\n        private static T ExtractTypeFromEventBody<T>(PartitionEvent evnt, ITestOutputHelper _output)\\n        {\\n            try\\n            {\\n                return JsonConvert.DeserializeObject<T>(Encoding.UTF8.GetString(evnt.Data.EventBody.ToArray()));\\n            }\\n            catch (JsonException)\\n            {\\n                _output.WriteLine(\\"[\\" + Encoding.UTF8.GetString(evnt.Data.EventBody.ToArray()) + \\"] is probably not JSON\\");\\n                return default(T);\\n            }\\n        }\\n    }\\n}\\n```\\n\\nLet\'s talk through what happens in the test above:\\n\\n1. We read in Event Hub connection configuration for the test from environment variables. (These will be supplied by an Azure Pipeline that we will create shortly.)\\n2. We post a message to the Event Hub.\\n3. We read a message back from the Event Hub.\\n4. We confirm that the message we read back matches the one we posted.\\n\\nNow that we have our test, we want to be able to execute it. For that we need an Azure Pipeline!\\n\\n## Azure Pipeline\\n\\nWe\'re going to add an `azure-pipelines.yml` file which Azure DevOps can use to power a pipeline:\\n\\n```yml\\nvariables:\\n  - name: eventHubNamespaceName\\n    value: evhns-demo\\n  - name: eventHubName\\n    value: evh-demo\\n\\npool:\\n  vmImage: ubuntu-latest\\n\\nsteps:\\n  - task: AzureCLI@2\\n    displayName: Get Service Principal Id\\n    inputs:\\n      azureSubscription: $(serviceConnection)\\n      scriptType: bash\\n      scriptLocation: inlineScript\\n      addSpnToEnvironment: true\\n      inlineScript: |\\n        PRINCIPAL_ID=$(az ad sp show --id $servicePrincipalId --query objectId -o tsv)\\n        echo \\"##vso[task.setvariable variable=PIPELINE_PRINCIPAL_ID;]$PRINCIPAL_ID\\"\\n\\n  - bash: az bicep build --file infra/main.bicep\\n    displayName: \'Compile Bicep to ARM\'\\n\\n  - task: AzureResourceManagerTemplateDeployment@3\\n    name: DeployEventHubInfra\\n    displayName: Deploy Event Hub infra\\n    inputs:\\n      deploymentScope: Resource Group\\n      azureResourceManagerConnection: $(serviceConnection)\\n      subscriptionId: $(subscriptionId)\\n      action: Create Or Update Resource Group\\n      resourceGroupName: $(azureResourceGroup)\\n      location: $(location)\\n      templateLocation: Linked artifact\\n      csmFile: \'infra/main.json\' # created by bash script\\n      overrideParameters: >-\\n        -eventHubNamespaceName $(eventHubNamespaceName)\\n        -eventHubName $(eventHubName)\\n        -principalId $(PIPELINE_PRINCIPAL_ID)\\n      deploymentMode: Incremental\\n\\n  - task: UseDotNet@2\\n    displayName: \'Install .NET SDK 5.0.x\'\\n    inputs:\\n      packageType: \'sdk\'\\n      version: 5.0.x\\n\\n  - task: AzureCLI@2\\n    displayName: dotnet integration test\\n    inputs:\\n      azureSubscription: $(serviceConnection)\\n      scriptType: pscore\\n      scriptLocation: inlineScript\\n      addSpnToEnvironment: true # allows access to service principal details in script\\n      inlineScript: |\\n        cd $(Build.SourcesDirectory)/src/IntegrationTests\\n        dotnet test\\n```\\n\\nWhen the pipeline is run, it does the following:\\n\\n1. Gets the service principal id from the service connection.\\n2. Compiles our Bicep into an ARM template\\n3. Deploys the compiled ARM template to Azure\\n4. Installs the dotnet SDK\\n5. Uses the [Azure CLI task](https://docs.microsoft.com/en-us/azure/devops/pipelines/tasks/deploy/azure-cli?view=azure-devops) which allows us to access service principal details in the pipeline to run our dotnet test.\\n\\nWe\'ll create a pipeline in Azure DevOps pointing to this file, and we\'ll also create the variables that it depends upon:\\n\\n- `azureResourceGroup` - the name of your resource group in Azure where the app will be deployed\\n- `location` - where your app is deployed, eg `northeurope`\\n- `serviceConnection` - the name of your AzureRM service connection in Azure DevOps\\n- `subscriptionId` - your Azure subscription id from the [Azure Portal](https://portal.azure.com)\\n- `tenantId` - the Azure tenant id from the [Azure Portal](https://portal.azure.com)\\n\\n## Running the pipeline\\n\\nNow we\'re ready to run our pipeline:\\n\\n![screenshot of pipeline running successfully](screenshot-azure-pipelines-tests-passing.png)\\n\\nHere we can see that the pipeline runs and the test passes. That means we\'ve successfully provisioned the Event Hub and permissioned our pipeline to be able to access it using Azure RBAC role assignments. We then wrote a test which used the pipeline credentials to interact with the Event Hub. To see the repo that demostrates this, [look here](https://dev.azure.com/johnnyreilly/blog-demos/_git/permissioning-azure-pipelines-bicep-role-assignments).\\n\\nJust to reiterate: we\'ve demonstrated this approach using Event Hubs. This is a generally useful approach which can be applied to any Azure resources that support Azure RBAC Role Assignments.\\n\\nThanks to [Jamie McCrindle](https://twitter.com/foldr) for helping out with permissioning the service connection / service principal. [His post on rotating `AZURE_CREDENTIALS` in GitHub with Terraform](https://foldr.uk/rotating-azure-credentials-in-github-with-terraform) provides useful background for those who would like to do similar permissioning using Terraform."},{"id":"google-apis-authentication-with-typescript","metadata":{"permalink":"/google-apis-authentication-with-typescript","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-09-10-google-apis-authentication-with-typescript/index.md","source":"@site/blog/2021-09-10-google-apis-authentication-with-typescript/index.md","title":"Google APIs: authentication with TypeScript","description":"This guide shows how to use TypeScript to authenticate and access Google APIs with OAuth 2.0, specifically the Google Calendar API.","date":"2021-09-10T00:00:00.000Z","tags":[{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":8.74,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"google-apis-authentication-with-typescript","title":"Google APIs: authentication with TypeScript","authors":"johnnyreilly","tags":["typescript"],"image":"./app-registration.png","hide_table_of_contents":false,"description":"This guide shows how to use TypeScript to authenticate and access Google APIs with OAuth 2.0, specifically the Google Calendar API."},"unlisted":false,"prevItem":{"title":"Permissioning Azure Pipelines with Bicep and Azure RBAC Role Assignments","permalink":"/permissioning-azure-pipelines-bicep-role-assignments"},"nextItem":{"title":"Bicep: syntax highlighting with PrismJS (and Docusaurus)","permalink":"/bicep-syntax-highlighting-with-prismjs"}},"content":"Google has a wealth of APIs which we can interact with. At the time of writing, there\'s more than two hundred available; including YouTube, Google Calendar and GMail (alongside many others). To integrate with these APIs, it\'s necessary to authenticate and then use that credential with the API. This post will take you through how to do just that using TypeScript. It will also demonstrate how to use one of those APIs: the Google Calendar API.\\n\\n\x3c!--truncate--\x3e\\n\\n## Creating an OAuth 2.0 Client ID on the Google Cloud Platform\\n\\nThe first thing we need to do is go to the [Google Cloud Platform to create a project](https://console.cloud.google.com/projectcreate). The name of the project doesn\'t matter particularly; although it can be helpful to name the project to align with the API you\'re intending to consume. That\'s what we\'ll do here as we plan to integrate with the Google Calendar API:\\n\\n![Screenshot of the Create Project screen in the Google Cloud Platform](google-cloud-platform-create-project.png)\\n\\nThe project is the container in which the OAuth 2.0 Client ID will be housed. Now we\'ve created the project, let\'s go to the [credentials screen](https://console.cloud.google.com/apis/credentials) and create an OAuth Client ID using the Create Credentials dropdown:\\n\\n![Screenshot of the Create Credentials dropdown in the Google Cloud Platform](create-credentials.png)\\n\\nYou\'ll likely have to create an OAuth consent screen before you can create the OAuth Client ID. Going through the journey of doing that feels a little daunting as many questions have to be answered. This is because the consent screen can be used for a variety of purposes beyond the API authentication we\'re looking at today.\\n\\nWhen challenged, you can generally accept the defaults and proceed. The user type you\'ll require will be \\"External\\":\\n\\n![Screenshot of the OAuth consent screen in the Google Cloud Platform](oauth-consent-screen.png)\\n\\nYou\'ll also be required to create an app registration - all that\'s really required here is a name (which can be anything) and your email address:\\n\\n![Screenshot of the OAuth consent screen in the Google Cloud Platform](app-registration.png)\\n\\nYou don\'t need to worry about scopes. You can either plan to publish the app, or alternately set yourself up to be a test user - you\'ll need to do one of these in order that you can authenticate with the app. Continuing to the end of the journey should provide you with the OAuth consent screen which you need in order that you may then create the OAuth Client ID.\\n\\nCreating the OAuth Client ID is slightly confusing as the \\"Application type\\" required is \\"TVs and Limited Input devices\\".\\n\\n![Screenshot of the create OAuth Client ID screen in the Google Cloud Platform](create-oauth-client-id-type.png)\\n\\nWe\'re using this type of application as we want to acquire a [refresh token](https://oauth.net/2/grant-types/refresh-token/) which we\'ll be able to use in future to aquire access tokens which will be used to access the Google APIs.\\n\\nOnce it\'s created, you\'ll be able to download the Client ID from the Google Cloud Platform:\\n\\n![Screenshot of the create OAuth Client ID screen in the Google Cloud Platform](oauth-client-id.png)\\n\\nWhen you download it, it should look something like this:\\n\\n```json\\n{\\n  \\"installed\\": {\\n    \\"client_id\\": \\"CLIENT_ID\\",\\n    \\"project_id\\": \\"PROJECT_ID\\",\\n    \\"auth_uri\\": \\"https://accounts.google.com/o/oauth2/auth\\",\\n    \\"token_uri\\": \\"https://oauth2.googleapis.com/token\\",\\n    \\"auth_provider_x509_cert_url\\": \\"https://www.googleapis.com/oauth2/v1/certs\\",\\n    \\"client_secret\\": \\"CLIENT_SECRET\\",\\n    \\"redirect_uris\\": [\\"urn:ietf:wg:oauth:2.0:oob\\", \\"http://localhost\\"]\\n  }\\n}\\n```\\n\\nYou\'ll need the `client_id`, `client_secret` and `redirect_uris` - but keep them in a safe place and don\'t commit `client_id` and `client_secret` to source control!\\n\\n## Acquiring a refresh token\\n\\nNow we\'ve got our `client_id` and `client_secret`, we\'re ready to write a simple node command line application which we can use to obtain a refresh token. This is actually a multi-stage process that will end up looking like this:\\n\\n- Provide the Google authentication provider with the `client_id` and `client_secret`, in return it will provide an authentication URL.\\n- Open the authentication URL in the browser and grant consent, the provider will hand over a code.\\n- Provide the Google authentication provider with the `client_id`, `client_secret` and the code, it will acquire and provide users with a refresh token.\\n\\nLet\'s start coding. We\'ll initialise a TypeScript Node project like so:\\n\\n```bash\\nmkdir src\\ncd src\\nnpm init -y\\nnpm install googleapis ts-node typescript yargs @types/yargs @types/node\\nnpx tsc --init\\n```\\n\\nWe\'ve added a number of dependencies that will allow us to write a TypeScript Node command line application. We\'ve also added a dependency to the [`googleapis`](https://www.npmjs.com/package/googleapis) package which describes itself as:\\n\\n> Node.js client library for using Google APIs. Support for authorization and authentication with OAuth 2.0, API Keys and JWT tokens is included.\\n\\nWe\'re going to make use of the OAuth 2.0 part. We\'ll start our journey by creating a file called `google-api-auth.ts`:\\n\\n```ts\\nimport { getArgs, makeOAuth2Client } from \'./shared\';\\n\\nasync function getToken() {\\n  const { clientId, clientSecret, code } = await getArgs();\\n  const oauth2Client = makeOAuth2Client({ clientId, clientSecret });\\n\\n  if (code) await getRefreshToken(code);\\n  else getAuthUrl();\\n\\n  async function getAuthUrl() {\\n    const url = oauth2Client.generateAuthUrl({\\n      // \'online\' (default) or \'offline\' (gets refresh_token)\\n      access_type: \'offline\',\\n\\n      // scopes are documented here: https://developers.google.com/identity/protocols/oauth2/scopes#calendar\\n      scope: [\\n        \'https://www.googleapis.com/auth/calendar\',\\n        \'https://www.googleapis.com/auth/calendar.events\',\\n      ],\\n    });\\n\\n    console.log(`Go to this URL to acquire a refresh token:\\\\n\\\\n${url}\\\\n`);\\n  }\\n\\n  async function getRefreshToken(code: string) {\\n    const token = await oauth2Client.getToken(code);\\n    console.log(token);\\n  }\\n}\\n\\ngetToken();\\n```\\n\\nAnd a common file named `shared.ts` which `google-api-auth.ts` imports and which we\'ll re-use later:\\n\\n```ts\\nimport { google } from \'googleapis\';\\nimport yargs from \'yargs/yargs\';\\nconst { hideBin } = require(\'yargs/helpers\');\\n\\nexport async function getArgs() {\\n  const argv = await Promise.resolve(yargs(hideBin(process.argv)).argv);\\n\\n  const clientId = argv[\'clientId\'] as string;\\n  const clientSecret = argv[\'clientSecret\'] as string;\\n\\n  const code = argv.code as string | undefined;\\n  const refreshToken = argv.refreshToken as string | undefined;\\n  const test = argv.test as boolean;\\n\\n  if (!clientId) throw new Error(\'No clientId \');\\n  console.log(\'We have a clientId\');\\n\\n  if (!clientSecret) throw new Error(\'No clientSecret\');\\n  console.log(\'We have a clientSecret\');\\n\\n  if (code) console.log(\'We have a code\');\\n  if (refreshToken) console.log(\'We have a refreshToken\');\\n\\n  return { code, clientId, clientSecret, refreshToken, test };\\n}\\n\\nexport function makeOAuth2Client({\\n  clientId,\\n  clientSecret,\\n}: {\\n  clientId: string;\\n  clientSecret: string;\\n}) {\\n  return new google.auth.OAuth2(\\n    /* YOUR_CLIENT_ID */ clientId,\\n    /* YOUR_CLIENT_SECRET */ clientSecret,\\n    /* YOUR_REDIRECT_URL */ \'urn:ietf:wg:oauth:2.0:oob\',\\n  );\\n}\\n```\\n\\nThe `getToken` function above does these things:\\n\\n1. If given a `client_id` and `client_secret` it will obtain an authentication URL.\\n2. If given a `client_id`, `client_secret` and `code` it will obtain a refresh token (scoped to access the Google Calendar API).\\n\\nWe\'ll add an entry to our `package.json` which will allow us to run our console app:\\n\\n```json\\n    \\"google-api-auth\\": \\"ts-node google-api-auth.ts\\"\\n```\\n\\nNow we\'re ready to acquire the refresh token. We\'ll run the following command (substituting in the appropriate values):\\n\\n`npm run google-api-auth -- --clientId CLIENT_ID --clientSecret CLIENT_SECRET`\\n\\nClick on the URL that is generated in the console, it should open up a consent screen in the browser which looks like this:\\n\\n![Screenshot of the consent screen](grant-consent.png)\\n\\nAuthenticate and grant consent and you should get a code:\\n\\n![Screenshot of the generated code](auth-code.png)\\n\\nThen (quickly) paste the acquired code into the following command:\\n\\n`npm run google-api-auth -- --clientId CLIENT_ID --clientSecret CLIENT_SECRET --code THISISTHECODE`\\n\\nThe `refresh_token` (alongside much else) will be printed to the console. Grab it and put it somewhere secure. Again, no storing in source control!\\n\\nIt\'s worth taking a moment to reflect on what we\'ve done. We\'ve acquired a refresh token which involved a certain amount of human interaction. We\'ve had to run a console command, do some work in a browser and run another commmand. You wouldn\'t want to do this repeatedly because it involves human interaction. Intentionally it cannot be automated. However, once you\'ve acquired the refresh token, you can use it repeatedly until it expires (which may be never or at least years in the future). So once you have the refresh token, and you\'ve stored it securely, you have what you need to be able to automate an API interaction.\\n\\n## Accessing the Google Calendar API\\n\\nLet\'s test out our refresh token by attempting to access the Google Calendar API. We\'ll create a `calendar.ts` file\\n\\n```ts\\nimport { google } from \'googleapis\';\\nimport { getArgs, makeOAuth2Client } from \'./shared\';\\n\\nasync function makeCalendarClient() {\\n  const { clientId, clientSecret, refreshToken } = await getArgs();\\n  const oauth2Client = makeOAuth2Client({ clientId, clientSecret });\\n  oauth2Client.setCredentials({\\n    refresh_token: refreshToken,\\n  });\\n\\n  const calendarClient = google.calendar({\\n    version: \'v3\',\\n    auth: oauth2Client,\\n  });\\n  return calendarClient;\\n}\\n\\nasync function getCalendar() {\\n  const calendarClient = await makeCalendarClient();\\n\\n  const { data: calendars, status } = await calendarClient.calendarList.list();\\n\\n  if (status === 200) {\\n    console.log(\'calendars\', calendars);\\n  } else {\\n    console.log(\'there was an issue...\', status);\\n  }\\n}\\n\\ngetCalendar();\\n```\\n\\nThe `getCalendar` function above uses the `client_id`, `client_secret` and `refresh_token` to access the Google Calendar API and retrieve the list of calendars.\\n\\nWe\'ll add an entry to our `package.json` which will allow us to run this function:\\n\\n```json\\n    \\"calendar\\": \\"ts-node calendar.ts\\",\\n```\\n\\nNow we\'re ready to test `calendar.ts`. We\'ll run the following command (substituting in the appropriate values):\\n\\n`npm run calendar -- --clientId CLIENT_ID --clientSecret CLIENT_SECRET --refreshToken REFRESH_TOKEN`\\n\\nWhen we run for the first time, we may encounter a self explanatory message which tells us that we need enable the calendar API for our application:\\n\\n```\\n(node:31563) UnhandledPromiseRejectionWarning: Error: Google Calendar API has not been used in project 77777777777777 before or it is disabled. Enable it by visiting https://console.developers.google.com/apis/api/calendar-json.googleapis.com/overview?project=77777777777777 then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry.\\n```\\n\\nOnce enabled, we can run successfully for the first time. Consequently we should see something like this showing up in the console:\\n\\n![Screenshot of calendars list response in the console](calendars-response.png)\\n\\nThis demonstrates that we\'re successfully integrating with a Google API using our refresh token.\\n\\n## Today the Google Calendar API, tomorrow the (Google API) world!\\n\\nWhat we\'ve demonstrated here is integrating with the Google Calendar API. However, that is not the limit of what we can do. As we discussed earlier, Google has more than two hundred APIs we can interact with, and the key to that interaction is following the same steps for authentication that this post outlines.\\n\\nLet\'s imagine that we want to integrate with the YouTube API or the GMail API. We\'d be able to follow the steps in this post, using different [scopes for the refresh token appropriate to the API](https://developers.google.com/identity/protocols/oauth2/scopes#calendar), and build an integration against that API. [Take a look at the available APIs](https://developers.google.com/apis-explorer) here.\\n\\nThe approach outlined by this post is the key to integrating with a multitude of Google APIs. Happy integrating!\\n\\nThe idea of this was sparked by [Martin Fowler\'s post](https://martinfowler.com/articles/command-line-google.html) on the topic which comes from a Ruby angle.\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/how-to-authenticate-access-google-apis-using-oauth-2-0/)\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.logrocket.com/how-to-authenticate-access-google-apis-using-oauth-2-0/\\" />\\n</head>"},{"id":"bicep-syntax-highlighting-with-prismjs","metadata":{"permalink":"/bicep-syntax-highlighting-with-prismjs","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-08-19-bicep-syntax-highlighting-with-prismjs/index.md","source":"@site/blog/2021-08-19-bicep-syntax-highlighting-with-prismjs/index.md","title":"Bicep: syntax highlighting with PrismJS (and Docusaurus)","description":"Learn how to write attractive code snippets about Bicep using PrismJS and Docusaurus. This post shows you how to add syntax highlighting for Bicep.","date":"2021-08-19T00:00:00.000Z","tags":[{"inline":false,"label":"Bicep","permalink":"/tags/bicep","description":"The Bicep language for Azure Resource Manager templates."}],"readingTime":2.17,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"bicep-syntax-highlighting-with-prismjs","title":"Bicep: syntax highlighting with PrismJS (and Docusaurus)","authors":"johnnyreilly","tags":["bicep"],"image":"./bicep-syntax-highlighting-with-prismjs.webp","hide_table_of_contents":false,"description":"Learn how to write attractive code snippets about Bicep using PrismJS and Docusaurus. This post shows you how to add syntax highlighting for Bicep."},"unlisted":false,"prevItem":{"title":"Google APIs: authentication with TypeScript","permalink":"/google-apis-authentication-with-typescript"},"nextItem":{"title":"Publish Azure Static Web Apps with Bicep and Azure DevOps","permalink":"/bicep-azure-static-web-apps-azure-devops"}},"content":"Bicep is an amazing language, it\'s also very new. If you want to write attractive code snippets about Bicep, you can by using PrismJS (and Docusaurus). This post shows you how.\\n\\n![title image reading \\"Bicep: syntax highlighting with PrismJS (and Docusaurus)\\" and some Azure logos](bicep-syntax-highlighting-with-prismjs.webp)\\n\\n\x3c!--truncate--\x3e\\n\\n## Syntax highlighting\\n\\nI\'ve been writing blog posts about Bicep for a little while. I was frustrated that the code snippets were entirely unhighlighted. I\'m keen my posts are as readable as possible, and so I [looked into adding support to PrismJS](https://github.com/PrismJS/prism/pull/3027) which is what [Docusaurus](https://docusaurus.io/) uses to power syntax highlighting.\\n\\nWhilst my regex fu is amateur at best, happily [Michael Schmidt](https://github.com/RunDevelopment) of the PrismJS family is considerably better. He took the support I added and [made it much better](https://github.com/PrismJS/prism/pull/3028).\\n\\n## Docusaurus meet Bicep\\n\\nIf you have any code snippets that start with three backticks and the word `bicep`...\\n\\n````\\n```bicep\\n// code goes here...\\n````\\n\\n... then ideally you\'d like to see some syntax highlighting in your post. Since Bicep isn\'t \\"in the box\\" for Docusaurus you need to [explicitly opt into support like so:](https://docusaurus.io/docs/next/markdown-features/code-blocks#supported-languages)\\n\\n```js\\n    prism: {\\n      additionalLanguages: [\\"powershell\\", \\"csharp\\", \\"docker\\", \\"bicep\\"],\\n    },\\n```\\n\\nAbove you can see a snippet from my own [`docusaurus.config.js`](https://github.com/johnnyreilly/blog.johnnyreilly.com/blob/b2df93efb72adc32d9f45de4f727e890e59a4919/blog-website/docusaurus.config.js#L185) which adds Bicep, alongside the other additional languages I use.\\n\\nWith this in place, you would typically get all the syntax highlighting support you need.\\n\\n## Early adoption workaround\\n\\nI\'m writing this post before the latest version of PrismJS has shipped. As such, Bicep support isn\'t available by default yet. But if you\'re an early adopter, you can get support right now. The secret is adding a `resolutions` section to your `package.json` which points to the GitHub Repo [where Prism lives](https://github.com/PrismJS/prism):\\n\\n```json\\n  \\"resolutions\\": {\\n    \\"prismjs\\": \\"PrismJS/prism\\"\\n  },\\n```\\n\\nThis will mean that Yarn (if you\'re using Docusaurus you\'re probably using Yarn) pulls `prismjs` directly from GitHub, as demonstrated by the `yarn.lock` file:\\n\\n```\\nprismjs@PrismJS/prism, prismjs@^1.23.0:\\n  version \\"1.24.1\\"\\n  resolved \\"https://codeload.github.com/PrismJS/prism/tar.gz/59f449d33dc9fd19302f21aad95fc0b5028ac830\\"\\n```\\n\\n## What does it look like?\\n\\nFinally, let\'s see if works. Here\'s a Bicep code snippet that I borrowed from [an earlier post](../2021-08-19-bicep-syntax-highlighting-with-prismjs/index.md):\\n\\n```bicep\\nparam repositoryUrl string\\nparam repositoryBranch string\\n\\nparam location string = \'westeurope\'\\nparam skuName string = \'Free\'\\nparam skuTier string = \'Free\'\\n\\nparam appName string\\n\\nresource staticWebApp \'Microsoft.Web/staticSites@2020-12-01\' = {\\n  name: appName\\n  location: location\\n  tags: tagsObj\\n  sku: {\\n    name: skuName\\n    tier: skuTier\\n  }\\n  properties: {\\n    // The provider, repositoryUrl and branch fields are required for successive deployments to succeed\\n    // for more details see: https://github.com/Azure/static-web-apps/issues/516\\n    provider: \'DevOps\'\\n    repositoryUrl: repositoryUrl\\n    branch: repositoryBranch\\n    buildProperties: {\\n      skipGithubActionWorkflowGeneration: true\\n    }\\n  }\\n}\\n```\\n\\nAs you can see, it\'s delightfully highlighted by PrismJS. Enjoy!"},{"id":"bicep-azure-static-web-apps-azure-devops","metadata":{"permalink":"/bicep-azure-static-web-apps-azure-devops","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-08-15-bicep-azure-static-web-apps-azure-devops/index.md","source":"@site/blog/2021-08-15-bicep-azure-static-web-apps-azure-devops/index.md","title":"Publish Azure Static Web Apps with Bicep and Azure DevOps","description":"Learn how to deploy Azure Static Web Apps using Bicep and Azure DevOps, including workarounds for common deployment issues.","date":"2021-08-15T00:00:00.000Z","tags":[{"inline":false,"label":"Azure Static Web Apps","permalink":"/tags/azure-static-web-apps","description":"The Azure Static Web Apps service."},{"inline":false,"label":"Bicep","permalink":"/tags/bicep","description":"The Bicep language for Azure Resource Manager templates."},{"inline":false,"label":"Azure Pipelines","permalink":"/tags/azure-pipelines","description":"The Azure Pipelines CI / CD service."},{"inline":false,"label":"Azure DevOps","permalink":"/tags/azure-devops","description":"The Azure DevOps suite of tools."}],"readingTime":4.5,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"bicep-azure-static-web-apps-azure-devops","title":"Publish Azure Static Web Apps with Bicep and Azure DevOps","authors":"johnnyreilly","tags":["azure static web apps","bicep","azure pipelines","azure devops"],"image":"./title-image.png","hide_table_of_contents":false,"description":"Learn how to deploy Azure Static Web Apps using Bicep and Azure DevOps, including workarounds for common deployment issues."},"unlisted":false,"prevItem":{"title":"Bicep: syntax highlighting with PrismJS (and Docusaurus)","permalink":"/bicep-syntax-highlighting-with-prismjs"},"nextItem":{"title":"TypeScript 4.4 and more readable code","permalink":"/typescript-4-4-more-readable-code"}},"content":"This post demonstrates how to deploy [Azure Static Web Apps](https://docs.microsoft.com/en-us/azure/static-web-apps/overview) using Bicep and Azure DevOps. It includes a few workarounds for the [\\"Provider is invalid. Cannot change the Provider. Please detach your static site first if you wish to use to another deployment provider.\\" issue](https://github.com/Azure/static-web-apps/issues/516).\\n\\n![title image reading \\"Publish Azure Static Web Apps with Bicep and Azure DevOps\\" and some Azure logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Bicep template\\n\\nThe first thing we\'re going to do is create a folder where our Bicep file for deploying our Azure Static Web App will live:\\n\\n```bash\\nmkdir infra/static-web-app -p\\n```\\n\\nThen we\'ll create a `main.bicep` file:\\n\\n```bicep\\nparam repositoryUrl string\\nparam repositoryBranch string\\n\\nparam location string = \'westeurope\'\\nparam skuName string = \'Free\'\\nparam skuTier string = \'Free\'\\n\\nparam appName string\\n\\nresource staticWebApp \'Microsoft.Web/staticSites@2020-12-01\' = {\\n  name: appName\\n  location: location\\n  sku: {\\n    name: skuName\\n    tier: skuTier\\n  }\\n  properties: {\\n    // The provider, repositoryUrl and branch fields are required for successive deployments to succeed\\n    // for more details see: https://github.com/Azure/static-web-apps/issues/516\\n    provider: \'DevOps\'\\n    repositoryUrl: repositoryUrl\\n    branch: repositoryBranch\\n    buildProperties: {\\n      skipGithubActionWorkflowGeneration: true\\n    }\\n  }\\n}\\n```\\n\\nThere\'s some things to draw attention to in the code above. The `provider`, `repositoryUrl` and `branch` fields are required for successive deployments to succeed. In our case we\'re deploying via Azure DevOps and so our provider is `\'DevOps\'`. For more details, [look at this issue](https://github.com/Azure/static-web-apps/issues/516).\\n\\n## Static Web App\\n\\nIn order that we can test out Azure Static Web Apps, what we need is a static web app. You could use pretty much anything here; we\'re going to use Docusaurus. We\'ll execute this single command:\\n\\n```bash\\nnpx @docusaurus/init@latest init static-web-app classic\\n```\\n\\nWhich will scaffold a Docusaurus site in a folder named `static-web-app`. We don\'t need to change it any further; let\'s just see if we can deploy it.\\n\\n## Azure Pipeline\\n\\nWe\'re going to add an `azure-pipelines.yml` file which Azure DevOps can use to power a pipeline:\\n\\n```yml\\ntrigger:\\n  - main\\n\\npool:\\n  vmImage: ubuntu-latest\\n\\nsteps:\\n  - checkout: self\\n    submodules: true\\n\\n  - bash: az bicep build --file infra/static-web-app/main.bicep\\n    displayName: \'Compile Bicep to ARM\'\\n\\n  - task: AzureResourceManagerTemplateDeployment@3\\n    name: DeployStaticWebAppInfra\\n    displayName: Deploy Static Web App infra\\n    inputs:\\n      deploymentScope: Resource Group\\n      azureResourceManagerConnection: $(serviceConnection)\\n      subscriptionId: $(subscriptionId)\\n      action: Create Or Update Resource Group\\n      resourceGroupName: $(azureResourceGroup)\\n      location: $(location)\\n      templateLocation: Linked artifact\\n      csmFile: \'infra/static-web-app/main.json\' # created by bash script\\n      overrideParameters: >-\\n        -repositoryUrl $(repo)\\n        -repositoryBranch $(Build.SourceBranchName)\\n        -appName $(staticWebAppName)\\n      deploymentMode: Incremental\\n      deploymentOutputs: deploymentOutputs\\n\\n  # Only necessary when consuming deploymentOutputs\\n  # - task: PowerShell@2\\n  #   name: \'SetDeploymentOutputVariables\'\\n  #   displayName: \'Set Deployment Output Variables\'\\n  #   inputs:\\n  #     targetType: inline\\n  #     script: |\\n  #       $armOutputObj = \'$(deploymentOutputs)\' | ConvertFrom-Json\\n  #       $armOutputObj.PSObject.Properties | ForEach-Object {\\n  #         $keyname = $_.Name\\n  #         $value = $_.Value.value\\n\\n  #         # Creates a standard pipeline variable\\n  #         Write-Output \\"##vso[task.setvariable variable=$keyName;issecret=true]$value\\"\\n\\n  #         # Display keys in pipeline\\n  #         Write-Output \\"output variable: $keyName\\"\\n  #       }\\n  #     pwsh: true\\n\\n  - task: AzureCLI@2\\n    displayName: \'Acquire API key for deployment\'\\n    inputs:\\n      azureSubscription: $(serviceConnection)\\n      scriptType: bash\\n      scriptLocation: inlineScript\\n      inlineScript: |\\n        APIKEY=$(az staticwebapp secrets list --name $(staticWebAppName) | jq -r \'.properties.apiKey\')\\n        echo \\"##vso[task.setvariable variable=apiKey;issecret=true]$APIKEY\\"\\n\\n  - task: AzureStaticWebApp@0\\n    name: DeployStaticWebApp\\n    displayName: Deploy Static Web App\\n    inputs:\\n      app_location: \'static-web-app\'\\n      # api_location: \'api\' # we don\'t have an API\\n      output_location: \'build\'\\n      azure_static_web_apps_api_token: $(apiKey)\\n```\\n\\nWhen the pipeline is run, it does the following:\\n\\n1. Compiles our Bicep into an ARM template\\n2. Deploys the compiled ARM template to Azure\\n3. Captures the deployment outputs (essentially the `deployment_token`) and converts them into variables to use in the pipeline\\n4. Deploys our Static Web App using the `deployment_token`\\n\\nThe pipeline depends upon a number of variables:\\n\\n- `azureResourceGroup` - the name of your resource group in Azure where the app will be deployed\\n- `location` - where your app is deployed, eg `northeurope`\\n- `repo` - the URL of your repository in Azure DevOps, eg https://dev.azure.com/johnnyreilly/_git/azure-static-web-apps\\n- `serviceConnection` - the name of your AzureRM service connection in Azure DevOps\\n- `staticWebAppName` - the name of your static web app, eg `azure-static-web-apps-johnnyreilly`\\n- `subscriptionId` - your Azure subscription id from the [Azure Portal](https://portal.azure.com)\\n\\nA successful pipeline looks something like this:\\n\\n![Screenshot of successfully running Azure Pipeline](successful-azure-pipelines-run-screenshot.png)\\n\\nWhat you might notice is that the `AzureStaticWebApp` is itself installing and building our application. This is handled by [Microsoft Oryx](https://github.com/Microsoft/Oryx). The upshot of this is that we don\'t need to manually run `npm install` and `npm build` ourselves; the `AzureStaticWebApp` task will take care of it for us.\\n\\nFinally, let\'s see if we\'ve deployed something successfully...\\n\\n![Screenshot of deployed Azure Static Web App](deployed-azure-static-web-app-screenshot.png)\\n\\nWe have! It\'s worth noting that you\'ll likely want to give your Azure Static Web App a lovelier URL, and perhaps even put it behind Azure Front Door as well.\\n\\n## `Provider is invalid` workaround 2\\n\\n[Shane Neff](https://www.linkedin.com/in/shaneneff/) was attempting to follow the instructions in this post and encountered issues. He shared his struggles with me as he encountered the [\\"Provider is invalid. Cannot change the Provider. Please detach your static site first if you wish to use to another deployment provider.\\" issue](https://github.com/Azure/static-web-apps/issues/516).\\n\\nHe was good enough to share his solution as well, which is inserting this task at the start of the pipeline (before the `az bicep build` step):\\n\\n```yml\\n- task: AzureCLI@2\\n  inputs:\\n    azureSubscription: \'<name of your service connection>\'\\n    scriptType: \'bash\'\\n    scriptLocation: \'inlineScript\'\\n    inlineScript: \'az staticwebapp disconnect -n <name of your app>\'\\n```\\n\\nI haven\'t had the problems that Shane has had myself, but I wanted to share his fix for the people out there who almost certainly are bumping on this."},{"id":"typescript-4-4-more-readable-code","metadata":{"permalink":"/typescript-4-4-more-readable-code","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-08-14-typescript-4-4-more-readable-code/index.md","source":"@site/blog/2021-08-14-typescript-4-4-more-readable-code/index.md","title":"TypeScript 4.4 and more readable code","description":"TypeScript 4.4 introduces \\"Control Flow Analysis of Aliased Conditions\\" which improves code readability by more expressive and less repetitive code.","date":"2021-08-14T00:00:00.000Z","tags":[{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":4.1,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"typescript-4-4-more-readable-code","title":"TypeScript 4.4 and more readable code","authors":"johnnyreilly","tags":["typescript"],"image":"./reactions-on-github.webp","hide_table_of_contents":false,"description":"TypeScript 4.4 introduces \\"Control Flow Analysis of Aliased Conditions\\" which improves code readability by more expressive and less repetitive code."},"unlisted":false,"prevItem":{"title":"Publish Azure Static Web Apps with Bicep and Azure DevOps","permalink":"/bicep-azure-static-web-apps-azure-devops"},"nextItem":{"title":"TypeScript, abstract classes, and constructors","permalink":"/typescript-abstract-classes-and-constructors"}},"content":"An exciting feature is shipping with TypeScript 4.4. It has the name [\\"Control Flow Analysis of Aliased Conditions\\"](https://devblogs.microsoft.com/typescript/announcing-typescript-4-4-beta/#cfa-aliased-conditions) which is quite a mouthful. This post unpacks what this feature is, and demonstrates the contribution it makes to improving the readability of code.\\n\\n\x3c!--truncate--\x3e\\n\\n## Updated 30th September 2021\\n\\nThis blog evolved to become a talk:\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/LxZx3ycrxI0\\" title=\\"YouTube video player\\" frameBorder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowFullScreen></iframe>\\n\\n\x3c!--truncate--\x3e\\n\\n## Indirect type narrowing via `const`\\n\\nOn June 24th 2021, an issue on the TypeScript GitHub repository with the title \\"Indirect type narrowing via `const`\\" was closed by [Anders Hejlsberg](https://www.twitter.com/ahejlsberg). The issue had been open since 2016 and it was closed as it was covered by [a pull request addressing control flow analysis of aliased conditional expressions and discriminants](https://github.com/microsoft/TypeScript/pull/44730).\\n\\nIt\'s fair to say that the TypeScript community was very excited about this, both judging from reactions on the issue:\\n\\n[![Screenshot of reactions on GitHub](reactions-on-github.webp)](https://github.com/microsoft/TypeScript/issues/12184#issuecomment-867928408)\\n\\nAnd also the general delight on Twitter:\\n\\n[![Screenshot of reactions on Twitter](reactions-on-twitter.webp)](https://www.twitter.com/johnny_reilly/status/1408162514504933378)\\n\\nWhat Zeh said is a great explanation of the significance of this feature:\\n\\n> Lack of type narrowing with consts made me repeat code, or avoid helpfully namef consts, too many times\\n\\nWith this feature we\'re going to have the possibility of more readable code, and less repetition. That\'s amazing!\\n\\n## The code we would like to write\\n\\nRather than starting with an explanation of what this new language feature is, let\'s instead start from the position of writing some code and seeing what\'s possible with TypeScript 4.4 that we couldn\'t tackle previously.\\n\\nHere\'s a simple function that adds all the parameters it receives and returns the total. It\'s a tolerant function and will allow people to supply numbers in the form of strings as well; so it would successfully process `\'2\'` as it would `2`. This is, of course, a slightly contrived example, but should be useful for demonstrating the new feature.\\n\\n```ts\\nfunction add(...thingsToAdd: (string | number)[]): number {\\n  let total = 0;\\n  for (const thingToAdd of thingsToAdd) {\\n    if (typeof thingToAdd === \'string\') {\\n      total += Number(thingToAdd);\\n    } else {\\n      total += thingToAdd;\\n    }\\n  }\\n  return total;\\n}\\n\\nconsole.log(add(1, \'7\', \'3\', 9));\\n```\\n\\n[Try it out in the TypeScript playground.](https://www.typescriptlang.org/play?ts=4.3.5#code/GYVwdgxgLglg9mABAQwCaoBQDodQBYxgDmAzgCpwCC6AXIhiVAE6FGIA+iYIAtgEYBTJgEoA2gF1hdbvyGIA3gChEKxABsBURFDhRkaxAF5EABgDcy1cDhN6EBI20FiFaqkRxgT1uSrphCpaqqjBeGFAAngAOAp7eLn7uhsmIAOSMLMSpAUrBeao6egYA1MYAcryCTOHORK7+FvkqAL6IAmokAoFNeYX6iKXxdYmNTc1BiOPBTJogTEh9ahbjivZgJHAaWGpwRBhomACMADRpAOypp6kAzJeIAJzCwkA)\\n\\nIf we look at this function, whilst it works, it\'s not super expressive. The `typeof thingToAdd === \'string\'` performs two purposes:\\n\\n1. It narrows the type from `string | number` to `string`\\n2. It branches the logic, such that the `string` can be coerced into a `number` and added to the total.\\n\\nYou can infer this from reading the code. However, what if we were to re-write it to capture intent? Let\'s try creating a `shouldCoerceToNumber` constant which expresses the action we need to take:\\n\\n```ts\\nfunction add(...thingsToAdd: (string | number)[]): number {\\n  let total = 0;\\n  for (const thingToAdd of thingsToAdd) {\\n    const shouldCoerceToNumber = typeof thingToAdd === \'string\';\\n    if (shouldCoerceToNumber) {\\n      total += Number(thingToAdd);\\n    } else {\\n      total += thingToAdd;\\n    }\\n  }\\n  return total;\\n}\\n\\nconsole.log(add(1, \'7\', \'3\', 9));\\n```\\n\\n[Try it out in the TypeScript playground.](https://www.typescriptlang.org/play?ts=4.3.5#code/GYVwdgxgLglg9mABAQwCaoBQDodQBYxgDmAzgCpwCC6AXIhiVAE6FGIA+iYIAtgEYBTJgEoA2gF1hdbvyGIA3gChEKxABsBURFDhRkaxAF5EABgDcy1cDhN6EBI20FiFaqkRxgT1uSrphCpaqqvZgjiR4cCBqqADCcEIQAhQAcryCtsZQAJ4ADgKe3i5+7oZliADkjCzEFRbBwTBeDJHRcQlMSanpQgFKDQPauvqIANTGabJMGPisrv71gwC+iAJqJAKBgw06egbjRUTzqIsDS0GI58FMmiBMSLv6FueKoSRwGlhqcEQYaJgARgANJUAOwVEEVADMEMQAE5hMIgA)\\n\\nThis is valid code; however TypeScript 4.3 is choking with an error:\\n\\n![Screenshot of the TypeScript playground running TypeScript 4.3 and throwing an error on our new code](doesnt-work-in-typescript-4-3.png)\\n\\nThe error being surfaced is:\\n\\n> `Operator \'+=\' cannot be applied to types \'number\' and \'string | number\'.(2365)`\\n\\nWhat\'s happening here, is TypeScript _does not remember_ that `shouldCoerceToNumber` represents a type narrowing of `thingToAdd` from `string | number` to `string`. So the type of `thingToAdd` remains unchanged from `string | number` when we write code that depends upon it.\\n\\nThis has terrible consequences. It means we can\'t write this more expressive code that we\'re interested in, and would be better for maintainers of our codebase. And this is what TypeScript 4.4, with our new feature, unlocks. Let\'s change the playground to use TypeScript 4.4 instead:\\n\\n![Screenshot of the TypeScript playground running TypeScript 4.4 and working with our new code - it shows the `thingToAdd` variable has been narrowed to a `string`](does-work-in-typescript-4-4.png)\\n\\n[Try it out in the TypeScript playground.](https://www.typescriptlang.org/play?ts=4.4.0-beta#code/GYVwdgxgLglg9mABAQwCaoBQDodQBYxgDmAzgCpwCC6AXIhiVAE6FGIA+iYIAtgEYBTJgEoA2gF1hdbvyGIA3gChEKxABsBURFDhRkaxAF5EABgDcy1cDhN6EBI20FiFaqkRxgT1uSrphCpaqqvZgjiR4cCBqqADCcEIQAhQAcryCtsZQAJ4ADgKe3i5+7oZliADkjCzEFRbBwTBeDJHRcQlMSanpQgFKDQPauvqIANTGabJMGPisrv71gwC+iAJqJAKBgw06egbjRUTzqIsDS0GI58FMmiBMSLv6FueKoSRwGlhqcEQYaJgARgANJUAOwVEEVADMEMQAE5hMIgA)\\n\\nDelightfully, we no longer have errors now we\'ve made the switch. And as the screenshot shows, the `thingToAdd` variable has been narrowed to a `string`. This is because Control Flow Analysis of Aliased Conditions is now in play.\\n\\nSo we\'re now writing more expressive code, and TypeScript is willing us on our way.\\n\\n## Read more\\n\\nThis feature is a tremendous addition to the TypeScript language. It should have a significant long-term positive impact on how people write code with TypeScript.\\n\\nTo read more, do check out the excellent [TypeScript 4.4 beta release notes](https://devblogs.microsoft.com/typescript/announcing-typescript-4-4-beta/#cfa-aliased-conditions). There\'s also some other exciting feature shipping with this release as well. Thanks very much to the TypeScript team for once again improving the language, and making a real contribution to people being able to write readable code.\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/typescript-4-4-and-more-readable-code/)\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.logrocket.com/typescript-4-4-and-more-readable-code/\\" />\\n</head>"},{"id":"typescript-abstract-classes-and-constructors","metadata":{"permalink":"/typescript-abstract-classes-and-constructors","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-08-01-typescript-abstract-classes-and-constructors/index.md","source":"@site/blog/2021-08-01-typescript-abstract-classes-and-constructors/index.md","title":"TypeScript, abstract classes, and constructors","description":"TypeScript abstract classes cannot be directly instantiated, but only used as a base for non-abstract subclasses with specific constructor usage rules.","date":"2021-08-01T00:00:00.000Z","tags":[{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":4.7,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"typescript-abstract-classes-and-constructors","title":"TypeScript, abstract classes, and constructors","authors":"johnnyreilly","tags":["typescript"],"image":"./vs-code-abstract-screenshot.png","hide_table_of_contents":false,"description":"TypeScript abstract classes cannot be directly instantiated, but only used as a base for non-abstract subclasses with specific constructor usage rules."},"unlisted":false,"prevItem":{"title":"TypeScript 4.4 and more readable code","permalink":"/typescript-4-4-more-readable-code"},"nextItem":{"title":"Directory.Build.props: C# 9 for all your projects","permalink":"/directory-build-props-c-sharp-9-for-all"}},"content":"TypeScript has the ability to define classes as abstract. This means they cannot be instantiated directly, only non-abstract subclasses can be. Let\'s take a look at what this means when it comes to constructor usage.\\n\\n\x3c!--truncate--\x3e\\n\\n## Making a scratchpad\\n\\nIn order that we can dig into this, let\'s create ourselves a scratchpad project to work with. We\'re going to create a node project and install TypeScript as a dependency.\\n\\n```bash\\nmkdir ts-abstract-constructors\\ncd ts-abstract-constructors\\nnpm init --yes\\nnpm install typescript @types/node --save-dev\\n```\\n\\nWe now have a `package.json` file set up. We need to initialise a TypeScript project as well:\\n\\n```\\nnpx tsc --init\\n```\\n\\nThis will give us a `tsconfig.json` file that will drive configuration of TypeScript. By default TypeScript transpiles to an older version of JavaScript that predates classes. So we\'ll update the config to target a newer version of the language that does include them:\\n\\n```json\\n    \\"target\\": \\"es2020\\",\\n    \\"lib\\": [\\"es2020\\"],\\n```\\n\\nLet\'s create ourselves a TypeScript file called `index.ts`. The name is not significant; we just need a file to develop in.\\n\\nFinally we\'ll add a script to our `package.json` that compiles our TypeScript to JavaScript, and then runs the JS with node:\\n\\n```json\\n\\"start\\": \\"tsc --project \\\\\\".\\\\\\" && node index.js\\"\\n```\\n\\n## Making an abstract class\\n\\nNow we\'re ready. Let\'s add an abstract class with a constructor to our `index.ts` file:\\n\\n```ts\\nabstract class ViewModel {\\n  id: string;\\n\\n  constructor(id: string) {\\n    this.id = id;\\n  }\\n}\\n```\\n\\nConsider the `ViewModel` class above. Let\'s say we\'re building some kind of CRUD app, we\'ll have different views. Each of those views will have a corresponding viewmodel which is a subclass of the `ViewModel` abstract class. The `ViewModel` class has a mandatory `id` parameter in the constructor. This is to ensure that every viewmodel has an `id` value. If this were a real app, `id` would likely be the value with which an entity was looked up in some kind of database.\\n\\nImportantly, all subclasses of `ViewModel` should either:\\n\\n- not implement a constructor at all, leaving the base class constructor to become the default constructor of the subclass _or_\\n\\n- implement their own constructor which invokes the `ViewModel` base class constructor.\\n\\n## Taking our abstract class for a spin\\n\\nNow we have it, let\'s see what we can do with our abstract class. First of all, can we instantiate our abstract class? We shouldn\'t be able to do this:\\n\\n```\\nconst viewModel = new ViewModel(\'my-id\');\\n\\nconsole.log(`the id is: ${viewModel.id}`);\\n```\\n\\nAnd sure enough, running `npm start` results in the following error (which is also being reported by our editor; VS Code).\\n\\n```shell\\nindex.ts:9:19 - error TS2511: Cannot create an instance of an abstract class.\\n\\nconst viewModel = new ViewModel(\'my-id\');\\n```\\n\\n![Screenshot of \\"Cannot create an instance of an abstract class.\\" error in VS Code](vs-code-abstract-screenshot.png)\\n\\nTremendous. However, it\'s worth remembering that `abstract` is a TypeScript concept. When we compile our TS, although it\'s throwing a compilation error, it still transpiles an `index.js` file that looks like this:\\n\\n```js\\n\'use strict\';\\nclass ViewModel {\\n  constructor(id) {\\n    this.id = id;\\n  }\\n}\\nconst viewModel = new ViewModel(\'my-id\');\\nconsole.log(`the id is: ${viewModel.id}`);\\n```\\n\\nAs we can see, there\'s no mention of `abstract`; it\'s just a straightforward `class`. In fact, if we directly execute the file with `node index.js` we can see an output of:\\n\\n```\\nthe id is: my-id\\n```\\n\\nSo the transpiled code is valid JavaScript even if the source code isn\'t valid TypeScript. This all reminds us that `abstract` is a TypeScript construct.\\n\\n## Subclassing without a new constructor\\n\\nLet\'s now create our first subclass of `ViewModel` and attempt to instantiate it:\\n\\n```ts\\nclass NoNewConstructorViewModel extends ViewModel {}\\n\\n// error TS2554: Expected 1 arguments, but got 0.\\nconst viewModel1 = new NoNewConstructorViewModel();\\n\\nconst viewModel2 = new NoNewConstructorViewModel(\'my-id\');\\n```\\n\\n![Screenshot of \\"error TS2554: Expected 1 arguments, but got 0.\\" error in VS Code](vs-code-no-new-constructor.webp)\\n\\nAs the TypeScript compiler tells us, the second of these instantiations is legitimate as it relies upon the constructor from the base class as we\'d hope. The first is not as there is no parameterless constructor.\\n\\n## Subclassing with a new constructor\\n\\nHaving done that, let\'s try subclassing and implementing a new constructor which has two parameters (to differentiate from the constructor we\'re overriding):\\n\\n```ts\\nclass NewConstructorViewModel extends ViewModel {\\n  data: string;\\n  constructor(id: string, data: string) {\\n    super(id);\\n    this.data = data;\\n  }\\n}\\n\\n// error TS2554: Expected 2 arguments, but got 0.\\nconst viewModel3 = new NewConstructorViewModel();\\n\\n// error TS2554: Expected 2 arguments, but got 1.\\nconst viewModel4 = new NewConstructorViewModel(\'my-id\');\\n\\nconst viewModel5 = new NewConstructorViewModel(\'my-id\', \'important info\');\\n```\\n\\n![Screenshot of \\"error TS2554: Expected 1 arguments, but got 1.\\" error in VS Code](vs-code-new-constructor.png)\\n\\nAgain, only one of the attempted instantiations is legitimate. `viewModel3` is not as there is no parameterless constructor. `viewModel4` is not as we have overridden the base class constructor with our new one that has two parameters. Hence `viewModel5` is our \\"Goldilocks\\" instantiation; it\'s just right!\\n\\nIt\'s also worth noting that we\'re calling `super` in the `NewConstructorViewModel` constructor. This invokes the constructor of the `ViewModel` base (or \\"super\\") class. TypeScript enforces that we pass the appropriate arguments (in our case a single `string`).\\n\\n## Wrapping it up\\n\\nWe\'ve seen that TypeScript ensures correct usage of constructors when we have an abstract class. Importantly, all subclasses of abstract classes either:\\n\\n- do not implement a constructor at all, leaving the base class constructor (the abstract constructor) to become the default constructor of the subclass _or_\\n\\n- implement their own constructor which invokes the base (or \\"super\\") class constructor with the correct arguments.\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/typescript-abstract-classes-and-constructors/)\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.logrocket.com/typescript-abstract-classes-and-constructors/\\" />\\n</head>"},{"id":"directory-build-props-c-sharp-9-for-all","metadata":{"permalink":"/directory-build-props-c-sharp-9-for-all","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-07-14-directory-build-props-c-sharp-9-for-all/index.md","source":"@site/blog/2021-07-14-directory-build-props-c-sharp-9-for-all/index.md","title":"Directory.Build.props: C# 9 for all your projects","description":"Learn how to use C# 9 with .NET Core by creating a `Directory.Build.props` file. All projects in the solution will support C#9 with no further steps.","date":"2021-07-14T00:00:00.000Z","tags":[{"inline":false,"label":"C#","permalink":"/tags/csharp","description":"The C# programming language."},{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":1.94,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"directory-build-props-c-sharp-9-for-all","title":"Directory.Build.props: C# 9 for all your projects","authors":"johnnyreilly","tags":["c#","asp.net"],"image":"./title-image.png","hide_table_of_contents":false,"description":"Learn how to use C# 9 with .NET Core by creating a `Directory.Build.props` file. All projects in the solution will support C#9 with no further steps."},"unlisted":false,"prevItem":{"title":"TypeScript, abstract classes, and constructors","permalink":"/typescript-abstract-classes-and-constructors"},"nextItem":{"title":"webpack? esbuild? Why not both?","permalink":"/webpack-esbuild-why-not-both"}},"content":".NET Core can make use of C# 9 by making some changes to your `.csproj` files. There is a way to opt all projects in a solution into this behaviour in a _single_ place, through using a `Directory.Build.props` file and / or a `Directory.Build.targets` file. Here\'s how to do it.\\n\\n![title image showing name of post and the C# logo](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## \\"have you the good news about `Directory.Build.props`\\"?\\n\\n[I wrote recently about using C# 9 with in-process Azure Functions.](../2021-07-01-c-sharp-9-azure-functions-in-process/index.md) What that amounted to, was using C# 9 with .NET Core.\\n\\nOne of the best things about blogging, is all that you get to learn along the way. After I put up that post, [Daniel Earwicker](https://twitter.com/danielearwicker) was kind enough to send this message:\\n\\n[![title image showing name of post and the C# logo](daniel-earwicker-tweet.png)](https://twitter.com/danielearwicker/status/1412678642203828226)\\n\\nI was intrigued that Daniel was able to configure all the projects in a solution to use the same approach using some strange incantations named `Directory.Build.props` and `Directory.Build.targets`. [Microsoft describes them thusly](https://docs.microsoft.com/en-us/visualstudio/msbuild/customize-your-build?view=vs-2019#directorybuildprops-and-directorybuildtargets):\\n\\n> Prior to MSBuild version 15, if you wanted to provide a new, custom property to projects in your solution, you had to manually add a reference to that property to every project file in the solution. Or, you had to define the property in a `.props` file and then explicitly import the `.props` file in every project in the solution, among other things.\\n>\\n> However, now you can add a new property to every project in one step by defining it in a single file called `Directory.Build.props` in the root folder that contains your source.\\n\\nLet\'s see if we can put it to use.\\n\\n## `Directory.Build.props`: C# 9 for all\\n\\nSo, rather than us updating each of our `.csproj` files, we should be able to create a `Directory.Build.props` file to sit alongside our `.sln` file in the root of our source code. We\'ll add this into the file:\\n\\n```xml\\n<Project>\\n <PropertyGroup>\\n    \x3c!-- use C# 9 --\x3e\\n    <LangVersion>9.0</LangVersion>\\n </PropertyGroup>\\n <ItemGroup>\\n    \x3c!-- allows some C# 9 support with .NET Core 3.1 https://github.com/manuelroemer/IsExternalInit --\x3e\\n    <PackageReference Include=\\"IsExternalInit\\" Version=\\"1.0.1\\">\\n      <IncludeAssets>runtime; build; native; contentfiles; analyzers; buildtransitive</IncludeAssets>\\n      <PrivateAssets>all</PrivateAssets>\\n    </PackageReference>\\n  </ItemGroup>\\n</Project>\\n```\\n\\nNow we\'re free to add projects into the solution, which will _already_ support C# 9 without us taking any further steps. It\'s as simple as that! Thanks to Daniel for sharing this super handy tip. \u2764\uFE0F\uD83C\uDF3B"},{"id":"webpack-esbuild-why-not-both","metadata":{"permalink":"/webpack-esbuild-why-not-both","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-07-11-webpack-esbuild-why-not-both/index.md","source":"@site/blog/2021-07-11-webpack-esbuild-why-not-both/index.md","title":"webpack? esbuild? Why not both?","description":"Using both webpack and esbuild for faster builds is possible with esbuild-loader. This post guides through using it with webpack and migrating to it.","date":"2021-07-11T00:00:00.000Z","tags":[{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."},{"inline":false,"label":"ts-loader","permalink":"/tags/ts-loader","description":"The TypeScript loader for webpack."},{"inline":false,"label":"Node.js","permalink":"/tags/node-js","description":"The Node.js runtime."}],"readingTime":7.09,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"webpack-esbuild-why-not-both","title":"webpack? esbuild? Why not both?","authors":"johnnyreilly","tags":["webpack","ts-loader","node.js"],"image":"./webpack-esbuild-why-not-both.webp","hide_table_of_contents":false,"description":"Using both webpack and esbuild for faster builds is possible with esbuild-loader. This post guides through using it with webpack and migrating to it."},"unlisted":false,"prevItem":{"title":"Directory.Build.props: C# 9 for all your projects","permalink":"/directory-build-props-c-sharp-9-for-all"},"nextItem":{"title":"Output connection strings and keys from Azure Bicep","permalink":"/output-connection-strings-and-keys-from-azure-bicep"}},"content":"Builds can be made faster using tools like [esbuild](https://github.com/evanw/esbuild). However, if you\'re invested in [webpack](https://github.com/webpack/webpack) but would still like to take advantage of speedier builds, there is a way. This post takes us through using esbuild alongside webpack using [esbuild-loader](https://github.com/privatenumber/esbuild-loader).\\n\\n![A screenshot of the \\"why not both\\" meme adapted to include webpack and esbuild](webpack-esbuild-why-not-both.webp)\\n\\n\x3c!--truncate--\x3e\\n\\n## Web development\\n\\nWith apologies to those suffering from JavaScript fatigue, once again the world of web development is evolving. It\'s long been common practice to run your JavaScript and TypeScript through some kind of Node.js based build tool, like webpack or rollup.js. These tools are written in the same language they compile to; JavaScript (or TypeScript). The new kids on the blog are tools like [esbuild](https://github.com/evanw/esbuild), [Vite](https://github.com/vitejs/vite) and [swc](https://github.com/swc-project/swc). The significant difference between these and their predecessors is that they are written in languages like Go and Rust. Go and Rust enjoy far greater performance than JavaScript. This translates into significantly faster builds. If you\'d like to read about esbuild directly there\'s a [great post](https://blog.logrocket.com/fast-javascript-bundling-with-esbuild/) about it.\\n\\nThese new tools are transformative and represent a likely future of build tooling for the web. In the long term, the likes of esbuild, Vite and friends may well come to displace the current standard build tools. So the webpacks, the rollups and so on.\\n\\nHowever, that\u2019s the long term. There\u2019s a lot of projects out there that are already heavily invested in their current build tooling. Mostly webpack. Migrating to a new build tool is no small piece of work. New projects might start with Vite, but existing ones are less likely to be ported. There\u2019s a reason webpack is so popular. It does a lot of things very well indeed. It\'s battle tested on large projects; it\'s mature and it handles many use cases.\\n\\nSo if you\u2019re a team that wants to have faster builds, but doesn\u2019t have the time to go through a big migration... Is there anything you can do? Yes. There\u2019s a middle ground to be explored. There\u2019s a relatively new project named [esbuild-loader](https://github.com/privatenumber/esbuild-loader) developed by [hiroki osame](https://twitter.com/privatenumbr). It\'s a webpack loader built on top of esbuild. It allows users to swap out `ts-loader` or `babel-loader` with itself, and massively improve build speeds.\\n\\nTo declare an interest here, I\'m the primary maintainer of [ts-loader](https://github.com/TypeStrong/ts-loader); a popular TypeScript loader that is commonly used with webpack. However, I feel strongly that the important thing here is developer productivity. As Node.js-based projects, `ts-loader` and `babel-loader` will never be able to compete with `esbuild-loader` in the same way. As a language, Go really, uh, goes!\\n\\nWhilst esbuild may not work for all use cases, it will for the majority. As such `esbuild-loader` represents a middle ground; and an early way to get access to the increased build speed that esbuild offers _without_ saying goodbye to webpack. This post will look at using `esbuild-loader` in your webpack setup.\\n\\n## Migrating an existing project to esbuild\\n\\nIt\'s very straightforward to migrate a project which uses either `babel-loader` or `ts-loader` to `esbuild-loader`. You install the dependency:\\n\\n```bash\\nnpm i -D esbuild-loader\\n```\\n\\nThen if we are currently using `babel-loader`, we make this change to our `webpack.config.js`:\\n\\n```diff\\n  module.exports = {\\n    module: {\\n      rules: [\\n-       {\\n-         test: /\\\\.js$/,\\n-         use: \'babel-loader\',\\n-       },\\n+       {\\n+         test: /\\\\.js$/,\\n+         loader: \'esbuild-loader\',\\n+         options: {\\n+           loader: \'jsx\',  // Remove this if you\'re not using JSX\\n+           target: \'es2015\'  // Syntax to compile to (see options below for possible values)\\n+         }\\n+       },\\n\\n        ...\\n      ],\\n    },\\n  }\\n```\\n\\nOr if we\'re using `ts-loader`, we make this change to our `webpack.config.js`:\\n\\n```diff\\n  module.exports = {\\n    module: {\\n      rules: [\\n-       {\\n-         test: /\\\\.tsx?$/,\\n-         use: \'ts-loader\'\\n-       },\\n+       {\\n+         test: /\\\\.tsx?$/,\\n+         loader: \'esbuild-loader\',\\n+         options: {\\n+           loader: \'tsx\',  // Or \'ts\' if you don\'t need tsx\\n+           target: \'es2015\'\\n+         }\\n+       },\\n\\n        ...\\n      ]\\n    },\\n  }\\n```\\n\\n## Creating a baseline application\\n\\nLet\'s try `esbuild-loader` out in practice. We\'re going to create a new React application using [Create React App](https://create-react-app.dev/):\\n\\n```bash\\nnpx create-react-app my-app --template typescript\\n```\\n\\nThis will scaffold out a new React application using TypeScript in the `my-app` directory. It\'s worth knowing that Create React App uses `babel-loader` behind the scenes.\\n\\nCRA also uses the [fork-ts-checker-webpack-plugin](https://github.com/TypeStrong/fork-ts-checker-webpack-plugin) to provide TypeScript type checking. This is very useful, as esbuild _just_ does transpilation and [does not intend to provide type checking support](https://esbuild.github.io/faq/#upcoming-roadmap). So it\'s tremendous we still have that plugin in place as otherwise we would lose type checking.\\n\\nSo we can understand the advantage of moving to esbuild, we first need a baseline to understand what performance looks like with babel-loader. We\'ll run `time npm run build` to execute a build of our simple app:\\n\\n![A screenshot of the completed build for Create React App](create-react-app-raw.png)\\n\\nOur complete build, TypeScript type checking, transpilation, minification and so on, all took 22.08 seconds. The question now is, what will happen if we drop esbuild into the mix?\\n\\n## CRACO\\n\\nOne way to customise a Create React App build is by running `npm run eject` and then customising the code that CRA pumps out. Doing so is fine, but it means you can\'t keep track with CRA\'s evolution. An alternative is to use a tool like [CRACO](https://github.com/gsoft-inc/craco) which allows us to tweak configuration in place. It describes itself this way:\\n\\n> *C*reate *R*eact *A*pp *C*onfiguration *O*verride is an easy and comprehensible configuration layer for create-react-app.\\n\\nWe\'re going to use CRACO, so we\'ll add `esbuild-loader` and CRACO as dependencies:\\n\\n```bash\\nnpm install @craco/craco esbuild-loader --save-dev\\n```\\n\\nThen we\'ll swap over our various `scripts` in our `package.json` to use `CRACO`:\\n\\n```json\\n\\"start\\": \\"craco start\\",\\n\\"build\\": \\"craco build\\",\\n\\"test\\": \\"craco test\\",\\n```\\n\\nOur app now uses CRACO, but we haven\'t yet configured it. So we\'ll add a `craco.config.js` file to the root of our project. This is where we swap out `babel-loader` for `esbuild-loader`:\\n\\n```js\\nconst {\\n  addAfterLoader,\\n  removeLoaders,\\n  loaderByName,\\n  getLoaders,\\n  throwUnexpectedConfigError,\\n} = require(\'@craco/craco\');\\nconst { ESBuildMinifyPlugin } = require(\'esbuild-loader\');\\n\\nconst throwError = (message) =>\\n  throwUnexpectedConfigError({\\n    packageName: \'craco\',\\n    githubRepo: \'gsoft-inc/craco\',\\n    message,\\n    githubIssueQuery: \'webpack\',\\n  });\\n\\nmodule.exports = {\\n  webpack: {\\n    configure: (webpackConfig, { paths }) => {\\n      const { hasFoundAny, matches } = getLoaders(\\n        webpackConfig,\\n        loaderByName(\'babel-loader\'),\\n      );\\n      if (!hasFoundAny) throwError(\'failed to find babel-loader\');\\n\\n      console.log(\'removing babel-loader\');\\n      const { hasRemovedAny, removedCount } = removeLoaders(\\n        webpackConfig,\\n        loaderByName(\'babel-loader\'),\\n      );\\n      if (!hasRemovedAny) throwError(\'no babel-loader to remove\');\\n      if (removedCount !== 2)\\n        throwError(\'had expected to remove 2 babel loader instances\');\\n\\n      console.log(\'adding esbuild-loader\');\\n\\n      const tsLoader = {\\n        test: /\\\\.(js|mjs|jsx|ts|tsx)$/,\\n        include: paths.appSrc,\\n        loader: require.resolve(\'esbuild-loader\'),\\n        options: {\\n          loader: \'tsx\',\\n          target: \'es2015\',\\n        },\\n      };\\n\\n      const { isAdded: tsLoaderIsAdded } = addAfterLoader(\\n        webpackConfig,\\n        loaderByName(\'url-loader\'),\\n        tsLoader,\\n      );\\n      if (!tsLoaderIsAdded) throwError(\'failed to add esbuild-loader\');\\n      console.log(\'added esbuild-loader\');\\n\\n      console.log(\'adding non-application JS babel-loader back\');\\n      const { isAdded: babelLoaderIsAdded } = addAfterLoader(\\n        webpackConfig,\\n        loaderByName(\'esbuild-loader\'),\\n        matches[1].loader, // babel-loader\\n      );\\n      if (!babelLoaderIsAdded)\\n        throwError(\'failed to add back babel-loader for non-application JS\');\\n      console.log(\'added non-application JS babel-loader back\');\\n\\n      console.log(\'replacing TerserPlugin with ESBuildMinifyPlugin\');\\n      webpackConfig.optimization.minimizer = [\\n        new ESBuildMinifyPlugin({\\n          target: \'es2015\',\\n        }),\\n      ];\\n\\n      return webpackConfig;\\n    },\\n  },\\n};\\n```\\n\\nSo what\'s happening here? The script looks for `babel-loader` usages in the default Create React App config. There will be two; one for TypeScript / JavaScript application code (we want to replace this) and one for non application JavaScript code. It\'s not too clear what non application JavaScript code there is or can be, so we\'ll leave it in place; it may be important. Significantly, the code we care about is the application code.\\n\\nYou cannot remove a _single_ loader using `CRACO`, so instead we\'ll remove both and we\'ll add back the non application JavaScript `babel-loader`. We\'ll also add `esbuild-loader` with the `{ loader: \'tsx\', target: \'es2015\' }` option set (to ensure we can process JSX/TSX).\\n\\nFinally we\'ll swap out using Terser for JavaScript minification for esbuild as well.\\n\\nOur migration is complete. The next time we build we\'ll have Create React App running using `esbuild-loader` _without_ having ejected. Once again we\'ll run `time npm run build` to execute a build of our simple app and determine how long it takes:\\n\\n![A screenshot of the completed build for Create React App with esbuild](create-react-app-esbuild.png)\\n\\nOur complete build, TypeScript type checking, transpilation, minification and so on, all took 13.85 seconds. By migrating to `esbuild-loader` we\'ve reduced our overall compilation time by approximately one third; this is a tremendous improvement!\\n\\nAs your codebase scales and your application grows, compilation time can skyrocket also. With `esbuild-loader` you should get ongoing benefits to your build time.\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/webpack-or-esbuild-why-not-both/)\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.logrocket.com/webpack-or-esbuild-why-not-both/\\" />\\n</head>"},{"id":"output-connection-strings-and-keys-from-azure-bicep","metadata":{"permalink":"/output-connection-strings-and-keys-from-azure-bicep","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-07-07-output-connection-strings-and-keys-from-azure-bicep/index.md","source":"@site/blog/2021-07-07-output-connection-strings-and-keys-from-azure-bicep/index.md","title":"Output connection strings and keys from Azure Bicep","description":"Learn how to acquire connection strings and access keys in Azure with Bicep using the `listKeys` helper, and optionally consume them in Azure Pipelines.","date":"2021-07-07T00:00:00.000Z","tags":[{"inline":false,"label":"Bicep","permalink":"/tags/bicep","description":"The Bicep language for Azure Resource Manager templates."},{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."}],"readingTime":6.38,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"output-connection-strings-and-keys-from-azure-bicep","title":"Output connection strings and keys from Azure Bicep","authors":"johnnyreilly","tags":["bicep","azure"],"image":"./title-image.png","hide_table_of_contents":false,"description":"Learn how to acquire connection strings and access keys in Azure with Bicep using the `listKeys` helper, and optionally consume them in Azure Pipelines."},"unlisted":false,"prevItem":{"title":"webpack? esbuild? Why not both?","permalink":"/webpack-esbuild-why-not-both"},"nextItem":{"title":"C# 9 in-process Azure Functions","permalink":"/c-sharp-9-azure-functions-in-process"}},"content":"If we\'re provisioning resources in Azure with Bicep, we may have a need to acquire the connection strings and keys of our newly deployed infrastructure. For example, the connection strings of an event hub or the access keys of a storage account. Perhaps we\'d like to use them to run an end-to-end test, perhaps we\'d like to store these secrets somewhere for later consumption. This post shows how to do that using Bicep and the `listKeys` helper. Optionally it shows how we could consume this in Azure Pipelines.\\n\\nPlease note that exporting keys / connection strings etc from Bicep / ARM templates is generally considered to be a less secure approach. This is because these values will be visible inside the deployments section of the Azure Portal. Anyone who has access to this will be able to see them. An alternative approach would be permissioning our pipeline to access the resources directly. You can read about that approach [here](../2021-09-12-permissioning-azure-pipelines-bicep-role-assignments/index.md).\\n\\nAlternatively, if you\'re just thinking about how to configure your Azure Container Apps / Azure Static Web Apps / Azure Function Apps etc with connection strings and keys there is another way. You can perform configuration directly within Bicep, without ever exposing secrets. [Read about that approach here](../2024-03-10-configure-azure-connection-strings-keys-in-azure-bicep/index.md).\\n\\n![image which contains the blog title](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Event Hub connection string\\n\\nFirst of all, let\'s provision an Azure Event Hub. This involves deploying an event hub namespace, an event hub in that namespace and an authorization rule. The following Bicep will do this for us:\\n\\n```bicep\\n// Create an event hub namespace\\n\\nvar eventHubNamespaceName = \'evhns-demo\'\\n\\nresource eventHubNamespace \'Microsoft.EventHub/namespaces@2021-01-01-preview\' = {\\n  name: eventHubNamespaceName\\n  location: resourceGroup().location\\n  sku: {\\n    name: \'Standard\'\\n    tier: \'Standard\'\\n    capacity: 1\\n  }\\n  properties: {\\n    zoneRedundant: true\\n  }\\n}\\n\\n// Create an event hub inside the namespace\\n\\nvar eventHubName = \'evh-demo\'\\n\\nresource eventHubNamespaceName_eventHubName \'Microsoft.EventHub/namespaces/eventhubs@2021-01-01-preview\' = {\\n  parent: eventHubNamespace\\n  name: eventHubName\\n  properties: {\\n    messageRetentionInDays: 7\\n    partitionCount: 1\\n  }\\n}\\n\\n// Grant Listen and Send on our event hub\\n\\nresource eventHubNamespaceName_eventHubName_ListenSend \'Microsoft.EventHub/namespaces/eventhubs/authorizationRules@2021-01-01-preview\' = {\\n  parent: eventHubNamespaceName_eventHubName\\n  name: \'ListenSend\'\\n  properties: {\\n    rights: [\\n      \'Listen\'\\n      \'Send\'\\n    ]\\n  }\\n  dependsOn: [\\n    eventHubNamespace\\n  ]\\n}\\n```\\n\\nWhen this is deployed to Azure, it will result in creating something like this:\\n\\n![screenshot of event hub connection strings in the Azure Portal](event-hub-connection-string.webp)\\n\\nAs we can see, there are connection strings available which can be used to access the event hub. How do we get a connection string that we can play with? It\'s easily achieved by appending the following to our Bicep:\\n\\n```bicep\\n// Determine our connection string\\n\\nvar eventHubNamespaceConnectionString = listKeys(eventHubNamespaceName_eventHubName_ListenSend.id, eventHubNamespaceName_eventHubName_ListenSend.apiVersion).primaryConnectionString\\n\\n// Output our variables\\n\\noutput eventHubNamespaceConnectionString string = eventHubNamespaceConnectionString\\noutput eventHubName string = eventHubName\\n```\\n\\nWhat we\'re doing here is using the [`listKeys`](https://docs.microsoft.com/en-us/azure/azure-resource-manager/bicep/bicep-functions-resource#list) helper on our authorization rule and retrieving the handy `primaryConnectionString`, which is then exposed as an output variable.\\n\\n## Storage Account connection string\\n\\nWe\'d like to obtain a connection string for a storage account also. Let\'s put together a Bicep file that creates a storage account and a container therein. (Incidentally, it\'s fairly common to have a storage account provisioned alongside an event hub to facilitate reading from an event hub.)\\n\\n```bicep\\n// Create a storage account\\n\\nvar storageAccountName = \'stdemo\'\\n\\nresource eventHubNamespaceName_storageAccount \'Microsoft.Storage/storageAccounts@2021-02-01\' = {\\n  name: storageAccountName\\n  location: resourceGroup().location\\n  sku: {\\n    name: \'Standard_LRS\'\\n    tier: \'Standard\'\\n  }\\n  kind: \'StorageV2\'\\n  properties: {\\n    networkAcls: {\\n      bypass: \'AzureServices\'\\n      defaultAction: \'Allow\'\\n    }\\n    accessTier: \'Hot\'\\n    allowBlobPublicAccess: false\\n    minimumTlsVersion: \'TLS1_2\'\\n    allowSharedKeyAccess: true\\n  }\\n}\\n\\n// create a container inside that storage account\\n\\nvar blobContainerName = \'test-container\'\\n\\nresource storageAccountName_default_containerName \'Microsoft.Storage/storageAccounts/blobServices/containers@2021-02-01\' = {\\n  name: \'${storageAccountName}/default/${blobContainerName}\'\\n  dependsOn: [\\n    eventHubNamespaceName_storageAccount\\n  ]\\n}\\n```\\n\\nWhen this is deployed to Azure, it will result in creating something like this:\\n\\n![screenshot of storage account access keys in the Azure Portal](storage-account-access-keys.png)\\n\\nAgain we can see, there are connection strings available in the Azure Portal, which can be used to access the storage account. However, things aren\'t quite as simple as previously; in that there doesn\'t seem to be a way to directly acquire a connection string. What we can do, is acquire a key; and construct ourselves a connection string with that. Here\'s how:\\n\\n```bicep\\n// Determine our connection string\\n\\nvar blobStorageConnectionString = \'DefaultEndpointsProtocol=https;AccountName=${eventHubNamespaceName_storageAccount.name};EndpointSuffix=${environment().suffixes.storage};AccountKey=${listKeys(eventHubNamespaceName_storageAccount.id, eventHubNamespaceName_storageAccount.apiVersion).keys[0].value}\'\\n\\n// Output our variable\\n\\noutput blobStorageConnectionString string = blobStorageConnectionString\\noutput blobContainerName string = blobContainerName\\n```\\n\\nIf you just wanted to know how to acquire connection strings from Bicep then you can stop now; we\'re done! But if you\'re curious on how the Bicep might connect to ~~the shoulder~~ Azure Pipelines... Read on.\\n\\n## From Bicep to Azure Pipelines\\n\\nIf we put together our snippets above into a single Bicep file it would look like this:\\n\\n```bicep\\n// Create an event hub namespace\\n\\nvar eventHubNamespaceName = \'evhns-demo\'\\n\\nresource eventHubNamespace \'Microsoft.EventHub/namespaces@2021-01-01-preview\' = {\\n  name: eventHubNamespaceName\\n  location: resourceGroup().location\\n  sku: {\\n    name: \'Standard\'\\n    tier: \'Standard\'\\n    capacity: 1\\n  }\\n  properties: {\\n    zoneRedundant: true\\n  }\\n}\\n\\n// Create an event hub inside the namespace\\n\\nvar eventHubName = \'evh-demo\'\\n\\nresource eventHubNamespaceName_eventHubName \'Microsoft.EventHub/namespaces/eventhubs@2021-01-01-preview\' = {\\n  parent: eventHubNamespace\\n  name: eventHubName\\n  properties: {\\n    messageRetentionInDays: 7\\n    partitionCount: 1\\n  }\\n}\\n\\n// Grant Listen and Send on our event hub\\n\\nresource eventHubNamespaceName_eventHubName_ListenSend \'Microsoft.EventHub/namespaces/eventhubs/authorizationRules@2021-01-01-preview\' = {\\n  parent: eventHubNamespaceName_eventHubName\\n  name: \'ListenSend\'\\n  properties: {\\n    rights: [\\n      \'Listen\'\\n      \'Send\'\\n    ]\\n  }\\n  dependsOn: [\\n    eventHubNamespace\\n  ]\\n}\\n\\n// Create a storage account\\n\\nvar storageAccountName = \'stdemo\'\\n\\nresource eventHubNamespaceName_storageAccount \'Microsoft.Storage/storageAccounts@2021-02-01\' = {\\n  name: storageAccountName\\n  location: resourceGroup().location\\n  sku: {\\n    name: \'Standard_LRS\'\\n    tier: \'Standard\'\\n  }\\n  kind: \'StorageV2\'\\n  properties: {\\n    networkAcls: {\\n      bypass: \'AzureServices\'\\n      defaultAction: \'Allow\'\\n    }\\n    accessTier: \'Hot\'\\n    allowBlobPublicAccess: false\\n    minimumTlsVersion: \'TLS1_2\'\\n    allowSharedKeyAccess: true\\n  }\\n}\\n\\n// create a container inside that storage account\\n\\nvar blobContainerName = \'test-container\'\\n\\nresource storageAccountName_default_containerName \'Microsoft.Storage/storageAccounts/blobServices/containers@2021-02-01\' = {\\n  name: \'${storageAccountName}/default/${blobContainerName}\'\\n  dependsOn: [\\n    eventHubNamespaceName_storageAccount\\n  ]\\n}\\n\\n// Determine our connection strings\\n\\nvar blobStorageConnectionString       = \'DefaultEndpointsProtocol=https;AccountName=${eventHubNamespaceName_storageAccount.name};EndpointSuffix=${environment().suffixes.storage};AccountKey=${listKeys(eventHubNamespaceName_storageAccount.id, eventHubNamespaceName_storageAccount.apiVersion).keys[0].value}\'\\nvar eventHubNamespaceConnectionString = listKeys(eventHubNamespaceName_eventHubName_ListenSend.id, eventHubNamespaceName_eventHubName_ListenSend.apiVersion).primaryConnectionString\\n\\n// Output our variables\\n\\noutput blobStorageConnectionString string = blobStorageConnectionString\\noutput blobContainerName string = blobContainerName\\noutput eventHubNamespaceConnectionString string = eventHubNamespaceConnectionString\\noutput eventHubName string = eventHubName\\n```\\n\\nThis might be consumed in an Azure Pipeline that looks like this:\\n\\n```yml\\n- bash: az bicep build --file infra/our-test-app/main.bicep\\n  displayName: \'Compile Bicep to ARM\'\\n\\n- task: AzureResourceManagerTemplateDeployment@3\\n  name: DeploySharedInfra\\n  displayName: Deploy Shared ARM Template\\n  inputs:\\n    deploymentScope: Resource Group\\n    azureResourceManagerConnection: ${{ parameters.serviceConnection }}\\n    subscriptionId: $(subscriptionId)\\n    action: Create Or Update Resource Group\\n    resourceGroupName: $(azureResourceGroup)\\n    location: $(location)\\n    templateLocation: Linked artifact\\n    csmFile: \'infra/our-test-app/main.json\' # created by bash script\\n    deploymentMode: Incremental\\n    deploymentOutputs: deployOutputs\\n\\n- task: PowerShell@2\\n  name: \'SetOutputVariables\'\\n  displayName: \'Set Output Variables\'\\n  inputs:\\n    targetType: inline\\n    script: |\\n      $armOutputObj = \'$(deployOutputs)\' | ConvertFrom-Json\\n      $armOutputObj.PSObject.Properties | ForEach-Object {\\n        $keyname = $_.Name\\n        $value = $_.Value.value\\n\\n        # Creates a standard pipeline variable\\n        Write-Output \\"##vso[task.setvariable variable=$keyName;]$value\\"\\n\\n        # Creates an output variable\\n        Write-Output \\"##vso[task.setvariable variable=$keyName;issecret=true;isOutput=true]$value\\"\\n\\n        # Display keys in pipeline\\n        Write-Output \\"output variable: $keyName\\"\\n      }\\n    pwsh: true\\n```\\n\\nAbove we can see:\\n\\n- the Bicep get compiled to ARM\\n- the ARM is deployed to Azure, with `deploymentOutputs` being passed out at the end\\n- the outputs are turned into secret output variables inside the pipeline (the names of which are printed to the console)\\n\\nWith the above in place, we now have all of our variables in place; `blobStorageConnectionString`, `blobContainerName`, `eventHubNamespaceConnectionString` and `eventHubName`. These could now be consumed in whatever way is useful. Consider the following:\\n\\n```yml\\n- task: UseDotNet@2\\n  displayName: \'Install .NET Core SDK 3.1.x\'\\n  inputs:\\n    packageType: \'sdk\'\\n    version: 3.1.x\\n\\n- task: DotNetCoreCLI@2\\n  displayName: \'dotnet run eventhub test\'\\n  inputs:\\n    command: \'run\'\\n    arguments: \'eventhub test --eventHubNamespaceConnectionString \\"$(eventHubNamespaceConnectionString)\\" --eventHubName \\"$(eventHubName)\\" --blobStorageConnectionString \\"$(blobStorageConnectionString)\\" --blobContainerName \\"$(blobContainerName)\\"\'\\n    workingDirectory: \'$(Build.SourcesDirectory)/OurTestApp\'\\n```\\n\\nHere we run a .NET application and pass it our connection strings. Please note, there\'s nothing .NET specific about what we\'re doing above - it could be any kind of application, bash script or similar that consumes our connection strings. The significant thing is that we can acquire connection strings in an automated fashion, for use in whichever manner pleases us."},{"id":"c-sharp-9-azure-functions-in-process","metadata":{"permalink":"/c-sharp-9-azure-functions-in-process","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-07-01-c-sharp-9-azure-functions-in-process/index.md","source":"@site/blog/2021-07-01-c-sharp-9-azure-functions-in-process/index.md","title":"C# 9 in-process Azure Functions","description":"Learn to use C# 9 with .NET Core 3.1 Azure Functions in this step-by-step guide by adding new elements to your .csproj file.","date":"2021-07-01T00:00:00.000Z","tags":[{"inline":false,"label":"C#","permalink":"/tags/csharp","description":"The C# programming language."},{"inline":false,"label":"Azure Functions","permalink":"/tags/azure-functions","description":"The Azure Functions service."},{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":4.56,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"c-sharp-9-azure-functions-in-process","title":"C# 9 in-process Azure Functions","authors":"johnnyreilly","tags":["c#","azure functions","asp.net"],"image":"./title-image.png","hide_table_of_contents":false,"description":"Learn to use C# 9 with .NET Core 3.1 Azure Functions in this step-by-step guide by adding new elements to your .csproj file."},"unlisted":false,"prevItem":{"title":"Output connection strings and keys from Azure Bicep","permalink":"/output-connection-strings-and-keys-from-azure-bicep"},"nextItem":{"title":"React 18 and TypeScript","permalink":"/react-18-and-typescript"}},"content":"C# 9 has some amazing features. Azure Functions are have two modes: isolated and in-process. Whilst isolated supports .NET 5 (and hence C# 9), in-process supports .NET Core 3.1 (C# 8). This post shows how we can use C# 9 with in-process Azure Functions running on .NET Core 3.1.\\n\\n![title image showing name of post and the Azure Functions logo](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Azure Functions: in-process and isolated\\n\\nHistorically .NET Azure Functions have been in-process. This changed with .NET 5 where a new model was introduced named \\"isolated\\". [To quote from the roadmap](https://techcommunity.microsoft.com/t5/apps-on-azure/net-on-azure-functions-roadmap/ba-p/2197916):\\n\\n> Running in an isolated process decouples .NET functions from the Azure Functions host\u2014allowing us to more easily support new .NET versions and address pain points associated with sharing a single process.\\n\\nHowever, the initial launch of isolated functions [does not have the full level of functionality enjoyed by in-process functions](https://docs.microsoft.com/en-us/azure/azure-functions/dotnet-isolated-process-guide#differences-with-net-class-library-functions). This will happen, according the roadmap:\\n\\n> Long term, our vision is to have full feature parity out of process, bringing many of the features that are currently exclusive to the in-process model to the isolated model. We plan to begin delivering improvements to the isolated model after the .NET 6 general availability release.\\n\\nIn the future, in-process functions will be retired in favour of isolated functions. However, it will be .NET 7 (scheduled to ship in November 2022) before that takes place:\\n\\n![the Azure Functions roadmap image illustrating the future of .NET functions taken from https://techcommunity.microsoft.com/t5/apps-on-azure/net-on-azure-functions-roadmap/ba-p/2197916](dotnet-functions-roadmap.webp)\\n\\nAs the image taken from the roadmap shows, when .NET 5 shipped, it did not support in-process Azure Functions. When .NET 6 ships in November, it should.\\n\\nIn the meantime, we would like to use C# 9.\\n\\n## Setting up a C# 8 project\\n\\nWe\'re have the [Azure Functions Core Tools](https://docs.microsoft.com/en-us/azure/azure-functions/functions-run-local) installed, so let\'s create a new function project:\\n\\n```bash\\nfunc new --worker-runtime dotnet --template \\"Http Trigger\\" --name \\"HelloRecord\\"\\n```\\n\\nThe above command scaffolds out a .NET Core 3.1 Azure function project which contains a single Azure function. The `--worker-runtime dotnet` parameter is what causes an in-process .NET Core 3.1 function being created. You should have a `.csproj` file that looks like this:\\n\\n```xml\\n<Project Sdk=\\"Microsoft.NET.Sdk\\">\\n  <PropertyGroup>\\n    <TargetFramework>netcoreapp3.1</TargetFramework>\\n    <AzureFunctionsVersion>v3</AzureFunctionsVersion>\\n  </PropertyGroup>\\n  <ItemGroup>\\n    <PackageReference Include=\\"Microsoft.NET.Sdk.Functions\\" Version=\\"3.0.11\\" />\\n  </ItemGroup>\\n  <ItemGroup>\\n    <None Update=\\"host.json\\">\\n      <CopyToOutputDirectory>PreserveNewest</CopyToOutputDirectory>\\n    </None>\\n    <None Update=\\"local.settings.json\\">\\n      <CopyToOutputDirectory>PreserveNewest</CopyToOutputDirectory>\\n      <CopyToPublishDirectory>Never</CopyToPublishDirectory>\\n    </None>\\n  </ItemGroup>\\n</Project>\\n```\\n\\nWe\'re running with C# 8 and .NET Core 3.1 at this point. What does it take to get us to C# 9?\\n\\n## What does it take to get to C# 9?\\n\\nThere\'s a [great post on Reddit addressing using C# 9 with .NET Core 3.1 which says:](https://www.reddit.com/r/csharp/comments/kiplz8/can_i_use_c90_with_aspnet_core_31/)\\n\\n> You can use `<LangVersion>9.0</LangVersion>`, and VS even includes support for suggesting a language upgrade.\\n>\\n> However, there are three categories of features in C#:\\n>\\n> 1. features that are entirely part of the compiler. Those will work.\\n> 2. features that require BCL additions. Since you\'re on the older BCL, those will need to be backported. For example, to use init; and record, you can use https://github.com/manuelroemer/IsExternalInit.\\n> 3. features that require runtime additions. Those cannot be added at all. For example, default interface members in C# 8, and covariant return types in C# 9.\\n\\nOf the above, 1 and 2 add a tremendous amount of value. The features of 3 are great, but more niche. Speaking personally, I care a great deal about [Record types](https://docs.microsoft.com/en-us/dotnet/csharp/whats-new/csharp-9#record-types). So let\'s apply this.\\n\\n## Adding C# 9 to the in-process function\\n\\nTo get C# into the mix, we want to make two changes:\\n\\n- add a `<LangVersion>9.0</LangVersion>` to the `<PropertyGroup>` element of our `.csproj` file\\n- add a package reference to the [`IsExternalInit`](https://github.com/manuelroemer/IsExternalInit)\\n\\nThe applied changes look like this:\\n\\n```diff\\n<Project Sdk=\\"Microsoft.NET.Sdk\\">\\n  <PropertyGroup>\\n    <TargetFramework>netcoreapp3.1</TargetFramework>\\n+    <LangVersion>9.0</LangVersion>\\n    <AzureFunctionsVersion>v3</AzureFunctionsVersion>\\n  </PropertyGroup>\\n  <ItemGroup>\\n    <PackageReference Include=\\"Microsoft.NET.Sdk.Functions\\" Version=\\"3.0.11\\" />\\n+    <PackageReference Include=\\"IsExternalInit\\" Version=\\"1.0.1\\" PrivateAssets=\\"all\\" />\\n  </ItemGroup>\\n  <ItemGroup>\\n    <None Update=\\"host.json\\">\\n      <CopyToOutputDirectory>PreserveNewest</CopyToOutputDirectory>\\n    </None>\\n    <None Update=\\"local.settings.json\\">\\n      <CopyToOutputDirectory>PreserveNewest</CopyToOutputDirectory>\\n      <CopyToPublishDirectory>Never</CopyToPublishDirectory>\\n    </None>\\n  </ItemGroup>\\n</Project>\\n```\\n\\nIf we used `dotnet add package IsExternalInit`, we might be using a different syntax in the `.csproj`. Be not afeard - that won\'t affect usage.\\n\\n## Making a C# 9 program\\n\\nNow we can theoretically use C# 9.... Let\'s use C# 9. We\'ll tweak our `HelloRecord.cs` file, add in a simple `record` named `MessageRecord` and tweak the `Run` method to use it:\\n\\n```cs\\nusing System;\\nusing System.IO;\\nusing System.Threading.Tasks;\\nusing Microsoft.AspNetCore.Mvc;\\nusing Microsoft.Azure.WebJobs;\\nusing Microsoft.Azure.WebJobs.Extensions.Http;\\nusing Microsoft.AspNetCore.Http;\\nusing Microsoft.Extensions.Logging;\\nusing Newtonsoft.Json;\\n\\nnamespace tmp\\n{\\n    public record MessageRecord(string message);\\n\\n    public static class HelloRecord\\n    {\\n        [FunctionName(\\"HelloRecord\\")]\\n        public static async Task<IActionResult> Run(\\n            [HttpTrigger(AuthorizationLevel.Function, \\"get\\", \\"post\\", Route = null)] HttpRequest req,\\n            ILogger log)\\n        {\\n            log.LogInformation(\\"C# HTTP trigger function processed a request.\\");\\n\\n            string name = req.Query[\\"name\\"];\\n\\n            string requestBody = await new StreamReader(req.Body).ReadToEndAsync();\\n            dynamic data = JsonConvert.DeserializeObject(requestBody);\\n            name = name ?? data?.name;\\n\\n            var responseMessage = new MessageRecord(string.IsNullOrEmpty(name)\\n                ? \\"This HTTP triggered function executed successfully. Pass a name in the query string or in the request body for a personalized response.\\"\\n                : $\\"Hello, {name}. This HTTP triggered function executed successfully.\\");\\n\\n            return new OkObjectResult(responseMessage);\\n        }\\n    }\\n}\\n```\\n\\nIf we kick off our function with `func start`:\\n\\n![screenshot of the output of the HelloRecord function](calling-hello-record.webp)\\n\\nWe can see we can compile, and output is as we might expect and hope. Likewise if we try and debug in VS Code, we can:\\n\\n![screenshot of the output of the HelloRecord function](debugging-hello-record.png)\\n\\n## Best before...\\n\\nSo, we\'ve now a way to use C# 9 (or most of it) with in-process .NET Core 3.1 apps. This should serve until .NET 6 ships in November 2021 and we\'re able to use C# 9 by default."},{"id":"react-18-and-typescript","metadata":{"permalink":"/react-18-and-typescript","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-06-30-react-18-and-typescript/index.md","source":"@site/blog/2021-06-30-react-18-and-typescript/index.md","title":"React 18 and TypeScript","description":"Upgrade React to `@next` and add new type definitions to use React 18 alpha with TypeScript. Use `ReactDOM.createRoot` API.","date":"2021-06-30T00:00:00.000Z","tags":[{"inline":false,"label":"React","permalink":"/tags/react","description":"The React library."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":3.44,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"react-18-and-typescript","title":"React 18 and TypeScript","authors":"johnnyreilly","tags":["react","typescript"],"image":"./createNode-error.png","hide_table_of_contents":false,"description":"Upgrade React to `@next` and add new type definitions to use React 18 alpha with TypeScript. Use `ReactDOM.createRoot` API."},"unlisted":false,"prevItem":{"title":"C# 9 in-process Azure Functions","permalink":"/c-sharp-9-azure-functions-in-process"},"nextItem":{"title":"Azure Functions and .NET 5: Query params, Dependency Injection, Bicep & Build","permalink":"/azure-functions-dotnet-5-query-params-di-bicep"}},"content":"[React 18 alpha has been released](https://reactjs.org/blog/2021/06/08/the-plan-for-react-18.html); but can we use it with TypeScript? The answer is \\"yes\\", but you need to do a couple of things to make that happen. This post will show you what to do.\\n\\n\x3c!--truncate--\x3e\\n\\n## Creating a React App with TypeScript\\n\\nLet\'s create ourselves a vanilla React TypeScript app with [Create React App](https://create-react-app.dev/):\\n\\n```shell\\nyarn create react-app my-app --template typescript\\n```\\n\\nNow let\'s upgrade the version of React to `@next`:\\n\\n```shell\\nyarn add react@next react-dom@next\\n```\\n\\nWhich will leave you with entries in the `package.json` which use React 18. It will likely look something like this:\\n\\n```json\\n    \\"react\\": \\"^18.0.0-alpha-e6be2d531\\",\\n    \\"react-dom\\": \\"^18.0.0-alpha-e6be2d531\\",\\n```\\n\\nIf we run `yarn start` we\'ll find ourselves running a React 18 app. Exciting!\\n\\n## Using the new APIs\\n\\nSo let\'s try using [`ReactDOM.createRoot`](https://github.com/reactwg/react-18/discussions/5) API. It\'s this API that opts our application into using new features of React 18. We\'ll open up `index.tsx` and make this change:\\n\\n```diff\\n-ReactDOM.render(\\n-  <React.StrictMode>\\n-    <App />\\n-  </React.StrictMode>,\\n-  document.getElementById(\'root\')\\n-);\\n+const root = ReactDOM.createRoot(document.getElementById(\'root\'));\\n+\\n+root.render(\\n+  <React.StrictMode>\\n+    <App />\\n+  </React.StrictMode>\\n+);\\n```\\n\\nIf we were running JavaScript alone, this would work. However, because we\'re using TypeScript as well, we\'re now confronted with an error:\\n\\n> `Property \'createRoot\' does not exist on type \'typeof import(\\"/code/my-app/node_modules/@types/react-dom/index\\")\'. TS2339`\\n\\n![a screenshot of the Property \'createRoot\' does not exist error](createNode-error.png)\\n\\nThis is the TypeScript compiler complaining that it doesn\'t know anything about `ReactDOM.createRoot`. This is because the type definitions that are currently in place in our application don\'t have that API defined.\\n\\nLet\'s upgrade our type definitions:\\n\\n```shell\\nyarn add @types/react @types/react-dom\\n```\\n\\nWe might reasonably hope that everything should work now. Alas it does not. The same error is presenting. TypeScript is not happy.\\n\\n## Telling TypeScript about the new APIs\\n\\nIf we take a look at the [PR that added support for the APIs](https://github.com/DefinitelyTyped/DefinitelyTyped/pull/53685), we\'ll find some tips. If you look at one of the [`next.d.ts`](https://github.com/DefinitelyTyped/DefinitelyTyped/blob/a07e9cfb005682fb6be0a2e85113eac131c3006f/types/react/next.d.ts) you\'ll find this info, courtesy of [Sebastian Silbermann](https://twitter.com/sebsilbermann):\\n\\n````ts\\n/**\\n * These are types for things that are present in the upcoming React 18 release.\\n *\\n * Once React 18 is released they can just be moved to the main index file.\\n *\\n * To load the types declared here in an actual project, there are three ways. The easiest one,\\n * if your `tsconfig.json` already has a `\\"types\\"` array in the `\\"compilerOptions\\"` section,\\n * is to add `\\"react/next\\"` to the `\\"types\\"` array.\\n *\\n * Alternatively, a specific import syntax can to be used from a typescript file.\\n * This module does not exist in reality, which is why the {} is important:\\n *\\n * ```ts\\n * import {} from \'react/next\'\\n * ```\\n *\\n * It is also possible to include it through a triple-slash reference:\\n *\\n * ```ts\\n * /// <reference types=\\"react/next\\" />\\n * ```\\n *\\n * Either the import or the reference only needs to appear once, anywhere in the project.\\n */\\n````\\n\\nLet\'s try the first item on the list. We\'ll edit our `tsconfig.json` and add a new entry to the `\\"compilerOptions\\"` section:\\n\\n```json\\n    \\"types\\": [\\"react/next\\", \\"react-dom/next\\"]\\n```\\n\\nIf we restart our build with `yarn start` we\'re now presented with a _different_ error:\\n\\n> `Argument of type \'HTMLElement | null\' is not assignable to parameter of type \'Element | Document | DocumentFragment | Comment\'. Type \'null\' is not assignable to type \'Element | Document | DocumentFragment | Comment\'. TS2345`\\n\\n![a screenshot of the null is not assignable error](null_is_not_assignable-error.png)\\n\\nNow this is actually nothing to do with issues with our new React type definitions. They are fine. This is TypeScript saying \\"it\'s not guaranteed that `document.getElementById(\'root\')` returns something that is not `null`... since we\'re in `strictNullChecks` mode you need to be sure `root` is not null\\".\\n\\nWe\'ll deal with that by testing we do have an element in play before invoking `ReactDOM.createRoot`:\\n\\n```diff\\n-const root = ReactDOM.createRoot(document.getElementById(\'root\'));\\n+const rootElement = document.getElementById(\'root\');\\n+if (!rootElement) throw new Error(\'Failed to find the root element\');\\n+const root = ReactDOM.createRoot(rootElement);\\n```\\n\\nNow that change is made, we have a working React 18 application, using TypeScript. Enjoy!\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/how-to-use-typescript-with-react-18-alpha/)\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.logrocket.com/how-to-use-typescript-with-react-18-alpha/\\" />\\n</head>"},{"id":"azure-functions-dotnet-5-query-params-di-bicep","metadata":{"permalink":"/azure-functions-dotnet-5-query-params-di-bicep","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-06-11-azure-functions-dotnet-5-query-params-di-bicep/index.md","source":"@site/blog/2021-06-11-azure-functions-dotnet-5-query-params-di-bicep/index.md","title":"Azure Functions and .NET 5: Query params, Dependency Injection, Bicep & Build","description":"The upgrade of Azure Functions from .NET Core 3.1 to .NET 5 is significant. This post shows part of the upgrade: Query params, Dependency Injection, Bicep & Build","date":"2021-06-11T00:00:00.000Z","tags":[{"inline":false,"label":"Azure Functions","permalink":"/tags/azure-functions","description":"The Azure Functions service."},{"inline":false,"label":"Bicep","permalink":"/tags/bicep","description":"The Bicep language for Azure Resource Manager templates."},{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":3.385,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-functions-dotnet-5-query-params-di-bicep","title":"Azure Functions and .NET 5: Query params, Dependency Injection, Bicep & Build","authors":"johnnyreilly","tags":["azure functions","bicep","asp.net"],"image":"./title-image.png","description":"The upgrade of Azure Functions from .NET Core 3.1 to .NET 5 is significant. This post shows part of the upgrade: Query params, Dependency Injection, Bicep & Build","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"React 18 and TypeScript","permalink":"/react-18-and-typescript"},"nextItem":{"title":"Azurite and Table Storage in a dev container","permalink":"/azurite-and-table-storage-dev-container"}},"content":"The upgrade of Azure Functions from .NET Core 3.1 to .NET 5 is significant. There\'s an excellent [guide](https://codetraveler.io/2021/05/28/creating-azure-functions-using-net-5/) for the general steps required to perform the upgrade. However there\'s a number of (unrelated) items which are not covered by that post:\\n\\n\x3c!--truncate--\x3e\\n\\n- Query params\\n- Dependency Injection\\n- Bicep\\n- Build\\n\\nThis post will show how to tackle these.\\n\\n![title image showing name of post and the Azure Functions logo](title-image.png)\\n\\n## Query params\\n\\nAs part of the move to .NET 5 functions, we say goodbye to [`HttpRequest`](https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.http.httprequest?view=aspnetcore-5.0) and hello to [`HttpRequestData`](https://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.functions.worker.http.httprequestdata?view=azure-dotnet). Now `HttpRequest` had a useful [`Query`](https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.http.httprequest.query?view=aspnetcore-5.0#Microsoft_AspNetCore_Http_HttpRequest_Query) property which allowed for the simple extraction of query parameters like so.\\n\\n```cs\\nvar from = req.Query[\\"from\\"]\\n```\\n\\n`HttpRequestData` has no such property. However, it\'s straightforward to make our own. It\'s simply a matter of using [`System.Web.HttpUtility.ParseQueryString`](https://docs.microsoft.com/en-us/dotnet/api/system.web.httputility.parsequerystring?view=net-5.0) on `req.Url.Query` and using that:\\n\\n```cs\\nvar query = System.Web.HttpUtility.ParseQueryString(req.Url.Query);\\nvar from = query[\\"from\\"]\\n```\\n\\n## Dependency Injection, local development and Azure Application Settings\\n\\nDependency Injection is a much more familiar shape in .NET 5 if you\'re familiar with .NET Core web apps. Once again we have a `Program.cs` file. To get the configuration built in such a way to support both local development and when deployed to Azure, there\'s a few things to do. When deployed to Azure you\'ll likely want to read from Azure Application Settings:\\n\\n![screenshot of Azure Application Settings](application-settings.png)\\n\\nTo tackle both of these, you\'ll want to use `AddJsonFile` and `AddEnvironmentVariables` in `ConfigureAppConfiguration`. A final `Program.cs` might look something like this:\\n\\n```cs\\nusing System;\\nusing System.Threading.Tasks;\\nusing Microsoft.Extensions.Configuration;\\nusing Microsoft.Extensions.DependencyInjection;\\nusing Microsoft.Extensions.Hosting;\\n\\nnamespace MyApp\\n{\\n    public class Program\\n    {\\n        public static Task Main(string[] args)\\n        {\\n            var host = new HostBuilder()\\n                .ConfigureAppConfiguration(configurationBuilder =>\\n                    configurationBuilder\\n                        .AddCommandLine(args)\\n                        // below is for local development\\n                        .AddJsonFile(\\"local.settings.json\\", optional: true, reloadOnChange: true)\\n                        // below is what you need to read Application Settings in Azure\\n                        .AddEnvironmentVariables()\\n                )\\n                .ConfigureFunctionsWorkerDefaults()\\n                .ConfigureServices(services =>\\n                {\\n                    services.AddLogging();\\n                    services.AddHttpClient();\\n                })\\n                .Build();\\n\\n            return host.RunAsync();\\n        }\\n    }\\n}\\n```\\n\\nWith this approach in place, when the application runs, it should construct a configuration driven by all the providers required to run our application.\\n\\n## Bicep\\n\\nWhen it comes to deploying to Azure via [Bicep](https://github.com/Azure/bicep), there\'s some small tweaks required:\\n\\n- `appSettings.FUNCTIONS_WORKER_RUNTIME` becomes `dotnet-isolated`\\n- `linuxFxVersion` becomes `DOTNET-ISOLATED|5.0`\\n\\nApplied to the resource itself the diff looks like this:\\n\\n```diff\\nresource functionAppName_resource \'Microsoft.Web/sites@2018-11-01\' = {\\n  name: functionAppName\\n  location: location\\n  tags: tags_var\\n  kind: \'functionapp,linux\'\\n  identity: {\\n    type: \'SystemAssigned\'\\n  }\\n  properties: {\\n    serverFarmId: appServicePlanName_resource.id\\n    siteConfig: {\\n      http20Enabled: true\\n      remoteDebuggingEnabled: false\\n      minTlsVersion: \'1.2\'\\n      appSettings: [\\n        {\\n          name: \'FUNCTIONS_EXTENSION_VERSION\'\\n          value: \'~3\'\\n        }\\n        {\\n          name: \'FUNCTIONS_WORKER_RUNTIME\'\\n-          value: \'dotnet\'\\n+          value: \'dotnet-isolated\'\\n        }\\n        {\\n          name: \'AzureWebJobsStorage\'\\n          value: \'DefaultEndpointsProtocol=https;AccountName=${storageAccountName};AccountKey=${listKeys(resourceId(\'Microsoft.Storage/storageAccounts\', storageAccountName), \'2019-06-01\').keys[0].value};EndpointSuffix=${environment().suffixes.storage}\'\\n        }\\n      ]\\n      connectionStrings: [\\n        {\\n          name: \'TableStorageConnectionString\'\\n          connectionString: \'DefaultEndpointsProtocol=https;AccountName=${storageAccountName};AccountKey=${listKeys(resourceId(\'Microsoft.Storage/storageAccounts\', storageAccountName), \'2019-06-01\').keys[0].value};EndpointSuffix=${environment().suffixes.storage}\'\\n        }\\n      ]\\n-      linuxFxVersion: \'DOTNETCORE|LTS\'\\n+      linuxFxVersion: \'DOTNET-ISOLATED|5.0\'\\n      ftpsState: \'Disabled\'\\n      managedServiceIdentityId: 1\\n    }\\n    clientAffinityEnabled: false\\n    httpsOnly: true\\n  }\\n}\\n```\\n\\n## Building .NET 5 functions\\n\\nBefore signing off, there\'s one more thing to slip in. When attempting to build .NET 5 Azure Functions with the .NET SDK _alone_, you\'ll encounter this error:\\n\\n```\\nThe framework \'Microsoft.NETCore.App\', version \'3.1.0\' was not found.\\n```\\n\\nDocs on this seem to be pretty short. The closest I came to docs was [this comment on Stack Overflow](https://stackoverflow.com/questions/66938752/net-5-the-framework-microsoft-netcore-app-version-3-1-0-was-not-found/66938753#66938753):\\n\\n> To build .NET 5 functions, the .NET Core 3 SDK is required. So this must be installed alongside the 5.0.x sdk.\\n\\nSo with Azure Pipelines you might have have something that looks like this:\\n\\n```yml\\nstages:\\n  - stage: build\\n    displayName: build\\n    pool:\\n      vmImage: \'ubuntu-latest\'\\n    jobs:\\n      - job: BuildAndTest\\n        displayName: \'Build and Test\'\\n        steps:\\n          # we need .NET Core SDK 3.1 too!\\n          - task: UseDotNet@2\\n            displayName: \'Install .NET Core SDK 3.1\'\\n            inputs:\\n              packageType: \'sdk\'\\n              version: 3.1.x\\n\\n          - task: UseDotNet@2\\n            displayName: \'Install .NET SDK 5.0\'\\n            inputs:\\n              packageType: \'sdk\'\\n              version: 5.0.x\\n\\n          - task: DotNetCoreCLI@2\\n            displayName: \'function app test\'\\n            inputs:\\n              command: test\\n\\n          - task: DotNetCoreCLI@2\\n            displayName: \'function app build\'\\n            inputs:\\n              command: build\\n              arguments: \'--configuration Release --output $(Build.ArtifactStagingDirectory)/MyApp\'\\n\\n          - task: DotNetCoreCLI@2\\n            displayName: \'function app publish\'\\n            inputs:\\n              command: publish\\n              arguments: \'--configuration Release --output $(Build.ArtifactStagingDirectory)/MyApp /p:SourceRevisionId=$(Build.SourceVersion)\'\\n              publishWebProjects: false\\n              modifyOutputPath: false\\n              zipAfterPublish: true\\n\\n          - publish: $(Build.ArtifactStagingDirectory)/MyApp\\n            artifact: functionapp\\n```\\n\\nHave fun building .NET 5 functions!"},{"id":"azurite-and-table-storage-dev-container","metadata":{"permalink":"/azurite-and-table-storage-dev-container","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-05-15-azurite-and-table-storage-dev-container/index.md","source":"@site/blog/2021-05-15-azurite-and-table-storage-dev-container/index.md","title":"Azurite and Table Storage in a dev container","description":"Learn to use Azurite v3 in a dev container to access the Table Storage API in preview for local development without a real database.","date":"2021-05-15T00:00:00.000Z","tags":[{"inline":false,"label":"VS Code","permalink":"/tags/vs-code","description":"The Visual Studio Code editor."}],"readingTime":6.51,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azurite-and-table-storage-dev-container","title":"Azurite and Table Storage in a dev container","authors":"johnnyreilly","tags":["vs code"],"image":"./dev-container-start.gif","hide_table_of_contents":false,"description":"Learn to use Azurite v3 in a dev container to access the Table Storage API in preview for local development without a real database."},"unlisted":false,"prevItem":{"title":"Azure Functions and .NET 5: Query params, Dependency Injection, Bicep & Build","permalink":"/azure-functions-dotnet-5-query-params-di-bicep"},"nextItem":{"title":"Create a Pipeline with the Azure DevOps API","permalink":"/create-pipeline-with-azure-devops-api"}},"content":"It\'s great to be able to develop locally without needing a \\"real\\" database to connect to. [Azurite](https://github.com/Azure/Azurite) is an Azure Storage emulator which exists to support just that. This post demonstrates how to run Azurite v3 in a [dev container](https://code.visualstudio.com/docs/remote/containers), such that you can access the Table Storage API, which is currently in preview.\\n\\n\x3c!--truncate--\x3e\\n\\n## Azurite in VS Code\\n\\n[Azurite v3.12.0](https://github.com/Azure/Azurite/releases/tag/v3.12.0) recently shipped, and with it came:\\n\\n> Preview of Table Service in npm package and docker image. (Visual Studio Code extension doesn\'t support Table Service in this release)\\n\\nYou\'ll note that whilst there\'s a VS Code extension for Azurite, it doesn\'t have support for the Table Service yet. However, we do have it available in the form of a Docker image. So whilst we may not be able to directly use the Table APIs of Azurite in VS Code, what we could do instead is use a dev container.\\n\\nWe\'ll start by making ourselves a new directory and open VS Code in that location:\\n\\n```bash\\nmkdir azurite-devcontainer\\ncode azurite-devcontainer\\n```\\n\\nWe\'re going to initialise a dev container there for function apps based upon [the example Azure Functions & C# - .NET Core 3.1 container](https://github.com/microsoft/vscode-dev-containers/tree/master/containers/azure-functions-dotnetcore-3.1). We\'ll use it later to test our Azurite connectivity. To do that let\'s create ourselves a `.devcontainer` directory:\\n\\n```bash\\nmkdir .devcontainer\\n```\\n\\nAnd inside there we\'ll add a `devcontainer.json`:\\n\\n```json\\n// For format details, see https://aka.ms/devcontainer.json. For config options, see the README at:\\n// https://github.com/microsoft/vscode-dev-containers/tree/v0.177.0/containers/azure-functions-dotnetcore-3.1\\n{\\n  \\"name\\": \\"Azurite and Azure Functions & C# - .NET Core 3.1\\",\\n  \\"dockerComposeFile\\": \\"docker-compose.yml\\",\\n  \\"service\\": \\"app\\",\\n  \\"workspaceFolder\\": \\"/workspace\\",\\n  \\"forwardPorts\\": [7071],\\n\\n  // Set *default* container specific settings.json values on container create.\\n  \\"settings\\": {\\n    \\"terminal.integrated.defaultProfile.linux\\": \\"/bin/bash\\"\\n  },\\n\\n  // Add the IDs of extensions you want installed when the container is created.\\n  \\"extensions\\": [\\n    \\"ms-azuretools.vscode-azurefunctions\\",\\n    \\"ms-dotnettools.csharp\\"\\n  ],\\n\\n  // Use \'postCreateCommand\' to run commands after the container is created.\\n  // \\"postCreateCommand\\": \\"dotnet restore\\",\\n\\n  // Comment out connect as root instead. More info: https://aka.ms/vscode-remote/containers/non-root.\\n  \\"remoteUser\\": \\"vscode\\"\\n}\\n```\\n\\nAs we can see, we\'re referencing a `docker-compose.yml` file; let\'s add that:\\n\\n```yml\\nversion: \'3\'\\n\\nservices:\\n  app:\\n    build:\\n      context: .\\n      dockerfile: Dockerfile\\n      args:\\n        # On Linux, you may need to update USER_UID and USER_GID below if not your local UID is not 1000.\\n        USER_UID: 10000\\n        USER_GID: 10000\\n\\n    init: true\\n    volumes:\\n      - ..:/workspace:cached\\n\\n    # Overrides default command so things don\'t shut down after the process ends.\\n    command: sleep infinity\\n\\n    # Uncomment the next line to use a non-root user for all processes.\\n    user: vscode\\n\\n  # run azurite and expose the relevant ports\\n  azurite:\\n    image: ./\'mcr.microsoft.com/azure-storage/azurite\'\\n    ports:\\n      - \'10000:10000\'\\n      - \'10001:10001\'\\n      - \'10002:10002\'\\n```\\n\\nIt consists of two services; `app` and `azurite`. `azurite` is the Docker image of Azurite, which exposes the Azurite ports so `app` can access it. Note the name of `azurite`; that will turn out to be significant later. We\'re actually only going to use the Table Storage port of `10002`, but this would allow us to use Blobs and Queues also. The `azurite` service is effectively going to be executing this command for us when it runs:\\n\\n```bash\\ndocker run -p 10000:10000 -p 10001:10001 -p 10002:10002 mcr.microsoft.com/azure-storage/azurite\\n```\\n\\nNow let\'s look at `app`. This is our Azure Functions container. It references a `Dockerfile` which we need to add:\\n\\n```dockerfile\\n# Find the Dockerfile for mcr.microsoft.com/azure-functions/dotnet:3.0-dotnet3-core-tools at this URL\\n# https://github.com/Azure/azure-functions-docker/blob/master/host/3.0/buster/amd64/dotnet/dotnet-core-tools.Dockerfile\\nFROM mcr.microsoft.com/azure-functions/dotnet:3.0-dotnet3-core-tools\\n```\\n\\nWe now have ourselves a dev container! VS Code should prompt us to reopen inside the container:\\n\\n![The dev container starting](dev-container-start.gif)\\n\\n## Make a function app\\n\\nNow we\'re inside our container, we\'re going to make ourselves a function app that will use Azurite. Let\'s fire up the terminal in VS Code and create a function app containing a simple HTTP function:\\n\\n```bash\\nmkdir src\\ncd src\\nfunc init TableStorage --dotnet\\ncd TableStorage\\n```\\n\\nWe need to add a package for the APIs which interact with Table Storage:\\n\\n```bash\\ndotnet restore\\ndotnet add package Microsoft.Azure.Cosmos.Table --version 1.0.8\\n```\\n\\nThe name is somewhat misleading, as it\'s both for Cosmos _and_ for Table Storage. Famously, naming things is hard \uD83D\uDE09.\\n\\nOur mission is to be able to write and read from Azurite Table Storage. We need something to read and write that we care about. I like to visit [Kew Gardens](https://www.kew.org/kew-gardens) and so let\'s imagine ourselves a system which tracks visitors to Kew.\\n\\nWe\'re going to add a class called `KewGardensVisit`:\\n\\n```cs\\nusing System;\\nusing Microsoft.Azure.Cosmos.Table;\\n\\nnamespace TableStorage\\n{\\n    public class KewGardenVisit : TableEntity\\n    {\\n        public KewGardenVisit() {}\\n        public KewGardenVisit(DateTime arrivedAt, string memberId)\\n        {\\n            PartitionKey = arrivedAt.ToString(\\"yyyy-MM-dd\\", System.Globalization.CultureInfo.InvariantCulture);\\n            RowKey = memberId;\\n\\n            ArrivedAt = arrivedAt;\\n        }\\n\\n        public DateTime ArrivedAt { get; set; }\\n    }\\n}\\n```\\n\\nNow we have our entity, let\'s add a class called `HelloAzuriteTableStorage` which will contain functions which interact with the storage:\\n\\n```cs\\nusing System;\\nusing System.Linq;\\nusing System.Threading.Tasks;\\nusing Microsoft.AspNetCore.Mvc;\\nusing Microsoft.Azure.WebJobs;\\nusing Microsoft.Azure.WebJobs.Extensions.Http;\\nusing Microsoft.AspNetCore.Http;\\nusing Microsoft.Extensions.Logging;\\nusing Microsoft.Azure.Cosmos.Table;\\n\\nnamespace TableStorage\\n{\\n    public static class HelloAzuriteTableStorage\\n    {\\n        // Note how we\'re addressing our azurite service\\n        const string AZURITE_TABLESTORAGE_CONNECTIONSTRING =\\n            \\"DefaultEndpointsProtocol=http;\\" +\\n            \\"AccountName=devstoreaccount1;\\" +\\n            \\"AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;\\" +\\n            \\"BlobEndpoint=http://azurite:10000/devstoreaccount1;\\" +\\n            \\"QueueEndpoint=http://azurite:10001/devstoreaccount1;\\" +\\n            \\"TableEndpoint=http://azurite:10002/devstoreaccount1;\\";\\n        const string TABLE_NAME = \\"KewGardenVisits\\";\\n\\n        [FunctionName(\\"SaveVisit\\")]\\n        public static async Task<IActionResult> RunSaveVisit(\\n            [HttpTrigger(AuthorizationLevel.Anonymous, \\"get\\", \\"post\\", Route = null)] HttpRequest req,\\n            ILogger log)\\n        {\\n            try\\n            {\\n                var table = await GetTable(log);\\n\\n                // Create the InsertOrReplace table operation\\n                var insertOrMergeOperation = TableOperation.InsertOrMerge(new KewGardenVisit(\\n                    arrivedAt: DateTime.UtcNow,\\n                    memberId: \\"JR123456743921\\"\\n                ));\\n\\n                // Execute the operation.\\n                TableResult result = await table.ExecuteAsync(insertOrMergeOperation);\\n                KewGardenVisit savedTelemetry = result.Result as KewGardenVisit;\\n\\n                if (result.RequestCharge.HasValue)\\n                    log.LogInformation(\\"Request Charge of InsertOrMerge Operation: \\" + result.RequestCharge);\\n\\n                return new OkObjectResult(savedTelemetry);\\n\\n            }\\n            catch (Exception e)\\n            {\\n                log.LogError(e, \\"Problem saving\\");\\n            }\\n\\n            return new BadRequestObjectResult(\\"There was an issue\\");\\n        }\\n\\n        [FunctionName(\\"GetTodaysVisits\\")]\\n        public static async Task<IActionResult> RunGetTodaysVisits(\\n            [HttpTrigger(AuthorizationLevel.Anonymous, \\"get\\", Route = null)] HttpRequest req,\\n            ILogger log)\\n        {\\n            try\\n            {\\n                var table = await GetTable(log);\\n\\n                var snowOnTheAdoTelemetries = table.CreateQuery<KewGardenVisit>()\\n                    .Where(x => x.PartitionKey == DateTime.UtcNow.ToString(\\"yyyy-MM-dd\\", System.Globalization.CultureInfo.InvariantCulture))\\n                    .ToArray();\\n\\n                return new OkObjectResult(snowOnTheAdoTelemetries);\\n\\n            }\\n            catch (Exception e)\\n            {\\n                log.LogError(e, \\"Problem loading\\");\\n                return new BadRequestObjectResult(\\"There was an issue\\");\\n            }\\n        }\\n\\n        private static async Task<CloudTable> GetTable(ILogger log)\\n        {\\n            // Construct a new TableClient using a TableSharedKeyCredential.\\n            var storageAccount = CloudStorageAccount.Parse(AZURITE_TABLESTORAGE_CONNECTIONSTRING); ;\\n\\n            // Create a table client for interacting with the table service\\n            CloudTableClient tableClient = storageAccount.CreateCloudTableClient(new TableClientConfiguration());\\n\\n            // Create a table client for interacting with the table service\\n            CloudTable table = tableClient.GetTableReference(TABLE_NAME);\\n            if (await table.CreateIfNotExistsAsync())\\n                log.LogInformation(\\"Created Table named: {0}\\", TABLE_NAME);\\n            else\\n                log.LogInformation(\\"Table {0} already exists\\", TABLE_NAME);\\n\\n            return table;\\n        }\\n    }\\n}\\n```\\n\\nThere\'s a couple of things to draw attention to here:\\n\\n- `AZURITE_TABLESTORAGE_CONNECTIONSTRING` - this mega string is based upon the [Azurite connection string docs](https://github.com/Azure/Azurite#connection-strings). The account name and key are the [Azurite default storage accounts](https://github.com/Azure/Azurite#default-storage-account). You\'ll note we target `TableEndpoint=http://azurite:10002/devstoreaccount1`. The `azurite` here is replacing the standard `127.0.0.1` where Azurite typically listens. This `azurite` name comes from the name of our service in the `docker-compose.yml` file.\\n\\n- We\'re creating two functions `SaveVisit` and `GetTodaysVisits`. `SaveVisit` creates an entry in our storage to represent someone\'s visit. It\'s a hardcoded value representing me, and we\'re exposing a write operation at a `GET` endpoint which is not very RESTful. But this is a demo and Roy Fielding would forgive us. `GetTodaysVisits` allows us to read back the visits that have happened today.\\n\\nLet\'s see if it works by entering `func start` and browsing to `http://localhost:7071/api/savevisit`\\n\\n![a screenshot of the response from the savevisits endpoint](savevisits.png)\\n\\nLooking good. Now let\'s see if we can query them at `http://localhost:7071/api/gettodaysvisits`:\\n\\n![a screenshot of the response from the gettodaysvisits endpoint](gettodaysvisits.png)\\n\\nDisco.\\n\\n## Can we swap out Azurite for The Real Thing\u2122\uFE0F?\\n\\nYou may be thinking _\\"This is great! But in the end I need to write to Azure Table Storage itself; not Azurite.\\"_\\n\\nThat\'s a fair point. Fortunately, it\'s only the connection string that determines where you read and write to. It would be fairly easy to dependency inject the appropriate connection string, or indeed a service that is connected to the storage you wish to target. If you want to make that happen, you can."},{"id":"create-pipeline-with-azure-devops-api","metadata":{"permalink":"/create-pipeline-with-azure-devops-api","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-05-08-create-pipeline-with-azure-devops-api/index.md","source":"@site/blog/2021-05-08-create-pipeline-with-azure-devops-api/index.md","title":"Create a Pipeline with the Azure DevOps API","description":"Learn how to create an Azure Pipeline using the Azure DevOps REST API with a personal access token and JSON file, as detailed in this post.","date":"2021-05-08T00:00:00.000Z","tags":[{"inline":false,"label":"Azure Pipelines","permalink":"/tags/azure-pipelines","description":"The Azure Pipelines CI / CD service."},{"inline":false,"label":"Azure DevOps","permalink":"/tags/azure-devops","description":"The Azure DevOps suite of tools."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":4.205,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"create-pipeline-with-azure-devops-api","title":"Create a Pipeline with the Azure DevOps API","authors":"johnnyreilly","tags":["azure pipelines","azure devops","typescript"],"image":"./new-pipeline.webp","hide_table_of_contents":false,"description":"Learn how to create an Azure Pipeline using the Azure DevOps REST API with a personal access token and JSON file, as detailed in this post."},"unlisted":false,"prevItem":{"title":"Azurite and Table Storage in a dev container","permalink":"/azurite-and-table-storage-dev-container"},"nextItem":{"title":"Blog Archive for Docusaurus","permalink":"/blog-archive-for-docusaurus"}},"content":"Creating an Azure Pipeline using the Azure DevOps REST API is possible, but badly documented. This post goes through how to do this; both using curl and using TypeScript.\\n\\n\x3c!--truncate--\x3e\\n\\n## curling a pipeline\\n\\nThe [documentation](https://docs.microsoft.com/en-us/rest/api/azure/devops/pipelines/pipelines/create?view=azure-devops-rest-7.1) for creating an Azure Pipeline using the Azure DevOps API is somewhat lacking. It doesn\'t mention key parameters like `repository` and `path` which are necessary to create a correctly configured pipeline. However, whilst it isn\'t documented officially, it can be done. You just need the recipe.\\n\\nHere\'s a curl to make you a pipeline:\\n\\n```bash\\ncurl  --user \'\':\'PERSONAL_ACCESS_TOKEN\' --header \\"Content-Type: application/json\\" --header \\"Accept:application/json\\" https://dev.azure.com/organisation-name/sandbox/_apis/pipelines?api-version=7.1 -d @makepipeline.json\\n```\\n\\nLooking at the above there\'s two things you need:\\n\\n1. A personal access token. You can make one of those here: https://dev.azure.com/organisation-name/_usersSettings/tokens (where `organisation-name` is the name of your organisation)\\n2. A `makepipeline.json` file, which contains the details of the pipeline you want to create:\\n\\n```json\\n{\\n  \\"folder\\": null,\\n  \\"name\\": \\"pipeline-made-by-api\\",\\n  \\"configuration\\": {\\n    \\"type\\": \\"yaml\\",\\n    \\"path\\": \\"/azure-pipelines.yml\\",\\n    \\"repository\\": {\\n      \\"id\\": \\"guid-of-repo-id\\",\\n      \\"name\\": \\"my-repo\\",\\n      \\"type\\": \\"azureReposGit\\"\\n    }\\n  }\\n}\\n```\\n\\nLet\'s talk through the significant properties above:\\n\\n- `folder` - can be `null` if you\'d like the pipeline to be created in the root of Pipelines; otherwise provide the folder name. Incidentally a `null` will be translated into a value of `\\\\\\\\` which appears to be the magic value which represents the root.\\n- `name` - your pipeline needs a name\\n- `path` - this is the path to the yaml pipelines file in the repo. Note we\'re creating the pipeline itself here; what\'s actually in the pipeline sits in that file.\\n- `repository.id` - this is the guid that represents the repo you\'re creating the pipeline for. You can find this out by going to your equivalent https://dev.azure.com/organisation-name/project-name/_settings/repositories (substituting in appropriate values) and looking up your repository there.\\n- `repository.name` - the name of your repo\\n\\nWhen you execute your curl you should be returned some JSON along these lines:\\n\\n```json\\n{\\n  \\"_links\\": {\\n    \\"self\\": {\\n      \\"href\\": \\"https://dev.azure.com/organisation-name/2184049d-8bc4-484a-91e6-00fca6b5b19f/_apis/pipelines/975?revision=1\\"\\n    },\\n    \\"web\\": {\\n      \\"href\\": \\"https://dev.azure.com/organisation-name/2184049d-8bc4-484a-91e6-00fca6b5b19f/_build/definition?definitionId=975\\"\\n    }\\n  },\\n  \\"configuration\\": {\\n    \\"path\\": \\"/azure-pipelines.yml\\",\\n    \\"repository\\": {\\n      \\"id\\": \\"9a72560d-1622-4016-93dd-32ac85b96d03\\",\\n      \\"type\\": \\"azureReposGit\\"\\n    },\\n    \\"type\\": \\"yaml\\"\\n  },\\n  \\"url\\": \\"https://dev.azure.com/organisation-name/2184049d-8bc4-484a-91e6-00fca6b5b19f/_apis/pipelines/975?revision=1\\",\\n  \\"id\\": 975,\\n  \\"revision\\": 1,\\n  \\"name\\": \\"pipeline-made-by-api\\",\\n  \\"folder\\": \\"\\\\\\\\\\"\\n}\\n```\\n\\nAnd inside Azure DevOps you\'ll now have a shiny new pipeline:\\n\\n![The new pipeline](new-pipeline.webp)\\n\\n## What if I want to use TypeScript?\\n\\nIf you\'d like to do the same in TypeScript, you can use the `azure-devops-node-api` package. Just like the official documentaion, the package is short of the necessary parameters to create a pipeline such as `repository` and `path`. But by combining a little `fetch` with the `azure-devops-node-api` you can get the job done.\\n\\nHere\'s a TypeScript example:\\n\\n```ts\\nimport * as nodeApi from \'azure-devops-node-api\';\\n\\nfunction makeWebApi({\\n  organization,\\n  personalAccessToken,\\n}: {\\n  organization: string;\\n  personalAccessToken: string;\\n}) {\\n  const authHandler = nodeApi.getPersonalAccessTokenHandler(\\n    personalAccessToken,\\n    /** allowCrossOriginAuthentication */ true,\\n  );\\n\\n  const webApi = new nodeApi.WebApi(\\n    `https://dev.azure.com/${organization}`,\\n    authHandler,\\n  );\\n\\n  return webApi;\\n}\\n\\ninterface RepositoryNameAndId {\\n  id: string;\\n  name: string;\\n}\\n\\nasync function getRepository({\\n  personalAccessToken,\\n  organization,\\n  projectName,\\n  repositoryName,\\n}: {\\n  personalAccessToken: string;\\n  organization: string;\\n  projectName: string;\\n  repositoryName: string;\\n}): Promise<RepositoryNameAndId> {\\n  const webApi = makeWebApi({ organization, personalAccessToken });\\n  const gitApi = await webApi.getGitApi();\\n\\n  const repository = await gitApi.getRepository(repositoryName, projectName);\\n  if (!repository?.id || !repository?.name) {\\n    throw new Error(`Repository ${repositoryName} not found`);\\n  }\\n\\n  return {\\n    name: repository.name,\\n    id: repository.id,\\n  };\\n}\\n\\nasync function createPipeline({\\n  personalAccessToken,\\n  organization,\\n  projectName,\\n  repositoryName,\\n  repositoryId,\\n  pathToAzurePipelinesYml,\\n  pipelineName,\\n  folderName,\\n}: {\\n  personalAccessToken: string;\\n  organization: string;\\n  projectName: string;\\n  repositoryName: string;\\n  repositoryId: string;\\n  /** eg \\"/azure-pipelines.yml\\" */\\n  pathToAzurePipelinesYml: string;\\n  pipelineName: string;\\n  folderName: string;\\n}): Promise<CreatePipelinePostResponse> {\\n  // https://learn.microsoft.com/en-us/rest/api/azure/devops/pipelines/pipelines/create?view=azure-devops-rest-7.1\\n  const url = `https://dev.azure.com/${organization}/${projectName}/_apis/pipelines?api-version=7.1`;\\n\\n  const payload: CreatePipelinePostPayload = {\\n      folder: folderName,\\n      name: pipelineName,\\n      configuration: {\\n        type: \'yaml\',\\n        path: pathToAzurePipelinesYml,\\n        repository: {\\n          id: repositoryId,\\n          name: repositoryName,\\n          type: \'azureReposGit\',\\n        },\\n      },\\n    },\\n  };\\n\\n  try {\\n    const response = await fetch(url, {\\n      method: \'POST\',\\n      headers: {\\n        Accept: \'application/json\',\\n        \'Content-Type\': \'application/json\',\\n        Authorization: `Basic ${Buffer.from(`PAT:${personalAccessToken}`).toString(\'base64\')}`,\\n        \'X-TFS-FedAuthRedirect\': \'Suppress\',\\n      },\\n      body: JSON.stringify(payload),\\n    });\\n\\n    if (!response.ok) {\\n      throw new Error(`HTTP error! status: ${response.status.toString()}`);\\n    }\\n\\n    const data = (await response.json()) as CreatePipelinePostResponse;\\n\\n    return data;\\n  } catch (error) {\\n    console.error(\'Error:\', error);\\n    throw error;\\n  }\\n}\\n\\ninterface CreatePipelinePostPayload {\\n  configuration: {\\n    type: \'yaml\';\\n    path: string;\\n    repository: {\\n      id: string;\\n      type: \'azureReposGit\';\\n      name: string;\\n    };\\n  };\\n  folder: null | string;\\n  name: string;\\n}\\n\\ninterface CreatePipelinePostResponse {\\n  _links: {\\n    self: {\\n      href: string;\\n    };\\n    web: {\\n      href: string;\\n    };\\n  };\\n  configuration: {\\n    path: string;\\n    repository: {\\n      id: string;\\n      type: string;\\n    };\\n    type: string;\\n  };\\n  folder: string;\\n  id: number;\\n  name: string;\\n  revision: number;\\n  url: string;\\n}\\n\\n// Get the repository so we can make the pipeline\\nconst repository = await getRepository({\\n  personalAccessToken: personalAccessToken,\\n  organization: azureDevOpsOrganization,\\n  projectName: azureDevOpsProject,\\n  repositoryName: azureDevOpsRepository,\\n});\\n\\nconst pipeline = await createPipeline({\\n  pathToAzurePipelinesYml: \'/.azuredevops/azure-pipeline.yml\',\\n  folderName: repository.name, // this could be `null` if you want the pipeline to be created in the root\\n  pipelineName: `${repository.name}-azure-pipeline`,\\n  organization: azureDevOpsOrganization,\\n  projectName: azureDevOpsProject,\\n  repositoryName: repository.name,\\n  repositoryId: repository.id,\\n  personalAccessToken: personalAccessToken,\\n});\\n```\\n\\nThe above code:\\n\\n1. Gets the repository id using the `azure-devops-node-api`\\n2. Creates a pipeline using `fetch` and the Azure DevOps REST API, with the types we\'ve created to help us understand the shape of the data we\'re working with"},{"id":"blog-archive-for-docusaurus","metadata":{"permalink":"/blog-archive-for-docusaurus","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-05-01-blog-archive-for-docusaurus/index.md","source":"@site/blog/2021-05-01-blog-archive-for-docusaurus/index.md","title":"Blog Archive for Docusaurus","description":"Learn how to add a blog archive to your Docusaurus blog and browse through historic posts. Follow the articles steps to implement.","date":"2021-05-01T00:00:00.000Z","tags":[{"inline":false,"label":"Docusaurus","permalink":"/tags/docusaurus","description":"The Docusaurus static site generator."},{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."}],"readingTime":5.395,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"blog-archive-for-docusaurus","title":"Blog Archive for Docusaurus","authors":"johnnyreilly","tags":["docusaurus","webpack"],"image":"./docusaurus-blog-archive.png","hide_table_of_contents":false,"description":"Learn how to add a blog archive to your Docusaurus blog and browse through historic posts. Follow the articles steps to implement."},"unlisted":false,"prevItem":{"title":"Create a Pipeline with the Azure DevOps API","permalink":"/create-pipeline-with-azure-devops-api"},"nextItem":{"title":"The Service Now API and TypeScript Conditional Types","permalink":"/service-now-api-and-typescript-conditional-types"}},"content":"Docusaurus doesn\'t ship with \\"blog archive\\" functionality. By which I mean, something that allows you to look at an overview of your historic blog posts. It turns out it is fairly straightforward to implement your own. This post does just that.\\n\\n![Docusaurus blog archive](docusaurus-blog-archive.png)\\n\\n## Updated 2021-09-01\\n\\nAs of [v2.0.0-beta.6](https://github.com/facebook/docusaurus/releases/tag/v2.0.0-beta.6), Docusauras _does_ ship with blog archive functionality that lives at the `archive` route. This is down to the work of [Gabriel Csapo](https://github.com/gabrielcsapo) in [this PR](https://github.com/facebook/docusaurus/pull/5428).\\n\\nIf you\'d like to know how to build your own, read on... But you may not need to!\\n\\n\x3c!--truncate--\x3e\\n\\n## Blogger\'s blog archive\\n\\nI recently went through the exercise of [migrating my blog from Blogger to Docusaurus](../2021-03-15-definitive-guide-to-migrating-from-blogger-to-docusaurus/index.md). I found that [Docusaurus](https://docusaurus.io/) was a tremendous platform upon which to build a blog, but it was missing a feature from Blogger that I valued highly; the blog archive:\\n\\n![Blogger blog archive](blogger-blog-archive-small.webp)\\n\\nThe blog archive is a way by which you can browse through your historic blog posts. A place where you can see all that you\'ve written and when. I find this very helpful. I didn\'t really want to make the jump without having something like that around.\\n\\n## Handrolling a Docusaurus blog archive\\n\\nLet\'s create our own blog archive in the land of the Docusaurus.\\n\\nWe\'ll create a new page under the `pages` directory called `blog-archive.js` and we\'ll add a link to it in our `docusaurus.config.js`:\\n\\n```json\\n    navbar: {\\n      // ...\\n      items: [\\n        // ...\\n        { to: \\"blog-archive\\", label: \\"Blog Archive\\", position: \\"left\\" },\\n        // ...\\n      ],\\n    },\\n```\\n\\n## Obtaining the blog data\\n\\nThis page will be powered by webpack\'s [`require.context`](https://webpack.js.org/guides/dependency-management/#requirecontext) function. `require.context` allows us to use webpack to obtain all of the blog modules:\\n\\n```js\\nrequire.context(\'../../blog\', false, //index.md/);\\n```\\n\\nThe code snippet above looks in the `blog` directory for files / modules ending with the suffix `\\"/index.md\\"`. Each one of these represents a blog post. The function returns a `context` object, which contains all of the data about these modules.\\n\\nBy reducing over that data we can construct an array of objects called `allPosts` that could drive a blog archive screen. Let\'s do this below, and we\'ll use [TypeScripts JSDoc support](https://www.typescriptlang.org/docs/handbook/jsdoc-supported-types.html) to type our JavaScript:\\n\\n```tsx\\n/**\\n * @typedef {Object} BlogPost - creates a new type named \'BlogPost\'\\n * @property {string} date - eg \\"2021-04-24T00:00:00.000Z\\"\\n * @property {string} formattedDate - eg \\"April 24, 2021\\"\\n * @property {string} title - eg \\"The Service Now API and TypeScript Conditional Types\\"\\n * @property {string} permalink - eg \\"/2021/04/24/service-now-api-and-typescript-conditional-types\\"\\n */\\n\\n/** @type {BlogPost[]} */\\nconst allPosts = ((ctx) => {\\n  /** @type {string[]} */\\n  const blogpostNames = ctx.keys();\\n\\n  return blogpostNames.reduce(\\n    (blogposts, blogpostName, i) => {\\n      const module = ctx(blogpostName);\\n      const { date, formattedDate, title, permalink } = module.metadata;\\n      return [\\n        ...blogposts,\\n        {\\n          date,\\n          formattedDate,\\n          title,\\n          permalink,\\n        },\\n      ];\\n    },\\n    /** @type {string[]}>} */ [],\\n  );\\n})(require.context(\'../../blog\', true, /index.md/));\\n```\\n\\nObserve the `metadata` property in the screenshot below:\\n\\n![require.context](require.context.png)\\n\\nThis gives us a flavour of the data available in the modules and shows how we pull out the bits that we need; `date`, `formattedDate`, `title` and `permalink`.\\n\\n## Presenting it\\n\\nNow we have our data in the form of `allPosts`, let\'s display it. We\'d like to break it up into posts by year, which we can do by reducing and looking at the `date` property which is an ISO-8601 style date string taking a format that begins `yyyy-mm-dd`:\\n\\n```tsx\\nconst postsByYear = allPosts.reduceRight((posts, post) => {\\n  const year = post.date.split(\'-\')[0];\\n  const yearPosts = posts.get(year) || [];\\n  return posts.set(year, [post, ...yearPosts]);\\n}, /** @type {Map<string, BlogPost[]>} */ new Map());\\n\\nconst yearsOfPosts = Array.from(postsByYear, ([year, posts]) => ({\\n  year,\\n  posts,\\n}));\\n```\\n\\nNow we\'re ready to blast it onto the screen. We\'ll create two components:\\n\\n- `Year` - which is a list of the posts for a given year and\\n- `BlogArchive` - which is the overall page and maps over `yearsOfPosts` to render `Year`s\\n\\n```tsx\\nfunction Year(\\n  /** @type {{ year: string; posts: BlogPost[]; }} */ { year, posts },\\n) {\\n  return (\\n    <div className={clsx(\'col col--4\', styles.feature)}>\\n      <h3>{year}</h3>\\n      <ul>\\n        {posts.map((post) => (\\n          <li key={post.date}>\\n            <Link to={post.permalink}>\\n              {post.formattedDate} - {post.title}\\n            </Link>\\n          </li>\\n        ))}\\n      </ul>\\n    </div>\\n  );\\n}\\n\\nfunction BlogArchive() {\\n  return (\\n    <Layout title=\\"Blog Archive\\">\\n      <header className={clsx(\'hero hero--primary\', styles.heroBanner)}>\\n        <div className=\\"container\\">\\n          <h1 className=\\"hero__title\\">Blog Archive</h1>\\n          <p className=\\"hero__subtitle\\">Historic posts</p>\\n        </div>\\n      </header>\\n      <main>\\n        {yearsOfPosts && yearsOfPosts.length > 0 && (\\n          <section className={styles.features}>\\n            <div className=\\"container\\">\\n              <div className=\\"row\\">\\n                {yearsOfPosts.map((props, idx) => (\\n                  <Year key={idx} {...props} />\\n                ))}\\n              </div>\\n            </div>\\n          </section>\\n        )}\\n      </main>\\n    </Layout>\\n  );\\n}\\n```\\n\\n## Bringing it all together\\n\\nWe\'re finished! We have a delightful looking blog archive plumbed into our blog:\\n\\n![Docusaurus blog archive](docusaurus-blog-archive.png)\\n\\nIt is possible that a blog archive may become natively available in Docusaurus in future. If you\'re interested in this, you can track [this issue](https://github.com/facebook/docusaurus/issues/4431).\\n\\nHere\'s the final code - which you can see [powering this screen](https://johnnyreilly.com/blog-archive). And you can see the code that backs it [here](https://github.com/johnnyreilly/blog.johnnyreilly.com/blob/main/blog-website/src/pages/blog-archive.js):\\n\\n```tsx\\nimport React from \'react\';\\nimport clsx from \'clsx\';\\nimport Layout from \'@theme/Layout\';\\nimport Link from \'@docusaurus/Link\';\\nimport styles from \'./styles.module.css\';\\n\\n/**\\n * @typedef {Object} BlogPost - creates a new type named \'BlogPost\'\\n * @property {string} date - eg \\"2021-04-24T00:00:00.000Z\\"\\n * @property {string} formattedDate - eg \\"April 24, 2021\\"\\n * @property {string} title - eg \\"The Service Now API and TypeScript Conditional Types\\"\\n * @property {string} permalink - eg \\"/2021/04/24/service-now-api-and-typescript-conditional-types\\"\\n */\\n\\n/** @type {BlogPost[]} */\\nconst allPosts = ((ctx) => {\\n  /** @type {string[]} */\\n  const blogpostNames = ctx.keys();\\n\\n  return blogpostNames.reduce(\\n    (blogposts, blogpostName, i) => {\\n      const module = ctx(blogpostName);\\n      const { date, formattedDate, title, permalink } = module.metadata;\\n      return [\\n        ...blogposts,\\n        {\\n          date,\\n          formattedDate,\\n          title,\\n          permalink,\\n        },\\n      ];\\n    },\\n    /** @type {BlogPost[]}>} */ [],\\n  );\\n  // @ts-ignore\\n})(require.context(\'../../blog\', true, /index.md/));\\n\\nconst postsByYear = allPosts.reduceRight((posts, post) => {\\n  const year = post.date.split(\'-\')[0];\\n  const yearPosts = posts.get(year) || [];\\n  return posts.set(year, [post, ...yearPosts]);\\n}, /** @type {Map<string, BlogPost[]>} */ new Map());\\n\\nconst yearsOfPosts = Array.from(postsByYear, ([year, posts]) => ({\\n  year,\\n  posts,\\n}));\\n\\nfunction Year(\\n  /** @type {{ year: string; posts: BlogPost[]; }} */ { year, posts },\\n) {\\n  return (\\n    <div className={clsx(\'col col--4\', styles.feature)}>\\n      <h3>{year}</h3>\\n      <ul>\\n        {posts.map((post) => (\\n          <li key={post.date}>\\n            <Link to={post.permalink}>\\n              {post.formattedDate} - {post.title}\\n            </Link>\\n          </li>\\n        ))}\\n      </ul>\\n    </div>\\n  );\\n}\\n\\nfunction BlogArchive() {\\n  return (\\n    <Layout title=\\"Blog Archive\\">\\n      <header className={clsx(\'hero hero--primary\', styles.heroBanner)}>\\n        <div className=\\"container\\">\\n          <h1 className=\\"hero__title\\">Blog Archive</h1>\\n          <p className=\\"hero__subtitle\\">Historic posts</p>\\n        </div>\\n      </header>\\n      <main>\\n        {yearsOfPosts && yearsOfPosts.length > 0 && (\\n          <section className={styles.features}>\\n            <div className=\\"container\\">\\n              <div className=\\"row\\">\\n                {yearsOfPosts.map((props, idx) => (\\n                  <Year key={idx} {...props} />\\n                ))}\\n              </div>\\n            </div>\\n          </section>\\n        )}\\n      </main>\\n    </Layout>\\n  );\\n}\\n\\nexport default BlogArchive;\\n```"},{"id":"service-now-api-and-typescript-conditional-types","metadata":{"permalink":"/service-now-api-and-typescript-conditional-types","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-04-24-service-now-api-and-typescript-conditional-types/index.md","source":"@site/blog/2021-04-24-service-now-api-and-typescript-conditional-types/index.md","title":"The Service Now API and TypeScript Conditional Types","description":"Learn how to model ServiceNow REST API results using TypeScript conditional types to minimise repetition and remain strongly typed.","date":"2021-04-24T00:00:00.000Z","tags":[{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":7.755,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"service-now-api-and-typescript-conditional-types","title":"The Service Now API and TypeScript Conditional Types","authors":"johnnyreilly","tags":["typescript"],"image":"./ts-ervice-now.png","hide_table_of_contents":false,"description":"Learn how to model ServiceNow REST API results using TypeScript conditional types to minimise repetition and remain strongly typed."},"unlisted":false,"prevItem":{"title":"Blog Archive for Docusaurus","permalink":"/blog-archive-for-docusaurus"},"nextItem":{"title":"ts-loader goes webpack 5","permalink":"/ts-loader-goes-webpack-5"}},"content":"The [Service Now REST API](https://docs.servicenow.com/bundle/paris-application-development/page/build/applications/concept/api-rest.html) is an API which allows you to interact with Service Now. It produces different shaped results based upon the [`sysparm_display_value` query parameter](https://docs.servicenow.com/bundle/paris-application-development/page/integrate/inbound-rest/concept/c_TableAPI.html#c_TableAPI__table-GET). This post looks at how we can model these API results with TypeScripts conditional types. The aim being to minimise repetition whilst remaining strongly typed. This post is specifically about the Service Now API, but the principles around conditional type usage are generally applicable.\\n\\n![Service Now and TypeScript](ts-ervice-now.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## The power of a query parameter\\n\\nThere is a query parameter which many endpoints in Service Nows Table API support named `sysparm_display_value`. The docs describe it thus:\\n\\n> Data retrieval operation for reference and choice fields.\\n> Based on this value, retrieves the display value and/or the actual value from the database.\\n>\\n> Valid values:\\n>\\n> - `true`: Returns the display values for all fields.\\n> - `false`: Returns the actual values from the database.\\n> - `all`: Returns both actual and display value\\n\\nLet\'s see what that looks like when it comes to loading a Change Request. Consider the following curls:\\n\\n```shell\\n# sysparm_display_value=all\\ncurl \\"https://ourcompanyinstance.service-now.com/api/now/table/change_request?sysparm_query=number=CHG0122585&sysparm_limit=1&sysparm_display_value=all\\" --request GET --header \\"Accept:application/json\\" --user \'API_USERNAME\':\'API_PASSWORD\' | jq \'.result[0] | { state, sys_id, number, requested_by, reason }\'\\n\\n# sysparm_display_value=true\\ncurl \\"https://ourcompanyinstance.service-now.com/api/now/table/change_request?sysparm_query=number=CHG0122585&sysparm_limit=1&sysparm_display_value=true\\" --request GET --header \\"Accept:application/json\\" --user \'API_USERNAME\':\'API_PASSWORD\' | jq \'.result[0] | { state, sys_id, number, requested_by, reason }\'\\n\\n# sysparm_display_value=false\\ncurl \\"https://ourcompanyinstance.service-now.com/api/now/table/change_request?sysparm_query=number=CHG0122585&sysparm_limit=1&sysparm_display_value=false\\" --request GET --header \\"Accept:application/json\\" --user \'API_USERNAME\':\'API_PASSWORD\' | jq \'.result[0] | { state, sys_id, number, requested_by, reason }\'\\n```\\n\\nWhen executed, they each load the same Change Request from Service Now with a different value for `sysparm_display_value`. You\'ll notice there\'s some [`jq`](https://stedolan.github.io/jq/) in the mix as well. This is because there\'s a _lot_ of data in a Change Request. Rather than display everything, we\'re displaying a subset of fields. The first curl has a `sysparm_display_value` value of `all`, the second `false` and the third `true`. What do the results look like?\\n\\n`sysparm_display_value=all`:\\n\\n```json\\n{\\n  \\"state\\": {\\n    \\"display_value\\": \\"Closed\\",\\n    \\"value\\": \\"3\\"\\n  },\\n  \\"sys_id\\": {\\n    \\"display_value\\": \\"4d54d7481b37e010d315cbb5464bcb95\\",\\n    \\"value\\": \\"4d54d7481b37e010d315cbb5464bcb95\\"\\n  },\\n  \\"number\\": {\\n    \\"display_value\\": \\"CHG0122595\\",\\n    \\"value\\": \\"CHG0122595\\"\\n  },\\n  \\"requested_by\\": {\\n    \\"display_value\\": \\"Sally Omer\\",\\n    \\"link\\": \\"https://ourcompanyinstance.service-now.com/api/now/table/sys_user/b15cf3ebdbe11300f196f3651d961999\\",\\n    \\"value\\": \\"b15cf3ebdbe11300f196f3651d961999\\"\\n  },\\n  \\"reason\\": {\\n    \\"display_value\\": null,\\n    \\"value\\": \\"\\"\\n  }\\n}\\n```\\n\\n`sysparm_display_value=true`:\\n\\n```json\\n{\\n  \\"state\\": \\"Closed\\",\\n  \\"sys_id\\": \\"4d54d7481b37e010d315cbb5464bcb95\\",\\n  \\"number\\": \\"CHG0122595\\",\\n  \\"requested_by\\": {\\n    \\"display_value\\": \\"Sally Omer\\",\\n    \\"link\\": \\"https://ourcompanyinstance.service-now.com/api/now/table/sys_user/b15cf3ebdbe11300f196f3651d961999\\"\\n  },\\n  \\"reason\\": null\\n}\\n```\\n\\n`sysparm_display_value=false`:\\n\\n```json\\n{\\n  \\"state\\": \\"3\\",\\n  \\"sys_id\\": \\"4d54d7481b37e010d315cbb5464bcb95\\",\\n  \\"number\\": \\"CHG0122595\\",\\n  \\"requested_by\\": {\\n    \\"link\\": \\"https://ourcompanyinstance.service-now.com/api/now/table/sys_user/b15cf3ebdbe11300f196f3651d961999\\",\\n    \\"value\\": \\"b15cf3ebdbe11300f196f3651d961999\\"\\n  },\\n  \\"reason\\": \\"\\"\\n}\\n```\\n\\nAs you can see, we have the same properties being returned each time, but with a different shape. Let\'s call out some interesting highlights:\\n\\n- `requested_by` is _always_ an object which contains `link`. It may also contain `value` and `display_value` depending upon `sysparm_display_value`\\n- `state`, `sys_id`, `number` and `reason` are objects containing `value` and `display_value` when `sysparm_display_value` is `all`. Otherwise, the value of `value` or `display_value` is surfaced up directly; not in an object.\\n- most values are strings, even if they represent another data type. So `state.value` is always a stringified number. The only exception to this rule is `reason.display_value` which can be `null`\\n\\n## Type Definition time\\n\\nWe want to create type definitions for these API results. We could of course create three different results, but that would involve duplication. Boo! It\'s worth bearing in mind we\'re looking at a subset of five properties in this example. In reality, there are many, many properties on a Change Request. Whilst this example is for a subset, if we wanted to go on to create the full type definition the duplication would become very impractical.\\n\\nWhat can we do? Well, if all of the underlying properties were of the same type, we could use a generic and be done. But given the underlying types can vary, that\'s not going to work. We can achieve this though through using a combination of generics and conditional types.\\n\\nLet\'s begin by creating a string literal type of the possible values of `sysparm_display_value`:\\n\\n```ts\\nexport type DisplayValue = \'all\' | \'true\' | \'false\';\\n```\\n\\n## Making a `PropertyValue` type\\n\\nNext we need to create a type that models the object with `display_value` and `value` properties.\\n\\n:::info a type for state, sys_id, number and reason\\n\\n- `state`, `sys_id`, `number` and `reason` are objects containing `value` and `display_value` when `sysparm_display_value` is `\'all\'`. Otherwise, the value of `value` or `display` is surfaced up directly; not in an object.\\n- most values are strings, even if they represent another data type. So `state.value` is always a stringified number. The only exception to this rule is `reason.display_value` which can be `null`\\n\\n:::\\n\\n```ts\\nexport interface ValueAndDisplayValue<TValue = string, TDisplayValue = string> {\\n  display_value: TDisplayValue;\\n  value: TValue;\\n}\\n```\\n\\nNote that this is a generic property with a default type of `string` for both `display_value` and `value`. Most of the time, `string` is the type in question so it\'s great that TypeScript allows us to cut down on the amount of syntax we use.\\n\\nNow we\'re going to create our first conditional type:\\n\\n```ts\\nexport type PropertyValue<\\n  TAllTrueFalse extends DisplayValue,\\n  TValue = string,\\n  TDisplayValue = string,\\n> = TAllTrueFalse extends \'all\'\\n  ? ValueAndDisplayValue<TValue, TDisplayValue>\\n  : TAllTrueFalse extends \'true\'\\n  ? TDisplayValue\\n  : TValue;\\n```\\n\\nThe `PropertyValue` will either be a `ValueAndDisplayValue`, a `TDisplayValue` or a `TValue`, depending upon whether `PropertyValue` is `\'all\'`, `\'true\'` or `\'false\'` respectively. That\'s hard to grok. Let\'s look at an example of each of those cases using the `reason` property, which allows a `TValue` of `string` and a `TDisplayValue` of `string | null`:\\n\\n```ts\\nconst reasonAll: PropertyValue<\'all\', string, string | null> = {\\n  display_value: null,\\n  value: \'\',\\n};\\nconst reasonTrue: PropertyValue<\'true\', string, string | null> = null;\\nconst reasonFalse: PropertyValue<\'false\', string, string | null> = \'\';\\n```\\n\\nConsider the type on the left and the value on the right. We\'re successfully modelling our `PropertyValue`s. I\'ve deliberately picked an edge case example to push our conditional type to its limits.\\n\\n## Service Now Change Request States\\n\\nLet\'s look at another usage. We\'ll create a type that repesents the possible values of a Change Request\'s `state` in Service Now. Do take a moment to appreciate these values. Many engineers were lost in the numerous missions to obtain these rare and secret enums. Alas, the Service Now API docs have some significant gaps.\\n\\n```ts\\n/** represents the possible Change Request \\"State\\" values in Service Now */\\nexport const STATE = {\\n  NEW: \'-5\',\\n  ASSESS: \'-4\',\\n  SENT_FOR_APPROVAL: \'-3\',\\n  SCHEDULED: \'-2\',\\n  APPROVED: \'-1\',\\n  WAITING: \'1\',\\n  IN_PROGRESS: \'2\',\\n  COMPLETE: \'3\',\\n  ERROR: \'4\',\\n  CLOSED: \'7\',\\n} as const;\\n\\nexport type State = (typeof STATE)[keyof typeof STATE];\\n```\\n\\nBy combining `State` and `PropertyValue`, we can strongly type the `state` property of Change Requests. Consider:\\n\\n```ts\\nconst stateAll: PropertyValue<\'all\', State> = {\\n  display_value: \'Closed\',\\n  value: \'3\',\\n};\\nconst stateTrue: PropertyValue<\'true\', State> = \'Closed\';\\nconst stateFalse: PropertyValue<\'false\', State> = \'3\';\\n```\\n\\nWith that in place, let\'s turn our attention to our other natural type that the `requested_by` property demonstrates.\\n\\n## Making a `LinkValue` type\\n\\n:::info a type for requested_by\\n\\n`requested_by` is _always_ an object which contains `link`. It may also contain `value` and `display_value` depending upon `sysparm_display_value`\\n\\n:::\\n\\n```ts\\ninterface Link {\\n  link: string;\\n}\\n\\n/** when TAllTrueFalse is \'false\' */\\nexport interface LinkAndValue extends Link {\\n  value: string;\\n}\\n\\n/** when TAllTrueFalse is \'true\' */\\nexport interface LinkAndDisplayValue extends Link {\\n  display_value: string;\\n}\\n\\n/** when TAllTrueFalse is \'all\' */\\nexport interface LinkValueAndDisplayValue\\n  extends LinkAndValue,\\n    LinkAndDisplayValue {}\\n```\\n\\nThe three types above model the different scenarios. Now we need a conditional type to make use of them:\\n\\n```ts\\nexport type LinkValue<TAllTrueFalse extends DisplayValue> =\\n  TAllTrueFalse extends \'all\'\\n    ? LinkValueAndDisplayValue\\n    : TAllTrueFalse extends \'true\'\\n    ? LinkAndDisplayValue\\n    : LinkAndValue;\\n```\\n\\nThis is hopefully simpler to read than the `PropertyValue` type, and if you look at the examples below you can see what usage looks like:\\n\\n```ts\\nconst requested_byAll: LinkValue<\'all\'> = {\\n  display_value: \'Sally Omer\',\\n  link: \'https://ourcompanyinstance.service-now.com/api/now/table/sys_user/b15cf3ebdbe11300f196f3651d961999\',\\n  value: \'b15cf3ebdbe11300f196f3651d961999\',\\n};\\nconst requested_byTrue: LinkValue<\'true\'> = {\\n  display_value: \'Sally Omer\',\\n  link: \'https://ourcompanyinstance.service-now.com/api/now/table/sys_user/b15cf3ebdbe11300f196f3651d961999\',\\n};\\nconst requested_byFalse: LinkValue<\'false\'> = {\\n  link: \'https://ourcompanyinstance.service-now.com/api/now/table/sys_user/b15cf3ebdbe11300f196f3651d961999\',\\n  value: \'b15cf3ebdbe11300f196f3651d961999\',\\n};\\n```\\n\\n## Making our complete type\\n\\nWith these primitives in place, we can now build ourself a (cut-down) type that models a Change Request:\\n\\n```ts\\nexport interface ServiceNowChangeRequest<TAllTrueFalse extends DisplayValue> {\\n  state: PropertyValue<TAllTrueFalse, State>;\\n  sys_id: PropertyValue<TAllTrueFalse>;\\n  number: PropertyValue<TAllTrueFalse>;\\n  requested_by: LinkValue<TAllTrueFalse>;\\n  reason: PropertyValue<TAllTrueFalse, string, string | null>;\\n  // there are *way* more properties in reality\\n}\\n```\\n\\nThis is a generic type which will accept `\'all\'`, `\'true\'` or `\'false\'` and will use that type to drive the type of the properties _inside_ the object. And now we have successfully typed our Service Now Change Request, thanks to TypeScript\'s conditional types.\\n\\nTo test it out, let\'s take the JSON responses we got back from our curls at the start, and see if we can make `ServiceNowChangeRequest`s with them.\\n\\n```ts\\nconst changeRequestFalse: ServiceNowChangeRequest<\'false\'> = {\\n  state: \'3\',\\n  sys_id: \'4d54d7481b37e010d315cbb5464bcb95\',\\n  number: \'CHG0122595\',\\n  requested_by: {\\n    link: \'https://ourcompanyinstance.service-now.com/api/now/table/sys_user/b15cf3ebdbe11300f196f3651d961999\',\\n    value: \'b15cf3ebdbe11300f196f3651d961999\',\\n  },\\n  reason: \'\',\\n};\\n\\nconst changeRequestTrue: ServiceNowChangeRequest<\'true\'> = {\\n  state: \'Closed\',\\n  sys_id: \'4d54d7481b37e010d315cbb5464bcb95\',\\n  number: \'CHG0122595\',\\n  requested_by: {\\n    display_value: \'Sally Omer\',\\n    link: \'https://ourcompanyinstance.service-now.com/api/now/table/sys_user/b15cf3ebdbe11300f196f3651d961999\',\\n  },\\n  reason: null,\\n};\\n\\nconst changeRequestAll: ServiceNowChangeRequest<\'all\'> = {\\n  state: {\\n    display_value: \'Closed\',\\n    value: \'3\',\\n  },\\n  sys_id: {\\n    display_value: \'4d54d7481b37e010d315cbb5464bcb95\',\\n    value: \'4d54d7481b37e010d315cbb5464bcb95\',\\n  },\\n  number: {\\n    display_value: \'CHG0122595\',\\n    value: \'CHG0122595\',\\n  },\\n  requested_by: {\\n    display_value: \'Sally Omer\',\\n    link: \'https://ourcompanyinstance.service-now.com/api/now/table/sys_user/b15cf3ebdbe11300f196f3651d961999\',\\n    value: \'b15cf3ebdbe11300f196f3651d961999\',\\n  },\\n  reason: {\\n    display_value: null,\\n    value: \'\',\\n  },\\n};\\n```\\n\\nWe can! Do take a look at this in the [TypeScript playground](https://www.typescriptlang.org/play?#code/KYDwDg9gTgLgBDAnmYcAiBLAzmANgQ0QDV9cBXVAXjgHJTca4AfWmKCxlmgM1K2BoBuAFDDQkWHAwA7GMCi8AxqhLlgAQWkATTDgLFSFADwAVVRTjUsbGQHMANHBO68hc1TjWodgHxwA3sJwwXBa2K6IAPoAbobAAFxOLvruIiFwsWqJZnEiAL6i4tDwSChwAApQECiwBmpGQSEm6ri4JuzAAGJ8qKBy2ljo4Slx9o3BOWqWnjbSDuNJw25x0152wn7Uza3tFN24-HB9wAO09DQLAPxw7po6S3XGkxSOzg-uPgvZLW0d+4fHU40NgcK6LPTLNRfJypUSKCDSaxwKDAfBYBE-RKVaryJDuIx0Vo0RxrOYk2a2ZhwaRkVqbAILABEYQhUUyFEZiRprTG6UZ7OAnLgjMZwgK8MR8BRaIRuwSFSqNTxcQJIIE5O8ZJmmspLG5uHp+pEEqR0vR0n+8uxSsewAJvAO6u1dg1dip+vpNCEogA9AAqP3I4BgFH8WSDGAAC1QkCwWAwACNcKgAMKR-BzVAAJWAAEcKEjGQBlGD4OSMjJxQYyOBF+TRDDKOAAOQgAHc4H6fWJwMU4Cb4EXmiYAKLTQLpZsjgDqiRoAFoAKzEhbqItFkfrufzgAsK-SG+bJkinQA8lnIupyuUs6eiOoADLbgDM+5CRZTAAkR2gAKoPn9twAJjfYIrxvO9ANoecAEZQLgad1AASRMJDmwAcTnODeRCNDIgg9Cs03Is5xAnDghTU8AFlygA0c51fci4BHLNbyzOc9yYlMH1PDc0DnAB2Fc8jgNF+wRawRB7CQSmQVASzLDxSmACBuFrYcRwAbQAa2ARBVIQOSDKHdRRwAXSkgcZkUzEFRxWp8UJBhHAUuR6QnEJmQeGI4iFRkU1wCB+C0RkmP5XzEkZZ9RTyY0JPgaxFLlLFFVxW1VQ6Yla1LNzpn8wLgsZOLJWsuRLRS+zlXqHgeiy1zgHpKKitEGQ5AUfAmwfGRtIZdJcG6xJSVsfJfQDOA22jaQnB+OVLSkQYasdRgu2kvtWvkJRUC66RtLudwjhAfotEGbaeo84IBUGikRuEf1Awmk5pp2P4enm1hMs7bsikkdb2s67q7mSSELEBY64FO3qQhZCIfKyZ05huu7xsmp7fj2V7sDOIlPtWn7ZA2jqtu625tCB20DqOk6Ae0dxHFOwH3hWfwClx2SylO-FtjRrpXtBwYyY+aYudm3nDpOMGnIudJrg5uIGdZdxoWFl7HQp8WFrVKWQhl6n7gVuJoXpmncjheKg3zYBrGALRIgTRBbNl6rzncplof0WGOUiloMFrCAAFt5FCpl+p2vzIxgGAwCweIfR9GRoktuRFES2w7AAOn4KAG2UedpHbNP4T9n18DADAfTztsfVLJNgB9LBECwSIyEzn0ExgxdFG4Z9gATLQE2AGCYOfAAGYfuBggBOAA2Lup8XGCtGnyeJ-wIO+QFPy247rue77geh9H8fp9n+fF6n5fV7FYrTTzAs5Btu3kvB4mVWBTKXb5N3CA9wUvf632A5QDXp5EO2kw4RyjjHOO0gE5W2TqWVOcwM71kbMAXO+dC7F1LuXdsVd8A1zrg3JuLct6d27r3fug8R5j0njPZ8c8F5LwnivGKwgrIogtlbB+iByrPx2o5B0-AaAfxAd1cBkdo6x3jonYA8D8CINsMgrOqD0FtgLv7LBZcK54IIfXRuzd5Ct3bmQ3elCD40OPvQ0+TCWFhQ3pFUhO8KH72oUfOhDCz4XxilJb68BfqbVrCg5QrY2xpgzLYYAOZOEwFMDNFWAIxanAFnEPw51SpWlSg5FUyt0aOhcjlBqaQQh6MiBgLQFUbSczibk-gPginBBpH7fuUAKlpSqc9GphSFgcLvtbW2iBEiOyeNUnmjo6ndNROaVpWT6g5NGfwV0WohrulpAaG6VlFDpkzFEu+vC6zKOCe2MJ2zb6J3tLVERjJErlkitFHCVyiFlL8juLQi4XkCR3AADhggmZ8AlgDDxgsPLQz5jEJgTG8qeO4EyKATJ8xcwDGSNOaX5L86FAVASAoueFiKekyO4UKNJjJQHiMgVImBMi5EKKUdnNBFd1FFxLlo3B1dkyEP0SQ4xTi95UMPrQk+jDz7MNXnYiKwpHHkJ5eYtxArPHCtFMEPI9yzQIj8t402JVNnhMiac6wT99m0pCcciJOyzlvw4Jc65v9hQBSCtbRFJSnmRReW8rQHzvm-P+YC4FoKO7gshdC2FOL7nIsDpFNFGKsXBsaIyPFXD+mEtdt5exwpvYAMDmFElkVw4SKgdIuBKd06Z1paohlmicGV1ZbXEpBioBGO3pKsxrj+VWMFV4xoSqY0qukEKfUYoNVIi1Sc6JtkDWoKNVsk1uqYmS3pMIDyVyCmJs-smsV+U7UhVFWoPy0UO33MdSFRIRKv5sjXS695Xyfl-IBUCkFYKIU7ihTCuFCKt2e2FOet1l7PU3p9fegNz6cV7pjaGoBR6k2sh-qiz86KYKYuxa+pkKb-IwcjQhhVcBO3BFjdOvpdtl2eRPVBv+Psiz+wzcHMR2aIGSOgbApOhakHFpUfSzBTKK06LZTWzlDbTEuL5ZYjxNiRVIbXRKvjvKLHuOsUKlhwHsPdoI9hojKb9RvutSKDtYogA)."},{"id":"ts-loader-goes-webpack-5","metadata":{"permalink":"/ts-loader-goes-webpack-5","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-04-20-ts-loader-goes-webpack-5/index.md","source":"@site/blog/2021-04-20-ts-loader-goes-webpack-5/index.md","title":"ts-loader goes webpack 5","description":"TypeScript webpack loader `ts-loader` has released version 9.0.0, with support for webpack 5 and a minimum supported Node version of 12.","date":"2021-04-20T00:00:00.000Z","tags":[{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."},{"inline":false,"label":"ts-loader","permalink":"/tags/ts-loader","description":"The TypeScript loader for webpack."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":6.285,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"ts-loader-goes-webpack-5","title":"ts-loader goes webpack 5","authors":"johnnyreilly","tags":["webpack","ts-loader","typescript"],"image":"./ts-loader-9.png","hide_table_of_contents":false,"description":"TypeScript webpack loader `ts-loader` has released version 9.0.0, with support for webpack 5 and a minimum supported Node version of 12."},"unlisted":false,"prevItem":{"title":"The Service Now API and TypeScript Conditional Types","permalink":"/service-now-api-and-typescript-conditional-types"},"nextItem":{"title":"Hello World Bicep","permalink":"/hello-world-bicep"}},"content":"`ts-loader` has just released [v9.0.0](https://github.com/TypeStrong/ts-loader/releases/tag/v9.0.0). This post goes through what this release is all about, and what it took to ship this version. For intrigue, it includes a brief scamper into my mental health along the way. Some upgrades go smoothly - this one had some hiccups. But we\'ll get into that.\\n\\n![hello world bicep](ts-loader-9.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## One big pull request\\n\\nAs of v8, `ts-loader` supported webpack 4 and webpack 5. However the webpack 5 support was best efforts, and not protected by any automated tests. `ts-loader` has two test packs:\\n\\n1. A [comparison test pack](https://github.com/TypeStrong/ts-loader/tree/main/test/comparison-tests#readme) that compares transpilation and webpack compilation output with known outputs.\\n2. An [execution test pack](https://github.com/TypeStrong/ts-loader/tree/main/test/execution-tests#readme) that executes Karma test packs written in TypeScript using `ts-loader`.\\n\\nThe test packs were tightly coupled to webpack 4 (and in the case of the comparison test pack, that\'s unavoidable). The mission was to port `ts-loader` to be built against (and have an automated test pack that ran against) webpack 5.\\n\\nThis ended up being a [very big pull request](https://github.com/TypeStrong/ts-loader/pull/1251). Work on it started back in February 2021 and we\'re shipping now in April of 2021. I\'d initially expected it would take a couple of days at most. I had underestimated.\\n\\nA number of people collaborated on this PR, either with code, feedback, testing or even just responding to questions. So I\'d like to say thank you to:\\n\\n- [John Wallsten](https://github.com/JonWallsten) - who did a lot of the work swapping `ts-loader` over to webpack 5 APIs\\n- [Nick Excell](https://github.com/appzuka)\\n- [Andrew Branch](https://github.com/andrewbranch)\\n- [Alexander Akait](https://github.com/alexander-akait) - who provided webpack 5 expertise and ideas\\n- [Tobias Koppers](https://github.com/sokra) - who got me out of a hole - more on that later\\n\\n## What\'s changed\\n\\nLet\'s go through what\'s different in v9. There\'s two breaking changes:\\n\\n- The minimum webpack version supported is now webpack 5. This simplifies the codebase, which previously had to if/else the various API registrations based on the version of webpack being used.\\n- The minimum node version supported is now node 12. [Node 10 reaches end of life status at the end of April 2021.](https://nodejs.org/en/about/releases/)\\n\\nAn interesting aspect of migrating to building against webpack 5 was dropping the dependency upon [`@types/webpack`](https://www.npmjs.com/package/@types/webpack) in favour of the types that now ship with webpack 5 itself. This was a mostly great experience; however we discovered some missing pieces.\\n\\nMost notably, the `LoaderContext` [wasn\'t strongly typed](https://github.com/webpack/webpack/blob/03961f33912ab6735d470b870eacff678735a9ed/lib/NormalModule.js#L424). `LoaderContext` is the value of `this` in the context of a running loader function. So it is probably the most interesting and important type from the perspective of a loader author.\\n\\nHistorically we used our own definition which had been adapted from the one in `@types/webpack`. [I\'ve looked into the possibility of a type being exposed in webpack itself.](https://github.com/webpack/webpack/issues/13162) However, it turns out, [it\'s complicated - with the `LoaderContext` type being effectively created across two packages](https://github.com/webpack/webpack/pull/13164#issuecomment-821410359). The type is initially created in `webpack` and then augmented later in `loader-runner`, prior to being supplied to loaders. You can read more on that [here](https://github.com/webpack/webpack/pull/13164#issuecomment-821410359).\\n\\nFor now we\'ve opted to stick with keeping [an interface in `ts-loader`](https://github.com/TypeStrong/ts-loader/pull/1251/commits/acbc71feed91fe14ec065dd9d31081af7a492f47) that models what arrives in the loader when executed. We have freshened it up somewhat, to model the webpack 5 world.\\n\\nAlongside these changes, a [number of dependencies were upgraded](https://github.com/TypeStrong/ts-loader/pull/1251/files#diff-7ae45ad102eab3b6d7e7896acd08c427a9b25b346470d7bc6507b6481575d519).\\n\\n## The hole\\n\\nBy the 19th of February most of the work was done. However, [we were experiencing different behaviour between Linux and Windows in our comparison test pack](https://github.com/TypeStrong/ts-loader/pull/1251#issuecomment-781967959).\\n\\nAs far as I was aware, we were doing all the appropriate work to ensure `ts-loader` and our test packs worked cross platform. But we were still experiencing problems whenever we ran the test pack on Windows. I\'d done no end of tweaking but nothing worked. I couldn\'t explain it. I couldn\'t fix it. I was finding that tough to deal with.\\n\\nI really want to be transparent about the warts and all aspect of open source software development. It is like all other types of software development; sometimes things go wrong and it can be tough to work out why. Right then, I was really quite unhappy. Things weren\'t working code-wise and I was at a loss to say why. This is not something that I dig.\\n\\nI also wasn\'t sleeping amazingly at this point. It was winter and we\'d been in lockdown in the UK for three months; as the COVID-19 pandemic ground relentlessly on. I love my family dearly. I really do. With that said, having my children around whilst I attempted to work was remarkably tough. I love those guys but, woah, was it stressful.\\n\\nI was feeling at a low ebb. And I wasn\'t sure what to do next. So, feeling tired and pretty fed up, I took a break.\\n\\n## \\"Anybody down there?\\"\\n\\nTime passed. In March [Alexander Akait](https://github.com/alexander-akait) checked in to see how things were going and volunteered to help. He also [suggested what turned out to be the fix](https://github.com/TypeStrong/ts-loader/pull/1251#issuecomment-799531375); namely replacing usage of `\'\\\\\'` with `\'/\'` in the assets supplied back to webpack. But crucially I implemented this wrong. Observe [this commit](https://github.com/TypeStrong/ts-loader/pull/1251/commits/4bcc5c9623acfd7ffbaf028781a8353b37243804):\\n\\n```ts\\nconst assetPath = path\\n  .relative(compilation.compiler.outputPath, outputFile.name)\\n  // According to @alexander-akait we should always \'/\' https://github.com/TypeStrong/ts-loader/pull/1251#issuecomment-799606985\\n  .replace(/\\\\//g, \'/\');\\n```\\n\\nIf you look closely at the `replace` you\'ll see that I\'m globally replacing `\'/\'` with `\'/\'` _rather_ than globally replacing `\'\\\\\'` with `\'/\'`. The wasted time this caused... I could weep.\\n\\nI generally thrashed around for a bit after this. Going in circles, like a six year old swimming wearing one armband. Then [Tobias kindly volunteered to help](https://github.com/TypeStrong/ts-loader/pull/1251#issuecomment-805143890). This much I\'ve learned from a career in software: if talented people offer their assistance, grab it with both hands!\\n\\nI\'d been trying be as \\"learn in public\\" as possible about the issues I was facing on the pull request. The idea being, to surface the problems in a public forum where others can read and advise. And also to attempt a textual kind of [rubber duck debugging](https://en.wikipedia.org/wiki/Rubber_duck_debugging).\\n\\nWhen Tobias pitched in, I wanted to make it as easy as possible for him to help. So I wrote up [a full description of what had changed](https://github.com/TypeStrong/ts-loader/pull/1251#issuecomment-805181069). What the divergent behaviour in test packs looked like. I shared my speculation for what might be causing the issue (I was wrong by the way). Finally I provided a simple way to get up and running with the broken code. The easier I could make it for others to collaborate on this, I figured, the greater the likelihood of an answer. Tobias got to an answer quickly:\\n\\n> The problem is introduced due to some normalization logic in the test case: see [#1273](https://github.com/TypeStrong/ts-loader/pull/1273)\\n>\\n> While the PR fixes the problem, I think the paths should be normalized earlier in the pipeline to make this normalization code unnecessary. Note that asset names should have only `/` as they are filenames and not paths. Only absolute paths have `\\\\`.\\n\\nTobias had raised a PR which introduced a workaround to resolved things in the test pack. This made me happy. More than that, he also identified that the issue lay in `ts-loader` itself. This caused me to look again at the changes I\'d made, including my `replace` addition. [With fresh eyes, I now realised this was a bug](https://github.com/TypeStrong/ts-loader/pull/1251#issuecomment-805907212), and [fixed](https://github.com/TypeStrong/ts-loader/pull/1251/commits/427714e43519289bb5745ca078133d1ace8fc2c1) it.\\n\\nI found then that I could revert Tobias\' workaround and still have passing tests. Result!\\n\\n## Release details\\n\\nNow that we\'ve got there; we\'ve shipped. You can get the latest version of `ts-loader` on [npm](https://www.npmjs.com/package/ts-loader/v/9.0.0) and you can find the release details on [GitHub](https://github.com/TypeStrong/ts-loader/releases/tag/v9.0.0).\\n\\nThanks everyone - I couldn\'t have done it without your help. \uD83C\uDF3B\u2764\uFE0F"},{"id":"hello-world-bicep","metadata":{"permalink":"/hello-world-bicep","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-04-10-hello-world-bicep/index.md","source":"@site/blog/2021-04-10-hello-world-bicep/index.md","title":"Hello World Bicep","description":"Bicep simplifies Azure Resource Management through concise syntax. The \\"Hello World\\" example highlights how Bicep outperforms ARM templates.","date":"2021-04-10T00:00:00.000Z","tags":[{"inline":false,"label":"Bicep","permalink":"/tags/bicep","description":"The Bicep language for Azure Resource Manager templates."},{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."}],"readingTime":2.68,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"hello-world-bicep","title":"Hello World Bicep","authors":"johnnyreilly","tags":["bicep","azure"],"image":"./hello-world-bicep.webp","hide_table_of_contents":false,"description":"Bicep simplifies Azure Resource Management through concise syntax. The \\"Hello World\\" example highlights how Bicep outperforms ARM templates."},"unlisted":false,"prevItem":{"title":"ts-loader goes webpack 5","permalink":"/ts-loader-goes-webpack-5"},"nextItem":{"title":"Bicep meet Azure Pipelines 2","permalink":"/bicep-meet-azure-pipelines-2"}},"content":"Bicep makes Azure Resource Management a great deal simpler than ARM templates. The selling point here is grokkability. This post takes a look at the [\\"Hello World\\" example recently added to the Bicep repo](https://github.com/Azure/bicep/pull/2011) to appreciate quite what a difference it makes.\\n\\n![hello world bicep](hello-world-bicep.webp)\\n\\n\x3c!--truncate--\x3e\\n\\n## More than configuration\\n\\nThe [\\"Hello World\\"](https://github.com/Azure/bicep/tree/187d4d2047dc83c69695ba79761f552bcb00c319/docs/examples/000/01-hello-world) added to the Bicep repo by [Chris Lewis](https://github.com/ChristopherGLewis) illustrates the simplest usage of Bicep:\\n\\n> This bicep file takes a `yourName` parameter and adds that to a `hello` variable and returns the concatenated string as an ARM output.\\n\\nThis is, when you consider it, the very essence of a computer program. Taking an input, doing some computation and providing an output. When I think about ARM templates, (and because Bicep is transpiled into ARM templates I mentally bracket the two together) I tend to think about resources being deployed. I focus on _configuration_, not _computation_\\n\\nThis is an imperfect mental model. ARM templates can do so much more than deploy by slinging strings and numbers. Thanks to the wealth of [template functions](https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/template-functions) that exist they have much more power. They can do computation.\\n\\nThe Hello World example focuses just on computation.\\n\\n## From terse to verbose\\n\\nThe Hello World example is made up of two significant files:\\n\\n1. `main.bicep` - the bicep code\\n2. `main.json` - the ARM template compiled from the Bicep file\\n\\nThe [`main.bicep`](https://github.com/Azure/bicep/blob/187d4d2047dc83c69695ba79761f552bcb00c319/docs/examples/000/01-hello-world/main.bicep) file amounts to 3 lines of code (I have omitted the comment line):\\n\\n```bicep\\nparam yourName string\\nvar hello = \'Hello World! - Hi\'\\n\\noutput helloWorld string = \'${hello} ${yourName}\'\\n```\\n\\n- the first line takes the _input_ of `yourName`\\n- the second line declares a `hello` variable\\n- the third line _computes_ the new value of `helloWorld` based upon `hello` and `yourName`, then passes it as _output_\\n\\nGosh is it ever simple. It\'s easy to read and it\'s simple to understand. Even if you don\'t know Bicep, if you\'ve experience in another language you can likely guess what\'s happening.\\n\\nLet\'s compare this with the [`main.json`](https://github.com/Azure/bicep/blob/187d4d2047dc83c69695ba79761f552bcb00c319/docs/examples/000/01-hello-world/main.json) that `main.bicep` is transpiled into:\\n\\n```json\\n{\\n  \\"$schema\\": \\"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\\",\\n  \\"contentVersion\\": \\"1.0.0.0\\",\\n  \\"metadata\\": {\\n    \\"_generator\\": {\\n      \\"name\\": \\"bicep\\",\\n      \\"version\\": \\"dev\\",\\n      \\"templateHash\\": \\"6989941473549654446\\"\\n    }\\n  },\\n  \\"parameters\\": {\\n    \\"yourName\\": {\\n      \\"type\\": \\"string\\"\\n    }\\n  },\\n  \\"functions\\": [],\\n  \\"variables\\": {\\n    \\"hello\\": \\"Hello World! - Hi\\"\\n  },\\n  \\"resources\\": [],\\n  \\"outputs\\": {\\n    \\"helloWorld\\": {\\n      \\"type\\": \\"string\\",\\n      \\"value\\": \\"[format(\'{0} {1}\', variables(\'hello\'), parameters(\'yourName\'))]\\"\\n    }\\n  }\\n}\\n```\\n\\nThe above ARM template expresses exactly the same thing as the Bicep alternative. But that 3 lines of logic has become 27 lines of JSON. We\'ve lost something in the transition. Intent is no longer clear. We\'ve gone from something easy to reason about, to something that is hard to reason about. You need to think a lot less to write the Bicep alternative and that\'s a _good_ thing.\\n\\nI was chatting to someone recently who expressed it well by saying:\\n\\n> ARM is the format that the resource providers understand, so really it\u2019s the Azure equivalent of Assembler \u2013 and I don\u2019t know anyone who enjoys coding in Assembler.\\n\\nThis is a great example of the value that Bicep provides. If you\'d like to play with the Hello World a little, why not [take it for a spin in the Bicep playground](https://aka.ms/bicepdemo#eJzT1w9OzC3ISVXISM3JyVcozy/KSeEqSCxKzFWozC8t8kvMTVUoLinKzEvnKkssgqqyVVD3ADPCQcoVFXQVPDLVubjyS0sKSksgasAyUJ0g9SrVYOFaBZVqmLm16gCvlitr)."},{"id":"bicep-meet-azure-pipelines-2","metadata":{"permalink":"/bicep-meet-azure-pipelines-2","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-03-23-bicep-meet-azure-pipelines-2/index.md","source":"@site/blog/2021-03-23-bicep-meet-azure-pipelines-2/index.md","title":"Bicep meet Azure Pipelines 2","description":"With Azure CLI, Bicep can be run in Azure Pipeline with minimal effort. Compile Bicep to ARM in a simple one-liner bash step.","date":"2021-03-23T00:00:00.000Z","tags":[{"inline":false,"label":"Bicep","permalink":"/tags/bicep","description":"The Bicep language for Azure Resource Manager templates."},{"inline":false,"label":"Azure Pipelines","permalink":"/tags/azure-pipelines","description":"The Azure Pipelines CI / CD service."},{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."}],"readingTime":1.67,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"bicep-meet-azure-pipelines-2","title":"Bicep meet Azure Pipelines 2","authors":"johnnyreilly","tags":["bicep","azure pipelines","azure"],"image":"./bicep-meet-azure-pipelines.webp","hide_table_of_contents":false,"description":"With Azure CLI, Bicep can be run in Azure Pipeline with minimal effort. Compile Bicep to ARM in a simple one-liner bash step."},"unlisted":false,"prevItem":{"title":"Hello World Bicep","permalink":"/hello-world-bicep"},"nextItem":{"title":"Bicep meet Azure Pipelines","permalink":"/bicep-meet-azure-pipelines"}},"content":"[Last time](../2021-03-20-bicep-meet-azure-pipelines/index.md) I wrote about how to use the Azure CLI to run Bicep within the context of an Azure Pipeline. The solution was relatively straightforward, and involved using `az deployment group create` in a task. There\'s an easier way.\\n\\n![Bicep meet Azure Pipelines](bicep-meet-azure-pipelines.webp)\\n\\n\x3c!--truncate--\x3e\\n\\n## The easier way\\n\\nThe target reader of the previous post was someone who was already using `AzureResourceManagerTemplateDeployment@3` in an Azure Pipeline to deploy an ARM template. Rather than replacing your existing `AzureResourceManagerTemplateDeployment@3` tasks, all you need do is insert a prior `bash` step that compiles the Bicep to ARM, which your existing template can then process. It looks like this:\\n\\n```yml\\n- bash: az bicep build --file infra/app-service/azuredeploy.bicep\\n  displayName: \'Compile Bicep to ARM\'\\n```\\n\\nThis will take your Bicep template of `azuredeploy.bicep`, transpile it into an ARM template named `azuredeploy.json` which a subsequent `AzureResourceManagerTemplateDeployment@3` task can process. Since this is just exercising the Azure CLI, using `bash` is not required; powershell etc would also be fine; it\'s just required that the Azure CLI is available in a pipeline.\\n\\nIn fact this simple task could even be a one-liner if you didn\'t fancy using the `displayName`. (Though I say keep it; optimising for readability is generally a good shout.) A full pipeline could look like this:\\n\\n```yml\\n- bash: az bicep build --file infra/app-service/azuredeploy.bicep\\n  displayName: \'Compile Bicep to ARM\'\\n\\n- task: AzureResourceManagerTemplateDeployment@3\\n  displayName: \'Deploy Hello Azure ARM\'\\n  inputs:\\n    azureResourceManagerConnection: \'$(azureSubscription)\'\\n    action: Create Or Update Resource Group\\n    resourceGroupName: \'$(resourceGroupName)\'\\n    location: \'North Europe\'\\n    templateLocation: Linked artifact\\n    csmFile: \'infra/app-service/azuredeploy.json\' # created by bash script\\n    csmParametersFile: \'infra/app-service/azuredeploy.parameters.json\'\\n    deploymentMode: Incremental\\n    deploymentOutputs: resourceGroupDeploymentOutputs\\n    overrideParameters: -applicationName $(Build.Repository.Name)\\n\\n- pwsh: |\\n    $outputs = ConvertFrom-Json \'$(resourceGroupDeploymentOutputs)\'\\n    foreach ($output in $outputs.PSObject.Properties) {\\n        Write-Host \\"##vso[task.setvariable variable=RGDO_$($output.Name)]$($output.Value.value)\\"\\n    }\\n  displayName: \'Turn ARM outputs into variables\'\\n```\\n\\nAnd when it\'s run, it may result in something along these lines:\\n\\n![Bicep in an Azure Pipeline](azure-pipeline-with-bicep.png)\\n\\nSo if you want to get using Bicep right now with minimal effort, this an on ramp that could work for you! Props to [Jamie McCrindle](https://twitter.com/foldr) for suggesting this."},{"id":"bicep-meet-azure-pipelines","metadata":{"permalink":"/bicep-meet-azure-pipelines","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-03-20-bicep-meet-azure-pipelines/index.md","source":"@site/blog/2021-03-20-bicep-meet-azure-pipelines/index.md","title":"Bicep meet Azure Pipelines","description":"Bicep is a more readable alternative to ARM templates. Though no Bicep task is available yet, Azure CLI can still deploy Bicep.","date":"2021-03-20T00:00:00.000Z","tags":[{"inline":false,"label":"Bicep","permalink":"/tags/bicep","description":"The Bicep language for Azure Resource Manager templates."},{"inline":false,"label":"Azure Pipelines","permalink":"/tags/azure-pipelines","description":"The Azure Pipelines CI / CD service."},{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."}],"readingTime":4.905,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"bicep-meet-azure-pipelines","title":"Bicep meet Azure Pipelines","authors":"johnnyreilly","tags":["bicep","azure pipelines","azure"],"image":"./bicep-meet-azure-pipelines.webp","hide_table_of_contents":false,"description":"Bicep is a more readable alternative to ARM templates. Though no Bicep task is available yet, Azure CLI can still deploy Bicep."},"unlisted":false,"prevItem":{"title":"Bicep meet Azure Pipelines 2","permalink":"/bicep-meet-azure-pipelines-2"},"nextItem":{"title":"RSS update; we moved to Docusaurus","permalink":"/rss-update-we-moved-to-docusaurus"}},"content":"[Bicep](https://github.com/Azure/bicep) is a terser and more readable alternative language to ARM templates. Running ARM templates in Azure Pipelines is straightforward. However, there isn\'t yet a first class experience for running Bicep in Azure Pipelines. This post demonstrates an approach that can be used until a Bicep task is available.\\n\\n![Bicep meet Azure Pipelines](bicep-meet-azure-pipelines.webp)\\n\\n\x3c!--truncate--\x3e\\n\\n## Bicep: mostly ARMless\\n\\nIf you\'ve been working with Azure and infrastructure as code, you\'ll likely have encountered [ARM templates](https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/overview). They\'re a domain specific language that lives inside JSON, used to define the infrastructure that is deployed to Azure; App Services, Key Vaults and the like.\\n\\nARM templates are quite verbose and not the easiest thing to read. This is a consequence of being effectively a language nestled inside another language. Bicep is an alternative language which is far more readable. Bicep transpiles down to ARM templates, in the same way that TypeScript transpiles down to JavaScript.\\n\\nBicep is quite new, but already it enjoys feature parity with ARM templates (as of [v0.3](https://github.com/Azure/bicep/releases/tag/v0.3.1)) and ships as part of the [Azure CLI](https://github.com/MicrosoftDocs/azure-docs-cli/blob/master/docs-ref-conceptual/release-notes-azure-cli/index.md#arm-1). However, as Bicep is new, it doesn\'t yet have a dedicated Azure Pipelines task for deployment. This should exist in future, perhaps as soon as the [v0.4 release](https://github.com/Azure/bicep/issues/1341). In the meantime there\'s an alternative way to achieve this which we\'ll go through.\\n\\n## App Service with Bicep\\n\\nLet\'s take a simple Bicep file, `azuredeploy.bicep`, which is designed to deploy an App Service resource to Azure. It looks like this:\\n\\n```bicep\\n@description(\'Tags that our resources need\')\\nparam tags object = {\\n  costCenter: \'todo: replace\'\\n  environment: \'todo: replace\'\\n  application: \'todo: replace with app name\'\\n  description: \'todo: replace\'\\n  managedBy: \'ARM\'\\n}\\n\\n@minLength(2)\\n@description(\'Base name of the resource such as web app name and app service plan\')\\nparam applicationName string\\n\\n@description(\'Location for all resources.\')\\nparam location string = resourceGroup().location\\n\\n@description(\'The SKU of App Service Plan\')\\nparam sku string\\n\\nvar appServicePlanName_var = \'plan-${applicationName}-${tags.environment}\'\\nvar linuxFxVersion = \'DOTNETCORE|5.0\'\\nvar fullApplicationName_var = \'app-${applicationName}-${uniqueString(applicationName)}\'\\n\\nresource appServicePlanName \'Microsoft.Web/serverfarms@2019-08-01\' = {\\n  name: appServicePlanName_var\\n  location: location\\n  sku: {\\n    name: sku\\n  }\\n  kind: \'linux\'\\n  tags: {\\n    CostCenter: tags.costCenter\\n    Environment: tags.environment\\n    Description: tags.description\\n    ManagedBy: tags.managedBy\\n  }\\n  properties: {\\n    reserved: true\\n  }\\n}\\n\\nresource fullApplicationName \'Microsoft.Web/sites@2018-11-01\' = {\\n  name: fullApplicationName_var\\n  location: location\\n  kind: \'app\'\\n  tags: {\\n    CostCenter: tags.costCenter\\n    Environment: tags.environment\\n    Description: tags.description\\n    ManagedBy: tags.managedBy\\n  }\\n  properties: {\\n    serverFarmId: appServicePlanName.id\\n    clientAffinityEnabled: true\\n    siteConfig: {\\n      appSettings: []\\n      linuxFxVersion: linuxFxVersion\\n      alwaysOn: false\\n      ftpsState: \'Disabled\'\\n      http20Enabled: true\\n      minTlsVersion: \'1.2\'\\n      remoteDebuggingEnabled: false\\n    }\\n    httpsOnly: true\\n  }\\n  identity: {\\n    type: \'SystemAssigned\'\\n  }\\n}\\n\\noutput fullApplicationName string = fullApplicationName_var\\n```\\n\\nWhen transpiled down to an ARM template, this Bicep file more than doubles in size:\\n\\n- `azuredeploy.bicep` - 1782 bytes\\n- `azuredeploy.json` - 3863 bytes\\n\\nThis tells you something of the advantage of Bicep. The template comes with an associated `azuredeploy.parameters.json` file:\\n\\n```json\\n{\\n  \\"$schema\\": \\"https://schema.management.azure.com/schemas/2019-04-01/deploymentParameters.json#\\",\\n  \\"contentVersion\\": \\"1.0.0.0\\",\\n  \\"parameters\\": {\\n    \\"tags\\": {\\n      \\"value\\": {\\n        \\"costCenter\\": \\"8888\\",\\n        \\"environment\\": \\"stg\\",\\n        \\"application\\": \\"hello-azure\\",\\n        \\"description\\": \\"App Service for hello-azure\\",\\n        \\"managedBy\\": \\"ARM\\"\\n      }\\n    },\\n    \\"sku\\": {\\n      \\"value\\": \\"B1\\"\\n    }\\n  }\\n}\\n```\\n\\nIt\'s worth remembering that you can use the same parameters files with Bicep that you can use with ARM templates. This is great for minimising friction when it comes to migrating.\\n\\n## Bicep in `azure-pipelines.yml`\\n\\nNow we have our Bicep file, we want to execute it from the context of an Azure Pipeline. If we were working directly with the ARM template we\'d likely have something like this in place:\\n\\n```yml\\n- task: AzureResourceManagerTemplateDeployment@3\\n  displayName: \'Deploy Hello Azure ARM\'\\n  inputs:\\n    azureResourceManagerConnection: \'$(azureSubscription)\'\\n    action: Create Or Update Resource Group\\n    resourceGroupName: \'$(resourceGroupName)\'\\n    location: \'North Europe\'\\n    templateLocation: Linked artifact\\n    csmFile: \'infra/app-service/azuredeploy.json\'\\n    csmParametersFile: \'infra/app-service/azuredeploy.parameters.json\'\\n    deploymentMode: Incremental\\n    deploymentOutputs: resourceGroupDeploymentOutputs\\n    overrideParameters: -applicationName $(Build.Repository.Name)\\n\\n- pwsh: |\\n    $outputs = ConvertFrom-Json \'$(resourceGroupDeploymentOutputs)\'\\n    foreach ($output in $outputs.PSObject.Properties) {\\n        Write-Host \\"##vso[task.setvariable variable=RGDO_$($output.Name)]$($output.Value.value)\\"\\n    }\\n  displayName: \'Turn ARM outputs into variables\'\\n```\\n\\nThere\'s two tasks above. The first is the native task for ARM deployments which takes our ARM template and our parameters and deploys them. The second task takes the output variables from the first task and converts them into Azure Pipeline variables such that they can be referenced later in the pipeline. In this case this variablifies our `fullApplicationName` output.\\n\\nThere is, as yet, no `BicepTemplateDeployment@1`. [Though it\'s coming](https://github.com/Azure/bicep/issues/1341). In the meantime, the marvellous [Alex Frankel](https://twitter.com/adotfrank) [advised](https://github.com/Azure/bicep/issues/1341#issuecomment-802010110):\\n\\n> I\'d recommend using the [Azure CLI task](https://docs.microsoft.com/azure/devops/pipelines/tasks/deploy/azure-cli?view=azure-devops) to deploy. As long as that task is updated to Az CLI version 2.20 or later, it will automatically install the bicep CLI when calling `az deployment group create -f main.bicep`.\\n\\nLet\'s give it a go!\\n\\n```yml\\n- task: AzureCLI@2\\n  displayName: \'Deploy Hello Azure Bicep\'\\n  inputs:\\n    azureSubscription: \'$(azureSubscription)\'\\n    scriptType: bash\\n    scriptLocation: inlineScript\\n    inlineScript: |\\n      az --version\\n\\n      echo \\"az deployment group create --resource-group \'$(resourceGroupName)\' --name appservicedeploy\\"\\n      az deployment group create --resource-group \'$(resourceGroupName)\' --name appservicedeploy \\\\\\n        --template-file infra/app-service/azuredeploy.bicep \\\\\\n        --parameters infra/app-service/azuredeploy.parameters.json \\\\\\n        --parameters applicationName=\'$(Build.Repository.Name)\'\\n\\n      echo \\"az deployment group show --resource-group \'$(resourceGroupName)\' --name appservicedeploy\\"\\n      deploymentoutputs=$(az deployment group show --resource-group \'$(resourceGroupName)\' --name appservicedeploy \\\\\\n        --query properties.outputs)\\n\\n      echo \'convert outputs to variables\'\\n      echo $deploymentoutputs | jq -c \'. | to_entries[] | [.key, .value.value]\' |\\n        while IFS=$\\"\\\\n\\" read -r c; do\\n          outputname=$(echo \\"$c\\" | jq -r \'.[0]\')\\n          outputvalue=$(echo \\"$c\\" | jq -r \'.[1]\')\\n          echo \\"setting variable RGDO_$outputname=$outputvalue\\"\\n          echo \\"##vso[task.setvariable variable=RGDO_$outputname]$outputvalue\\"\\n        done\\n```\\n\\nThe above is just a single Azure CLI task (as advised). It invokes `az deployment group create` passing the relevant parameters. It then acquires the output properties using `az deployment group show`. Finally it once again converts these outputs to Azure Pipeline variables with some [`jq`](https://stedolan.github.io/jq/) smarts.\\n\\nThis works right now, and running it results in something like the output below. So if you\'re excited about Bicep and don\'t want to wait for 0.4 to start moving on this, then this can get you going. To track the progress of the custom task, [keep an eye on this issue](https://github.com/Azure/bicep/issues/1341).\\n\\n![Bicep in an Azure Pipeline](bicep-in-a-pipeline.png)\\n\\n## Update: an even simpler alternative\\n\\nThere is even a simpler way to do this which I discovered subsequent to writing this. [Have a read](../2021-03-23-bicep-meet-azure-pipelines-2/index.md)."},{"id":"rss-update-we-moved-to-docusaurus","metadata":{"permalink":"/rss-update-we-moved-to-docusaurus","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-03-17-rss-update-we-moved-to-docusaurus/index.md","source":"@site/blog/2021-03-17-rss-update-we-moved-to-docusaurus/index.md","title":"RSS update; we moved to Docusaurus","description":"A blogger migrated to Docusaurus and GitHub Pages, shares feed updates, with new Atom and RSS feeds and all historic links still working.","date":"2021-03-17T00:00:00.000Z","tags":[{"inline":false,"label":"Docusaurus","permalink":"/tags/docusaurus","description":"The Docusaurus static site generator."}],"readingTime":0.56,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"rss-update-we-moved-to-docusaurus","title":"RSS update; we moved to Docusaurus","authors":"johnnyreilly","tags":["docusaurus"],"image":"./rss.png","hide_table_of_contents":false,"description":"A blogger migrated to Docusaurus and GitHub Pages, shares feed updates, with new Atom and RSS feeds and all historic links still working."},"unlisted":false,"prevItem":{"title":"Bicep meet Azure Pipelines","permalink":"/bicep-meet-azure-pipelines"},"nextItem":{"title":"The definitive guide to migrating from Blogger to Docusaurus","permalink":"/definitive-guide-to-migrating-from-blogger-to-docusaurus"}},"content":"My blog lived happily on [Blogger](https://icanmakethiswork.blogspot.com/) for the past decade. It\'s now built with [Docusaurus](https://v2.docusaurus.io/) and hosted on [GitHub Pages](https://pages.github.com/). To understand the why, [read my last post](../2021-03-15-definitive-guide-to-migrating-from-blogger-to-docusaurus/index.md). This post serves purely to share details of feed updates for RSS / Atom subscribers.\\n\\n\x3c!--truncate--\x3e\\n\\nThe Atom feed at this location no longer exists: https://johnnyreilly.com/feeds/posts/default\\n\\nThe following feeds are new and different:\\n\\n- RSS - https://johnnyreilly.com/rss.xml\\n- Atom - https://johnnyreilly.com/atom.xml\\n\\nThe new format might mess with any feed reader you have set up. I do apologise for the friction; hopefully it shouldn\'t cause you too much drama.\\n\\nFinally, all historic links should continue to work with the new site; redirects have been implemented."},{"id":"definitive-guide-to-migrating-from-blogger-to-docusaurus","metadata":{"permalink":"/definitive-guide-to-migrating-from-blogger-to-docusaurus","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-03-15-definitive-guide-to-migrating-from-blogger-to-docusaurus/index.md","source":"@site/blog/2021-03-15-definitive-guide-to-migrating-from-blogger-to-docusaurus/index.md","title":"The definitive guide to migrating from Blogger to Docusaurus","description":"Learn how to transfer a Blogger website to Docusaurus without losing content. Use a TypeScript console app to convert HTML to Markdown.","date":"2021-03-15T00:00:00.000Z","tags":[{"inline":false,"label":"Docusaurus","permalink":"/tags/docusaurus","description":"The Docusaurus static site generator."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":13.55,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"definitive-guide-to-migrating-from-blogger-to-docusaurus","title":"The definitive guide to migrating from Blogger to Docusaurus","authors":"johnnyreilly","tags":["docusaurus","typescript"],"image":"./title-image.png","hide_table_of_contents":false,"description":"Learn how to transfer a Blogger website to Docusaurus without losing content. Use a TypeScript console app to convert HTML to Markdown."},"unlisted":false,"prevItem":{"title":"RSS update; we moved to Docusaurus","permalink":"/rss-update-we-moved-to-docusaurus"},"nextItem":{"title":"Managed Identity, Azure SQL and Entity Framework","permalink":"/managed-identity-azure-sql-entity-framework"}},"content":"This post documents how to migrate a blog from Blogger to Docusaurus.\\n\\n![title image reading \\"The definitive guide to migrating from Blogger to Docusaurus\\" with the Blogger and Docusaurus logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Updated 5th November 2022\\n\\nThis post started out as an investigation into migrating from Blogger to Docusaurus. In the end I very much made the leap, and would recommend doing so to others. I\'ve transformed this post into a \\"definitive guide\\" on how to migrate. I intend to maintain this on an ongoing basis for the benefit of the community.\\n\\nBecause I rather like what I originally wrote when I was in \\"investigation mode\\", I have largely left it in place. However, there are new sections which have been added in to augment what\'s there.\\n\\n## Introduction\\n\\n[Docusaurus](https://v2.docusaurus.io/) is, amongst other things, a Markdown powered blogging platform. My blog has lived happily on [Blogger](https://www.blogger.com/) for the past decade. I\'m considering moving, but losing my historic content as part of the move was never an option. This post goes through what it would look like to move from Blogger to Docusaurus _without_ losing your content.\\n\\nIt is imperative that the world never forgets what I was doing with jQuery in 2012.\\n\\n## Blog as code\\n\\nEverything is better when it\'s code. Infrastructure as code. Awesome right? So naturally \\"blog as code\\" must be better than just a blog. More seriously, [Markdown](https://en.wikipedia.org/wiki/Markdown) is a tremendous documentation format. Simple, straightforward and, like Goldilocks, \\"just right\\". For a long time I\'ve written everything as Markdown. My years of toil down the Open Source mines have preconditioned me to be very MD-disposed.\\n\\nI started out writing this blog a long time ago as pure HTML. Not the smoothest of writing formats. At some point I got into the habit of spinning up a new repo in GitHub for a new blogpost, writing it in Markdown and piping it through a variety of tools to convert it into HTML for publication on Blogger. As time passed I felt I\'d be a lot happier if I wasn\'t creating a repo each time. What if I did all my blogging in a single repo and used that as the code that represented my blog?\\n\\nJust having that thought laid the seeds for what was to follow:\\n\\n1. An investigation into importing my content from Blogger into a GitHub repo\\n2. An experimental port to Docusaurus\\n\\nWe\'re going to go this now. First, let\'s create ourselves a Docusaurus site for our blog:\\n\\n```\\nnpx create-docusaurus@latest blog-website classic\\n```\\n\\nThis creates a standard Docusaurus site in the `blog-website` directory. In there we\'ll find a `docusaurus.config.js` file. There\'s much that can be configured here. It\'s worth remembering that Docusaurus is a tool for building documentation sites that also happens to feature a blog component. We\'re going to use it as a blog only. So we\'ll deactivate the docs component and configure the blog component to be the home page of our site, following the [Docusaurus documentation](https://docusaurus.io/docs/blog#blog-only-mode):\\n\\n```js\\nmodule.exports = {\\n  // ...\\n  presets: [\\n    [\\n      \'@docusaurus/preset-classic\',\\n      /** @type {import(\'@docusaurus/preset-classic\').Options} */\\n      ({\\n        docs: false, // Deactivate docs\\n        blog: {\\n          blogTitle: \'I CAN MAKE THIS WORK\',\\n          blogDescription: \'The blog of johnnyreilly\',\\n          blogSidebarCount: 5,\\n          postsPerPage: 1,\\n          path: \'./blog\',\\n          routeBasePath: \'/\', // Make blog the home page\\n          showReadingTime: true,\\n          editUrl:\\n            \'https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/\',\\n        },\\n        theme: {\\n          customCss: require.resolve(\'./src/css/custom.css\'),\\n        },\\n      }),\\n    ],\\n  ],\\n  // ...\\n};\\n```\\n\\n## Downloading your Blogger content\\n\\nIn order that we can migrate, we must obtain the blog content. This is a mass of HTML that lived inside Blogger\'s database. (One assumes they have a database; I haven\'t actually checked.) There\'s a `Back up content` option inside Blogger\'s settings to allow this:\\n\\n![Download content from Blogger](screenshot-blogger-back-up-content.webp)\\n\\nIt provides you with an XML file with a dispiritingly small size. Ten years blogging? You\'ll get change out of 4Mb it turns out.\\n\\n## From HTML in XML to Markdown\\n\\nWe now want to take that XML and:\\n\\n- Extract each blog post (and it\'s associated metadata; title / tags and whatnot)\\n- Convert the HTML content of each blog post from HTML to Markdown, and save it as a Markdown file\\n- Download the images used in the blogpost so they can be stored in the repo as well\\n\\nTo do this we\'re going to whip up a smallish TypeScript console app. Let\'s initialise it with the packages we\'re going to need:\\n\\n```\\nmkdir from-blogger-to-docusaurus\\ncd from-blogger-to-docusaurus\\nnpx typescript --init\\nyarn init -y\\nyarn add @types/he@^1.1.2 @types/jsdom@^20.0.0 @types/node@^18.11.9 @types/showdown@^2.0.0 axios@^1.1.3 fast-xml-parser@^3.21.1 he@^1.2.0 jsdom@^20.0.2 showdown@^2.1.0 ts-node@^10.9.1 typescript@^4.8.4\\n```\\n\\nWe\'re using:\\n\\n- [`fast-xml-parser`](https://github.com/NaturalIntelligence/fast-xml-parser) to parse XML\\n- [`he`](https://github.com/mathiasbynens/he), [`jsdom`](https://github.com/jsdom/jsdom) and [`showdown`](https://github.com/showdownjs/showdown) to convert HTML to Markdown\\n- [`axios`](https://github.com/axios/axios) to download images\\n- [`typescript`](https://github.com/microsoft/TypeScript) to code in and [`ts-node`](https://github.com/TypeStrong/ts-node) to make our TypeScript Node.js console app.\\n\\nNow we have all the packages we need, it\'s time to write our script.\\n\\n```ts\\nimport fs from \'fs\';\\nimport path from \'path\';\\nimport showdown from \'showdown\';\\nimport he from \'he\';\\nimport jsdom from \'jsdom\';\\nimport axios from \'axios\';\\nimport fastXmlParser from \'fast-xml-parser\';\\n\\nconst bloggerXmlPath = \'./blog-03-17-2021.xml\';\\nconst docusaurusDirectory = \'../blog-website\';\\nconst notMarkdownable: string[] = [];\\n\\nconst author = \'johnnyreilly\';\\nconst author_name = \'John Reilly\';\\nconst author_url = \'https://twitter.com/johnny_reilly\';\\nconst author_image_url = \'https://johnnyreilly.com/img/profile.jpg\';\\n\\nasync function makePostsFromXML() {\\n  const blogDir = path.resolve(docusaurusDirectory, \'blog\');\\n\\n  await deleteExistingFiles(blogDir);\\n\\n  await makeAuthorsYml(blogDir);\\n\\n  const posts = await getPosts();\\n\\n  for (const post of posts) {\\n    await makePostIntoMarkDownAndDownloadImages(post);\\n  }\\n  if (notMarkdownable.length)\\n    console.log(\\n      \'These blog posts could not be turned into MarkDown - go find out why!\',\\n      notMarkdownable,\\n    );\\n}\\n\\nasync function deleteExistingFiles(directory: string) {\\n  const filesAndFolders = await fs.promises.readdir(directory);\\n  for (const file of filesAndFolders) {\\n    try {\\n      await fs.promises.unlink(path.join(directory, file));\\n    } catch (e) {\\n      await fs.promises.rm(path.join(directory, file), {\\n        recursive: true,\\n        force: true,\\n      });\\n    }\\n  }\\n}\\n\\n/**\\n * Make an authors.yml file\\n *\\n * johnnyreilly:\\n *   name: John Reilly\\n *   url: https://twitter.com/johnny_reilly\\n *   image_url: https://johnnyreilly.com/img/profile.jpg\\n */\\nasync function makeAuthorsYml(directory: string) {\\n  const authorsYml = `${author}:\\n  name: ${author_name}\\n  url: ${author_url}\\n  image_url: ${author_image_url}\\n`;\\n\\n  await fs.promises.writeFile(\\n    path.join(directory, \'authors.yml\'),\\n    authorsYml,\\n    \'utf-8\',\\n  );\\n}\\n\\nasync function getPosts(): Promise<Post[]> {\\n  const xml = await fs.promises.readFile(bloggerXmlPath, \'utf-8\');\\n\\n  const options = {\\n    attributeNamePrefix: \'@_\',\\n    attrNodeName: \'attr\', //default is \'false\'\\n    textNodeName: \'#text\',\\n    ignoreAttributes: false,\\n    ignoreNameSpace: false,\\n    allowBooleanAttributes: true,\\n    parseNodeValue: true,\\n    parseAttributeValue: true,\\n    trimValues: true,\\n    cdataTagName: \'__cdata\', //default is \'false\'\\n    cdataPositionChar: \'\\\\\\\\c\',\\n    parseTrueNumberOnly: false,\\n    arrayMode: true, //\\"strict\\"\\n    attrValueProcessor: (val: string, attrName: string) =>\\n      he.decode(val, { isAttributeValue: true }), //default is a=>a\\n    tagValueProcessor: (val: string, tagName: string) => he.decode(val), //default is a=>a\\n  };\\n\\n  const traversalObj = fastXmlParser.getTraversalObj(xml, options);\\n  const blog = fastXmlParser.convertToJson(traversalObj, options);\\n\\n  const postsRaw = blog.feed[0].entry.filter(\\n    (entry: any) =>\\n      entry.category.some(\\n        (category: any) =>\\n          category.attr[\'@_term\'] ===\\n          \'http://schemas.google.com/blogger/2008/kind#post\',\\n      ) &&\\n      entry.link.some(\\n        (link: any) =>\\n          link.attr[\'@_href\'] && link.attr[\'@_type\'] === \'text/html\',\\n      ) &&\\n      entry.published < \'2021-03-07\',\\n  );\\n\\n  const posts: Post[] = postsRaw.map((entry: any) => {\\n    return {\\n      title: entry.title[0][\'#text\'],\\n      content: entry.content[0][\'#text\'],\\n      published: entry.published,\\n      link: entry.link.find(\\n        (link: any) =>\\n          link.attr[\'@_href\'] && link.attr[\'@_type\'] === \'text/html\',\\n      )\\n        ? entry.link.find(\\n            (link: any) =>\\n              link.attr[\'@_href\'] && link.attr[\'@_type\'] === \'text/html\',\\n          ).attr[\'@_href\']\\n        : undefined,\\n      tags:\\n        Array.isArray(entry.category) &&\\n        entry.category.some(\\n          (category: any) =>\\n            category.attr[\'@_scheme\'] === \'http://www.blogger.com/atom/ns#\',\\n        )\\n          ? entry.category\\n              .filter(\\n                (category: any) =>\\n                  category.attr[\'@_scheme\'] ===\\n                    \'http://www.blogger.com/atom/ns#\' &&\\n                  category.attr[\'@_term\'] !== \'constructor\',\\n              ) // \'constructor\' will make docusaurus choke\\n              .map((category: any) => category.attr[\'@_term\'])\\n          : [],\\n    };\\n  });\\n\\n  for (const post of posts) {\\n    const { content, ...others } = post;\\n    console.log(others, content.length);\\n    if (!content || !others.title || !others.published)\\n      throw new Error(\'No content\');\\n  }\\n\\n  return posts.filter((post) => post.link);\\n}\\n\\nasync function makePostIntoMarkDownAndDownloadImages(post: Post) {\\n  const converter = new showdown.Converter({\\n    ghCodeBlocks: true,\\n  });\\n  const linkSections = post.link.split(\'/\');\\n  const linkSlug = linkSections[linkSections.length - 1];\\n  const blogdirname =\\n    post.published.substring(0, 10) + \'-\' + linkSlug.replace(\'.html\', \'\');\\n\\n  const blogdirPath = path.resolve(docusaurusDirectory, \'blog\', blogdirname);\\n\\n  if (!fs.existsSync(blogdirPath)) {\\n    fs.mkdirSync(blogdirPath);\\n  }\\n\\n  const contentProcessed = post.content\\n    // remove stray <br /> tags\\n    .replace(/<br\\\\s*\\\\/?>/gi, \'\\\\n\')\\n    // translate <code class=\\"lang-cs\\" into <code class=\\"language-cs\\"> to be showdown friendly\\n    .replace(/code class=\\"lang-/gi, \'code class=\\"language-\')\\n    // convert \x3c!-- into \x3c!---\\n    .replace(/\x3c!--/gi, \'\\\\n\x3c!---\\\\n\')\\n    .replace(/--\x3e/gi, \'\\\\n---\x3e\\\\n\');\\n  const images: string[] = [];\\n  const dom = new jsdom.JSDOM(contentProcessed);\\n  let markdown = \'\';\\n  try {\\n    markdown = converter\\n      .makeMarkdown(contentProcessed, dom.window.document)\\n      // bigger titles\\n      .replace(/#### /g, \'## \')\\n\\n      // <div style=\\"width:100%;height:0;padding-bottom:56%;position:relative;\\"><iframe src=\\"https://giphy.com/embed/l7JDTHpsXM26k\\" width=\\"100%\\" height=\\"100%\\" style=\\"position:absolute\\" frameborder=\\"0\\" class=\\"giphy-embed\\" allowfullscreen=\\"\\"></iframe></div>\\n\\n      // The mechanism below extracts the underlying iframe\\n      .replace(/<div.*(<iframe.*\\">).*<\\\\/div>/g, (replacer) => {\\n        const dom = new jsdom.JSDOM(replacer);\\n        const iframe = dom?.window?.document?.querySelector(\'iframe\');\\n        return iframe?.outerHTML ?? \'\';\\n      })\\n\\n      // The mechanism below strips class and style attributes from iframes - react hates them\\n      .replace(/<iframe.*<\\\\/iframe>/g, (replacer) => {\\n        const dom = new jsdom.JSDOM(replacer);\\n        const iframe = dom?.window?.document?.querySelector(\'iframe\');\\n        iframe?.removeAttribute(\'class\');\\n        iframe?.removeAttribute(\'style\');\\n        return iframe?.outerHTML ?? \'\';\\n      })\\n\\n      // capitalise appropriately\\n      .replace(/frameborder/g, \'frameBorder\')\\n      .replace(/allowfullscreen/g, \'allowFullScreen\')\\n      .replace(/charset/g, \'charSet\')\\n\\n      // Deals with these:\\n      // [![null](<https://4.bp.blogspot.com/-b9-GrL0IXaY/Xmqj4GRhKXI/AAAAAAAAT5s/ZoceUInSY5EWXeCr2LkGV9Zvea8S6-mUgCPcBGAYYCw/s640/hello_world_idb_keyval.png> =640x484)](<https://4.bp.blogspot.com/-b9-GrL0IXaY/Xmqj4GRhKXI/AAAAAAAAT5s/ZoceUInSY5EWXeCr2LkGV9Zvea8S6-mUgCPcBGAYYCw/s1600/hello_world_idb_keyval.png>)We successfully wrote something into IndexedDB, read it back and printed that value to the console. Amazing!\\n      .replace(\\n        /\\\\[!\\\\[null\\\\]\\\\(<(.*?)\\\\].*?>\\\\)/g,\\n        (match) =>\\n          `![](${match.slice(match.indexOf(\'<\') + 1, match.indexOf(\'>\'))})\\\\n\\\\n`,\\n      )\\n\\n      // Blogger tends to put images in HTML that looks like this:\\n      // <div class=\\"separator\\" style=\\"clear: both;\\"><a href=\\"https://1.bp.blogspot.com/-UwrtZigWg78/YDqN82KbjVI/AAAAAAAAZTE/Umezr1MGQicnxMMr5rQHD4xKINg9fasDACLcBGAsYHQ/s783/traffic-to-app-service.png\\" style=\\"display: block; padding: 1em 0; text-align: center; \\"><img alt=\\"traffic to app service\\" border=\\"0\\" width=\\"600\\" data-original-height=\\"753\\" data-original-width=\\"783\\" src=\\"https://1.bp.blogspot.com/-UwrtZigWg78/YDqN82KbjVI/AAAAAAAAZTE/Umezr1MGQicnxMMr5rQHD4xKINg9fasDACLcBGAsYHQ/s600/traffic-to-app-service.png\\"></a></div>\\n\\n      // The mechanism below extracts the underlying image path and it\'s alt text\\n      .replace(\\n        /(<div.*>)*\\\\w*(<a .*>)*(<img .*\\">)(<\\\\/a>)*.*(<\\\\/div>)*/g,\\n        (replacer) => {\\n          const div = new jsdom.JSDOM(replacer);\\n          const img = div?.window?.document?.querySelector(\'img\');\\n          const alt = img?.getAttribute(\'alt\') ?? \'\';\\n          const src = img?.getAttribute(\'src\') ?? \'\';\\n\\n          if (src) images.push(src);\\n\\n          return `![${alt}](${src})`;\\n        },\\n      );\\n  } catch (e) {\\n    console.log(post.link);\\n    console.log(e);\\n    notMarkdownable.push(post.link);\\n    return;\\n  }\\n\\n  for (const url of images) {\\n    try {\\n      const localUrl = await downloadImage(url, blogdirPath);\\n      markdown = markdown.replace(url, localUrl);\\n    } catch (e) {\\n      console.error(`Failed to download ${url}`);\\n    }\\n  }\\n\\n  const content = `---\\ntitle: \\"${post.title}\\"\\nauthors: ${author}\\ntags: [${post.tags.join(\', \')}]\\nhide_table_of_contents: false\\n---\\n${markdown}\\n`;\\n\\n  await fs.promises.writeFile(\\n    path.resolve(docusaurusDirectory, \'blog\', blogdirPath, \'index.md\'),\\n    content,\\n  );\\n}\\n\\nasync function downloadImage(url: string, directory: string) {\\n  console.log(`Downloading ${url}`);\\n  const pathParts = new URL(url).pathname.split(\'/\');\\n  const filename = decodeURIComponent(pathParts[pathParts.length - 1]);\\n\\n  const pathTo = path.join(directory, filename);\\n\\n  const writer = fs.createWriteStream(pathTo);\\n\\n  const response = await axios({\\n    url,\\n    method: \'GET\',\\n    responseType: \'stream\',\\n  });\\n\\n  response.data.pipe(writer);\\n\\n  return new Promise<string>((resolve, reject) => {\\n    writer.on(\'finish\', () => resolve(filename));\\n    writer.on(\'error\', reject);\\n  });\\n}\\n\\ninterface Post {\\n  title: string;\\n  content: string;\\n  published: string;\\n  link: string;\\n  tags: string[];\\n}\\n\\n// do it!\\nmakePostsFromXML();\\n```\\n\\nTo summarise what the script does, it:\\n\\n- deletes the default blog posts\\n- creates a new `authors.yml` file with my details in\\n- parses the blog XML into an array of `Post`s\\n- each post is then converted from HTML into Markdown, a Docusaurus header is created and prepended, then the `index.md` file is saved to the `blog-website/blog/{POST_NAME}` directory\\n- the images of each post are downloaded with Axios and saved to the `blog-website/blog/{POST_NAME}` directory\\n\\n[To see the full code, you can find it on the GitHub repository that now represents the blog.](https://github.com/johnnyreilly/blog.johnnyreilly.com/tree/main/from-blogger-to-docusaurus)\\n\\nIf you\'re trying to do this yourself, you\'ll want to change some of the variable values in the script; such as the author details.\\n\\n## Bringing it all together\\n\\nTo run the script, we add the following script to the `package.json`:\\n\\n```json\\n  \\"scripts\\": {\\n    \\"start\\": \\"ts-node index.ts\\"\\n  },\\n```\\n\\nAnd have ourselves a merry little `yarn start` to kick off the process. In a very short period of time, if you crack open the `blogs` directory of your Docusaurus site you\'ll see a collection of folders, Markdown files and images. These represent your blog and are ready to power Docusaurus:\\n\\n![Markdown files](blogs-as-markdown.webp)\\n\\nI have slightly papered over some details here. For my own case I discovered that I hadn\'t always written perfect HTML when blogging. I had to go in and fix the HTML in a number of historic blogs and re-download, to get cleanish Markdown.\\n\\nI also learned that a number of my blog\'s images had vanished from Blogger at some point. This makes me all the more convinced that storing your blog in a repo is a good idea. Things should not \\"go missing\\".\\n\\nIf we now run `yarn start` in the `blog-website` directory we can see the blog in action:\\n\\n![Blog in Docusaurus](docusaurus.png)\\n\\nCongratulations! We\'re now the proud owners of a Docusaurus blog site based upon our Blogger content.\\n\\nIf you\'ve got some curiously named image files you might encounter some minor issues that need fixing up. This should get you 95% the way there though. Docusaurus does a great job of telling you when there\'s issues.\\n\\n## Redirecting from Blogger URLs to Docusaurus URLs\\n\\nThe final step is to redirect from the old Blogger URLs to the new Docusaurus URLs. Blogger URLs look like this: `/2019/10/definitely-typed-movie.html`. On the other hand, Docusaurus URLs look like this: [`/2019/10/08/definitely-typed-movie`](https://johnnyreilly.com/definitely-typed-the-movie).\\n\\nI\'ll want to redirect from the former to the latter. I\'ll use the `@docusaurus/plugin-client-redirects` plugin to do this. Inside the `docusaurus.config.js` file, I\'ll add the following to the `plugins` section:\\n\\n```js\\nmodule.exports = {\\n  // ...\\n  plugins: [\\n    // ...\\n    [\\n      \'client-redirects\',\\n      /** @type {import(\'@docusaurus/plugin-client-redirects\').Options} */\\n      ({\\n        createRedirects: function (existingPath) {\\n          if (existingPath.match(urlRegex)) {\\n            const [, year, month, date, slug] = existingPath.split(\'/\');\\n            const oldUrl = `/${year}/${month}/${slug}.html`;\\n\\n            // eg redirect from /2019/10/definitely-typed-movie.html -> /2019/10/08/definitely-typed-movie\\n            console.log(`redirect from ${oldUrl} -> ${existingPath}`);\\n\\n            return [oldUrl, `/${year}/${month}/${slug}`];\\n          }\\n        },\\n      }),\\n    ],\\n    // ...\\n  ],\\n};\\n```\\n\\nThe function above will be run during the build process for each URL. And consequently a client side redirect will be created to go from the landing URL to the Docusaurus URL. The `console.log` is there to help me see what\'s going on. I don\'t actually need it.\\n\\nHaving this in place should protect my SEO when the domain switches from Blogger to Docusaurus. Long term I shouldn\'t need this approach in place.\\n\\n## Comments\\n\\nI\'d always had comments on my blog. First with Blogger\'s in-built functionality and then with [Disqus](https://disqus.com/). One thing that Docusaurus doesn\'t support by default is comments for blog posts. [There\'s a feature request for it here.](https://docusaurus.io/feature-requests/p/comments-in-documents-or-blogs) However, it doesn\'t exist right now.\\n\\nFor a while I considered this a dealbreaker, and wasn\'t planning to complete the migration. But then I had a discussion with [Josh Goldberg](https://twitter.com/JoshuaKGoldberg) as to the value of comments. Essentially that they are nice, but not essential.\\n\\n![discussion on Twitter with Josh Goldberg on the topic of the value of comments in blog posts](screenshot-do-we-need-comments-josh-goldberg.webp)\\n\\nI rather came to agree with the notion that comments were only slightly interesting as I looked back at the comments I\'d received on my blog over the years. So I decided to go ahead _without_ comments. I remain happy with that choice, so thanks Josh!\\n\\nHowever, if it\'s important to you, there are ways to support comments. One example is using [Giscus](https://giscus.app/); [here is a guide on how to integrate it](https://dipakparmar.medium.com/how-to-add-giscus-to-your-docs-site-built-with-docusaurus-d57fa7f8e2f3).\\n\\n## DNS and RSS\\n\\nAt this point I had a repository that represented my blog. I had a Docusaurus site that represented my blog. When I ran `yarn build` I got a Docusaurus site that looked like my blog. I had a redirect mechanism in place to protect my SEO.\\n\\nI was ready to make the switch.\\n\\nHosting is a choice. When I initially migrated, I made use of GitHub Pages. I also experimented with Netlify. [Finally I moved to using Azure Static Web Apps to make use of preview environments.](../2023-02-01-migrating-from-github-pages-to-azure-static-web-apps/index.md) There are many choices out there - you can pick the one that works best for you.\\n\\nOnce your site is up, the last stage of the migration is updating your DNS to point to the Docusaurus site. I use [Cloudflare](https://www.cloudflare.com/) to manage my domain names and so that\'s where I made the switch.\\n\\n![screenshot of the DNS settings in Cloudflare](screenshot-cloudflare-dns.webp)\\n\\n## RSS / Atom feeds\\n\\nIf you\'re like me, you\'ll want to keep your RSS feed. I didn\'t want to disrupting people who consumed my RSS feed as I migrated.\\n\\nHappily, [Docusaurus ships with RSS / Atom in the box](https://docusaurus.io/docs/blog#feed). Even happier still, most of the feed URLs in Blogger match the same URLs in Docusaurus. There was one exception in the form of the `/feeds/posts/default` feed which is an Atom feed. Docusaurus has an `atom.xml` feed but it\'s not in the same place.\\n\\nThis isn\'t a significant issue as I can create a page rule in Cloudflare to redirect from the old URL (https://johnnyreilly.com/feeds/posts/default) to the new URL (https://johnnyreilly.com/atom.xml):\\n\\n![screenshot of the page rule in Cloudflare](screenshot-cloudflare-atom-page-rule.png)\\n\\n## Conclusion\\n\\nI\'ve migrated to Docusaurus and have been happily running there for a while now. I\'m very happy with the result.\\n\\nThis post is intended to be a community resource that helps folk migrate from Blogger to Docusaurus. If you should find issues with the migration, please do let me know and help make this resource even better."},{"id":"managed-identity-azure-sql-entity-framework","metadata":{"permalink":"/managed-identity-azure-sql-entity-framework","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-03-10-managed-identity-azure-sql-entity-framework/index.md","source":"@site/blog/2021-03-10-managed-identity-azure-sql-entity-framework/index.md","title":"Managed Identity, Azure SQL and Entity Framework","description":"Managed Identity provides secure, developer-friendly access to Azure SQL databases without the need for usernames and passwords.","date":"2021-03-10T00:00:00.000Z","tags":[{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."}],"readingTime":4.93,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"managed-identity-azure-sql-entity-framework","title":"Managed Identity, Azure SQL and Entity Framework","authors":"johnnyreilly","tags":["azure"],"image":"./entity-framework-core-nuget.png","hide_table_of_contents":false,"description":"Managed Identity provides secure, developer-friendly access to Azure SQL databases without the need for usernames and passwords."},"unlisted":false,"prevItem":{"title":"The definitive guide to migrating from Blogger to Docusaurus","permalink":"/definitive-guide-to-migrating-from-blogger-to-docusaurus"},"nextItem":{"title":"NSwag: TypeScript and CSharp client generation based on an API","permalink":"/generate-typescript-and-csharp-clients-with-nswag"}},"content":"Managed Identity offers a very secure way for applications running in Azure to connect to Azure SQL databases. It\'s an approach that does not require code changes; merely configuration of connection string and associated resources. Hence it has a good developer experience. Importantly, it allows us to avoid exposing our database to username / password authentication, and hence making it a tougher target for bad actors.\\n\\n\x3c!--truncate--\x3e\\n\\nThis post talks us through using managed identity for connecting to Azure SQL.\\n\\n## `Integrated Security=true`\\n\\nEveryone is deploying to the cloud. Few are the organisations that view deployment to data centers they manage as the future. This is generally a good thing, however in the excitement of the new, it\'s possible to forget some of the good properties that \\"on premise\\" deployment afforded when it came to connectivity and authentication.\\n\\nI speak of course, of our old friend `Integrated Security=true`. When you seek to connect a web application to a database, you\'ll typically use some kind of database connection string. And back in the day, it may have looked something like this:\\n\\n```\\nData Source=myServer;Initial Catalog=myDB;Integrated Security=true;\\n```\\n\\nThe above provides a database server, a database and also `Integrated Security=true`. When you see `Integrated Security=true`, what you\'re essentially looking at is an instruction to use the identity that an application is running under (typically called a \\"service account\\") as the authentication credential to secure access to the database. Under the covers, this amounts to [Windows Authentication](https://docs.microsoft.com/en-us/dotnet/framework/data/adonet/sql/authentication-in-sql-server).\\n\\nThe significant thing about this approach is that it is more secure than using usernames and passwords in the connection string. If you have to use username and password to authenticate, then you need to persist them somewhere - so you need to make sure that\'s secure. Also, if someone manages to acquire that username and password, they\'re free to get access to the database and do malicious things.\\n\\nBottom line: the less you are sharing authentication credentials, the better your security. Integrated Security is a harder nut to crack than username and password. The thing to note about the above phrase is \\"Windows Authentication\\". Web Apps in Azure / AWS etc do not typically use Windows Authentication when it comes to connecting to the database. Connecting with username / password is far more common.\\n\\nWhat if there was a way to have the developer experience of `Integrated Security=true` without needing to use Windows Authentication? There is.\\n\\n## Managed Identity\\n\\nThe docs express the purpose of [managed identity](https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview) well:\\n\\n> A common challenge for developers is the management of secrets and credentials to secure communication between different services. On Azure, managed identities eliminate the need for developers having to manage credentials by providing an identity for the Azure resource in Azure AD and using it to obtain Azure Active Directory (Azure AD) tokens\\n\\nHistorically a certain amount of ceremony was required to use managed identity to connect to a database, and could involve augmenting a `DbContext` like so:\\n\\n```cs\\npublic MyDbContext(DbContextOptions options) : base(options) {\\n    var conn = (Microsoft.Data.SqlClient.SqlConnection)Database.GetDbConnection();\\n    var credential = new DefaultAzureCredential();\\n    var token = credential\\n        .GetToken(\\n            new Azure.Core.TokenRequestContext(new[] { \\"https://database.windows.net/.default\\" })\\n        );\\n    conn.AccessToken = token.Token;\\n}\\n```\\n\\nThis mechanism works, and has the tremendous upside of no longer requiring credentials be passed in a connection string. However, as you can see this isn\'t the simplest of setups. And also, what if you don\'t want to use managed identity when you\'re developing locally? This approach has baggage and forces us to make code changes.\\n\\n## Connection String alone\\n\\nThe wonderful aspect of the original `Integrated Security=true` approach, was that there were no code changes required; one need only supply the connection string. Just configuration.\\n\\nThis is now possible with Azure SQL thanks to [this PR](https://github.com/dotnet/SqlClient/pull/730) to the [Microsoft.Data.SqlClient](https://www.nuget.org/packages/Microsoft.Data.SqlClient/) nuget package. (Incidentally, [Microsoft.Data.SqlClient is the successor to System.Data.SqlClient.](https://devblogs.microsoft.com/dotnet/introducing-the-new-microsoftdatasqlclient/))\\n\\nSupport for connection string managed identities [shipped with v2.1](https://github.com/dotnet/SqlClient/blob/master/release-notes/2.1/2.1.0/index.md#Azure-Active-Directory-Managed-Identity-authentication). Connection strings can look slightly different depending on the type of managed identity you\'re using:\\n\\n```\\n// For System Assigned Managed Identity\\n\\"Server:{serverURL}; Authentication=Active Directory MSI; Initial Catalog={db};\\"\\n\\n// For System Assigned Managed Identity\\n\\"Server:{serverURL}; Authentication=Active Directory Managed Identity; Initial Catalog={db};\\"\\n\\n// For User Assigned Managed Identity\\n\\"Server:{serverURL}; Authentication=Active Directory MSI; User Id={ObjectIdOfManagedIdentity}; Initial Catalog={db};\\"\\n\\n// For User Assigned Managed Identity\\n\\"Server:{serverURL}; Authentication=Active Directory Managed Identity; User Id={ObjectIdOfManagedIdentity}; Initial Catalog={db};\\"\\n```\\n\\nRegardless of the approach, you can see that none of the connection strings have credentials in them. And that\'s special.\\n\\n## Usage with Entity Framework Core 5\\n\\nIf you\'re using Entity Framework Core, you might be struggling to get this working and encountering strange error messages. In my ASP.NET project I had a dependendency on\\n[Microsoft.EntityFrameworkCore.SqlServer@5](https://www.nuget.org/packages/Microsoft.EntityFrameworkCore.SqlServer/5.0.4).\\n\\n![Microsoft.EntityFrameworkCore.SqlServer@5 in NuGet](entity-framework-core-nuget.png)\\n\\nIf you look close above, you\'ll see that the package has a dependency on Microsoft.Data.SqlClient, but crucially on 2.0.1 or greater. So if `dotnet` has installed a version of Microsoft.Data.SqlClient which is _less_ than 2.1 then the functionality required will not be installed. The resolution is simple, ensure that the required version is installed:\\n\\n```\\ndotnet add package Microsoft.Data.SqlClient --version 2.1.2\\n```\\n\\nThe version which we want to use is 2.1 (or greater) and fortunately that is compatible with Entity Framework Core 5. Incidentally, when Entity Framework Core 6 ships it will no longer be necessary to manually specify this dependency as it already requires Microsoft.Data.SqlClient@2.1 as a minimum.\\n\\n## User Assigned Managed Identity\\n\\nIf you\'re using user assigned managed identity, you\'ll need to supply the object id of your managed identity, which you can find in the [Azure Portal](https://portal.azure.com/):\\n\\n![Managed Identity object id](managed-identity-object-id.webp)\\n\\nYou can configure this in ARM as well, but cryptically, the object id goes by the nom de plume of `principalId` (thanks to my partner in crime [John McCormick](https://github.com/jmccor99) for puzzling that out):\\n\\n```json\\n\\"CONNECTIONSTRINGS__OURDBCONNECTION\\": \\"[concat(\'Server=tcp:\', parameters(\'sqlServerName\') , \'.database.windows.net,1433;Initial Catalog=\', parameters(\'sqlDatabaseName\'),\';Authentication=Active Directory MSI\',\';User Id=\', reference(resourceId(\'Microsoft.ManagedIdentity/userAssignedIdentities/\', parameters(\'managedIdentityName\')), \'2018-11-30\').principalId)]\\"\\n```\\n\\nThat\'s it! With managed identity handling your authentication you can sleep easy, knowing you should be in a better place security wise."},{"id":"generate-typescript-and-csharp-clients-with-nswag","metadata":{"permalink":"/generate-typescript-and-csharp-clients-with-nswag","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-03-06-generate-typescript-and-csharp-clients-with-nswag/index.md","source":"@site/blog/2021-03-06-generate-typescript-and-csharp-clients-with-nswag/index.md","title":"NSwag: TypeScript and CSharp client generation based on an API","description":"NSwag simplifies APIs by auto-generating OpenAPI specs and clients. Learn to create TypeScript clients from NSwag using a .NET console app.","date":"2021-03-06T00:00:00.000Z","tags":[{"inline":false,"label":"Swagger","permalink":"/tags/swagger","description":"The Swagger API documentation framework - now known as OpenAPI."},{"inline":false,"label":"C#","permalink":"/tags/csharp","description":"The C# programming language."},{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":8.54,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"generate-typescript-and-csharp-clients-with-nswag","title":"NSwag: TypeScript and CSharp client generation based on an API","authors":"johnnyreilly","tags":["swagger","c#","azure","typescript"],"image":"./use-generated-client.gif","hide_table_of_contents":false,"description":"NSwag simplifies APIs by auto-generating OpenAPI specs and clients. Learn to create TypeScript clients from NSwag using a .NET console app."},"unlisted":false,"prevItem":{"title":"Managed Identity, Azure SQL and Entity Framework","permalink":"/managed-identity-azure-sql-entity-framework"},"nextItem":{"title":"Goodbye Client Affinity, Hello Data Protection with Azure","permalink":"/goodbye-client-affinity-hello-data-protection-with-azure"}},"content":"Generating clients for APIs is a tremendous way to reduce the amount of work you have to do when you\'re building a project. Why handwrite that code when it can be auto-generated for you quickly and accurately by a tool like [NSwag](https://github.com/RicoSuter/NSwag)? To quote the docs:\\n\\n\x3c!--truncate--\x3e\\n\\n> The NSwag project provides tools to generate OpenAPI specifications from existing ASP.NET Web API controllers and client code from these OpenAPI specifications. The project combines the functionality of Swashbuckle (OpenAPI/Swagger generation) and AutoRest (client generation) in one toolchain.\\n\\nThere\'s some great posts out there that show you how to generate the clients with NSwag using an `nswag.json` file directly from a .NET project.\\n\\nHowever, what if you want to use NSwag purely for its client generation capabilities? You may have an API written with another language / platform that exposes a Swagger endpoint, that you simply wish to create a client for. How do you do that? Also, if you want to do some special customisation of the clients you\'re generating, you may find yourself struggling to configure that in `nswag.json`. In that case, it\'s possible to hook into NSwag directly to do this with a simple .NET console app.\\n\\nThis post will:\\n\\n- Create a .NET API which exposes a Swagger endpoint. (Alternatively, you could use any other Swagger endpoint; [for example an Express API](https://blog.logrocket.com/documenting-your-express-api-with-swagger/).)\\n- Create a .NET console app which can create both TypeScript and CSharp clients from a Swagger endpoint.\\n- Create a script which, when run, creates a TypeScript client.\\n- Consume the API using the generated client in a simple TypeScript application.\\n\\nYou will need both [Node.js](https://nodejs.org/en/) and the [.NET SDK](https://dotnet.microsoft.com/download) installed.\\n\\n## Create an API\\n\\nWe\'ll now create an API which exposes a [Swagger / Open API](https://swagger.io/resources/open-api/) endpoint. Whilst we\'re doing that we\'ll create a TypeScript React app which we\'ll use later on. We\'ll drop to the command line and enter the following commands which use the .NET SDK, node and the `create-react-app` package:\\n\\n```shell\\nmkdir src\\ncd src\\nnpx create-react-app client-app --template typescript\\nmkdir server-app\\ncd server-app\\ndotnet new api -o API\\ncd API\\ndotnet add package NSwag.AspNetCore\\n```\\n\\nWe now have a .NET API with a dependency on NSwag. We\'ll start to use it by replacing the `Startup.cs` that\'s been generated with the following:\\n\\n```cs\\nusing Microsoft.AspNetCore.Builder;\\nusing Microsoft.AspNetCore.Hosting;\\nusing Microsoft.Extensions.Configuration;\\nusing Microsoft.Extensions.DependencyInjection;\\nusing Microsoft.Extensions.Hosting;\\n\\nnamespace API\\n{\\n    public class Startup\\n    {\\n        const string ALLOW_DEVELOPMENT_CORS_ORIGINS_POLICY = \\"AllowDevelopmentSpecificOrigins\\";\\n        const string LOCAL_DEVELOPMENT_URL = \\"http://localhost:3000\\";\\n\\n        public Startup(IConfiguration configuration)\\n        {\\n            Configuration = configuration;\\n        }\\n\\n        public IConfiguration Configuration { get; }\\n\\n        // This method gets called by the runtime. Use this method to add services to the container.\\n        public void ConfigureServices(IServiceCollection services)\\n        {\\n\\n            services.AddControllers();\\n\\n            services.AddCors(options => {\\n                options.AddPolicy(name: ALLOW_DEVELOPMENT_CORS_ORIGINS_POLICY,\\n                    builder => {\\n                        builder.WithOrigins(LOCAL_DEVELOPMENT_URL)\\n                            .AllowAnyMethod()\\n                            .AllowAnyHeader()\\n                            .AllowCredentials();\\n                    });\\n            });\\n\\n            // Register the Swagger services\\n            services.AddSwaggerDocument();\\n        }\\n\\n        // This method gets called by the runtime. Use this method to configure the HTTP request pipeline.\\n        public void Configure (IApplicationBuilder app, IWebHostEnvironment env)\\n        {\\n            if (env.IsDevelopment())\\n            {\\n                app.UseDeveloperExceptionPage();\\n            }\\n            else\\n            {\\n                app.UseExceptionHandler(\\"/Error\\");\\n                app.UseHsts ();\\n                app.UseHttpsRedirection();\\n            }\\n\\n            app.UseDefaultFiles();\\n            app.UseStaticFiles();\\n\\n            app.UseRouting();\\n\\n            app.UseAuthorization();\\n\\n            // Register the Swagger generator and the Swagger UI middlewares\\n            app.UseOpenApi();\\n            app.UseSwaggerUi3();\\n\\n            if (env.IsDevelopment())\\n                app.UseCors(ALLOW_DEVELOPMENT_CORS_ORIGINS_POLICY);\\n\\n            app.UseEndpoints(endpoints =>\\n            {\\n                endpoints.MapControllers();\\n            });\\n        }\\n    }\\n}\\n```\\n\\nThe significant changes in the above `Startup.cs` are:\\n\\n1. Exposing a Swagger endpoint with `UseOpenApi` and `UseSwaggerUi3`. NSwag will automagically create Swagger endpoints in your application for all your controllers. The .NET template ships with a `WeatherForecastController`.\\n2. Allowing [Cross-Origin Requests (CORS)](https://docs.microsoft.com/en-us/aspnet/core/security/cors) which is useful during development (and will facilitate a demo later).\\n\\nBack in the root of our project we\'re going to initialise an npm project. We\'re going to use this to put in place a number of handy [`npm scripts`](https://docs.npmjs.com/cli/v6/using-npm/scripts) that will make our project easier to work with. So we\'ll `npm init` and accept all the defaults.\\n\\nNow we\'re going add some dependencies which our scripts will use: `npm install cpx cross-env npm-run-all start-server-and-test`\\n\\nWe\'ll also add ourselves some `scripts` to our `package.json`:\\n\\n```json\\n\\"scripts\\": {\\n    \\"postinstall\\": \\"npm run install:client-app && npm run install:server-app\\",\\n    \\"install:client-app\\": \\"cd src/client-app && npm install\\",\\n    \\"install:server-app\\": \\"cd src/server-app/API && dotnet restore\\",\\n    \\"build\\": \\"npm run build:client-app && npm run build:server-app\\",\\n    \\"build:client-app\\": \\"cd src/client-app && npm run build\\",\\n    \\"postbuild:client-app\\": \\"cpx \\\\\\"src/client-app/build/**/*.*\\\\\\" \\\\\\"src/server-app/API/wwwroot/\\\\\\"\\",\\n    \\"build:server-app\\": \\"cd src/server-app/API && dotnet build --configuration release\\",\\n    \\"start\\": \\"run-p start:client-app start:server-app\\",\\n    \\"start:client-app\\": \\"cd src/client-app && npm start\\",\\n    \\"start:server-app\\": \\"cross-env ASPNETCORE_URLS=http://*:5000 ASPNETCORE_ENVIRONMENT=Development dotnet watch --project src/server-app/API run --no-launch-profile\\"\\n  }\\n```\\n\\nLet\'s walk through what the above scripts provide us with:\\n\\n- Running `npm install` in the root of our project will not only install dependencies for our root `package.json`, thanks to our `postinstall`, `install:client-app` and `install:server-app` scripts it will install the React app and .NET app dependencies as well.\\n- Running `npm run build` will build our client and server apps.\\n- Running `npm run start` will start both our React app and our .NET app. Our React app will be started at [http://localhost:3000](http://localhost:3000). Our .NET app will be started at [http://localhost:5000](http://localhost:5000) (some environment variables are passed to it with [`cross-env`](https://github.com/kentcdodds/cross-env) ).\\n\\nOnce `npm run start` has been run, you will find a Swagger endpoint at [http://localhost:5000/swagger](http://localhost:5000/swagger):\\n\\n![swagger screenshot](swagger.webp)\\n\\n## The client generator project\\n\\nNow we\'ve scaffolded our Swagger-ed API, we want to put together the console app that will generate our typed clients.\\n\\n```shell\\ncd src/server-app\\ndotnet new console -o APIClientGenerator\\ncd APIClientGenerator\\ndotnet add package NSwag.CodeGeneration.CSharp\\ndotnet add package NSwag.CodeGeneration.TypeScript\\ndotnet add package NSwag.Core\\n```\\n\\nWe now have a console app with dependencies on the code generation portions of NSwag. Now let\'s change up `Program.cs` to make use of this:\\n\\n```cs\\nusing System;\\nusing System.IO;\\nusing System.Threading.Tasks;\\nusing NJsonSchema;\\nusing NJsonSchema.CodeGeneration.TypeScript;\\nusing NJsonSchema.Visitors;\\nusing NSwag;\\nusing NSwag.CodeGeneration.CSharp;\\nusing NSwag.CodeGeneration.TypeScript;\\n\\nnamespace APIClientGenerator\\n{\\n    class Program\\n    {\\n        static async Task Main(string[] args)\\n        {\\n            if (args.Length != 3)\\n                throw new ArgumentException(\\"Expecting 3 arguments: URL, generatePath, language\\");\\n\\n            var url = args[0];\\n            var generatePath = Path.Combine(Directory.GetCurrentDirectory(), args[1]);\\n            var language = args[2];\\n\\n            if (language != \\"TypeScript\\" && language != \\"CSharp\\")\\n                throw new ArgumentException(\\"Invalid language parameter; valid values are TypeScript and CSharp\\");\\n\\n            if (language == \\"TypeScript\\")\\n                await GenerateTypeScriptClient(url, generatePath);\\n            else\\n                await GenerateCSharpClient(url, generatePath);\\n        }\\n\\n        async static Task GenerateTypeScriptClient(string url, string generatePath) =>\\n            await GenerateClient(\\n                document: await OpenApiDocument.FromUrlAsync(url),\\n                generatePath: generatePath,\\n                generateCode: (OpenApiDocument document) =>\\n                {\\n                    var settings = new TypeScriptClientGeneratorSettings();\\n\\n                    settings.TypeScriptGeneratorSettings.TypeStyle = TypeScriptTypeStyle.Interface;\\n                    settings.TypeScriptGeneratorSettings.TypeScriptVersion = 3.5M;\\n                    settings.TypeScriptGeneratorSettings.DateTimeType = TypeScriptDateTimeType.String;\\n\\n                    var generator = new TypeScriptClientGenerator(document, settings);\\n                    var code = generator.GenerateFile();\\n\\n                    return code;\\n                }\\n            );\\n\\n        async static Task GenerateCSharpClient(string url, string generatePath) =>\\n            await GenerateClient(\\n                document: await OpenApiDocument.FromUrlAsync(url),\\n                generatePath: generatePath,\\n                generateCode: (OpenApiDocument document) =>\\n                {\\n                    var settings = new CSharpClientGeneratorSettings\\n                    {\\n                        UseBaseUrl = false\\n                    };\\n\\n                    var generator = new CSharpClientGenerator(document, settings);\\n                    var code = generator.GenerateFile();\\n                    return code;\\n                }\\n            );\\n\\n        private async static Task GenerateClient(OpenApiDocument document, string generatePath, Func<OpenApiDocument, string> generateCode)\\n        {\\n            Console.WriteLine($\\"Generating {generatePath}...\\");\\n\\n            var code = generateCode(document);\\n\\n            await System.IO.File.WriteAllTextAsync(generatePath, code);\\n        }\\n    }\\n}\\n```\\n\\nWe\'ve created ourselves a simple .NET console application that creates TypeScript and CSharp clients for a given Swagger URL. It expects three arguments:\\n\\n- `url` \\\\- the url of the `swagger.json` file to generate a client for.\\n- `generatePath` \\\\- the path where the generated client file should be placed, relative to this project.\\n- `language` \\\\- the language of the client to generate; valid values are \\"TypeScript\\" and \\"CSharp\\".\\n\\nTo create a TypeScript client with it then we\'d use the following command:\\n\\n```shell\\ndotnet run --project src/server-app/APIClientGenerator http://localhost:5000/swagger/v1/swagger.json src/client-app/src/clients.ts TypeScript\\n```\\n\\nHowever, for this to run successfully, we\'ll first have to ensure the API is running. It would be great if we had a single command we could run that would:\\n\\n- bring up the API\\n- generate a client\\n- bring down the API\\n\\nLet\'s make that.\\n\\n## Building a \\"make a client\\" script\\n\\nIn the root of the project we\'re going to add the following `scripts`:\\n\\n```json\\n\\"generate-client:server-app\\": \\"start-server-and-test generate-client:server-app:serve http-get://localhost:5000/swagger/v1/swagger.json generate-client:server-app:generate\\",\\n\\"generate-client:server-app:serve\\": \\"cross-env ASPNETCORE_URLS=http://*:5000 ASPNETCORE_ENVIRONMENT=Development dotnet run --project src/server-app/API --no-launch-profile\\",\\n\\"generate-client:server-app:generate\\": \\"dotnet run --project src/server-app/APIClientGenerator http://localhost:5000/swagger/v1/swagger.json src/client-app/src/clients.ts TypeScript\\",\\n```\\n\\nLet\'s walk through what\'s happening here. Running `npm run generate-client:server-app` will:\\n\\n- Use the [`start-server-and-test`](https://github.com/bahmutov/start-server-and-test) package to spin up our server-app by running the `generate-client:server-app:serve` script.\\n- `start-server-and-test` waits for the Swagger endpoint to start responding to requests. When it does start responding, `start-server-and-test` runs the `generate-client:server-app:generate` script which runs our APIClientGenerator console app and provides it with the URL where our swagger can be found, the path of the file to generate and the language of \\"TypeScript\\"\\n\\nIf you were wanting to generate a C# client (say if you were writing a [Blazor](https://blog.logrocket.com/js-free-frontends-blazor/) app) then you could change the `generate-client:server-app:generate` script as follows:\\n\\n```json\\n\\"generate-client:server-app:generate\\": \\"dotnet run --project src/server-app/ApiClientGenerator http://localhost:5000/swagger/v1/swagger.json clients.cs CSharp\\",\\n```\\n\\n## Consume our generated API client\\n\\nLet\'s run the `npm run generate-client:server-app` command. It creates a `clients.ts` file which nestles nicely inside our `client-app`. We\'re going to exercise that in a moment. First of all, let\'s enable proxying from our `client-app` to our `server-app` following the instructions in the [Create React App docs](https://create-react-app.dev/docs/proxying-api-requests-in-development/) and adding the following to our `client-app/package.json`:\\n\\n```json\\n\\"proxy\\": \\"http://localhost:5000\\"\\n```\\n\\nNow let\'s start our apps with `npm run start`. We\'ll then replace the contents of `App.tsx` with:\\n\\n```jsx\\nimport React from \\"react\\";\\nimport \\"./App.css\\";\\nimport { WeatherForecast, WeatherForecastClient } from \\"./clients\\";\\n\\nfunction App() {\\n  const [weather, setWeather] = React.useState<WeatherForecast[] | null>();\\n  React.useEffect(() => {\\n    async function loadWeather() {\\n      const weatherClient = new WeatherForecastClient(/* baseUrl */ \\"\\");\\n      const forecast = await weatherClient.get();\\n      setWeather(forecast);\\n    }\\n    loadWeather();\\n  }, [setWeather]);\\n\\n  return (\\n    <div className=\\"App\\">\\n      <header className=\\"App-header\\">\\n        {weather ? (\\n          <table>\\n            <thead>\\n              <tr>\\n                <th>Date</th>\\n                <th>Summary</th>\\n                <th>Centigrade</th>\\n                <th>Fahrenheit</th>\\n              </tr>\\n            </thead>\\n            <tbody>\\n              {weather.map(({ date, summary, temperatureC, temperatureF }) => (\\n                <tr key={date}>\\n                  <td>{new Date(date).toLocaleDateString()}</td>\\n                  <td>{summary}</td>\\n                  <td>{temperatureC}</td>\\n                  <td>{temperatureF}</td>\\n                </tr>\\n              ))}\\n            </tbody>\\n          </table>\\n        ) : (\\n          <p>Loading weather...</p>\\n        )}\\n      </header>\\n    </div>\\n  );\\n}\\n\\nexport default App;\\n```\\n\\nInside the `React.useEffect` above you can see we create a new instance of the auto-generated `WeatherForecastClient`. We then call `weatherClient.get()` which sends the `GET` request to the server to acquire the data and provides it in a strongly typed fashion (`get()` returns an array of `WeatherForecast`). This is then displayed on the page like so:\\n\\n![load data from server](use-generated-client.gif)\\n\\nAs you an see we\'re loading data from the server using our auto-generated client. We\'re reducing the amount of code we have to write _and_ we\'re reducing the likelihood of errors.\\n\\nThis post was originally posted on [LogRocket](https://blog.logrocket.com/generate-typescript-csharp-clients-nswag-api/).\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.logrocket.com/generate-typescript-csharp-clients-nswag-api/\\" />\\n</head>"},{"id":"goodbye-client-affinity-hello-data-protection-with-azure","metadata":{"permalink":"/goodbye-client-affinity-hello-data-protection-with-azure","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-02-27-goodbye-client-affinity-hello-data-protection-with-azure/index.md","source":"@site/blog/2021-02-27-goodbye-client-affinity-hello-data-protection-with-azure/index.md","title":"Goodbye Client Affinity, Hello Data Protection with Azure","description":"How to use ASP.NET Data Protection to remove the need for sticky sessions with Client Affinity","date":"2021-02-27T00:00:00.000Z","tags":[{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."},{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."},{"inline":false,"label":"Easy Auth","permalink":"/tags/easy-auth","description":"The Azure Easy Auth feature used for authentication and authorization."}],"readingTime":3.455,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"goodbye-client-affinity-hello-data-protection-with-azure","title":"Goodbye Client Affinity, Hello Data Protection with Azure","description":"How to use ASP.NET Data Protection to remove the need for sticky sessions with Client Affinity","authors":"johnnyreilly","tags":["azure","asp.net","easy auth"],"image":"./traffic-to-app-service.png","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"NSwag: TypeScript and CSharp client generation based on an API","permalink":"/generate-typescript-and-csharp-clients-with-nswag"},"nextItem":{"title":"Making Easy Auth tokens survive releases on Linux Azure App Service","permalink":"/easy-auth-tokens-survive-releases-on-linux-azure-app-service"}},"content":"I\'ve written lately about [zero downtime releases with Azure App Service](../2021-02-11-azure-app-service-health-checks-and-zero-downtime-deployments/index.md). Zero downtime releases are only successful if your authentication mechanism survives a new deployment. We looked in my last post at [how to achieve this with Azure\'s in-built authentication mechanism; Easy Auth](../2021-02-16-easy-auth-tokens-survive-releases-on-linux-azure-app-service/index.md).\\n\\n\x3c!--truncate--\x3e\\n\\nWe\'re now going to look at how the same goal can be achieved if your ASP.NET application is authenticating another way. We achieve this through use of the [ASP.NET Data Protection](https://docs.microsoft.com/en-us/aspnet/core/security/data-protection/configuration/overview) system. Andrew Lock has written [an excellent walkthrough on the topic](https://andrewlock.net/an-introduction-to-the-data-protection-system-in-asp-net-core/) and I encourage you to read it.\\n\\nWe\'re interested in the ASP.NET data-protection system because it encrypts and decrypts sensitive data including the authentication cookie. It\'s wonderful that the data protection does this, but at the same time it presents a problem. We would like to route traffic to _multiple_ instances of our application\u2026 So traffic could go to instance 1, instance 2 of our app etc.\\n\\n![traffic to app service](traffic-to-app-service.png)\\n\\nHow can we ensure the different instances of our app can read the authentication cookies regardless of the instance that produced them? How can we ensure that instance 1 can read cookies produced by instance 2 and vice versa? And for that matter, we\'d like all instances to be able to read cookies whether they were produced by an instance in a production or staging slot.\\n\\nWe\'re aiming to avoid the use of \\"sticky sessions\\" and ARRAffinity cookies. These ensure that traffic is continually routed to the same instance. Routing to the same instance explicitly prevents us from stopping routing traffic to an old instance and starting routing to a new one.\\n\\nWith the data protection activated and multiple instances of your app service you immediately face the issue that different instances of the app will be unable to read cookies they did not create. This is the default behaviour of data protection. [To quote the docs:](https://docs.microsoft.com/en-us/aspnet/core/host-and-deploy/web-farm?view=aspnetcore-5.0#data-protection)\\n\\n> Data Protection relies upon a set of cryptographic keys stored in a key ring. When the Data Protection system is initialized, it applies default settings that store the key ring locally. Under the default configuration, a unique key ring is stored on each node of the web farm. Consequently, each web farm node can\'t decrypt data that\'s encrypted by an app on any other node.\\n\\nThe problem here is the data protection keys (the key ring) is being stored locally on each instance. What are the implications of this? Well, For example, instance 2 doesn\'t have access to the keys instance 1 is using and so can\'t decrypt instance 1 cookies.\\n\\n## Sharing is caring\\n\\nWhat we need to do is move away from storing keys locally, and to storing it in a _shared_ place instead. We\'re going to store data protection keys in Azure Blob Storage and protect the keys with Azure Key Vault:\\n\\n![persist keys to azure blob](data-protection-zero-downtime.png)\\n\\nAll instances of the application can access the key ring and consequently sharing cookies is enabled. [As the documentation attests](https://docs.microsoft.com/en-us/aspnet/core/security/data-protection/configuration/overview?view=aspnetcore-5.0#protectkeyswithazurekeyvault), enabling this is fairly simple. It amounts to adding the following packages to your ASP.NET app:\\n\\n- [`Azure.Extensions.AspNetCore.DataProtection.Blobs`](https://www.nuget.org/packages/Azure.Extensions.AspNetCore.DataProtection.Blobs)\\n- [`Azure.Extensions.AspNetCore.DataProtection.Keys`](https://www.nuget.org/packages/Azure.Extensions.AspNetCore.DataProtection.Keys)\\n\\nAnd adding the following to the `ConfigureServices` in your ASP.NET app:\\n\\n```cs\\nservices.AddDataProtection().SetApplicationName(\\"OurWebApp\\")\\n        // azure credentials require storage blob contributor role permissions\\n        // eg https://my-storage-account.blob.core.windows.net/keys/key\\n        .PersistKeysToAzureBlobStorage(new Uri($\\"https://{Configuration[\\"StorageAccountName\\"]}.blob.core.windows.net/keys/key\\"), new DefaultAzureCredential())\\n\\n        // azure credentials require key vault crypto role permissions\\n        // eg https://my-key-vault.vault.azure.net/keys/dataprotection\\n        .ProtectKeysWithAzureKeyVault(new Uri($\\"https://{Configuration[\\"KeyVaultName\\"]}.vault.azure.net/keys/dataprotection\\"), new DefaultAzureCredential());\\n```\\n\\nIn the above example you can see we\'re passing the name of our Storage account and Key Vault via configuration.\\n\\nThere\'s one more crucial piece of the puzzle here; and it\'s role assignments, better known as permissions. Your App Service needs to be able to read and write to Azure Key Vault and the Azure Blob Storage. The permissions of [Storage Blob Data Contributor](https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#storage-blob-data-contributor) and [Key Vault Crypto Officer](https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#key-vault-crypto-officer-preview) are sufficient to enable this. (If you\'d like to see what configuring that looks like via ARM templates then [check out this post](../2021-02-08-arm-templates-security-role-assignments/index.md).)\\n\\nWith this in place we\'re able to route traffic to any instance of our application, secure in the knowledge that it will be able to read the cookies. Furthermore, we\'ve enabled zero downtime releases as a direct consequence."},{"id":"easy-auth-tokens-survive-releases-on-linux-azure-app-service","metadata":{"permalink":"/easy-auth-tokens-survive-releases-on-linux-azure-app-service","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-02-16-easy-auth-tokens-survive-releases-on-linux-azure-app-service/index.md","source":"@site/blog/2021-02-16-easy-auth-tokens-survive-releases-on-linux-azure-app-service/index.md","title":"Making Easy Auth tokens survive releases on Linux Azure App Service","description":"To prevent authentication issues during restarts or deployments, Microsoft is recommending Blob Storage for Token Cache to store and fetch tokens.","date":"2021-02-16T00:00:00.000Z","tags":[{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."},{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."},{"inline":false,"label":"Easy Auth","permalink":"/tags/easy-auth","description":"The Azure Easy Auth feature used for authentication and authorization."},{"inline":false,"label":"Authentication and Authorization","permalink":"/tags/auth","description":"Matters related to identity and permissioning"}],"readingTime":3.915,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"easy-auth-tokens-survive-releases-on-linux-azure-app-service","title":"Making Easy Auth tokens survive releases on Linux Azure App Service","authors":"johnnyreilly","image":"./easy-auth-zero-downtime-deployment.webp","tags":["azure","asp.net","easy auth","auth"],"hide_table_of_contents":false,"description":"To prevent authentication issues during restarts or deployments, Microsoft is recommending Blob Storage for Token Cache to store and fetch tokens."},"unlisted":false,"prevItem":{"title":"Goodbye Client Affinity, Hello Data Protection with Azure","permalink":"/goodbye-client-affinity-hello-data-protection-with-azure"},"nextItem":{"title":"Azure App Service, Health checks and zero downtime deployments","permalink":"/azure-app-service-health-checks-and-zero-downtime-deployments"}},"content":"I [wrote recently about zero downtime deployments on Azure App Service](../2021-02-11-azure-app-service-health-checks-and-zero-downtime-deployments/index.md). Many applications require authentication, and ours is no exception. In our case we\'re using Azure Active Directory facilitated by [\\"Easy Auth\\"](https://docs.microsoft.com/en-us/azure/app-service/overview-authentication-authorization) which provides authentication to our App Service.\\n\\n\x3c!--truncate--\x3e\\n\\nOur app uses a Linux App Service. It\'s worth knowing that Linux App Services run as a Docker container. As a consequence, Easy Auth works in a slightly different way; effectively as a middleware. [To quote the docs on Easy Auth](https://docs.microsoft.com/en-us/azure/app-service/overview-authentication-authorization#on-containers):\\n\\n> This module handles several things for your app:\\n>\\n> - Authenticates users with the specified provider\\n> - Validates, stores, and refreshes tokens\\n> - Manages the authenticated session\\n> - Injects identity information into request headers The module runs separately from your application code and is configured using app settings. No SDKs, specific languages, or changes to your application code are required.\\n>\\n> The authentication and authorization module runs in a separate container, isolated from your application code. Using what\'s known as the [Ambassador](https://docs.microsoft.com/en-us/azure/architecture/patterns/ambassador) pattern, it interacts with the incoming traffic to perform similar functionality as on Windows.\\n\\nHowever, [Microsoft have acknowledged there is a potential bug in Easy Auth support at present](https://social.msdn.microsoft.com/Forums/en-US/dde551b2-c86d-474b-b0a6-cc66163785a1/restarting-azure-app-service-on-linux-with-azure-active-directory-authentication-resets-authme#b59951ab-623a-4442-a221-80c157387bbe). When the app service is restarted, the stored tokens are removed, and **authentication begins to fail**. As you might well imagine, authentication similarly starts to fail when a new app service is introduced - as is the case during deployment.\\n\\nThis is really significant. You may well have \\"zero downtime deployment\\", but it doesn\'t amount to a hill of beans if the moment you\'ve deployed your users find they\'re effectively logged out. [The advice from Microsoft](https://social.msdn.microsoft.com/Forums/en-US/dde551b2-c86d-474b-b0a6-cc66163785a1/restarting-azure-app-service-on-linux-with-azure-active-directory-authentication-resets-authme#b59951ab-623a-4442-a221-80c157387bbe) is to use [Blob Storage for Token Cache](https://docs.microsoft.com/en-gb/archive/blogs/jpsanders/azure-app-service-authentication-using-a-blob-storage-for-token-cache):\\n\\n[Chris Gillum](https://twitter.com/cgillum) said in a [blog on the topic](https://cgillum.tech/2016/03/07/app-service-token-store/):\\n\\n> you can provision an Azure Blob Storage container and configure your web app with a SaS URL (with read/write/list access) pointing to that blob container. This SaS URL can then be saved to the `WEBSITE_AUTH_TOKEN_CONTAINER_SASURL` app setting. When this app setting is present, all tokens will be stored in and fetched from the specified blob container.\\n\\nTo turn that into something visual, what\'s suggested is this:\\n\\n![diagram of Easy Auth with blog storage](easy-auth-zero-downtime-deployment.webp)\\n\\n## SaS-sy ARM Templates\\n\\nI have the good fortune to work with some very talented people. One of them, [John McCormick](https://github.com/jmccor99) turned his hand to putting this proposed solution into `azure-pipelines.yml` and ARM template-land. First of all, let\'s look at our `azure-pipelines.yml`. We add the following, prior to our deployment job:\\n\\n```yml\\n- job: SASGen\\n        displayName: Generate SAS Token\\n\\n        steps:\\n          - task: AzurePowerShell@4\\n            name: ObtainSasTokenTask\\n            inputs:\\n              azureSubscription: $(serviceConnection)\\n              ScriptType: inlineScript\\n              Inline: |\\n                $startTime = Get-Date\\n                $expiryTime = $startTime.AddDays(90)\\n                $storageAcc = Get-AzStorageAccount -ResourceGroupName $(azureResourceGroup) -Name $(storageAccountName)\\n                $ctx = $storageAcc.Context\\n                $sas = New-AzStorageContainerSASToken -Context $ctx -Name \\"tokens\\" -Permission \\"rwl\\" -Protocol HttpsOnly -StartTime $startTime -ExpiryTime $expiryTime -FullUri\\n                Write-Host \\"##vso[task.setvariable variable=sasToken;issecret=true;isOutput=true]$sas\\"\\n              azurePowerShellVersion: \'LatestVersion\'\\n\\n      - job: DeployAppARMTemplates\\n        variables:\\n          sasToken: $[dependencies.SASGen.outputs[\'ObtainSasTokenTask.sasToken\'] ]\\n        displayName: Deploy App ARM Templates\\n        dependsOn:\\n        - SASGen\\n\\n        steps:\\n          - task: AzureResourceManagerTemplateDeployment@3\\n            displayName: Deploy app-service ARM Template\\n            inputs:\\n              deploymentScope: Resource Group\\n              azureResourceManagerConnection: $(serviceConnection)\\n              subscriptionId: $(subscriptionId)\\n              action: Create Or Update Resource Group\\n              resourceGroupName: $(azureResourceGroup)\\n              location: $(location)\\n              templateLocation: Linked artifact\\n              csmFile: \'infra/app-service/azuredeploy.json\'\\n              csmParametersFile: \'infra/azuredeploy.parameters.json\'\\n              overrideParameters: >-\\n                -sasUrl $(sasToken)\\n              deploymentMode: Incremental\\n```\\n\\nThere\'s two notable things happening above:\\n\\n1. In the `SASGen` job, a PowerShell script runs that [generates a SaS token URL](https://docs.microsoft.com/en-us/powershell/module/az.storage/new-azstoragecontainersastoken?view=azps-5.5.0) with read, write and list permissions that will last for 90 days. (Incidentally, there is a way to do this via [ARM templates, and without PowerShell](https://stackoverflow.com/a/56127006/761388) \\\\- but alas it didn\'t seem to work when we experimented with it.)\\n2. The generated (secret) token URL (`sasUrl`) is passed as a parameter to our App Service ARM template. The ARM template sets an appsetting for the app service:\\n\\n```json\\n{\\n    \\"apiVersion\\": \\"2020-09-01\\",\\n    \\"name\\": \\"appsettings\\",\\n    \\"type\\": \\"config\\",\\n    \\"properties\\": {\\n        \\"WEBSITE_AUTH_TOKEN_CONTAINER_SASURL\\": \\"[parameters(\'sasUrl\')]\\"\\n    }\\n},\\n```\\n\\nIf you google `WEBSITE_AUTH_TOKEN_CONTAINER_SASURL` you will not find a geat deal. Documentation is short. What you will find is [Jeff Sanders excellent blog on the topic](http://jsandersblog.azurewebsites.net/2017/08/10/azure-app-service-authentication-using-a-blob-storage-for-token-cache/). It is, in terms of content, it has some commonality with this post; except in Jeff\'s example he\'s manually implementing the workaround in the Azure Portal.\\n\\n## What\'s actually happening?\\n\\nWith this in place, every time someone logs into your app a JSON token is written to the storage like so:\\n\\n![token in storage account](token.webp)\\n\\nIf you take the trouble to look inside you\'ll find something like this tucked away:\\n\\n```json\\n{\\n  \\"encrypted\\": true,\\n  \\"tokens\\": {\\n    \\"aad\\": \\"herewith_a_very_very_long_encrypted_token\\"\\n  },\\n  \\"version\\": 1\\n}\\n```\\n\\nWith this in place, you can safely restart your app service and / or deploy a new one, safe in the knowledge that the tokens will live on in the storage account, and that consequently you will not be unauthenticating users."},{"id":"azure-app-service-health-checks-and-zero-downtime-deployments","metadata":{"permalink":"/azure-app-service-health-checks-and-zero-downtime-deployments","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-02-11-azure-app-service-health-checks-and-zero-downtime-deployments/index.md","source":"@site/blog/2021-02-11-azure-app-service-health-checks-and-zero-downtime-deployments/index.md","title":"Azure App Service, Health checks and zero downtime deployments","description":"Azure App Service enables zero downtime deployments using health checks and deployment slots. Automated swapping slots ensure constant service.","date":"2021-02-11T00:00:00.000Z","tags":[{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."}],"readingTime":7.49,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-app-service-health-checks-and-zero-downtime-deployments","title":"Azure App Service, Health checks and zero downtime deployments","authors":"johnnyreilly","tags":["azure"],"hide_table_of_contents":false,"description":"Azure App Service enables zero downtime deployments using health checks and deployment slots. Automated swapping slots ensure constant service."},"unlisted":false,"prevItem":{"title":"Making Easy Auth tokens survive releases on Linux Azure App Service","permalink":"/easy-auth-tokens-survive-releases-on-linux-azure-app-service"},"nextItem":{"title":"Azure RBAC: role assignments and ARM templates","permalink":"/arm-templates-security-role-assignments"}},"content":"I\'ve been working recently on zero downtime deployments using Azure App Service. They\'re facilitated by a combination of [Health checks](https://docs.microsoft.com/en-us/azure/app-service/monitor-instances-health-check) and [deployment slots](https://docs.microsoft.com/en-us/azure/app-service/deploy-staging-slots). This post will talk about why this is important and how it works.\\n\\n\x3c!--truncate--\x3e\\n\\n## Why zero downtime deployments?\\n\\nHistorically (and for many applications, currently) deployment results in downtime. A period of time during the release where an application is not available to users whilst the new version is deployed. There are a number of downsides to releases with downtime:\\n\\n1. Your users cannot use your application. This will frustrate them and make them sad.\\n2. Because you\'re a kind person and you want your users to be happy, you\'ll optimise to make their lives better. You\'ll release when the fewest users are accessing your application. It will likely mean you\'ll end up working late, early or at weekends.\\n3. Again because you want to reduce impact on users, you\'ll release less often. This means that every release will bring with it a greater collection of changes. This is turn will often result in a large degree of focus on manually testing each release, to reduce the likelihood of bugs ending up in users hands. This is a noble aim, but it drags the teams focus away from shipping.\\n\\nPut simply: downtime in releases impacts customer happiness and leads to reduced pace for teams. It\'s a vicious circle.\\n\\nBut if we turn it around, what does it look like if releases have _no_ downtime at all?\\n\\n1. Your users can always use your application. This will please them.\\n2. Your team is now safe to release at any time, day or night. They will likely release more often as a consequence.\\n3. If your team has sufficient automated testing in place, they\'re now in a position where they can move to [Continuous Deployment](https://www.atlassian.com/continuous-delivery/principles/continuous-integration-vs-delivery-vs-deployment).\\n4. Releases become boring. This is good. They \\"just work\u2122\uFE0F\\" and so the team can focus instead on building the cool features that are going to make users lives even better.\\n\\n## Manual zero downtime releases with App Services\\n\\nApp Services have the ability to scale out. To [quote the docs](https://azure.microsoft.com/en-us/blog/scaling-up-and-scaling-out-in-windows-azure-web-sites/):\\n\\n> A scale out operation is the equivalent of creating multiple copies of your web site and adding a load balancer to distribute the demand between them. When you scale out ... there is no need to configure load balancing separately since this is already provided by the platform.\\n\\nAs you can see, scaling out works by having multiple instances of your app. Deployment slots are exactly this, but with an extra twist. If you add a deployment slot to your App Service, then you **no longer deploy to production**. Instead you deploy to your staging slot. Your staging slot is accessible in the same way your production slot is accessible. So whilst your users may go to [https://my-glorious-app.io](https://my-glorious-app.io), your staging slot may live at [https://my-glorious-app-stage.azurewebsites.net](https://my-glorious-app-stage.azurewebsites.net) instead. Because this is accessible, this is testable. You are in a position to test the deployed application before making it generally available.\\n\\n![diagram of network traffic going to various App Service Deployment Slots](app-service-with-slots.png)\\n\\nOnce you\'re happy that everything looks good, you can \\"swap slots\\". What this means, is the version of the app living in the staging slot, gets moved into the production slot. So that which lived at [https://my-glorious-app-stage.azurewebsites.net](https://my-glorious-app-stage.azurewebsites.net) moves to [https://my-glorious-app.io](https://my-glorious-app.io). For a more details on what that involves [read this](https://docs.microsoft.com/en-us/azure/app-service/deploy-staging-slots#what-happens-during-a-swap). The significant take home is this: there is no downtime. Traffic stops being routed to the old instance and starts being routed to the new one. It\'s as simple as that.\\n\\nI should mention at this point that there\'s a [number of zero downtime strategies out there](https://opensource.com/article/17/5/colorful-deployments) and slots can help support a number of these. This includes canary deployments, where a subset of traffic is routed to the new version prior to it being opened out more widely. In our case, we\'re looking at rolling deployments, where we replace the currently running instances of our application with the new ones; but it\'s worth being aware that there are other strategies that slots can facilitate.\\n\\nSo what does it look like when slots swap? Well, to test that out, we swapped slots on our two App Service instances. We repeatedly CURLed our apps [`api/build`](../2021-01-29-surfacing-azure-pipelines-build-info-in-an-aspnet-react-app/index.md) endpoint that exposes the build information; to get visibility around which version of our app we were routing traffic to. This is what we saw:\\n\\n```\\nThu Jan 21 11:51:51 GMT 2021\\n{\\"buildNumber\\":\\"20210121.5\\",\\"buildId\\":\\"17992\\",\\"commitHash\\":\\"c2122919df54bfa6a0d20bceb9f06890f822b26e\\"}\\nThu Jan 21 11:51:54 GMT 2021\\n{\\"buildNumber\\":\\"20210121.6\\",\\"buildId\\":\\"18015\\",\\"commitHash\\":\\"062ac1488fcf1737fe1dbab0d05c095786218f30\\"}\\nThu Jan 21 11:51:57 GMT 2021\\n{\\"buildNumber\\":\\"20210121.5\\",\\"buildId\\":\\"17992\\",\\"commitHash\\":\\"c2122919df54bfa6a0d20bceb9f06890f822b26e\\"}\\nThu Jan 21 11:52:00 GMT 2021\\n{\\"buildNumber\\":\\"20210121.6\\",\\"buildId\\":\\"18015\\",\\"commitHash\\":\\"062ac1488fcf1737fe1dbab0d05c095786218f30\\"}\\nThu Jan 21 11:52:03 GMT 2021\\n{\\"buildNumber\\":\\"20210121.6\\",\\"buildId\\":\\"18015\\",\\"commitHash\\":\\"062ac1488fcf1737fe1dbab0d05c095786218f30\\"}\\nThu Jan 21 11:52:05 GMT 2021\\n{\\"buildNumber\\":\\"20210121.6\\",\\"buildId\\":\\"18015\\",\\"commitHash\\":\\"062ac1488fcf1737fe1dbab0d05c095786218f30\\"}\\nThu Jan 21 11:52:08 GMT 2021\\n{\\"buildNumber\\":\\"20210121.5\\",\\"buildId\\":\\"17992\\",\\"commitHash\\":\\"c2122919df54bfa6a0d20bceb9f06890f822b26e\\"}\\nThu Jan 21 11:52:10 GMT 2021\\n{\\"buildNumber\\":\\"20210121.6\\",\\"buildId\\":\\"18015\\",\\"commitHash\\":\\"062ac1488fcf1737fe1dbab0d05c095786218f30\\"}\\nThu Jan 21 11:52:12 GMT 2021\\n{\\"buildNumber\\":\\"20210121.5\\",\\"buildId\\":\\"17992\\",\\"commitHash\\":\\"c2122919df54bfa6a0d20bceb9f06890f822b26e\\"}\\nThu Jan 21 11:52:15 GMT 2021\\n{\\"buildNumber\\":\\"20210121.6\\",\\"buildId\\":\\"18015\\",\\"commitHash\\":\\"062ac1488fcf1737fe1dbab0d05c095786218f30\\"}\\nThu Jan 21 11:52:17 GMT 2021\\n{\\"buildNumber\\":\\"20210121.6\\",\\"buildId\\":\\"18015\\",\\"commitHash\\":\\"062ac1488fcf1737fe1dbab0d05c095786218f30\\"}\\n```\\n\\nThe first new version of our application showed up in a production slot at 11:51:54, and the last old version showed up at 11:52:12. So it took a total of 15 seconds to complete the transition from hitting only instances of the old application to hitting only instances of the new application. During that 15 seconds either old or new versions of the application would be serving traffic. Significantly, there was always a version of the application returning responses.\\n\\nThis is _very_ exciting! We have zero downtime deployments!\\n\\n## Rollbacks for bonus points\\n\\nWe now have the new version of the app (`buildNumber: 20210121.6`) in the production slot, and the old version of the app (`buildNumber: 20210121.5`) in the staging slot.\\n\\nSlots have a tremendous rollback story. If it emerges that there was some uncaught issue in your release and you\'d like to revert to the previous version, you can! Just as we swapped just now to move `buildNumber: 20210121.6` from the staging slot to the production slot and `buildNumber: 20210121.5` the other way, we can swap right back and revert our release like so:\\n\\n![diagram of network traffic going to various App Service Deployment Slots exposing build number](app-service-with-slots-and-build-number.png)\\n\\nOnce again users going to [https://my-glorious-app.io](https://my-glorious-app.io) are hitting `buildNumber: 20210121.5`.\\n\\nThis is also _very_ exciting! We have zero downtime deployments _and_ rollbacks!\\n\\n## Automated zero downtime releases with Health checks\\n\\nThe final piece of the puzzle here automation. You\'re a sophisticated team, you\'ve put a great deal of energy into automating your tests. You don\'t want your release process to be manual for this very reason; you trust your test coverage. You want to move to Continuous Deployment.\\n\\nFortunately, automating swapping slots is a breeze with `azure-pipelines.yml`. Consider the following:\\n\\n```yml\\n- job: DeployApp\\n        displayName: Deploy app\\n        dependsOn:\\n        - DeployARMTemplates\\n\\n        steps:\\n        - download: current\\n          artifact: webapp\\n\\n        - task: AzureWebApp@1\\n          displayName: \'Deploy Web Application\'\\n          inputs:\\n            azureSubscription: $(serviceConnection)\\n            resourceGroupName: $(azureResourceGroup)\\n            appName: $(appServiceName)\\n            package: $(Pipeline.Workspace)/webapp/**/*.zip\\n            slotName: stage\\n            deployToSlotOrASE: true\\n            deploymentMethod: auto\\n\\n      - job: SwapSlots\\n        displayName: Swap Slots\\n        dependsOn:\\n        - DeployApp\\n\\n        steps:\\n          - task: AzureAppServiceManage@0\\n            displayName: Swap Slots\\n            inputs:\\n              action: \'Swap Slots\'\\n              azureSubscription: $(serviceConnection)\\n              resourceGroupName: $(azureResourceGroup)\\n              webAppName: $(appServiceName)\\n              SourceSlot: \'stage\'\\n```\\n\\nThe first job here, deploys our previously built `webapp` to the `stage` slot. The second job swaps the slot.\\n\\nWhen I first considered this, the question rattling around in the back of my mind was this: how does App Service know when it\'s safe to swap? What if we swap before our app has fully woken up and started serving responses?\\n\\nIt so happens that using [Health checks, App Service caters for this beautifully](https://docs.microsoft.com/en-us/azure/app-service/monitor-instances-health-check). A health check endpoint is a URL in your application which, when hit, checks the dependencies of your application. \\"Is the database accessible?\\" \\"Are the APIs I depend upon accessible?\\" The diagram from the docs expresses it very well:\\n\\n![diagram of traffic hitting the health check endpoint](health-check-failure-diagram.webp)\\n\\nThis approach is very similar to [liveness, readiness and startup probes in Kubernetes](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/). To make use of Health checks, in our ARM template for our App Service we have configured a `healthCheckPath`:\\n\\n```json\\n\\"siteConfig\\": {\\n    \\"linuxFxVersion\\": \\"[parameters(\'linuxFxVersion\')]\\",\\n    \\"alwaysOn\\": true,\\n    \\"http20Enabled\\": true,\\n    \\"minTlsVersion\\": \\"1.2\\",\\n    \\"healthCheckPath\\": \\"/api/health\\",\\n    //...\\n}\\n```\\n\\nThis tells App Service where to look to check the health. The health check endpoint itself is provided by the `MapHealthChecks` in our `Startup.cs` of our .NET application:\\n\\n```cs\\napp.UseEndpoints(endpoints => {\\n    endpoints.MapControllerRoute(\\n        name: \\"default\\",\\n        pattern: \\"{controller}/{action=Index}/{id?}\\");\\n\\n    endpoints.MapHealthChecks(\\"/api/health\\");\\n});\\n```\\n\\nYou read a full list of all the ways App Service uses Health checks [here](https://docs.microsoft.com/en-us/azure/app-service/monitor-instances-health-check#what-app-service-does-with-health-checks). Pertinent for zero downtime deployments is this:\\n\\n> when scaling up or out, App Service pings the Health check path to ensure new instances are ready.\\n\\nThis is the magic sauce. App Service doesn\'t route traffic to an instance until it\'s given the thumbs up that it\'s ready in the form of passing health checks. This is excellent; it is this that makes automated zero downtime releases a reality.\\n\\nProps to the various Azure teams that have made this possible; I\'m very impressed by the way in which the Health checks and slots can be combined together to support some tremendous use cases."},{"id":"arm-templates-security-role-assignments","metadata":{"permalink":"/arm-templates-security-role-assignments","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-02-08-arm-templates-security-role-assignments/index.md","source":"@site/blog/2021-02-08-arm-templates-security-role-assignments/index.md","title":"Azure RBAC: role assignments and ARM templates","description":"ARM templates can help define Azure Role-Based Access Control. By creating role assignments, users can grant Managed Identities access to resources.","date":"2021-02-08T00:00:00.000Z","tags":[{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."}],"readingTime":6.02,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"arm-templates-security-role-assignments","title":"Azure RBAC: role assignments and ARM templates","authors":"johnnyreilly","image":"./with-great-power-comes-great-responsibility.webp","tags":["azure"],"hide_table_of_contents":false,"description":"ARM templates can help define Azure Role-Based Access Control. By creating role assignments, users can grant Managed Identities access to resources."},"unlisted":false,"prevItem":{"title":"Azure App Service, Health checks and zero downtime deployments","permalink":"/azure-app-service-health-checks-and-zero-downtime-deployments"},"nextItem":{"title":"ASP.NET, Serilog and Application Insights","permalink":"/aspnet-serilog-and-application-insights"}},"content":"This post is about Azure\'s role assignments and ARM templates. Role assignments can be thought of as \\"permissions for Azure\\".\\n\\n\x3c!--truncate--\x3e\\n\\nIf you\'re deploying to Azure, there\'s a good chance you\'re using [ARM templates](https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/overview) to do so. Once you\'ve got past \\"Hello World\\", you\'ll probably find yourself in a situation when you\'re deploying multiple types of resource to make your solution. For instance, you may be deploying an [App Service](https://docs.microsoft.com/en-us/azure/app-service/quickstart-arm-template?pivots=platform-linux#review-the-template) alongside [Key Vault](https://docs.microsoft.com/en-us/azure/templates/microsoft.keyvault/vaults) and [Storage](https://docs.microsoft.com/en-us/azure/templates/microsoft.storage/storageaccounts).\\n\\nOne of the hardest things when it comes to deploying software and having it work, is permissions. Without adequate permissions configured, the most beautiful code can do _nothing_. Incidentally, this is a good thing. We\'re deploying to the web; many people are there, not all good. As a different kind of web-head once said:\\n\\n![Spider-man saying with great power, comes great responsibility](with-great-power-comes-great-responsibility.webp)\\n\\nAzure has great power and [suggests you use it wisely](https://docs.microsoft.com/en-us/azure/security/fundamentals/identity-management-best-practices#use-role-based-access-control).\\n\\n> Access management for cloud resources is critical for any organization that uses the cloud. [Azure role-based access control (Azure RBAC)](https://docs.microsoft.com/en-us/azure/role-based-access-control/overview) helps you manage who has access to Azure resources, what they can do with those resources, and what areas they have access to.\\n>\\n> Designating groups or individual roles responsible for specific functions in Azure helps avoid confusion that can lead to human and automation errors that create security risks. Restricting access based on the [need to know](https://en.wikipedia.org/wiki/Need_to_know) and [least privilege](https://en.wikipedia.org/wiki/Principle_of_least_privilege) security principles is imperative for organizations that want to enforce security policies for data access.\\n\\nThis is good advice. With that in mind, how can we ensure that the different resources we\'re deploying to Azure can talk to one another?\\n\\n## Role (up for your) assignments\\n\\nThe answer is roles. There\'s a number of roles that exist in Azure that can be assigned to users, groups, service principals and managed identities. In our own case we\'re using managed identity for our resources. What we can do is use [\\"role assignments\\"](https://docs.microsoft.com/en-us/azure/role-based-access-control/overview#how-azure-rbac-works) to give our managed identity access to given resources. [Arturo Lucatero](https://twitter.com/ArLucaID) gives a great short explanation of this:\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/Dzhm-garKBM\\" frameBorder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowFullScreen=\\"\\"></iframe>\\n\\nWhilst this explanation is delightfully simple, the actual implementation when it comes to ARM templates is a little more involved. Because now it\'s time to talk \\"magic\\" GUIDs. Consider the following truncated ARM template, which gives our managed identity (and hence our App Service which uses this identity) access to Key Vault and Storage:\\n\\n```json\\n{\\n  \\"$schema\\": \\"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\\",\\n  // ...\\n  \\"variables\\": {\\n    // ...\\n    \\"managedIdentity\\": \\"[concat(\'mi-\', parameters(\'applicationName\'), \'-\', parameters(\'environment\'), \'-\', \'001\')]\\",\\n    \\"appInsightsName\\": \\"[concat(\'appi-\', parameters(\'applicationName\'), \'-\', parameters(\'environment\'), \'-\', \'001\')]\\",\\n    \\"keyVaultName\\": \\"[concat(\'kv-\', parameters(\'applicationName\'), \'-\', parameters(\'environment\'), \'-\', \'001\')]\\",\\n    \\"storageAccountName\\": \\"[concat(\'st\', parameters(\'applicationName\'), parameters(\'environment\'), \'001\')]\\",\\n    \\"storageBlobDataContributor\\": \\"[subscriptionResourceId(\'Microsoft.Authorization/roleDefinitions\', \'ba92f5b4-2d11-453d-a403-e96b0029c9fe\')]\\",\\n    \\"keyVaultSecretsOfficer\\": \\"[subscriptionResourceId(\'Microsoft.Authorization/roleDefinitions\', \'b86a8fe4-44ce-4948-aee5-eccb2c155cd7\')]\\",\\n    \\"keyVaultCryptoOfficer\\": \\"[subscriptionResourceId(\'Microsoft.Authorization/roleDefinitions\', \'14b46e9e-c2b7-41b4-b07b-48a6ebf60603\')]\\",\\n    \\"uniqueRoleGuidKeyVaultSecretsOfficer\\": \\"[guid(resourceId(\'Microsoft.KeyVault/vaults\',  variables(\'keyVaultName\')), variables(\'keyVaultSecretsOfficer\'), resourceId(\'Microsoft.KeyVault/vaults\', variables(\'keyVaultName\')))]\\",\\n    \\"uniqueRoleGuidKeyVaultCryptoOfficer\\": \\"[guid(resourceId(\'Microsoft.KeyVault/vaults\',  variables(\'keyVaultName\')), variables(\'keyVaultCryptoOfficer\'), resourceId(\'Microsoft.KeyVault/vaults\', variables(\'keyVaultName\')))]\\",\\n    \\"uniqueRoleGuidStorageAccount\\": \\"[guid(resourceId(\'Microsoft.Storage/storageAccounts\',  variables(\'storageAccountName\')), variables(\'storageBlobDataContributor\'), resourceId(\'Microsoft.Storage/storageAccounts\', variables(\'storageAccountName\')))]\\"\\n  },\\n  \\"resources\\": [\\n    {\\n      \\"type\\": \\"Microsoft.ManagedIdentity/userAssignedIdentities\\",\\n      \\"name\\": \\"[variables(\'managedIdentity\')]\\",\\n      \\"apiVersion\\": \\"2018-11-30\\",\\n      \\"location\\": \\"[parameters(\'location\')]\\"\\n    },\\n    // ...\\n    {\\n      \\"type\\": \\"Microsoft.Storage/storageAccounts/providers/roleAssignments\\",\\n      \\"apiVersion\\": \\"2020-04-01-preview\\",\\n      \\"name\\": \\"[concat(variables(\'storageAccountName\'), \'/Microsoft.Authorization/\', variables(\'uniqueRoleGuidStorageAccount\'))]\\",\\n      \\"dependsOn\\": [\\n        \\"[resourceId(\'Microsoft.ManagedIdentity/userAssignedIdentities\', variables(\'managedIdentity\'))]\\"\\n      ],\\n      \\"properties\\": {\\n        \\"roleDefinitionId\\": \\"[variables(\'storageBlobDataContributor\')]\\",\\n        \\"principalId\\": \\"[reference(resourceId(\'Microsoft.ManagedIdentity/userAssignedIdentities/\', variables(\'managedIdentity\')), \'2018-11-30\').principalId]\\",\\n        \\"scope\\": \\"[resourceId(\'Microsoft.Storage/storageAccounts\', variables(\'storageAccountName\'))]\\",\\n        \\"principalType\\": \\"ServicePrincipal\\"\\n      }\\n    },\\n    {\\n      \\"type\\": \\"Microsoft.KeyVault/vaults/providers/roleAssignments\\",\\n      \\"apiVersion\\": \\"2018-01-01-preview\\",\\n      \\"name\\": \\"[concat(variables(\'keyVaultName\'), \'/Microsoft.Authorization/\', variables(\'uniqueRoleGuidKeyVaultSecretsOfficer\'))]\\",\\n      \\"dependsOn\\": [\\n        \\"[resourceId(\'Microsoft.ManagedIdentity/userAssignedIdentities\', variables(\'managedIdentity\'))]\\"\\n      ],\\n      \\"properties\\": {\\n        \\"roleDefinitionId\\": \\"[variables(\'keyVaultSecretsOfficer\')]\\",\\n        \\"principalId\\": \\"[reference(resourceId(\'Microsoft.ManagedIdentity/userAssignedIdentities/\', variables(\'managedIdentity\')), \'2018-11-30\').principalId]\\",\\n        \\"scope\\": \\"[resourceId(\'Microsoft.KeyVault/vaults\', variables(\'keyVaultName\'))]\\",\\n        \\"principalType\\": \\"ServicePrincipal\\"\\n      }\\n    },\\n    {\\n      \\"type\\": \\"Microsoft.KeyVault/vaults/providers/roleAssignments\\",\\n      \\"apiVersion\\": \\"2018-01-01-preview\\",\\n      \\"name\\": \\"[concat(variables(\'keyVaultName\'), \'/Microsoft.Authorization/\', variables(\'uniqueRoleGuidKeyVaultCryptoOfficer\'))]\\",\\n      \\"dependsOn\\": [\\n        \\"[resourceId(\'Microsoft.ManagedIdentity/userAssignedIdentities\', variables(\'managedIdentity\'))]\\"\\n      ],\\n      \\"properties\\": {\\n        \\"roleDefinitionId\\": \\"[variables(\'keyVaultCryptoOfficer\')]\\",\\n        \\"principalId\\": \\"[reference(resourceId(\'Microsoft.ManagedIdentity/userAssignedIdentities/\', variables(\'managedIdentity\')), \'2018-11-30\').principalId]\\",\\n        \\"scope\\": \\"[resourceId(\'Microsoft.KeyVault/vaults\', variables(\'keyVaultName\'))]\\",\\n        \\"principalType\\": \\"ServicePrincipal\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nLet\'s take a look at these three variables:\\n\\n```json\\n\\"storageBlobDataContributor\\": \\"[subscriptionResourceId(\'Microsoft.Authorization/roleDefinitions\', \'ba92f5b4-2d11-453d-a403-e96b0029c9fe\')]\\",\\n\\"keyVaultSecretsOfficer\\": \\"[subscriptionResourceId(\'Microsoft.Authorization/roleDefinitions\', \'b86a8fe4-44ce-4948-aee5-eccb2c155cd7\')]\\",\\n\\"keyVaultCryptoOfficer\\": \\"[subscriptionResourceId(\'Microsoft.Authorization/roleDefinitions\', \'14b46e9e-c2b7-41b4-b07b-48a6ebf60603\')]\\",\\n```\\n\\nThe three variables above contain the subscription resource ids for the roles [Storage Blob Data Contributor](https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#storage-blob-data-contributor), [Key Vault Secrets Officer](https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#key-vault-secrets-officer-preview) and [Key Vault Crypto Officer](https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#key-vault-crypto-officer-preview). The first question on your mind is likely: \\"what is `ba92f5b4-2d11-453d-a403-e96b0029c9fe` and where does it come from?\\" Great question! Well, each of these GUIDs represents a built-in role in Azure RBAC. The `ba92f5b4-2d11-453d-a403-e96b0029c9fe` represents the Storage Blob Data Contributor role.\\n\\nHow can I look these up? Well, there\'s two ways; [there\'s an article which documents them here](https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles) or you could crack open the [Cloud Shell](https://azure.microsoft.com/en-gb/features/cloud-shell/) and look up a role by GUID like so:\\n\\n```ps\\nGet-AzRoleDefinition | ? {$_.id -eq \\"ba92f5b4-2d11-453d-a403-e96b0029c9fe\\" }\\n\\nName             : Storage Blob Data Contributor\\nId               : ba92f5b4-2d11-453d-a403-e96b0029c9fe\\nIsCustom         : False\\nDescription      : Allows for read, write and delete access to Azure Storage blob containers and data\\nActions          : {Microsoft.Storage/storageAccounts/blobServices/containers/delete, Microsoft.Storage/storageAccounts/blobServices/containers/read,\\n                   Microsoft.Storage/storageAccounts/blobServices/containers/write, Microsoft.Storage/storageAccounts/blobServices/generateUserDelegationKey/action}\\nNotActions       : {}\\nDataActions      : {Microsoft.Storage/storageAccounts/blobServices/containers/blobs/delete, Microsoft.Storage/storageAccounts/blobServices/containers/blobs/read,\\n                   Microsoft.Storage/storageAccounts/blobServices/containers/blobs/write, Microsoft.Storage/storageAccounts/blobServices/containers/blobs/move/action\u2026}\\nNotDataActions   : {}\\nAssignableScopes : {/}\\n```\\n\\nOr by name like so:\\n\\n```ps\\nGet-AzRoleDefinition | ? {$_.name -like \\"*Crypto Officer*\\" }\\n\\nName             : Key Vault Crypto Officer\\nId               : 14b46e9e-c2b7-41b4-b07b-48a6ebf60603\\nIsCustom         : False\\nDescription      : Perform any action on the keys of a key vault, except manage permissions. Only works for key vaults that use the \'Azure role-based access control\' permission model.\\nActions          : {Microsoft.Authorization/*/read, Microsoft.Insights/alertRules/*, Microsoft.Resources/deployments/*, Microsoft.Resources/subscriptions/resourceGroups/read\u2026}\\nNotActions       : {}\\nDataActions      : {Microsoft.KeyVault/vaults/keys/*}\\nNotDataActions   : {}\\nAssignableScopes : {/}\\n```\\n\\nAs you can see, the `Actions` section of the output above (and in even more detail on the [linked article](https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles)) provides information about what the different roles can do. So if you\'re looking to enable one Azure resource to talk to another, you should be able to refer to these to identify a role that you might want to use.\\n\\n## Creating a role assignment\\n\\nSo now we understand how you identify the roles in question, let\'s take the final leap and look at assigning those roles to our managed identity. For each role assignment, you\'ll need a `roleAssignments` resource defined that looks like this:\\n\\n```json\\n{\\n  \\"type\\": \\"Microsoft.KeyVault/vaults/providers/roleAssignments\\",\\n  \\"apiVersion\\": \\"2018-01-01-preview\\",\\n  \\"name\\": \\"[concat(variables(\'keyVaultName\'), \'/Microsoft.Authorization/\', variables(\'uniqueRoleGuidKeyVaultCryptoOfficer\'))]\\",\\n  \\"dependsOn\\": [\\n    \\"[resourceId(\'Microsoft.ManagedIdentity/userAssignedIdentities\', variables(\'managedIdentity\'))]\\"\\n  ],\\n  \\"properties\\": {\\n    \\"roleDefinitionId\\": \\"[variables(\'keyVaultCryptoOfficer\')]\\",\\n    \\"principalId\\": \\"[reference(resourceId(\'Microsoft.ManagedIdentity/userAssignedIdentities/\', variables(\'managedIdentity\')), \'2018-11-30\').principalId]\\",\\n    \\"scope\\": \\"[resourceId(\'Microsoft.KeyVault/vaults\', variables(\'keyVaultName\'))]\\",\\n    \\"principalType\\": \\"ServicePrincipal\\"\\n  }\\n}\\n```\\n\\nLet\'s go through the above, significant property by significant property (it\'s also worth checking the official reference [here](https://docs.microsoft.com/en-us/azure/templates/microsoft.authorization/roleassignments)):\\n\\n- `type` \\\\- the type of role assignment we want to create, for a key vault it\'s `\\"Microsoft.KeyVault/vaults/providers/roleAssignments\\"`, for storage it\'s `\\"Microsoft.Storage/storageAccounts/providers/roleAssignments\\"`. The pattern is that it\'s the resource type, followed by `\\"/providers/roleAssignments\\"`.\\n- `dependsOn` \\\\- before we can create a role assignment, we need the service principal we desire to permission (in our case a managed identity) to exist\\n- `properties.roleDefinitionId` \\\\- the role that we\'re assigning, provided as an id. So for this example it\'s the `keyVaultCryptoOfficer` variable, which was earlier defined as `[subscriptionResourceId(\'Microsoft.Authorization/roleDefinitions\', \'ba92f5b4-2d11-453d-a403-e96b0029c9fe\')]`. (Note the use of the GUID)\\n- `properties.principalId` \\\\- the id of the principal we\'re adding permissions for. In our case this is a managed identity (a type of service principal).\\n- `properties.scope` \\\\- we\'re modifying another resource; our key vault isn\'t defined in this ARM template and we want to specify the resource we\'re granting permissions to.\\n- `properties.principalType` \\\\- the type of principal that we\'re creating an assignment for; in our this is `\\"ServicePrincipal\\"` \\\\- our managed identity.\\n\\nThere is an alternate approach that you can use where the `type` is `\\"Microsoft.Authorization/roleAssignments\\"`. Whilst this also works, it displayed errors in the [Azure tooling for VS Code](https://marketplace.visualstudio.com/items?itemName=msazurermtools.azurerm-vscode-tools). As such, we\'ve opted not to use that approach in our ARM templates.\\n\\nMany thanks to the awesome [John McCormick](https://github.com/jmccor99) who wrangled permissions with me until we bent Azure RBAC to our will."},{"id":"aspnet-serilog-and-application-insights","metadata":{"permalink":"/aspnet-serilog-and-application-insights","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-01-30-aspnet-serilog-and-application-insights/index.md","source":"@site/blog/2021-01-30-aspnet-serilog-and-application-insights/index.md","title":"ASP.NET, Serilog and Application Insights","description":"Learn how to integrate Serilog into Azures Application Insights for better diagnostic logging by following these steps and adding dependencies.","date":"2021-01-30T00:00:00.000Z","tags":[{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."},{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":3.875,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"aspnet-serilog-and-application-insights","title":"ASP.NET, Serilog and Application Insights","authors":"johnnyreilly","image":"./title-image.png","tags":["azure","asp.net"],"hide_table_of_contents":false,"description":"Learn how to integrate Serilog into Azures Application Insights for better diagnostic logging by following these steps and adding dependencies."},"unlisted":false,"prevItem":{"title":"Azure RBAC: role assignments and ARM templates","permalink":"/arm-templates-security-role-assignments"},"nextItem":{"title":"Azure Pipelines Build Info in an ASP.NET React app","permalink":"/surfacing-azure-pipelines-build-info-in-an-aspnet-react-app"}},"content":"If you\'re deploying an ASP.NET application to Azure App Services / Azure Container Apps or similar, there\'s a decent chance you\'ll also be using the fantastic [Serilog](https://serilog.net/) and will want to plug it into Azure\'s [Application Insights](https://docs.microsoft.com/en-us/azure/azure-monitor/app/app-insights-overview).\\n\\n![title image reading \\"ASP.NET, Serilog and Application Insights\\" with ASP.NET, Serilog and Application Insights logos](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Updated: 26/11/2022\\n\\nThis post will show you how it\'s done, and it\'ll also build upon the [build info work from our previous post](2021-01-29-surfacing-azure-pipelines-build-info-in-an-aspnet-react-app/index.md). In what way? Great question. Well logs are a tremendous diagnostic tool. If you have logs which display some curious behaviour, and you\'d like to replicate that in another environment, you really want to take exactly that version of the codebase out to play. Our last post introduced build info into our application in the form of our `AppVersionInfo` class that looks something like this:\\n\\n```json\\n{\\n  \\"buildNumber\\": \\"20210130.1\\",\\n  \\"buildId\\": \\"123456\\",\\n  \\"branchName\\": \\"main\\",\\n  \\"commitHash\\": \\"7089620222c30c1ad88e4b556c0a7908ddd34a8e\\"\\n}\\n```\\n\\nWe\'d initially exposed an endpoint in our application which surfaced up this information. Now we\'re going to take that self same information and bake it into our log messages by making use of [Serilog\'s enrichment functionality](https://github.com/serilog/serilog/wiki/Enrichment). Build info and Serilog\'s enrichment are the double act your logging has been waiting for.\\n\\n## Let\'s plug it together\\n\\nWe\'re going to need a number of Serilog dependencies added to our `.csproj`:\\n\\n```xml\\n<PackageReference Include=\\"Serilog.AspNetCore\\" Version=\\"3.4.0\\" />\\n<PackageReference Include=\\"Serilog.Enrichers.Environment\\" Version=\\"2.1.3\\" />\\n<PackageReference Include=\\"Serilog.Enrichers.Thread\\" Version=\\"3.1.0\\" />\\n<PackageReference Include=\\"Serilog.Sinks.ApplicationInsights\\" Version=\\"3.1.0\\" />\\n<PackageReference Include=\\"Serilog.Sinks.Async\\" Version=\\"1.4.0\\" />\\n```\\n\\nThe earlier in your application lifetime you get logging wired up, the happier you will be. Earlier, means more information when you\'re diagnosing issues. So we want to start in our `Program.cs`; `Startup.cs` would be just _way_ too late.\\n\\n```cs\\npublic class Program {\\n    const string APP_NAME = \\"MyAmazingApp\\";\\n\\n    public static int Main(string[] args) {\\n        AppVersionInfo.InitialiseBuildInfoGivenPath(Directory.GetCurrentDirectory());\\n        LoggerConfigurationExtensions.SetupLoggerConfiguration(APP_NAME, AppVersionInfo.GetBuildInfo());\\n\\n        try\\n        {\\n            Log.Information(\\"Starting web host\\");\\n            CreateHostBuilder(args).Build().Run();\\n            return 0;\\n        }\\n        catch (Exception ex)\\n        {\\n            Log.Fatal(ex, \\"Host terminated unexpectedly\\");\\n            return 1;\\n        }\\n        finally\\n        {\\n            Log.CloseAndFlush();\\n        }\\n    }\\n\\n    public static IHostBuilder CreateHostBuilder(string[] args) =>\\n        Host.CreateDefaultBuilder(args)\\n            .UseSerilog((hostBuilderContext, services, loggerConfiguration) => {\\n                loggerConfiguration.ConfigureBaseLogging(APP_NAME, AppVersionInfo.GetBuildInfo());\\n                loggerConfiguration.AddApplicationInsightsLogging(services, hostBuilderContext.Configuration);\\n            })\\n            .ConfigureWebHostDefaults(webBuilder => {\\n                webBuilder\\n                    .UseStartup<Startup>();\\n            });\\n}\\n```\\n\\nIf you look at the code above you\'ll see that the first line of code that executes is `AppVersionInfo.InitialiseBuildInfoGivenPath`. This initialises our `AppVersionInfo` so we have meaningful build info to pump into our logs. The next thing we do is to configure Serilog with `LoggerConfigurationExtensions.SetupLoggerConfiguration`. This provides us with a configured logger so we are free to log any issues that take place during startup. (Incidentally, after startup you\'ll likely inject an `ILogger` into your classes rather than using the static `Log` directly.)\\n\\nFinally, we call `CreateHostBuilder` which in turn calls `UseSerilog` to plug Serilog into ASP.NET. If you take a look inside the body of `UseSerilog` you\'ll see we configure the logging of ASP.NET (in the same way we did for Serilog) and we hook into Application Insights as well. There\'s been a number of references to `LoggerConfigurationExtensions`. Let\'s take a look at it:\\n\\n```cs\\ninternal static class LoggerConfigurationExtensions {\\n    internal static void SetupLoggerConfiguration(string appName, BuildInfo buildInfo) {\\n        Log.Logger = new LoggerConfiguration()\\n            .ConfigureBaseLogging(appName, buildInfo)\\n            .CreateLogger();\\n    }\\n\\n    internal static LoggerConfiguration ConfigureBaseLogging(\\n        this LoggerConfiguration loggerConfiguration,\\n        string appName,\\n        BuildInfo buildInfo\\n    ) {\\n        loggerConfiguration\\n            .MinimumLevel.Debug()\\n            .MinimumLevel.Override(\\"Microsoft\\", LogEventLevel.Information)\\n            // AMAZING COLOURS IN THE CONSOLE!!!!\\n            .WriteTo.Async(a => a.Console(theme: AnsiConsoleTheme.Code))\\n            .Enrich.FromLogContext()\\n            .Enrich.WithMachineName()\\n            .Enrich.WithThreadId()\\n            // Build information as custom properties\\n            .Enrich.WithProperty(nameof(buildInfo.BuildId), buildInfo.BuildId)\\n            .Enrich.WithProperty(nameof(buildInfo.BuildNumber), buildInfo.BuildNumber)\\n            .Enrich.WithProperty(nameof(buildInfo.BranchName), buildInfo.BranchName)\\n            .Enrich.WithProperty(nameof(buildInfo.CommitHash), buildInfo.CommitHash)\\n            .Enrich.WithProperty(\\"ApplicationName\\", appName);\\n\\n        return loggerConfiguration;\\n    }\\n\\n    internal static LoggerConfiguration AddApplicationInsightsLogging(this LoggerConfiguration loggerConfiguration, IServiceProvider services, IConfiguration configuration)\\n    {\\n        if (!string.IsNullOrWhiteSpace(configuration.GetValue<string>(\\"APPINSIGHTS_INSTRUMENTATIONKEY\\")))\\n        {\\n            loggerConfiguration.WriteTo.ApplicationInsights(\\n                services.GetRequiredService<TelemetryConfiguration>(),\\n                TelemetryConverter.Traces);\\n        }\\n\\n        return loggerConfiguration;\\n    }\\n}\\n```\\n\\nIf we take a look at the `ConfigureBaseLogging` method above, we can see that our logs are being enriched with the build info, property by property. We\'re also giving ourselves a beautifully coloured console thanks to Serilog\'s glorious [theme support](https://github.com/serilog/serilog-sinks-console#themes):\\n\\n![screenshot of the console featuring coloured output](coloured-console.png)\\n\\nTake a moment to admire the salmon pinks. Is it not lovely?\\n\\nFinally we come to the main act. Plugging in Application Insights is as simple as dropping in `loggerConfiguration.WriteTo.ApplicationInsights` into our configuration. You\'ll note that this depends upon the existence of an application setting of `APPINSIGHTS_INSTRUMENTATIONKEY` - this is the secret sauce that we need to be in place so we can pipe logs merrily to Application Insights. So you\'ll need this configuration in place so this works.\\n\\n![screenshot of application insights with our output](application-insights-properties.webp)\\n\\nAs you can see, we now have the likes of `BuildNumber`, `CommitHash` and friends visible on each log. Happy diagnostic days!\\n\\nI\'m indebted to the marvellous [Marcel Michau](https://twitter.com/MarcelMichau) who showed me how to get the fiddlier parts of how to get Application Insights plugged in the right way. Thanks chap!"},{"id":"surfacing-azure-pipelines-build-info-in-an-aspnet-react-app","metadata":{"permalink":"/surfacing-azure-pipelines-build-info-in-an-aspnet-react-app","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-01-29-surfacing-azure-pipelines-build-info-in-an-aspnet-react-app/index.md","source":"@site/blog/2021-01-29-surfacing-azure-pipelines-build-info-in-an-aspnet-react-app/index.md","title":"Azure Pipelines Build Info in an ASP.NET React app","description":"Surface build metadata using Azure Pipelines and ASP.NET for both client and server builds in your app with this tutorial.","date":"2021-01-29T00:00:00.000Z","tags":[{"inline":false,"label":"Azure Pipelines","permalink":"/tags/azure-pipelines","description":"The Azure Pipelines CI / CD service."}],"readingTime":6.54,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"surfacing-azure-pipelines-build-info-in-an-aspnet-react-app","title":"Azure Pipelines Build Info in an ASP.NET React app","authors":"johnnyreilly","image":"./about-page.png","tags":["azure pipelines"],"hide_table_of_contents":false,"description":"Surface build metadata using Azure Pipelines and ASP.NET for both client and server builds in your app with this tutorial."},"unlisted":false,"prevItem":{"title":"ASP.NET, Serilog and Application Insights","permalink":"/aspnet-serilog-and-application-insights"},"nextItem":{"title":"Azure App Service, Easy Auth and Roles with .NET and Microsoft.Identity.Web","permalink":"/azure-easy-auth-and-roles-with-net-and-microsoft-identity-web"}},"content":"How do you answer the question: \\"what version of my application is running in Production right now?\\" This post demonstrates how to surface the build metadata that represents the version of your app, from your app using Azure Pipelines and ASP.NET.\\n\\n\x3c!--truncate--\x3e\\n\\nMany is the time where I\'ve been pondering over why something isn\'t working as expected and burned a disappointing amount of time before realising that I\'m playing with an old version of an app. Wouldn\'t it be great give our app a way to say: \\"Hey! I\'m version 1.2.3.4 of your app; built from this commit hash, I was built on Wednesday, I was the nineth build that day and I was built from the `main` branch. And I\'m an Aries.\\" Or something like that.\\n\\nThis post was inspired by [Scott Hanselman\'s similar post on the topic](https://www.hanselman.com/blog/adding-a-git-commit-hash-and-azure-devops-build-number-and-build-id-to-an-aspnet-website). Ultimately this ended up going in a fairly different direction and so seemed worthy of a post of its own.\\n\\nA particular difference is that this is targeting SPAs. Famously, cache invalidation is hard. It\'s possible for the HTML/JS/CSS of your app to be stale due to aggressive caching. So we\'re going to make it possible to see build information for both when the SPA (or \\"client\\") is built, as well as when the .NET app (or \\"server\\") is built. We\'re using a specific type of SPA here; a [React](https://reactjs.org/) SPA built with [TypeScript](https://www.typescriptlang.org/) and [Material UI](https://material-ui.com/), however the principles here are general; you could surface this up any which way you choose.\\n\\n## Putting build info into `azure-pipelines.yml`\\n\\nThe first thing we\'re going to do is to inject our build details into two identical `buildinfo.json` files; one that sits in the server codebase and which will be used to drive the server build information, and one that sits in the client codebase to drive the client equivalent. They\'ll end up looking something like this:\\n\\n```json\\n{\\n  \\"buildNumber\\": \\"20210130.1\\",\\n  \\"buildId\\": \\"123456\\",\\n  \\"branchName\\": \\"main\\",\\n  \\"commitHash\\": \\"7089620222c30c1ad88e4b556c0a7908ddd34a8e\\"\\n}\\n```\\n\\nWe generate this by adding the following `yml` to the beginning of our `azure-pipelines.yml` (crucially before the client or server build take place):\\n\\n```yml\\n- script: |\\n      echo -e -n \\"{\\\\\\"buildNumber\\\\\\":\\\\\\"$(Build.BuildNumber)\\\\\\",\\\\\\"buildId\\\\\\":\\\\\\"$(Build.BuildId)\\\\\\",\\\\\\"branchName\\\\\\":\\\\\\"$(Build.SourceBranchName)\\\\\\",\\\\\\"commitHash\\\\\\":\\\\\\"$(Build.SourceVersion)\\\\\\"}\\" > \\"$(Build.SourcesDirectory)/src/client-app/src/buildinfo.json\\"\\n      echo -e -n \\"{\\\\\\"buildNumber\\\\\\":\\\\\\"$(Build.BuildNumber)\\\\\\",\\\\\\"buildId\\\\\\":\\\\\\"$(Build.BuildId)\\\\\\",\\\\\\"branchName\\\\\\":\\\\\\"$(Build.SourceBranchName)\\\\\\",\\\\\\"commitHash\\\\\\":\\\\\\"$(Build.SourceVersion)\\\\\\"}\\" > \\"$(Build.SourcesDirectory)/src/server-app/Server/buildinfo.json\\"\\n    displayName: \\"emit build details as JSON\\"\\n    failOnStderr: true\\n```\\n\\nAs you can see, we\'re placing the following variables that are available at build time in Azure Pipelines, into the `buildinfo.json`:\\n\\n- `BuildNumber` - The name of the completed build; which usually takes the form of a date in the `yyyyMMdd` format, suffixed by `.x` where `x` is a number that increments representing the number of builds that have taken place on the given day.\\n- `BuildId` - The ID of the record for the completed build.\\n- `SourceVersion` - This is the commit hash of the source code in Git\\n- `SourceBranchName` - The name of the branch in Git.\\n\\n[There\'s many variables available in Azure Pipelines that can be used](https://docs.microsoft.com/en-us/azure/devops/pipelines/build/variables?view=azure-devops&tabs=yaml#build-variables-devops-services) - we\'ve picked out the ones most interesting to us.\\n\\n## Surfacing the server build info\\n\\nOur pipeline is dropping the `buildinfo.json` over pre-existing stub `buildinfo.json` files in both our client and server codebases. The stub files look like this:\\n\\n```json\\n{\\n  \\"buildNumber\\": \\"yyyyMMdd.x\\",\\n  \\"buildId\\": \\"xxxxxx\\",\\n  \\"branchName\\": \\"\\",\\n  \\"commitHash\\": \\"LOCAL_BUILD\\"\\n}\\n```\\n\\nIn our .NET app, the `buildinfo.json` file has been dropped in the root of the app. And as luck would have it, all JSON files are automatically included in a .NET build and so it will be available at runtime. We want to surface this file through an API, and we also want to use it to stamp details into our logs.\\n\\nSo we need to parse the file, and for that we\'ll use this:\\n\\n```cs\\nusing System;\\nusing System.IO;\\nusing System.Text.Json;\\n\\nnamespace Server {\\n    public record BuildInfo(string BranchName, string BuildNumber, string BuildId, string CommitHash);\\n\\n    public static class AppVersionInfo {\\n        private const string _buildFileName = \\"buildinfo.json\\";\\n        private static BuildInfo _fileBuildInfo = new(\\n            BranchName: \\"\\",\\n            BuildNumber: DateTime.UtcNow.ToString(\\"yyyyMMdd\\") + \\".0\\",\\n            BuildId: \\"xxxxxx\\",\\n            CommitHash: $\\"Not yet initialised - call {nameof(InitialiseBuildInfoGivenPath)}\\"\\n        );\\n\\n        public static void InitialiseBuildInfoGivenPath(string path) {\\n            var buildFilePath = Path.Combine(path, _buildFileName);\\n            if (File.Exists(buildFilePath)) {\\n                try {\\n                    var buildInfoJson = File.ReadAllText(buildFilePath);\\n                    var buildInfo = JsonSerializer.Deserialize<BuildInfo>(buildInfoJson, new JsonSerializerOptions {\\n                        PropertyNamingPolicy = JsonNamingPolicy.CamelCase\\n                    });\\n                    if (buildInfo == null) throw new Exception($\\"Failed to deserialise {_buildFileName}\\");\\n\\n                    _fileBuildInfo = buildInfo;\\n                } catch (Exception) {\\n                    _fileBuildInfo = new BuildInfo(\\n                        BranchName: \\"\\",\\n                        BuildNumber: DateTime.UtcNow.ToString(\\"yyyyMMdd\\") + \\".0\\",\\n                        BuildId: \\"xxxxxx\\",\\n                        CommitHash: \\"Failed to load build info from buildinfo.json\\"\\n                    );\\n                }\\n            }\\n        }\\n\\n        public static BuildInfo GetBuildInfo() => _fileBuildInfo;\\n    }\\n}\\n```\\n\\nThe above code reads the `buildinfo.json` file and deserialises it into a `BuildInfo` record which is then surfaced up by the `GetBuildInfo` method. We initialise this at the start of our `Program.cs` like so:\\n\\n```cs\\npublic static int Main(string[] args) {\\n    AppVersionInfo.InitialiseBuildInfoGivenPath(Directory.GetCurrentDirectory());\\n    // Now we\'re free to call AppVersionInfo.GetBuildInfo()\\n    // ....\\n}\\n```\\n\\nNow we need a controller to surface this information up. We\'ll add ourselves a `BuildInfoController.cs`:\\n\\n```cs\\nusing Microsoft.AspNetCore.Authorization;\\nusing Microsoft.AspNetCore.Mvc;\\n\\nnamespace Server.Controllers {\\n    [ApiController]\\n    public class BuildInfoController : ControllerBase {\\n        [AllowAnonymous]\\n        [HttpGet(\\"api/build\\")]\\n        public BuildInfo GetBuild() => AppVersionInfo.GetBuildInfo();\\n    }\\n}\\n```\\n\\nThis exposes an `api/build` endpoint in our .NET app that, when hit, will display the following JSON:\\n\\n![screenshot of api/build output](api-build-screenshot.png)\\n\\n## Surfacing the client build info\\n\\nOur server now lets the world know which version it is running and this is tremendous. Now let\'s make our client do the same.\\n\\nVery little is required to achieve this. Again we have a `buildinfo.json` sat in the root of our codebase. We\'re able to import it as a module in TypeScript because we\'ve set the following property in our `tsconfig.json`:\\n\\n```json\\n\\"resolveJsonModule\\": true,\\n```\\n\\nAs a consequence, consumption is as simple as:\\n\\n```ts\\nimport clientBuildInfo from \'./buildinfo.json\';\\n```\\n\\nWhich provides us with a `clientBuildInfo` which TypeScript automatically derives as this type:\\n\\n```ts\\ntype ClientBuildInfo = {\\n  buildNumber: string;\\n  buildId: string;\\n  branchName: string;\\n  commitHash: string;\\n};\\n```\\n\\nHow you choose to use that information is entirely your choice. We\'re going to add ourselves an \\"about\\" screen in our app, which displays both client info (loaded using the mechanism above) and server info (`fetch`ed from the `/api/build` endpoint).\\n\\n```tsx\\nimport {\\n  Card,\\n  CardContent,\\n  CardHeader,\\n  createStyles,\\n  Grid,\\n  makeStyles,\\n  Theme,\\n  Typography,\\n  Zoom,\\n} from \'@material-ui/core\';\\nimport React from \'react\';\\nimport clientBuildInfo from \'../../buildinfo.json\';\\nimport { projectsPurple } from \'../shared/colors\';\\nimport { Loading } from \'../shared/Loading\';\\nimport { TransitionContainer } from \'../shared/TransitionContainer\';\\n\\nconst useStyles = (cardColor: string) =>\\n  makeStyles((theme: Theme) =>\\n    createStyles({\\n      card: {\\n        padding: theme.spacing(0),\\n        backgroundColor: cardColor,\\n        color: theme.palette.common.white,\\n        minHeight: theme.spacing(28),\\n      },\\n      avatar: {\\n        backgroundColor: theme.palette.getContrastText(cardColor),\\n        color: cardColor,\\n      },\\n      main: {\\n        padding: theme.spacing(2),\\n      },\\n    }),\\n  )();\\n\\ntype Styles = ReturnType<typeof useStyles>;\\n\\nconst AboutPage: React.FC = () => {\\n  const [serverBuildInfo, setServerBuildInfo] =\\n    React.useState<typeof clientBuildInfo>();\\n\\n  React.useEffect(() => {\\n    fetch(\'/api/build\')\\n      .then((response) => response.json())\\n      .then(setServerBuildInfo);\\n  }, []);\\n\\n  const classes = useStyles(projectsPurple);\\n\\n  return (\\n    <TransitionContainer>\\n      <Grid container spacing={3}>\\n        <Grid item xs={12} sm={12} container alignItems=\\"center\\">\\n          <Grid item>\\n            <Typography variant=\\"h4\\" component=\\"h1\\">\\n              About\\n            </Typography>\\n          </Grid>\\n        </Grid>\\n      </Grid>\\n      <Grid container spacing={1}>\\n        <BuildInfo\\n          classes={classes}\\n          title=\\"Client Version\\"\\n          {...clientBuildInfo}\\n        />\\n      </Grid>\\n      <br />\\n      <Grid container spacing={1}>\\n        {serverBuildInfo ? (\\n          <BuildInfo\\n            classes={classes}\\n            title=\\"Server Version\\"\\n            {...serverBuildInfo}\\n          />\\n        ) : (\\n          <Loading />\\n        )}\\n      </Grid>\\n    </TransitionContainer>\\n  );\\n};\\n\\ninterface Props {\\n  classes: Styles;\\n  title: string;\\n  branchName: string;\\n  buildNumber: string;\\n  buildId: string;\\n  commitHash: string;\\n}\\n\\nconst BuildInfo: React.FC<Props> = ({\\n  classes,\\n  title,\\n  branchName,\\n  buildNumber,\\n  buildId,\\n  commitHash,\\n}) => (\\n  <Zoom mountOnEnter unmountOnExit in={true}>\\n    <Card className={classes.card}>\\n      <CardHeader title={title} />\\n      <CardContent className={classes.main}>\\n        <Typography variant=\\"body1\\" component=\\"p\\">\\n          <b>Build Number</b> {buildNumber}\\n        </Typography>\\n        <Typography variant=\\"body1\\" component=\\"p\\">\\n          <b>Build Id</b> {buildId}\\n        </Typography>\\n        <Typography variant=\\"body1\\" component=\\"p\\">\\n          <b>Branch Name</b> {branchName}\\n        </Typography>\\n        <Typography variant=\\"body1\\" component=\\"p\\">\\n          <b>Commit Hash</b> {commitHash}\\n        </Typography>\\n      </CardContent>\\n    </Card>\\n  </Zoom>\\n);\\n\\nexport default AboutPage;\\n```\\n\\nWhen the above page is viewed it looks like this:\\n\\n![screenshot of our web app surfacing up the build information](about-page.png)\\n\\nAnd that\'s it! Our app is clearly telling us what version is being run, both on the server and in the client. Thanks to Scott Hanselman for his work which inspired this."},{"id":"azure-easy-auth-and-roles-with-net-and-microsoft-identity-web","metadata":{"permalink":"/azure-easy-auth-and-roles-with-net-and-microsoft-identity-web","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-01-17-azure-easy-auth-and-roles-with-net-and-microsoft-identity-web/index.md","source":"@site/blog/2021-01-17-azure-easy-auth-and-roles-with-net-and-microsoft-identity-web/index.md","title":"Azure App Service, Easy Auth and Roles with .NET and Microsoft.Identity.Web","description":"The `Microsoft.Identity.Web` library has authorization issues with roles. A `IClaimsTransformation` can map claims to fix the problem.","date":"2021-01-17T00:00:00.000Z","tags":[{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."},{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."},{"inline":false,"label":"Easy Auth","permalink":"/tags/easy-auth","description":"The Azure Easy Auth feature used for authentication and authorization."},{"inline":false,"label":"Authentication and Authorization","permalink":"/tags/auth","description":"Matters related to identity and permissioning"}],"readingTime":2.52,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-easy-auth-and-roles-with-net-and-microsoft-identity-web","title":"Azure App Service, Easy Auth and Roles with .NET and Microsoft.Identity.Web","authors":"johnnyreilly","tags":["azure","asp.net","easy auth","auth"],"hide_table_of_contents":false,"description":"The `Microsoft.Identity.Web` library has authorization issues with roles. A `IClaimsTransformation` can map claims to fix the problem."},"unlisted":false,"prevItem":{"title":"Azure Pipelines Build Info in an ASP.NET React app","permalink":"/surfacing-azure-pipelines-build-info-in-an-aspnet-react-app"},"nextItem":{"title":"Azure App Service, Easy Auth and Roles with .NET","permalink":"/azure-easy-auth-and-roles-with-dotnet-and-core"}},"content":"[I wrote recently about how to get Azure App Service Easy Auth to work with roles](../2021-01-14-azure-easy-auth-and-roles-with-dotnet-and-core/index.md). This involved borrowing the approach used by [MaximeRouiller.Azure.AppService.EasyAuth](https://github.com/MaximRouiller/MaximeRouiller.Azure.AppService.EasyAuth).\\n\\nAs a consequence of writing that post I came to learn that official support for Azure Easy Auth had landed in October 2020 in v1.2 of [Microsoft.Identity.Web](https://github.com/AzureAD/microsoft-identity-web/wiki/1.2.0#integration-with-azure-app-services-authentication-of-web-apps-running-with-microsoftidentityweb). This was great news; I was delighted.\\n\\nHowever, it turns out that the same authorization issue that `MaximeRouiller.Azure.AppService.EasyAuth` suffers from, is visited upon `Microsoft.Identity.Web` as well. This post shows hoew to resolve it with `IClaimsTransformation`.\\n\\n\x3c!--truncate--\x3e\\n\\nIf you\'re looking for information about Easy Auth and authentication with .NET and Azure Container Apps, [you can find it here](../2023-06-11-azure-container-apps-easy-auth-and-dotnet-authentication/index.md).\\n\\n## Getting set up\\n\\nWe\'re using a .NET 5 project, running in an Azure App Service (Linux). In our `.csproj` we have:\\n\\n```xml\\n<PackageReference Include=\\"Microsoft.Identity.Web\\" Version=\\"1.4.1\\" />\\n```\\n\\nIn our `Startup.cs` we\'re using:\\n\\n```cs\\npublic void ConfigureServices(IServiceCollection services) {\\n    //...\\n    services.AddMicrosoftIdentityWebAppAuthentication(Configuration);\\n    //...\\n}\\n\\npublic void Configure(IApplicationBuilder app, IWebHostEnvironment env) {\\n    //...\\n    app.UseAuthentication();\\n    app.UseAuthorization();\\n    //...\\n}\\n```\\n\\n## You gotta `roles` with it\\n\\nWhilst the authentication works, authorization does not. So whilst my app knows who I am - the authorization is not working with relation to **roles**.\\n\\nWhen directly using `Microsoft.Identity.Web` when running locally, we see these claims:\\n\\n```json\\n[\\n  // ...\\n  {\\n    \\"type\\": \\"http://schemas.microsoft.com/ws/2008/06/identity/claims/role\\",\\n    \\"value\\": \\"Administrator\\"\\n  },\\n  {\\n    \\"type\\": \\"http://schemas.microsoft.com/ws/2008/06/identity/claims/role\\",\\n    \\"value\\": \\"Reader\\"\\n  }\\n  // ...\\n]\\n```\\n\\nHowever, we get different behaviour with EasyAuth; it provides roles related claims with a **different type**:\\n\\n```json\\n[\\n  // ...\\n  {\\n    \\"type\\": \\"roles\\",\\n    \\"value\\": \\"Administrator\\"\\n  },\\n  {\\n    \\"type\\": \\"roles\\",\\n    \\"value\\": \\"Reader\\"\\n  }\\n  // ...\\n]\\n```\\n\\nThis means that roles related authorization _does not work_ with Easy Auth:\\n\\n```cs\\n[Authorize(Roles = \\"Reader\\")]\\n[HttpGet(\\"api/reader\\")]\\npublic string GetWithReader() =>\\n    \\"this is a secure endpoint that users with the Reader role can access\\";\\n```\\n\\nThis is because .NET is looking for claims with a `type` of `\\"http://schemas.microsoft.com/ws/2008/06/identity/claims/role\\"` and not finding them with Easy Auth.\\n\\n## Claims transformation with `IClaimsTransformation`\\n\\nThere is a way to work around this issue .NET using `IClaimsTransformation`. This is a poorly documented feature, but fortunately [Gunnar Peipman\'s blog does a grand job of explaining it](https://gunnarpeipman.com/aspnet-core-adding-claims-to-existing-identity/).\\n\\nInside our `Startup.cs` I\'ve registered a claims transformer:\\n\\n```cs\\nservices.AddScoped<IClaimsTransformation, AddRolesClaimsTransformation>();\\n```\\n\\nAnd that claims transformer looks like this:\\n\\n```cs\\npublic class AddRolesClaimsTransformation : IClaimsTransformation {\\n    private readonly ILogger<AddRolesClaimsTransformation> _logger;\\n\\n    public AddRolesClaimsTransformation(ILogger<AddRolesClaimsTransformation> logger) {\\n        _logger = logger;\\n    }\\n\\n    public Task<ClaimsPrincipal> TransformAsync(ClaimsPrincipal principal) {\\n        var mappedRolesClaims = principal.Claims\\n            .Where(claim => claim.Type == \\"roles\\")\\n            .Select(claim => new Claim(ClaimTypes.Role, claim.Value))\\n            .ToList();\\n\\n        // Clone current identity\\n        var clone = principal.Clone();\\n\\n        if (clone.Identity is not ClaimsIdentity newIdentity) return Task.FromResult(principal);\\n\\n        // Add role claims to cloned identity\\n        foreach (var mappedRoleClaim in mappedRolesClaims)\\n            newIdentity.AddClaim(mappedRoleClaim);\\n\\n        if (mappedRolesClaims.Count > 0)\\n            _logger.LogInformation(\\"Added roles claims {mappedRolesClaims}\\", mappedRolesClaims);\\n        else\\n            _logger.LogInformation(\\"No roles claims added\\");\\n\\n        return Task.FromResult(clone);\\n    }\\n}\\n```\\n\\nThe class above creates a new principal with `\\"roles\\"` claims mapped across to `\\"http://schemas.microsoft.com/ws/2008/06/identity/claims/role\\"`. This is enough to get .NET treating roles the way you\'d hope.\\n\\n[I\'ve raised an issue against the `Microsoft.Identity.Web` repo](https://github.com/AzureAD/microsoft-identity-web/issues/881) about this. Perhaps one day this workaround will no longer be necessary."},{"id":"azure-easy-auth-and-roles-with-dotnet-and-core","metadata":{"permalink":"/azure-easy-auth-and-roles-with-dotnet-and-core","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-01-14-azure-easy-auth-and-roles-with-dotnet-and-core/index.md","source":"@site/blog/2021-01-14-azure-easy-auth-and-roles-with-dotnet-and-core/index.md","title":"Azure App Service, Easy Auth and Roles with .NET","description":"\\"Easy Auth\\" in Azure App Service doesnt currently work with .NET Core and .NET due to discrepancies. Open-source middleware can help solve the issue.","date":"2021-01-14T00:00:00.000Z","tags":[{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."},{"inline":false,"label":"Authentication and Authorization","permalink":"/tags/auth","description":"Matters related to identity and permissioning"},{"inline":false,"label":"Easy Auth","permalink":"/tags/easy-auth","description":"The Azure Easy Auth feature used for authentication and authorization."}],"readingTime":5.755,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-easy-auth-and-roles-with-dotnet-and-core","title":"Azure App Service, Easy Auth and Roles with .NET","authors":"johnnyreilly","tags":["azure","auth","easy auth"],"hide_table_of_contents":false,"description":"\\"Easy Auth\\" in Azure App Service doesnt currently work with .NET Core and .NET due to discrepancies. Open-source middleware can help solve the issue."},"unlisted":false,"prevItem":{"title":"Azure App Service, Easy Auth and Roles with .NET and Microsoft.Identity.Web","permalink":"/azure-easy-auth-and-roles-with-net-and-microsoft-identity-web"},"nextItem":{"title":"react-query: strongly typing useQueries","permalink":"/strongly-typing-react-query-s-usequeries"}},"content":"Azure App Service has a feature which is intended to allow Authentication and Authorization to be applied outside of your application code. It\'s called [\\"Easy Auth\\"](https://docs.microsoft.com/en-us/azure/app-service/overview-authentication-authorization). Unfortunately, in the context of App Services it doesn\'t work with .NET Core and .NET. Perhaps it would be better to say: of the various .NETs, it supports .NET Framework. [To quote the docs](https://docs.microsoft.com/en-us/azure/app-service/overview-authentication-authorization#userapplication-claims):\\n\\n> At this time, ASP.NET Core does not currently support populating the current user with the Authentication/Authorization feature. However, some [3rd party, open source middleware components](https://github.com/MaximRouiller/MaximeRouiller.Azure.AppService.EasyAuth) do exist to help fill this gap.\\n\\nThanks to [Maxime Rouiller](https://twitter.com/MaximRouiller) there\'s a way forward here. However, as I was taking this for a spin today, I discovered another issue.\\n\\n\x3c!--truncate--\x3e\\n\\nIf this post is interesting to you, you may also want to [look at this one where we try to use Microsoft.Identity.Web for the same purpose.](../2021-01-17-azure-easy-auth-and-roles-with-net-and-microsoft-identity-web/index.md)\\n\\nIf you\'re looking for information about Easy Auth and authentication with .NET and Azure Container Apps, [you can find it here](../2023-06-11-azure-container-apps-easy-auth-and-dotnet-authentication/index.md).\\n\\n## Where are our roles?\\n\\nConsider the following .NET controller:\\n\\n```cs\\n[Authorize(Roles = \\"Administrator,Reader\\")]\\n[HttpGet(\\"api/admin-reader\\")]\\npublic string GetWithAdminOrReader() =>\\n    \\"this is a secure endpoint that users with the Administrator or Reader role can access\\";\\n\\n[Authorize(Roles = \\"Administrator\\")]\\n[HttpGet(\\"api/admin\\")]\\npublic string GetWithAdmin() =>\\n    \\"this is a secure endpoint that users with the Administrator role can access\\";\\n\\n[Authorize(Roles = \\"Reader\\")]\\n[HttpGet(\\"api/reader\\")]\\npublic string GetWithReader() =>\\n    \\"this is a secure endpoint that users with the Reader role can access\\";\\n```\\n\\nThe three endpoints above restrict access based upon roles. However, even with Maxime\'s marvellous shim in the mix, authorization doesn\'t work when deployed to an Azure App Service. Why? Well, it comes down to how roles are mapped to claims.\\n\\nLet\'s back up a bit. First of all we\'ve added a dependency to our project:\\n\\n```shell\\ndotnet add package MaximeRouiller.Azure.AppService.EasyAuth\\n```\\n\\nNext we\'ve updated our `Startup.ConfigureServices` such that it looks like this:\\n\\n```cs\\nif (Env.IsDevelopment()) {\\n    services.AddMicrosoftIdentityWebAppAuthentication(Configuration);\\nelse\\n    services.AddAuthentication(\\"EasyAuth\\").AddEasyAuthAuthentication((o) => { });\\n```\\n\\nWith the above in place, either the Microsoft Identity platform will directly be used for authentication, or Maxime\'s package will be used as the default authentication scheme. The driver for this is `Env` which is an `IHostEnvironment` that was injected to the `Startup.cs`. Running locally, both authentication and authorization will work. However, deployed to an Azure App Service, only authentication will work.\\n\\nIt turns out that directly using the Microsoft Identity platform, we see roles claims coming through like so:\\n\\n```json\\n[\\n  // ...\\n  {\\n    \\"type\\": \\"http://schemas.microsoft.com/ws/2008/06/identity/claims/role\\",\\n    \\"value\\": \\"Administrator\\"\\n  },\\n  {\\n    \\"type\\": \\"http://schemas.microsoft.com/ws/2008/06/identity/claims/role\\",\\n    \\"value\\": \\"Reader\\"\\n  }\\n  // ...\\n]\\n```\\n\\nBut in Azure we see roles claims showing up with a different `type`:\\n\\n```json\\n[\\n  // ...\\n  {\\n    \\"type\\": \\"roles\\",\\n    \\"value\\": \\"Administrator\\"\\n  },\\n  {\\n    \\"type\\": \\"roles\\",\\n    \\"value\\": \\"Reader\\"\\n  }\\n  // ...\\n]\\n```\\n\\nThis is the crux of the problem; .NET and .NET Core are looking in a different place for roles.\\n\\n## Role up, role up!\\n\\nThere wasn\'t an obvious way to make this work with Maxime\'s package. So we ended up lifting the source code of Maxime\'s package and tweaking it. Take a look:\\n\\n```cs\\nusing Microsoft.AspNetCore.Authentication;\\nusing Microsoft.Extensions.DependencyInjection;\\nusing Microsoft.Extensions.Logging;\\nusing Microsoft.Extensions.Options;\\nusing System;\\nusing System.Collections.Generic;\\nusing System.Linq;\\nusing System.Security.Claims;\\nusing System.Text.Encodings.Web;\\nusing System.Text.Json;\\nusing System.Text.Json.Serialization;\\nusing System.Threading.Tasks;\\n\\n/// <summary>\\n/// Based on https://github.com/MaximRouiller/MaximeRouiller.Azure.AppService.EasyAuth\\n/// Essentially EasyAuth only supports .NET Framework: https://docs.microsoft.com/en-us/azure/app-service/app-service-authentication-how-to#access-user-claims\\n/// This allows us to get support for Authentication and Authorization (using roles) with .NET\\n/// </summary>\\nnamespace EasyAuth {\\n    public static class EasyAuthAuthenticationBuilderExtensions {\\n        public static AuthenticationBuilder AddEasyAuthAuthentication(\\n            this IServiceCollection services) =>\\n            services.AddAuthentication(\\"EasyAuth\\").AddEasyAuthAuthenticationScheme(o => { });\\n\\n        public static AuthenticationBuilder AddEasyAuthAuthenticationScheme(\\n            this AuthenticationBuilder builder,\\n            Action<EasyAuthAuthenticationOptions> configure) =>\\n                builder.AddScheme<EasyAuthAuthenticationOptions, EasyAuthAuthenticationHandler>(\\n                    \\"EasyAuth\\",\\n                    \\"EasyAuth\\",\\n                    configure);\\n    }\\n\\n    public class EasyAuthAuthenticationOptions : AuthenticationSchemeOptions {\\n        public EasyAuthAuthenticationOptions() {\\n            Events = new object();\\n        }\\n    }\\n\\n    public class EasyAuthAuthenticationHandler : AuthenticationHandler<EasyAuthAuthenticationOptions> {\\n        public EasyAuthAuthenticationHandler(\\n            IOptionsMonitor<EasyAuthAuthenticationOptions> options,\\n            ILoggerFactory logger,\\n            UrlEncoder encoder,\\n            ISystemClock clock)\\n            : base(options, logger, encoder, clock) {\\n        }\\n\\n        protected override Task<AuthenticateResult> HandleAuthenticateAsync() {\\n            try {\\n                var easyAuthEnabled = string.Equals(Environment.GetEnvironmentVariable(\\"WEBSITE_AUTH_ENABLED\\", EnvironmentVariableTarget.Process), \\"True\\", StringComparison.InvariantCultureIgnoreCase);\\n                if (!easyAuthEnabled) return Task.FromResult(AuthenticateResult.NoResult());\\n\\n                var easyAuthProvider = Context.Request.Headers[\\"X-MS-CLIENT-PRINCIPAL-IDP\\"].FirstOrDefault();\\n                var msClientPrincipalEncoded = Context.Request.Headers[\\"X-MS-CLIENT-PRINCIPAL\\"].FirstOrDefault();\\n                if (string.IsNullOrWhiteSpace(easyAuthProvider) ||\\n                    string.IsNullOrWhiteSpace(msClientPrincipalEncoded))\\n                    return Task.FromResult(AuthenticateResult.NoResult());\\n\\n                var decodedBytes = Convert.FromBase64String(msClientPrincipalEncoded);\\n                var msClientPrincipalDecoded = System.Text.Encoding.Default.GetString(decodedBytes);\\n                var clientPrincipal = JsonSerializer.Deserialize<MsClientPrincipal>(msClientPrincipalDecoded);\\n                if (clientPrincipal == null) return Task.FromResult(AuthenticateResult.NoResult());\\n\\n                var mappedRolesClaims = clientPrincipal.Claims\\n                    .Where(claim => claim.Type == \\"roles\\")\\n                    .Select(claim => new Claim(ClaimTypes.Role, claim.Value))\\n                    .ToList();\\n\\n                var claims = clientPrincipal.Claims.Select(claim => new Claim(claim.Type, claim.Value)).ToList();\\n                claims.AddRange(mappedRolesClaims);\\n\\n                var principal = new ClaimsPrincipal();\\n                principal.AddIdentity(new ClaimsIdentity(claims, clientPrincipal.AuthenticationType, clientPrincipal.NameType, clientPrincipal.RoleType));\\n\\n                var ticket = new AuthenticationTicket(principal, easyAuthProvider);\\n                var success = AuthenticateResult.Success(ticket);\\n                Context.User = principal;\\n\\n                return Task.FromResult(success);\\n            } catch (Exception ex) {\\n                return Task.FromResult(AuthenticateResult.Fail(ex));\\n            }\\n        }\\n    }\\n\\n    public class MsClientPrincipal {\\n        [JsonPropertyName(\\"auth_typ\\")]\\n        public string? AuthenticationType { get; set; }\\n        [JsonPropertyName(\\"claims\\")]\\n        public IEnumerable<UserClaim> Claims { get; set; } = Array.Empty<UserClaim>();\\n        [JsonPropertyName(\\"name_typ\\")]\\n        public string? NameType { get; set; }\\n        [JsonPropertyName(\\"role_typ\\")]\\n        public string? RoleType { get; set; }\\n    }\\n\\n    public class UserClaim {\\n        [JsonPropertyName(\\"typ\\")]\\n        public string Type { get; set; } = string.Empty;\\n        [JsonPropertyName(\\"val\\")]\\n        public string Value { get; set; } = string.Empty;\\n    }\\n}\\n```\\n\\nThere\'s a number of changes in the above code to Maxime\'s package. Three changes that are not significant and one that is. First the insignificant changes:\\n\\n1. It uses [`System.Text.Json`](https://docs.microsoft.com/en-us/dotnet/standard/serialization/system-text-json-how-to?pivots=dotnet-5-0) in place of JSON.NET\\n2. It uses [C#s nullable reference types](../2020-12-20-nullable-reference-types-csharp-strictnullchecks/index.md)\\n3. It changes the extension method signature such that instead of entering `services.AddAuthentication().AddEasyAuthAuthentication((o) => { })` we now need only enter `services.AddEasyAuthAuthentication()`\\n\\nNow the significant change:\\n\\nWhere the middleware encounters claims in the `X-MS-CLIENT-PRINCIPAL` header with the `Type` of `\\"roles\\"` it creates brand new claims for each, with the same `Value` but with the official `Type` supplied by `ClaimsTypes.Role` of `\\"http://schemas.microsoft.com/ws/2008/06/identity/claims/role\\"`. The upshot of this, is that when the processed claims are inspected in Azure they now look more like this:\\n\\n```json\\n[\\n  // ...\\n  {\\n    \\"type\\": \\"roles\\",\\n    \\"value\\": \\"Administrator\\"\\n  },\\n  {\\n    \\"type\\": \\"roles\\",\\n    \\"value\\": \\"Reader\\"\\n  },\\n  // ...\\n  {\\n    \\"type\\": \\"http://schemas.microsoft.com/ws/2008/06/identity/claims/role\\",\\n    \\"value\\": \\"Administrator\\"\\n  },\\n  {\\n    \\"type\\": \\"http://schemas.microsoft.com/ws/2008/06/identity/claims/role\\",\\n    \\"value\\": \\"Reader\\"\\n  }\\n]\\n```\\n\\nAs you can see, we now have both the originally supplied roles _as well_ as roles of the type that .NET and .NET Core expect. Consequently, roles based behaviour starts to work. Thanks to Maxime for his fine work on the initial solution. It would be tremendous if neither the code in this blog post nor Maxime\'s shim were required. Still, until that glorious day!\\n\\n## Update: Potential ways forward\\n\\nWhen I was tweeting this post, Maxime was good enough to respond and suggest that this may be resolved within Azure itself in future:\\n\\n> Oh, so that\'s why they removed the name? \uD83D\uDE32\uD83D\uDE1C Jokes aside, we hope that this package won\'t be necessary for the future. I know that [@mattchenderson](https://twitter.com/mattchenderson?ref_src=twsrc%5Etfw) is part of a working group to update Easy Auth. Might want to make sure you follow him as well. \uD83D\uDE01\\n>\\n> \u2014 Maxime Rouiller (@MaximRouiller) [January 14, 2021](https://twitter.com/MaximRouiller/status/1349804324713615366?ref_src=twsrc%5Etfw)\\n\\nThere\'s a prospective PR that would add an event to Maxime\'s API. If something along these lines was merged, then my workaround would no longer be necessary. Follow the PR [here](https://github.com/MaximRouiller/MaximeRouiller.Azure.AppService.EasyAuth/pull/13)."},{"id":"strongly-typing-react-query-s-usequeries","metadata":{"permalink":"/strongly-typing-react-query-s-usequeries","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-01-03-strongly-typing-react-query-s-usequeries/index.md","source":"@site/blog/2021-01-03-strongly-typing-react-query-s-usequeries/index.md","title":"react-query: strongly typing useQueries","description":"Learn how to strongly type `useQueries` in `react-query` with `useQueriesTyped`. A wrapper function provides the strongly-typed API.","date":"2021-01-03T00:00:00.000Z","tags":[{"inline":false,"label":"React","permalink":"/tags/react","description":"The React library."}],"readingTime":7.84,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"strongly-typing-react-query-s-usequeries","title":"react-query: strongly typing useQueries","authors":"johnnyreilly","image":"./strongly-typing-usequeries.webp","tags":["react"],"hide_table_of_contents":false,"description":"Learn how to strongly type `useQueries` in `react-query` with `useQueriesTyped`. A wrapper function provides the strongly-typed API."},"unlisted":false,"prevItem":{"title":"Azure App Service, Easy Auth and Roles with .NET","permalink":"/azure-easy-auth-and-roles-with-dotnet-and-core"},"nextItem":{"title":"Create React App with ts-loader and CRACO","permalink":"/create-react-app-with-ts-loader-and-craco"}},"content":"`react-query` has a weakly typed hook named `useQueries`. It\'s possible to turn that into a strong typed hook; this post shows you how.\\n\\n![title image that says \\"react-query: strongly typings useQueries\\"](strongly-typing-usequeries.webp)\\n\\n\x3c!--truncate--\x3e\\n\\n## Updated April 2022\\n\\nYou don\'t need this blog post! Just use a `react-query@3.28.0` or greater; [artysidorenko](https://github.com/artysidorenko) [contributed a PR that moved this behaviour into the package](https://github.com/tannerlinsley/react-query/pull/2634).\\n\\n## What is `useQueries`?\\n\\nIf you haven\'t used [`react-query`](https://react-query.tanstack.com/) then I heartily recommend it. It provides (to quote the docs):\\n\\n> Hooks for fetching, caching and updating asynchronous data in React\\n\\nWith version 3 of `react-query`, a new hook was added: [`useQueries`](https://react-query.tanstack.com/reference/useQueries). This hook allows you fetch a variable number of queries at the same time. An example of what usage looks like is this ([borrowed from the excellent docs](https://react-query.tanstack.com/guides/parallel-queries#dynamic-parallel-queries-with-usequeries)):\\n\\n```tsx\\nfunction App({ users }) {\\n  const userQueries = useQueries(\\n    users.map((user) => {\\n      return {\\n        queryKey: [\'user\', user.id],\\n        queryFn: () => fetchUserById(user.id),\\n      };\\n    }),\\n  );\\n}\\n```\\n\\nWhilst `react-query` is written in TypeScript, the way that `useQueries` is presently written strips the types that are supplied to it. Consider [the signature of the `useQueries`](https://github.com/tannerlinsley/react-query/blob/d25ab3ef8260ea1c02b52b7082c3ce4120596b31/src/react/useQueries.ts#L8):\\n\\n```ts\\nexport function useQueries(queries: UseQueryOptions[]): UseQueryResult[] {\\n```\\n\\nThis returns an array of [`UseQueryResult`](https://github.com/tannerlinsley/react-query/blob/d25ab3ef8260ea1c02b52b7082c3ce4120596b31/src/react/types.ts#L42):\\n\\n```ts\\nexport type UseQueryResult<\\n  TData = unknown,\\n  TError = unknown,\\n> = UseBaseQueryResult<TData, TError>;\\n```\\n\\nAs you can see, no type parameters are passed to `UseQueryResult` in the `useQueries` signature and so it takes the default types of `unknown`. This forces the consumer to either assert the type that they believe to be there, or to use type narrowing to ensure the type. The former approach exposes a possibility of errors (the user can specify incorrect types) and the latter approach requires our code to perform type narrowing operations which are essentially unnecessary (the type hasn\'t changed since it was returned; it\'s simply been discarded).\\n\\nWhat if there was a way to strongly type `useQueries` so we neither risked specifying incorrect types, nor wasted precious lines of code and CPU cycles performing type narrowing? There is my friends, read on!\\n\\n## `useQueriesTyped` - a strongly typed wrapper for `useQueries`\\n\\nIt\'s possible to wrap the `useQueries` hook with our own `useQueriesTyped` hook which exposes a strongly typed API. It looks like this:\\n\\n```ts\\nimport { useQueries, UseQueryOptions, UseQueryResult } from \'react-query\';\\n\\ntype Awaited<T> = T extends PromiseLike<infer U> ? Awaited<U> : T;\\n\\nexport function useQueriesTyped<TQueries extends readonly UseQueryOptions[]>(\\n  queries: [...TQueries],\\n): {\\n  [ArrayElement in keyof TQueries]: UseQueryResult<\\n    TQueries[ArrayElement] extends { select: infer TSelect }\\n      ? TSelect extends (data: any) => any\\n        ? ReturnType<TSelect>\\n        : never\\n      : Awaited<\\n          ReturnType<\\n            NonNullable<\\n              Extract<TQueries[ArrayElement], UseQueryOptions>[\'queryFn\']\\n            >\\n          >\\n        >\\n  >;\\n} {\\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\\n  return useQueries(\\n    queries as UseQueryOptions<unknown, unknown, unknown>[],\\n  ) as any;\\n}\\n```\\n\\nLet\'s unpack this. The first and most significant thing to note here is that `queries` moves from being `UseQueryOptions[]` to being `TQueries extends readonly UseQueryOptions[]` \\\\- far more fancy! The reason for this change is we want the type parameters to flow through on an element by element basis in the supplied array. [TypeScript 4\'s variadic tuple types](https://www.typescriptlang.org/docs/handbook/release-notes/typescript-4-0.html#variadic-tuple-types) should allow us to support this. So the new array signature looks like this:\\n\\n```ts\\nqueries: [...TQueries];\\n```\\n\\nWhere `TQueries` is\\n\\n```ts\\nTQueries extends readonly UseQueryOptions[]\\n```\\n\\nWhat this means is, that each element of the rest parameters array must have a type of `readonly UseQueryOptions`. Otherwise the compiler will shout at us (and rightly so).\\n\\nSo that\'s what\'s coming in.... What\'s going out? Well the return type of `useQueriesTyped` is the tremendously verbose:\\n\\n```ts\\n{\\n  [ArrayElement in keyof TQueries]: UseQueryResult<\\n    TQueries[ArrayElement] extends { select: infer TSelect }\\n      ? TSelect extends (data: any) => any\\n        ? ReturnType<TSelect>\\n        : never\\n      : Awaited<\\n          ReturnType<\\n            NonNullable<\\n              Extract<TQueries[ArrayElement], UseQueryOptions>[\'queryFn\']\\n            >\\n          >\\n        >\\n  >\\n}\\n```\\n\\nLet\'s walk this through. First of all we\'ll look at this bit:\\n\\n```ts\\n{ [ArrayElement in keyof TQueries]: /* the type has been stripped to protect your eyes */ }\\n```\\n\\nOn the face of it, it looks like we\'re returning an `Object`, not an `Array`. There\'s nuance here; [JavaScript `Array`s are `Object`s](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array).\\n\\nMore specifically, by approaching the signature this way, we can acquire the `ArrayElement` type which represents each of the keys of the array. Consider this array:\\n\\n```ts\\n[1, \'two\', new Date()];\\n```\\n\\nFor the above, `ArrayElement` would take the values `0`, `1` and `2`. And this is going to prove useful in a moment as we\'re going to index into our `TQueries` object to surface up the return types for each element of our return array from there.\\n\\nNow let\'s look at the return type for each element. The signature of that looks like this:\\n\\n```ts\\nUseQueryResult<\\n  TQueries[ArrayElement] extends { select: infer TSelect }\\n    ? TSelect extends (data: any) => any\\n      ? ReturnType<TSelect>\\n      : never\\n    : Awaited<\\n        ReturnType<\\n          NonNullable<\\n            Extract<TQueries[ArrayElement], UseQueryOptions>[\'queryFn\']\\n          >\\n        >\\n      >\\n>;\\n```\\n\\nGosh... Well there\'s a lot going on here. Let\'s start in the middle and work our way out.\\n\\n```ts\\nTQueries[ArrayElement];\\n```\\n\\nThe above code indexes into our `TQueries` array for each element of our strongly typed indexer `ArrayElement`. So it might resolve the first element of an array to `{ queryKey: \'key1\', queryFn: () =&gt; 1 }`, for example. Next:\\n\\n```ts\\nExtract < TQueries[ArrayElement], UseQueryOptions > [\'queryFn\'];\\n```\\n\\nWe\'re now taking the type of each element provided, and grabbing the type of the `queryFn` property. It\'s this type which contains the type of the data that will be passed back, that we want to make use of. So for an examples of `[{ queryKey: \'key1\', queryFn: () =&gt; 1 }, { queryKey: \'key2\', queryFn: () =&gt; \'two\' }, { queryKey: \'key3\', queryFn: () =&gt; new Date() }]` we\'d have the type: `const result: [() =&gt; number, () =&gt; string, () =&gt; Date]`.\\n\\n```ts\\nNonNullable<Extract<TQueries[ArrayElement], UseQueryOptions>[\'queryFn\']>;\\n```\\n\\nThe next stage is using `NonNullable` on our `queryFn`, given that on `UseQueryOptions` it\'s an optional type. In our use case it is not optional / nullable and so we need to enforce that.\\n\\n```ts\\nReturnType<\\n  NonNullable<Extract<TQueries[ArrayElement], UseQueryOptions>[\'queryFn\']>\\n>;\\n```\\n\\nNow we want to get the return type of our `queryFn` \\\\- as that\'s the data type we\'re interested. So we use TypeScript\'s `ReturnType` for that.\\n\\n```ts\\nReturnType<\\n  NonNullable<Extract<TQueries[ArrayElement], UseQueryOptions>[\'queryFn\']>\\n>;\\n```\\n\\nHere we\'re using [TypeScript 4.1\'s recursive conditional types](https://devblogs.microsoft.com/typescript/announcing-typescript-4-1/#recursive-conditional-types) to unwrap a `Promise` (or not) to the relevant type. This allows us to get the actual type we\'re interested in, as opposed to the `Promise` of that type. Finally we have the type we need! So we can do this:\\n\\n```ts\\ntype Awaited<T> = T extends PromiseLike<infer U> ? Awaited<U> : T;\\n\\nAwaited<\\n  ReturnType<\\n    NonNullable<Extract<TQueries[ArrayElement], UseQueryOptions>[\'queryFn\']>\\n  >\\n>;\\n```\\n\\nIt\'s at this point where we reach a conditional type in our type definition. Essentially, we have two different typing behaviours in play:\\n\\n1. Where we\'re inferring the return type of the query\\n2. Where we\'re inferring the return type of a `select`. A `select` option can be used to transform or select a part of the data returned by the query function. It has the signature: `select: (data: TData) => TSelect`\\n\\nWe\'ve been unpacking the first of these so far. Now we encounter the conditional type that chooses between them:\\n\\n```ts\\nTQueries[ArrayElement] extends { select: infer TSelect }\\n      ? TSelect extends (data: any) => any\\n        ? ReturnType<TSelect>\\n        : never\\n      : Awaited< /*...*/ >\\n  >\\n```\\n\\nWhat\'s happening here is:\\n\\n- if a query includes a `select` option, we infer what that is and then subsequently extract the return type of the `select`.\\n- otherwise we use the query return type (as we we\'ve previously examined)\\n\\nFinally, whichever type we end up with, we supply that type as a parameter to `UseQueryResult`. And that is what is going to surface up our types to our users.\\n\\n## Usage\\n\\nSo what does using our `useQueriesTyped` hook look like?\\n\\nWell, supplying `queryFn`s with different signatures looks like this:\\n\\n```ts\\nconst result = useQueriesTyped(\\n  { queryKey: \'key1\', queryFn: () => 1 },\\n  { queryKey: \'key2\', queryFn: () => \'two\' },\\n);\\n// const result: [QueryObserverResult<number, unknown>, QueryObserverResult<string, unknown>]\\n\\nif (result[0].data) {\\n  // number\\n}\\nif (result[1].data) {\\n  // string\\n}\\n```\\n\\nAs you can see, we\'re being returned a `Tuple` and the exact types are flowing through.\\n\\nNext let\'s look at a `.map` example with identical types in our supplied array:\\n\\n```ts\\nconst resultWithAllTheSameTypes = useQueriesTyped(\\n  ...[1, 2].map((x) => ({ queryKey: `${x}`, queryFn: () => x })),\\n);\\n// const resultWithAllTheSameTypes: QueryObserverResult<number, unknown>[]\\n\\nif (resultWithAllTheSameTypes[0].data) {\\n  // number\\n}\\n```\\n\\nThe return type of `number` is flowing through for each element.\\n\\nFinally let\'s look at how `.map` handles arrays with different types of elements:\\n\\n```ts\\nconst resultWithDifferentTypes = useQueriesTyped(\\n  ...[1, \'two\', new Date()].map((x) => ({\\n    queryKey: `${x}`,\\n    queryFn: () => x,\\n  })),\\n);\\n//const resultWithDifferentTypes: QueryObserverResult<string | number | Date, unknown>[]\\n\\nif (resultWithDifferentTypes[0].data) {\\n  // string | number | Date\\n}\\n\\nif (resultWithDifferentTypes[1].data) {\\n  // string | number | Date\\n}\\n\\nif (resultWithDifferentTypes[2].data) {\\n  // string | number | Date\\n}\\n```\\n\\nAdmittedly this last example is a somewhat unlikely scenario. But again we can see the types flowing through - though further narrowing would be required here to get to the exact type.\\n\\n## In the box?\\n\\nIt\'s great that we can wrap `useQueries` to get a strongly typed experience. It would be tremendous if this functionality was available by default. [There\'s a discussion going on around this](https://github.com/tannerlinsley/react-query/pull/1527). It\'s possible that this wrapper may no longer need to exist, and that would be amazing. In the meantime; enjoy!"},{"id":"create-react-app-with-ts-loader-and-craco","metadata":{"permalink":"/create-react-app-with-ts-loader-and-craco","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2021-01-02-create-react-app-with-ts-loader-and-craco/index.md","source":"@site/blog/2021-01-02-create-react-app-with-ts-loader-and-craco/index.md","title":"Create React App with ts-loader and CRACO","description":"Create React App now supports TypeScript with React, using Babel webpack loader or `ts-loader`. You can use CRACO to customize configurations.","date":"2021-01-02T00:00:00.000Z","tags":[{"inline":false,"label":"fork-ts-checker-webpack-plugin","permalink":"/tags/fork-ts-checker-webpack-plugin","description":"The fork-ts-checker-webpack-plugin for webpack."},{"inline":false,"label":"ts-loader","permalink":"/tags/ts-loader","description":"The TypeScript loader for webpack."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":3.41,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"create-react-app-with-ts-loader-and-craco","title":"Create React App with ts-loader and CRACO","authors":"johnnyreilly","tags":["fork-ts-checker-webpack-plugin","ts-loader","typescript"],"hide_table_of_contents":false,"description":"Create React App now supports TypeScript with React, using Babel webpack loader or `ts-loader`. You can use CRACO to customize configurations."},"unlisted":false,"prevItem":{"title":"react-query: strongly typing useQueries","permalink":"/strongly-typing-react-query-s-usequeries"},"nextItem":{"title":"Azure Pipelines meet Jest","permalink":"/azure-pipelines-meet-jest"}},"content":"[Create React App](https://create-react-app.dev/) is a fantastic way to get up and running building a web app with React. It also supports using TypeScript with React. Simply entering the following:\\n\\n\x3c!--truncate--\x3e\\n\\n```shell\\nnpx create-react-app my-app --template typescript\\n```\\n\\nWill give you a great TypeScript React project to get building with. There\'s two parts to the TypeScript support that exist:\\n\\n1. Transpilation AKA \\"turning our TypeScript into JavaScript\\". Back since [Babel 7 launched, Babel has enjoyed great support for transpiling TypeScript into JavaScript](https://devblogs.microsoft.com/typescript/typescript-and-babel-7/). Create React App leverages this; using the Babel webpack loader, [babel-loader](https://github.com/babel/babel-loader), for transpilation.\\n2. Type checking AKA \\"seeing if our code compiles\\". Create React App uses the [`fork-ts-checker-webpack-plugin`](https://github.com/TypeStrong/fork-ts-checker-webpack-plugin) to run the TypeScript type checker on a separate process and report any issues that may exist.\\n\\nThis is a great setup and works very well for the majority of use cases. However, what if we\'d like to tweak this setup? What if we\'d like to swap out `babel-loader` for `ts-loader` for compilation purposes? Can we do that?\\n\\nYes you can! And that\'s what we\'re going to do using a tool named [`CRACO`](https://github.com/gsoft-inc/craco) \\\\- the pithy shortening of \\"Create React App Configuration Override\\". This is a tool that allows us to:\\n\\n> Get all the benefits of create-react-app and customization without using \'eject\' by adding a single `craco.config.js` file at the root of your application and customize your eslint, babel, postcss configurations and many more.\\n\\n## ~~`babel-loader`~~ `ts-loader`\\n\\nSo let\'s do the swap. First of all we\'re going to need to add `CRACO` and `ts-loader` to our project:\\n\\n```shell\\nnpm install @craco/craco ts-loader --save-dev\\n```\\n\\nThen we\'ll swap over our various `scripts` in our `package.json` to use `CRACO`:\\n\\n```json\\n\\"start\\": \\"craco start\\",\\n\\"build\\": \\"craco build\\",\\n\\"test\\": \\"craco test\\",\\n```\\n\\nFinally we\'ll add a `craco.config.js` file to the root of our project. This is where we swap out `babel-loader` for `ts-loader`:\\n\\n```js\\nconst {\\n  addAfterLoader,\\n  removeLoaders,\\n  loaderByName,\\n  getLoaders,\\n  throwUnexpectedConfigError,\\n} = require(\'@craco/craco\');\\n\\nconst throwError = (message) =>\\n  throwUnexpectedConfigError({\\n    packageName: \'craco\',\\n    githubRepo: \'gsoft-inc/craco\',\\n    message,\\n    githubIssueQuery: \'webpack\',\\n  });\\n\\nmodule.exports = {\\n  webpack: {\\n    configure: (webpackConfig, { paths }) => {\\n      const { hasFoundAny, matches } = getLoaders(\\n        webpackConfig,\\n        loaderByName(\'babel-loader\'),\\n      );\\n      if (!hasFoundAny) throwError(\'failed to find babel-loader\');\\n\\n      console.log(\'removing babel-loader\');\\n      const { hasRemovedAny, removedCount } = removeLoaders(\\n        webpackConfig,\\n        loaderByName(\'babel-loader\'),\\n      );\\n      if (!hasRemovedAny) throwError(\'no babel-loader to remove\');\\n      if (removedCount !== 2)\\n        throwError(\'had expected to remove 2 babel loader instances\');\\n\\n      console.log(\'adding ts-loader\');\\n\\n      const tsLoader = {\\n        test: /\\\\.(js|mjs|jsx|ts|tsx)$/,\\n        include: paths.appSrc,\\n        loader: require.resolve(\'ts-loader\'),\\n        options: { transpileOnly: true },\\n      };\\n\\n      const { isAdded: tsLoaderIsAdded } = addAfterLoader(\\n        webpackConfig,\\n        loaderByName(\'url-loader\'),\\n        tsLoader,\\n      );\\n      if (!tsLoaderIsAdded) throwError(\'failed to add ts-loader\');\\n      console.log(\'added ts-loader\');\\n\\n      console.log(\'adding non-application JS babel-loader back\');\\n      const { isAdded: babelLoaderIsAdded } = addAfterLoader(\\n        webpackConfig,\\n        loaderByName(\'ts-loader\'),\\n        matches[1].loader, // babel-loader\\n      );\\n      if (!babelLoaderIsAdded)\\n        throwError(\'failed to add back babel-loader for non-application JS\');\\n      console.log(\'added non-application JS babel-loader back\');\\n\\n      return webpackConfig;\\n    },\\n  },\\n};\\n```\\n\\nSo what\'s happening here? The script looks for `babel-loader` usages in the default Create React App config. There will be two; one for TypeScript / JavaScript application code (we want to replace this) and one for non application JavaScript code. I\'m actually not too clear what non application JavaScript code there is or can be, but we\'ll leave it in place; it may be important.\\n\\nYou cannot remove a _single_ loader using `CRACO`, so instead we\'ll remove both and we\'ll add back the non application JavaScript `babel-loader`. We\'ll also add `ts-loader` with the `transpileOnly: true` option set (to ensure `ts-loader` doesn\'t do type checking).\\n\\nNow the next time we run `npm start` we\'ll have Create React App running using `ts-loader` and _without_ having ejected. If we want to adjust the options of `ts-loader` further then we\'re completely at liberty to do so, adjusting the `options` in our `craco.config.js`.\\n\\nIf you value debugging your original source code rather than the transpiled JavaScript, remember to set the `\\"sourceMap\\": true` property in your `tsconfig.json`.\\n\\nFinally, if we wanted to go even further, we could remove the `fork-ts-checker-webpack-plugin` and move `ts-loader` to use `transpileOnly: false` so it performs type checking also. However, generally it may be better to stay with the setup with post outlines for performance reasons."},{"id":"azure-pipelines-meet-jest","metadata":{"permalink":"/azure-pipelines-meet-jest","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2020-12-30-azure-pipelines-meet-jest/index.md","source":"@site/blog/2020-12-30-azure-pipelines-meet-jest/index.md","title":"Azure Pipelines meet Jest","description":"Learn how to integrate Jest with Azure Pipelines to run tests as a part of your pipeline and utilize results reporting in the Azure Pipelines UI.","date":"2020-12-30T00:00:00.000Z","tags":[{"inline":false,"label":"Azure Pipelines","permalink":"/tags/azure-pipelines","description":"The Azure Pipelines CI / CD service."},{"inline":false,"label":"Automated Testing","permalink":"/tags/automated-testing","description":"How to perform the automation of tests."}],"readingTime":3.32,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-pipelines-meet-jest","title":"Azure Pipelines meet Jest","authors":"johnnyreilly","image":"./test-results.webp","tags":["azure pipelines","automated testing"],"hide_table_of_contents":false,"description":"Learn how to integrate Jest with Azure Pipelines to run tests as a part of your pipeline and utilize results reporting in the Azure Pipelines UI."},"unlisted":false,"prevItem":{"title":"Create React App with ts-loader and CRACO","permalink":"/create-react-app-with-ts-loader-and-craco"},"nextItem":{"title":"dotnet-format: Prettier your C# with lint-staged & husky","permalink":"/prettier-your-csharp-with-dotnet-format-and-lint-staged"}},"content":"This post explains how to integrate the tremendous test runner [Jest](https://jestjs.io/) with the continuous integration platform [Azure Pipelines](https://azure.microsoft.com/en-gb/services/devops/pipelines/?nav=min). Perhaps we\'re setting up a new project and we\'ve created a new React app with [Create React App](https://create-react-app.dev/). This ships with Jest support out of the box. How do we get that plugged into Pipelines such that:\\n\\n1. Tests run as part of our pipeline\\n2. A failing test fails the build\\n3. Test results reported in Azure Pipelines UI\\n\\nRelated: there is a [post on Vitest and Azure Pipelines](../2023-08-05-azure-pipelines-meet-vitest/index.md).\\n\\n\x3c!--truncate--\x3e\\n\\n## Tests run as part of our pipeline\\n\\nFirst of all, lets get the tests running. Crack open your `azure-pipelines.yml` file and, in the appropriate place add the following:\\n\\n```yml\\n- task: Npm@1\\n  displayName: npm run test\\n  inputs:\\n    command: \'custom\'\\n    workingDir: \'src/client-app\'\\n    customCommand: \'run test\'\\n```\\n\\nThe above will, when run, trigger a `npm run test` in the `src/client-app` folder of my project (it\'s here where my React app lives). You\'d imagine this would just work\u2122\uFE0F - but life is not that simple. This is because Jest, by default, runs in watch mode. This is blocking and so not appropriate for CI.\\n\\nIn our `src/client-app/package.json` let\'s create a new script that runs the tests but _not_ in watch mode:\\n\\n```json\\n\\"test:ci\\": \\"npm run test -- --watchAll=false\\",\\n```\\n\\nand switch our `azure-pipelines.yml` to use it:\\n\\n```yml\\n- task: Npm@1\\n  displayName: npm run test\\n  inputs:\\n    command: \'custom\'\\n    workingDir: \'src/client-app\'\\n    customCommand: \'run test:ci\'\\n```\\n\\nBoom! We\'re now running tests as part of our pipeline. And also, failing tests will fail the build, because of Jest\'s default behaviour of exiting with status code 1 on failed tests.\\n\\n## Tests results are reported in Azure Pipelines UI\\n\\nPipelines has a really nice UI for reporting test results. If you\'re using something like .NET then you\'ll find that test results just magically show up there. We\'d like that for our Jest tests as well. And we can have it.\\n\\nThe way we achieve this is by:\\n\\n1. Producing test results in a format that can be subsequently processed\\n2. Using those test results to publish to Azure Pipelines\\n\\nThe way that you configure Jest test output is through usage of [`reporters`](https://jestjs.io/docs/en/cli#--reporters). However, Create React App doesn\'t support these. However that\'s not an issue, as the marvellous [Dan Abramov](https://twitter.com/dan_abramov) demonstrates [here](https://github.com/facebook/create-react-app/issues/2474#issuecomment-306340526).\\n\\nWe need to install the [`jest-junit`](https://github.com/jest-community/jest-junit) package to our `client-app`:\\n\\n```\\nnpm install jest-junit --save-dev\\n```\\n\\nAnd we\'ll tweak our `test:ci` script to use the `jest-junit` reporter as well:\\n\\n```json\\n\\"test:ci\\": \\"npm run test -- --watchAll=false --reporters=default --reporters=jest-junit\\",\\n```\\n\\nWe also need to add some configuration to our `package.json` in the form of a `jest-junit` element:\\n\\n```json\\n\\"jest-junit\\": {\\n        \\"suiteNameTemplate\\": \\"{filepath}\\",\\n        \\"outputDirectory\\": \\".\\",\\n        \\"outputName\\": \\"junit.xml\\"\\n    }\\n```\\n\\nThe above configuration will use the name of the test file as the suite name in the results, which should speed up the tracking down of the failing test. The other values specify where the test results should be published to, in this case the root of our `client-app` with the filename `junit.xml`.\\n\\nNow our CI is producing our test results, how do we get them into Pipelines? For that we need the [Publish test results task](https://docs.microsoft.com/en-us/azure/devops/pipelines/tasks/test/publish-test-results?view=azure-devops&tabs=trx%2Cyaml) and a new step in our `azure-pipelines.yml` _after_ our `npm run test` step:\\n\\n```yml\\n- task: Npm@1\\n  displayName: npm run test\\n  inputs:\\n    command: \'custom\'\\n    workingDir: \'src/client-app\'\\n    customCommand: \'run test:ci\'\\n\\n- task: PublishTestResults@2\\n  displayName: \'supply npm test results to pipelines\'\\n  condition: succeededOrFailed() # because otherwise we won\'t know what tests failed\\n  inputs:\\n    testResultsFiles: \'src/client-app/junit.xml\'\\n```\\n\\nThis will read the test results from our `src/client-app/junit.xml` file and pump them into Pipelines. Do note that we\'re _always_ running this step; so if the previous step failed (as it would in the case of a failing test) we still pump out the details of what that failure was. Like so:\\n\\n![screenshot of test results being published to Azure Pipelines regardless of passing or failing tests](test-and-publish-steps.png)\\n\\nAnd that\'s it! Azure Pipelines and Jest integrated.\\n\\n![screenshot of test results published to Azure Pipelines](test-results.webp)"},{"id":"prettier-your-csharp-with-dotnet-format-and-lint-staged","metadata":{"permalink":"/prettier-your-csharp-with-dotnet-format-and-lint-staged","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2020-12-22-prettier-your-csharp-with-dotnet-format-and-lint-staged/index.md","source":"@site/blog/2020-12-22-prettier-your-csharp-with-dotnet-format-and-lint-staged/index.md","title":"dotnet-format: Prettier your C# with lint-staged & husky","description":"Standardise C# formatting with `dotnet format` and `lint-staged`. Customise formatting and integrate with `husky` in this guide.","date":"2020-12-22T00:00:00.000Z","tags":[],"readingTime":4.305,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"prettier-your-csharp-with-dotnet-format-and-lint-staged","title":"dotnet-format: Prettier your C# with lint-staged & husky","authors":"johnnyreilly","image":"./title-image.png","tags":[],"hide_table_of_contents":false,"description":"Standardise C# formatting with `dotnet format` and `lint-staged`. Customise formatting and integrate with `husky` in this guide."},"unlisted":false,"prevItem":{"title":"Azure Pipelines meet Jest","permalink":"/azure-pipelines-meet-jest"},"nextItem":{"title":"Make Microsoft.Identity.Web respond with 403 forbidden instead of a 302 redirect","permalink":"/how-to-make-azure-ad-403"}},"content":"Consistent formatting in a codebase is a good thing. We can achieve this in dotnet using `dotnet format`, used in combination with the npm packages `husky` and `lint-staged`. This post shows how.\\n\\n![title image reading \\"dotnet-format: Prettier your CSharp with lint-staged and husky\\" and the dotnet-format logo](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\nIf you\'re interested in formatting, you might be interested in **linting**. Whilst we use ESLint in JavaScript, there\'s Roslyn Analyzers for C# and you can [read about it here](../2022-04-06-eslint-your-csharp-in-vs-code-with-roslyn-analyzers/index.md).\\n\\n## Why format?\\n\\nConsistent formatting makes code less confusing to newcomers and it allows whoever is working on the codebase to reliably focus on the task at hand. Not \\"fixing curly braces because Janice messed them up with her last commit\\". (A `git commit` message that would be tragic in so many ways.)\\n\\nOnce we\'ve agreed that we want to have consistent formatting, we want it to be enforced. Enter, stage left, [Prettier](https://prettier.io/), the fantastic tool for formatting code. It rocks; I\'ve been using on my JavaScript / TypeScript for the longest time. But what about C#? Well, there is a [Prettier plugin for C#](https://github.com/warrenseine/prettier-plugin-csharp).... Sort of. It appears to be abandoned and contains the worrying message in the `README/index.md`:\\n\\n> Please note that this plugin is under active development, and might not be ready to run on production code yet. It will break your code.\\n\\nNot a ringing endorsement.\\n\\n## `dotnet-format`: a new hope\\n\\n[Margarida Pereira](https://twitter.com/margaridagp) recently pointed me in the direction of [`dotnet-format`](https://github.com/dotnet/format) which is a formatter for .NET. It\'s a .NET tool which:\\n\\n> is a code formatter for dotnet that applies style preferences to a project or solution. Preferences will be read from an `.editorconfig` file, if present, otherwise a default set of preferences will be used.\\n\\nIt can be installed with:\\n\\n```shell\\ndotnet tool install -g dotnet-format\\n```\\n\\nThe [VS Code C# extension will make use of this formatter](https://github.com/dotnet/format/issues/648#issuecomment-614905524), we just need to set the following in our `settings.json`:\\n\\n```json\\n\\"omnisharp.enableRoslynAnalyzers\\": true,\\n\\"omnisharp.enableEditorConfigSupport\\": true\\n```\\n\\n## Customising our formatting\\n\\nIf we\'d like to deviate from the [default formatting options](https://docs.microsoft.com/en-us/dotnet/fundamentals/code-analysis/code-style-rule-options) then create ourselves an `.editorconfig` file in the root of our project. Let\'s say we prefer more of the [K & R style](https://en.wikipedia.org/wiki/Indentation_style#K&R_style) approach to braces instead of the C# default of [Allman style](https://en.wikipedia.org/wiki/Indentation_style#Allman_style). To make `dotnet-format` use that we\'d set the following:\\n\\n```ini\\n# Remove the line below if you want to inherit .editorconfig settings from higher directories\\nroot = true\\n\\n# See https://github.com/dotnet/format/blob/master/docs/Supported-.editorconfig-options/index.md for reference\\n[*.cs]\\ncsharp_new_line_before_open_brace = none\\ncsharp_new_line_before_catch = false\\ncsharp_new_line_before_else = false\\ncsharp_new_line_before_finally = false\\ncsharp_new_line_before_members_in_anonymous_types = false\\ncsharp_new_line_before_members_in_object_initializers = false\\ncsharp_new_line_between_query_expression_clauses = true\\n```\\n\\nWith this in place it\'s K & R all the way baby!\\n\\n## `lint-staged` / `husky` integration\\n\\nIt\'s become somewhat standard to use the marvellous [`husky`](https://github.com/typicode/husky) and [`lint-staged`](https://github.com/okonet/lint-staged) to enforce code quality. To quote the docs:\\n\\n> Run linters against staged git files and don\'t let \uD83D\uDCA9 slip into our code base!\\n\\nTo add this to our (otherwise C# codebase), we\'re going to need a `package.json` file:\\n\\n```sh\\nnpm init --yes\\n```\\n\\nWe\'ll install `husky` and `lint-staged`:\\n\\n```sh\\nnpx husky-init && npm install\\nnpm install lint-staged --save-dev\\n```\\n\\nWe should have a new file living at `.husky/pre-commit` which is our pre-commit hook.\\n\\nWithin that file we should replace `npm test` with `npx lint-staged --relative`. This is the command that will be run on commit. `lint-staged` will be run and we\'re specifying `relative` so that **relative** file paths will be used. This is important as `dotnet format`\'s `--include` accepts \\"a list of relative file or folder paths to include in formatting\\". **Absolute paths (the default) won\'t work - and if we pass them to `dotnet format`, it will not format the files.**\\n\\nFinally we add the following entry to the `package.json`:\\n\\n```json\\n  \\"lint-staged\\": {\\n    \\"*.cs\\": \\"dotnet format --include\\"\\n  }\\n```\\n\\nThis is the task that will be invoked by `lint-staged` against files with a `.cs` suffix on commit. When `lint-staged` runs, it will pass a list of relative file paths to `dotnet format`. So if we\'d staged two files it might end up executing a command like this:\\n\\n`dotnet format --include src/server-app/Server/Controllers/UserController.cs src/server-app/Server/Controllers/WeatherForecastController.cs`\\n\\nWe should end up with a `package.json` that looks something like this:\\n\\n```json\\n{\\n  \\"name\\": \\"app\\",\\n  \\"version\\": \\"1.0.0\\",\\n  \\"description\\": \\"[![Shared Build Status](https://dev.azure.com/investec/maas/_apis/build/status/shared?repoName=maas)](https://dev.azure.com/investec/maas/_build/latest?definitionId=1128&repoName=maas)\\",\\n  \\"main\\": \\"index.js\\",\\n  \\"dependencies\\": {\\n    \\"husky\\": \\"^7.0.2\\"\\n  },\\n  \\"devDependencies\\": {\\n    \\"husky\\": \\"^7.0.0\\",\\n    \\"lint-staged\\": \\"^11.1.2\\"\\n  },\\n  \\"scripts\\": {\\n    \\"test\\": \\"echo \\\\\\"Error: no test specified\\\\\\" && exit 1\\",\\n    \\"prepare\\": \\"husky install\\"\\n  },\\n  \\"lint-staged\\": {\\n    \\"*.cs\\": \\"dotnet format --include\\"\\n  },\\n  \\"repository\\": {\\n    \\"type\\": \\"git\\",\\n    \\"url\\": \\"https://investec@dev.azure.com/investec/maas/_git/maas\\"\\n  },\\n  \\"keywords\\": [],\\n  \\"author\\": \\"\\",\\n  \\"license\\": \\"ISC\\"\\n}\\n```\\n\\nBy and large we don\'t have to think about this; the important take home is that we\'re now enforcing standardised formatting for all C# files upon commit. Everything that goes into the codebase will be formatted in a consistent fashion.\\n\\n## CSharpier - update 16/05/2021\\n\\nThere is an alternative to the CSharp Prettier project. It\'s being worked on by\\n[Bela VanderVoort](https://github.com/belav) and it goes by the name of [csharpier](https://github.com/belav/csharpier). When comparing CSharpier and dotnet-format, Bela put it like this:\\n\\n> I could see CSharpier being the non-configurable super opinionated formatter and dotnet-format being for the people that do want to have options.\\n\\nCheck it out!"},{"id":"how-to-make-azure-ad-403","metadata":{"permalink":"/how-to-make-azure-ad-403","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2020-12-21-how-to-make-azure-ad-403/index.md","source":"@site/blog/2020-12-21-how-to-make-azure-ad-403/index.md","title":"Make Microsoft.Identity.Web respond with 403 forbidden instead of a 302 redirect","description":"The `Microsoft.Identity.Web` library redirects to AccessDenied with a 302 (redirect) status code. Learn to return a 403 (forbidden) status code instead.","date":"2020-12-21T00:00:00.000Z","tags":[{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."},{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."}],"readingTime":2.71,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"how-to-make-azure-ad-403","title":"Make Microsoft.Identity.Web respond with 403 forbidden instead of a 302 redirect","authors":"johnnyreilly","tags":["asp.net","azure"],"image":"./Forbidden.webp","hide_table_of_contents":false,"description":"The `Microsoft.Identity.Web` library redirects to AccessDenied with a 302 (redirect) status code. Learn to return a 403 (forbidden) status code instead."},"unlisted":false,"prevItem":{"title":"dotnet-format: Prettier your C# with lint-staged & husky","permalink":"/prettier-your-csharp-with-dotnet-format-and-lint-staged"},"nextItem":{"title":"Nullable reference types; CSharp\'s very own strictNullChecks","permalink":"/nullable-reference-types-csharp-strictnullchecks"}},"content":"By default `Microsoft.Identity.Web` responds to unauthorized requests with a 302 (redirect). Do you want a 403 (forbidden) instead? Here\'s how.\\n\\n\x3c!--truncate--\x3e\\n\\nIf you\'re using the tremendous [Azure Active Directory for authentication with ASP.NET](https://docs.microsoft.com/en-us/azure/active-directory/develop/scenario-web-app-sign-user-app-configuration?tabs=aspnetcore) then there\'s a good chance you\'re using the [`Microsoft.Identity.Web`](https://github.com/AzureAD/microsoft-identity-web) library. It\'s this that allows us to drop the following statement into the `ConfigureServices` method of our `Startup` class:\\n\\n```cs\\nservices.AddMicrosoftIdentityWebAppAuthentication(Configuration);\\n```\\n\\nWhich (combined with configuration in our `appsettings.json` files) hooks us up with Azure AD for authentication. This is 95% awesome. The 5% is what we\'re here for. Here\'s a screenshot of the scenario that troubles us:\\n\\n![a screenshot of Chrome Devtools showing a 302](AccessDenied.webp)\\n\\nWe\'ve made a request to `/WeatherForecast`; a secured endpoint (a controller decorated with the `Authorize` attribute). We\'re authenticated; the app knows who we are. But we\'re not authorized / allowed to access this endpoint. We don\'t have permission. The HTTP specification caters directly for this scenario with [status code `403 Forbidden`](https://tools.ietf.org/html/rfc7231#section-6.5.3):\\n\\n> The 403 (Forbidden) status code indicates that the server understood the request but refuses to authorize it.\\n\\nHowever, `Microsoft.Identity.Web` is ploughing another furrow. Instead of returning `403`, it\'s returning `302 Found` and redirecting the browser to `https://localhost:5001/Account/AccessDenied?ReturnUrl=%2FWeatherForecast`. Now the intentions here are _great_. If you wanted to implement a page in your application at that endpoint that displayed some kind of useful message it would be really useful. However, what if you want the more HTTP-y behaviour instead? In the case of a HTTP request triggered by JavaScript (typical for Single Page Applications) then this redirect isn\'t that helpful. JavaScript doesn\'t really know what to do with the `302` and whilst you could code around this, it\'s not desirable.\\n\\nWe want `403` - we don\'t want `302`.\\n\\n## Give us `403`\\n\\nYou can have this behaviour by dropping the following code after your `services.AddMicrosoftIdentityWebAppAuthentication`:\\n\\n```cs\\nservices.Configure<CookieAuthenticationOptions>(CookieAuthenticationDefaults.AuthenticationScheme, options =>\\n{\\n    options.Events.OnRedirectToAccessDenied = new Func<RedirectContext<CookieAuthenticationOptions>, Task>(context =>\\n    {\\n        context.Response.StatusCode = StatusCodes.Status403Forbidden;\\n        return context.Response.CompleteAsync();\\n    });\\n});\\n```\\n\\nThis code hijacks the redirect to AccessDenied and transforms it into a `403` instead. Tremendous! What does this look like?\\n\\n![a screenshot of Chrome Devtools showing a 403](Forbidden.webp)\\n\\nThis is the behaviour we want!\\n\\n## Extra customisation bonus points\\n\\nYou may want to have some nuance to the way you handle unauthorized requests. Because of the nature of `OnRedirectToAccessDenied` this is entirely possible; you have complete access to the requests coming in which you can use to direct behaviour. To take a single example, let\'s say we want to direct normal browsing behaviour (AKA humans clicking about in Chrome) which is not authorized to a given screen, otherwise provide `403`s. What would that look like?\\n\\n```cs\\nservices.Configure<CookieAuthenticationOptions>(CookieAuthenticationDefaults.AuthenticationScheme, options =>\\n{\\n    options.Events.OnRedirectToAccessDenied = new Func<RedirectContext<CookieAuthenticationOptions>, Task>(context =>\\n    {\\n        var isRequestForHtml = context.Request.Headers[\\"Accept\\"].ToString().Contains(\\"text/html\\");\\n        if (isRequestForHtml) {\\n            context.Response.StatusCode = StatusCodes.Status302Found;\\n            context.Response.Headers[\\"Location\\"] = \\"/unauthorized\\";\\n        }\\n        else {\\n            context.Response.StatusCode = StatusCodes.Status403Forbidden;\\n        }\\n\\n        return context.Response.CompleteAsync();\\n    });\\n});\\n```\\n\\nSo above, we check the request `Accept` headers and see if they contain `\\"text/html\\"`; which we\'re using as a signal that the request came from a users browsing. (This may not be bulletproof; better suggestions gratefully received.) If the request does contain a `\\"text/html\\"``Accept` header then we redirect the client to an `/unauthorized` screen, otherwise we return `403` as we did before. Super flexible and powerful!"},{"id":"nullable-reference-types-csharp-strictnullchecks","metadata":{"permalink":"/nullable-reference-types-csharp-strictnullchecks","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2020-12-20-nullable-reference-types-csharp-strictnullchecks/index.md","source":"@site/blog/2020-12-20-nullable-reference-types-csharp-strictnullchecks/index.md","title":"Nullable reference types; CSharp\'s very own strictNullChecks","description":"C# introduces nullable reference types similar to TypeScripts `strictNullChecks`. Enabling raises warnings and solves null reference risks.","date":"2020-12-20T00:00:00.000Z","tags":[{"inline":false,"label":"C#","permalink":"/tags/csharp","description":"The C# programming language."}],"readingTime":3.86,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"nullable-reference-types-csharp-strictnullchecks","title":"Nullable reference types; CSharp\'s very own strictNullChecks","authors":"johnnyreilly","tags":["c#"],"hide_table_of_contents":false,"description":"C# introduces nullable reference types similar to TypeScripts `strictNullChecks`. Enabling raises warnings and solves null reference risks."},"unlisted":false,"prevItem":{"title":"Make Microsoft.Identity.Web respond with 403 forbidden instead of a 302 redirect","permalink":"/how-to-make-azure-ad-403"},"nextItem":{"title":"azure-pipelines-task-lib and isOutput setVariable","permalink":"/azure-pipelines-task-lib-and-isoutput-setvariable"}},"content":"\'Tis the season to play with new compiler settings! I\'m a very keen TypeScript user and have been merrily using [`strictNullChecks`](https://www.typescriptlang.org/docs/handbook/release-notes/typescript-2-0.html#--strictnullchecks) since it shipped. I was dimly aware that C# was also getting a similar feature by the name of [nullable reference types](https://docs.microsoft.com/en-us/dotnet/csharp/tutorials/nullable-reference-types).\\n\\n\x3c!--truncate--\x3e\\n\\nIt\'s only now that I\'ve got round to taking at look at this marvellous feature. I thought I\'d share what moving to nullable reference types looked like for me; and what code changes I found myself making as a consequence.\\n\\n## Turning on nullable reference types\\n\\nTo turn on nullable reference types in a C# project you should pop open the `.csproj` file and ensure it contains a `<Nullable>enable</Nullable>`. So if you had a .NET Core 3.1 codebase it might look like this:\\n\\n```xml\\n<PropertyGroup>\\n    <TargetFramework>netcoreapp3.1</TargetFramework>\\n    <Nullable>enable</Nullable>\\n</PropertyGroup>\\n```\\n\\nWhen you compile from this point forward, possible null reference types are reported as warnings. Consider this C#:\\n\\n```cs\\n[ApiController]\\npublic class UserController : ControllerBase\\n{\\n    private readonly ILogger<UserController> _logger;\\n\\n    public UserController(ILogger<UserController> logger)\\n    {\\n        _logger = logger;\\n    }\\n\\n    [AllowAnonymous]\\n    [HttpGet(\\"UserName\\")]\\n    public string GetUserName()\\n    {\\n        if (User.Identity.IsAuthenticated) {\\n            _logger.LogInformation(\\"{User} is getting their username\\", User.Identity.Name);\\n            return User.Identity.Name;\\n        }\\n\\n        _logger.LogInformation(\\"The user is not authenticated\\");\\n        return null;\\n    }\\n}\\n```\\n\\nA `dotnet build` results in this:\\n\\n```shell\\ndotnet build --configuration release\\n\\nMicrosoft (R) Build Engine version 16.7.1+52cd83677 for .NET\\nCopyright (C) Microsoft Corporation. All rights reserved.\\n\\n  Determining projects to restore...\\n  Restored /Users/jreilly/code/app/src/server-app/Server/app.csproj (in 471 ms).\\nControllers/UserController.cs(38,24): warning CS8603: Possible null reference return. [/Users/jreilly/code/app/src/server-app/Server/app.csproj]\\nControllers/UserController.cs(42,20): warning CS8603: Possible null reference return. [/Users/jreilly/code/app/src/server-app/Server/app.csproj]\\n  app -> /Users/jreilly/code/app/src/server-app/Server/bin/release/netcoreapp3.1/app.dll\\n  app -> /Users/jreilly/code/app/src/server-app/Server/bin/release/netcoreapp3.1/app.Views.dll\\n\\nBuild succeeded.\\n\\nControllers/UserController.cs(38,24): warning CS8603: Possible null reference return. [/Users/jreilly/code/app/src/server-app/Server/app.csproj]\\nControllers/UserController.cs(42,20): warning CS8603: Possible null reference return. [/Users/jreilly/code/app/src/server-app/Server/app.csproj]\\n    2 Warning(s)\\n    0 Error(s)\\n```\\n\\nYou see the two `\\"Possible null reference return.\\"` warnings? Bingo\\n\\n## Fail the build with `WarningsAsErrors`\\n\\nThis is good - information is being surfaced up. But it\'s a warning. I could ignore it. I like compilers to get really up in my face and force me to make a change. I\'m not into warnings; I\'m into errors. Know what works for you. If you\'re similarly minded, you can upgrade nullable reference warnings to errors by tweaking the `.csproj` a touch further. Add yourself a `<WarningsAsErrors>nullable</WarningsAsErrors>` element. So maybe your `.csproj` now looks like this:\\n\\n```xml\\n<PropertyGroup>\\n    <TargetFramework>netcoreapp3.1</TargetFramework>\\n    <Nullable>enable</Nullable>\\n    <WarningsAsErrors>nullable</WarningsAsErrors>\\n</PropertyGroup>\\n```\\n\\nAnd a `dotnet build` will result in this:\\n\\n```shell\\ndotnet build --configuration release\\n\\nMicrosoft (R) Build Engine version 16.7.1+52cd83677 for .NET\\nCopyright (C) Microsoft Corporation. All rights reserved.\\n\\n  Determining projects to restore...\\n  Restored /Users/jreilly/code/app/src/server-app/Server/app.csproj (in 405 ms).\\nControllers/UserController.cs(38,24): error CS8603: Possible null reference return. [/Users/jreilly/code/app/src/server-app/Server/app.csproj]\\nControllers/UserController.cs(42,20): error CS8603: Possible null reference return. [/Users/jreilly/code/app/src/server-app/Server/app.csproj]\\n\\nBuild FAILED.\\n\\nControllers/UserController.cs(38,24): error CS8603: Possible null reference return. [/Users/jreilly/code/app/src/server-app/Server/app.csproj]\\nControllers/UserController.cs(42,20): error CS8603: Possible null reference return. [/Users/jreilly/code/app/src/server-app/Server/app.csproj]\\n    0 Warning(s)\\n    2 Error(s)\\n```\\n\\nYay! Errors!\\n\\n## What do they mean?\\n\\n\\"`Possible null reference return`\\" isn\'t the clearest of errors. What does that actually amount to? Well, it amounts to the compiler saying \\"you\'re a liar! (maybe)\\". Let\'s look again at the code where this error is reported:\\n\\n```cs\\n[AllowAnonymous]\\n[HttpGet(\\"UserName\\")]\\npublic string GetUserName()\\n{\\n    if (User.Identity.IsAuthenticated) {\\n        _logger.LogInformation(\\"{User} is getting their username\\", User.Identity.Name);\\n        return User.Identity.Name;\\n    }\\n\\n    _logger.LogInformation(\\"The user is not authenticated\\");\\n    return null;\\n}\\n```\\n\\nWe\'re getting that error reported where we\'re returning `null` and where we\'re returning `User.Identity.Name` which _may_ be `null`. And we\'re getting that because as far as the compiler is concerned `string` has changed. Before we turned on nullable reference types the compiler considered `string` to mean `string` _OR_`null`. Now, `string` means `string`.\\n\\nThis is the same sort of behaviour as TypeScripts `strictNullChecks`. With TypeScript, before you turn on `strictNullChecks`, as far as the compiler is concerned, `string` means `string`_OR_`null`_OR_`undefined` (JavaScript didn\'t feel one null-ish value was enough and so has two - don\'t ask). Once `strictNullChecks` is on, `string` means `string`.\\n\\nIt\'s a lot clearer. And that\'s why the compiler is getting antsy. The method signature is `string`, but it can see `null` potentially being returned. It doesn\'t like it. By and large that\'s good. We want the compiler to notice this as that\'s the entire point. We want to catch accidental `null`s before they hit a user. This is _great_! However, what do you do if have a method (as we do) that legitimately returns a `string` or `null`?\\n\\n## Widening the type to include `null`\\n\\nWe change the signature from this:\\n\\n```cs\\npublic string GetUserName()\\n```\\n\\nTo this:\\n\\n```cs\\npublic string? GetUserName()\\n```\\n\\nThat\'s right, the simple addition of `?` marks a reference type (like a string) as potentially being `null`. Adding that means that we\'re potentially returning `null`, but we\'re sure about it; there\'s intention here - it\'s not accidental. Wonderful!"},{"id":"azure-pipelines-task-lib-and-isoutput-setvariable","metadata":{"permalink":"/azure-pipelines-task-lib-and-isoutput-setvariable","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2020-12-09-azure-pipelines-task-lib-and-isoutput-setvariable/index.md","source":"@site/blog/2020-12-09-azure-pipelines-task-lib-and-isoutput-setvariable/index.md","title":"azure-pipelines-task-lib and isOutput setVariable","description":"This is a workaround for custom Azure Pipelines task extension to output variable since the library does not support \\"isOutput=true.\\"","date":"2020-12-09T00:00:00.000Z","tags":[{"inline":false,"label":"Azure Pipelines","permalink":"/tags/azure-pipelines","description":"The Azure Pipelines CI / CD service."}],"readingTime":1.62,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-pipelines-task-lib-and-isoutput-setvariable","title":"azure-pipelines-task-lib and isOutput setVariable","authors":"johnnyreilly","tags":["azure pipelines"],"hide_table_of_contents":false,"description":"This is a workaround for custom Azure Pipelines task extension to output variable since the library does not support \\"isOutput=true.\\""},"unlisted":false,"prevItem":{"title":"Nullable reference types; CSharp\'s very own strictNullChecks","permalink":"/nullable-reference-types-csharp-strictnullchecks"},"nextItem":{"title":"Visual Studio Marketplace: images in Markdown!","permalink":"/images-in-markdown-for-azure-devops-marketplace"}},"content":"Some blog posts are insightful treatises on the future of web development, some are \\"here\'s how I solved my problem\\". This is most assuredly the latter.\\n\\nI\'m writing an [custom pipelines task extension for Azure Pipelines](https://docs.microsoft.com/en-us/azure/devops/extend/develop/add-build-task?view=azure-devops). It\'s written with TypeScript and the [azure-pipelines-task-lib](https://github.com/microsoft/azure-pipelines-task-lib).\\n\\n\x3c!--truncate--\x3e\\n\\nThe pipeline needs to output a variable. Azure Pipelines does that using the `setvariable` command combined with [isOutput=true](https://docs.microsoft.com/en-us/azure/devops/pipelines/process/variables?view=azure-devops&tabs=yaml%2Cbatch#set-a-multi-job-output-variable). This looks something like this: `##vso[task.setvariable variable=myOutputVar;isOutput=true]this is the value\\"`.\\n\\nThe bad news is that the lib [doesn\'t presently support `isOutput=true`](https://github.com/microsoft/azure-pipelines-task-lib/issues/688). Gosh it makes me sad. Hopefully in future it will be resolved. But what now?\\n\\nFor now we can hack ourselves a workaround:\\n\\n```ts\\nimport * as tl from \'azure-pipelines-task-lib/task\';\\nimport * as tcm from \'azure-pipelines-task-lib/taskcommand\';\\nimport * as os from \'os\';\\n\\n/**\\n * Sets a variable which will be output as well.\\n *\\n * @param     name    name of the variable to set\\n * @param     val     value to set\\n * @param     secret  whether variable is secret.  Multi-line secrets are not allowed.  Optional, defaults to false\\n * @returns   void\\n */\\nexport function setOutputVariable(\\n  name: string,\\n  val: string,\\n  secret = false,\\n): void {\\n  // use the implementation of setVariable to set all the internals,\\n  // then subsequently set the output variable manually\\n  tl.setVariable(name, val, secret);\\n\\n  const varValue = val || \'\';\\n\\n  // write the command\\n  // see https://docs.microsoft.com/en-us/azure/devops/pipelines/process/variables?view=azure-devops&tabs=yaml%2Cbatch#set-a-multi-job-output-variable\\n  _command(\\n    \'task.setvariable\',\\n    {\\n      variable: name || \'\',\\n      isOutput: \'true\',\\n      issecret: (secret || false).toString(),\\n    },\\n    varValue,\\n  );\\n}\\n\\nconst _outStream = process.stdout;\\n\\nfunction _writeLine(str: string): void {\\n  _outStream.write(str + os.EOL);\\n}\\n\\nfunction _command(command: string, properties: any, message: string) {\\n  const taskCmd = new tcm.TaskCommand(command, properties, message);\\n  _writeLine(taskCmd.toString());\\n}\\n```\\n\\nThe above is effectively a wrapper for the existing [`setVariable`](https://github.com/microsoft/azure-pipelines-task-lib/blob/90e9cde0e509cba77185a80ef3af2fc898fb026c/node/task.ts#L162). However, once it\'s called into the initial implementation, `setOutputVariable` then writes out the same variable once more, but this time bolting on `isOutput=true`.\\n\\nFinally, I\'ve raised a PR to see if `isOutput` can be added directly to the library. [You can track progress on that here.](https://github.com/microsoft/azure-pipelines-task-lib/pull/691)"},{"id":"images-in-markdown-for-azure-devops-marketplace","metadata":{"permalink":"/images-in-markdown-for-azure-devops-marketplace","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2020-11-28-images-in-markdown-for-azure-devops-marketplace/index.md","source":"@site/blog/2020-11-28-images-in-markdown-for-azure-devops-marketplace/index.md","title":"Visual Studio Marketplace: images in Markdown!","description":"Publish your README/index.md and associated images to Visual Studio Marketplace.","date":"2020-11-28T00:00:00.000Z","tags":[{"inline":false,"label":"Azure DevOps","permalink":"/tags/azure-devops","description":"The Azure DevOps suite of tools."}],"readingTime":2.41,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"images-in-markdown-for-azure-devops-marketplace","title":"Visual Studio Marketplace: images in Markdown!","authors":"johnnyreilly","tags":["azure devops"],"image":"./azure-devops-marketplace.webp","description":"Publish your README/index.md and associated images to Visual Studio Marketplace.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"azure-pipelines-task-lib and isOutput setVariable","permalink":"/azure-pipelines-task-lib-and-isoutput-setvariable"},"nextItem":{"title":"Bulletproof uniq with TypeScript generics (yay code reviews!)","permalink":"/bulletproof-uniq-with-typescript"}},"content":"I\'ve recently found myself developing [custom pipelines task extensions for Azure DevOps](https://docs.microsoft.com/en-us/azure/devops/extend/develop/add-build-task?view=azure-devops). The extensions being developed end up in the [Azure DevOps Marketplace](https://marketplace.visualstudio.com/azuredevops). What you see there when you look at existing extensions is some pretty lovely documentation.\\n\\n![screenshot of a rich Markdown powered screen with images in Visual Studio Marketplace](azure-devops-marketplace.webp)\\n\\n\x3c!--truncate--\x3e\\n\\n## How can our tasks look as lovely?\\n\\nThat, my friends, is the question to answer. Good documentation is key to success. Here\'s the ask: when a custom task is installed it becomes available in the marketplace, we want it to:\\n\\n- contain documentation\\n- that documentation should support images... For a picture, famously, speaks a thousand words\\n\\n## Mark(Down) our manifest\\n\\nTo get documentation showing up in the marketplace, we need to take a look at the `vss-extension.json` file which lies at the root of our extension folder. It\'s a kind of manifest file and is documented [here](https://docs.microsoft.com/en-us/azure/devops/extend/develop/manifest?view=azure-devops).\\n\\n[Tucked away in the docs, you\'ll find mention of a `content` property and the words:](https://docs.microsoft.com/en-us/azure/devops/extend/develop/manifest?view=azure-devops#discovery-attributes)\\n\\n> Dictionary of content files that describe your extension to users... Each file is assumed to be in [GitHub Flavored Markdown format](https://help.github.com/articles/github-flavored-markdown/). The path of each item is the path to the markdown file in the extension. Valid keys: `details`.\\n\\nThis means we can have a Markdown file in our repo which documents our task. To stay consistent with most projects, a solid choice is to use the `README/index.md` that sits in the root of the project to this end.\\n\\nSo the simple addition of this:\\n\\n```json\\n{\\n  //...\\n  \\"content\\": {\\n    \\"details\\": {\\n      \\"path\\": \\"README/index.md\\"\\n    }\\n  }\\n  //...\\n}\\n```\\n\\nGives us documentation in the marketplace. Yay!\\n\\n## Now the images...\\n\\nIf we are referencing images in our `README/index.md` then, as it stands right now, they won\'t show up in the marketplace. It\'ll be broken link city. Imagine some Markdown like this:\\n\\n```md\\n![alt text](images/screenshot.png)\\n```\\n\\nThis is entirely correct and supported, but won\'t work by default. This is because these images need to be specified in the [`files` property](https://docs.microsoft.com/en-us/azure/devops/extend/develop/manifest?view=azure-devops#files) of the `vss-extension.json`.\\n\\n```json\\n{\\n  //...\\n  \\"content\\": {\\n    \\"details\\": {\\n      \\"path\\": \\"README/index.md\\"\\n    }\\n  },\\n  \\"files\\": [\\n    {\\n      \\"path\\": \\"images\\",\\n      \\"addressable\\": true\\n    }\\n  ]\\n  //...\\n}\\n```\\n\\nConsider the above; the `path` of `images` includes everything inside the `images` folder in the task. However, it\'s crucial that the [`\\"addressable\\": true`](https://docs.microsoft.com/en-us/azure/devops/extend/develop/manifest?view=azure-devops#properties-1) is present as well. It\'s this that makes the files in this `path` URL-addressable. And without that, the images won\'t be displayed.\\n\\nThat\'s it! We\'re done! We can have rich, image inclusive, documentation in our custom tasks.\\n\\nA final note: it\'s possible to specify individual files rather than whole paths in the `files` directory and you might want to do that if you\'re being very careful around file size. There is a maximum size for a custom task and it\'s easy to breach it. But by and large I find that \\"allowlisting\\" a single directory is easier."},{"id":"bulletproof-uniq-with-typescript","metadata":{"permalink":"/bulletproof-uniq-with-typescript","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2020-11-14-bulletproof-uniq-with-typescript/index.md","source":"@site/blog/2020-11-14-bulletproof-uniq-with-typescript/index.md","title":"Bulletproof uniq with TypeScript generics (yay code reviews!)","description":"Code reviews provide opportunities for improvement. A developer shares how their colleagues comment led to the creation of a better \u201Cuniq\u201D function.","date":"2020-11-14T00:00:00.000Z","tags":[{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":3.86,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"bulletproof-uniq-with-typescript","title":"Bulletproof uniq with TypeScript generics (yay code reviews!)","authors":"johnnyreilly","tags":["typescript"],"hide_table_of_contents":false,"description":"Code reviews provide opportunities for improvement. A developer shares how their colleagues comment led to the creation of a better \u201Cuniq\u201D function."},"unlisted":false,"prevItem":{"title":"Visual Studio Marketplace: images in Markdown!","permalink":"/images-in-markdown-for-azure-devops-marketplace"},"nextItem":{"title":"Throttling data requests with React Hooks","permalink":"/throttle-data-requests-with-react-hooks"}},"content":"Never neglect the possibilities of a code review. There are times when you raise a PR and all you want is for everyone to hit approve so you can merge, merge and ship, ship! This can be a missed opportunity. For as much as I\'d like to imagine my code is perfect, it\'s patently not. There\'s always scope for improvement.\\n\\n\x3c!--truncate--\x3e\\n\\n## \\"What\'s this?\\"\\n\\nThis week afforded me that opportunity. I was walking through a somewhat complicated PR on a call and someone said \\"what\'s this?\\". They\'d spotted an expression much like this in my code:\\n\\n```ts\\nconst myValues = [...new Set(allTheValuesSupplied)];\\n```\\n\\nWhat is that? Well, it\'s a number of things:\\n\\n1. [It\'s a way to get the unique values in a collection.](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Set#Remove_duplicate_elements_from_the_array)\\n2. It\'s a pro-tip and a coding BMX trick.\\n\\nWhat do I mean? Well, this is indeed a technique for getting the unique values in a collection. But it relies upon you knowing a bunch of things:\\n\\n- [`Set`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Set) contains unique values. If you add multiple identical values, only a single value will be stored.\\n- The [`Set` constructor](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Set/Set) takes [iterable objects](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Iteration_protocols#The_iterable_protocol). This means we can `new` up a `Set` with an array that we want to \\"unique-ify\\" and we will have a `Set` that contains those unique values.\\n- If you want to go on to do filtering / mapping etc on your unique values, you\'ll need to get them out of the `Set`. This is because (regrettably) ECMAScript iterables don\'t implicitly support these operations and neither are methods such as these part of the `Set` API. The easiest way to do that is to [spread](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Spread_syntax) into a new array which you can then operate upon.\\n\\nI have this knowledge. Lots of people have this knowledge. But whilst this may be the case, using this technique goes against what I would generally consider to be a good tenet of programming: comprehensibility. When you read this code above, it doesn\'t immediately tell you what it\'s doing. This is a strike against it.\\n\\nFurther to that, it\'s \\"noisy\\". Even if the reader does have this knowledge, as they digest the code, they have to mentally unravel it. \\"Oh it\'s a `Set`, we\'re passing in values, then spreading it out, it\'s probably intended to get the unique values.... Right, cool, cool.... Continue!\\"\\n\\n <iframe src=\\"https://giphy.com/embed/4NnSe87mg3h25JYIDh\\" width=\\"100%\\" height=\\"100%\\" frameBorder=\\"0\\" allowFullScreen=\\"\\"></iframe>\\n\\n[Margarida Pereira](https://twitter.com/margaridagp) explicitly called this out and I found myself agreeing. Let\'s make a `uniq` function!\\n\\n## `uniq` v1\\n\\nI wrote a very simple `uniq` function which looked like this:\\n\\n```ts\\n/**\\n * Return the unique values found in the passed iterable\\n */\\nfunction uniq<TElement>(iterableToGetUniqueValuesOf: Iterable<TElement>) {\\n  return [...new Set(iterableToGetUniqueValuesOf)];\\n}\\n```\\n\\nUsage of this was simple:\\n\\n```ts\\nuniq([1, 1, 1, 3, 1, 1, 2]); // produces [1, 3, 2]\\nuniq([\'John\', \'Guida\', \'Ollie\', \'Divya\', \'John\']); // produces [\\"John\\", \\"Guida\\", \\"Ollie\\", \\"Divya\\"]\\n```\\n\\nAnd I thought this was tremendous. I committed and pushed. I assumed there was no more to be done. Guida (Margarida) then made this very helpful comment:\\n\\n> BTW, I found a big bold warning that `new Set()` compares objects by reference (unless they\'re primitives) so it might be worth adding a comment to warn people that uniq/distinct compares objects by reference: [https://codeburst.io/javascript-array-distinct-5edc93501dc4](https://codeburst.io/javascript-array-distinct-5edc93501dc4)\\n\\nShe was right! If a caller was to, say, pass a collection of objects to `uniq` then they\'d end up highly disappointed. Consider:\\n\\n```ts\\nuniq([{ name: \'John\' }, { name: \'John\' }]); // produces [{ name: \\"John\\" }, { name: \\"John\\" }]\\n```\\n\\nWe can do better!\\n\\n## `uniq` v2\\n\\nI like compilers shouting at me. Or more accurately, I like compilers telling me when something isn\'t valid / supported / correct. I wanted `uniq` to mirror the behaviour of `Set` \\\\- to only support primitives such as `string`, `number` etc. So I made a new version of `uniq` that hardened up the generic contraints:\\n\\n```ts\\n/**\\n * Return the unique values found in the passed iterable\\n */\\nfunction uniq<TElement extends string | number | bigint | boolean | symbol>(\\n  iterableToGetUniqueValuesOf: Iterable<TElement>,\\n) {\\n  return [...new Set(iterableToGetUniqueValuesOf)];\\n}\\n```\\n\\nWith this in place, the compiler started shouting in the most helpful way. When I re-attemped `[{ name: \\"John\\" }, { name: \\"John\\" }]` the compiler hit me with:\\n\\n`Argument of type \'{ name: string; }[]\' is not assignable to parameter of type \'Iterable&lt;string | number | bigint | boolean | symbol&gt;\'.`\\n\\n[Take a look.](https://www.typescriptlang.org/play?#code/FAYw9gdgzmA2CmA6WYDmAKArhAlgR3QG0BvAAggEMBbeALlICIApMACwgdIF8AaUsyjXrM2HbgF0AlJNCQYCZGiy4ChEewZ91HKTOAB6AFSHgpQ6QBK8AC6YAThFLXW8UtnyZXANwqxPUUgAzMGwAE1IcR2dXAAcKKCh4cJxreDsKACMEU0N9YEDsEGscSDcVAB4AFQBRBBoIa1J4AA9UiFCAqGs7SNRSAB9yTCoMtIHSDJxUSMbBjLA4eApHQagATxG4AD50U1J9lLTMhEqwAHEbAFUVTwA1X38AeUD6AElU9Kz4Ktr4eustsBJPw9vs7DZ7I5CIgYRB4AB3UgAZRs6EOnxO5yuN3g9z88Cgz0k4gA3MAuMAgA)\\n\\nThis is good. This is descriptive code that only allows legitimate inputs. It should lead to less confusion and a reduced likelihood of issues in Production. It\'s also a nice example of how code review can result in demonstrably better code. Thanks Guida!"},{"id":"throttle-data-requests-with-react-hooks","metadata":{"permalink":"/throttle-data-requests-with-react-hooks","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2020-11-10-throttle-data-requests-with-react-hooks/index.md","source":"@site/blog/2020-11-10-throttle-data-requests-with-react-hooks/index.md","title":"Throttling data requests with React Hooks","description":"A custom React Hook `useThrottleRequests` is used to solve the problem of loading large amounts of data gradually and displaying loading progress.","date":"2020-11-10T00:00:00.000Z","tags":[{"inline":false,"label":"React","permalink":"/tags/react","description":"The React library."}],"readingTime":13.18,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"throttle-data-requests-with-react-hooks","title":"Throttling data requests with React Hooks","authors":"johnnyreilly","tags":["react"],"hide_table_of_contents":false,"description":"A custom React Hook `useThrottleRequests` is used to solve the problem of loading large amounts of data gradually and displaying loading progress."},"unlisted":false,"prevItem":{"title":"Bulletproof uniq with TypeScript generics (yay code reviews!)","permalink":"/bulletproof-uniq-with-typescript"},"nextItem":{"title":"Azure DevOps Client for Node.js - GitApi / WikiApi limitations","permalink":"/azure-devops-node-api-git-api-getrefs-wiki-api"}},"content":"When an application loads data, typically relatively few HTTP requests will be made. For example, if we imagine we\'re making a student administration application, then a \\"view\\" screen might make a single HTTP request to load that student\'s data before displaying it.\\n\\n\x3c!--truncate--\x3e\\n\\nOccasionally there\'s a need for an application to make a large number of HTTP requests. Consider a reporting application which loads data and then aggregates it for presentation purposes.\\n\\nThis need presents two interesting problems to solve:\\n\\n1. how do we load data gradually?\\n2. how do we present loading progress to users?\\n\\nThis post will talk about how we can tackle these and demonstrate using a custom React Hook.\\n\\n## Let\'s bring Chrome to its knees\\n\\nWe\'ll begin our journey by spinning up a TypeScript React app with [Create React App](https://create-react-app.dev/):\\n\\n```shell\\nnpx create-react-app throttle-requests-react-hook --template typescript\\n```\\n\\nBecause we\'re going to be making a number of asynchronous calls, we\'re going to simplify the code by leaning on the widely used [`react-use`](https://github.com/streamich/react-use) for a [`useAsync`](https://github.com/streamich/react-use/blob/master/docs/useAsync/index.md) hook.\\n\\n```shell\\ncd throttle-requests-react-hook\\nyarn add react-use\\n```\\n\\nWe\'ll replace the `App.css` file with this:\\n\\n```css\\n.App {\\n  text-align: center;\\n}\\n\\n.App-header {\\n  background-color: #282c34;\\n  min-height: 100vh;\\n  display: flex;\\n  flex-direction: column;\\n  align-items: center;\\n  justify-content: center;\\n  font-size: calc(10px + 2vmin);\\n  color: white;\\n}\\n\\n.App-labelinput > * {\\n  margin: 0.5em;\\n  font-size: 24px;\\n}\\n\\n.App-link {\\n  color: #61dafb;\\n}\\n\\n.App-button {\\n  font-size: calc(10px + 2vmin);\\n  margin-top: 0.5em;\\n  padding: 1em;\\n  background-color: cornflowerblue;\\n  color: #ffffff;\\n  text-align: center;\\n}\\n\\n.App-progress {\\n  padding: 1em;\\n  background-color: cadetblue;\\n  color: #ffffff;\\n}\\n\\n.App-results {\\n  display: flex;\\n  flex-wrap: wrap;\\n}\\n\\n.App-results > * {\\n  padding: 1em;\\n  margin: 0.5em;\\n  background-color: darkblue;\\n  flex: 1 1 300px;\\n}\\n```\\n\\nThen we\'ll replace the `App.tsx` contents with this:\\n\\n```tsx\\nimport React, { useState } from \'react\';\\nimport { useAsync } from \'react-use\';\\nimport \'./App.css\';\\n\\nfunction use10_000Requests(startedAt: string) {\\n  const responses = useAsync(async () => {\\n    if (!startedAt) return;\\n\\n    // make 10,000 unique HTTP requests\\n    const results = await Promise.all(\\n      Array.from(Array(10_000)).map(async (_, index) => {\\n        const response = await fetch(\\n          `/manifest.json?querystringValueToPreventCaching=${startedAt}_request-${index}`,\\n        );\\n        const json = await response.json();\\n        return json;\\n      }),\\n    );\\n\\n    return results;\\n  }, [startedAt]);\\n\\n  return responses;\\n}\\n\\nfunction App() {\\n  const [startedAt, setStartedAt] = useState(\'\');\\n  const responses = use10_000Requests(startedAt);\\n\\n  return (\\n    <div className=\\"App\\">\\n      <header className=\\"App-header\\">\\n        <h1>The HTTP request machine</h1>\\n        <button\\n          className=\\"App-button\\"\\n          onClick={(_) => setStartedAt(new Date().toISOString())}\\n        >\\n          Make 10,000 requests\\n        </button>\\n        {responses.loading && <div>{progressMessage}</div>}\\n        {responses.error && <div>Something went wrong</div>}\\n        {responses.value && (\\n          <div className=\\"App-results\\">\\n            {responses.value.length} requests completed successfully\\n          </div>\\n        )}\\n      </header>\\n    </div>\\n  );\\n}\\n\\nexport default App;\\n```\\n\\nThe app that we\'ve built is very simple; it\'s a button which, when you press it, fires 10,000 HTTP requests in parallel using the [Fetch API](https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API). The data being requested in this case is an arbitrary JSON file; the `manifest.json`. If you look closely you\'ll see we\'re doing some querystring tricks with our URL to avoid getting cached data.\\n\\nIn fact, for this demo we\'re not interested in the results of these HTTP requests; rather we\'re interested in how the browser copes with this approach. (Spoiler: not well!) It\'s worth considering that requesting a text file from a server running on the same machine as the browser should be fast.\\n\\nSo we\'ll run `yarn start` and go to [http://localhost:3000](http://localhost:3000) to get to the app. Running with Devtools open results in the following unhappy affair:\\n\\n![](i-want-it-all.gif)\\n\\nThe GIF above has been edited significantly for length. In reality it took 20 seconds for the first request to be fired, prior to that Chrome was unresponsive. When requests did start to fire, a significant number failed with `net::ERR_INSUFFICIENT_RESOURCES`. Further to that, those requests that were fired sat in \\"Stalled\\" state prior to being executed. This is a consequence of [Chrome limiting the number of connections - all browsers do this](https://developers.google.com/web/tools/chrome-devtools/network/reference#timing):\\n\\n> There are already six TCP connections open for this origin, which is the limit. Applies to HTTP/1.0 and HTTP/1.1 only.\\n\\nIn summary, the problems with the current approach are:\\n\\n1. the browser becoming unresponsive\\n2. failing HTTP requests due to insufficient resources\\n3. no information displayed to the user around progress\\n\\n## Throttle me this\\n\\nInstead of hammering the browser by firing all the requests at once, we could instead implement a throttle. A throttle is a mechanism which allows you to limit the rate at which operations are performed. In this case we want to limit the rate at which HTTP requests are made. A throttle will tackle problems 1 and 2 - essentially keeping the browser free and easy and ensuring that requests are all successfully sent. We also want to keep our users informed around how progress is going. It\'s time to unveil the `useThrottleRequests` hook:\\n\\n```ts\\nimport { useMemo, useReducer } from \'react\';\\nimport { AsyncState } from \'react-use/lib/useAsync\';\\n\\n/** Function which makes a request */\\nexport type RequestToMake = () => Promise<void>;\\n\\n/**\\n * Given an array of requestsToMake and a limit on the number of max parallel requests\\n * queue up those requests and start firing them\\n * - inspired by Rafael Xavier\'s approach here: https://stackoverflow.com/a/48007240/761388\\n *\\n * @param requestsToMake\\n * @param maxParallelRequests the maximum number of requests to make - defaults to 6\\n */\\nasync function throttleRequests(\\n  requestsToMake: RequestToMake[],\\n  maxParallelRequests = 6,\\n) {\\n  // queue up simultaneous calls\\n  const queue: Promise<void>[] = [];\\n  for (let requestToMake of requestsToMake) {\\n    // fire the async function, add its promise to the queue,\\n    // and remove it from queue when complete\\n    const promise = requestToMake().then((res) => {\\n      queue.splice(queue.indexOf(promise), 1);\\n      return res;\\n    });\\n    queue.push(promise);\\n\\n    // if the number of queued requests matches our limit then\\n    // wait for one to finish before enqueueing more\\n    if (queue.length >= maxParallelRequests) {\\n      await Promise.race(queue);\\n    }\\n  }\\n  // wait for the rest of the calls to finish\\n  await Promise.all(queue);\\n}\\n\\n/**\\n * The state that represents the progress in processing throttled requests\\n */\\nexport type ThrottledProgress<TData> = {\\n  /** the number of requests that will be made */\\n  totalRequests: number;\\n  /** the errors that came from failed requests */\\n  errors: Error[];\\n  /** the responses that came from successful requests */\\n  values: TData[];\\n  /** a value between 0 and 100 which represents the percentage of requests that have been completed (whether successfully or not) */\\n  percentageLoaded: number;\\n  /** whether the throttle is currently processing requests */\\n  loading: boolean;\\n};\\n\\nfunction createThrottledProgress<TData>(\\n  totalRequests: number,\\n): ThrottledProgress<TData> {\\n  return {\\n    totalRequests,\\n    percentageLoaded: 0,\\n    loading: false,\\n    errors: [],\\n    values: [],\\n  };\\n}\\n\\n/**\\n * A reducing function which takes the supplied `ThrottledProgress` and applies a new value to it\\n */\\nfunction updateThrottledProgress<TData>(\\n  currentProgress: ThrottledProgress<TData>,\\n  newData: AsyncState<TData>,\\n): ThrottledProgress<TData> {\\n  const errors = newData.error\\n    ? [...currentProgress.errors, newData.error]\\n    : currentProgress.errors;\\n\\n  const values = newData.value\\n    ? [...currentProgress.values, newData.value]\\n    : currentProgress.values;\\n\\n  const percentageLoaded =\\n    currentProgress.totalRequests === 0\\n      ? 0\\n      : Math.round(\\n          ((errors.length + values.length) / currentProgress.totalRequests) *\\n            100,\\n        );\\n\\n  const loading =\\n    currentProgress.totalRequests === 0\\n      ? false\\n      : errors.length + values.length < currentProgress.totalRequests;\\n\\n  return {\\n    totalRequests: currentProgress.totalRequests,\\n    loading,\\n    percentageLoaded,\\n    errors,\\n    values,\\n  };\\n}\\n\\ntype ThrottleActions<TValue> =\\n  | {\\n      type: \'initialise\';\\n      totalRequests: number;\\n    }\\n  | {\\n      type: \'requestSuccess\';\\n      value: TValue;\\n    }\\n  | {\\n      type: \'requestFailed\';\\n      error: Error;\\n    };\\n\\n/**\\n * Create a ThrottleRequests and an updater\\n */\\nexport function useThrottleRequests<TValue>() {\\n  function reducer(\\n    throttledProgressAndState: ThrottledProgress<TValue>,\\n    action: ThrottleActions<TValue>,\\n  ): ThrottledProgress<TValue> {\\n    switch (action.type) {\\n      case \'initialise\':\\n        return createThrottledProgress(action.totalRequests);\\n\\n      case \'requestSuccess\':\\n        return updateThrottledProgress(throttledProgressAndState, {\\n          loading: false,\\n          value: action.value,\\n        });\\n\\n      case \'requestFailed\':\\n        return updateThrottledProgress(throttledProgressAndState, {\\n          loading: false,\\n          error: action.error,\\n        });\\n    }\\n  }\\n\\n  const [throttle, dispatch] = useReducer(\\n    reducer,\\n    createThrottledProgress<TValue>(/** totalRequests */ 0),\\n  );\\n\\n  const updateThrottle = useMemo(() => {\\n    /**\\n     * Update the throttle with a successful request\\n     * @param values from request\\n     */\\n    function requestSucceededWithData(value: TValue) {\\n      return dispatch({\\n        type: \'requestSuccess\',\\n        value,\\n      });\\n    }\\n\\n    /**\\n     * Update the throttle upon a failed request with an error message\\n     * @param error error\\n     */\\n    function requestFailedWithError(error: Error) {\\n      return dispatch({\\n        type: \'requestFailed\',\\n        error,\\n      });\\n    }\\n\\n    /**\\n     * Given an array of requestsToMake and a limit on the number of max parallel requests\\n     * queue up those requests and start firing them\\n     * - based upon https://stackoverflow.com/a/48007240/761388\\n     *\\n     * @param requestsToMake\\n     * @param maxParallelRequests the maximum number of requests to make - defaults to 6\\n     */\\n    function queueRequests(\\n      requestsToMake: RequestToMake[],\\n      maxParallelRequests = 6,\\n    ) {\\n      dispatch({\\n        type: \'initialise\',\\n        totalRequests: requestsToMake.length,\\n      });\\n\\n      return throttleRequests(requestsToMake, maxParallelRequests);\\n    }\\n\\n    return {\\n      queueRequests,\\n      requestSucceededWithData,\\n      requestFailedWithError,\\n    };\\n  }, [dispatch]);\\n\\n  return {\\n    throttle,\\n    updateThrottle,\\n  };\\n}\\n```\\n\\nThe `useThrottleRequests` hook returns 2 properties:\\n\\n- `throttle` \\\\- a `ThrottledProgress&lt;TData&gt;` that contains the following data:\\n\\n  - `totalRequests` \\\\- the number of requests that will be made\\n  - `errors` \\\\- the errors that came from failed requests\\n  - `values` \\\\- the responses that came from successful requests\\n  - `percentageLoaded` \\\\- a value between 0 and 100 which represents the percentage of requests that have been completed (whether successfully or not)\\n  - `loading` \\\\- whether the throttle is currently processing requests\\n\\n- `updateThrottle` \\\\- an object which exposes 3 functions:\\n\\n  - `queueRequests` \\\\- the function to which you pass the requests that should be queued and executed in a throttled fashion\\n  - `requestSucceededWithData` \\\\- the function which is called if a request succeeds to provide the data\\n  - `requestFailedWithError` \\\\- the function which is called if a request fails to provide the error\\n\\nThat\'s a lot of words to describe our `useThrottleRequests` hook. Let\'s look at what it looks like by migrating our `use10_000Requests` hook to (no pun intended) use it. Here\'s a new implementation of `App.tsx`:\\n\\n```tsx\\nimport React, { useState } from \'react\';\\nimport { useAsync } from \'react-use\';\\nimport { useThrottleRequests } from \'./useThrottleRequests\';\\nimport \'./App.css\';\\n\\nfunction use10_000Requests(startedAt: string) {\\n  const { throttle, updateThrottle } = useThrottleRequests();\\n  const [progressMessage, setProgressMessage] = useState(\'not started\');\\n\\n  useAsync(async () => {\\n    if (!startedAt) return;\\n\\n    setProgressMessage(\'preparing\');\\n\\n    const requestsToMake = Array.from(Array(10_000)).map(\\n      (_, index) => async () => {\\n        try {\\n          setProgressMessage(`loading ${index}...`);\\n\\n          const response = await fetch(\\n            `/manifest.json?querystringValueToPreventCaching=${startedAt}_request-${index}`,\\n          );\\n          const json = await response.json();\\n\\n          updateThrottle.requestSucceededWithData(json);\\n        } catch (error) {\\n          console.error(`failed to load ${index}`, error);\\n          updateThrottle.requestFailedWithError(error);\\n        }\\n      },\\n    );\\n\\n    await updateThrottle.queueRequests(requestsToMake);\\n  }, [startedAt, updateThrottle, setProgressMessage]);\\n\\n  return { throttle, progressMessage };\\n}\\n\\nfunction App() {\\n  const [startedAt, setStartedAt] = useState(\'\');\\n\\n  const { progressMessage, throttle } = use10_000Requests(startedAt);\\n\\n  return (\\n    <div className=\\"App\\">\\n      <header className=\\"App-header\\">\\n        <h1>The HTTP request machine</h1>\\n        <button\\n          className=\\"App-button\\"\\n          onClick={(_) => setStartedAt(new Date().toISOString())}\\n        >\\n          Make 10,000 requests\\n        </button>\\n        {throttle.loading && <div>{progressMessage}</div>}\\n        {throttle.values.length > 0 && (\\n          <div className=\\"App-results\\">\\n            {throttle.values.length} requests completed successfully\\n          </div>\\n        )}\\n        {throttle.errors.length > 0 && (\\n          <div className=\\"App-results\\">\\n            {throttle.errors.length} requests errored\\n          </div>\\n        )}\\n      </header>\\n    </div>\\n  );\\n}\\n\\nexport default App;\\n```\\n\\nLooking at the new `use10_000Requests` hook, there\'s a few subtle differences to our prior implementation. First of all, we\'re now exposing the `throttle`; a `ThrottleProgress&lt;TData&gt;`. Our updated hook also exposes a `progressMessage` which is a simple `string` stored with `useState` that we update as our throttle runs. In truth the information being surfaced here isn\'t that interesting. The `progressMessage` is in place just to illustrate that you could capture some data from your requests as they complete for display purposes; a running total for instance.\\n\\nSo, how does our new hook approach perform?\\n\\n![](i-want-it-all-with-hook.gif)\\n\\nVery well indeed! Please note that the above GIF has again been edited for brevity. If we look back at the problems we faced with the prior approach, how do we compare?\\n\\n1. ~~the browser becoming unresponsive~~ \\\\- the browser remains responsive.\\n2. ~~failing HTTP requests due to insufficient resources~~ \\\\- the browser does not experience failing HTTP requests.\\n3. ~~no information displayable to the user around progress~~ \\\\- details of progress are displayed to the user throughout.\\n\\nTremendous!\\n\\n## What shall we build?\\n\\nOur current example is definitely contrived. Let\'s try and apply our `useThrottleRequests` hook to a more realistic scenario. We\'re going to build an application which, given a repo on GitHub, lists all the contributors blogs. (You can specify a blog URL on your GitHub profile; many people use this to specify their Twitter profile.)\\n\\nWe can build this thanks to the excellent [GitHub REST API](https://docs.github.com/en/free-pro-team@latest/rest). It exposes two endpoints of interest given our goal.\\n\\n### 1\\\\. List repository contributors\\n\\n[List repository contributors](https://docs.github.com/en/free-pro-team@latest/rest/reference/repos#list-repository-contributors) lists contributors to the specified repository at this URL: `GET https://api.github.com/repos/{owner}/{repo}/contributors`. The response is an array of objects, crucially featuring a `url` property that points to the user in question\'s API endpoint:\\n\\n```js\\n[\\n  // ...\\n  {\\n    // ...\\n    url: \'https://api.github.com/users/octocat\',\\n    // ...\\n  },\\n  // ...\\n];\\n```\\n\\n### 2\\\\. Get a user\\n\\n[Get a user](https://docs.github.com/en/free-pro-team@latest/rest/reference/users#get-a-user) is the API that the `url` property above is referring to. When called it returns an object representing the publicly available information about a user:\\n\\n```js\\n{\\n  // ...\\n  \\"name\\": \\"The Octocat\\",\\n  // ...\\n  \\"blog\\": \\"https://github.blog\\",\\n  // ...\\n}\\n```\\n\\n## Blogging devs v1.0\\n\\nWe\'re now ready to build our blogging devs app; let\'s replace the existing `App.tsx` with:\\n\\n```tsx\\nimport React, { useCallback, useMemo, useState } from \'react\';\\nimport { useAsync } from \'react-use\';\\nimport { useThrottleRequests } from \'./useThrottleRequests\';\\nimport \'./App.css\';\\n\\ntype GitHubUser = { name: string; blog?: string };\\n\\nfunction timeout(ms: number) {\\n  return new Promise((resolve) => setTimeout(resolve, ms));\\n}\\n\\nfunction useContributors(contributorsUrlToLoad: string) {\\n  const { throttle, updateThrottle } = useThrottleRequests<GitHubUser>();\\n  const [progressMessage, setProgressMessage] = useState(\'\');\\n\\n  useAsync(async () => {\\n    if (!contributorsUrlToLoad) return;\\n\\n    setProgressMessage(\'loading contributors\');\\n\\n    // load contributors from GitHub\\n    const contributorsResponse = await fetch(contributorsUrlToLoad);\\n    const contributors: { url: string }[] = await contributorsResponse.json();\\n\\n    setProgressMessage(`loading ${contributors.length} contributors...`);\\n\\n    // For each entry in result, retrieve the given user from GitHub\\n    const requestsToMake = contributors.map(({ url }, index) => async () => {\\n      try {\\n        setProgressMessage(\\n          `loading ${index} / ${contributors.length}: ${url}...`,\\n        );\\n\\n        const response = await fetch(url);\\n        const json: GitHubUser = await response.json();\\n\\n        // wait for 1 second before completing the request\\n        // - makes for better demos\\n        await timeout(1000);\\n\\n        updateThrottle.requestSucceededWithData(json);\\n      } catch (error) {\\n        console.error(`failed to load ${url}`, error);\\n        updateThrottle.requestFailedWithError(error);\\n      }\\n    });\\n\\n    await updateThrottle.queueRequests(requestsToMake);\\n\\n    setProgressMessage(\'\');\\n  }, [contributorsUrlToLoad, updateThrottle, setProgressMessage]);\\n\\n  return { throttle, progressMessage };\\n}\\n\\nfunction App() {\\n  // The owner and repo to query; we\'re going to default\\n  // to using DefinitelyTyped as an example repo as it\\n  // is one of the most contributed to repos on GitHub\\n  const [owner, setOwner] = useState(\'DefinitelyTyped\');\\n  const [repo, setRepo] = useState(\'DefinitelyTyped\');\\n  const handleOwnerChange = useCallback(\\n    (event: React.ChangeEvent<HTMLInputElement>) =>\\n      setOwner(event.target.value),\\n    [setOwner],\\n  );\\n  const handleRepoChange = useCallback(\\n    (event: React.ChangeEvent<HTMLInputElement>) => setRepo(event.target.value),\\n    [setRepo],\\n  );\\n\\n  const contributorsUrl = `https://api.github.com/repos/${owner}/${repo}/contributors`;\\n\\n  const [contributorsUrlToLoad, setUrlToLoad] = useState(\'\');\\n  const { progressMessage, throttle } = useContributors(contributorsUrlToLoad);\\n\\n  const bloggers = useMemo(\\n    () => throttle.values.filter((contributor) => contributor.blog),\\n    [throttle],\\n  );\\n\\n  return (\\n    <div className=\\"App\\">\\n      <header className=\\"App-header\\">\\n        <h1>Blogging devs</h1>\\n\\n        <p>\\n          Show me the{\' \'}\\n          <a\\n            className=\\"App-link\\"\\n            href={contributorsUrl}\\n            target=\\"_blank\\"\\n            rel=\\"noopener noreferrer\\"\\n          >\\n            contributors for {owner}/{repo}\\n          </a>{\' \'}\\n          who have blogs.\\n        </p>\\n\\n        <div className=\\"App-labelinput\\">\\n          <label htmlFor=\\"owner\\">GitHub Owner</label>\\n          <input\\n            id=\\"owner\\"\\n            type=\\"text\\"\\n            value={owner}\\n            onChange={handleOwnerChange}\\n          />\\n          <label htmlFor=\\"repo\\">GitHub Repo</label>\\n          <input\\n            id=\\"repo\\"\\n            type=\\"text\\"\\n            value={repo}\\n            onChange={handleRepoChange}\\n          />\\n        </div>\\n\\n        <button\\n          className=\\"App-button\\"\\n          onClick={(e) => setUrlToLoad(contributorsUrl)}\\n        >\\n          Load bloggers from GitHub\\n        </button>\\n\\n        {progressMessage && (\\n          <div className=\\"App-progress\\">{progressMessage}</div>\\n        )}\\n\\n        {throttle.percentageLoaded > 0 && (\\n          <>\\n            <h3>Behold {bloggers.length} bloggers:</h3>\\n            <div className=\\"App-results\\">\\n              {bloggers.map((blogger) => (\\n                <div key={blogger.name}>\\n                  <div>{blogger.name}</div>\\n                  <a\\n                    className=\\"App-link\\"\\n                    href={blogger.blog}\\n                    target=\\"_blank\\"\\n                    rel=\\"noopener noreferrer\\"\\n                  >\\n                    {blogger.blog}\\n                  </a>\\n                </div>\\n              ))}\\n            </div>\\n          </>\\n        )}\\n\\n        {throttle.errors.length > 0 && (\\n          <div className=\\"App-results\\">\\n            {throttle.errors.length} requests errored\\n          </div>\\n        )}\\n      </header>\\n    </div>\\n  );\\n}\\n\\nexport default App;\\n```\\n\\nThe application gives users the opportunity to enter the organisation and repository of a GitHub project. Then, when the button is clicked, it:\\n\\n- loads the contributors\\n- for each contributor it loads the individual user (separate HTTP request for each)\\n- as it loads it communicates how far through the loading progress it has got\\n- as users are loaded, it renders a tile for each user with a listed blog\\n\\nJust to make the demo a little clearer we\'ve artificially slowed the duration of each request by a second. What does it look like when you put it together? Well like this:\\n\\n![](blogging-devs.gif)\\n\\nWe have built a React Hook which allows us to:\\n\\n- gradually load data\\n- without blocking the UI of the browser\\n- and which provides progress data to keep users informed.\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/throttling-data-requests-with-react-hooks/)\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.logrocket.com/throttling-data-requests-with-react-hooks/\\" />\\n</head>"},{"id":"azure-devops-node-api-git-api-getrefs-wiki-api","metadata":{"permalink":"/azure-devops-node-api-git-api-getrefs-wiki-api","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2020-10-31-azure-devops-node-api-git-api-getrefs-wiki-api/index.md","source":"@site/blog/2020-10-31-azure-devops-node-api-git-api-getrefs-wiki-api/index.md","title":"Azure DevOps Client for Node.js - GitApi / WikiApi limitations","description":"The Azure DevOps Node.js client library has limitations and missing features. Workarounds are possible for using Azure DevOps REST API directly.","date":"2020-10-31T00:00:00.000Z","tags":[{"inline":false,"label":"Node.js","permalink":"/tags/node-js","description":"The Node.js runtime."},{"inline":false,"label":"Azure DevOps","permalink":"/tags/azure-devops","description":"The Azure DevOps suite of tools."}],"readingTime":4.695,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-devops-node-api-git-api-getrefs-wiki-api","title":"Azure DevOps Client for Node.js - GitApi / WikiApi limitations","authors":"johnnyreilly","tags":["node.js","azure devops"],"image":"./title-image.png","hide_table_of_contents":false,"description":"The Azure DevOps Node.js client library has limitations and missing features. Workarounds are possible for using Azure DevOps REST API directly."},"unlisted":false,"prevItem":{"title":"Throttling data requests with React Hooks","permalink":"/throttle-data-requests-with-react-hooks"},"nextItem":{"title":"Safari: The Mysterious Case of the Empty Download","permalink":"/safari-empty-download-content-type"}},"content":"The Azure DevOps Client library for Node.js has limitations and missing features, `IGitApi.getRefs` is missing pagination and `IWikiApi` is missing page create or update. This post details some of these issues and illustrates a workaround using the Azure DevOps REST API.\\n\\n![A title image that reads \\"Azure DevOps Client for Node.js - working around limitations\\"](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## The Azure DevOps REST API and Client Libraries\\n\\nI\'ve been taking a good look at the [REST API for Azure DevOps](https://docs.microsoft.com/en-us/rest/api/azure/devops/?view=azure-devops-rest-6.1). I\'m delighted to say that it\'s a very full API. However, there\'s quirks.\\n\\nI\'m writing a tool that interrogates Azure DevOps in order that it can construct release documentation. That release documentation we would like to publish to the project wiki.\\n\\nTo make integration with Azure DevOps even easier, the ADO team have put a good amount of work into [client libraries](https://docs.microsoft.com/en-us/rest/api/azure/devops/?view=azure-devops-rest-6.1#client-libraries) that allow you to code in your language of choice. In my case I\'m writing a Node.js tool (using TypeScript) and happily the client lib for Node is written and published with TypeScript too. Tremendous! However, there is a \\"but\\" coming....\\n\\n## `GitApi` and `WikiApi` shortcomings\\n\\nAs I\'ve been using the Node client lib, I\'ve found minor quirks. Such as the [`GitApi.getRefs` missing the pagination parts of the API](https://github.com/microsoft/azure-devops-node-api/issues/415).\\n\\nWhilst the `GitApi` was missing some parameters on a method, the `WikiApi` was [missing whole endpoints, such as the Pages - Create Or Update](https://github.com/microsoft/azure-devops-node-api/issues/416) one. The various [client libraries are auto-generated](https://github.com/microsoft/azure-devops-node-api/blob/master/CONTRIBUTING/index.md#general-contribution-guide) which makes contribution a difficult game. The lovely [Matt Cooper](https://github.com/vtbassmatt) has [alerted the team](https://github.com/microsoft/azure-devops-node-api/issues/415#issuecomment-717991914)\\n\\n> These clients are generated from the server-side controllers, and at a glance, I don\'t understand why those two parameters weren\'t included. Full transparency, we don\'t dedicate a lot of cycles here, but I will get it on the team\'s radar to investigate/improve.\\n\\nIn the meantime, I still had a tool to write.\\n\\n## Handrolled Wiki API\\n\\nWhilst the Node.js client lib was missing some crucial pieces, there did seem to be a way forward. Using the API directly; not using the client lib to do our HTTP and using [axios](https://github.com/axios/axios) instead. Happily the types we needed were still available for be leveraged.\\n\\nLooking at the docs it seemed it ought to be simple:\\n\\n[https://docs.microsoft.com/en-us/rest/api/azure/devops/?view=azure-devops-rest-6.1#assemble-the-request](https://docs.microsoft.com/en-us/rest/api/azure/devops/?view=azure-devops-rest-6.1#assemble-the-request)\\n\\nBut when I attempted this I found my requests erroring out with 203 Non-Authoritative Informations. It didn\'t make sense. I couldn\'t get a single request to be successful, they all failed. It occurred to me that the answer was hiding in `node_modules`. I\'d managed to make successful requests to the API using the client lib. What was it doing that I wasn\'t?\\n\\nThe answer ended up being an authorization one-liner:\\n\\n```ts\\nconst request = await axios({\\n        url,\\n        headers: {\\n            Accept: \'application/json\',\\n            \'Content-Type\': \'application/json\',\\n            // This!\\n            Authorization: `Basic ${Buffer.from(`PAT:${adoPersonalAccessToken}`).toString(\'base64\')}`,\\n            \'X-TFS-FedAuthRedirect\': \'Suppress\',\\n        },\\n    });\\n}\\n```\\n\\nWith this in hand everything started to work and I found myself able to write my own clients to fill in the missing pieces from the client lib:\\n\\n```ts\\nimport axios from \'axios\';\\nimport {\\n  WikiPage,\\n  WikiPageCreateOrUpdateParameters,\\n  WikiType,\\n} from \'azure-devops-node-api/interfaces/WikiInterfaces\';\\nimport { IWikiApi } from \'azure-devops-node-api/WikiApi\';\\n\\nasync function getWikiPage({\\n  adoUrl,\\n  adoProject,\\n  adoPat,\\n  wikiId,\\n  path,\\n}: {\\n  adoUrl: string;\\n  adoProject: string;\\n  adoPat: string;\\n  wikiId: string;\\n  path: string;\\n}) {\\n  try {\\n    const url = `${makeBaseApiUrl({\\n      adoUrl,\\n      adoProject,\\n    })}/wiki/wikis/${wikiId}/pages?${apiVersion}&path=${path}&includeContent=True&recursionLevel=full`;\\n    const request = await axios({\\n      url,\\n      headers: makeHeaders(adoPat),\\n    });\\n\\n    const page: WikiPage = request.data;\\n    return page;\\n  } catch (error) {\\n    return undefined;\\n  }\\n}\\n\\nasync function createWikiPage({\\n  adoUrl,\\n  adoProject,\\n  adoPat,\\n  wikiId,\\n  path,\\n  data,\\n}: {\\n  adoUrl: string;\\n  adoProject: string;\\n  adoPat: string;\\n  wikiId: string;\\n  path: string;\\n  data: WikiPageCreateOrUpdateParameters;\\n}) {\\n  try {\\n    const url = `${makeBaseApiUrl({\\n      adoUrl,\\n      adoProject,\\n    })}/wiki/wikis/${wikiId}/pages?${apiVersion}&path=${path}`;\\n\\n    const request = await axios({\\n      method: \'PUT\',\\n      url,\\n      headers: makeHeaders(adoPat),\\n      data,\\n    });\\n\\n    const newPage: WikiPage = request.data;\\n    return newPage;\\n  } catch (error) {\\n    return undefined;\\n  }\\n}\\n\\nconst apiVersion = \'api-version=6.0\';\\n\\n/**\\n * Create the headers necessary to ake Azure DevOps happy\\n * @param adoPat Personal Access Token from ADO\\n */\\nfunction makeHeaders(adoPat: string) {\\n  return {\\n    Accept: \'application/json\',\\n    \'Content-Type\': \'application/json\',\\n    Authorization: `Basic ${Buffer.from(`PAT:${adoPat}`).toString(\'base64\')}`,\\n    \'X-TFS-FedAuthRedirect\': \'Suppress\',\\n  };\\n}\\n\\n/**\\n * eg https://dev.azure.com/{organization}/{project}/_apis\\n */\\nfunction makeBaseApiUrl({\\n  adoUrl,\\n  adoProject,\\n}: {\\n  adoUrl: string;\\n  adoProject: string;\\n}) {\\n  return `${adoUrl}/${adoProject}/_apis`;\\n}\\n```\\n\\nWith this I was able to write code like this:\\n\\n```ts\\nlet topLevelPage = await getWikiPage({\\n  adoUrl,\\n  adoProject,\\n  adoPat,\\n  wikiId,\\n  path: config.wikiTopLevelName,\\n});\\n\\nif (!topLevelPage)\\n  topLevelPage = await createWikiPage({\\n    adoUrl,\\n    adoProject,\\n    adoPat,\\n    wikiId,\\n    path: config.wikiTopLevelName,\\n    data: { content: \'\' },\\n  });\\n```\\n\\nand the wikis were ours!\\n\\n## Handrolled Git API\\n\\nSimilarly it\'s possible to write a client for the Git API that reuses the types from the client lib.\\n\\n```ts\\n/**\\n * Get the refs for the repo using Axios\\n * IGitApi.getRefs seems to be missing pagination parts of API, see: https://github.com/microsoft/azure-devops-node-api/issues/415\\n */\\nexport async function getRefs({\\n  adoUrl,\\n  adoProject,\\n  repositoryId,\\n  filter,\\n  logger,\\n}: {\\n  adoUrl: string;\\n  adoProject: string;\\n  repositoryId: string;\\n  adoPat: string;\\n  filter: string;\\n}): Promise<GitRef[]> {\\n  const batchSize = 100;\\n  let continuationToken = \'\';\\n  const refs: GitRef[] = [];\\n  // eslint-disable-next-line no-constant-condition\\n  while (true) {\\n    try {\\n      const url = `${makeBaseApiUrl({\\n        adoUrl,\\n        adoProject,\\n      })}/git/repositories/${repositoryId}/refs?${apiVersion}&filter=${filter}&peelTags=True&$top=${batchSize}&continuationToken=${continuationToken}`;\\n\\n      const response = await axios({\\n        method: \'GET\',\\n        url,\\n        headers: makeHeaders(adoPat),\\n        data,\\n      });\\n\\n      continuationToken = response.headers[\'x-ms-continuationtoken\'] || \'\';\\n\\n      const nextRefs: { value: GitRef[] } = response.data;\\n\\n      refs.push(...nextRefs.value);\\n\\n      const noMoreRefs = nextRefs.value.length === 0 || !continuationToken;\\n      if (noMoreRefs) break;\\n    } catch (err: any) {\\n      logger.error(\\n        \'Failed to load refs\',\\n        err?.message,\\n        err?.response?.status,\\n        err?.response?.data,\\n      );\\n      throw new Error(\'Failed to load refs\');\\n    }\\n  }\\n\\n  return refs;\\n}\\n```\\n\\n## Conclusion\\n\\nThe client lib is great, but it\'s not perfect. It\'s missing some APIs and it\'s missing some features. But as we can see, it\'s possible to work around the shortcomings and write our own clients to fill in the gaps."},{"id":"safari-empty-download-content-type","metadata":{"permalink":"/safari-empty-download-content-type","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2020-10-19-safari-empty-download-content-type/index.md","source":"@site/blog/2020-10-19-safari-empty-download-content-type/index.md","title":"Safari: The Mysterious Case of the Empty Download","description":"Safari requires a `Content-Type` header in responses to avoid empty downloads. Providing a `Content-Type` header resolved an authentication issue.","date":"2020-10-19T00:00:00.000Z","tags":[],"readingTime":2.22,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"safari-empty-download-content-type","title":"Safari: The Mysterious Case of the Empty Download","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false,"description":"Safari requires a `Content-Type` header in responses to avoid empty downloads. Providing a `Content-Type` header resolved an authentication issue."},"unlisted":false,"prevItem":{"title":"Azure DevOps Client for Node.js - GitApi / WikiApi limitations","permalink":"/azure-devops-node-api-git-api-getrefs-wiki-api"},"nextItem":{"title":"Autofac 6, integration tests and .NET generic hosting","permalink":"/autofac-6-integration-tests-and-generic-hosting"}},"content":"Safari wants a `Content-Type` header in responses. Even if the response is `Content-Length: 0`. Without this, Safari can attempt to trigger an empty download. Don\'t argue; just go with it; some browsers are strange.\\n\\n\x3c!--truncate--\x3e\\n\\n## The longer version\\n\\nEvery now and then a mystery presents itself. A puzzle which just doesn\'t make sense and yet stubbornly continues to exist. I happened upon one of these the other day and to say it was frustrating does it no justice at all.\\n\\nIt all came back to the default iOS and Mac browser; Safari. When our users log into our application, they are redirected to a shared login provider which, upon successful authentication, hands over a cookie containing auth details and redirects back to our application. A middleware in our app reads what it needs from the cookie and then creates a cookie of its own which is to be used throughout the session. As soon as the cookie is set, the page refreshes and the app boots up in an authenticated state.\\n\\nThat\'s the background. This mechanism had long been working fine with Chrome (which the majority of our users browse with), Edge, Firefox and Internet Explorer. But we started to get reports from Safari users that, once they\'d supplied their credentials, they\'d not be authenticated and redirected back to our application. Instead they\'d be prompted to download an empty document and the redirect would not take place.\\n\\nAs a team we could not fathom why this should be the case; it just didn\'t make sense. There followed hours of experimentation before [Hennie](https://twitter.com/hennie_spies) noticed something. It was at the point when the redirect back to our app from the login provider took place. Specifically the initial response that came back which contained our custom cookie and a `Refresh: 0` header to trigger a refresh in the browser. There was no content in the response, save for headers. It was `Content-Length: 0` all the way.\\n\\nHennie noticed that there was no `Content-Type` set and wondered if that was significant. It didn\'t seem like it would be a necessary header given there was no content. But Safari reckons not with logic. As an experiment we tried setting the response header to `Content-Type: text/html`. It worked! No mystery download, no failed redirect (which it turned out was actually a successful redirect which wasn\'t being surfaced in Safari\'s network request tab).\\n\\nIt appears that always providing a `Content-Type` header in your responses is wise if only for the case of Safari. In fact, it\'s generally unlikely that this won\'t be set anyway, but it can happen as we have experienced. Hopefully we\'ve suffered so you don\'t have to."},{"id":"autofac-6-integration-tests-and-generic-hosting","metadata":{"permalink":"/autofac-6-integration-tests-and-generic-hosting","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2020-10-02-autofac-6-integration-tests-and-generic-hosting/index.md","source":"@site/blog/2020-10-02-autofac-6-integration-tests-and-generic-hosting/index.md","title":"Autofac 6, integration tests and .NET generic hosting","description":"Integration tests using Autofac have been affected by a long-standing issue in .NET Core 3.0. Alternative approaches may not last long.","date":"2020-10-02T00:00:00.000Z","tags":[{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."},{"inline":false,"label":"Automated Testing","permalink":"/tags/automated-testing","description":"How to perform the automation of tests."}],"readingTime":2.235,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"autofac-6-integration-tests-and-generic-hosting","title":"Autofac 6, integration tests and .NET generic hosting","authors":"johnnyreilly","tags":["asp.net","automated testing"],"image":"./autofac-integration-tests.webp","hide_table_of_contents":false,"description":"Integration tests using Autofac have been affected by a long-standing issue in .NET Core 3.0. Alternative approaches may not last long."},"unlisted":false,"prevItem":{"title":"Safari: The Mysterious Case of the Empty Download","permalink":"/safari-empty-download-content-type"},"nextItem":{"title":"Why your team needs a newsfeed","permalink":"/why-your-team-needs-newsfeed"}},"content":"I [blogged a little while ago around to support integration tests using Autofac](../2020-05-21-autofac-webapplicationfactory-integration-tests/index.md). This was specific to Autofac but documented a workaround for a [long standing issue with `ConfigureTestContainer` that was introduced into .NET core 3.0](https://github.com/dotnet/aspnetcore/issues/14907) which affects [all third-party containers](https://docs.microsoft.com/en-us/aspnet/core/fundamentals/dependency-injection?view=aspnetcore-3.1#default-service-container-replacement) that use `ConfigureTestContainer` in their tests.\\n\\n![A title image for the blog featuring the Autofac logo](autofac-integration-tests.webp)\\n\\n\x3c!--truncate--\x3e\\n\\nI\'ll not repeat the contents of the previous post - it all still stands. However, with Autofac 6 the approach documented there will cease to work. This is because the previous approach relied upon `ContainerBuilder` not being sealed. [As of Autofac 6 it is.](https://github.com/autofac/Autofac/issues/1120)\\n\\nHappily the tremendous [Alistair Evans](https://twitter.com/evocationist) came up with an [alternative approach](https://github.com/autofac/Autofac/issues/1207#issuecomment-701961371) which is listed below:\\n\\n```cs\\n/// <summary>\\n/// Based upon https://github.com/dotnet/AspNetCore.Docs/tree/master/aspnetcore/test/integration-tests/samples/3.x/IntegrationTestsSample\\n/// </summary>\\n/// <typeparam name=\\"TStartup\\"></typeparam>\\npublic class AutofacWebApplicationFactory<TStartup> : WebApplicationFactory<TStartup> where TStartup : class\\n{\\n    protected override IHost CreateHost(IHostBuilder builder)\\n    {\\n        builder.UseServiceProviderFactory<ContainerBuilder>(new CustomServiceProviderFactory());\\n        return base.CreateHost(builder);\\n    }\\n}\\n\\n/// <summary>\\n/// Based upon https://github.com/dotnet/aspnetcore/issues/14907#issuecomment-620750841 - only necessary because of an issue in ASP.NET Core\\n/// </summary>\\npublic class CustomServiceProviderFactory : IServiceProviderFactory<ContainerBuilder>\\n{\\n    private AutofacServiceProviderFactory _wrapped;\\n    private IServiceCollection _services;\\n\\n    public CustomServiceProviderFactory()\\n    {\\n        _wrapped = new AutofacServiceProviderFactory();\\n    }\\n\\n    public ContainerBuilder CreateBuilder(IServiceCollection services)\\n    {\\n        // Store the services for later.\\n        _services = services;\\n\\n        return _wrapped.CreateBuilder(services);\\n    }\\n\\n    public IServiceProvider CreateServiceProvider(ContainerBuilder containerBuilder)\\n    {\\n        var sp = _services.BuildServiceProvider();\\n#pragma warning disable CS0612 // Type or member is obsolete\\n        var filters = sp.GetRequiredService<IEnumerable<IStartupConfigureContainerFilter<ContainerBuilder>>>();\\n#pragma warning restore CS0612 // Type or member is obsolete\\n\\n        foreach (var filter in filters)\\n        {\\n            filter.ConfigureContainer(b => { })(containerBuilder);\\n        }\\n\\n        return _wrapped.CreateServiceProvider(containerBuilder);\\n    }\\n}\\n```\\n\\nUsing this in place of the previous approach should allow you continue running your integration tests with Autofac 6. Thanks Alistair!\\n\\n## Concern for third-party containers\\n\\nWhilst this gets us back up and running, [Alistair pointed out that this approach depends upon a deprecated interface](https://github.com/autofac/Autofac/issues/1207#issuecomment-702250044). This is the [`IStartupConfigureContainerFilter`](https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.hosting.istartupconfigurecontainerfilter-1.configurecontainer?view=aspnetcore-3.1) which [has been marked as `Obsolete` since mid 2019](https://github.com/dotnet/aspnetcore/pull/11505). What this means is, at some point, this approach will stop working.\\n\\nThe marvellous David Fowler has said that [`ConfigureTestContainer` issue should be resolved in .NET](https://github.com/autofac/Autofac/issues/1207#issuecomment-702361608). However it\'s worth noting that this has been an issue since .NET Core 3 shipped and unfortunately the wonderful [Chris Ross has advised that it\'s not likely to be fixed for .NET 5](https://github.com/dotnet/aspnetcore/issues/14907#issuecomment-702287717).\\n\\nI\'m very keen this does get resolved in .NET. Building tests upon an `Obsolete` attribute doesn\'t fill me with confidence. I\'m a long time user of Autofac and I\'d like to continue to be. Here\'s hoping that\'s made possible by a fix landing in .NET. If this is something you care about, it may be worth upvoting / commenting on [the issue in GitHub](https://github.com/dotnet/aspnetcore/issues/14907) so the team are aware of desire around this being resolved."},{"id":"why-your-team-needs-newsfeed","metadata":{"permalink":"/why-your-team-needs-newsfeed","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2020-09-04-why-your-team-needs-newsfeed/index.md","source":"@site/blog/2020-09-04-why-your-team-needs-newsfeed/index.md","title":"Why your team needs a newsfeed","description":"A newsfeed was built to narrow the gap between an online platform team and their users. It generates real-time stories in Markdown with links.","date":"2020-09-04T00:00:00.000Z","tags":[],"readingTime":4.975,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"why-your-team-needs-newsfeed","title":"Why your team needs a newsfeed","authors":"johnnyreilly","hide_table_of_contents":false,"description":"A newsfeed was built to narrow the gap between an online platform team and their users. It generates real-time stories in Markdown with links."},"unlisted":false,"prevItem":{"title":"Autofac 6, integration tests and .NET generic hosting","permalink":"/autofac-6-integration-tests-and-generic-hosting"},"nextItem":{"title":"Devcontainers AKA performance in a secure sandbox","permalink":"/devcontainers-aka-performance-in-secure"}},"content":"I\'m part of a team that builds an online platform. I\'m often preoccupied by how to narrow the gap between our users and \\"us\\" - the people that build the platform. It\'s important we understand how people use and interact with what we\'ve built. If we don\'t then we\'re liable to waste our time and energy building the wrong things. Or the wrong amount of the right things.\\n\\n\x3c!--truncate--\x3e\\n\\nOn a recent holiday I spent a certain amount of time pondering how to narrow the gap between our user and us. We have lots of things that help us; we use various analytics tools like [mixpanel](https://mixpanel.com/), we\'ve got a mini analytics platform of our own, we have teams notifications that pop up client feedback and so on. They are all great, but they\'re somewhat disparate; they don\'t give us a clear insight as to who uses our platform and how they do so. The information is there, but it\'s tough to grok. It doesn\'t make for a joined up story.\\n\\nReaching around for how to solve this I had an idea: what if our platform had a newsfeed? The kind of thing that social media platforms the likes of Twitter and Facebook have used to great effect; a stream of mini-activities which show how the community interacts with the product. People logging in and browsing around, using features on the platform. If we could see this in near real time we\'d be brought closer to our users; we\'d have something that would help us have real empathy and understanding. We\'d see our product as the stories of users interacting with it.\\n\\n## How do you build a newsfeed?\\n\\nThis was an experiment that seemed worth pursuing. So I decided to build a proof of concept and see what happened. Now I intended to put the \\"M\\" into MVP with this; I went in with a number of intentional constraints:\\n\\n1. The news feed wouldn\'t auto update (users have the F5 key for that)\\n2. We\'d host the newsfeed in our own mini analytics platform (which is already used by the team to understand how people use the platform)\\n3. News stories wouldn\'t be stored anywhere; we\'d generate them on the fly by querying various databases / APIs. The cost of this would be that our news stories wouldn\'t be \\"persistent\\"; you wouldn\'t be able to address them with a URL; there\'d be no way to build \\"like\\" or \\"share\\" functionality.\\n\\nAll of the above constraints are, importantly, reversable decisions. If we want auto update it could be built later. If we want the newsfeed to live somewhere else we could move it. If we wanted news stories to be persisted then we could do that.\\n\\n## Implementation\\n\\nWith these constraints in mind, I turned my attention to the implementation. I built a `NewsFeedService` that would be queried for news stories. The interface I decided to build looked like this:\\n\\n```\\nNewsFeedService.getNewsFeed(from: Date, to: Date): NewsFeed\\n\\ntype NewsFeed {\\n    startedAt: Date;\\n    ended at: Date;\\n    stories: NewsStory[];\\n}\\n\\ntype NewsStory {\\n    /** When the story happened */\\n    happenedAt: Date;\\n    /** A code that represents the type of story this is; eg USER_SESSION */\\n    storyCode: string\\n    /** The story details in markdown format */\\n    story: string;\\n}\\n```\\n\\nEach query to `NewsFeedService.getNewsFeed` would query various databases / APIs related to our product, looking for interesting events. Whether it be users logging in, users performing some kind of action, whatever. For each interested event a news story like this would be produced:\\n\\n> Jane Smith logged in at 10:03am for 25 minutes. They placed [an order](https://my-glorious-platform.io/orders/janes-order) worth \xa33,000.\\n\\nNow the killer feature here is [Markdown](https://en.wikipedia.org/wiki/Markdown#:~:text=Markdown%20is%20a%20lightweight%20markup,using%20a%20plain%20text%20editor.). Our stories are written in Markdown. Why is Markdown cool? Well [to quote the creators of Markdown](https://web.archive.org/web/20040402182332/http://daringfireball.net/projects/markdown/):\\n\\n> Markdown allows you to write using an easy-to-read, easy-to-write plain text format, then convert it to structurally valid XHTML (or HTML).\\n\\nThis crucially includes the ability to include links. This was significant because I want us to be able to be able to click on pieces of information in the stories and be taken to the relevant place in the platform to see more details. Just as you see status updates on, for example, Twitter which lead you on to more details:\\n\\n> This is the history of [@DefinitelyTyped](https://twitter.com/DefinitelyTyped?ref_src=twsrc%5Etfw): [https://t.co/AY6s3bWnKP](https://t.co/AY6s3bWnKP) Thanks to [@SeaRyanC](https://twitter.com/SeaRyanC?ref_src=twsrc%5Etfw) & [@drosenwasser](https://twitter.com/drosenwasser?ref_src=twsrc%5Etfw) of the [@typescript](https://twitter.com/typescript?ref_src=twsrc%5Etfw) team, [@blakeembrey](https://twitter.com/blakeembrey?ref_src=twsrc%5Etfw) inventor of typings, [@vvakame](https://twitter.com/vvakame?ref_src=twsrc%5Etfw), [@\\\\_stevefenton](https://twitter.com/_stevefenton?ref_src=twsrc%5Etfw), [@basarat](https://twitter.com/basarat?ref_src=twsrc%5Etfw), and of course [@borisyankov](https://twitter.com/borisyankov?ref_src=twsrc%5Etfw) for telling me their parts of the story\u2764\uFE0F\uD83C\uDF3B\\n>\\n> \u2014 John Reilly (@johnny_reilly) [October 8, 2019](https://twitter.com/johnny_reilly/status/1181542739994976256?ref_src=twsrc%5Etfw)\\n\\nAgain consider this example news story:\\n\\n> Jane Smith logged in at 10:03am for 25 minutes. They placed [an order](https://my-glorious-platform.io/orders/janes-order) worth \xa33,000.\\n\\nConsider that story but without a link. It\'s not the same is it? A newsfeed without links would be missing a trick. Markdown gives us links. And happily due to my extensive work down the open source mines, I speak it like a native.\\n\\nThe first consumer of the newsfeed was to be our own mini analytics platform, which is a React app. Converting the markdown stories to React is a solved problem thanks to the wonderful [react-markdown](https://github.com/rexxars/react-markdown). You can simply sling Markdown at it and out comes HTML. Et voil\xe0 a news feed!\\n\\n## What\'s next?\\n\\nSo that\'s it! We\'ve built a (primitive) news feed. We can now see in real time how are users are getting on. We\'re closer to them, we understand them better as a consequence. If we want to take it further there\'s a number of things we could do:\\n\\n1. We could make the feed auto-update\\n2. We could push news stories to other destinations. Markdown is a gloriously portable format which can be used in a variety of environments. For instance the likes of Slack and [Teams](../2019-12-18-teams-notification-webhooks/index.md) accept it and apps like these are generally open on people\'s desktops and phones all the time anyway. Another way to narrow the gap between us and and our users.\\n\\nIt\'s very exciting!"},{"id":"devcontainers-aka-performance-in-secure","metadata":{"permalink":"/devcontainers-aka-performance-in-secure","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2020-08-09-devcontainers-aka-performance-in-secure/index.md","source":"@site/blog/2020-08-09-devcontainers-aka-performance-in-secure/index.md","title":"Devcontainers AKA performance in a secure sandbox","description":"Speedy ASP.NET Core and JavaScript development is made possible by devcontainers, which isolate tools and code to improve productivity.","date":"2020-08-09T00:00:00.000Z","tags":[],"readingTime":6.545,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"devcontainers-aka-performance-in-secure","title":"Devcontainers AKA performance in a secure sandbox","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false,"description":"Speedy ASP.NET Core and JavaScript development is made possible by devcontainers, which isolate tools and code to improve productivity."},"unlisted":false,"prevItem":{"title":"Why your team needs a newsfeed","permalink":"/why-your-team-needs-newsfeed"},"nextItem":{"title":"Devcontainers and SSL interception","permalink":"/devcontainers-and-ssl-interception"}},"content":"Many corporate machines arrive in engineers hands with a preponderance of pre-installed background tools; from virus checkers to backup utilities to port blockers; the list is long.\\n\\n\x3c!--truncate--\x3e\\n\\nThe reason that these tools are installed is generally noble. However, the implementation can often be problematic. The tools may be set up in such a way as they impact and interfere with one another. Really powerful machines with 8 CPUs and hardy SSDs can be slowed to a crawl. Put simply: the good people responsible for ensuring security are rarely encouraged to incentivise performance alongside it. And so don\'t.\\n\\nThe unfortunate consequence of considering the role of security without regard to performance is this: sluggish computers. The further consequence (and this is the one I want you to think long and hard about) is _low developer productivity_. And that sucks. It impacts what an organisation is able to do, how fast an organisation is able to move. Put simply: it can be the difference between success and failure.\\n\\nThe most secure computer is off. But you won\'t ship much with it. Encouraging your organisation to consider tackling security with performance in mind is worthwhile. It\'s a long game though. In the meantime what can we do?\\n\\n## \\"Hide from the virus checkers\\\\*\\\\*\\\\* in a devcontainer\\"\\n\\nDevcontainers, the infrastructure as code equivalent for developing software, have an underappreciated quality: unlocking your machine\'s performance.\\n\\nDevcontainers are isolated secure sandboxes in which you can build software. To quote the [docs](https://code.visualstudio.com/docs/remote/containers):\\n\\n> A `devcontainer.json` file in your project tells VS Code how to access (or create) a development container with a well-defined tool and runtime stack. This container can be used to run an application or to sandbox tools, libraries, or runtimes needed for working with a codebase.\\n>\\n> Workspace files are mounted from the local file system or copied or _cloned into the container_.\\n\\nWe\'re going to set up a devcontainer to code an ASP.NET Core application with a JavaScript (well TypeScript) front end. If there\'s one thing that\'s sure to catch a virus checkers beady eye, it\'s `node_modules`. `node_modules` contains more files than a black hole has mass. Consider a project with 5,000 source files. One trusty `yarn` later and the folder now has a tidy 250,000 files. The virus checker is now really sitting up and taking notice.\\n\\nOur project has a `git commit` hook set up with [Husky](https://github.com/typicode/husky) that formats our TypeScript files with [Prettier](https://prettier.io/). Every commit the files are formatted to align with the project standard. With all the virus checkers in place a `git commit` takes around 45 seconds. Inside a devcontainer we can drop this to 5 seconds. That\'s nine times faster. I\'ll repeat that: that\'s **nine times faster**!\\n\\nThe \\"cloned into a container\\" above is key to what we\'re going to do. We\'re _not_ going to mount our local file system into the devcontainer. Oh no. We\'re going to build a devcontainer with ASP.NET CORE and JavaScript in. Then, inside there, we\'re going to clone our repo. Then we can develop, build and debug all inside the container. It will feel like we\'re working on our own machine because VS Code does such a damn fine job. In reality, we\'re connecting to another computer (a Linux computer to boot) that is running in isolation to our own. In our case that machine is sharing our hardware; but that\'s just an implementation detail. It could be anywhere (and in the future may well be).\\n\\n## Make me a devcontainer...\\n\\nEnough talk... We\'re going to need a `.devcontainer/devcontainer.json`:\\n\\n```json\\n{\\n  \\"name\\": \\"my devcontainer\\",\\n  \\"dockerComposeFile\\": \\"../docker-compose.devcontainer.yml\\",\\n  \\"service\\": \\"my-devcontainer\\",\\n  \\"workspaceFolder\\": \\"/workspace\\",\\n\\n  // Set *default* container specific settings.json values on container create.\\n  \\"settings\\": {\\n    \\"terminal.integrated.shell.linux\\": \\"/bin/zsh\\"\\n  },\\n\\n  // Add the IDs of extensions you want installed when the container is created.\\n  \\"extensions\\": [\\n    \\"ms-dotnettools.csharp\\",\\n    \\"dbaeumer.vscode-eslint\\",\\n    \\"esbenp.prettier-vscode\\",\\n    \\"ms-mssql.mssql\\",\\n    \\"eamodio.gitlens\\",\\n    \\"ms-azuretools.vscode-docker\\",\\n    \\"k--kato.docomment\\",\\n    \\"Leopotam.csharpfixformat\\"\\n  ],\\n\\n  // Use \'postCreateCommand\' to clone the repo into the workspace folder when the devcontainer starts\\n  // and copy in the .env file\\n  \\"postCreateCommand\\": \\"git clone git@github.com:my-org/my-repo.git . && cp /.env /workspace/.env\\"\\n\\n  // \\"remoteUser\\": \\"vscode\\"\\n}\\n```\\n\\nNow the `docker-compose.devcontainer.yml` which lives in the root of the project. It provisions a SQL Server container (using the official image) and our devcontainer:\\n\\n```yml\\nversion: \'3.7\'\\nservices:\\n  my-devcontainer:\\n    image: my-devcontainer\\n    build:\\n      context: .\\n      dockerfile: Dockerfile.devcontainer\\n    command: /bin/zsh -c \\"while sleep 1000; do :; done\\"\\n    volumes:\\n      # mount .zshrc from home - make sure it doesn\'t contain Windows line endings\\n      - ~/.zshrc:/root/.zshrc\\n\\n    # user: vscode\\n    ports:\\n      - \'5000:5000\'\\n      - \'8080:8080\'\\n    environment:\\n      - CONNECTIONSTRINGS__MYDATABASECONNECTION\\n    depends_on:\\n      - db\\n  db:\\n    image: mcr.microsoft.com/mssql/server:2019-latest\\n    privileged: true\\n    ports:\\n      - 1433:1433\\n    environment:\\n      SA_PASSWORD: \'Your_password123\'\\n      ACCEPT_EULA: \'Y\'\\n```\\n\\nThe devcontainer will be built with the `Dockerfile.devcontainer` in the root of our repo. It relies upon your SSH keys and a `.env` file being available to be copied in:\\n\\n```Dockerfile\\n#-----------------------------------------------------------------------------------------------------------\\n# Based upon: https://github.com/microsoft/vscode-dev-containers/tree/master/containers/dotnetcore\\n#-----------------------------------------------------------------------------------------------------------\\nARG VARIANT=\\"3.1-bionic\\"\\nFROM mcr.microsoft.com/dotnet/core/sdk:${VARIANT}\\n\\n# Because MITM certificates\\nCOPY ./docker/certs/. /usr/local/share/ca-certificates/\\nENV NODE_EXTRA_CA_CERTS=/usr/local/share/ca-certificates/mitm.pem\\nRUN update-ca-certificates\\n\\n# This Dockerfile adds a non-root user with sudo access. Use the \\"remoteUser\\"\\n# property in devcontainer.json to use it. On Linux, the container user\'s GID/UIDs\\n# will be updated to match your local UID/GID (when using the dockerFile property).\\n# See https://aka.ms/vscode-remote/containers/non-root-user for details.\\nARG USERNAME=vscode\\nARG USER_UID=1000\\nARG USER_GID=$USER_UID\\n\\n# Options for common package install script\\nARG INSTALL_ZSH=\\"true\\"\\nARG UPGRADE_PACKAGES=\\"true\\"\\nARG COMMON_SCRIPT_SOURCE=\\"https://raw.githubusercontent.com/microsoft/vscode-dev-containers/master/script-library/common-debian.sh\\"\\nARG COMMON_SCRIPT_SHA=\\"dev-mode\\"\\n\\n# Settings for installing Node.js.\\nARG INSTALL_NODE=\\"true\\"\\nARG NODE_SCRIPT_SOURCE=\\"https://raw.githubusercontent.com/microsoft/vscode-dev-containers/master/script-library/node-debian.sh\\"\\nARG NODE_SCRIPT_SHA=\\"dev-mode\\"\\n\\n# ARG NODE_VERSION=\\"lts/*\\"\\nARG NODE_VERSION=\\"14\\"\\nENV NVM_DIR=/usr/local/share/nvm\\n\\n# Have nvm create a \\"current\\" symlink and add to path to work around https://github.com/microsoft/vscode-remote-release/issues/3224\\nENV NVM_SYMLINK_CURRENT=true\\nENV PATH=${NVM_DIR}/current/bin:${PATH}\\n\\n# Configure apt and install packages\\nRUN apt-get update \\\\\\n    && export DEBIAN_FRONTEND=noninteractive \\\\\\n    #\\n    # Verify git, common tools / libs installed, add/modify non-root user, optionally install zsh\\n    && apt-get -y install --no-install-recommends curl ca-certificates 2>&1 \\\\\\n    && curl -sSL ${COMMON_SCRIPT_SOURCE} -o /tmp/common-setup.sh \\\\\\n    && ([ \\"${COMMON_SCRIPT_SHA}\\" = \\"dev-mode\\" ] || (echo \\"${COMMON_SCRIPT_SHA} */tmp/common-setup.sh\\" | sha256sum -c -)) \\\\\\n    && /bin/bash /tmp/common-setup.sh \\"${INSTALL_ZSH}\\" \\"${USERNAME}\\" \\"${USER_UID}\\" \\"${USER_GID}\\" \\"${UPGRADE_PACKAGES}\\" \\\\\\n    #\\n    # Install Node.js\\n    && curl -sSL ${NODE_SCRIPT_SOURCE} -o /tmp/node-setup.sh \\\\\\n    && ([ \\"${NODE_SCRIPT_SHA}\\" = \\"dev-mode\\" ] || (echo \\"${COMMON_SCRIPT_SHA} */tmp/node-setup.sh\\" | sha256sum -c -)) \\\\\\n    && /bin/bash /tmp/node-setup.sh \\"${NVM_DIR}\\" \\"${NODE_VERSION}\\" \\"${USERNAME}\\" \\\\\\n    #\\n    # Clean up\\n    && apt-get autoremove -y \\\\\\n    && apt-get clean -y \\\\\\n    && rm -f /tmp/common-setup.sh /tmp/node-setup.sh \\\\\\n    && rm -rf /var/lib/apt/lists/* \\\\\\n    #\\n    # Workspace\\n    && mkdir workspace \\\\\\n    && chown -R ${NONROOT_USER}:root workspace\\n\\n\\n# Install Vim\\nRUN apt-get update && apt-get install -y \\\\\\n    vim \\\\\\n    && rm -rf /var/lib/apt/lists/*\\n\\n# Set up a timezone in the devcontainer - necessary for anything timezone dependent\\nENV TZ=Europe/London\\nRUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone \\\\\\n && apt-get update \\\\\\n && apt-get install --no-install-recommends -y \\\\\\n    apt-utils \\\\\\n    tzdata  \\\\\\n && apt-get autoremove -y \\\\\\n && apt-get clean -y \\\\\\n && rm -rf /var/lib/apt/lists/*\\n\\nENV DOTNET_RUNNING_IN_CONTAINER=true\\n\\n# Copy across SSH keys so you can git clone\\nRUN mkdir /root/.ssh\\nRUN chmod 700 /root/.ssh\\n\\nCOPY .ssh/id_rsa /root/.ssh\\nRUN chmod 600 /root/.ssh/id_rsa\\n\\nCOPY .ssh/id_rsa.pub /root/.ssh\\nRUN chmod 644 /root/.ssh/id_rsa.pub\\n\\nCOPY .ssh/known_hosts /root/.ssh\\nRUN chmod 644 /root/.ssh/known_hosts\\n\\n# Disable initial git clone prompt\\nRUN echo \\"StrictHostKeyChecking no\\" >> /etc/ssh/ssh_config\\n\\n# Copy across .env file so you can customise environment variables\\n# This will be copied into the root of the repo post git clone\\nCOPY .env /.env\\nRUN chmod 644 /.env\\n\\n# Install dotnet entity framework tools\\nRUN dotnet tool install dotnet-ef --tool-path /usr/local/bin --version 3.1.2\\n```\\n\\nWith this devcontainer you\'re good to go for an ASP.NET Core / JavaScript developer setup that is blazing fast! Remember to fire up Docker and give it goodly access to the resources of your host machine. All the CPUs, lots of memory and all the performance that there ought to be.\\n\\n_\\\\* \\"virus checkers\\" is a euphemism here for all the background tools that may be running. It was that or calling them \\"we are legion\\"_"},{"id":"devcontainers-and-ssl-interception","metadata":{"permalink":"/devcontainers-and-ssl-interception","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2020-07-11-devcontainers-and-ssl-interception/index.md","source":"@site/blog/2020-07-11-devcontainers-and-ssl-interception/index.md","title":"Devcontainers and SSL interception","description":"Developers may need to overcome MITM certificate issues to use devcontainers, which can optimize productivity for new starters when developing software.","date":"2020-07-11T00:00:00.000Z","tags":[],"readingTime":3.225,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"devcontainers-and-ssl-interception","title":"Devcontainers and SSL interception","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false,"description":"Developers may need to overcome MITM certificate issues to use devcontainers, which can optimize productivity for new starters when developing software."},"unlisted":false,"prevItem":{"title":"Devcontainers AKA performance in a secure sandbox","permalink":"/devcontainers-aka-performance-in-secure"},"nextItem":{"title":"Task.WhenAll / Select is a footgun \uD83D\uDC5F\uD83D\uDD2B","permalink":"/taskwhenall-select-is-footgun"}},"content":"[Devcontainers](https://code.visualstudio.com/docs/remote/containers) are cool. They are the infrastructure as code equivalent for developing software.\\n\\n\x3c!--truncate--\x3e\\n\\nImagine your new starter joins the team, you\'d like them to be contributing code on _day 1_. But if the first thing that happens is you hand them a sheaf of paper upon which are the instructions for how to get their machines set up for development, well, maybe it\'s going to be a while. But if your project has a devcontainer then you\'re off to the races. One trusty `git clone`, fire up VS Code and they can get going.\\n\\nThat\'s the dream right?\\n\\nI\'ve recently been doing some work getting a project I work on set up with a devcontainer. As I\'ve worked on that I\'ve become aware of some of the hurdles that might hamper your adoption of devcontainers in a corporate environment.\\n\\n## Certificates: I\'m starting with the man in the middle\\n\\nIt is a common practice in company networks to perform [SSL interception](https://docs.citrix.com/en-us/citrix-adc/13/forward-proxy/ssl-interception.html). Not SSL inception; that\'d be more fun.\\n\\n <iframe src=\\"https://giphy.com/embed/l7JDTHpsXM26k\\" width=\\"100%\\" height=\\"100%\\" frameBorder=\\"0\\" allowFullScreen=\\"\\"></iframe>\\n\\nSSL interception is the practice of installing a \\"man-in-the-middle\\" (MITM) CA certificate on users machines. When SSL traffic takes place from a users machine, it goes through a proxy. That proxy performs the SSL on behalf of that user and, if it\'s happy, supplies another certificate back to the users machine which satisfies the MITM CA certificate. So rather than seeing, for example, Google\'s certificate from [https://google.com](https://google.com) you\'d see the one resulting from the SSL interception. You can read more [here](https://security.stackexchange.com/questions/107542/is-it-common-practice-for-companies-to-mitm-https-traffic).\\n\\nNow this is a little known and less understood practice. I barely understand it myself. Certificates are _hard_. Even having read the above you may be none the wiser about why this is relevant. Let\'s get to the broken stuff.\\n\\n## \\"Devcontainers don\'t work at work!\\"\\n\\nSo, you\'re ready to get going with your first devcontainer. You fire up the [vscode-dev-containers](https://github.com/Microsoft/vscode-dev-containers) repo and find the container that\'s going to work for you. Copy pasta the `.devcontainer` into your repo, install the [Remote Development](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.vscode-remote-extensionpack) extension into VS Code and enter the `Remote-Containers: Reopen Folder in Container`. Here comes the future!\\n\\nBut when it comes to performing SSL inside the devcontainer, trouble awaits. Here\'s what a `yarn install` results in:\\n\\n```\\nyarn install v1.22.4\\n[1/4] Resolving packages...\\n[2/4] Fetching packages...\\nerror An unexpected error occurred: \\"https://registry.yarnpkg.com/@octokit/core/-/core-2.5.0.tgz: self signed certificate in certificate chain\\".\\n```\\n\\nOh no!\\n\\nGosh but it\'s okay - you\'re just bumping on the SSL interception. Why though? Well it\'s like this: when you fire up your devcontainer it builds a new Docker container. It\'s as well to imagine the container as a virtual operating system. So what\'s the difference between this operating system and the one our machine is running? Well a number of things, but crucially our host operating system has the MITM CA certificate installed. So when we SSL, we have the certificate that will match up with what the proxy sends back to us certificate-wise. And inside our trusty devcontainer we don\'t have that. Hence the sadness.\\n\\n## Devcontainer + MITM cert = working\\n\\nWe need to do two things to get this working:\\n\\n1. Acquire the requisite CA certificate(s) from your friendly neighbourhood networking team. Place them in a `certs` folder inside your repo, in the `.devcontainer` folder.\\n2. Add the following lines to your `.devcontainer/Dockerfile`, just after the initial `FROM` statement:\\n\\n```\\n# Because MITM certificates\\nCOPY certs/. /usr/local/share/ca-certificates/\\nENV NODE_EXTRA_CA_CERTS=/usr/local/share/ca-certificates/mitm.pem\\nRUN update-ca-certificates\\n```\\n\\nWhich does the following:\\n\\n- Copies the certs into the devcontainer\\n- This is a Node example and so we set an environment variable called [`NODE_EXTRA_CA_CERTS`](https://nodejs.org/api/cli.html#cli_node_extra_ca_certs_file) which points to the path of your MITM CA certificate file inside your devcontainer.\\n- updates the directory `/etc/ssl/certs` to hold SSL certificates and generates `ca-certificates.crt`\\n\\nWith these in place then you should be able to build your devcontainer with no SSL trauma. Enjoy!"},{"id":"taskwhenall-select-is-footgun","metadata":{"permalink":"/taskwhenall-select-is-footgun","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2020-06-21-taskwhenall-select-is-footgun/index.md","source":"@site/blog/2020-06-21-taskwhenall-select-is-footgun/index.md","title":"Task.WhenAll / Select is a footgun \uD83D\uDC5F\uD83D\uDD2B","description":"The writer warns against LINQ code that causes concurrent API requests and shares plans for better metrics and a development container.","date":"2020-06-21T00:00:00.000Z","tags":[{"inline":false,"label":"C#","permalink":"/tags/csharp","description":"The C# programming language."}],"readingTime":5.975,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"taskwhenall-select-is-footgun","title":"Task.WhenAll / Select is a footgun \uD83D\uDC5F\uD83D\uDD2B","authors":"johnnyreilly","tags":["c#"],"hide_table_of_contents":false,"description":"The writer warns against LINQ code that causes concurrent API requests and shares plans for better metrics and a development container."},"unlisted":false,"prevItem":{"title":"Devcontainers and SSL interception","permalink":"/devcontainers-and-ssl-interception"},"nextItem":{"title":"Autofac, WebApplicationFactory and integration tests","permalink":"/autofac-webapplicationfactory-integration-tests"}},"content":"This post differs from my typical fayre. Most often I write \\"here\'s how to do a thing\\". This is not that. It\'s more \\"don\'t do this thing I did\\". And maybe also, \\"how can we avoid a situation like this happening again in future?\\". On this topic I very much don\'t have all the answers - but by putting my thoughts down maybe I\'ll learn and maybe others will educate me. I would love that!\\n\\n\x3c!--truncate--\x3e\\n\\n## Doing things that don\'t scale\\n\\nThe platform that I work on once had zero users. We used to beg people to log in and see what we had built. Those days are (happily) but a memory. We\'re getting popular.\\n\\nAs our platform has grown in popularity it has revealed some bad choices we made. Approaches that look fine on the surface (and that work just dandy when you have no users) may start to cause problems as your number of users grows.\\n\\nI wanted to draw attention to one approach in particular that impacted us severely. In this case \\"impacted us severely\\" is a euphemism for \\"brought the site down and caused a critical incident\\".\\n\\nYou don\'t want this to happen to you. Trust me. So, what follows is a cautionary tale. The purpose of which is simply this: reader, do you have code of this ilk in your codebase? If you do: out, damn\'d spot! out, I say!\\n\\n## So cool, so terrible\\n\\nI love LINQ. I love a declarative / functional style of coding. It appeals to me on some gut level. I find it tremendously readable. Read any C# of mine and the odds are pretty good that you\'ll find some LINQ in the mix.\\n\\nImagine this scenario: you have a collection of user ids. You want to load the details of each user represented by their id from an API. You want to bag up all of those users into some kind of collection and send it back to the calling code.\\n\\nReading that, if you\'re like me, you\'re imagining some kind of map operation which loads the user details for each user id. Something like this:\\n\\n```cs\\nvar users = userIds.Select(userId => GetUserDetails(userId)).ToArray(); // users is User[]\\n```\\n\\nLovely. But you\'ll note that I\'m loading users from an API. Oftentimes, APIs are asynchronous. Certainly, in my case they were. So rather than calling a `GetUserDetails` function I found myself calling a `GetUserDetailsAsync` function, behind which an HTTP request is being sent and, later, a response is being returned.\\n\\nSo how do we deal with this? [`Task.WhenAll`](https://docs.microsoft.com/en-us/dotnet/api/system.threading.tasks.task.whenall?view=netcore-3.1#System_Threading_Tasks_Task_WhenAll__1_System_Collections_Generic_IEnumerable_System_Threading_Tasks_Task___0___) my friends!\\n\\n```cs\\nvar userTasks = userIds.Select(userId => GetUserDetailsAsync(userId));\\nvar users = await Task.WhenAll(tasks); // users is User[]\\n```\\n\\nIt worked great! Right up until to the point where it didn\'t. These sorts of shenanigans were fine when we had a minimal number of users... But there came a point where problems arose. It got to the point where that simple looking mapping operation became a cause of many, many, _many_ HTTP requests being fired concurrently. Then bad things started to happen. Not only did we realise we were launching a denial of service attack on the API we were consuming, we were bringing our own application to collapse.\\n\\nNot a proud day.\\n\\n## What is the problem?\\n\\nThrough log analysis, code reading and speculation, (with the help of the invaluable [Robski](https://www.linkedin.com/in/robert-grzankowski-53618114)) we came to realise that the cause of our woes was the `Task.WhenAll` / `Select` combination. Exercising that codepath was a surefire way to bring the application to its knees.\\n\\nAs I read around on the topic I happened upon [Mark Heath](https://www.twitter.com/mark_heath)\'s excellent list of [Async antipatterns](https://markheath.net/post/async-antipatterns). Number #6 on the list is \\"Excessive parallelization\\". It describes a nearly identical scenario to my own:\\n\\n> Now, this does \\"work\\", but what if there were 10,000 orders? We\'ve flooded the thread pool with thousands of tasks, potentially preventing other useful work from completing. If `ProcessOrderAsync` makes downstream calls to another service like a database or a microservice, we\'ll potentially overload that with too high a volume of calls.\\n\\nWe\'re definitely overloading the API we\'re consuming with too high a volume of calls. I have to admit that I\'m less clear on the direct reason that a `Task.WhenAll` / `Select` combination could prove fatal to our application. Mark suggests this approach will flood the thread pool with tasks. As I read around on `async` and `await` it\'s repeated again and again that a `Task` is not the same thing as a `Thread`. I have to hold my hands up here and say that I don\'t understand the implementation of `async` / `await` in C# well enough. [These docs are helpful but I still don\'t think the penny has fully dropped for me yet.](https://docs.microsoft.com/en-us/dotnet/standard/async-in-depth#deeper-dive-into-tasks-for-an-io-bound-operation) I will continue to read.\\n\\nOne thing we learned as we debugged the production k8s pod was that, prior to its collapse, our app appeared to be opening up 1 million connections to the API we were consuming. Which seemed a bit much. Worthy of investigation. It\'s worth saying that we\'re not certain this is exactly what is happening; we have less instrumentation in place than we\'d like. But some fancy wc grepping on Robski\'s behalf suggested this was the case.\\n\\n## What will we change in future?\\n\\nA learning that came out of this for us was this: we need more metrics exposed. We don\'t understand our application\'s behaviour under load as well as we\'d like. So we\'re planning to do some work with [App Metrics](https://www.app-metrics.io/) and [Grafana](https://grafana.com/) so we\'ve a better idea of how our application performs. If you want to improve something, first measure it.\\n\\nAnother fly in the ointment was that we were unable to reproduce the issue when running locally. It\'s worth saying here that I develop on a Windows machine and, when deployed, our application runs in a (Linux) Docker container. So there\'s a difference and a distance between our development experience and our running one.\\n\\nI\'m planning to migrate to developing in a [devcontainer](https://code.visualstudio.com/docs/remote/containers) where that\'s possible. That should narrow the gap between our production experience and our development one. Reducing the difference between the two is always useful as it means you\'re less likely to get different behaviour (ie \\"problems\\") in production as compared to development. I\'m curious as to whether I\'ll be able to replicate that behaviour in a devcontainer.\\n\\n## What did we do right now?\\n\\nTo solve the immediate issue we were able to pivot away to a completely different approach. We moved aggregation from our ASP.NET Core web application to our TypeScript / React client with a (pretty sweet) custom hook. The topic for a subsequent blog post.\\n\\nMoving to a different approach solved my immediate issue. But it left me puzzling. What was actually going wrong? Is it thread pool exhaustion? Is it something else? So many possibilities!\\n\\nIf anyone has any insights they\'d like to share that would be incredible! I\'ve also [asked a question on Stack Overflow](https://stackoverflow.com/questions/62490098/task-whenall-with-select-is-a-footgun-but-why/62490705) which has kindly had answers from generous souls. [James Skimming](https://twitter.com/jamesskimming)\'s answer lead me to [Steve Gordon\'s excellent post on connection pooling](https://www.stevejgordon.co.uk/httpclient-connection-pooling-in-dotnet-core) which I\'m still absorbing and seems like it could be relevant."},{"id":"autofac-webapplicationfactory-integration-tests","metadata":{"permalink":"/autofac-webapplicationfactory-integration-tests","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2020-05-21-autofac-webapplicationfactory-integration-tests/index.md","source":"@site/blog/2020-05-21-autofac-webapplicationfactory-integration-tests/index.md","title":"Autofac, WebApplicationFactory and integration tests","description":"A bug in ASP.NET Core v3.0 thwarts swapping in Autofac as an IOC container in WebApplicationFactory tests. A workaround exists.","date":"2020-05-21T00:00:00.000Z","tags":[{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."},{"inline":false,"label":"Automated Testing","permalink":"/tags/automated-testing","description":"How to perform the automation of tests."}],"readingTime":3.565,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"autofac-webapplicationfactory-integration-tests","title":"Autofac, WebApplicationFactory and integration tests","authors":"johnnyreilly","tags":["asp.net","automated testing"],"image":"./autofac-webapplicationfactory-tests.webp","hide_table_of_contents":false,"description":"A bug in ASP.NET Core v3.0 thwarts swapping in Autofac as an IOC container in WebApplicationFactory tests. A workaround exists."},"unlisted":false,"prevItem":{"title":"Task.WhenAll / Select is a footgun \uD83D\uDC5F\uD83D\uDD2B","permalink":"/taskwhenall-select-is-footgun"},"nextItem":{"title":"From react-window to react-virtual","permalink":"/from-react-window-to-react-virtual"}},"content":"**Updated 2nd Oct 2020:** _for an approach that works with Autofac 6 and `ConfigureTestContainer` see [this post](../2020-10-02-autofac-6-integration-tests-and-generic-hosting/index.md)._\\n\\n![A title image for the blog featuring the Autofac logo](autofac-webapplicationfactory-tests.webp)\\n\\n\x3c!--truncate--\x3e\\n\\nThis is one of those occasions where I\'m not writing up my own work so much as my discovery after in depth googling.\\n\\nIntegration tests with ASP.NET Core are the best. They spin up an in memory version of your application and let you fire requests at it. They\'ve gone through a number of iterations since ASP.NET Core has been around. You may also be familiar with the `TestServer` approach of earlier versions. For some time, the advised approach has been using [`WebApplicationFactory`](https://docs.microsoft.com/en-us/aspnet/core/test/integration-tests?view=aspnetcore-3.1#basic-tests-with-the-default-webapplicationfactory).\\n\\nWhat makes this approach particularly useful / powerful is that you can swap out dependencies of your running app with fakes / stubs etc. Just like unit tests! But potentially more useful because they run your whole app and hence give you a greater degree of confidence. What does this mean? Well, imagine you changed a piece of middleware in your application; this could potentially break functionality. Unit tests would probably not reveal this. Integration tests would.\\n\\nThere is a fly in the ointment. A hair in the gazpacho. ASP.NET Core ships with dependency injection in the box. It has its own Inversion of Control container which is perfectly fine. However, many people are accustomed to using other IOC containers such as [Autofac](https://autofac.org/).\\n\\nWhat\'s the problem? Well, swapping out dependencies registered using ASP.NET Core\'s IOC requires using a hook called [`ConfigureTestServices`](https://docs.microsoft.com/en-us/aspnet/core/test/integration-tests?view=aspnetcore-3.1#inject-mock-services). There\'s an equivalent hook for swapping out services registered using a custom IOC container: [`ConfigureTestContainer`](https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.testhost.webhostbuilderextensions.configuretestcontainer?view=aspnetcore-3.0). Unfortunately, there is a bug in ASP.NET Core as of version 3.0: [When using GenericHost, in tests `ConfigureTestContainer` is not executed](https://github.com/dotnet/aspnetcore/issues/14907)\\n\\nThis means you cannot swap out dependencies that have been registered with Autofac and the like. According to the tremendous [David Fowler](https://www.twitter.com/davidfowl) of the ASP.NET team, [this will hopefully be resolved](https://github.com/dotnet/aspnetcore/issues/14907#issuecomment-592102145).\\n\\nIn the meantime, [there\'s a workaround thanks to various commenters on the thread](https://github.com/dotnet/aspnetcore/issues/14907#issuecomment-620750841). Instead of using `WebApplicationFactory` directly, subclass it and create a custom `AutofacWebApplicationFactory` (the name is not important). This custom class overrides the behavior of `ConfigureServices` and `CreateHost` with a `CustomServiceProviderFactory`:\\n\\n```cs\\nnamespace My.Web.Tests.Helpers {\\n    /// <summary>\\n    /// Based upon https://github.com/dotnet/AspNetCore.Docs/tree/master/aspnetcore/test/integration-tests/samples/3.x/IntegrationTestsSample\\n    /// </summary>\\n    /// <typeparam name=\\"TStartup\\"></typeparam>\\n    public class AutofacWebApplicationFactory<TStartup> : WebApplicationFactory<TStartup> where TStartup : class {\\n        protected override void ConfigureWebHost(IWebHostBuilder builder) {\\n            builder.ConfigureServices(services => {\\n                    services.AddSingleton<IAuthorizationHandler>(new PassThroughPermissionedRolesHandler());\\n                })\\n                .ConfigureTestServices(services => {\\n                }).ConfigureTestContainer<Autofac.ContainerBuilder>(builder => {\\n                    // called after Startup.ConfigureContainer\\n                });\\n        }\\n\\n        protected override IHost CreateHost(IHostBuilder builder) {\\n            builder.UseServiceProviderFactory(new CustomServiceProviderFactory());\\n            return base.CreateHost(builder);\\n        }\\n    }\\n\\n    /// <summary>\\n    /// Based upon https://github.com/dotnet/aspnetcore/issues/14907#issuecomment-620750841 - only necessary because of an issue in ASP.NET Core\\n    /// </summary>\\n    public class CustomServiceProviderFactory : IServiceProviderFactory<CustomContainerBuilder> {\\n        public CustomContainerBuilder CreateBuilder(IServiceCollection services) => new CustomContainerBuilder(services);\\n\\n        public IServiceProvider CreateServiceProvider(CustomContainerBuilder containerBuilder) =>\\n        new AutofacServiceProvider(containerBuilder.CustomBuild());\\n    }\\n\\n    public class CustomContainerBuilder : Autofac.ContainerBuilder {\\n        private readonly IServiceCollection services;\\n\\n        public CustomContainerBuilder(IServiceCollection services) {\\n            this.services = services;\\n            this.Populate(services);\\n        }\\n\\n        public Autofac.IContainer CustomBuild() {\\n            var sp = this.services.BuildServiceProvider();\\n#pragma warning disable CS0612 // Type or member is obsolete\\n            var filters = sp.GetRequiredService<IEnumerable<IStartupConfigureContainerFilter<Autofac.ContainerBuilder>>>();\\n#pragma warning restore CS0612 // Type or member is obsolete\\n\\n            foreach (var filter in filters) {\\n                filter.ConfigureContainer(b => { }) (this);\\n            }\\n\\n            return this.Build();\\n        }\\n    }\\n}\\n```\\n\\nI\'m going to level with you; I don\'t understand all of this code. I\'m not au fait with the inner workings of ASP.NET Core or Autofac but I can tell you what this allows. With this custom `WebApplicationFactory` in play you get `ConfigureTestContainer` back in the mix! You get to write code like this:\\n\\n```cs\\nusing System;\\nusing System.Net;\\nusing System.Net.Http.Headers;\\nusing System.Threading.Tasks;\\nusing FakeItEasy;\\nusing FluentAssertions;\\nusing Microsoft.AspNetCore.TestHost;\\nusing Microsoft.Extensions.DependencyInjection;\\nusing Xunit;\\nusing Microsoft.Extensions.Options;\\nusing Autofac;\\nusing System.Net.Http;\\nusing Newtonsoft.Json;\\n\\nnamespace My.Web.Tests.Controllers\\n{\\n    public class MyControllerTests : IClassFixture<AutofacWebApplicationFactory<My.Web.Startup>> {\\n        private readonly AutofacWebApplicationFactory<My.Web.Startup> _factory;\\n\\n        public MyControllerTests(\\n            AutofacWebApplicationFactory<My.Web.Startup> factory\\n        ) {\\n            _factory = factory;\\n        }\\n\\n        [Fact]\\n        public async Task My() {\\n            var fakeSomethingService = A.Fake<IMySomethingService>();\\n            var fakeConfig = Options.Create(new MyConfiguration {\\n                SomeConfig = \\"Important thing\\",\\n                OtherConfigMaybeAnEmailAddress = \\"johnny_reilly@hotmail.com\\"\\n            });\\n\\n            A.CallTo(() => fakeSomethingService.DoSomething(A<string>.Ignored))\\n                .Returns(Task.FromResult(true));\\n\\n            void ConfigureTestServices(IServiceCollection services) {\\n                services.AddSingleton(fakeConfig);\\n            }\\n\\n            void ConfigureTestContainer(ContainerBuilder builder) {\\n                builder.RegisterInstance(fakeSomethingService);\\n            }\\n\\n            var client = _factory\\n                .WithWebHostBuilder(builder => {\\n                    builder.ConfigureTestServices(ConfigureTestServices);\\n                    builder.ConfigureTestContainer<Autofac.ContainerBuilder>(ConfigureTestContainer);\\n                })\\n                .CreateClient();\\n\\n            // Act\\n            var request = StringContent(\\"{\\\\\\"sommat\\\\\\":\\\\\\"to see\\\\\\"}\\");\\n            request.Headers.ContentType = MediaTypeHeaderValue.Parse(\\"application/json\\");\\n            var response = await client.PostAsync(\\"/something/submit\\", request);\\n\\n            // Assert\\n            response.StatusCode.Should().Be(HttpStatusCode.OK);\\n\\n            A.CallTo(() => fakeSomethingService.DoSomething(A<string>.Ignored))\\n                .MustHaveHappened();\\n        }\\n\\n    }\\n}\\n```"},{"id":"from-react-window-to-react-virtual","metadata":{"permalink":"/from-react-window-to-react-virtual","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2020-05-10-from-react-window-to-react-virtual/index.md","source":"@site/blog/2020-05-10-from-react-window-to-react-virtual/index.md","title":"From react-window to react-virtual","description":"Switch from `react-window` to `react-virtual` for simpler code, TypeScript support and improved perceived performance.","date":"2020-05-10T00:00:00.000Z","tags":[{"inline":false,"label":"React","permalink":"/tags/react","description":"The React library."}],"readingTime":2.535,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"from-react-window-to-react-virtual","title":"From react-window to react-virtual","authors":"johnnyreilly","tags":["react"],"hide_table_of_contents":false,"description":"Switch from `react-window` to `react-virtual` for simpler code, TypeScript support and improved perceived performance."},"unlisted":false,"prevItem":{"title":"Autofac, WebApplicationFactory and integration tests","permalink":"/autofac-webapplicationfactory-integration-tests"},"nextItem":{"title":"Up to the clouds!","permalink":"/up-to-clouds"}},"content":"The tremendous [Tanner Linsley](https://twitter.com/tannerlinsley) recently released [`react-virtual`](https://github.com/tannerlinsley/react-virtual). `react-virtual` provides \\"hooks for virtualizing scrollable elements in React\\".\\n\\n\x3c!--truncate--\x3e\\n\\nI was already using the (also excellent) [`react-window`](https://github.com/bvaughn/react-window) for this purpose. `react-window` does the virtualising job and does it very well indeed However, I was both intrigued by the lure of the new shiny thing. I\'ve also never been the biggest fan of `react-window`\'s API. So I tried switching over from `react-window` to `react-virtual` as an experiment. To my delight, the experiment went so well I didn\'t look back!\\n\\nWhat did I get out of the switch?\\n\\n- Simpler code / nicer developer ergonomics. The API for `react-virtual` allowed me to simplify my code and lose a layer of components.\\n- TypeScript support in the box\\n- Improved perceived performance. I didn\'t run any specific tests to quantify this, but I can say that the same functionality now feels snappier.\\n\\nI tweeted my delight at this and Tanner asked if there was commit diff I could share. I couldn\'t as it\'s a private codebase, but I thought it could form the basis of a blogpost.\\n\\n> Nice! Do you have a commit diff we could see?\\n>\\n> \u2014 Tanner Linsley \u269B\uFE0F (@tannerlinsley) [May 10, 2020](https://twitter.com/tannerlinsley/status/1259503283103608832?ref_src=twsrc%5Etfw)\\n\\nIn case you hadn\'t guessed, this is that blog post...\\n\\n## Make that change\\n\\nSo what does the change look like? Well first remove `react-window` from your project:\\n\\n```\\nyarn remove react-window @types/react-window\\n```\\n\\nAdd the dependency to `react-virtual`:\\n\\n```\\nyarn add react-virtual\\n```\\n\\nChange your imports from:\\n\\n```ts\\nimport { FixedSizeList, ListChildComponentProps } from \'react-window\';\\n```\\n\\nto:\\n\\n```ts\\nimport { useVirtual } from \'react-virtual\';\\n```\\n\\nChange your component code from:\\n\\n```ts\\ntype ImportantDataListProps = {\\n  classes: ReturnType<typeof useStyles>;\\n  importants: ImportantData[];\\n};\\n\\nconst ImportantDataList: React.FC<ImportantDataListProps> = React.memo(\\n  (props) => (\\n    <FixedSizeList\\n      height={400}\\n      width={\'100%\'}\\n      itemSize={80}\\n      itemCount={props.importants.length}\\n      itemData={props}\\n    >\\n      {RenderRow}\\n    </FixedSizeList>\\n  ),\\n);\\n\\ntype ListItemProps = {\\n  classes: ReturnType<typeof useStyles>;\\n  importants: ImportantData[];\\n};\\n\\nfunction RenderRow(props: ListChildComponentProps) {\\n  const { index, style } = props;\\n  const { importants, classes } = props.data as ListItemProps;\\n  const important = importants[index];\\n\\n  return (\\n    <ListItem button style={style} key={index}>\\n      <ImportantThing classes={classes} important={important} />\\n    </ListItem>\\n  );\\n}\\n```\\n\\nOf the above you can delete the `ListItemProps` type and the associate `RenderRow` function. You won\'t need them again! There\'s no longer a need to pass down data to the child element and then extract it for usage; it all comes down into a single simpler component.\\n\\nReplace the `ImportantDataList` component with this:\\n\\n```ts\\nconst ImportantDataList: React.FC<ImportantDataListProps> = React.memo(\\n  (props) => {\\n    const parentRef = React.useRef<HTMLDivElement>(null);\\n\\n    const rowVirtualizer = useVirtual({\\n      size: props.importants.length,\\n      parentRef,\\n      estimateSize: React.useCallback(() => 80, []), // This is just a best guess\\n      overscan: 5,\\n    });\\n\\n    return (\\n      <div\\n        ref={parentRef}\\n        style={{\\n          width: `100%`,\\n          height: `500px`,\\n          overflow: \'auto\',\\n        }}\\n      >\\n        <div\\n          style={{\\n            height: `${rowVirtualizer.totalSize}px`,\\n            width: \'100%\',\\n            position: \'relative\',\\n          }}\\n        >\\n          {rowVirtualizer.virtualItems.map((virtualRow) => (\\n            <div\\n              key={virtualRow.index}\\n              ref={virtualRow.measureRef}\\n              className={props.classes.hoverRow}\\n              style={{\\n                position: \'absolute\',\\n                top: 0,\\n                left: 0,\\n                width: \'100%\',\\n                height: `${virtualRow.size}px`,\\n                transform: `translateY(${virtualRow.start}px)`,\\n              }}\\n            >\\n              <ImportantThing\\n                classes={props.classes}\\n                important={props.importants[virtualRow.index]}\\n              />\\n            </div>\\n          ))}\\n        </div>\\n      </div>\\n    );\\n  },\\n);\\n```\\n\\nAnd you are done! Thanks Tanner for this tremendous library!"},{"id":"up-to-clouds","metadata":{"permalink":"/up-to-clouds","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2020-04-04-up-to-clouds/index.md","source":"@site/blog/2020-04-04-up-to-clouds/index.md","title":"Up to the clouds!","description":"Migrating ASP.NET Core app from on-prem to cloud with Kubernetes, Docker, Jenkins, Vault & Azure AD Single Sign-On for greater efficiency.","date":"2020-04-04T00:00:00.000Z","tags":[{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":10.65,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"up-to-clouds","title":"Up to the clouds!","authors":"johnnyreilly","tags":["asp.net"],"hide_table_of_contents":false,"description":"Migrating ASP.NET Core app from on-prem to cloud with Kubernetes, Docker, Jenkins, Vault & Azure AD Single Sign-On for greater efficiency."},"unlisted":false,"prevItem":{"title":"From react-window to react-virtual","permalink":"/from-react-window-to-react-virtual"},"nextItem":{"title":"Offline storage in a PWA","permalink":"/offline-storage-in-pwa"}},"content":"This last four months has been quite the departure for me. Most typically I find myself building applications; for this last period of time I\'ve been taking the platform that I work on, and been migrating it from running on our on premise servers to running in the cloud.\\n\\n\x3c!--truncate--\x3e\\n\\nThis turned out to be much more difficult than I\'d expected and for reasons that often surprised me. We knew where we wanted to get to, but not all of what we\'d need to do to get there. So many things you can only learn by doing. Whilst these experiences are still fresh in my mind I wanted to document some of the challenges we faced.\\n\\n## The mission\\n\\nAt the start of January, the team decided to make a concerted effort to take our humble ASP.NET Core application and migrate it to the cloud. We sat down with some friends from the DevOps team who are part of our organisation. We\'re fortunate in that these marvellous people are very talented engineers indeed. It was going to be a collaboration between our two teams of budding cloudmongers that would make this happen.\\n\\nNow our application is young. It is not much more than a year old. However it is growing _fast_. And as we did the migration from on premise to the cloud, that wasn\'t going to stop. Development of the application was to continue as is, shipping new versions daily. Without impeding that, we were to try and get the application migrated to the cloud.\\n\\nI would liken it to boarding a speeding train, fighting your way to the front, taking the driver hostage and then diverting the train onto a different track. It was challenging. Really, really challenging.\\n\\nSo many things had to change for us to get from on premise servers to the cloud, all the while keeping our application a going (and shipping) concern. Let\'s go through them one by one.\\n\\n## Kubernetes and Docker\\n\\nOur application was built using ASP.NET Core. A technology that is entirely cloud friendly (that\'s one of the reasons we picked it). We were running on a collection of hand installed, hand configured Windows servers. That had to change. We wanted to move our application to run on Kubernetes; so we didn\'t have to manually configure servers. Rather k8s would manage the provisioning and deployment of containers running our application. Worth saying now: I knew _nothing_ about Kubernetes. Or nearly nothing. I learned a bunch along the way, but, as I\'ve said, this was a collaboration between our team and the mighty site reliability engineers of the DevOps team. They knew a _lot_ about this k8s stuff and moreoften than not, our team stood back and let them work their magic.\\n\\nIn order that we could migrate to running in k8s, we first needed to containerise our application. We needed a `Dockerfile`. There followed a good amount of experimentation as we worked out how to build ourselves images. There\'s an art to building an optimal Docker image.\\n\\nSo that we can cover a lot of ground, this post will remain relatively high level. So here\'s a number of things that we encountered along the way that are worth considering:\\n\\n- Multi-stage builds were an absolute necessity for us. We\'d build the front end of our app (React / TypeScript) using one stage with a [Node base image](https://hub.docker.com/_/node). Then we\'d build our app using a [.NET Core SDK base image](https://hub.docker.com/_/microsoft-dotnet-core-sdk/). Finally, we\'d use a [ASP.Net](https://hub.docker.com/_/microsoft-dotnet-core-aspnet) image to run the app; copying in the output of previous stages.\\n- Our application accesses various SQL Server databases. We struggled to get our application to connect to them. The issue related to the SSL configuration of our runner image. The fix was simple but frustrating; use a `-bionic` image as it has the configuration you need. We found that gem [here](https://github.com/dotnet/SqlClient/issues/222#issuecomment-535802822).\\n- Tests. Automated tests. We want to run them in our build; but how? Once more multi-stage builds to the rescue. We\'d build our application, then in a separate stage we\'d run the tests; copying in the app from the build stage. If the tests failed, the build failed. If they passed then the intermediate stage containing the tests would be discarded by Docker. No unnecessary bloat of the image; all that testing goodness still; now in containerised form!\\n\\n## Jenkins\\n\\nOur on premise world used TeamCity for our continuous integration needs and Octopus for deployment. We liked these tools well enough; particularly Octopus. However, the DevOps team were very much of the mind that we should be use Jenkins instead. And [Pipeline](https://jenkins.io/doc/book/pipeline/). It was here that we initially struggled. To quote the docs:\\n\\n> Jenkins Pipeline (or simply \\"Pipeline\\" with a capital \\"P\\") is a suite of plugins which supports implementing and integrating continuous delivery pipelines into Jenkins.\\n\\nWhilst continuous delivery is super cool, and is something our team was interested in, we weren\'t ready for it yet. We didn\'t yet have the kind of automated testing in place that gave us the confidence that we\'d need to move to it. One day, but not today. For now there was still some manual testing done on each release, prior to shipping. Octopus suited us very well here as it allowed us to deploy, on demand, a build of our choice to a given environment. So the question was: what to do? Fortunately the immensely talented Aby Egea came up with a mechanism that supported that very notion. A pipeline that would, optionally, deploy our build to a specified environment. So we were good!\\n\\nOne thing we got to really appreciate about Jenkins was that the build is scripted with a [Jenkinsfile](https://jenkins.io/doc/book/pipeline/jenkinsfile/). This was in contrast to our TeamCity world where it was all manually configured. [Configuration as code](https://jenkins.io/projects/jcasc/) is truly a wonderful thing as your build pipeline becomes part of your codebase; open for everyone to see and understand. If anyone wants to change the build pipeline it has to get code reviewed like everything else. It was as code in our `Jenkinsfile` that the deployment mechanism lived.\\n\\n## Vault\\n\\nAnother thing that we used Octopus for was secrets. Applications run on configuration; these are settings that drive the behaviour of your application. A subset of configuration is \\"secrets\\". Secrets are configuration that can\'t be stored in source code; they would represent a risk if they did. For instance a database connection string. We\'d been merrily using Octopus for this; as Octopus deploys an application to a server it enriches the `appsettings.json` file with any required secrets.\\n\\nWithout Octopus in the mix, how were we to handle our secrets? The answer is with [Hashicorp Vault](https://www.vaultproject.io/). We\'d store our secrets in there and, thanks to clever work by [Robski](https://uk.linkedin.com/in/robert-grzankowski-53618114) of the DevOps team, when our container was brought up by Kubernetes, it would mount into the filesystem an `appsettings.Vault.json` file which we read thanks to our trusty friend [`.AddJsonFile`](https://docs.microsoft.com/en-us/aspnet/core/fundamentals/configuration/?view=aspnetcore-3.1#json-configuration-provider) with `optional: true`. (As the file didn\'t exist in our development environment.)\\n\\nHey presto! Safe secrets in k8s.\\n\\n## Networking\\n\\nOur on premise servers sat on the company network. They could see _everything_ that there was to see. All the other servers around them on the network, bleeping and blooping. The opposite was true in AWS. There was nothing to see. Nothing to access. As it should be. It\'s safer that way should a machine become compromised. For each database and each API our application depended upon, we needed to specifically allowlist access.\\n\\n## Kerberos\\n\\nThere\'s always a fly in the ointment. A nasty surprise on a dark night. Ours was realising that our application depended upon an API that was secured using [Windows Authentication](https://docs.microsoft.com/en-us/iis/configuration/system.webserver/security/authentication/windowsauthentication/). Our application was accessing it by running under a service account which had been permissioned to access it. However, in AWS, our application wasn\'t running as under a service account on the company network. Disappointingly, in the short term the API was not going to support an alternate authentication mechanism.\\n\\nWhat to do? Honestly it wasn\'t looking good. We were considering proxying through one of our Windows servers just to get access to that API. I was tremendously disappointed. At this point our hero arrived; one [JMac](https://twitter.com/foldr) hacked together a Kerberos sidecar approach one weekend. You can see a similar approach [here](https://github.com/edseymour/kinit-sidecar). This got us to a point that allowed us to access the API we needed to.\\n\\nI\'m kind of amazed that there isn\'t better documentation out there around have a Kerberos sidecar in a k8s setup. Tragically Windows Authentication is a widely used authentication mechanism. That being the case, having good docs to show how you can get a Kerberos sidecar in place would likely greatly advance the ability of enterprises to migrate to the cloud. The best docs I\'ve found are [here](https://blog.openshift.com/kerberos-sidecar-container/). It is super hard though. _So hard!_\\n\\n## Hangfire\\n\\nWe were using [Hosted Services](https://docs.microsoft.com/en-us/aspnet/core/fundamentals/host/hosted-services?view=aspnetcore-3.1&tabs=visual-studio) to perform background task running in our app. The nature of our background tasks meant that it was important to only run a single instance of a background task at a time. Or bad things would happen. This was going to become a problem since we had ambitions to be able to horizontally scale our application; to add new pods as running our app as demand determined.\\n\\nSo we started to use [Hangfire](https://www.hangfire.io/) to perform task running in our app. With Hangfire, when a job is picked up it gets locked so other servers can\'t pick it up. That\'s what we need.\\n\\nHangfire is pretty awesome. However it turns out that there\'s quirks when you move to a containerised environment. We have a number of recurring jobs that are scheduled to run at certain dates and times. In order that Hangfire can ascertain what time it is, it needs a timezone. It turns out that timezones on Windows != timezones in Docker / Linux.\\n\\nThis was a problem because, as we limbered up for the great migration, we were trying to run our cloud implementation side by side with our on premise one. And Windows picked a fight with Linux over timezones. You can see others bumping into this condition [here](https://github.com/HangfireIO/Hangfire/issues/1268). We learned this the hard way; jobs mysteriously stopping due to timezone related errors. Windows Hangfire not able to recognise Linux Hangfire timezones and vica versa.\\n\\nThe TL;DR is that we had to do a hard switch with Hangfire; it couldn\'t run side by side. Not the end of the world, but surprising.\\n\\n## Azure Active Directory Single Sign-On\\n\\nHistorically our application had used two modes of authentication; Windows Authentication and cookies. Windows Authentication doesn\'t generally play nicely with Docker. It\'s doable, but it\'s not the hill you want to die on. So we didn\'t; we swapped out Windows Authentication for [Azure AD SSO](https://docs.microsoft.com/en-us/azure/active-directory/manage-apps/what-is-single-sign-on) and didn\'t look back.\\n\\nWe also made some changes so our app would support cookies auth alongside Azure AD auth; [I\'ve written about this previously](../2020-03-22-dual-boot-authentication-with-aspnetcore/index.md).\\n\\n## Do the right thing and tell people about it\\n\\nWe\'re there now; we\'ve made the move. It was a difficult journey but one worth making; it sets up our platform for where we want to take it in the future. Having infrastructure as code makes all kinds of approaches possible that weren\'t before. Here\'s some things we\'re hoping to get out of the move:\\n\\n- blue green deployments - shipping without taking down our platform\\n- provision environments on demand - currently we have a highly contended situation when it comes to test environments. With k8s and AWS we can look at spinning up environments as we need them and throwing them away also\\n- autoscaling for need - we can start to look at spinning up new containers in times of high load and removing excessive containers in times of low load\\n\\nWe\'ve also become more efficient as a team. We are no longer maintaining servers, renewing certificates, installing software, RDPing onto boxes. All that time and effort we can plough back into making awesome experiences for our users.\\n\\nThere\'s a long list of other benefits and it\'s very exciting indeed! It\'s not enough for us to have done this though. It\'s important that we tell the story of what we\'ve done and how and why we\'ve done it. That way people have empathy for the work. Also they can start to think about how they could start to reap similar benefits themselves. By talking to others about the road we\'ve travelled, we can save them time and help them to travel a similar road. This is good for them and it\'s good for us; it helps our relationships and it helps us all to move forwards together.\\n\\nA rising tide lifts all boats. By telling others about our journey, we raise the water level. Up to the clouds!"},{"id":"offline-storage-in-pwa","metadata":{"permalink":"/offline-storage-in-pwa","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2020-03-29-offline-storage-in-pwa/index.md","source":"@site/blog/2020-03-29-offline-storage-in-pwa/index.md","title":"Offline storage in a PWA","description":"Learn how to use IndexedDB for offline storage in your web app or PWA with the IDB-Keyval library and a React custom hook.","date":"2020-03-29T00:00:00.000Z","tags":[],"readingTime":9.07,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"offline-storage-in-pwa","title":"Offline storage in a PWA","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false,"description":"Learn how to use IndexedDB for offline storage in your web app or PWA with the IDB-Keyval library and a React custom hook."},"unlisted":false,"prevItem":{"title":"Up to the clouds!","permalink":"/up-to-clouds"},"nextItem":{"title":"Dual boot authentication with ASP.NET","permalink":"/dual-boot-authentication-with-aspnetcore"}},"content":"When you are building any kind of application it\'s typical to want to store information which persists beyond a single user session. Sometimes that will be information that you\'ll want to live in some kind of centralised database, but not always.\\n\\n\x3c!--truncate--\x3e\\n\\nAlso, you may want that data to still be available if your user is offline. Even if they can\'t connect to the network, the user may still be able to use the app to do meaningful tasks; but the app will likely require a certain amount of data to drive that.\\n\\nHow can we achieve this in the context of a PWA?\\n\\n## The problem with `localStorage`\\n\\nIf you were building a classic web app you\'d probably be reaching for [`Window.localStorage`](https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage) at this point. `Window.localStorage` is a long existing API that stores data beyond a single session. It has a simple API and is very easy to use. However, it has a couple of problems:\\n\\n1. `Window.localStorage` is synchronous. Not a tremendous problem for every app, but if you\'re building something that has significant performance needs then this could become an issue.\\n2. `Window.localStorage` cannot be used in the context of a `Worker` or a `ServiceWorker`. The APIs are not available there.\\n3. `Window.localStorage` stores only `string`s. Given [`JSON.stringify`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/JSON/stringify) and [`JSON.parse`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/JSON/parse) that\'s not a big problem. But it\'s an inconvenience.\\n\\nThe second point here is the significant one. If we\'ve a need to access our offline data in the context of a `ServiceWorker` (and if you\'re offline you\'ll be using a `ServiceWorker`) then what do you do?\\n\\n## IndexedDB to the rescue?\\n\\nFortunately, `localStorage` is not the only game in town. There\'s alternative offline storage mechanism available in browsers with the curious name of [IndexedDB](https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API). To quote the docs:\\n\\n> IndexedDB is a transactional database system, like an SQL-based RDBMS. However, unlike SQL-based RDBMSes, which use fixed-column tables, IndexedDB is a JavaScript-based object-oriented database. IndexedDB lets you store and retrieve objects that are indexed with a key; any objects supported by the structured clone algorithm can be stored. You need to specify the database schema, open a connection to your database, and then retrieve and update data within a series of transactions.\\n\\nIt\'s clear that IndexedDB is _very_ powerful. But it doesn\'t sound very simple. A further look at the [MDN example](https://github.com/mdn/to-do-notifications/blob/8b3e1708598e42062b0136608b1c5fbb66520f0a/scripts/todo.js#L48) of how to interact with IndexedDB does little to remove that thought.\\n\\nWe\'d like to be able to access data offline; but in a simple fashion. Like we could with `localStorage` which has a wonderfully straightforward API. If only someone would build an astraction on top of IndexedDB to make our lives easier...\\n\\nSomeone did.\\n\\n## IDB-Keyval to the rescue!\\n\\nThe excellent [Jake Archibald](https://twitter.com/jaffathecake) of Google has written [IDB-Keyval](https://github.com/jakearchibald/idb-keyval) which is:\\n\\n> A super-simple-small promise-based keyval store implemented with IndexedDB\\n\\nThe API is essentially equivalent to `localStorage` with a few lovely differences:\\n\\n1. The API is promise based; all functions return a `Promise`; this makes it a non-blocking API.\\n2. The API is not restricted to `string`s as `localStorage` is. To quote the docs: _this is IDB-backed, you can store anything structured-clonable (numbers, arrays, objects, dates, blobs etc)_\\n3. Because this is abstraction built on top of IndexedDB, it can be used both in the context of a typical web app and also in a `Worker` or a `ServiceWorker` if required.\\n\\n## Simple usage\\n\\nLet\'s take a look at what usage of `IDB-Keyval` might be like. For that we\'re going to need an application. It would be good to be able to demonstrate both simple usage and also how usage in the context of an application might look.\\n\\nLet\'s spin up a TypeScript React app with [Create React App](https://create-react-app.dev/):\\n\\n```shell\\nnpx create-react-app offline-storage-in-a-pwa --template typescript\\n```\\n\\nThis creates us a simple app. Now let\'s add IDB-Keyval to it:\\n\\n```shell\\nyarn add idb-keyval\\n```\\n\\nThen, let\'s update the `index.tsx` file to add a function that tests using IDB-Keyval:\\n\\n```tsx\\nimport React from \'react\';\\nimport ReactDOM from \'react-dom\';\\nimport { set, get } from \'idb-keyval\';\\nimport \'./index.css\';\\nimport App from \'./App\';\\nimport * as serviceWorker from \'./serviceWorker\';\\n\\nReactDOM.render(<App />, document.getElementById(\'root\'));\\n\\nserviceWorker.register();\\n\\nasync function testIDBKeyval() {\\n  await set(\'hello\', \'world\');\\n  const whatDoWeHave = await get(\'hello\');\\n  console.log(\\n    `When we queried idb-keyval for \'hello\', we found: ${whatDoWeHave}`,\\n  );\\n}\\n\\ntestIDBKeyval();\\n```\\n\\nAs you can see, we\'ve added a `testIDBKeyval` function which does the following:\\n\\n1. Adds a value of `\'world\'` to IndexedDB using IDB-Keyval for the key of `\'hello\'`\\n2. Queries IndexedDB using IDB-Keyval for the key of `\'hello\'` and stores it in the variable `whatDoWeHave`\\n3. Logs out what we found.\\n\\nYou\'ll also note that `testIDBKeyval` is an `async` function. This is so that we can use `await` when we\'re interacting with IDB-Keyval. Given that its API is `Promise` based, it is `await` friendly. Where you\'re performing more than an a single asynchronous operation at a time, it\'s often valuable to use `async` / `await` to increase the readability of your codebase.\\n\\nWhat happens when we run our application with `yarn start`? Let\'s do that and take a look at the devtools:\\n\\n![](hello_world_idb_keyval.webp)\\n\\nWe successfully wrote something into IndexedDB, read it back and printed that value to the console. Amazing!\\n\\n## Usage in React\\n\\nWhat we\'ve done so far is slightly abstract. It would be good to implement a real-world use case. Let\'s create an application which gives users the choice between using a \\"Dark mode\\" version of the app or not. To do that we\'ll replace our `App.tsx` with this:\\n\\n```tsx\\nimport React, { useState } from \'react\';\\nimport \'./App.css\';\\n\\nconst sharedStyles = {\\n  height: \'30rem\',\\n  fontSize: \'5rem\',\\n  textAlign: \'center\',\\n} as const;\\n\\nfunction App() {\\n  const [darkModeOn, setDarkModeOn] = useState(true);\\n  const handleOnChange = ({ target }: React.ChangeEvent<HTMLInputElement>) =>\\n    setDarkModeOn(target.checked);\\n\\n  const styles = {\\n    ...sharedStyles,\\n    ...(darkModeOn\\n      ? {\\n          backgroundColor: \'black\',\\n          color: \'white\',\\n        }\\n      : {\\n          backgroundColor: \'white\',\\n          color: \'black\',\\n        }),\\n  };\\n\\n  return (\\n    <div style={styles}>\\n      <input\\n        type=\\"checkbox\\"\\n        value=\\"darkMode\\"\\n        checked={darkModeOn}\\n        id=\\"darkModeOn\\"\\n        name=\\"darkModeOn\\"\\n        style={{ width: \'3rem\', height: \'3rem\' }}\\n        onChange={handleOnChange}\\n      />\\n      <label htmlFor=\\"darkModeOn\\">Use dark mode?</label>\\n    </div>\\n  );\\n}\\n\\nexport default App;\\n```\\n\\nWhen you run the app you can see how it works:\\n\\n![](use-dark-mode.gif)\\n\\nLooking at the code you\'ll be able to see that this is implemented using React\'s `useState` hook. So any user preference selected will be lost on a page refresh. Let\'s see if we can take this state and move it into IndexedDB using `IDB-Keyval`.\\n\\nWe\'ll change the code like so:\\n\\n```tsx\\nimport React, { useState, useEffect } from \'react\';\\nimport { set, get } from \'idb-keyval\';\\nimport \'./App.css\';\\n\\nconst sharedStyles = {\\n  height: \'30rem\',\\n  fontSize: \'5rem\',\\n  textAlign: \'center\',\\n} as const;\\n\\nfunction App() {\\n  const [darkModeOn, setDarkModeOn] = useState<boolean | undefined>(undefined);\\n\\n  useEffect(() => {\\n    get<boolean>(\'darkModeOn\').then((value) =>\\n      // If a value is retrieved then use it; otherwise default to true\\n      setDarkModeOn(value ?? true),\\n    );\\n  }, [setDarkModeOn]);\\n\\n  const handleOnChange = ({ target }: React.ChangeEvent<HTMLInputElement>) => {\\n    setDarkModeOn(target.checked);\\n\\n    set(\'darkModeOn\', target.checked);\\n  };\\n\\n  const styles = {\\n    ...sharedStyles,\\n    ...(darkModeOn\\n      ? {\\n          backgroundColor: \'black\',\\n          color: \'white\',\\n        }\\n      : {\\n          backgroundColor: \'white\',\\n          color: \'black\',\\n        }),\\n  };\\n\\n  return (\\n    <div style={styles}>\\n      {darkModeOn === undefined ? (\\n        <>Loading preferences...</>\\n      ) : (\\n        <>\\n          <input\\n            type=\\"checkbox\\"\\n            value=\\"darkMode\\"\\n            checked={darkModeOn}\\n            id=\\"darkModeOn\\"\\n            name=\\"darkModeOn\\"\\n            style={{ width: \'3rem\', height: \'3rem\' }}\\n            onChange={handleOnChange}\\n          />\\n          <label htmlFor=\\"darkModeOn\\">Use dark mode?</label>\\n        </>\\n      )}\\n    </div>\\n  );\\n}\\n\\nexport default App;\\n```\\n\\nThe changes here are:\\n\\n1. `darkModeOn` is now initialised to `undefined` and the app displays a loading message until `darkModeOn` has a value.\\n2. The app attempts to app load a value from IDB-Keyval with the key `\'darkModeOn\'` and set `darkModeOn` with the retrieved value. If no value is retrieved then it sets `darkModeOn` to `true`.\\n3. When the checkbox is changed, the corresponding value is both applied to `darkModeOn` and saved to IDB-Keyval with the key `\'darkModeOn\'`\\n\\nAs you can see, this means that we are persisting preferences beyond page refresh in a fashion that will work both online _and_ offline!\\n\\n![](use-dark-mode-with-idb-keyval.gif)\\n\\n## Usage as a React hook\\n\\nFinally it\'s time for bonus points. Wouldn\'t it be nice if we could move this functionality into a reusable React hook? Let\'s do it!\\n\\nLet\'s create a new `usePersistedState.ts` file:\\n\\n```ts\\nimport { useState, useEffect, useCallback } from \'react\';\\nimport { set, get } from \'idb-keyval\';\\n\\nexport function usePersistedState<TState>(\\n  keyToPersistWith: string,\\n  defaultState: TState,\\n) {\\n  const [state, setState] = useState<TState | undefined>(undefined);\\n\\n  useEffect(() => {\\n    get<TState>(keyToPersistWith).then((retrievedState) =>\\n      // If a value is retrieved then use it; otherwise default to defaultValue\\n      setState(retrievedState ?? defaultState),\\n    );\\n  }, [keyToPersistWith, setState, defaultState]);\\n\\n  const setPersistedValue = useCallback(\\n    (newValue: TState) => {\\n      setState(newValue);\\n      set(keyToPersistWith, newValue);\\n    },\\n    [keyToPersistWith, setState],\\n  );\\n\\n  return [state, setPersistedValue] as const;\\n}\\n```\\n\\nThis new hook is modelled after the API of [`useState`](https://reactjs.org/docs/hooks-reference.html#usestate) and is named `usePersistentState`. It requires that a key be supplied which is the key that will be used to save the data. It also requires a default value to use in the case that nothing is found during the lookup.\\n\\nIt returns (just like `useState`) a stateful value, and a function to update it. Finally, let\'s switch over our `App.tsx` to use our shiny new hook:\\n\\n```tsx\\nimport React from \'react\';\\nimport \'./App.css\';\\nimport { usePersistedState } from \'./usePersistedState\';\\n\\nconst sharedStyles = {\\n  height: \'30rem\',\\n  fontSize: \'5rem\',\\n  textAlign: \'center\',\\n} as const;\\n\\nfunction App() {\\n  const [darkModeOn, setDarkModeOn] = usePersistedState<boolean>(\\n    \'darkModeOn\',\\n    true,\\n  );\\n\\n  const handleOnChange = ({ target }: React.ChangeEvent<HTMLInputElement>) =>\\n    setDarkModeOn(target.checked);\\n\\n  const styles = {\\n    ...sharedStyles,\\n    ...(darkModeOn\\n      ? {\\n          backgroundColor: \'black\',\\n          color: \'white\',\\n        }\\n      : {\\n          backgroundColor: \'white\',\\n          color: \'black\',\\n        }),\\n  };\\n\\n  return (\\n    <div style={styles}>\\n      {darkModeOn === undefined ? (\\n        <>Loading preferences...</>\\n      ) : (\\n        <>\\n          <input\\n            type=\\"checkbox\\"\\n            value=\\"darkMode\\"\\n            checked={darkModeOn}\\n            id=\\"darkModeOn\\"\\n            name=\\"darkModeOn\\"\\n            style={{ width: \'3rem\', height: \'3rem\' }}\\n            onChange={handleOnChange}\\n          />\\n          <label htmlFor=\\"darkModeOn\\">Use dark mode?</label>\\n        </>\\n      )}\\n    </div>\\n  );\\n}\\n\\nexport default App;\\n```\\n\\n## Conclusion\\n\\nThis post has demonstrate how a web application or a PWA can safely store data that is persisted between sessions using native browser capabilities easily. IndexedDB powered the solution we\'ve built. We used used [IDB-Keyval](https://github.com/jakearchibald/idb-keyval) for the delightful and familiar abstraction it offers over IndexedDB. It\'s allowed us to come up with a solution with a similarly lovely API. It\'s worth knowing that there are alternatives to IDB-Keyval available such as [localForage](https://github.com/localForage/localForage). If you are building for older browsers which may lack good IndexedDB support then this would be a good choice. But be aware that with greater backwards compatibility comes greater download size. Do consider this and make the tradeoffs that make sense for you.\\n\\nFinally, I\'ve finished this post illustrating what usage would look like in a React context. Do be aware that there\'s nothing React specific about our offline storage mechanism. So if you\'re rolling with Vue, Angular or something else entirely: _this is for you too_! Offline storage is a feature that provide much greater user experiences. Please do consider making use of it in your applications.\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/offline-storage-for-pwas/)\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.logrocket.com/offline-storage-for-pwas/\\" />\\n</head>\\n\\n[The source code for this project can be found here.](https://github.com/johnnyreilly/offline-storage-in-a-pwa)"},{"id":"dual-boot-authentication-with-aspnetcore","metadata":{"permalink":"/dual-boot-authentication-with-aspnetcore","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2020-03-22-dual-boot-authentication-with-aspnetcore/index.md","source":"@site/blog/2020-03-22-dual-boot-authentication-with-aspnetcore/index.md","title":"Dual boot authentication with ASP.NET","description":"The article explains how to have different authentication methods for two classes of users accessing an app. Code snippets are provided.","date":"2020-03-22T00:00:00.000Z","tags":[{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."},{"inline":false,"label":"Authentication and Authorization","permalink":"/tags/auth","description":"Matters related to identity and permissioning"},{"inline":false,"label":"Azure","permalink":"/tags/azure","description":"The Microsoft cloud platform."}],"readingTime":8.05,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"dual-boot-authentication-with-aspnetcore","title":"Dual boot authentication with ASP.NET","authors":"johnnyreilly","tags":["asp.net","auth","azure"],"hide_table_of_contents":false,"description":"The article explains how to have different authentication methods for two classes of users accessing an app. Code snippets are provided."},"unlisted":false,"prevItem":{"title":"Offline storage in a PWA","permalink":"/offline-storage-in-pwa"},"nextItem":{"title":"Web Workers, comlink, TypeScript and React","permalink":"/web-workers-comlink-typescript-and-react"}},"content":"This is a post about having two kinds of authentication working at the same time in ASP.Net Core. But choosing which authentication method to use dynamically at runtime; based upon the criteria of your choice.\\n\\n\x3c!--truncate--\x3e\\n\\nAlready this sounds complicated; let\'s fix that. Perhaps I should describe my situation to you. I\'ve an app which has two classes of user. One class, let\'s call them \\"customers\\" (because... uh... they\'re customers). The customers access our application via a public facing website. Traffic rolls through Cloudflare and into our application. The public facing URL is something fancy like [https://mega-app.com](https://mega-app.com). That\'s one class of user.\\n\\nThe other class of user we\'ll call \\"our peeps\\"; because they are _us_. We use the app that we build. Traffic from \\"us\\" comes from a different hostname; only addressable on our network. So URLs from requests that we make are more along the lines of [https://strictly4mypeeps.io](https://strictly4mypeeps.io).\\n\\nSo far, so uncontroversial. Now it starts to get interesting. Our customers log into our application using their super secret credentials. It\'s [cookie based authentication](https://docs.microsoft.com/en-us/aspnet/core/security/authentication/cookie?view=aspnetcore-3.1#create-an-authentication-cookie). But for our peeps we do something different. Having to enter your credentials each time you use the app is friction. It gets in the way. So for us we have [Azure AD](https://docs.microsoft.com/en-us/aspnet/core/security/authentication/azure-active-directory/?view=aspnetcore-3.1) in the mix. Azure AD is how we authenticate ourselves; and that means we don\'t spend 5% of each working day entering credentials.\\n\\n## Let us speak of the past\\n\\nNow our delightful little application grew up in a simpler time. A time where you went to the marketplace, picked out some healthy looking servers, installed software upon them, got them attached to the internet, deployed an app onto them and said \\"hey presto, we\'re live!\\".\\n\\nWay back when, we had some servers on the internet, that\'s how our customers got to our app. Our peeps, us, we went to other servers that lived on our network. So we had multiple instances of our app, deployed to different machines. The ones on the internet were configured to use cookie based auth, the ones on our internal network were Azure AD.\\n\\nAs I said, a simpler time.\\n\\n## A new hope\\n\\nWe\'ve been going through the process of cloudifying our app. Bye, bye servers, hello [Docker](https://www.docker.com/) and [Kubernetes](https://kubernetes.io/). So exciting! As we change the way our app is built and deployed; we\'ve been thinking about whether the choices we make still make sense.\\n\\nWhen it came to authentication, my initial thoughts were to continue the same road we\'re travelling; just in containers and pods. So where we had \\"internal\\" servers, we\'d have \\"internal\\" pods, and where we\'d have \\"external\\" servers we\'d have external pods. I had the good fortune to be working with the amazingly talented [Robski](https://uk.linkedin.com/in/robert-grzankowski-53618114). Robski knows far more about K8s and networking than I\'m ever likely to. He\'d regularly say things like \\"ingress\\" and \\"MTLS\\" whilst I stared blankly at him. He definitely knows stuff.\\n\\nRobski challenged my plans. \\"We don\'t need it. Have one pod that does both sorts of auth. If you do that, your implementation is simpler and scaling is more straightforward. You\'ll only need half the pods because you won\'t need internal _and_ external ones; one pod can handle both sets of traffic. You\'ll save money.\\"\\n\\nI loved the idea but I didn\'t think that ASP.Net Core supported it. \\"It\'s just not a thing Robski; ASP.Net Core doesn\'t suppport it.\\" Robski didn\'t believe me. That turned out to a _very good thing_. There followed a period of much googling and experimentation. One day of hunting in, I was still convinced there was no way to do it that would allow me to look in the mirror without self loathing. Then Robski sent me this:\\n\\n![screenshot of WhatsApp message with a link in it](robski-dynamic-auth.webp)\\n\\nIt was a link to the amazing [David Fowler](https://twitter.com/davidfowl) talking about [some API I\'d never heard of called `SchemeSelector`](https://github.com/aspnet/Security/issues/1469#issuecomment-335027005). It turned out that this was the starting point for exactly what we needed; a way to dynamically select an authentication scheme at runtime.\\n\\n## Show me the code\\n\\nThis API did end up landing in ASP.Net Core, but with the name `ForwardDefaultSelector`. Not the most descriptive of names and I\'ve struggled to find any documentation on it at all. What I did discover was [an answer on StackOverflow by the marvellous Barbara Post](https://stackoverflow.com/a/51897159/761388). I was able to take the approach Barbara laid out and use it to my own ends. I ended up with this snippet of code added to my `Startup.ConfigureServices`:\\n\\n```cs\\nservices\\n    .AddAuthentication(sharedOptions => {\\n        sharedOptions.DefaultScheme = \\"WhichAuthDoWeUse\\";\\n        sharedOptions.DefaultAuthenticateScheme = \\"WhichAuthDoWeUse\\";\\n        sharedOptions.DefaultChallengeScheme = \\"WhichAuthDoWeUse\\";\\n    })\\n    .AddPolicyScheme(\\"WhichAuthDoWeUse\\", \\"Azure AD or Cookies\\", options => {\\n        options.ForwardDefaultSelector = context => {\\n            var (isExternalRequest, requestUrl) = context.Request.GetIsExternalRequestAndDomain();\\n            if (isExternalRequest) {\\n                _logger.LogInformation(\\n                    \\"Request ({RequestURL}) has come from external domain ({Domain}) so using Cookie Authentication\\",\\n                    requestUrl, ExternalBaseUrl);\\n\\n                return CookieAuthenticationDefaults.AuthenticationScheme;\\n           }\\n\\n           _logger.LogInformation(\\n               \\"Request ({RequestURL}) has not come from external domain ({Domain}) so using Azure AD Authentication\\",\\n               requestUrl, ExternalBaseUrl);\\n\\n            return AzureADDefaults.AuthenticationScheme;\\n        };\\n    })\\n    .AddAzureAD(options => {\\n        Configuration.Bind(\\"AzureAd\\", options);\\n    })\\n    .AddCookie(options => {\\n        options.Cookie.SecurePolicy = CookieSecurePolicy.Always;\\n        options.Cookie.SameSite = SameSiteMode.Strict;\\n        options.Cookie.HttpOnly = true;\\n        options.Events.OnRedirectToAccessDenied = (context) => {\\n            context.Response.StatusCode = Microsoft.AspNetCore.Http.StatusCodes.Status401Unauthorized;\\n            return Task.CompletedTask;\\n        };\\n\\n        options.Events.OnRedirectToLogin = (context) => {\\n            context.Response.StatusCode = Microsoft.AspNetCore.Http.StatusCodes.Status401Unauthorized;\\n            return Task.CompletedTask;\\n        };\\n    });\\n```\\n\\nIf you look at this code it\'s doing these things:\\n\\n1. Registering three types of authentication: Cookie, Azure AD and \\"WhichAuthDoWeUse\\"\\n2. Registers the default `Scheme` to be \\"WhichAuthDoWeUse\\".\\n\\n\\"WhichAuthDoWeUse\\" is effectively an `if` statement that says, _\\"if this is an external `Request` use Cookies authentication, otherwise use Azure AD\\"_. Given that \\"WhichAuthDoWeUse\\" is the default scheme, this code runs for each request, to determine which authentication method to use.\\n\\nAlongside this mechanism I added these extension methods:\\n\\n```cs\\nusing System;\\nusing Microsoft.AspNetCore.Http;\\nusing Microsoft.AspNetCore.Http.Extensions;\\n\\nnamespace My.App.Auth {\\n    public static class AuthExtensions {\\n        public const string ExternalBaseUrl = \\"https://mega-app.com\\";\\n        public const string InternalBaseUrl = \\"https://strictly4mypeeps.io\\";\\n\\n        /// <summary>\\n        /// Determines if a request is an \\"external\\" URL (eg begins \\"https://mega-app.com\\")\\n        /// or an \\"internal\\" URL (eg begins \\"https://strictly4mypeeps.io\\")\\n        /// </summary>\\n        public static (bool, string) GetIsExternalRequestAndDomain(this HttpRequest request) {\\n            var (requestUrl, domain) = GetRequestUrlAndDomain(request);\\n\\n            var isExternalUrl = domain == ExternalBaseUrl;\\n\\n            var isUnknownPath = domain == null; // This scenario is extremely unlikely but has been observed once during testing so we will cater for it\\n\\n            var isExternalRequest = isExternalUrl || isUnknownPath; // If unknown we\'ll treat as \\"external\\" for a safe fallback\\n\\n            return (isExternalRequest, requestUrl);\\n        }\\n\\n        /// <summary>\\n        /// Determines if a request is an \\"external\\" URL (eg begins \\"https://mega-app.com\\")\\n        /// or an \\"internal\\" URL (eg begins \\"https://strictly4mypeeps.io\\")\\n        /// </summary>\\n        public static (bool, string) GetIsInternalRequestAndDomain(this HttpRequest request) {\\n            var (requestUrl, domain) = GetRequestUrlAndDomain(request);\\n\\n            var isInternalRequest = domain == InternalBaseUrl;\\n\\n            return (isInternalRequest, requestUrl);\\n        }\\n\\n        private static (string, string) GetRequestUrlAndDomain(HttpRequest request) {\\n            string requestUrl = null;\\n            string domain = null;\\n            if (request.Host.HasValue) {\\n                requestUrl = request.GetEncodedUrl();\\n                domain = new Uri(requestUrl).GetLeftPart(UriPartial.Authority);\\n            }\\n\\n            return (requestUrl, domain);\\n        }\\n    }\\n}\\n```\\n\\nFinally, I updated the `SpaController.cs` (which serves initial requests to our Single Page Application) to cater for having two types of Auth in play:\\n\\n```cs\\n        /// <summary>\\n        /// ASP.NET will try and load the index.html using the FileServer if we don\'t have a route\\n        /// here to match `/`. These attributes can\'t be on Index or the spa fallback doesn\'t work\\n        /// Note: this is almost perfect except that if someone actually calls /index.html they\'ll get\\n        /// the FileServer one, not the one from this file.\\n        /// </summary>\\n        [HttpGet(\\"/\\")]\\n        [AllowAnonymous]\\n        public async Task<IActionResult> SpaFallback([FromQuery] string returnUrl) {\\n            var redirectUrlIfUserIsInternalAndNotAuthenticated = GetRedirectUrlIfUserIsInternalAndNotAuthenticated(returnUrl);\\n\\n            if (redirectUrlIfUserIsInternalAndNotAuthenticated != null)\\n                return LocalRedirect(redirectUrlIfUserIsInternalAndNotAuthenticated);\\n\\n            return await Index(); // Index just serves up our SPA index.html\\n        }\\n\\n        /// <summary>\\n        /// SPA landing with authorisation - this endpoint will typically not be directly navigated to by a user;\\n        /// rather it will be redirected to from the IndexWithoutAuthorisation and SpaFallback actions above\\n        /// in the case where a user is *not* authenticated but has come from an internal URL eg https://strictlyformypeeps.io\\n        /// </summary>\\n        [HttpGet(\\"/login-with-azure-ad\\")]\\n        [Authorize]\\n        public async Task<IActionResult> IndexWithAuthorisation()\\n        {\\n            return await Index(); // Index just serves up our SPA index.html\\n        }\\n\\n        /// <summary>\\n        /// This method returns a RedirectURL if a request is coming from an internal URL\\n        /// eg https://int.prd.our.cloud and is not authenticated.  In this case\\n        /// we likely want to trigger authentication by redirecting to an authorized endpoint\\n        /// </summary>\\n        string GetRedirectUrlIfUserIsInternalAndNotAuthenticated(string returnUrl)\\n        {\\n            // If a user is authenticated then we don\'t need to trigger authentication\\n            var isAuthenticated = User?.Identity?.Name != null;\\n            if (isAuthenticated)\\n                return null;\\n\\n            // This scenario is extremely unlikely but has been observed once during testing so we will cater for it\\n            var (isInternalRequest, requestUrl) = Request.GetIsInternalRequestAndDomain();\\n\\n            if (isInternalRequest) {\\n                var redirectUrl = $\\"/login-with-azure-ad{(string.IsNullOrEmpty(returnUrl) ? \\"\\" : \\"?returnUrl=\\" + WebUtility.UrlEncode(returnUrl))}\\";\\n                _logger.LogInformation(\\n                    \\"Request ({RequestURL}) has come from internal domain ({InternalDomain}) but is not authenticated; redirecting to {RedirectURL}\\",\\n                    requestUrl, AuthExtensions.InternalBaseUrl, redirectUrl);\\n\\n                return redirectUrl;\\n            }\\n\\n            return null;\\n        }\\n```\\n\\nThe code above allows anonymous requests to land in our app through the `AllowAnonymous` attribute. However, it checks the request when it comes in to see if:\\n\\n1. It\'s an internal request (i.e. the Request URL starts \\"[https://strictly4mypeeps.io/\\"](https://strictly4mypeeps.io/\\"))\\n2. The current user is _not_ authenticated.\\n\\nIn this case the user is redirected to the [https://strictly4mypeeps.io/login-with-azure-ad](https://strictly4mypeeps.io/login-with-azure-ad) route which is decorated with the `Authorize` attribute. This will trigger authentication for our unauthenticated internal users and drive them through the Azure AD login process.\\n\\n## The mystery of no documentation\\n\\nI\'m so surprised that this approach hasn\'t yet been better documented on the (generally superb) ASP.Net Core docs. It\'s such a potentially useful approach; and in our case, money saving too! I hope the official docs feature something on this in future. If they do, and I\'ve just missed it (possible!) then please hit me up in the comments."},{"id":"web-workers-comlink-typescript-and-react","metadata":{"permalink":"/web-workers-comlink-typescript-and-react","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2020-02-21-web-workers-comlink-typescript-and-react/index.md","source":"@site/blog/2020-02-21-web-workers-comlink-typescript-and-react/index.md","title":"Web Workers, comlink, TypeScript and React","description":"Learn how to use Web Workers in a React app using Googles comlink library. Offload long-running calculations to a separate thread.","date":"2020-02-21T00:00:00.000Z","tags":[{"inline":false,"label":"React","permalink":"/tags/react","description":"The React library."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":9.805,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"web-workers-comlink-typescript-and-react","title":"Web Workers, comlink, TypeScript and React","authors":"johnnyreilly","tags":["react","typescript"],"hide_table_of_contents":false,"description":"Learn how to use Web Workers in a React app using Googles comlink library. Offload long-running calculations to a separate thread."},"unlisted":false,"prevItem":{"title":"Dual boot authentication with ASP.NET","permalink":"/dual-boot-authentication-with-aspnetcore"},"nextItem":{"title":"From create-react-app to PWA","permalink":"/from-create-react-app-to-pwa"}},"content":"JavaScript is famously single threaded. However, if you\'re developing for the web, you may well know that this is not quite accurate. There are [`Web Workers`](https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Using_web_workers):\\n\\n> A worker is an object created using a constructor (e.g. `Worker()`) that runs a named JavaScript file \u2014 this file contains the code that will run in the worker thread; workers run in another global context that is different from the current window.\\n\\nIf you\'re using Vite to build your React app, you may [prefer to read this post](../2024-06-23-web-workers-comlink-vite-tanstack-query/index.md).\\n\\n\x3c!--truncate--\x3e\\n\\nGiven that there is a way to use other threads for background processing, why doesn\'t this happen all the time? Well there\'s a number of reasons; not the least of which is the ceremony involved in interacting with Web Workers. Consider the following example that illustrates moving a calculation into a worker:\\n\\n```js\\n// main.js\\nfunction add2NumbersUsingWebWorker() {\\n  const myWorker = new Worker(\'worker.js\');\\n\\n  myWorker.postMessage([42, 7]);\\n  console.log(\'Message posted to worker\');\\n\\n  myWorker.onmessage = function (e) {\\n    console.log(\'Message received from worker\', e.data);\\n  };\\n}\\n\\nadd2NumbersUsingWebWorker();\\n\\n// worker.js\\nonmessage = function (e) {\\n  console.log(\'Worker: Message received from main script\');\\n  const result = e.data[0] * e.data[1];\\n  if (isNaN(result)) {\\n    postMessage(\'Please write two numbers\');\\n  } else {\\n    const workerResult = \'Result: \' + result;\\n    console.log(\'Worker: Posting message back to main script\');\\n    postMessage(workerResult);\\n  }\\n};\\n```\\n\\n_This is not simple._ It\'s hard to understand what\'s happening. Also, this approach only supports a single method call. I\'d much rather write something that looked more like this:\\n\\n```js\\n// main.js\\nfunction add2NumbersUsingWebWorker() {\\n  const myWorker = new Worker(\'worker.js\');\\n\\n  const total = myWorker.add2Numbers([42, 7]);\\n  console.log(\'Message received from worker\', total);\\n}\\n\\nadd2NumbersUsingWebWorker();\\n\\n// worker.js\\nexport function add2Numbers(firstNumber, secondNumber) {\\n  const result = firstNumber + secondNumber;\\n  return isNaN(result) ? \'Please write two numbers\' : \'Result: \' + result;\\n}\\n```\\n\\nThere\'s a way to do this using a library made by Google called [comlink](https://github.com/GoogleChromeLabs/comlink). This post will demonstrate how we can use this. We\'ll use TypeScript and webpack. We\'ll also examine how to integrate this approach into a React app.\\n\\n## A use case for a Web Worker\\n\\nLet\'s make ourselves a TypeScript web app. We\'re going to use `create-react-app` for this:\\n\\n```shell\\nnpx create-react-app webworkers-comlink-typescript-react --template typescript\\n```\\n\\nCreate a `takeALongTimeToDoSomething.ts` file alongside `index.tsx`:\\n\\n```ts\\nexport function takeALongTimeToDoSomething() {\\n  console.log(\'Start our long running job...\');\\n  const seconds = 5;\\n  const start = new Date().getTime();\\n  const delay = seconds * 1000;\\n\\n  while (true) {\\n    if (new Date().getTime() - start > delay) {\\n      break;\\n    }\\n  }\\n  console.log(\'Finished our long running job\');\\n}\\n```\\n\\nTo `index.tsx` add this code:\\n\\n```ts\\nimport { takeALongTimeToDoSomething } from \'./takeALongTimeToDoSomething\';\\n\\n// ...\\n\\nconsole.log(\'Do something\');\\ntakeALongTimeToDoSomething();\\nconsole.log(\'Do another thing\');\\n```\\n\\nWhen our application runs we see this behaviour:\\n\\n![janky application](blocking.gif)\\n\\nThe app starts and logs `Do something` and `Start our long running job...` to the console. It then blocks the UI until the `takeALongTimeToDoSomething` function has completed running. During this time the screen is empty and unresponsive. This is a poor user experience.\\n\\n## Hello `worker-plugin` and `comlink`\\n\\nTo start using comlink we\'re going to need to eject our `create-react-app` application. The way `create-react-app` works is by giving you a setup that handles a high percentage of the needs for a typical web app. When you encounter an unsupported use case, you can run the `yarn eject` command to get direct access to the configuration of your setup.\\n\\nWeb Workers are not that commonly used in day to day development at present. Consequently there isn\'t yet a \\"plug\'n\'play\\" solution for workers supported by `create-react-app`. There\'s a number of potential ways to support this use case and you can track the various discussions happening against `create-react-app` that covers this. For now, let\'s eject with:\\n\\n```bash\\nyarn eject\\n```\\n\\nThen let\'s install the packages we\'re going to be using:\\n\\n- [`worker-plugin`](https://github.com/GoogleChromeLabs/worker-plugin) \\\\- this webpack plugin automatically compiles modules loaded in Web Workers\\n- `comlink` - this library provides the RPC-like experience that we want from our workers\\n\\n```bash\\nyarn add comlink worker-plugin\\n```\\n\\nWe now need to tweak our `webpack.config.js` to use the `worker-plugin`:\\n\\n```js\\nconst WorkerPlugin = require(\'worker-plugin\');\\n\\n// ....\\n\\n    plugins: [\\n      new WorkerPlugin(),\\n\\n// ....\\n```\\n\\nDo note that there\'s a number of `plugins` statements in the `webpack.config.js`. You want the top level one; look out for the `new HtmlWebpackPlugin` statement and place your `new WorkerPlugin(),` before that.\\n\\n## Workerize our slow process\\n\\nNow we\'re ready to take our long running process and move it into a worker. Inside the `src` folder, create a new folder called `my-first-worker`. Our worker is going to live in here. Into this folder we\'re going to add a `tsconfig.json` file:\\n\\n```json\\n{\\n  \\"compilerOptions\\": {\\n    \\"strict\\": true,\\n    \\"target\\": \\"esnext\\",\\n    \\"module\\": \\"esnext\\",\\n    \\"lib\\": [\\"webworker\\", \\"esnext\\"],\\n    \\"moduleResolution\\": \\"node\\",\\n    \\"noUnusedLocals\\": true,\\n    \\"sourceMap\\": true,\\n    \\"allowJs\\": false,\\n    \\"baseUrl\\": \\".\\"\\n  }\\n}\\n```\\n\\nThis file exists to tell TypeScript that this is a Web Worker. Do note the `\\"lib\\": [ \\"webworker\\"` usage which does exactly that.\\n\\nAlongside the `tsconfig.json` file, let\'s create an `index.ts` file. This will be our worker:\\n\\n```ts\\nimport { expose } from \'comlink\';\\nimport { takeALongTimeToDoSomething } from \'../takeALongTimeToDoSomething\';\\n\\nconst exports = {\\n  takeALongTimeToDoSomething,\\n};\\nexport type MyFirstWorker = typeof exports;\\n\\nexpose(exports);\\n```\\n\\nThere\'s a number of things happening in our small worker file. Let\'s go through this statement by statement:\\n\\n```ts\\nimport { expose } from \'comlink\';\\n```\\n\\nHere we\'re importing the `expose` method from comlink. Comlink\u2019s goal is to make *expose*d values from one thread available in the other. The `expose` method can be viewed as the comlink equivalent of `export`. It is used to export the RPC style signature of our worker. We\'ll see it\'s use later.\\n\\n```ts\\nimport { takeALongTimeToDoSomething } from \'../takeALongTimeToDoSomething\';\\n```\\n\\nHere we\'re going to import our `takeALongTimeToDoSomething` function that we wrote previously, so we can use it in our worker.\\n\\n```ts\\nconst exports = {\\n  takeALongTimeToDoSomething,\\n};\\n```\\n\\nHere we\'re creating the public facing API that we\'re going to expose.\\n\\n```ts\\nexport type MyFirstWorker = typeof exports;\\n```\\n\\nWe\'re going to want our worker to be strongly typed. This line creates a type called `MyFirstWorker` which is derived from our `exports` object literal.\\n\\n```ts\\nexpose(exports);\\n```\\n\\nFinally we expose the `exports` using comlink. We\'re done; that\'s our worker finished. Now let\'s consume it. Let\'s change our `index.tsx` file to use it. Replace our import of `takeALongTimeToDoSomething`:\\n\\n```ts\\nimport { takeALongTimeToDoSomething } from \'./takeALongTimeToDoSomething\';\\n```\\n\\nWith an import of `wrap` from comlink that creates a local `takeALongTimeToDoSomething` function that wraps interacting with our worker:\\n\\n```ts\\nimport { wrap } from \'comlink\';\\n\\nfunction takeALongTimeToDoSomething() {\\n  const worker = new Worker(\'./my-first-worker\', {\\n    name: \'my-first-worker\',\\n    type: \'module\',\\n  });\\n  const workerApi = wrap<import(\'./my-first-worker\').MyFirstWorker>(worker);\\n  workerApi.takeALongTimeToDoSomething();\\n}\\n```\\n\\nNow we\'re ready to demo our application using our function offloaded into a Web Worker. It now behaves like this:\\n\\n![not janky app](non-blocking.gif)\\n\\nThere\'s a number of exciting things to note here:\\n\\n1. The application is now non-blocking. Our long running function is now not preventing the UI from updating\\n2. The functionality is lazily loaded via a `my-first-worker.chunk.worker.js` that has been created by the `worker-plugin` and `comlink`.\\n\\n## Using Web Workers in React\\n\\nThe example we\'ve showed so far demostrates how you could use Web Workers and why you might want to. However, it\'s a far cry from a real world use case. Let\'s take the next step and plug our Web Worker usage into our React application. What would that look like? Let\'s find out.\\n\\nWe\'ll return `index.tsx` back to it\'s initial state. Then we\'ll make a simple adder function that takes some values and returns their total. To our `takeALongTimeToDoSomething.ts` module let\'s add:\\n\\n```ts\\nexport function takeALongTimeToAddTwoNumbers(number1: number, number2: number) {\\n  console.log(\'Start to add...\');\\n  const seconds = 5;\\n  const start = new Date().getTime();\\n  const delay = seconds * 1000;\\n  while (true) {\\n    if (new Date().getTime() - start > delay) {\\n      break;\\n    }\\n  }\\n  const total = number1 + number2;\\n  console.log(\'Finished adding\');\\n  return total;\\n}\\n```\\n\\nLet\'s start using our long running calculator in a React component. We\'ll update our `App.tsx` to use this function and create a simple adder component:\\n\\n```tsx\\nimport React, { useState } from \'react\';\\nimport \'./App.css\';\\nimport { takeALongTimeToAddTwoNumbers } from \'./takeALongTimeToDoSomething\';\\n\\nconst App: React.FC = () => {\\n  const [number1, setNumber1] = useState(1);\\n  const [number2, setNumber2] = useState(2);\\n\\n  const total = takeALongTimeToAddTwoNumbers(number1, number2);\\n\\n  return (\\n    <div className=\\"App\\">\\n      <h1>Web Workers in action!</h1>\\n\\n      <div>\\n        <label>Number to add: </label>\\n        <input\\n          type=\\"number\\"\\n          onChange={(e) => setNumber1(parseInt(e.target.value))}\\n          value={number1}\\n        />\\n      </div>\\n      <div>\\n        <label>Number to add: </label>\\n        <input\\n          type=\\"number\\"\\n          onChange={(e) => setNumber2(parseInt(e.target.value))}\\n          value={number2}\\n        />\\n      </div>\\n      <h2>Total: {total}</h2>\\n    </div>\\n  );\\n};\\n\\nexport default App;\\n```\\n\\nWhen you try it out you\'ll notice that entering a single digit locks the UI for 5 seconds whilst it adds the numbers. From the moment the cursor stops blinking to the moment the screen updates the UI is non-responsive:\\n\\n![janky](blocking-react.gif)\\n\\nSo far, so classic. Let\'s Web Workerify this!\\n\\nWe\'ll update our `my-first-worker/index.ts` to import this new function:\\n\\n```ts\\nimport { expose } from \'comlink\';\\nimport {\\n  takeALongTimeToDoSomething,\\n  takeALongTimeToAddTwoNumbers,\\n} from \'../takeALongTimeToDoSomething\';\\n\\nconst exports = {\\n  takeALongTimeToDoSomething,\\n  takeALongTimeToAddTwoNumbers,\\n};\\nexport type MyFirstWorker = typeof exports;\\n\\nexpose(exports);\\n```\\n\\nAlongside our `App.tsx` file let\'s create an `App.hooks.ts` file.\\n\\n```ts\\nimport { wrap, releaseProxy } from \'comlink\';\\nimport { useEffect, useState, useMemo } from \'react\';\\n\\n/**\\n * Our hook that performs the calculation on the worker\\n */\\nexport function useTakeALongTimeToAddTwoNumbers(\\n  number1: number,\\n  number2: number,\\n) {\\n  // We\'ll want to expose a wrapping object so we know when a calculation is in progress\\n  const [data, setData] = useState({\\n    isCalculating: false,\\n    total: undefined as number | undefined,\\n  });\\n\\n  // acquire our worker\\n  const { workerApi } = useWorker();\\n\\n  useEffect(() => {\\n    // We\'re starting the calculation here\\n    setData({ isCalculating: true, total: undefined });\\n\\n    workerApi\\n      .takeALongTimeToAddTwoNumbers(number1, number2)\\n      .then((total) => setData({ isCalculating: false, total })); // We receive the result here\\n  }, [workerApi, setData, number1, number2]);\\n\\n  return data;\\n}\\n\\nfunction useWorker() {\\n  // memoise a worker so it can be reused; create one worker up front\\n  // and then reuse it subsequently; no creating new workers each time\\n  const workerApiAndCleanup = useMemo(() => makeWorkerApiAndCleanup(), []);\\n\\n  useEffect(() => {\\n    const { cleanup } = workerApiAndCleanup;\\n\\n    // cleanup our worker when we\'re done with it\\n    return () => {\\n      cleanup();\\n    };\\n  }, [workerApiAndCleanup]);\\n\\n  return workerApiAndCleanup;\\n}\\n\\n/**\\n * Creates a worker, a cleanup function and returns it\\n */\\nfunction makeWorkerApiAndCleanup() {\\n  // Here we create our worker and wrap it with comlink so we can interact with it\\n  const worker = new Worker(\'./my-first-worker\', {\\n    name: \'my-first-worker\',\\n    type: \'module\',\\n  });\\n  const workerApi = wrap<import(\'./my-first-worker\').MyFirstWorker>(worker);\\n\\n  // A cleanup function that releases the comlink proxy and terminates the worker\\n  const cleanup = () => {\\n    workerApi[releaseProxy]();\\n    worker.terminate();\\n  };\\n\\n  const workerApiAndCleanup = { workerApi, cleanup };\\n\\n  return workerApiAndCleanup;\\n}\\n```\\n\\nThe `useWorker` and `makeWorkerApiAndCleanup` functions make up the basis of a shareable worker hooks approach. It would take very little work to paramaterise them so this could be used elsewhere. That\'s outside the scope of this post but would be extremely straightforward to accomplish.\\n\\nTime to test! We\'ll change our `App.tsx` to use the new `useTakeALongTimeToAddTwoNumbers` hook:\\n\\n```tsx\\nimport React, { useState } from \'react\';\\nimport \'./App.css\';\\nimport { useTakeALongTimeToAddTwoNumbers } from \'./App.hooks\';\\n\\nconst App: React.FC = () => {\\n  const [number1, setNumber1] = useState(1);\\n  const [number2, setNumber2] = useState(2);\\n\\n  const total = useTakeALongTimeToAddTwoNumbers(number1, number2);\\n\\n  return (\\n    <div className=\\"App\\">\\n      <h1>Web Workers in action!</h1>\\n\\n      <div>\\n        <label>Number to add: </label>\\n        <input\\n          type=\\"number\\"\\n          onChange={(e) => setNumber1(parseInt(e.target.value))}\\n          value={number1}\\n        />\\n      </div>\\n      <div>\\n        <label>Number to add: </label>\\n        <input\\n          type=\\"number\\"\\n          onChange={(e) => setNumber2(parseInt(e.target.value))}\\n          value={number2}\\n        />\\n      </div>\\n      <h2>\\n        Total:{\' \'}\\n        {total.isCalculating ? (\\n          <em>Calculating...</em>\\n        ) : (\\n          <strong>{total.total}</strong>\\n        )}\\n      </h2>\\n    </div>\\n  );\\n};\\n\\nexport default App;\\n```\\n\\nNow our calculation takes place off the main thread and the UI is no longer blocked!\\n\\n![not janky](non-blocking-react.gif)\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/integrating-web-workers-in-a-react-app-with-comlink/)\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.logrocket.com/integrating-web-workers-in-a-react-app-with-comlink/\\" />\\n</head>\\n\\n[The source code for this project can be found here.](https://github.com/johnnyreilly/webworkers-comlink-typescript-react)"},{"id":"from-create-react-app-to-pwa","metadata":{"permalink":"/from-create-react-app-to-pwa","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2020-01-31-from-create-react-app-to-pwa/index.md","source":"@site/blog/2020-01-31-from-create-react-app-to-pwa/index.md","title":"From create-react-app to PWA","description":"Learn how to build a basic Progressive Web App with React and TypeScript, as well as how to add features like code splitting and deployment.","date":"2020-01-31T00:00:00.000Z","tags":[{"inline":false,"label":"React","permalink":"/tags/react","description":"The React library."}],"readingTime":10.745,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"from-create-react-app-to-pwa","title":"From create-react-app to PWA","authors":"johnnyreilly","tags":["react"],"hide_table_of_contents":false,"description":"Learn how to build a basic Progressive Web App with React and TypeScript, as well as how to add features like code splitting and deployment."},"unlisted":false,"prevItem":{"title":"Web Workers, comlink, TypeScript and React","permalink":"/web-workers-comlink-typescript-and-react"},"nextItem":{"title":"LICENSE to kill your PWA","permalink":"/license-to-kill-your-pwa"}},"content":"Progressive Web Apps are a (terribly named) wonderful idea. You can build an app _once_ using web technologies which serves all devices and form factors. It can be accessible over the web, but also surface on the home screen of your Android / iOS device. That app can work offline, have a splash screen when it launches and have notifications too.\\n\\n\x3c!--truncate--\x3e\\n\\nPWAs can be a money saver for your business. The alternative, should you want an app experience for your users, is building the same application using three different technologies (one for web, one for Android and one for iOS). When you take this path it\'s hard to avoid a multiplication of cost and complexity. It often leads to dividing up the team as each works on a different stack. It\'s common to lose a certain amount of focus as a consequence. PWAs can help here; they are a compelling alternative, not just from a developers standpoint, but from a resourcing one too.\\n\\nHowever, the downside of PWAs is that they are more complicated than normal web apps. Writing one from scratch is just less straightforward than a classic web app. There are easy onramps to building a PWA that help you fall into the pit of success. This post will highlight one of these. How you can travel from zero to a PWA of your very own using React and TypeScript.\\n\\nThis post presumes knowledge of:\\n\\n- React\\n- TypeScript\\n- Node\\n\\n## From console to web app\\n\\nTo create our PWA we\'re going to use [`create-react-app`](https://create-react-app.dev/). This excellent project has long had inbuilt support for making PWAs. In recent months that support has matured to a very satisfactory level. To create ourselves a TypeScript React app using `create-react-app` enter this `npx` command at the console:\\n\\n```shell\\nnpx create-react-app pwa-react-typescript --template typescript\\n```\\n\\nThis builds you a react web app built with TypeScript; it can be tested locally with:\\n\\n```shell\\ncd pwa-react-typescript\\nyarn start\\n```\\n\\n## From web app to PWA\\n\\nFrom web app to PWA is incredibly simple; it\u2019s just a question of opting in to offline behaviour. If you open up the `index.tsx` file in your newly created project you\'ll find this code:\\n\\n```ts\\n// If you want your app to work offline and load faster, you can change\\n// unregister() to register() below. Note this comes with some pitfalls.\\n// Learn more about service workers: https://bit.ly/CRA-PWA\\nserviceWorker.unregister();\\n```\\n\\nAs the hint suggests, swap `serviceWorker.unregister()` for `serviceWorker.register()` and you now have a PWA. Amazing! What does this mean? Well to [quote the docs](https://create-react-app.dev/docs/making-a-progressive-web-app/#why-opt-in):\\n\\n> - All static site assets are cached so that your page loads fast on subsequent visits, regardless of network connectivity (such as 2G or 3G). Updates are downloaded in the background.\\n> - Your app will work regardless of network state, even if offline. This means your users will be able to use your app at 10,000 feet and on the subway.\\n>\\n> ... it will take care of generating a service worker file that will automatically precache all of your local assets and keep them up to date as you deploy updates. The service worker will use a [cache-first strategy](https://developers.google.com/web/fundamentals/instant-and-offline/offline-cookbook/#cache-falling-back-to-network)for handling all requests for local assets, including [navigation requests](https://developers.google.com/web/fundamentals/primers/service-workers/high-performance-loading#first_what_are_navigation_requests) for your HTML, ensuring that your web app is consistently fast, even on a slow or unreliable network.\\n\\nUnder the bonnet, `create-react-app` is achieving this through the use of technology called [\\"Workbox\\"](https://developers.google.com/web/tools/workbox). Workbox describes itself as:\\n\\n> a set of libraries and Node modules that make it easy to cache assets and take full advantage of features used to build [Progressive Web Apps](https://developers.google.com/web/progressive-web-apps/).\\n\\nThe good folks of Google are aware that writing your own PWA can be tricky. There\'s much new behaviour to configure and be aware of; it\'s easy to make mistakes. Workbox is there to help ease the way forward by implementing default strategies for caching / offline behaviour which can be controlled through configuration.\\n\\nA downside of the usage of `Workbox` in `create-react-app` is that (as with most things `create-react-app`) there\'s little scope for configuration of your own if the defaults don\'t serve your purpose. This may change in the future, indeed [there\'s an open PR that adds this support](https://github.com/facebook/create-react-app/pull/5369).\\n\\n## Icons and splash screens and A2HS, oh my!\\n\\nBut it\'s not just an offline experience that makes this a PWA. Other important factors are:\\n\\n- That the app can be added to your home screen (A2HS AKA \\"installed\\").\\n- That the app has a name and an icon which can be customised.\\n- That there\'s a splash screen displayed to the user as the app starts up.\\n\\nAll of the above is \\"in the box\\" with `create-react-app`. Let\'s start customizing these.\\n\\nFirst of all, we\'ll give our app a name. Fire up `index.html` and replace `&lt;title&gt;React App&lt;/title&gt;` with `&lt;title&gt;My PWA&lt;/title&gt;`. (Feel free to concoct a more imaginative name than the one I\'ve suggested.) Next open up `manifest.json` and replace:\\n\\n```json\\n\\"short_name\\": \\"React App\\",\\n  \\"name\\": \\"Create React App Sample\\",\\n```\\n\\nwith:\\n\\n```json\\n\\"short_name\\": \\"My PWA\\",\\n  \\"name\\": \\"My PWA\\",\\n```\\n\\nYour app now has a name. The question you might be asking is: what is this `manifest.json` file? Well to [quote the good folks of Google](https://developers.google.com/web/fundamentals/web-app-manifest):\\n\\n> The [web app manifest](https://developer.mozilla.org/en-US/docs/Web/Manifest) is a simple JSON file that tells the browser about your web application and how it should behave when \'installed\' on the user\'s mobile device or desktop. Having a manifest is required by Chrome to show the [Add to Home Screen prompt](https://developers.google.com/web/fundamentals/app-install-banners/).\\n>\\n> A typical manifest file includes information about the app name, icons it should use, the start_url it should start at when launched, and more.\\n\\nSo the `manifest.json` is essentially metadata about your app. Here\'s what it should look like right now:\\n\\n```json\\n{\\n  \\"short_name\\": \\"My PWA\\",\\n  \\"name\\": \\"My PWA\\",\\n  \\"icons\\": [\\n    {\\n      \\"src\\": \\"favicon.ico\\",\\n      \\"sizes\\": \\"64x64 32x32 24x24 16x16\\",\\n      \\"type\\": \\"image/x-icon\\"\\n    },\\n    {\\n      \\"src\\": \\"logo192.png\\",\\n      \\"type\\": \\"image/png\\",\\n      \\"sizes\\": \\"192x192\\"\\n    },\\n    {\\n      \\"src\\": \\"logo512.png\\",\\n      \\"type\\": \\"image/png\\",\\n      \\"sizes\\": \\"512x512\\"\\n    }\\n  ],\\n  \\"start_url\\": \\".\\",\\n  \\"display\\": \\"standalone\\",\\n  \\"theme_color\\": \\"#000000\\",\\n  \\"background_color\\": \\"#ffffff\\"\\n}\\n```\\n\\nYou can use the above properties (and others not yet configured) to control how your app behaves. For instance, if you want to replace icons your app uses then it\'s a simple matter of:\\n\\n- placing new logo files in the `public` folder\\n- updating references to them in the `manifest.json`\\n- finally, for older Apple devices, updating the `&lt;link rel=\\"apple-touch-icon\\" ... /&gt;` in the `index.html`.\\n\\n## Where are we?\\n\\nSo far, we have a basic PWA in place. It\'s installable. You can run it locally and develop it with `yarn start`. You can build it for deployment with `yarn build`.\\n\\nWhat this isn\'t, is recognisably a web app. In the sense that it doesn\'t have support for different pages / URLs. We\'re typically going to want to break up our application this way. Let\'s do that now. We\'re going to use [`react-router`](https://github.com/ReactTraining/react-router); the de facto routing solution for React. To add it to our project (and the required type definitions for TypeScript) we use:\\n\\n```\\nyarn add react-router-dom @types/react-router-dom\\n```\\n\\nNow let\'s split up our app into a couple of pages. We\'ll replace the existing `App.tsx` with this:\\n\\n```tsx\\nimport React from \'react\';\\nimport { BrowserRouter as Router, Switch, Route, Link } from \'react-router-dom\';\\nimport About from \'./About\';\\nimport Home from \'./Home\';\\n\\nconst App: React.FC = () => (\\n  <Router>\\n    <nav>\\n      <ul>\\n        <li>\\n          <Link to=\\"/\\">Home</Link>\\n        </li>\\n        <li>\\n          <Link to=\\"/about\\">About</Link>\\n        </li>\\n      </ul>\\n    </nav>\\n    <Switch>\\n      <Route path=\\"/about\\">\\n        <About />\\n      </Route>\\n      <Route path=\\"/\\">\\n        <Home />\\n      </Route>\\n    </Switch>\\n  </Router>\\n);\\n\\nexport default App;\\n```\\n\\nThis will be our root page. It has the responsiblity of using `react-router` to render the pages we want to serve, and also to provide the links that allow users to navigate to those pages. In making our changes we\'ll have broken our test (which checked for a link we\'ve now deleted), so we\'ll fix it like so:\\n\\nReplace the `App.test.tsx` with this:\\n\\n```tsx\\nimport React from \'react\';\\nimport { render } from \'@testing-library/react\';\\nimport App from \'./App\';\\n\\ntest(\'renders about link\', () => {\\n  const { getByText } = render(<App />);\\n  const linkElement = getByText(/about/i);\\n  expect(linkElement).toBeInTheDocument();\\n});\\n```\\n\\nYou\'ll have noticed that in our new `App.tsx` we import two new components (or pages); `About` and `Home`. Let\'s create those. First `About.tsx`:\\n\\n```tsx\\nimport React from \'react\';\\n\\nconst About: React.FC = () => <h1>This is a PWA</h1>;\\n\\nexport default About;\\n```\\n\\nThen `Home.tsx`:\\n\\n```tsx\\nimport React from \'react\';\\n\\nconst Home: React.FC = () => <h1>Welcome to your PWA!</h1>;\\n\\nexport default Home;\\n```\\n\\n## Code splitting\\n\\nNow we\'ve split up our app into multiple sections, we\'re going to split the code too. A good way to improve loading times for PWAs is to ensure that the code is not built into big files. At the moment our app builds a `single-file.js`. If you run `yarn build` you\'ll see what this looks like:\\n\\n```\\n47.88 KB  build/static/js/2.89bc6648.chunk.js\\n  784 B     build/static/js/runtime-main.9c116153.js\\n  555 B     build/static/js/main.bc740179.chunk.js\\n  269 B     build/static/css/main.5ecd60fb.chunk.css\\n```\\n\\nNotice the `build/static/js/main.bc740179.chunk.js` file. This is our `single-file.js`. It represents the compiled output of building the TypeScript files that make up our app. It will grow and grow as our app grows, eventually becoming problematic from a user loading speed perspective.\\n\\n`create-react-app` is built upon webpack. There is excellent support for code splitting in webpack and hence [create-react-app supports it by default](https://reactjs.org/docs/code-splitting.html#code-splitting). Let\'s apply it to our app. Again we\'re going to change `App.tsx`.\\n\\nWhere we previously had:\\n\\n```tsx\\nimport About from \'./About\';\\nimport Home from \'./Home\';\\n```\\n\\nLet\'s replace with:\\n\\n```tsx\\nconst About = lazy(() => import(\'./About\'));\\nconst Home = lazy(() => import(\'./Home\'));\\n```\\n\\nThis is the syntax to lazily load components in React. You\'ll note that it internally uses the [dynamic `import()` syntax](https://github.com/tc39/proposal-dynamic-import) which webpack uses as a \\"split point\\".\\n\\nLet\'s also give React something to render whilst it waits for the dynamic imports to be resolved. Just inside our `&lt;Router&gt;` component we\'ll add a `&lt;Suspense&gt;` component too:\\n\\n```tsx\\n<Router>\\n  <Suspense fallback={<div>Loading...</div>}>{/*...*/}</Suspense>\\n</Router>\\n```\\n\\nThe `&lt;Suspense&gt;` component will render the `&lt;div&gt;Loading...&lt;/div&gt;` whilst it waits for a routes code to be dynamically loaded. So our final `App.tsx` component ends up looking like this:\\n\\n```tsx\\nimport React, { lazy, Suspense } from \'react\';\\nimport { BrowserRouter as Router, Switch, Route, Link } from \'react-router-dom\';\\nconst About = lazy(() => import(\'./About\'));\\nconst Home = lazy(() => import(\'./Home\'));\\n\\nconst App: React.FC = () => (\\n  <Router>\\n    <Suspense fallback={<div>Loading...</div>}>\\n      <nav>\\n        <ul>\\n          <li>\\n            <Link to=\\"/\\">Home</Link>\\n          </li>\\n          <li>\\n            <Link to=\\"/about\\">About</Link>\\n          </li>\\n        </ul>\\n      </nav>\\n      <Switch>\\n        <Route path=\\"/about\\">\\n          <About />\\n        </Route>\\n        <Route path=\\"/\\">\\n          <Home />\\n        </Route>\\n      </Switch>\\n    </Suspense>\\n  </Router>\\n);\\n\\nexport default App;\\n```\\n\\nThis is now a code split application. How can we tell? If we run `yarn build` again we\'ll see something like this:\\n\\n```\\n47.88 KB          build/static/js/2.89bc6648.chunk.js\\n  1.18 KB (+428 B)  build/static/js/runtime-main.415ab5ea.js\\n  596 B (+41 B)     build/static/js/main.e60948bb.chunk.js\\n  269 B             build/static/css/main.5ecd60fb.chunk.css\\n  233 B             build/static/js/4.0c85e1cb.chunk.js\\n  228 B             build/static/js/3.eed49094.chunk.js\\n```\\n\\nNote that we now have multiple `*.chunk.js` files. Our initial `main.*.chunk.js` and then `3.*.chunk.js` representing `Home.tsx` and `4.*.chunk.js` representing `About.tsx`.\\n\\nAs we continue to build out our app from this point we\'ll have a great approach in place to ensure that users load files as they need to and that those files should not be too large. Great performance which will scale.\\n\\n## Deploy your PWA\\n\\nNow that we have our basic PWA in place, let\'s deploy it so the outside world can appreciate it. We\'re going to use [Netlify](https://www.netlify.com/) for this.\\n\\nThe source code of our PWA lives on GitHub here: https://github.com/johnnyreilly/pwa-react-typescript\\n\\nWe\'re going to log into Netlify, click on the \\"Create a new site\\" option and select GitHub as the provider. We\'ll need to authorize Netlify to access our GitHub.\\n\\n![screenshot of the Netlify auth flow](netlify-auth.webp)\\n\\nYou may need to click the \\"Configure Netlify on GitHub\\" button to grant permissions for Netlify to access your repo like so:\\n\\n![screenshot of Netlify permissions flow](netlify-repo-permissions.webp)\\n\\nThen you can select your repo from within Netlify. All of the default settings that Netlify provides should work for our use case:\\n\\n![screenshot of Netlify deploy settings](netlify-deploy-settings.png)\\n\\nLet\'s hit the magic \\"Deploy site\\" button! In a matter of minutes you\'ll find that Netlify has deployed your PWA.\\n\\n![screenshot of Netlify deployed site](netlify-deployed.webp)\\n\\nIf we browse to the URL provided by Netlify we\'ll be able to see the deployed PWA in action. (You also have the opportunity to set up a custom domain name that you would typically want outside of a simple demo such as this.) Importantly this will be served over HTTPS which will allow our Service Worker to operate.\\n\\nNow that we know it\'s there, let\'s see how what we\'ve built holds up according to the professionals. We\'re going to run the Google Chrome Developer Tools Audit against our PWA:\\n\\n![screenshot of the PWA audit looking good](pwa-audit.png)\\n\\nThat is a good start for our PWA!\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/from-create-react-app-to-pwa/)\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.logrocket.com/from-create-react-app-to-pwa/\\" />\\n</head>\\n\\n[The source code for this project can be found here.](https://github.com/johnnyreilly/pwa-react-typescript)"},{"id":"license-to-kill-your-pwa","metadata":{"permalink":"/license-to-kill-your-pwa","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2020-01-21-license-to-kill-your-pwa/index.md","source":"@site/blog/2020-01-21-license-to-kill-your-pwa/index.md","title":"LICENSE to kill your PWA","description":"Creating `.LICENSE` files caused issues for a PWA. The `terser-webpack-plugin` was changed to make `.LICENSE.txt` files instead.","date":"2020-01-21T00:00:00.000Z","tags":[],"readingTime":3.79,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"license-to-kill-your-pwa","title":"LICENSE to kill your PWA","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false,"description":"Creating `.LICENSE` files caused issues for a PWA. The `terser-webpack-plugin` was changed to make `.LICENSE.txt` files instead."},"unlisted":false,"prevItem":{"title":"From create-react-app to PWA","permalink":"/from-create-react-app-to-pwa"},"nextItem":{"title":"EF Core 3.1 breaks left join with no navigation property","permalink":"/ef-core-31-breaks-left-join-with-no-navigation-property"}},"content":"## Update: 26/01/2020 - LICENSE to kill revoked!\\n\\n\x3c!--truncate--\x3e\\n\\nFollowing the original publication of this post I received this tweet suggesting we should change the behaviour of the underlying `terser-webpack-plugin`:\\n\\n> Send a PR to change the name to .LICENSE.txt by default.\\n>\\n> \u2014 Tobias Koppers (@wSokra) [January 22, 2020](https://twitter.com/wSokra/status/1220069497660411904?ref_src=twsrc%5Etfw)\\n\\nThat seemed like an excellent idea! I raised [this PR](https://github.com/webpack-contrib/terser-webpack-plugin/pull/210) which changes the behaviour such that instead of `.LICENSE` files being produced, `.LICENSE.txt` files are pumped out instead. Crucially they are IIS (and other servers) friendly. The great news is that future users of webpack / create-react-app etc will not face this problem at all; result!\\n\\n## The tragedy\\n\\nRecently my beloved PWA died. I didn\'t realise it at first. It wasn\'t until a week or so after the tragedy that I realised he\'d gone. In his place was the stale memory of service workers gone by. Last week\'s code; cached and repeatedly served up to a disappointed audience. Terrible news.\\n\\nWhat had happened? What indeed. The problem was quirky and (now that I know the answer) I\'m going to share it with you. Because it\'s entirely non-obvious.\\n\\n## The mystery\\n\\nOnce I realised that I was repeatedly being served up an old version of my PWA, I got to wondering.... Why? What\'s happening? What\'s wrong? What did I do? I felt bad. I stared at the ceiling. I sighed and opened my Chrome devtools. With no small amount of sadness I went to the `Application` tab, hit `Service Workers` and then `Unregister`.\\n\\nThen I hit refresh and took a look at console. I saw this:\\n\\n![](LICENSE-cannot-be-cached.webp)\\n\\nWhat does this mean? Something about a \\"bad-precaching-response\\". And apparently this was happening whilst trying to load this resource: `/static/js/6.20102e99.chunk.js.LICENSE?__WB_REVISION__=e2fc36`\\n\\nThis `404` was preventing pre-caching from executing successfully. This was what was killing my PWA. This was the perpetrator. How to fix this? Read on!\\n\\n## The investigation\\n\\nTime to find out what\'s going on. I dropped that URL into my browser to see what would happen. `404` city man:\\n\\n![](LICENSE-file-screwing-me-over.webp)\\n\\nSo, to disk. I took a look at what was actually on the server in that location. Sure enough, the file existed. When I opened it up I found this:\\n\\n```js\\n/**\\n * A better abstraction over CSS.\\n *\\n * @copyright Oleg Isonen (Slobodskoi) / Isonen 2014-present\\n * @website https://github.com/cssinjs/jss\\n * @license MIT\\n */\\n\\n/*\\nobject-assign\\n(c) Sindre Sorhus\\n@license MIT\\n*/\\n\\n/** @license React v16.12.0\\n * react.production.min.js\\n *\\n * Copyright (c) Facebook, Inc. and its affiliates.\\n *\\n * This source code is licensed under the MIT license found in the\\n * LICENSE file in the root directory of this source tree.\\n */\\n\\n/** @license React v16.12.0\\n * react-dom.production.min.js\\n *\\n * Copyright (c) Facebook, Inc. and its affiliates.\\n *\\n * This source code is licensed under the MIT license found in the\\n * LICENSE file in the root directory of this source tree.\\n */\\n\\n/** @license React v0.18.0\\n * scheduler.production.min.js\\n *\\n * Copyright (c) Facebook, Inc. and its affiliates.\\n *\\n * This source code is licensed under the MIT license found in the\\n * LICENSE file in the root directory of this source tree.\\n */\\n\\n/** @license React v16.12.0\\n * react-is.production.min.js\\n *\\n * Copyright (c) Facebook, Inc. and its affiliates.\\n *\\n * This source code is licensed under the MIT license found in the\\n * LICENSE file in the root directory of this source tree.\\n */\\n```\\n\\nWhat is this? Well, as the name of the file suggests, it\'s licenses. For some reason, my build was scraping the licenses from the head of some of my files and placing them in a separate `6.20102e99.chunk.js.LICENSE` file. Doing some more digging I happened upon [this discussion against the `create-react-app`](https://github.com/facebook/create-react-app/issues/6441) project. It\'s worth saying, that my PWA was an ejected `create-react-app` project.\\n\\nIt turned out the the issue was related to the [`terser-webpack-plugin`](https://github.com/webpack-contrib/terser-webpack-plugin). The default behaviour performs this kind of license file extraction. The app was being served by an IIS server and it wasn\'t configured to support the `.LICENSE` file type.\\n\\n## The resolution\\n\\nThe simplest solution was simply this: wave goodbye to `LICENSE` files. If you haven\'t ejected from your `create-react-app` then this might be a problem. But since I had, I was able to make this tweak to the terser settings in the `webpack.config.js`:\\n\\n```js\\nnew TerserPlugin({\\n    /* TURN OFF LICENSE FILES - SEE https://github.com/facebook/create-react-app/issues/6441 */\\n    extractComments: false,\\n    /* TURN OFF LICENSE FILES - Tweak by John Reilly */\\n    terserOptions: {\\n        // ....\\n```\\n\\nAnd with this we say goodbye to our `404`s and hello to a resurrected PWA!"},{"id":"ef-core-31-breaks-left-join-with-no-navigation-property","metadata":{"permalink":"/ef-core-31-breaks-left-join-with-no-navigation-property","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2020-01-02-ef-core-31-breaks-left-join-with-no-navigation-property/index.md","source":"@site/blog/2020-01-02-ef-core-31-breaks-left-join-with-no-navigation-property/index.md","title":"EF Core 3.1 breaks left join with no navigation property","description":"When upgrading from .NET Core 2.2 to 3.1, an invalid LEFT JOIN error was encountered. The issue was resolved by adding Navigation property.","date":"2020-01-02T00:00:00.000Z","tags":[{"inline":false,"label":"SQL Server","permalink":"/tags/sql-server","description":"The SQL Server database."}],"readingTime":2.38,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"ef-core-31-breaks-left-join-with-no-navigation-property","title":"EF Core 3.1 breaks left join with no navigation property","authors":"johnnyreilly","tags":["sql server"],"hide_table_of_contents":false,"description":"When upgrading from .NET Core 2.2 to 3.1, an invalid LEFT JOIN error was encountered. The issue was resolved by adding Navigation property."},"unlisted":false,"prevItem":{"title":"LICENSE to kill your PWA","permalink":"/license-to-kill-your-pwa"},"nextItem":{"title":"Teams notification webhooks","permalink":"/teams-notification-webhooks"}},"content":"Just recently my team took on the challenge of upgrading our codebase from .NET Core 2.2 to .NET Core 3.1. Along the way we encountered a quirky issue which caused us much befuddlement. Should you be befuddled too, then maybe this can help you.\\n\\n\x3c!--truncate--\x3e\\n\\nWhilst running our app, we started encountering an error with an Entity Framework Query that looked like this:\\n\\n```cs\\nvar stuffWeCareAbout = await context.Things\\n    .Include(thing => thing.ThisIsFine)\\n    .Include(thing => thing.Problematic)\\n    .Where(thing => thing.CreatedOn > startFromThisTime && thing.CreatedOn < endAtThisTime)\\n    .OrderByDescending(thing => thing.CreatedOn)\\n    .ToArrayAsync();\\n```\\n\\n## Join me!\\n\\nAs EF Core tried to join from the `Things` table to the `Problematic` table (some obfuscation in table names here), that which worked in .NET Core 2.2 was _not_ working in .NET Core 3.1. Digging into the issue, we discovered EF Core was generating an invalid `LEFT JOIN`:\\n\\n```sql\\nfail: Microsoft.EntityFrameworkCore.Database.Command[20102]\\n      Failed executing DbCommand (18ms) [Parameters=[@__startFromThisTime_0=\'?\' (DbType = DateTime2), @__endAtThisTime_1=\'?\' (DbType = DateTime2)], CommandType=\'Text\', CommandTimeout=\'30\']\\n      SELECT [o].[ThingId], [o].[AnonymousId], [o].[CreatedOn],  [o].[Status], [o].[UpdatedOn], [o0].[Id], [o0].[ThingId], [o0].[Name], [o1].[ThingId], [o1].[Status], [o1].[CreatedOn], [o1].[ThingThingId], [o1].[SentOn]\\n      FROM [Things] AS [o]\\n      LEFT JOIN [ThisIsFines] AS [o0] ON [o].[ThingId] = [o0].[ThingId]\\n      LEFT JOIN [Problematic] AS [o1] ON [o].[ThingId] = [o1].[ThingThingId]\\n      WHERE ([o].[CreatedOn] @__startFromThisTime_0) AND ([o].[CreatedOn] < @__endAtThisTime_1)\\n      ORDER BY [o].[CreatedOn] DESC, [o].[ThingId], [o1].[ThingId], [o1].[Status]\\nMicrosoft.EntityFrameworkCore.Database.Command: Error: Failed executing DbCommand (18ms) [Parameters=[@__startFromThisTime_0=\'?\' (DbType = DateTime2), @__endAtThisTime_1=\'?\' (DbType = DateTime2)], CommandType=\'Text\', CommandTimeout=\'30\']\\nSELECT [o].[ThingId], [o].[AnonymousId], [o].[CreatedOn], [o].[Status], [o].[UpdatedOn], [o0].[Id], [o0].[ThingId], [o0].[Name], [o1].[ThingId], [o1].[Status], [o1].[CreatedOn], [o1].[ThingThingId], [o1].[SentOn]\\nFROM [Things] AS [o]\\nLEFT JOIN [ThisIsFines] AS [o0] ON [o].[ThingId] = [o0].[ThingId]\\nLEFT JOIN [Problematic] AS [o1] ON [o].[ThingId] = [o1].[ThingThingId]\\nWHERE ([o].[CreatedOn] @__startFromThisTime_0) AND ([o].[CreatedOn] < @__endAtThisTime_1)\\nORDER BY [o].[CreatedOn] DESC, [o].[ThingId], [o1].[ThingId], [o1].[Status]\\n```\\n\\nDo you see it? Probably not; it took us a while too... The issue lay here:\\n\\n```sql\\nLEFT JOIN [Problematic] AS [o1] ON [o].[ThingId] = [o1].[ThingThingId]\\n```\\n\\nThis should actually have been:\\n\\n```sql\\nLEFT JOIN [Problematic] AS [o1] ON [o].[ThingId] = [o1].[ThingId]\\n```\\n\\nFor some reason EF Core was looking for `ThingThingId` where it should have looked for `ThingId`. But why?\\n\\n## Navigation properties to the rescue!\\n\\nThis was the `Problematic` class:\\n\\n```cs\\nusing System;\\nusing System.ComponentModel.DataAnnotations;\\nusing System.ComponentModel.DataAnnotations.Schema;\\n\\nnamespace Treasury.Data.Entities\\n{\\n    public class Problematic\\n    {\\n        [ForeignKey(\\"Thing\\")]\\n        [Required]\\n        public Guid ThingId { get; set; }\\n        [Required]\\n        public DateTime CreatedOn { get; set; }\\n        public DateTime SentOn { get; set; }\\n    }\\n}\\n```\\n\\nIf you look closely you\'ll see it has a `ForeignKey` but _no_ accompanying Navigation property. So let\'s add one:\\n\\n```cs\\nusing System;\\nusing System.ComponentModel.DataAnnotations;\\nusing System.ComponentModel.DataAnnotations.Schema;\\n\\nnamespace Our.App\\n{\\n    public class Problematic\\n    {\\n        [ForeignKey(\\"Thing\\")]\\n        [Required]\\n        public Guid ThingId { get; set; }\\n        [Required]\\n        public DateTime CreatedOn { get; set; }\\n        public DateTime SentOn { get; set; }\\n\\n        /* THIS NAVIGATION PROPERTY IS WHAT WE NEEDED!!! */\\n        public virtual Thing Thing { get; set; }\\n    }\\n}\\n```\\n\\nWith this in place our app starts generating the SQL we need."},{"id":"teams-notification-webhooks","metadata":{"permalink":"/teams-notification-webhooks","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2019-12-18-teams-notification-webhooks/index.md","source":"@site/blog/2019-12-18-teams-notification-webhooks/index.md","title":"Teams notification webhooks","description":"Learn how to automate notifications using Microsoft Teams and Markdown webhooks, and discover how to use ASP.Net Core to send notifications.","date":"2019-12-18T00:00:00.000Z","tags":[],"readingTime":3.195,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"teams-notification-webhooks","title":"Teams notification webhooks","authors":"johnnyreilly","tags":[],"image":"./teams-notification.gif","hide_table_of_contents":false,"description":"Learn how to automate notifications using Microsoft Teams and Markdown webhooks, and discover how to use ASP.Net Core to send notifications."},"unlisted":false,"prevItem":{"title":"EF Core 3.1 breaks left join with no navigation property","permalink":"/ef-core-31-breaks-left-join-with-no-navigation-property"},"nextItem":{"title":"Definitely Typed: The Movie","permalink":"/definitely-typed-the-movie"}},"content":"Teams notifications are mighty useful. You can send them using Markdown via a webhook.\\n\\n\x3c!--truncate--\x3e\\n\\nThis post will explain the following:\\n\\n1. How you can automate the sending of notifications using Teams.\\n2. How Teams supports Markdown in notifications.\\n3. How you can use ASP.Net Core to automate sending notifications.\\n\\n## Notifications via Webhooks\\n\\nNow, it\'s not obvious from Teams that there is a simple webhooks integration for Teams, but there is. It\'s tucked away under \\"Connectors\\". If you want to create a webhook of your own, find your team, your channel, click on the menu, then connectors and create a hook. Like so:\\n\\n![animation of setting up a webhook connector in Teams](teams-webhook-connector.gif)\\n\\nWith the URL you\'ve just obtained, you are now free to send notifications to that channel via a simple `curl`:\\n\\n```shell\\ncurl -H \\"Content-Type: application/json\\" -d \\"{\\\\\\"text\\\\\\": \\\\\\"Hello World\\\\\\"}\\" https://outlook.office.com/webhook/big-long-guid1/IncomingWebhook/big-long-guid2\\n```\\n\\n## Markdown\\n\\nLet\'s see if we can make this more interesting. It turns out that the the webhook can receive JSON as the body of the payload. And there\'s 3 properties we\'d like our JSON to contain:\\n\\n1. `title` - this is optional and is the title of your notification if supplied.\\n2. `textFormat` - provide the value `\\"markdown\\"` and then...\\n3. `text` - provide your markdown notification content!\\n\\nSo if we have a notification payload file called `down.json`:\\n\\n```json\\n{\\n  \\"title\\": \\"Your Notification Title\\",\\n  \\"textFormat\\": \\"markdown\\",\\n  \\"text\\": \\"*Wow*\\\\nThis is [markdown](https://en.wikipedia.org/wiki/Markdown)!\\\\n![do a little dance!](https://media.giphy.com/media/YJ5OlVLZ2QNl6/giphy.gif)\\\\n**Huzzah**!\\"\\n}\\n```\\n\\nWe can trigger it with this `curl`:\\n\\n```shell\\ncurl -H \\"Content-Type: application/json\\" -d @down.json https://outlook.office.com/webhook/big-long-guid1/IncomingWebhook/big-long-guid2\\n```\\n\\nAs you can see from the example above, you can use all the qualities of Markdown that you know and love. Text, bold text, italics, links and even images too. It\'s _great_!\\n\\n![animation of Teams notification](teams-notification.gif)\\n\\n## ASP.Net Core\\n\\nFinally, I wanted to illustrate just how simple the WebHooks API makes plugging notifications into an existing app. In our case we\'re going to use ASP.Net Core, but really there\'s nothing particular about how we\'re going to do this.\\n\\nHere\'s a class called `TeamsNotificationService`. It exposes 2 methods:\\n\\n- `SendNotification` which allows the consumer to just provide a `title` and a `message` - you could consume this from anywhere in your app and use it to publish the notification of your choice.\\n- `SendExcitingNotification` which actually uses `SendNotification` and illustrates how you might provide an exciting notification to publish out.\\n\\n```cs\\nusing System;\\nusing System.Collections.Generic;\\nusing System.Net.Http;\\nusing System.Net.Http.Headers;\\nusing System.Threading.Tasks;\\n\\nnamespace My.Services {\\n    public interface ITeamsNotificationService {\\n        Task SendNotification(string title, string message);\\n        Task SendExcitingNotification(Guid someAppId, string person);\\n    }\\n\\n    public class TeamsNotificationService : ITeamsNotificationService {\\n\\n        // in Startup.ConfigureServices you\'re going to want to add this line:\\n        // services.AddHttpClient(TeamsNotificationService.TEAMS_NOTIFIER_CLIENT);\\n\\n        public const string TEAMS_NOTIFIER_CLIENT = \\"TEAMS_NOTIFIER_CLIENT\\";\\n\\n        private readonly ILogger<TeamsNotificationService> logger;\\n        private readonly IHttpClientFactory _clientFactory;\\n\\n\\n        public TeamsNotificationService(\\n            ILogger<TeamsNotificationService> logger,\\n            IHttpClientFactory clientFactory\\n        ) {\\n            _logger = logger;\\n            _clientFactory = clientFactory;\\n        }\\n\\n        private HttpClient CreateClient() {\\n            var client = _clientFactory.CreateClient(TEAMS_NOTIFIER);\\n\\n            client.DefaultRequestHeaders.Clear();\\n            client.DefaultRequestHeaders.Accept.Clear();\\n            client.DefaultRequestHeaders.Accept.Add(new MediaTypeWithQualityHeaderValue(\\"application/json\\"));\\n\\n            return client;\\n        }\\n\\n        public async Task SendNotification(string title, string message) {\\n            try {\\n                var client = CreateClient();\\n\\n                var messageContents = string.IsNullOrEmpty(title)\\n                    ? new JsonContent(new { text = message, textFormat = \\"markdown\\" })\\n                    : new JsonContent(new { title = title, text = message, textFormat = \\"markdown\\" });\\n\\n                var webhookUrl = \\"https://outlook.office.com/webhook/big-long-guid1/IncomingWebhook/big-long-guid2\\";\\n                var response = await client.PostAsync(webhookUrl, messageContents);\\n\\n                _logger.LogInformation(\\"Sent {title} notification to Teams using {url}; received this response: {responseStatusCode}\\", title, url, response.StatusCode);\\n            }\\n            catch (Exception exc) {\\n                _logger.LogError(exc, $\\"Failed to send {title} notification to Teams\\");\\n            }\\n        }\\n\\n        public async Task SendExcitingNotification(Guid someAppId, string person) {\\n            var celebration = GetCelebration();\\n            await SendNotification(\\n                title: \\"Incredible Thing Alert!\\",\\n                message: $@\\"**{person}** has done something incredible! &#x1F44B;\\n\\n![celebration time!]({celebration})\\n\\n[Go see for yourself](https://my.app/some-page/{someAppId})\\"\\n            );\\n        }\\n\\n        string GetCelebration() => GetRandomItem(_celebrations);\\n        string GetRandomItem(string[] arrayOfStrings) => arrayOfStrings[new Random().Next(0, arrayOfStrings.Length)];\\n\\n        string[] _celebrations = new string[] {\\n            \\"https://media.giphy.com/media/KYElw07kzDspaBOwf9/giphy.gif\\",\\n            \\"https://media.giphy.com/media/GStLeae4F7VIs/giphy.gif\\",\\n            \\"https://media.giphy.com/media/NbXTwsoD7hvag/giphy.gif\\",\\n            \\"https://media.giphy.com/media/d86kftzaeizO8/giphy.gif\\",\\n            \\"https://media.giphy.com/media/YJ5OlVLZ2QNl6/giphy.gif\\",\\n            \\"https://media.giphy.com/media/kyLYXonQYYfwYDIeZl/giphy.gif\\",\\n            \\"https://media.giphy.com/media/KYElw07kzDspaBOwf9/giphy.gif\\",\\n            \\"https://media.giphy.com/media/6nuiJjOOQBBn2/giphy.gif\\",\\n            \\"https://media.giphy.com/media/hZj44bR9FVI3K/giphy.gif\\",\\n            \\"https://media.giphy.com/media/31lPv5L3aIvTi/giphy.gif\\"\\n        };\\n    }\\n}\\n```\\n\\nIt\'s as simple as that \uD83D\uDE04"},{"id":"definitely-typed-the-movie","metadata":{"permalink":"/definitely-typed-the-movie","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2019-10-08-definitely-typed-the-movie/index.md","source":"@site/blog/2019-10-08-definitely-typed-the-movie/index.md","title":"Definitely Typed: The Movie","description":"The history of the TypeScript GitHub project Definitely Typed. And some TypeScript history as well","date":"2019-10-08T00:00:00.000Z","tags":[{"inline":false,"label":"Definitely Typed","permalink":"/tags/definitely-typed","description":"The Definitely Typed project for TypeScript type definitions"},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":48.78,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"definitely-typed-the-movie","title":"Definitely Typed: The Movie","authors":"johnnyreilly","tags":["definitely typed","typescript"],"image":"./title-image.png","hide_table_of_contents":false,"description":"The history of the TypeScript GitHub project Definitely Typed. And some TypeScript history as well"},"unlisted":false,"prevItem":{"title":"Teams notification webhooks","permalink":"/teams-notification-webhooks"},"nextItem":{"title":"Start Me Up: ts-loader meet .tsbuildinfo","permalink":"/start-me-up-ts-loader-meet-tsbuildinfo"}},"content":"This post is a a little different from most that sit on my site. It\'s the story of the Definitely Typed project, of which I was an early member. It had a seismic impact on the development of TypeScript. When exchanging messages with [Andrew Branch](https://github.com/andrewbranch) (member of the TypeScipt team), I realised it was an untold story, and perhaps I should tell it, before I forget! So I did, and this is it.\\n\\nI named it \\"Definitely Typed: The Movie\\" as the name entertained me. Little did I know, that a few years later, a documentary would be made about TypeScript, and I\'d be in it; in part thanks to writing this history. You can see [more about that here](../2023-09-20-typescript-documentary/index.md).\\n\\nFor now, back to Definitely Typed...\\n\\n![A title image that reads \\"Definitely Typed: The Movie\\"](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Preface\\n\\nI\'d like to tell you a story. It\'s the tale of the ecosystem that grew up around a language: TypeScript. TypeScript is, for want of a better description, JavaScript after a trip to Saville Row. Essentially the same language, but a little more together, a little less wild west. JS with a decent haircut and a new suit. These days, the world seems to be written in TypeScript. And when you pause to consider just how young the language is, well, that\'s kind of amazing.\\n\\nWho could have predicted it would end up like this? When I was a boy I remember coming down the stairs in my childhood home. Shuffling to the edge of each step on my bottom before thumping down to the one beneath. When I look at those same stairs now they\'re so small. I barely notice the difference between one step and the next. But back then each step seemed giant, each one so far apart. Definitely Typed had any number of steps in its evolution. They all seemed so significant then; whereas now they\'re just a memory. Let\'s remember together\u2026\\n\\n## Prolog(ue)\\n\\nWhen it was first unveiled to the world by Anders Hejlsberg back in 2012, there was nothing to suggest TypeScript was going to be seismic in its effects. The language brought two important things to the table. First of all, the ability to write JavaScript with optional static typing (imagine this as \\"belts and braces\\" for JS). The second feature was interoperability with existing JavaScript.\\n\\nThe reason TypeScript has the traction that it does, is a consequence of the latter feature. The JavaScript ecosystem was already a roaring success by 2012. Many useful libraries were out there, authored in vanilla JavaScript. jQuery, Backbone, Knockout were all going concerns. People were building things.\\n\\nWisely, having TypeScript able to work with existing JavaScript libraries was a goal of the language right from the off. This made sense; otherwise it would have been like unveiling Netflix to the world whilst saying \\"sorry you can\'t use a television set to watch this\\". Remember, JS was great as is - people wanted static typing so they could be more productive and so they could sleep better at night. (\\"Oh wait, did I write that unit test to check all the properties? Dammit, it\'s 3am!\\") If TypeScript had hove onto the scene requiring that everything was written _in_ TypeScript then I would not be writing this. It didn\'t.\\n\\nInteroperability was made possible by the concept of \\"type definitions\\". Analogous to header files in C, these are TypeScript files with a `.d.ts` suffix that tell the compiler about an existing JavaScript library which is in scope. This means you can write TypeScript and use jQuery or [insert your favourite library name here]. Even though they are not written in TypeScript.\\n\\nAt the time of the initial TypeScript announcement (v0.8.1) there was no concept of a repository of type definitions. I mean, there was every chance that TypeScript wasn\'t going to be a big deal. Success wasn\'t guaranteed. But it happened. You\'re reading this in a world where Definitely Typed is one of the most popular repos on GitHub and where type definitions from it are published out to npm for consumption by developers greedy for static types. A world where the TypeScript team has pretty much achieved its goal of \\"types on every desk\\".\\n\\nI want to tell you the story of the history of type definitions in the TypeScript world. I\'m pretty well placed to do this since I\'ve been involved since the early days. Others involved have been kind enough to give me their time and tell me their stories. There\'s likely to be errors and omissions, and that\'s on me. It\'s an amazing tale though; I\'m fortunate to get to tell it.\\n\\n## The First Type Definition\\n\\nI was hanging out for something like TypeScript. I\'d been busily developing rich client applications in JS and, whilst I loved the language, I was dearly missing static typing. All the things broke all of the time and I wanted help. I wanted a compiler to take me by the hand and say \\"hey John, you just did a silly thing. Don\'t do it John; you\'ll only be filled with regret...\\". The TypeScript team wrote that compiler.\\n\\nWhen TypeScript was announced, it was important that the world could see that interop with JS was a first class citizen. Accordingly, a jQuery type definition was demonstrated as well. At the time, jQuery was the number one JavaScript library downloaded on the internet. So naturally it was the obvious choice for a demo. The type definition was fairly rough and ready but it worked. [You can see Anders Hejlsberg showing off the jQuery definition 43 minutes into this presentation introducing TypeScript.](https://youtu.be/3dqZW_DqHIQ?t=2584)\\n\\nConsumption was straightforward, if perhaps quirky. You took the `jquery.d.ts` file, copied it into your project location. Back then, to let the compiler know that a JS library had come to the party you had to use a kind of comment pragma in the header of your TypeScript files. For example: `/// <reference path=\\"jquery/jquery.d.ts\\" />`. This let TypeScript know that the type definition living at that path was relevant for the current script and it should scope it in.\\n\\nThere was no discussion of \u201Chow do we type the world\u201D? Even if they wanted to, the TypeScript team didn\'t really have the resources at that point to support this. They\'d got as far as they had on the person power of four or five developers and some testers as well. There was a problem clearly waiting to be solved. As luck would have it, in Bulgaria a man named Boris Yankov had been watching the TypeScript announcement.\\n\\n## Boris Yankov\\n\\n![photograph of Boris Yankov looking mean, moody and magnificent](boris_yankov.webp)\\n\\nBoris Yankov was a handsome thirty year old man, living in the historic Bulgarian city of Plovdiv. He was swarthy with dark hair; like Ben Affleck if had been hanging out in Eastern Europe for a couple of years.\\n\\nBoris was a backend developer who\'d found himself doing more and more frontend. More JavaScript. He was accustomed to C# on the backend with static typing a-gogo. From his point of view JS was brittle. It was super easy to break things and have no idea until runtime that you\'d done so. It seemed so backward. He was ready for something TypeScript shaped.\\n\\n\\"What people forget is how different it was back then. Microsoft made this announcement, but probably most of the people that were listening were part of the MS ecosystem. I certainly was. Remember, back then if you had a Mac or did Linux you probably didn\'t think about MS too much.\\"\\n\\nBoris thought TypeScript just seemed like this interesting and weird thing that Microsoft were doing. He was excited by types; he was missing them and there was a real need there. A problem to solve. There were already people trying to address this. But the attempts so far had been underwhelming. Boris had encountered Google Closure Compiler; a tool built by Google which, amongst other things, introduces some measure of type safety to JavaScript by reading annotations in JSDoc format. Boris viewed GCC as a tentative first step. [One which lead the way for things like TypeScript and Flow to follow.](https://github.com/google/closure-compiler/wiki/Annotating-JavaScript-for-the-Closure-Compiler)\\n\\nThe other aspect of TypeScript that excited Boris was transpilation. Transpilation is the term coined to describe what TypeScript does when it comes to emit output. It takes in TypeScript and pumps out JavaScript. The question is: what sort of JavaScript? One choice the TypeScript team could have made was just having the compiler stripping out types from the codebase. If it worked that way then you\'d get out the JavaScript equivalent of the TypeScript you wrote. You wrote a `class`? TypeScript emits a `class`; just; one shorn of types and interfaces.\\n\\nThe TypeScript team made a different choice. They wrote the compiler such that the user could write ES6 style TypeScript syntax and have the TypeScript compiler transpile that down to ES5 or even ES3 syntax. This made TypeScript a much more interesting proposition than it already was, for a couple of reasons.\\n\\nES6 had been in the works for some time at this point. The release was shaping up to be the biggest incremental change to JavaScript that had so far happened. Or that would ever happen. Prior to this, JavaScript had experienced no small amount of tension and disagreement as it sought to evolve and develop. These played out in the form of the abandoned fourth edition of the language. There were arguments, harsh words, public disagreements and finally a failure to ship ECMAScript 4. In an alternate universe this was the end of the road for JavaScript. However, in our universe JavaScript got another throw of the dice.\\n\\nIt\'s telling that ES5 was for a long time known also as ES3.1; reflecting that it was initially planned to be the stepping stone between ES3 and ES4. In reality it ended up being the stepping stone between ES3 and ES6. As it turned out, it was a vital one too, [it allowed the TC39 to recalibrate after a very public shelving of plans.](https://en.m.wikipedia.org/wiki/ECMAScript)\\n\\nThe band was back together (albeit with a new rhythm section) and ES6 was going to be _massive_. JavaScript was going to get new constructs such as `Map`, `Set`, new scoping possibilities with `let` and `const`, `Promise`s which paved the way for new kinds of async programming, the contentious `class`es\u2026. And who can forget where they were when they first heard about \\"fat\\" arrow functions?\\n\\nPeople salivated at the idea of it all. Such new shiny toys! But how could we use them? Whilst all this new hotness was on the way, where could you actually run your new style code? Complete browser implementations of ES6 wouldn\'t start to materialise until 2018. Given the slowness of people to upgrade and the need to support the lowest common denominator of browser this could have meant that all the excitement was trapped in a never tomorrow situation.\\n\\nBack to TypeScript. The team had a solution for this issue. In their wisdom, the TypeScript team allowed us to write ES6 TypeScript and the compiler could (with some limitations) transpile it down to ES3 JavaScript. The audacity of this was immense. The TypeScript team brought the future back to the past. What\'s more, they made it work in Internet Explorer 6. Now that\'s rock\'n\'roll. It\'s nothing short of miraculous!\\n\\nThe significance of transpilation to TypeScript cannot be overstated.\\n\\nYou might be thinking to yourself, \\"that\'s just Babel, right?\\" Right. It\'s just that Babel didn\'t exist then. 6to5 was still an idea waiting for Sebastian McKenzie to think of. Even if you were kind of \\"meh\\" on types, the attraction of using a tool which allowed you to use new JavaScript constructs without breaking your customers was a significant draw. People may have come for types, but once they\'d experienced the joy of a lexically bound `this` in a fat arrow function they were _never_ going back.\\n\\nSuccess has many parents. TypeScript is a successful project. One reason for this is that it\'s an excellent product that fills a definite need. Another reason is one that can\'t be banked upon; timing. TypeScript has enjoyed phenomenal timing. Appearing just when JavaScript was going off like a rocket and having the twin benefits of types and future JS today when nothing else offered anything close, that\'s perfect timing. It got people\'s curiosity. Now it got Boris\'s attention.\\n\\n## Definitely Typed\\n\\n![The Definitely Typed logo](dt-logo-smallish.webp)\\n\\nBoris had been feeling unproductive. He would build applications in JS and watch them unaccountably break as he made simple tweaks to them. He was constantly changing things, breaking them, fixing them and hoping he hadn\'t broken something else along the way. It was exhausting. He saw the promise in what TypeScript was offering and decided to give it a go.\\n\\nIt was great. He fired up Visual Studio and converted a `.js` file to end with the mystical TypeScript suffix of `.ts`. In front of his eyes, red squiggly lines started to appear here and there in his code. As he looked at the visual noise he could see this was TypeScript delivering on its promise. It was finding the bugs he hadn\'t spotted. These migrations were also addictive; the more information you could feed the compiler, the more problems it found. Boris felt it was time to start writing type definitions, whatever they were.\\n\\nBoris quickly learned how to write a type definition and set to work. Most libraries weren\'t well documented and so he found himself reading the source code of libraries he used in order that he could write the definitions. At first, the definitions were just files dropped in his ASP.NET MVC projects that he copied around. That wasn\'t going to scale; there needed to be somewhere he could go to grab type definitions when he needed them. And so on October 5th 2012 he created a repository under his profile at GitHub called \\"DefinitelyTyped\\": [https://github.com/DefinitelyTyped/DefinitelyTyped/commit/647369a322be470d84f8d226e297267a7d1a0796](https://github.com/DefinitelyTyped/DefinitelyTyped/commit/647369a322be470d84f8d226e297267a7d1a0796)\\n\\nBoris took his type definitions and put them into this repository. Were you ever curious what the first definition added was? Close your eyes and think... You might imagine it was the (then number one JavaScript library on the web) jQuery. In fact it was Modernizr. Then Underscore followed, and then jQuery. Take a look:\\n\\n[https://github.com/DefinitelyTyped/DefinitelyTyped/commits?after=4a4cf23ff4301835a45bb138bbb62bf5f0759255+699&author=borisyankov](https://github.com/DefinitelyTyped/DefinitelyTyped/commits?after=4a4cf23ff4301835a45bb138bbb62bf5f0759255+699&author=borisyankov)\\n\\n![A screenshot of the initial commits to Definitely Typed on GitHub](Initial-CommitsDefinitelyTyped.webp)\\n\\nIt wasn\'t complicated; it was just a folder with subfolders underneath; each folder representing a project. One for jQuery, one for jQuery UI, one for Knockout.... You get the idea. It\'s not so different now.\\n\\nBoris had laid simple but dependable foundations. Definitely Typed had been born.\\n\\n## How Do You Test a Type Definition?\\n\\nBoris was careful too. Right from the first type definition he added tests alongside them. Now tests for a type definition were a conundrum. How do you write a test for interfaces that don\'t exist in the runtime environment? Code that is expunged as part of the compilation process. Well, the answer Boris came to was this: a compilation test.\\n\\nSomeone once said: compilation is the first unit test... But it\'s a doozy. They\'re right. The value you get from compilation, from a computer checking the assertions your code makes, is significant. Simply put, it takes a large amount of tests to get the same level of developer confidence. Computers are wonderful at attention to detail in a way that puts even the most anally retentive human being to shame.\\n\\nSo if Boris had written a definition called `mylib.d.ts`, he\'d write a file that exercises this type definition. A `mylib.tests.ts` if you will. This file would contain code that exercises the type definition in the way that it should correctly be used. This is code that will never be executed in the way that tests normally are; a test program is never actually run. Rather these tests exist solely for compilation time. (In much the same way that TypeScript types only exist for compilation time.) Boris\'s plan was this: no compilation errors in `mylib.tests.ts` represents passing tests. Compilation errors in `mylib.tests.ts` represents failing tests. It was functional, brutal and also beautiful in it\'s simplicity.\\n\\nSo, imagine your definition looked like this:\\n\\n```ts\\ndeclare function turnANumberIntoAString(\\n  numberToMakeStringOutOf: number,\\n): string;\\n```\\n\\nYou might write a compilation test that looks like this:\\n\\n```ts\\nconst itIsAString: string = turnANumberIntoAString(42);\\n```\\n\\nThis test ensures that you can use your function in the way you\'d expect. It returns the types you\'d desire (a `string` in this case) and it accepts the parameters you\'d expect (a single `number` for this example). If someone changed the definition in future, such that a different type was returned or a different set of parameters was required it would break the test. The test code wouldn\'t compile anymore. That\'s the nature of our \\"test\\". It\'s blunt but effective.\\n\\nThis is the very first test committed to Definitely Typed; a test for Modernizr.\\n\\n[https://github.com/DefinitelyTyped/DefinitelyTyped/blob/a976315cacbcfc151d5f57b25f4325dc7deca6f2/Tests/modernizr.ts](https://github.com/DefinitelyTyped/DefinitelyTyped/blob/a976315cacbcfc151d5f57b25f4325dc7deca6f2/Tests/modernizr.ts)\\n\\nThis idea represents what tests look like throughout Definitely Typed today. They\'re now \\"run\\" as part of Continuous Integration and type definitions are tested in concert with one another to ensure that a change to one type definition doesn\'t break another. But there is nothing fundamentally different in place today to what Boris originally came up with.\\n\\n## Independence\\n\\nVery quickly, Definitely Typed became a known project. People like Steve Fenton (author of the first book about TypeScript) were vocal supporters of the project. The TypeScript team talked up the project and were entirely supportive of its existence. In fact, at every given opportunity Anders Hejlsberg would sing its praises. For a while you could guarantee that any TypeScript talk by Anders would include a variant of \\"this guy called Boris started a project called Definitely Typed\\". The impression he gave was that he was kind of amazed, and thoroughly delighted, the project existed.\\n\\nThe TypeScript team were completely uninvolved with Definitely Typed. That in itself is worth considering. The perception of Microsoft by developers generally in 2012 was at best, highly suspicious. \\"Embrace, extend, extinguish\\" - a strategy attributed to MS was very much a current perspective. This was born out in online comments and conversations at meetups. [The Hacker News comments on the TypeScript release were a mixed bag.](https://news.ycombinator.com/item?id=4597716) The reaction on social media was rather less generous. Certainly it was harsh enough to prompt Scott Hanselman to write something of [a defence of TypeScripts right to exist](https://www.hanselman.com/blog/WhyDoesTypeScriptHaveToBeTheAnswerToAnything.aspx).\\n\\nGiven that TypeScript had arrived with the promise of transforming the JavaScript developer experience, the developer community was understandably cautious. Was Microsoft doing a good or ill? Could they be trusted? There were already signs that MS was changing. For example, it had been shipping open source libraries such as jQuery with ASP.Net MVC for some time. Microsoft was starting to engage with the world of open source software.\\n\\nHow Microsoft interacted with the (very open source driven) JS community was going to be key to the success (or not) of TypeScript. What happened with the establishment of Definitely Typed very much indicated TypeScripts direction of travel.\\n\\nOn day one of its existence, Boris took type definitions written by Microsoft and made them available via Definitely Typed. A ballsy move. It would have been completely possible for MS to object to this. They didn\'t.\\n\\nPeople like Diullei Gomes started submitting pull requests to improve the existing definitions and add new ones. Diullei even wrote the first command line tooling which allowed people to install type definitions: TSD. Within a surprisingly short period, DT had become the default home of type definitions on the web. There were briefly alternative Definitely Typed styled collections of type definitions elsewhere on GitHub but they didn\'t last.\\n\\nThis all happened completely independently of the TypeScript team. Definitely Typed existing actually allowed TypeScript itself to prosper. It was worth persevering with this bleeding edge language because of the interoperability Definitely Typed was providing to the community. So the hands off attitude of MS was both surprising and encouraging. It showed trust of the community; something that hadn\'t hitherto been a commonly noted characteristic of MS.\\n\\nBoris started adding contributors to Definitely Typed to help him with the work. Definitely Typed was no longer a one man band, it had taken an important step. It was built and maintained by an increasing number of creative and generous people. All motivated by a simple aim: the best developer experience when working with TypeScript and existing JS libraries.\\n\\n## Basarat Ali Syed\\n\\n![A photograph of Basarat](basarat.webp)\\n\\nBasarat Ali Syed was a 27 year old who had recently moved to Melbourne, Australia from Pakistan. You might know of him for a number of reasons, not least being the TypeScript equivalent of Jon Skeet. That, incidentally, is not a coincidence. Basarat had watched Jon Skeet\'s impressive work, being _the_ gold standard in C# answers and thought \\"there\'s something worth emulating here\\".\\n\\nBas was working for a startup who had a JS frontend. About six months before TypeScript was announced to the world he watched Anders Hejlsberg do a presentation on JavaScript which included Anders saying to the audience \\"don\'t you just wish you had type safety?\\" with a twinkle in his eye. TypeScript was of course well underway by this time; just not yet public. Bas remembered the comment and, when TypeScript was announced, he was ready. He made it his personal mission to be the goto person answering questions about TypeScript on Stack Overflow.\\n\\nIn those early days of TypeScript, if you put a question about TypeScript onto Stack Overflow there was a very good chance that Bas would answer it. And Bas was more helpful than your typical SO answerer. Not only would he provide helpful commentary and useful guidance, he would often find him answering \\"yeah, the problem isn\'t your code, it\'s the type definition. It needs improvement. In fact, I\'ve raised a PR to fix it here\u2026\\"\\n\\nBoris saw the drip, drip of Basarat PRs turning into a flood. So, very quickly, he invited Basarat join Definitely Typed. Now Bas could not just suggest changes, he could ensure they were made. Step by step the quality of type definitions improved.\\n\\nBasarat describes himself as a \\"serial OSS contributor and mover on-er\\". It\'s certainly true. As well as his Stack Overflow work, he\'s been someone involved in the early days of any number of open source projects. Not just Definitely Typed. Bas also worked on the TypeScript port of the JavaScript task runner; Grunt TS. He met up with Pete Hunt (he of React) at a Decompress conference and together they hacked together a POC webpack TypeScript loader. (That POC ultimately lead to James Brantly creating ts-loader which I maintain.) Bas wrote the atom-typescript plugin which offers first class support for TypeScript in Atom. Not content with that he went on to write a full blown editor of his own called alm-tools.\\n\\nThis is not an exhaustive list of his achievements and already I\'m tired. Besides this he wrote the TypeScript Deep Dive book and the VS Code TypeScript God extension. And more.\\n\\nBas had the level of self knowledge required to realise that getting others involved was key to the success of open source projects. Particularly given that he knew he had a predilection to eventually move on, to work on other things. So Bas kept his eyes open and welcomed in new maintainers for projects he was working on. Bas\' actions in particular were to be crucial. Bas grew the Definitely Typed team; he invited others in, he got people involved.\\n\\nOn December 28th 2013 Basarat decided that a regular contributor to Definitely Typed might be a potential team member. Bas opened up Twitter and sent a Direct Message to John Reilly.\\n\\n![A screenshot of direct message Basarat sent to John Reilly in Twitter](twitter-direct-message-from-basarat.webp)\\n\\n## John Reilly\\n\\n![A photograph of John Reilly](johnny_reilly.webp)\\n\\nThat\'s me. Or [johnny_reilly on Twitter](https://twitter.com/johnny_reilly), [johnny_reilly on Fosstodon](https://fosstodon.org/@johnny_reilly) and [johnnyreilly on GitHub](https://github.com/johnnyreilly). Relatively few people call me Johnny. I\'m named that online because back when I applied for an email address, someone had already bagsied `johnreilly@hotmail.com`. (Hotmail was what everyone used back when I came online - I am _that_ old.) So rather than sully my handle with a number or a middle name I settled for `johnny_reilly` and went with the underscore as someone already had the email address without one. I haven\'t looked back and have generally tried to keep that nom de plume wherever I lay my hat online. I have learned to my chagrin that GitHub doesn\'t support the `_` character in usernames. This bothers me more than is reasonable.\\n\\nIn contrast to others I was a relatively late starter to TypeScript. I was intrigued right from the initial announcement, but held off from properly getting my hands dirty until generics was added to the language in 0.9. (This predisposition towards generics in a language perhaps explains why I didn\'t get too far with Golang.)\\n\\nAt that point I was working in London for a private equity house. It was based in the historic and affluent area of St James. St James is an interesting part of London, caught midway between the Government, Buckingham Palace and the heart of the West End. It\'s old fashioned, dripping with money and physically delightful. It\'s the sort of place film crews dash towards when they\'re called upon to show old fashioned London in all its pomp. It rocks.\\n\\nMy team hated JavaScript. Absolutely loathed it. I was the solo voice saying \\"but it\'s really cool!\\" whilst they all but burned effigies of Brendan Eich in each code review. However, to my delight (and their abject horror) the project we were working on could only be implemented using JS. Essentially the house wanted an application offering rich interactivity which had to be a web app. So\u2026 JS. We were coding then with a combination of jQuery and Knockout JS. And, in large part due to the majority of the team being unfamiliar with JS, we were shipping bugs. The kind of bugs that could be caught by a compiler. By static typing. Not to put too fine a point on it; by TypeScript.\\n\\nSo I proposed an experiment: \\"Let\'s take one screen and develop it with TypeScript. Let\'s leave the rest of the app as is; JavaScript as usual. And then once we\'re done with that screen let\'s see how we feel about it. TypeScript might not be that great. But that\'s fine, if it isn\'t we\'ll take the generated JS, keep that and throw away the TypeScript. Deal?\\"\\n\\nThe team were on board and, one sprint review later, we decided that all future JS functionality would be implemented with TypeScript. We were in!\\n\\nFrom day one of using TypeScript I was in love. I had the functionality of JavaScript, the future semantics of JavaScript and I was making less mistakes. Our team had become more productive. We were shipping faster and more reliably with fewer errors. People were noticing; our reputation as a team was improving, in part due to our usage of TypeScript. We had a jetpack.\\n\\nHowever. I wasn\'t satisfied. As I tapped away at my keyboard I found type definitions to be\u2026 imperfect. And that niggled. Did it ever niggle. By then [Jason Jarrett](https://github.com/staxmanade) had wired up Definitely Typed packages to be published out to Nuget. Devs using ASP.NET MVC 4 (as I then was) were busily installing type definitions alongside AutoFac and other dependencies. Whilst most of those dependencies arrived like polished diamonds, finished products ready to be plugged into the project and start adding value. The type definitions by contrast felt very beta. And of course, they were. TypeScript was beta. The definitions reflected the newness of the language.\\n\\nI could make it better.\\n\\nI started submitting pull requests. The first problem I decided to solve was IntelliSense. I wanted IntelliSense for jQuery. If you went to [https://api.jquery.com](https://api.jquery.com) there was rich documentation for every method jQuery exposed. I wanted to see that documentation inside Visual Studio as I coded. If I keyed in `$.appendTo(` I wanted VS to be filled with the content from [https://api.jquery.com/appendTo/](https://api.jquery.com/appendTo/) . That was my mission. For each overload of the method I\'d add something akin to this to the type definition file:\\n\\n```ts\\n/**\\n * Insert every element in the set of matched elements to the end of the target.\\n *\\n * @param value A selector, element, HTML string, array of elements, or jQuery\\n *              object; the matched set of elements will be inserted at the end\\n *              of the element(s) specified by this parameter.\\n */\\nappendTo(target: string): JQuery;\\n```\\n\\nIt was a tedious task plugging it all in, but the pleasure I got from having rich IntelliSense in VS more than made up for it to me. Along the way I added and fixed sections of the jQuery API that hadn\'t been implemented, or had been implemented incorrectly. It got to a point where jQuery was a good example of what a type definition should look like. That remains the case to this day; surprisingly few type definitions enjoy the JSDoc richness of jQuery. [I have tried to encourage more use of this with blog posts code reviews and the like, but it\'s never got the traction I\'d hoped.](../2014-05-05-typescript-jsdoc-and-intellisense/index.md)\\n\\nI\'m fairly relentless when I put my mind to something. I work very hard to make things come to pass. What this meant at one point was the Definitely Typed maintainers receiving multiple PRs a day. Which prompted Bas to wonder \\"I wonder if he\'d like to join us?\\"\\n\\nI happily accepted Bas\' invitation and soon found myself reading this email:\\n\\n> From: Bas\\n>\\n> Sent: 28 December 2013 11:47\\n>\\n> To: Boris Yankov; johnny\\\\_reilly@hotmail.com; Bas; vvakame; Bart van der Schoor; Diullei Gomes; steve fenton; Jason Jarret Subject: DefinitelyTyped team introduction\\n>\\n> Dear All,\\n>\\n> Meet John Reilly (github : https://github.com/johnnyreilly , twitter : https://twitter.com/johnny\\\\_reilly) who will be helping with Definitely Typed definitions.\\n>\\n> Boris manages the project and he can add you as a collaborator.\\n>\\n> Additional team member introductions:\\n>\\n> Admin : Boris Yankov\\n>\\n> TSD package manager : https://github.com/DefinitelyTyped/tsd : Diullei / Bart van der Schoor\\n>\\n> NUGET: https://github.com/DefinitelyTyped/NugetAutomation : Json Jarret\\n>\\n> Passionate TypeScript users like yourself: Wakame, Myself and SteveFenton .\\n>\\n> Cheers, Bas (Basarat)\\n\\nSome of those names you\'ll recognise; some perhaps not. Jason Jarrett wrote the Nuget distribution mechanism for type definitions that ended up existing for far longer than anyone (least of all Jason) anticipated. Steve Fenton was largely a cheerleader for Definitely Typed in its early days. Diullei and Bart, amongst other things, worked on the initial command line tooling for DT: TSD.\\n\\nAfter being powered up in Definitely Typed, my contributions only increased. Anything that I was using in my day to day work, I wanted to have an amazing TypeScript experience. I wanted the language to thrive and I was pretty sure I could help by trying to get users the best-in-class developer experience as they used JS libraries. I\'ve always found good developer experience a strong motivation; the idea being, if someone loves their tools, they\'ll do great work. The end customer (of whatever they\'re building) gets a better product sooner. Great developer experience is a force multiplier for building software.\\n\\n## Policy time\\n\\nTypeScript was now at version 0.9.1. Still very much beta. Back then every release was breaking. Breaking. Very much with a capital \\"B\\".\\n\\nTypeScript had, since the very early days, made a commitment to track the ECMAScript standard. All JavaScript is valid TypeScript. However, there was briefly a period where this might not have been so. One of the things people most remember from the initial release is that they could now write classes. These were already the standardised classes of ES6 but it almost wasn\'t to be. For a brief period there had been consideration of doing something subtly different. In fact Anders would describe the TypeScript team\'s journey towards embracing the standards as a tale tinged with regret. In doing so they\'d had to say goodbye to a different implementation of classes which he\'d preferred but which they\'d ditched because they weren\'t standard.\\n\\nAlongside differences like this there were other delineations. Types had different names in the past which, as time went by, were renamed to align with standards. `boolean` was originally `bool` for instance; likely a reflection of Anders involvement with C#.\\n\\nThese sorts of changes, alongside any number of others, meant that each release of TypeScript sometimes entirely broke the definitions in Definitely Typed. Most notable was the 0.9.1 -> 0.9.5 migration. This was both an exercise in serious pain endurance and also a testament to the already strong commitment to TypeScript that existed. The reason people were willing to put the effort in to keep these migrations going was because they believed it was worth it. They believed in TypeScript. This PR is testament to that: [https://github.com/DefinitelyTyped/DefinitelyTyped/pull/1385](https://github.com/DefinitelyTyped/DefinitelyTyped/pull/1385)\\n\\nA level of flux meant that for a long time Definitely Typed committed only to support the latest version of TypeScript and the latest version of packages therein. These days it\'s not so brutal, but then it had to be as a matter of necessity.\\n\\nThe compiler was changing too fast and there were too few people involved to allow for any realistic alternative. As is often the case in software development, it was \\"good enough\\". Any other choice would probably have increased the workload of maintainers to a point where the project would no longer be a going concern. It was a choice with downsides; trade-offs. But it was the choice that best served the future of Definitely Typed and TypeScript.\\n\\n## Masahiro Wakame\\n\\n![A photograph of Masahiro Wakame](masahiro_wakame.webp)\\n\\nTime passed. Autumn turned into winter, winter into spring. TypeScript reached 1.0. It wasn\'t beta anymore. As each release came, the changes in the compiler became more gradual. This was a blessing for the Definitely Typed team. The projects popularity was ticking up and up. New definitions were added each day. The trickle of issues and PRs had become a stream, then a river. A river very much ready to burst its banks.\\n\\nIt was taking its toll. Inside Definitely Typed roles were shifting. Boris was starting to step back from day to day reviewing of PRs. New members were joining the project, like Igor Oleinikov. But the pace was insatiable.\\n\\nSome people left the project entirely, burned out by the never ending issues and PRs. Basarat started contributing less, beginning to turn his attention to one of his many sidejams. Fortunately, it turned out that before Basarat stepped back, he had done a very fine thing. In Tokyo, Japan was a 28 year old developer named Masahiro Wakame.\\n\\nMas was using JS to build the web applications he worked on. But ECMAScript 5 wasn\'t hitting the mark for him. For a time Masahiro used CoffeeScript (Jeremy Ashkenas Ruby style JS alternative). He liked it, but, as he put it: \\"I was shooting my foot everyday\\". Looking out for that elusive solution he landed on Dart. It looked amazing. But it wasn\'t ECMAScript. Masahiro worried he\'d be locked in. He\'d built some libraries and a testing framework using Dart. But he didn\'t feel he could suggest that his company adopted it; it was too different and only he knew it. He was left with the \\"what if I go under a bus?\\" problem. If he left the company, his colleagues would find it hard to move away from using Dart. This made him very hesitant. He didn\'t feel he could justify the choice.\\n\\nThen Masahiro heard about TypeScript. Like Goldilocks and the three bears, this third language sounded just right. He loved the type safety. It also had a compelling proposition: the transpiled JS that TypeScript generated was human readable and idiomatic. Generating idiomatic JS as opposed to some kind of strange byte code was a goal of the language from the early days, as Anders Hejlsberg would repeatedly explain. This generation of \\"real JS\\" made test driving TypeScript a low risk proposition. One that appealed to the likes of Masahiro. No lock-in. You decide TypeScript isn\'t for you? Fine. Take the generated JS files and shake the TypeScript dust off your sandals. Masahiro consequently went all in on TypeScript. This was his bet. And he was going to cover his bet by trying to make the ecosystem even stronger.\\n\\nMasahiro started out trying to improve the testing framework in DT; sending in pull requests. Before too long, Basarat messaged him to say \\"do you wanna become a committer?\\" [Masahiro became a committer.](https://github.com/DefinitelyTyped/DefinitelyTyped/pull/1358)\\n\\nIt turned out that MH had a special qualities that DT was going to sorely need: he was willing and able to review PR after PR, day after day. His stamina was incredible.\\n\\nWhilst it may not have been obvious from the outside, by now Definitely Typed was a slightly troubled project. The speed at which issues and PRs landed was relentless. Anyone who had once set GitHub to \\"watching\\" for Definitely Typed soon unsubscribed. It was becoming unmanageable. And whilst almost everyone else in the project was in the process of burning out / moving on / stepping back and similar, Masahiro kept going. He kept showing up. He kept reviewing. He kept merging. At his peek he was spending 2 hours a day, every day, glued to his screen in Tokyo and reviewing PRs for GitHub. The pulse of Definitely Typed may have slowed. But Masahiro kept the heart beating.\\n\\nAs Masahiro kept the lights on, in a hotel room in Buenos Aires an Australian named Blake Embrey was making plans...\\n\\n## Blake Embrey\\n\\n![A photograph of Blake Embrey](blake_embrey.webp)\\n\\nBlake was a 21 year old Australian. He was a nomadic developer, travelling around the world and working remotely. He travelled from country to country armed with a suitcase and his trusty MacBook in search of WiFi. He found himself dialing into standups from caf\xe9s in Vietnam at 1am to provide Jira updates, coding from airports as he criss-crossed the globe. It was an unusual life.\\n\\nA friend showed Blake TypeScript somewhere around the TypeScript 1.2 era. He was interested. He was mostly working on backend NodeJS at the time. He could see the potential that TypeScript had to help him. Around the TypeScript 1.5 era Blake started to take a really good look at what was possible. From his vantage point, there was good and there was also bad. And he thought he could help.\\n\\nAs a module author and developer, he loved TypeScript. It allowed him to write, publish and consume 100% type-safe JavaScript. Features like autocompletion, type validation and ES6 features became part of his typical workflow. It was so good!\\n\\nHowever, one step in this development lifecycle was broken to his mind. The problem was module shaped. Yeah, modules. Wade into the controversy!\\n\\nThanks again to Basarat, Blake soon became a DT contributor. Of all the contributors to Definitely Typed, Blake was the first one who was looking hard at the module problem. This was because whilst he wanted TypeScript to solve the same problems as everyone else, he wanted to solve them in a world of package dependencies. He wanted to solve for Node.\\n\\nNow it\'s worth taking a moment to draw a comparison between web development then, and web development now. Because it\'s changed. The phrase \\"web development fatigue\\" exists with good reason. Web development in 2014 as compared to web development in 2019 is a very different proposition. Historically, JavaScript has not had a good story around modularisation. The language meandered forwards without ever gaining an official approach to modularisation until ES6. So for twenty years, if you wanted to write a large JS application you had to think hard about how to solve this problem. And even when modules were nailed down it was longer still until module loading was standardised.\\n\\nBut that didn\'t stop us. JavaScript apps were still being built on the frontend and the backend. On the frontend an approach to modularisation emerged called the Asynchronous Module Definition. It had some adoption but in the main that wasn\'t how people rolled. The frontend was generally a sea of global variables. People would write programs that depended upon global variables and worked hard to make sure that they didn\u2019t collide. Everything did this. Everything. Underscore? It was a global variable called `_`. jQuery? It was two global variables: `$` and `jQuery`. That\'s just what people did. I\'m a person. I did that. If you were there you probably did too.\\n\\nOn the server side, in Node JS land, a different standard had emerged: CommonJS. Unlike AMD, CommonJS was simply how the Node JS community worked. Everything was a CommonJS module. Alongside Node, npm was growing and growing. Exposing Node developers to a rich ecosystem of modules or packages that they could drop into their apps with merely a tap tap tap of `npm install super-cool-package` and then `var scp = require(\'super-cool-package\')`.\\n\\nAnd therein, as the Bard would have it, lay the rub. You see, in the frontend it was simpler. Uglier but simpler. By and large, the global variables were fine. They weren\'t beautiful but they were functional. It may have impaired the development of frontend apps, but it certainly didn\'t stop it.\\n\\nAnd since a design goal of TypeScript was to meet JavaScript developers where they were and try and make their lives better, the initial focus of Definitely Typed was necessarily types that existed in the global namespace. So `jquery.d.ts` would declare global `$` and `jQuery` variables and underneath them all the jQuery methods and variables that were implemented. Alongside jQuery, maybe an application would have jQuery UI which would extend the `$` variable and add extra functionality. In addition maybe there\'d be a couple of jQuery plugins in play too. (It\'s worth saying that jQuery was the crack cocaine of web development back in the day. People just couldn\'t get enough.)\\n\\nTypeScript catered for this world by allowing type definitions to extend interfaces created by other definition files. The focus of most of the Definitely Typed contributors up to this point was frontend and hence DT was an ocean of global type definitions.\\n\\nOf course, this is not what the frontend world looks like these days. The frontend now is all about npm thanks to tools like Browserify, webpack, Rollup and the like. Client and server side development is mighty similar these days. Or at least, it\'s swimming in more of the same waters. There\'s a good TypeScript story to tell about this as well. But there wasn\'t always. Back to Blake.\\n\\nBlake had published a bunch of modules on npm. But no one had ever been able to consume the type definitions from them. Why was that? Well, without delving into great detail it comes down to type definitions of a package generally conflicting with type definitions that a user installs themselves.\\n\\nThis essentially came down to how TSD worked and what Definitely Typed contained. TSD was a pretty simple tool; by and large it worked by copying files from Definitely Typed into a users project. The files copied would contain type definitions which contained global types. So even though you cared solely about external modules, because of Definitely Typed you found yourself installing globals alongside which lead to conflicts between different type definitions. Different type definitions punching it out whilst the TypeScript compiler stood in between ineffectually shouting \\"leave it alone mate - it\'s not worth it!\\"\\n\\nHow could we have a world where external modules and global were treated distinctly? Blake had ideas\u2026 Plan one was to rewrite TSD to support external modules; the type of modules that were standard in Node land. After working hard on that for some time, Blake came to conclusion that solving global variables alongside external modules was a hard problem. A very hard problem. And perhaps that just running with external modules, [a new start if you will, represented the best way forwards](https://github.com/DefinitelyTyped/tsd/issues/150).\\n\\n## Typings\\n\\n![A screenshot of the Typings project](typings.webp)\\n\\nBlake made [typings](https://github.com/typings/typings). Typings was a number of things; it was a new command line tool to replace TSD, it was a new approach to distributing type definitions and it was a registry. But Typings was a registry which pointed out to the web. Typings installation was entirely decentralized and the typings themselves could be downloaded from almost anywhere - GitHub, NPM, Bower and even over HTTP or the filesystem. Those type definitions could be external modules or globals.\\n\\nIt was radical. From centralisation to decentralisation. As Blake described it:\\n\\n> This decentralization solves the biggest pain point I see with maintaining DefinitelyTyped. How does an author of one typings package maintain their file in DefinitelyTyped when they get notifications on thousands of others? How do you make sure typings maintain quality when you have 1000s to review? The solution in typings is you don\u2019t, the community does. If typings are incorrect, I can just write and install my own from wherever I want, something that TSD doesn\u2019t really allow. There\u2019s no merge or review process you need to wait for (300+ open pull requests!).\\n>\\n> However, decentralization comes with the cost of discoverability. To solve this, a registry exists that maintains locations of where the best typing can currently be installed from, for any version. If there\u2019s a newer typing, patches, or the old typing author has somehow disappeared, you can replace the entry with your own so people will be directed to your typings from now on.\\n\\nThe world started to use Typings as the default CLI for type definitions. `typings.json` files started appearing in people\'s repos. Typings allowed consumption of types both from the Typings registry and from Definitely Typed and so there was an easy on ramp for people to start using Typings.\\n\\nLittle by little, people started consuming type definitions that came from the typings registry rather than from Definitely Typed. Typings began to thrive whilst DT continued to choke. The community was beginning to diverge.\\n\\n## The TypeScript Team\\n\\n![A photograph of the TypeScript team](TypeScriptTeam.webp)\\n\\nOver in Seattle, the TypeScript team was thinking hard about the type definition ecosystem. About Definitely Typed and Typings. And about tooling and distribution.\\n\\nAt this point, there wasn\'t a dedicated registry for type definitions. There was GitHub. By and large, all type definitions lived in GitHub. Since GitHub is a git based source control provider it was possible for it to be used as a makeshift registry. So that\'s exactly what Definitely Typed and Typings were doing; piggy backing on GitHub and MacGyvering \\"infrastructure\\". It worked.\\n\\nThere wasn\'t a great versioning story. Definitely Typed just didn\'t do versioning. The latest and greatest was supported. Nothing else. The Typings approach was more nuanced. It did have an approach for versioning. It supported it by dint of allowing a version number in the registry to point to a specific git hash in a repo. It was an elegant and smart approach. Blake Embrey was one sharp cat.\\n\\nInnovative though it was, the decentralised Typings approach presented potential security risks as it pointed out to the web making auditing harder. Alongside this, The TypeScript team was pondering ways they could reduce friction for developers that wanted to use TypeScript.\\n\\nBy now, the JavaScript ecosystem had started to coalesce around npm as the registry du jour. Bower and jspm were starting to fade in popularity. NuGet (the .NET package manager) was no longer being encouraged as a place to house JS. npm was standard. TypeScript users found themselves using npm to install jQuery and reaching for tsd or Typings to install the associated type definitions. That\'s two commands. With two package managers. Each with subtly different syntax. And then perhaps you had to fiddle with the `tsconfig.json` to get the compiler looking in the right places. It worked. But it didn\'t feel \u2026. idiomatic. It didn\'t feel like TypeScript was meeting their users where they were.\\n\\nThe likes of Daniel Rosenwasser, Mohamed Hegazy and Ryan Cavanaugh found themselves pondering the problem. Alongside this, they were thinking more about what a first class module support experience in TypeScript would look like, motivated in part by the critical mass around npm, which was entirely module / package based.\\n\\nThat wasn\u2019t the only thing on their minds; there was also the testing story. Definitely Typed had a straightforward testing story due to being a real mega-repo. Everything lived together and could be tested together. Thanks to the hard work of the Definitely Typed team this was already in place; every PR spun up Travis and tested all the type definitions individually and in concert with one another. Typings didn\u2019t have this. What\u2019s more, it would be hard to build. The decentralised nature of Typings meant that you\u2019d need to build infrastructure to crawl the Typings registry, download the type definitions and then perform the tests. It was non-trivial and unlikely to be speedy.\\n\\nThere was one more factor in play. The TypeScript team were aware that for the longest time they\'d been working on the language. But they\'d become distant from one of the most significant aspects of how the language was used. They weren\u2019t well enough informed about the rough edges in the type definition space. They weren\u2019t feeling their users pain. They needed to address this and really there was only one thing to do... It was time for the TypeScript team to start eating their own dogfood.\\n\\n## A Plan Emerges\\n\\nThe TypeScript team reached out to Blake Embrey and started to talk about ways forward. They started collaborating over Slack.\\n\\n![A screenshot of the collaboration on Slack](typings_typescript_collaboration.webp)\\n\\nThe TypeScript team had also been in contact with the Definitely Typed team. They were, at this point, aware that Definitely Typed was being kept going mainly due to the hard graft of Masahiro Wakame. As Daniel observed \u201Cvvakame was a champ\u201D.\\n\\nAt this point I have to stick my own hand up and confess to thinking that Definitely Typed was not long for this world. Steve Ognibene (another DT member) and others were all feeling similarly. It seemed inevitable.\\n\\n![A photograph of Steve Ognibe](steveognibe.webp)\\n\\nThe TypeScript team were about to change that. After talking, thinking, thinking and talking they put together a plan. It was going to change TypeScript and change Definitely Typed. It was also going to effectively end Typings.\\n\\nIt\u2019s worth saying at this point that the TypeScript team didn\u2019t enter into this lightly. They were hugely impressed by Typings. It was, to quote Daniel Rosenwasser, \u201Can impressive piece of work\u201D. It also had the most amazing command line experience. Everyone on the team felt that it was an incredible endeavour and had their proverbial hats off to Blake Embrey. But Definitely Typed had critical mass and, whilst it had known problems, they were problems that could be likely solved (or ameliorated) through automation. The Typings approach was very innovative, but it presented other issues which seemed harder to solve. The TypeScript team made a bet. They placed their money on Definitely Typed.\\n\\nTo remove friction in the type acquisition space they decided to change the compiler. It would now look out for a special scoped namespace on npm named @types. Type definitions from Definitely Typed would be published out to @types. They would land as type definition packages that matched the non @types package. This meant that TypeScript was now sharing the same infrastructure as the rest of the JS ecosystem: npm. And consequently, installation of a package like jQuery in a TypeScript workflow now looked like this: `npm install jquery @types/jquery`. One command, one tool, one registry.\\n\\nThey published their plans here: [https://github.com/Microsoft/TypeScript/issues/9184](https://github.com/Microsoft/TypeScript/issues/9184)\\n\\nThere was more. The TypeScript team had really enjoyed knowing that this open source project which ran completely independently from the TypeScript team existed. And whilst they were focused directly on the language that was reasonable. But with the changes that were being planned, TypeScript was about to start explicitly depending upon Definitely Typed. It had been unofficially true up until that point. But now it was different; TypeScript were going to automate publishing Definitely Typed packages to the special @types scope in npm which the TypeScript compiler gave preference to. TypeScript and Definitely Typed were going from dating to being engaged.\\n\\nIt was time for the TypeScript team to get involved.\\n\\nThe team committed to doing weekly rotations of a TypeScript team member working on Definitely Typed. Reviewing PRs, merging them and, crucially, helping with automation and testing.\\n\\nTypeScript was now part of Definitely Typed. Definitely Typed was part of TypeScript.\\n\\n## TypeScript 2.0 / Definitely Typed 2.0\\n\\nBlake was immensely disappointed. He\'d put his heart and soul into Typings. It was a massive amount of work and he\'d not only started a project, he\'d started a community that he felt responsible for.\\n\\nAlthough that work had arguably kickstarted the discussion of what the future of type acquisition in TypeScript should look like, Typings wouldn\'t be coming along for the ride. It was a burner rocket, carrying the good ship TypeScript into outer orbit, dropping back to Earth once it\'s job was done.\\n\\nVery much, Blake had in mind all the people that had contributed to Typings. That all their work was going to be abandoned. He felt a sense of responsibility. It was both frustrating and heartbreaking.\\n\\nWhen TypeScript 2.0 shipped, in the release announcement was the following statement: [https://devblogs.microsoft.com/typescript/announcing-typescript-2-0/](https://devblogs.microsoft.com/typescript/announcing-typescript-2-0/)\\n\\n> We\u2019d like to thank Blake Embrey for his work on Typings and helping us bring this solution forward.\\n\\nBlake really appreciated the recognition. In years to come Blake would come to feel that the decisions made were the right ones. That they lead to TypeScripts continued success and served the community well. But he has regrets. He says now \\"I am disappointed we didn\'t get to integrate the two philosophies for managing types. It hurt Typings registry contributors without a story in place, I didn\u2019t want to let down and alienate potential contributors of type definitions.\\"\\n\\nA young Australian man had helped change the direction of TypeScript. It was time for him to take a well earned rest.\\n\\nIn the meantime, the TypeScript team was starting to get stuck into the work of giving Definitely Typed a make-over.\\n\\n![Screenshot of the rota for Definitely Typed work for the TypeScript team](rotation.png)\\n\\nAt this point, Definitely Typed had more than 500 open pull requests. Most of which had been open for a very long time. The most urgent and pressing problem was getting that down. The TypeScript team committed to, in perpetuity, a weekly rotation where one team member would review PRs. This would, in future, mean that PRs were handled in a timely fashion and that the number of open PRs was generally kept beneath 100.\\n\\nAlongside this, changes were being made to the TypeScript compiler. In large part these related to enabling automatic type acquisition through the @types scope. To make that work, the TypeScript team realised pretty quickly that many of the type definitions would not work as is. Ryan wrote up this report:\\n\\n![Ryan Cavanagh\'s report on what to do in Definitely Typed](RyansDefTypReport.webp)\\n\\nAt this point in time there were around 1700 type definitions. Pretty much all of them required some massaging. Roughly speaking, with TS 2.0, the language was going to move from a name based type acquisition approach to a file based one. New features were added to TypeScript 2.0 such as the `export as namespace` syntax to support a type definition supporting both being used in modules (where there are `import` / `export`s) but also in script files (where there aren\'t)\\n\\nRyan Cavanaugh put together scripts that migrated 1200 of the type definitions to TypeScript 2.0 syntax. The remaining 500 were delicately transitioned by hand by diligent TypeScript team members. It was a task of utter drudgery that still sparks flickers of PTSD in those who were involved. It was like being in the digital equivalent of a Dickensian workhouse.\\n\\nThis was one of the reasons why going with the centralised approach of Definitely Typed instead of the decentralised one of Typings was necessary. Because the TypeScript team were involved in DT they could help make things happen. They could do the hard work. In a decentralised world that wouldn\'t be possible; everything would constantly be held up, waiting.\\n\\nIt took a long time to get the types 2.0 branch to a point where CI went green. All this time, merges we\'re taking place between the master branch and the future one. It was hard, unglamorous work. As Ryan put it, \\"I partied hard when CI went green for the first time on types 2.0.\\"\\n\\n![Screenshot of @types going green](types20goinggreen.webp)\\n\\nThe first and most obvious addition was the automation of TypeScript definitions being published out to npm.\\n\\nNext came a solution for the \\"notification flood\\" issue. It was no longer feasible for a user to have Definitely Typed set up as \\"watching\\" in GitHub. That way lead an unstoppable deluge of information about issues and pull requests. The result of that was that users were generally unaware of changes / issues and so on. People, as much as they wanted to be, were becoming disconnected from the type definitions they were interested in in DT.\\n\\nThe solution for this problem was, as with so many problems, a bot. It would send notifications to the users who had historically worked on a type definition when someone sent a PR. This was hugely useful. It made it possible for people to become effective stewards of the type definitions they knew about. It meant people could effectively remain involved with DT; giving them targeted information. It was the solution to a communications problem.\\n\\nAs Ryan Cavanaugh put it when he looked back upon TypeScripts story, he had this to say: \u201CDefinitely Typed is the best thing that could exist from our perspective\u201D.\\n\\nHe was speaking from the perspective of a TypeScript team member. He could as well be speaking for the developer world at large. Definitely Typed is an organic monster of open source goodness; bringing types to the world thanks to nearly 10,000 contributors. Each person of which has donated at least an hour or their time for the greater good. Far more than that in many cases. It\u2019s incredible. I\u2019m glad I get to be part of it. I never would have guessed it would have turned out like this."},{"id":"start-me-up-ts-loader-meet-tsbuildinfo","metadata":{"permalink":"/start-me-up-ts-loader-meet-tsbuildinfo","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2019-09-30-start-me-up-ts-loader-meet-tsbuildinfo/index.md","source":"@site/blog/2019-09-30-start-me-up-ts-loader-meet-tsbuildinfo/index.md","title":"Start Me Up: ts-loader meet .tsbuildinfo","description":"TypeScript 3.4 introduced `.tsbuildinfo` files for faster compilations. With TypeScript 3.6, APIs landed to enable third party tool integration.","date":"2019-09-30T00:00:00.000Z","tags":[{"inline":false,"label":"ts-loader","permalink":"/tags/ts-loader","description":"The TypeScript loader for webpack."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":1.735,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"start-me-up-ts-loader-meet-tsbuildinfo","title":"Start Me Up: ts-loader meet .tsbuildinfo","authors":"johnnyreilly","tags":["ts-loader","typescript"],"hide_table_of_contents":false,"description":"TypeScript 3.4 introduced `.tsbuildinfo` files for faster compilations. With TypeScript 3.6, APIs landed to enable third party tool integration."},"unlisted":false,"prevItem":{"title":"Definitely Typed: The Movie","permalink":"/definitely-typed-the-movie"},"nextItem":{"title":"Coming Soon: Definitely Typed","permalink":"/coming-soon-definitely-typed"}},"content":"With TypeScript 3.4, [a new behaviour landed and a magical new file type appeared; `.tsbuildinfo`](https://www.typescriptlang.org/docs/handbook/release-notes/typescript-3-4.html)\\n\\n\x3c!--truncate--\x3e\\n\\n> TypeScript 3.4 introduces a new flag called `--incremental` which tells TypeScript to save information about the project graph from the last compilation. The next time TypeScript is invoked with `--incremental`, it will use that information to detect the least costly way to type-check and emit changes to your project.\\n>\\n> ...\\n>\\n> These `.tsbuildinfo` files can be safely deleted and don\u2019t have any impact on our code at runtime - they\u2019re purely used to make compilations faster.\\n\\nThis was all very exciting, but until the release of TypeScript 3.6 there were no APIs available to allow third party tools like `ts-loader` to hook into them. The wait is over! Because with TypeScript 3.6 the APIs landed: [https://www.typescriptlang.org/docs/handbook/release-notes/typescript-3-6.html#apis-to-support---build-and---incremental](https://www.typescriptlang.org/docs/handbook/release-notes/typescript-3-6.html#apis-to-support---build-and---incremental)\\n\\nThis was the handiwork of the very excellent [@sheetalkamat](https://twitter.com/sheetalkamat) of the TypeScript team - you can see her PR here: [https://github.com/microsoft/TypeScript/pull/31432](https://github.com/microsoft/TypeScript/pull/31432)\\n\\nWhat\'s more, Sheetal took the PR for a test drive using `ts-loader`, and her hard work has just shipped with [`v6.2.0`](https://github.com/TypeStrong/ts-loader/releases/tag/v6.2.0):\\n\\n- [https://github.com/TypeStrong/ts-loader/pull/1012](https://github.com/TypeStrong/ts-loader/pull/1012)\\n- [https://github.com/TypeStrong/ts-loader/pull/1017](https://github.com/TypeStrong/ts-loader/pull/1017)\\n\\nIf you\'re a `ts-loader` user, and you\'re using TypeScript 3.6+ then you can get the benefit of this now. That is, if you make use of the `experimentalWatchApi: true` option. With this set:\\n\\n1. ts-loader will both emit and consume the `.tsbuildinfo` artefact.\\n\\n2. This applies both when a project has `tsconfig.json` options `composite` or `incremental` set to `true`.\\n\\n3. The net result of people using this should be faster cold starts in build time where a previous compilation has taken place.\\n\\n## `ts-loader v7.0.0`\\n\\nWe would love for you to take this new functionality for a spin. Partly because we think it will make your life better. And partly because we\'re planning to make using the watch API the default behaviour of `ts-loader` when we come to ship `v7.0.0`.\\n\\nIf you can take this for a spin before we make that change we\'d be so grateful. Thanks so much to Sheetal for persevering away on this feature. It\'s amazing work and so very appreciated."},{"id":"coming-soon-definitely-typed","metadata":{"permalink":"/coming-soon-definitely-typed","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2019-09-14-coming-soon-definitely-typed/index.md","source":"@site/blog/2019-09-14-coming-soon-definitely-typed/index.md","title":"Coming Soon: Definitely Typed","description":"The story of Definitely Typed, a project aiming to provide type definitions for JavaScript libraries, from creation to top 10 in GitHub in 2018.","date":"2019-09-14T00:00:00.000Z","tags":[{"inline":false,"label":"Definitely Typed","permalink":"/tags/definitely-typed","description":"The Definitely Typed project for TypeScript type definitions"},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":0.99,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"coming-soon-definitely-typed","title":"Coming Soon: Definitely Typed","authors":"johnnyreilly","tags":["definitely typed","typescript"],"hide_table_of_contents":false,"description":"The story of Definitely Typed, a project aiming to provide type definitions for JavaScript libraries, from creation to top 10 in GitHub in 2018."},"unlisted":false,"prevItem":{"title":"Start Me Up: ts-loader meet .tsbuildinfo","permalink":"/start-me-up-ts-loader-meet-tsbuildinfo"},"nextItem":{"title":"Symbiotic Definitely Typed","permalink":"/symbiotic-definitely-typed"}},"content":"A long time ago (well, 2012) in a galaxy far, far away (okay; Plovdiv, Bulgaria)....\\n\\n\x3c!--truncate--\x3e\\n\\n[Definitely Typed](https://github.com/DefinitelyTyped/DefinitelyTyped) began!\\n\\nThis is a project that set out to provide type definitions for every JavaScript library that lacked them. An ambitious goal. Have you ever wondered what the story that lay behind it was?\\n\\nPerhaps you know that the project was started by a shadowy figure named \\"Boris Yankov\\". And maybe you know that the TypeScript team is now part of the Definitely Typed team. There\'s a lot more to tell.\\n\\nThis autumn, I\'d like to tell you the story of how Definitely Typed came to be what it is. From an individual commit in a repo that Boris created in 2012 to [the number 10 project by contributions on GitHub in 2018](https://octoverse.github.com/projects). I\'m part of that story. Basarat Ali Syed is part of that story. Masahi Wakame too. Blake Embrey. Steve Fenton. Igor Oleinikov. It\'s an amazing and unexpected tale. One that turns upon the actions of individuals. They changed your life and I\'d love you to learn how.\\n\\nSo, coming soon to a blog post near you, is the story of Definitely Typed. It\'s very exciting! Stay tuned..."},{"id":"symbiotic-definitely-typed","metadata":{"permalink":"/symbiotic-definitely-typed","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2019-08-17-symbiotic-definitely-typed/index.md","source":"@site/blog/2019-08-17-symbiotic-definitely-typed/index.md","title":"Symbiotic Definitely Typed","description":"New approach by `react-testing-library` improves TypeScript experience. Type definitions are maintained separately for `@testing-library/react`.","date":"2019-08-17T00:00:00.000Z","tags":[{"inline":false,"label":"Definitely Typed","permalink":"/tags/definitely-typed","description":"The Definitely Typed project for TypeScript type definitions"},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":5.62,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"symbiotic-definitely-typed","title":"Symbiotic Definitely Typed","authors":"johnnyreilly","tags":["definitely typed","typescript"],"hide_table_of_contents":false,"description":"New approach by `react-testing-library` improves TypeScript experience. Type definitions are maintained separately for `@testing-library/react`."},"unlisted":false,"prevItem":{"title":"Coming Soon: Definitely Typed","permalink":"/coming-soon-definitely-typed"},"nextItem":{"title":"ASP.NET Core authentication: hard-coding a claim in development","permalink":"/asp-net-authentication-hard-coding-claims"}},"content":"I did ponder calling this post \\"how to enable a good TypeScript developer experience for npm modules that aren\'t written in TypeScript\\"... Not exactly pithy though.\\n\\n\x3c!--truncate--\x3e\\n\\nDefinitely Typed is the resource which allows developers to use TypeScript with existing JavaScript libraries that ship without their own type definitions.\\n\\nDT began as a way to enable interop between JS and TS. When DT started, everything on npm was JavaScript. Over time it has become more common for libraries (eg [Mobx](https://github.com/mobxjs/mobx) / [Angular](https://github.com/angular/angular)) to be written (or rewritten) in TypeScript. For publishing, they are compiled down to JS with perfect type definitions generated from the TypeScript alongside the compiled JavaScript. These libraries do not need to exist in Definitely Typed anymore.\\n\\nAnother pattern that has emerged over time is that of type definitions being removed from Definitely Typed to live and be maintained alongside the libraries they support. An example of this is [MomentJS](https://github.com/moment/moment).\\n\\nThis week, I think for the first time, there emerged another approach. [Kent C Dodds](https://kentcdodds.com/)\' `react-testing-library` had started out with the MomentJS approach of hosting type definitions alongside the JavaScript source code. [Alex Krolic raised a PR which proposed removing the type definitions from the RTL repo in favor of having the community maintain them at DefinitelyTyped.](https://github.com/testing-library/react-testing-library/pull/437)\\n\\nI\'ll directly quote Kent\'s explanation of the motivation for this:\\n\\n> We were getting a lot of drive-by contributions to the TypeScript typings and many pull requests would either sit without being reviewed by someone who knows TypeScript well enough, or be merged by a maintainer who just hoped the contributor knew what they were doing. This resulted in a poor experience for TypeScript users who could experience type definition churn and delays, and it became a burden on project maintainers as well (most of us don\'t know TypeScript very well). Moving the type definitions to DefinitelyTyped puts the maintenance in much more capable hands.\\n\\nI have to admit I was reticent about this idea in the first place. I like the idea that types ship with the package they support. It\'s a good developer experience; users install your package and it works with TypeScript straight out of the box. However Alex\'s PR addressed a real issue: what do you do when the authors of a package aren\'t interested / equipped / don\'t have the time to support TypeScript? Or don\'t want to deal with the noise of TypeScript related PRs which aren\'t relevant to them. What then?\\n\\nAlex was saying, let\'s not force it. Let the types and the library be maintained separately. This can and is done well already; React is a case in point. The React team does not work on the type definitions for React, that\'s done (excellently) by a crew of dedicated React lovers in Definitely Typed.\\n\\nIt\'s a fair point. The thing that was sad about this move was that the developer experience was going to have more friction. Users would have to `yarn add -D @testing-library/react` and then subsequently `yarn add -D @types/testing-library__react` to get the types.\\n\\nThis two step process isn\'t the end of the world, but it does make it marginally harder for TypeScript users to get up and running. It reduces the developer joy. As a side note, this is made more unlovely by `@testing-library/react` being a scoped package. [Types for a scoped package have a quirky convention for publishing.](https://stackoverflow.com/questions/47296731/how-can-i-install-typescript-declarations-for-scoped-namespaced-packages-via-ty) A fictional scoped package of `@foo/bar` would be published to npm as: `@types/foo__bar`. This is functional but non-obvious; it\'s tricky to discover. A two step process instead of a one step process is a non-useful friction that it would be great to eliminate.\\n\\nFortunately, Kent and [Daniel K](https://github.com/FredyC) had one of these moments:\\n\\n![](hang-on-lads-ive-got-a-great-idea.webp)\\n\\nKent suggested that at the same time as dropping the type definitions that were shipped with the library, we try making `@types/testing-library__react` a dependency of `@testing-library/react`. This would mean that people installing `@testing-library/react` would get `@types/testing-library__react` installed _automatically_. So from the developers point of view, it\'s as though the type definitions shipped with the package directly.\\n\\nTo cut a long story short reader, that\'s what happened. If you\'re using `@testing-library/react` from 9.1.2 you\'re getting Definitely Typed under the covers. This was [nicely illustrated by Kent](https://github.com/testing-library/react-testing-library/pull/437#issuecomment-521763117) showing what the TypeScript consumption experience looked like before the Definitely Typed switch:\\n\\n![](RTL-9.1.1.webp)\\n\\nAnd here\'s what it looked like after:\\n\\n![](RTL-9.1.2.webp)\\n\\nIdentical! i.e it worked. I grant you this is one of the more boring before / after comparisons there is\u2026 But hopefully you can see it demonstrates that this is giving us exactly what we need.\\n\\nTo quote Kent once more:\\n\\n> By adding the type definitions to the dependencies of React Testing Library, the experience for users is completely unchanged. So it\'s a huge improvement for the maintenance of the type definitions without any breaking changes for the users of those definitions.\\n\\nThis is clearly an approach that\'s useful; it adds value. It would be tremendous to see other libraries that aren\'t written in TypeScript but would like to enable a good TypeScript experience for those people that do use TS also adopting this approach.\\n\\n## Update: Use a Loose Version Range in `package.json`\\n\\nWhen I [tweeted this article](https://twitter.com/johnny_reilly/status/1162843916661592064) it prompted this helpful response from [Andrew Branch](https://twitter.com/atcb) of the TypeScript team:\\n\\n> \\\\> use a loose version range This is my advice as well and should probably be mentioned in the article TBH.\\n>\\n> \u2014 Kent C. Dodds (@kentcdodds) [August 18, 2019](https://twitter.com/kentcdodds/status/1162876792287293440?ref_src=twsrc%5Etfw)\\n\\nAndrew makes the useful point that if you are adding support for TypeScript via an `@types/...` dependency then it\'s wise to do so with a loose version range. [In the case of RTL we did it like this:](https://github.com/testing-library/react-testing-library/blob/c4ba755e42938018ec67dbc716037cfafca15e03/package.json#L46)\\n\\n```json\\n\\"@types/testing-library__react\\": \\"^9.1.0\\"\\n```\\n\\ni.e. Any type definition with a version of `9.1` or greater (whilst still lower than `10.0.0`) is considered valid. You could go even looser than that. If you really don\'t want to think about TypeScript beyond adding the dependency then a completely loose version range would do:\\n\\n```json\\n\\"@types/testing-library__react\\": \\"*\\"\\n```\\n\\nThis will always install the latest version of the `@types/testing-library__react` dependency and (importantly) allow users to override if there\'s a problematic `@types/testing-library__react` out there. This level of looseness is not really advised though. As in the scenario when a library (and associated type definitions) do a major release, users of the old major would get the wrong definitions by default when installing or upgrading (in range).\\n\\nProbably the most helpful approach is the approach followed by RTL; fixing the major version but allowing all minor and patch releases _inside_ a major version.\\n\\n## Updated 2: Further Discussions!\\n\\nThe technique used in this blog post sparked an interesting conversation with members of the TypeScript team when it was applied to [`https://github.com/testing-library/jest-dom`](https://github.com/testing-library/jest-dom). [The conversation can be read here](https://github.com/testing-library/jest-dom/issues/123#issuecomment-523586977)."},{"id":"asp-net-authentication-hard-coding-claims","metadata":{"permalink":"/asp-net-authentication-hard-coding-claims","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2019-08-02-asp-net-authentication-hard-coding-claims/index.md","source":"@site/blog/2019-08-02-asp-net-authentication-hard-coding-claims/index.md","title":"ASP.NET Core authentication: hard-coding a claim in development","description":"The DevelopmentModeAuthenticationHandler allows ASP.NET Core developers to hard code user authentication claims during development, easing testing.","date":"2019-08-02T00:00:00.000Z","tags":[{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."},{"inline":false,"label":"Authentication and Authorization","permalink":"/tags/auth","description":"Matters related to identity and permissioning"}],"readingTime":2.775,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"asp-net-authentication-hard-coding-claims","title":"ASP.NET Core authentication: hard-coding a claim in development","authors":"johnnyreilly","tags":["asp.net","auth"],"hide_table_of_contents":false,"description":"The DevelopmentModeAuthenticationHandler allows ASP.NET Core developers to hard code user authentication claims during development, easing testing."},"unlisted":false,"prevItem":{"title":"Symbiotic Definitely Typed","permalink":"/symbiotic-definitely-typed"},"nextItem":{"title":"Using TypeScript and ESLint with webpack (fork-ts-checker-webpack-plugin new feature!)","permalink":"/typescript-and-eslint-meet-fork-ts-checker-webpack-plugin"}},"content":"This post demonstrates how you can hard code user authentication claims in ASP.NET Core; a useful technique to facilate testing during development.\\n\\n\x3c!--truncate--\x3e\\n\\nI was recently part of a hackathon team that put together an API in just 30 hours. We came second. (Not bitter, not bitter...)\\n\\nWe were moving pretty quickly during the hackathon and, when we came to the end of it, we had a working API which we were able to demo. The good news is that the API is going to graduate to be a product! We\'re going to ship this. Before we can do that though, there\'s a little tidy up to do.\\n\\nThe first thing I remembered / realised when I picked up the codebase again, was the shortcuts we\'d made on the developer experience. We\'d put the API together using ASP.Net Core. We\'re handling authentication using JWTs which is nicely supported. When we\'re deployed, an external facing proxy calls our application with the appropriate JWT and everything works as you\'d hope.\\n\\nThe question is, what\'s it like to develop against this on your laptop? Getting a JWT for when I\'m debugging locally is too much friction. I want to be able to work on the problem at hand, going away to get a JWT each time is a timesuck. So what to do? Well, during the hackathon, we just commented out `[Authorize]` attributes and hardcoded user ids in our controllers. This works, but it\'s a messy developer experience; it\'s easy to forget to uncomment things you\'ve commented and break things. There must be a better way.\\n\\nThe solution I landed on was this: in development mode (which we only use whilst debugging) we hardcode an authenticated user. The way our authentication works is that we have a claim on our principal called something like `\\"our-user-id\\"`, the value of which is our authenticated user id. So in the `ConfigureServices` method of our `Startup.cs` we have a conditional authentication registration like this:\\n\\n```cs\\n// Whilst developing, we don\'t want to authenticate; we hardcode to a particular users id\\nif (Env.IsDevelopment()) {\\n    services.AddAuthentication(nameof(DevelopmentModeAuthenticationHandler))\\n        .AddScheme<DevelopmentModeAuthenticationOptions, DevelopmentModeAuthenticationHandler>(\\n            nameof(DevelopmentModeAuthenticationHandler),\\n            options => {\\n                options.UserIdToSetInClaims = \\"this-is-a-user-id\\";\\n            }\\n        );\\n}\\nelse {\\n    // The application typically uses this\\n    services.AddAuthentication(JwtBearerDefaults.AuthenticationScheme)\\n        .AddJwtBearer(options => {\\n            // ...\\n        });\\n}\\n```\\n\\nAs you can see, we\'re using a special `DevelopmentModeAuthenticationHandler` authentication scheme in development mode, instead of JWT. As we register that, we declare the user id that we want to use. Whenever the app runs using the `DevelopmentModeAuthenticationHandler` auth, all requests will arrive using a principal with an `\\"our-user-id\\"` claim with a value of `\\"this-is-a-user-id\\"` (or whatever you\'ve set it to.)\\n\\nThe `DevelopmentModeAuthenticationHandler` looks like this:\\n\\n```cs\\nusing System.Collections.Generic;\\nusing System.Security.Claims;\\nusing System.Text.Encodings.Web;\\nusing System.Threading.Tasks;\\nusing Microsoft.AspNetCore.Authentication;\\nusing Microsoft.Extensions.Logging;\\nusing Microsoft.Extensions.Options;\\n\\nnamespace OurApp\\n{\\n    public class DevelopmentModeAuthenticationOptions : AuthenticationSchemeOptions\\n    {\\n        public string UserIdToSetInClaims { get; set; }\\n    }\\n\\n    public class DevelopmentModeAuthenticationHandler : AuthenticationHandler<DevelopmentModeAuthenticationOptions> {\\n        private readonly ILoggingService _loggingService;\\n\\n        public DevelopmentModeAuthenticationHandler(\\n            IOptionsMonitor<DevelopmentModeAuthenticationOptions> options,\\n            ILoggerFactory logger,\\n            UrlEncoder encoder,\\n            ISystemClock clock\\n        ) : base(options, logger, encoder, clock) {\\n        }\\n\\n        protected override Task<AuthenticateResult> HandleAuthenticateAsync() {\\n            var claims = new List<Claim> { new Claim(\\"our-user-id\\", Options.UserIdToSetInClaims) };\\n\\n            var identity = new ClaimsIdentity(claims, nameof(DevelopmentModeAuthenticationHandler));\\n            var ticket = new AuthenticationTicket(new ClaimsPrincipal(identity), Scheme.Name);\\n\\n            return Task.FromResult(AuthenticateResult.Success(ticket));\\n        }\\n    }\\n}\\n```\\n\\nNow, developing locally is frictionless! We don\'t comment out `[Authorize]` attributes, we don\'t hard code user ids in controllers."},{"id":"typescript-and-eslint-meet-fork-ts-checker-webpack-plugin","metadata":{"permalink":"/typescript-and-eslint-meet-fork-ts-checker-webpack-plugin","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2019-07-13-typescript-and-eslint-meet-fork-ts-checker-webpack-plugin/index.md","source":"@site/blog/2019-07-13-typescript-and-eslint-meet-fork-ts-checker-webpack-plugin/index.md","title":"Using TypeScript and ESLint with webpack (fork-ts-checker-webpack-plugin new feature!)","description":"The `fork-ts-checker-webpack-plugin` adds support for ESLint. Replace TSLint with related packages in `package.json` and configure with `.eslintrc.js`.","date":"2019-07-13T00:00:00.000Z","tags":[{"inline":false,"label":"fork-ts-checker-webpack-plugin","permalink":"/tags/fork-ts-checker-webpack-plugin","description":"The fork-ts-checker-webpack-plugin for webpack."},{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."},{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."},{"inline":false,"label":"ESLint","permalink":"/tags/eslint","description":"The ESLint linter."}],"readingTime":4.615,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"typescript-and-eslint-meet-fork-ts-checker-webpack-plugin","title":"Using TypeScript and ESLint with webpack (fork-ts-checker-webpack-plugin new feature!)","authors":"johnnyreilly","tags":["fork-ts-checker-webpack-plugin","webpack","javascript","typescript","eslint"],"hide_table_of_contents":false,"description":"The `fork-ts-checker-webpack-plugin` adds support for ESLint. Replace TSLint with related packages in `package.json` and configure with `.eslintrc.js`."},"unlisted":false,"prevItem":{"title":"ASP.NET Core authentication: hard-coding a claim in development","permalink":"/asp-net-authentication-hard-coding-claims"},"nextItem":{"title":"TypeScript / webpack - you down with PnP? Yarn, you know me!","permalink":"/typescript-webpack-you-down-with-pnp"}},"content":"The `fork-ts-checker-webpack-plugin` has, since its inception, performed two classes of checking:\\n\\n\x3c!--truncate--\x3e\\n\\n1. Compilation errors which the TypeScript compiler surfaces up\\n2. Linting issues which TSLint reports\\n\\n[You may have caught the announcement that TSLint is being deprecated and ESLint is the future of linting in the TypeScript world.](https://eslint.org/blog/2019/01/future-typescript-eslint) This plainly has a bearing on linting in `fork-ts-checker-webpack-plugin`.\\n\\n[I\'ve been beavering away at adding support for ESLint to the fork-ts-checker-webpack-plugin.](https://github.com/TypeStrong/fork-ts-checker-webpack-plugin/pull/305) I\'m happy to say, the plugin now supports ESLint. Do you want to get your arms all around ESLint with `fork-ts-checker-webpack-plugin`? Read on!\\n\\n## How do you migrate from TSLint to ESLint?\\n\\nWell, first of all you need the latest and greatest `fork-ts-checker-webpack-plugin`. Support for ESLint shipped with [v1.4.0](https://github.com/TypeStrong/fork-ts-checker-webpack-plugin/releases/tag/v1.4.0).\\n\\nYou need to change the options you supply to the plugin in your `webpack.config.js` to look something like this:\\n\\n```js\\nnew ForkTsCheckerWebpackPlugin({ eslint: true });\\n```\\n\\nYou\'ll also need the various ESLint related packages to your `package.json`:\\n\\n```js\\nyarn add eslint @typescript-eslint/parser @typescript-eslint/eslint-plugin --dev\\n```\\n\\n- `eslint`\\n- `@typescript-eslint/parser`: The parser that will allow ESLint to lint TypeScript code\\n- `@typescript-eslint/eslint-plugin`: A plugin that contains ESLint rules that are TypeScript specific\\n\\nIf you want, you can pass options to ESLint using the `eslintOptions` option as well. These will be passed through to the underlying ESLint CLI Engine when it is instantiated. Docs on the supported options are [documented here](https://eslint.org/docs/developer-guide/nodejs-api#cliengine).\\n\\n## Go Configure\\n\\nNow you\'re ready to use ESLint, you just need to give it some configuration. Typically, an `.eslintrc.js` is what you want here.\\n\\n```js\\nconst path = require(\'path\');\\nmodule.exports = {\\n  parser: \'@typescript-eslint/parser\', // Specifies the ESLint parser\\n  plugins: [\'@typescript-eslint\'],\\n  env: {\\n    browser: true,\\n    jest: true,\\n  },\\n  extends: [\\n    \'plugin:@typescript-eslint/recommended\', // Uses the recommended rules from the @typescript-eslint/eslint-plugin\\n  ],\\n  parserOptions: {\\n    project: path.resolve(__dirname, \'./tsconfig.json\'),\\n    tsconfigRootDir: __dirname,\\n    ecmaVersion: 2018, // Allows for the parsing of modern ECMAScript features\\n    sourceType: \'module\', // Allows for the use of imports\\n  },\\n  rules: {\\n    // Place to specify ESLint rules. Can be used to overwrite rules specified from the extended configs\\n    // e.g. \\"@typescript-eslint/explicit-function-return-type\\": \\"off\\",\\n    \'@typescript-eslint/explicit-function-return-type\': \'off\',\\n    \'@typescript-eslint/no-unused-vars\': \'off\',\\n  },\\n};\\n```\\n\\nIf you\'re a React person (and I am!) then you\'ll also need: `yarn add eslint-plugin-react`. Then enrich your `eslintrc.js` a little:\\n\\n```js\\nconst path = require(\'path\');\\nmodule.exports = {\\n  parser: \'@typescript-eslint/parser\', // Specifies the ESLint parser\\n  plugins: [\\n    \'@typescript-eslint\',\\n    \'react\',\\n    // \'prettier\' commented as we don\'t want to run prettier through eslint because performance\\n  ],\\n  env: {\\n    browser: true,\\n    jest: true,\\n  },\\n  extends: [\\n    \'plugin:@typescript-eslint/recommended\', // Uses the recommended rules from the @typescript-eslint/eslint-plugin\\n    \'prettier/@typescript-eslint\', // Uses eslint-config-prettier to disable ESLint rules from @typescript-eslint/eslint-plugin that would conflict with prettier\\n    // \'plugin:react/recommended\', // Uses the recommended rules from @eslint-plugin-react\\n    \'prettier/react\', // disables react-specific linting rules that conflict with prettier\\n    // \'plugin:prettier/recommended\' // Enables eslint-plugin-prettier and displays prettier errors as ESLint errors. Make sure this is always the last configuration in the extends array.\\n  ],\\n  parserOptions: {\\n    project: path.resolve(__dirname, \'./tsconfig.json\'),\\n    tsconfigRootDir: __dirname,\\n    ecmaVersion: 2018, // Allows for the parsing of modern ECMAScript features\\n    sourceType: \'module\', // Allows for the use of imports\\n    ecmaFeatures: {\\n      jsx: true, // Allows for the parsing of JSX\\n    },\\n  },\\n  rules: {\\n    // Place to specify ESLint rules. Can be used to overwrite rules specified from the extended configs\\n    // e.g. \\"@typescript-eslint/explicit-function-return-type\\": \\"off\\",\\n    \'@typescript-eslint/explicit-function-return-type\': \'off\',\\n    \'@typescript-eslint/no-unused-vars\': \'off\',\\n\\n    // These rules don\'t add much value, are better covered by TypeScript and good definition files\\n    \'react/no-direct-mutation-state\': \'off\',\\n    \'react/no-deprecated\': \'off\',\\n    \'react/no-string-refs\': \'off\',\\n    \'react/require-render-return\': \'off\',\\n\\n    \'react/jsx-filename-extension\': [\\n      \'warn\',\\n      {\\n        extensions: [\'.jsx\', \'.tsx\'],\\n      },\\n    ], // also want to use with \\".tsx\\"\\n    \'react/prop-types\': \'off\', // Is this incompatible with TS props type?\\n  },\\n  settings: {\\n    react: {\\n      version: \'detect\', // Tells eslint-plugin-react to automatically detect the version of React to use\\n    },\\n  },\\n};\\n```\\n\\nYou can add Prettier into the mix too. You can see how it is used in the above code sample. But given the impact that has on performance I wouldn\'t recommend it; hence it\'s commented out. [There\'s a good piece by Rob Cooper\'s for more details on setting up Prettier and VS Code with TypeScript and ESLint.](https://dev.to/robertcoopercode/using-eslint-and-prettier-in-a-typescript-project-53jb)\\n\\n## Performance and Power Tools\\n\\nIt\'s worth noting that support for TypeScript in ESLint is still brand new. As such, the rule of \\"Make it Work, Make it Right, Make it Fast\\" applies.... ESLint with TypeScript still has some performance issues which should be ironed out in the fullness of time. You can [track them here](https://github.com/typescript-eslint/typescript-eslint/issues/389).\\n\\nThis is important to bear in mind as, when I converted a large codebase over to using ESLint, I discovered that initial performance of linting was terribly slow. Something that\'s worth doing right now is identifying which rules are costing you most timewise and tweaking based on whether you think they\'re earning their keep.\\n\\nThe [`TIMING` environment variable](https://eslint.org/docs/developer-guide/working-with-rules#per-rule-performance) can be used to provide a report on the relative cost performance wise of running each rule. A nice way to plug this into your workflow is to add the `cross-env` package to your project: `yarn add cross-env -D` and then add 2 scripts to your `package.json`:\\n\\n```\\n\\"lint\\": \\"eslint ./\\",\\n\\"lint-rule-timings\\": \\"cross-env TIMING=1 yarn lint\\"\\n```\\n\\n- `lint` \\\\- just runs the linter standalone\\n- `lint-rule-timings` \\\\- does the same but with the `TIMING` environment variable set to 1 so a report will be generated.\\n\\nI\'d advise, making use of `lint-rule-timings` to identify which rules are costing you performance and then turning `off` rules as you need to. Remember, different rules have different value.\\n\\n[Finally, if you\'d like to see how it\'s done, here\'s an example of porting from TSLint to ESLint.](https://github.com/TypeStrong/ts-loader/pull/960)"},{"id":"typescript-webpack-you-down-with-pnp","metadata":{"permalink":"/typescript-webpack-you-down-with-pnp","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2019-06-07-typescript-webpack-you-down-with-pnp/index.md","source":"@site/blog/2019-06-07-typescript-webpack-you-down-with-pnp/index.md","title":"TypeScript / webpack - you down with PnP? Yarn, you know me!","description":"Yarn PnP speeds up module installation and eliminates node_modules. Converting to it is easy but some rough edges exist.","date":"2019-06-07T00:00:00.000Z","tags":[{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."},{"inline":false,"label":"Node.js","permalink":"/tags/node-js","description":"The Node.js runtime."}],"readingTime":5.52,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"typescript-webpack-you-down-with-pnp","title":"TypeScript / webpack - you down with PnP? Yarn, you know me!","authors":"johnnyreilly","tags":["webpack","typescript","node.js"],"hide_table_of_contents":false,"description":"Yarn PnP speeds up module installation and eliminates node_modules. Converting to it is easy but some rough edges exist."},"unlisted":false,"prevItem":{"title":"Using TypeScript and ESLint with webpack (fork-ts-checker-webpack-plugin new feature!)","permalink":"/typescript-and-eslint-meet-fork-ts-checker-webpack-plugin"},"nextItem":{"title":"TypeScript and high CPU usage - watch don\'t stare!","permalink":"/typescript-and-high-cpu-usage-watch"}},"content":"Yarn PnP is an innovation by the Yarn team designed to speed up module resolution by node. To quote the [(excellent) docs](https://yarnpkg.com/en/docs/pnp):\\n\\n\x3c!--truncate--\x3e\\n\\n> Plug\u2019n\u2019Play is an alternative installation strategy unveiled in September 2018...\\n>\\n> The way regular installs work is simple: Yarn generates a `node_modules` directory that Node is then able to consume. In this context, Node doesn\u2019t know the first thing about what a package is: it only reasons in terms of files. \u201CDoes this file exist here? No? Let\u2019s look in the parent `node_modules` then. Does it exist here? Still no? Too bad\u2026 parent folder it is!\u201D - and it does this until it matches something that matches one of the possibilities. That\u2019s vastly inefficient.\\n>\\n> When you think about it, Yarn knows everything about your dependency tree - it evens installs it! So why is Node tasked with locating your packages on the disk? Why don\u2019t we simply query Yarn, and let it tell us where to look for a package X required by a package Y? That\u2019s what Plug\u2019n\u2019Play (abbreviated PnP) is. Instead of generating a node_modules directory and leaving the resolution to Node, we now generate a single .pnp.js file and let Yarn tell us where to find our packages.\\n\\nYarn has been worked upon, amongst others, by the excellent [Ma\xebl Nison](https://twitter.com/arcanis). You can hear him talking about it in person [in this talk at JSConfEU](https://youtu.be/XePfzVs852s).\\n\\nThanks particularly to Ma\xebl\'s work, it\'s possible to use Yarn PnP with TypeScript using webpack with `ts-loader` _and_`fork-ts-checker-webpack-plugin`. This post intends to show you just how simple it is to convert a project that uses either to work with Yarn PnP.\\n\\n## Vanilla `ts-loader`\\n\\nYour project is built using standalone `ts-loader`; i.e. a simple setup that handles both transpilation and type checking.\\n\\nFirst things first, add this property to your `package.json`: (this is only required if you are using Yarn 1; this tag will be optional starting from the v2, where projects will switch to PnP by default.)\\n\\n```\\n{\\n    \\"installConfig\\": {\\n        \\"pnp\\": true\\n    }\\n}\\n```\\n\\nAlso, because this is webpack, we\'re going to need to add an extra dependency in the form of `pnp-webpack-plugin`:\\n\\n```\\nyarn add -D pnp-webpack-plugin\\n```\\n\\nTo quote the excellent docs, make the following amends to your `webpack.config.js`:\\n\\n```\\nconst PnpWebpackPlugin = require(`pnp-webpack-plugin`);\\n\\nmodule.exports = {\\n    module: {\\n        rules: [{\\n            test: /\\\\.ts$/,\\n            loader: require.resolve(\'ts-loader\'),\\n            options: PnpWebpackPlugin.tsLoaderOptions(),\\n        }],\\n    },\\n    resolve: {\\n        plugins: [ PnpWebpackPlugin, ],\\n    },\\n    resolveLoader: {\\n        plugins: [ PnpWebpackPlugin.moduleLoader(module), ],\\n    },\\n};\\n```\\n\\nIf you have any options you want to pass to `ts-loader`, just pass them as parameter of `pnp-webpack-plugin`\'s `tsLoaderOptions` function and it will take care of forwarding them properly. Behind the scenes the `tsLoaderOptions` function is providing `ts-loader` with the options necessary to switch into Yarn PnP mode.\\n\\nCongratulations; you now have `ts-loader` functioning with Yarn PnP support!\\n\\n## `fork-ts-checker-webpack-plugin` with `ts-loader`\\n\\nYou may well be using `fork-ts-checker-webpack-plugin` to handle type checking whilst `ts-loader` gets on with the transpilation. This workflow is also supported using `pnp-webpack-plugin`. You\'ll have needed to follow the same steps as the `ts-loader` setup. It\'s just the `webpack.config.js` tweaks that will be different.\\n\\n```\\nconst PnpWebpackPlugin = require(`pnp-webpack-plugin`);\\n\\nmodule.exports = {\\n    plugins: {\\n        new ForkTsCheckerWebpackPlugin(PnpWebpackPlugin.forkTsCheckerOptions({\\n            useTypescriptIncrementalApi: false, // not possible to use this until: https://github.com/microsoft/TypeScript/issues/31056\\n        })),\\n    }\\n    module: {\\n        rules: [{\\n            test: /\\\\.ts$/,\\n            loader: require.resolve(\'ts-loader\'),\\n            options: PnpWebpackPlugin.tsLoaderOptions({ transpileOnly: true }),\\n        }],\\n    },\\n    resolve: {\\n        plugins: [ PnpWebpackPlugin, ],\\n    },\\n    resolveLoader: {\\n        plugins: [ PnpWebpackPlugin.moduleLoader(module), ],\\n    },\\n};\\n```\\n\\nAgain if you have any options you want to pass to `ts-loader`, just pass them as parameter of `pnp-webpack-plugin`\'s `tsLoaderOptions` function. As we\'re using `fork-ts-checker-webpack-plugin` we\'re going to want to stop `ts-loader` doing type checking with the `transpileOnly: true` option.\\n\\nWe\'re now initialising `fork-ts-checker-webpack-plugin` with `pnp-webpack-plugin`\'s `forkTsCheckerOptions` function. Behind the scenes the `forkTsCheckerOptions` function is providing the `fork-ts-checker-webpack-plugin` with the options necessary to switch into Yarn PnP mode.\\n\\nAnd that\'s it! You now have `ts-loader` and `fork-ts-checker-webpack-plugin` functioning with Yarn PnP support!\\n\\n## Living on the Bleeding Edge\\n\\nWhilst you can happily develop and build using Yarn PnP, it\'s worth bearing in mind that this is a new approach. As such, there\'s some rough edges right now.\\n\\nIf you\'re interested in Yarn PnP, it\'s worth taking the v2 of Yarn (Berry) for a spin. You can find it here: [https://github.com/yarnpkg/berry](https://github.com/yarnpkg/berry). It\'s where most of the Yarn PnP work happens, and it includes zip loading - two birds, one stone!\\n\\nBecause there isn\'t first class support for Yarn PnP in TypeScript itself yet, you cannot make use of the Watch API through `fork-ts-checker-webpack-plugin`. (You can read about that issue [here](https://github.com/microsoft/TypeScript/issues/31056))\\n\\nAs you\'ve likely noticed, the webpack configuration required makes for a noisy `webpack.config.js`. Further to that, VS Code (which is powered by TypeScript remember) has no support for Yarn PnP yet and so will present resolution errors to you. If you can ignore the sea of red squigglies all over your source files in the editor and just look at your webpack build you\'ll be fine.\\n\\nThere is a tool called `PnPify` that adds support for PnP to TypeScript (in particular tsc). You can find more information here: [https://yarnpkg.github.io/berry/advanced/pnpify](https://yarnpkg.github.io/berry/advanced/pnpify). For tsc it would be:\\n\\n```\\n$> yarn pnpify tsc [...]\\n```\\n\\nThe gist is that it simulates the existence of `node_modules` by leveraging the data from the PnP file. As such it\'s not a perfect fix (`pnp-webpack-plugin` is a better integration), but it\'s a very useful tool to have to unblock yourself when using a project that doesn\'t support it.\\n\\nPnPify actually allows us to use TypeScript in VSCode with PnP! Its documentation is here: [https://yarnpkg.github.io/berry/advanced/pnpify#vscode-support](https://yarnpkg.github.io/berry/advanced/pnpify#vscode-support)\\n\\nAll of these hindrances should hopefully be resolved in future. Ideally, one day a good developer experience can be the default experience. In the meantime, you can still dev - just be prepared for the rough edges. Here\'s some useful resources to track the future of support:\\n\\n- You can follow more on built in webpack support here: [https://github.com/webpack/enhanced-resolve/issues/162](https://github.com/webpack/enhanced-resolve/issues/162)\\n- And on built in TypeScript support here: [https://github.com/Microsoft/TypeScript/issues/18896](https://github.com/Microsoft/TypeScript/issues/18896)\\n- Finally, there it\'s worth watching the [nodejs/module](https://github.com/nodejs/modules) repository, which debates amongst other things how to properly integrate loaders with Node.\\n\\nThis last one would be nice because:\\n\\n- We\'d stop having to patch require\\n- We probably wouldn\'t have to use yarn node if Node itself was able to find the loader somehow (such as if it was listed in the package.json metadata)\\n\\nThanks to Ma\xebl for his tireless work on Yarn. To my mind Ma\xebl is certainly a candidate for the hardest worker in open source. I\'ve been shamelessly borrowing his excellent docs for this post - thanks for writing so excellently Ma\xebl!"},{"id":"typescript-and-high-cpu-usage-watch","metadata":{"permalink":"/typescript-and-high-cpu-usage-watch","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2019-05-23-typescript-and-high-cpu-usage-watch/index.md","source":"@site/blog/2019-05-23-typescript-and-high-cpu-usage-watch/index.md","title":"TypeScript and high CPU usage - watch don\'t stare!","description":"High CPU usage in watch mode on idle due to TypeScripts fs.watchFile. fs.watch recommended instead. Env variable controls file watching.","date":"2019-05-23T00:00:00.000Z","tags":[{"inline":false,"label":"fork-ts-checker-webpack-plugin","permalink":"/tags/fork-ts-checker-webpack-plugin","description":"The fork-ts-checker-webpack-plugin for webpack."},{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":2.745,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"typescript-and-high-cpu-usage-watch","title":"TypeScript and high CPU usage - watch don\'t stare!","authors":"johnnyreilly","tags":["fork-ts-checker-webpack-plugin","webpack","typescript"],"hide_table_of_contents":false,"description":"High CPU usage in watch mode on idle due to TypeScripts fs.watchFile. fs.watch recommended instead. Env variable controls file watching."},"unlisted":false,"prevItem":{"title":"TypeScript / webpack - you down with PnP? Yarn, you know me!","permalink":"/typescript-webpack-you-down-with-pnp"},"nextItem":{"title":"react-select with less typing lag","permalink":"/react-select-with-less-typing-lag"}},"content":"I\'m one of the maintainers of the [fork-ts-checker-webpack-plugin](https://github.com/Realytics/fork-ts-checker-webpack-plugin). Hi there!\\n\\n\x3c!--truncate--\x3e\\n\\nRecently, various issues have been raised against create-react-app (which uses fork-ts-checker-webpack-plugin) as well as against the plugin itself. They\'ve been related to the level of CPU usage in watch mode on idle; i.e. it\'s high!\\n\\n- [https://github.com/Realytics/fork-ts-checker-webpack-plugin/issues/236](https://github.com/Realytics/fork-ts-checker-webpack-plugin/issues/236)\\n- [https://github.com/facebook/create-react-app/issues/6792](https://github.com/facebook/create-react-app/issues/6792)\\n\\n## Why High?\\n\\nNow, under the covers, the `fork-ts-checker-webpack-plugin` uses the TypeScript watch API.\\n\\nThe marvellous [John](https://github.com/NeKJ) (not me - another John) did some digging and discovered the root cause came down to the way that the TypeScript watch API watches files:\\n\\n> TS uses internally the `fs.watch` and `fs.watchFile` API functions of nodejs for their watch mode. The latter function [is even not recommended by nodejs documentation](https://nodejs.org/api/fs.html#fs_fs_watchfile_filename_options_listener) for performance reasons, and urges to use `fs.watch` instead.\\n>\\n> **NodeJS doc:**\\n>\\n> > Using fs.watch() is more efficient than fs.watchFile and fs.unwatchFile. fs.watch should be used instead of fs.watchFile and fs.unwatchFile when possible.\\n\\n## \\"there is another\\"\\n\\nJohn also found that there are other file watching behaviours offered by TypeScript. What\'s more, the file watching behaviour is _configurable with an environment variable_. That\'s right, if an environment variable called `TSC_WATCHFILE` is set, it controls the file watching approach used. Big news!\\n\\nJohn did some rough benchmarking of the performance of the different options that be set on his PC running linux 64 bit. Here\'s how it came out:\\n\\n```\\n| Value                                 | CPU usage on idle |\\n| ------------------------------------- | ----------------- |\\n| TS default _(TSC_WATCHFILE not set)_  | **7\\\\.4%**         |\\n| UseFsEventsWithFallbackDynamicPolling | 0\\\\.2%             |\\n| UseFsEventsOnParentDirectory          | 0\\\\.2%             |\\n| PriorityPollingInterval               | **6\\\\.2%**         |\\n| DynamicPriorityPolling                | 0\\\\.5%             |\\n| UseFsEvents                           | 0\\\\.2%             |\\n```\\n\\nAs you can see, the default performs poorly. On the other hand, an option like `UseFsEventsWithFallbackDynamicPolling` is comparative greasy lightning.\\n\\n## workaround!\\n\\nTo get this better experience into your world now, you could just set an environment variable on your machine. However, that doesn\'t scale; let\'s instead look at introducing the environment variable into your project explicitly.\\n\\nWe\'re going to do this in a cross platform way using [`cross-env`](https://github.com/kentcdodds/cross-env). This is a mighty useful utility by Kent C Dodds which allows you to set environment variables in a way that will work on Windows, Mac and Linux. Imagine it as the jQuery of the environment variables world :-)\\n\\nLet\'s add it as a `devDependency`:\\n\\n```\\nyarn add -D cross-env\\n```\\n\\nThen take a look at your `package.json`. You\'ve probably got a `start` script that looks something like this:\\n\\n```\\n\\"start\\": \\"webpack-dev-server --progress --color --mode development --config webpack.config.development.js\\",\\n```\\n\\nOr if you\'re a create-react-app user maybe this:\\n\\n```\\n\\"start\\": \\"react-scripts start\\",\\n```\\n\\nPrefix your `start` script with `cross-env TSC_WATCHFILE=UseFsEventsWithFallbackDynamicPolling`. This will, when run, initialise an environment variable called `TSC_WATCHFILE` with the value `UseFsEventsWithFallbackDynamicPolling`. Then it will start your development server as it did before. When TypeScript is fired up by webpack it will see this environment variable and use it to configure the file watching behaviour to one of the more performant options.\\n\\nSo, in the case of a `create-react-app` user, your finished `start` script would look like this:\\n\\n```\\n\\"start\\": \\"cross-env TSC_WATCHFILE=UseFsEventsWithFallbackDynamicPolling react-scripts start\\",\\n```\\n\\n## The Future\\n\\nThere\'s a possibility that the default watch behaviour may change in TypeScript in future. It\'s currently under discussion, you can read more [here](https://github.com/microsoft/TypeScript/issues/31048)."},{"id":"react-select-with-less-typing-lag","metadata":{"permalink":"/react-select-with-less-typing-lag","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2019-04-27-react-select-with-less-typing-lag/index.md","source":"@site/blog/2019-04-27-react-select-with-less-typing-lag/index.md","title":"react-select with less typing lag","description":"Fix lagging in `react-select`. Change `filterOption` to `ignoreAccents: false` for faster typing experience with 1000+ items.","date":"2019-04-27T00:00:00.000Z","tags":[{"inline":false,"label":"React","permalink":"/tags/react","description":"The React library."}],"readingTime":2.04,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"react-select-with-less-typing-lag","title":"react-select with less typing lag","authors":"johnnyreilly","tags":["react"],"hide_table_of_contents":false,"description":"Fix lagging in `react-select`. Change `filterOption` to `ignoreAccents: false` for faster typing experience with 1000+ items."},"unlisted":false,"prevItem":{"title":"TypeScript and high CPU usage - watch don\'t stare!","permalink":"/typescript-and-high-cpu-usage-watch"},"nextItem":{"title":"Template Tricks for a Dainty DOM","permalink":"/template-tricks-for-dainty-dom"}},"content":"This is going out to all those people using [`react-select`](https://react-select.com) with 1000+ items to render. To those people typing into the select and saying out loud \\"it\'s _so_ laggy.... This can\'t be... It\'s 2019... I mean, right?\\" To the people who read this [GitHub issue](https://github.com/JedWatson/react-select/issues/3128) top to bottom 30 times and still came back unsure of what to do. This is for you.\\n\\n\x3c!--truncate--\x3e\\n\\nI\'m lying. Mostly this goes out to me. I have a select box. I need it to render 2000+ items. I want it to be lovely. I want my users to be delighted as they use it. I want them to type in and (_this is the crucial part!_) for the control to feel responsive. Not laggy. Not like each keypress is going to Jupiter and back before it renders to the screen.\\n\\nAmongst the various gems on the GitHub issue are shared CodeSandboxes illustrating ways to integrate react-select with react-window. That\'s great and they do improve things. However, they don\'t do much to improve the laggy typing feel. There\'s [brief mention](https://github.com/JedWatson/react-select/issues/3128#issuecomment-431397942) of a props tweak you can make to react-select; this:\\n\\n```js\\nfilterOption={createFilter({ ignoreAccents: false })}\\n```\\n\\nWhat does this do? Well, this improves the typing lag experience _massively_. For why? Well, [if you look at the code](https://github.com/JedWatson/react-select/blob/292bad3298f2cafad6767f2134bd79a9c27e4073/src/filters.js#L21) you find that the default value is `ignoreAccents: true`. This default makes react-select invoke an expensive (and scary sounding) function called [`stripDiacritics`](https://github.com/JedWatson/react-select/blob/292bad3298f2cafad6767f2134bd79a9c27e4073/src/diacritics.js#L90). Not once but twice. Ouchy. And this kills performance.\\n\\nBut if you\'re okay with accents not being ignored (and _spoiler_: I am) then this is the option for you.\\n\\nHere\'s a CodeSandbox which also includes the `ignoreAccents: false` tweak. Enjoy!\\n\\n[![Edit johnnyreilly/react-window-with-react-select-less-laggy](play-codesandbox.svg)](https://codesandbox.io/s/zn70lqp31m?fontsize=14)\\n\\n```js\\nimport React, { Component } from \'react\';\\nimport ReactDOM from \'react-dom\';\\nimport Select, { createFilter } from \'react-select\';\\nimport { FixedSizeList as List } from \'react-window\';\\n\\nimport \'./styles.css\';\\n\\nconst options = [];\\nfor (let i = 0; i < 2500; i = i + 1) {\\n  options.push({ value: i, label: `Option ${i}` });\\n}\\n\\nconst height = 35;\\n\\nclass MenuList extends Component {\\n  render() {\\n    const { options, children, maxHeight, getValue } = this.props;\\n    const [value] = getValue();\\n    const initialOffset = options.indexOf(value) * height;\\n\\n    return (\\n      <List\\n        height={maxHeight}\\n        itemCount={children.length}\\n        itemSize={height}\\n        initialScrollOffset={initialOffset}\\n      >\\n        {({ index, style }) => <div style={style}>{children[index]}</div>}\\n      </List>\\n    );\\n  }\\n}\\n\\nconst App = () => (\\n  <Select\\n    filterOption={createFilter({ ignoreAccents: false })} // this makes all the difference!\\n    components={{ MenuList }}\\n    options={options}\\n  />\\n);\\n\\nReactDOM.render(<App />, document.getElementById(\'root\'));\\n```"},{"id":"template-tricks-for-dainty-dom","metadata":{"permalink":"/template-tricks-for-dainty-dom","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2019-03-24-template-tricks-for-dainty-dom/index.md","source":"@site/blog/2019-03-24-template-tricks-for-dainty-dom/index.md","title":"Template Tricks for a Dainty DOM","description":"Wrapping data in HTML templates can help with performance. This trick kept rendering server-side but only rendered content when necessary.","date":"2019-03-24T00:00:00.000Z","tags":[],"readingTime":5.27,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"template-tricks-for-dainty-dom","title":"Template Tricks for a Dainty DOM","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false,"description":"Wrapping data in HTML templates can help with performance. This trick kept rendering server-side but only rendered content when necessary."},"unlisted":false,"prevItem":{"title":"react-select with less typing lag","permalink":"/react-select-with-less-typing-lag"},"nextItem":{"title":"Google Analytics API and ASP.Net Core","permalink":"/google-analytics-api-and-aspnet-core"}},"content":"I\'m somewhat into code golf. Placing restrictions on what you\'re \\"allowed\\" to do in code and seeing what the happens as a result. I\'d like to share with you something that came out of some recent dabblings.\\n\\n\x3c!--truncate--\x3e\\n\\nTypically I spend a good amount of time playing with TypeScript. Either working on build tools or making web apps with it. (Usually with a portion of React on the side.) This is something different.\\n\\nI have a side project on the go which is essentially a mini analytics dashboard. For the purposes of this piece let\'s call it \\"StatsDash\\". When I was starting it I thought: let\'s try something different. Let\'s build StatsDash with HTML _only_. The actual HTML is hand cranked by me and generated in ASP.Net Core / C# using a combination of LINQ and string interpolation. (Who needs Razor? \uD83D\uDE0E) I\'ll say it\'s pretty fun - but the back end is not what I want to focus on.\\n\\nI got something up and running pretty quickly in pure HTML. The first lesson I learned was this: HTML alone is hella ugly. So I relaxed my criteria; I allowed CSS to come play as long as I didn\'t have to write any / much myself. There followed some experimentation with different CSS frameworks. For a while I rolled with Bootstrap (old school!), then Bulma and finally I settled on [Materialized](https://materializecss.com/). Materialized is a heavily inspired by Google\'s Material Design and is hence quite beautiful. With my HTML and Materialize\'s CSS we were rolling. Beautiful stats - no JS.\\n\\n## \\"Oh All Right; Just a Splash\\"\\n\\nLovely as things were, StatsDash quickly got to the point where there was too much information on the screen. It was time to make some changes. If data is to convey a message, it must first be comprehensible.\\n\\nI needed a way to hide and show data as people interacted with StatsDash. I wanted to achieve this _without_ starting to render on the client side and also without going back to the server each time.\\n\\nIf you want interactions in your UI all roads lead to JS. It\'s certainly possible to do some tricks with CSS but that\'s a round of code golf I\'m ill equipped to play. So, I took a look at what Materialized had to offer. Usefully it has a [Modal](https://materializecss.com/modals.html) component. With that in play I\'d be able to separate the detailed information into different modals which the users could show and hide as required. Perfect!\\n\\nIt required a little JS. What\'s a line or two between friends? Dear reader, I compromised once more.\\n\\n## The DOM Bunker\\n\\nWith my handy modals, StatsDash was now a one stop shop for a great deal of information. Info which took the form of DOM nodes. Lots of them. And by \\"lots of them\\" I want you to think along the lines of \\"space is big, really big...\\".\\n\\nThis was impacting users. Clicking to open a modal resulted in a noticeable lag. It would take 2+ seconds for the browser to respond. Users found themselves clicking multiple times; wondering why nothing seemed to occur. In the end the modal would shuffle into view. However, this wasn\'t the best experience. The lack of responsiveness was getting in the way of users enjoying all StatsDash had to offer.\\n\\nRunning an audit of StatsDash in Chrome DevTools there was no doubt we had a DOM problem:\\n\\n![](DOM-massive.webp)\\n\\nWhat to do? I still didn\'t want to go back to the server on each click in StatsDash. And I didn\'t want to start writing rendering code on the client as well either. I have in the past mixed client and server side rendering and I know well that it\'s a first class ticket to a confusing codebase.\\n\\n## Smuggling DOM in Templates\\n\\nThere\'s a mechanism that supports this use case directly: the `&lt;template&gt;` element. [To quote MDN](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/template):\\n\\n> The HTML Content Template (`&lt;template&gt;`) element is a mechanism for holding client-side content that is not to be rendered when a page is loaded but may subsequently be instantiated during runtime using JavaScript.\\n\\n> Think of a template as a content fragment that is being stored for subsequent use in the document.\\n\\nThis is _exactly_ what I\'m after. I can keep my rendering server side, but instead wrap content that isn\'t immediately visible to users inside a `&lt;template&gt;` element and render that only when users need it.\\n\\nSo in the case of my modals (where most of my DOM lives), I can tuck the contents of each modal into a `&lt;template&gt;` element. Then, when the user clicks to open a modal we move that template content into the DOM so they can see it. Likewise, as they close a modal we can clear out the modal\'s DOM content to ease the load on the dear old browser.\\n\\n## \\"That Sounds Complicated...\\"\\n\\nIt\'s not. Let me show you how easily this is accomplished. First of all, wrap all your modal contents into `&lt;template&gt;` elements. They should look a little something like this:\\n\\n```html\\n<div>\\n  <button data-target=\\"modalId\\" class=\\"btn modal-trigger\\">\\n    Open the Modal!\\n  </button>\\n\\n  <template>\\n    \x3c!--\\n        loads of DOM nodes\\n        --\x3e\\n  </template>\\n\\n  <div id=\\"modalId\\" class=\\"modal modal-fixed-footer\\"></div>\\n</div>\\n```\\n\\nNext, where you initialise your modals you need to make a little tweak:\\n\\n```js\\ndocument.addEventListener(\'DOMContentLoaded\', function () {\\n  M.Modal.init(document.querySelectorAll(\'.modal\'), {\\n    onOpenStart: (modalDiv) => {\\n      const template = modalDiv.parentNode.querySelector(\'template\');\\n\\n      modalDiv.appendChild(document.importNode(template.content, true));\\n    },\\n    onCloseEnd: (modalDiv) => {\\n      while (modalDiv.firstChild) {\\n        modalDiv.removeChild(modalDiv.firstChild);\\n      }\\n    },\\n  });\\n});\\n```\\n\\nThat\'s it! As you can see, before we open our modals, the `onOpenStart` callback will fire which creates the actual DOM elements based upon the `template`. And when the modals finish closing the `onCloseEnd` callback runs to remove those DOM elements once more.\\n\\nFor this minimal change, the client gets a dramatically different user experience. StatsDash went from super laggy to satisfyingly fast. Using `template`s, The number of initial DOM nodes dropped from more than _20,000_ to _200_. That\'s right \uD83D\uDCAF times smaller!\\n\\n## Do It Yourself\\n\\nThe code examples above rely upon the Materialize modals. However the principles used here are broadly applicable. It\'s easy for you to take the approach outlined here and apply it in a different situation.\\n\\nIf you\'re interested in some of the other exciting things you can do with templates then I recommend [Eric Bidelman\'s post on the topic](https://www.html5rocks.com/en/tutorials/webcomponents/template/)."},{"id":"google-analytics-api-and-aspnet-core","metadata":{"permalink":"/google-analytics-api-and-aspnet-core","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2019-03-22-google-analytics-api-and-aspnet-core/index.md","source":"@site/blog/2019-03-22-google-analytics-api-and-aspnet-core/index.md","title":"Google Analytics API and ASP.Net Core","description":"Accessing Google Analytics API from ASP.Net Core can be tough due to lack of examples. This article provides an example code to get page access stats.","date":"2019-03-22T00:00:00.000Z","tags":[{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":1.89,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"google-analytics-api-and-aspnet-core","title":"Google Analytics API and ASP.Net Core","authors":"johnnyreilly","tags":["asp.net"],"hide_table_of_contents":false,"description":"Accessing Google Analytics API from ASP.Net Core can be tough due to lack of examples. This article provides an example code to get page access stats."},"unlisted":false,"prevItem":{"title":"Template Tricks for a Dainty DOM","permalink":"/template-tricks-for-dainty-dom"},"nextItem":{"title":"fork-ts-checker-webpack-plugin v1.0","permalink":"/fork-ts-checker-webpack-plugin-v1"}},"content":"I recently had need to be able to access the API for Google Analytics from ASP.Net Core. Getting this up and running turned out to be surprisingly tough because of an absence of good examples. So here it is; an example of how you can access a simple page access stat using [the API](https://www.nuget.org/packages/Google.Apis.AnalyticsReporting.v4/):\\n\\n\x3c!--truncate--\x3e\\n\\n```cs\\nasync Task<SomeKindOfDataStructure[]> GetUsageFromGoogleAnalytics(DateTime startAtThisDate, DateTime endAtThisDate)\\n{\\n    // Create the DateRange object. Here we want data from last week.\\n    var dateRange = new DateRange\\n    {\\n        StartDate = startAtThisDate.ToString(\\"yyyy-MM-dd\\"),\\n        EndDate = endAtThisDate.ToString(\\"yyyy-MM-dd\\")\\n    };\\n    // Create the Metrics and dimensions object.\\n    // var metrics = new List<Metric> { new Metric { Expression = \\"ga:sessions\\", Alias = \\"Sessions\\" } };\\n    // var dimensions = new List<Dimension> { new Dimension { Name = \\"ga:pageTitle\\" } };\\n    var metrics = new List<Metric> { new Metric { Expression = \\"ga:uniquePageviews\\" } };\\n    var dimensions = new List<Dimension> {\\n        new Dimension { Name = \\"ga:date\\" },\\n        new Dimension { Name = \\"ga:dimension1\\" }\\n    };\\n\\n    // Get required View Id from configuration\\n    var viewId = $\\"ga:{\\"[VIEWID]\\"}\\";\\n\\n    // Create the Request object.\\n    var reportRequest = new ReportRequest\\n    {\\n        DateRanges = new List<DateRange> { dateRange },\\n        Metrics = metrics,\\n        Dimensions = dimensions,\\n        FiltersExpression = \\"ga:pagePath==/index.html\\",\\n        ViewId = viewId\\n    };\\n\\n    var getReportsRequest = new GetReportsRequest {\\n        ReportRequests = new List<ReportRequest> { reportRequest }\\n    };\\n\\n    //Invoke Google Analytics API call and get report\\n    var analyticsService = GetAnalyticsReportingServiceInstance();\\n    var response = await (analyticsService.Reports.BatchGet(getReportsRequest)).ExecuteAsync();\\n\\n    var logins = response.Reports[0].Data.Rows.Select(row => new SomeKindOfDataStructure {\\n        Date = new DateTime(\\n            year: Convert.ToInt32(row.Dimensions[0].Substring(0, 4)),\\n            month: Convert.ToInt32(row.Dimensions[0].Substring(4, 2)),\\n            day: Convert.ToInt32(row.Dimensions[0].Substring(6, 2))),\\n        NumberOfLogins = Convert.ToInt32(row.Metrics[0].Values[0])\\n    })\\n    .OrderByDescending(login => login.Date)\\n    .ToArray();\\n\\n    return logins;\\n}\\n\\n/// <summary>\\n/// Intializes and returns Analytics Reporting Service Instance\\n/// </summary>\\nAnalyticsReportingService GetAnalyticsReportingServiceInstance() {\\n    var googleAuthFlow = new GoogleAuthorizationCodeFlow(new GoogleAuthorizationCodeFlow.Initializer {\\n        ClientSecrets = new ClientSecrets {\\n            ClientId = \\"[CLIENTID]\\",\\n            ClientSecret = \\"[CLIENTSECRET]\\"\\n        }\\n    });\\n\\n    var responseToken = new TokenResponse {\\n        AccessToken = \\"[ANALYTICSTOKEN]\\",\\n        RefreshToken = \\"[REFRESHTOKEN]\\",\\n        Scope = AnalyticsReportingService.Scope.AnalyticsReadonly, //Read-only access to Google Analytics,\\n        TokenType = \\"Bearer\\",\\n    };\\n\\n    var credential = new UserCredential(googleAuthFlow, \\"\\", responseToken);\\n\\n    // Create the  Analytics service.\\n    return new AnalyticsReportingService(new BaseClientService.Initializer {\\n        HttpClientInitializer = credential,\\n        ApplicationName = \\"my-super-applicatio\\",\\n    });\\n}\\n```\\n\\nYou can see above that you need various credentials to be able to use the API. You can acquire these by logging into GA. Enjoy!"},{"id":"fork-ts-checker-webpack-plugin-v1","metadata":{"permalink":"/fork-ts-checker-webpack-plugin-v1","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2019-03-06-fork-ts-checker-webpack-plugin-v1/index.md","source":"@site/blog/2019-03-06-fork-ts-checker-webpack-plugin-v1/index.md","title":"fork-ts-checker-webpack-plugin v1.0","description":"`fork-ts-checker-webpack-plugin` released v1.0.0 with TypeScript 3+ support, incremental watch API by default, and compatibility with webpack and TSLint.","date":"2019-03-06T00:00:00.000Z","tags":[{"inline":false,"label":"fork-ts-checker-webpack-plugin","permalink":"/tags/fork-ts-checker-webpack-plugin","description":"The fork-ts-checker-webpack-plugin for webpack."},{"inline":false,"label":"ts-loader","permalink":"/tags/ts-loader","description":"The TypeScript loader for webpack."},{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":1.895,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"fork-ts-checker-webpack-plugin-v1","title":"fork-ts-checker-webpack-plugin v1.0","authors":"johnnyreilly","tags":["fork-ts-checker-webpack-plugin","ts-loader","webpack","typescript"],"hide_table_of_contents":false,"description":"`fork-ts-checker-webpack-plugin` released v1.0.0 with TypeScript 3+ support, incremental watch API by default, and compatibility with webpack and TSLint."},"unlisted":false,"prevItem":{"title":"Google Analytics API and ASP.Net Core","permalink":"/google-analytics-api-and-aspnet-core"},"nextItem":{"title":"ASP.NET Core: Proxying HTTP Requests with an AllowList","permalink":"/aspnet-core-allowlist-proxying-http-requests"}},"content":"[It\'s time for the first major version of `fork-ts-checker-webpack-plugin`](https://github.com/Realytics/fork-ts-checker-webpack-plugin/releases/tag/v1.0.0). It\'s been a long time coming :-)\\n\\n\x3c!--truncate--\x3e\\n\\n## A Little History\\n\\nThe `fork-ts-checker-webpack-plugin` was originally the handiwork of [Piotr Ole\u015B](https://github.com/piotr-oles). He raised an issue with [`ts-loader`](https://github.com/TypeStrong/ts-loader/issues/537) suggesting it could be the McCartney to `ts-loader`\'s Lennon:\\n\\n> Hi everyone!\\n>\\n> I\'ve created webpack plugin: [fork-ts-checker-webpack-plugin](https://github.com/Realytics/fork-ts-checker-webpack-plugin) that plays nicely with `ts-loader`. The idea is to compile project with `transpileOnly: true` and check types on separate process (async). With this approach, webpack build is not blocked by type checker and we have semantic check with fast incremental build. More info on github repo :)\\n>\\n> So if you like it and you think it would be good to add some info in README/index.md about this plugin, I would be greatful.\\n>\\n> Thanks :)\\n\\nWe did like it. We did think it would be good. We took him up on his kind offer.\\n\\nSince that time many people have had their paws on the `fork-ts-checker-webpack-plugin` codebase. We love them all.\\n\\n## One Point Oh\\n\\nWe could have had our first major release a long time ago. The idea first occurred when webpack 5 alpha appeared. \\"Huh, look at that, a major version number.... Maybe we should do that?\\" \\"_Great_ idea chap - do it!\\" So here it is; fresh out the box: v1.0.0\\n\\nThere are actually no breaking changes that we\'re aware of; users of 0.x `fork-ts-checker-webpack-plugin` should be be able to upgrade without any drama.\\n\\n## Incremental Watch API on by Default\\n\\nUsers of TypeScript 3+ may notice a performance improvement as by default the plugin now uses the [incremental watch API](https://github.com/Microsoft/TypeScript/pull/20234) in TypeScript.\\n\\nShould this prove problematic you can opt out of using it by supplying `useTypescriptIncrementalApi: false`. We are aware of an [issue with Vue and the incremental API](https://github.com/Realytics/fork-ts-checker-webpack-plugin/issues/219). We hope it will be fixed soon - a generous member of the community is taking a look. In the meantime, we will _not_ default to using the incremental watch API when in Vue mode.\\n\\n## Compatibility\\n\\nAs it stands, the plugin supports webpack 2, 3, 4 and 5 alpha. It is compatible with TypeScript 2.1+ and TSLint 4+.\\n\\nRight that\'s it - enjoy it! And thanks everyone for contributing - we really dig your help. Much love."},{"id":"aspnet-core-allowlist-proxying-http-requests","metadata":{"permalink":"/aspnet-core-allowlist-proxying-http-requests","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2019-02-22-aspnet-core-allowlist-proxying-http-requests/index.md","source":"@site/blog/2019-02-22-aspnet-core-allowlist-proxying-http-requests/index.md","title":"ASP.NET Core: Proxying HTTP Requests with an AllowList","description":"ASP.NET Core can proxy HTTP requests selectively, allowing only acceptable traffic via a middleware for specified paths.","date":"2019-02-22T00:00:00.000Z","tags":[{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":6.49,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"aspnet-core-allowlist-proxying-http-requests","title":"ASP.NET Core: Proxying HTTP Requests with an AllowList","authors":"johnnyreilly","tags":["asp.net"],"image":"./hang-on-lads-ive-got-a-great-idea.webp","hide_table_of_contents":false,"description":"ASP.NET Core can proxy HTTP requests selectively, allowing only acceptable traffic via a middleware for specified paths."},"unlisted":false,"prevItem":{"title":"fork-ts-checker-webpack-plugin v1.0","permalink":"/fork-ts-checker-webpack-plugin-v1"},"nextItem":{"title":"TypeScript and webpack: Watch It","permalink":"/typescript-and-webpack-watch-it"}},"content":"This post demonstrates a mechanism for proxying HTTP requests in ASP.NET Core. It doesn\'t proxy all requests; it only proxies requests that match entries on an \\"allowlist\\" - so we only proxy the traffic that we\'ve actively decided is acceptable as determined by taking the form of an expected URL and HTTP verb (GET / POST etc).\\n\\n\x3c!--truncate--\x3e\\n\\n## Why do we need to proxy?\\n\\nOnce upon a time there lived a young team who were building a product. They were ready to go live with their beta and so they set off on a journey to a mystical land they had heard tales of. This magical kingdom was called \\"Production\\". However, Production was a land with walls and but one gate. That gate was jealously guarded by a defender named \\"InfoSec\\". InfoSec was there to make sure that only the the right people, noble of thought and pure of deed were allowed into the promised land. InfoSec would ask questions like \\"are you serving over HTTPS\\" and \\"what are you doing about cross site scripting\\"?\\n\\nThe team felt they had good answers to InfoSec\'s questions. However, just as they were about to step through the gate, InfoSec held up their hand and said \\"your application wants to access a database... database access needs to take place on our own internal network. Not over the publicly accessible internet.\\"\\n\\nThe team, with one foot in the air, paused. They swallowed and said \\"can you give us five minutes?\\"\\n\\n![image taken from the end of the classic movie \\"The Italian Job\\" of the bus hanging half off a mountainside](hang-on-lads-ive-got-a-great-idea.webp)\\n\\n## The Proxy Regroup\\n\\nAnd so it came to pass that the teams product (which took the form of ASP.Net Core web application) had to be changed. Where once there had been a single application, there would now be two; one that lived on the internet (the _web_ app) and one that lived on the companies private network (the _API_ app). The API app would do all the database access. In fact the product team opted to move all significant operations into the API as well. This left the web app with two purposes:\\n\\n1. the straightforward serving of HTML, CSS, JS and images\\n2. the proxying of API calls through to the API app\\n\\n## Proxy Part 1\\n\\nIn the early days of this proxying the team reached for [`AspNetCore.Proxy`](https://github.com/twitchax/AspNetCore.Proxy). It\'s a great open source project that allows you to proxy HTTP requests. It gives you complete control over the construction of proxy requests, so that you can have a request come into your API and end up proxying it to a URL with a completely different path on the proxy server.\\n\\n## Proxy Part 2\\n\\nThe approach offered by `AspNetCore.Proxy` is fantastically powerful in terms of control. However, we didn\'t actually need that level of configurability. In fact, it resulted in us writing a great deal of boilerplate code. You see in our case we\'d opted to proxy path for path, changing only the server name on each proxied request. So if a GET request came in going to https://web.app.com/api/version then we would want to proxy it to a GET request to https://api.app.com/api/version. You see? All we did was swap https://web.app.com for https://api.app.com. Nothing more. We did that as a rule. We knew we _always_ wanted to do just this.\\n\\nSo we ended up spinning up our own solution which allowed just the specification of paths we wanted to proxy with their corresponding HTTP verbs. Let\'s talk through it. Usage of our approach ended up as a middleware within our web app\'s `Startup.cs`:\\n\\n```cs\\npublic void Configure(IApplicationBuilder app) {\\n    // ...\\n\\n    app.UseProxyAllowList(\\n        // where ServerToProxyToBaseUrl is the server you want requests to be proxied to\\n        // eg \\"https://the-server-we-proxy-to\\"\\n        proxyAddressTweaker: (requestPath) => $\\"{ServerToProxyToBaseUrl}{requestPath}\\",\\n        allowListProxyRoutes: new [] {\\n            // An anonymous request\\n            AllowListProxy.AnonymousRoute(\\"api/version\\", HttpMethod.Get),\\n\\n            // An authenticated request; to send this we must know who the user is\\n            AllowListProxy.Route(\\"api/account/{accountId:int}/all-the-secret-info\\", HttpMethod.Get, HttpMethod.Post),\\n    });\\n\\n\\n    app.UseMvc();\\n\\n    // ...\\n}\\n```\\n\\nIf you look at the code above you can see that we are proxing requests to a single server: `ServerToProxyToBaseUrl`. We\'re also only proxying requests which match an entry on our allowlist (as represented by `allowListProxyRoutes`). So in this case we\'re proxying two different requests:\\n\\n1. `GET` requests to `api/version` are proxied through as _anonymous_`GET` requests.\\n2. `GET` and `POST` requests to `api/account/{accountId:int}/all-the-secret-info` are proxied through as `GET` and `POST` requests. These requests require that a user be authenticated first.\\n\\nThe `AllowListProxy` proxy class we\'ve been using looks like this:\\n\\n```cs\\nusing System;\\nusing System.Collections.Generic;\\nusing System.Net.Http;\\n\\nnamespace My.Web.Proxy {\\n    public class AllowListProxy {\\n        public string Path { get; set; }\\n        public IEnumerable<HttpMethod> Methods { get; set; }\\n        public bool IsAnonymous { get; set; }\\n\\n        private AllowListProxy(string path, bool isAnonymous, params HttpMethod[] methods) {\\n            if (methods == null || methods.Length == 0)\\n                throw new ArgumentException($\\"You need at least a single HttpMethod to be specified for {path}\\");\\n\\n            Path = path;\\n            IsAnonymous = isAnonymous;\\n            Methods = methods;\\n        }\\n\\n        public static AllowListProxy Route(string path, params HttpMethod[] methods) =>\\n            new AllowListProxy(path, isAnonymous: false, methods: methods);\\n\\n        public static AllowListProxy AnonymousRoute(string path, params HttpMethod[] methods) =>\\n            new AllowListProxy(path, isAnonymous: true, methods: methods);\\n    }\\n}\\n```\\n\\nThe middleware for proxying (our `UseProxyAllowList`) looks like this:\\n\\n```cs\\nusing System;\\nusing System.Collections.Generic;\\nusing System.ComponentModel;\\nusing System.Linq;\\nusing System.Net.Http;\\nusing System.Reflection;\\nusing System.Threading.Tasks;\\nusing Microsoft.AspNetCore.Authentication;\\nusing Microsoft.AspNetCore.Builder;\\nusing Microsoft.AspNetCore.Http;\\nusing Microsoft.AspNetCore.Routing;\\nusing Microsoft.Extensions.DependencyModel;\\nusing Microsoft.Extensions.DependencyInjection;\\nusing Serilog;\\n\\nnamespace My.Web.Proxy {\\n    public static class ProxyRouteExtensions {\\n        /// <summary>\\n        /// Middleware which proxies the supplied allowlist routes\\n        /// </summary>\\n        public static void UseProxyAllowList(\\n            this IApplicationBuilder app,\\n            Func<string, string> proxyAddressTweaker,\\n            Action<HttpContext, HttpRequestMessage> preSendProxyRequestAction,\\n            IEnumerable<AllowListProxy> allowListProxyRoutes\\n        ) {\\n            app.UseRouter(builder => {\\n                foreach (var allowListProxy in allowListProxyRoutes) {\\n                    foreach (var method in allowListProxy.Methods) {\\n                        builder.MapMiddlewareVerb(method.ToString(), allowListProxy.Path, proxyApp => {\\n                            proxyApp.UseProxy_Challenge(allowListProxy.IsAnonymous);\\n                            proxyApp.UseProxy_Run(proxyAddressTweaker, preSendProxyRequestAction);\\n                        });\\n                    }\\n                }\\n            });\\n        }\\n\\n        private static void UseProxy_Challenge(this IApplicationBuilder app, bool allowAnonymous) {\\n            app.Use((context, next) =>\\n            {\\n                var routePath = context.Request.Path.Value;\\n\\n                var weAreAuthenticatedOrWeDontNeedToBe =\\n                    context.User.Identity.IsAuthenticated || allowAnonymous;\\n                if (weAreAuthenticatedOrWeDontNeedToBe)\\n                    return next();\\n\\n                return context.ChallengeAsync();\\n            });\\n        }\\n\\n        private static void UseProxy_Run(\\n            this IApplicationBuilder app,\\n            Func<string, string> proxyAddressTweaker,\\n            Action<HttpContext, HttpRequestMessage> preSendProxyRequestAction\\n            )\\n        {\\n            app.Run(async context => {\\n                var proxyAddress = \\"\\";\\n                try {\\n                    proxyAddress = proxyAddressTweaker(context.Request.Path.Value);\\n\\n                    var proxyRequest = context.Request.CreateProxyHttpRequest(proxyAddress);\\n\\n                    if (preSendProxyRequestAction != null)\\n                        preSendProxyRequestAction(context, proxyRequest);\\n\\n                    var httpClients = context.RequestServices.GetService<IHttpClients>(); // IHttpClients is just a wrapper for HttpClient - insert your own here\\n\\n                    var proxyResponse = await httpClients.SendRequestAsync(proxyRequest,\\n                            HttpCompletionOption.ResponseHeadersRead, context.RequestAborted)\\n                        .ConfigureAwait(false);\\n\\n                    await context.CopyProxyHttpResponse(proxyResponse).ConfigureAwait(false);\\n                }\\n                catch (OperationCanceledException ex) {\\n                    if (ex.CancellationToken.IsCancellationRequested)\\n                        return;\\n\\n                    if (!context.Response.HasStarted)\\n                    {\\n                        context.Response.StatusCode = 408;\\n                        await context.Response\\n                            .WriteAsync(\\"Request timed out.\\");\\n                    }\\n                }\\n                catch (Exception e) {\\n                    if (!context.Response.HasStarted)\\n                    {\\n                        context.Response.StatusCode = 500;\\n                        await context.Response\\n                            .WriteAsync(\\n                                $\\"Request could not be proxied.\\\\n\\\\n{e.Message}\\\\n\\\\n{e.StackTrace}.\\");\\n                    }\\n                }\\n            });\\n        }\\n\\n        public static void AddOrReplaceHeader(this HttpRequestMessage request, string headerName, string headerValue) {\\n            // It\'s possible for there to be multiple headers with the same name; we only want a single header to remain.  Our one.\\n            while (request.Headers.TryGetValues(headerName, out var existingAuthorizationHeader)) {\\n                request.Headers.Remove(headerName);\\n            }\\n            request.Headers.TryAddWithoutValidation(headerName, headerValue);\\n        }\\n\\n        public static HttpRequestMessage CreateProxyHttpRequest(this HttpRequest request, string uriString) {\\n            var uri = new Uri(uriString + request.QueryString);\\n\\n            var requestMessage = new HttpRequestMessage();\\n            var requestMethod = request.Method;\\n            if (!HttpMethods.IsGet(requestMethod) &&\\n                !HttpMethods.IsHead(requestMethod) &&\\n                !HttpMethods.IsDelete(requestMethod) &&\\n                !HttpMethods.IsTrace(requestMethod)) {\\n                var streamContent = new StreamContent(request.Body);\\n                requestMessage.Content = streamContent;\\n            }\\n\\n            // Copy the request headers.\\n            if (requestMessage.Content != null)\\n                foreach (var header in request.Headers)\\n                    if (!requestMessage.Headers.TryAddWithoutValidation(header.Key, header.Value.ToArray()))\\n                        requestMessage.Content?.Headers.TryAddWithoutValidation(header.Key, header.Value.ToArray());\\n\\n            requestMessage.Headers.Host = uri.Authority;\\n            requestMessage.RequestUri = uri;\\n            requestMessage.Method = new HttpMethod(request.Method);\\n\\n            return requestMessage;\\n        }\\n\\n        public static async Task CopyProxyHttpResponse(this HttpContext context, HttpResponseMessage responseMessage) {\\n            var response = context.Response;\\n\\n            response.StatusCode = (int) responseMessage.StatusCode;\\n            foreach (var header in responseMessage.Headers) {\\n                response.Headers[header.Key] = header.Value.ToArray();\\n            }\\n\\n            if (responseMessage.Content != null) {\\n                foreach (var header in responseMessage.Content.Headers) {\\n                    response.Headers[header.Key] = header.Value.ToArray();\\n                }\\n            }\\n\\n            response.Headers.Remove(\\"transfer-encoding\\");\\n\\n            using(var responseStream = await responseMessage.Content.ReadAsStreamAsync().ConfigureAwait(false)) {\\n                await responseStream.CopyToAsync(response.Body, 81920, context.RequestAborted).ConfigureAwait(false);\\n            }\\n        }\\n    }\\n}\\n```\\n\\nThis works out to be a flexible and simple approach to allowlist proxying."},{"id":"typescript-and-webpack-watch-it","metadata":{"permalink":"/typescript-and-webpack-watch-it","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2019-01-13-typescript-and-webpack-watch-it/index.md","source":"@site/blog/2019-01-13-typescript-and-webpack-watch-it/index.md","title":"TypeScript and webpack: Watch It","description":"TypeScripts \\"watch\\" API shortens time between incremental builds for quicker development; updates are available for fork-ts-checker-webpack-plugin.","date":"2019-01-13T00:00:00.000Z","tags":[{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":2.375,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"typescript-and-webpack-watch-it","title":"TypeScript and webpack: Watch It","authors":"johnnyreilly","tags":["webpack","typescript"],"hide_table_of_contents":false,"description":"TypeScripts \\"watch\\" API shortens time between incremental builds for quicker development; updates are available for fork-ts-checker-webpack-plugin."},"unlisted":false,"prevItem":{"title":"ASP.NET Core: Proxying HTTP Requests with an AllowList","permalink":"/aspnet-core-allowlist-proxying-http-requests"},"nextItem":{"title":"GitHub Actions and Yarn","permalink":"/github-actions-and-yarn"}},"content":"All I ask for is a compiler and a tight feedback loop. Narrowing the gap between making a change to a program and seeing the effect of that is a productivity boon. The TypeScript team are wise cats and dig this. They\'ve taken strides to improve the developer experience of TypeScript users by [introducing a \\"watch\\" API which can be leveraged by other tools](https://github.com/Microsoft/TypeScript/wiki/Using-the-Compiler-API#writing-an-incremental-program-watcher). To quote the docs:\\n\\n\x3c!--truncate--\x3e\\n\\n> TypeScript 2.7 introduces two new APIs: one for creating \\"watcher\\" programs that provide set of APIs to trigger rebuilds, and a \\"builder\\" API that watchers can take advantage of... This can speed up large projects with many files.\\n\\nRecently the wonderful [0xorial](https://github.com/0xorial) [opened a PR to add support for the watch API](https://github.com/Realytics/fork-ts-checker-webpack-plugin/pull/198) to the [`fork-ts-checker-webpack-plugin`](https://github.com/Realytics/fork-ts-checker-webpack-plugin).\\n\\nI took this PR for a spin on a large project that I work on. With my machine, I was averaging 12 seconds between incremental builds. (I will charitably describe the machine in question as \\"challenged\\"; hobbled by one of the most aggressive virus checkers known to mankind. Fist bump InfoSec \uD83E\uDD1C\uD83E\uDD1B\uD83D\uDE09) Switching to using the watch API dropped this to a mere 1.5 seconds!\\n\\n## You Can Watch Too\\n\\n0xorial\'s PR was merged toot suite and was been released as [`fork-ts-checker-webpack-plugin@1.0.0-alpha.2`](https://github.com/Realytics/fork-ts-checker-webpack-plugin/releases/tag/v1.0.0-alpha.2). If you\'d like to take this for a spin then you can. Just:\\n\\n1. Up your version of the plugin to `fork-ts-checker-webpack-plugin@next` in your `package.json`\\n2. Add `useTypescriptIncrementalApi: true` to the plugin when you initialise it in your `webpack.config.js`.\\n\\nThat\'s it.\\n\\n## Mary Poppins\\n\\nSorry, I was trying to paint a word picture of something you might watch that was also comforting. Didn\'t quite work...\\n\\nAnyway, you might be thinking \\"wait, just hold on a minute.... he said `@next` \\\\- I am _not_ that bleeding edge.\\" Well, it\'s not like that. Don\'t be scared.\\n\\n`fork-ts-checker-webpack-plugin` has merely been updated for webpack 5 (which is in alpha) and the `@next` reflects that. To be clear, the `@next` version of the plugin still supports (remarkably!) webpack 2, 3 and 4 as well as 5 alpha. Users of current and historic versions of webpack should feel safe using the `@next` version; for webpack 2, 3 and 4 expect stability. webpack 5 users should expect potential changes to align with webpack 5 as it progresses.\\n\\n## Roadmap\\n\\nThis is available now and we\'d love for you to try it out. As you can see, at the moment it\'s opt-in. You have to explicitly choose to use the new behaviour. Depending upon how testing goes, we may look to make this the default behaviour for the plugin in future (assuming users are running a high enough version of TypeScript). It would be great to hear from people if they have any views on that, or feedback in general.\\n\\nMuch \u2764\uFE0F y\'all. And many thanks to the very excellent [0xorial](https://github.com/0xorial) for the hard work."},{"id":"github-actions-and-yarn","metadata":{"permalink":"/github-actions-and-yarn","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2019-01-05-github-actions-and-yarn/index.md","source":"@site/blog/2019-01-05-github-actions-and-yarn/index.md","title":"GitHub Actions and Yarn","description":"Automate npm publishing using GitHub Actions; use `npm` GitHub Action with yarn or any Docker container with Node/npm installed.","date":"2019-01-05T00:00:00.000Z","tags":[{"inline":false,"label":"GitHub Actions","permalink":"/tags/github-actions","description":"The GitHub Actions CI / CD service."},{"inline":false,"label":"Node.js","permalink":"/tags/node-js","description":"The Node.js runtime."}],"readingTime":4,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"github-actions-and-yarn","title":"GitHub Actions and Yarn","authors":"johnnyreilly","tags":["github actions","node.js"],"hide_table_of_contents":false,"description":"Automate npm publishing using GitHub Actions; use `npm` GitHub Action with yarn or any Docker container with Node/npm installed."},"unlisted":false,"prevItem":{"title":"TypeScript and webpack: Watch It","permalink":"/typescript-and-webpack-watch-it"},"nextItem":{"title":"You Might Not Need thread-loader","permalink":"/you-might-not-need-thread-loader"}},"content":"I\'d been meaning to automate the npm publishing of [`ts-loader`](https://github.com/TypeStrong/ts-loader) for the longest time. I had attempted to use Travis to do this in the same way as [`fork-ts-checker-webpack-plugin`](https://github.com/Realytics/fork-ts-checker-webpack-plugin). Alas using secure environment variables in Travis has unfortunate implications for ts-loader\'s test pack.\\n\\n\x3c!--truncate--\x3e\\n\\nBe not afeard. I\'ve heard there\'s a new shiny thing from GitHub that I could use instead... It\'s a sign; I must use it!\\n\\nGitHub Actions are still in beta. Technically Actions are [code run in Docker containers](https://developer.github.com/actions/creating-github-actions/) in response to events. This didn\'t mean a great deal to me until I started thinking about what I wanted to do with `ts-loader`\'s publishing flow.\\n\\n## Automate What?\\n\\nEach time I publish a release of `ts-loader` I execute the following node commands by hand:\\n\\n1. `yarn install` \\\\- to install `ts-loader`\'s dependencies\\n2. `yarn build` \\\\- to build `ts-loader`\\n3. `yarn test` \\\\- to run `ts-loader`\'s test packs\\n4. `npm publish` \\\\- to publish the release of `ts-loader` to npm\\n\\nHaving read up on GitHub Actions it seemed like they were born to handle this sort of task.\\n\\n## GitHub Action for `npm`\\n\\nI quickly discovered that someone out there <s>loves me</s>\\n\\nhad [already written a GitHub Action for `npm`](https://github.com/actions/npm).\\n\\nThe example in the `README/index.md` could be easily tweaked to meet my needs with one caveat: I had to use `npm` in place of `yarn`. I didn\'t want to switch from `yarn`. What to do?\\n\\nWell, remember when I said actions are code run in Docker containers? Another way to phrase that is to say: GitHub Actions are Docker images. Let\'s look under the covers of the `npm` GitHub Action. As we peer inside the [`Dockerfile`](https://github.com/actions/npm/blob/e7aaefed7c9f2e83d493ff810f17fa5ccd7ed437/Dockerfile#L1) what do we find?\\n\\n```\\nFROM node:10-slim\\n```\\n\\nHmmmm.... Interesting. The base image of the `npm` GitHub Action is `node:10-slim`. Looking it up, it seems the `-slim` Docker images come with [`yarn` included](https://github.com/nodejs/docker-node/blob/master/Dockerfile-slim.template). Which means we should be able to use `yarn` inside the `npm` GitHub Action. Nice!\\n\\n## GitHub Action for `npm` for `yarn`\\n\\nUsing `yarn` from the GitHub Action for `npm` is delightfully simple. Here\'s what running `npm install` looks like:\\n\\n```\\n# install with npm\\naction \\"install\\" {\\n  uses = \\"actions/npm@1.0.0\\"\\n  args = \\"install\\"\\n}\\n```\\n\\nPivoting to use `yarn install` instead of `npm install` is as simple as:\\n\\n```\\n# install with yarn\\naction \\"install\\" {\\n  uses = \\"actions/npm@1.0.0\\"\\n  runs = \\"yarn\\"\\n  args = \\"install\\"\\n}\\n```\\n\\nYou can see we\'ve introduced the `runs = \\"yarn\\"` and after that the `args` are whatever you need them to be.\\n\\n## Going With The Workflow\\n\\nA GitHub Workflow that implements the steps I need would look like this:\\n\\n```\\nworkflow \\"build, test and publish on release\\" {\\n  on = \\"push\\"\\n  resolves = \\"publish\\"\\n}\\n\\n# install with yarn\\naction \\"install\\" {\\n  uses = \\"actions/npm@1.0.0\\"\\n  runs = \\"yarn\\"\\n  args = \\"install\\"\\n}\\n\\n# build with yarn\\naction \\"build\\" {\\n  needs = \\"install\\"\\n  uses = \\"actions/npm@1.0.0\\"\\n  runs = \\"yarn\\"\\n  args = \\"build\\"\\n}\\n\\n# test with yarn\\naction \\"test\\" {\\n  needs = \\"build\\"\\n  uses = \\"actions/npm@1.0.0\\"\\n  runs = \\"yarn\\"\\n  args = \\"test\\"\\n}\\n\\n# filter for a new tag\\naction \\"check for new tag\\" {\\n  needs = \\"Test\\"\\n  uses = \\"actions/bin/filter@master\\"\\n  args = \\"tag\\"\\n}\\n\\n# publish with npm\\naction \\"publish\\" {\\n  needs = \\"check for new tag\\"\\n  uses = \\"actions/npm@1.0.0\\"\\n  args = \\"publish\\"\\n  secrets = [\\"NPM_AUTH_TOKEN\\"]\\n}\\n```\\n\\nAs you can see, this is a direct automation of steps 1-4 I listed earlier. Since all these actions are executed in the same container, we can skip from `yarn` to `npm` with gay abandon.\\n\\nWhat\'s absolutely amazing is, when I got access to GitHub Actions [my hand crafted workflow](https://github.com/TypeStrong/ts-loader/blob/master/.github/main.workflow) looked like it should work first time! I know, right? Don\'t you love it when that happens? [Alas there\'s presently a problem with filters in GitHub Actions](https://github.com/actions/bin/issues/13). But that\'s by the by, if you\'re just looking to use a GitHub Action with yarn instead of npm then you are home free.\\n\\n## You Don\'t Actually Need the npm GitHub Action\\n\\nYou heard me right. Docker containers be Docker containers. You don\'t actually need to use this:\\n\\n```\\nuses = \\"actions/npm@1.0.0\\"\\n```\\n\\nYou can use _any_ Docker container which has node / npm installed! So if you\'d like to use say node 11 instead you could just do this:\\n\\n```\\nuses = \\"docker://node:11\\"\\n```\\n\\nWhich would use the node 11 image on [docker hub](https://hub.docker.com/_/node).\\n\\nWhich is pretty cool. You know what\'s even more incredible? Inside a workflow you can switch `uses` mid-workflow and keep the output. That\'s right; you can have a work flow with say three actions running `uses = \\"docker://node:11\\"` and then a fourth running `uses = \\"actions/npm@1.0.0\\"`. That\'s _so_ flexible and powerful!\\n\\nThanks to [Matt Colyer](https://github.com/mcolyer) and [Landon Schropp](https://github.com/LandonSchropp) for [schooling me on the intricicies of GitHub Actions](https://github.com/actions/npm/issues/9). Much \u2764"},{"id":"you-might-not-need-thread-loader","metadata":{"permalink":"/you-might-not-need-thread-loader","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2018-12-22-you-might-not-need-thread-loader/index.md","source":"@site/blog/2018-12-22-you-might-not-need-thread-loader/index.md","title":"You Might Not Need thread-loader","description":"Jan Nicklas, the creator of webpack-config-plugins, suggests limiting the use of thread-loader for costly operations via `poolTimeout: Infinity`.","date":"2018-12-22T00:00:00.000Z","tags":[{"inline":false,"label":"fork-ts-checker-webpack-plugin","permalink":"/tags/fork-ts-checker-webpack-plugin","description":"The fork-ts-checker-webpack-plugin for webpack."},{"inline":false,"label":"ts-loader","permalink":"/tags/ts-loader","description":"The TypeScript loader for webpack."},{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."}],"readingTime":3.68,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"you-might-not-need-thread-loader","title":"You Might Not Need thread-loader","authors":"johnnyreilly","tags":["fork-ts-checker-webpack-plugin","ts-loader","webpack"],"hide_table_of_contents":false,"description":"Jan Nicklas, the creator of webpack-config-plugins, suggests limiting the use of thread-loader for costly operations via `poolTimeout: Infinity`."},"unlisted":false,"prevItem":{"title":"GitHub Actions and Yarn","permalink":"/github-actions-and-yarn"},"nextItem":{"title":"IMemoryCache and GetOrCreateForTimeSpanAsync","permalink":"/dotnet-imemorycache-getorcreatefortimespanasync"}},"content":"It all started with a GitHub issue. [Ernst Ammann reported](https://github.com/namics/webpack-config-plugins/issues/24):\\n\\n\x3c!--truncate--\x3e\\n\\n> Without the thread-loader, compilation takes three to four times less time on changes. We could remove it.\\n\\nIf you\'re not aware of the [`webpack-config-plugins`](https://github.com/namics/webpack-config-plugins) project then I commend it to you. Famously, webpack configuration can prove tricky. `webpack-config-plugins` borrows the idea of presets from Babel. It provides a number of pluggable webpack configurations which give a best practice setup for different webpack use cases. So if you\'re no expert with webpack and you want a good setup for building your TypeScript / Sass / JavaScript then `webpack-config-plugins` has got your back.\\n\\nOne of the people behind the project is the very excellent [Jan Nicklas](https://github.com/jantimon) who is well known for his work on the [`html-webpack-plugin`](https://github.com/jantimon/html-webpack-plugin).\\n\\nIt was Jan who responded to Ernst\'s issue and decided to look into it.\\n\\n## All I Want For Christmas is Faster Builds\\n\\nEveryone wants fast builds. I do. You do. We all do. `webpack-config-plugins` is about giving these to the user in a precooked package.\\n\\nThere\'s a webpack loader called [`thread-loader`](https://github.com/webpack-contrib/thread-loader) which spawns multiple processes and splits up work between them. It was originally inspired by the work in the happypack project which does a similar thing.\\n\\nI wrote [a blog post](https://medium.com/p/83cc568dea79) some time ago which gave details about ways to speed up your TypeScript builds by combining the [`ts-loader`](https://github.com/TypeStrong/ts-loader) project (which I manage) with the [`fork-ts-checker-webpack-plugin`](https://github.com/Realytics/fork-ts-checker-webpack-plugin) project (which I\'m heavily involved with).\\n\\nThat post was written back in the days of webpack 2 / 3. It advocated use of both `happypack` / `thread-loader` to drop your build times even further. As you\'ll see, now that we\'re well into the world of webpack 4 (with webpack 5 waiting in the wings) the advantage of `happypack` / `thread-loader` are no longer so profound.\\n\\n`webpack-config-plugins` follows the advice I set out in my post; it uses `thread-loader` in its pluggable configurations. Now, back to Ernst\'s issue.\\n\\n## `thread-loader`: Infinity War\\n\\nJan quickly identified the problem. He did that rarest of things; he read the documentation which said:\\n\\n```js\\n// timeout for killing the worker processes when idle\\n      // defaults to 500 (ms)\\n      // can be set to Infinity for watching builds to keep workers alive\\n      poolTimeout: 2000,\\n```\\n\\nThe `webpack-config-plugins` configurations (running in watch mode) were subject to the thread loaders being killed after 500ms. They got resurrected when they were next needed; but that\'s not as instant as you might hope. Jan then did a test:\\n\\n```sh\\n(default pool - 30 runs - 1000 components ) average: 2.668068965517241\\n(no thread-loader - 30 runs - 1000 components ) average: 1.2674137931034484\\n(Infinity pool - 30 runs - 1000 components ) average: 1.371827586206896\\n```\\n\\nThis demonstrates that using `thread-loader` in watch mode with `poolTimeout: Infinity` performs significantly better than when it defaults to 500ms. But perhaps more significantly, not using `thread-loader` performs even better still.\\n\\n## \\"Maybe You\'ve Thread Enough\\"\\n\\nWhen I tested using `thread-loader` in watch mode with `poolTimeout: Infinity` on my own builds I got the same benefit Jan had. I also got _even_ more benefit from dropping `thread-loader` entirely.\\n\\nA likely reason for this benefit is that typically when you\'re developing, you\'re working on one file at a time. Hence you only transpile one file at a time:\\n\\n![](ts-profile2.webp)\\n\\nSo there\'s not a great deal of value that `thread-loader` can add here; mostly it\'s twiddling thumbs and adding an overhead. [To quote the docs:](https://github.com/webpack-contrib/thread-loader/blob/master/README/index.md#usage)\\n\\n> Each worker is a separate node.js process, which has an overhead of \\\\~600ms. There is also an overhead of inter-process communication.\\n>\\n> Use this loader only for expensive operations!\\n\\nNow, my build is not your build. I can\'t guarantee that you\'ll get the same results as Jan and I experienced; but I would encourage you to investigate if you\'re using `thread-loader` correctly and whether it\'s actually helping you. In these days of webpack 4+ perhaps it isn\'t.\\n\\nThere are still scenarios where `thread-loader` still provides an advantage. It can speed up production builds. It can speed up the initial startup of watch mode. [In fact Jan has subsequently actually improved the `thread-loader` to that specific end.](https://github.com/webpack-contrib/thread-loader/pull/52) Yay Jan!\\n\\nIf this is all too much for you, and you want to hand off the concern to someone else then perhaps all of this serves as a motivation to just sit back, put your feet up and start using [`webpack-config-plugins`](https://github.com/namics/webpack-config-plugins) instead of doing your own configuration."},{"id":"dotnet-imemorycache-getorcreatefortimespanasync","metadata":{"permalink":"/dotnet-imemorycache-getorcreatefortimespanasync","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2018-12-10-dotnet-imemorycache-getorcreatefortimespanasync/index.md","source":"@site/blog/2018-12-10-dotnet-imemorycache-getorcreatefortimespanasync/index.md","title":"IMemoryCache and GetOrCreateForTimeSpanAsync","description":"IMemoryCache is a tremendous caching mechanism for .NET. This post demonstrates how to write a helper to allow you to get or create an item for a given TimeSpan.","date":"2018-12-10T00:00:00.000Z","tags":[{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":1.565,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"dotnet-imemorycache-getorcreatefortimespanasync","title":"IMemoryCache and GetOrCreateForTimeSpanAsync","authors":"johnnyreilly","tags":["asp.net"],"description":"IMemoryCache is a tremendous caching mechanism for .NET. This post demonstrates how to write a helper to allow you to get or create an item for a given TimeSpan.","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"You Might Not Need thread-loader","permalink":"/you-might-not-need-thread-loader"},"nextItem":{"title":"Snapshot Testing for C#","permalink":"/snapshot-testing-for-c"}},"content":"One thing that ASP.Net Core really got right was caching. [`IMemoryCache`](https://docs.microsoft.com/en-us/aspnet/core/performance/caching/memory) is a caching implementation that does just what I want.\\n\\n\x3c!--truncate--\x3e\\n\\n## TimeSpan, TimeSpan Expiration Y\'all\\n\\nTo make usage of the `IMemoryCache` _even_ more lovely I\'ve written an extension method. I follow pretty much one cache strategy: `SetAbsoluteExpiration` and I just vary the expiration by an amount of time. This extension method implements that in a simple way; I call it `GetOrCreateForTimeSpanAsync` - catchy right? It looks like this:\\n\\n```cs\\nusing System;\\nusing System.Threading.Tasks;\\nusing Microsoft.Extensions.Caching.Memory;\\n\\nnamespace My.Helpers {\\n\\n    public static class CacheHelpers {\\n\\n        public static async Task<TItem?> GetOrCreateForTimeSpanAsync<TItem>(\\n            this IMemoryCache cache,\\n            string key,\\n            Func<Task<TItem?>> itemGetterAsync,\\n            TimeSpan timeToCache\\n        ) {\\n            if (!cache.TryGetValue(key, out object? result))\\n            {\\n                result = await itemGetterAsync();\\n                if (result == null)\\n                    return default(TItem);\\n\\n                var cacheEntryOptions = new MemoryCacheEntryOptions()\\n                    .SetAbsoluteExpiration(timeToCache);\\n\\n                cache.Set(key, result, cacheEntryOptions);\\n            }\\n\\n            return (TItem)result;\\n        }\\n    }\\n}\\n```\\n\\nUsage looks like this:\\n\\n```cs\\nprivate Task<SuperInterestingThing> GetSuperInterestingThingFromCache(Guid superInterestingThingId) =>\\n    _cache.GetOrCreateForTimeSpanAsync(\\n        key: $\\"{nameof(MyClass)}:GetSuperInterestingThing:{superInterestingThingId}\\",\\n        itemGetterAsync: () => GetSuperInterestingThing(superInterestingThingId),\\n        timeToCache: TimeSpan.FromMinutes(5)\\n    );\\n```\\n\\nWhere `_cache` is an instance of `IMemoryCache` that can be dependency injected into your class. This helper allows the consumer to provide three things:\\n\\n- The `key` key for the item to be cached with\\n- A `itemGetterAsync` which is the method that is used to retrieve a new value if an item cannot be found in the cache\\n- A `timeToCache` which is the period of time that an item should be cached\\n\\nIf an item can\'t be looked up by the `itemGetterAsync` then _nothing_ will be cached and a the `default` value of the expected type will be returned. This is important because lookups can fail, and there\'s nothing worse than a lookup failing and you caching `null` as a result.\\n\\nGo on, ask me how I know.\\n\\nThis is a simple, clear and helpful API which makes interacting with `IMemoryCache` even more lovely than it was."},{"id":"snapshot-testing-for-c","metadata":{"permalink":"/snapshot-testing-for-c","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2018-11-17-snapshot-testing-for-c/index.md","source":"@site/blog/2018-11-17-snapshot-testing-for-c/index.md","title":"Snapshot Testing for C#","description":"Snapshot testing is an efficient test technique for comparing outputs with JSON. Its applicable to C# too, using Fluent Assertions and a helper tool.","date":"2018-11-17T00:00:00.000Z","tags":[{"inline":false,"label":"C#","permalink":"/tags/csharp","description":"The C# programming language."},{"inline":false,"label":"Automated Testing","permalink":"/tags/automated-testing","description":"How to perform the automation of tests."}],"readingTime":5.69,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"snapshot-testing-for-c","title":"Snapshot Testing for C#","authors":"johnnyreilly","tags":["c#","automated testing"],"hide_table_of_contents":false,"description":"Snapshot testing is an efficient test technique for comparing outputs with JSON. Its applicable to C# too, using Fluent Assertions and a helper tool."},"unlisted":false,"prevItem":{"title":"IMemoryCache and GetOrCreateForTimeSpanAsync","permalink":"/dotnet-imemorycache-getorcreatefortimespanasync"},"nextItem":{"title":"Making a Programmer","permalink":"/making-a-programmer"}},"content":"If you\'re a user of Jest, you\'ve no doubt heard of and perhaps made use of [snapshot testing](https://jestjs.io/docs/en/snapshot-testing).\\n\\n\x3c!--truncate--\x3e\\n\\nSnapshot testing is an awesome tool that is generally discussed in the context of JavaScript React UI testing. But snapshot testing has a wider application than that. Essentially it is profoundly useful where you have functions which produce a complex structured output. It could be a React UI, it could be a list of FX prices. The type of data is immaterial; it\'s the amount of it that\'s key.\\n\\nTypically there\'s a direct correlation between the size and complexity of the output of a method and the length of the tests that will be written for it. Let\'s say you\'re outputting a class that contains 20 properties. Congratulations! You get to write 20 assertions in one form or another for each test case. Or a single assertion whereby you supply the expected output by hand specifying each of the 20 properties. Either way, that\'s not going to be fun. And just imagine the time it would take to update multiple test cases if you wanted to change the behaviour of the method in question. Ouchy.\\n\\nTime is money kid. What you need is snapshot testing. Say goodbye to handcrafted assertions and hello to JSON serialised output checked into source control. Let\'s unpack that a little bit. The usefulness of snapshot testing that I want in C# is predominantly about removing the need to write and maintain multiple assertions. Instead you write tests that compare the output of a call to your method with JSON serialised output you\'ve generated on a previous occasion.\\n\\nThis approach takes less time to write, less time to maintain and the solid readability of JSON makes it more likely you\'ll pick up on bugs. It\'s so much easier to scan JSON than it is a list of assertions.\\n\\n## Putting the Snapshot into C#\\n\\nNow if you\'re writing tests in JavaScript or TypeScript then Jest already has your back with CLI snapshot generation and `shouldMatchSnapshot`. However getting to nearly the same place in C# is delightfully easy. What are we going to need?\\n\\nFirst up, a serializer which can take your big bad data structures and render them as JSON. Also we\'ll use it to rehydrate our data structure into an object ready for comparison. We\'re going to use [Json.NET](https://www.newtonsoft.com/json).\\n\\nNext up we need a way to compare our outputs with our rehydrated snapshots - we need a C# `shouldMatchSnapshot`. There\'s many choices out there, but for my money [Fluent Assertions](https://fluentassertions.com) is king of the hill.\\n\\nFinally we\'re going to need Snapshot, a little helper utility I put together:\\n\\n```cs\\nusing System;\\nusing System.IO;\\nusing Newtonsoft.Json;\\nusing Newtonsoft.Json.Serialization;\\n\\nnamespace Test.Utilities {\\n    public static class Snapshot {\\n        private static readonly JsonSerializer StubSerializer = new JsonSerializer {\\n            ContractResolver = new CamelCasePropertyNamesContractResolver(),\\n            NullValueHandling = NullValueHandling.Ignore\\n        };\\n\\n        private static JsonTextWriter MakeJsonTextWriter(TextWriter sw) => new JsonTextWriter(sw) {\\n            Formatting = Formatting.Indented,\\n            IndentChar = \' \',\\n            Indentation = 2\\n        };\\n\\n        /// <summary>\\n        /// Make yourself some JSON! Usage looks like this:\\n        /// Stubs.Make($\\"{System.AppDomain.CurrentDomain.BaseDirectory}..\\\\\\\\..\\\\\\\\..\\\\\\\\data.json\\", myData);\\n        /// </summary>\\n        public static void Make<T>(string stubPath, T data) {\\n            try {\\n                if (string.IsNullOrEmpty(stubPath))\\n                    throw new ArgumentNullException(nameof(stubPath));\\n                if (data == null)\\n                    throw new ArgumentNullException(nameof(data));\\n\\n                using(var sw = new StreamWriter(stubPath))\\n                using(var writer = MakeJsonTextWriter(sw)) {\\n                    StubSerializer.Serialize(writer, data);\\n                }\\n            } catch (Exception exc) {\\n                throw new Exception($\\"Failed to make {stubPath}\\", exc);\\n            }\\n        }\\n\\n        public static string Serialize<T>(T data) {\\n            using (var sw = new StringWriter())\\n            using(var writer = MakeJsonTextWriter(sw)) {\\n                StubSerializer.Serialize(writer, data);\\n                return sw.ToString();\\n            }\\n        }\\n\\n        public static string Load(string filename) {\\n            var content = new StreamReader(\\n                File.OpenRead(filename)\\n            ).ReadToEnd();\\n\\n            return content;\\n        }\\n    }\\n}\\n```\\n\\nLet\'s look at the methods: `Make` and `Load`. Make is what we\'re going to use to create our snapshots. Load is what we\'re going to use to, uh, load our snapshots.\\n\\nWhat does usage look like? Great question. Let\'s go through the process of writing a C# snapshot test.\\n\\n## Taking Snapshot for a Spin\\n\\nFirst of all, we\'re going to need a method to test that outputs a data structure which is more than just a scalar value. Let\'s use this:\\n\\n```cs\\npublic class Leopard {\\n    public string Name { get; set; }\\n    public int Spots { get; set; }\\n}\\n\\npublic class LeopardService {\\n    public Leopard[] GetTheLeopards() {\\n        return new Leopard[] {\\n            new Leopard { Spots = 42, Name = \\"Nimoy\\" },\\n            new Leopard { Spots = 900, Name = \\"Dotty\\" }\\n        };\\n    }\\n}\\n```\\n\\nYes - our trusty `LeopardService`. As you can see, the `GetTheLeopards` method returns an array of `Leopard`s. For now, let\'s write a test using `Snapshot`: (ours is an XUnit test; but `Snapshot` is agnostic of this)\\n\\n```cs\\n[Fact]\\npublic void GetTheLeopards_should_return_expected_Leopards() {\\n    // Arrange\\n    var leopardService = new LeopardService();\\n\\n    // Act\\n    var leopards = leopardService.GetTheLeopards();\\n\\n    // UNCOMMENT THE LINE BELOW *ONLY* WHEN YOU WANT TO GENERATE THE SNAPSHOT\\n    Snapshot.Make($\\"{System.AppDomain.CurrentDomain.BaseDirectory}..\\\\\\\\..\\\\\\\\..\\\\\\\\Snapshots\\\\\\\\leopardsSnapshot.json\\", leopards);\\n\\n    // Assert\\n    var snapshotLeopards = JsonConvert.DeserializeObject<leopard[]>(Snapshot.Load(\\"Snapshots/leopardsSnapshot.json\\"));\\n    snapshotLeopards.Should().BeEquivalentTo(leopards);\\n}\\n</leopard[]>\\n```\\n\\nBefore we run this for the first time we need to setup our testing project to be ready for snapshots. First of all we add a `Snapshot` folder to the test project. The we also add the following to the `.csproj`:\\n\\n```xml\\n<ItemGroup>\\n    <Content Include=\\"Snapshots\\\\**\\">\\n      <CopyToOutputDirectory>Always</CopyToOutputDirectory>\\n    </Content>\\n  </ItemGroup>\\n```\\n\\nThis includes the snapshots in the compile output for when tests are being run.\\n\\nNow let\'s run the test. It will generate a `leopardsSnapshot.json` file:\\n\\n```json\\n[\\n  {\\n    \\"name\\": \\"Nimoy\\",\\n    \\"spots\\": 42\\n  },\\n  {\\n    \\"name\\": \\"Dotty\\",\\n    \\"spots\\": 900\\n  }\\n]\\n```\\n\\nWith our snapshot in place, we comment out the `Snapshot.Make...` line and we have a passing test. Let\'s commit our code, push and go about our business.\\n\\n## Time Passes...\\n\\nSomeone decides that the implementation of `GetTheLeopards` needs to change. Defying expectations it seems that Dotty the leopard should now have 90 spots. I know... Business requirements, right?\\n\\nIf we make that change we\'d ideally expect our trusty test to fail. Let\'s see what happens:\\n\\n```\\n----- Test Execution Summary -----\\n\\nLeopard.Tests.Services.LeopardServiceTests.GetTheLeopards_should_return_expected_Leopards:\\n    Outcome: Failed\\n    Error Message:\\n    Expected item[1].Spots to be 90, but found 900.\\n```\\n\\nBoom! We are protected!\\n\\nSince this is a change we\'re completely happy with we want to update our `leopardsSnapshot.json` file. We could make our test pass by manually updating the JSON. That\'d be fine. But why work when you don\'t have to? Let\'s uncomment our `Snapshot.Make...` line and run the test the once.\\n\\n```json\\n[\\n  {\\n    \\"name\\": \\"Nimoy\\",\\n    \\"spots\\": 42\\n  },\\n  {\\n    \\"name\\": \\"Dotty\\",\\n    \\"spots\\": 90\\n  }\\n]\\n```\\n\\nThat\'s right, we have an updated snapshot! Minimal effort.\\n\\n## Next Steps\\n\\nThis is a basic approach to getting the goodness of snapshot testing in C#. It could be refined further. To my mind the uncommenting / commenting of code is not the most elegant way to approach this and so there\'s some work that could be done around this area.\\n\\nHappy snapshotting!"},{"id":"making-a-programmer","metadata":{"permalink":"/making-a-programmer","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2018-10-27-making-a-programmer/index.md","source":"@site/blog/2018-10-27-making-a-programmer/index.md","title":"Making a Programmer","description":"Learn programming in a relaxed atmosphere with a coding bootcamp that values repetition, feedback and team facilitation.","date":"2018-10-27T00:00:00.000Z","tags":[],"readingTime":6.02,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"making-a-programmer","title":"Making a Programmer","authors":"johnnyreilly","hide_table_of_contents":false,"description":"Learn programming in a relaxed atmosphere with a coding bootcamp that values repetition, feedback and team facilitation."},"unlisted":false,"prevItem":{"title":"Snapshot Testing for C#","permalink":"/snapshot-testing-for-c"},"nextItem":{"title":"Brand New Fonting Awesomeness","permalink":"/font-awesome-brand-icons-react"}},"content":"I recently had the good fortune to help run a coding bootcamp. The idea was simple: there are many people around us who are interested in programming but don\'t know where to start. Let\'s take some folk who do and share the knowledge.\\n\\n\x3c!--truncate--\x3e\\n\\nThe bootcamp went tremendously! (Well, I say that... Frankly I had a blast. \uD83D\uDE00 )\\n\\nCoding padawans walked in at the start with laptops and questions, and six weeks later they left with the groundwork of development experience. We ran a session for an hour during lunchtime once a week. Between that, people would have the opportunity to learn online, do exercises and reach out to the facilitators and their fellow apprentices for help.\\n\\nWe\'d never done this before. We were student teachers; learning how to teach as we ran the course. So what did we do? Are you curious? Read on, Macduff!\\n\\n## Code Review\\n\\nIt\'s worth saying now that we started our course with a plan: the plan was that we would be ready to change the plan. Or to put it another way, we were ready to pivot as we went.\\n\\nWe (by which I mean myself and the other course organisers) are interested in feedback. Sitting back and saying \\"Hey! We did this thing.... What do you think about it?\\" Because sometimes your plans are great. Do more of that hotness! But also, not all your ideas pan out... Maybe bail on those guys. Finally, never forget: other folk have brain tickling notions too.... We\'re with Picasso on this: good artists copy; great artists steal.\\n\\nWe\'re heavily invested in feedback in both what we build and how we build it. So we were totally going to apply this to doing something we\'d never done before. So seized were we of this that we made feedback part of the session. For the last five minutes each week we\'d run a short retrospective. We\'d stick up happy, sad and \\"meh\\" emojis to the wall, hand out post-its and everyone got to stick up their thoughts.\\n\\n![](not-so-sure-about-this-feedback.webp)\\n\\nFrom that we learned what was working, what wasn\'t and when we were very lucky there were suggestions too. We listened to all the feedback and the next week\'s session would be informed by what we\'d just learned.\\n\\n## Merging to Master\\n\\nSo, what did we end up with? What did our coding bootcamp look like?\\n\\nWell, to start each session we kicked off with an icebreaker. We very much wanted the sessions to be interactive experiences; we wanted them to feel playful and human. So an icebreaker was a good way to get things off on the right foot.\\n\\nThe IBs were connected with the subject at hand. For example: Human FizzBuzz. We took the classic interview question and applied it to wannabe coders. We explained the rules, and went round in a circle, each person was the next iteration of the loop. As each dev-in-training opened their mouths they had to say a number or \\"Fizz\\" or \\"Buzz\\" or \\"FizzBuzz\\". (It turns out this is harder than you think; and makes for a surprisingly entertaining parlour game. I intend to do this at my next dinner party.)\\n\\nAfter that we covered the rules of the game. (Yup, learning is a game and it\'s a good \'un.) Certainly the most important rule was this: <u>there are <strong>_no_</strong> stupid questions</u>\\n\\n. If people think there are, then they might be hesitant to ask. And any question benched is a learning opportunity lost. We don\'t want that.\\n\\n\\"Ask any question!\\" we said each week. Kudos to the people who have the courage to pipe up. We salute you! You\'re likely putting voice to a common area of misunderstanding.\\n\\nThen we\'d move onto the main content. The initial plan was to make use of the excellent [EdX Python course](https://www.edx.org/learn/python) Between each session our learners would do a module and then we\'d come together and talk around that topic somewhat. Whilst this was a good initial plan it did make the learning experience somewhat passive and less interactive than we\'d hoped.\\n\\nOne week we tried something different. It turns out that the amazing [JMac](https://twitter.com/foldr) has quite the skill for writing programming exercises. Small coding challenges that people can tackle independently. JMac put together a [repl.it](https://repl.it/) of exercises and encouraged the class to get stuck in. They did. So much so that at the end of the session it was hard to get everyone\'s attention to let them know the session was over. They were in the zone. When we did finally disrupt their flow, the feedback was pretty unanimous: we\'d hit paydirt.\\n\\n![](we-dug-this-feedback.webp)\\n\\nConsequently, that was the format going onwards. JMac would come up with a number of exercises for the class. Wisely they were constructed so that they gently levelled up in terms of complexity as you went on. You\'d get the dopamine hit of satisfaction as you did the earliest challenges that would give you the confidence to tackle the more complex later problems. If peeps got stuck they could ask someone to advise them, a facilitator or a peer. Or they could google it.... Like any other dev.\\n\\nHaving the chance to talk with others when you\'re stuck is fantastic. You can talk through a problem. The act of doing that is a useful exercise. When you talk through a problem out loud you can unlock your understanding and often get to the point where you can tackle this yourself. This is [rubber duck debugging](https://en.wikipedia.org/wiki/Rubber_duck_debugging). Any dev does this in their everyday; it makes complete sense to have it as part of a coding bootcamp.\\n\\nWe learned that it was useful, very useful, to have repitition in the exercises. Repitition. Repitition. Repitition. As the exercises started each week they would typically begin by recapping and repeating the content covered the previous week. The best way to learn is to practice. It\'s not for nothing the Karate Kid had to \\"wax on, wax off\\".\\n\\nFinally, we did this together. The course wasn\'t run by one person; we had a gang! We had three facilitators who helped to run the sessions; JMac, Jonesy and myself. We also had the amazing [Janice](https://twitter.com/janicewarden) who handled the general organisation and logistics. And made us laugh. A lot. This was obviously great from a camaraderie and sharing the load perspective. It turns out that having that number of facilitators in the session meant that everyone who needed help could get it. It\'s worth noting that having more than a single facilitator is useful in terms of the dynamic it creates. You can bounce things off one another; you can use each other for examples and illustrations. You can crack each other up. Done well it reduces the instructor / learner divide and that breaking down of barriers is something worth seeking.\\n\\n## RTM\\n\\nWe\'ve run a bootcamp once now. Where we are is informed by the experience we\'ve just had. A different group of learners may well have resulted in a slightly different format; though I have a feeling not overly dissimilar. We feel pretty sure that what we\'ve got is pretty solid. That said, just as the attendees are learning about development, we\'re still learning about learning!"},{"id":"font-awesome-brand-icons-react","metadata":{"permalink":"/font-awesome-brand-icons-react","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2018-10-07-font-awesome-brand-icons-react/index.md","source":"@site/blog/2018-10-07-font-awesome-brand-icons-react/index.md","title":"Brand New Fonting Awesomeness","description":"Learn how to use brand icons with Font Awesome 5 in React with these helpful instructions on @fortawesome/free-brands-svg-icons.","date":"2018-10-07T00:00:00.000Z","tags":[{"inline":false,"label":"React","permalink":"/tags/react","description":"The React library."}],"readingTime":1.435,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"font-awesome-brand-icons-react","title":"Brand New Fonting Awesomeness","authors":"johnnyreilly","tags":["react"],"hide_table_of_contents":false,"description":"Learn how to use brand icons with Font Awesome 5 in React with these helpful instructions on @fortawesome/free-brands-svg-icons."},"unlisted":false,"prevItem":{"title":"Making a Programmer","permalink":"/making-a-programmer"},"nextItem":{"title":"ts-loader Project References: First Blood","permalink":"/ts-loader-project-references-first-blood"}},"content":"Love me some [Font Awesome](https://fontawesome.com). Absolutely wonderful. However, I came a cropper when following the instructions [on using the all new Font Awesome 5 with React](https://fontawesome.com/how-to-use/on-the-web/using-with/react). The instructions for standard icons work _fine_. But if you want to use brand icons then this does not help you out much. There\'s 2 problems:\\n\\n\x3c!--truncate--\x3e\\n\\n1. Font Awesome\'s brand icons are not part of [`@fortawesome/free-solid-svg-icons`](https://www.npmjs.com/package/@fortawesome/free-solid-svg-icons) package\\n2. The method of icon usage illustrated (i.e. with the `FontAwesomeIcon` component) doesn\'t work. It doesn\'t render owt.\\n\\n## Brand Me Up Buttercup\\n\\nYou want brands? Well you need the [`@fortawesome/free-brands-svg-icons`](https://www.npmjs.com/package/@fortawesome/free-brands-svg-icons). Obvs, right?\\n\\n```sh\\nyarn add @fortawesome/fontawesome-svg-core\\nyarn add @fortawesome/free-brands-svg-icons\\nyarn add @fortawesome/react-fontawesome\\n```\\n\\nNow usage:\\n\\n```jsx\\nimport * as React from \'react\';\\nimport { FontAwesomeIcon } from \'@fortawesome/react-fontawesome\';\\nimport { faReact } from \'@fortawesome/free-brands-svg-icons\';\\n\\nexport const Framework = () => (\\n  <div>\\n    Favorite Framework: <FontAwesomeIcon icon={faReact} />\\n  </div>\\n);\\n```\\n\\nHere we\'ve ditched the \\"library / magic-string\\" approach from the documentation for one which explicitly imports and uses the required icons. I suspect this will be good for tree-shaking as well but, hand-on-heart, I haven\'t rigorously tested that. I\'m not sure why the approach I\'m using isn\'t documented actually. Mysterious! I\'ve seen no ill-effects from using it but perhaps YMMV. Proceed with caution...\\n\\n## Update: It is documented!\\n\\nYup - information on this approach is out there; but it\'s less obvious than you might hope. [Read all about it here.](https://github.com/FortAwesome/react-fontawesome#explicit-import) For what it\'s worth, the explicit import approach seems to be playing second fiddle to the library / magic-string one. I\'m not too sure why. For my money, explicit imports are clearer, less prone to errors and better setup for optimisation. Go figure...\\n\\nFeel free to set me straight in the comments!"},{"id":"ts-loader-project-references-first-blood","metadata":{"permalink":"/ts-loader-project-references-first-blood","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2018-09-23-ts-loader-project-references-first-blood/index.md","source":"@site/blog/2018-09-23-ts-loader-project-references-first-blood/index.md","title":"ts-loader Project References: First Blood","description":"ts-loader now supports TypeScripts project references. However, composite projects built with `outDir` on Windows cannot be consumed by ts-loader... yet","date":"2018-09-23T00:00:00.000Z","tags":[{"inline":false,"label":"ts-loader","permalink":"/tags/ts-loader","description":"The TypeScript loader for webpack."},{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":3.39,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"ts-loader-project-references-first-blood","title":"ts-loader Project References: First Blood","authors":"johnnyreilly","tags":["ts-loader","webpack","typescript"],"hide_table_of_contents":false,"description":"ts-loader now supports TypeScripts project references. However, composite projects built with `outDir` on Windows cannot be consumed by ts-loader... yet"},"unlisted":false,"prevItem":{"title":"Brand New Fonting Awesomeness","permalink":"/font-awesome-brand-icons-react"},"nextItem":{"title":"Semantic Versioning and Definitely Typed","permalink":"/semantic-versioning-and-definitely-typed"}},"content":"So [project references](https://www.typescriptlang.org/docs/handbook/project-references.html) eh? They shipped with [TypeScript 3](https://blogs.msdn.microsoft.com/typescript/2018/07/30/announcing-typescript-3-0/#project-references). We\'ve just shipped initial support for project references in [`ts-loader v5.2.0`](https://github.com/TypeStrong/ts-loader/releases/tag/v5.2.0). All the hard work was done by the amazing [Andrew Branch](https://twitter.com/atcb). In fact I\'d recommend taking a gander at [the PR](https://github.com/TypeStrong/ts-loader/pull/817). Yay Andrew!\\n\\n\x3c!--truncate--\x3e\\n\\nThis post will take us through the nature of the support for project references in ts-loader now and what we hope the future will bring. It <strike>rips off shamelessly</strike>\\n\\nborrows from the [`README/index.md`](https://github.com/TypeStrong/ts-loader#projectreferences-boolean-defaultfalse) documentation that Andrew wrote as part of the PR. Because I am not above stealing.\\n\\n## TL;DR\\n\\nUsing project references currently requires building referenced projects outside of ts-loader. We don\u2019t want to keep it that way, but we\u2019re releasing what we\u2019ve got now. To try it out, you\u2019ll need to pass `projectReferences: true` to `loaderOptions`.\\n\\n## Like `tsc`, but _not_ like `tsc --build`\\n\\nts-loader has partial support for [project references](https://www.typescriptlang.org/docs/handbook/project-references.html) in that it will _load_ dependent composite projects that are already built, but will not currently _build/rebuild_ those upstream projects. The best way to explain exactly what this means is through an example. Say you have a project with a project reference pointing to the `lib/` directory:\\n\\n```sh\\ntsconfig.json\\napp.ts\\nlib/\\n  tsconfig.json\\n  niftyUtil.ts\\n```\\n\\nAnd we\u2019ll assume that the root `tsconfig.json` has `{ \\"references\\": { \\"path\\": \\"lib\\" } }`, which means that any import of a file that\u2019s part of the `lib` sub-project is treated as a reference to another project, not just a reference to a TypeScript file. Before discussing how ts-loader handles this, it\u2019s helpful to review at a really basic level what `tsc` itself does here. If you were to run `tsc` on this tiny example project, the build would fail with the error:\\n\\n```sh\\nerror TS6305: Output file \'lib/niftyUtil.d.ts\' has not been built from source file \'lib/niftyUtil.ts\'.\\n```\\n\\nUsing project references actually instructs `tsc`_not_ to build anything that\u2019s part of another project from source, but rather to look for any `.d.ts` and `.js` files that have already been generated from a previous build. Since we\u2019ve never built the project in `lib` before, those files don\u2019t exist, so building the root project fails. Still just thinking about how `tsc` works, there are two options to make the build succeed: either run `tsc -p lib/tsconfig.json`_first_, or simply run `tsc --build`, which will figure out that `lib` hasn\u2019t been built and build it first for you.\\n\\nOk, so how is that relevant to ts-loader? Because the best way to think about what ts-loader does with project references is that it acts like `tsc`, but _not_ like `tsc --build`. If you run ts-loader on a project that\u2019s using project references, and any upstream project hasn\u2019t been built, you\u2019ll get the exact same `error TS6305` that you would get with `tsc`. If you modify a source file in an upstream project and don\u2019t rebuild that project, `ts-loader` won\u2019t have any idea that you\u2019ve changed anything\u2014it will still be looking at the output from the last time you _built_ that file.\\n\\n## \u201CHey, don\u2019t you think that sounds kind of useless and terrible?\u201D\\n\\nWell, sort of. You can consider it a work-in-progress. It\u2019s true that on its own, as of today, ts-loader doesn\u2019t have everything you need to take advantage of project references in webpack. In practice, though, _consuming_ upstream projects and _building_ upstream projects are somewhat separate concerns. Building them will likely come in a future release. For background, see the [original issue](https://github.com/TypeStrong/ts-loader/issues/815).\\n\\n## `outDir` Windows problemo.\\n\\nAt the moment, composite projects built using the [`outDir` compiler option](https://www.typescriptlang.org/docs/handbook/compiler-options.html) cannot be consumed using ts-loader on Windows. If you try to, ts-loader throws a \\"`has not been built from source file`\\" error. [You can see Andrew and I puzzling over it in the PR.](https://github.com/TypeStrong/ts-loader/pull/817#issuecomment-422245998) We don\'t know why yet; it\'s possible there\'s a bug in `tsc`. It\'s more likely there\'s a bug in `ts-loader`. Hopefully it\'s going to get solved at some point. (Hey, maybe you\'re the one to solve it!) Either way, we didn\'t want to hold back from releasing. So if you\'re building on Windows then avoid building `composite` projects using `outDir`."},{"id":"semantic-versioning-and-definitely-typed","metadata":{"permalink":"/semantic-versioning-and-definitely-typed","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2018-09-15-semantic-versioning-and-definitely-typed/index.md","source":"@site/blog/2018-09-15-semantic-versioning-and-definitely-typed/index.md","title":"Semantic Versioning and Definitely Typed","description":"Definitely Typed lacks semantic versioning, causing build failures. Use specific package versions, and breaking changes can be positive.","date":"2018-09-15T00:00:00.000Z","tags":[{"inline":false,"label":"Definitely Typed","permalink":"/tags/definitely-typed","description":"The Definitely Typed project for TypeScript type definitions"},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":4.61,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"semantic-versioning-and-definitely-typed","title":"Semantic Versioning and Definitely Typed","authors":"johnnyreilly","tags":["definitely typed","typescript"],"image":"./i-must-break-you.webp","hide_table_of_contents":false,"description":"Definitely Typed lacks semantic versioning, causing build failures. Use specific package versions, and breaking changes can be positive."},"unlisted":false,"prevItem":{"title":"ts-loader Project References: First Blood","permalink":"/ts-loader-project-references-first-blood"},"nextItem":{"title":"Using TypeScript and webpack alias: goodbye relative paths","permalink":"/typescript-webpack-alias-goodbye-relative-paths"}},"content":"This a tale of things that are and things that aren\'t. It\'s a tale of semantic versioning, the lack thereof and heartbreak. It\'s a story of terror and failing builds. But it has a bittersweet ending wherein our heroes learn a lesson and understand the need for compromise. We all come out better and wiser people. Hopefully there\'s something for everybody; let\'s start with an exciting opener and see where it goes...\\n\\n\x3c!--truncate--\x3e\\n\\n## Definitely Typed\\n\\nThis is often the experience people have of using type definitions from Definitely Typed:\\n\\n![Ivan Drago saying \\"I must break you\\"](i-must-break-you.webp)\\n\\nSpecifically, people are used to the idea of semantic versioning and expect it from types published to npm by Definitely Typed. They wait in vain. [I\'ve written before about the Definitely Typed / @types semantic version compromise.](../2017-02-14-typescript-types-and-repeatable-builds/index.md) And I wanted to talk about it a little further as (watching the issues raised on DT) I don\'t think the message has quite got out there. To summarise:\\n\\n1. npm is built on top of [semantic versioning](http://semver.org/) and they [take it seriously](https://docs.npmjs.com/getting-started/semantic-versioning). When a package is published it should be categorised as a major release (breaking changes), a minor release (extra functionality which is backwards compatible) or a patch release (backwards compatible bug fixes).\\n\\n2. Definitely Typed publishes type definitions to npm under the `@types` namespace\\n\\n3. To make consumption of type definitions easier, the versioning of a type definition package will seek to emulate the versioning of the npm package it supports. For example, right now [`react-router`](https://www.npmjs.com/package/react-router)\'s latest version is `4.3.1`. The corresponding type definition [`@types/react-router`](https://www.npmjs.com/package/@types/react-router)\'s latest version is `4.0.31`. (It\'s fairly common for type definition versions to lag behind the package they type.)\\n\\nIf there\'s a breaking change to the `react-router` type definition then the new version published will have a version number that begins `\\"4.0.\\"`. If you are relying on semantic versioning this will break you.\\n\\n## I Couldn\'t Help But Notice Your Pain\\n\\nIf you\'re reading this and can\'t quite believe that @types would be so inconsiderate as to break the conventions of the ecosystem it lives in, I understand. But hopefully you can see there are reasons for this. In the end, being able to use npm as a delivery mechanism for versioned type definitions associated with another package has a cost; that cost is semantic versioning for the type definitions themselves. It wasn\'t a choice taken lightly; it\'s a pragmatic compromise.\\n\\n\\"But what about my failing builds? Fine, people are going to change type definitions, but why should I burn because of their choices?\\"\\n\\nExcellent question. Truly. Well here\'s my advice: don\'t expect semantic versioning where there is none. Use specific package versions. You can do that directly with your `package.json`. For example replace something like this: `\\"@types/react-router\\": \\"^4.0.0\\"` with a specific version number: `\\"@types/react-router\\": \\"4.0.31\\"`. With this approach it\'s a specific activity to upgrade your type definitions. A chore if you will; but a chore that guarantees builds will not fail unexpectedly due to changing type defs.\\n\\nMy own personal preference is [yarn](https://yarnpkg.com/lang/en/). Mother, I\'m in love with a `yarn.lock` file. It is the alternative npm client that came out of Facebook. It pins the exact versions of all packages used in your `yarn.lock` file and guarantees to install the same versions each time. Problem solved; and it even allows me to keep the semantic versioning in my `package.json` as is.\\n\\nThis has some value in that when I upgrade I probably want to upgrade to a newer version following the semantic versioning convention. I should just expect that I\'ll need to check valid compilation when I do so. yarn even has it\'s own built in utility that tells you when things are out of date: `yarn outdated`:\\n\\n![Screenshot of outdated dependencies in yarn](yarn-outdated.webp)\\n\\nSo lovely.\\n\\n## You Were Already Broken - I Just Showed You How\\n\\nBefore I finish I wanted to draw out one reason why breaking changes can be a reason for happiness. Because sometimes your code is wrong. An update to a type definition may highlight that. This is analogous to when the TypeScript compiler ships a new version. When I upgrade to a newer version of TypeScript it lights up errors in my codebase that I hadn\'t spotted. Yay compiler!\\n\\nAn example of this is [a PR I submitted to DefinitelyTyped earlier this week](https://github.com/DefinitelyTyped/DefinitelyTyped/pull/28868). This PR changed how `react-router` models the parameters of a `Match`. Until now, an object was expected; the user could define any object they liked. However, `react-router` will only produce `string` values for a parameter. [If you look at the underlying code it\'s nothing more than an `exec` on a regular expression.](https://github.com/ReactTraining/react-router/blob/34ff1f8077d95edf01e9d5ca8ea4708b8d0290e2/packages/react-router/modules/matchPath.js#L36)\\n\\nMy PR enforces this at type level by changing this:\\n\\n```ts\\nexport interface match<P> {\\n  params: P;\\n  // ...\\n}\\n```\\n\\nTo this\\n\\n```ts\\nexport interface match<Params extends { [K in keyof Params]?: string } = {}> {\\n  params: Params;\\n  // ...\\n}\\n```\\n\\nSo any object definition supplied must have `string` values (and you don\'t actually need to supply an object definition; that\'s optional now).\\n\\nI expected this PR to break people [and it did](https://github.com/DefinitelyTyped/DefinitelyTyped/issues/28894). But this is a useful break. If they were relying upon their parameters to be types other than strings they would be experiencing some unexpected behaviour. In fact, it\'s exactly this that prompted my PR in the first place. A colleague had defined his parameters as `number`s and couldn\'t understand why they weren\'t behaving like `number`s. Because they weren\'t `number`s! And wonderfully, this will now be caught at compile time; not runtime. Yay!"},{"id":"typescript-webpack-alias-goodbye-relative-paths","metadata":{"permalink":"/typescript-webpack-alias-goodbye-relative-paths","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2018-08-21-typescript-webpack-alias-goodbye-relative-paths/index.md","source":"@site/blog/2018-08-21-typescript-webpack-alias-goodbye-relative-paths/index.md","title":"Using TypeScript and webpack alias: goodbye relative paths","description":"Use TypeScript with webpack alias to avoid long relative paths in imports. Try `path mapping` or `resolve.alias`. Use `tsconfig-paths-webpack-plugin`.","date":"2018-08-21T00:00:00.000Z","tags":[{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":2.865,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"typescript-webpack-alias-goodbye-relative-paths","title":"Using TypeScript and webpack alias: goodbye relative paths","authors":"johnnyreilly","tags":["webpack","typescript"],"hide_table_of_contents":false,"description":"Use TypeScript with webpack alias to avoid long relative paths in imports. Try `path mapping` or `resolve.alias`. Use `tsconfig-paths-webpack-plugin`."},"unlisted":false,"prevItem":{"title":"Semantic Versioning and Definitely Typed","permalink":"/semantic-versioning-and-definitely-typed"},"nextItem":{"title":"Azure App Service: nested configuration for ASP.NET running in Web App for Containers using Application Settings","permalink":"/azure-app-service-web-app-containers-asp-net-nested-configuration"}},"content":"This post shows how you can use TypeScript with webpack `alias` to move away from using relative paths in your `import` statements.\\n\\n\x3c!--truncate--\x3e\\n\\n## Long relative paths\\n\\nI write a lot of TypeScript. Because I like modularity, I split up my codebases into discreet modules and `import` from them as necessary.\\n\\nTake a look at this `import`:\\n\\n```ts\\nimport * as utils from \'../../../../../../../shared/utils\';\\n```\\n\\nNow take a look at this import:\\n\\n```ts\\nimport * as utils from \'shared/utils\';\\n```\\n\\nWhich do you prefer? If the answer was \\"the first\\" then read no further. You have all you need, go forth and be happy. If the answer was \\"the second\\" then stick around; I can help!\\n\\n## TypeScript\\n\\nThere\'s been a solution for this in TypeScript-land for some time. You can read the detail [in the \\"path mapping\\" docs here](https://www.typescriptlang.org/docs/handbook/module-resolution.html#path-mapping).\\n\\nLet\'s take a slightly simpler example; we have a folder structure that looks like this:\\n\\n```console\\nprojectRoot\\n\u251C\u2500\u2500 components\\n\u2502 \u2514\u2500\u2500 page.tsx (imports \'../shared/utils\')\\n\u251C\u2500\u2500 shared\\n\u2502 \u251C\u2500\u2500 folder1\\n\u2502 \u2514\u2500\u2500 folder2\\n\u2502 \u2514\u2500\u2500 utils.ts\\n\u2514\u2500\u2500 tsconfig.json\\n```\\n\\nWe would like `page.tsx` to import `\'shared/utils\'` instead of `\'../shared/utils\'`. We can, if we augment our `tsconfig.json` with the following properties:\\n\\n```json\\n{\\n  \\"compilerOptions\\": {\\n    \\"baseUrl\\": \\".\\",\\n    \\"paths\\": {\\n      \\"components/*\\": [\\"components/*\\"],\\n      \\"shared/*\\": [\\"shared/*\\"]\\n    }\\n  }\\n}\\n```\\n\\nThen we can use option 2. We can happily write:\\n\\n```ts\\nimport * as utils from \'shared/utils\';\\n```\\n\\nMy code compiles, yay.... Ship it!\\n\\nLet\'s not get over-excited. Actually, we\'re only part-way there; you can compile this code with the TypeScript compiler.... But is that enough?\\n\\nI bundle my TypeScript with [ts-loader](https://github.com/TypeStrong/ts-loader) and webpack. If I try and use my new exciting import statement above with my build system then disappointment is in my future. webpack will be all like \\"import whuuuuuuuut?\\"\\n\\nYou see, webpack doesn\'t know what we told the TypeScript compiler in the `tsconfig.json`. Why would it? It was our little secret.\\n\\n## webpack `resolve.alias` to the rescue!\\n\\nThis same functionality has existed in webpack for a long time; actually much longer than it has existed in TypeScript. It\'s the [`resolve.alias`](https://webpack.js.org/configuration/resolve/#resolve-alias) functionality.\\n\\nSo, looking at that I should be able to augment my `webpack.config.js` like so:\\n\\n```js\\nmodule.exports = {\\n  //...\\n  resolve: {\\n    alias: {\\n      components: path.resolve(process.cwd(), \'components/\'),\\n      shared: path.resolve(process.cwd(), \'shared/\'),\\n    },\\n  },\\n};\\n```\\n\\nAnd now both webpack and TypeScript are up to speed with how to resolve modules.\\n\\n## DRY with the [`tsconfig-paths-webpack-plugin`](https://github.com/dividab/tsconfig-paths-webpack-plugin)\\n\\nWhen I look at the `tsconfig.json` and the `webpack.config.js` something occurs to me: I don\'t like to repeat myself. As well as that, I don\'t like to repeat myself. It\'s so... Repetitive.\\n\\nThe declarations you make in the `tsconfig.json` are re-stated in the `webpack.config.js`. Who wants to maintain two sets of code where one would do? Not me.\\n\\nFortunately, you don\'t have to. There\'s the [`tsconfig-paths-webpack-plugin`](https://github.com/dividab/tsconfig-paths-webpack-plugin) for webpack which will do the job for you. You can replace your verbose `resolve.alias` with this:\\n\\n```ts\\nmodule.exports = {\\n  //...\\n  resolve: {\\n    plugins: [\\n      new TsconfigPathsPlugin({\\n        /*configFile: \\"./path/to/tsconfig.json\\" */\\n      }),\\n    ],\\n  },\\n};\\n```\\n\\nThis does the hard graft of reading your `tsconfig.json` and translating path mappings into webpack `alias`es. From this point forward, you need only edit the `tsconfig.json` and everything else will just work.\\n\\nThanks to [Jonas Kello](https://github.com/jonaskello), author of the plugin; it\'s tremendous! Thanks also to [Sean Larkin](https://twitter.com/TheLarkInn) and [Stanislav Panferov](https://github.com/s-panferov) (of [awesome-typescript-loader](https://github.com/s-panferov/awesome-typescript-loader)) who together worked on the original plugin that I understand the `tsconfig-paths-webpack-plugin` is based on. Great work!"},{"id":"azure-app-service-web-app-containers-asp-net-nested-configuration","metadata":{"permalink":"/azure-app-service-web-app-containers-asp-net-nested-configuration","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2018-07-28-azure-app-service-web-app-containers-asp-net-nested-configuration/index.md","source":"@site/blog/2018-07-28-azure-app-service-web-app-containers-asp-net-nested-configuration/index.md","title":"Azure App Service: nested configuration for ASP.NET running in Web App for Containers using Application Settings","description":"Learn how to configure an ASP.NET application in Azure App Service Web App for Containers without colons. Use a double underscore instead.","date":"2018-07-28T00:00:00.000Z","tags":[],"readingTime":1.895,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"azure-app-service-web-app-containers-asp-net-nested-configuration","title":"Azure App Service: nested configuration for ASP.NET running in Web App for Containers using Application Settings","authors":"johnnyreilly","tags":[],"image":"./appservice_classic.webp","hide_table_of_contents":false,"description":"Learn how to configure an ASP.NET application in Azure App Service Web App for Containers without colons. Use a double underscore instead."},"unlisted":false,"prevItem":{"title":"Using TypeScript and webpack alias: goodbye relative paths","permalink":"/typescript-webpack-alias-goodbye-relative-paths"},"nextItem":{"title":"Cypress and Auth0","permalink":"/cypress-and-auth0"}},"content":"How can we configure an ASP.NET application with nested properties [Azure App Service Web App for Containers](https://azure.microsoft.com/en-gb/services/app-service/containers/) using Application Settings in Azure? Colons don\'t work.\\n\\n\x3c!--truncate--\x3e\\n\\n## Containers on App Service\\n\\nApp Services have long been a super simple way to spin up a web app in Azure. The barrier to entry is low, maintenance is easy. It just works. App Services recently got a turbo boost in the form of [Azure App Service on Linux](https://docs.microsoft.com/en-us/azure/app-service/containers/app-service-linux-intro). Being able to deploy to Linux is exciting enough; but another reason this is notable because [you can deploy Docker images that will be run as app services](https://docs.microsoft.com/en-us/azure/app-service/containers/tutorial-custom-docker-image).\\n\\nI cannot over-emphasise just how easy this makes getting a Docker image into Production. Yay Azure!\\n\\n## The Mystery of Configuration\\n\\nApplications need configuration. ASP.Net Core applications are typically configured by an `appsettings.json` file which might look like so:\\n\\n```json\\n{\\n  \\"Parent\\": {\\n    \\"ChildOne\\": \\"I\'m a little teapot\\",\\n    \\"ChildTwo\\": \\"Short and stout\\"\\n  }\\n}\\n```\\n\\nWith a classic App Service you could override a setting in the `appsettings.json` by updating \\"Application settings\\" within the Azure portal. You\'d do this in the style of creating an Application setting called `Parent:ChildOne` or `Parent:ChildTwo`. To be clear: using colons to target a specific piece of config.\\n\\n![screenshot of an App Service Application Settings in the Azure Portal, nested properties configured using colons](appservice_classic.webp)\\n\\nYou can read about this approach [here](https://blogs.msdn.microsoft.com/waws/2018/06/12/asp-net-core-settings-for-azure-app-service/). Now there\'s something I want you to notice; consider the colons below:\\n\\n![screenshot of an App Service specific Application Setting nested property configured using colons - all good](appservice_colons_fine.png)\\n\\nIf you try and follow the same steps when you\'re using Web App for Containers / i.e. [a Docker image deployed to an Azure App Service on Linux ](https://docs.microsoft.com/en-us/azure/app-service/containers/app-service-linux-intro) you **cannot** use colons:\\n\\n![screenshot of a Web App for Containers specific Application Setting nested property configured using colons - errors](appservice_container_colons_bad.webp)\\n\\nWhen you hover over the error you see this message: `This field can only contain letters, numbers (0-9), periods (\\".\\"), and underscores (\\"_\\")`. Using `.` does not work alas.\\n\\n## How do we configure without colons?\\n\\nIt\'s simple. Where you would use `:` on a classic App Service, you should use a `__` (double underscore) on an App Service with containers. So `Parent__ChildOne` instead of `Parent:ChildOne`. It\'s as simple as that."},{"id":"cypress-and-auth0","metadata":{"permalink":"/cypress-and-auth0","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2018-07-09-cypress-and-auth0/index.md","source":"@site/blog/2018-07-09-cypress-and-auth0/index.md","title":"Cypress and Auth0","description":"The article explains how to automate Auth0 login using Cypress, by using the auth0-js client library, and creating a custom command.","date":"2018-07-09T00:00:00.000Z","tags":[{"inline":false,"label":"Authentication and Authorization","permalink":"/tags/auth","description":"Matters related to identity and permissioning"},{"inline":false,"label":"Automated Testing","permalink":"/tags/automated-testing","description":"How to perform the automation of tests."}],"readingTime":4.43,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"cypress-and-auth0","title":"Cypress and Auth0","authors":"johnnyreilly","tags":["auth","automated testing"],"hide_table_of_contents":false,"description":"The article explains how to automate Auth0 login using Cypress, by using the auth0-js client library, and creating a custom command."},"unlisted":false,"prevItem":{"title":"Azure App Service: nested configuration for ASP.NET running in Web App for Containers using Application Settings","permalink":"/azure-app-service-web-app-containers-asp-net-nested-configuration"},"nextItem":{"title":"VSTS and EF Core Migrations","permalink":"/vsts-and-ef-core-migrations"}},"content":"[Cypress](https://www.cypress.io/) is a fantastic way to write UI tests for your web apps. Just world class. Wait, no. Galaxy class. I\'m going to go one further: universe class. You get my drift.\\n\\n\x3c!--truncate--\x3e\\n\\nHere\'s a pickle for you. You have functionality that lies only behind the walled garden of authentication. You want to write tests for these capabilities. Assuming that authentication takes place within your application that\'s no great shakes. Authentication is part of your app; it\'s no big deal using Cypress to automate logging in.\\n\\nAuth is a serious business and, as Cypress is best in class for UI testing, I\'ll say that Auth0 is romping home with the same title in the auth-as-a-service space. My app is using Auth0 for authentication. What\'s important to note about this is the flow. Typically when using auth-as-a-service, the user is redirected to the auth provider\'s site to authenticate and then be redirected back to the application post-login.\\n\\n[Brian Mann](https://github.com/brian-mann) (of Cypress fame) has been [fairly clear when talking about testing with this sort of authentication flow](https://github.com/cypress-io/cypress/issues/1342#issuecomment-366747803):\\n\\n> You\'re trying to test SSO - and we have recipes showing you exactly how to do this.\\n>\\n> Also best practice is never to visit or test 3rd party sites not under your control. You don\'t control `microsoftonline`, so there\'s no reason to use the UI to test this. You can programmatically test the integration between it and your app with `cy.request` \\\\- which is far faster, more reliable, and still gives you 100% confidence.\\n\\nI want to automate logging into Auth0 from my Cypress tests. But hopefully in a good way. Not a bad way. Wouldn\'t want to make Brian sad.\\n\\n## Commanding Auth0\\n\\nTo automate our login, we\'re going to use the [auth0-js client library](https://github.com/auth0/auth0.js). This is the same library the application uses; but we\'re going to do something subtly different with it.\\n\\nThe application uses [`authorize`](https://github.com/auth0/auth0.js#api) to log users in. This function redirects the user into the Auth0 lock screen, and then, post authentication, redirects the user back to the application with a token in the URL. The app parses the token (using the auth0 client library) and sets the token and the expiration of said token in the browser sessionStorage.\\n\\nWhat we\'re going to do is automate our login by using `login` instead. First of all, we need to add `auth0-js` as a dependency of our e2e tests:\\n\\n```js\\nyarn add auth0-js --dev\\n```\\n\\nNext, we\'re going to create ourselves a custom command called loginAsAdmin:\\n\\n```js\\nconst auth0 = require(\'auth0-js\');\\n\\nCypress.Commands.add(\'loginAsAdmin\', (overrides = {}) => {\\n  Cypress.log({\\n    name: \'loginAsAdminBySingleSignOn\',\\n  });\\n\\n  const webAuth = new auth0.WebAuth({\\n    domain: \'my-super-duper-domain.eu.auth0.com\', // Get this from https://manage.auth0.com/#/applications and your application\\n    clientID: \'myclientid\', // Get this from https://manage.auth0.com/#/applications and your application\\n    responseType: \'token id_token\',\\n  });\\n\\n  webAuth.client.login(\\n    {\\n      realm: \'Username-Password-Authentication\',\\n      username: \'mytestemail@something.co.uk\',\\n      password: \'SoVeryVeryVery$ecure\',\\n      audience: \'myaudience\', // Get this from https://manage.auth0.com/#/apis and your api, use the identifier property\\n      scope: \'openid email profile\',\\n    },\\n    function (err, authResult) {\\n      // Auth tokens in the result or an error\\n      if (authResult && authResult.accessToken && authResult.idToken) {\\n        const token = {\\n          accessToken: authResult.accessToken,\\n          idToken: authResult.idToken,\\n          // Set the time that the access token will expire at\\n          expiresAt: authResult.expiresIn * 1000 + new Date().getTime(),\\n        };\\n\\n        window.sessionStorage.setItem(\\n          \'my-super-duper-app:storage_token\',\\n          JSON.stringify(token),\\n        );\\n      } else {\\n        console.error(\'Problem logging into Auth0\', err);\\n        throw err;\\n      }\\n    },\\n  );\\n});\\n```\\n\\nThis command logs in using the `auth0-js` API and then sets the result into `sessionStorage` in the same way that our app does. This allows our app to read the value out of `sessionStorage` and use it. We\'re also going to put together one other command:\\n\\n```js\\nCypress.Commands.add(\'visitHome\', (overrides = {}) => {\\n  cy.visit(\'/\', {\\n    onBeforeLoad: (win) => {\\n      win.sessionStorage.clear();\\n    },\\n  });\\n});\\n```\\n\\nThis visits the root of our application and wipes the `sessionStorage`. This is necessary because Cypress doesn\'t clear down `sessionStorage` between tests. ([That\'s going to change though.](https://github.com/cypress-io/cypress/issues/413))\\n\\n## Using It\\n\\nLet\'s write a test that uses our new commands to see if it gets access to our admin functionality:\\n\\n```js\\ndescribe(\'access secret admin functionality\', () => {\\n  it(\'should be able to navigate to\', () => {\\n    cy.visitHome()\\n      .loginAsAdmin()\\n      .get(\'[href=\\"/secret-adminny-stuff\\"]\') // This link should only be visible to admins\\n      .click()\\n      .url()\\n      .should(\'contain\', \'secret-adminny-stuff/\'); // non-admins should be redirected away from this url\\n  });\\n});\\n```\\n\\nWell, the test looks good but it\'s failing. If I fire up the Chrome Dev Tools in Cypress (did I mention that Cypress is absolutely fabulous?) then I see this response tucked away in the network tab:\\n\\n```json\\n{error: \\"unauthorized_client\\",\u2026} error : \\"unauthorized_client\\" error_description : \\"Grant type \'http://auth0.com/oauth/grant-type/password-realm\' not allowed for the client.\\"\\n```\\n\\nHmmm... So sad. If you go to [https://manage.auth0.com/#/applications](https://manage.auth0.com/#/applications), select your application, `Show Advanced Settings` and `Grant Types` you\'ll see a `Password` option is unselected.\\n\\nSelect it, Save Changes and try again.\\n\\n![](auth0-enable-password-grant-type.webp)\\n\\nYou now have a test which automates your Auth0 login using Cypress and goes on to test your application functionality with it!\\n\\n## One More Thing...\\n\\nIt\'s worth saying that it\'s worth setting up different tenants in Auth0 to support your testing scenarios. This is generally a good idea so you can separate your testing accounts from Production accounts. Further to that, you don\'t need to have your Production setup supporting the `Password``Grant Type`.\\n\\nAlso, if you\'re curious about what the application under test is like then read [this](../2018-01-14-auth0-typescript-and-aspnet-core/index.md)."},{"id":"vsts-and-ef-core-migrations","metadata":{"permalink":"/vsts-and-ef-core-migrations","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2018-06-24-vsts-and-ef-core-migrations/index.md","source":"@site/blog/2018-06-24-vsts-and-ef-core-migrations/index.md","title":"VSTS and EF Core Migrations","description":"Learn how to migrate Entity Framework database migrations during an ASP.NET Core project deployment with a console app using VSTS and Azure.","date":"2018-06-24T00:00:00.000Z","tags":[{"inline":false,"label":"Azure DevOps","permalink":"/tags/azure-devops","description":"The Azure DevOps suite of tools."},{"inline":false,"label":"SQL Server","permalink":"/tags/sql-server","description":"The SQL Server database."}],"readingTime":5.01,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"vsts-and-ef-core-migrations","title":"VSTS and EF Core Migrations","authors":"johnnyreilly","tags":["azure devops","sql server"],"hide_table_of_contents":false,"description":"Learn how to migrate Entity Framework database migrations during an ASP.NET Core project deployment with a console app using VSTS and Azure."},"unlisted":false,"prevItem":{"title":"Cypress and Auth0","permalink":"/cypress-and-auth0"},"nextItem":{"title":"VSTS... YAML up!","permalink":"/vsts-yaml-up"}},"content":"Let me start by telling you a dirty secret. I have an ASP.Net Core project that I build with VSTS. It is deployed to Azure through a CI / CD setup in VSTS. That part I\'m happy with. Proud of even. Now to the sordid hiddenness: try as I might, I\'ve never found a nice way to deploy Entity Framework database migrations as part of the deployment flow. So I have [blushes with embarrassment] been using the `Startup` of my ASP.Net core app to run the migrations on my database. There. I said it. You all know. Absolutely filthy. Don\'t judge me.\\n\\n\x3c!--truncate--\x3e\\n\\nIf you care to google, you\'ll find various discussions around this, and various ways to tackle it. Most of which felt like too much hard work and so I never attempted.\\n\\nIt\'s also worth saying that being on VSTS made me less likely to give these approaches a go. Why? Well, the feedback loop for debugging a CI / CD setup is truly sucky. Make a change. Wait for it to trickle through the CI / CD flow (10 mins at least). Spot a problem, try and fix. Start waiting again. Repeat until you succeed. Or, if you\'re using the free tier of VSTS, repeat until you run out of build minutes. You have a limited number of build minutes per month with VSTS. Last time I fiddled with the build, I bled my way through a full month\'s minutes in 2 days. I have now adopted the approach of only playing with the setup in the last week of the month. That way if I end up running out of minutes, at least I\'ll roll over to the new allowance in a matter of days.\\n\\nDigression over. I could take the guilt of my EF migrations secret no longer, I decided to try and tackle it another way. I used the approach suggested by [Andre Broers](https://github.com/broersa)[here](https://github.com/aspnet/EntityFrameworkCore/issues/9841#issuecomment-395712061):\\n\\n> I worked around by adding a dotnetcore consoleapp project where I run the migration via the Context. In the Build I build this consoleapp in the release I execute it.\\n\\n## Console Yourself\\n\\nFirst things first, we need a console app added to our solution. Fire up PowerShell in the root of your project and:\\n\\n```console\\nmd MyAwesomeProject.MigrateDatabase\\ncd .\\\\MyAwesomeProject.MigrateDatabase\\\\\\ndotnet new console\\n```\\n\\nNext we need that project to know about Entity Framework and also our DbContext (which I store in a dedicated project):\\n\\n```console\\ndotnet add package Microsoft.EntityFrameworkCore.Design\\ndotnet add package Microsoft.EntityFrameworkCore.SqlServer\\ndotnet add reference ..\\\\MyAwesomeProject.Database\\\\MyAwesomeProject.Database.csproj\\n```\\n\\nAdd our new project to our solution: (I always forget to do this)\\n\\n```console\\ncd ../\\ndotnet sln add .\\\\MyAwesomeProject.MigrateDatabase\\\\MyAwesomeProject.MigrateDatabase.csproj\\n```\\n\\nYou should now be the proud possessor of a `.csproj` file that looks like this:\\n\\n```xml\\n<Project Sdk=\\"Microsoft.NET.Sdk\\">\\n\\n  <PropertyGroup>\\n    <OutputType>Exe</OutputType>\\n    <TargetFramework>netcoreapp2.1</TargetFramework>\\n  </PropertyGroup>\\n\\n  <ItemGroup>\\n    <PackageReference Include=\\"Microsoft.EntityFrameworkCore.Design\\" Version=\\"2.1.1\\" />\\n    <PackageReference Include=\\"Microsoft.EntityFrameworkCore.SqlServer\\" Version=\\"2.1.1\\" />\\n  </ItemGroup>\\n\\n  <ItemGroup>\\n    <ProjectReference Include=\\"..\\\\MyAwesomeProject.Database\\\\MyAwesomeProject.Database.csproj\\" />\\n  </ItemGroup>\\n\\n</Project>\\n```\\n\\nReplace the contents of the `Program.cs` file with this:\\n\\n```cs\\nusing System;\\nusing System.IO;\\nusing MyAwesomeProject.Database;\\nusing Microsoft.EntityFrameworkCore;\\n\\nnamespace MyAwesomeProject.MigrateDatabase {\\n    class Program {\\n        // Example usage:\\n        // dotnet MyAwesomeProject.MigrateDatabase.dll \\"Server=(localdb)\\\\\\\\mssqllocaldb;Database=MyAwesomeProject;Trusted_Connection=True;\\"\\n        static void Main(string[] args) {\\n            if (args.Length == 0)\\n                throw new Exception(\\"No connection string supplied!\\");\\n\\n            var myAwesomeProjectConnectionString = args[0];\\n\\n            // Totally optional debug information\\n            Console.WriteLine(\\"About to migrate this database:\\");\\n            var connectionBits = myAwesomeProjectConnectionString.Split(\\";\\");\\n            foreach (var connectionBit in connectionBits) {\\n                if (!connectionBit.StartsWith(\\"Password\\", StringComparison.CurrentCultureIgnoreCase))\\n                    Console.WriteLine(connectionBit);\\n            }\\n\\n            try {\\n                var optionsBuilder = new DbContextOptionsBuilder<MyAwesomeProjectContext>();\\n                optionsBuilder.UseSqlServer(myAwesomeProjectConnectionString);\\n\\n                using(var context = new MyAwesomeProjectContext(optionsBuilder.Options)) {\\n                    context.Database.Migrate();\\n                }\\n                Console.WriteLine(\\"This database is migrated like it\'s the Serengeti!\\");\\n            } catch (Exception exc) {\\n                var failedToMigrateException = new Exception(\\"Failed to apply migrations!\\", exc);\\n                Console.WriteLine($\\"Didn\'t succeed in applying migrations: {exc.Message}\\");\\n                throw failedToMigrateException;\\n            }\\n        }\\n    }\\n}\\n```\\n\\nThis code takes the database connection string passed as an argument, spins up a db context with that, and migrates like it\'s the Serengeti.\\n\\n## Build It!\\n\\nThe next thing we need is to ensure that this is included as part of the build process in VSTS. The following commands need to be run during the build to include the MigrateDatabase project in the build output in a `MigrateDatabase` folder:\\n\\n```cs\\ncd MyAwesomeProject.MigrateDatabase\\ndotnet build\\ndotnet publish --configuration Release --output $(build.artifactstagingdirectory)/MigrateDatabase\\n```\\n\\nThere\'s various ways to accomplish this which I wont reiterate now. [I recommend YAML](../2018-06-16-vsts-yaml-up/index.md).\\n\\n## Deploy It!\\n\\nNow to execute our console app as part of the deployment process we need to add a CommandLine task to our VSTS build definition. It should execute the following command:\\n\\n```cs\\ndotnet MyAwesomeProject.MigrateDatabase.dll \\"$(ConnectionStrings.MyAwesomeProjectDatabaseConnection)\\"\\n```\\n\\nIn the following folder:\\n\\n```cs\\n$(System.DefaultWorkingDirectory)/my-awesome-project-YAML/drop/MigrateDatabase\\n```\\n\\nDo note that the command uses the `ConnectionStrings.MyAwesomeProjectDatabaseConnection` variable which you need to create and set to the value of your connection string.\\n\\n![](Screenshot-2018-06-24-10.55.27.webp)\\n\\n## Give It A Whirl\\n\\nLet\'s find out what happens when the rubber hits the road. I\'ll add a new entity to my database project:\\n\\n```cs\\nusing System;\\n\\nnamespace MyAwesomeProject.Database.Entities {\\n    public class NewHotness {\\n        public Guid NewHotnessId { get; set; }\\n    }\\n}\\n```\\n\\nAnd reference it in my DbContext:\\n\\n```cs\\nusing MyAwesomeProject.Database.Entities;\\nusing Microsoft.EntityFrameworkCore;\\n\\nnamespace MyAwesomeProject.Database {\\n    public class MyAwesomeProjectContext : DbContext {\\n        public MyAwesomeProjectContext(DbContextOptions<MyAwesomeProjectContext> options) : base(options) { }\\n\\n        // ...\\n\\n        public DbSet<NewHotness> NewHotnesses { get; set; }\\n\\n        // ...\\n    }\\n}\\n```\\n\\nLet\'s let EF know by adding a migration to my project:\\n\\n```cs\\ndotnet ef migrations add TestOurMigrationsApproach\\n```\\n\\nCommit my change, push it to VSTS, wait for the build to run and a deployment to take place.... Okay. It\'s done. Looks good.\\n\\n![](Screenshot-2018-06-24-09.02.22.webp)\\n\\nLet\'s take a look in the database:\\n\\n```console\\nselect * from NewHotnesses\\ngo\\n```\\n\\n![](Screenshot-2018-06-24-08.59.00.webp)\\n\\nIt\'s there! We are migrating our database upon deployment; and not in our ASP.Net Core app itself. I feel a burden lifted.\\n\\n## Wrapping Up\\n\\nThe EF Core team are aware of the lack of guidance around deploying migrations and have recently announced plans to fix that in the docs. You can track the progress of this issue [here](https://github.com/aspnet/EntityFramework.Docs/issues/691). There\'s good odds that once they come out with this I\'ll find there\'s a better way than the approach I\'ve outlined in this post. Until that glorious day!"},{"id":"vsts-yaml-up","metadata":{"permalink":"/vsts-yaml-up","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2018-06-16-vsts-yaml-up/index.md","source":"@site/blog/2018-06-16-vsts-yaml-up/index.md","title":"VSTS... YAML up!","description":"Visual Studio Team Services now has a YAML build definition preview feature that enables users to keep their build scripts with their code.","date":"2018-06-16T00:00:00.000Z","tags":[{"inline":false,"label":"Azure DevOps","permalink":"/tags/azure-devops","description":"The Azure DevOps suite of tools."}],"readingTime":4.4,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"vsts-yaml-up","title":"VSTS... YAML up!","authors":"johnnyreilly","tags":["azure devops"],"hide_table_of_contents":false,"description":"Visual Studio Team Services now has a YAML build definition preview feature that enables users to keep their build scripts with their code."},"unlisted":false,"prevItem":{"title":"VSTS and EF Core Migrations","permalink":"/vsts-and-ef-core-migrations"},"nextItem":{"title":"Compromising: A Guide for Developers","permalink":"/compromising-guide-for-developers"}},"content":"For the longest time I\'ve been using the likes of [Travis](https://travis-ci.org/) and [AppVeyor](https://www.appveyor.com/) to build open source projects that I work on. They rock. I\'ve also recently been dipping my toes back in the water of [Visual Studio Team Services](https://www.visualstudio.com/team-services/). VSTS offers a whole stack of stuff, but my own area of interest has been the Continuous Integration / Continuous Deployment offering.\\n\\n\x3c!--truncate--\x3e\\n\\nHistorically I have been underwhelmed by the CI proposition of Team Foundation Server / VSTS. It was difficult to debug, difficult to configure, difficult to understand. If it worked... Great! If it didn\'t (and it often didn\'t), you were toast. But things done changed! I don\'t know when it happened, but VSTS is now super configurable. You add tasks / configure them, build and you\'re done! It\'s really nice.\\n\\nHowever, there\'s been something I\'ve been missing from Travis, AppVeyor et al. Keeping my build script with my code. Travis has `.travis.yml`, AppVeyor has `appveyor.yml`. VSTS, what\'s up?\\n\\n## The New Dawn\\n\\nUp until now, really not much. It just wasn\'t possible. Until it was:\\n\\n> If you prefer a build definition in YAML then we\u2019re currently hard at work on that. You can enable it as a preview feature: [https://t.co/hau9Sv8brf](https://t.co/hau9Sv8brf)\\n>\\n> \u2014 Martin Woodward (@martinwoodward) [March 4, 2018](https://twitter.com/martinwoodward/status/970250739510534144?ref_src=twsrc%5Etfw)\\n\\nWhen I started testing it out I found things to like and some things I didn\'t understand. Crucially, my CI now builds based upon `.vsts-ci.yml`. YAML baby!\\n\\n## It Begins!\\n\\nYou can get to \\"Hello World\\" by looking at [the docs here](https://docs.microsoft.com/en-us/vsts/pipelines/build/yaml?view=vsts) and [the examples here](https://github.com/Microsoft/vsts-agent/blob/master/docs/preview/yamlgettingstarted/index.md). But what you really want is your existing build, configured in the UI, exported to YAML. That doesn\'t seem to quite exist, but there\'s something that gets you part way. Take a look:\\n\\n![screenshot of restore task in VSTS](vsts-screenshot-of-restore-task.webp)\\n\\nIf you notice, in the top right of the screen, each task now allows you click on a new \\"View YAML\\" button. It\'s kinda [Ronseal](https://en.wikipedia.org/wiki/Ronseal):\\n\\n![screenshot of copy to clipboard in VSTS](vsts-screenshot-of-copy-to-clipboard.png)\\n\\nUsing this hotness you can build yourself a `.vsts-ci.yml` file task by task.\\n\\n## A Bump in the Road\\n\\nIf you look closely at the message above you\'ll see there\'s a message about an undefined variable.\\n\\n```yml\\n#Your build definition references an undefined variable named \u2018Parameters.RestoreBuildProjects\u2019. Create or edit the build definition for this YAML file, define the variable on the Variables tab. See https://go.microsoft.com/fwlink/?linkid=865972\\nsteps:\\n  - task: DotNetCoreCLI@2\\n    displayName: Restore\\n    inputs:\\n      command: restore\\n      projects: \'$(Parameters.RestoreBuildProjects)\'\\n```\\n\\nTry as I might, I couldn\'t locate `Parameters.RestoreBuildProjects`. So no working CI build for me. Then I remembered [Zoltan Erdos](https://github.com/zerdos). He\'s hard to forget. Or rather, I remembered an idea of his which I will summarise thusly: \\"Have a `package.json` in the root of your repo, use the `scripts` for individual tasks and you have a cross platform task runner\\".\\n\\nThis is a powerful idea and one I decided to put to work. My project is React and TypeScript on the front end, and ASP.Net Core on the back. I wanted a `package.json` in the root of the repo which I could install dependencies, build, test and publish my whole app. I could call into that from my `.vsts-ci.yml` file. Something like this:\\n\\n```json\\n{\\n  \\"name\\": \\"my-amazing-project\\",\\n  \\"version\\": \\"1.0.0\\",\\n  \\"author\\": \\"John Reilly <johnny_reilly@hotmail.com>\\",\\n  \\"license\\": \\"MIT\\",\\n  \\"private\\": true,\\n  \\"scripts\\": {\\n    \\"preinstall\\": \\"yarn run install:clientapp && yarn run install:web\\",\\n    \\"install:clientapp\\": \\"cd MyAmazingProject.ClientApp && yarn install\\",\\n    \\"install:web\\": \\"dotnet restore\\",\\n    \\"prebuild\\": \\"yarn install\\",\\n    \\"build\\": \\"yarn run build:clientapp && yarn run build:web\\",\\n    \\"build:clientapp\\": \\"cd MyAmazingProject.ClientApp && yarn run build\\",\\n    \\"build:web\\": \\"dotnet build --configuration Release\\",\\n    \\"postbuild\\": \\"yarn test\\",\\n    \\"test\\": \\"yarn run test:clientapp && yarn run test:web\\",\\n    \\"test:clientapp\\": \\"cd MyAmazingProject.ClientApp && yarn test\\",\\n    \\"test:web\\": \\"cd MyAmazingProject.Web.Tests && dotnet test\\",\\n    \\"publish:web\\": \\"cd MyAmazingProject.Web && dotnet publish MyAmazingProject.Web.csproj --configuration Release\\"\\n  }\\n}\\n</johnny_reilly@hotmail.com>\\n```\\n\\nIt doesn\'t matter if I have \\"an undefined variable named \u2018Parameters.RestoreBuildProjects\u2019\\". I now have no need to use all the individual tasks in a build. I can convert them into a couple of scripts in my `package.json`. So here\'s where I\'ve ended up for now. I\'ve a `.vsts-ci.yml` file which looks like this:\\n\\n```yml\\nqueue: Hosted VS2017\\n\\nsteps:\\n  - task: geeklearningio.gl-vsts-tasks-yarn.yarn-installer-task.YarnInstaller@2\\n    displayName: install yarn itself\\n    inputs:\\n      checkLatest: true\\n  - task: geeklearningio.gl-vsts-tasks-yarn.yarn-task.Yarn@2\\n    displayName: yarn build and test\\n    inputs:\\n      Arguments: build\\n  - task: geeklearningio.gl-vsts-tasks-yarn.yarn-task.Yarn@2\\n    displayName: yarn publish:web\\n    inputs:\\n      Arguments: \'run publish:web --output $(build.artifactstagingdirectory)/MyAmazingProject\'\\n  - task: PublishBuildArtifacts@1\\n    displayName: publish build artifact\\n    inputs:\\n      PathtoPublish: \'$(build.artifactstagingdirectory)\'\\n```\\n\\nThis file does the following:\\n\\n1. Installs yarn. (By the way VSTS, what\'s with not having yarn installed by default? I\'ll say this for the avoidance of doubt: in the npm cli space: yarn has won.)\\n2. Install our dependencies, build the front end and back end, run all the tests. Effectively `yarn build`.\\n3. Publish our web app to a directory. Effectively `yarn run publish:web`. This is only separate because we want to pass in the output directory and so it\'s just easier for it to be a separate step.\\n4. Publish the build artefact to TFS. (This will go on to be picked up by the continuous deployment mechanism and published out to Azure.)\\n\\nI much prefer this to what I had before. I feel there\'s much more that can be done here as well. I\'m looking forward to the continuous deployment piece becoming scriptable too.\\n\\nThanks to Zoltan and props to the TFVS team!"},{"id":"compromising-guide-for-developers","metadata":{"permalink":"/compromising-guide-for-developers","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2018-05-13-compromising-guide-for-developers/index.md","source":"@site/blog/2018-05-13-compromising-guide-for-developers/index.md","title":"Compromising: A Guide for Developers","description":"Weighing opinions with a voting system can reduce friction and boost productivity when working with developers of different opinions.","date":"2018-05-13T00:00:00.000Z","tags":[],"readingTime":2.92,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"compromising-guide-for-developers","title":"Compromising: A Guide for Developers","authors":"johnnyreilly","hide_table_of_contents":false,"description":"Weighing opinions with a voting system can reduce friction and boost productivity when working with developers of different opinions."},"unlisted":false,"prevItem":{"title":"VSTS... YAML up!","permalink":"/vsts-yaml-up"},"nextItem":{"title":"Using Reflection to Identify Unwanted Dependencies","permalink":"/using-reflection-to-identify-unwanted-dependencies"}},"content":"It is a truth universally acknowledged, that a single developer, will not be short of an opinion. Opinions on tabs vs spaces. Upon OOP vs FP. Upon `class`es vs `function`s. Just opinions, opinions, opinions. Opinions that are felt with all the sincerity of a Witchfinder General. And, alas, not always the same level of empathy.\\n\\n\x3c!--truncate--\x3e\\n\\nGiven the wealth of strongly felt desires, it\'s kind of amazing that developers ever manage to work together. It\'s rare to find a fellow dev that agrees entirely with your predilections. So how do people ever get past the \\"you don\'t use semi-colons; what\'s wrong with you\\"? Well, not easily to be honest. It involves compromise.\\n\\n## On Compromise\\n\\nWe\'ve all been in the position where we realise that there\'s something we don\'t like in a codebase. The ordering of members in a `class`, naming conventions, a lack of tests... Something.\\n\\nThen comes the moment of trepidation. You suggest a change. You suggest difference. It\'s time to find out if you\'re working with psychopaths. It\'s not untypical to find that you just have to go with the flow.\\n\\n- \\"You\'ve been using 3 spaces?\\"\\n- \\"Yes we use 3 spaces.\\"\\n- \\"Okay... So we\'ll be using 3 spaces...\\" [backs away carefully]\\n\\nI\'ve been in this position so many times I\'ve learned to adapt. It helps that I\'m a malleable sort anyway. But what if there were another way?\\n\\n## Weighting Opinion\\n\\nSometimes your opinion is... Well.... Just an opinion. Other opinions are legitimate. At least in theory. If you can acknowledge that, you already have a level of self knowledge not gifted to all in the dev community. If you\'re able to get that far I feel there\'s something you might want to consider.\\n\\nLet me frame this up: there\'s a choice to be made around an approach that could be used in a codebase. There are 2 camps in the team; 1 camp advocating for 1 approach. The other for a different approach. Either one is functionally legitimate. They work. It\'s just a matter of preference of choice. How do you choose now? Let\'s look at a technique for splitting the difference.\\n\\nVoting helps. But let\'s say 50% of the team wants 1 approach and 50% wants the other. What then? Or, to take a more interesting idea, what say 25% want 1 approach and 75% want the other? If it\'s just 1 person, 1 vote then the 75% wins and that\'s it.\\n\\nBut before we all move on, let\'s consider another factor. How much do people care? What if the 25% are really, really invested in the choice they\'re advocating for and the 75% just have a mild preference? From that point forwards the 25% are likely going to be less happy. Maybe they\'ll even burn inside. They\'re certainly going to be less productive.\\n\\nIt\'s because of situations like this that weighting votes becomes useful. Out of 5, how much do you care? If one person cares \\"5 out of 5\\" and the other three are \\"1 out of 5\\".... Well go with the 25% It matters to them and that it matters to them should matter to you.\\n\\nI\'ll contend that rolling like this makes for more content, happier and more productive teams. Making strength of feeling a factor in choices reduces friction and increases the peace.\\n\\n![](Bestival_2008_Increase_the_Peace_banner.webp)\\n\\nI\'ve only recently discovered this technique and I can\'t claim credit for it. I learned it from the awesome [Jamie McCrindle](https://twitter.com/foldr). I commend to you! Be happier!"},{"id":"using-reflection-to-identify-unwanted-dependencies","metadata":{"permalink":"/using-reflection-to-identify-unwanted-dependencies","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2018-04-28-using-reflection-to-identify-unwanted-dependencies/index.md","source":"@site/blog/2018-04-28-using-reflection-to-identify-unwanted-dependencies/index.md","title":"Using Reflection to Identify Unwanted Dependencies","description":"Learn how to identify unwelcome dependencies in complex web apps by walking a dependency tree using reflection-based tests.","date":"2018-04-28T00:00:00.000Z","tags":[{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":2.545,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"using-reflection-to-identify-unwanted-dependencies","title":"Using Reflection to Identify Unwanted Dependencies","authors":"johnnyreilly","tags":["asp.net"],"hide_table_of_contents":false,"description":"Learn how to identify unwelcome dependencies in complex web apps by walking a dependency tree using reflection-based tests."},"unlisted":false,"prevItem":{"title":"Compromising: A Guide for Developers","permalink":"/compromising-guide-for-developers"},"nextItem":{"title":"It\'s Not Dead 2: mobx-react-devtools and the undead","permalink":"/its-not-dead-2-mobx-react-devtools-and-the-undead"}},"content":"I having a web app which is fairly complex. It\'s made up of services, controllers and all sorts of things. So far, so unremarkable. However, I needed to ensure that the controllers did not attempt to access the database via any of their dependencies. Or their dependencies, dependencies. Or their dependencies. You get my point.\\n\\n\x3c!--truncate--\x3e\\n\\nThe why is not important here. What\'s significant is the idea of walking a dependency tree and identifying, via a reflection based test, when such unwelcome dependencies occur, and where.\\n\\nWhen they do occur the test should fail, like this:\\n\\n```sh\\n[xUnit.net 00:00:01.6766691]     My.Web.Tests.HousekeepingTests.My_Api_Controllers_do_not_depend_upon_the_database [FAIL]\\n[xUnit.net 00:00:01.6782295]       Expected dependsUponTheDatabase.Any() to be False because My.Api.Controllers.ThingyController depends upon the database through My.Data.Services.OohItsAService, but found True.\\n```\\n\\nWhat follows is an example of how you can accomplish this. It is exceedingly far from the most beautiful code I\'ve ever written. But it works. One reservation I have about it is that it doesn\'t use the Dependency Injection mechanism used at runtime (AutoFac). If I had more time I would amend the code to use that instead; it would become an easier test to read if I did. Also it would better get round the limitations of the code below. Essentially the approach relies on the assumption of there being 1 interface and 1 implementation. That\'s often not true in complex systems. But this is good enough to roll with for now.\\n\\n```cs\\nusing System;\\nusing System.Collections.Generic;\\nusing System.Linq;\\nusing System.Reflection;\\nusing FluentAssertions;\\nusing My.Data;\\nusing My.Web.Controllers;\\nusing Xunit;\\n\\nnamespace My.Web.Tests {\\n    public class OiYouThereGetOutTests {\\n        [Fact]\\n        public void My_Controllers_do_not_depend_upon_the_database() {\\n            var myConcreteTypes = GetMyAssemblies()\\n                .SelectMany(assembly => assembly.GetTypes())\\n                .ToArray();\\n\\n            var controllerTypes = typeof(My.Web.Startup).Assembly.GetTypes()\\n                .Where(myWebType =>\\n                    myWebType != typeof(Microsoft.AspNetCore.Mvc.Controller) &&\\n                    typeof(Microsoft.AspNetCore.Mvc.Controller).IsAssignableFrom(myWebType));\\n\\n            foreach (var controllerType in controllerTypes) {\\n                var allTheTypes = GetDependentTypes(controllerType, myConcreteTypes);\\n                allTheTypes.Count.Should().BeGreaterThan(0);\\n                var dependsUponTheDatabase = allTheTypes.Where(keyValue => keyValue.Key == typeof(MyDbContext));\\n                dependsUponTheDatabase.Any().Should().Be(false, because: $\\"{controllerType} depends upon the database through {string.Join(\\", \\", dependsUponTheDatabase.Select(dod => dod.Value))}\\");\\n            }\\n        }\\n\\n        private static Dictionary<Type, Type> GetDependentTypes(Type type, Type[] typesToCheck, Dictionary<Type, Type> typesSoFar = null) {\\n            var types = typesSoFar ?? new Dictionary<Type, Type>();\\n            foreach (var constructor in type.GetConstructors().Where(ctor => ctor.IsPublic)) {\\n                foreach (var parameter in constructor.GetParameters()) {\\n                    if (parameter.ParameterType.IsInterface) {\\n                        if (parameter.ParameterType.IsGenericType) {\\n                            foreach (var genericType in parameter.ParameterType.GenericTypeArguments) {\\n                                AddIfMissing(types, genericType, type);\\n                            }\\n                        } else {\\n                            var typesImplementingInterface = TypesImplementingInterface(parameter.ParameterType, typesToCheck);\\n                            foreach (var typeImplementingInterface in typesImplementingInterface) {\\n                                AddIfMissing(types, typeImplementingInterface, type);\\n                                AddIfMissing(types, GetDependentTypes(typeImplementingInterface, typesToCheck, types).Keys.ToList(), type);\\n                            }\\n                        }\\n                    } else {\\n                        AddIfMissing(types, parameter.ParameterType, type);\\n                        AddIfMissing(types, GetDependentTypes(parameter.ParameterType, typesToCheck, types).Keys.ToList(), type);\\n                    }\\n                }\\n            }\\n            return types;\\n        }\\n\\n        private static void AddIfMissing(Dictionary<Type, Type> types, Type typeToAdd, Type parentType) {\\n            if (!types.Keys.Contains(typeToAdd))\\n                types.Add(typeToAdd, parentType);\\n        }\\n\\n        private static void AddIfMissing(Dictionary<Type, Type> types, IList<Type> typesToAdd, Type parentType) {\\n            foreach (var typeToAdd in typesToAdd) {\\n                AddIfMissing(types, typeToAdd, parentType);\\n            }\\n        }\\n\\n        private static Type[] TypesImplementingInterface(Type interfaceType, Type[] typesToCheck) =>\\n            typesToCheck.Where(type => !type.IsInterface && interfaceType.IsAssignableFrom(type)).ToArray();\\n\\n        private static bool IsRealClass(Type testType) =>\\n            testType.IsAbstract == false &&\\n            testType.IsGenericType == false &&\\n            testType.IsGenericTypeDefinition == false &&\\n            testType.IsInterface == false;\\n\\n        private static Assembly[] GetMyAssemblies() =>\\n            AppDomain\\n            .CurrentDomain\\n            .GetAssemblies()\\n            // Not strictly necessary but it reduces the amount of types returned\\n            .Where(assembly => assembly.GetName().Name.StartsWith(\\"My\\"))\\n            .ToArray();\\n    }\\n}\\n```"},{"id":"its-not-dead-2-mobx-react-devtools-and-the-undead","metadata":{"permalink":"/its-not-dead-2-mobx-react-devtools-and-the-undead","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2018-03-26-its-not-dead-2-mobx-react-devtools-and-the-undead/index.md","source":"@site/blog/2018-03-26-its-not-dead-2-mobx-react-devtools-and-the-undead/index.md","title":"It\'s Not Dead 2: mobx-react-devtools and the undead","description":"Using `mobx-react-devtools` with `process.env.NODE_ENV` caused problems with webpack production mode. A different approach fixed the issue.","date":"2018-03-26T00:00:00.000Z","tags":[{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."}],"readingTime":2.04,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"its-not-dead-2-mobx-react-devtools-and-the-undead","title":"It\'s Not Dead 2: mobx-react-devtools and the undead","authors":"johnnyreilly","tags":["webpack"],"hide_table_of_contents":false,"description":"Using `mobx-react-devtools` with `process.env.NODE_ENV` caused problems with webpack production mode. A different approach fixed the issue."},"unlisted":false,"prevItem":{"title":"Using Reflection to Identify Unwanted Dependencies","permalink":"/using-reflection-to-identify-unwanted-dependencies"},"nextItem":{"title":"Uploading Images to Cloudinary with the Fetch API","permalink":"/uploading-images-to-cloudinary-with-fetch"}},"content":"I spent today digging through our webpack 4 config trying to work out why a production bundle contained code like this:\\n\\n```js\\nif(\\"production\\"!==e.env.NODE_ENV){//...\\n```\\n\\nMy expectation was that with webpack 4 and `\'mode\': \'production\'` this meant that behind the scenes all `process.env.NODE_ENV` statements should be converted to `\'production\'`. Subsequently Uglify would automatically get its groove on with the resulting `if(\\"production\\"!==\\"production\\") ...` and et voil\xe0!... Strip the dead code.\\n\\nIt seemed that was not the case. I was seeing (regrettably) undead code. And who here actually likes the undead?\\n\\n\x3c!--truncate--\x3e\\n\\n## Who Betrayed Me?\\n\\nMy beef was with webpack. It done did me wrong. Or... So I thought. webpack did nothing wrong. It is pure and good and unjustly complained about. It was my other love: [mobx](https://github.com/mobxjs/mobx). Or to be more specific: [mobx-react-devtools](https://github.com/mobxjs/mobx-react-devtools).\\n\\nIt turns out that the way you use `mobx-react-devtools` reliably makes the difference. It\'s the cause of the stray `(\\"production\\"!==e.env.NODE_ENV)` statements in our bundle output. After a **long** time I happened upon [this issue](https://github.com/mobxjs/mobx-react-devtools/issues/66#issuecomment-365151531) which contained a gem by one [Giles Butler](https://github.com/gilesbutler). His suggested way to reference `mobx-react-devtools` is (as far as I can tell) the solution!\\n\\nOn a dummy project I had the `mobx-react-devtools` advised code in place:\\n\\n```js\\nimport * as React from \'react\';\\nimport { Layout } from \'./components/layout\';\\nimport DevTools from \'mobx-react-devtools\';\\n\\nexport const App: React.SFC<{}> = (_props) => (\\n  <div className=\\"ui container\\">\\n    <Layout />\\n    {process.env.NODE_ENV !== \'production\' ? (\\n      <DevTools position={{ bottom: 20, right: 20 }} />\\n    ) : null}\\n  </div>\\n);\\n```\\n\\nWith this I had a build size of 311kb. Closer examination of my bundle revealed that my `bundle.js` was riddled with `(\\"production\\"!==e.env.NODE_ENV)` statements. Sucks, right?\\n\\nThen I tried this instead:\\n\\n```js\\nimport * as React from \'react\';\\nimport { Layout } from \'./components/layout\';\\nconst { Fragment } = React;\\n\\nconst DevTools =\\n  process.env.NODE_ENV !== \'production\'\\n    ? require(\'mobx-react-devtools\').default\\n    : Fragment;\\n\\nexport const App: React.SFC<{}> = (_props) => (\\n  <div className=\\"ui container\\">\\n    <Layout />\\n    <DevTools position={{ bottom: 20, right: 20 }} />\\n  </div>\\n);\\n```\\n\\nWith this approach I got a build size of 191kb. This was thanks to the dead code being actually stripped. That\'s a saving of 120kb!\\n\\n## Perhaps We Change the Advice?\\n\\nThere\'s a suggestion that the README should be changed to reflect this advice - until that happens, I wanted to share this solution. Also, I\'ve a nagging feeling that I\'ve missed something pertinent here; if someone knows something that I should... Tell me please!"},{"id":"uploading-images-to-cloudinary-with-fetch","metadata":{"permalink":"/uploading-images-to-cloudinary-with-fetch","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2018-03-25-uploading-images-to-cloudinary-with-fetch/index.md","source":"@site/blog/2018-03-25-uploading-images-to-cloudinary-with-fetch/index.md","title":"Uploading Images to Cloudinary with the Fetch API","description":"Learn how to handle image uploads to Cloudinary using Fetch instead of SuperAgent with a sample code demonstrating the replacement of FormData.","date":"2018-03-25T00:00:00.000Z","tags":[{"inline":false,"label":"React","permalink":"/tags/react","description":"The React library."}],"readingTime":1.03,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"uploading-images-to-cloudinary-with-fetch","title":"Uploading Images to Cloudinary with the Fetch API","authors":"johnnyreilly","tags":["react"],"hide_table_of_contents":false,"description":"Learn how to handle image uploads to Cloudinary using Fetch instead of SuperAgent with a sample code demonstrating the replacement of FormData."},"unlisted":false,"prevItem":{"title":"It\'s Not Dead 2: mobx-react-devtools and the undead","permalink":"/its-not-dead-2-mobx-react-devtools-and-the-undead"},"nextItem":{"title":"It\'s Not Dead: webpack and dead code elimination limitations","permalink":"/its-not-dead-webpack-and-dead-code"}},"content":"I was recently checking out a [very good post](https://css-tricks.com/image-upload-manipulation-react/) which explained how to upload images using [React Dropzone](https://github.com/react-dropzone/react-dropzone) and [SuperAgent](https://github.com/visionmedia/superagent) to [Cloudinary](https://cloudinary.com/).\\n\\n\x3c!--truncate--\x3e\\n\\nIt\'s a brilliant post; you should totally read it. Even if you hate images, uploads and JavaScript. However, there was one thing in there that I didn\'t want; SuperAgent. It\'s lovely but I\'m a [Fetch](https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API) guy. That\'s just how I roll. The question is, how do I do the below using Fetch?\\n\\n```js\\nhandleImageUpload(file) {\\n    let upload = request.post(CLOUDINARY_UPLOAD_URL)\\n                     .field(\'upload_preset\', CLOUDINARY_UPLOAD_PRESET)\\n                     .field(\'file\', file);\\n\\n    upload.end((err, response) => {\\n      if (err) {\\n        console.error(err);\\n      }\\n\\n      if (response.body.secure_url !== \'\') {\\n        this.setState({\\n          uploadedFileCloudinaryUrl: response.body.secure_url\\n        });\\n      }\\n    });\\n  }\\n```\\n\\nWell it actually took me longer to work out than I\'d like to admit. But now I have, let me save you the bother. To do the above using Fetch you just need this:\\n\\n```js\\nhandleImageUpload(file) {\\n    const formData = new FormData();\\n    formData.append(\\"file\\", file);\\n    formData.append(\\"upload_preset\\", CLOUDINARY_UPLOAD_PRESET); // Replace the preset name with your own\\n\\n    fetch(CLOUDINARY_UPLOAD_URL, {\\n      method: \'POST\',\\n      body: formData\\n    })\\n      .then(response => response.json())\\n      .then(data => {\\n        if (data.secure_url !== \'\') {\\n          this.setState({\\n            uploadedFileCloudinaryUrl: data.secure_url\\n          });\\n        }\\n      })\\n      .catch(err => console.error(err))\\n  }\\n```\\n\\nTo get a pre-canned project to try this with take a look at [Damon\'s repo](https://github.com/damonbauer/react-cloudinary)."},{"id":"its-not-dead-webpack-and-dead-code","metadata":{"permalink":"/its-not-dead-webpack-and-dead-code","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2018-03-07-its-not-dead-webpack-and-dead-code/index.md","source":"@site/blog/2018-03-07-its-not-dead-webpack-and-dead-code/index.md","title":"It\'s Not Dead: webpack and dead code elimination limitations","description":"webpack eliminates dead code through DefinePlugin. Directly use `process.env.NODE_ENV !== production` for smarter code elimination by UglifyJSPlugin.","date":"2018-03-07T00:00:00.000Z","tags":[{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."}],"readingTime":2.125,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"its-not-dead-webpack-and-dead-code","title":"It\'s Not Dead: webpack and dead code elimination limitations","authors":"johnnyreilly","tags":["webpack"],"hide_table_of_contents":false,"description":"webpack eliminates dead code through DefinePlugin. Directly use `process.env.NODE_ENV !== production` for smarter code elimination by UglifyJSPlugin."},"unlisted":false,"prevItem":{"title":"Uploading Images to Cloudinary with the Fetch API","permalink":"/uploading-images-to-cloudinary-with-fetch"},"nextItem":{"title":"ts-loader 4 / fork-ts-checker-webpack-plugin 0.4","permalink":"/ts-loader-400-fork-ts-checker-webpack"}},"content":"webpack has long supported the notion of dead code elimination. webpack facilitates this through use of the `DefinePlugin`. The compile time value of `process.env.NODE_ENV` is set either to `\'production\'` or something else. If it\'s set to `\'production\'` then some dead code hackery can happen. [Libraries like React make use of this to serve up different, and crucially smaller, production builds.](https://reactjs.org/docs/optimizing-performance.html#webpack)\\n\\n\x3c!--truncate--\x3e\\n\\nEvery now and then you can be surprised. Your assumptions turn out to be wrong.\\n\\nA (pre-webpack 4) production config file will typically contain this code:\\n\\n```js\\nnew webpack.DefinePlugin({\\n    \'process.env.NODE_ENV\': JSON.stringify(\'production\')\\n}),\\nnew UglifyJSPlugin(),\\n```\\n\\nThe result of the above config is that webpack will inject the value \'production\' everywhere in the codebase where a `process.env.NODE_ENV` can be found. (In fact, as of webpack 4 setting this magic value is out-of-the-box behaviour for Production mode; yay the #0CJS!)\\n\\nWhat this means is, if you\'ve written:\\n\\n```js\\nif (process.env.NODE_ENV !== \'production\') {\\n  // Do a development mode only thing\\n}\\n```\\n\\nwebpack can and will turn this into\\n\\n```js\\nif (\'production\' !== \'production\') {\\n  // Do a development mode only thing\\n}\\n```\\n\\nThe [UglifyJSPlugin](https://github.com/webpack-contrib/uglifyjs-webpack-plugin) is there to minify the JavaScript in your bundles. As an added benefit, this plugin is smart enough to know that `\'production\' !== \'production\'` is always `false`. And because it\'s smart, it chops the code. Dead code elimated.\\n\\nYou can read more about this [in the webpack docs](https://webpack.js.org/guides/production/#specify-the-environment).\\n\\n## Limitations\\n\\nGiven what I\'ve said, consider the following code:\\n\\n```js\\nexport class Config {\\n  // Other properties\\n\\n  get isDevelopment() {\\n    return process.env.NODE_ENV !== \'production\';\\n  }\\n}\\n```\\n\\nThis is a config class that exposes the expression `process.env.NODE_ENV !== \'production\'` with the friendly name `isDevelopment`. You\'d think that dead code elimination would be your friend here. It\'s not.\\n\\nMy personal expection was that dead code elimination would treat `Config.isDevelopment` and the expression `process.env.NODE_ENV !== \'production\'` identically. Because they\'re identical.\\n\\nHowever, this turns out not to be the case. Dead code elimination works just as you would hope when using the expression `process.env.NODE_ENV !== \'production\'` directly in code. However webpack **only** performs dead code elimination for the **direct** usage of the `process.env.NODE_ENV !== \'production\'` expression. I\'ll say that again: if you want dead code elimination then use the injected values; not an encapsulated version of them. It turns out you cannot rely on webpack flowing values through and performing dead code elimination on that basis.\\n\\nThe TL;DR: if you want to elimate dead code then \\\\*always\\\\* use `process.env.NODE_ENV !== \'production\'`; don\'t abstract it. It doesn\'t work.\\n\\nUglifyJS is smart. But not that smart."},{"id":"ts-loader-400-fork-ts-checker-webpack","metadata":{"permalink":"/ts-loader-400-fork-ts-checker-webpack","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2018-02-25-ts-loader-400-fork-ts-checker-webpack/index.md","source":"@site/blog/2018-02-25-ts-loader-400-fork-ts-checker-webpack/index.md","title":"ts-loader 4 / fork-ts-checker-webpack-plugin 0.4","description":"webpack 4 has been released, along with updates for ts-loader and fork-ts-checker-webpack-plugin. See links for details and examples.","date":"2018-02-25T00:00:00.000Z","tags":[{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."},{"inline":false,"label":"fork-ts-checker-webpack-plugin","permalink":"/tags/fork-ts-checker-webpack-plugin","description":"The fork-ts-checker-webpack-plugin for webpack."},{"inline":false,"label":"ts-loader","permalink":"/tags/ts-loader","description":"The TypeScript loader for webpack."}],"readingTime":0.58,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"ts-loader-400-fork-ts-checker-webpack","title":"ts-loader 4 / fork-ts-checker-webpack-plugin 0.4","authors":"johnnyreilly","tags":["webpack","fork-ts-checker-webpack-plugin","ts-loader"],"hide_table_of_contents":false,"description":"webpack 4 has been released, along with updates for ts-loader and fork-ts-checker-webpack-plugin. See links for details and examples."},"unlisted":false,"prevItem":{"title":"It\'s Not Dead: webpack and dead code elimination limitations","permalink":"/its-not-dead-webpack-and-dead-code"},"nextItem":{"title":"Finding webpack 4 (use a Map)","permalink":"/finding-webpack-4-use-map"}},"content":"webpack 4 has shipped!\\n\\n\x3c!--truncate--\x3e\\n\\n## `ts-loader`\\n\\nThe [`ts-loader`](https://github.com/TypeStrong/ts-loader) 4 is available too. For details see our release [here](https://github.com/TypeStrong/ts-loader/releases/tag/v4.0.0). To start using `ts-loader` 4:\\n\\n- When using `yarn`: `yarn add ts-loader@4.1.0 -D`\\n- When using `npm`: `npm install ts-loader@4.1.0 -D`\\n\\nRemember to use this in concert with the webpack 4. To see a working example take a look at [the \\"vanilla\\" example](https://github.com/johnnyreilly/ts-loader/tree/master/examples/vanilla).\\n\\n## `fork-ts-checker-webpack-plugin`\\n\\nThere\'s more! You may like to use the [`fork-ts-checker-webpack-plugin`](https://github.com/Realytics/fork-ts-checker-webpack-plugin), (aka the ts-loader turbo-booster). The webpack compatible version has been [released to npm as 0.4.1](https://github.com/Realytics/fork-ts-checker-webpack-plugin/releases/tag/v0.4.1):\\n\\n- When using `yarn`: `yarn add fork-ts-checker-webpack-plugin@0.4.1 -D`\\n- When using `npm`: `npm install fork-ts-checker-webpack-plugin@0.4.1 -D`\\n\\nTo see a working example take a look at [the \\"fork-ts-checker\\" example](https://github.com/johnnyreilly/ts-loader/tree/master/examples/fork-ts-checker)."},{"id":"finding-webpack-4-use-map","metadata":{"permalink":"/finding-webpack-4-use-map","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2018-01-29-finding-webpack-4-use-map/index.md","source":"@site/blog/2018-01-29-finding-webpack-4-use-map/index.md","title":"Finding webpack 4 (use a Map)","description":"webpack 4s new plugin architecture requires migrating from \\"kebab-case\\" to \\"camelCase\\". A migration guide for plugins and loaders is available.","date":"2018-01-29T00:00:00.000Z","tags":[{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."}],"readingTime":4.55,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"finding-webpack-4-use-map","title":"Finding webpack 4 (use a Map)","authors":"johnnyreilly","tags":["webpack"],"hide_table_of_contents":false,"description":"webpack 4s new plugin architecture requires migrating from \\"kebab-case\\" to \\"camelCase\\". A migration guide for plugins and loaders is available."},"unlisted":false,"prevItem":{"title":"ts-loader 4 / fork-ts-checker-webpack-plugin 0.4","permalink":"/ts-loader-400-fork-ts-checker-webpack"},"nextItem":{"title":"webpack 4 - ts-loader / fork-ts-checker-webpack-plugin betas","permalink":"/webpack-4-ts-loader-fork-ts-checker"}},"content":"## Update: 03/02/2018\\n\\n\x3c!--truncate--\x3e\\n\\nTobias Koppers has written a migration guide for plugins / loaders as well - take a read [here](https://medium.com/webpack/webpack-4-migration-guide-for-plugins-loaders-20a79b927202). It\'s very useful.\\n\\n## webpack 4\\n\\nwebpack 4 is on the horizon. [The beta dropped last Friday](https://medium.com/webpack/webpack-4-beta-try-it-today-6b1d27d7d7e2). So what do you, as a plugin / loader author need to do? What needs to change to make your loader / plugin webpack 4 friendly?\\n\\nThis is a guide that should inform you about the changes you might need to make. It\'s based on my own experiences migrating [`ts-loader`](https://github.com/TypeStrong/ts-loader) and the [`fork-ts-checker-webpack-plugin`](https://github.com/Realytics/fork-ts-checker-webpack-plugin). If you\'d like to see this in action then take a look at the PRs related to these. The ts-loader PR can be found [here](https://github.com/TypeStrong/ts-loader/pull/710). The fork-ts-checker-webpack-plugin PR can be found [here](https://github.com/Realytics/fork-ts-checker-webpack-plugin/pull/93).\\n\\n## Plugins\\n\\nOne of the notable changes to webpack with v4 is the change to the plugin architecture. In terms of implications it\'s worth reading the comments made by [Tobias Koppers](https://twitter.com/wsokra)[here](https://github.com/webpack/webpack/issues/6244#issuecomment-357502113) and [here](https://github.com/webpack/webpack/issues/6064#issuecomment-349405474).\\n\\nPreviously, if your plugin was tapping into a compiler hook you\'d write code that looked something like this:\\n\\n```js\\nthis.compiler.plugin(\'watch-close\', () => {\\n  // do your thing here\\n});\\n```\\n\\nWith webpack 4 things done changed. You\'d now write something like this:\\n\\n```js\\nthis.compiler.hooks.watchClose.tap(\\n  \'name-to-identify-your-plugin-goes-here\',\\n  () => {\\n    // do your thing here\\n  },\\n);\\n```\\n\\nHopefully that\'s fairly clear; we\'re using the new `hooks` property and tapping into our event of choice by `camelCasing` what was previously `kebab-cased`. So in this case `plugin(\'watch-close\' =&gt; hooks.watchClose.tap`.\\n\\nIn the example above we were attaching to a sync hook. Now let\'s look at an async hook:\\n\\n```js\\nthis.compiler.plugin(\'watch-run\', (watching, callback) => {\\n  // do your thing here\\n  callback();\\n});\\n```\\n\\nThis would change to be:\\n\\n```js\\nthis.compiler.hooks.watchRun.tapAsync(\\n  \'name-to-identify-your-plugin-goes-here\',\\n  (compiler, callback) => {\\n    // do your thing here\\n    callback();\\n  },\\n);\\n```\\n\\nNote that rather than using `tap` here, we\'re using `tapAsync`. If you\'re more into promises there\'s a `tapPromise` you could use instead.\\n\\n## Custom Hooks\\n\\nPrior to webpack 4, you could use your own custom hooks within your plugin. Usage was as simple as this:\\n\\n```js\\nthis.compiler.applyPluginsAsync(\'fork-ts-checker-service-before-start\', () => {\\n  // do your thing here\\n});\\n```\\n\\nYou can still use custom hooks with webpack 4, but there\'s a little more ceremony involved. Essentially, you need to tell webpack up front what you\'re planning. Not hard, I promise you.\\n\\nFirst of all, you\'ll need to add the package [`tapable`](https://www.npmjs.com/package/tapable) as a dependency. Then, inside your plugin you\'ll need to import the type of hook that you want to use; in the case of the `fork-ts-checker-webpack-plugin` we used both a sync and an async hook:\\n\\n```js\\nconst AsyncSeriesHook = require(\'tapable\').AsyncSeriesHook;\\nconst SyncHook = require(\'tapable\').SyncHook;\\n```\\n\\nThen, inside your `apply` method you need to register your hooks:\\n\\n```js\\nif (\\n  this.compiler.hooks.forkTsCheckerServiceBeforeStart ||\\n  this.compiler.hooks.forkTsCheckerCancel ||\\n  // other hooks...\\n  this.compiler.hooks.forkTsCheckerEmit\\n) {\\n  throw new Error(\'fork-ts-checker-webpack-plugin hooks are already in use\');\\n}\\nthis.compiler.hooks.forkTsCheckerServiceBeforeStart = new AsyncSeriesHook([]);\\n\\nthis.compiler.hooks.forkTsCheckerCancel = new SyncHook([]);\\n// other sync hooks...\\nthis.compiler.hooks.forkTsCheckerDone = new SyncHook([]);\\n```\\n\\nIf you\'re interested in backwards compatibility then you should use the `_pluginCompat` to wire that in:\\n\\n```js\\nthis.compiler._pluginCompat.tap(\'fork-ts-checker-webpack-plugin\', (options) => {\\n  switch (options.name) {\\n    case \'fork-ts-checker-service-before-start\':\\n      options.async = true;\\n      break;\\n    case \'fork-ts-checker-cancel\':\\n    // other sync hooks...\\n    case \'fork-ts-checker-done\':\\n      return true;\\n  }\\n  return undefined;\\n});\\n```\\n\\nWith your registration in place, you just need to replace your calls to `compiler.applyPlugins(\'sync-hook-name\', ` and `compiler.applyPluginsAsync(\'async-hook-name\', ` with calls to `compiler.hooks.syncHookName.call(` and `compiler.hooks.asyncHookName.callAsync(`. So to migrate our `fork-ts-checker-service-before-start` hook we\'d write:\\n\\n```js\\nthis.compiler.hooks.forkTsCheckerServiceBeforeStart.callAsync(() => {\\n  // do your thing here\\n});\\n```\\n\\n## Loaders\\n\\nLoaders are impacted by the changes to the plugin architecture. Mostly this means applying the same plugin changes as discussed above. `ts-loader` hooks into 2 plugin events:\\n\\n```js\\nloader._compiler.plugin(\'after-compile\' /* callback goes here */);\\nloader._compiler.plugin(\'watch-run\' /* callback goes here */);\\n```\\n\\nWith webpack 4 these become:\\n\\n```js\\nloader._compiler.hooks.afterCompile.tapAsync(\\n  \'ts-loader\' /* callback goes here */,\\n);\\nloader._compiler.hooks.watchRun.tapAsync(\'ts-loader\' /* callback goes here */);\\n```\\n\\nNote again, we\'re using the string `\\"ts-loader\\"` to identify our loader.\\n\\n## I need a `Map`\\n\\nWhen I initially ported to webpack 4, `ts-loader` simply wasn\'t working. In the end I tied this down to problems in our `watch-run` callback. There\'s 2 things of note here.\\n\\nFirstly, as per [the changelog](https://github.com/webpack/webpack/releases/tag/v4.0.0-beta.0), the `watch-run` hook now has the `Compiler` as the first parameter. Previously this was a subproperty on the supplied `watching` parameter. So swapping over to use the compiler directly was necessary. Incidentally, `ts-loader` previously made use of the `watching.startTime` property that was supplied in webpack\'s 1, 2 and 3. It seems to be coping without it; so hopefully that\'s fine.\\n\\nSecondly, with webpack 4 it\'s \\"ES2015 all the things!\\" That is to say, with webpack now requiring a minimum of node 6, the codebase is free to start using ES2015. So if you\'re a consumer of `compiler.fileTimestamps` (and `ts-loader` is) then it\'s time to make a change to cater for the different API that a `Map` offers instead of indexing into an object literal with a `string` key.\\n\\nWhat this means is, code that would once have looked like this:\\n\\n```js\\nObject.keys(watching.compiler.fileTimestamps)\\n  .filter(\\n    (filePath) =>\\n      watching.compiler.fileTimestamps[filePath] > lastTimes[filePath],\\n  )\\n  .forEach((filePath) => {\\n    lastTimes[filePath] = times[filePath];\\n    // ...\\n  });\\n```\\n\\nNow looks more like this:\\n\\n```js\\nfor (const [filePath, date] of compiler.fileTimestamps) {\\n  if (date > lastTimes.get(filePath)) {\\n    continue;\\n  }\\n\\n  lastTimes.set(filePath, date);\\n  // ...\\n}\\n```\\n\\n## Happy Porting!\\n\\nI hope your own port to webpack 4 goes well. Do let me know if there\'s anything I\'ve missed out / any inaccuracies etc and I\'ll update this guide."},{"id":"webpack-4-ts-loader-fork-ts-checker","metadata":{"permalink":"/webpack-4-ts-loader-fork-ts-checker","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2018-01-28-webpack-4-ts-loader-fork-ts-checker/index.md","source":"@site/blog/2018-01-28-webpack-4-ts-loader-fork-ts-checker/index.md","title":"webpack 4 - ts-loader / fork-ts-checker-webpack-plugin betas","description":"The TypeScript ts-loader beta to work with webpack 4 is now available, along with the fork-ts-checker-webpack-plugin, which complements ts-loader.","date":"2018-01-28T00:00:00.000Z","tags":[{"inline":false,"label":"fork-ts-checker-webpack-plugin","permalink":"/tags/fork-ts-checker-webpack-plugin","description":"The fork-ts-checker-webpack-plugin for webpack."},{"inline":false,"label":"ts-loader","permalink":"/tags/ts-loader","description":"The TypeScript loader for webpack."},{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."}],"readingTime":0.99,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"webpack-4-ts-loader-fork-ts-checker","title":"webpack 4 - ts-loader / fork-ts-checker-webpack-plugin betas","authors":"johnnyreilly","tags":["fork-ts-checker-webpack-plugin","ts-loader","webpack"],"hide_table_of_contents":false,"description":"The TypeScript ts-loader beta to work with webpack 4 is now available, along with the fork-ts-checker-webpack-plugin, which complements ts-loader."},"unlisted":false,"prevItem":{"title":"Finding webpack 4 (use a Map)","permalink":"/finding-webpack-4-use-map"},"nextItem":{"title":"Auth0, TypeScript and ASP.NET Core","permalink":"/auth0-typescript-and-aspnet-core"}},"content":"[The first webpack 4 beta dropped on Friday](https://medium.com/webpack/webpack-4-beta-try-it-today-6b1d27d7d7e2). Very exciting! Following hot on the heels of those announcements, I\'ve some news to share too. Can you guess what it is?\\n\\n\x3c!--truncate--\x3e\\n\\n## `ts-loader`\\n\\nYes! The [`ts-loader`](https://github.com/TypeStrong/ts-loader) beta to work with webpack 4 is available. To get hold of the beta:\\n\\n- When using `yarn`: `yarn add ts-loader@4.0.0-beta.0 -D`\\n- When using `npm`: `npm install ts-loader@4.0.0-beta.0 -D`\\n\\nRemember to use this in concert with the webpack 4 beta. To see a working example take a look at [the \\"vanilla\\" example](https://github.com/johnnyreilly/ts-loader/tree/master/examples/vanilla).\\n\\n## `fork-ts-checker-webpack-plugin`\\n\\nThere\'s more! You may like to use the [`fork-ts-checker-webpack-plugin`](https://github.com/Realytics/fork-ts-checker-webpack-plugin), (which goes lovely with `ts-loader` and a biscuit). There is a beta available for that too:\\n\\n- When using `yarn`: `yarn add johnnyreilly/fork-ts-checker-webpack-plugin#4.0.0-beta.1 -D`\\n- When using `npm`: `npm install johnnyreilly/fork-ts-checker-webpack-plugin#4.0.0-beta.1 -D`\\n\\nTo see a working example take a look at [the \\"fork-ts-checker\\" example](https://github.com/johnnyreilly/ts-loader/tree/master/examples/fork-ts-checker).\\n\\n## PRs\\n\\nIf you would like to track the progress of these betas then I encourage you to take a look at the PRs they were built from. The ts-loader PR can be found [here](https://github.com/TypeStrong/ts-loader/pull/710). The fork-ts-checker-webpack-plugin PR can be found [here](https://github.com/Realytics/fork-ts-checker-webpack-plugin/pull/93).\\n\\nThese are betas so things may change further; though hopefully not significantly."},{"id":"auth0-typescript-and-aspnet-core","metadata":{"permalink":"/auth0-typescript-and-aspnet-core","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2018-01-14-auth0-typescript-and-aspnet-core/index.md","source":"@site/blog/2018-01-14-auth0-typescript-and-aspnet-core/index.md","title":"Auth0, TypeScript and ASP.NET Core","description":"Auth0 makes authentication and authorization simple. They offer Auth-As-A-Service, quick start and easy customization of settings.","date":"2018-01-14T00:00:00.000Z","tags":[{"inline":false,"label":"React","permalink":"/tags/react","description":"The React library."},{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."},{"inline":false,"label":"Authentication and Authorization","permalink":"/tags/auth","description":"Matters related to identity and permissioning"},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":9.375,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"auth0-typescript-and-aspnet-core","title":"Auth0, TypeScript and ASP.NET Core","authors":"johnnyreilly","tags":["react","asp.net","auth","typescript"],"hide_table_of_contents":false,"description":"Auth0 makes authentication and authorization simple. They offer Auth-As-A-Service, quick start and easy customization of settings."},"unlisted":false,"prevItem":{"title":"webpack 4 - ts-loader / fork-ts-checker-webpack-plugin betas","permalink":"/webpack-4-ts-loader-fork-ts-checker"},"nextItem":{"title":"ts-loader 2017 retrospective","permalink":"/ts-loader-2017-retrospective"}},"content":"Most applications I write have some need for authentication and perhaps authorisation too. In fact, most apps most people write fall into that bracket. Here\'s the thing: Auth done well is a \\\\*big\\\\* chunk of work. And the minute you start thinking about that you almost invariably lose focus on the thing you actually want to build and ship.\\n\\n\x3c!--truncate--\x3e\\n\\nSo this Christmas I decided it was time to take a look into offloading that particular problem onto someone else. I knew there were third parties who provided Auth-As-A-Service - time to give them a whirl. On the recommendation of a friend, I made Auth0 my first port of call. Lest you be expecting a full breakdown of the various players in this space, let me stop you now; I liked Auth0 so much I strayed no further. Auth0 kicks AAAS. (I\'m so sorry)\\n\\n## What I wanted to build\\n\\nMy criteria for \\"auth success\\" was this:\\n\\n- I want to build a SPA, specifically a React SPA. Ideally, I shouldn\'t need a back end of my own at all\\n- I want to use TypeScript on my client.\\n\\nBut, for when I do implement a back end:\\n\\n- I want that to be able to use the client side\'s Auth tokens to allow access to Auth routes on my server.\\n- \u200EI want to able to identify the user, given the token, to provide targeted data\\n- Oh, and I want to use .NET Core 2 for my server.\\n\\nAnd in achieving all of the I want to add minimal code to my app. Not War and Peace. My code should remain focused on doing what it does.\\n\\n## Boil a Plate\\n\\nI ended up with unqualified ticks for all my criteria, but it took some work to find out. I will say that Auth0 do travel the extra mile in terms of getting you up and running. When you create a new Client in Auth0 you\'re given the option to download a quick start using the technology of your choice.\\n\\nThis was a massive plus for me. I took the quickstart provided and ran with it to get me to the point of meeting my own criteria. You can use this boilerplate for your own ends. Herewith, a walkthrough:\\n\\n## The Walkthrough\\n\\nFork and clone the repo at this location: [https://github.com/johnnyreilly/auth0-react-typescript-asp-net-core](https://github.com/johnnyreilly/auth0-react-typescript-asp-net-core).\\n\\nWhat have we got? 2 folders, ClientApp contains the React app, Web contains the ASP.NET Core app. Now we need to get setup with Auth0 and customise our config.\\n\\n## Setup Auth0\\n\\nHere\'s how to get the app set up with Auth0; you\'re going to need to sign up for a (free) Auth0 account. Then login into Auth0 and go to the management portal.\\n\\n### Client\\n\\n- Create a Client with the name of your choice and use the Single Page Web Applications template.\\n- From the new Client Settings page take the Domain and Client ID and update the similarly named properties in the `appsettings.Development.json` and `appsettings.Production.json` files with these settings.\\n- To the Allowed Callback URLs setting add the URLs: `http://localhost:3000/callback,http://localhost:5000/callback` \\\\- the first of these faciliates running in Debug mode, the second in Production mode. If you were to deploy this you\'d need to add other callback URLs in here too.\\n\\n### API\\n\\n- Create an API with the name of your choice (I recommend the same as the Client to avoid confusion), an identifier which can be anything you like; I like to use the URL of my app but it\'s your call.\\n- From the new API Settings page take the Identifier and update the Audience property in the `appsettings.Development.json` and `appsettings.Production.json` files with that value.\\n\\n## Running the App\\n\\n### Production build\\n\\nBuild the client app with `yarn build` in the `ClientApp` folder. (Don\'t forget to `yarn install` first.) Then, in the `Web` folder `dotnet restore`, `dotnet run` and open your browser to [`http://localhost:5000`](http://localhost:5000)\\n\\n### Debugging\\n\\nRun the client app using webpack-dev-server using `yarn start` in the `ClientApp` folder. Fire up VS Code in the root of the repo and hit F5 to debug the server. Then open your browser to [`http://localhost:3000`](http://localhost:3000)\\n\\n## The Tour\\n\\nWhen you fire up the app you\'re presented with \\"you are not logged in!\\" message and the option to login. Do it, it\'ll take you to the Auth0 \\"lock\\" screen where you can sign up / login. Once you do that you\'ll be asked to confirm access:\\n\\n![](Screenshot-2018-01-13-18.40.21.webp)\\n\\nAll this is powered by Auth0\'s [auth0-js](https://www.npmjs.com/package/auth0-js) npm package. (Excellent type definition files are available from Definitely Typed; I\'m using the [@types/auth0-js](https://www.npmjs.com/package/@types/auth0-js) package DT publishes.) Usage of which is super simple; it exposes an `authorize` method that when called triggers the Auth0 lock screen. Once you\'ve \\"okayed\\" you\'ll be taken back to the app which will use the `parseHash` method to extract the access token that Auth0 has provided. Take a look at how our `authStore` makes use of auth0-js: (don\'t be scared; it uses mobx - but you could use anything)\\n\\n### authStore.ts\\n\\n```ts\\nimport { Auth0UserProfile, WebAuth } from \'auth0-js\';\\nimport { action, computed, observable, runInAction } from \'mobx\';\\nimport { IAuth0Config } from \'../../config\';\\nimport { StorageFacade } from \'../storageFacade\';\\n\\ninterface IStorageToken {\\n  accessToken: string;\\n  idToken: string;\\n  expiresAt: number;\\n}\\n\\nconst STORAGE_TOKEN = \'storage_token\';\\n\\nexport class AuthStore {\\n  @observable.ref auth0: WebAuth;\\n  @observable.ref userProfile: Auth0UserProfile;\\n  @observable.ref token: IStorageToken;\\n\\n  constructor(\\n    config: IAuth0Config,\\n    private storage: StorageFacade,\\n  ) {\\n    this.auth0 = new WebAuth({\\n      domain: config.domain,\\n      clientID: config.clientId,\\n      redirectUri: config.redirectUri,\\n      audience: config.audience,\\n      responseType: \'token id_token\',\\n      scope: \'openid email profile do:admin:thing\', // the do:admin:thing scope is custom and defined in the scopes section of our API in the Auth0 dashboard\\n    });\\n  }\\n\\n  initialise() {\\n    const token = this.parseToken(this.storage.getItem(STORAGE_TOKEN));\\n    if (token) {\\n      this.setSession(token);\\n    }\\n    this.storage.addEventListener(this.onStorageChanged);\\n  }\\n\\n  parseToken(tokenString: string) {\\n    const token = JSON.parse(tokenString || \'{}\');\\n    return token;\\n  }\\n\\n  onStorageChanged = (event: StorageEvent) => {\\n    if (event.key === STORAGE_TOKEN) {\\n      this.setSession(this.parseToken(event.newValue));\\n    }\\n  };\\n\\n  @computed get isAuthenticated() {\\n    // Check whether the current time is past the\\n    // access token\'s expiry time\\n    return this.token && new Date().getTime() < this.token.expiresAt;\\n  }\\n\\n  login = () => {\\n    this.auth0.authorize();\\n  };\\n\\n  handleAuthentication = () => {\\n    this.auth0.parseHash((err, authResult) => {\\n      if (authResult && authResult.accessToken && authResult.idToken) {\\n        const token = {\\n          accessToken: authResult.accessToken,\\n          idToken: authResult.idToken,\\n          // Set the time that the access token will expire at\\n          expiresAt: authResult.expiresIn * 1000 + new Date().getTime(),\\n        };\\n\\n        this.setSession(token);\\n      } else if (err) {\\n        // tslint:disable-next-line:no-console\\n        console.log(err);\\n        alert(`Error: ${err.error}. Check the console for further details.`);\\n      }\\n    });\\n  };\\n\\n  @action\\n  setSession(token: IStorageToken) {\\n    this.token = token;\\n    this.storage.setItem(STORAGE_TOKEN, JSON.stringify(token));\\n  }\\n\\n  getAccessToken = () => {\\n    const accessToken = this.token.accessToken;\\n    if (!accessToken) {\\n      throw new Error(\'No access token found\');\\n    }\\n    return accessToken;\\n  };\\n\\n  @action\\n  loadProfile = async () => {\\n    const accessToken = this.token.accessToken;\\n    if (!accessToken) {\\n      return;\\n    }\\n\\n    this.auth0.client.userInfo(accessToken, (err, profile) => {\\n      if (err) {\\n        throw err;\\n      }\\n\\n      if (profile) {\\n        runInAction(() => (this.userProfile = profile));\\n        return profile;\\n      }\\n\\n      return undefined;\\n    });\\n  };\\n\\n  @action\\n  logout = () => {\\n    // Clear access token and ID token from local storage\\n    this.storage.removeItem(STORAGE_TOKEN);\\n\\n    this.token = null;\\n    this.userProfile = null;\\n  };\\n}\\n```\\n\\nOnce you\'re logged in the app offers you more in the way of navigation options. A \\"Profile\\" screen shows you the details your React app has retrieved from Auth0 about you. This is backed by the `client.userInfo` method on `auth0-js`. There\'s also a \\"Ping\\" screen which is where your React app talks to your ASP.NET Core server. The screenshot below illustrates the result of hitting the \\"Get Private Data\\" button:\\n\\n![](Screenshot-2018-01-13-18.47.49.webp)\\n\\nThe \\"Get Server to Retrieve Profile Data\\" button is interesting as it illustrates that the server can get access to your profile data as well. There\'s nothing insecure here; it gets the details using the access token retrieved from Auth0 by the ClientApp and passed to the server. It\'s the API we set up in Auth0 that is in play here. The app uses the Domain and the access token to talk to Auth0 like so:\\n\\n### UserController.cs\\n\\n```cs\\n// Retrieve the access_token claim which we saved in the OnTokenValidated event\\n    var accessToken = User.Claims.FirstOrDefault(c => c.Type == \\"access_token\\").Value;\\n\\n    // If we have an access_token, then retrieve the user\'s information\\n    if (!string.IsNullOrEmpty(accessToken))\\n    {\\n        var domain = _config[\\"Auth0:Domain\\"];\\n        var apiClient = new AuthenticationApiClient(domain);\\n        var userInfo = await apiClient.GetUserInfoAsync(accessToken);\\n\\n        return Ok(userInfo);\\n    }\\n```\\n\\nWe can also access the `sub` claim, which uniquely identifies the user:\\n\\n### UserController.cs\\n\\n```cs\\n// We\'re not doing anything with this, but hey! It\'s useful to know where the user id lives\\n    var userId = User.Claims.FirstOrDefault(c => c.Type == System.Security.Claims.ClaimTypes.NameIdentifier).Value; // our userId is the sub value\\n```\\n\\nThe reason our ASP.NET Core app works with Auth0 and that we have access to the access token here in the first place is because of our startup code:\\n\\n### Startup.cs\\n\\n```cs\\npublic void ConfigureServices(IServiceCollection services)\\n    {\\n        var domain = $\\"https://{Configuration[\\"Auth0:Domain\\"]}/\\";\\n        services.AddAuthentication(options =>\\n        {\\n            options.DefaultAuthenticateScheme = JwtBearerDefaults.AuthenticationScheme;\\n            options.DefaultChallengeScheme = JwtBearerDefaults.AuthenticationScheme;\\n        }).AddJwtBearer(options =>\\n        {\\n            options.Authority = domain;\\n            options.Audience = Configuration[\\"Auth0:Audience\\"];\\n            options.Events = new JwtBearerEvents\\n            {\\n                OnTokenValidated = context =>\\n                {\\n                    if (context.SecurityToken is JwtSecurityToken token)\\n                    {\\n                        if (context.Principal.Identity is ClaimsIdentity identity)\\n                        {\\n                            identity.AddClaim(new Claim(\\"access_token\\", token.RawData));\\n                        }\\n                    }\\n\\n                    return Task.FromResult(0);\\n                }\\n            };\\n        });\\n\\n        // ....\\n```\\n\\n## Authorization\\n\\nWe\'re pretty much done now; just one magic button to investigate: \\"Get Admin Data\\". If you presently try and access the admin data you\'ll get a `403 Forbidden`. It\'s forbidden because that endpoint relies on the `\\"do:admin:thing\\"` scope in our claims:\\n\\n### UserController.cs\\n\\n```cs\\n[Authorize(Scopes.DoAdminThing)]\\n    [HttpGet(\\"api/userDoAdminThing\\")]\\n    public IActionResult GetUserDoAdminThing()\\n    {\\n        return Ok(\\"Admin endpoint\\");\\n    }\\n```\\n\\n### Scopes.cs\\n\\n```cs\\npublic static class Scopes\\n    {\\n         // the do:admin:thing scope is custom and defined in the scopes section of our API in the Auth0 dashboard\\n        public const string DoAdminThing = \\"do:admin:thing\\";\\n    }\\n```\\n\\nThis wired up in our ASP.NET Core app like so:\\n\\n### Startup.cs\\n\\n```cs\\nservices.AddAuthorization(options =>\\n    {\\n        options.AddPolicy(Scopes.DoAdminThing, policy => policy.Requirements.Add(new HasScopeRequirement(Scopes.DoAdminThing, domain)));\\n    });\\n\\n    // register the scope authorization handler\\n    services.AddSingleton<iauthorizationhandler, hasscopehandler=\\"\\">();\\n</iauthorizationhandler,>\\n```\\n\\n### HasScopeHandler.cs\\n\\n```cs\\npublic class HasScopeHandler : AuthorizationHandler<hasscoperequirement>\\n    {\\n        protected override Task HandleRequirementAsync(AuthorizationHandlerContext context, HasScopeRequirement requirement)\\n        {\\n            // If user does not have the scope claim, get out of here\\n            if (!context.User.HasClaim(c => c.Type == \\"scope\\" && c.Issuer == requirement.Issuer))\\n                return Task.CompletedTask;\\n\\n            // Split the scopes string into an array\\n            var scopes = context.User.FindFirst(c => c.Type == \\"scope\\" && c.Issuer == requirement.Issuer).Value.Split(\' \');\\n\\n            // Succeed if the scope array contains the required scope\\n            if (scopes.Any(s => s == requirement.Scope))\\n                context.Succeed(requirement);\\n\\n            return Task.CompletedTask;\\n        }\\n    }\\n</hasscoperequirement>\\n```\\n\\nThe reason we\'re 403ing at present is because when our `HasScopeHandler` executes, `requirement.Scope` has the value of `\\"do:admin:thing\\"` and our `scopes` do not contain that value. To add it, go to your API in the Auth0 management console and add it:\\n\\n![](Screenshot-2018-01-14-08.26.54.webp)\\n\\nNote that you can control how this scope is acquired using \\"Rules\\" in the Auth0 management portal.\\n\\nYou won\'t be able to access the admin endpoint yet because you\'re still rocking with the old access token; pre-newly-added scope. But when you next login to Auth0 you\'ll see a prompt like this:\\n\\n![](Screenshot-2018-01-14-08.32.59.webp)\\n\\nWhich demonstrates that you\'re being granted an extra scope. With your new shiny access token you can now access the oh-so-secret Admin endpoint.\\n\\nI had some more questions about Auth0 as I\'m still new to it myself. To see my question (and the very helpful answer!) go here: [https://community.auth0.com/questions/13786/get-user-data-server-side-what-is-a-good-approach](https://community.auth0.com/questions/13786/get-user-data-server-side-what-is-a-good-approach)"},{"id":"ts-loader-2017-retrospective","metadata":{"permalink":"/ts-loader-2017-retrospective","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2017-12-24-ts-loader-2017-retrospective/index.md","source":"@site/blog/2017-12-24-ts-loader-2017-retrospective/index.md","title":"ts-loader 2017 retrospective","description":"ts-loader has improved in 2017, now sitting at v3.2.0 and supporting webpack 2 and 3. Future plans include using the new watch API.","date":"2017-12-24T00:00:00.000Z","tags":[{"inline":false,"label":"ts-loader","permalink":"/tags/ts-loader","description":"The TypeScript loader for webpack."},{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":3.4,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"ts-loader-2017-retrospective","title":"ts-loader 2017 retrospective","authors":"johnnyreilly","tags":["ts-loader","webpack","typescript"],"hide_table_of_contents":false,"description":"ts-loader has improved in 2017, now sitting at v3.2.0 and supporting webpack 2 and 3. Future plans include using the new watch API."},"unlisted":false,"prevItem":{"title":"Auth0, TypeScript and ASP.NET Core","permalink":"/auth0-typescript-and-aspnet-core"},"nextItem":{"title":"The TypeScript webpack PWA","permalink":"/the-typescript-webpack-pwa"}},"content":"2017 is drawing to a close, and it\'s been a big, big year in webpack-land. It\'s been a big year for `ts-loader` too. At the start of the year v1.3.3 was the latest version available, officially supporting webpack 1. (Old school!) We end the year with `ts-loader` sitting pretty at v3.2.0 and supporting webpack 2 and 3.\\n\\n\x3c!--truncate--\x3e\\n\\nMany releases were shipped and that was down to a whole bunch of folk. People helped out with bug fixes, features, advice and docs improvements. **All of these help.**`ts-loader` wouldn\'t be where it is without you so thanks to everyone that helped out - you rock!\\n\\n![Profile image of https://github.com/christiantinauer](christiantinauer.jpg)\\n\\n![Profile image of https://github.com/Pajn](Pajn.jpg)\\n\\n![Profile image of https://github.com/maier49](maier49.jpg)\\n\\n![Profile image of https://github.com/false](false.jpg)\\n\\n![Profile image of https://github.com/roddypratt](roddypratt.jpg)\\n\\n![Profile image of https://github.com/ldrick](ldrick.jpg)\\n\\n![Profile image of https://github.com/mattlewis92](mattlewis92.jpg)\\n\\n![Profile image of https://github.com/Venryx](Venryx.jpg)\\n\\n![Profile image of https://github.com/WillMartin](WillMartin.jpg)\\n\\n![Profile image of https://github.com/Loilo](Loilo.jpg)\\n\\n![Profile image of https://github.com/Brooooooklyn](Brooooooklyn.jpg)\\n\\n![Profile image of https://github.com/mengxy](mengxy.jpg)\\n\\n![Profile image of https://github.com/bsouthga](bsouthga.jpg)\\n\\n![Profile image of https://github.com/zinserjan](zinserjan.jpg)\\n\\n![Profile image of https://github.com/sokra](sokra.jpg)\\n\\n![Profile image of https://github.com/vhqtvn](vhqtvn.jpg)\\n\\n![Profile image of https://github.com/HerringtonDarkholme](HerringtonDarkholme.jpg)\\n\\n![Profile image of https://github.com/johnnyreilly](johnnyreilly.jpg)\\n\\n![Profile image of https://github.com/jbrantly](jbrantly.jpg)\\n\\n![Profile image of https://github.com/octref](octref.jpg)\\n\\n![Profile image of https://github.com/rhyek](rhyek.jpg)\\n\\n![Profile image of https://github.com/develar](develar.jpg)\\n\\n![Profile image of https://github.com/donaldpipowitch](donaldpipowitch.jpg)\\n\\n![Profile image of https://github.com/schmuli](schmuli.jpg)\\n\\n![Profile image of https://github.com/longlho](longlho.jpg)\\n\\n![Profile image of https://github.com/Igorbek](Igorbek.jpg)\\n\\n![Profile image of https://github.com/aindlq](aindlq.jpg)\\n\\n![Profile image of https://github.com/wearymonkey](wearymonkey.jpg)\\n\\n![Profile image of https://github.com/bancek](bancek.jpg)\\n\\n![Profile image of https://github.com/mredbishop](mredbishop.jpg)\\n\\nI\'m really grateful to all of you. Thanks so much! (Apologies for those I\'ve missed anyone out - I know there\'s more still.)\\n\\n## `fork-ts-checker-webpack-plugin` build speed improvements\\n\\nAlongside other\'s direct contributions to `ts-loader`, other projects improved the experience of using `ts-loader`. [Piotr Ole\u015B](https://github.com/piotr-oles) dropped his [`fork-ts-checker-webpack-plugin`](https://github.com/Realytics/fork-ts-checker-webpack-plugin) this year which nicely increased build speed when used with `ts-loader`.\\n\\nThat opened up the possibility of adding [HappyPack](https://github.com/amireh/happypack) support. I had the good fortune to work with webpack\'s [Tobias Koppers](https://github.com/sokra) and ExtraHop\'s [Alex Birmingham](https://github.com/abirmingham) on [improving TypeScript build speed further](https://www.extrahop.com/company/blog/2017/extrahop-webpack-accelerating-build-times/).\\n\\nSo what does the future hold?\\n\\n## ts-loader 4.0 (Live webpack or Die Hard)\\n\\nThe web marches on and webpack gallops alongside. Here\'s what\'s in the pipeline for ts-loader in 2018:\\n\\n### Start using the new watch API\\n\\n[A new watch API is being made available in the TypeScript API](https://github.com/Microsoft/TypeScript/pull/20234). We have [a PR](https://github.com/TypeStrong/ts-loader/pull/685) from the amazing [Sheetal Nandi](https://github.com/sheetalkamat) which adds support to ts-loader. Given that\'s quite a big PR we want to merge that before anything else lands. The watch API is still being finalised but once it lands in TypeScript we\'ll look to merge the PR and ship a new version of `ts-loader`.\\n\\n### Drop custom module resolution\\n\\nHistorically `ts-loader` has had it\'s own module resolution mechanism in place. We\'re going to look to move to use the TypeScript mechanism instead. The old module resolution be deprecated but will remain available behind a flag for a time. In future we\'ll look to drop the old mechanism entirely.\\n\\n### Drop support for TypeScript 2.3 and below\\n\\nThe codebase can be made simpler if we drop support for older versions of TypeScript so that\'s what we plan to do with our next breaking changes release.\\n\\n### webpack v4 is in alpha now\\n\\nIf any changes need to happen to ts-loader to support webpack 4 then they will be. Personally I\'m planning to help out with [`fork-ts-checker-webpack-plugin`](https://github.com/Realytics/fork-ts-checker-webpack-plugin) as there will likely be some changes required there.\\n\\n### `contextAsConfigBasePath` will be replaced with a `context`\\n\\nThe option that landed in the last month doesn\'t quite achieve the aims of the original PR\'s author [Christian Tinauer](https://github.com/christiantinauer). Consequently it\'s going to be replaced with a new option. This is queued up and ready to go [here](https://github.com/TypeStrong/ts-loader/pull/688).\\n\\n### `reportFiles` option to be added\\n\\n[Michel Rasschaert](https://github.com/freeman) is presently working on adding a `reportFiles` option to `ts-loader`. You can see the PR in progress [here](https://github.com/TypeStrong/ts-loader/pull/701).\\n\\n## Merry Christmas!\\n\\nYou can expect to see the first releases of ts-loader 4.0 in 2018. In the meantime, I\'d like to wish you Merry Christmas and a Happy New Year! And once more, thanks and thanks again to all you generous people who help build `ts-loader`. You\'re wonderful and so I\'m glad you do what you do... joyeux Noel!"},{"id":"the-typescript-webpack-pwa","metadata":{"permalink":"/the-typescript-webpack-pwa","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2017-11-19-the-typescript-webpack-pwa/index.md","source":"@site/blog/2017-11-19-the-typescript-webpack-pwa/index.md","title":"The TypeScript webpack PWA","description":"Learn how to turn a TypeScript, webpack setup into a PWA using Workbox. With service workers, build offline-capable web apps.","date":"2017-11-19T00:00:00.000Z","tags":[{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":3.335,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"the-typescript-webpack-pwa","title":"The TypeScript webpack PWA","authors":"johnnyreilly","tags":["webpack","typescript"],"hide_table_of_contents":false,"description":"Learn how to turn a TypeScript, webpack setup into a PWA using Workbox. With service workers, build offline-capable web apps."},"unlisted":false,"prevItem":{"title":"ts-loader 2017 retrospective","permalink":"/ts-loader-2017-retrospective"},"nextItem":{"title":"TypeScript Definitions, webpack and Module Types","permalink":"/typescript-definitions-webpack-and-module-types"}},"content":"So, there you sit, conflicted. You\'ve got a lovely build setup; it\'s a thing of beauty. Precious, polished like a diamond, sharpened like a circular saw. There at the core of your carefully crafted setup sits webpack. Heaving, mysterious... powerful.\\n\\n\x3c!--truncate--\x3e\\n\\nThere\'s more. Not only are you sold on webpack, you\'re all in TypeScript too. But now you\'ve heard tell of \\"Progressive Web Applications\\" and \\"Service Workers\\".... And you want to be dealt in. You want to build web apps that work offline. It can\'t work can it? Your build setup\'s going to be like the creature in the episode where they\'ve taken one too many jumps and it\'s gone into the foetal position.\\n\\nSo this is the plan kids. Let\'s take a simple TypeScript, webpack setup and make it a PWA. Like Victoria Wood said...\\n\\n## [Let\'s Do It Tonight](https://youtu.be/lNU5KVa_Tu8)\\n\\nHow to begin? Well first comes the plagiarism; [here\'s a simple TypeScript webpack setup](https://github.com/TypeStrong/ts-loader/tree/master/examples/core-js). Rob it. Stick a gun to its head and order it onto your hard drive. `yarn install` to pick up your dependencies and then `yarn start` to see what you\'ve got. Something like this:\\n\\n![](Screenshot-2017-11-19-18.29.15.webp)\\n\\nBeautiful right? And if we `yarn build` we end up with a simple output:\\n\\n![](Screenshot-2017-11-19-18.34.12.webp)\\n\\nTo test what we\'ve built out we want to use a simple web server to serve up the `dist` folder. I\'ve got the npm package [http-server](https://www.npmjs.com/package/http-server) installed globally for just such an eventuality. So let\'s `http-server ./dist` and I\'m once again looking at our simple app; it looks exactly the same as when I `yarn start`. Smashing. What would we see if we were offline? Well thanks to the magic of Chrome DevTools we can find out. Offline and refresh our browser...\\n\\n![](Screenshot-2017-11-19-20.05.19.webp)\\n\\nNot very user friendly. Once we\'re done, we should be able to refresh and still see our app.\\n\\n## [Work(box) It](https://youtu.be/UODX_pYpVxk)\\n\\n[Workbox](https://developers.google.com/web/tools/workbox/) is a project that makes the setting up of Service Workers (aka the magic that powers PWAs) easier. It supports webpack use cases through the [workbox-webpack-plugin](https://www.npmjs.com/package/workbox-webpack-plugin); so let\'s give it a whirl. Incidentally, there\'s a [cracking example](https://developers.google.com/web/tools/workbox/get-started/webpack) on the Workbox site.\\n\\n`yarn add workbox-webpack-plugin --dev` adds the plugin to our project. To make use of it, punt your way over to the `webpack.production.config.js` and add an entry for the plugin. We also need to set the `hash` parameter of the html-webpack-plugin to be false; if it\'s true it\'ll cause problems for the ServiceWorker.\\n\\n```js\\nconst WorkboxPlugin = require(\'workbox-webpack-plugin\');\\n\\n//...\\n\\nmodule.exports = {\\n  //...\\n\\n  plugins: [\\n    //...\\n\\n    new HtmlWebpackPlugin({\\n      hash: false,\\n      inject: true,\\n      template: \'src/index.html\',\\n      minify: {\\n        removeComments: true,\\n        collapseWhitespace: true,\\n        removeRedundantAttributes: true,\\n        useShortDoctype: true,\\n        removeEmptyAttributes: true,\\n        removeStyleLinkTypeAttributes: true,\\n        keepClosingSlash: true,\\n        minifyJS: true,\\n        minifyCSS: true,\\n        minifyURLs: true,\\n      },\\n    }),\\n\\n    new WorkboxPlugin({\\n      // we want our service worker to cache the dist directory\\n      globDirectory: \'dist\',\\n      // these are the sorts of files we want to cache\\n      globPatterns: [\'**/*.{html,js,css,png,svg,jpg,gif,json}\'],\\n      // this is where we want our ServiceWorker to be created\\n      swDest: path.resolve(\'dist\', \'sw.js\'),\\n      // these options encourage the ServiceWorkers to get in there fast\\n      // and not allow any straggling \\"old\\" SWs to hang around\\n      clientsClaim: true,\\n      skipWaiting: true,\\n    }),\\n  ],\\n\\n  //...\\n};\\n```\\n\\nWith this in place, `yarn build` will generate a ServiceWorker. Now to alter our code to register it. Open up `index.tsx` and add this to the end of the file:\\n\\n```js\\nif (\'serviceWorker\' in navigator) {\\n  window.addEventListener(\'load\', () => {\\n    navigator.serviceWorker\\n      .register(\'/sw.js\')\\n      .then((registration) => {\\n        // tslint:disable:no-console\\n        console.log(\'SW registered: \', registration);\\n      })\\n      .catch((registrationError) => {\\n        console.log(\'SW registration failed: \', registrationError);\\n      });\\n  });\\n}\\n```\\n\\nPut it together and...\\n\\n## What Have We Got?\\n\\nLet\'s `yarn build` again.\\n\\n![](Screenshot-2017-11-19-21.55.18.webp)\\n\\nOooohh look! A service worker is with us. Does it work? Let\'s find out... `http-server ./dist` Browse to [http://localhost:8080](http://localhost:8080) and let\'s have a look at the console.\\n\\n![](Screenshot-2017-11-19-21.34.54.webp)\\n\\nLooks very exciting. So now the test; let\'s go offline and refresh:\\n\\n![](Screenshot-2017-11-19-22.01.37.webp)\\n\\nYou are looking at the 200s of success. You\'re now running with webpack and TypeScript and you have built a Progressive Web Application. Feel good about life."},{"id":"typescript-definitions-webpack-and-module-types","metadata":{"permalink":"/typescript-definitions-webpack-and-module-types","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2017-10-20-typescript-definitions-webpack-and-module-types/index.md","source":"@site/blog/2017-10-20-typescript-definitions-webpack-and-module-types/index.md","title":"TypeScript Definitions, webpack and Module Types","description":"Inconsistent module exports cause confusion while using the npm package big.js, leading to `one definition to rule them all.`","date":"2017-10-20T00:00:00.000Z","tags":[{"inline":false,"label":"Definitely Typed","permalink":"/tags/definitely-typed","description":"The Definitely Typed project for TypeScript type definitions"},{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":3.595,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"typescript-definitions-webpack-and-module-types","title":"TypeScript Definitions, webpack and Module Types","authors":"johnnyreilly","tags":["definitely typed","webpack","typescript"],"hide_table_of_contents":false,"description":"Inconsistent module exports cause confusion while using the npm package big.js, leading to `one definition to rule them all.`"},"unlisted":false,"prevItem":{"title":"The TypeScript webpack PWA","permalink":"/the-typescript-webpack-pwa"},"nextItem":{"title":"Working with Extrahop on webpack and ts-loader","permalink":"/working-with-extrahop-on-webpack-and-ts"}},"content":"A funny thing happened on the way to the registry the other day. Something changed in an npm package I was using and confusion arose. You can read my unfiltered confusion [here](https://github.com/Microsoft/TypeScript/issues/18791) but here\'s the slightly clearer explanation.\\n\\n\x3c!--truncate--\x3e\\n\\n## The TL;DR\\n\\nWhen modules are imported, your loader will decide which module format it wants to use. CommonJS / AMD etc. The loader decides. It\'s important that the export is of the same \\"shape\\" regardless of the module format. For 2 reasons:\\n\\n1. You want to be able to reliably use the module regardless of the choice that your loader has made for which export to use.\\n2. Because when it comes to writing type definition files for modules, there is support for a _single_ external definition. Not one for each module format.\\n\\n![](one-definition-to-rule-them-all.webp)\\n\\n## The DR\\n\\nOnce upon a time we decided to use [big.js](https://github.com/MikeMcl/big.js/) in our project. It\'s popular and my old friend [Steve Ognibene](https://twitter.com/nycdotnet) apparently originally wrote the type definitions which can be found [here](https://github.com/DefinitelyTyped/DefinitelyTyped/tree/master/types/big.js). Then the definitions were updated by [Miika H\xe4nninen](https://github.com/googol). And then there was pain.\\n\\n## UMD / CommonJS \\\\*\\\\*and\\\\*\\\\* Global exports oh my!\\n\\nMy usage code was as simple as this:\\n\\n```js\\nimport * as BigJs from \'big.js\';\\nconst lookABigJs = new BigJs(1);\\n```\\n\\nIf you execute it in a browser it works. It makes me a `Big`. However the TypeScript compiler is \\\\*\\\\*not\\\\*\\\\* happy. No siree. Nope. It\'s bellowing at me:\\n\\n```ts\\n[ts] Cannot use \'new\' with an expression whose type lacks a call or construct signature.\\n```\\n\\nSo I think: \\"Huh! I guess Miika just missed something off when he updated the definition files. No bother. I\'ll fix it.\\" I take a look at how `big.js` exposes itself to the outside world. At the time, thusly:\\n\\n```js\\n//AMD.\\nif (typeof define === \'function\' && define.amd) {\\n  define(function () {\\n    return Big;\\n  });\\n\\n  // Node and other CommonJS-like environments that support module.exports.\\n} else if (typeof module !== \'undefined\' && module.exports) {\\n  module.exports = Big;\\n  module.exports.Big = Big;\\n  //Browser.\\n} else {\\n  global.Big = Big;\\n}\\n```\\n\\nNow, we were using webpack as our script bundler / loader. webpack is supersmart; it can take all kinds of module formats. So although it\'s more famous for supporting CommonJS, it can roll with AMD. That\'s exactly what\'s happening here. When webpack encounters the above code, it goes with the AMD export. So at runtime, `import * as BigJs from \'big.js\';` lands up resolving to the `return Big;` above.\\n\\nNow this turns out to be super-relevant. I took a look at the relevant portion of the definition file and found this:\\n\\n```js\\nexport const Big: BigConstructor;\\n```\\n\\nWhich tells me that `Big` is being exported as a subproperty of the module. That makes sense; that lines up with the `module.exports.Big = Big;` statement in the the big.js source code. There\'s a \\"gotcha\\" coming; can you guess what it is?\\n\\nThe problem is that our type definition is not exposing `Big` as a default export. So even though it\'s there; TypeScript won\'t let us use it. What\'s killing us further is that webpack is loading the AMD export which _doesn\'t_ have `Big` as a subproperty of the module. It only has it as a default.\\n\\n[Kitson Kelly](https://twitter.com/kitsonk) expressed the problem well when he said:\\n\\n> there is a different shape depending on which loader is being used and I am not sure that makes a huge amount of sense. The AMD shape is different than the CommonJS shape. While that is technically possible, that feels like that is an issue.\\n\\n## One Definition to Rule Them All\\n\\nHe\'s right; it is an issue. From a TypeScript perspective there is no way to write a definition file that allows for different module \\"shapes\\" depending upon the module type. If you really wanted to do that you\'re reduced to writing multiple definition files. That\'s blind alley anyway; what you want is a module to expose itself with the same \\"shape\\" regardless of the module type. What you want is this:\\n\\n`AMD === CommonJS === Global`\\n\\nAnd that\'s what we now have! Thanks to [Michael McLaughlin](https://github.com/mikemcl), author of big.js, [version 4.0 unified the export shape of the package](https://github.com/MikeMcl/big.js/pull/87#issuecomment-332663587). Miika H\xe4nninen submitted another [PR](https://github.com/DefinitelyTyped/DefinitelyTyped/pull/20096) which fixed up the type definitions. And once again the world is a beautiful place!"},{"id":"working-with-extrahop-on-webpack-and-ts","metadata":{"permalink":"/working-with-extrahop-on-webpack-and-ts","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2017-10-19-working-with-extrahop-on-webpack-and-ts/index.md","source":"@site/blog/2017-10-19-working-with-extrahop-on-webpack-and-ts/index.md","title":"Working with Extrahop on webpack and ts-loader","description":"John shares his excitement for working with talented individuals on open source software - its everywhere and a privilege to work on.","date":"2017-10-19T00:00:00.000Z","tags":[{"inline":false,"label":"ts-loader","permalink":"/tags/ts-loader","description":"The TypeScript loader for webpack."},{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."}],"readingTime":0.625,"hasTruncateMarker":false,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"working-with-extrahop-on-webpack-and-ts","title":"Working with Extrahop on webpack and ts-loader","authors":"johnnyreilly","tags":["ts-loader","webpack"],"hide_table_of_contents":false,"description":"John shares his excitement for working with talented individuals on open source software - its everywhere and a privilege to work on."},"unlisted":false,"prevItem":{"title":"TypeScript Definitions, webpack and Module Types","permalink":"/typescript-definitions-webpack-and-module-types"},"nextItem":{"title":"fork-ts-checker-webpack-plugin code clickability","permalink":"/fork-ts-checker-webpack-plugin-code"}},"content":"I\'m quite proud of this: [https://www.extrahop.com/company/blog/2017/extrahop-webpack-accelerating-build-times/](https://www.extrahop.com/company/blog/2017/extrahop-webpack-accelerating-build-times/)\\n\\nIf you didn\'t know, I spend a good amount of my spare time hacking on open source software. You may not know what that is. I would describe OSS as software made with \u2764 by people, for other people to use.\\n\\nYou are currently reading this on a platform that was built using OSS. It\'s all around you, every day. It\'s on your phone, on your computer, on your TV. It\'s everywhere.\\n\\nIt\'s my hobby, it\'s part of my work. This specifically was one of those tremendously rare occasions when I got paid directly to work on my hobby, with people much brighter than me. It was brilliant. I loved it; it was a privilege.\\n\\nHere\'s to Open Source!"},{"id":"fork-ts-checker-webpack-plugin-code","metadata":{"permalink":"/fork-ts-checker-webpack-plugin-code","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2017-09-12-fork-ts-checker-webpack-plugin-code/index.md","source":"@site/blog/2017-09-12-fork-ts-checker-webpack-plugin-code/index.md","title":"fork-ts-checker-webpack-plugin code clickability","description":"The `fork-ts-checker-webpack-plugin` can speed up builds, but TypeScript errors in the terminal are not clickable. The cause and solution are explained.","date":"2017-09-12T00:00:00.000Z","tags":[{"inline":false,"label":"VS Code","permalink":"/tags/vs-code","description":"The Visual Studio Code editor."},{"inline":false,"label":"fork-ts-checker-webpack-plugin","permalink":"/tags/fork-ts-checker-webpack-plugin","description":"The fork-ts-checker-webpack-plugin for webpack."},{"inline":false,"label":"ts-loader","permalink":"/tags/ts-loader","description":"The TypeScript loader for webpack."},{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."}],"readingTime":2.09,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"fork-ts-checker-webpack-plugin-code","title":"fork-ts-checker-webpack-plugin code clickability","authors":"johnnyreilly","tags":["vs code","fork-ts-checker-webpack-plugin","ts-loader","webpack"],"hide_table_of_contents":false,"description":"The `fork-ts-checker-webpack-plugin` can speed up builds, but TypeScript errors in the terminal are not clickable. The cause and solution are explained."},"unlisted":false,"prevItem":{"title":"Working with Extrahop on webpack and ts-loader","permalink":"/working-with-extrahop-on-webpack-and-ts"},"nextItem":{"title":"TypeScript + webpack: Super Pursuit Mode","permalink":"/typescript-webpack-super-pursuit-mode"}},"content":"My name is John Reilly and I\'m a VS Code addict. There I said it. I\'m also a big fan of TypeScript and webpack. I\'ve recently switched to using the awesome [`fork-ts-checker-webpack-plugin`](https://www.npmjs.com/package/fork-ts-checker-webpack-plugin) to speed up my builds.\\n\\n\x3c!--truncate--\x3e\\n\\nOne thing I love is using VS Code both as my editor and my terminal. Using the fork-ts-checker-webpack-plugin I noticed a problem when TypeScript errors showed up in the terminal:\\n\\n![](Screenshot-2017-09-12-06.12.25.webp)\\n\\nTake a look at the red file location in the console above. What\'s probably not obvious from the above screenshot is that it is **not clickable**. I\'m used to being able to click on link in the console and bounce straight to the error location. It\'s a really productive workflow; see a problem, click on it, be taken to the cause, fix it.\\n\\nI want to click on \\"`C:/source/ts-loader/examples/fork-ts-checker/src/fileWithError.ts(2,7)`\\" and have VS Code open up `fileWithError.ts`, ideally at line 2 and column 7. But here it\'s not working. Why?\\n\\nWell, I initially got this slightly wrong; I thought it was about the formatting of the file path. It is. I thought that having the line number and column number in parentheses after the path (eg `\\"(2,7)\\"`) was screwing over VS Code. It isn\'t. Something else is. Look closely at the screenshot; what do you see? Do you notice how the colour of the line number / column number is different to the path? In the words of [Delbert Wilkins](https://youtu.be/281jMxOvP5k): that\'s crucial.\\n\\nYup, the colour change between the path and the line number / column number is the problem. I\'ve submitted a [PR to fix this](https://github.com/Realytics/fork-ts-checker-webpack-plugin/pull/48) that I hope will get merged. In the meantime you can avoid this issue by dropping this code into your `webpack.config.js`:\\n\\n```js\\nvar chalk = require(\'chalk\');\\nvar os = require(\'os\');\\n\\nfunction clickableFormatter(message, useColors) {\\n  var colors = new chalk.constructor({ enabled: useColors });\\n  var messageColor = message.isWarningSeverity()\\n    ? colors.bold.yellow\\n    : colors.bold.red;\\n  var fileAndNumberColor = colors.bold.cyan;\\n  var codeColor = colors.grey;\\n  return [\\n    messageColor(message.getSeverity().toUpperCase() + \' in \') +\\n      fileAndNumberColor(\\n        message.getFile() +\\n          \'(\' +\\n          message.getLine() +\\n          \',\' +\\n          message.getCharacter() +\\n          \')\',\\n      ) +\\n      messageColor(\':\'),\\n\\n    codeColor(message.getFormattedCode() + \': \') + message.getContent(),\\n  ].join(os.EOL);\\n}\\n\\nmodule.exports = {\\n  // Other config...\\n  module: {\\n    rules: [\\n      {\\n        test: /\\\\.tsx?$/,\\n        loader: \'ts-loader\',\\n        options: { transpileOnly: true },\\n      },\\n    ],\\n  },\\n  resolve: {\\n    extensions: [\'.ts\', \'.tsx\', \'js\'],\\n  },\\n  plugins: [\\n    new ForkTsCheckerWebpackPlugin({ formatter: clickableFormatter }), // Here we get our clickability back\\n  ],\\n};\\n```\\n\\nWith that in place, what do you we have? This:\\n\\n![](Screenshot-2017-09-12-06.35.48.webp)\\n\\nVS Code clickability; it\'s a beautiful thing."},{"id":"typescript-webpack-super-pursuit-mode","metadata":{"permalink":"/typescript-webpack-super-pursuit-mode","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2017-09-07-typescript-webpack-super-pursuit-mode/index.md","source":"@site/blog/2017-09-07-typescript-webpack-super-pursuit-mode/index.md","title":"TypeScript + webpack: Super Pursuit Mode","description":"Learn how to improve build speeds with TypeScript and webpack using fork-ts-checker-webpack-plugin, HappyPack, and thread-loader/cache-loader.","date":"2017-09-07T00:00:00.000Z","tags":[{"inline":false,"label":"fork-ts-checker-webpack-plugin","permalink":"/tags/fork-ts-checker-webpack-plugin","description":"The fork-ts-checker-webpack-plugin for webpack."},{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":6.665,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"typescript-webpack-super-pursuit-mode","title":"TypeScript + webpack: Super Pursuit Mode","authors":"johnnyreilly","tags":["fork-ts-checker-webpack-plugin","webpack","typescript"],"hide_table_of_contents":false,"description":"Learn how to improve build speeds with TypeScript and webpack using fork-ts-checker-webpack-plugin, HappyPack, and thread-loader/cache-loader."},"unlisted":false,"prevItem":{"title":"fork-ts-checker-webpack-plugin code clickability","permalink":"/fork-ts-checker-webpack-plugin-code"},"nextItem":{"title":"Oh the Glamour of Open Source","permalink":"/oh-glamour-of-open-source"}},"content":"_[This post also featured as a webpack Medium publication](https://medium.com/webpack/typescript-webpack-super-pursuit-mode-83cc568dea79)._\\n\\n\x3c!--truncate--\x3e\\n\\nIf you\'re like me then you\'ll like TypeScript and you\'ll like module bundling with webpack. You may also like speedy builds. That\'s completely understandable. The fact of the matter is, you sacrifice a bit of build speed to have webpack in the mix. Wouldn\'t it be great if we could even up the difference?\\n\\nI\'m the primary maintainer of ts-loader, a TypeScript loader for webpack. Just recently a couple of PRs were submitted that said, in other words: ts-loader is like this:\\n\\n![](KITT.webp)\\n\\nBut it could be like this:\\n\\n![](webkitt.webp)\\n\\nApologies for the image quality above; there appear to be no high quality pictures out there of KITT in Super Pursuit Mode for me to defame with [Garan Jenkin](https://github.com/plemont)\'s atrocious puns.\\n\\n## fork-ts-checker-webpack-plugin\\n\\n[\\"Faster type checking with forked process\\"](https://github.com/TypeStrong/ts-loader/issues/537) read the enticing name of the issue. It turned out to be [Piotr Ole\u015B](https://github.com/piotr-oles) ([@OlesDev](https://twitter.com/OlesDev)) telling the world about his beautiful creation. He\'d put together a mighty fine plugin that can be used alongside ts-loader called the [fork-ts-checker-webpack-plugin](https://github.com/Realytics/fork-ts-checker-webpack-plugin). The name is a bit of a mouthful but the purpose is mouth-watering. To quote the README, it is a:\\n\\n> webpack plugin that runs typescript type checker on a separate process.\\n\\nWhat does this mean and how does this fit with ts-loader? Well, ts-loader does 2 jobs:\\n\\n1. It transpiles your TypeScript into JavaScript and hands it off to webpack\\n2. It collects any TypeScript compilation errors and reports them to webpack\\n\\nWhat this plugin does is say, \\"forget about #2 - we\'ve got this.\\" It removes the responsibility for type checking from ts-loader, so the only work ts-loader does is transpilation. In the meantime, the all important type checking is still happening. To be honest, there would be little reason to recommend this approach otherwise. The difference is `fork-ts-checker-webpack-plugin` is doing the heavy lifting **in a separate process**. This provides a nice performance boost to your workflow. ts-loader is doing **less** and that\'s a <u>good thing</u>\\n\\n.\\n\\nThe approach used here is similar to that employed by awesome-typescript-loader. ATL is another TypeScript loader for webpack by the excellent [Stanislav Panferov](https://github.com/s-panferov). ATL also has a technique for performing typechecking in a forked process. fork-ts-checker-webpack-plugin was an effort by Piotr to implement something similar but with improved incremental build performance.\\n\\nHow do we use it? Add fork-ts-checker-webpack-plugin as a `devDependency` of your project and then amend the `webpack.config.js` to set ts-loader into `transpileOnly` mode and drop the plugin into the mix:\\n\\n```js\\nvar ForkTsCheckerWebpackPlugin = require(\'fork-ts-checker-webpack-plugin\');\\n\\nvar webpackConfig = {\\n  // other config...\\n  context: __dirname, // to automatically find tsconfig.json\\n  module: {\\n    rules: [\\n      {\\n        test: /\\\\.tsx?$/,\\n        loader: \'ts-loader\',\\n        options: {\\n          // disable type checker - we will use it in fork plugin\\n          transpileOnly: true,\\n        },\\n      },\\n    ],\\n  },\\n  plugins: [new ForkTsCheckerWebpackPlugin()],\\n};\\n```\\n\\nIf you\'d like to see an example of how to use the plugin then take a look at a [simple example](https://github.com/TypeStrong/ts-loader/tree/master/examples/fork-ts-checker) and a [more involved one](https://github.com/TypeStrong/ts-loader/tree/master/examples/react-babel-karma-gulp-fork-ts-checker).\\n\\n## HappyPack\\n\\nNot so long ago I didn\'t know what <strike>happyness</strike>\\n\\n[HappyPack](https://github.com/amireh/happypack) was. \\"Happiness in the form of faster webpack build times.\\" That\'s what it is.\\n\\n> HappyPack makes webpack builds faster by allowing you to transform multiple files in parallel.\\n\\nIt does this by spinning up multiple threads, each with their own loaders inside. We wanted to do this with ts-loader; to have multiple instances of ts-loader running. Work can then be divided up across these separate loaders. Isn\'t multi-threading great?\\n\\nts-loader did not initially play nicely with HappyPack; essentially this is because ts-loader touches parts of webpack\'s API that HappyPack replaces. The entirely wonderful [Artem Kozlov](https://github.com/aindlq) submitted a [PR which added HappyPack support to ts-loader](https://github.com/TypeStrong/ts-loader/pull/547). Support essentially amounts to switching ts-loader to run in `transpileOnly` mode and ensuring that there is no attempt to talk to parts of the webpack API that HappyPack removes.\\n\\nIt would be hard to recommend using HappyPack as is because, as with `transpileOnly` mode you lose all typechecking. Where it becomes worthwhile is where it is combined with the fork-ts-checker-webpack-plugin so you keep the typechecking.\\n\\nEnough with the chitter chatter; how can we achieve this? Add HappyPack as a `devDependency` of your project and then amend the `webpack.config.js` as follows:\\n\\n```js\\nvar HappyPack = require(\'happypack\');\\nvar ForkTsCheckerWebpackPlugin = require(\'fork-ts-checker-webpack-plugin\');\\n\\nmodule.exports = {\\n  // other config...\\n  context: __dirname, // to automatically find tsconfig.json\\n  module: {\\n    rules: [\\n      {\\n        test: /\\\\.tsx?$/,\\n        exclude: /node_modules/,\\n        loader: \'happypack/loader?id=ts\',\\n      },\\n    ],\\n  },\\n  plugins: [\\n    new HappyPack({\\n      id: \'ts\',\\n      threads: 2,\\n      loaders: [\\n        {\\n          path: \'ts-loader\',\\n          query: { happyPackMode: true },\\n        },\\n      ],\\n    }),\\n    new ForkTsCheckerWebpackPlugin({ checkSyntacticErrors: true }),\\n  ],\\n};\\n```\\n\\nNote that the ts-loader options are now configured via the HappyPack `query` and that we\'re setting ts-loader with the `happyPackMode` option set.\\n\\nThere\'s one other thing to note which is important; we\'re now passing the `checkSyntacticErrors` option to the fork plugin. This ensures that the plugin checks for both syntactic errors (eg `const array = [{} {}];`) and semantic errors (eg `const x: number = \'1\';`). By default the plugin only checks for semantic errors. This is because when ts-loader is used with `transpileOnly` set, ts-loader will still report syntactic errors. But when used in `happyPackMode` it does not.\\n\\nIf you\'d like to see an example of how to use HappyPack then once again we have a [simple example](https://github.com/TypeStrong/ts-loader/tree/master/examples/happypack) and a [more involved one](https://github.com/TypeStrong/ts-loader/tree/master/examples/react-babel-karma-gulp-happypack).\\n\\n## `thread-loader` \\\\+ `cache-loader`\\n\\nYou might have some reservations about using HappyPack. First of all the quirky configuration required makes your webpack config rather less comprehensible. Also, HappyPack is not officially blessed by webpack. It is a side project developed externally from webpack and there\'s no guarantees that new versions of webpack won\'t break it. Neither of these are reasons not to use HappyPack but they are things to bear in mind.\\n\\nWhat if there were a way to parallelise our builds which dealt with these issues? Well, there is! By using [thread-loader](https://github.com/webpack-contrib/thread-loader) and [cache-loader](https://github.com/webpack-contrib/cache-loader) in combination you can both feel happy that you\'re using an official webpack workflow and you can have a config that\'s less confusing.\\n\\nWhat would that config look like? This:\\n\\n```js\\nvar ForkTsCheckerWebpackPlugin = require(\'fork-ts-checker-webpack-plugin\');\\n\\nmodule.exports = {\\n  // other config...\\n  context: __dirname, // to automatically find tsconfig.json\\n  module: {\\n    rules: {\\n      test: /\\\\.tsx?$/,\\n      use: [\\n        { loader: \'cache-loader\' },\\n        {\\n          loader: \'thread-loader\',\\n          options: {\\n            // there should be 1 cpu for the fork-ts-checker-webpack-plugin\\n            workers: require(\'os\').cpus().length - 1,\\n          },\\n        },\\n        {\\n          loader: \'ts-loader\',\\n          options: {\\n            happyPackMode: true, // IMPORTANT! use happyPackMode mode to speed-up compilation and reduce errors reported to webpack\\n          },\\n        },\\n      ],\\n    },\\n  },\\n  plugins: [new ForkTsCheckerWebpackPlugin({ checkSyntacticErrors: true })],\\n};\\n```\\n\\nAs you can see the configuration is much cleaner than with HappyPack. Interestingly ts-loader still needs to run in \\"`happyPackMode`\\" and that\'s because thread-loader is essentially behaving in the same fashion as with HappyPack and so ts-loader needs to behave in the same way. Probably ts-loader should have a more generic flag name than \\"`happyPackMode`\\". (Famously, naming things is hard; so if you\'ve a good idea, tell me!)\\n\\nThese loaders are new and so tread carefully. My own experiences have been pretty positive but your mileage may vary. Do note that, as with HappyPack, the thread-loader is highly configurable.\\n\\nIf you\'d like to see an example of how to use thread-loader and cache-loader then once again we have a [simple example](https://github.com/TypeStrong/ts-loader/tree/master/examples/thread-loader) and a [more involved one](https://github.com/TypeStrong/ts-loader/tree/master/examples/react-babel-karma-gulp-thread-loader).\\n\\n## All This Could Be Yours...\\n\\n> Wow! It looks like we can cut our build time by 4 minutes! [\\\\#webpack](https://twitter.com/hashtag/webpack?src=hash)[@typescriptlang](https://twitter.com/typescriptlang) // cc [@johnny_reilly](https://twitter.com/johnny_reilly)[pic.twitter.com/gjvy9SLBAT](https://t.co/gjvy9SLBAT)\\n>\\n> \u2014 Donald Pipowitch (@PipoPeperoni) [June 23, 2017](https://twitter.com/PipoPeperoni/status/878148978356834304)\\n\\n<script async=\\"\\" src=\\"//platform.twitter.com/widgets.js\\" charSet=\\"utf-8\\"><\/script>\\n\\nIn this post we\'re improving build speeds with TypeScript and webpack in 3 ways:\\n\\n<dl><dt>fork-ts-checker-webpack-plugin</dt><dd>With this plugin in play ts-loader only performs transpilation. ts-loader is doing less so the build is faster.</dd><dt>HappyPack</dt><dd>With HappyPack in the mix, the build is parallelised. That parallelisation means the build is faster.</dd><dt>thread-loader / cache-loader</dt><dd>With thread-loader and cache-loader, again the build is parallelised and the build is faster.</dd></dl>\\n\\n<iframe src=\\"https://giphy.com/embed/Bo2WsocASVBm0\\" width=\\"240\\" height=\\"180\\" frameBorder=\\"0\\" allowFullScreen=\\"\\"></iframe>"},{"id":"oh-glamour-of-open-source","metadata":{"permalink":"/oh-glamour-of-open-source","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2017-08-30-oh-glamour-of-open-source/index.md","source":"@site/blog/2017-08-30-oh-glamour-of-open-source/index.md","title":"Oh the Glamour of Open Source","description":"A programmer recounts a sleepless night spent fixing a gap in an open source project, but accidentally deletes the repo and eventually seeks help.","date":"2017-08-30T00:00:00.000Z","tags":[],"readingTime":1.21,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"oh-glamour-of-open-source","title":"Oh the Glamour of Open Source","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false,"description":"A programmer recounts a sleepless night spent fixing a gap in an open source project, but accidentally deletes the repo and eventually seeks help."},"unlisted":false,"prevItem":{"title":"TypeScript + webpack: Super Pursuit Mode","permalink":"/typescript-webpack-super-pursuit-mode"},"nextItem":{"title":"Karma: From PhantomJS to Headless Chrome","permalink":"/karma-from-phantomjs-to-headless-chrome"}},"content":"Here\'s how my life panned out in the early hours of Wednesday 30th September 2017:\\n\\n\x3c!--truncate--\x3e\\n\\n <dl><dt>2 am</dt><dd>awoken by Lisette having a nightmare</dd><dt>3 am</dt><dd>gave up hope of getting back to sleep upstairs and headed for the sofa</dd><dt>4 am</dt><dd>still not asleep and discovered a serious gap in an open source project I help out with</dd><dt>4:30 am</dt><dd> come up with idea for a fix</dd><dt>4:45 am</dt><dd> accidentally delete a repo that I and many others care about from GitHub</dd><dt>4:50 am</dt><dd> recover said repo from backups (sweet mercy how could I be so stupid?)</dd><dt>4:55 am</dt><dd> actually succeed in cloning the repo I want to hack on </dd><dt>5:30 am</dt><dd> implement fix and <a href=\\"https://github.com/Realytics/fork-ts-checker-webpack-plugin/pull/43\\">send PR</a></dd><dt>5:35 am</dt><dd> go for a walk round the river</dd><dt>6:30 am</dt><dd> realise I didn\'t submit a test for the changed functionality</dd><dt>6:35 am</dt><dd> write test only to discover I can\'t run the test pack on Windows</dd><dt>6:40 am</dt><dd> add test to PR anyway so I can see test results when Travis runs on each commit.</dd><dt>7 am</dt><dd>despair at the duration of my feedback loop, totally fail to get my tests to pass</dd><dt>7:10 am</dt><dd> stub my toe really badly on a train set Benjamin has been busily assembling beneath my feet</dd><dt>7:11 am</dt><dd> give in and literally beg the project owner in Paris to fix the tests for me. He takes pity on me and agrees. Possibly because I gave him emoji tulips \uD83C\uDF37</dd><dt>7:12 am</dt><dd> feel like a slight failure and profoundly tired.</dd></dl>\\n\\nOh the glamour of open source."},{"id":"karma-from-phantomjs-to-headless-chrome","metadata":{"permalink":"/karma-from-phantomjs-to-headless-chrome","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2017-08-27-karma-from-phantomjs-to-headless-chrome/index.md","source":"@site/blog/2017-08-27-karma-from-phantomjs-to-headless-chrome/index.md","title":"Karma: From PhantomJS to Headless Chrome","description":"Replace PhantomJS with new Chrome Headless to run Chrome without a UI. Migrate a test and add Chrome to your build environment.","date":"2017-08-27T00:00:00.000Z","tags":[{"inline":false,"label":"Automated Testing","permalink":"/tags/automated-testing","description":"How to perform the automation of tests."}],"readingTime":1.91,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"karma-from-phantomjs-to-headless-chrome","title":"Karma: From PhantomJS to Headless Chrome","authors":"johnnyreilly","tags":["automated testing"],"hide_table_of_contents":false,"description":"Replace PhantomJS with new Chrome Headless to run Chrome without a UI. Migrate a test and add Chrome to your build environment."},"unlisted":false,"prevItem":{"title":"Oh the Glamour of Open Source","permalink":"/oh-glamour-of-open-source"},"nextItem":{"title":"A Haiku on the Problem with SemVer: Us","permalink":"/a-haiku-on-problem-with-semver-us"}},"content":"Like pretty much everyone else I\'ve been using PhantomJS to run my JavaScript (or compiled-to-JS) unit tests. It\'s been great. So when I heard the news that [PhantomJS was dead](https://news.ycombinator.com/item?id=14105489) I was genuinely sad. However, the King is dead.... Long live the King! For there is a new hope; it\'s called [Chrome Headless ](https://developers.google.com/web/updates/2017/04/headless-chrome). It\'s not a separate version of Chrome; rather the ability to run Chrome without a UI is now baked into Google\'s favourite browser as of v59. (For those history buffs I might as well be clear: the main reason PhantomJS died is because Chrome Headless was in the works.)\\n\\n\x3c!--truncate--\x3e\\n\\n## Making the Switch\\n\\nAs long as you\'re running Chrome v59 or greater then you can switch. I\'ve just made ts-loader\'s execution test pack run with Chrome Headless instead of PhantomJS and I\'ve rarely been happier. Honest. Some context: the execution test pack runs Jasmine unit tests via the [Karma test runner](https://karma-runner.github.io/1.0/index.html). The move was surprisingly easy and you can see just how minimal it was in the PR [here](https://github.com/TypeStrong/ts-loader/pull/611/files). If you want to migrate a test that runs tests via Karma then this will take you through what you need to do.\\n\\n## `package.json`\\n\\nYou no longer need `phantomjs-prebuilt` as a dev dependency of your project. That\'s the PhantomJS browser disappearing in the rear view mirror. Next we need to replace `karma-phantomjs-launcher` with `karma-chrome-launcher`. These packages are responsible for firing up the browser that the tests are run in and we no longer want to invoke PhantomJS; we\'re Chrome all the way baby.\\n\\n## `karma.conf.js`\\n\\nYou need to tell Karma to use Chrome Headless instead of PhantomJS. You do that by replacing\\n\\n```js\\nbrowsers: [ \'PhantomJS\' ],\\n```\\n\\nwith\\n\\n```js\\nbrowsers: [ \'ChromeHeadless\' ],\\n```\\n\\nThat\'s it; job done!\\n\\n## Continuous Integration\\n\\nThere\'s always one more thing isn\'t there? Yup, ts-loader has CI builds that run on [Windows with AppVeyor](https://ci.appveyor.com/project/JohnReilly/ts-loader/branch/master) and [Linux with Travis](https://travis-ci.org/TypeStrong/ts-loader). The AppVeyor build went green on the first run; that\'s because Chrome is installed by default in the AppVeyor build environment. (yay!)\\n\\nTravis went red. (boooo!) Travis doesn\'t have Chrome installed by default. But it\'s no biggie; you just need to tweak your `.travis.yml` like so:\\n\\n```yml\\ndist: trusty\\naddons:\\n  chrome: stable\\n```\\n\\nThis includes Chrome in the Travis build environment. Green. Boom!"},{"id":"a-haiku-on-problem-with-semver-us","metadata":{"permalink":"/a-haiku-on-problem-with-semver-us","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2017-07-29-a-haiku-on-problem-with-semver-us/index.md","source":"@site/blog/2017-07-29-a-haiku-on-problem-with-semver-us/index.md","title":"A Haiku on the Problem with SemVer: Us","description":"A Haiku on the Problem with SemVer: Us","date":"2017-07-29T00:00:00.000Z","tags":[],"readingTime":0.06,"hasTruncateMarker":false,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"a-haiku-on-problem-with-semver-us","title":"A Haiku on the Problem with SemVer: Us","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false,"description":"A Haiku on the Problem with SemVer: Us"},"unlisted":false,"prevItem":{"title":"Karma: From PhantomJS to Headless Chrome","permalink":"/karma-from-phantomjs-to-headless-chrome"},"nextItem":{"title":"Dynamic import: I\'ve been awaiting you...","permalink":"/dynamic-import-ive-been-await-ing-you"}},"content":"Version numbers wrong\\nWe release breaking changes\\nWe don\'t know we do"},{"id":"dynamic-import-ive-been-await-ing-you","metadata":{"permalink":"/dynamic-import-ive-been-await-ing-you","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2017-07-02-dynamic-import-ive-been-await-ing-you/index.md","source":"@site/blog/2017-07-02-dynamic-import-ive-been-await-ing-you/index.md","title":"Dynamic import: I\'ve been awaiting you...","description":"TypeScript 2.4 gains asynchronous, dynamic import expression for modules with no browser support. Webpack2 supports this feature.","date":"2017-07-02T00:00:00.000Z","tags":[{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":5.075,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"dynamic-import-ive-been-await-ing-you","title":"Dynamic import: I\'ve been awaiting you...","authors":"johnnyreilly","tags":["webpack","typescript"],"hide_table_of_contents":false,"description":"TypeScript 2.4 gains asynchronous, dynamic import expression for modules with no browser support. Webpack2 supports this feature."},"unlisted":false,"prevItem":{"title":"A Haiku on the Problem with SemVer: Us","permalink":"/a-haiku-on-problem-with-semver-us"},"nextItem":{"title":"Windows Defender Step Away From npm","permalink":"/windows-defender-step-away-from-npm"}},"content":"One of the most exciting features to ship with TypeScript 2.4 was support for the dynamic import expression. To quote the [release blog post](https://blogs.msdn.microsoft.com/typescript/2017/06/27/announcing-typescript-2-4/#dynamic-import-expressions):\\n\\n\x3c!--truncate--\x3e\\n\\n> Dynamic `import` expressions are a new feature in ECMAScript that allows you to asynchronously request a module at any arbitrary point in your program. These modules come back as `Promise`s of the module itself, and can be `await`\\\\-ed in an async function, or can be given a callback with `.then`.\\n>\\n> ...\\n>\\n> Many bundlers have support for automatically splitting output bundles (a.k.a. \u201Ccode splitting\u201D) based on these `import()` expressions, so consider using this new feature with the `esnext` module target. Note that this feature won\u2019t work with the `es2015` module target, since the feature is anticipated for ES2018 or later.\\n\\nAs the post makes clear, this adds support for a very bleeding edge ECMAScript feature. This is not fully standardised yet; it\'s currently at [stage 3](https://github.com/tc39/proposals) on the TC39 proposals list. That means it\'s at the [Candidate](https://tc39.github.io/process-document/) stage and is unlikely to change further. If you\'d like to read more about it then take a look at the official proposal [here](https://github.com/tc39/proposal-dynamic-import).\\n\\nWhilst this is super-new, we are still able to use this feature. We just have to jump through a few hoops first.\\n\\n## TypeScript Setup\\n\\nFirst of all, you need to install TypeScript 2.4. With that in place you need to make some adjustments to your `tsconfig.json` in order that the relevant compiler switches are flipped. What do you need? First of all you need to be targeting ECMAScript 2015 as a minimum. That\'s important specifically because ES2015 contained `Promise`s which is what dynamic `import`s produce. The second thing you need is to target the module type of `esnext`. You\'re likely targeting `es2015` now, `esnext` is that **plus** dynamic `import`s.\\n\\nHere\'s a `tsconfig.json` I made earlier which has the relevant settings set:\\n\\n```json\\n{\\n  \\"compilerOptions\\": {\\n    \\"allowSyntheticDefaultImports\\": true,\\n    \\"lib\\": [\\"dom\\", \\"es2015\\"],\\n    \\"target\\": \\"es2015\\",\\n    \\"module\\": \\"esnext\\",\\n    \\"moduleResolution\\": \\"node\\",\\n    \\"noImplicitAny\\": true,\\n    \\"noUnusedLocals\\": true,\\n    \\"noUnusedParameters\\": true,\\n    \\"removeComments\\": false,\\n    \\"preserveConstEnums\\": true,\\n    \\"sourceMap\\": true,\\n    \\"skipLibCheck\\": true\\n  }\\n}\\n```\\n\\n## Babel Setup\\n\\nAt the time of writing, browser support for dynamic `import` is non-existent. This will likely be the case for some time but it needn\'t hold us back. Babel can step in here and compile our super-new JS into JS that will run in our browsers today.\\n\\nYou\'ll need to decide for yourself how much you want Babel to do for you. In my case I\'m targeting old school browsers which don\'t yet support ES2015. You may not need to. However, the one thing that you\'ll certainly need is the [Syntax Dynamic Import](https://babeljs.io/docs/plugins/syntax-dynamic-import/) plugin. It\'s this that allows Babel to process dynamic `import` statements.\\n\\nThese are the options I\'m passing to Babel:\\n\\n```js\\nvar babelOptions = {\\n  plugins: [\'syntax-dynamic-import\'],\\n  presets: [\\n    [\\n      \'es2015\',\\n      {\\n        modules: false,\\n      },\\n    ],\\n  ],\\n};\\n```\\n\\nYou\'re also going to need something that actually execute the `import`s. In my case I\'m using webpack...\\n\\n## webpack\\n\\nwebpack 2 supports [`import()`](https://webpack.js.org/api/module-methods/#import-). So if you webpack set up with [ts-loader](https://github.com/TypeStrong/ts-loader) (or awesome-typescript-loader etc), chaining into [babel-loader](https://github.com/babel/babel-loader) you should find you have a setup that supports dynamic `import`. That means a `webpack.config.js` that looks something like this:\\n\\n```js\\nvar path = require(\'path\');\\nvar webpack = require(\'webpack\');\\n\\nvar babelOptions = {\\n  plugins: [\'syntax-dynamic-import\'],\\n  presets: [\\n    [\\n      \'es2015\',\\n      {\\n        modules: false,\\n      },\\n    ],\\n  ],\\n};\\n\\nmodule.exports = {\\n  entry: \'./app.ts\',\\n  output: {\\n    filename: \'bundle.js\',\\n  },\\n  module: {\\n    rules: [\\n      {\\n        test: /\\\\.ts(x?)$/,\\n        exclude: /node_modules/,\\n        use: [\\n          {\\n            loader: \'babel-loader\',\\n            options: babelOptions,\\n          },\\n          {\\n            loader: \'ts-loader\',\\n          },\\n        ],\\n      },\\n      {\\n        test: /\\\\.js$/,\\n        exclude: /node_modules/,\\n        use: [\\n          {\\n            loader: \'babel-loader\',\\n            options: babelOptions,\\n          },\\n        ],\\n      },\\n    ],\\n  },\\n  resolve: {\\n    extensions: [\'.ts\', \'.tsx\', \'.js\'],\\n  },\\n};\\n```\\n\\n## ts-loader example\\n\\nI\'m one of the maintainers of [ts-loader](https://github.com/TypeStrong/ts-loader) which is a TypeScript loader for webpack. When support for dynamic `import`s landed I wanted to add a test to cover usage of the new syntax with ts-loader.\\n\\nWe have 2 test packs for ts-loader, one of which is our \\"execution\\" test pack. It is so named because it works by spinning up webpack with ts-loader and then using [karma](https://github.com/karma-runner/karma) to execute a set of tests. Each \\"test\\" in our execution test pack is actually a mini-project with its own test suite (generally [jasmine](https://jasmine.github.io/) but that\'s entirely configurabe). Each complete with its own `webpack.config.js`, `karma.conf.js` and either a `typings.json` or `package.json` for bringing in dependencies. So it\'s a full test of whether code slung with ts-loader and webpack actually executes when the output is plugged into a browser.\\n\\nThis is the test pack for dynamic `import`s:\\n\\n```js\\nimport a from \\"../src/a\\";\\nimport b from \\"../src/b\\";\\n\\ndescribe(\\"app\\", () => {\\n  it(\\"a to be \'a\' and b to be \'b\' (classic)\\", () => {\\n    expect(a).toBe(\\"a\\");\\n    expect(b).toBe(\\"b\\");\\n  });\\n\\n  it(\\"import results in a module with a default export\\", done => {\\n    import(\\"../src/c\\").then(c => {\\n      // .default is the default export\\n      expect(c.default).toBe(\\"c\\");\\n\\n      done();\\n    }\\n  });\\n\\n  it(\\"import results in a module with an export\\", done => {\\n    import(\\"../src/d\\").then(d => {\\n      // .default is the default export\\n      expect(d.d).toBe(\\"d\\");\\n\\n      done();\\n    }\\n  });\\n\\n  it(\\"await import results in a module with a default export\\", async done => {\\n    const c = await import(\\"../src/c\\");\\n\\n    // .default is the default export\\n    expect(c.default).toBe(\\"c\\");\\n\\n    done();\\n  });\\n\\n  it(\\"await import results in a module with an export\\", async done => {\\n    const d = await import(\\"../src/d\\");\\n\\n    expect(d.d).toBe(\\"d\\");\\n\\n    done();\\n  });\\n});\\n```\\n\\nAs you can see, it\'s possible to use the dynamic `import` as a `Promise` directly. Alternatively, it\'s possible to consume the imported module using TypeScripts support for `async` / `await`. For my money the latter option makes for much clearer code.\\n\\nIf you\'re looking for a complete example of how to use the new syntax then you could do worse than taking the existing test pack and tweaking it to your own ends. The only change you\'d need to make is to strip out the `resolveLoader` statements in `webpack.config.js` and `karma.conf.js`. (They exist to lock the test in case to the freshly built ts-loader stored locally. You\'ll not need this.)\\n\\nYou can find the test in question [here](https://github.com/TypeStrong/ts-loader/tree/master/test/execution-tests/2.4.1_babel-importCodeSplitting). Happy code splitting!"},{"id":"windows-defender-step-away-from-npm","metadata":{"permalink":"/windows-defender-step-away-from-npm","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2017-06-11-windows-defender-step-away-from-npm/index.md","source":"@site/blog/2017-06-11-windows-defender-step-away-from-npm/index.md","title":"Windows Defender Step Away From npm","description":"A bug causing issues with Windows Defender has been fixed with the release of VS Code 1.14. The bug was causing problems with the program open.","date":"2017-06-11T00:00:00.000Z","tags":[{"inline":false,"label":"VS Code","permalink":"/tags/vs-code","description":"The Visual Studio Code editor."},{"inline":false,"label":"Node.js","permalink":"/tags/node-js","description":"The Node.js runtime."}],"readingTime":1.68,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"windows-defender-step-away-from-npm","title":"Windows Defender Step Away From npm","authors":"johnnyreilly","tags":["vs code","node.js"],"hide_table_of_contents":false,"description":"A bug causing issues with Windows Defender has been fixed with the release of VS Code 1.14. The bug was causing problems with the program open."},"unlisted":false,"prevItem":{"title":"Dynamic import: I\'ve been awaiting you...","permalink":"/dynamic-import-ive-been-await-ing-you"},"nextItem":{"title":"TypeScript: Spare the Rod, Spoil the Code","permalink":"/typescript-spare-rod-spoil-code"}},"content":"## Updated 18/06/2017\\n\\nWhilst things did improve by fiddling with Windows Defender it wasn\'t a 100% fix which makes me wary. Interestingly, VS Code was always open when I did experience the issue and I haven\'t experienced it when it\'s been closed. So it may be the cause. I\'ve opened [an issue for this against the VS Code repo](https://github.com/Microsoft/vscode/issues/28593) \\\\- it sounds like other people may be affected as I was. Perhaps this is VS Code and not Windows Defender. Watch that space...\\n\\n## Updated 12/07/2017\\n\\nThe issue was VS Code. The bug has now been fixed and shipped last night with [VS Code 1.14.0](https://code.visualstudio.com/updates/v1_14). Yay!\\n\\n\x3c!--truncate--\x3e\\n\\nI\'ve recently experienced many of my `npm install`s failing for no consistent reason. The error message would generally be something along the lines of:\\n\\n```sh\\nnpm ERR! Error: EPERM: operation not permitted, rename \'C:\\\\dev\\\\training\\\\drrug\\\\node_modules\\\\.staging\\\\@exponent\\\\ngrok-fc327f2a\' -> \'C:\\\\dev\\\\training\\\\drrug\\\\node_modules\\\\@exponent\\\\ngrok\'\\n```\\n\\nI spent a good deal of time changing the versions of node and npm I was running; all seemingly to no avail. Regular flakiness which I ascribed to node / npm. I was starting to give up when I read of [other people experiencing similar issues](https://github.com/react-community/create-react-native-app/issues/191#issuecomment-304073970). Encouragingly [Fernando Meira](https://github.com/fmeira) suggested a solution:\\n\\n> I got the same problem just doing an npm install. Run with antivirus disabled (if you use Windows Defender, turn off Real-Time protection and Cloud-based protection). That worked for me!\\n\\nI didn\'t really expect this to work - Windows Defender has been running in the background of my Windows 10 laptop since I\'ve had it. There\'s been no problems with npm installs up until a week or so ago. But given the experience I and others have had I thought I should put it out there: it looks like Windows Defender has it in for npm. Go figure.\\n\\nAlas Windows Defender doesn\'t stay dead for long; it\'s like a zombie that rises from the grave no matter how many times you kill it. So you might want to try configuring it to ignore node.exe:\\n\\n![](Screenshot-2017-06-11-15.05.47.webp)\\n\\nOr switching to Linux..."},{"id":"typescript-spare-rod-spoil-code","metadata":{"permalink":"/typescript-spare-rod-spoil-code","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2017-05-20-typescript-spare-rod-spoil-code/index.md","source":"@site/blog/2017-05-20-typescript-spare-rod-spoil-code/index.md","title":"TypeScript: Spare the Rod, Spoil the Code","description":"TypeScript settings catch bugs. Use compiler options to increase strictness gradually & catch errors such as unused variables with VS Code.","date":"2017-05-20T00:00:00.000Z","tags":[{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":2.08,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"typescript-spare-rod-spoil-code","title":"TypeScript: Spare the Rod, Spoil the Code","authors":"johnnyreilly","tags":["typescript"],"hide_table_of_contents":false,"description":"TypeScript settings catch bugs. Use compiler options to increase strictness gradually & catch errors such as unused variables with VS Code."},"unlisted":false,"prevItem":{"title":"Windows Defender Step Away From npm","permalink":"/windows-defender-step-away-from-npm"},"nextItem":{"title":"Setting Build Version Using AppVeyor and ASP.Net Core","permalink":"/setting-build-version-using-appveyor"}},"content":"I\'ve recently started a new role. Perhaps unsurprisingly, part of the technology stack is TypeScript. A couple of days into the new codebase I found a bug. Well, I say I found a bug, TypeScript and VS Code found the bug - I just let everyone else know.\\n\\n\x3c!--truncate--\x3e\\n\\nThe flexibility that TypeScript offers in terms of compiler settings is second to none. You can turn up the dial of strictness to your hearts content. Or down. I\'m an \\"up\\" man myself.\\n\\nThe project that I am working on has the dial set fairly low; it\'s pretty much using the default compiler values which are (sensibly) not too strict. I have to say this makes sense for helping people get on board with using TypeScript. Start from a point of low strictness and turn it up when you\'re ready. As you might have guessed, I cranked the dial up on day one on my own machine. I should say that as I did this, I didn\'t foist this on the project at large - I kept it just to my build... I\'m not \\\\***that**\\\\* guy!\\n\\nI made the below changes to the `tsconfig.json` file. Details of what each of these settings does can be found in the documentation [here](https://www.typescriptlang.org/docs/handbook/compiler-options.html).\\n\\n```json\\n\\"noImplicitAny\\": true,\\n    \\"noImplicitThis\\": true,\\n    \\"noUnusedLocals\\": true,\\n    \\"noImplicitReturns\\": true,\\n    \\"noUnusedParameters\\": true,\\n```\\n\\nI said I found a bug. The nature of the bug was an unused variable; a variable was created in a function but then not used. Here\'s a super simple example:\\n\\n```ts\\nfunction sayHi(name: string) {\\n  const greeting = `Hi ${name}`;\\n  return name;\\n}\\n```\\n\\nIt\'s an easy mistake to make. I\'ve made this mistake before myself. But with the `noUnusedLocals` compiler setting in place it\'s now an easy mistake to catch; VS Code lets you know loud and clear:\\n\\n![](Screenshot-2017-05-20-05.58.54.webp)\\n\\nThe other compiler settings will similarly highlight simple mistakes it\'s possible to make and I\'d recommend using them. I should say I\'ve written this from the perspective of a VS Code user, but this really applies generally to TypeScript usage. So whether you\'re an [alm.tools](http://alm.tools/) guy, a WebStorm gal or something else entirely then this too can be yours!\\n\\nI\'d also say that the `strictNullChecks` compiler setting is worth looking into. However, switching an already established project to using that can involve fairly extensive code changes and will also require a certain amount of education of, and buy in from, your team. So whilst I\'d recommend it too, I\'d save that one until last."},{"id":"setting-build-version-using-appveyor","metadata":{"permalink":"/setting-build-version-using-appveyor","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2017-04-25-setting-build-version-using-appveyor/index.md","source":"@site/blog/2017-04-25-setting-build-version-using-appveyor/index.md","title":"Setting Build Version Using AppVeyor and ASP.Net Core","description":"AppVeyor doesnt have support for setting version of a binary in dot net core, but it can be done easily through PowerShell.","date":"2017-04-25T00:00:00.000Z","tags":[{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":1.03,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"setting-build-version-using-appveyor","title":"Setting Build Version Using AppVeyor and ASP.Net Core","authors":"johnnyreilly","tags":["asp.net"],"hide_table_of_contents":false,"description":"AppVeyor doesnt have support for setting version of a binary in dot net core, but it can be done easily through PowerShell."},"unlisted":false,"prevItem":{"title":"TypeScript: Spare the Rod, Spoil the Code","permalink":"/typescript-spare-rod-spoil-code"},"nextItem":{"title":"I\'m looking for work!","permalink":"/im-looking-for-work"}},"content":"AppVeyor has [support for setting the version of a binary during a build](https://www.appveyor.com/docs/build-configuration/#assemblyinfo-patching). However - this deals with the classic ASP.Net world of `AssemblyInfo`. I didn\'t find any reference to support for doing the same with dot net core. Remember, dot net core [relies upon a `&lt;Version&gt;` or a `&lt;VersionPrefix&gt;` setting in the `.csproj` file](https://docs.microsoft.com/en-us/dotnet/articles/core/tools/project-json-to-csproj#version). Personally, `&lt;Version&gt;` is my jam.\\n\\n\x3c!--truncate--\x3e\\n\\nHowever, coming up with your own bit of powershell that stamps the version during the build is a doddle; here we go:\\n\\n```ps\\nParam($projectFile, $buildNum)\\n\\n$content = [IO.File]::ReadAllText($projectFile)\\n\\n$regex = new-object System.Text.RegularExpressions.Regex (\'(<version>)([\\\\d]+.[\\\\d]+.[\\\\d]+)(.[\\\\d]+)(<\\\\/Version>)\',\\n         [System.Text.RegularExpressions.RegexOptions]::MultiLine)\\n\\n$version = $null\\n$match = $regex.Match($content)\\nif($match.Success) {\\n    # from \\"<version>1.0.0.0</version>\\" this will extract \\"1.0.0\\"\\n    $version = $match.groups[2].value\\n}\\n\\n# suffix build number onto $version. eg \\"1.0.0.15\\"\\n$version = \\"$version.$buildNum\\"\\n\\n# update \\"<version>1.0.0.0</version>\\" to \\"<version>$version</version>\\"\\n$content = $regex.Replace($content, \'${1}\' + $version + \'${4}\')\\n\\n# update csproj file\\n[IO.File]::WriteAllText($projectFile, $content)\\n\\n# update AppVeyor build\\nUpdate-AppveyorBuild -Version $version\\n</version>\\n```\\n\\nYou can invoke this script as part of the build process in AppVeyor by adding something like this to your `appveyor.yml`.\\n\\n```yml\\nbefore_build:\\n  - ps: .\\\\ModifyVersion.ps1 $env:APPVEYOR_BUILD_FOLDER\\\\src\\\\Proverb.Web\\\\Proverb.Web.csproj $env:APPVEYOR_BUILD_NUMBER\\n```\\n\\nIt will keep the first 3 parts of the version in your `.csproj` (eg \\"1.0.0\\") and suffix on the build number supplied by AppVeyor."},{"id":"im-looking-for-work","metadata":{"permalink":"/im-looking-for-work","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2017-03-30-im-looking-for-work/index.md","source":"@site/blog/2017-03-30-im-looking-for-work/index.md","title":"I\'m looking for work!","description":"Full stack developer John Reilly seeks new work after finishing recent contract. 15 years\u2019 experience includes telecoms, advertising, tech and finance.","date":"2017-03-30T00:00:00.000Z","tags":[],"readingTime":2.505,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"im-looking-for-work","title":"I\'m looking for work!","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false,"description":"Full stack developer John Reilly seeks new work after finishing recent contract. 15 years\u2019 experience includes telecoms, advertising, tech and finance."},"unlisted":false,"prevItem":{"title":"Setting Build Version Using AppVeyor and ASP.Net Core","permalink":"/setting-build-version-using-appveyor"},"nextItem":{"title":"Debugging ASP.Net Core in VS or Code","permalink":"/debugging-aspnet-core-in-vs-or-code"}},"content":"My name is John Reilly. I\'m a full stack developer based in London, UK. I\'m just coming to the end of a contract (due to finish in April 2017) and I\'m starting to look for my next role.\\n\\n\x3c!--truncate--\x3e\\n\\nI have more than 15 years experience developing software commercially. I\'ve worked in a number of industries including telecoms, advertising, technology (I worked at Microsoft for a time) and, of course, finance. The bulk of my experience is in the finance sector. I\'ve provided consultancy services, building and maintaining applications for both large and small companies; from enterprise to startup.\\n\\nMy most recent work has been full stack web work; using React on the front end and SignalR (ASP.Net) on the back end. I\'m pragmatic about the tools that I use to deliver software solutions and not tied to any particular technology. That said, I\'ve gravitated towards the handiwork of [Anders Hejlsberg](https://en.wikipedia.org/wiki/Anders_Hejlsberg); starting out with Delphi and being both an early C# and TypeScript adopter. I\'ve built everything from high volume trade feeds with no UI beyond a log file, WinForms apps for call centres, to fully fledged rich web applications with a heavy emphasis on UX.\\n\\nI enjoy the challenges of understanding problems and coming up with useful solutions to them. I\'m thrilled when something I\'ve built makes someone\'s life easier. I love to learn and to share my knowledge; both in person and also through writing this blog. (This is the first time I\'ve used a post to seek work.)\\n\\nIn my spare time I\'m involved with various open source projects including [ts-loader](https://github.com/typestrong/ts-loader) and [DefinitelyTyped](https://github.com/DefinitelyTyped/DefinitelyTyped) ([member of the core team](https://github.com/orgs/DefinitelyTyped/people)). Get in contact with me if you\'re interested in learning more about me. Mail me at [johnny_reilly@hotmail.com](mailto:johnny_reilly@hotmail.com) and I can provide you with a CV. You can also find me on [GitHub](https://github.com/johnnyreilly).\\n\\n## Updated 25/04/2016: Position Filled\\n\\nI\'m happy to say that I\'ve lined up work for the next 6 months or so. Once again I\'ll be working in the financial services industry with one interesting twist. [In a blog post ages ago I bet that native apps would start to be replaced with SPAs.](../2014-02-12-wpf-and-mystic-meg-or-playing/index.md) This has started to happen. I\'ve started to see companies taking a \\"web-first-and-only\\" approach to building apps. In that vein, that\'s exactly what I\'m off to build.\\n\\nAs a result of publishing this blog post I\'ve had some interesting conversations with companies and got to think hard about the direction the industry is taking. I remain excited by JavaScript / TypeScript and React. I\'m hopeful of the possibilities offered by the container world of Docker etc. I\'m enjoying .NET Core and have very high hopes for it. I remain curious about Web Assembly.\\n\\nBefore I sign off, I know at some point I\'ll be looking for work once again. If there\'s a system you\'d like built, if there\'s some mentoring and training you\'d like done or if you\'d just like to have a conversation I\'m always available to talk. Drop me a line at [johnny_reilly@hotmail.com](mailto:johnny_reilly@hotmail.com)."},{"id":"debugging-aspnet-core-in-vs-or-code","metadata":{"permalink":"/debugging-aspnet-core-in-vs-or-code","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2017-03-28-debugging-aspnet-core-in-vs-or-code/index.md","source":"@site/blog/2017-03-28-debugging-aspnet-core-in-vs-or-code/index.md","title":"Debugging ASP.Net Core in VS or Code","description":"Learn how John became a fan of VS Code for TypeScript and how they managed to debug ASP.Net Core using the extension for C#.","date":"2017-03-28T00:00:00.000Z","tags":[{"inline":false,"label":"VS Code","permalink":"/tags/vs-code","description":"The Visual Studio Code editor."},{"inline":false,"label":"Visual Studio","permalink":"/tags/visual-studio","description":"The Visual Studio IDE."},{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":3.705,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"debugging-aspnet-core-in-vs-or-code","title":"Debugging ASP.Net Core in VS or Code","authors":"johnnyreilly","tags":["vs code","visual studio","asp.net"],"hide_table_of_contents":false,"description":"Learn how John became a fan of VS Code for TypeScript and how they managed to debug ASP.Net Core using the extension for C#."},"unlisted":false,"prevItem":{"title":"I\'m looking for work!","permalink":"/im-looking-for-work"},"nextItem":{"title":"Under the Duck: An Afternoon in Open Source","permalink":"/under-duck-afternoon-in-open-source"}},"content":"I\'ve been using Visual Studio for a long time. Very good it is too. However, it is heavyweight; it does far more than I need. What I really want when I\'m working is a fast snappy editor, with intellisense and debugging. What I\'ve basically described is [VS Code](https://code.visualstudio.com/). It rocks and has long become my go-to editor for TypeScript.\\n\\n\x3c!--truncate--\x3e\\n\\nSince I\'m a big C# fan as well I was delighted that editing C# was also possible in Code. What I want now is to be able to debug ASP.Net Core in Visual Studio OR VS Code. Can it be done? Let\'s see....\\n\\nI fire up Visual Studio and `File -&gt; New Project` (yes it\'s a verb now). Select .NET Core and then ASP.Net Core Web Application. OK. We\'ll go for a Web Application. Let\'s not bother with authentication. OK. Wait a couple of seconds and Visual Studio serves up a new project. Hit F5 and we\'re debugging in Visual Studio.\\n\\nSo far, so straightforward. What will VS Code make of this?\\n\\nI cd my way to the root of my new ASP.Net Core Web Application and type the magical phrase \\"code .\\". Up it fires. I feel lucky, let\'s hit \\"F5\\". Huh, a dropdown shows up saying `\\"Select Environment\\"` and offering me the options of Chrome and Node. Neither do I want. It\'s about this time I remember this is a clean install of VS Code and doesn\'t yet have the C# extension installed. In fact, if I open a C# file it up it tells me and recommends that I install. Well that\'s nice. I take it up on the kind offer; install and reload.\\n\\nWhen it comes back up I see the following entries in the \\"output\\" tab:\\n\\n```ts\\nUpdating C# dependencies...\\nPlatform: win32, x86_64 (win7-x64)\\n\\nDownloading package \'OmniSharp (.NET 4.6 / x64)\' (20447 KB) .................... Done!\\nDownloading package \'.NET Core Debugger (Windows / x64)\' (39685 KB) .................... Done!\\n\\nInstalling package \'OmniSharp (.NET 4.6 / x64)\'\\nInstalling package \'.NET Core Debugger (Windows / x64)\'\\n\\nFinished\\n```\\n\\nNote that mention of \\"debugger\\" there? Sounds super-promising. There\'s also some prompts: `\\"There are unresolved dependencies from \'WebApplication1/WebApplication1.csproj\'. Please execute the restore command to continue\\"`\\n\\nSo it wants me to `dotnet restore`. It\'s even offering to do that for me! Have at you; I let it.\\n\\n```ts\\nWelcome to .NET Core!\\n---------------------\\nLearn more about .NET Core @ https://aka.ms/dotnet-docs. Use dotnet --help to see available commands or go to https://aka.ms/dotnet-cli-docs.\\n\\nTelemetry\\n--------------\\nThe .NET Core tools collect usage data in order to improve your experience. The data is anonymous and does not include command-line arguments. The data is collected by Microsoft and shared with the community.\\nYou can opt out of telemetry by setting a DOTNET_CLI_TELEMETRY_OPTOUT environment variable to 1 using your favorite shell.\\nYou can read more about .NET Core tools telemetry @ https://aka.ms/dotnet-cli-telemetry.\\n\\nConfiguring...\\n-------------------\\nA command is running to initially populate your local package cache, to improve restore speed and enable offline access. This command will take up to a minute to complete and will only happen once.\\nDecompressing Decompressing 100% 4026 ms\\nExpanding 100% 34814 ms\\n  Restoring packages for c:\\\\Source\\\\Debugging\\\\WebApplication1\\\\WebApplication1\\\\WebApplication1.csproj...\\n  Restoring packages for c:\\\\Source\\\\Debugging\\\\WebApplication1\\\\WebApplication1\\\\WebApplication1.csproj...\\n  Restore completed in 734.05 ms for c:\\\\Source\\\\Debugging\\\\WebApplication1\\\\WebApplication1\\\\WebApplication1.csproj.\\n  Generating MSBuild file c:\\\\Source\\\\Debugging\\\\WebApplication1\\\\WebApplication1\\\\obj\\\\WebApplication1.csproj.nuget.g.props.\\n  Writing lock file to disk. Path: c:\\\\Source\\\\Debugging\\\\WebApplication1\\\\WebApplication1\\\\obj\\\\project.assets.json\\n  Restore completed in 1.26 sec for c:\\\\Source\\\\Debugging\\\\WebApplication1\\\\WebApplication1\\\\WebApplication1.csproj.\\n\\n  NuGet Config files used:\\n      C:\\\\Users\\\\johnr\\\\AppData\\\\Roaming\\\\NuGet\\\\NuGet.Config\\n      C:\\\\Program Files (x86)\\\\NuGet\\\\Config\\\\Microsoft.VisualStudio.Offline.config\\n\\n  Feeds used:\\n      https://api.nuget.org/v3/index.json\\n      C:\\\\Program Files (x86)\\\\Microsoft SDKs\\\\NuGetPackages\\\\\\nDone: 0.\\n```\\n\\nThe other prompt says `\\"Required assets to build and debug are missing from \'WebApplication1\'. Add them?\\"`. This also sounds very promising and I give it the nod. This creates a `.vscode` directory and 2 enclosed files; `launch.json` and `tasks.json`.\\n\\nSo lets try that F5 thing again... http://localhost:5000/ is now serving the same app. That looks pretty good. So lets add a breakpoint to the `HomeController` and see if we can hit it:\\n\\n![](firstgo.webp)\\n\\nWell I can certainly add a breakpoint but all those red squigglies are unnerving me. Let\'s clean the slate. If you want to simply do that in VS Code hold down `CTRL+SHIFT+P` and then type \\"reload\\". Pick \\"Reload window\\". A couple of seconds later we\'re back in and Code is looking much happier. Can we hit our breakpoint?\\n\\n![](secondgo.webp)\\n\\nYes we can! So you\'re free to develop in either Code or VS; the choice is yours. I think that\'s pretty awesome - and well done to all the peeople behind Code who\'ve made this a pretty seamless experience!"},{"id":"under-duck-afternoon-in-open-source","metadata":{"permalink":"/under-duck-afternoon-in-open-source","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2017-02-23-under-duck-afternoon-in-open-source/index.md","source":"@site/blog/2017-02-23-under-duck-afternoon-in-open-source/index.md","title":"Under the Duck: An Afternoon in Open Source","description":"A minute-by-minute account of how open source developers fixed an issue with ts-loader and webpack, demonstrating the collaborative nature of the community.","date":"2017-02-23T00:00:00.000Z","tags":[{"inline":false,"label":"ts-loader","permalink":"/tags/ts-loader","description":"The TypeScript loader for webpack."},{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."}],"readingTime":5.235,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"under-duck-afternoon-in-open-source","title":"Under the Duck: An Afternoon in Open Source","authors":"johnnyreilly","tags":["ts-loader","webpack"],"hide_table_of_contents":false,"description":"A minute-by-minute account of how open source developers fixed an issue with ts-loader and webpack, demonstrating the collaborative nature of the community."},"unlisted":false,"prevItem":{"title":"Debugging ASP.Net Core in VS or Code","permalink":"/debugging-aspnet-core-in-vs-or-code"},"nextItem":{"title":"@types is rogue","permalink":"/typescript-types-and-repeatable-builds"}},"content":"Have you ever wondered what happens behind the scenes of open source projects? One that I\'m involved with is [ts-loader](https://github.com/typestrong/ts-loader); a TypeScript loader for webpack. Yesterday was an interesting day in the life of ts-loader and webpack; things unexpectedly broke. Oh and don\'t worry, they\'re fixed now.\\n\\n\x3c!--truncate--\x3e\\n\\nHow things panned out reflects well on the webpack community. I thought it might be instructive to take a look at the legs furiously paddling underneath the duck of open source. What follows is a minute by minute account of my life on the afternoon of Wednesday 22nd February 2017:\\n\\n### 3:55pm\\n\\nI\'m sat at my desk in the City of London. I have to leave at 4pm to go to the dentist. I\'m working away on a project which is built and bundled using ts-loader and webpack. However, having just npm installed and tried to spin up webpack in watch mode, I discover that everything is broken. Watch mode is not working - there\'s an error being thrown in ts-loader. It\'s to do with a webpack property called `mtimes`. ts-loader depends upon it and it looks like it is no longer always passed through. Go figure.\\n\\n### 4:01pm\\n\\nI\'ve got to go. I\'m 15 minutes from Bank station. So, I grab my bag and scarper out the door. On my phone I notice [an issue](https://github.com/TypeStrong/ts-loader/issues/479) has been raised - other people are being affected by the problem too. As I trot down the various alleys that lead to the station I wonder whether I can work around this issue. Using GitHub to fork, edit code and submit a PR on a mobile phone is possible. Just. But it\'s certainly not easy...\\n\\n[My PR is in](https://github.com/TypeStrong/ts-loader/pull/481), the various test packs are starting to execute somewhere out there in Travis and Appveyor-land. Then I notice [Ed Bishop](https://github.com/mredbishop) has submitted a [near identical PR](https://github.com/TypeStrong/ts-loader/pull/480). Yay Ed! I\'m always keen to encourage people to contribute and so I intend to merge that PR rather than my own.\\n\\n### 4:12pm\\n\\nRubbish. The Waterloo and City Line is out of action. I need to get across London to reach Waterloo or I\'ll miss my appointment. It\'s time to start running....\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/4IBGernmtKA\\" frameBorder=\\"0\\" allowFullScreen=\\"\\"></iframe>\\n\\n### 4:15pm\\n\\nIt\'s rather nagging at me that behaviour has changed without warning. This has been reliably in place the entire time I\'ve been involved with ts-loader / webpack. Why now? I don\'t see any obvious mentions on the webpack GitHub repo. So I head over to the webpack Slack channel and ask: (conversation slightly abridged)\\n\\n> #### johnny_reilly\\n>\\n> Hey all, has something happened to `mtimes`? Behaviour seems to have changed - now undefined occasionally during watch mode. A PR has been raised against ts-loader to work around this [https://github.com/TypeStrong/ts-loader/pull/480#issuecomment-281714600](https://github.com/TypeStrong/ts-loader/pull/480#issuecomment-281714600)\\n>\\n> However I\'m wondering if this should actually be merged given behaviour has changed unexpectedly\\n>\\n> #### sokra\\n>\\n> ah...\\n>\\n> i removed it. I thought it was unused.\\n>\\n> #### johnny_reilly\\n>\\n> It\'s definitely not!\\n>\\n> #### sokra\\n>\\n> it\'s not in the public API^^\\n>\\n> Any reason why you are not using `getTimes()`?\\n>\\n> ...\\n>\\n> #### johnny_reilly\\n>\\n> Okay, I\'m on a train and won\'t be near a computer for a while. ts-loader is presently broken because it depends on mtimes. Would it be possible for you to add this back at least for now. I\'m aware many people depend on ts-loader and are now broken. #### sokra\\n>\\n> sure, I readd it but deprecate it.\\n>\\n> ...\\n>\\n> #### sean.larkin\\n>\\n> @sokra is this the change you just made for that watchpack bug fix? Or unlrelated, just wanted to track if I didn\'t already have the change/issue #### sokra\\n>\\n> [https://github.com/webpack/watchpack/pull/48](https://github.com/webpack/watchpack/pull/48)\\n>\\n> #### johnny_reilly\\n>\\n> This is what the present code does:\\n>\\n> ```js\\n> const watcher =\\n>   watching.compiler.watchFileSystem.watcher ||\\n>   watching.compiler.watchFileSystem.wfs.watcher;\\n> ```\\n>\\n> And then `.mtimes`\\n>\\n> Should I be able to do `.getTimes()` instead?\\n>\\n> #### sokra\\n>\\n> actually you can\'t rely on `watchFileSystem` being `NodeJsWatchFileSystem`. But this is another topic\\n>\\n> ...\\n>\\n> but yes\\n>\\n> #### johnny_reilly\\n>\\n> Thanks @sokra - when I get to a keyboard I\'ll swap `mtimes` for `getTimes()` and report back.\\n\\n### 5:28pm\\n\\nDespite various trains being out of action / missing in action I\'ve made it to the dentists; phew! I go in for my checkup and plan to take a look at the issue later that evening. In the meantime I\'ve hoping that Tobias ([Sokra](https://twitter.com/wsokra)) will get chance to republish so that ts-loader users aren\'t too impacted.\\n\\n### 6:00pm\\n\\nDone at the dentist and I\'m heading home. Whilst I\'ve been opening wide and squinting at the ceiling, [TypeScript 2.2 has shipped](https://blogs.msdn.microsoft.com/typescript/2017/02/22/announcing-typescript-2-2/). Whilst this is super exciting, according to Greenkeeper, [the new version has broken the build](https://github.com/TypeStrong/ts-loader/pull/483). Arrrrghhhh...\\n\\nI start to look into this and realise we\'re not broken because of TypeScript 2.2; we were broken because of the `mtimes`. Tobias has now re-added `mtimes` and published. With that in place I requeue a build and.... drum roll.... we\'re green!\\n\\nThe good news just keeps on coming as [Luka Zakraj\u0161ek](https://twitter.com/bancek) has submitted a [PR which uses `getTimes()` in place of `mtimes`](https://github.com/TypeStrong/ts-loader/pull/482). And the tests pass. Awesome! MERGE. I just need to cut a release and we\'re done.\\n\\n### 6:15pm\\n\\nI\'m home. My youngest son has been suffering from chicken pox all week and as a result my wife has been in isolation, taking care of him. We chat whilst the boys watch Paw Patrol as the bath runs. I flick open the laptop and start doing the various housekeeping tasks around cutting a release. This is interrupted by various bathtime / bedtime activities and I abandon work for now.\\n\\n### 7:30pm\\n\\nThe boys are down and I get on with the release; updating the changelog, bumping the version number and running the tests. For various reasons this takes longer than it normally does.\\n\\n### 8:30pm\\n\\nFinally we\'re there; ts-loader 2.0.1 ships: [https://github.com/TypeStrong/ts-loader/releases/tag/v2.0.1](https://github.com/TypeStrong/ts-loader/releases/tag/v2.0.1).\\n\\nI\'m tremendously grateful to everyone that helped out - thank you all!\\n\\n> ts-loader 2.0.1 has shipped; thanks [@wsokra](https://twitter.com/wSokra)[@bancek](https://twitter.com/bancek) and @mredbishop [https://t.co/I00c7sJyFo](https://t.co/I00c7sJyFo)[\\\\#typescript](https://twitter.com/hashtag/typescript?src=hash)\\n>\\n> [\u2014 John Reilly (@johnny_reilly) February 22, 2017](https://twitter.com/johnny_reilly/status/834515296077627392)"},{"id":"typescript-types-and-repeatable-builds","metadata":{"permalink":"/typescript-types-and-repeatable-builds","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2017-02-14-typescript-types-and-repeatable-builds/index.md","source":"@site/blog/2017-02-14-typescript-types-and-repeatable-builds/index.md","title":"@types is rogue","description":"Type definitions from Definitely Typed under @types namespace on npm cannot be trusted to follow semantic versioning, leading to breakages.","date":"2017-02-14T00:00:00.000Z","tags":[],"readingTime":2.03,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"typescript-types-and-repeatable-builds","title":"@types is rogue","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false,"description":"Type definitions from Definitely Typed under @types namespace on npm cannot be trusted to follow semantic versioning, leading to breakages."},"unlisted":false,"prevItem":{"title":"Under the Duck: An Afternoon in Open Source","permalink":"/under-duck-afternoon-in-open-source"},"nextItem":{"title":"Hands-free HTTPS","permalink":"/hands-free-https"}},"content":"Or perhaps I should call this \\"@types and repeatable builds\\"....\\n\\n\x3c!--truncate--\x3e\\n\\nThe other day, on a React / TypeScript project I work on, the nightly CI build started failing. But nothing had changed in the project... What gives? After digging I discovered the reason; spome of the type definitions which my project depends upon had changed. Why did this break my build? Let\u2019s learn some more...\\n\\nWe acquire type definitions via npm. Type definitions from Definitely Typed are published to npm by an [automated process](https://github.com/Microsoft/types-publisher) and they are all published under the @types namespace on npm. So, the [react type definition](https://www.npmjs.com/package/react) is published as the [@types/react](https://www.npmjs.com/package/@types/react) package, the node type definition is published as the [@types/node](https://www.npmjs.com/package/@types/node) package. The hip bone\'s connected to the thigh bone. You get the picture.\\n\\nThe npm ecosystem is essentially built on top of [semantic versioning](http://semver.org/) and they [take it seriously](https://docs.npmjs.com/getting-started/semantic-versioning). Essentially, when a package is published it should be categorised as a major release (breaking changes), a minor release (extra functionality which is backwards compatible) or a patch release (backwards compatible bug fixes).\\n\\nNow we get to the meat of the matter: @types is rogue. You cannot trust the version numbers on @types packages to respect semantic versioning. They don\'t.\\n\\nThe main reason for this is that when it comes to versioning, the @types type definition essentially looks to mirror the version of the package they are seeking to type. _THIS MEANS THE TYPE DEFINITION CANNOT DO ITS OWN SEMANTIC VERSIONING._ A simple change in a type definition can lead to breakages in consuming code. That\'s what happened to me. Let\'s say an exported interface name changes; all code that relies upon the old name will now break. You see? Pain.\\n\\n## How do we respond to this?\\n\\nMy own take has been to pin the version numbers of @types packages; fixing to specific definitions. No `\\"~\\"` or `\\"^\\"` for my `@types devDependencies`.\\n\\nNo respect semantic versioning? No problem. You can go much further with repeatable builds and made use of [facebook\'s new npm client yarn](https://code.facebook.com/posts/1840075619545360) and [lockfiles](https://yarnpkg.com/blog/2016/11/24/lockfiles-for-all/) (very popular BTW) but I haven\'t felt the need yet. This should be ample for now.\\n\\nThe other question that may be nagging at your subconscious is this: what\u2019s an easy way to know when new packages are available for my project dependencies? Well, the `Get-Package -Updates` (nuget hat tip) for npm that I\u2019d recommend is this: [npm-check-updates](https://www.npmjs.com/package/npm-check-updates). It does the job wonderfully."},{"id":"hands-free-https","metadata":{"permalink":"/hands-free-https","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2017-02-01-hands-free-https/index.md","source":"@site/blog/2017-02-01-hands-free-https/index.md","title":"Hands-free HTTPS","description":"CloudFlare provides free HTTPS certificates. As HTTPS becomes the web default, it is essential for search engine ranking and service workers.","date":"2017-02-01T00:00:00.000Z","tags":[],"readingTime":1.625,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"hands-free-https","title":"Hands-free HTTPS","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false,"description":"CloudFlare provides free HTTPS certificates. As HTTPS becomes the web default, it is essential for search engine ranking and service workers."},"unlisted":false,"prevItem":{"title":"@types is rogue","permalink":"/typescript-types-and-repeatable-builds"},"nextItem":{"title":"webpack: resolveLoader / alias with query / options","permalink":"/webpack-resolveloader-alias-with-query"}},"content":"I have had a \\\\***great**\\\\* week. You? Take a look at this blog. Can you see what I can see? Here\'s a clue:\\n\\n![](Screenshot-2017-01-29-14.45.57.webp)\\n\\n\x3c!--truncate--\x3e\\n\\nYup, look at the top left hand corner.... see that beautiful padlock? Yeah - that\'s what\'s thrilled me. You see I have a dream; that one day on the red hills of the internet, the sons of former certificates and the sons of former certificate authorities will be able to sit down together at the table of HTTPS. Peace, love and TLS for all.\\n\\nThe world is turning and slowly but surely HTTPS is becoming the default of the web. [Search results get ranked higher if they\'re HTTPS.](https://security.googleblog.com/2014/08/https-as-ranking-signal_6.html)[HTTP/2 is, to all intents and purposes, a HTTPS-only game.](https://en.wikipedia.org/wiki/HTTP/2#Encryption)[Service Workers are HTTPS-only.](https://developer.mozilla.org/en/docs/Web/API/Service_Worker_API)\\n\\nI care about all of these. So it\'s _essential_ that I have HTTPS. But. But. But... Certificates, the administration that goes with them. It\'s boring. I mean, it just is. I want to be building interesting apps, I don\'t want to be devoting my time to acquiring certificates and fighting my way through the (never simple) administration of them. I\'m dimly aware that there\'s free certificates to be had thanks to the fine work of [LetsEncrypt](https://letsencrypt.org/). I believe that work is being done on reduce the onerous admin burden as well. And that\'s great. But I\'m still avoiding it...\\n\\nWhat if I told you you could have HTTPS on your blog, on your Azure websites, on your anywhere.... _FOR FREE. IN FIVE MINUTES?_. Well, you can thanks to [CloudFlare](https://www.cloudflare.com/). I did; you should too.\\n\\nThis is where I point you off to a number of resources to help you on your HTTPS way:\\n\\n1. [Read Troy Hunt\'s \\"How to get your SSL for free on a Shared Azure website with CloudFlare\\"](https://www.troyhunt.com/how-to-get-your-ssl-for-free-on-shared/)\\n2. [Watch Troy Hunt\'s Pluralsight course \\"Getting Started with CloudFlare\u2122 Security\\"](https://www.pluralsight.com/courses/cloudflare-security-getting-started)\\n3. [Go to Cloudflare\'s website and sign up](https://www.cloudflare.com/)\\n\\nIt just works. And that makes me very happy indeed."},{"id":"webpack-resolveloader-alias-with-query","metadata":{"permalink":"/webpack-resolveloader-alias-with-query","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2017-01-06-webpack-resolveloader-alias-with-query/index.md","source":"@site/blog/2017-01-06-webpack-resolveloader-alias-with-query/index.md","title":"webpack: resolveLoader / alias with query / options","description":"Webpacks enhanced-resolve has a bug with aliased loaders. A workaround involves suffixing the aliased path with query options.","date":"2017-01-06T00:00:00.000Z","tags":[{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."}],"readingTime":1.38,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"webpack-resolveloader-alias-with-query","title":"webpack: resolveLoader / alias with query / options","authors":"johnnyreilly","tags":["webpack"],"hide_table_of_contents":false,"description":"Webpacks enhanced-resolve has a bug with aliased loaders. A workaround involves suffixing the aliased path with query options."},"unlisted":false,"prevItem":{"title":"Hands-free HTTPS","permalink":"/hands-free-https"},"nextItem":{"title":"webpack: configuring a loader with query / options","permalink":"/webpack-configuring-loader-with-query"}},"content":"Sometimes you write a post for the ages. Sometimes you write one you hope is out of date before you hit \\"publish\\". This is one of those.\\n\\n\x3c!--truncate--\x3e\\n\\nThere\'s a [bug](https://github.com/webpack/enhanced-resolve/issues/41) in webpack\'s enhanced-resolve. It means that you cannot configure an aliased loader using the `query` (or `options` in the webpack 2 nomenclature). Let me illustrate; consider the following code:\\n\\n```js\\nmodule.exports = {\\n  // ...\\n  module: {\\n    loaders: [\\n      {\\n        test: /\\\\.ts$/,\\n        loader: \'ts-loader\',\\n        query: {\\n            entryFileIsJs: true\\n        }\\n      }\\n    ]\\n  }\\n}\\n\\nmodule.exports.resolveLoader = { alias: { \'ts-loader\': require(\'path\').join(__dirname, \\"../../index.js\\")\\n```\\n\\nAt the time of writing, if you alias a loader as above, then the `query` / `options` will \\\\*_not_\\\\* be passed along. This is bad, particularly given the requirement in webpack 2 that configuration is no longer possible through extending the [`webpack.config.js`](https://webpack.js.org/guides/migrating/#loader-configuration-is-through-options). So what to do? Well, when this was a problem previously the marvellous [James Brantly](https://www.twitter.com/jbrantly) had a [workaround](https://github.com/webpack/webpack/issues/1289#issuecomment-125767499). I\'ve taken that and run with it:\\n\\n```js\\nvar config = {\\n  // ...\\n  module: {\\n    loaders: [\\n      {\\n        test: /\\\\.ts$/,\\n        loader: \'ts-loader\',\\n        query: {\\n          entryFileIsJs: true,\\n        },\\n      },\\n    ],\\n  },\\n};\\n\\nmodule.exports = config;\\n\\nvar loaderAliasPath = require(\'path\').join(__dirname, \'../../../index.js\');\\nvar rules = config.module.loaders || config.module.rules;\\nrules.forEach(function (rule) {\\n  var options = rule.query || rule.options;\\n  rule.loader = rule.loader.replace(\\n    \'ts-loader\',\\n    loaderAliasPath + (options ? \'?\' + JSON.stringify(options) : \'\'),\\n  );\\n});\\n```\\n\\nThis approach stringifies the `query` / `options` and suffixes it to the aliased path. This works as long as the options you\'re passing are JSON-able (yes it\'s a word).\\n\\nAs I said earlier; hopefully by the time you read this the workaround will no longer be necessary again. But just in case...."},{"id":"webpack-configuring-loader-with-query","metadata":{"permalink":"/webpack-configuring-loader-with-query","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2017-01-01-webpack-configuring-loader-with-query/index.md","source":"@site/blog/2017-01-01-webpack-configuring-loader-with-query/index.md","title":"webpack: configuring a loader with query / options","description":"webpack 2 now enforces a strict schema for `webpack.config.js`. Loaders should be configured using `query` or `options`.","date":"2017-01-01T00:00:00.000Z","tags":[{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."}],"readingTime":2.805,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"webpack-configuring-loader-with-query","title":"webpack: configuring a loader with query / options","authors":"johnnyreilly","tags":["webpack"],"hide_table_of_contents":false,"description":"webpack 2 now enforces a strict schema for `webpack.config.js`. Loaders should be configured using `query` or `options`."},"unlisted":false,"prevItem":{"title":"webpack: resolveLoader / alias with query / options","permalink":"/webpack-resolveloader-alias-with-query"},"nextItem":{"title":"Using ts-loader with webpack 2","permalink":"/using-ts-loader-with-webpack-2"}},"content":"[webpack 2 is on it\'s way](https://medium.com/webpack/webpack-2-2-the-release-candidate-2e614d05d75f#.ntniu44u6). As one of the maintainers of [ts-loader](https://github.com/TypeStrong/ts-loader/) I\'ve been checking out that ts-loader works with webpack 2. It does: phew!\\n\\n\x3c!--truncate--\x3e\\n\\nts-loader has a continuous integration build that runs against webpack 1. When webpack 2 ships we\'re planning to move to running CI against webpack 2. However, webpack 2 has some breaking changes. The one that\'s particularly of relevance to our test packs is that a strict schema is now enforced for `webpack.config.js` with webpack 2. This has been the case since webpack 2 hit beta 23. Check the [PR that added it](https://github.com/webpack/webpack/pull/2974). You can see some of the [frankly tortured discussion that this generated as well](https://github.com/webpack/webpack/issues/3018).\\n\\nLet\'s all take a moment and realise that working on open source is sometimes a rather painful experience. Take a breath. Breathe out. Ready to carry on? Great.\\n\\nThere are 2 ways to configure loader options for ts-loader (and in fact this stands for most loaders). Loader options can be set either using a `query` when specifying the loader or through the `ts` (insert the name of alternative loaders here) property in the `webpack.config.js`.\\n\\nThe implicatations of the breaking change are: with webpack 2 you can **no longer** configure ts-loader (or any other loader) with a `ts` (insert the name of alternative loaders here) property in the `webpack.config.js`. It **must** be done through the `query` / `options`. The following code is no longer valid with webpack 2:\\n\\n```js\\nmodule.exports = {\\n  ...\\n  module: {\\n    loaders: [{\\n      test: /\\\\.tsx?$/,\\n      loader: \'ts-loader\'\\n    }]\\n  },\\n  // specify option using `ts` property - **only do this if you are using webpack 1**\\n  ts: {\\n    transpileOnly: false\\n  }\\n}\\n```\\n\\nThis change means that we have needed to adjust how our test pack works. We can no longer make use of `ts` for configuration. Since I wasn\'t terribly aware of `query` I thought it made sense to share my learnings.\\n\\n## What exactly is `query` / `options`?\\n\\nGood question. Well, strictly speaking it\'s 2 possible things; both ways to configure a webpack loader. Classically `query` was a string which could be appended to the name of the loader much like a [`query string`](https://en.wikipedia.org/wiki/Query_string) but actually with [greater powers](https://github.com/webpack/loader-utils#parsequery):\\n\\n```js\\nmodule.exports = {\\n  ...\\n  module: {\\n    loaders: [{\\n      test: /\\\\.tsx?$/,\\n      loader: \'ts-loader?\' + JSON.stringify({\\n        transpileOnly: false\\n      })\\n    }]\\n  }\\n}\\n```\\n\\nBut it can also be a separately specified object that\'s supplied alongside a loader (I understand this is relatively new behaviour):\\n\\n```js\\nmodule.exports = {\\n  ...\\n  module: {\\n    loaders: [{\\n      test: /\\\\.tsx?$/,\\n      loader: \'ts-loader\'\\n      query: {\\n        transpileOnly: false\\n      }\\n    }]\\n  }\\n}\\n```\\n\\n## webpack 2 is coming - look busy!\\n\\nSo if you\'re planning to move to webpack 2, be aware of this breaking change. You can start moving to using configuration via query right now with webpack 1. You don\'t need to be using webpack 2 to make the jump. So jump!\\n\\nFinally, and by way of a PS, `query` is renamed to `options` in webpack 2; a much better name to my mind. There\'s actually a bunch of other renames on the way as well - check out the [migration guide](https://webpack.js.org/guides/migrating/#module-loaders-is-now-module-rules) for more on this. The important thing to note is that **the old names work in webpack 2**. But you should plan to move to the new naming at some point as they\'ll likely disappear when webpack 3 ships."},{"id":"using-ts-loader-with-webpack-2","metadata":{"permalink":"/using-ts-loader-with-webpack-2","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2016-12-19-using-ts-loader-with-webpack-2/index.md","source":"@site/blog/2016-12-19-using-ts-loader-with-webpack-2/index.md","title":"Using ts-loader with webpack 2","description":"TypeScript loader ts-loader has made its loader compatible with webpack 2. The update allows greater compatibility between the two applications.","date":"2016-12-19T00:00:00.000Z","tags":[{"inline":false,"label":"ts-loader","permalink":"/tags/ts-loader","description":"The TypeScript loader for webpack."},{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."}],"readingTime":7.85,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"using-ts-loader-with-webpack-2","title":"Using ts-loader with webpack 2","authors":"johnnyreilly","tags":["ts-loader","webpack"],"hide_table_of_contents":false,"description":"TypeScript loader ts-loader has made its loader compatible with webpack 2. The update allows greater compatibility between the two applications."},"unlisted":false,"prevItem":{"title":"webpack: configuring a loader with query / options","permalink":"/webpack-configuring-loader-with-query"},"nextItem":{"title":"webpack: syncing the enhanced-resolve","permalink":"/webpack-syncing-enhanced-resolve"}},"content":"Hands up, despite being one of the maintainers of [ts-loader](https://github.com/TypeStrong/ts-loader) (a TypeScript loader for webpack) I have not been tracking webpack v2. My reasons? Well, I\'m keen on cutting edge but bleeding edge is often not a ton of fun as dealing with regularly breaking changes is frustrating. I\'m generally happy to wait for things to settle down a bit before leaping aboard. However, [webpack 2 RC\'d last week](https://github.com/webpack/webpack/releases/tag/v2.2.0-rc.0) and so it\'s time to take a look!\\n\\n\x3c!--truncate--\x3e\\n\\n## Porting our example\\n\\nLet\'s take [ts-loader\'s webpack 1 example](https://github.com/TypeStrong/ts-loader/tree/master/examples/webpack1-gulp-react-flux-babel-karma) and try and port it to webpack 2. Will it work? Probably; I\'m aware of other people using ts-loader with webpack 2. It\'ll be a voyage of discovery. Like Darwin on the Beagle, I shall document our voyage for a couple of reasons:\\n\\n- I\'m probably going to get some stuff wrong. That\'s fine; one of the best ways to learn is to make mistakes. So do let me know where I go wrong.\\n- I\'m doing this based on what I\'ve read in the new docs; they\'re very much a work in progress and the mistakes I make here may lead to those docs improving even more. That matters; **documentation matters**. I\'ll be leaning heavily on the [Migrating from v1 to v2](https://webpack.js.org/guides/migrating/) guide.\\n\\nSo here we go. Our example is one which uses TypeScript for static typing and uses Babel to transpile from ES-super-modern (yes - it\'s a thing) to ES-older-than-that. Our example also uses React; but that\'s somewhat incidental. It only uses webpack for typescript / javascript and karma. It uses gulp to perform various other tasks; so if you\'re reliant on webpack for less / sass compilation etc then I have no idea whether that works.\\n\\nFirst of all, let\'s install the latest RC of webpack:\\n\\n```ts\\nnpm install webpack@2.2.0-rc.1 --save-dev\\n```\\n\\n## `webpack.config.js`\\n\\nLet\'s look at our existing `webpack.config.js`:\\n\\n```js\\n\'use strict\';\\n\\nvar path = require(\'path\');\\n\\nmodule.exports = {\\n  cache: true,\\n  entry: {\\n    main: \'./src/main.tsx\',\\n    vendor: [\'babel-polyfill\', \'fbemitter\', \'flux\', \'react\', \'react-dom\'],\\n  },\\n  output: {\\n    path: path.resolve(__dirname, \'./dist/scripts\'),\\n    filename: \'[name].js\',\\n    chunkFilename: \'[chunkhash].js\',\\n  },\\n  module: {\\n    loaders: [\\n      {\\n        test: /\\\\.ts(x?)$/,\\n        exclude: /node_modules/,\\n        loader:\\n          \'babel-loader?presets[]=es2016&presets[]=es2015&presets[]=react!ts-loader\',\\n      },\\n      {\\n        test: /\\\\.js$/,\\n        exclude: /node_modules/,\\n        loader: \'babel\',\\n        query: {\\n          presets: [\'es2016\', \'es2015\', \'react\'],\\n        },\\n      },\\n    ],\\n  },\\n  plugins: [],\\n  resolve: {\\n    extensions: [\'\', \'.webpack.js\', \'.web.js\', \'.ts\', \'.tsx\', \'.js\'],\\n  },\\n};\\n```\\n\\nThere\'s a number of things we need to do here. First of all, we can get rid of the empty extension under resolve; I understand that\'s unnecessary now. Also, I\'m going to get rid of `\'.webpack.js\'` and `\'.web.js\'`; I never used them anyway. Also, just having `\'babel\'` as a loader won\'t fly anymore. We need that suffix as well.\\n\\nNow I could start renaming `loaders` to `rules` as the terminology is changing. But I\'d like to deal with that later since I know the old school names are still supported at present. More interestingly, I seem to remember hearing that one of the super exciting things about webpack is that it supports modules directly now. (I think that\'s supposed to be good for tree-shaking but I\'m not totally certain.)\\n\\nInitially I thought I was supposed to switch to a custom babel preset called [`babel-preset-es2015-webpack`](https://www.npmjs.com/package/babel-preset-es2015-webpack). However it has a big \\"DEPRECATED\\" mark at the top and it says I should just use `babel-preset-es2015` (which I already am) with the following option specified:\\n\\n```js\\n{\\n    \\"presets\\": [\\n        [\\n            \\"es2015\\",\\n            {\\n                \\"modules\\": false\\n            }\\n        ]\\n    ]\\n}\\n```\\n\\nLooking at our existing config you\'ll note that for `js` files we\'re using `query` (`options` in the new world I understand) to configure babel usage. We\'re using [query parameters](https://webpack.github.io/docs/using-loaders.html#query-parameters) for `ts` files. I have _zero_ idea how to configure preset options using query parameters. Fiddling with `query` / `options` didn\'t seem to work. So, I\'ve decided to abandon using query entirely and drop in a [`.babelrc`](http://babeljs.io/docs/usage/babelrc/) file using our presets combined with the [`modules`](https://babeljs.io/docs/plugins/#plugin-preset-options) setting:\\n\\n```js\\n{\\n   \\"presets\\": [\\n      \\"react\\",\\n      [\\n         \\"es2015\\",\\n         {\\n            \\"modules\\": false\\n         }\\n      ],\\n      \\"es2016\\"\\n   ]\\n}\\n```\\n\\nAs an aside; apparently these are applied in reverse order. So `es2016` is applied first, `es2015` second and `react` third. I\'m not totally certain this is correct; the `<a href=\\"http://babeljs.io/docs/usage/babelrc/\\">.babelrc</a> docs` are a little unclear.\\n\\nWith our query options extracted we\'re down to a simpler `webpack.config.js`:\\n\\n```js\\n\'use strict\';\\n\\nvar path = require(\'path\');\\n\\nmodule.exports = {\\n  cache: true,\\n  entry: {\\n    main: \'./src/main.tsx\',\\n    vendor: [\'babel-polyfill\', \'fbemitter\', \'flux\', \'react\', \'react-dom\'],\\n  },\\n  output: {\\n    path: path.resolve(__dirname, \'./dist/scripts\'),\\n    filename: \'[name].js\',\\n    chunkFilename: \'[chunkhash].js\',\\n  },\\n  module: {\\n    loaders: [\\n      {\\n        test: /\\\\.ts(x?)$/,\\n        exclude: /node_modules/,\\n        loader: \'babel-loader!ts-loader\',\\n      },\\n      {\\n        test: /\\\\.js$/,\\n        exclude: /node_modules/,\\n        loader: \'babel-loader\',\\n      },\\n    ],\\n  },\\n  plugins: [],\\n  resolve: {\\n    extensions: [\'.ts\', \'.tsx\', \'.js\'],\\n  },\\n};\\n```\\n\\n## `plugins`\\n\\nIn our example the `plugins` section of our `webpack.config.js` is extended in a separate process. Whilst we\'re developing we also set the `debug` flag to be `true`. [It seems we need to introduce a `LoaderOptionsPlugin` to do this for us.](https://webpack.js.org/guides/migrating/#debug)\\n\\nAs we introduce our `LoaderOptionsPlugin` we also need to make sure that we provide it with `options`. How do I know this? Well [someone raised an issue against ts-loader](https://github.com/TypeStrong/ts-loader/issues/283). I don\'t think this is actually an issue with ts-loader; I think it\'s just a webpack 2 thing. I could be wrong; answers on a postcard please.\\n\\nEither way, to get up and running we just need the `LoaderOptionsPlugin` in play. Consequently, most of what follows in our `webpack.js` file is unchanged:\\n\\n```js\\n// .....\\n\\nvar webpackConfig = require(\'../webpack.config.js\');\\nvar packageJson = require(\'../package.json\');\\n\\n// .....\\n\\nfunction buildProduction(done) {\\n  // .....\\n\\n  myProdConfig.plugins = myProdConfig.plugins.concat(\\n    // .....\\n\\n    // new webpack.optimize.DedupePlugin(), Not a thing anymore apparently\\n    new webpack.optimize.UglifyJsPlugin(),\\n\\n    // I understand this here matters...\\n    // but it doesn\'t seem to make any difference; perhaps I\'m missing something?\\n    new webpack.LoaderOptionsPlugin({\\n      minimize: true,\\n      debug: false,\\n    }),\\n\\n    failPlugin,\\n  );\\n\\n  // .....\\n}\\n\\nfunction createDevCompiler() {\\n  var myDevConfig = webpackConfig;\\n  myDevConfig.devtool = \'inline-source-map\';\\n  // myDevConfig.debug = true; - not allowed in webpack 2\\n\\n  myDevConfig.plugins = myDevConfig.plugins.concat(\\n    new webpack.optimize.CommonsChunkPlugin({\\n      name: \'vendor\',\\n      filename: \'vendor.js\',\\n    }),\\n    new WebpackNotifierPlugin({\\n      title: \'webpack build\',\\n      excludeWarnings: true,\\n    }),\\n\\n    // this is the webpack 2 hotness!\\n    new webpack.LoaderOptionsPlugin({\\n      debug: true,\\n      options: myDevConfig,\\n    }),\\n    // it ends here - there wasn\'t much really....\\n  );\\n\\n  // create a single instance of the compiler to allow caching\\n  return webpack(myDevConfig);\\n}\\n\\n// .....\\n```\\n\\n## `LoaderOptionsPlugin` we hardly new ya\\n\\nAfter a little more experimentation it seems that the `LoaderOptionsPlugin` is not necessary at all for our own use case. In fact it\'s probably not best practice to get used to using it as it\'s only intended to live a short while whilst people move from webpack 1 to webpack 2. In that vein let\'s tweak our `webpack.js` file once more:\\n\\n```js\\nfunction buildProduction(done) {\\n  // .....\\n\\n  myProdConfig.plugins = myProdConfig.plugins.concat(\\n    // .....\\n\\n    new webpack.optimize.UglifyJsPlugin({\\n      compress: {\\n        warnings: true,\\n      },\\n    }),\\n\\n    failPlugin,\\n  );\\n\\n  // .....\\n}\\n\\nfunction createDevCompiler() {\\n  var myDevConfig = webpackConfig;\\n  myDevConfig.devtool = \'inline-source-map\';\\n\\n  myDevConfig.plugins = myDevConfig.plugins.concat(\\n    new webpack.optimize.CommonsChunkPlugin({\\n      name: \'vendor\',\\n      filename: \'vendor.js\',\\n    }),\\n    new WebpackNotifierPlugin({\\n      title: \'webpack build\',\\n      excludeWarnings: true,\\n    }),\\n  );\\n\\n  // create a single instance of the compiler to allow caching\\n  return webpack(myDevConfig);\\n}\\n\\n// .....\\n```\\n\\n## `karma.conf.js`\\n\\nFinally Karma. Our `karma.conf.js` with webpack 1 looked like this:\\n\\n```js\\n/* eslint-disable no-var, strict */\\n\'use strict\';\\n\\nvar webpackConfig = require(\'./webpack.config.js\');\\n\\nmodule.exports = function (config) {\\n  // Documentation: https://karma-runner.github.io/0.13/config/configuration-file.html\\n  config.set({\\n    browsers: [\'PhantomJS\'],\\n\\n    files: [\\n      // This ensures we have the es6 shims in place and then loads all the tests\\n      \'test/main.js\',\\n    ],\\n\\n    port: 9876,\\n\\n    frameworks: [\'jasmine\'],\\n\\n    logLevel: config.LOG_INFO, //config.LOG_DEBUG\\n\\n    preprocessors: {\\n      \'test/main.js\': [\'webpack\', \'sourcemap\'],\\n    },\\n\\n    webpack: {\\n      devtool: \'inline-source-map\',\\n      debug: true,\\n      module: webpackConfig.module,\\n      resolve: webpackConfig.resolve,\\n    },\\n\\n    webpackMiddleware: {\\n      quiet: true,\\n      stats: {\\n        colors: true,\\n      },\\n    },\\n\\n    // reporter options\\n    mochaReporter: {\\n      colors: {\\n        success: \'bgGreen\',\\n        info: \'cyan\',\\n        warning: \'bgBlue\',\\n        error: \'bgRed\',\\n      },\\n    },\\n  });\\n};\\n```\\n\\nWe just need to chop out the `debug` statement from the `webpack` section like so:\\n\\n```js\\nmodule.exports = function(config) {\\n\\n  // .....\\n\\n    webpack: {\\n      devtool: \'inline-source-map\',\\n      module: webpackConfig.module,\\n      resolve: webpackConfig.resolve\\n    },\\n\\n  // .....\\n\\n  });\\n};\\n```\\n\\n## Compare and contrast\\n\\nWe now have a repo that works with webpack 2 rc 1. Yay! If you\'d like to see it then take a look [here](https://github.com/TypeStrong/ts-loader/tree/master/examples/webpack2-gulp-react-flux-babel-karma).\\n\\nI thought I\'d compare performance / output size of compiling with webpack 1 to webpack 2. First of all in debug / development mode:\\n\\n```ts\\n// webpack 1\\n\\nVersion: webpack 1.14.0\\nTime: 5063ms\\n    Asset     Size  Chunks             Chunk Names\\n  main.js  37.2 kB       0  [emitted]  main\\nvendor.js  2.65 MB       1  [emitted]  vendor\\n\\n// webpack 2\\n\\nVersion: webpack 2.2.0-rc.1\\nTime: 5820ms\\n    Asset     Size  Chunks                    Chunk Names\\n  main.js  38.7 kB       0  [emitted]         main\\nvendor.js  2.63 MB       1  [emitted]  [big]  vendor\\n```\\n\\nSize and compilation time is not massively different from webpack 1 to webpack 2. It\'s all about the same. I\'m not sure if that\'s to be expected or not.... Though I\'ve a feeling in production mode I\'m supposed to feel the benefits of tree shaking so let\'s have a go:\\n\\n```ts\\n// webpack 1\\n\\nVersion: webpack 1.14.0\\nTime: 5788ms\\n                         Asset     Size  Chunks             Chunk Names\\n  main.269c66e1bc13b7426cee.js  10.5 kB       0  [emitted]  main\\nvendor.269c66e1bc13b7426cee.js   231 kB       1  [emitted]  vendor\\n\\n// webpack 2\\n\\nVersion: webpack 2.2.0-rc.1\\nTime: 5659ms\\n                         Asset     Size  Chunks             Chunk Names\\n  main.33e0d70eeec29206e9b6.js  9.22 kB       0  [emitted]  main\\nvendor.33e0d70eeec29206e9b6.js   233 kB       1  [emitted]  vendor\\n```\\n\\nTo my surprise this looks pretty much unchanged before and after as well. This may be a sign I have missed something crucial out. Or maybe that\'s to be expected. Do give me a heads up if I\'ve missed something..."},{"id":"webpack-syncing-enhanced-resolve","metadata":{"permalink":"/webpack-syncing-enhanced-resolve","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2016-12-11-webpack-syncing-enhanced-resolve/index.md","source":"@site/blog/2016-12-11-webpack-syncing-enhanced-resolve/index.md","title":"webpack: syncing the enhanced-resolve","description":"How to create a sync webpack resolver instead of the default async resolver using `enhanced-resolve`.","date":"2016-12-11T00:00:00.000Z","tags":[{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."}],"readingTime":2.37,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"webpack-syncing-enhanced-resolve","title":"webpack: syncing the enhanced-resolve","authors":"johnnyreilly","tags":["webpack"],"hide_table_of_contents":false,"description":"How to create a sync webpack resolver instead of the default async resolver using `enhanced-resolve`."},"unlisted":false,"prevItem":{"title":"Using ts-loader with webpack 2","permalink":"/using-ts-loader-with-webpack-2"},"nextItem":{"title":"My Subconscious is a Better Developer Than I Am","permalink":"/my-subconscious-is-better-developer"}},"content":"Like Captain Ahab I resolve to sync the white whale that is webpack\'s [`enhanced-resolve`](https://github.com/webpack/enhanced-resolve)... English you say? Let me start again:\\n\\n\x3c!--truncate--\x3e\\n\\nSo, you\'re working on a webpack loader. (In my case the typescript loader; [`ts-loader`](https://github.com/TypeStrong/ts-loader)) You have need of webpack\'s resolve capabilities. You dig around and you discover that that superpower is lodged in the very heart of the enhanced-resolve package. Fantastic. But wait, there\'s more: your needs are custom. You need a sync, not an async resolver. (Try saying that quickly.) You regard the description of `enhanced-resolve` with some concern:\\n\\n> \\"Offers an async require.resolve function. It\'s highly configurable.\\"\\n\\nWell that doesn\'t sound too promising. Let\'s have a look at the docs. Ah. Hmmm. You know how it goes with webpack. Why document anything clearly when people could just guess wildly until they near insanity and gibber? Right? It\'s well established that webpack\'s attitude to docs has been traditionally akin to Gordon Gecko\'s view on lunch.\\n\\n![](documentation-is-for-wimps.webp)\\n\\nIn all fairness, things are beginning to change on that front. In fact the [new docs](https://webpack.js.org/) look very promising. But regrettably, the docs on the enhanced-resolve repo are old school. Which is to say: opaque. However, I\'m here to tell you that if a sync resolver is your baby then, contrary to appearances, `enhanced-resolve` has your back.\\n\\n## Sync, for lack of a better word, is good\\n\\nNestled inside enhanced-resolve is the [`ResolverFactory.js`](https://github.com/webpack/enhanced-resolve/blob/3f3f4cd1fcbafa1e98c3c6470fed1277817ed607/lib/ResolverFactory.js) which can be used to make a resolver. However, you can supply it with a million options and that\'s just like giving someone a gun with a predilection for feet.\\n\\nWhat you want is an example of how you could make a sync resolver. Well, surprise surprise it\'s right in front of your nose. Tucked away in [`node.js`](https://github.com/webpack/enhanced-resolve/blob/3f3f4cd1fcbafa1e98c3c6470fed1277817ed607/lib/node.js) (I do \\\\***not**\\\\* get the name) is exactly what you\'re after. It contains a number of factory functions which will construct a ready-made resolver for you; sync or async. Perfect! So here\'s how I\'m rolling:\\n\\n```js\\nconst node = require(\'enhanced-resolve/lib/node\');\\n\\nfunction makeSyncResolver(options) {\\n  return node.create.sync(options.resolve);\\n}\\n\\nconst resolveSync = makeSyncResolver(loader.options);\\n```\\n\\nThe loader options used above you\'ll be familiar with as the `resolve` section of your `webpack.config.js`. You can read more about them [here](https://github.com/webpack/enhanced-resolve/blob/master/README/index.md) and [here](https://webpack.js.org/configuration/resolve/).\\n\\nWhat you\'re left with at this point is a function; a `resolveSync` function if you will that takes 3 arguments:\\n\\n<dl><dt>context</dt><dd>I don\'t know what this is. So when using the function I just supply <code>undefined</code>; and that seems to be OK. Weird, right?</dd><dt>path</dt><dd>This is the path to your code (I think). So, a valid value to supply - handily lifted from the ts-loader test pack - would be: <code>C:\\\\source\\\\ts-loader\\\\.test\\\\babel-issue92</code></dd><dt>request</dt><dd>The actual module you\'re interested in; so using the same test the relevant value would be <code>./submodule/submodule</code></dd></dl>\\n\\nPut it all together and what have you got?\\n\\n```js\\nconst resolvedFileName = resolveSync(\\n  undefined,\\n  \'C:source\\\\ts-loader.test\\\\babel-issue92\',\\n  \'./submodule/submodule\',\\n);\\n\\n// resolvedFileName: C:\\\\source\\\\ts-loader\\\\.test\\\\babel-issue92\\\\submodule\\\\submodule.tsx\\n```\\n\\nBoom."},{"id":"my-subconscious-is-better-developer","metadata":{"permalink":"/my-subconscious-is-better-developer","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2016-11-12-my-subconscious-is-better-developer/index.md","source":"@site/blog/2016-11-12-my-subconscious-is-better-developer/index.md","title":"My Subconscious is a Better Developer Than I Am","description":"In which I wonder if my subconscious is a better developer than I am, as solutions seem to come to mind, bypassing the work I consciously put in.","date":"2016-11-12T00:00:00.000Z","tags":[],"readingTime":1.84,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"my-subconscious-is-better-developer","title":"My Subconscious is a Better Developer Than I Am","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false,"description":"In which I wonder if my subconscious is a better developer than I am, as solutions seem to come to mind, bypassing the work I consciously put in."},"unlisted":false,"prevItem":{"title":"webpack: syncing the enhanced-resolve","permalink":"/webpack-syncing-enhanced-resolve"},"nextItem":{"title":"But you can\'t die... I love you!","permalink":"/but-you-cant-die-i-love-you-ts-loader"}},"content":"Occasionally I flatter myself that I\'m alright at this development lark. Such egotistical talk is foolish. What makes me pause even more when I consider the proposition is this: my subconscious is a better developer than I am.\\n\\n\x3c!--truncate--\x3e\\n\\nWhat\'s this fellow talking about?\\n\\nThere\'s two of me. Not identical twins; masquerading as a single man (spoiler: I am not a Christopher Nolan movie). No. There\'s me, the chap who\'s tapping away at his keyboard and solving a problem. And there\'s the other chap too.\\n\\nI have days when I\'m working away at something and I\'ll hit a brick wall. I produce solutions that work but are not elegant. I\'m not proud of them. Or worse, I fail to come up with something that solves the problem I\'m facing. So I go home. I see my family, I have some food, I do something else. I context switch. I go to sleep.\\n\\nWhen I awake, sometimes (not always) I\'ll have waiting in my head a better solution. I can see the solution in my head. I can turn it over and compare it to what, if anything, I currently have and see the reasons the new approach is better. Great, right? Up to a point.\\n\\nWhat concerns me is this: I didn\'t work this out from first principles. The idea arrived sight unseen in my head. It totally works but whose work actually is it? I feel like I\'m taking credit for someone else\'s graft. This is probably why I\'m so keen on the MIT License. Don\'t want to be caught out.\\n\\nI think I\'d like it better if I was a better developer than my subconscious. I\'d come up with the gold and mock the half baked ideas he shows me in the morning. Alas it is not to be.\\n\\nI draw some comfort from the knowledge that I\'m not alone in my experience. I\'ve chatted to other devs in the same boat. There\'s probably two of you as well. Amarite? There\'s probably three of Jon Skeet; each more brilliant than the last...\\n\\n![a poster from the film Being John Malkovich](beingjohnm.webp)\\n\\nPS I posted this to Hacker News and [the comments left by people are pretty fascinating](https://news.ycombinator.com/item?id=12942461)."},{"id":"but-you-cant-die-i-love-you-ts-loader","metadata":{"permalink":"/but-you-cant-die-i-love-you-ts-loader","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2016-11-01-but-you-cant-die-i-love-you-ts-loader/index.md","source":"@site/blog/2016-11-01-but-you-cant-die-i-love-you-ts-loader/index.md","title":"But you can\'t die... I love you!","description":"How John Reilly becomes main caretaker of ts-loader, fixing bugs and actively maintaining the project to encourage communal contributions.","date":"2016-11-01T00:00:00.000Z","tags":[{"inline":false,"label":"ts-loader","permalink":"/tags/ts-loader","description":"The TypeScript loader for webpack."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":4.86,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"but-you-cant-die-i-love-you-ts-loader","title":"But you can\'t die... I love you!","authors":"johnnyreilly","tags":["ts-loader","typescript"],"image":"./title-image.png","hide_table_of_contents":false,"description":"How John Reilly becomes main caretaker of ts-loader, fixing bugs and actively maintaining the project to encourage communal contributions."},"unlisted":false,"prevItem":{"title":"My Subconscious is a Better Developer Than I Am","permalink":"/my-subconscious-is-better-developer"},"nextItem":{"title":"React Component Curry","permalink":"/react-component-curry"}},"content":"That\'s how I was feeling on the morning of October 6th 2016. I\'d been feeling that way for some time. The target of my concern? [ts-loader](https://github.com/TypeStrong/ts-loader). ts-loader is a loader for [webpack; the module bundler](https://webpack.github.io/). ts-loader allows you use TypeScript with webpack. I\'d been a merry user of it for at least a year or so. But, at that point, all was not well in the land of ts-loader. Come with me and I\'ll tell you a story...\\n\\n![a poster that reads: \\"But you can\'t die... I love you!\\"](title-image.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Going Red\\n\\nAt some point, I became a member of the [TypeStrong](https://github.com/TypeStrong) organisation on GitHub. I\'m honestly not entirely sure how. I think it may have been down to the very excellent [Basarat](https://youtube.com/basaratali) (he of [ALM](http://alm.tools/) / [atom-typescript](https://github.com/TypeStrong/atom-typescript) / the list goes on fame) but I couldn\'t clearly say.\\n\\nEither way, [James Brantly](https://github.com/jbrantly)\'s ts-loader was also one of TypeStrong\'s projects. Since I used it, I occasionally contributed. Not much to be honest; mostly it was documentation tweaks. I mean I never really looked at the main code at all. It worked (thanks to other people). I just plugged it into my projects and ploughed on my merry way. I liked it. It was well established; with friendly maintainers. It had a continuous integration test pack that ran against multiple versions of TypeScript on both Windows and Linux. I trusted it. Then one day the continuous integration tests went red. And stayed red.\\n\\nThis is where we came in. On the morning of October 6th I was mulling what to do about this. I knew there was another alternative out there (awesome-typescript-loader) but I was a little wary of it. My understanding of ATL was that it targeted webpack 2.0 which has long been in beta. Where I ply my trade (mostly developing software for the financial sector in the City of London) beta is not a word that people trust. They don\'t do beta. What\'s more I was quite happy with ts-loader; I didn\'t want to switch if I didn\'t have to. I also rather suspected (rightly) that there wasn\'t much wrong; ts-loader just needed a little bit of love. So I thought: I bet I can help here.\\n\\n## The Statement of Intent\\n\\nSo that evening I raised [an issue against ts-loader](https://github.com/TypeStrong/ts-loader/issues/296). Not a \\"sort it out chap\\" issue. No. That wouldn\'t be terribly helpful. I raised a \\"here\'s how I can help\\" issue. I present an abridged version below:\\n\\n> Okay here\'s the deal; I\'ve been using ts-loader for a long time but my contributions up until now have mostly been documentation. Fixing of tests etc. As the commit history shows this is [@jbrantly](https://github.com/jbrantly)\'s baby and kudos to him.\\n>\\n> He\'s not been able to contribute much of late and since he\'s the main person who\'s worked on ts-loader not much has happened for a while; the code is a bit stale. As I\'m a member of TypeStrong I\'m going to have a go at improving the state of the project. I\'m going to do this as carefully as I can. This issue is intended as a meta issue to make it visible what I\'m plannning to do / doing.\\n>\\n> My immediate goal is to get a newer version of ts-loader built and shipped. Essentially all the bug fixes / tweaks since the last release should ship.\\n>\\n> ...\\n>\\n> I don\'t have npm publish rights for ts-loader. Fortunately both [@jbrantly](https://github.com/jbrantly) and [@blakeembrey](https://github.com/blakeembrey) do - and hopefully one of them will either be able to help out with a publish or let me have the requisite rights to do it.\\n>\\n> I can\'t promise this is all going to work; I\'ve got a limited amount of spare time I\'m afraid. Whatever happens it\'s going to take me a little while. But I\'m going to see where I can take this. Best foot forward! Please bear with me...\\n\\nI did wonder what would happen next. This happened next:\\n\\n[![tweet from James Brantly on October 11, 2016 that reads: \\"My #opensourceguilt has been lifted thanks to @johnny_reilly stepping up to take over ts-loader. Thanks man!\\"](screenshot-james-brantly-tweet.webp)](https://twitter.com/jbrantly/status/785931975064444928)\\n\\n## Caretaker, not [BDFL](https://en.wikipedia.org/wiki/Benevolent_dictator_for_life)\\n\\nSo that\'s how it came to pass that I became the present main caretaker of ts-loader. James very kindly gave me the rights to publish to npm and soon enough I did. I fixed up the existing integration test pack; made it less brittle. I wrote a new integration test pack (that performs a different sort of testing; execution rather than comparison). I merged pull requests, I closed issues. I introduced a regression (whoops!), a community member helped me fix it (thanks [Mike Mazmanyan](https://github.com/dopare)!). In the last month ts-loader has shipped 6 times.\\n\\nThe thing that matters most in the last paragraph are the phrases \\"I merged pull requests\\" and \\"a community member helped me fix it\\". I\'m wary of one man bands; you should be to. I want projects to be a thing communally built and maintained. If I go under a bus I want someone else to be able to carry on without me. So be part of this; I want you to help!\\n\\nI\'ve got plans to do a lot more. I\'m in the process of [refactoring ts-loader to make it more modular and hence easier for others to contribute](https://github.com/TypeStrong/ts-loader/pull/343). (Also it must be said, refactoring something is an excellent way to try and learn a codebase.) Version 1.0 of ts-loader should ship this week.\\n\\nI\'m working with [Herrington Darkholme](https://github.com/HerringtonDarkholme) (awesome name BTW!) to [add a hook-in point](https://github.com/TypeStrong/ts-loader/issues/270) that will allow ts-loader to support [vuejs](http://vuejs.org/). Stuff is happening and will continue to. But don\'t be shy; be part of this! ts-loader awaits your PRs and is happy to have as many caretakers as possible!\\n\\n![a poster that reads: \\"keep calm, I\'m a caretaker\\"](caretaker.webp)"},{"id":"react-component-curry","metadata":{"permalink":"/react-component-curry","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2016-10-05-react-component-curry/index.md","source":"@site/blog/2016-10-05-react-component-curry/index.md","title":"React Component Curry","description":"React 0.14 introduces stateless functional components to reduce code for components where state isnt required, while also allowing for currying.","date":"2016-10-05T00:00:00.000Z","tags":[{"inline":false,"label":"React","permalink":"/tags/react","description":"The React library."}],"readingTime":1.395,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"react-component-curry","title":"React Component Curry","authors":"johnnyreilly","tags":["react"],"hide_table_of_contents":false,"description":"React 0.14 introduces stateless functional components to reduce code for components where state isnt required, while also allowing for currying."},"unlisted":false,"prevItem":{"title":"But you can\'t die... I love you!","permalink":"/but-you-cant-die-i-love-you-ts-loader"},"nextItem":{"title":"TypeScript 2.0, ES2016 and Babel","permalink":"/typescript-20-es2016-and-babel"}},"content":"Everyone loves curry don\'t they? I don\'t know about you but I\'m going for one on Friday.\\n\\n\x3c!--truncate--\x3e\\n\\nWhen React 0.14 shipped, it came with a new way to write React components. Rather than as an ES2015 class or using `React.createClass` there was now another way: stateless functional components.\\n\\nThese are components which have no state (the name gives it away) and a simple syntax; they are a function which takes your component props as a single parameter and they return JSX. Think of them as the render method of a standard component just with props as a parameter.\\n\\nThe advantage of these components is that they can reduce the amount of code you have to write for a component which requires no state. This is even more true if you\'re using ES2015 syntax as you have arrow functions and destructuring to help.Embrace the terseness!\\n\\n## Mine\'s a Balti\\n\\nThere is another advantage of this syntax. If you have a number of components which share similar implementation you can easily make component factories by currying:\\n\\n```jsx\\nfunction iconMaker(fontAwesomeClassName: string) {\\n  return (props) => <i className={`fa ${fontAwesomeClassName}`} />;\\n}\\n\\nconst ThumbsUpIcon = iconMaker(\'fa-thumbs-up\');\\nconst TrophyIcon = iconMaker(\'fa-trophy\');\\n\\n// Somewhere in else inside a render function:\\n\\n<p>\\n  This is totally <ThumbsUpIcon />\\n  .... You should win a <TrophyIcon />\\n</p>;\\n```\\n\\nSo our `iconMaker` is a function which, when called with a [Font Awesome](http://fontawesome.io/) class name produces a function which, when invoked, will return a the HTML required to render that icon. This is a super simple example, a bhaji if you will, but you can imagine how useful this technique can be when you\'ve more of a banquet in mind."},{"id":"typescript-20-es2016-and-babel","metadata":{"permalink":"/typescript-20-es2016-and-babel","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2016-09-22-typescript-20-es2016-and-babel/index.md","source":"@site/blog/2016-09-22-typescript-20-es2016-and-babel/index.md","title":"TypeScript 2.0, ES2016 and Babel","description":"Upgrading from ES2015 to ES2016 using TypeScript compiler and Babel can be done in a few steps, including a change to tsconfig.json.","date":"2016-09-22T00:00:00.000Z","tags":[{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."},{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."}],"readingTime":2.32,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"typescript-20-es2016-and-babel","title":"TypeScript 2.0, ES2016 and Babel","authors":"johnnyreilly","tags":["typescript","javascript"],"hide_table_of_contents":false,"description":"Upgrading from ES2015 to ES2016 using TypeScript compiler and Babel can be done in a few steps, including a change to tsconfig.json."},"unlisted":false,"prevItem":{"title":"React Component Curry","permalink":"/react-component-curry"},"nextItem":{"title":"Integration Tests with SQL Server Database Snapshots","permalink":"/integration-tests-with-sql-server"}},"content":"[TypeScript 2.0 has shipped!](https://blogs.msdn.microsoft.com/typescript/2016/09/22/announcing-typescript-2-0/) Naturally I\'m excited. For some time I\'ve been using TypeScript to emit ES2015 code which I pass onto Babel to transpile to ES \\"old school\\". You can see how [here](../2015-12-16-es6-typescript-babel-react-flux-karma/index.md).\\n\\n\x3c!--truncate--\x3e\\n\\nMerely upgrading my `package.json` to use `\\"typescript\\": \\"^2.0.3\\"` from `\\"typescript\\": \\"^1.8.10\\"` was painless. TypeScript now supports ES2016 (the previous major release 1.8 supported ES2015). I wanted to move on from writing ES2015 to writing ES2016 using my chosen build process. Fortunately, it\'s supported. Phew. However, due to some advances in ecmascript feature modularisation within the TypeScript compiler the upgrade path is slightly different. I figured that I\'d just be able to update the [`target`](https://www.typescriptlang.org/docs/handbook/compiler-options.html) in my `tsconfig.json` to `\\"es2016\\"` from `\\"es2015\\"`, add in the ES2016 preset for Babel and jobs a good \'un. Not so. There were a few more steps to follow. Here\'s the recipe:\\n\\n## `tsconfig.json` changes\\n\\nWell, there\'s no `\\"es2016\\"` target for TypeScript. You carry on with a target of `\\"es2015\\"`. What I need is a new entry: `\\"lib\\": [\\"dom\\", \\"es2015\\", \\"es2016\\"]`. This tells the compiler that we\'re expecting to be emitting to an environment which supports a browser (`\\"dom\\"`), and both ES2016 and ES2015. Our \\"environment\\" is Babel and it\'s going to pick up the baton from this point. My complete `tsconfig.json` looks like this:\\n\\n```json\\n{\\n  \\"compileOnSave\\": false,\\n  \\"compilerOptions\\": {\\n    \\"allowSyntheticDefaultImports\\": true,\\n    \\"lib\\": [\\"dom\\", \\"es2015\\", \\"es2016\\"],\\n    \\"jsx\\": \\"preserve\\",\\n    \\"module\\": \\"es2015\\",\\n    \\"moduleResolution\\": \\"node\\",\\n    \\"noEmitOnError\\": false,\\n    \\"noImplicitAny\\": true,\\n    \\"preserveConstEnums\\": true,\\n    \\"removeComments\\": false,\\n    \\"suppressImplicitAnyIndexErrors\\": true,\\n    \\"target\\": \\"es2015\\"\\n  }\\n}\\n```\\n\\n## Babel changes\\n\\nI needed the Babel preset for ES2016; with a quick [`npm install --save-dev babel-preset-es2016`](https://www.npmjs.com/package/babel-preset-es2016) that was sorted. Now just to kick webpack into gear...\\n\\n## webpack changes\\n\\nMy webpack config plugs together TypeScript and Babel with the help of [ts-loader](https://www.npmjs.com/package/ts-loader) and [babel-loader](https://www.npmjs.com/package/babel-loader). It allows the transpilation of my (few) JavaScript files so I can write ES2015. However, mainly it allows the transpilation of my (many) TypeScript files so I can write ES2015-flavoured TypeScript. I\'ll now tweak the `loaders` so they cater for ES2016 as well.\\n\\n```js\\nvar webpack = require(\'webpack\');\\n\\nmodule.exports = {\\n  // ....\\n\\n  module: {\\n    loaders: [\\n      {\\n        // Now transpiling ES2016 TS\\n        test: /\\\\.ts(x?)$/,\\n        exclude: /node_modules/,\\n        loader:\\n          \'babel-loader?presets[]=es2016&presets[]=es2015&presets[]=react!ts-loader\',\\n      },\\n      {\\n        // Now transpiling ES2016 JS\\n        test: /\\\\.js$/,\\n        exclude: /node_modules/,\\n        loader: \'babel\',\\n        query: {\\n          presets: [\'es2016\', \'es2015\', \'react\'],\\n        },\\n      },\\n    ],\\n  },\\n\\n  // ....\\n};\\n```\\n\\n## Wake Up and Smell the Jasmine\\n\\nAnd we\'re there; it works. How do I know? Well; here\'s the proof:\\n\\n```ts\\nit(\'Array.prototype.includes works\', () => {\\n  const result = [1, 2, 3].includes(2);\\n  expect(result).toBe(true);\\n});\\n\\nit(\'Exponentiation operator works\', () => {\\n  expect(1 ** 2 === Math.pow(1, 2)).toBe(true);\\n});\\n```\\n\\nMuch love to the TypeScript team for an awesome job; I can\'t wait to get stuck into some of the exciting new features of TypeScript 2.0. `strictNullChecks` FTW!"},{"id":"integration-tests-with-sql-server","metadata":{"permalink":"/integration-tests-with-sql-server","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2016-09-12-integration-tests-with-sql-server/index.md","source":"@site/blog/2016-09-12-integration-tests-with-sql-server/index.md","title":"Integration Tests with SQL Server Database Snapshots","description":"Discover the benefits of using database snapshots for integration tests to reduce complexity & errors in this informative article.","date":"2016-09-12T00:00:00.000Z","tags":[{"inline":false,"label":"SQL Server","permalink":"/tags/sql-server","description":"The SQL Server database."},{"inline":false,"label":"Automated Testing","permalink":"/tags/automated-testing","description":"How to perform the automation of tests."}],"readingTime":5.14,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"integration-tests-with-sql-server","title":"Integration Tests with SQL Server Database Snapshots","authors":"johnnyreilly","tags":["sql server","automated testing"],"hide_table_of_contents":false,"description":"Discover the benefits of using database snapshots for integration tests to reduce complexity & errors in this informative article."},"unlisted":false,"prevItem":{"title":"TypeScript 2.0, ES2016 and Babel","permalink":"/typescript-20-es2016-and-babel"},"nextItem":{"title":"The Ternary Operator <3 Destructuring","permalink":"/the-ternary-operator-meets-destructuring"}},"content":"## Once More With Feeling\\n\\n\x3c!--truncate--\x3e\\n\\nThis is a topic that I have written about [before](../2014-01-24-integration-testing-with-entity/index.md).... But not well. I recently had cause to dust down my notes on how to use snapshotting in your integration tests. To my dismay, referring back to my original blog post was less helpful than I\'d hoped. Now I\'ve cracked the enigma code that my original scribings turned out to be, it\'s time to turn my relearnings back into something genuinely useful.\\n\\n## What\'s the Scenario?\\n\\nYou have a test database. You want to write integration tests. So what\'s the problem? Well, these tests will add records, delete records, update records within the tables of the database. They will mutate the data. And that\'s exactly what they ought to do; they\'re testing that our code uses the database in the way we would hope and expect.\\n\\nSo how do we handle this? Well, we could handle this by writing code at the end of each test that is responsible for reverting the database back to the state that it was in at the start of the test. So if we had a test that added a record and tested it, we\'d need the test to be responsible for removing that record before any subsequent tests run. Now that\'s a totally legitimate approach but it adds tax. Each test becomes more complicated and requires more code.\\n\\nSo what\'s another approach? Perhaps we could take a backup of our database before our first test runs. Then, at the end of each test, we could restore our backup to roll the database back to its initial state. Perfect, right? Less code to write, less scope for errors. So what\'s the downside? Backups are slowwwww. Restores likewise. We could be waiting minutes between each test that runs. That\'s not acceptable.\\n\\nThere is another way though: [database snapshots](https://msdn.microsoft.com/en-us/library/ms175158.aspx) \\\\- a feature that\'s been nestling inside SQL Server for a goodly number of years. For our use case, to all intents and purposes, database snapshots offers the same functionality as backups and restores. You can backup a database (take a snapshot of a database at a point in time), you can restore a database (roll back the database to the point of the snapshot). More importantly, you can do either operation in \\\\*_under a second_\\\\*. As it happens, Microsoft advocate using this approach themselves:\\n\\n> In a testing environment, it can be useful when repeatedly running a test protocol for the database to contain identical data at the start of each round of testing. Before running the first round, an application developer or tester can create a database snapshot on the test database. After each test run, the database can be quickly returned to its prior state by reverting the database snapshot.\\n\\nSold!\\n\\n## Talk is cheap, show me the code\\n\\nIn the end it comes down to 3 classes; `DatabaseSnapshot.cs` which does the actual snapshotting work and 2 classes that make use of it.\\n\\n### DatabaseSnapshot.cs\\n\\nThis is our `DatabaseSnapshot` class. Isn\'t it pretty?\\n\\n```cs\\nusing System.Data;\\nusing System.Data.SqlClient;\\n\\nnamespace Testing.Shared\\n{\\n    public class DatabaseSnapshot\\n    {\\n        private readonly string _dbName;\\n        private readonly string _dbSnapShotPath;\\n        private readonly string _dbSnapShotName;\\n        private readonly string _dbConnectionString;\\n\\n        public DatabaseSnapshot(string dbName, string dbSnapshotPath, string dbSnapshotName, string dbConnectionString)\\n        {\\n            _dbName = dbName;\\n            _dbSnapshotPath = dbSnapshotPath;\\n            _dbSnapshotName = dbSnapshotName;\\n            _dbConnectionString = dbConnectionString;\\n        }\\n\\n        public void CreateSnapshot()\\n        {\\n            if (!System.IO.Directory.Exists(_dbSnapshotPath))\\n                System.IO.Directory.CreateDirectory(_dbSnapshotPath);\\n\\n            var sql = $\\"CREATE DATABASE { _dbSnapshotName } ON (NAME=[{ _dbName }], FILENAME=\'{ _dbSnapshotPath }{ _dbSnapshotName }\') AS SNAPSHOT OF [{_dbName }]\\";\\n\\n            ExecuteSqlAgainstMaster(sql);\\n        }\\n\\n        public void DeleteSnapshot()\\n        {\\n            var sql = $\\"DROP DATABASE { _dbSnapshotName }\\";\\n\\n            ExecuteSqlAgainstMaster(sql);\\n        }\\n\\n        public void RestoreSnapshot()\\n        {\\n            var sql = \\"USE master;\\\\r\\\\n\\" +\\n\\n                $\\"ALTER DATABASE {_dbName} SET SINGLE_USER WITH ROLLBACK IMMEDIATE;\\\\r\\\\n\\" +\\n\\n                $\\"RESTORE DATABASE {_dbName}\\\\r\\\\n\\" +\\n                $\\"FROM DATABASE_SNAPSHOT = \'{ _dbSnapshotName }\';\\\\r\\\\n\\" +\\n\\n                $\\"ALTER DATABASE {_dbName} SET MULTI_USER;\\\\r\\\\n\\";\\n\\n            ExecuteSqlAgainstMaster(sql);\\n        }\\n\\n        private void ExecuteSqlAgainstMaster(string sql, params SqlParameter[] parameters)\\n        {\\n            using (var conn = new SqlConnection(_dbConnectionString))\\n            {\\n                conn.Open();\\n                var cmd = new SqlCommand(sql, conn) { CommandType = CommandType.Text };\\n                cmd.Parameters.AddRange(parameters);\\n                cmd.ExecuteNonQuery();\\n                conn.Close();\\n            }\\n        }\\n    }\\n}\\n```\\n\\nIt exposes 3 methods:\\n\\n<dl><dt>CreateSnapshot</dt><dd>This method creates the snapshot of the database. We will run this right at the start, before any of our tests run.</dd><dt>DeleteSnapshot</dt><dd>Deletes the snapshot we created. We will run this at the end, after all our tests have finished running.</dd><dt>RestoreSnapshot</dt><dd>Restores the database back to the snapshot we took earlier. We run this after each test has completed. This method relies on a connection to the database (perhaps unsurprisingly). It switches the database in use away from the database that is being restored prior to actually running the restore. It happens to shift to the master database (I believe that\'s entirely incidental; although I haven\'t tested).</dd></dl>\\n\\n### SetupAndTeardown.cs\\n\\nThis class is responsible for setting up the snapshot we\'re going to use in our tests right before any of the tests have run (in the `FixtureSetup` method). It\'s also responsible for deleting the snapshot once all the tests have finished running (in the `FixtureTearDown` method). It should be noted that in this example I\'m using NUnit and this class is written to depend on the hooks NUnit exposes for running code at the very beginning and end of the test cycle. All test frameworks have these hooks; if you\'re using something other than NUnit then it\'s just a case of swapping in the relevant attribute (everything tends to attribute driven in the test framework world).\\n\\n```cs\\nusing NUnit.Framework;\\n\\nnamespace Testing.Shared\\n{\\n   [SetUpFixture]\\n   public class SetupAndTeardown\\n   {\\n      public static DatabaseSnapshot DatabaseSnapshot;\\n\\n      [SetUp]\\n      public void FixtureSetup()\\n      {\\n         DatabaseSnapshot = new DatabaseSnapshot(\\"MyDbName\\", \\"C:\\\\\\\\\\", \\"MySnapshot\\", \\"Data Source=.;initial catalog=MyDbName;integrated security=True;\\");\\n\\n         try\\n         {\\n            // Try to delete the snapshot in case it was left over from aborted test runs\\n            DatabaseSnapshot.DeleteSnapShot();\\n         }\\n         catch { /* this should fail with snapshot does not exist */ }\\n\\n         DatabaseSnapshot.CreateSnapShot();\\n      }\\n\\n      [TearDown]\\n      public void FixtureTearDown()\\n      {\\n         DatabaseSnapshot.DeleteSnapShot();\\n      }\\n   }\\n}\\n```\\n\\n### TestBase.cs\\n\\nAll of our test classes are made to inherit from this class:\\n\\n```cs\\nusing NUnit.Framework;\\n\\nnamespace Testing.Shared\\n{\\n   public class TestBase\\n   {\\n      [TearDown]\\n      public void TearDown()\\n      {\\n         SetupAndTeardown.DatabaseSnapshot.RestoreSnapShot();\\n      }\\n   }\\n}\\n```\\n\\nWhich restores the database back to the snapshot position at the end of each test. And that... Is that!"},{"id":"the-ternary-operator-meets-destructuring","metadata":{"permalink":"/the-ternary-operator-meets-destructuring","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2016-08-19-the-ternary-operator-meets-destructuring/index.md","source":"@site/blog/2016-08-19-the-ternary-operator-meets-destructuring/index.md","title":"The Ternary Operator <3 Destructuring","description":"ES2015 destructuring allows setting multiple variables using the ternary operator. Change the return type of each branch to an object for this to work.","date":"2016-08-19T00:00:00.000Z","tags":[{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."},{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."}],"readingTime":2.21,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"the-ternary-operator-meets-destructuring","title":"The Ternary Operator <3 Destructuring","authors":"johnnyreilly","tags":["typescript","javascript"],"hide_table_of_contents":false,"description":"ES2015 destructuring allows setting multiple variables using the ternary operator. Change the return type of each branch to an object for this to work."},"unlisted":false,"prevItem":{"title":"Integration Tests with SQL Server Database Snapshots","permalink":"/integration-tests-with-sql-server"},"nextItem":{"title":"Understanding webpack\'s DefinePlugin (and using with TypeScript)","permalink":"/using-webpacks-defineplugin-with-typescript"}},"content":"I\'m addicted to the [ternary operator](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Conditional_Operator). For reasons I can\'t explain, I cannot get enough of:\\n\\n\x3c!--truncate--\x3e\\n\\n```js\\nconst thisOrThat = someCondition ? \'this\' : \'or that\';\\n```\\n\\nThe occasion regularly arises where I need to turn my lovely terse code into an if statement in order to set 2 variables instead of 1. I\'ve been heartbroken; I hate doing:\\n\\n```ts\\nlet legWear: string, coat: boolean;\\nif (weather === \'good\') {\\n  legWear = \'shorts\';\\n  coat = false;\\n} else {\\n  legWear = \'jeans\';\\n  coat = true;\\n}\\n```\\n\\nJust going from setting one variable to setting two has been really traumatic:\\n\\n- I\'ve had do stop using `const` and moved to `let`. This has made my code less \\"truthful\\" in the sense that I never intend to reassign these variables again; they are intended to be immutable.\\n- I\'ve gone from 1 line of code to _9 lines of code_. That\'s 9x the code for increasing the number of variables in play by 1. That\'s... heavy.\\n- This third point only applies if you\'re using TypeScript (and I am): I have to specify the types of my variables up front if I want type safety.\\n\\nES2015 gives us another option. We can move back to the ternary operator if we change the return type of each branch to be an object sharing the same signature. Then, using destructuring, we can pull out those object properties into `const`s:\\n\\n```ts\\nconst { legWear, coat } =\\n  weather === \'good\'\\n    ? { legWear: \'shorts\', coat: false }\\n    : { legWear: \'jeans\', coat: true };\\n```\\n\\nWith this approach we\'re keeping usage of `const` instead of `let` and we\'re only marginally increasing the amount of code we\'re writing. If you\'re using TypeScript you\'re back to being able to rely on the compiler correctly inferring your types; you don\'t need to specify. Awesome.\\n\\n## Crowdfund You A Tuple\\n\\nI thought I was done and then I saw this:\\n\\n> [@johnny_reilly](https://twitter.com/johnny_reilly) even neater with tuples: const [str, num] = test ? [\\"yes\\", 100] : [\\"no\\", 50];\\n>\\n> \u2014 Illustrated Pamphlet (@Rickenhacker) [August 20, 2016](https://twitter.com/Rickenhacker/status/766913766323781632)\\n\\n<script async=\\"\\" src=\\"//platform.twitter.com/widgets.js\\" charSet=\\"utf-8\\"><\/script>\\n\\n[Daniel](https://twitter.com/Rickenhacker) helpfully points out that there\'s an even terser syntax available to us:\\n\\n```ts\\nconst [legWear, coat] =\\n  weather === \'good\' ? [\'shorts\', false] : [\'jeans\', true];\\n```\\n\\nThe above is ES2015 array destructuring. We get exactly the same effect but it\'s a little terser as we don\'t have to repeat the prop names as we do when using object destructuring. From a TypeScript perspective the assignment side of the above is a [Tuple](https://github.com/Microsoft/TypeScript/pull/428) which allows our type inference to flow through in the manner we\'d hope.\\n\\nLovely. Thanks!"},{"id":"using-webpacks-defineplugin-with-typescript","metadata":{"permalink":"/using-webpacks-defineplugin-with-typescript","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2016-07-23-using-webpacks-defineplugin-with-typescript/index.md","source":"@site/blog/2016-07-23-using-webpacks-defineplugin-with-typescript/index.md","title":"Understanding webpack\'s DefinePlugin (and using with TypeScript)","description":"webpack\'s DefinePlugin allows you to create global constants which can be configured at compile time; here\'s how to use it with TypeScript","date":"2016-07-23T00:00:00.000Z","tags":[{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":2.47,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"using-webpacks-defineplugin-with-typescript","title":"Understanding webpack\'s DefinePlugin (and using with TypeScript)","description":"webpack\'s DefinePlugin allows you to create global constants which can be configured at compile time; here\'s how to use it with TypeScript","authors":"johnnyreilly","tags":["webpack","typescript"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"The Ternary Operator <3 Destructuring","permalink":"/the-ternary-operator-meets-destructuring"},"nextItem":{"title":"Creating an ES2015 Map from an Array in TypeScript","permalink":"/create-es2015-map-from-array-in-typescript"}},"content":"I\'ve been searching for a way to describe what the DefinePlugin actually does. The [docs](https://github.com/webpack/docs/wiki/list-of-plugins#defineplugin) say:\\n\\n> Define free variables. Useful for having development builds with debug logging or adding global constants.\\n\\n\x3c!--truncate--\x3e\\n\\nI think I would describe it like this: the DefinePlugin allows you to create global constants which can be _configured at compile time_. I find this very useful for allowing different behaviour between development builds and release builds. This post will demonstrate usage of this approach, talk about what\'s actually happening and how to get this working nicely with TypeScript.\\n\\n## What Globals?\\n\\nFor our example we want to define 2 global constants; a string called `__VERSION__` and a boolean called `__IN_DEBUG__`. The names are deliberately wacky to draw attention to the fact that these are not your everyday, common-or-garden variables. Them\'s \\"special\\". These constants will be initialised with different values depending on whether we are in a debug build or a production build. Usage of these constants in our code might look like this:\\n\\n```ts\\nif (__IN_DEBUG__) {\\n  console.log(`This app is version ${__VERSION__}`);\\n}\\n```\\n\\nSo, if `__IN_DEBUG__` is set to `true` this code would log out to the console the version of the app.\\n\\n## Configuring our Globals\\n\\nTo introduce these constants to webpack we\'re going to add this to our webpack configuration:\\n\\n```ts\\nvar webpack = require(\'webpack\');\\n\\n// ...\\n\\nplugins: [\\n  new webpack.DefinePlugin({\\n    __IN_DEBUG__: JSON.stringify(false),\\n    __VERSION__: JSON.stringify(\'1.0.0.\' + Date.now()),\\n  }),\\n  // ...\\n];\\n// ...\\n```\\n\\nWhat\'s going on here? Well, each key of the object literal above represents one of our global constants. When you look at the value, just imagine each outer `JSON.stringify( ... )` is not there. It\'s just noise. Imagine instead that you\'re seeing this:\\n\\n```ts\\n__IN_DEBUG__: false,\\n__VERSION__: \'1.0.0.\' + Date.now()\\n```\\n\\nA little clearer, right? `__IN_DEBUG__` is given the boolean value `false` and `__VERSION__` is given the string value of `1.0.0.` plus the ticks off of `Date.now()`. What\'s happening here is well explained in Pete Hunt\'s excellent [webpack howto](https://github.com/petehunt/webpack-howto#6-feature-flags): \\"definePlugin takes raw strings and inserts them\\". `JSON.stringify` facilitates this; it produces a string representation of a value that can be inlined into code. When the inlining takes place the actual output would be something like this:\\n\\n```ts\\nif (false) {\\n  // Because at compile time, __IN_DEBUG__ === false\\n  console.log(`This app is version ${\'1.0.0.1469268116580\'}`); // And __VERSION__ === \\"1.0.0.1469268116580\\"\\n}\\n```\\n\\nAnd if you\'ve got some [UglifyJS](https://github.com/mishoo/UglifyJS) or similar in the mix then, in the example above, this would actually strip out the statement above entirely since it\'s clearly a [NOOP](https://en.wikipedia.org/wiki/NOP). Yay the dead code removal! If `__IN_DEBUG__` was `false` then (perhaps obviously) this statement would be left in place as it wouldn\'t be dead code.\\n\\n## TypeScript and Define\\n\\nThe final piece of the puzzle is making TypeScript happy. It doesn\'t know anything about our global constants. So we need to tell it:\\n\\n```ts\\ndeclare var __IN_DEBUG__: boolean;\\ndeclare var __VERSION__: string;\\n```\\n\\nAnd that\'s it. Compile time constants are a go!"},{"id":"create-es2015-map-from-array-in-typescript","metadata":{"permalink":"/create-es2015-map-from-array-in-typescript","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2016-06-02-create-es2015-map-from-array-in-typescript/index.md","source":"@site/blog/2016-06-02-create-es2015-map-from-array-in-typescript/index.md","title":"Creating an ES2015 Map from an Array in TypeScript","description":"TypeScript `Map` initialization from an `Array` is discussed with a workaround using a type assertion of ` as [string, string]`.","date":"2016-06-02T00:00:00.000Z","tags":[{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."},{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."}],"readingTime":2.105,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"create-es2015-map-from-array-in-typescript","title":"Creating an ES2015 Map from an Array in TypeScript","authors":"johnnyreilly","tags":["typescript","javascript"],"hide_table_of_contents":false,"description":"TypeScript `Map` initialization from an `Array` is discussed with a workaround using a type assertion of ` as [string, string]`."},"unlisted":false,"prevItem":{"title":"Understanding webpack\'s DefinePlugin (and using with TypeScript)","permalink":"/using-webpacks-defineplugin-with-typescript"},"nextItem":{"title":"The Mysterious Case of webpack, AngularJS and jQuery","permalink":"/the-mysterious-case-of-webpack-angular-and-jquery"}},"content":"I\'m a great lover of ES2015\'s [`Map`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Map). However, just recently I tumbled over something I find a touch inconvenient about how you initialise a new `Map` from the contents of an `Array` in TypeScript.\\n\\n\x3c!--truncate--\x3e\\n\\n## This Doesn\'t Work\\n\\nWe\'re going try to something like this: (pilfered from the MDN docs)\\n\\n```ts\\nvar kvArray = [\\n  [\'key1\', \'value1\'],\\n  [\'key2\', \'value2\'],\\n];\\n\\n// Use the regular Map constructor to transform a 2D key-value Array into a map\\nvar myMap = new Map(kvArray);\\n```\\n\\nSimple enough right? Well I\'d rather assumed that I should be able to do something like this in TypeScript:\\n\\n```ts\\nconst iAmAnArray [\\n  { value: \\"value1\\", text: \\"hello\\" }\\n  { value: \\"value2\\", text: \\"map\\" }\\n];\\n\\nconst iAmAMap = new Map<string, string>(\\n  iAmAnArray.map(x => [x.value, x.text])\\n);\\n```\\n\\nHowever, to my surprise this errored out with:\\n\\n```\\n[ts] Argument of type \'string[][]\' is not assignable to parameter of type \'Iterable<[string, string]>\'.\\n  Types of property \'[Symbol.iterator]\' are incompatible.\\n    Type \'() => IterableIterator<string[]>\' is not assignable to type \'() => Iterator<[string, string]>\'.\\n      Type \'IterableIterator<string[]>\' is not assignable to type \'Iterator<[string, string]>\'.\\n        Types of property \'next\' are incompatible.\\n          Type \'(value?: any) => IteratorResult<string[]>\' is not assignable to type \'(value?: any) => IteratorResult<[string, string]>\'.\\n            Type \'IteratorResult<string[]>\' is not assignable to type \'IteratorResult<[string, string]>\'.\\n              Type \'string[]\' is not assignable to type \'[string, string]\'.\\n                Property \'0\' is missing in type \'string[]\'.\\n```\\n\\nDisappointing right? It\'s expecting `Iterable&lt;[string, string]&gt;` and an `Array` with 2 elements that are strings is _not_ inferred to be that.\\n\\n## This Does\\n\\nIt emerges that there is a way to do this though; you just need to give the compiler a clue. You need to include a type assertion of ` as [string, string]` which tells the compiler that what you\'ve just declared is a `Tuple` of `string` and `string`. (Please note that `[string, string]` corresponds to the types of the `Key` and `Value` of your `Map` and should be set accordingly.)\\n\\nSo a working version of the code looks like this:\\n\\n```ts\\nconst iAmAnArray [\\n  { value: \\"value1\\", text: \\"hello\\" }\\n  { value: \\"value2\\", text: \\"map\\" }\\n];\\n\\nconst iAmAMap = new Map<string, string>(\\n  iAmAnArray.map(x => [x.value, x.text] as [string, string])\\n);\\n```\\n\\nOr, to be terser, this:\\n\\n```ts\\nconst iAmAnArray [\\n  { value: \\"value1\\", text: \\"hello\\" }\\n  { value: \\"value2\\", text: \\"map\\" }\\n];\\n\\nconst iAmAMap = new Map( // Look Ma!  No type annotations\\n  iAmAnArray.map(x => [x.value, x.text] as [string, string])\\n);\\n```\\n\\nI\'ve raised this as an issue with the TypeScript team; you can find details [here](https://github.com/Microsoft/TypeScript/issues/8936)."},{"id":"the-mysterious-case-of-webpack-angular-and-jquery","metadata":{"permalink":"/the-mysterious-case-of-webpack-angular-and-jquery","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2016-05-24-the-mysterious-case-of-webpack-angular-and-jquery/index.md","source":"@site/blog/2016-05-24-the-mysterious-case-of-webpack-angular-and-jquery/index.md","title":"The Mysterious Case of webpack, AngularJS and jQuery","description":"Angular can use jQuery instead of jQLite, but this becomes complicated when using webpack. We need to use the ProvidePlugin function in webpack.config.js.","date":"2016-05-24T00:00:00.000Z","tags":[{"inline":false,"label":"Angular JS","permalink":"/tags/angularjs","description":"The original Angular JS framework."},{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."},{"inline":false,"label":"jQuery","permalink":"/tags/jquery","description":"The jQuery library."}],"readingTime":1.895,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"the-mysterious-case-of-webpack-angular-and-jquery","title":"The Mysterious Case of webpack, AngularJS and jQuery","authors":"johnnyreilly","tags":["angularjs","webpack","jquery"],"hide_table_of_contents":false,"description":"Angular can use jQuery instead of jQLite, but this becomes complicated when using webpack. We need to use the ProvidePlugin function in webpack.config.js."},"unlisted":false,"prevItem":{"title":"Creating an ES2015 Map from an Array in TypeScript","permalink":"/create-es2015-map-from-array-in-typescript"},"nextItem":{"title":"Inlining Angular Templates with WebPack and TypeScript","permalink":"/inlining-angular-templates-with-webpack"}},"content":"You may know that [Angular ships with a cutdown version of jQuery called jQLite](https://docs.angularjs.org/api/ng/function/angular.element). It\'s still possible to use the full-fat jQuery; to quote the docs:\\n\\n\x3c!--truncate--\x3e\\n\\n> To use `jQuery`, simply ensure it is loaded before the `angular.js` file.\\n\\nNow the wording rather implies that you\'re not using any module loader / bundler. Rather that all files are being loaded via `script` tags and relies on the global variables that result from that. True enough, if you take a look at the [Angular source](https://github.com/angular/angular.js/blob/eaa1119d4252bed08dfa42f984ef9502d0f02775/src/Angular.js#L1791) you can see how this works:\\n\\n```ts\\n// bind to jQuery if present;\\nvar jqName = jq();\\njQuery = isUndefined(jqName)\\n  ? window.jQuery // use jQuery (if present)\\n  : !jqName\\n  ? undefined // use jqLite\\n  : window[jqName]; // use jQuery specified by `ngJq`\\n```\\n\\nAmongst other things it looks for a `jQuery` variable which has been placed onto the `window` object. If it is found then jQuery is used; if it is not then it\'s `jqLite` all the way.\\n\\n## But wait! I\'m using webpack\\n\\nMe too! And one of the reasons is that we get to move away from reliance upon the global scope and towards proper modularisation. So how do we get Angular to use jQuery given the code we\'ve seen above? Well, your first thought might be to `npm install` yourself some `jQuery` and then make sure you\'ve got something like this in your entry file:\\n\\n```ts\\nimport \'jquery\'; // This\'ll fix it... Right?\\nimport * as angular from \'angular\';\\n```\\n\\nWrong.\\n\\n## You need the `ProvidePlugin`\\n\\nIn your `webpack.config.js` you need to add the following entry to your plugins:\\n\\n```ts\\nnew webpack.ProvidePlugin({\\n          \\"window.jQuery\\": \\"jquery\\"\\n      }),\\n```\\n\\nThis uses the webpack [`ProvidePlugin`](https://github.com/webpack/docs/wiki/list-of-plugins#provideplugin) and, at the point of webpackification (\xa9 2016 John Reilly) all references in the code to `window.jQuery` will be replaced with a reference to the webpack module that contains jQuery. So when you look at the bundled file you\'ll see that the code that checks the `window` object for `jQuery` has become this:\\n\\n```ts\\njQuery = isUndefined(jqName)\\n  ? __webpack_provided_window_dot_jQuery // use jQuery (if present)\\n  : !jqName\\n  ? undefined // use jqLite\\n  : window[jqName]; // use jQuery specified by `ngJq`\\n```\\n\\nThat\'s right; webpack is providing Angular with jQuery whilst still _not_ placing a `jQuery` variable onto the `window`. Neat huh?"},{"id":"inlining-angular-templates-with-webpack","metadata":{"permalink":"/inlining-angular-templates-with-webpack","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2016-05-13-inlining-angular-templates-with-webpack/index.md","source":"@site/blog/2016-05-13-inlining-angular-templates-with-webpack/index.md","title":"Inlining Angular Templates with WebPack and TypeScript","description":"`raw-loader` package in webpack configuration for Angular 1.x projects preloads templates and enables compile-time error checking.","date":"2016-05-13T00:00:00.000Z","tags":[{"inline":false,"label":"Angular JS","permalink":"/tags/angularjs","description":"The original Angular JS framework."},{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."}],"readingTime":2.9,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"inlining-angular-templates-with-webpack","title":"Inlining Angular Templates with WebPack and TypeScript","authors":"johnnyreilly","tags":["angularjs","webpack"],"hide_table_of_contents":false,"description":"`raw-loader` package in webpack configuration for Angular 1.x projects preloads templates and enables compile-time error checking."},"unlisted":false,"prevItem":{"title":"The Mysterious Case of webpack, AngularJS and jQuery","permalink":"/the-mysterious-case-of-webpack-angular-and-jquery"},"nextItem":{"title":"Instant Stubs with JSON.Net (just add hot water)","permalink":"/instant-stubs-with-jsonnet"}},"content":"This technique actually applies to pretty much any web stack where you have to supply templates; it just so happens that I\'m using Angular 1.x in this case. Also I have an extra technique which is useful to handle the [ng-include](https://docs.angularjs.org/api/ng/directive/ngInclude) scenario.\\n\\n\x3c!--truncate--\x3e\\n\\n## Preamble\\n\\nFor some time I\'ve been using webpack to bundle my front end. I write ES6 TypeScript; import statements and all. This is all sewn together using the glorious [ts-loader](https://www.npmjs.com/package/ts-loader) to compile and emit ES6 code which is handed off to the wonderful [babel-loader](https://www.npmjs.com/package/babel-loader) which transpiles it to ESold code. All with full source map support. It\'s wonderful.\\n\\nHowever, up until now I\'ve been leaving Angular to perform the relevant http requests at runtime when it needs to pull in templates. That works absolutely fine but my preference is to preload those templates. In fact I\'ve [written before](../2015-02-17-using-gulp-in-asp-net-instead-of-web-optimization/index.md) about using the [gulp angular template cache](https://www.npmjs.com/package/gulp-angular-templatecache) to achieve just that aim.\\n\\nSo I was wondering; in this modular world what would be the equivalent approach? Sure I could still use the gulp angular template cache approach but I would like something a little more deliberate and a little less magic. Also, I\'ve discovered (to my cost) that when using the existing approach, it\'s possible to break the existing implementation without realising it; only finding out there\'s a problem in Production when unexpected http requests start happening. Finding these problems out at compile time rather than runtime is always to be strived for. So how?\\n\\n## [raw-loader](https://www.npmjs.com/package/raw-loader)!\\n\\nraw-loader allows you load file content using `require` statements. This works well with the use case of inlining html. So I drop it into my `webpack.config.js` like so:\\n\\n```js\\nvar path = require(\'path\');\\n\\nmodule.exports = {\\n  cache: true,\\n  entry: {\\n    main: \'./src/main.ts\',\\n\\n    vendor: [\\n      \'babel-polyfill\',\\n      \'angular\',\\n      \'angular-animate\',\\n      \'angular-sanitize\',\\n      \'angular-ui-bootstrap\',\\n      \'angular-ui-router\',\\n    ],\\n  },\\n  output: {\\n    path: path.resolve(__dirname, \'./dist/scripts\'),\\n    filename: \'[name].js\',\\n    chunkFilename: \'[chunkhash].js\',\\n  },\\n  module: {\\n    loaders: [\\n      {\\n        test: /\\\\.ts(x?)$/,\\n        exclude: /node_modules/,\\n        loader: \'babel-loader?presets[]=es2015!ts-loader\',\\n      },\\n      {\\n        test: /\\\\.js$/,\\n        exclude: /node_modules/,\\n        loader: \'babel\',\\n        query: {\\n          presets: [\'es2015\'],\\n        },\\n      },\\n      {\\n        // THIS IS THE MAGIC!\\n        test: /\\\\.html$/,\\n        exclude: /node_modules/,\\n        loader: \'raw\',\\n      },\\n    ], // THAT WAS THE MAGIC!\\n  },\\n  plugins: [\\n    // ....\\n  ],\\n  resolve: {\\n    extensions: [\'\', \'.ts\', \'.tsx\', \'.js\'],\\n  },\\n};\\n```\\n\\nWith this in place, if someone requires a file with the `html` suffix then raw-loader comes in. So now we can swap this:\\n\\n```js\\n$stateProvider.state(\'state1\', {\\n  url: \'/state1\',\\n  templateUrl: \'partials/state1.html\',\\n});\\n```\\n\\nFor this:\\n\\n```js\\n$stateProvider.state(\'state1\', {\\n  url: \'/state1\',\\n  template: require(\'./partials/state1.html\'),\\n});\\n```\\n\\nNow initially TypeScript is going to complain about your `require` statement. That\'s fair; outside of node-land it doesn\'t know what `require` is. No bother, you just need to drop in a one line simple definition file to sort this out; let me present `webpack-require.d.ts`:\\n\\n```ts\\ndeclare var require: (filename: string) => any;\\n```\\n\\nYou\'ve now inlined your template. And for bonus points, if you were to make a mistake in your path then webpack would shout at you at compile time; which is a _good, good_ thing.\\n\\n## ng-include\\n\\nThe one use case that this doesn\'t cover is where your templates import other templates through use of the [ng-include](https://docs.angularjs.org/api/ng/directive/ngInclude) directive. They will still trigger http requests as the templates are served. The simple way to prevent that is by priming the angular `<a href=\\"https://docs.angularjs.org/api/ng/service/$templateCache\\">$templateCache</a>` like so:\\n\\n```js\\napp.run([\\n  \'$templateCache\',\\n  ($templateCache: ng.ITemplateCacheService) => {\\n    $templateCache.put(\'justSome.html\', require(\'./justSome.html\'));\\n    // Other templates go here...\\n  },\\n]);\\n```\\n\\nNow when the app spins up it already has everything it needs pre-cached."},{"id":"instant-stubs-with-jsonnet","metadata":{"permalink":"/instant-stubs-with-jsonnet","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2016-04-25-instant-stubs-with-jsonnet/index.md","source":"@site/blog/2016-04-25-instant-stubs-with-jsonnet/index.md","title":"Instant Stubs with JSON.Net (just add hot water)","description":"A utility class can create stubs to test an untested system with complex I/O. Serializing complex data to JSON files eases the process.","date":"2016-04-25T00:00:00.000Z","tags":[{"inline":false,"label":"Automated Testing","permalink":"/tags/automated-testing","description":"How to perform the automation of tests."},{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":4.05,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"instant-stubs-with-jsonnet","title":"Instant Stubs with JSON.Net (just add hot water)","authors":"johnnyreilly","tags":["automated testing","asp.net"],"hide_table_of_contents":false,"description":"A utility class can create stubs to test an untested system with complex I/O. Serializing complex data to JSON files eases the process."},"unlisted":false,"prevItem":{"title":"Inlining Angular Templates with WebPack and TypeScript","permalink":"/inlining-angular-templates-with-webpack"},"nextItem":{"title":"Concatting IEnumerables in C#","permalink":"/concatting-ienumerables-in-csharp"}},"content":"I\'d like you to close your eyes and imagine a scenario. You\'re handed a prototype system. You\'re told it works. It has no documentation. It has 0 unit tests. The hope is that you can take it on, refactor it, make it better and (crucially) not break it. Oh, and you don\'t really understand what the code does or why it does it either; information on that front is, alas, sorely lacking.\\n\\n\x3c!--truncate--\x3e\\n\\nThis has happened to me; it\'s alas not that unusual. The common advice handed out in this situation is: \\"add unit tests before you change it\\". That\'s good advice. We need to take the implementation that embodies the correctness of the system and create unit tests that set that implementation in stone. However, what say the system that you\'re hoping to add tests to takes a number of large and complex inputs from some external source and produces a similarly large and complex output?\\n\\nYou could start with integration tests. They\'re good but slow and crucially they depend upon the external inputs being available and unchanged (which is perhaps unlikely). What you could do (what I have done) is debug a working working system. At each point that an input is obtained I have painstakingly transcribed the data which allows me to subsequently hand code stub data. There comes a point when this is plainly untenable; it\'s just too much data to transcribe. At this point the temptation is to think \\"it\'s okay; I can live without the tests. I\'ll just be super careful with my refactoring... It\'ll be fine It\'ll be fine It\'ll be fine It\'ll be fine\\".\\n\\nActually, it probably won\'t be fine. And even if it is (miracles do happen) you\'re going to be fairly stressed as you wonder if you\'ve been careful enough. What if there was another way? A way that wasn\'t quite so hard but that allowed you to add tests without requiring 3 months hand coding....\\n\\n## Instant Stubs\\n\\nWhat I\'ve come up with is a super simple utility class for creating stubs / fakes. (I\'m aware the naming of such things [can be a little contentious](http://martinfowler.com/articles/mocksArentStubs.html).)\\n\\n```cs\\nusing Newtonsoft.Json;\\nusing System;\\nusing System.IO;\\n\\nnamespace MakeFakeData.UnitTests\\n{\\n  public static class Stubs\\n  {\\n    private static JsonSerializer _serializer = new JsonSerializer { NullValueHandling = NullValueHandling.Ignore };\\n\\n    public static void Make<T>(string stubPath, T data)\\n    {\\n      try\\n      {\\n        if (string.IsNullOrEmpty(stubPath))\\n          throw new ArgumentNullException(nameof(stubPath));\\n        if (data == null)\\n          throw new ArgumentNullException(nameof(data));\\n\\n        using (var sw = new StreamWriter(stubPath))\\n        using (var writer = new JsonTextWriter(sw) {\\n            Formatting = Formatting.Indented,\\n            IndentChar = \' \',\\n            Indentation = 2})\\n        {\\n          _serializer.Serialize(writer, data);\\n        }\\n      }\\n      catch (Exception exc)\\n      {\\n        throw new Exception($\\"Failed to make {stubPath}\\", exc);\\n      }\\n    }\\n\\n    public static T Load<T>(string stubPath)\\n    {\\n      try\\n      {\\n        if (string.IsNullOrEmpty(stubPath))\\n          throw new ArgumentNullException(nameof(stubPath));\\n\\n        using (var file = File.OpenText(stubPath))\\n        using (var reader = new JsonTextReader(file))\\n        {\\n          return _serializer.Deserialize<T>(reader);\\n        }\\n      }\\n      catch (Exception exc)\\n      {\\n        throw new Exception($\\"Failed to load {stubPath}\\", exc);\\n      }\\n    }\\n  }\\n}\\n```\\n\\nAs you can see this class uses [JSON.Net](http://www.newtonsoft.com/json) and exposes 2 methods:\\n\\n<dl><dt>Make</dt><dd>Takes a given piece of data and uses JSON.Net to serialise it as JSON to a file. (nb I choose to format the JSON for readability and exclude null values; both totally optional)</dd><dt>Load</dt><dd>Takes the given path and loads the associated JSON file and deserialises it back into an object.</dd></dl>\\n\\nThe idea is this: we take our working implementation and, wherever it extracts data from an external source, we insert a temporary statement like this:\\n\\n```cs\\nvar data = _dataService.GetComplexData();\\n\\n    // Just inserted so we can generate the stub data...\\n    Stubs.Make($\\"{System.AppDomain.CurrentDomain.BaseDirectory}\\\\\\\\data.json\\", data);\\n```\\n\\nThe next time you run the implementation you\'ll find the app generates a `data.json` file containing the complex data serialized to JSON. Strip out your `Stubs.Make` statements from the implementation and we\'re ready for the next stage.\\n\\n## Using your JSON\\n\\nWhat you need to do now is to take the new and shiny `data.json` file and move it to your unit test project. It needs to be included within the unit test project. Also, for each JSON file you have, the `Build Action` in VS needs to be set to `Content` and the `Copy to Output Directory` to `Copy if newer`.\\n\\nThen within your unit tests you can write code like this:\\n\\n```ts\\nvar dummyData = Stubs.Load<ComplexDataType>(\'Stubs/data.json\');\\n```\\n\\nWhich pulls in your data from the JSON file and deserialises it into the original types. With this in hand you can plug together a unit test based on an existing implementation which depends on external data much faster than the hand-cranked method of old.\\n\\nFinally, before the wildebeest of TDD descend upon me howling and wailing, let me say again; I anticipate this being useful when you\'re trying to add tests to something that already exists but is untested. Clearly it would be better not to be in this situaion in the first place."},{"id":"concatting-ienumerables-in-csharp","metadata":{"permalink":"/concatting-ienumerables-in-csharp","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2016-03-22-concatting-ienumerables-in-csharp/index.md","source":"@site/blog/2016-03-22-concatting-ienumerables-in-csharp/index.md","title":"Concatting IEnumerables in C#","description":"Author proposes clean alternatives to `IEnumerable`s concatenation which entail creating custom extensions & using nulls for null-conditional operator.","date":"2016-03-22T00:00:00.000Z","tags":[{"inline":false,"label":"C#","permalink":"/tags/csharp","description":"The C# programming language."}],"readingTime":2.525,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"concatting-ienumerables-in-csharp","title":"Concatting IEnumerables in C#","authors":"johnnyreilly","tags":["c#"],"hide_table_of_contents":false,"description":"Author proposes clean alternatives to `IEnumerable`s concatenation which entail creating custom extensions & using nulls for null-conditional operator."},"unlisted":false,"prevItem":{"title":"Instant Stubs with JSON.Net (just add hot water)","permalink":"/instant-stubs-with-jsonnet"},"nextItem":{"title":"Atom - Recovering from Corrupted Packages","permalink":"/atom-recovering-from-corrupted-packages"}},"content":"I hate LINQ\'s [`Enumerable.Concat`](https://msdn.microsoft.com/en-us/library/bb302894%28v=vs.110%29.aspx?f=255&MSPPError=-2147217396) when bringing together `IEnumerable`s. Not the behaviour (I love that!) but rather how code ends up looking when you use it. Consider this:\\n\\n\x3c!--truncate--\x3e\\n\\n```cs\\nvar concatenated = myCollection?.Select(x => new ConcatObj(x)) ?? new ConcatObj[0].Concat(\\n   myOtherCollection?.Select(x => new ConcatObj(x)) ?? new ConcatObj[0]\\n);\\n```\\n\\nIn this example I\'m bringing together 2 collections, either of which may be null (more on that later). I think we can all agree this doesn\'t represent a world of readability. I\'ve also had to create a custom class `ConcatObj` because you can\'t create an empty array for an anonymous type in C#.\\n\\n## Attempt #1: `ConcatMany`\\n\\nAfter toying around with a bunch of different ideas I created this extension method:\\n\\n```cs\\npublic static class FunctionalExtensions\\n{\\n    public static IEnumerable<T> ConcatMany<T>(\\n        this IEnumerable<T> original,\\n        params IEnumerable<T>[] enumerablesToConcat) => original.Concat(\\n            enumerablesToConcat.Where(e => e != null).SelectMany(c => c)\\n        );\\n}\\n```\\n\\nThanks to the joy of `params` this extension allows me to bring together multiple IEnumerables into a single one but has the advantage of considerably cleaner calling code:\\n\\n```cs\\nvar concatenated = Enumerable.Empty<ConcatObj>().ConcatMany(\\n    myCollection?.Select(x => new ConcatObj(x)),\\n    myOtherCollection?.Select(x => new ConcatObj(x))\\n    );\\n```\\n\\nFor my money this is more readable and intent is clearer. Particularly as the number of contributing IEnumerables goes up. The downside is that I can\u2019t use anonymous objects because you need to tell the compiler what the type is when using `<a href=\\"https://msdn.microsoft.com/en-us/library/bb341042%28v=vs.110%29.aspx?f=255&amp;MSPPError=-2147217396\\">Enumerable.Empty</a>`.\\n\\nWouldn\'t it be nice to have both:\\n\\n1. Readable code and\\n2. Anonymous objects?\\n\\n## Attempt #2: `EnumerableExtensions.Create`\\n\\nAfter batting round a few ideas (thanks Matt) I settled on this implementation:\\n\\n```cs\\npublic static class EnumerableExtensions\\n{\\n    public static IEnumerable<TSource> Create<TSource>(params IEnumerable<TSource>[] enumerables)\\n    {\\n        return Concat(enumerables.Where(e => e != null));\\n    }\\n\\n    private static IEnumerable<TSource> Concat<TSource>(IEnumerable<IEnumerable<TSource>> enumerables)\\n    {\\n        foreach (var enumerable in enumerables)\\n        {\\n            foreach (var item in enumerable)\\n            {\\n                yield return item;\\n            }\\n        }\\n    }\\n}\\n```\\n\\nWhich allows for calling code like this:\\n\\n```cs\\nvar concatenated = EnumerableExtensions.Create(\\n    myCollection?.Select(x => new { Anonymous = x.Types }),\\n    myOtherCollection?.Select(x => new { Anonymous = x.Types })\\n);\\n```\\n\\nThat\'s right; anonymous types are back! Strictly speaking the `Concat` method above could be converted into a single `SelectMany` (and boy does ReSharper like telling me) but I\'m quite happy with it as is. And to be honest, I so rarely get to use `yield` in my own code; I thought it might be nice to give it a whirl \uD83D\uDE0A\\n\\n## What Gives Elvis?\\n\\nIf you look closely at the implementation you\'ll notice that I purge all `null`s when I\'m bringing together the `Enumerable`s. For why? Some may legitimately argue this is a bad idea. However, there is method in my \\"bad practice\\".\\n\\nI\'ve chosen to treat `null` as \\"not important\\" for this use case. I\'m doing this because it emerges that ASP.NET MVC deserialises empty collections as nulls. (See [here](http://aspnetwebstack.codeplex.com/SourceControl/latest#src/System.Web.Mvc/ValueProviderResult.cs) and play spot the `return null;`) Which is a pain. But thanks to the null purging behaviour of `EnumerableExtensions.Create` I can trust in the [null-conditional (Elvis)](https://csharp.today/c-6-features-null-conditional-and-and-null-coalescing-operators/) operator to not do me wrong."},{"id":"atom-recovering-from-corrupted-packages","metadata":{"permalink":"/atom-recovering-from-corrupted-packages","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2016-03-17-atom-recovering-from-corrupted-packages/index.md","source":"@site/blog/2016-03-17-atom-recovering-from-corrupted-packages/index.md","title":"Atom - Recovering from Corrupted Packages","description":"Atom packages corrupt? Locate `.apm` folder at `[Your name]/.atom`, then delete. On reopening Atom, packages will be restored.","date":"2016-03-17T00:00:00.000Z","tags":[],"readingTime":0.71,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"atom-recovering-from-corrupted-packages","title":"Atom - Recovering from Corrupted Packages","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false,"description":"Atom packages corrupt? Locate `.apm` folder at `[Your name]/.atom`, then delete. On reopening Atom, packages will be restored."},"unlisted":false,"prevItem":{"title":"Concatting IEnumerables in C#","permalink":"/concatting-ienumerables-in-csharp"},"nextItem":{"title":"TFS 2012 meet PowerShell, Karma and BuildNumber","permalink":"/tfs-2012-meet-powershell-karma-and-buildnumber"}},"content":"Every now and then when I try and update my packages in [Atom](https://atom.io/) I find this glaring back at me:\\n\\n![](Screenshot-2016-03-17-06.17.03.webp)\\n\\n\x3c!--truncate--\x3e\\n\\nUg. The problem is that my atom packages have become corrupt. Quite how I couldn\'t say. But that\'s the problem. Atom, as I know from bitter experience, will not recover from this. It just sits there feeling sorry for itself. However, getting back to where you belong is simpler than you imagine:\\n\\n1. Shutdown Atom\\n2. In the file system go to `[Your name]/.atom` (and bear in mind this is Windows; Macs / Linux may be different)\\n\\n![](Screenshot-2016-03-17-06.17.53.webp)\\n\\n3. You\'ll see an `.apm` folder that contains all your packages. Delete this.\\n\\nWhen you next fire up Atom these packages will automagically come back but this time they shouldn\'t be corrupt. Instead you should see the happiness of normality restored:\\n\\n![](Screenshot-2016-03-17-06.23.18.webp)"},{"id":"tfs-2012-meet-powershell-karma-and-buildnumber","metadata":{"permalink":"/tfs-2012-meet-powershell-karma-and-buildnumber","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2016-03-04-tfs-2012-meet-powershell-karma-and-buildnumber/index.md","source":"@site/blog/2016-03-04-tfs-2012-meet-powershell-karma-and-buildnumber/index.md","title":"TFS 2012 meet PowerShell, Karma and BuildNumber","description":"This guide explains how to run PowerShell scripts for TFS 2012 build processes and how to publish Karma test results in the platform.","date":"2016-03-04T00:00:00.000Z","tags":[{"inline":false,"label":"Azure DevOps","permalink":"/tags/azure-devops","description":"The Azure DevOps suite of tools."}],"readingTime":5.405,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"tfs-2012-meet-powershell-karma-and-buildnumber","title":"TFS 2012 meet PowerShell, Karma and BuildNumber","authors":"johnnyreilly","tags":["azure devops"],"hide_table_of_contents":false,"description":"This guide explains how to run PowerShell scripts for TFS 2012 build processes and how to publish Karma test results in the platform."},"unlisted":false,"prevItem":{"title":"Atom - Recovering from Corrupted Packages","permalink":"/atom-recovering-from-corrupted-packages"},"nextItem":{"title":"Creating Angular UI Routes in the Controller","permalink":"/creating-angular-ui-routes-in-controller"}},"content":"To my lasting regret, TFS 2012 has no direct support for PowerShell. Such a shame as PowerShell scripts can do a lot of heavy lifting in a build process. Well, here we\'re going to brute force TFS 2012 into running PowerShell scripts. And along the way we\'ll also get Karma test results publishing into TFS 2012 as an example usage. Nice huh? Let\'s go!\\n\\n\x3c!--truncate--\x3e\\n\\n## PowerShell via `csproj`\\n\\nIt\'s time to hack the `csproj` (or whatever project file you have) again. We\'re going to add an `AfterBuild` target to the end of the file. This target will be triggered after the build completes (as the name suggests):\\n\\n```xml\\n<Target Name=\\"AfterBuild\\">\\n    <Message Importance=\\"High\\" Text=\\"AfterBuild: PublishUrl = $(PublishUrl), BuildUri = $(BuildUri), Configuration = $(Configuration), ProjectDir = $(ProjectDir), TargetDir = $(TargetDir), TargetFileName = $(TargetFileName), BuildNumber = $(BuildNumber), BuildDefinitionName = $(BuildDefinitionName)\\" />\\n    <Exec Command=\\"powershell.exe -NonInteractive -ExecutionPolicy RemoteSigned \\"& \'$(ProjectDir)AfterBuild.ps1\' \'$(Configuration)\' \'$(ProjectDir)\' \'$(TargetDir)\' \'$(PublishUrl)\' \'$(BuildNumber)\' \'$(BuildDefinitionName)\'\\"\\" />\\n  </Target>\\n```\\n\\nThere\'s 2 things happening in this target:\\n\\n1. A message is printed out during compilation which contains details of the various compile time variables. This is nothing more than a `console.log` statement really; it\'s useful for debugging and so I keep it around. You\'ll notice one of them is called `$(BuildNumber)`; more on that later.\\n2. A command is executed; PowerShell! This invokes PowerShell with the `-NonInteractive` and `-ExecutionPolicy RemoteSigned` flags. It passes a script to be executed called `AfterBuild.ps1` that lives in the root of the project directory. To that script a number of parameters are supplied; compile time variables that we may use in the script.\\n\\n## Where\'s my `BuildNumber` and `BuildDefinitionName`?\\n\\nSo you\'ve checked in your changes and kicked off a build on the server. You\'re picking over the log messages and you\'re thinking: \\"Where\'s my `BuildNumber`?\\". Well, TFS 2012 does not have that set as a variable at compile time by default. This stumped me for a while but thankfully I wasn\'t the only person wondering... As ever, [Stack Overflow had the answer](http://stackoverflow.com/a/7330453/761388). So, deep breath, it\'s time to hack the TFS build template file.\\n\\nCheckout the `DefaultTemplate.11.1.xaml` file from TFS and open it in your text editor of choice. It\'s _find and replace_ time! (There are probably 2 instances that need replacement.) Perform a _find_ for the below\\n\\n```js\\n[String.Format(&quot;/p:SkipInvalidConfigurations=true {0}&quot;, MSBuildArguments)]\\n```\\n\\nAnd _replace_ it with this:\\n\\n```js\\n[\\n  String.Format(\\n    \'/p:SkipInvalidConfigurations=true /p:BuildNumber={1} /p:BuildDefinitionName={2} {0}\',\\n    MSBuildArguments,\\n    BuildDetail.BuildNumber,\\n    BuildDetail.BuildDefinition.Name,\\n  ),\\n];\\n```\\n\\nPretty long line eh? Let\'s try breaking that up to make it more readable: (but remember in the XAML it needs to be a one liner)\\n\\n```js\\n[String.Format(\\"/p:SkipInvalidConfigurations=true\\n    /p:BuildNumber={1}\\n    /p:BuildDefinitionName={2} {0}\\", MSBuildArguments, BuildDetail.BuildNumber, BuildDetail.BuildDefinition.Name)]\\n```\\n\\nWe\'re just adding 2 extra parameters of `BuildNumber` and `BuildDefinitionName` to the invocation of MSBuild. Once we\'ve checked this back in, `BuildNumber` and `BuildDefinitionName` will be available on future builds. Yay! _Important! You must have a build name that does not feature spaces; probably there\'s a way to pass spaces here but I\'m not sure what it is._\\n\\n## `AfterBuild.ps1`\\n\\nYou can use your `AfterBuild.ps1` script to do any number of things. In my case I\'m going to use MSTest to publish some test results which have been generated by Karma into TFS:\\n\\n```ps\\nparam ([string]$configuration, [string]$projectDir, [string]$targetDir, [string]$publishUrl, [string]$buildNumber, [string] $buildDefinitionName)\\n\\n$ErrorActionPreference = \'Stop\'\\nClear\\n\\nfunction PublishTestResults([string]$resultsFile) {\\n Write-Host \'PublishTests\'\\n $mstest = \'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 14.0\\\\Common7\\\\IDE\\\\MSTest.exe\'\\n\\n Write-Host \\"Using $mstest at $pwd\\"\\n Write-Host \\"Publishing: $resultsFile\\"\\n\\n & $mstest /publishresultsfile:$resultsFile /publish:http://my-tfs-server:8080/tfs /teamproject:MyProject /publishbuild:$buildNumber /platform:\'Any CPU\' /flavor:Release\\n}\\n\\nfunction FailBuildIfThereAreTestFailures([string]$resultsFile) {\\n $results = [xml](GC $resultsFile)\\n $outcome = $results.TestRun.ResultSummary.outcome\\n $fgColor = if($outcome -eq \\"Failed\\") { \\"Red\\" } else { \\"Green\\" }\\n $total = $results.TestRun.ResultSummary.Counters.total\\n $passed = $results.TestRun.ResultSummary.Counters.passed\\n $failed = $results.TestRun.ResultSummary.Counters.failed\\n\\n $failedTests = $results.TestRun.Results.UnitTestResult | Where-Object { $_.outcome -eq \\"Failed\\" }\\n\\n Write-Host Test Results: $outcome -ForegroundColor $fgColor -BackgroundColor \\"Black\\"\\n Write-Host Total tests: $total\\n Write-Host Passed: $passed\\n Write-Host Failed: $failed\\n Write-Host\\n\\n $failedTests | % { Write-Host Failed test: $_.testName\\n  Write-Host $_.Output.ErrorInfo.Message\\n  Write-Host $_.Output.ErrorInfo.StackTrace }\\n\\n Write-Host\\n\\n if($outcome -eq \\"Failed\\") {\\n  Write-Host \\"Failing build as there are broken tests\\"\\n  $host.SetShouldExit(1)\\n }\\n}\\n\\nfunction Run() {\\n  Write-Host \\"Running AfterBuild.ps1 using Configuration: $configuration, projectDir: $projectDir, targetDir: $targetDir, publishUrl: $publishUrl, buildNumber: $buildNumber, buildDefinitionName: $buildDefinitionName\\"\\n\\n if($buildNumber) {\\n  $resultsFile = \\"$projectDir\\\\test-results.trx\\"\\n  PublishTestResults $resultsFile\\n  FailBuildIfThereAreTestFailures $resultsFile\\n }\\n}\\n\\n# Off we go...\\nRun\\n```\\n\\nAssuming we have a build number this script kicks off the `PublishTestResults` function above. So we won\'t attempt to publish test results when compiling in Visual Studio on our dev machine. The script looks for `MSTest.exe` in a certain location on disk (the default VS 2015 installation location in fact; it may be installed elsewhere on your build machine). MSTest is then invoked and passed a file called `test-results.trx` which is is expected to live in the root of the project. All being well, the test results will be registered with the running build and will be visible when you look at test results in TFS.\\n\\nFinally in `FailBuildIfThereAreTestFailures` we parse the `test-results.trx` file (and for this I\'m totally in the debt of [David Robert\'s helpful Gist](https://gist.github.com/davidroberts63/5655441)). We write out the results to the host so it\'ll show up in the MSBuild logs. Also, and this is crucial, if there are any failures we fail the build by exiting PowerShell with a code of 1. We are deliberately swallowing any error that Karma raises earlier when it detects failed tests. We do this because we want to publish the failed test results to TFS _before_ we kill the build.\\n\\n## Bonus Karma: `test-results.trx`\\n\\nIf you\'ve read a [previous post of mine](../2016-02-19-visual-studio-tsconfigjson-and-external/index.md) you\'ll be aware that it\'s possible to get MSBuild to kick off npm build tasks. Specifically I have MSBuild kicking off an `npm run build`. My `package.json` looks like this:\\n\\n```json\\n\\"scripts\\": {\\n    \\"test\\": \\"karma start --reporters mocha,trx --single-run --browsers PhantomJS\\",\\n    \\"clean\\": \\"gulp delete-dist-contents\\",\\n    \\"watch\\": \\"gulp watch\\",\\n    \\"build\\": \\"gulp build\\",\\n    \\"postbuild\\": \\"npm run test\\"\\n  },\\n```\\n\\nYou can see that the `postbuild` hook kicks off the `test` script in turn. And that `test` script kicks off a single run of karma. I\'m not going to go over setting up Karma at all here; there are other posts out there that cover that admirably. But I wanted to share news of the [karma trx reporter](https://www.npmjs.com/package/karma-trx-reporter). This reporter is the thing that produces our `test-results.trx` file; trx being the format that TFS likes to deal with.\\n\\nSo now we\'ve got a PowerShell hook into our build process (something very useful in itself) which we are using to publish Karma test results into TFS 2012. They said it couldn\'t be done. They were wrong. Huzzah!!!!!!!"},{"id":"creating-angular-ui-routes-in-controller","metadata":{"permalink":"/creating-angular-ui-routes-in-controller","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2016-02-29-creating-angular-ui-routes-in-controller/index.md","source":"@site/blog/2016-02-29-creating-angular-ui-routes-in-controller/index.md","title":"Creating Angular UI Routes in the Controller","description":"Dont let your Angular UI Router link get too big - move the URL generation to the controller! Use the $state.href() method instead of ui-sref.","date":"2016-02-29T00:00:00.000Z","tags":[{"inline":false,"label":"Angular JS","permalink":"/tags/angularjs","description":"The original Angular JS framework."}],"readingTime":1.87,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"creating-angular-ui-routes-in-controller","title":"Creating Angular UI Routes in the Controller","authors":"johnnyreilly","tags":["angularjs"],"hide_table_of_contents":false,"description":"Dont let your Angular UI Router link get too big - move the URL generation to the controller! Use the $state.href() method instead of ui-sref."},"unlisted":false,"prevItem":{"title":"TFS 2012 meet PowerShell, Karma and BuildNumber","permalink":"/tfs-2012-meet-powershell-karma-and-buildnumber"},"nextItem":{"title":"Visual Studio, tsconfig.json and external TypeScript compilation","permalink":"/visual-studio-tsconfigjson-and-external"}},"content":"So you\'re creating a link with the Angular UI Router. You\'re passing more than a few parameters and it\'s getting kinda big. Something like this:\\n\\n\x3c!--truncate--\x3e\\n\\n```xml\\n<a class=\\"contains-icon\\"\\n      ui-sref=\\"Entity.Edit({ entityId: (vm.selectedEntityId ? vm.selectedEntityId: null), initialData: vm.initialData })\\">\\n        <i class=\\"fa fa-pencil\\"></i>Edit\\n   </a>\\n```\\n\\nSee? It\'s too long to fit on the screen without wrapping. It\'s clearly mad and bad.\\n\\nGenerally I try to keep the logic in a view to a minimum. It makes the view harder to read, it makes behaviour of the app harder to reason about. Also, it\'s not testable and (if you\'re using some kind of static typing like TypeScript) it is entirely out of the realms that a compiler can catch. So what to do? Move the URL generation to the controller. That\'s what I decided to do after I had a typo in my view which I didn\'t catch until post-commit.\\n\\n## `ui-sref` in the Controller\\n\\nActually, that\'s not exactly what you want to do. If you look at the [Angular UI Router docs](http://angular-ui.github.io/ui-router/site/#/api/ui.router.state.directive:ui-sref) you will see that `ui-sref` is:\\n\\n> ...a directive that binds a link (`&lt;a&gt;` tag) to a state. If the state has an associated URL, the directive will automatically generate & update the href attribute via the [`$state.href()`](http://angular-ui.github.io/ui-router/site/#/api/ui.router.state.$state#methods_href) method.\\n\\nSo what we actually want to do is use the `$state.href()` method in our controller. To take our example above we\'ll create another method on our controller called `getEditUrl`\\n\\n```js\\nexport class EntityController {\\n  $state: angular.ui.IStateService;\\n\\n  static $inject = [\'$state\'];\\n  constructor($state: angular.ui.IStateService) {\\n    this.$state = $state;\\n  }\\n\\n  //... Other stuff\\n\\n  getEditUrl() {\\n    return this.$state.href(\'Entity.Edit\', {\\n      selectedEntityId: this.selectedEntityId ? this.selectedEntityId : null,\\n      initialData: this.initialData,\\n    });\\n  }\\n}\\n```\\n\\nYou can see I\'m using TypeScript here; but feel free to strip out the type annotations and go with raw ES6 classes; that\'ll still give you testability if not static typing.\\n\\nNow we\'ve added the `getEditUrl` method we just need to reference it in our view:\\n\\n```xml\\n<a class=\\"contains-icon\\" ng-href=\\"{{vm.getEditUrl()}}\\"><i class=\\"fa fa-pencil\\"></i>Edit</a>\\n```\\n\\nNote we\'ve ditched usage of the `ui-sref` directive and gone with Angular\'s native [`ng-href`](https://docs.angularjs.org/api/ng/directive/ngHref). Within that directive we execute our `getEditUrl` as an expression which gives us our route. As a bonus, our view is much less cluttered and comprehensible as a result. How lovely."},{"id":"visual-studio-tsconfigjson-and-external","metadata":{"permalink":"/visual-studio-tsconfigjson-and-external","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2016-02-19-visual-studio-tsconfigjson-and-external/index.md","source":"@site/blog/2016-02-19-visual-studio-tsconfigjson-and-external/index.md","title":"Visual Studio, tsconfig.json and external TypeScript compilation","description":"Visual Studio will not gain support for tsconfig.json until TypeScript 1.8, so using external compilation may be preferable.","date":"2016-02-19T00:00:00.000Z","tags":[{"inline":false,"label":"Visual Studio","permalink":"/tags/visual-studio","description":"The Visual Studio IDE."},{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."},{"inline":false,"label":"Azure DevOps","permalink":"/tags/azure-devops","description":"The Azure DevOps suite of tools."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":5.86,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"visual-studio-tsconfigjson-and-external","title":"Visual Studio, tsconfig.json and external TypeScript compilation","authors":"johnnyreilly","tags":["visual studio","webpack","azure devops","typescript"],"hide_table_of_contents":false,"description":"Visual Studio will not gain support for tsconfig.json until TypeScript 1.8, so using external compilation may be preferable."},"unlisted":false,"prevItem":{"title":"Creating Angular UI Routes in the Controller","permalink":"/creating-angular-ui-routes-in-controller"},"nextItem":{"title":"TFS 2012, .NET 4.5 and C# 6","permalink":"/tfs-2012-net-45-and-c-6"}},"content":"TypeScript first gained support for [`tsconfig.json`](https://github.com/Microsoft/TypeScript/wiki/tsconfig.json) back with the [1\\\\.5 release](https://blogs.msdn.microsoft.com/typescript/2015/07/20/announcing-typescript-1-5/). However, to my lasting regret and surprise Visual Studio will not be gaining meaningful support for it until [TypeScript 1.8](https://github.com/Microsoft/TypeScript/wiki/What%27s-new-in-TypeScript#improved-support-for-tsconfigjson-in-visual-studio-2015) ships. However, if you want it now, it\'s already available to use in [beta](https://blogs.msdn.microsoft.com/typescript/2016/01/28/announcing-typescript-1-8-beta/).\\n\\n\x3c!--truncate--\x3e\\n\\nI\'ve already leapt aboard. Whilst there\'s a number of bugs in the beta it\'s still totally usable. So use it.\\n\\n## External TypeScript Compilation and the VS build\\n\\nWhilst `tsconfig.json` is useful and super cool it has limitations. It allows you to deactivate compilation upon file saving using [`compileOnSave`](https://github.com/Microsoft/TypeScript/issues/2326#issuecomment-178294169). [What it doesn\'t allow is deactivation of the TypeScript compilation that happens as part of a Visual Studio build.](https://github.com/Microsoft/TypeScript/issues/7091) That may not matter for the vanilla workflow of just dropping TypeScript files in a Visual Studio web project and having VS invoke the TypeScript compilation. However it comes to matter when your workflow deviates from the norm, as mine does. Using external compilation of TypeScript within Visual Studio is a little tricky. My own use case is somewhat atypical but perhaps not uncommon.\\n\\nI\'m working on a project which has been built using TypeScript since TS 0.9. Not surprisingly, this started off using the default Visual Studio / TypeScript workflow. Active development on the project ceased around 9 months ago. Now it\'s starting up again. It\'s a reasonable sized web app and the existing functionality is, in the main, fine. But the users want to add some new screens.\\n\\nLike any developer, I want to build with the latest and greatest. In my case, this means I want to write modular ES6 using TypeScript. With this approach my code can be leaner and I have less script ordering drama in my life. (Yay import statements!) This can be done by bringing together webpack, TypeScript ([ts-loader](https://github.com/TypeStrong/ts-loader)) and [Babel](http://babeljs.io/) ([babel-loader](https://github.com/babel/babel-loader)). There\'s an example of this approach [here](../2015-12-16-es6-typescript-babel-react-flux-karma/index.md). Given the size of the existing codebase I\'d rather leave the legacy TypeScript as is and isolate my new approach to the screens I\'m going to build. Obviously I\'d like to have a common build process for all the codebase at some point but I\'ve got a deadline to meet and so a half-old / half-new approach is called for (at least for the time being).\\n\\n## Goodbye TypeScript Compilation in VS\\n\\nWriting modular ES6 TypeScript which is fully transpiled to old-school JS is _not possible_ using the Visual Studio tooling at present. For what it\'s worth I think that SystemJS compilation may make this more possible in the future but I don\'t really know enough about it to be sure. That\'s why I\'m bringing webpack / Babel into the mix right now. I don\'t want Visual Studio to do anything for the ES6 code; I don\'t want it to compile. I want to deactivate my TypeScript compilation for the ES6 code. I can\'t do this from the `tsconfig.json` so I\'m in a bit of a hole. What to do?\\n\\nWell, as of (I think) TypeScript 1.7 it\'s possible to deactivate TypeScript compilation in Visual Studio. To [quote](https://github.com/Microsoft/TypeScript/issues/2294#issuecomment-129367578):\\n\\n> there is an easier way to disable TypeScriptCompile:\\n>\\n> Just add `&lt;TypeScriptCompileBlocked&gt;true&lt;/TypeScriptCompileBlocked&gt;` to the `.csproj`, e.g. in the first `&lt;PropertyGroup&gt;`.\\n\\nAwesomeness!\\n\\nBut wait, this means that the legacy TypeScript isn\'t being compiled any longer. Bear in mind, I\'m totally happy with the existing / legacy TypeScript compilation. Nooooooooooooooo!!!!!!!!!!!!!!!\\n\\n## Hello TypeScript Compilation outside VS\\n\\nHave no fear, I gotcha. What we\'re going to do is ensure that Visual Studio triggers 2 external TypeScript builds as part of its own build process:\\n\\n- The modular ES6 TypeScript (new)\\n- The legacy TypeScript (old)\\n\\nHow do we do this? Through the magic of build targets. We need to add this to our `.csproj`: (I add it near the end; I\'m not sure if location matters though)\\n\\n```xml\\n<PropertyGroup>\\n    <CompileDependsOn>\\n      $(CompileDependsOn);\\n      WebClientBuild;\\n    </CompileDependsOn>\\n    <CleanDependsOn>\\n      $(CleanDependsOn);\\n      WebClientClean\\n    </CleanDependsOn>\\n    <CopyAllFilesToSingleFolderForPackageDependsOn>\\n      CollectGulpOutput;\\n      CollectLegacyTypeScriptOutput;\\n      $(CopyAllFilesToSingleFolderForPackageDependsOn);\\n    </CopyAllFilesToSingleFolderForPackageDependsOn>\\n    <CopyAllFilesToSingleFolderForMsdeployDependsOn>\\n      CollectGulpOutput;\\n      CollectLegacyTypeScriptOutput;\\n      $(CopyAllFilesToSingleFolderForPackageDependsOn);\\n    </CopyAllFilesToSingleFolderForMsdeployDependsOn>\\n  </PropertyGroup>\\n  <Target Name=\\"WebClientBuild\\">\\n    <Exec Command=\\"npm install\\" />\\n    <Exec Command=\\"npm run build-legacy-typescript\\" />\\n    <Exec Command=\\"npm run build -- --mode $(ConfigurationName)\\" />\\n  </Target>\\n  <Target Name=\\"WebClientClean\\">\\n    <Exec Command=\\"npm run clean\\" />\\n  </Target>\\n  <Target Name=\\"CollectGulpOutput\\">\\n    <ItemGroup>\\n      <_CustomFiles Include=\\"dist\\\\**\\\\*\\" />\\n      <FilesForPackagingFromProject Include=\\"%(_CustomFiles.Identity)\\">\\n        <DestinationRelativePath>dist\\\\%(RecursiveDir)%(Filename)%(Extension)</DestinationRelativePath>\\n      </FilesForPackagingFromProject>\\n    </ItemGroup>\\n    <Message Text=\\"CollectGulpOutput list: %(_CustomFiles.Identity)\\" />\\n  </Target>\\n  <Target Name=\\"CollectLegacyTypeScriptOutput\\">\\n    <ItemGroup>\\n      <_CustomFiles Include=\\"Scripts\\\\**\\\\*.js\\" />\\n      <FilesForPackagingFromProject Include=\\"%(_CustomFiles.Identity)\\">\\n        <DestinationRelativePath>Scripts\\\\%(RecursiveDir)%(Filename)%(Extension)</DestinationRelativePath>\\n      </FilesForPackagingFromProject>\\n    </ItemGroup>\\n    <Message Text=\\"CollectLegacyTypeScriptOutput list: %(_CustomFiles.Identity)\\" />\\n  </Target>\\n```\\n\\nThere\'s a few things going on here; let\'s take them one by one.\\n\\n## The `WebClientBuild` Target\\n\\nThis target triggers our external builds. One by one it runs the following commands:\\n\\n<dl><dt><code>npm install</code></dt><dd>Installs the npm packages.</dd><dt><code>npm run build-legacy-typescript</code></dt><dd>Runs the <code>\\"build-legacy-typescript\\"</code><code>script</code> in our <code>package.json</code></dd><dt><code>npm run build -- --mode $(ConfigurationName)</code></dt><dd>Runs the <code>\\"build\\"</code><code>script</code> in our <code>package.json</code> and passes through a <code>mode</code> parameter of either <code>\\"Debug\\"</code> or <code>\\"Release\\"</code> from MSBuild - indicating whether we\'re creating a debug or a release build.</dd></dl>\\n\\nAs you\'ve no doubt gathered, I\'m following the convention of using the `scripts` element of my `package.json` as repository for the various build tasks I might have for a web project. It looks like this:\\n\\n```json\\n{\\n  // ...\\n  \\"scripts\\": {\\n    \\"test\\": \\"karma start --reporters mocha,junit --single-run --browsers PhantomJS\\",\\n    \\"build-legacy-typescript\\": \\"tsc -v&&tsc --project Scripts\\",\\n    \\"clean\\": \\"gulp delete-dist-contents\\",\\n    \\"watch\\": \\"gulp watch\\",\\n    \\"build\\": \\"gulp build\\"\\n  }\\n  // ...\\n}\\n```\\n\\nAs you can see, `\\"build-legacy-typescript\\"` invokes `tsc` (which is registered as a `devDependency`) to print out the version of the compiler. Then it invokes `tsc` again using the [`project`](https://github.com/Microsoft/TypeScript/wiki/Compiler-Options) flag directly on the `Scripts` directory. This is where the legacy TypeScript and its associated `tsconfig.json` resides. Et voil\xe1, the old / existing TypeScript is compiled just as it was previously by VS itself.\\n\\nNext, the `\\"build\\"` invokes a `gulp` task called, descriptively, `\\"build\\"`. This task caters for our brand new codebase of modular ES6 TypeScript. When run, this task will invoke webpack, copy static files, build less etc. Quick digression: you can see there\'s a `\\"watch\\"` script that does the same thing on a file-watching basis; I use that during development.\\n\\n## The `WebClientClean` Target\\n\\nThe task that runs to clean up artefacts created by `WebClientBuild`.\\n\\n## The `CollectLegacyTypeScriptOutput` and `CollectGulpOutput` Targets\\n\\nSince we\'re compiling our TypeScript outside of VS we need to tell MSBuild / MSDeploy about the externally compiled assets in order that they are included in the publish pipeline. Here I\'m standing on the shoulders of [Steve Cadwallader\'s excellent post](http://www.codecadwallader.com/2015/03/15/integrating-gulp-into-your-tfs-builds-and-web-deploy/). Thanks Steve!\\n\\n`CollectLegacyTypeScriptOutput` and `CollectGulpOutput` respectively include all the built files contained in the `\\"Scripts\\"` and `\\"dist\\"` folders when a publish takes place. You don\'t need this for when you\'re building on your own machine but if you\'re looking to publish (either from your machine or from TFS) then you will need exactly this. Believe me that last sentence was typed with a memory of _great_ pain and frustration.\\n\\nSo in the end, as far as TypeScript is concerned, I\'m using Visual Studio solely as an editor. It\'s the hooks in the `.csproj` that ensure that compilation happens. It seems a little quirky that we still need to have the original TypeScript targets in the `.csproj` file as well; but it works. That\'s all that matters."},{"id":"tfs-2012-net-45-and-c-6","metadata":{"permalink":"/tfs-2012-net-45-and-c-6","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2016-02-01-tfs-2012-net-45-and-c-6/index.md","source":"@site/blog/2016-02-01-tfs-2012-net-45-and-c-6/index.md","title":"TFS 2012, .NET 4.5 and C# 6","description":"Use C# 6 features on .NET 4.5 with Visual Studio 2015, set MSBuild Arguments and install Microsoft.Net.Compilers on the old build server.","date":"2016-02-01T00:00:00.000Z","tags":[{"inline":false,"label":"C#","permalink":"/tags/csharp","description":"The C# programming language."},{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."},{"inline":false,"label":"Azure DevOps","permalink":"/tags/azure-devops","description":"The Azure DevOps suite of tools."}],"readingTime":0.855,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"tfs-2012-net-45-and-c-6","title":"TFS 2012, .NET 4.5 and C# 6","authors":"johnnyreilly","tags":["c#","asp.net","azure devops"],"hide_table_of_contents":false,"description":"Use C# 6 features on .NET 4.5 with Visual Studio 2015, set MSBuild Arguments and install Microsoft.Net.Compilers on the old build server."},"unlisted":false,"prevItem":{"title":"Visual Studio, tsconfig.json and external TypeScript compilation","permalink":"/visual-studio-tsconfigjson-and-external"},"nextItem":{"title":"Coded UI and the Curse of the Docking Station","permalink":"/coded-ui-and-curse-of-docking-station"}},"content":"So, you want to use C# 6 language features and you\u2019re working on an older project that\u2019s still rocking .NET 4.5. Well, with [some caveats](http://stackoverflow.com/a/28921749/761388), you can.\\n\\n\x3c!--truncate--\x3e\\n\\nThe new compiler will compile targeting older framework versions. Well that\u2019s all lovely; let\u2019s all go home.\\n\\nNow. What say you\u2019ve got an old, old build server? It\u2019s TFS 2012 Update 2, creaking away, still glad to alive and kind of wondering why it hasn\u2019t been upgraded or retired. This is where you want to compile .NET 4.5 from C# 6. Well it can be done. Here\u2019s how it\u2019s done:\\n\\n1. Install Visual Studio 2015 on the build server (I\u2019m told this can be achieved using [Microsoft Build Tools 2015](https://www.microsoft.com/en-us/download/details.aspx?id=48159) but I haven\u2019t tried it myelf so caveat emptor)\\n2. set the `MSBuild Arguments` in the build definition to `/p:VisualStudioVersion=14.0` (i.e. Visual Studio 2015 mode)\\n\\n![](EditBuildConfiguration.webp)\\n\\n3. in each project that uses C# 6 syntax, install the NuGet package [Microsoft.Net.Compilers](https://www.nuget.org/packages/Microsoft.Net.Compilers) with a quick `install-package Microsoft.Net.Compilers`\\n\\nThat\u2019s it; huzzah! String interpolation here I come\u2026"},{"id":"coded-ui-and-curse-of-docking-station","metadata":{"permalink":"/coded-ui-and-curse-of-docking-station","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2016-01-14-coded-ui-and-curse-of-docking-station/index.md","source":"@site/blog/2016-01-14-coded-ui-and-curse-of-docking-station/index.md","title":"Coded UI and the Curse of the Docking Station","description":"Coded UI tests struggle with docking stations due to resolution changes, causing the mouse to miss the element its aiming for.","date":"2016-01-14T00:00:00.000Z","tags":[{"inline":false,"label":"Automated Testing","permalink":"/tags/automated-testing","description":"How to perform the automation of tests."}],"readingTime":2.425,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"coded-ui-and-curse-of-docking-station","title":"Coded UI and the Curse of the Docking Station","authors":"johnnyreilly","tags":["automated testing"],"hide_table_of_contents":false,"description":"Coded UI tests struggle with docking stations due to resolution changes, causing the mouse to miss the element its aiming for."},"unlisted":false,"prevItem":{"title":"TFS 2012, .NET 4.5 and C# 6","permalink":"/tfs-2012-net-45-and-c-6"},"nextItem":{"title":"UseStaticFiles for ASP.Net Framework","permalink":"/usestaticfiles-for-aspnet-vold"}},"content":"I\u2019ve a love / hate relationship with Coded UI. Well hate / hate might be more accurate. Hate perhaps married with a very grudging respect still underpinned by a wary bitterness. Yes, that\u2019s about the size of it. Why? Well, when Coded UI works, it\u2019s fab. But it\u2019s flaky as anything. Anybody who\u2019s used the technology is presently nodding sagely and holding back the tears. It\u2019s all a bit... tough.\\n\\n\x3c!--truncate--\x3e\\n\\nI\u2019ve recently discovered another quirk to add to the list. Docking stations. I was back working on a project which had a Coded UI test suite. I\u2019d heard tell that there were problems with the tests and was just taking a look at them. The first hurdle I fell at was getting the tests to run locally. The tests had first been developed on a standard desktop build and, as much as this can ever be said of Coded UI tests, they worked. However, the future had happened. The company in question was no longer using the old school desktop towers. Nope, they\u2019d reached for the sky and equipped the whole office with Surface Pro 3\u2019s, hot desks, docking stations and big, big monitors. It looked terribly flash.\\n\\nCoded UI was not happy.\\n\\nThe `Mouse.Click` behaviour wasn\u2019t working. Most tests need the ability for users to click on buttons, dropdowns etc. That\u2019s part of a normal UI. And so it was with these tests. This is where they fell over. The reason they fell over at this point didn\u2019t become clear for a while. It wasn\u2019t until we tried tweaking our implementation of the tests that we realised what was happening. The tests normally found buttons / dropdowns etc on the screen and then attempted to perform a `Mouse.Click` upon them. We changed the implementation to be subtly different. Instead of just clicking on the element we amended the test to move the mouse to the button and then perform the click.\\n\\nAha!\\n\\nRather than steadily moving towards an element and clicking, the pointer was swerving like a drunk man crossing the road at 3am. It completely missed the element it was aiming for and clicked upon a seemingly random area of the screen. This is Coded UI doing \u201Cpin the tail on the donkey\u201D.\\n\\nAfter more time than I\'d like to admit I happened upon the solution. I tended to dock my Surface and then tune my monitor resolution to the one most optimal for coding. (ie really high res.) This is what messes with Coded UI\'s head; the resolution change. If I wanted to be able to run tests successfully all I had to do was switch back to the resolution I initially booted with. Alternately I could restart my computer so it launched with the resolution I was presently using.\\n\\nOnce you do follow this guidance Coded UI has a moment of clarity, gets sober and starts `Mouse.Click`\\\\-ing like a pro."},{"id":"usestaticfiles-for-aspnet-vold","metadata":{"permalink":"/usestaticfiles-for-aspnet-vold","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2016-01-01-usestaticfiles-for-aspnet-vold/index.md","source":"@site/blog/2016-01-01-usestaticfiles-for-aspnet-vold/index.md","title":"UseStaticFiles for ASP.Net Framework","description":"Learn how to prevent exposing static files to the public when working with ASP.Net Framework. Discover how to implement an allowlist approach.","date":"2016-01-01T00:00:00.000Z","tags":[{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":6.075,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"usestaticfiles-for-aspnet-vold","title":"UseStaticFiles for ASP.Net Framework","authors":"johnnyreilly","tags":["asp.net"],"hide_table_of_contents":false,"description":"Learn how to prevent exposing static files to the public when working with ASP.Net Framework. Discover how to implement an allowlist approach."},"unlisted":false,"prevItem":{"title":"Coded UI and the Curse of the Docking Station","permalink":"/coded-ui-and-curse-of-docking-station"},"nextItem":{"title":"Live Reload Considered Harmful","permalink":"/live-reload-considered-harmful"}},"content":"This is a guide on how _not_ to expose all your static files to the world at large when working with the ASP.Net Framework. How to move from a blocklisting approach to a allowlisting approach.\\n\\n\x3c!--truncate--\x3e\\n\\nNot clear? Stick around; I\'ll get better. Oh and that\'s not all, we\'ve also got.... drumroll:\\n\\n## Support for [HTML5 History API](https://html.spec.whatwg.org/multipage/browsers.html#the-history-interface)!\\n\\nWhat that means, in as close to English as I can get it, is real URLs for Single Page Applications. None of that hash-based routing malarkey. So, `https://i-am-your-domain.com/i-am-your-route` rather than `https://i-am-your-domain.com/<em>#/</em>i-am-your-route`. (For a more in depth look at the different sorts of routing SPA\'s can use then take a look at the [excellent docs](http://rackt.org/history/stable/GettingStarted.html) by the folk behind [React Router](https://github.com/rackt/react-router). These concepts are not React specific and can be applied to any SPA technology.)\\n\\n## `UseStaticFiles`\\n\\nYou may be aware that historically ASP.Net has been somewhat unusual in its approach to serving static files. Essentially, all the files in a project are theoretically servable. Okay, that\'s not entirely true; things like the `web.config` files etc are not going to be handed over to someone browsing your site. But other files that you might well want kept away from prying eyes may be. So your [TypeScript](http://www.typescriptlang.org/) files, your [Less](http://lesscss.org/) files are all up for grabs unless you take action to block access to them. This is, and has always been, bad.\\n\\nThe ASP.Net team know this and things are changing with ASP.Net 5. With the new stack you have to say \\"these are the static files we want people to access\\" in the form of an `<a href=\\"https://msdn.microsoft.com/en-us/library/dn782589(v=vs.113).aspx\\">app.UseStaticFiles()</a>` declaration. This is mighty similar to how you do things in other frameworks such as [Express](http://expressjs.com/en/starter/static-files.html). To quote the [docs](https://docs.asp.net/en/latest/fundamentals/static-files.html#serving-static-files):\\n\\n> By default, static files are stored in the webroot of your project. The location of the webroot is defined in the project\u2019s `project.json` file where the default is wwwroot.\\n>\\n> ```json\\n> \\"webroot\\": \\"wwwroot\\"\\n> ```\\n>\\n> Static files can be stored in any folder under the webroot and accessed with a relative path to that root. For example, when you create a default Web application project using Visual Studio, there are several folders created within the webroot folder - `css`, `images` and `js`. In order to directly access an image in the images subfolder, the URL would look like the following:\\n>\\n> `http://&lt;yourApp&gt;/images/&lt;imageFileName&gt;`\\n\\nSo how do we get this behaviour with ASP.Net vOld? Well, it\'s just a matter of `web.config` URL rewrite twiddling:\\n\\n```xml\\n<configuration>\\n  \x3c!-- other config --\x3e\\n\\n  <system.webServer>\\n    <rewrite>\\n      <rules>\\n        <rule name=\\"Map empty URLs to the index.html\\">\\n          <match url=\\"^$\\" />\\n          <action type=\\"Rewrite\\" url=\\"/index.html\\" />\\n        </rule>\\n        <rule name=\\"Map all requests with a \'.\' in to the \'build\' directory\\" stopProcessing=\\"true\\">\\n          <match url=\\"^(.*[.].*)$\\" />\\n          <action type=\\"Rewrite\\" url=\\"/build/{R:1}\\" />\\n        </rule>\\n      </rules>\\n    </rewrite>\\n  </system.webServer>\\n</configuration>\\n```\\n\\nMy `webroot` is named `build` rather than `wwwroot`. The 2 URL rewrite rules above do the following:\\n\\n<dl><dt>Map empty URLs to the index.html</dt><dd>Empty URLs (ie the URL for the root of your site) are mapped to <code>index.html</code>. The <code>index.html</code> in the <code>build</code> folder is the home page of this particular site and the next rule will make sure that we hit that. (Since we haven\'t set <code>stopProcessing</code> to <code>true</code> for this particular rule the next rule will be processed after this one.)</dd><dt>Map all requests with a \'.\' in to the \'build\' directory</dt><dd>All URLs with a \\".\\" in the title (including <code>index.html</code>) are redirected to the <code>build</code> folder. All static files have a \\".\\" in them because filenames have suffixes. This essentially means all requests for files are served from the <code>build</code> folder. In this case we have set <code>stopProcessing</code> to <code>true</code> which means that any URLs that matched this rule will be not be affected by any subsequent rules.</dd></dl>\\n\\nSo if anyone sneakily tries to sneakily browse to say, `http://&lt;yourApp&gt;/js/app.ts` then they\'ll be nicely redirected to the non-existent `build/js/app.ts`. 404 in your face!\\n\\n## \\"I am SPArtucus\\"\\n\\nWhen you have a Single Page Application you want the same web experience as a server side rendered web app. What I mean by this is: routing just works. You want people to be able to go to `https://i-am-your-domain.com/i-am-your-route` and get your site at the specified route. Happily, whether you\'re using React Router, Angular UI Router or something else, the client side is covered. They can be configured to detect the route that you enter at and serve up the SPA in the relevant state. But you have to meet them halfway; the server needs to do its bit.\\n\\nWhen a URL is requested that doesn\'t look like a request for a static file, let\'s make the (reasonable) assumption that this is a route URL and serve up the shell SPA page. So, for my own example of an Angular 1.x app I want the server to hand over `/build/index.html`.\\n\\nThis is the magic that makes real URLs and SPAs work. Provided the client hasn\'t requested a static file, every request to the server will be responded to with our very own \\"I am SPArtucus\\"; the shell SPA page. This is catered for by the addition of another new rule to our `web.config`:\\n\\n```xml\\n<configuration>\\n  \x3c!-- other config --\x3e\\n\\n  <system.webServer>\\n    <rewrite>\\n      <rules>\\n        <rule name=\\"Map empty URLs to the index.html\\">\\n          <match url=\\"^$\\" />\\n          <action type=\\"Rewrite\\" url=\\"/index.html\\" />\\n        </rule>\\n        <rule name=\\"Map all requests with a \'.\' in to the \'build\' directory\\" stopProcessing=\\"true\\">\\n          <match url=\\"^(.*[.].*)$\\" />\\n          <action type=\\"Rewrite\\" url=\\"/build/{R:1}\\" />\\n        </rule>\\n        <rule name=\\"Map all other URLs to the index.html - this to support our SPA routes\\">\\n          <match url=\\"^.*$\\" />\\n          <action type=\\"Rewrite\\" url=\\"/build/index.html\\" />\\n        </rule>\\n      </rules>\\n    </rewrite>\\n  </system.webServer>\\n</configuration>\\n```\\n\\n<dl><dt>Map all other URLs to the index.html - this to support our SPA routes</dt><dd>Our new rule says \\"whatever URL turns up, if it hasn\'t been catered for by an existing rule, well it must be a SPA route, so we\'ll serve up the shell SPA page of <code>build/index.html</code>\\".</dd></dl>\\n\\n## Data! Data! Data!.. I can\'t make bricks without clay.\\n\\nSherlock Holmes was onto something; most applications are nothing without data. What you\'ve got at present is an application that carefully restricts access to static files and, for all other requests, serves up our shell SPA page. So let\'s relax our final rule a little to make data access a thing:\\n\\n```xml\\n<configuration>\\n  \x3c!-- other config --\x3e\\n\\n  <system.webServer>\\n    <rewrite>\\n      <rules>\\n        <rule name=\\"Map empty URLs to the index.html\\">\\n          <match url=\\"^$\\" />\\n          <action type=\\"Rewrite\\" url=\\"/index.html\\" />\\n        </rule>\\n        <rule name=\\"Map all requests with a \'.\' in to the \'build\' directory\\" stopProcessing=\\"true\\">\\n          <match url=\\"^(.*[.].*)$\\" />\\n          <action type=\\"Rewrite\\" url=\\"/build/{R:1}\\" />\\n        </rule>\\n        <rule name=\\"Map non-api URLs to the index.html - this to support our SPA routes\\">\\n          <match url=\\"^(api/).*$\\" negate=\\"true\\" ignoreCase=\\"true\\" />\\n          <action type=\\"Rewrite\\" url=\\"/build/index.html\\" />\\n        </rule>\\n      </rules>\\n    </rewrite>\\n  </system.webServer>\\n</configuration>\\n```\\n\\n<dl><dt>Map non-api URLs to the index.html - this to support our SPA routes</dt><dd>This amended rule says \\"whatever URL turns up, <em>unless it begins <code>\\"api/\\"</code></em>, if it hasn\'t been catered for by an existing rule, well it must be a SPA route, so we\'ll serve up the shell SPA page of <code>build/index.html</code>\\".</dd></dl>\\n\\nThis allows us to perform data access by prefixing all the Web API routes with `\\"api/\\"`. I\'ve picked this because it is the default location for ASP.Net Web API routes. It is (like most things) entirely configurable. To see a working implementation of this complete approach then take a look at the PoorClaresAngular project [here](https://github.com/johnnyreilly/poorclaresarundel/tree/15e7d4ddc0f1c06fe326b44c3bdc71ceb554bf73)."},{"id":"live-reload-considered-harmful","metadata":{"permalink":"/live-reload-considered-harmful","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2015-12-20-live-reload-considered-harmful/index.md","source":"@site/blog/2015-12-20-live-reload-considered-harmful/index.md","title":"Live Reload Considered Harmful","description":"Live Reload is a popular development tool that refreshes web pages automatically, however, its usefulness is questionable. It can disrupt app design.","date":"2015-12-20T00:00:00.000Z","tags":[],"readingTime":2.46,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"live-reload-considered-harmful","title":"Live Reload Considered Harmful","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false,"description":"Live Reload is a popular development tool that refreshes web pages automatically, however, its usefulness is questionable. It can disrupt app design."},"unlisted":false,"prevItem":{"title":"UseStaticFiles for ASP.Net Framework","permalink":"/usestaticfiles-for-aspnet-vold"},"nextItem":{"title":"ES6 + TypeScript + Babel + React + Flux + Karma: The Secret Recipe","permalink":"/es6-typescript-babel-react-flux-karma"}},"content":"I\'ve seen it go by many names; [live reload](http://livereload.com/), hot reload, [browser sync](https://browsersync.io/)... the list goes on. It\'s been the subject of a million demos. It\'s the focus of a thousand npm packages. Someone tweaks a file and... wait for it... _doesn\'t have to refresh their browser to see the changes_... The future is now!\\n\\n\x3c!--truncate--\x3e\\n\\nForgive me the sarcasm, but I have come to the conclusion that whilst live reload is impressive... for my own purposes, it is not actually that useful. It certainly shouldn\'t be the default goto that it seems to have become.\\n\\nHear me out people, I may be the voice crying out in the wilderness but I\'m right dammit.\\n\\n![](tumblr_mxjpcobvcg...6_r2_250-4abb938.gif)\\n\\n## Why is Live Reload a Thing?\\n\\nWhat is live reload? Well having to hit F5 after you\'ve made a change... That seems like such hard work right? To quote [Phil Haack](http://haacked.com/archive/2011/12/13/better-git-with-powershell.aspx/):\\n\\n> ... we\u2019re software developers.... It\u2019s time to AWW TOE MATE!\\n\\nYup, automation. Anything that a developer can theoretically automate.... will be automated. Usually this is a good thing but automation can be addictive. And on this occasion it\'s time for an intervention.\\n\\nWhat else could be the attraction? Well, this is speculation but I would say that the implementation actually has something to do with it. Live reload is almost invariably powered by [WebSockets](https://en.wikipedia.org/wiki/WebSocket) and they are certainly cool. Developers I know what you are like. You\'re attracted by the new shiny thing. You can\'t resist the allure of WS. And there with live reload idling away in the background you\'re all bleeding edge. I can say all this because this is exactly what I am like.\\n\\n## Why is Live Reload a BAD Thing?\\n\\nWell the OCD part of me is instinctively repelled by the extra `script` tag of alien code that live reload foists upon your app. How very dare that `&lt;script src=\\"http://localhost:35729/livereload.js?snipver=1\\"&gt;&lt;/script&gt;` push its way into my pristine DOM. It\'s an outrage.\\n\\nPerhaps a more convincing rationale is how useful it is to have 2 different versions of your app up on screen at the same time. I like to try things out when I\'m working. I get a screen working one way and then I tweak and play with my implementation. I have the app of 10 minutes ago sat side by side with the newly adjusted one. Assess, compare and and declare a winner. That\'s so useful and live reload does away with it. That\'s a problem.\\n\\nFinally, I\'m an obsessive \'Ctrl-S\'-er. I\'ve been burned by unsaved changes too many times. I\'m saving every couple of keypresses. With live reload that usually means I have the noise of a dead application in the corner of my eye as LR obsessively forces the latest brokenness upon me. That sucks.\\n\\nI\'ve no doubt there are situations where live reload is useful. But for my money that\'s the exception rather than the rule. Let the madness end now. Just say \\"no\\", kids."},{"id":"es6-typescript-babel-react-flux-karma","metadata":{"permalink":"/es6-typescript-babel-react-flux-karma","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2015-12-16-es6-typescript-babel-react-flux-karma/index.md","source":"@site/blog/2015-12-16-es6-typescript-babel-react-flux-karma/index.md","title":"ES6 + TypeScript + Babel + React + Flux + Karma: The Secret Recipe","description":"Learn how to set up a powerful TypeScript-React workflow with webpack, gulp, Karma, and inject in this comprehensive article.","date":"2015-12-16T00:00:00.000Z","tags":[{"inline":false,"label":"React","permalink":"/tags/react","description":"The React library."},{"inline":false,"label":"ts-loader","permalink":"/tags/ts-loader","description":"The TypeScript loader for webpack."},{"inline":false,"label":"webpack","permalink":"/tags/webpack","description":"The webpack module bundler."},{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."},{"inline":false,"label":"Automated Testing","permalink":"/tags/automated-testing","description":"How to perform the automation of tests."}],"readingTime":13.63,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"es6-typescript-babel-react-flux-karma","title":"ES6 + TypeScript + Babel + React + Flux + Karma: The Secret Recipe","authors":"johnnyreilly","tags":["react","ts-loader","webpack","javascript","automated testing"],"hide_table_of_contents":false,"description":"Learn how to set up a powerful TypeScript-React workflow with webpack, gulp, Karma, and inject in this comprehensive article."},"unlisted":false,"prevItem":{"title":"Live Reload Considered Harmful","permalink":"/live-reload-considered-harmful"},"nextItem":{"title":"IQueryable... IEnumerable... Hmmm...","permalink":"/iqueryable-ienumerable-hmmm"}},"content":"I wrote [a while ago](../2015-09-10-things-done-changed/index.md) about how I was using some different tools in a current project:\\n\\n\x3c!--truncate--\x3e\\n\\n- React with JSX\\n- Flux\\n- ES6 with Babel\\n- Karma for unit testing\\n\\nI have fully come to love and appreciate all of the above. I really like working with them. However. There was still an ache in my soul and a thorn in my side. Whilst I love the syntax of ES6 and even though I\'ve come to appreciate the clarity of JSX, I have been missing something. Perhaps you can guess? It\'s static typing.\\n\\nIt\'s actually been really good to have the chance to work without it because it\'s made me realise what a productivity boost having static typing actually is. The number of silly mistakes burning time that a compiler could have told me.... Sigh.\\n\\nBut the pain is over. The dark days are gone. It\'s possible to have strong typing, courtesy of TypeScript, plugged into this workflow. It\'s yours for the taking. Take it. Take it now!\\n\\n## What a Guy Wants\\n\\nI decided a couple of months ago what I wanted to have in my setup:\\n\\n1. I want to be able to write React / JSX in TypeScript. Naturally I couldn\'t achieve that by myself but handily the TypeScript team decided to add support for JSX with [TypeScript 1.6](https://blogs.msdn.com/b/typescript/archive/2015/09/16/announcing-typescript-1-6.aspx). Ooh yeah.\\n2. I wanted to be able to write ES6. When I realised [the approach for writing ES6 and having the transpilation handled by TypeScript wasn\'t clear](https://github.com/Microsoft/TypeScript/issues/3956) I had another idea. I thought [\\"what if I write ES6 and hand off the transpilation to Babel?\\"](https://github.com/Microsoft/TypeScript/issues/4765) i.e. Use TypeScript for type checking, not for transpilation. I realised that [James Brantly had my back](http://www.jbrantly.com/es6-modules-with-typescript-and-webpack/#configuringwebpack) here already. Enter [webpack](https://webpack.github.io/) and [ts-loader](https://github.com/TypeStrong/ts-loader).\\n3. Debugging. Being able to debug my code is non-negotiable for me. If I can\'t debug it I\'m less productive. (I\'m also bitter and twisted inside.) I should say that I wanted to be able to debug my _original_ source code. Thanks to the magic of [sourcemaps](https://docs.google.com/document/d/1U1RGAehQwRypUTovF1KRlpiOFze0b-_2gc6fAH0KY0k/edit?usp=sharing), that mad thing is possible.\\n4. Karma for unit testing. I\'ve become accustomed to writing my tests in ES6 and running them on a continual basis with [Karma](https://karma-runner.github.io/0.13/index.html). This allows for a rather good debugging story as well. I didn\'t want to lose this when I moved to TypeScript. I didn\'t.\\n\\nSo I\'ve talked about what I want and I\'ve alluded to some of the solutions that there are. The question now is how to bring them all together. This post is, for the most part, going to be about correctly orchestrating a number of [gulp tasks](http://gulpjs.com/) to achieve the goals listed above. If you\'re after the [Blue Peter \\"here\'s one I made earlier\\"](https://en.wikipedia.org/wiki/Blue_Peter) moment then take a look at [the es6-babel-react-flux-karma repo](https://github.com/Microsoft/TypeScriptSamples/tree/master/es6-babel-react-flux-karma) in the [Microsoft/TypeScriptSamples repo on Github](https://github.com/Microsoft/TypeScriptSamples).\\n\\n## gulpfile.js\\n\\n```js\\n/* eslint-disable no-var, strict, prefer-arrow-callback */\\n\'use strict\';\\n\\nvar gulp = require(\'gulp\');\\nvar gutil = require(\'gulp-util\');\\nvar connect = require(\'gulp-connect\');\\nvar eslint = require(\'gulp-eslint\');\\nvar webpack = require(\'./gulp/webpack\');\\nvar staticFiles = require(\'./gulp/staticFiles\');\\nvar tests = require(\'./gulp/tests\');\\nvar clean = require(\'./gulp/clean\');\\nvar inject = require(\'./gulp/inject\');\\n\\nvar lintSrcs = [\'./gulp/**/*.js\'];\\n\\ngulp.task(\'delete-dist\', function (done) {\\n  clean.run(done);\\n});\\n\\ngulp.task(\'build-process.env.NODE_ENV\', function () {\\n  process.env.NODE_ENV = \'production\';\\n});\\n\\ngulp.task(\\n  \'build-js\',\\n  [\'delete-dist\', \'build-process.env.NODE_ENV\'],\\n  function (done) {\\n    webpack.build().then(function () {\\n      done();\\n    });\\n  },\\n);\\n\\ngulp.task(\\n  \'build-other\',\\n  [\'delete-dist\', \'build-process.env.NODE_ENV\'],\\n  function () {\\n    staticFiles.build();\\n  },\\n);\\n\\ngulp.task(\'build\', [\'build-js\', \'build-other\', \'lint\'], function () {\\n  inject.build();\\n});\\n\\ngulp.task(\'lint\', function () {\\n  return gulp.src(lintSrcs).pipe(eslint()).pipe(eslint.format());\\n});\\n\\ngulp.task(\'watch\', [\'delete-dist\'], function () {\\n  process.env.NODE_ENV = \'development\';\\n  Promise.all([\\n    webpack.watch(), //,\\n    //less.watch()\\n  ])\\n    .then(function () {\\n      gutil.log(\\n        \'Now that initial assets (js and css) are generated inject will start...\',\\n      );\\n      inject.watch(postInjectCb);\\n    })\\n    .catch(function (error) {\\n      gutil.log(\'Problem generating initial assets (js and css)\', error);\\n    });\\n\\n  gulp.watch(lintSrcs, [\'lint\']);\\n  staticFiles.watch();\\n  tests.watch();\\n});\\n\\ngulp.task(\'watch-and-serve\', [\'watch\'], function () {\\n  postInjectCb = stopAndStartServer;\\n});\\n\\nvar postInjectCb = null;\\nvar serverStarted = false;\\nfunction stopAndStartServer() {\\n  if (serverStarted) {\\n    gutil.log(\'Stopping server\');\\n    connect.serverClose();\\n    serverStarted = false;\\n  }\\n  startServer();\\n}\\n\\nfunction startServer() {\\n  gutil.log(\'Starting server\');\\n  connect.server({\\n    root: \'./dist\',\\n    port: 8080,\\n  });\\n  serverStarted = true;\\n}\\n```\\n\\nLet\'s start picking this apart; what do we actually have here? Well, we have 2 gulp tasks that I want you to notice:\\n\\n<dl><dt>build</dt><dd><p>This is likely the task you would use when deploying. It takes all of your source code, builds it, provides cache-busting filenames (eg <code>main.dd2fa20cd9eac9d1fb2f.js</code>), injects your shell SPA page with references to the files and deploys everything to the <code>./dist/</code> directory. So that\'s TypeScript, static assets like images and CSS all made ready for Production.</p><p>The build task also implements this advice:</p><blockquote cite=\\"https://facebook.github.io/react/blog/2015/09/10/react-v0.14-rc1.html\\">When deploying your app, set the <code>NODE_ENV</code> environment variable to <code>production</code> to use the production build of React which does not include the development warnings and runs significantly faster. </blockquote></dd><dt>watch-and-serve</dt><dd><p>This task represents \\"development mode\\" or \\"debug mode\\". It\'s what you\'ll likely be running as you develop your app. It does the same as the build task but with some important distinctions.</p><ul><li>As well as building your source it also runs your tests using Karma</li><li>This task is not triggered on a once-only basis, rather your files are watched and each tweak of a file will result in a new build and a fresh run of your tests. Nice eh?</li><li>It spins up a simple web server and serves up the contents of <code>./dist</code> (i.e. your built code) in order that you can easily test out your app.</li><li>In addition, whilst it builds your source it does <em>not</em> minify your code and it emits sourcemaps. For why? For debugging! You can go to <code>http://localhost:8080/</code> in your browser of choice, fire up the dev tools and you\'re off to the races; debugging like gangbusters. It also doesn\'t bother to provide cache-busting filenames as Chrome dev tools are smart enough to not cache localhost.</li><li>Oh and Karma.... If you\'ve got problems with a failing test then head to <code>http://localhost:9876/</code> and you can debug the tests in your dev tools.</li><li>Finally, it runs ESLint in the console. Not all of my files are TypeScript; essentially the build process (aka \\"gulp-y\\") files are all vanilla JS. So they\'re easily breakable. ESLint is there to provide a little reassurance on that front.</li></ul></dd></dl>\\n\\nNow let\'s dig into each of these in a little more detail\\n\\n## WebPack\\n\\nLet\'s take a look at what\'s happening under the covers of `webpack.build()` and `webpack.watch()`.\\n\\n[WebPack](https://github.com/webpack/webpack) with [ts-loader](https://github.com/TypeStrong/ts-loader) and [babel-loader](https://github.com/babel/babel-loader) is what we\'re using to compile our ES6 TypeScript. ts-loader uses the TypeScript compiler to, um, compile TypeScript and emit ES6 code. This is then passed on to the babel-loader which transpiles it from ES6 down to ES-old-school. It all gets brought together in 2 files; `main.js` which contains the compiled result of the code written by us and `vendor.js` which contains the compiled result of 3rd party / vendor files. The reason for this separation is that vendor files are likely to change fairly rarely whilst our own code will constantly be changing. This separation allows for quicker compile times upon file changes as, for the most part, the vendor files will not need to included in this process.\\n\\nOur `gulpfile.js` above uses the following task:\\n\\n```js\\n\'use strict\';\\n\\nvar gulp = require(\'gulp\');\\nvar gutil = require(\'gulp-util\');\\nvar webpack = require(\'webpack\');\\nvar WebpackNotifierPlugin = require(\'webpack-notifier\');\\nvar webpackConfig = require(\'../webpack.config.js\');\\n\\nfunction buildProduction(done) {\\n  // modify some webpack config options\\n  var myProdConfig = Object.create(webpackConfig);\\n  myProdConfig.output.filename = \'[name].[hash].js\';\\n\\n  myProdConfig.plugins = myProdConfig.plugins.concat(\\n    // make the vendor.js file with cachebusting filename\\n    new webpack.optimize.CommonsChunkPlugin({\\n      name: \'vendor\',\\n      filename: \'vendor.[hash].js\',\\n    }),\\n    new webpack.optimize.DedupePlugin(),\\n    new webpack.optimize.UglifyJsPlugin(),\\n  );\\n\\n  // run webpack\\n  webpack(myProdConfig, function (err, stats) {\\n    if (err) {\\n      throw new gutil.PluginError(\'webpack:build\', err);\\n    }\\n    gutil.log(\\n      \'[webpack:build]\',\\n      stats.toString({\\n        colors: true,\\n      }),\\n    );\\n\\n    if (done) {\\n      done();\\n    }\\n  });\\n}\\n\\nfunction createDevCompiler() {\\n  // show me some sourcemap love people\\n  var myDevConfig = Object.create(webpackConfig);\\n  myDevConfig.devtool = \'inline-source-map\';\\n  myDevConfig.debug = true;\\n\\n  myDevConfig.plugins = myDevConfig.plugins.concat(\\n    // Make the vendor.js file\\n    new webpack.optimize.CommonsChunkPlugin({\\n      name: \'vendor\',\\n      filename: \'vendor.js\',\\n    }),\\n    new WebpackNotifierPlugin({\\n      title: \'webpack build\',\\n      excludeWarnings: true,\\n    }),\\n  );\\n\\n  // create a single instance of the compiler to allow caching\\n  return webpack(myDevConfig);\\n}\\n\\nfunction buildDevelopment(done, devCompiler) {\\n  // run webpack\\n  devCompiler.run(function (err, stats) {\\n    if (err) {\\n      throw new gutil.PluginError(\'webpack:build-dev\', err);\\n    }\\n    gutil.log(\\n      \'[webpack:build-dev]\',\\n      stats.toString({\\n        chunks: false, // dial down the output from webpack (it can be noisy)\\n        colors: true,\\n      }),\\n    );\\n\\n    if (done) {\\n      done();\\n    }\\n  });\\n}\\n\\nfunction bundle(options) {\\n  var devCompiler;\\n\\n  function build(done) {\\n    if (options.shouldWatch) {\\n      buildDevelopment(done, devCompiler);\\n    } else {\\n      buildProduction(done);\\n    }\\n  }\\n\\n  if (options.shouldWatch) {\\n    devCompiler = createDevCompiler();\\n\\n    gulp.watch(\'src/**/*\', function () {\\n      build();\\n    });\\n  }\\n\\n  return new Promise(function (resolve, reject) {\\n    build(function (err) {\\n      if (err) {\\n        reject(err);\\n      } else {\\n        resolve(\'webpack built\');\\n      }\\n    });\\n  });\\n}\\n\\nmodule.exports = {\\n  build: function () {\\n    return bundle({ shouldWatch: false });\\n  },\\n  watch: function () {\\n    return bundle({ shouldWatch: true });\\n  },\\n};\\n```\\n\\nHopefully this is fairly self-explanatory; essentially `buildDevelopment` performs the development build (providing sourcemap support) and `buildProduction` builds for Production (providing minification support). Both are driven by this `webpack.config.js`:\\n\\n```js\\n/* eslint-disable no-var, strict, prefer-arrow-callback */\\n\'use strict\';\\n\\nvar path = require(\'path\');\\n\\nmodule.exports = {\\n  cache: true,\\n  entry: {\\n    // The entry point of our application; the script that imports all other scripts in our SPA\\n    main: \'./src/main.tsx\',\\n\\n    // The packages that are to be included in vendor.js\\n    vendor: [\'babel-polyfill\', \'events\', \'flux\', \'react\'],\\n  },\\n\\n  // Where the output of our compilation ends up\\n  output: {\\n    path: path.resolve(__dirname, \'./dist/scripts\'),\\n    filename: \'[name].js\',\\n    chunkFilename: \'[chunkhash].js\',\\n  },\\n\\n  module: {\\n    loaders: [\\n      {\\n        // The loader that handles ts and tsx files.  These are compiled\\n        // with the ts-loader and the output is then passed through to the\\n        // babel-loader.  The babel-loader uses the es2015 and react presets\\n        // in order that jsx and es6 are processed.\\n        test: /\\\\.ts(x?)$/,\\n        exclude: /node_modules/,\\n        loader: \'babel-loader?presets[]=es2015&presets[]=react!ts-loader\',\\n      },\\n      {\\n        // The loader that handles any js files presented alone.\\n        // It passes these to the babel-loader which (again) uses the es2015\\n        // and react presets.\\n        test: /\\\\.js$/,\\n        exclude: /node_modules/,\\n        loader: \'babel\',\\n        query: {\\n          presets: [\'es2015\', \'react\'],\\n        },\\n      },\\n    ],\\n  },\\n  plugins: [],\\n  resolve: {\\n    // Files with the following extensions are fair game for webpack to process\\n    extensions: [\'\', \'.webpack.js\', \'.web.js\', \'.ts\', \'.tsx\', \'.js\'],\\n  },\\n};\\n```\\n\\n## Inject\\n\\nYour compiled output needs to be referenced from some kind of HTML page. So we\'ve got this:\\n\\n```html\\n<!doctype html>\\n<html lang=\\"en\\">\\n  <head>\\n    <meta charset=\\"utf-8\\" />\\n    <meta http-equiv=\\"X-UA-Compatible\\" content=\\"IE=edge\\" />\\n    <meta name=\\"viewport\\" content=\\"width=device-width, initial-scale=1\\" />\\n\\n    <title>ES6 + Babel + React + Flux + Karma: The Secret Recipe</title>\\n\\n    \x3c!-- inject:css --\x3e\\n    \x3c!-- endinject --\x3e\\n    <link\\n      rel=\\"stylesheet\\"\\n      href=\\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css\\"\\n    />\\n  </head>\\n  <body>\\n    <div id=\\"content\\"></div>\\n    \x3c!-- inject:js --\x3e\\n    \x3c!-- endinject --\x3e\\n  </body>\\n</html>\\n```\\n\\nWhich is no more than a boilerplate HTML page with a couple of key features:\\n\\n- a single `&lt;div /&gt;` element in the `&lt;body /&gt;` which is where our React app is going to be rendered.\\n- `&lt;!-- inject:css --&gt;` and `&lt;!-- inject:js --&gt;` placeholders where css and js is going to be injected by `gulp-inject`.\\n- a single `&lt;link /&gt;` to the Bootstrap CDN. This sample app doesn\'t actually serve up any css generated as part of the project. It could but it doesn\'t. When it comes to injection time no css will actually be injected. This has been left in place as, more typically, a project would have some styling served up.\\n\\nThis is fed into our inject task in `inject.build()` and `inject.watch()`. They take css and javascript and, using our shell template, create a new page which has the css and javascript dropped into their respective placeholders:\\n\\n```js\\n\'use strict\';\\n\\nvar gulp = require(\'gulp\');\\nvar inject = require(\'gulp-inject\');\\nvar glob = require(\'glob\');\\n\\nfunction injectIndex(options) {\\n  var postInjectCb = options.postInjectCb;\\n  var postInjectCbTriggerId = null;\\n  function run() {\\n    var target = gulp.src(\'./src/index.html\');\\n    var sources = gulp.src(\\n      [\\n        //\'./dist/styles/main*.css\',\\n        \'./dist/scripts/vendor*.js\',\\n        \'./dist/scripts/main*.js\',\\n      ],\\n      { read: false },\\n    );\\n\\n    return target\\n      .on(\'end\', function () {\\n        // invoke postInjectCb after 1s\\n        if (postInjectCbTriggerId || !postInjectCb) {\\n          return;\\n        }\\n\\n        postInjectCbTriggerId = setTimeout(function () {\\n          postInjectCb();\\n          postInjectCbTriggerId = null;\\n        }, 1000);\\n      })\\n      .pipe(\\n        inject(sources, {\\n          ignorePath: \'/dist/\',\\n          addRootSlash: false,\\n          removeTags: true,\\n        }),\\n      )\\n      .pipe(gulp.dest(\'./dist\'));\\n  }\\n\\n  var jsCssGlob = \'dist/**/*.{js,css}\';\\n\\n  function checkForInitialFilesThenRun() {\\n    glob(jsCssGlob, function (er, files) {\\n      var filesWeNeed = [\\n        \'dist/scripts/main\',\\n        \'dist/scripts/vendor\' /*, \'dist/styles/main\'*/,\\n      ];\\n\\n      function fileIsPresent(fileWeNeed) {\\n        return files.some(function (file) {\\n          return file.indexOf(fileWeNeed) !== -1;\\n        });\\n      }\\n\\n      if (filesWeNeed.every(fileIsPresent)) {\\n        run(\'initial build\');\\n      } else {\\n        checkForInitialFilesThenRun();\\n      }\\n    });\\n  }\\n\\n  checkForInitialFilesThenRun();\\n\\n  if (options.shouldWatch) {\\n    gulp.watch(jsCssGlob, function (evt) {\\n      if (evt.path && evt.type === \'changed\') {\\n        run(evt.path);\\n      }\\n    });\\n  }\\n}\\n\\nmodule.exports = {\\n  build: function () {\\n    return injectIndex({ shouldWatch: false });\\n  },\\n  watch: function (postInjectCb) {\\n    return injectIndex({ shouldWatch: true, postInjectCb: postInjectCb });\\n  },\\n};\\n```\\n\\nThis also triggers the server to serve up the new content.\\n\\n## Static Files\\n\\nYour app will likely rely on a number of static assets; images, fonts and whatnot. This script picks up the static assets you\'ve defined and places them in the `dist` folder ready for use:\\n\\n```js\\n\'use strict\';\\n\\nvar gulp = require(\'gulp\');\\nvar cache = require(\'gulp-cached\');\\n\\nvar targets = [\\n  // In my own example I don\'t use any of the targets below, they\\n  // are included to give you more of a feel of how you might use this\\n  { description: \'FONTS\', src: \'./fonts/*\', dest: \'./dist/fonts\' },\\n  { description: \'STYLES\', src: \'./styles/*\', dest: \'./dist/styles\' },\\n  { description: \'FAVICON\', src: \'./favicon.ico\', dest: \'./dist\' },\\n  { description: \'IMAGES\', src: \'./images/*\', dest: \'./dist/images\' },\\n];\\n\\nfunction copy(options) {\\n  // Copy files from their source to their destination\\n  function run(target) {\\n    gulp\\n      .src(target.src)\\n      .pipe(cache(target.description))\\n      .pipe(gulp.dest(target.dest));\\n  }\\n\\n  function watch(target) {\\n    gulp.watch(target.src, function () {\\n      run(target);\\n    });\\n  }\\n\\n  targets.forEach(run);\\n\\n  if (options.shouldWatch) {\\n    targets.forEach(watch);\\n  }\\n}\\n\\nmodule.exports = {\\n  build: function () {\\n    return copy({ shouldWatch: false });\\n  },\\n  watch: function () {\\n    return copy({ shouldWatch: true });\\n  },\\n};\\n```\\n\\n## Karma\\n\\nFinally, we\'re ready to get our tests set up to run continually with Karma. `tests.watch()` triggers the following task:\\n\\n```js\\n\'use strict\';\\n\\nvar Server = require(\'karma\').Server;\\nvar path = require(\'path\');\\nvar gutil = require(\'gulp-util\');\\n\\nmodule.exports = {\\n  watch: function () {\\n    // Documentation: https://karma-runner.github.io/0.13/dev/public-api.html\\n    var karmaConfig = {\\n      configFile: path.join(__dirname, \'../karma.conf.js\'),\\n      singleRun: false,\\n\\n      plugins: [\\n        \'karma-webpack\',\\n        \'karma-jasmine\',\\n        \'karma-mocha-reporter\',\\n        \'karma-sourcemap-loader\',\\n        \'karma-phantomjs-launcher\',\\n        \'karma-phantomjs-shim\',\\n      ], // karma-phantomjs-shim only in place until PhantomJS hits 2.0 and has function.bind\\n      reporters: [\'mocha\'],\\n    };\\n\\n    new Server(karmaConfig, karmaCompleted).start();\\n\\n    function karmaCompleted(exitCode) {\\n      gutil.log(\'Karma has exited with:\', exitCode);\\n      process.exit(exitCode);\\n    }\\n  },\\n};\\n```\\n\\nWhen running in watch mode it\'s possible to debug the tests by going to: `http://localhost:9876/`. It\'s also possible to run the tests standalone with a simple `npm run test`. Running them like this also outputs the results to an [XML file in JUnit format](http://stackoverflow.com/q/442556/761388); this can be useful for integrating into CI solutions that don\'t natively pick up test results.\\n\\nWhichever approach we use for running tests, we use the following `karma.conf.js` file to configure Karma:\\n\\n```js\\n/* eslint-disable no-var, strict */\\n\'use strict\';\\n\\nvar webpackConfig = require(\'./webpack.config.js\');\\n\\nmodule.exports = function (config) {\\n  // Documentation: https://karma-runner.github.io/0.13/config/configuration-file.html\\n  config.set({\\n    browsers: [\'PhantomJS\'],\\n\\n    files: [\\n      \'test/import-babel-polyfill.js\', // This ensures we have the es6 shims in place from babel\\n      \'test/**/*.tests.ts\',\\n      \'test/**/*.tests.tsx\',\\n    ],\\n\\n    port: 9876,\\n\\n    frameworks: [\'jasmine\', \'phantomjs-shim\'],\\n\\n    logLevel: config.LOG_INFO, //config.LOG_DEBUG\\n\\n    preprocessors: {\\n      \'test/import-babel-polyfill.js\': [\'webpack\', \'sourcemap\'],\\n      \'src/**/*.{ts,tsx}\': [\'webpack\', \'sourcemap\'],\\n      \'test/**/*.tests.{ts,tsx}\': [\'webpack\', \'sourcemap\'],\\n    },\\n\\n    webpack: {\\n      devtool: \'eval-source-map\', //\'inline-source-map\', - inline-source-map doesn\'t work at present\\n      debug: true,\\n      module: webpackConfig.module,\\n      resolve: webpackConfig.resolve,\\n    },\\n\\n    webpackMiddleware: {\\n      quiet: true,\\n      stats: {\\n        colors: true,\\n      },\\n    },\\n\\n    // reporter options\\n    mochaReporter: {\\n      colors: {\\n        success: \'bgGreen\',\\n        info: \'cyan\',\\n        warning: \'bgBlue\',\\n        error: \'bgRed\',\\n      },\\n    },\\n\\n    junitReporter: {\\n      outputDir: \'test-results\', // results will be saved as $outputDir/$browserName.xml\\n      outputFile: undefined, // if included, results will be saved as $outputDir/$browserName/$outputFile\\n      suite: \'\',\\n    },\\n  });\\n};\\n```\\n\\nAs you can see, we\'re still using our webpack configuration from earlier to configure much of how the transpilation takes place.\\n\\nAnd that\'s it; we have a workflow for developing in TypeScript using React with tests running in an automated fashion. I appreciated this has been a rather long blog post but I hope I\'ve clarified somewhat how this all plugs together and works. Do leave a comment if you think I\'ve missed something.\\n\\n## Babel 5 -> Babel 6\\n\\nThis post has actually been sat waiting to be published for some time. I\'d got this solution up and running with Babel 5. Then they shipped Babel 6 and (as is the way with \\"breaking changes\\") [broke sourcemap support](https://phabricator.babeljs.io/T2864) and thus torpedoed this workflow. Happily that\'s now [been resolved](https://github.com/babel/babel/pull/3108). But if you should experience any wonkiness - it\'s worth checking that you\'re using the latest and greatest of Babel 6."},{"id":"iqueryable-ienumerable-hmmm","metadata":{"permalink":"/iqueryable-ienumerable-hmmm","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2015-11-30-iqueryable-ienumerable-hmmm/index.md","source":"@site/blog/2015-11-30-iqueryable-ienumerable-hmmm/index.md","title":"IQueryable... IEnumerable... Hmmm...","description":"The debate surrounding passing IQueryable<T> as IEnumerable<T> is discussed. Changing the method signature is proposed as a solution.","date":"2015-11-30T00:00:00.000Z","tags":[{"inline":false,"label":"C#","permalink":"/tags/csharp","description":"The C# programming language."}],"readingTime":4.365,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"iqueryable-ienumerable-hmmm","title":"IQueryable... IEnumerable... Hmmm...","authors":"johnnyreilly","tags":["c#"],"hide_table_of_contents":false,"description":"The debate surrounding passing IQueryable<T> as IEnumerable<T> is discussed. Changing the method signature is proposed as a solution."},"unlisted":false,"prevItem":{"title":"ES6 + TypeScript + Babel + React + Flux + Karma: The Secret Recipe","permalink":"/es6-typescript-babel-react-flux-karma"},"nextItem":{"title":"The Names Have Been Changed...","permalink":"/the-names-have-been-changed"}},"content":"So there I was, tip-tapping away at my keyboard when I became aware of the slowly loudening noise of a debate. It wasn\'t about poverty, war, civil rights or anything like that. No; this was far more contentious. It was about the behaviour of `<a href=\\"https://msdn.microsoft.com/en-gb/library/bb351562(v=vs.100).aspx\\">IQueryable&lt;T&gt;</a>` when mixed with `<a href=\\"https://msdn.microsoft.com/en-gb/library/9eekhta0(v=vs.100).aspx\\">IEnumerable&lt;T&gt;</a>`. I know, right, how could I not get involved?\\n\\n\x3c!--truncate--\x3e\\n\\nThe code that was being debated was a database query that was being facilitated by Entity Framework. Now let me ask you a question: what is the problem with the methods below?\\n\\n```cs\\nprivate IEnumerable<Sage> GetSagesWithSayings()\\n{\\n    IQueryable<Sage> sageWithSayings =\\n        from s in DbContext.Sages.Include(x => x.Sayings)\\n        select s;\\n\\n    return sageWithSayings;\\n}\\n\\npublic IEnumerable<Sage> GetSagesWithSayingsBornWithinTheLast100Years()\\n{\\n    var aHundredYearsAgo = DateTime.Now.AddYears(-100);\\n    var sageWithSayings = GetSagesWithSayings().Where(x => x.DateOfBirth > aHundredYearsAgo);\\n\\n    return sageWithSayings;\\n}\\n```\\n\\nI\'ve rather emphasised the problem by expressly declaring types in the `GetSagesWithSayings` method. More typically the `IQueryable&lt;Sage&gt;` would be hiding itself beneath a `var` making the problem less obvious. But you get the point; it\'s something to do with an `IQueryable&lt;Sage&gt;` being passed back as an `IEnumerable&lt;Sage&gt;`.\\n\\nThe debate was raging around what this piece of code (or one much like it) actually did. One side positing \\"it\'ll get every record from the database and then throw away what it doesn\'t need in C#-land...\\" The opposing view being \\"are you sure about that? Doesn\'t it just get the records from the last hundred years from the database?\\"\\n\\nSo it comes down the SQL that ends up being generated. On the one hand it\'s going to get everything from the Sages table...\\n\\n```sql\\nselect ...\\nfrom Sages ...\\n```\\n\\nOr does it include a filter clause as well?\\n\\n```sql\\nselect ...\\nfrom Sages ...\\nwhere DateOfBirth > \'1915-11-30\'\\n```\\n\\nYou probably know the answer... It gets everything. Every record is brought back from the database and those that are older than 100 years are then casually thrown away. So kinda wasteful. That\'s the problem. But why? And what does that tell us?\\n\\n## LINQ to Objects vs LINQ to ... ?\\n\\n> The term \\"LINQ to Objects\\" refers to the use of LINQ queries with any `IEnumerable` or `IEnumerable&lt;T&gt;` collection directly, without the use of an intermediate LINQ provider or API such as LINQ to SQL or LINQ to XML.\\n\\n> The `<a href=\\"https://msdn.microsoft.com/en-gb/library/bb351562(v=vs.100).aspx\\">IQueryable&lt;T&gt;</a>` interface is intended for implementation by query providers.\\n>\\n> This interface inherits the `<a href=\\"https://msdn.microsoft.com/en-gb/library/9eekhta0(v=vs.100).aspx\\">IEnumerable&lt;T&gt;</a>` interface so that if it represents a query, the results of that query can be enumerated. Enumeration forces the expression tree associated with an `<a href=\\"https://msdn.microsoft.com/en-gb/library/bb351562(v=vs.100).aspx\\">IQueryable&lt;T&gt;</a>` object to be executed. Queries that do not return enumerable results are executed when the `<a href=\\"https://msdn.microsoft.com/en-gb/library/bb549414(v=vs.100).aspx\\">Execute&lt;TResult&gt;(Expression)</a>` method is called.\\n>\\n> The definition of \\"executing an expression tree\\" is specific to a query provider. For example, it may involve translating the expression tree to a query language appropriate for an underlying data source.\\n\\nI know - check me out with my \\"quotes\\".\\n\\nNow, `IEnumerable` and `IQueryable` are similar; for instance they are both considered \\"lazy\\" as they offer deferred execution. But there is an important difference between `IEnumerable` and `IQueryable`; namely that `IQueryable` hands off information about a query to another provider in order that they may decide how to do the necessary work. `IEnumerable` does not; its work is done in memory by operating on the data it has.\\n\\nSo let\'s apply this to our issue. We have an `IQueryable&lt;Sage&gt;` and we return it as an `IEnumerable&lt;Sage&gt;`. By doing this we haven\'t changed the underlying type; it\'s still an `IQueryable&lt;Sage&gt;`. But by upcasting to `IEnumerable&lt;Sage&gt;` we have told the compiler that we don\'t have an `IQueryable&lt;Sage&gt;`. We\'ve lied. I trust you\'re feeling guilty.\\n\\nNo doubt whoever raised you told you not to tell lies. This was probably the very situation they had in mind. The implications of our dirty little fib come back to haunt us when we start to chain on subsequent filters. So when we perform our filter of `.Where(x =&gt; x.DateOfBirth &gt; aHundredYearsAgo)` the compiler isn\'t going to get LINQ to Entities\'s extension methods in on this. No, it\'s going to get the LINQ to object extension methods instead.\\n\\nThis is the cause of our problem. When it comes to execution we\'re not getting the database to do the heavy lifting because we\'ve moved away from using `IQueryable`.\\n\\n## Fixing the Problem\\n\\nThere are 2 courses of action open to you. The obvious course of action (and 99% of the time what you\'d look to do) is change the signature of the `` method to return an IQueryable like so:\\n\\n```cs\\nprivate IQueryable<Sage> GetSagesWithSayings()\\n    var sageWithSayings = // I prefer \'var\', don\'t you?\\n        from s in DbContext.Sages.Include(x => x.Sayings)\\n        select s;\\n\\n    return sageWithSayings;\\n}\\n```\\n\\nThe other alternative is what I like to think of as \\"the escape hatch\\": `<a href=\\"https://msdn.microsoft.com/en-gb/library/bb353734(v=vs.100).aspx\\">AsQueryable</a>`. This takes an `IEnumerable`, checks if it\'s actually an `IQueryable` slumming it and casts back to that if it is. You might use this in a situation where you didn\'t have control over the data access code. Using it looks like this: (and would work whether `GetSagesWithSayings` was returning `IEnumerable`_or_`IQueryable`)\\n\\n```cs\\npublic IEnumerable<Sage> GetSagesWithSayingsBornWithinTheLast100Years()\\n{\\n    var aHundredYearsAgo = DateTime.Now.AddYears(-100);\\n    var sageWithSayings =GetSagesWithSayings().AsQueryable().Where(x => x.DateOfBirth > aHundredYearsAgo);\\n\\n    return sageWithSayings;\\n}\\n```"},{"id":"the-names-have-been-changed","metadata":{"permalink":"/the-names-have-been-changed","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2015-10-23-the-names-have-been-changed/index.md","source":"@site/blog/2015-10-23-the-names-have-been-changed/index.md","title":"The Names Have Been Changed...","description":"John changes the domain name of his blog from .io to .com to save money and has set up a redirect from old site to new one.","date":"2015-10-23T00:00:00.000Z","tags":[],"readingTime":0.75,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"the-names-have-been-changed","title":"The Names Have Been Changed...","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false,"description":"John changes the domain name of his blog from .io to .com to save money and has set up a redirect from old site to new one."},"unlisted":false,"prevItem":{"title":"IQueryable... IEnumerable... Hmmm...","permalink":"/iqueryable-ienumerable-hmmm"},"nextItem":{"title":"jQuery Validation Globalize hits 1.0","permalink":"/jquery-validation-globalize-hits-10"}},"content":"...to protect my wallet.\\n\\n\x3c!--truncate--\x3e\\n\\nSubsequent to this blog getting [a proper domain name a year ago](../2014-12-05-whats-in-a-name/index.md) it\'s now got a new one. That\'s right, `blog.icanmakethiswork.io` is dead! Long live `blog.johnnyreilly.com`!\\n\\nThere\'s nothing particularly exciting about this, it\'s more that `.io` domain names are _wayyyyy_ expensive. And also I noticed that johnnyreilly.com was available. By an accident of history I\'ve ended up either being johnny_reilly or johnnyreilly online. (\\"johnreilly@hotmail.com\\" was already taken back in 2000 and \\"johnny\\\\_reilly@hotmail.com\\" was available. I\'ve subsequently become [@johnny_reilly](https://twitter.com/johnny_reilly) on Twitter, [johnnyreilly](https://github.com/johnnyreilly) on GitHub so I guess you could say it\'s stuck.)\\n\\nSo I thought I\'d kill 2 birds with one stone and make the switch. I\'ve set up a redirect on [blog.icanmakethiswork.io](http://blog.icanmakethiswork.io) and so, anyone who goes to the old site should be 301\'d over here. At least until my old domain name expires. Last time it\'ll change I promise. Well.... until next time anyway..."},{"id":"jquery-validation-globalize-hits-10","metadata":{"permalink":"/jquery-validation-globalize-hits-10","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2015-10-05-jquery-validation-globalize-hits-10/index.md","source":"@site/blog/2015-10-05-jquery-validation-globalize-hits-10/index.md","title":"jQuery Validation Globalize hits 1.0","description":"jQuery Validation Globalize plugin now supports Globalize 1.x, with minor code changes. Users can customize date parsing format.","date":"2015-10-05T00:00:00.000Z","tags":[{"inline":false,"label":"Globalize","permalink":"/tags/globalize","description":"The Globalize library."},{"inline":false,"label":"jQuery","permalink":"/tags/jquery","description":"The jQuery library."}],"readingTime":2.87,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"jquery-validation-globalize-hits-10","title":"jQuery Validation Globalize hits 1.0","authors":"johnnyreilly","tags":["globalize","jquery"],"hide_table_of_contents":false,"description":"jQuery Validation Globalize plugin now supports Globalize 1.x, with minor code changes. Users can customize date parsing format."},"unlisted":false,"prevItem":{"title":"The Names Have Been Changed...","permalink":"/the-names-have-been-changed"},"nextItem":{"title":"Definitely Typed Shouldn\'t Exist","permalink":"/authoring-npm-modules-with-typescript"}},"content":"This is just a quick post - the tl;dr is this: jQuery Validation Globalize has been ported to Globalize 1.x. Yay! In one of those twists of fate I\'m not actually using this plugin in my day job anymore but I thought it might be useful to other people. So here you go. You can read more about this plugin in an [older post](../2012-09-06-globalize-and-jquery-validate/index.md) and you can see a demo of it in action [here](http://johnnyreilly.github.io/jQuery.Validation.Unobtrusive.Native/AdvancedDemo/Globalize.html).\\n\\n\x3c!--truncate--\x3e\\n\\nThe code did not change drastically - essentially it was just a question of swapping `parseFloat` for `parseNumber` and `parseDate` for a slightly different `parseDate`. So, we went from this:\\n\\n```js\\n(function ($, Globalize) {\\n  // Clone original methods we want to call into\\n  var originalMethods = {\\n    min: $.validator.methods.min,\\n    max: $.validator.methods.max,\\n    range: $.validator.methods.range,\\n  };\\n\\n  // Tell the validator that we want numbers parsed using Globalize\\n\\n  $.validator.methods.number = function (value, element) {\\n    var val = Globalize.parseFloat(value);\\n    return this.optional(element) || $.isNumeric(val);\\n  };\\n\\n  // Tell the validator that we want dates parsed using Globalize\\n\\n  $.validator.methods.date = function (value, element) {\\n    var val = Globalize.parseDate(value);\\n    return this.optional(element) || val instanceof Date;\\n  };\\n\\n  // Tell the validator that we want numbers parsed using Globalize,\\n  // then call into original implementation with parsed value\\n\\n  $.validator.methods.min = function (value, element, param) {\\n    var val = Globalize.parseFloat(value);\\n    return originalMethods.min.call(this, val, element, param);\\n  };\\n\\n  $.validator.methods.max = function (value, element, param) {\\n    var val = Globalize.parseFloat(value);\\n    return originalMethods.max.call(this, val, element, param);\\n  };\\n\\n  $.validator.methods.range = function (value, element, param) {\\n    var val = Globalize.parseFloat(value);\\n    return originalMethods.range.call(this, val, element, param);\\n  };\\n})(jQuery, Globalize);\\n```\\n\\nTo this:\\n\\n```js\\n(function ($, Globalize) {\\n  // Clone original methods we want to call into\\n  var originalMethods = {\\n    min: $.validator.methods.min,\\n    max: $.validator.methods.max,\\n    range: $.validator.methods.range,\\n  };\\n\\n  // Globalize options - initially just the date format used for parsing\\n  // Users can customise this to suit them\\n  $.validator.methods.dateGlobalizeOptions = {\\n    dateParseFormat: { skeleton: \'yMd\' },\\n  };\\n\\n  // Tell the validator that we want numbers parsed using Globalize\\n  $.validator.methods.number = function (value, element) {\\n    var val = Globalize.parseNumber(value);\\n    return this.optional(element) || $.isNumeric(val);\\n  };\\n\\n  // Tell the validator that we want dates parsed using Globalize\\n  $.validator.methods.date = function (value, element) {\\n    var val = Globalize.parseDate(\\n      value,\\n      $.validator.methods.dateGlobalizeOptions.dateParseFormat,\\n    );\\n    return this.optional(element) || val instanceof Date;\\n  };\\n\\n  // Tell the validator that we want numbers parsed using Globalize,\\n  // then call into original implementation with parsed value\\n\\n  $.validator.methods.min = function (value, element, param) {\\n    var val = Globalize.parseNumber(value);\\n    return originalMethods.min.call(this, val, element, param);\\n  };\\n\\n  $.validator.methods.max = function (value, element, param) {\\n    var val = Globalize.parseNumber(value);\\n    return originalMethods.max.call(this, val, element, param);\\n  };\\n\\n  $.validator.methods.range = function (value, element, param) {\\n    var val = Globalize.parseNumber(value);\\n    return originalMethods.range.call(this, val, element, param);\\n  };\\n})(jQuery, Globalize);\\n```\\n\\nAll of which is pretty self-explanatory. The only thing I\'d like to draw out is that Globalize 0.1.x didn\'t force you to specify a date parsing format and, as I recall, would attempt various methods of parsing. For that reason jQuery Validation Globalize 1.0 exposes a `$.validator.methods.dateGlobalizeOptions` which allows you to specify the data parsing format you want to use. This means, should you be using a different format than the out of the box one then you can tweak it like so:\\n\\n```js\\n$.validator.methods.dateGlobalizeOptions.dateParseFormat = // your data parsing format goes here...\\n```\\n\\nTheoretically, this functionality could be tweaked to allow the user to specify multiple possible date parsing formats to attempt. I\'m not certain if that\'s a good idea though, so it remains unimplemented for now."},{"id":"authoring-npm-modules-with-typescript","metadata":{"permalink":"/authoring-npm-modules-with-typescript","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2015-09-23-authoring-npm-modules-with-typescript/index.md","source":"@site/blog/2015-09-23-authoring-npm-modules-with-typescript/index.md","title":"Definitely Typed Shouldn\'t Exist","description":"Using TypeScript definition files with npm packages can produce accurate typing information. Making npm a first class citizen may replace Definitely Typed.","date":"2015-09-23T00:00:00.000Z","tags":[{"inline":false,"label":"Node.js","permalink":"/tags/node-js","description":"The Node.js runtime."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":10.775,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"authoring-npm-modules-with-typescript","title":"Definitely Typed Shouldn\'t Exist","authors":"johnnyreilly","tags":["node.js","typescript"],"hide_table_of_contents":false,"description":"Using TypeScript definition files with npm packages can produce accurate typing information. Making npm a first class citizen may replace Definitely Typed."},"unlisted":false,"prevItem":{"title":"jQuery Validation Globalize hits 1.0","permalink":"/jquery-validation-globalize-hits-10"},"nextItem":{"title":"Things Done Changed","permalink":"/things-done-changed"}},"content":"I\'m a member of the Definitely Typed team - and hopefully I won\'t be kicked out for writing this. My point is this: `.d.ts` files should live with the package they provide typing information for, in npm / GitHub etc. Not separately.\\n\\n\x3c!--truncate--\x3e\\n\\nTypeScript 1.6 has just been released. Yay! In the [release blog post](https://blogs.msdn.com/b/typescript/archive/2015/09/16/announcing-typescript-1-6.aspx) it says this:\\n\\n> We\u2019ve changed module resolution when doing CommonJS output to work more closely to how Node does module resolution. If a module name is non-relative, we now follow these steps to find the associated typings:\\n>\\n> 1. Check in `node_modules` for `MODULE_NAME.d.ts`\\n> 2. Search `node_modules\\\\MODULE_NAME\\\\package.json` for a `typings` field\\n> 3. Look for `node_modules\\\\MODULE_NAME\\\\index.d.ts`\\n> 4. Then we go one level higher and repeat the process\\n>\\n> **Please note:** when we search through node_modules, we assume these are the packaged node modules which have type information and a corresponding `.js` file. As such, we resolve only `.d.ts` files (not `.ts` file) for non-relative names.\\n>\\n> Previously, we treated all module names as relative paths, and therefore we would never properly look in node_modules... We will continue to improve module resolution, including improvements to AMD, in upcoming releases.\\n\\nThe TL;DR is this: consuming npm packages which come with definition files should JUST WORK\u2122... npm is now a first class citizen in TypeScriptLand. So everyone who has a package on npm should now feel duty bound to include a `.d.ts` when they publish and Definitely Typed can shut up shop. Simple right?\\n\\n## Wrong!\\n\\nYeah, it\'s never going to happen. Surprising as it is, there are many people who are quite happy without TypeScript in their lives (I know - mad right?). These poor unfortunates are unlikely to ever take the extra steps necessary to write definition files. For this reason, there will probably _always_ be a need for a provider of typings such as Definitely Typed. As well as that, the vast majority of people using TypeScript probably don\'t use npm to manage dependencies. There are, however, an increasing number of users who are using npm. Some (like me) may even be using tools like [Browserify](http://browserify.org/) (with the [TSIFY plugin](https://github.com/smrq/tsify)) or [webpack](https://webpack.github.io/) (with the [ts-loader](https://github.com/TypeStrong/ts-loader)) to bring it all together. My feeling is that, over time, using npm will become more common; particularly given the improvements being made to module resolution in the language.\\n\\nAn advantage of shipping typings with an npm package is this: those typings should accurately describe their accompanying package. In Definitely Typed we only aim to support the latest and greatest typings. So if you find yourself looking for the typings of an older version of a package you\'re going to have to pick your way back through the history of a `.d.ts` file and hope you happen upon the version you\'re looking for. Not a fantastic experience.\\n\\nSo I guess what I\'m saying is this: if you\'re an npm package author then it would be fantastic to start shipping a package with typings in the box. If you\'re using npm to consume packages then using Definitely Typed ought to be the second step you might take after installing a package; the step you only need to take if the package doesn\'t come with typings. Using DT should be a fallback, not a default.\\n\\n## Authoring npm modules with TypeScript\\n\\nYup - that\'s what this post is actually about. See how I lured you in with my mild trolling and pulled the old switcheroo? That\'s edutainment my friend. So, how do we write npm packages in TypeScript and publish them with their typings? Apparently Gandhi [didn\'t actually say](http://www.nytimes.com/2011/08/30/opinion/falser-words-were-never-spoken.html?_r=0) \\"Be the change you wish to see in the world.\\" Which is a shame. But anyway, I\'m going to try and embrace the sentiment here.\\n\\nNot so long ago I wrote a small npm module called [globalize-so-what-cha-want](https://www.npmjs.com/package/globalize-so-what-cha-want). It is used to determine what parts of Globalize 1.x you need depending on the modules you\'re planning to use. It also, contains a little demo UI / online tool written in React which powers [this](http://johnnyreilly.github.io/globalize-so-what-cha-want/).\\n\\nFor this post, the purpose of the package is rather irrelevant. And even though I\'ve just told you about it, I want you to pretend that the online tool doesn\'t exist. Pretend I never mentioned it.\\n\\nWhat is relevant, and what I want you to think about, is this: I wrote globalize-so-what-cha-want in plain old, honest to goodness JavaScript. Old school.\\n\\n[But, my love of static typing could be held in abeyance for only so long.](https://www.youtube.com/watch?v=V4YPFHyGWaY&feature=youtu.be&t=49s) Once the initial package was written, unit tested and published I got the itch. THIS SHOULD BE WRITTEN IN TYPESCRIPT!!! Well, it didn\'t have to be but I wanted it to be. Despite having used TypeScript since the early days I\'d only been using it for front end work; not for writing npm packages. My mission was clear: port globalize-so-what-cha-want to TypeScript and re-publish to npm.\\n\\n## Port, port, port!!!\\n\\nAt this point globalize-so-what-cha-want consisted of a single `index.js` file in the root of the package. My end goal was to end up with that file still sat there, but now generated from TypeScript. Alongside it I wanted to see a `index.d.ts` which was generated from the same TypeScript.\\n\\n[`index.js` before](https://github.com/johnnyreilly/globalize-so-what-cha-want/tree/6cce84289134a555fe8462247b43eddb051303e3) looked like this:\\n\\n```js\\n/* jshint varstmt: false, esnext: false */\\nvar DEPENDENCY_TYPES = {\\n  SHARED_JSON: \'Shared JSON (used by all locales)\',\\n  LOCALE_JSON: \'Locale specific JSON (supplied for each locale)\',\\n};\\n\\nvar moduleDependencies = {\\n  core: {\\n    dependsUpon: [],\\n    cldrGlobalizeFiles: [\\n      \'cldr.js\',\\n      \'cldr/event.js\',\\n      \'cldr/supplemental.js\',\\n      \'globalize.js\',\\n    ],\\n    json: [\\n      {\\n        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,\\n        dependency: \'cldr/supplemental/likelySubtags.json\',\\n      },\\n    ],\\n  },\\n\\n  currency: {\\n    dependsUpon: [\'number\', \'plural\'],\\n    cldrGlobalizeFiles: [\'globalize/currency.js\'],\\n    json: [\\n      {\\n        dependencyType: DEPENDENCY_TYPES.LOCALE_JSON,\\n        dependency: \'cldr/main/{locale}/currencies.json\',\\n      },\\n      {\\n        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,\\n        dependency: \'cldr/supplemental/currencyData.json\',\\n      },\\n    ],\\n  },\\n\\n  date: {\\n    dependsUpon: [\'number\'],\\n    cldrGlobalizeFiles: [\'globalize/date.js\'],\\n    json: [\\n      {\\n        dependencyType: DEPENDENCY_TYPES.LOCALE_JSON,\\n        dependency: \'cldr/main/{locale}/ca-gregorian.json\',\\n      },\\n      {\\n        dependencyType: DEPENDENCY_TYPES.LOCALE_JSON,\\n        dependency: \'cldr/main/{locale}/timeZoneNames.json\',\\n      },\\n      {\\n        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,\\n        dependency: \'cldr/supplemental/timeData.json\',\\n      },\\n      {\\n        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,\\n        dependency: \'cldr/supplemental/weekData.json\',\\n      },\\n    ],\\n  },\\n\\n  message: {\\n    dependsUpon: [\'core\'],\\n    cldrGlobalizeFiles: [\'globalize/message.js\'],\\n    json: [],\\n  },\\n\\n  number: {\\n    dependsUpon: [\'core\'],\\n    cldrGlobalizeFiles: [\'globalize/number.js\'],\\n    json: [\\n      {\\n        dependencyType: DEPENDENCY_TYPES.LOCALE_JSON,\\n        dependency: \'cldr/main/{locale}/numbers.json\',\\n      },\\n      {\\n        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,\\n        dependency: \'cldr/supplemental/numberingSystems.json\',\\n      },\\n    ],\\n  },\\n\\n  plural: {\\n    dependsUpon: [\'core\'],\\n    cldrGlobalizeFiles: [\'globalize/plural.js\'],\\n    json: [\\n      {\\n        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,\\n        dependency: \'cldr/supplemental/plurals.json\',\\n      },\\n      {\\n        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,\\n        dependency: \'cldr/supplemental/ordinals.json\',\\n      },\\n    ],\\n  },\\n\\n  relativeTime: {\\n    dependsUpon: [\'number\', \'plural\'],\\n    cldrGlobalizeFiles: [\'globalize/relative-time.js\'],\\n    json: [\\n      {\\n        dependencyType: DEPENDENCY_TYPES.LOCALE_JSON,\\n        dependency: \'cldr/main/{locale}/dateFields.json\',\\n      },\\n    ],\\n  },\\n};\\n\\nfunction determineRequiredCldrData(globalizeOptions) {\\n  return determineRequired(\\n    globalizeOptions,\\n    _populateDependencyCurrier(\'json\', function (json) {\\n      return json.dependency;\\n    }),\\n  );\\n}\\n\\nfunction determineRequiredCldrGlobalizeFiles(globalizeOptions) {\\n  return determineRequired(\\n    globalizeOptions,\\n    _populateDependencyCurrier(\\n      \'cldrGlobalizeFiles\',\\n      function (cldrGlobalizeFile) {\\n        return cldrGlobalizeFile;\\n      },\\n    ),\\n  );\\n}\\n\\nfunction determineRequired(globalizeOptions, populateDependencies) {\\n  var modules = Object.keys(globalizeOptions);\\n  modules.forEach(function (module) {\\n    if (!moduleDependencies[module]) {\\n      throw new TypeError(\\"There is no \'\\" + module + \\"\' module\\");\\n    }\\n  });\\n\\n  var requireds = [];\\n  modules.forEach(function (module) {\\n    if (globalizeOptions[module]) {\\n      populateDependencies(module, requireds);\\n    }\\n  });\\n\\n  return requireds;\\n}\\n\\nfunction _populateDependencyCurrier(requiredArray, requiredArrayGetter) {\\n  var popDepFn = function (module, requireds) {\\n    var dependencies = moduleDependencies[module];\\n\\n    dependencies.dependsUpon.forEach(function (requiredModule) {\\n      popDepFn(requiredModule, requireds);\\n    });\\n\\n    dependencies[requiredArray].forEach(function (required) {\\n      var newRequired = requiredArrayGetter(required);\\n      if (requireds.indexOf(newRequired) === -1) {\\n        requireds.push(newRequired);\\n      }\\n    });\\n\\n    return requireds;\\n  };\\n\\n  return popDepFn;\\n}\\n\\nmodule.exports = {\\n  determineRequiredCldrData: determineRequiredCldrData,\\n  determineRequiredCldrGlobalizeFiles: determineRequiredCldrGlobalizeFiles,\\n};\\n```\\n\\nYou can even kind of tell that it was written in JavaScript thanks to the jshint rules at the top.\\n\\nI fired up Atom and created a new folder `src/lib` and inside there I created `index.ts` (yes, `index.js` renamed) and `tsconfig.json`. By the way, you\'ll notice I\'m not leaving Atom - I\'m making use of the magnificent [atom-typescript](https://atom.io/packages/atom-typescript) which you should totally be using too. It rocks.\\n\\n![](Screenshot-2015-09-23-05.51.14.webp)\\n\\nNow I\'m not going to bore you with what I had to do to port the JS to TS (not much). If you\'re interested, the source is [here](https://github.com/johnnyreilly/globalize-so-what-cha-want/blob/master/src/lib/index.ts). What\'s more interesting is the `tsconfig.json` \\\\- as it\'s this that is going to lead the generation of the JS and TS that we need:\\n\\n```json\\n{\\n  \\"compileOnSave\\": true,\\n  \\"compilerOptions\\": {\\n    \\"module\\": \\"commonjs\\",\\n    \\"declaration\\": true,\\n    \\"target\\": \\"es5\\",\\n    \\"noImplicitAny\\": true,\\n    \\"suppressImplicitAnyIndexErrors\\": true,\\n    \\"removeComments\\": false,\\n    \\"preserveConstEnums\\": true,\\n    \\"sourceMap\\": false,\\n    \\"outDir\\": \\"../../\\"\\n  },\\n  \\"files\\": [\\"index.ts\\"]\\n}\\n```\\n\\nThe things to notice are:\\n\\n<dl><dt>module</dt><dd>Publishing a commonjs module means it will play well with npm</dd><dt>declaration</dt><dd>This is what makes TypeScript generate <code>index.d.ts</code></dd><dt>outDir</dt><dd>We want to regenerate the <code>index.js</code> in the root (2 directories above this)</dd></dl>\\n\\nSo now, what do we get when we build in Atom? Well, we\'re generating an [`index.js`](https://github.com/johnnyreilly/globalize-so-what-cha-want/blob/master/index.js) file which looks like this:\\n\\n```js\\nvar DEPENDENCY_TYPES = {\\n  SHARED_JSON: \'Shared JSON (used by all locales)\',\\n  LOCALE_JSON: \'Locale specific JSON (supplied for each locale)\',\\n};\\nvar moduleDependencies = {\\n  core: {\\n    dependsUpon: [],\\n    cldrGlobalizeFiles: [\\n      \'cldr.js\',\\n      \'cldr/event.js\',\\n      \'cldr/supplemental.js\',\\n      \'globalize.js\',\\n    ],\\n    json: [\\n      {\\n        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,\\n        dependency: \'cldr/supplemental/likelySubtags.json\',\\n      },\\n    ],\\n  },\\n  currency: {\\n    dependsUpon: [\'number\', \'plural\'],\\n    cldrGlobalizeFiles: [\'globalize/currency.js\'],\\n    json: [\\n      {\\n        dependencyType: DEPENDENCY_TYPES.LOCALE_JSON,\\n        dependency: \'cldr/main/{locale}/currencies.json\',\\n      },\\n      {\\n        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,\\n        dependency: \'cldr/supplemental/currencyData.json\',\\n      },\\n    ],\\n  },\\n  date: {\\n    dependsUpon: [\'number\'],\\n    cldrGlobalizeFiles: [\'globalize/date.js\'],\\n    json: [\\n      {\\n        dependencyType: DEPENDENCY_TYPES.LOCALE_JSON,\\n        dependency: \'cldr/main/{locale}/ca-gregorian.json\',\\n      },\\n      {\\n        dependencyType: DEPENDENCY_TYPES.LOCALE_JSON,\\n        dependency: \'cldr/main/{locale}/timeZoneNames.json\',\\n      },\\n      {\\n        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,\\n        dependency: \'cldr/supplemental/timeData.json\',\\n      },\\n      {\\n        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,\\n        dependency: \'cldr/supplemental/weekData.json\',\\n      },\\n    ],\\n  },\\n  message: {\\n    dependsUpon: [\'core\'],\\n    cldrGlobalizeFiles: [\'globalize/message.js\'],\\n    json: [],\\n  },\\n  number: {\\n    dependsUpon: [\'core\'],\\n    cldrGlobalizeFiles: [\'globalize/number.js\'],\\n    json: [\\n      {\\n        dependencyType: DEPENDENCY_TYPES.LOCALE_JSON,\\n        dependency: \'cldr/main/{locale}/numbers.json\',\\n      },\\n      {\\n        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,\\n        dependency: \'cldr/supplemental/numberingSystems.json\',\\n      },\\n    ],\\n  },\\n  plural: {\\n    dependsUpon: [\'core\'],\\n    cldrGlobalizeFiles: [\'globalize/plural.js\'],\\n    json: [\\n      {\\n        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,\\n        dependency: \'cldr/supplemental/plurals.json\',\\n      },\\n      {\\n        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,\\n        dependency: \'cldr/supplemental/ordinals.json\',\\n      },\\n    ],\\n  },\\n  relativeTime: {\\n    dependsUpon: [\'number\', \'plural\'],\\n    cldrGlobalizeFiles: [\'globalize/relative-time.js\'],\\n    json: [\\n      {\\n        dependencyType: DEPENDENCY_TYPES.LOCALE_JSON,\\n        dependency: \'cldr/main/{locale}/dateFields.json\',\\n      },\\n    ],\\n  },\\n};\\nfunction determineRequired(globalizeOptions, populateDependencies) {\\n  var modules = Object.keys(globalizeOptions);\\n  modules.forEach(function (module) {\\n    if (!moduleDependencies[module]) {\\n      throw new TypeError(\\"There is no \'\\" + module + \\"\' module\\");\\n    }\\n  });\\n  var requireds = [];\\n  modules.forEach(function (module) {\\n    if (globalizeOptions[module]) {\\n      populateDependencies(module, requireds);\\n    }\\n  });\\n  return requireds;\\n}\\nfunction _populateDependencyCurrier(requiredArray, requiredArrayGetter) {\\n  var popDepFn = function (module, requireds) {\\n    var dependencies = moduleDependencies[module];\\n    dependencies.dependsUpon.forEach(function (requiredModule) {\\n      popDepFn(requiredModule, requireds);\\n    });\\n    dependencies[requiredArray].forEach(function (required) {\\n      var newRequired = requiredArrayGetter(required);\\n      if (requireds.indexOf(newRequired) === -1) {\\n        requireds.push(newRequired);\\n      }\\n    });\\n    return requireds;\\n  };\\n  return popDepFn;\\n}\\n/**\\n * The string array returned will contain a list of the required cldr json data you need. I don\'t believe ordering matters for the json but it is listed in the same dependency order as the cldr / globalize files are.\\n *\\n * @param options The globalize modules being used.\\n */\\nfunction determineRequiredCldrData(globalizeOptions) {\\n  return determineRequired(\\n    globalizeOptions,\\n    _populateDependencyCurrier(\'json\', function (json) {\\n      return json.dependency;\\n    }),\\n  );\\n}\\nexports.determineRequiredCldrData = determineRequiredCldrData;\\n/**\\n * The string array returned will contain a list of the required cldr / globalize files you need, listed in the order they are required.\\n *\\n * @param options The globalize modules being used.\\n */\\nfunction determineRequiredCldrGlobalizeFiles(globalizeOptions) {\\n  return determineRequired(\\n    globalizeOptions,\\n    _populateDependencyCurrier(\\n      \'cldrGlobalizeFiles\',\\n      function (cldrGlobalizeFile) {\\n        return cldrGlobalizeFile;\\n      },\\n    ),\\n  );\\n}\\nexports.determineRequiredCldrGlobalizeFiles =\\n  determineRequiredCldrGlobalizeFiles;\\n```\\n\\nAside from one method moving internally and me adding some JSDoc, the only really notable change is the end of the file. TypeScript, when generating commonjs, doesn\'t use the `module.exports = {}` approach. Rather, it drops exported functions onto the `exports` object as functions are exported. Functionally this is _identical_.\\n\\nNow for our big finish: happily sat alongside is `index.js` is the [`index.d.ts`](https://github.com/johnnyreilly/globalize-so-what-cha-want/blob/master/index.d.ts) file:\\n\\n```ts\\nexport interface Options {\\n  currency?: boolean;\\n  date?: boolean;\\n  message?: boolean;\\n  number?: boolean;\\n  plural?: boolean;\\n  relativeTime?: boolean;\\n}\\n/**\\n * The string array returned will contain a list of the required cldr json data you need. I don\'t believe ordering matters for the json but it is listed in the same dependency order as the cldr / globalize files are.\\n *\\n * @param options The globalize modules being used.\\n */\\nexport declare function determineRequiredCldrData(\\n  globalizeOptions: Options,\\n): string[];\\n/**\\n * The string array returned will contain a list of the required cldr / globalize files you need, listed in the order they are required.\\n *\\n * @param options The globalize modules being used.\\n */\\nexport declare function determineRequiredCldrGlobalizeFiles(\\n  globalizeOptions: Options,\\n): string[];\\n```\\n\\nWe\'re there, huzzah! This has been now published to npm - anyone consuming this package can use TypeScript straight out of the box. I really hope that publishing npm packages in this fashion becomes much more commonplace. Time will tell.\\n\\n## PS I\'m not the only one\\n\\nI was just about to hit \\"publish\\" when I happened upon [Basarat](https://twitter.com/basarat)\'s [ts-npm-module](https://github.com/basarat/ts-npm-module) which is a project on GitHub which demo\'s how to publish and consume TypeScript using npm. I\'d say great minds think alike but I\'m pretty sure Basarat\'s mind is far greater than mine! (Cough, atom-typescript, cough.) Either way, it\'s good to see validation for the approach I\'m suggesting.\\n\\n## PPS Update 23/09/2015 09:51\\n\\nOne of the useful things about writing a blog is that you get to learn. Since I published I\'ve become aware of a few things somewhat relevant to this post. First of all, there is still work ongoing in TypeScript land around this topic. Essentially there are problems resolving dependency conflicts when different dependencies have different versions - you can take part in the ongoing discussion [here](https://github.com/Microsoft/TypeScript/issues/4665). There\'s also some useful resources to look at:\\n\\n- [https://github.com/Microsoft/TypeScript/wiki/Typings-for-npm-packages](https://github.com/Microsoft/TypeScript/wiki/Typings-for-npm-packages)\\n- [https://basarat.gitbooks.io/typescript/content/docs/node/nodejs.html](https://basarat.gitbooks.io/typescript/content/docs/node/nodejs.html)"},{"id":"things-done-changed","metadata":{"permalink":"/things-done-changed","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2015-09-10-things-done-changed/index.md","source":"@site/blog/2015-09-10-things-done-changed/index.md","title":"Things Done Changed","description":"Embracing change is key to being a developer; John discusses some of the tools that have taken his fancy, including React and ES6.","date":"2015-09-10T00:00:00.000Z","tags":[{"inline":false,"label":"React","permalink":"/tags/react","description":"The React library."},{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."}],"readingTime":10.13,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"things-done-changed","title":"Things Done Changed","authors":"johnnyreilly","tags":["react","javascript"],"hide_table_of_contents":false,"description":"Embracing change is key to being a developer; John discusses some of the tools that have taken his fancy, including React and ES6."},"unlisted":false,"prevItem":{"title":"Definitely Typed Shouldn\'t Exist","permalink":"/authoring-npm-modules-with-typescript"},"nextItem":{"title":"(Top One, Nice One) Get Sorted","permalink":"/top-one-nice-one-get-sorted"}},"content":"Some people fear change. Most people actually. I\'m not immune to that myself, but not in the key area of technology. Any developer that fears change when it comes to the tools and languages that he / she is using is in the _wrong_ business. Because what you\'re using to cut code today will not last. The language will evolve, the tools and frameworks that you love will die out and be replaced by new ones that are different and strange. In time, the language you feel you write as a native will fall out of favour, replaced by a new upstart.\\n\\n\x3c!--truncate--\x3e\\n\\nMy first gig was writing telecoms software using Delphi. I haven\'t touched Delphi (or telecoms for that matter) for over 10 years now. Believe me, I grok that things change.\\n\\nThat is the developer\'s lot. If you\'re able to accept that then you\'ll be just fine. For my part I\'ve always rather relished the new and so I embrace it. However, I\'ve met a surprising number of devs that are outraged when they realise that the language and tools they have used since their first job are not going to last. They do not go gentle into that good dawn. They rage, rage against the death of WebForms. My apologies to Dylan Thomas.\\n\\nI recently started a new contract. This always brings a certain amount of change. This is part of the fun of contracting. However, the change was more significant in this case. As a result, the tools that I\'ve been using for the last couple of months have been rather different to those that I\'m used to. I\'ve been outside my comfort zone. I\'ve loved it. And now I want to reflect upon it. Because, in the words of Socrates, \\"the unexamined life is not worth living\\".\\n\\n## The Shock of the New (Toys)\\n\\nI\'d been brought in to work on a full stack ASP.Net project. However, I\'ve initially been working on a separate project which is _entirely_ different. A web client app which has nothing to do with ASP.Net at all. It\'s a greenfield app which is built using the following:\\n\\n1. [React](https://facebook.github.io/react/) / [Flux](https://facebook.github.io/flux/docs/overview.html)\\n2. [WebSockets](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API) / [Protocol Buffers](https://developers.google.com/protocol-buffers/)\\n3. [Browserify](http://browserify.org/)\\n4. [ES6 with Babel](http://babeljs.io/)\\n5. [Karma](https://karma-runner.github.io)\\n6. [Gulp](http://gulpjs.com/)\\n7. [Atom](https://atom.io/)\\n\\nWhere to begin? Perhaps at the end - Atom.\\n\\n## How Does it Feel to be on Your Own?\\n\\nWhen all around you, as far as the eye can see, are monitors displaying Visual Studio in all its grey glory whilst I was hammering away on Atom. It felt pretty good actually.\\n\\nThe app I was working on was a React / Flux app. You know what that means? JSX! At the time the project began Visual Studio did not have good editor support for JSX (something that the shipping of VS 2015 may have remedied but I haven\'t checked). So, rather than endure a life dominated with red squigglies I jumped ship and moved over to using GitHub\'s Atom to edit code.\\n\\nI rather like it. Whilst VS is a full blown IDE, Atom is a text editor. A very pretty one. And crucially, one that can be extended by plugins of which there is a rich ecosystem. You want JSX support? [There\'s a plugin for that](https://atom.io/packages/jshint). You want something that formats JSON nicely for you? [There\'s a plugin for that too](https://atom.io/packages/pretty-json).\\n\\nMy only criticism of Atom really is that it doesn\'t handle large files well and it crashes a lot. I\'m quite surprised by both of these characteristics given that in contrast to VS it is so small. You\'d think the basics would be better covered. Go figure. It still rocks though. It looks so sexy - how could I not like it?\\n\\n## Someone to watch over me\\n\\nI\'ve been using Gulp for a while now. It\'s a great JavaScript task runner and incredibly powerful. Previously though, I\'ve used it as part of a manual build step (even plumbing it into my csproj). With the current project I\'ve moved over to using the watch functionality of gulp. So I\'m scrapping triggering gulp tasks manually. Instead we have gulp configured to gaze lovingly at the source files and, when they change, re-run the build steps.\\n\\nThis is nice for a couple of reasons:\\n\\n- When I want to test out the app the build is already done - I don\'t have to wait for it to happen.\\n- When I do bad things I find out faster. So I\'ve got JSHint being triggered by my watch. If I write code that makes JSHint sad (and I haven\'t noticed the warnings from the [atom plugin](https://atom.io/packages/jshint)) then they\'ll appear in the console. Likewise, my unit tests are continuously running in response to file changes (in an [ncrunch](http://www.ncrunch.net/)\\\\-y sorta style) and so I know straight away if I\'m breaking tests. Rather invaluable in the dynamic world of JavaScript.\\n\\n## Karma, Karma, Karma, Chameleon\\n\\nIn the past, when using Visual Studio, it made sense to use the mighty [Chutzpah](http://mmanela.github.io/chutzpah/) which allows you to run JS unit tests from within VS itself. I needed a new way to run my Jasmine unit tests. The obvious choice was Karma (built by the Angular team I think?). It\'s really flexible.\\n\\nYou\'re using Browserify? [No bother](https://www.npmjs.com/package/karma-browserify). You\'re writing ES6 and transpiling with Babel? Not a problem. You want code coverage? [That we can do](https://www.npmjs.com/package/karma-coverage). You want an integration for TeamCity? [That too is sorted](https://www.npmjs.com/package/karma-teamcity-reporter)....\\n\\nKarma is fantastic. Fun fact: originally it was called Testacular. I kind of get why they changed the name but the world is all the duller for it. A side effect of the name change is that due to invalid search results I know a lot more about Buddhism than I used to.\\n\\n## I cry Babel, Babel, look at me now\\n\\nCan you not wait for the future? Me neither. Even though it\'s 2015 and Back to the Future II takes place in [only a month\'s time](http://www.october212015.com/). So I\'m not waiting for a million and one browsers to implement ES6 and IE 8 to finally die dammit. Nope, I have a plan and it\'s [Babel](http://babeljs.io/). Transpile the tears away, write ES6 and have Babel spit out EStoday.\\n\\nI\'ve found this pretty addictive. Once you\'ve started using ES6 features it\'s hard to go back. Take [destructuring](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Destructuring_assignment) \\\\- I can\'t get enough of it.\\n\\nWhilst I love Babel, it has caused me some sadness as well. My beloved TypeScript is currently not in the mix, Babel is instead sat squarely where I want TS to be. I\'m without static types and rather bereft. You can certainly live without them but having done so for a while I\'m pretty clear now that static typing is a massive productivity win. You don\'t have to hold the data structures that you\'re working on in your head so much, code completion gives you what you need there, you can focus more on the problem that you\'re trying to solve. You also burn less time on silly mistakes. If you accidentally change the return type of a function you\'re likely to know straight away. Refactoring is so much harder without static types. I could go on.\\n\\nAll this goes to say: I want my static typing back. It wasn\'t really an option to use TypeScript in the beginning because we were using JSX and TS didn\'t support it. However! TypeScript is due to add support for JSX in [TS 1.6 (currently in beta)](https://blogs.msdn.com/b/typescript/archive/2015/09/02/announcing-typescript-1-6-beta-react-jsx-better-error-checking-and-more.aspx). I\'ve plans to see if I can get TypeScript to emit ES6 and then keep using Babel to do the transpilation. Whether this will work, I don\'t know yet. But it seems likely. So I\'m hoping for a bright future.\\n\\n## Browserify (there are no song lyrics that can be crowbarred in)\\n\\nEmitting scripts in the required order has been a perpetual pain for everyone in the web world for the longest time. I\'ve taken about 5 different approaches to solving this problem over the years. None felt particularly good. So Browserify.\\n\\nBrowserify solves the problem of script ordering for you by allowing you to define an entry point to your application and getting you to write `require` (npm style) or `import` ([ES6 modules](http://exploringjs.com/es6/ch_modules.html)) to bring in the modules that you need. This allows Browserify (which we\'re using with Babel thanks to the [babelify transform](https://github.com/babel/babelify)) to create a ginormous js file that contains the scripts served as needed. Thanks to the magic of source maps it also allows us to debug our original code (yup, the original ES6!) Browserify has the added bonus of allowing us free reign to pull in npm packages to use in our app without a great deal of ceremony.\\n\\nBrowserify is pretty fab - my only real reservation is that if you step outside the normal use cases you can quickly find yourself in deep water. Take for instance web workers. We were briefly looking into using them as an optimisation (breaking IO onto a separate process from the UI). A prime reason for backing away from this is that [Web Workers don\'t play particularly well with Browserify](https://github.com/substack/webworkify/issues/14). And when you\'ve got Babel (or [Babelify](https://github.com/babel/babelify)) in the mix the problems just multiply. That apart, I really dig Browserify. I think I\'d like to give WebPack a go as well as I understand it fulfills a similar purpose.\\n\\n## WebSockets / Protocol Buffers\\n\\nThe application I\'m working on is plugging into an existing system which uses WebSockets for communication. Since WebSockets are native to the web we\'ve been able to plumb these straight into our app. We\'re also using Protocol Buffers as another optimisation; a way to save a few extra bytes from going down the wire. I don\'t have much to say about either, just some observations really:\\n\\n1. WebSockets is a slightly different way of working - permanently open connections as opposed to the request / response paradigm of classic HTTP\\n2. WebSockets are wicked fast (due in part to those permanent connections). So performance is _amazing_. Fast like native, type amazing. In our case performance is pretty important and so this has been really great.\\n\\n## React / Flux\\n\\nFinally, React and Flux. I was completely new to these when I came onto the project and I quickly came to love them. There was a prejudice for me to overcome and that was JSX. When I first saw it I felt a little sick. \\"Have we learned _NOTHING_???\\" I wailed. \\"Don\'t we know that embedding strings in our controllers is a _BAD_ thing?\\" I was wrong. I had an epiphany. I discovered that JSX is not, as I first imagined, embedded HTML strings. Actually it\'s syntactic sugar for object creation. A simple example:\\n\\n```jsx\\nvar App;\\n\\n// Some JSX:\\nvar app = <App version=\\"1.0.0\\" />;\\n\\n// What that JSX transpiles into:\\nvar app = React.createElement(App, { version: \'1.0.0\' });\\n```\\n\\nNow that I\'m well used to JSX and React I\'ve really got to like it. I keep my views / components as dumb as possible and do all the complex logic in the stores. The stores are just standard JavaScript and so, pretty testable (simple Jasmine gives you all you need - I haven\'t felt the need for [Jest](https://facebook.github.io/jest/)). The components / views are also completely testable. I\'d advise anyone coming to React afresh to make use of the [`ReactShallowRenderer`](https://facebook.github.io/react/docs/test-utils.html#shallow-rendering) for these purposes. This means you can test without a DOM - much better all round.\\n\\nI don\'t have a great deal to say about Flux; I think my views on it aren\'t fully formed yet. I do really like predictability that unidirectional data flow gives you. However, I\'m mindful that the app that I\'ve been writing is very much about displaying data and doesn\'t require much user input. I know that I\'m living without 2-way data binding and I do wonder if I would come to miss it. Time will tell.\\n\\nI really want to get back to static typing. That either means TypeScript (which I know and love) or Facebook\'s Flow. ([A Windows version of Flow is in the works](https://github.com/facebook/flow/issues/6).) I\'ll be very happy if I get either into the mix... Watch this space."},{"id":"top-one-nice-one-get-sorted","metadata":{"permalink":"/top-one-nice-one-get-sorted","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2015-08-13-top-one-nice-one-get-sorted/index.md","source":"@site/blog/2015-08-13-top-one-nice-one-get-sorted/index.md","title":"(Top One, Nice One) Get Sorted","description":"John creates a way to use .NETs LINQ feature to sort JavaScript arrays. The tools allow sorting by one or more criteria.","date":"2015-08-13T00:00:00.000Z","tags":[{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."},{"inline":false,"label":"C#","permalink":"/tags/csharp","description":"The C# programming language."}],"readingTime":8.575,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"top-one-nice-one-get-sorted","title":"(Top One, Nice One) Get Sorted","authors":"johnnyreilly","tags":["javascript","c#"],"hide_table_of_contents":false,"description":"John creates a way to use .NETs LINQ feature to sort JavaScript arrays. The tools allow sorting by one or more criteria."},"unlisted":false,"prevItem":{"title":"Things Done Changed","permalink":"/things-done-changed"},"nextItem":{"title":"Upgrading to Globalize 1.x for Dummies","permalink":"/upgrading-to-globalize-1x-for-dummies"}},"content":"I was recently reading [a post by Jaime Gonz\xe1lez Garc\xeda](http://www.barbarianmeetscoding.com/blog/2015/07/09/mastering-the-arcane-art-of-javascript-mancy-for-c-sharp-developers-chapter-7-using-linq-in-javascript/) which featured the following mind-bending proposition:\\n\\n\x3c!--truncate--\x3e\\n\\n> What if I told you that JavaScript has [LINQ](https://msdn.microsoft.com/en-us/library/bb397926.aspx)??\\n\\nIt got me thinking about one of favourite features of LINQ: [ordering using OrderBy, ThenBy...](http://www.dotnetperls.com/orderby-extension) The ability to simply expose a collection of objects in a given order with a relatively terse and descriptive syntax. It is fantastically convenient, expressive and something I\'ve been missing in JavaScript. But if Jaime is right... Well, let\'s see what we can do.\\n\\n## Sort\\n\\nJavaScript arrays have a [sort](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/sort) method. To quote MDN:\\n\\n> `arr.sort([compareFunction])`### `compareFunction`\\n>\\n> Optional. Specifies a function that defines the sort order. If omitted, the array is sorted according to each character\'s Unicode code point value, according to the string conversion of each element.\\n\\nWe want to use the `sort` function to introduce some LINQ-ish ordering goodness. Sort of. See what I did there?\\n\\nBefore we get going it\'s worth saying that LINQ\'s `OrderBy` and JavaScript\'s `sort` are not the same thing. `sort` actually changes the order of the array. However, `OrderBy` returns an `IOrderedEnumerable` which when iterated returns the items of the collection in a particular order. An important difference. If preserving the original order of my array was important to me (spoiler: mostly it isn\'t) then I could make a call to [`slice`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/slice) prior to calling `sort`.\\n\\n`sort` also returns the array to the caller which is nice for chaining and means we can use it in a similar fashion to the way we use `OrderBy`. With that in mind, we\'re going to create comparer functions which will take a lambda / arrow function (ES6 alert!) and return a function which will compare based on the supplied lambda.\\n\\n## String Comparer\\n\\nLet\'s start with ordering by string properties:\\n\\n```js\\nfunction stringComparer(propLambda) {\\n  return (obj1, obj2) => {\\n    const obj1Val = propLambda(obj1) || \'\';\\n    const obj2Val = propLambda(obj2) || \'\';\\n    return obj1Val.localeCompare(obj2Val);\\n  };\\n}\\n```\\n\\nWe need some example data to sort: (I can only apologise for my lack of inspiration here)\\n\\n```js\\nconst foodInTheHouse = [\\n  { what: \'cake\', daysSincePurchase: 2 },\\n  { what: \'apple\', daysSincePurchase: 8 },\\n  { what: \'orange\', daysSincePurchase: 6 },\\n  { what: \'apple\', daysSincePurchase: 2 },\\n];\\n```\\n\\nIf we were doing a sort by strings in LINQ we wouldn\'t need to implement our own comparer. And the code we\'d write would look something like this:\\n\\n```js\\nvar foodInTheHouseSorted = foodInTheHouse.OrderBy((x) => x.what);\\n```\\n\\nWith that in mind, here\'s how it would look to use our shiny and new `stringComparer`:\\n\\n```js\\nconst foodInTheHouseSorted = foodInTheHouse.sort(stringComparer((x) => x.what));\\n\\n// foodInTheHouseSorted: [\\n//   { what: \'apple\',  daysSincePurchase: 8 },\\n//   { what: \'apple\',  daysSincePurchase: 2 },\\n//   { what: \'cake\',   daysSincePurchase: 2 },\\n//   { what: \'orange\', daysSincePurchase: 6 }\\n// ]\\n\\n// PS Don\'t forget, for our JavaScript: foodInTheHouse === foodInTheHouseSorted\\n//    But for the LINQ:                 foodInTheHouse !=  foodInTheHouseSorted\\n//\\n//    However, if I\'d done this:\\n\\nconst foodInTheHouseSlicedAndSorted = foodInTheHouse\\n  .slice()\\n  .sort(stringComparer((x) => x.what));\\n\\n//    then:                             foodInTheHouse !== foodInTheHouseSlicedAndSorted\\n//\\n//    I shan\'t mention this again.\\n```\\n\\n## Number Comparer\\n\\nWell that\'s strings sorted (quite literally). Now, what about numbers?\\n\\n```js\\nfunction numberComparer(propLambda) {\\n  return (obj1, obj2) => {\\n    const obj1Val = propLambda(obj1);\\n    const obj2Val = propLambda(obj2);\\n    if (obj1Val > obj2Val) {\\n      return 1;\\n    } else if (obj1Val < obj2Val) {\\n      return -1;\\n    }\\n    return 0;\\n  };\\n}\\n```\\n\\nIf we use the `numberComparer` on our original array it looks like this:\\n\\n```js\\nconst foodInTheHouseSorted = foodInTheHouse.sort(\\n  numberComparer((x) => x.daysSincePurchase),\\n);\\n\\n// foodInTheHouseSorted: [\\n//   { what: \'cake\',   daysSincePurchase: 2 },\\n//   { what: \'apple\',  daysSincePurchase: 2 },\\n//   { what: \'orange\', daysSincePurchase: 6 },\\n//   { what: \'apple\',  daysSincePurchase: 8 }\\n// ]\\n```\\n\\n## Descending Into the Pit of Success\\n\\nWell this is all kinds of fabulous. But something\'s probably nagging at you... What about `OrderByDescending`? What about when I want to sort in the reverse order? May I present the `reverse` function:\\n\\n```js\\nfunction reverse(comparer) {\\n  return (obj1, obj2) => comparer(obj1, obj2) * -1;\\n}\\n```\\n\\nAs the name suggests, this function takes a given comparer that\'s handed to it and returns a function that inverts the results of executing that comparer. Clear as mud? A comparer can return 3 types of return values:\\n\\n- 0 - implies equality for `obj1` and `obj2`\\n- positive - implies `obj1` is greater than `obj2` by the ordering criterion\\n- negative - implies `obj1` is less than `obj2` by the ordering criterion\\n\\nOur `reverse` function takes the comparer it is given and returns a new comparer that will return a positive value where the old one would have returned a negative and vica versa. (Equality is unaffected.) An alternative implementation would have been this:\\n\\n```js\\nfunction reverse(comparer) {\\n  return (obj1, obj2) => comparer(obj2, obj1);\\n}\\n```\\n\\nWhich is more optimal and even simpler as it just swaps the values supplied to the comparer. Whatever tickles your fancy. Either way, when used it looks like this:\\n\\n```js\\nconst foodInTheHouseSorted = foodInTheHouse.sort(\\n  reverse(stringComparer((x) => x.what)),\\n);\\n\\n// foodInTheHouseSorted: [\\n//   { what: \'orange\', daysSincePurchase: 6 },\\n//   { what: \'cake\',   daysSincePurchase: 2 },\\n//   { what: \'apple\',  daysSincePurchase: 8 },\\n//   { what: \'apple\',  daysSincePurchase: 2 }\\n// ]\\n```\\n\\nIf you\'d rather not have a function wrapping a function inline then you could create `stringComparerDescending`, a `numberComparerDescending` etc implementations. Arguably it might make for a nicer API. I\'m not unhappy with the present approach myself and so I\'ll leave it as is. But it\'s an option.\\n\\n## `ThenBy`\\n\\nSo far we can sort arrays by strings, we can sort arrays by numbers and we can do either in descending order. It\'s time to take it to the next level people. That\'s right `ThenBy`; I want to be able to sort by one criteria and then by a subcriteria. So perhaps I want to eat the food in the house in alphabetical order, but if I have multiple apples I want to eat the ones I bought most recently first (because the other ones look old, brown and yukky). This may also be a sign I haven\'t thought my life through, but it\'s a choice that people make. People that I know. People I may have married.\\n\\nIt\'s time to compose our comparers together. May I present... drum roll.... the `composeComparers` function:\\n\\n```js\\nfunction composeComparers(...comparers) {\\n  return (obj1, obj2) => {\\n    const comparer = comparers.find((c) => c(obj1, obj2) !== 0);\\n    return comparer ? comparer(obj1, obj2) : 0;\\n  };\\n}\\n```\\n\\nThis fine function takes any number of comparers that have been supplied to it. It then returns a comparer function which, when called, iterates through each of the original comparers and executes them until it finds one that returns a value that is not 0 (ie represents that the 2 items are not equal). It then sends that non-zero value back or if all was equal then sends back 0.\\n\\n```js\\nconst foodInTheHouseSorted = foodInTheHouse.sort(\\n  composeComparers(\\n    stringComparer((x) => x.what),\\n    numberComparer((x) => x.daysSincePurchase),\\n  ),\\n);\\n\\n// foodInTheHouseSorted: [\\n//   { what: \'apple\',  daysSincePurchase: 2 },\\n//   { what: \'apple\',  daysSincePurchase: 8 },\\n//   { what: \'cake\',   daysSincePurchase: 2 },\\n//   { what: \'orange\', daysSincePurchase: 6 }\\n// ]\\n```\\n\\n## `composeComparers`: The Sequel\\n\\nI\'m not gonna lie - I was feeling quite pleased with this approach. I shared it with my friend (and repeated colleague) [Peter Foldi](http://blog.peterfoldi.com/). The next day I found this in my inbox:\\n\\n```js\\nfunction composeComparers(...comparers) {\\n  return (obj1, obj2) =>\\n    comparers.reduce((prev, curr) => prev || curr(obj1, obj2), 0);\\n}\\n```\\n\\nDammit he\'s improved it. It\'s down to 1 line of code, it doesn\'t execute a non-zero returning comparer twice and it doesn\'t rely on [`find`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/find) which only arrives with ES6. So if you wanted to backport to ES5 then this is a better choice.\\n\\nThe only criticism I can make of it is that it iterates through each of the comparers even when it doesn\'t need to execute them. But that\'s just carping really.\\n\\n## `composeComparers`: The Ultimate\\n\\nSo naturally I thought I was done. Showing Peter\'s improvements to the estimable Matthew Horsley I learned that this was not so. Because he reached for the keyboard and entered this:\\n\\n```js\\nfunction composeComparers(...comparers) {\\n  // README: <a href=\\"https://wiki.haskell.org/Function_composition\\">https://wiki.haskell.org/Function_composition</a>\\n  return comparers.reduce((prev, curr) => (a, b) => prev(a, b) || curr(a, b));\\n}\\n```\\n\\nThat\'s right, he\'s created a function which takes a number of comparers and reduced them up front into a single comparer function. This means that when the sort takes place there is no longer a need to iterate through the comparers, just execute them.\\n\\nI know.\\n\\n![animated gif with the heading \\"mind-equals-blown\\"](mind-equals-blown.gif)\\n\\nI\'ll get my coat...\\n\\n```js\\nfunction composeComparers(...comparers) {\\n  return comparers.reduce((prev, curr) => (a, b) => prev(a, b) || curr(a, b));\\n}\\n\\nfunction stringComparer(propLambda) {\\n  return (obj1, obj2) => {\\n    const obj1Val = propLambda(obj1) || \'\';\\n    const obj2Val = propLambda(obj2) || \'\';\\n    return obj1Val.localeCompare(obj2Val);\\n  };\\n}\\n\\nfunction numberComparer(propLambda) {\\n  return (obj1, obj2) => {\\n    const obj1Val = propLambda(obj1);\\n    const obj2Val = propLambda(obj2);\\n    if (obj1Val > obj2Val) {\\n      return 1;\\n    } else if (obj1Val < obj2Val) {\\n      return -1;\\n    }\\n    return 0;\\n  };\\n}\\n\\nfunction reverse(comparer) {\\n  return (obj1, obj2) => comparer(obj2, obj1);\\n}\\n\\n/* - Example usage\\nconst foodInTheHouse = [\\n  { what: \'cake\',   daysSincePurchase: 2 },\\n  { what: \'apple\',  daysSincePurchase: 8 },\\n  { what: \'orange\', daysSincePurchase: 6 },\\n  { what: \'apple\',  daysSincePurchase: 2 },\\n];\\nconst foodInTheHouseSorted = foodInTheHouse.sort(composeComparers(\\n    stringComparer(x => x.what),\\n    reverse(numberComparer(x => x.daysSincePurchase))\\n));\\nconsole.log(foodInTheHouseSorted);\\n*/\\n```\\n\\n## Updated 08/10/2018: Now TypeScript\\n\\nYou want to do this with TypeScript? Use this:\\n\\n```ts\\ntype Comparer<TObject> = (obj1: TObject, obj2: TObject) => number;\\n\\nexport function stringComparer<TObject>(\\n  propLambda: (obj: TObject) => string,\\n): Comparer<TObject> {\\n  return (obj1: TObject, obj2: TObject) => {\\n    const obj1Val = propLambda(obj1) || \'\';\\n    const obj2Val = propLambda(obj2) || \'\';\\n    return obj1Val.localeCompare(obj2Val);\\n  };\\n}\\n\\nexport function numberComparer<TObject>(\\n  propLambda: (obj: TObject) => number,\\n): Comparer<TObject> {\\n  return (obj1: TObject, obj2: TObject) => {\\n    const obj1Val = propLambda(obj1);\\n    const obj2Val = propLambda(obj2);\\n    if (obj1Val > obj2Val) {\\n      return 1;\\n    } else if (obj1Val < obj2Val) {\\n      return -1;\\n    }\\n    return 0;\\n  };\\n}\\n\\nexport function reverse<TObject>(comparer: Comparer<TObject>) {\\n  return (obj1: TObject, obj2: TObject) => comparer(obj2, obj1);\\n}\\n\\nexport function composeComparers<TObject>(...comparers: Comparer<TObject>[]) {\\n  return comparers.reduce((prev, curr) => (a, b) => prev(a, b) || curr(a, b));\\n}\\n```"},{"id":"upgrading-to-globalize-1x-for-dummies","metadata":{"permalink":"/upgrading-to-globalize-1x-for-dummies","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2015-07-30-upgrading-to-globalize-1x-for-dummies/index.md","source":"@site/blog/2015-07-30-upgrading-to-globalize-1x-for-dummies/index.md","title":"Upgrading to Globalize 1.x for Dummies","description":"Migrating to Globalize 1.0, which modularized the code, requires a significant amount of work, as shown by John Reilly\u2019s examples.","date":"2015-07-30T00:00:00.000Z","tags":[{"inline":false,"label":"Globalize","permalink":"/tags/globalize","description":"The Globalize library."}],"readingTime":9.415,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"upgrading-to-globalize-1x-for-dummies","title":"Upgrading to Globalize 1.x for Dummies","authors":"johnnyreilly","tags":["globalize"],"hide_table_of_contents":false,"description":"Migrating to Globalize 1.0, which modularized the code, requires a significant amount of work, as shown by John Reilly\u2019s examples."},"unlisted":false,"prevItem":{"title":"(Top One, Nice One) Get Sorted","permalink":"/top-one-nice-one-get-sorted"},"nextItem":{"title":"npm please stop hurting Visual Studio","permalink":"/npm-please-stop-hurting-visual-studio"}},"content":"Globalize has hit 1.0. Anyone who reads my blog will likely be aware that I\'m a long time user of [Globalize 0.1.x](../2012-05-07-globalizejs-number-and-date/index.md). I\'ve been a little daunted by the leap that the move from 0.1.x to 1.x represents. It appears to be the very definition of \\"breaking changes\\". :-) But hey, this is Semantic Versioning being used correctly so how could I complain? Either way, I\'ve decided to write up the migration here as I\'m not expecting this to be easy.\\n\\n\x3c!--truncate--\x3e\\n\\nTo kick things off I\'ve set up a very [simple repo](https://github.com/johnnyreilly/globalize-migration/tree/v0.1.x) that consists of a single page that depends upon Globalize 0.1.x to render a number and a date in German. It looks like this:\\n\\n```html\\n<html>\\n  <head>\\n    <title>Globalize demo</title>\\n    <link\\n      href=\\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css\\"\\n      rel=\\"stylesheet\\"\\n    />\\n  </head>\\n  <body>\\n    <div class=\\"container-fluid\\">\\n      <h4>Globalize demo for the <em id=\\"locale\\"></em> culture / locale</h4>\\n      <p>\\n        This is a the number <strong id=\\"number\\"></strong> formatted by\\n        Globalize: <strong id=\\"numberFormatted\\"></strong>\\n      </p>\\n      <p>\\n        This is a the number <strong id=\\"date\\"></strong> formatted by Globalize:\\n        <strong id=\\"dateFormatted\\"></strong>\\n      </p>\\n    </div>\\n\\n    <script src=\\"bower_components/globalize/lib/globalize.js\\"><\/script>\\n    <script src=\\"bower_components/globalize/lib/cultures/globalize.culture.de-DE.js\\"><\/script>\\n    <script>\\n      var locale = \'de-DE\';\\n      var number = 12345.67;\\n      var date = new Date(2012, 5, 15);\\n\\n      Globalize.culture(locale);\\n      document.querySelector(\'#locale\').innerText = locale;\\n      document.querySelector(\'#number\').innerText = number;\\n      document.querySelector(\'#date\').innerText = date;\\n      document.querySelector(\'#numberFormatted\').innerText = Globalize.format(\\n        number,\\n        \'n2\',\\n      );\\n      document.querySelector(\'#dateFormatted\').innerText = Globalize.format(\\n        date,\\n        \'d\',\\n      );\\n    <\/script>\\n  </body>\\n</html>\\n```\\n\\nWhen it\'s run it looks like this:\\n\\n![](Screenshot-2015-07-29-06.03.04.webp)\\n\\nLet\'s see how we go about migrating this super simple example.\\n\\n## Updated our Bower dependencies\\n\\nFirst things first, we want to move Globalize from 0.1.x to 1.x using Bower. To do that we update our `bower.json`:\\n\\n```js\\n\\"dependencies\\": {\\n    \\"globalize\\": \\"^1.0.0\\"\\n  }\\n```\\n\\nNow we enter: `bower update`. And we\'re off!\\n\\n```sh\\nbower globalize#^1.0.0          cached git://github.com/jquery/globalize.git#1.0.0\\nbower globalize#^1.0.0        validate 1.0.0 against git://github.com/jquery/globalize.git#^1.0.0\\nbower cldr-data#>=25            cached git://github.com/rxaviers/cldr-data-bower.git#27.0.3\\nbower cldr-data#>=25          validate 27.0.3 against git://github.com/rxaviers/cldr-data-bower.git#>=25\\nbower cldrjs#0.4.1              cached git://github.com/rxaviers/cldrjs.git#0.4.1\\nbower cldrjs#0.4.1            validate 0.4.1 against git://github.com/rxaviers/cldrjs.git#0.4.1\\nbower globalize#^1.0.0         install globalize#1.0.0\\nbower cldr-data#>=25           install cldr-data#27.0.3\\nbower cldrjs#0.4.1             install cldrjs#0.4.1\\n\\nglobalize#1.0.0 bower_components\\\\globalize\\n\u251C\u2500\u2500 cldr-data#27.0.3\\n\u2514\u2500\u2500 cldrjs#0.4.1\\n\\ncldr-data#27.0.3 bower_components\\\\cldr-data\\n\\ncldrjs#0.4.1 bower_components\\\\cldrjs\\n\u2514\u2500\u2500 cldr-data#27.0.3\\n```\\n\\nThis all looks happy enough. Except it\'s actually not.\\n\\n## We need fuel\\n\\nOr as I like to call it cldr-data. We just pulled down Globalize 1.x but we didn\'t pull down the data that Globalize 1.x relies upon. This is one of the differences between Globalize 0.1.x and 1.x. Globalize 1.x does not include the \\"culture\\" data. By which I mean all the `globalize.culture.de-DE.js` type files. Instead Globalize 1.x relies upon [CLDR - Unicode Common Locale Data Repository](http://cldr.unicode.org/). It does this in the form of [cldr-json](https://github.com/unicode-cldr/cldr-json).\\n\\nNow before you start to worry, you shouldn\'t actually need to go and get this by yourself, the lovely [Rafael Xavier de Souza](https://github.com/rxaviers) has saved you a job by putting together [Bower](https://github.com/rxaviers/cldr-data-bower) and [npm](https://github.com/rxaviers/cldr-data-npm) modules to do the hard work for you.\\n\\nI\'m using Bower for my client side package management and so I\'ll use that. Looking at the Bower dependencies downloaded when I upgraded my package I can see there is a `cldr-data` package. Yay! However it appears to be missing the associated json files. Boo!\\n\\nTo the documentation Batman. It says you need a `.bowerrc` file in your repo which contains this:\\n\\n```js\\n{\\n  \\"scripts\\": {\\n    \\"preinstall\\": \\"npm install cldr-data-downloader@0.2.x\\",\\n    \\"postinstall\\": \\"node ./node_modules/cldr-data-downloader/bin/download.js -i bower_components/cldr-data/index.json -o bower_components/cldr-data/\\"\\n  }\\n}\\n```\\n\\nUnfortunately, because I\'ve already upgraded to v1 adding this file alone doesn\'t do anything for me. To get round that I delete my `bower_components` folder and enter `bower install`. Boom!\\n\\n```\\nbower globalize#^1.0.0          cached git://github.com/jquery/globalize.git#1.0.0\\nbower globalize#^1.0.0        validate 1.0.0 against git://github.com/jquery/globalize.git#^1.0.0\\nbower cldrjs#0.4.1                        cached git://github.com/rxaviers/cldrjs.git#0.4.1\\nbower cldrjs#0.4.1                      validate 0.4.1 against git://github.com/rxaviers/cldrjs.git#0.4.1\\nbower cldr-data#>=25                      cached git://github.com/rxaviers/cldr-data-bower.git#27.0.3\\nbower cldr-data#>=25                    validate 27.0.3 against git://github.com/rxaviers/cldr-data-bower.git#>=25\\nbower                                 preinstall npm install cldr-data-downloader@0.2.x\\nbower                                 preinstall cldr-data-downloader@0.2.3 node_modules\\\\cldr-data-downloader\\nbower                                 preinstall \u251C\u2500\u2500 progress@1.1.8\\nbower                                 preinstall \u251C\u2500\u2500 q@1.0.1\\nbower                                 preinstall \u251C\u2500\u2500 request-progress@0.3.1 (throttleit@0.0.2)\\nbower                                 preinstall \u251C\u2500\u2500 nopt@3.0.3 (abbrev@1.0.7)\\nbower                                 preinstall \u251C\u2500\u2500 mkdirp@0.5.0 (minimist@0.0.8)\\nbower                                 preinstall \u251C\u2500\u2500 adm-zip@0.4.4\\nbower                                 preinstall \u251C\u2500\u2500 npmconf@2.0.9 (uid-number@0.0.5, ini@1.3.4, inherits@2.0.1, once@1.3.2, osenv@0.1.3, config-chain@1.1.9, semver@4.3.6)\\nbower                                 preinstall \u2514\u2500\u2500 request@2.53.0 (caseless@0.9.0, forever-agent@0.5.2, aws-sign2@0.5.0, stringstream@0.0.4, tunnel-agent@0.4.1, oauth-sign@0.6.0, isstream@0.1.2, json-stringify-safe@5.0.1, qs@2.3.3, node-uuid@1.4.3, combined-stream@0.0.7, mime-types@2.0.14, form-data@0.2.0, tough-cookie@2.0.0, bl@0.9.4, http-signature@0.10.1, hawk@2.3.1)\\nbower globalize#^1.0.0                   install globalize#1.0.0\\nbower cldrjs#0.4.1                       install cldrjs#0.4.1\\nbower cldr-data#>=25                     install cldr-data#27.0.3\\nbower                                postinstall node ./node_modules/cldr-data-downloader/bin/download.js -i bower_components/cldr-data/index.json -o bower_components/cldr-data/\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-core/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-dates-modern/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-buddhist-modern/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-chinese-modern/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-coptic-modern/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-dangi-modern/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-ethiopic-modern/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-hebrew-modern/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-indian-modern/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-islamic-modern/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-japanese-modern/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-persian-modern/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-roc-modern/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-localenames-modern/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-misc-modern/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-numbers-modern/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-segments-modern/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-units-modern/archive/27.0.3.zip`\\nbower                                postinstall Received 28728K total.\\nbower                                postinstall Received 28753K total.\\nbower                                postinstall Unpacking it into `./bower_components\\\\cldr-data`\\n\\nglobalize#1.0.0 bower_components\\\\globalize\\n\u251C\u2500\u2500 cldr-data#27.0.3\\n\u2514\u2500\u2500 cldrjs#0.4.1\\n\\ncldrjs#0.4.1 bower_components\\\\cldrjs\\n\u2514\u2500\u2500 cldr-data#27.0.3\\n\\ncldr-data#27.0.3 bower_components\\\\cldr-data\\n```\\n\\nThat\'s right - I\'m golden. And if I didn\'t want to do that I could have gone straight to the command line and entered this: (look familiar?)\\n\\n```\\nnpm install cldr-data-downloader@0.2.x\\nnode ./node_modules/cldr-data-downloader/bin/download.js -i bower_components/cldr-data/index.json -o bower_components/cldr-data/\\n```\\n\\n## Some bitching and moaning.\\n\\nIf, like me, you were a regular user of Globalize 0.1.x then you know that you needed very little to get going. As you can see from our example you just serve up `Globalize.js` and the culture files you are interested in (eg `globalize.culture.de-DE.js`). That\'s it - you have all you need; job\'s a good\'un. This is all very convenient and entirely lovely.\\n\\nGlobalize 1.x has a different approach and one that (I have to be honest) I\'m not entirely on board with. The thing that you need to know about the new Globalize is that _nothing comes for free_. It\'s been completely modularised and [you have to include extra libraries depending on the functionality you require.](https://github.com/jquery/globalize#pick-the-modules-you-need) On top of that you then have to work out the [portions of the cldr data that you require for those modules](https://github.com/jquery/globalize#2-cldr-content) and supply them. This means that getting up and running with Globalize 1.x is much harder. Frankly I think it\'s a little painful.\\n\\nI realise this is a little [\\"Who moved my cheese\\"](https://en.wikipedia.org/wiki/Who_Moved_My_Cheese%3F). I\'ll get over it. I do actually see the logic of this. It is certainly good that the culture date is not frozen in aspic but will evolve as the world does. But it\'s undeniable that in our brave new world Globalize is no longer a doddle to pick up. Or at least right now.\\n\\n## Take the modules and run\\n\\nSo. What do we actually need? Well I\'ve consulted the [documentation](https://github.com/jquery/globalize#pick-the-modules-you-need) and I think I\'m clear. Our simple demo cares about dates and numbers. So I\'m going to guess that means I need:\\n\\n- [`globalize.js`](https://github.com/jquery/globalize#core-module)\\n- [`globalize/date.js`](https://github.com/jquery/globalize#date-module)\\n- [`globalize/number.js`](https://github.com/jquery/globalize#number-module)\\n\\nOn top of that I\'m also going to need the various cldr dependencies too. That\'s not all. Given that I\'ve decided which modules I will use I now need to acquire the associated cldr data. According to the docs [here](https://github.com/jquery/globalize#2-cldr-content) we\'re going to need:\\n\\n- `cldr/supplemental/likelySubtags.json`\\n- `cldr/main/<i>locale</i>/ca-gregorian.json`\\n- `cldr/main/<i>locale</i>/timeZoneNames.json`\\n- `cldr/supplemental/timeData.json`\\n- `cldr/supplemental/weekData.json`\\n- `cldr/main/locale/numbers.json`\\n- `cldr/supplemental/numberingSystems.json`\\n\\nFiguring that all out felt like really hard work. But I think that now we\'re ready to do the actual migration.\\n\\n### Updated 30/08/2015: Globalize \xb7 So What\'cha Want\\n\\nTo make working out what you need when using Globalize I\'ve built [Globalize \xb7 So What\'cha Want](http://johnnyreilly.github.io/globalize-so-what-cha-want/). You\'re so very welcome.\\n\\n## The Actual Migration\\n\\nTo do this I\'m going to lean heavily upon [an example put together by Rafael](https://github.com/jquery/globalize/blob/master/examples/plain-javascript/index.html). The migrated code looks like this:\\n\\n```html\\n<html>\\n  <head>\\n    <title>Globalize demo</title>\\n    <link\\n      href=\\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css\\"\\n      rel=\\"stylesheet\\"\\n    />\\n  </head>\\n  <body>\\n    <div class=\\"container-fluid\\">\\n      <h4>Globalize demo for the <em id=\\"locale\\"></em> culture / locale</h4>\\n      <p>\\n        This is a the number <strong id=\\"number\\"></strong> formatted by\\n        Globalize: <strong id=\\"numberFormatted\\"></strong>\\n      </p>\\n      <p>\\n        This is a the number <strong id=\\"date\\"></strong> formatted by Globalize:\\n        <strong id=\\"dateFormatted\\"></strong>\\n      </p>\\n    </div>\\n\\n    \x3c!-- First, we load Globalize\'s dependencies (`cldrjs` and its supplemental module). --\x3e\\n    <script src=\\"bower_components/cldrjs/dist/cldr.js\\"><\/script>\\n    <script src=\\"bower_components/cldrjs/dist/cldr/event.js\\"><\/script>\\n    <script src=\\"bower_components/cldrjs/dist/cldr/supplemental.js\\"><\/script>\\n\\n    \x3c!-- Next, we load Globalize and its modules. --\x3e\\n    <script src=\\"bower_components/globalize/dist/globalize.js\\"><\/script>\\n    <script src=\\"bower_components/globalize/dist/globalize/number.js\\"><\/script>\\n\\n    \x3c!-- Load after globalize/number.js --\x3e\\n    <script src=\\"bower_components/globalize/dist/globalize/date.js\\"><\/script>\\n\\n    <script>\\n      var locale = \'de\';\\n\\n      Promise.all([\\n        // Core\\n        fetch(\'bower_components/cldr-data/supplemental/likelySubtags.json\'),\\n\\n        // Date\\n        fetch(\\n          \'bower_components/cldr-data/main/\' + locale + \'/ca-gregorian.json\',\\n        ),\\n        fetch(\\n          \'bower_components/cldr-data/main/\' + locale + \'/timeZoneNames.json\',\\n        ),\\n        fetch(\'bower_components/cldr-data/supplemental/timeData.json\'),\\n        fetch(\'bower_components/cldr-data/supplemental/weekData.json\'),\\n\\n        // Number\\n        fetch(\'bower_components/cldr-data/main/\' + locale + \'/numbers.json\'),\\n        fetch(\'bower_components/cldr-data/supplemental/numberingSystems.json\'),\\n      ])\\n        .then(function (responses) {\\n          return Promise.all(\\n            responses.map(function (response) {\\n              return response.json();\\n            }),\\n          );\\n        })\\n        .then(Globalize.load)\\n        .then(function () {\\n          var number = 12345.67;\\n          var date = new Date(2012, 5, 15);\\n\\n          var globalize = Globalize(locale);\\n          document.querySelector(\'#locale\').innerText = locale;\\n          document.querySelector(\'#number\').innerText = number;\\n          document.querySelector(\'#date\').innerText = date;\\n          document.querySelector(\'#numberFormatted\').innerText =\\n            globalize.formatNumber(number, {\\n              minimumFractionDigits: 2,\\n              useGrouping: true,\\n            });\\n          document.querySelector(\'#dateFormatted\').innerText =\\n            globalize.formatDate(date, {\\n              date: \'medium\',\\n            });\\n        });\\n    <\/script>\\n  </body>\\n</html>\\n```\\n\\nBy the way, I\'m using [fetch](http://jakearchibald.com/2015/thats-so-fetch/) and [promises](http://www.html5rocks.com/en/tutorials/es6/promises/) to load the cldr-data. This isn\'t mandatory - I use it because Chrome lets me. (I\'m so bleeding edge.) Some standard jQuery ajax calls would do just as well. There\'s an example of that approach [here](https://github.com/jquery/globalize/blob/master/doc/cldr/index.md#how-do-i-load-cldr-data-into-globalize).\\n\\n## Observations\\n\\nWe\'ve gone from not a lot of code to... well, more than a little. A medium amount. Almost all of that extra code relates to getting Globalize 1.x to spin up so it\'s ready to work. We\'ve also gone from 2 HTTP requests to 13 which is unlucky for you. 6 of them took place via ajax after the page had loaded. It\'s worth noting that we\'re not even loading all of Globalize either. On top of that there\'s the old order-of-loading shenanigans to deal with. All of these can be mitigated by introducing a custom build step of your own to concatenate and minify the associated cldr / Globalize files.\\n\\nLoading the data via ajax isn\'t mandatory by the way. If you wanted to you could create your own style of `globalize.culture.de.js` files which would allow you load the page without recourse to post-page load HTTP requests. Something like this Gulp task I\'ve knocked up would do the trick:\\n\\n```js\\ngulp.task(\'make-globalize-culture-de-js\', function () {\\n  var locale = \'de\';\\n  var jsonWeNeed = [\\n    require(\'./bower_components/cldr-data/supplemental/likelySubtags.json\'),\\n    require(\'./bower_components/cldr-data/main/\' +\\n      locale +\\n      \'/ca-gregorian.json\'),\\n    require(\'./bower_components/cldr-data/main/\' +\\n      locale +\\n      \'/timeZoneNames.json\'),\\n    require(\'./bower_components/cldr-data/supplemental/timeData.json\'),\\n    require(\'./bower_components/cldr-data/supplemental/weekData.json\'),\\n    require(\'./bower_components/cldr-data/main/\' + locale + \'/numbers.json\'),\\n    require(\'./bower_components/cldr-data/supplemental/numberingSystems.json\'),\\n  ];\\n\\n  var jsonStringWithLoad =\\n    \'Globalize.load(\' +\\n    jsonWeNeed\\n      .map(function (json) {\\n        return JSON.stringify(json);\\n      })\\n      .join(\', \') +\\n    \');\';\\n\\n  var fs = require(\'fs\');\\n  fs.writeFile(\\n    \'./globalize.culture.\' + locale + \'.js\',\\n    jsonStringWithLoad,\\n    function (err) {\\n      if (err) {\\n        console.log(err);\\n      } else {\\n        console.log(\'The file was created!\');\\n      }\\n    },\\n  );\\n});\\n```\\n\\nThe above is standard node/io type code by the way; just take the contents of the function and run in node and you should be fine. If you do use this approach then you\'re very much back to the simplicity of Globalize 0.1.x\'s approach.\\n\\nAnd here is the page in all its post migration glory:\\n\\n![](Screenshot-2015-07-30-20.21.19.webp)\\n\\nIt looks exactly the same except \'de-DE\' has become simply \'de\' (since that\'s how the cldr rolls).\\n\\nThe migrated code is [there for the taking](https://github.com/johnnyreilly/globalize-migration). Make sure you remember to `bower install` \\\\- and you\'ll need to host the demo on a simple server since it makes ajax calls.\\n\\nBefore I finish off I wanted to say \\"well done!\\" to all the people who have worked on Globalize. It\'s an important project and I do apologise for my being a little critical of it here. I should say that I think it\'s just the getting started that\'s hard. Once you get over that hurdle you\'ll be fine. Hopefully this post will help people do just that. Pip, pip!"},{"id":"npm-please-stop-hurting-visual-studio","metadata":{"permalink":"/npm-please-stop-hurting-visual-studio","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2015-06-29-npm-please-stop-hurting-visual-studio/index.md","source":"@site/blog/2015-06-29-npm-please-stop-hurting-visual-studio/index.md","title":"npm please stop hurting Visual Studio","description":"Windows handling of long paths can be problematic when using Visual Studio with npm; using rimraf for deletions can help until npm 3.0 comes out.","date":"2015-06-29T00:00:00.000Z","tags":[{"inline":false,"label":"Node.js","permalink":"/tags/node-js","description":"The Node.js runtime."}],"readingTime":4.49,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"npm-please-stop-hurting-visual-studio","title":"npm please stop hurting Visual Studio","authors":"johnnyreilly","tags":["node.js"],"hide_table_of_contents":false,"description":"Windows handling of long paths can be problematic when using Visual Studio with npm; using rimraf for deletions can help until npm 3.0 comes out."},"unlisted":false,"prevItem":{"title":"Upgrading to Globalize 1.x for Dummies","permalink":"/upgrading-to-globalize-1x-for-dummies"},"nextItem":{"title":"Back to the Future with Code First Migrations","permalink":"/Back-to-the-Future-with-Code-First-Migrations"}},"content":"I don\'t know about you but I personally feel that the following sentence may well be the saddest in the English language:\\n\\n\x3c!--truncate--\x3e\\n\\n`2&gt;ASPNETCOMPILER : error ASPRUNTIME: The specified path, file name, or both are too long. The fully qualified file name must be less than 260 characters, and the directory name must be less than 248 characters.`\\n\\nThe message above would suggest there is some kind of ASP.Net issue going on. There isn\'t - the problem actually lies with Windows. It\'s [not the first time it\'s come up](../2014-12-12-gulp-npm-long-paths-and-visual-studio-fight/index.md) but for those of you not aware there is something you need to know about Windows: _It handles long paths badly._\\n\\nThere\'s a number of caveats which people may attach the above sentence. But essentially what I have said is true. And it becomes brutally apparent to you the moment you start using a few node / npm powered tools in your workflow. You will likely see that horrible message and you won\'t be able to get much further forward. Sigh. I thought this was the future...\\n\\nThis post is about how to deal with the long path issue when using npm with Visual Studio. This should very much be a short term workaround as [npm 3.0](https://github.com/npm/npm/releases/tag/v3.0.0) is planned to make long paths with npm a thing of the past. But until that golden dawn....\\n\\n## The Latest Infraction\\n\\nI\'m a big fan of Gulp and Bower. They rock. [Steve Cadwallader](https://twitter.com/codecadwallader) wrote an excellent blog post about [integrating Gulp into your Visual Studio build](http://www.codecadwallader.com/2015/03/15/integrating-gulp-into-your-tfs-builds-and-web-deploy/). Essentially the Gist of his post is this: forget using [Task Runner Explorer](https://visualstudiogallery.msdn.microsoft.com/8e1b4368-4afb-467a-bc13-9650572db708) to trigger your Gulp / Grunt jobs. No, actually plug it into the build process by tweaking your `.csproj` file. The first time I used this approach it was a dream come true. It just worked and I was a very happy man.\\n\\nSince this approach was so marvellous I took a look at the demo / docs part of [jQuery Validation Unobtrusive Native](https://github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native) with a view to applying it there. I originally wrote this back in 2013 and at the time used NuGet for both server and client side package management. I decided to migrate it to use Bower for the client side packages (which I planned to combine with a Gulp script which was going to pull out the required JS / CSS etc as needed). However it wasn\'t the plain sailing I\'d imagined. The actual switchover from NuGet to Bower was simple. Just a case of removing NuGet packages and adding their associated Bower counterpart. The problem came when the migration was done and I hit \\"compile\\". That\'s when I got to see `2&gt;ASPNETCOMPILER : error ASPRUNTIME: The specified path, file name, or both are too long...` etc\\n\\nFor reasons that I don\'t fully understand, Visual Studio is really upset by the presence in the project structure of one almighty long path. Oddly enough, not a path that\'s actually part of the Visual Studio project in question at all. Rather one that has come along as a result of our Gulp / Bower / npm shenanigans. Quick as a flash, I whipped out Daniel Schroeder\'s [Path Length Checker](https://pathlengthchecker.codeplex.com/) to see where the problem lay:\\n\\n![](bower-with-the-long-paths.png)\\n\\nAnd lo, the fault lay with Bower. Poor show, Bower, poor show.\\n\\n## rimraf to the Rescue\\n\\n[rimraf](https://github.com/isaacs/rimraf) is \\"the [UNIX command](<https://en.wikipedia.org/wiki/Rm_(Unix)>)`rm -rf` for node\\". (By the way, what is it with node and the pathological hatred of capital letters?)\\n\\nWhat this means is: rimraf can delete. Properly. So let\'s get it: `npm install -g rimraf`. Then at any time at the command line we can dispose of a long path in 2 shakes of lamb\'s tail.\\n\\nIn my current situation the contents of the `node_modules` folder is causing me heartache. But with rimraf in play I can get rid of it with the magic words: `rimraf ./node_modules`. Alakazam! So let\'s poke this command into the extra commands that I\'ve already shoplifted from Steve\'s blog post. I\'ll end up with the following section of XML at the end of my `.csproj`:\\n\\n```xml\\n<PropertyGroup>\\n    <CompileDependsOn>\\n      $(CompileDependsOn);\\n      GulpBuild;\\n    </CompileDependsOn>\\n    <CleanDependsOn>\\n      $(CleanDependsOn);\\n      GulpClean\\n    </CleanDependsOn>\\n    <CopyAllFilesToSingleFolderForPackageDependsOn>\\n      CollectGulpOutput;\\n      $(CopyAllFilesToSingleFolderForPackageDependsOn);\\n    </CopyAllFilesToSingleFolderForPackageDependsOn>\\n    <CopyAllFilesToSingleFolderForMsdeployDependsOn>\\n      CollectGulpOutput;\\n      $(CopyAllFilesToSingleFolderForPackageDependsOn);\\n    </CopyAllFilesToSingleFolderForMsdeployDependsOn>\\n  </PropertyGroup>\\n  <Target Name=\\"GulpBuild\\">\\n    <Exec Command=\\"npm install\\" />\\n    <Exec Command=\\"bower install\\" />\\n    <Exec Command=\\"gulp\\" />\\n    <Exec Command=\\"rimraf ./node_modules\\" />\\n  </Target>\\n  <Target Name=\\"GulpClean\\">\\n    <Exec Command=\\"npm install\\" />\\n    <Exec Command=\\"gulp clean\\" />\\n    <Exec Command=\\"rimraf ./node_modules\\" />\\n  </Target>\\n  <Target Name=\\"CollectGulpOutput\\">\\n    <ItemGroup>\\n      <_CustomFiles Include=\\"build\\\\**\\\\*\\" />\\n      <FilesForPackagingFromProject Include=\\"%(_CustomFiles.Identity)\\">\\n        <DestinationRelativePath>build\\\\%(RecursiveDir)%(Filename)%(Extension)</DestinationRelativePath>\\n      </FilesForPackagingFromProject>\\n    </ItemGroup>\\n    <Message Text=\\"CollectGulpOutput list: %(_CustomFiles.Identity)\\" />\\n  </Target>\\n```\\n\\nSo let\'s focus on the important bits in the `GulpBuild` target:\\n\\n- `&lt;Exec Command=\\"npm install\\" /&gt;` \\\\- install the node packages our project uses as specified in `package.json`. This will include Gulp and Bower. The latter package is going to contain super-long, Windows wrecking paths.\\n- `&lt;Exec Command=\\"bower install\\" /&gt;` \\\\- install the bower packages specified in `bower.json` using Bower (which was installed by npm just now).\\n- `&lt;Exec Command=\\"gulp\\" /&gt;` \\\\- do a little dance, make a little love, copy a few files, get down tonight.\\n- `&lt;Exec Command=\\"rimraf ./node_modules\\" /&gt;` \\\\- remove the `node_modules` folder populated by the `npm install` command.\\n\\nWith that addition of `rimraf ./node_modules` to the build phase the problem goes away. During each build a big, big Windows path is being constructed but then it\'s wiped again before it has chance to upset anyone. I\'ve also added the same to the `GulpClean` target.\\n\\nYou are very welcome."},{"id":"Back-to-the-Future-with-Code-First-Migrations","metadata":{"permalink":"/Back-to-the-Future-with-Code-First-Migrations","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2015-06-19-Back-to-the-Future-with-Code-First-Migrations/index.md","source":"@site/blog/2015-06-19-Back-to-the-Future-with-Code-First-Migrations/index.md","title":"Back to the Future with Code First Migrations","description":"Code First Migrations order is determined by file name, not renaming, and requires changing the IMigrationMetadata.Id property to match.","date":"2015-06-19T00:00:00.000Z","tags":[{"inline":false,"label":"SQL Server","permalink":"/tags/sql-server","description":"The SQL Server database."}],"readingTime":2.26,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"Back-to-the-Future-with-Code-First-Migrations","title":"Back to the Future with Code First Migrations","authors":"johnnyreilly","tags":["sql server"],"hide_table_of_contents":false,"description":"Code First Migrations order is determined by file name, not renaming, and requires changing the IMigrationMetadata.Id property to match."},"unlisted":false,"prevItem":{"title":"npm please stop hurting Visual Studio","permalink":"/npm-please-stop-hurting-visual-studio"},"nextItem":{"title":"Angular UI Bootstrap Datepicker Weirdness","permalink":"/angular-ui-bootstrap-datepicker-weirdness"}},"content":"Code First Migrations. They look a little like this in Visual Studio:\\n\\n![](Migrations.png)\\n\\n\x3c!--truncate--\x3e\\n\\nThe thing I want you to notice about the image above is not the pithily named migrations. It isn\'t the natty opacity on everything but the migration files (which I can assure you took me to the very limits of my [GIMP](http://www.gimp.org/) expertise). No, whilst exciting in themselves what I want you to think about is _the order in which migrations are applied_. Essentially how the `__MigrationHistory` table in SQL Server ends up being populated in this manner:\\n\\n![](MigrationHistory.webp)\\n\\nBecause, myself, I didn\'t really think about this until it came time for me to try and change the ordering of some migrations manually. Do you know how migrations end up the order they do? I bet you don\'t. But either way, let\'s watch and see what happens to the pre-enlightenment me as I attempt to take a migration which appears _before_ a migration I have created locally and move it to _after_ that same migration.\\n\\n## Great Scott! It\'s clearly filename driven\\n\\nThat\'s right - it\'s blindingly obvious to me. All I need do is take the migration I want to move forwards in time and rename it in Visual Studio. So take our old migration (\\"2014 is so pass\xe9 darling\\"):\\n\\n![](Screenshot-2015-06-19-13.07.50.png)\\n\\nAnd rename it to make it new and shiny (\\"2015! Gorgeous - I love it sweetie!\\"):\\n\\n![](Screenshot-2015-06-19-13.08.46.png)\\n\\nPerfection right? Wrong! What you\'ve done makes not the slightest jot of difference.\\n\\n## Whoa, this is heavy! Gimme the project file\\n\\nHow could I be so dim? I mean it makes perfect sense - before the days of [TypeScript\'s `tsconfig.json`](../2015-02-27-hey-tsconfigjson-where-have-you-been/index.md) the default ordering of `*.ts` files being passed to the TypeScript compiler was determined by the ordering of the `*.ts` files in the `.csproj` file. It must be the same for Code First Migrations.\\n\\nSo, simply spin up [Notepad++](https://notepad-plus-plus.org/) and let\'s play hack the XML until each file is referenced in the required order.\\n\\nWell, I\'m glad we sorted that out. A quick test to reassure myself of my astuteness. Drum roll.... Fail!! Things are just as they were. Shame on you John Reilly, shame on you.\\n\\n## Designer.cs... Your kids are gonna love it\\n\\n![](Screenshot-2015-06-19-13.35.40.webp)\\n\\nI want you to look very carefully at this and tell me what you see. We\'re looking at the mysterious `201508121401253_AddSagacityToSage.Designer.cs` file that sits underneath the main `201508121401253_AddSagacityToSage.cs` file. What could it be.... Give in?\\n\\nThe `IMigrationMetadata.Id` property is returning `<u>201408121401253</u>_AddSagacityToSage`. That is the _old_ date! Remember? The pass\xe9 one. If you change that property to line up with the file name change you\'re done. It works.\\n\\n![](where-were-going.webp)\\n\\nLet\'s say it together: \\"Automatic Migrations? Where we\'re going, we don\'t need Automatic Migrations.\\""},{"id":"angular-ui-bootstrap-datepicker-weirdness","metadata":{"permalink":"/angular-ui-bootstrap-datepicker-weirdness","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2015-05-23-angular-ui-bootstrap-datepicker-weirdness/index.md","source":"@site/blog/2015-05-23-angular-ui-bootstrap-datepicker-weirdness/index.md","title":"Angular UI Bootstrap Datepicker Weirdness","description":"Add a calendar glyph to your Angular UI Bootstrap Datepicker popup by passing along $event and calling stopPropagation() to avoid an issue.","date":"2015-05-23T00:00:00.000Z","tags":[{"inline":false,"label":"Angular JS","permalink":"/tags/angularjs","description":"The original Angular JS framework."},{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."}],"readingTime":2.485,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"angular-ui-bootstrap-datepicker-weirdness","title":"Angular UI Bootstrap Datepicker Weirdness","authors":"johnnyreilly","tags":["angularjs","javascript"],"hide_table_of_contents":false,"description":"Add a calendar glyph to your Angular UI Bootstrap Datepicker popup by passing along $event and calling stopPropagation() to avoid an issue."},"unlisted":false,"prevItem":{"title":"Back to the Future with Code First Migrations","permalink":"/Back-to-the-Future-with-Code-First-Migrations"},"nextItem":{"title":"NgValidationFor Baby Steps","permalink":"/ngvalidationfor-baby-steps"}},"content":"The [Angular UI Bootstrap Datepicker](https://angular-ui.github.io/bootstrap/#/datepicker) is fan-dabby-dozy. But it has a ... pecularity. You can use the picker like this:\\n\\n\x3c!--truncate--\x3e\\n\\n```html\\n<div ng-app=\\"peskyDatepicker\\">\\n  <div ng-controller=\\"DatepickerDemoCtrl as vm\\">\\n    <input\\n      type=\\"text\\"\\n      class=\\"form-control\\"\\n      datepicker-popup=\\"mediumDate\\"\\n      is-open=\\"vm.valuationDatePickerIsOpen\\"\\n      ng-click=\\"vm.valuationDatePickerOpen()\\"\\n      ng-model=\\"vm.valuationDate\\"\\n    />\\n  </div>\\n</div>\\n```\\n\\n```js\\nangular\\n  .module(\'peskyDatepicker\', [\'ui.bootstrap\'])\\n  .controller(\'DatepickerDemoCtrl\', [\\n    function () {\\n      var vm = this;\\n\\n      vm.valuationDate = new Date();\\n      vm.valuationDatePickerIsOpen = false;\\n\\n      vm.valuationDatePickerOpen = function () {\\n        this.valuationDatePickerIsOpen = true;\\n      };\\n    },\\n  ]);\\n```\\n\\nThe above code produces a textbox which, when clicked upon, renders the datepicker popup (which vanishes upon date selection). This works because the `ng-click` directive calls the `valuationDatePickerOpen` function on the controller which sets the `valuationDatePickerIsOpen` property to be `true` and that property happens to be bound to the `is-open` attribute. Your knee bone connected to your thigh bone, Your thigh bone connected to your hip bone... This makes sense. This works. Great.\\n\\nBut I want something a little prettier - I want to use the lovely calendar glyph to trigger the datepicker popup like in the docs. That should be really easy right? I just tweak the HTML to add a calendar button and the associated `ng-click=\\"vm.valuationDatePickerOpen()\\"`:\\n\\n```html\\n<div ng-app=\\"peskyDatepicker\\">\\n  <div ng-controller=\\"DatepickerDemoCtrl as vm\\">\\n    <p class=\\"input-group\\">\\n      <input\\n        type=\\"text\\"\\n        class=\\"form-control\\"\\n        datepicker-popup=\\"mediumDate\\"\\n        is-open=\\"vm.valuationDatePickerIsOpen\\"\\n        ng-click=\\"vm.valuationDatePickerOpen()\\"\\n        ng-model=\\"vm.valuationDate\\"\\n      />\\n      <span class=\\"input-group-btn\\">\\n        <button\\n          type=\\"button\\"\\n          class=\\"btn btn-default\\"\\n          ng-click=\\"vm.valuationDatePickerOpen()\\"\\n        >\\n          <i class=\\"glyphicon glyphicon-calendar\\"></i>\\n        </button>\\n      </span>\\n    </p>\\n  </div>\\n</div>\\n```\\n\\nMiraculously, this _doesn\'t_ work. Which is strange - I mean it ought to... The same `ng-click` directive is sat on our new calendar button as is in place on the datepicker itself. So what\'s happening? Well let\'s do some investigation. If you take a look at the docs you\'ll see that their example with the calendar glyph is subtly different to our own. Namely, when the opener function is invoked, the official docs pass along `$event`. To what end? Well, the docs opener function does something that our own does not. This:\\n\\n```js\\n$scope.open = function ($event) {\\n  $event.preventDefault();\\n  $event.stopPropagation();\\n\\n  $scope.opened = true;\\n};\\n```\\n\\nIgnore all the `$scope` malarkey - I want you to pay attention to what is happening with `$event`. `preventDefault` and `stopPropogation` are being called. This is probably relevant.\\n\\nI decided to do a little experimentation. I created a Plunk which demonstrates the datepicker and uses `$watch` to track what happens to `valuationDatePickerIsOpen`. The Plunk featured 2 calendar glyphs - the left one doesn\'t pass along `$event` to `valuationDatePickerOpen` when it is clicked and the right one does. When `$event` is passed we call `preventDefault` and `stopPropogation`.\\n\\n<iframe src=\\"https://embed.plnkr.co/dJyF531w0QRGiAScRf15/preview\\" width=\\"100%\\" height=\\"450\\"></iframe>\\n\\nAfter a little experimentation of my own I discovered that calling `$event.stopPropogation()` is the magic bullet. Without that in place `valuationDatePickerIsOpen` gets set to `true` and then immediately back to `false` again. I do not know why. There may be an entirely sane reason for this - if so then please do post a comment and let me know. It wouldn\'t hurt for the Angular UI Bootstrap Datepicker docs to mention this. [Perhaps it\'s time to submit a PR....](https://github.com/angular-ui/bootstrap/issues/3705)"},{"id":"ngvalidationfor-baby-steps","metadata":{"permalink":"/ngvalidationfor-baby-steps","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2015-05-11-ngvalidationfor-baby-steps/index.md","source":"@site/blog/2015-05-11-ngvalidationfor-baby-steps/index.md","title":"NgValidationFor Baby Steps","description":"The NgValidationFor project translates data annotations to Angular validation directive attributes while minimising dependencies.","date":"2015-05-11T00:00:00.000Z","tags":[{"inline":false,"label":"Angular JS","permalink":"/tags/angularjs","description":"The original Angular JS framework."},{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":3.385,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"ngvalidationfor-baby-steps","title":"NgValidationFor Baby Steps","authors":"johnnyreilly","tags":["angularjs","asp.net"],"hide_table_of_contents":false,"description":"The NgValidationFor project translates data annotations to Angular validation directive attributes while minimising dependencies."},"unlisted":false,"prevItem":{"title":"Angular UI Bootstrap Datepicker Weirdness","permalink":"/angular-ui-bootstrap-datepicker-weirdness"},"nextItem":{"title":"A tale of Angular, html5mode, ASP.Net MVC and ASP.Net Web API","permalink":"/a-tale-of-angular-html5mode-aspnet-mvc"}},"content":"I thought as I start the [NgValidationFor project](../2015-04-24-tonight-ill-start-open-source-project/index.md) I\'d journal my progress. I\'m writing this with someone particular in mind: me. Specifically, me in 2 years who will no doubt wonder why I made some of the choices I did. Everyone else, move along now - nothing to see. Unless the inner workings of someone else\'s mind are interesting to you... In which case: welcome!\\n\\n\x3c!--truncate--\x3e\\n\\n## Getting up and running\\n\\nI\'ve got a project on [GitHub](https://github.com/johnnyreilly/NgValidationFor) and I\'m starting to think about implementations. One thing that bit me on [jVUN](http://johnnyreilly.github.io/jQuery.Validation.Unobtrusive.Native/) was being tied to a specific version of ASP.Net MVC. For each major release of ASP.Net MVC I needed separate builds / NuGet packages and the like. A pain. Particularly when it came to bug fixes for prior versions - the breaking changes with each version of MVC meant far more work was required when it came to shipping fixes for MVC 4 / MVC 3.\\n\\nSo with that in mind I\'m going to try and limit my dependencies. I\'m not saying I will never depend upon ASP.Net MVC - I may if I think it becomes useful to give the users a nicer API or if there\'s another compelling reason. But to start with I\'m just going to focus on the translation of data annotations to Angular validation directive attributes.\\n\\nTo that end I\'m going to begin with just a class library and an associated test project. I\'m going to try and minimise the dependencies that NgValidationFor has. At least initially I may even see if I can sensibly avoid depending on `System.Web` (mindful of the upcoming ASP.Net 5 changes). Let\'s see.\\n\\nA little time passes.......\\n\\n## So what have we got?\\n\\nMy first efforts have resulted in the implementation of the `<a href=\\"https://msdn.microsoft.com/en-us/library/system.componentmodel.dataannotations.requiredattribute(v=vs.110).aspx\\">RequiredAttribute</a>`. This is the [code right now](https://github.com/johnnyreilly/NgValidationFor/tree/6cf862a7638d3ed933cd0e075a1807b1414847da). It\'s made up of:\\n\\n1. NgValidationFor.Core - the core part of the project which converts data annotations into AngularJS 1.x validation directive attributes.\\n2. NgValidationFor.Core.UnitTests - the unit tests for the core\\n3. NgValidationFor.Documentation - this is an ASP.Net MVC project which will become a documentation site for NgValidationFor. It also doubles as a way for me to try out NgValidationFor.\\n4. NgValidationFor.Documentation.UnitTests - unit tests for the documentation (there\'s none yet as I\'m still spiking - but when I\'m a little clearer, they will be)\\n\\nHow can it be used? Well fairly easily. Take this simple model:\\n\\n```cs\\nusing System.ComponentModel.DataAnnotations;\\n\\nnamespace NgValidationFor.Documentation.Models\\n{\\n    public class RequiredDemoModel\\n    {\\n        [Required]\\n        public string RequiredField { get; set; }\\n    }\\n}\\n```\\n\\nWhen used in an MVC View for which `RequiredDemoModel` is the Model, NgValiditionFor can be used thusly:\\n\\n```html\\n@using NgValidationFor.Core @using NgValidationFor.Documentation.Models @model\\nRequiredDemoModel\\n<input\\n  type=\\"text\\"\\n  name=\\"userName\\"\\n  ng-model=\\"user.name\\"\\n  @Html.Raw(Model.GetAttributes(x=\\"\\"\\n/>\\nModel.RequiredField)) >\\n```\\n\\nWhich results in this HTML:\\n\\n```html\\n<input type=\\"text\\" name=\\"userName\\" ng-model=\\"user.name\\" required=\\"required\\" />\\n```\\n\\nTada!!!! It works.\\n\\n## So what now?\\n\\nYes it works, but I\'m not going to pretend it\'s pretty. I don\'t like having to wrap the usage of NgValidationFor with `Html.Raw(...)`. I\'m having to do that because `GetAttributes` returns a `string`. This string is then HTML encoded by MVC. To avoid my quotation marks turning into `&amp;quot;` I need to actually be exposing an `<a href=\\"https://msdn.microsoft.com/en-us/library/system.web.ihtmlstring(v=vs.110).aspx\\">IHtmlString</a>`. So I\'m going to need to depend upon `System.Web`. That\'s not so bad - at least I\'m not tied to a specific MVC version.\\n\\nI\'m not too keen on the implementation I\'ve come up with for NgValidationFor either. It\'s a single static method at the minute which does everything. It breaks the [Single Responsibility Priniciple](https://en.wikipedia.org/wiki/Single_responsibility_principle) and the [Open/Closed Principle](https://en.wikipedia.org/wiki/Open/closed_principle). I need to take a look at that - I want people to be able to extend this and I need to think about a good and simple way to achieve that.\\n\\nFinally, usage. `Model.GetAttributes(x =&gt; Model.RequiredField)` feels wrong to me. I think I\'m happy with having this used as an extension method but it needs to be clearer what\'s happening. Perhaps `Model.NgValidationFor(x =&gt; Model.RequiredField)` would be better. I need to try a few things out and come up with a nicer way to use NgValidationFor."},{"id":"a-tale-of-angular-html5mode-aspnet-mvc","metadata":{"permalink":"/a-tale-of-angular-html5mode-aspnet-mvc","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2015-05-05-a-tale-of-angular-html5mode-aspnet-mvc/index.md","source":"@site/blog/2015-05-05-a-tale-of-angular-html5mode-aspnet-mvc/index.md","title":"A tale of Angular, html5mode, ASP.Net MVC and ASP.Net Web API","description":"This article offers tips on how to preserve specific routes while redirecting non-specified URLs to the root angular app page for ASP.Net MVC and Web API.","date":"2015-05-05T00:00:00.000Z","tags":[{"inline":false,"label":"Angular JS","permalink":"/tags/angularjs","description":"The original Angular JS framework."},{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":2.95,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"a-tale-of-angular-html5mode-aspnet-mvc","title":"A tale of Angular, html5mode, ASP.Net MVC and ASP.Net Web API","authors":"johnnyreilly","tags":["angularjs","asp.net"],"hide_table_of_contents":false,"description":"This article offers tips on how to preserve specific routes while redirecting non-specified URLs to the root angular app page for ASP.Net MVC and Web API."},"unlisted":false,"prevItem":{"title":"NgValidationFor Baby Steps","permalink":"/ngvalidationfor-baby-steps"},"nextItem":{"title":"Tonight I\'ll Start an Open Source Project...","permalink":"/tonight-ill-start-open-source-project"}},"content":"So. You want to kick hash based routing to the kerb. You want _real_ URLs. You\'ve read the HTML5 mode section of the [Angular $location docs](https://docs.angularjs.org/guide/$location) and you\'re good to go. It\'s just a matter of dropping `$locationProvider.html5Mode(true)` into your app initialisation right?\\n\\nWrong.\\n\\n\x3c!--truncate--\x3e\\n\\nYou want your URLs to be shareable. If, when you copy the URL out of your browser and send it someone else, they do not get taken to the same position in the application as you do then I\'ve got news for you: THAT\'S NOT REALLY A URL. And just using `$locationProvider.html5Mode(true)` has done nothing useful for you. You want to ensure that, if the URL entered in the browser does not relate to a specific server-side end-point, the self-same HTML root page is _always_ served up. Then Angular can load the correct resources for the URL you have entered and get you to the required state.\\n\\nThere are tips to be found in Angular UI\'s [How to: Configure your server to work with html5Mode](https://github.com/angular-ui/ui-router/wiki/Frequently-Asked-Questions#how-to-configure-your-server-to-work-with-html5mode) doc. However they required a little extra fiddling to get my ASP.Net back end working quite as I wanted. To save you pain, here are my cultural learnings.\\n\\n## ASP.Net MVC\\n\\nI had an ASP.Net MVC app which I wanted to use `html5mode` with. To do this is simply a matter of tweaking your `RouteConfig.cs` like so:\\n\\n```cs\\npublic class RouteConfig\\n    {\\n        public static void RegisterRoutes(RouteCollection routes)\\n        {\\n            routes.IgnoreRoute(\\"{resource}.axd/{*pathInfo}\\");\\n\\n            // Here go the routes that you still want to be able to hit\\n            routes.MapRoute(\\n                name: \\"IAmARouteThatYouStillWantToHit\\",\\n                url: \\"ThatsWhyIAmRegisteredFirst\\",\\n                defaults: new { controller = \\"Hittable\\", action = \\"Index\\" }\\n            );\\n\\n            // Everything else will hit Home/Index which serves up the root angular app page\\n            routes.MapRoute(\\n                name: \\"Default\\",\\n                url: \\"{*anything}\\", // THIS IS THE MAGIC!!!!\\n                defaults: new { controller = \\"Home\\", action = \\"Index\\" }\\n            );\\n        }\\n```\\n\\nWith this in place my existing routes work just as I would hope. Any route that doesn\'t fit that registered can be assumed to be `html5mode` related and will serve up the root angular app page as I\'d hope.\\n\\n## ASP.Net Web API\\n\\nLater I realised that the app in question was mostly static content. Certainly the root angular app page was and so it seemed wasteful to require an ASP.Net MVC controller to serve up that static content. So I stripped out MVC from the app entirely, choosing to serve raw HTML instead. For the dynamic parts I switched to using Web API. This was \\"hittable\\" as long as I had my `WebApiConfig.cs` and my `system.webServer` section in my `web.config` lined up correctly, viz:\\n\\n```cs\\npublic static class WebApiConfig\\n    {\\n        public static void Register(HttpConfiguration config)\\n        {\\n            // Web API routes\\n            config.MapHttpAttributeRoutes();\\n\\n            config.Routes.MapHttpRoute(\\n                name: \\"DefaultApi\\",\\n                routeTemplate: \\"api/{controller}/{id}\\",\\n                defaults: new { id = RouteParameter.Optional }\\n            );\\n\\n            // other stuff\\n        }\\n    }\\n```\\n\\n```xml\\n<configuration>\\n\\n    <system.webServer>\\n\\n        <defaultDocument>\\n            <files>\\n                <clear />\\n                <add value=\\"build/index.html\\" /> \x3c!-- This is the root document for the Angular app --\x3e\\n            </files>\\n        </defaultDocument>\\n\\n        <rewrite>\\n            <rules>\\n                <rule name=\\"Main Rule\\" stopProcessing=\\"true\\">\\n                    <match url=\\".*\\" />\\n                    <conditions logicalGrouping=\\"MatchAll\\">\\n                        \x3c!-- Allows \\"api/\\" prefixed URLs to still hit Web API controllers\\n                             as defined in WebApiConfig --\x3e\\n                        <add input=\\"{REQUEST_URI}\\" pattern=\\"api/\\" ignoreCase=\\"true\\" negate=\\"true\\" />\\n\\n                        \x3c!-- Static files and directories can be served so partials etc can be loaded --\x3e\\n                        <add input=\\"{REQUEST_FILENAME}\\" matchType=\\"IsFile\\" negate=\\"true\\" />\\n                        <add input=\\"{REQUEST_FILENAME}\\" matchType=\\"IsDirectory\\" negate=\\"true\\" />\\n                    </conditions>\\n                    <action type=\\"Rewrite\\" url=\\"/\\" />\\n                </rule>\\n            </rules>\\n        </rewrite>\\n\\n    </system.webServer>\\n\\n</configuration>\\n```\\n\\nWith this in place I can happily hit \\"api\\" prefixed URLs and still land on my Web API controllers whilst other URLs will serve up the root angular app page. Lovely."},{"id":"tonight-ill-start-open-source-project","metadata":{"permalink":"/tonight-ill-start-open-source-project","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2015-04-24-tonight-ill-start-open-source-project/index.md","source":"@site/blog/2015-04-24-tonight-ill-start-open-source-project/index.md","title":"Tonight I\'ll Start an Open Source Project...","description":"A new AngularJS validation mechanism aims to propagate data annotations on ASP.NET MVC server models into ng-* directive attributes in HTML.","date":"2015-04-24T00:00:00.000Z","tags":[{"inline":false,"label":"Angular JS","permalink":"/tags/angularjs","description":"The original Angular JS framework."},{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":5.075,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"tonight-ill-start-open-source-project","title":"Tonight I\'ll Start an Open Source Project...","authors":"johnnyreilly","tags":["angularjs","asp.net"],"hide_table_of_contents":false,"description":"A new AngularJS validation mechanism aims to propagate data annotations on ASP.NET MVC server models into ng-* directive attributes in HTML."},"unlisted":false,"prevItem":{"title":"A tale of Angular, html5mode, ASP.Net MVC and ASP.Net Web API","permalink":"/a-tale-of-angular-html5mode-aspnet-mvc"},"nextItem":{"title":"How to activate your emoji keyboard on Android 5.0 (Lollipop)","permalink":"/how-to-activate-your-emoji-keyboard-on-android"}},"content":"### Further posts on this topic\\n\\n\x3c!--truncate--\x3e\\n\\n- [NgValidationFor Baby Steps](../2015-05-11-ngvalidationfor-baby-steps/index.md)\\n\\nI\'m excited. Are you? I\'m babysitting for a friend, I\'ve my laptop, time to kill and (crucially) an idea...\\n\\n## The Idea\\n\\nYou\'re likely aware of the various form element directives that AngularJS offers. For instance the [input directive](https://docs.angularjs.org/api/ng/directive/input):\\n\\n> HTML input element control. When used together with ngModel, it provides data-binding, input state control, and _validation_.\\n\\nYou\'ll notice that I emphasised the word \\"validation\\" there. That\'s important - that\'s my idea. I\'m using AngularJS to build SPA\'s and for the server side I\'m using ASP.Net MVC / Web API. Crucially, my templates are actually ASP.Net MVC Partial Views. That\'s key.\\n\\nWhen I send data back from my SPA back to the server it gets unmarshalled / deserialized into a C# class (view model) of some kind. When data goes the other way it\'s flowing back from a JSON\'d view model and being used by my Angular code.\\n\\nNow historically if I was building a fairly vanilla MVC app then I\'d be making use of all the `TextboxFor` extension methods etc to generate my input elements. For example, with a view model like this:\\n\\n```cs\\nusing System.ComponentModel.DataAnnotations;\\n\\nnamespace App.ViewModels\\n{\\n public class RequiredModel\\n {\\n  [Required]\\n  public string RequiredField{ get; set; }\\n }\\n}\\n```\\n\\nI\'d have a view like this:\\n\\n```html\\n@model App.ViewModels.RequiredModel @using (Html.BeginForm()) {\\n<div class=\\"row\\">\\n  @Html.LabelFor(x => x.TextBox, \\"Something must be entered:\\")\\n  @Html.TextBoxFor(x => x.TextBox, true)\\n</div>\\n}\\n```\\n\\nAnd that would generate HTML like this:\\n\\n```html\\n<form action=\\"/Demo/Required\\" method=\\"post\\">\\n  <div class=\\"row\\">\\n    <label for=\\"TextBox\\">Something must be entered:</label>\\n    <input\\n      data-msg-required=\\"The TextBox field is required.\\"\\n      data-rule-required=\\"true\\"\\n      id=\\"TextBox\\"\\n      name=\\"TextBox\\"\\n      type=\\"text\\"\\n      value=\\"\\"\\n    />\\n  </div>\\n</form>\\n```\\n\\nIf you look at the HTML you\'ll see that the `Required` data annotations have been propogated into the HTML in the HTML in the form of `data-rule-*` and `data-msg-*` attributes. The code above is built using my [jQuery.Validation.Unobtrusive.Native project](http://johnnyreilly.github.io/jQuery.Validation.Unobtrusive.Native/) which in turn was inspired by / based upon the [Unobtrusive Client Validation in ASP.NET MVC](http://bradwilson.typepad.com/blog/2010/10/mvc3-unobtrusive-validation.html). That\'s right - I\'ve done this before - or at least something quite like it.\\n\\nThere\'s clearly a strong crossover between AngularJS\'s input directive parameters and unobtrusive client validation. I\'m planning to take the principles (and maybe some of the code) that I used on that project and see if I can\'t make something useful with it here. [Server side validation is jolly important](../2014-08-01-angularjs-meet-aspnet-server-validation/index.md) but I can probably save a few compute cycles on the server by making use of client side validation as well. If I\'m right then I should able to come up with a mechanism that saves me from manually duplicating my server validation on the client.\\n\\n## The Aim\\n\\nI want to be able to use HTML Helpers to propogate validation metadata from the server view models into angular form validation directive attributes. Quite a mouthful I know. What does that actually mean? Well I\'ve got 2 ideas. Possibly I want to be able to code something like this:\\n\\n```html\\n@model App.ViewModels.RequiredModel @using (Html.BeginForm()) {\\n<div class=\\"row\\">\\n  @Html.LabelFor(x => x.TextBox, \\"Something must be entered:\\")\\n  @Html.NgTextBoxFor(x => x.TextBox)\\n</div>\\n}\\n```\\n\\nAnd have HTML like this generated:\\n\\n```html\\n<form action=\\"/Demo/Required\\" method=\\"post\\">\\n  <div class=\\"row\\">\\n    <label for=\\"TextBox\\">Something must be entered:</label>\\n    <input\\n      ng-required=\\"true\\"\\n      id=\\"TextBox\\"\\n      name=\\"TextBox\\"\\n      type=\\"text\\"\\n      value=\\"\\"\\n    />\\n  </div>\\n</form>\\n```\\n\\nThe reservation I have about this approach is that it rather takes you away from the HTML. Yes it works (and to your seasoned MVC-er it will feel quite natural in some ways) but it feels rather heavy handed. But I\'d like what I\'m building to be easy for users to plug into existing code without a ton of rework. So, the other idea I\'m toying with is having HTML helpers that just return a string of attributes. So if I had an angular form that looked like this:\\n\\n```html\\n<div ng-controller=\\"ExampleController\\">\\n  <form>\\n    <div class=\\"row\\">\\n      <label\\n        >Something must be entered:\\n        <input name=\\"RequiredField\\" type=\\"text\\" value=\\"\\" />\\n      </label>\\n    </div>\\n  </form>\\n</div>\\n```\\n\\nI could tweak it to push in the validation directive attributes like this:\\n\\n```html\\n@model App.ViewModels.RequiredModel\\n<div ng-controller=\\"ExampleController\\">\\n  <form>\\n    <div class=\\"row\\">\\n      <label\\n        >Something must be entered:\\n        <input\\n          name=\\"RequiredField\\"\\n          type=\\"text\\"\\n          value=\\"\\"\\n          @Html.NgValidationFor(x=\\"\\"\\n        />\\n        x.RequiredField) />\\n      </label>\\n    </div>\\n  </form>\\n</div>\\n```\\n\\nAnd end up with HTML like this:\\n\\n```html\\n<div ng-controller=\\"ExampleController\\">\\n  <form>\\n    <div class=\\"row\\">\\n      <label\\n        >Something must be entered:\\n        <input name=\\"RequiredField\\" type=\\"text\\" value=\\"\\" ng-required=\\"true\\" />\\n      </label>\\n    </div>\\n  </form>\\n</div>\\n```\\n\\nThis is a simplified example of course - it\'s likely that any number of validation directive attributes might be returned from `NgValidationFor`. And crucially if these attributes were changed on the server view model then the validation changes would automatically end up in the client HTML with this approach.\\n\\n## The Approach\\n\\nAt least to start off with I\'m going to aim at creating the second of my approaches. I may come back and implement the first at some point but I think the second is a better place to start.\\n\\nI\'m kind of surprised no-one else has built this already actually - but I\'m not aware of anything. I\'ve had a little duckduckgo around and found no takers. The closest I\'ve come is the excellent [BreezeJS](http://www.breezejs.com/sites/all/apidocs/classes/Validator.html). BreezeJS does way more than I want it to - I\'m planning to restrict the scope of this project to simply turning data annotations on my ASP.Net MVC server models into `ng-*` directive attributes in HTML. That\'s it.\\n\\nSo, general housekeeping.... I\'m going to host this project on [GitHub](http://www.github.com), I\'m going to have Continuous Integration with [AppVeyor](http://www.appveyor.com/) and I\'m planning to publish this via [NuGet](http://www.nuget.org/) (when and if I\'ve created something useful).\\n\\nI just need a name and I\'ll begin. What shall I call it? Some options:\\n\\n- Angular ASP.Net MVC Extensions\\n- angular-aspnet-mvc-extensions\\n- Angular MVC Element Extensions\\n- Angular Validation Html Helpers\\n- NgValidationFor (the name of the HTML helper I made up)\\n\\nHmmmm.... None of them is particularly lighting my fire. The first four are all a bit [RonSeal](https://en.wikipedia.org/wiki/Ronseal) \\\\- which is fine.... Ug. The last one... It\'s a bit more pithy. Okay - I\'ll go with \\"NgValidationFor\\" at least for now. If something better occurs I can always change my mind.\\n\\n[And we\'re off!](https://github.com/johnnyreilly/NgValidationFor)"},{"id":"how-to-activate-your-emoji-keyboard-on-android","metadata":{"permalink":"/how-to-activate-your-emoji-keyboard-on-android","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2015-04-17-how-to-activate-your-emoji-keyboard-on-android/index.md","source":"@site/blog/2015-04-17-how-to-activate-your-emoji-keyboard-on-android/index.md","title":"How to activate your emoji keyboard on Android 5.0 (Lollipop)","description":"Learn how to get emoji on your Android phone by activating the \\"iWnn IME Japanese\\" keyboard and selecting the \\"Emoji\\" option.","date":"2015-04-17T00:00:00.000Z","tags":[],"readingTime":0.895,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"how-to-activate-your-emoji-keyboard-on-android","title":"How to activate your emoji keyboard on Android 5.0 (Lollipop)","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false,"description":"Learn how to get emoji on your Android phone by activating the \\"iWnn IME Japanese\\" keyboard and selecting the \\"Emoji\\" option."},"unlisted":false,"prevItem":{"title":"Tonight I\'ll Start an Open Source Project...","permalink":"/tonight-ill-start-open-source-project"},"nextItem":{"title":"PartialView.ToString()","permalink":"/partialview-tostring"}},"content":"A departure from from my normal content - I need to tell you about [emoji](http://en.wikipedia.org/wiki/Emoji)! You\'ll probably already know about them - just imagine a emoticon but about 300,000 times better. They really add spice to to textual content. Oh and they\'re Japanese - which is also way cool.\\n\\n\x3c!--truncate--\x3e\\n\\nSince I\'ve discovered emoji I\'ve felt a pressing need to have them on my (Android) phone. This is harder than you might imagine. But totally do-able.... Here\'s how you get the emoji love on your Android Lollipop phone:\\n\\n- goto settings (the cog)\\n- select \\"Language and Input\\"\\n- select your \\"Current keyboard\\" and then select the \\"Choose keyboards\\" option\\n- look for a keyboard that says \\"iWnn IME Japanese\\". Select it\\n- drop back to the \\"Language and Input\\" menu where you will see \\"iWnn IME Japanese\\" is now there.\\n- select it and deactivate \\"Japanese\\" and activate \\"Emoji\\" like this:\\n\\n![screenshot of input languages in android](screenshot_input_languages.png)\\n\\n- now you should find your keyboard contains a little globe icon. When you select it.... Emoji!!!!\\n\\n![screenshot of emoji keyboard](screenshot_emoji.jpg)"},{"id":"partialview-tostring","metadata":{"permalink":"/partialview-tostring","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2015-03-20-partialview-tostring/index.md","source":"@site/blog/2015-03-20-partialview-tostring/index.md","title":"PartialView.ToString()","description":"Learn three ways to turn a `PartialViewResult` into a `string` to reuse the result returned by a controller in a JSON payload.","date":"2015-03-20T00:00:00.000Z","tags":[{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":3.71,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"partialview-tostring","title":"PartialView.ToString()","authors":"johnnyreilly","tags":["asp.net"],"hide_table_of_contents":false,"description":"Learn three ways to turn a `PartialViewResult` into a `string` to reuse the result returned by a controller in a JSON payload."},"unlisted":false,"prevItem":{"title":"How to activate your emoji keyboard on Android 5.0 (Lollipop)","permalink":"/how-to-activate-your-emoji-keyboard-on-android"},"nextItem":{"title":"Hey tsconfig.json, where have you been all my life?","permalink":"/hey-tsconfigjson-where-have-you-been"}},"content":"In the name of [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself) I found myself puzzling how one could take a `PartialViewResult` and render it as a `string`. Simple, right?\\n\\n\x3c!--truncate--\x3e\\n\\nIn fact, in my head this was already a solved problem. I mean I\'ve written about this [before](../2012-07-16-rendering-partial-view-to-string/index.md) already! Except I haven\'t. Not really - what I did back then was link to what someone else had written and say \\"yay! well done chap - like he said!\\". It turns out that was a bad move. That blog appears to be gone and so I\'m back to where I was. Ug. Lesson learned.\\n\\n## What are we trying to do?\\n\\nSo, for the second time of asking, here is how to take a `PartialViewResult` and turn it into a `string`. It\'s an invaluable technique to deal with certain scenarios.\\n\\nIn my own case I have a toolbar in my application that is first pushed into the UI in my `_Layout.cshtml` by means of a trusty `@Html.Action(\\"Toolbar\\")`. I wanted to be able to re-use the `PartialViewResult` returned by `Toolbar` on my controller inside a `JSON` payload. And despite the title of this post, `PartialView.ToString()`_doesn\'t_ quite cut the mustard. Obvious really, if it did then why would I be writing this and you be reading this?\\n\\nThe solution is actually fairly simple. And, purely for swank, I\'m going to offer it you 3 ways. Whatever\'s your poison.\\n\\n## Inheritance (it\'s so yesterday darling)\\n\\nYes there was a time when everything was inheritance based. You were rewarded handsomely for making sure that was the case. However, times have changed and (with good reason) people tend to [favour composition over inheritance](https://en.wikipedia.org/wiki/Composition_over_inheritance). So, perhaps just for the memories, let first offer you the inheritance based approach:\\n\\n```cs\\nprotected string ConvertPartialViewToString(PartialViewResult partialView)\\n{\\n  using (var sw = new StringWriter())\\n  {\\n    partialView.View = ViewEngines.Engines\\n      .FindPartialView(ControllerContext, partialView.ViewName).View;\\n\\n    var vc = new ViewContext(\\n      ControllerContext, partialView.View, partialView.ViewData, partialView.TempData, sw);\\n    partialView.View.Render(vc, sw);\\n\\n    var partialViewString = sw.GetStringBuilder().ToString();\\n\\n    return partialViewString;\\n  }\\n}\\n```\\n\\nThe idea being that the above method is placed onto a base controller which your controllers subclass. Thus using this method inside one of the controllers is as simple as:\\n\\n```cs\\nvar toolbarHtml = ConvertPartialViewToString(partialViewResult);\\n```\\n\\n## Extension method (sexier syntax)\\n\\nSo the next choice is implementing this as an extension method. Here\'s my static class which adds `ConvertToString` onto `PartialViewResult`:\\n\\n```cs\\nusing System.IO;\\nusing System.Web.Mvc;\\n\\nnamespace My.Utilities.Extensions\\n{\\n  public static class PartialViewResultExtensions\\n  {\\n    public static string ConvertToString(this PartialViewResult partialView,\\n                                              ControllerContext controllerContext)\\n    {\\n      using (var sw = new StringWriter())\\n      {\\n        partialView.View = ViewEngines.Engines\\n          .FindPartialView(controllerContext, partialView.ViewName).View;\\n\\n        var vc = new ViewContext(\\n          controllerContext, partialView.View, partialView.ViewData, partialView.TempData, sw);\\n        partialView.View.Render(vc, sw);\\n\\n        var partialViewString = sw.GetStringBuilder().ToString();\\n\\n        return partialViewString;\\n      }\\n    }\\n  }\\n}\\n```\\n\\nI don\'t know about you but I do love an extension method - it often makes for much more readable code. In this case we can use:\\n\\n```cs\\nvar toolbarHtml = partialViewResult.ConvertToString(ControllerContext);\\n```\\n\\nWhich I think we can all agree is really rather lovely. Perhaps it would be more lovely if I didn\'t have to pass `ControllerContext` \\\\- but hey! Still quite nice.\\n\\n## Favouring Composition over Inheritance (testable)\\n\\nAlthough ASP.Net MVC was designed to be testable there are times when you think \\"really? Can it be that hard?\\". In fact for a well thought through discussion on the topic I advise you read [this](http://volaresystems.com/blog/post/2010/08/19/Dont-mock-HttpContext). (I\'m aware of the irony implicit in linking to another blog post in a blog post that I only wrote because I first linked to another blog which vanished.... Infinite recursion anybody?)\\n\\nThe conclusion of the linked blog post is twofold\\n\\n1. Don\'t mock HTTPContext\\n2. Use the [facade pattern](https://en.wikipedia.org/wiki/Facade_pattern) instead\\n\\nHaving testable code is not a optional bauble in my view - it\'s a necessity. So with my final approach that\'s exactly what I\'ll do.\\n\\n```cs\\nusing System.Web.Mvc;\\n\\nnamespace My.Interfaces\\n{\\n  public interface IMvcInternals\\n  {\\n    string ConvertPartialViewToString(PartialViewResult partialView, ControllerContext controllerContext);\\n  }\\n}\\n\\n// ....\\n\\nusing System.IO;\\nusing System.Web.Mvc;\\nusing My.Interfaces;\\n\\nnamespace My.Utilities\\n{\\n  public class MvcInternals : IMvcInternals\\n  {\\n    public string ConvertPartialViewToString(PartialViewResult partialView,\\n                                             ControllerContext controllerContext)\\n    {\\n      using (var sw = new StringWriter())\\n      {\\n        partialView.View = ViewEngines.Engines\\n          .FindPartialView(controllerContext, partialView.ViewName).View;\\n\\n        var vc = new ViewContext(\\n          controllerContext, partialView.View, partialView.ViewData, partialView.TempData, sw);\\n        partialView.View.Render(vc, sw);\\n\\n        var partialViewString = sw.GetStringBuilder().ToString();\\n\\n        return partialViewString;\\n      }\\n    }\\n  }\\n}\\n```\\n\\nSo here I have a simple interface with a `ConvertPartialViewToString` method on it. This interface can be passed into a controller and then used like this:\\n\\n```cs\\nvar toolbarHtml = _mvcInternals.ConvertPartialViewToString(partialViewResult, ControllerContext);\\n```\\n\\nAh... that\'s the sweet mellifluous sound of easily testable code."},{"id":"hey-tsconfigjson-where-have-you-been","metadata":{"permalink":"/hey-tsconfigjson-where-have-you-been","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2015-02-27-hey-tsconfigjson-where-have-you-been/index.md","source":"@site/blog/2015-02-27-hey-tsconfigjson-where-have-you-been/index.md","title":"Hey tsconfig.json, where have you been all my life?","description":"The creation of a \\"tsconfig.json\\" file will eliminate the need for \\"reference\\" comments when using TypeScript, reducing barriers between IDEs.","date":"2015-02-27T00:00:00.000Z","tags":[{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":4.375,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"hey-tsconfigjson-where-have-you-been","title":"Hey tsconfig.json, where have you been all my life?","authors":"johnnyreilly","tags":["typescript"],"hide_table_of_contents":false,"description":"The creation of a \\"tsconfig.json\\" file will eliminate the need for \\"reference\\" comments when using TypeScript, reducing barriers between IDEs."},"unlisted":false,"prevItem":{"title":"PartialView.ToString()","permalink":"/partialview-tostring"},"nextItem":{"title":"Using Gulp to inject scripts and styles tags directly into your HTML","permalink":"/using-gulp-in-asp-net-instead-of-web-optimization"}},"content":"Sometimes, you just miss things. Something seismic happens and you had no idea. So it was with `tsconfig.json`.\\n\\n\x3c!--truncate--\x3e\\n\\nThis blog post started life with the name \\"TypeScript: Some IDEs are more equal than others\\". I\'d intended to use it summarise a discussion on the [TypeScript GitHub repo](https://github.com/Microsoft/TypeScript/issues/1066) about implicit referencing including a fist shaken at the sky at the injustice of it all. But whilst I was writing it I dicovered things had changed without my knowledge. That\'s a rather wonderful thing.\\n\\n## Implicit Referencing\\n\\nImplicit referencing, if you\'re not aware, is the thing that separates Visual Studio from all other IDEs / text editors. Implicit referencing means that in Visual Studio you don\'t need to make use of comments at the head of each TypeScript file in order to tell the compiler where it can find the related TypeScript files.\\n\\nThe `reference` comments aren\'t necessary when using Visual Studio because the VS project file is used to drive the files passed to the TypeScript compiler (tsc).\\n\\nThe upshot of this is that, at time of writing, you can generally look at a TypeScript codebase and tell whether it was written using Visual Studio by opening it up a file at random and eyeballing for something like this at the top:\\n\\n```ts\\n/// <reference path=\\"other-file.ts\\" />\\n```\\n\\n_\\"A-ha! They\'re using \\"reference\\" comments Watson. From this I deduce that the individuals in question are using the internal module approach and using Visual Studio as their IDE. Elementary, my dear fellow, quite elementary.\\"_\\n\\nThis has important implications. Important I tell you, yes important! Well, important if you want to reduce the barriers between Visual Studio and everyone else. And I do. Whilst I love Visual Studio - it\'s been my daily workhorse for many years - I also love stepping away from it and using something more stripped down. I also like working with other people without mandating that they need to use Visual Studio as well. In the words of Rodney King, \\"can\'t we all get along?\\".\\n\\n## Cross-IDE TypeScript projects\\n\\nI feel I should be clear - you can already set up TypeScript projects to work regardless of IDE. But there\'s friction. It\'s not clear cut. You can see a full on discussion around this [here](https://github.com/Microsoft/TypeScript/issues/1066) but in the end it comes down to making a choice between these 3 options:\\n\\n1. Set `<TypeScriptEnabled>false</TypeScriptEnabled>` in a project file. [This flag effectively deactivates implicit referencing.](https://github.com/Microsoft/TypeScript/issues/1066#issuecomment-63727612) This approach requires that all developers (regardless of IDE) use `/// &lt;reference`s to build context. Compiler options in VS can be controlled using the project file as is.\\n2. Using Visual Studio without any csproj tweaks. This approach requires that all files will need `/// &lt;reference`s at their heads in order to build compilation context _outside_ of Visual Studio. It\'s possible that `/// &lt;reference`s and the csproj could get out of line - care is required to avoid this. Compiler options in VS can be controlled using the project file as is.\\n3. Using just files in Visual Studio with `/// &lt;reference`s to build compilation context. This scenario also requires that all developers (regardless of IDE) use `/// &lt;reference`s to build context. In Visual Studio there will be no control over compiler options.\\n\\nAs you can see - this is sub-optimal. But don\'t worry - there\'s a new sheriff in town....\\n\\n## `tsconfig.json`\\n\\nI\'d decided to give [Atom TypeScript plugin](https://github.com/TypeStrong/atom-typescript) a go as I heard much enthusiastic noise about it. I fired it up and pointed it at a a TypeScript AngularJS project built in Visual Studio. I was mentally preparing myself for the job of adding all the /// references in when I suddenly noticed a file blinking at me:\\n\\n![](Screenshot-2015-02-27-16.05.29.webp)\\n\\n`tsconfig.json`? What\'s that? Time to read [the docs](https://github.com/TypeStrong/atom-typescript#project-support):\\n\\n> Supported via tsconfig.json ([read more](https://github.com/TypeStrong/atom-typescript/blob/master/docs/tsconfig/index.md)) which is going to be the defacto Project file format for the next versions of TypeScript.\\n\\n\\"read more\\"? Oh yes indeedy - I think I will \\"read more\\"!\\n\\n> A unified project format for TypeScript ([see merged PR on Microsoft/TypeScript](https://github.com/Microsoft/TypeScript/pull/1692)). The TypeScript compiler (1.4 and above) only cares about compilerOptions and files. We add additional features to this [with the typescript team\'s approval to extend the file as long as we don\'t conflict:](https://github.com/Microsoft/TypeScript/issues/1955)\\n>\\n> - [compilerOptions](https://github.com/TypeStrong/atom-typescript/blob/e2fa67c4715189b71430f766ed9a92d9fb3255f9/lib/main/tsconfig/tsconfig.ts#L8-L35) similar to what you would pass on the commandline to tsc.\\n> - [filesGlob](https://github.com/TypeStrong/atom-typescript/blob/master/docs/tsconfig/index.md#filesglob): To make it easier for you to just add / remove files in your project we add filesGlob which accepts an array of glob / minimatch / RegExp patterns (similar to grunt)to specify source files.\\n> - [format](https://github.com/TypeStrong/atom-typescript/blob/master/docs/tsconfig/index.md#format): Code formatting options\\n> - [version](https://github.com/TypeStrong/atom-typescript/blob/master/docs/tsconfig/index.md#version): The TypeScript version\\n\\nThat\'s right folks, we don\'t need `/// &lt;reference`s comments anymore. In a blinding flash of light it all changes. We\'re going from the dark end of the street, to the bright side of the road. `tsconfig.json` is here to ease away the pain and make it all better. Let\'s enjoy it while we can.\\n\\nThis change should ship with TypeScript 1.5 (hopefully) for those using Visual Studio. For those using Atom TypeScript (and as of today that\'s includes me) the carnival celebrations can begin now!\\n\\nThanks to [@basarat](https://github.com/basarat) who have quoted at length and [Daniel Earwicker](https://smellegantcode.wordpress.com/) who is the reason that I came to discover `tsconfig.json`."},{"id":"using-gulp-in-asp-net-instead-of-web-optimization","metadata":{"permalink":"/using-gulp-in-asp-net-instead-of-web-optimization","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2015-02-17-using-gulp-in-asp-net-instead-of-web-optimization/index.md","source":"@site/blog/2015-02-17-using-gulp-in-asp-net-instead-of-web-optimization/index.md","title":"Using Gulp to inject scripts and styles tags directly into your HTML","description":"Learn how to use Gulp to directly inject scripts and styles into your HTML, which speeds up app times and makes the setup simpler.","date":"2015-02-17T00:00:00.000Z","tags":[{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."},{"inline":false,"label":"Node.js","permalink":"/tags/node-js","description":"The Node.js runtime."}],"readingTime":10.47,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"using-gulp-in-asp-net-instead-of-web-optimization","title":"Using Gulp to inject scripts and styles tags directly into your HTML","authors":"johnnyreilly","tags":["asp.net","node.js"],"hide_table_of_contents":false,"description":"Learn how to use Gulp to directly inject scripts and styles into your HTML, which speeds up app times and makes the setup simpler."},"unlisted":false,"prevItem":{"title":"Hey tsconfig.json, where have you been all my life?","permalink":"/hey-tsconfigjson-where-have-you-been"},"nextItem":{"title":"The Convent with Continuous Delivery","permalink":"/the-convent-with-continuous-delivery"}},"content":"This is very probably the dullest title for a blog post I\'ve ever come up with. Read on though folks - it\'s definitely going to pick up...\\n\\n\x3c!--truncate--\x3e\\n\\nI [wrote last year](../2014-11-04-using-gulp-in-visual-studio-instead-of-web-optimization/index.md) about my first usage of Gulp in an ASP.Net project. I used Gulp to replace the Web Optimization functionality that is due to disappear when ASP.Net v5 ships. What I came up with was an approach that provided pretty much the same functionality; raw source in debug mode, bundling + minification in release mode.\\n\\nIt worked by having a launch page which was straight HTML. Embedded within this page was JavaScript that would, at runtime, load the required JavaScript / CSS and inject it dynamically into the document. This approach worked but it had a number of downsides:\\n\\n1. Each time you fired up the app the following sequence of events would happen: - jQuery would load (purely there to simplify the making of various startup AJAX calls)\\n\\n- the page would make an AJAX call to the server to load various startup data, including whether the app is running in debug or release mode\\n- Depending on the result of the startup data either the debug or release package manifest would be loaded.\\n- For each entry in the package manifest `script` and `link` tags would be created and added to the document. These would generate further requests to the server to load the resources.\\n\\nQuite a lot going on here isn\'t there? Accordingly, initial startup time was slower than you might hope. 2. The \\"F\\" word: [FOUC](https://en.wikipedia.org/wiki/Flash_of_unstyled_content). Flash Of Unstyled Content - whilst all the hard work of the page load was going on (before the CSS had been loaded) the page would look rather ... bare. Not a terrible thing but none too slick either. 3. The gulpfile built both the debug and the release package each time it was run. This meant the gulp task generally did double the work that it needed to do.\\n\\nI wanted to see if I could tackle these issues. I\'ve recently been watching [John Papa](https://twitter.com/John_Papa)\'s excellent [Pluralsight course on Gulp](http://www.pluralsight.com/courses/javascript-build-automation-gulpjs) and picked up a number of useful tips. With that in hand let\'s see what we can come up with...\\n\\n## Death to dynamic loading\\n\\nThe main issue with the approach I\'ve been using is the dynamic loading. It makes the app slower and more complicated. So the obvious solution is to have my gulpfile inject scripts and css into the template. To that end it\'s [wiredep](https://www.npmjs.com/package/wiredep) & [gulp-inject](https://www.npmjs.com/package/gulp-inject) to the rescue!\\n\\ngulp-inject (as the name suggests) is used to inject `script` and `link` tags into source code. I\'m using [Bower](http://bower.io/) as my client side package manager and so I\'m going to use wiredep to determine the vendor scripts I need. It will determine what packages my app is using from looking at my `bower.json`, and give me a list of file paths in _dependency order_ (which I can then pass on to gulp-inject in combination with my own app script files). This means I don\'t have to think about ordering bower dependencies myself and I no longer need to separately maintain a list of these files within my gulpfile.\\n\\nSo, let\'s get the launch page (`index.html`) ready for gulp-inject:\\n\\n```html\\n<!doctype html>\\n<html>\\n  <head>\\n    <meta http-equiv=\\"X-UA-Compatible\\" content=\\"IE=edge, chrome=1\\" />\\n    <style>\\n      .ng-hide {\\n        display: none !important;\\n      }\\n    </style>\\n    <title ng-bind=\\"title\\">Proverb</title>\\n    <meta charset=\\"utf-8\\" />\\n    <meta\\n      name=\\"viewport\\"\\n      content=\\"width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no\\"\\n    />\\n\\n    \x3c!-- inject:css --\x3e\\n    \x3c!-- endinject --\x3e\\n\\n    <link rel=\\"icon\\" type=\\"image/png\\" href=\\"content/images/icon.png\\" />\\n  </head>\\n  <body>\\n    <div>\\n      <div ng-include=\\"\'app/layout/shell.html\'\\"></div>\\n      <div id=\\"splash-page\\" ng-show=\\"false\\" class=\\"dissolve-animation\\">\\n        <div class=\\"page-splash\\">\\n          <div class=\\"page-splash-message\\">Proverb</div>\\n\\n          <div class=\\"progress\\">\\n            <div\\n              class=\\"progress-bar progress-bar-striped active\\"\\n              role=\\"progressbar\\"\\n              style=\\"width: 20%;\\"\\n            >\\n              <span class=\\"sr-only\\">loading...</span>\\n            </div>\\n          </div>\\n        </div>\\n      </div>\\n    </div>\\n\\n    <script src=\\"https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js\\"><\/script>\\n    <script>\\n      window.jQuery ||\\n        document.write(\'<script src=\\"/build/jquery.min.js\\">\\\\x3C/script>\');\\n    <\/script>\\n\\n    \x3c!-- inject:js --\x3e\\n    \x3c!-- endinject --\x3e\\n\\n    <script>\\n      (function () {\\n        // Load startup data from the server\\n        $.getJSON(\'api/Startup\').done(function (startUpData) {\\n          angularApp.start({\\n            thirdPartyLibs: {\\n              moment: window.moment,\\n              toastr: window.toastr,\\n              underscore: window._,\\n            },\\n            appConfig: startUpData,\\n          });\\n        });\\n      })();\\n    <\/script>\\n  </body>\\n</html>\\n```\\n\\nThe important thing to notice here are the `&lt;!-- inject:css --&gt;` and `&lt;!-- inject:js --&gt;` injection placeholders. It\'s here that our script and style tags will be injected into the template. You\'ll notice that jQuery is _not_ being injected - and that\'s because I\'ve opted to use a CDN for jQuery and then only fallback to serving jQuery myself if the CDN fails.\\n\\nThe other thing to notice here is that our launch page has become oh so much simpler in comparison with the dynamic loading approach. Which is fab.\\n\\nNow before we start looking at our gulpfile I want to split out the configuration into a standalone file called gulpfile.config.js:\\n\\n```js\\nvar tsjsmapjsSuffix = \'.{ts,js.map,js}\';\\n\\nvar bower = \'bower_components/\';\\nvar app = \'app/\';\\n\\nvar config = {\\n  base: \'.\',\\n  buildDir: \'./build/\',\\n  debug: \'debug\',\\n  release: \'release\',\\n  css: \'css\',\\n\\n  bootFile: app + \'index.html\',\\n  bootjQuery: bower + \'jquery/dist/jquery.min.js\',\\n\\n  // The fonts we want Gulp to process\\n  fonts: [bower + \'fontawesome/fonts/*.*\'],\\n\\n  images: \'images/**/*.{gif,jpg,png}\',\\n\\n  // The scripts we want Gulp to process\\n  scripts: [\\n    // Bootstrapping\\n    app + \'app\' + tsjsmapjsSuffix,\\n    app + \'config.route\' + tsjsmapjsSuffix,\\n\\n    // common Modules\\n    app + \'common/common\' + tsjsmapjsSuffix,\\n    app + \'common/logger\' + tsjsmapjsSuffix,\\n    app + \'common/spinner\' + tsjsmapjsSuffix,\\n\\n    // common.bootstrap Modules\\n    app + \'common/bootstrap/bootstrap.dialog\' + tsjsmapjsSuffix,\\n\\n    // directives\\n    app + \'directives/**/*\' + tsjsmapjsSuffix,\\n\\n    // services\\n    app + \'services/**/*\' + tsjsmapjsSuffix,\\n\\n    // controllers\\n    app + \'about/**/*\' + tsjsmapjsSuffix,\\n    app + \'admin/**/*\' + tsjsmapjsSuffix,\\n    app + \'dashboard/**/*\' + tsjsmapjsSuffix,\\n    app + \'layout/**/*\' + tsjsmapjsSuffix,\\n    app + \'sages/**/*\' + tsjsmapjsSuffix,\\n    app + \'sayings/**/*\' + tsjsmapjsSuffix,\\n  ],\\n\\n  // The styles we want Gulp to process\\n  styles: [\'content/styles.css\'],\\n\\n  wiredepOptions: {\\n    exclude: [/jquery/],\\n    ignorePath: \'..\',\\n  },\\n};\\n\\nconfig.debugFolder = config.buildDir + config.debug + \'/\';\\nconfig.releaseFolder = config.buildDir + config.release + \'/\';\\n\\nconfig.templateFiles = [\\n  app + \'**/*.html\',\\n  \'!\' + config.bootFile, // Exclude the launch page\\n];\\n\\nmodule.exports = config;\\n```\\n\\nNow to the meat of the matter - let me present the gulpfile:\\n\\n```js\\n/// <vs AfterBuild=\'default\' />\\nvar gulp = require(\'gulp\');\\n\\n// Include Our Plugins\\nvar concat = require(\'gulp-concat\');\\nvar ignore = require(\'gulp-ignore\');\\nvar minifyCss = require(\'gulp-minify-css\');\\nvar uglify = require(\'gulp-uglify\');\\nvar rev = require(\'gulp-rev\');\\nvar del = require(\'del\');\\nvar path = require(\'path\');\\nvar templateCache = require(\'gulp-angular-templatecache\');\\nvar eventStream = require(\'event-stream\');\\nvar order = require(\'gulp-order\');\\nvar gulpUtil = require(\'gulp-util\');\\nvar wiredep = require(\'wiredep\');\\nvar inject = require(\'gulp-inject\');\\n\\n// Get our config\\nvar config = require(\'./gulpfile.config.js\');\\n\\n/**\\n * Get the scripts or styles the app requires by combining bower dependencies and app dependencies\\n *\\n * @param {string} jsOrCss Should be \\"js\\" or \\"css\\"\\n */\\nfunction getScriptsOrStyles(jsOrCss) {\\n  var bowerScriptsAbsolute = wiredep(config.wiredepOptions)[jsOrCss];\\n\\n  var bowerScriptsRelative = bowerScriptsAbsolute.map(\\n    function makePathRelativeToCwd(file) {\\n      return path.relative(\'\', file);\\n    },\\n  );\\n\\n  var appScripts = bowerScriptsRelative.concat(\\n    jsOrCss === \'js\' ? config.scripts : config.styles,\\n  );\\n\\n  return appScripts;\\n}\\n\\n/**\\n * Get the scripts the app requires\\n */\\nfunction getScripts() {\\n  return getScriptsOrStyles(\'js\');\\n}\\n\\n/**\\n * Get the styles the app requires\\n */\\nfunction getStyles() {\\n  return getScriptsOrStyles(\'css\');\\n}\\n\\n/**\\n * Get the scripts and the templates combined streams\\n *\\n * @param {boolean} isDebug\\n */\\nfunction getScriptsAndTemplates(isDebug) {\\n  var options = isDebug ? { base: config.base } : undefined;\\n  var appScripts = gulp.src(getScripts(), options);\\n\\n  //Get the view templates for $templateCache\\n  var templates = gulp\\n    .src(config.templateFiles)\\n    .pipe(templateCache({ module: \'app\', root: \'app/\' }));\\n\\n  var combined = eventStream.merge(appScripts, templates);\\n\\n  return combined;\\n}\\n\\ngulp.task(\'clean\', function (cb) {\\n  gulpUtil.log(\'Delete the build folder\');\\n\\n  return del([config.buildDir], cb);\\n});\\n\\ngulp.task(\'boot-dependencies\', [\'clean\'], function () {\\n  gulpUtil.log(\'Get dependencies needed for boot (jQuery and images)\');\\n\\n  var jQuery = gulp.src(config.bootjQuery);\\n  var images = gulp.src(config.images, { base: config.base });\\n\\n  var combined = eventStream\\n    .merge(jQuery, images)\\n    .pipe(gulp.dest(config.buildDir));\\n\\n  return combined;\\n});\\n\\ngulp.task(\'inject-debug\', [\'styles-debug\', \'scripts-debug\'], function () {\\n  gulpUtil.log(\'Inject debug links and script tags into \' + config.bootFile);\\n\\n  var scriptsAndStyles = [].concat(getScripts(), getStyles());\\n\\n  return gulp\\n    .src(config.bootFile)\\n    .pipe(\\n      inject(\\n        gulp\\n          .src(\\n            [\\n              config.debugFolder + \'**/*.{js,css}\',\\n              \'!build\\\\\\\\debug\\\\\\\\bower_components\\\\\\\\spin.js\', // Exclude weird spin js path\\n            ],\\n            { read: false },\\n          )\\n          .pipe(order(scriptsAndStyles)),\\n      ),\\n    )\\n    .pipe(gulp.dest(config.buildDir));\\n});\\n\\ngulp.task(\'inject-release\', [\'styles-release\', \'scripts-release\'], function () {\\n  gulpUtil.log(\'Inject release links and script tags into \' + config.bootFile);\\n\\n  return gulp\\n    .src(config.bootFile)\\n    .pipe(\\n      inject(gulp.src(config.releaseFolder + \'**/*.{js,css}\', { read: false })),\\n    )\\n    .pipe(gulp.dest(config.buildDir));\\n});\\n\\ngulp.task(\'scripts-debug\', [\'clean\'], function () {\\n  gulpUtil.log(\'Copy across all JavaScript files to build/debug\');\\n\\n  return getScriptsAndTemplates(true).pipe(gulp.dest(config.debugFolder));\\n});\\n\\ngulp.task(\'scripts-release\', [\'clean\'], function () {\\n  gulpUtil.log(\'Concatenate & Minify JS for release into a single file\');\\n\\n  return getScriptsAndTemplates(false)\\n    .pipe(ignore.exclude(\'**/*.{ts,js.map}\')) // Exclude ts and js.map files - not needed in release mode\\n    .pipe(concat(\'app.js\')) // Make a single file\\n    .pipe(uglify()) // Make the file titchy tiny small\\n    .pipe(rev()) // Suffix a version number to it\\n    .pipe(gulp.dest(config.releaseFolder)); // Write single versioned file to build/release folder\\n});\\n\\ngulp.task(\'styles-debug\', [\'clean\'], function () {\\n  gulpUtil.log(\'Copy across all CSS files to build/debug\');\\n\\n  return gulp\\n    .src(getStyles(), { base: config.base })\\n    .pipe(gulp.dest(config.debugFolder));\\n});\\n\\ngulp.task(\'styles-release\', [\'clean\'], function () {\\n  gulpUtil.log(\'Copy across all files in config.styles to build/debug\');\\n\\n  return gulp\\n    .src(getStyles())\\n    .pipe(concat(\'app.css\')) // Make a single file\\n    .pipe(minifyCss()) // Make the file titchy tiny small\\n    .pipe(rev()) // Suffix a version number to it\\n    .pipe(gulp.dest(config.releaseFolder + \'/\' + config.css)); // Write single versioned file to build/release folder\\n});\\n\\ngulp.task(\'fonts-debug\', [\'clean\'], function () {\\n  gulpUtil.log(\'Copy across all fonts in config.fonts to debug location\');\\n\\n  return gulp\\n    .src(config.fonts, { base: config.base })\\n    .pipe(gulp.dest(config.debugFolder));\\n});\\n\\ngulp.task(\'fonts-release\', [\'clean\'], function () {\\n  gulpUtil.log(\'Copy across all fonts in config.fonts to release location\');\\n\\n  return gulp\\n    .src(config.fonts)\\n    .pipe(gulp.dest(config.releaseFolder + \'/fonts\'));\\n});\\n\\ngulp.task(\'build-debug\', [\'boot-dependencies\', \'inject-debug\', \'fonts-debug\']);\\n\\ngulp.task(\'build-release\', [\\n  \'boot-dependencies\',\\n  \'inject-release\',\\n  \'fonts-release\',\\n]);\\n\\n// Use the web.config to determine whether the default task should create a debug or a release build\\n// If the web.config contains this: \'<compilation debug=\\"true\\"\' then we do a default build, otherwise\\n// we do a release build.  It\'s a little hacky but generally works\\nvar fs = require(\'fs\');\\nvar data = fs.readFileSync(__dirname + \'/web.config\', \'UTF-8\');\\nvar inDebug = !!data.match(/<compilation debug=\\"true\\"/);\\n\\ngulp.task(\'default\', [inDebug ? \'build-debug\' : \'build-release\']);\\n```\\n\\nThat\'s a big old lump of code. So let\'s go through this a task by task...\\n\\n### clean\\n\\nDeletes the `build` folder so we have a clean slate to build into.\\n\\n### boot-dependencies\\n\\nCopy across all files that are needed to allow the page to \\"boot\\" / startup. At present this is only jQuery and images.\\n\\n### inject-debug and inject-release\\n\\nThis is the magic. This picks up the launch page (`index.html`), takes the JavaScript and CSS and injects the corresponding `script` and `link` tags into the page and writing it to the `build` folder. Either the original source code or the bundled / minified equivalent will be used depending on whether it\'s debug or release.\\n\\n### scripts-debug and scripts-release\\n\\nHere we collect up the following:\\n\\n- the Bower specified JavaScript files\\n- the TypeScript + associated JavaScript files\\n- and we use our template files to construct a `templates.js` file to prime the Angular template cache\\n\\nIf it\'s the scripts-debug task we copy all these files into the `build/debug` folder. If it\'s the scripts-release task we also bundle, minify and strip the TypeScript out too and copy into the `build/release` folder.\\n\\n### styles-debug and styles-release\\n\\nHere we collect up the following:\\n\\n- the Bower specified CSS files\\n- our own app CSS\\n\\nIf it\'s the styles-debug task we copy all these files into the `build/debug` folder. If it\'s the styles-release task we also bundle and minify and copy into the `build/release` folder.\\n\\n### fonts-debug and fonts-release\\n\\nWhether it\'s the debug or the release build we copy across the font-awesome assets and place them in a location which works for the associated CSS (as the CSS will depend upon font-awesome).\\n\\n### build-debug, build-release and default\\n\\nbuild-debug and build-release (as their name suggests) either perform a build for release or a build for debug. If you remember, the web optimization library in ASP.Net serves up the raw code (\\"debug\\" code) if the `compilation debug` flag in the `web.config` is set to `true`. If it is set to `false` then we get the bundled and minified code (\\"release\\" code) instead. Our default task tries its best to emulate this behaviour by doing a very blunt regex against the `web.config`. Simply, if it can match `&lt;compilation debug=\\"true\\"` then it runs the debug build. Otherwise, the release build. It could be more elegant but there\'s a dearth of XML readers on npm that support synchronous parsing (which you kinda need for this scenario).\\n\\nWhat I intend to do soon is switch from using the web.config to drive the gulp build to using the approach outlined [here](http://www.codecadwallader.com/2015/03/15/integrating-gulp-into-your-tfs-builds-and-web-deploy/). Namely plugging the build directly into Visual Studio\'s build process and using the type of build there.\\n\\nHopefully what I\'ve written here makes it fairly clear how to use Gulp to directly inject scripts and styles directly into your HTML. If you want to look directly at the source then check out the Proverb.Web folder in [this repo](https://github.com/johnnyreilly/proverb-offline)."},{"id":"the-convent-with-continuous-delivery","metadata":{"permalink":"/the-convent-with-continuous-delivery","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2015-02-11-the-convent-with-continuous-delivery/index.md","source":"@site/blog/2015-02-11-the-convent-with-continuous-delivery/index.md","title":"The Convent with Continuous Delivery","description":"Programmer has open-sourced the Poor Clares Arundel website, making tweaks and site updating easier, with continuous delivery and collaboration.","date":"2015-02-11T00:00:00.000Z","tags":[],"readingTime":3.535,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"the-convent-with-continuous-delivery","title":"The Convent with Continuous Delivery","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false,"description":"Programmer has open-sourced the Poor Clares Arundel website, making tweaks and site updating easier, with continuous delivery and collaboration."},"unlisted":false,"prevItem":{"title":"Using Gulp to inject scripts and styles tags directly into your HTML","permalink":"/using-gulp-in-asp-net-instead-of-web-optimization"},"nextItem":{"title":"TypeScript: In Praise of Union Types","permalink":"/typescript-using-functions-with-union-types"}},"content":"I\'ve done it. I\'ve open sourced the [website that I maintain for my aunt what is a nun](http://www.poorclaresarundel.org/). Because I think we can all agree that nuns need open source and continuous integration about as much as anyone else.\\n\\n\x3c!--truncate--\x3e\\n\\nFor a long time now I\'ve been maintaining a website for one of my (many) aunts that is a Poor Clare. ([That\'s a subtype of \\"nun\\" you OO enthusiasts.](https://en.wikipedia.org/wiki/Subtyping)) It\'s not a terribly exciting site - it\'s mostly static content. It\'s built with a combination of AngularJS / TypeScript / Bootstrap and ASP.Net MVC. It\'s hosted on [Azure Websites](http://azure.microsoft.com/en-us/documentation/services/websites/). In fact I have written about it (slightly more cagily) before [here](../2014-06-01-migrating-from-angularjs-to-angularts/index.md).\\n\\nI\'ll say up front: presentation-wise the site is not a work of art. However the nuns seem pretty happy with it. (Or perhaps secretly they\'re forgiving me the shonkiness and sparing my feelings - who can say?) If I put my mind to it the site could look much more lovely. But there\'s only so much time I can spare - and that\'s actually one of the reasons I\'ve set up Continuous Delivery.\\n\\n## Why on earth did you bother?\\n\\nWell, you\'d be surprised how often tweaks can be requested. Sometimes it appears to be forgotten for months at a time, and then all of a sudden my inbox is daily filled with a list of minor alterations. You know, slight text changes and the like.\\n\\nSo what I was generally doing was getting home of an evening, waiting until the children were in bed, chomping down some food and then firing up Visual Studio to make the changes and hit \\"Publish\\". Yes that\'s right; I was essentially using Visual Studio to edit text files and push a website out to Azure. The very definition of using a sledgehammer to crack a nut I think we can all agree.\\n\\nIt occurred to me that if I had Continuous Delivery set up then I could make these tweaks and not have to worry about the site being published. Which would be nice. I wouldn\'t need Visual Studio anymore - any text editor would do. Also nice. Finally, if the source control was accessible online then I could probably get away with doing most tweaks on my mobile phone whilst I was travelling home. Timesaver!\\n\\n## How did you go about it?\\n\\nSince [Visual Studio Online (then \\"Team Foundation Service\\")](http://www.visualstudioonline.com) was released I have been using it to host the source code. So the obvious solution was to use the tools offered there to do the deployment. However, this wasn\'t the smooth experience you might have hoped for. I had quite a frustrating afternoon trying things out before deciding it was becoming more trouble than it was worth. VSO appeared to make it supremely hard to customise builds.\\n\\nJust recently though I have been having the most wonderful experience with [AppVeyor](http://www.appveyor.com/). AppVeyor market themselves as _\\"#1 Continuous Delivery service for Windows\\"_ \\\\- I think they\'re right. Their build process is entirely flexible and customisable. It is, in short, a joy to use. (The support is fantastic too - very helpful indeed. Go [Feodor](https://github.com/FeodorFitsner)!)\\n\\nIf you look just below the header you\'ll read a very important sentence: _\\"Free for open-source projects\\"_. You hear that? By the time I\'d finished reading that sentence I\'d decided that the Poor Clares website was about to become an open source project.\\n\\nAnd now it is.\\n\\n## Where is it?\\n\\nThe source on [GitHub](https://github.com/johnnyreilly/poorclaresarundel). The builds and deployment are taken care of by [AppVeyor](https://ci.appveyor.com/project/JohnReilly/poorclaresarundel).\\n\\n## Will you take pull requests?\\n\\nIf they\'re serious, then yes, certainly! My long term plan is to try and get the nuns set up as collaborators in GitHub. That way they can make their own minor tweaks without me getting involved.\\n\\nOn another front, I do wonder if open-sourcing Poor Clares, Arundel might have other hidden benefits. There\'s a number of things I\'m not too keen on in the code. Up until now I think my attitude was possibly \\"it works so that\'s good enough\\". It was only me aware of the shortcomings. Now it\'s public I\'ll probably have more of an incentive to tidy up the rough edges. That\'s the theory anyway - Embarrassment Driven Development anyone? :-)"},{"id":"typescript-using-functions-with-union-types","metadata":{"permalink":"/typescript-using-functions-with-union-types","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2015-01-20-typescript-using-functions-with-union-types/index.md","source":"@site/blog/2015-01-20-typescript-using-functions-with-union-types/index.md","title":"TypeScript: In Praise of Union Types","description":"TypeScript 1.4s Union Types offer a way to specify a value that is of one of many different types and results in a much terser definition file.","date":"2015-01-20T00:00:00.000Z","tags":[{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":6.295,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"typescript-using-functions-with-union-types","title":"TypeScript: In Praise of Union Types","authors":"johnnyreilly","tags":["typescript"],"hide_table_of_contents":false,"description":"TypeScript 1.4s Union Types offer a way to specify a value that is of one of many different types and results in a much terser definition file."},"unlisted":false,"prevItem":{"title":"The Convent with Continuous Delivery","permalink":"/the-convent-with-continuous-delivery"},"nextItem":{"title":"Deploying from ASP.Net MVC to GitHub Pages using AppVeyor part 2","permalink":"/deploying-aspnet-mvc-to-github-pages-with-appveyor-part-2"}},"content":"## (& How to Express Functions in UTs)\\n\\nHave you heard the good news my friend? I refer, of course, to the shipping of TypeScript 1.4 and my _favourite_ language feature since generics.... Union Types.\\n\\n\x3c!--truncate--\x3e\\n\\nIn the [1\\\\.4 announcement](https://blogs.msdn.com/b/typescript/archive/2015/01/16/announcing-typescript-1-4.aspx) Jonathan Turner described Union Types thusly:\\n\\n> JavaScript functions may take a number of possible argument types. Up to now, we\u2019ve supported this using function overloads. Starting with TypeScript 1.4, we\u2019ve generalized this capability and now allow you to specify that that a value is one of a number of different types using a union type:\\n>\\n> ```ts\\n> function f(x: number | number[]) {\\n>   if (typeof x === \'number\') {\\n>     return x + 10;\\n>   } else {\\n>     // return sum of numbers\\n>   }\\n> }\\n> ```\\n>\\n> Once you have a value of a union type, you can use a typeof and instanceof checks to use the value in a type-safe way. You\'ll notice we use this in the above example and can treat x as a number type inside of the if-block.\\n>\\n> Union types are a new kind of type and work any place you specify a type.\\n\\nLovely right? But what\'s missing? Well, to my mind, the most helpful aspect of Union Types. Definition file creation.\\n\\n## A little history\\n\\n### That\'s right - the days before Union Types are now \\"history\\" :-)\\n\\nWhen creating definition files (`*.d.ts`) in the past there was a problem with TypeScript. A limitation. JavaScript often relies on \\"option bags\\" to pass configuration into a method. An \\"option bag\\" is essentially a JavaScript object literal which contains properties which are used to perform configuration. A good example of this is the `route` parameter passed into Angular\'s ngRoute `<a href=\\"https://docs.angularjs.org/api/ngRoute/provider/$routeProvider#when\\">when</a>` method.\\n\\nI\'d like to draw your attention to 2 of the properties that can be passed in (quoted from the documentation):\\n\\n> - controller \u2013 `{(string|function()=}` \u2013 Controller fn that should be associated with newly created scope or the name of a registered controller if passed as a string.\\n> - template \u2013 `{string=|function()=}` \u2013 html template as a string or a function that returns an html template as a string which should be used by ngView or ngInclude directives. This property takes precedence over templateUrl.\\n>\\n>   If template is a function, it will be called with the following parameters:\\n>\\n>   `{Array.&lt;Object&gt;}` \\\\- route parameters extracted from the current $location.path() by applying the current route\\n\\nBoth of these properties can be of more than 1 type.\\n\\n- `controller` can be a `string`_or_ a `function`.\\n- `template` can be a `string`_or_ a `function` that returns a `string` and has `$routeParams` as a parameter.\\n\\nThere\'s the rub. Whilst it was possible to overload functions in TypeScript pre 1.4, it was <u>not</u>\\n\\npossible to overload interface members. This meant the only way to model these sorts of properties was by seeking out a best common type which would fit all scenarios. This invariably meant using the `any` type. Whilst that worked it didn\'t lend any consuming code a great deal of type safety. Let\'s look at a truncated version of [`angular-route.d.ts`](https://github.com/borisyankov/DefinitelyTyped/blob/c71628e0765eb8e240d8eabd2225f64ea2e2fdb8/angularjs/angular-route.d.ts) for these properties prior to union types:\\n\\n```ts\\ndeclare module ng.route {\\n  // ...\\n\\n  interface IRoute {\\n    /**\\n     * {(string|function()=}\\n     * Controller fn that should be associated with newly created scope or\\n     * the name of a registered controller if passed as a string.\\n     */\\n    controller?: any;\\n\\n    /**\\n     * {string=|function()=}\\n     * Html template as a string or a function that returns an html template\\n     * as a string which should be used by ngView or ngInclude directives. This\\n     * property takes precedence over templateUrl.\\n     *\\n     * If template is a function, it will be called with the following parameters:\\n     *\\n     * {Array.<Object>} - route parameters extracted from the current\\n     * $location.path() by applying the current route\\n     */\\n    template?: any;\\n\\n    // ...\\n  }\\n\\n  // ...\\n}\\n```\\n\\nIt\'s `any` city... Kind of sticks in the craw doesn\'t it?\\n\\n## A new dawn\\n\\nTypeScript 1.4 has shipped and Union Types are with us. We can do better than `any`. So what does [`angular-route.d.ts`](https://github.com/borisyankov/DefinitelyTyped/blob/30ce45e0e706322f34608ab6fa5de141bba59c90/angularjs/angular-route.d.ts) look like now we have Union Types?\\n\\n```ts\\ndeclare module ng.route {\\n  // ...\\n\\n  interface IRoute {\\n    /**\\n     * {(string|function()=}\\n     * Controller fn that should be associated with newly created scope or\\n     * the name of a registered controller if passed as a string.\\n     */\\n    controller?: string | Function;\\n\\n    /**\\n     * {string=|function()=}\\n     * Html template as a string or a function that returns an html template\\n     * as a string which should be used by ngView or ngInclude directives. This\\n     * property takes precedence over templateUrl.\\n     *\\n     * If template is a function, it will be called with the following parameters:\\n     *\\n     * {Array.<Object>} - route parameters extracted from the current\\n     * $location.path() by applying the current route\\n     */\\n    template?:\\n      | string\\n      | { ($routeParams?: ng.route.IRouteParamsService): string };\\n\\n    // ...\\n  }\\n\\n  // ...\\n}\\n```\\n\\nWith these changes in place we are now accurately modelling the `route` option bags in TypeScript. Hoorah!!!\\n\\nLet\'s dig in a little. If you look at the `controller` definition it\'s pretty straightforward. `string|Function` \\\\- clearly the `controller` can be a `string`_or_ a `Function`. Simple.\\n\\nNow let\'s look at the `template` definition by itself:\\n\\n```ts\\ntemplate?: string | { ($routeParams?: ng.route.IRouteParamsService) : string; }\\n```\\n\\nAs with the `controller` the `template` can be a string - that is pretty clear. But what\'s that hovering on the other side of the \\"\\\\|\\"? What could `{ ($routeParams?: ng.route.IRouteParamsService) : string; }` be exactly?\\n\\nWell, in a word, it\'s a `Function`. The `controller` would allow any kind of function at all. However the `template` definition is deliberately more restrictive. This defines a function which must return a `string` and which receives an optional parameter of `$routeParams` of type `ng.route.IRouteParamsService`.\\n\\n## State of the Union\\n\\nHopefully you can now see just how useful Union Types are and how you can express specific sorts of function definitions as part of a Union Type.\\n\\nThe thing that prompted me first to write this post was seeing that there don\'t appear to be any examples out there of how to express functions inside Union Types. I only landed on the syntax myself after a little experimentation in Visual Studio after I\'d installed TS 1.4. I\'ve started work on bringing Union Types to the typings inside [DefinitelyTyped](https://github.com/borisyankov/DefinitelyTyped) and so you\'ll start to see them appearing more and more. But since it\'s rather \\"hidden knowledge\\" at present I wanted to do my bit to make it a little better known.\\n\\nAs [Daniel](https://twitter.com/Rickenhacker) helpfully points out in the comments there is an alternate syntax - lambda style. So instead of this:\\n\\n```ts\\ntemplate?: string | { ($routeParams?: ng.route.IRouteParamsService) : string; }\\n```\\n\\nYou could write this:\\n\\n```ts\\ntemplate?: string | (($routeParams?: ng.route.IRouteParamsService) => string);\\n```\\n\\nJust remember to place parentheses around the lambda to clearly delineate it.\\n\\n## Bonfire of the Overloads\\n\\nBefore I sign off I should mention the ability Union Types give you to define a much terser definition file. Basically the \\"\\\\|\\" operator makes for a bonfire of the overloads. Where you previously may have had 6 overloads for the same method (each with identical JSDoc) you now only need 1. Which is beautiful (and DRY).\\n\\nIt\'s surprising just what a difference it makes. This is [`jQuery.d.ts`](https://github.com/borisyankov/DefinitelyTyped/blob/9bd7fe69d98337db56144c3da131d413f5b7e895/jquery/jquery.d.ts) last week (pre TypeScript 1.4). This is [`jQuery.d.ts`](https://github.com/borisyankov/DefinitelyTyped/blob/9f64372a065541fe2b8f6c5c5cd9b55a1d631f19/jquery/jquery.d.ts) now - with Union Types aplenty. Last week it was \\\\~4000 lines of code. This week it\'s \\\\~3200 lines of code. With the same functionality. Union Types are _FANTASTIC_!"},{"id":"deploying-aspnet-mvc-to-github-pages-with-appveyor-part-2","metadata":{"permalink":"/deploying-aspnet-mvc-to-github-pages-with-appveyor-part-2","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2015-01-07-deploying-aspnet-mvc-to-github-pages-with-appveyor-part-2/index.md","source":"@site/blog/2015-01-07-deploying-aspnet-mvc-to-github-pages-with-appveyor-part-2/index.md","title":"Deploying from ASP.Net MVC to GitHub Pages using AppVeyor part 2","description":"To save time, automating open source projects is key. Using AppVeyor and creating static sites with tools like Wget can help update documentation.","date":"2015-01-07T00:00:00.000Z","tags":[{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":4.78,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"deploying-aspnet-mvc-to-github-pages-with-appveyor-part-2","title":"Deploying from ASP.Net MVC to GitHub Pages using AppVeyor part 2","authors":"johnnyreilly","tags":["asp.net"],"hide_table_of_contents":false,"description":"To save time, automating open source projects is key. Using AppVeyor and creating static sites with tools like Wget can help update documentation."},"unlisted":false,"prevItem":{"title":"TypeScript: In Praise of Union Types","permalink":"/typescript-using-functions-with-union-types"},"nextItem":{"title":"Deploying from ASP.Net MVC to GitHub Pages using AppVeyor part 1","permalink":"/deploying-aspnet-mvc-to-github-pages-with-appveyor-part-1"}},"content":"\\"Automation, automation, automation.\\" Those were and are Tony Blair\'s priorities for keeping open source projects well maintained.\\n\\n\x3c!--truncate--\x3e\\n\\nOK, that\'s not quite true... But what is certainly true is that maintaining an open source project takes time. And there\'s only so much free time that anyone has. For that reason, wherever you can it makes sense to _AUTOMATE_!\\n\\n[Last time](../2014-12-29-deploying-aspnet-mvc-to-github-pages-with-appveyor-part-1/index.md) we looked at how you can take an essentially static ASP.Net MVC site (in this case my jVUNDemo documentation site) and generate an entirely static version using Wget. This static site has been pushed to [GitHub Pages](https://pages.github.com/) and is serving as the documentation for [jQuery Validation Unobtrusive Native](http://johnnyreilly.github.io/jQuery.Validation.Unobtrusive.Native/) (and for bonus points is costing me no money at all).\\n\\nSo what next? Well, automation clearly! If I make a change to jQuery Validation Unobtrusive Native then AppVeyor already bounds in and performs a [continuous integration build](https://ci.appveyor.com/project/JohnReilly/jquery-validation-unobtrusive-native) for me. It picks up the [latest source](https://github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native) from GitHub, pulls in my dependencies, performs a build and runs my tests. Lovely.\\n\\nSo the obvious thing to do is to take this process and plug in the generation of my static site and the publication thereof to GitHub pages. The minute a change is made to my project the documentation should be updated without me having to break sweat. That\'s the goal.\\n\\n## GitHub Personal Access Token\\n\\nIn order to complete our chosen mission we\'re going to need a GitHub Personal Access Token. We\'re going to use it when we clone, update and push our GitHub Pages branch. To get one we biff over to Settings / Applications in GitHub and click the \\"Generate New Token\\" button.\\n\\n![](GitHubApplicationSettings.webp)\\n\\nThe token I\'m using for my project has the following scopes selected:\\n\\n![](GitHub-Personal-Access-Token.webp)\\n\\n## `appveyor.yml`\\n\\nWith our token in hand we turn our attention to AppVeyor build configuration. This is possible using a file called [`appveyor.yml`](http://www.appveyor.com/docs/build-configuration) stored in the root of your repo. You can also use the AppVeyor web UI to do this. However, for the purposes of ease of demonstration I\'m using the file approach. The [jQuery Validation Unobtrusive Native `appveyor.yml`](https://github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native/blob/master/appveyor.yml) looks like this:\\n\\n```yml\\n---\\n#---------------------------------#\\n#      general configuration      #\\n#---------------------------------#\\n\\n# version format\\nversion: 1.0.{build}\\n\\n#---------------------------------#\\n#    environment configuration    #\\n#---------------------------------#\\n\\n# environment variables\\nenvironment:\\n  GithubEmail: johnny_reilly@hotmail.com\\n  GithubUsername: johnnyreilly\\n  GithubPersonalAccessToken:\\n    secure: T4M/N+e/baksVoeWoYKPWIpfahOsiSFw/+Zc81VuThZmWEqmrRtgEHUyin0vCWhl\\n\\nbranches:\\n  only:\\n    - master\\n\\ninstall:\\n  - ps: choco install wget\\n\\nbuild:\\n  verbosity: minimal\\n\\nafter_test:\\n  - ps: ./makeStatic.ps1 $env:APPVEYOR_BUILD_FOLDER\\n  - ps: ./pushStatic.ps1 $env:APPVEYOR_BUILD_FOLDER $env:GithubEmail $env:GithubUsername $env:GithubPersonalAccessToken\\n```\\n\\nThere\'s a number of things you should notice from the yml file:\\n\\n- We create 3 environment variables: GithubEmail, GithubUsername and GithubPersonalAccessToken (more on this in a moment).\\n- We only build the master branch.\\n- We use [Chocolatey](https://chocolatey.org/packages/Wget) to install Wget which is used by the `makeStatic.ps1` Powershell script.\\n- After the tests have completed we run 2 Powershell scripts. First [`makeStatic.ps1`](https://github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native/blob/master/makeStatic.ps1) which builds the static version of our site. This is the exact same script we discussed in the previous post - we\'re just passing it the build folder this time (one of AppVeyor\'s environment variables). Second, we run [`pushStatic.ps1`](https://github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native/blob/master/pushStatic.ps1) which publishes the static site to GitHub Pages.\\n\\nWe pass 4 arguments to `pushStatic.ps1`: the build folder, my email address, my username and my personal access token. For the sake of security the GithubPersonalAccessToken has been encrypted as indicated by the `secure` keyword. This is a capability available in AppVeyor [here](https://ci.appveyor.com/tools/encrypt).\\n\\n![](AppVeyor-encrypt.webp)\\n\\nThis allows me to mask my personal access token rather than have it available as free text for anyone to grab.\\n\\n## `pushStatic.ps1`\\n\\nFinally we can turn our attention to how our Powershell script `pushStatic.ps1` goes about pushing our changes up to GitHub Pages:\\n\\n```ps\\nparam([string]$buildFolder, [string]$email, [string]$username, [string]$personalAccessToken)\\n\\nWrite-Host \\"- Set config settings....\\"\\ngit config --global user.email $email\\ngit config --global user.name $username\\ngit config --global push.default matching\\n\\nWrite-Host \\"- Clone gh-pages branch....\\"\\ncd \\"$($buildFolder)\\\\..\\\\\\"\\nmkdir gh-pages\\ngit clone --quiet --branch=gh-pages https://$($username):$($personalAccessToken)@github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native.git .\\\\gh-pages\\\\\\ncd gh-pages\\ngit status\\n\\nWrite-Host \\"- Clean gh-pages folder....\\"\\nGet-ChildItem -Attributes !r | Remove-Item -Recurse -Force\\n\\nWrite-Host \\"- Copy contents of static-site folder into gh-pages folder....\\"\\ncopy-item -path ..\\\\static-site\\\\* -Destination $pwd.Path -Recurse\\n\\ngit status\\n$thereAreChanges = git status | select-string -pattern \\"Changes not staged for commit:\\",\\"Untracked files:\\" -simplematch\\nif ($thereAreChanges -ne $null) {\\n    Write-host \\"- Committing changes to documentation...\\"\\n    git add --all\\n    git status\\n    git commit -m \\"skip ci - static site regeneration\\"\\n    git status\\n    Write-Host \\"- Push it....\\"\\n    git push --quiet\\n    Write-Host \\"- Pushed it good!\\"\\n}\\nelse {\\n    write-host \\"- No changes to documentation to commit\\"\\n}\\n```\\n\\nSo what\'s happening here? Let\'s break it down:\\n\\n- Git is configured with the passed in username and email address.\\n- A folder is created that sits alongside the build folder called \\"gh-pages\\".\\n- We clone the \\"gh-pages\\" branch of jQuery Validation Unobtrusive Native into our \\"gh-pages\\" directory. You\'ll notice that we are using our GitHub personal access token in the clone URL itself.\\n- We delete the contents of the \\"gh-pages\\" directory leaving it empty.\\n- We copy across the contents of the \\"static-site\\" folder (created by `makeStatic.ps1`) into the \\"gh-pages\\".\\n- We use `git status` to check if there are any changes. (This method is completely effective but a little crude to my mind - there\'s probably better approaches to this.... shout me in the comments if you have a suggestion.)\\n- If we have no changes then we do nothing.\\n- If we have changes then we stage them, commit them and push them to GitHub Pages. Then we sign off with an allusion to [80\'s East Coast hip-hop](<https://en.wikipedia.org/wiki/Push_It_(Salt-n-Pepa_song)>)... \'Cos that\'s how we roll.\\n\\nWith this in place, any changes to the docs will be automatically published out to our \\"gh-pages\\" branch. Our documentation will always be up to date thanks to the goodness of AppVeyor\'s Continuous Integration service."},{"id":"deploying-aspnet-mvc-to-github-pages-with-appveyor-part-1","metadata":{"permalink":"/deploying-aspnet-mvc-to-github-pages-with-appveyor-part-1","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2014-12-29-deploying-aspnet-mvc-to-github-pages-with-appveyor-part-1/index.md","source":"@site/blog/2014-12-29-deploying-aspnet-mvc-to-github-pages-with-appveyor-part-1/index.md","title":"Deploying from ASP.Net MVC to GitHub Pages using AppVeyor part 1","description":"John (creator of jQuery Validation Unobtrusive Native) found a way to use GitHub Pages and automate deployment by creating a static version of the app.","date":"2014-12-29T00:00:00.000Z","tags":[],"readingTime":5.805,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"deploying-aspnet-mvc-to-github-pages-with-appveyor-part-1","title":"Deploying from ASP.Net MVC to GitHub Pages using AppVeyor part 1","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false,"description":"John (creator of jQuery Validation Unobtrusive Native) found a way to use GitHub Pages and automate deployment by creating a static version of the app."},"unlisted":false,"prevItem":{"title":"Deploying from ASP.Net MVC to GitHub Pages using AppVeyor part 2","permalink":"/deploying-aspnet-mvc-to-github-pages-with-appveyor-part-2"},"nextItem":{"title":"Gulp, npm, long paths and Visual Studio.... Fight!","permalink":"/gulp-npm-long-paths-and-visual-studio-fight"}},"content":"There\'s a small open source project I\'m responsible for called [jQuery Validation Unobtrusive Native](https://github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native). (A catchy name is a must for any good open source project. Alas I\'m not quite meeting my own exacting standards on this particular point... I should have gone with my gut and called it \\"Livingstone\\" instead. Too late now...)\\n\\n\x3c!--truncate--\x3e\\n\\nThe project itself is fairly simple in purpose. It\'s essentially a bridge between ASP.Net MVC\'s inbuilt support for driving validation from data attributes and jQuery Validation\'s native support for the same. It is, in the end, a collection of ASP.Net MVC HTML helper extensions. It is not massively complicated.\\n\\nI believe it was Tony Blair that said \\"documentation, documentation, documentation\\" were his priorities for open source projects. Or maybe it was someone else... Anyway, the point stands. Documentation is supremely important if you want your project to be in any way useful to anyone other than yourself. A project, no matter how fantastic, which lacks decent documentation is a missed opportunity.\\n\\nAnyway I\'m happy to say that jQuery Validation Unobtrusive Native _has_ documentation! And pretty good documentation at that. The documentation takes the form of the [jVUNDemo](https://github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native/tree/master/jVUNDemo) project which is part of the jQuery Validation Unobtrusive Native repo. jVUNDemo is an ASP.Net MVC web application which is built on top of the jQuery Validation Unobtrusive Native helpers. It demonstrates the helpers in action and documents how you might go about using them. It looks a bit like this:\\n\\n![](Screenshot-2014-12-29-06.22.46.webp)\\n\\nWhen I first put jVUNDemo together I hosted it on Azure so the world could see it in all it\'s finery. And that worked just fine. However, there\'s something you ought to know about me:\\n\\n## I\'m quite cheap\\n\\nNo really, I am. My attention was grabbed by the essentially \\"free\\" nature of [GitHub Pages](https://pages.github.com/). I was immediately seized by the desire to somehow deploy jVUNDemo to GitHub Pages and roll around joyfully in all that lovely free hosting.\\n\\n\\"But\\", I hear you cry, \\"you can\'t deploy an ASP.Net MVC application to Git Hub Pages... It only hosts static sites!\\" Quite right. Sort of. This is where I get to pull my ace of spades: jVUNDemo is not really an \\"app\\" so much as a static site. Yup, once the HTML that makes up each page is generated there isn\'t any app like business to do. It\'s just a collection of text and 1 screen demo\'s really.\\n\\nThat being the case, there\'s no reason why I shouldn\'t be able to make use of GitHub Pages. So that\'s what I decided to do. Whilst I was at it I also wanted to automate the deployment process. When I tweak jVUNDemo I don\'t want to have to manually push out a new version of jVUNDemo to wherever it\'s being hosted. No, I\'m a developer so I\'ll automate it.\\n\\nI\'ve broken this up into 2 posts. This first one will cover how you generate a static site from an ASP.Net MVC site. The second will be about how to use [AppVeyor](http://www.appveyor.com/) to ensure that on each build your documentation is getting republished.\\n\\n## You Wget me?\\n\\nSo, static site generation. There\'s a well known Unix utility called [Wget](https://en.wikipedia.org/wiki/Wget) which covers exactly that ground and so we\'re going to use it. It downloads and saves HTML, it recursively walks the links inside the site and grabs those pages too and it converts our routes so they are locally browsable (\\"Demo/Required\\" becomes \\"Demo/Required.html\\").\\n\\nYou can use [Chocolatey](https://chocolatey.org/packages/Wget) to get a copy of Wget. We\'re also going to need IIS Express to host jVUNDemo whilst Wget converts it. Once jVUNDemo has been built successfully you should be be able to kick off the process like so:\\n\\n```ps\\ncd C:\\\\projects\\\\jquery-validation-unobtrusive-native\\n.\\\\makeStatic.ps1 $pwd.path\\n```\\n\\nThis invokes the `makeStatic` Powershell script in the root of the jQuery Validation Unobtrusive Native repo:\\n\\n```ps\\nparam([string]$buildFolder)\\n\\n$jVUNDemo = \\"$($buildFolder)\\\\jVUNDemo\\"\\n$staticSiteParentPath = (get-item $buildFolder).Parent.FullName\\n$staticSite = \\"static-site\\"\\n$staticSitePath = \\"$($staticSiteParentPath)\\\\$($staticSite)\\"\\n$wgetLogPath = \\"$($staticSiteParentPath)\\\\wget.log\\"\\n$port = 57612\\n$servedAt = \\"http://localhost:$($port)/\\"\\nwrite-host \\"jVUNDemo location: $jVUNDemo\\"\\nwrite-host \\"static site parent location: $staticSiteParentPath\\"\\nwrite-host \\"static site location: $staticSitePath\\"\\nwrite-host \\"wget log path: $wgetLogPath\\"\\n\\nwrite-host \\"Spin up jVUNDemo site at $($servedAt)\\"\\n$process = Start-Process \'C:\\\\Program Files (x86)\\\\IIS Express\\\\iisexpress.exe\' -NoNewWindow -ArgumentList \\"/path:$($jVUNDemo) /port:$($port)\\"\\n\\nwrite-host \\"Wait a moment for IIS to startup\\"\\nStart-sleep -s 15\\n\\nif (Test-Path $staticSitePath) {\\n    write-host \\"Removing $($staticSitePath)...\\"\\n    Remove-Item -path $staticSitePath -Recurse -Force\\n}\\n\\nwrite-host \\"Create static version of demo site here: $($staticSitePath)\\"\\nPush-Location $staticSiteParentPath\\n# 2>&1 used to combine stderr and stdout\\nwget.exe --recursive --convert-links -E --directory-prefix=$staticSite --no-host-directories $servedAt > $wgetLogPath 2>&1\\nwrite-host \\"lastExitCode: $($lastExitCode)\\"\\ncat $wgetLogPath\\nPop-Location\\n\\nwrite-host \\"Shut down jVUNDemo site\\"\\nstop-process -Name iisexpress\\n\\nif (Test-Path $staticSitePath) {\\n    write-host \\"Contents of $($staticSitePath)\\"\\n    ls $staticSitePath\\n}\\n```\\n\\nThe above Powershell does the following:\\n\\n1. Starts up IIS Express serving jVUNDemo on http://localhost:57612/\\n2. Waits 15 seconds for IIS Express to get itself together (probably a shorter wait time would be sufficient)\\n3. Points Wget at jVUNDemo and bellows \\"go!!!!\\"\\n4. Wget downloads and saves the static version of jVUNDemo to a directory called \\"static-site\\"\\n5. Stops IIS Express\\n6. Prints out the contents of the \\"static-site\\" directory\\n\\nWhen run, it pumps something like this out:\\n\\n```\\njVUNDemo location: C:\\\\projects\\\\jquery-validation-unobtrusive-native\\\\jVUNDemo\\nstatic site parent location: C:\\\\projects\\nstatic site location: C:\\\\projects\\\\static-site\\nwget log path: C:\\\\projects\\\\wget.log\\nSpin up jVUNDemo site at http://localhost:57612/\\nWait a moment for IIS to startup\\nCreate static version of demo site here: C:\\\\projects\\\\static-site\\nwget.exe : --2014-12-29 07:49:56--  http://localhost:57612/\\nResolving localhost...\\n127.0.0.1\\nConnecting to localhost|127.0.0.1|:57612... connected.\\nHTTP request sent, awaiting response...\\n200 OK\\n\\n..... lots of HTTP requests.....\\n\\nDownloaded: 30 files, 1.0M in 0.09s (10.8 MB/s)\\nConverting static-site/Demo/CreditCard.html... 34-0\\nConverting static-site/Demo/Number.html... 34-0\\nConverting static-site/Demo/Range.html... 34-0\\nConverting static-site/Demo/Email.html... 34-0\\nConverting static-site/AdvancedDemo/CustomValidation.html... 35-0\\nConverting static-site/Demo/Date.html... 36-0\\nConverting static-site/Home/License.html... 27-0\\nConverting static-site/index.html... 29-0\\nConverting static-site/AdvancedDemo/Dynamic.html... 35-0\\nConverting static-site/Demo/MaxLengthMinLength.html... 34-0\\nConverting static-site/Demo/Required.html... 34-0\\nConverting static-site/AdvancedDemo.html... 32-0\\nConverting static-site/Demo/Remote.html... 35-0\\nConverting static-site/Demo/EqualTo.html... 34-0\\nConverting static-site/AdvancedDemo/Globalize.html... 38-0\\nConverting static-site/Demo/Url.html... 34-0\\nConverting static-site/Demo.html... 37-0\\nConverting static-site/Home/GettingStarted.html... 29-0\\nConverting static-site/Home/Download.html... 27-0\\nConverting static-site/AdvancedDemo/Tooltip.html... 34-0\\nConverted 20 files in 0.03 seconds.\\n\\nShut down jVUNDemo site\\nContents of C:\\\\projects\\\\static-site\\n\\n\\n    Directory: C:\\\\projects\\\\static-site\\n\\n\\nMode                LastWriteTime     Length Name\\n----                -------------     ------ ----\\nd----        12/29/2014   7:50 AM            AdvancedDemo\\nd----        12/29/2014   7:50 AM            Content\\nd----        12/29/2014   7:50 AM            Demo\\nd----        12/29/2014   7:50 AM            Home\\nd----        12/29/2014   7:50 AM            Scripts\\n-a---        12/29/2014   7:50 AM       5967 AdvancedDemo.html\\n-a---        12/29/2014   7:50 AM       6802 Demo.html\\n-a---        12/29/2014   7:47 AM      12862 favicon.ico\\n-a---        12/29/2014   7:50 AM       8069 index.html\\n```\\n\\nAnd that\'s it for part 1 my friends! You now have a static version of the ASP.Net MVC site to dazzle the world with. I should say for the purposes of full disclosure that there are 2 pages in the site which are not entirely \\"static\\" friendly. For these 2 pages I\'ve put messages in that are displayed when the page is served up in a static format explaining the limitations. Their full glory can still be experienced by cloning the project and running locally.\\n\\n[Next time](../2015-01-07-deploying-aspnet-mvc-to-github-pages-with-appveyor-part-2/index.md) we\'ll take the mechanism detailed above and plug it into AppVeyor for some Continuous Integration happiness."},{"id":"gulp-npm-long-paths-and-visual-studio-fight","metadata":{"permalink":"/gulp-npm-long-paths-and-visual-studio-fight","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2014-12-12-gulp-npm-long-paths-and-visual-studio-fight/index.md","source":"@site/blog/2014-12-12-gulp-npm-long-paths-and-visual-studio-fight/index.md","title":"Gulp, npm, long paths and Visual Studio.... Fight!","description":"Installing gulp-angular-templatecache plugin caused issues with Visual Studio. A temporary solution is to install lodash.bind at root level.","date":"2014-12-12T00:00:00.000Z","tags":[{"inline":false,"label":"Visual Studio","permalink":"/tags/visual-studio","description":"The Visual Studio IDE."},{"inline":false,"label":"Node.js","permalink":"/tags/node-js","description":"The Node.js runtime."}],"readingTime":2.715,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"gulp-npm-long-paths-and-visual-studio-fight","title":"Gulp, npm, long paths and Visual Studio.... Fight!","authors":"johnnyreilly","tags":["visual studio","node.js"],"hide_table_of_contents":false,"description":"Installing gulp-angular-templatecache plugin caused issues with Visual Studio. A temporary solution is to install lodash.bind at root level."},"unlisted":false,"prevItem":{"title":"Deploying from ASP.Net MVC to GitHub Pages using AppVeyor part 1","permalink":"/deploying-aspnet-mvc-to-github-pages-with-appveyor-part-1"},"nextItem":{"title":"What\'s in a (Domain) Name?","permalink":"/whats-in-a-name"}},"content":"## How I managed to gulp-angular-templatecache working inside Visual Studio\\n\\nEvery now and then something bites you unexpectedly. After a certain amount of pain, the answer comes to you and you know you want to save others from falling into the same deathtrap.\\n\\nThere I was minding my own business and having a play with a Gulp plugin called [gulp-angular-templatecache](https://www.npmjs.com/package/gulp-angular-templatecache). If you\'re not aware of it, it \\"Concatenates and registers AngularJS templates in the $templateCache\\". I was planning to use it so that all the views in an [Angular app of mine](https://github.com/johnnyreilly/proverb-offline) were loaded up-front rather than on demand. (It\'s a first step in making an \\"offline-first\\" version of that particular app.)\\n\\n\x3c!--truncate--\x3e\\n\\nI digress already. No sooner had I tapped in:\\n\\n```ps\\nnpm install gulp-angular-templatecache --saveDev\\n```\\n\\nThen I noticed my Visual Studio project was no longer compiling. It was dying a death on build with this error:\\n\\n```ps\\nASPNETCOMPILER : error ASPRUNTIME: The specified path, file name, or both are too long. The fully qualified file name must be less than 260 characters, and the directory name must be less than 248 characters.\\n```\\n\\nI was dimly aware that there were issues with the nested [node_modules](https://github.com/joyent/node/issues/6960) leading to Windows-killing paths. This sounded just like that.... And it was! `gulp-angular-templatecache` had a dependency on `gulp-footer` which had a dependency on `lodash.assign` which had a dependency on `lodash._basecreatecallback` which had.... You see where I\'m going? It seems that the lovely lodash has created the path from hell.\\n\\nFor reasons that aren\'t particularly clear this kills Visual Studio\'s build process. This is slightly surprising given that our rogue path is sat in the `node_modules` directory which isn\'t part of the project in Visual Studio. That being the case you\'d imagine that you could do what you liked there. But no, it seems VS is a delicate flower and we must be careful not to offend. Strange.\\n\\n## It\'s Workaround Time!\\n\\nAfter a _great deal_ of digging I found the answer nestled in the middle of an [answer on Stack Overflow](http://stackoverflow.com/a/24144479/761388). To quote:\\n\\n> If you will add \\"lodash.bind\\" module to your project\'s package.json as dependency it will be installed in one level with gulp and not as gulp\'s dependency\\n\\nThat\'s right, I just needed to tap enter this at the root of my project:\\n\\n```ps\\nnpm install lodash.bind --saveDev\\n```\\n\\nAnd all was sweetness and light once more - no more complaints from VS.\\n\\n## The Future\\n\\nIt looks like lodash are [on course to address this issue](https://github.com/lodash/lodash-cli/issues/23). So one day this this workaround won\'t be necessary anymore which is good.\\n\\nHowever, the general long path issue concerning node / npm hasn\'t vanished for Windows users. Given VS 2015 is planning to make Gulp and Grunt 1st class citizens of Visual Studio I\'m going to guess that sort of issue is likely to arise again and again for other packages. I\'m hoping that means that someone will actually fix the underlying path issues that upset Windows with npm.\\n\\nIt sounds like npm are planning to take [some steps](https://github.com/joyent/node/issues/6960#issuecomment-46704998) which is great. But I can\'t be alone in having a slightly nagging feeling that Windows isn\'t quite a first class citizen for node / io / npm yet. I really hope it will become one."},{"id":"whats-in-a-name","metadata":{"permalink":"/whats-in-a-name","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2014-12-05-whats-in-a-name/index.md","source":"@site/blog/2014-12-05-whats-in-a-name/index.md","title":"What\'s in a (Domain) Name?","description":"\\"icanmakethiswork\\" blog has a new domain due to Johns concern about potential changes in Google hosting, now \\"blog.icanmakethiswork.io\\".","date":"2014-12-05T00:00:00.000Z","tags":[],"readingTime":2.61,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"whats-in-a-name","title":"What\'s in a (Domain) Name?","authors":"johnnyreilly","hide_table_of_contents":false,"description":"\\"icanmakethiswork\\" blog has a new domain due to Johns concern about potential changes in Google hosting, now \\"blog.icanmakethiswork.io\\"."},"unlisted":false,"prevItem":{"title":"Gulp, npm, long paths and Visual Studio.... Fight!","permalink":"/gulp-npm-long-paths-and-visual-studio-fight"},"nextItem":{"title":"Pretending to be someone you\'re not and the dark pit of despair","permalink":"/Coded-UI-IE-11-and-the-runas-problem"}},"content":"The observant amongst you may have noticed that this blog has a brand new and shiny domain name! That\'s right, after happily trading under \\"icanmakethiswork.blogspot.com\\" for the longest time it\'s now \\"blog.icanmakethiswork.io\\". Trumpets and fanfare!\\n\\n\x3c!--truncate--\x3e\\n\\nWhy the change? Well let\'s break that question down a little. First of all, why change at all? Secondly, why change to blog.icanmakethiswork.io?\\n\\n## Why do things have to change at all?\\n\\nI mean, weren\'t we happy? Wasn\'t it all good? Well quite. For the record, I have no complaints of Blogger who have hosted my blog since it began. They\'ve provided good tools and a good service and I\'m happy with them.\\n\\nThat said, I\'ve been toying with the idea for a while now of trying out a few other blogging solutions - possibly even hosting it myself. Whilst my plans are far from definite at the moment I\'m aware that I don\'t own icanmakethiswork.blogspot.com - I can\'t take it with me. So if I want to make a move to change my blogging solution a first step is establishing my own domain name for my blog. I\'ve done that now. If and when I up sticks, people will hopefully come with me as the URL for my blog should not change.\\n\\nAlso, in the back of my mind I\'m aware that Google owns Blogger. Given their recent spate of closing services it\'s certainly possible that the Google reaper could one day call for Blogger. So it makes sense to be ready to move my blog elsewhere should that day come.\\n\\n## Why blog.icanmakethiswork.io?\\n\\nWhy indeed? And why the \\".io\\" suffix? Doesn\'t that just make you a desperate follower of fashion?\\n\\nGood questions all, and \\"no, I hope not\\". My original plans were to use the domain name \\"icanmakethiswork.com\\". icanmakethiswork was the name of the blog and it made sense to keep it in the URL. So off I went to register the domain name when to my surprise I discovered this:\\n\\n![](Screenshot-2014-12-05-05.39.00.webp)\\n\\nMy domain is being [cybersquatted](https://en.wikipedia.org/wiki/Cybersquatting)! I mean.... What??!!!!\\n\\nI started to wonder \\"is there another icanmakethiswork out there\\"? Am I not the [one and only](http://youtu.be/z8f2mW1GFSI)? So I checked with DuckDuckGo (\\"The search engine that doesn\'t track you.\\") and look what I found:\\n\\n![](Screenshot-2014-12-05-05.41.59.webp)\\n\\nA whole screen of me. Just me.\\n\\nAs of June 3rd 2014 someone has been sitting on my blog name. I was actually rather outraged by this. I became even more so as I discovered that there was a mechanism (not free) by which I could try and buy it off the squatter. I could instead be like my life idol Madonna and go to court to get it back. But frankly in this sense I\'m more like Rachel Green in Friends; not litigous.\\n\\nSo that\'s why I went for icanmakethiswork.io instead. Path of least resistance and all that. I\'d still like icanmakethiswork.com to be mine but I\'m not going to court and I\'m not paying the squatter. Maybe one day I\'ll get it. Who knows?\\n\\nEither way, from now on this is blog.icanmakethiswork.io - please stick around!\\n\\n## Is anything else going to change?\\n\\nNot for now, no."},{"id":"Coded-UI-IE-11-and-the-runas-problem","metadata":{"permalink":"/Coded-UI-IE-11-and-the-runas-problem","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2014-11-26-Coded-UI-IE-11-and-the-runas-problem/index.md","source":"@site/blog/2014-11-26-Coded-UI-IE-11-and-the-runas-problem/index.md","title":"Pretending to be someone you\'re not and the dark pit of despair","description":"Workaround for issues with Coded UI impersonation feature. Tests can be unreliable, but the fix works well.","date":"2014-11-26T00:00:00.000Z","tags":[{"inline":false,"label":"Automated Testing","permalink":"/tags/automated-testing","description":"How to perform the automation of tests."}],"readingTime":10.43,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"Coded-UI-IE-11-and-the-runas-problem","title":"Pretending to be someone you\'re not and the dark pit of despair","authors":"johnnyreilly","tags":["automated testing"],"hide_table_of_contents":false,"description":"Workaround for issues with Coded UI impersonation feature. Tests can be unreliable, but the fix works well."},"unlisted":false,"prevItem":{"title":"What\'s in a (Domain) Name?","permalink":"/whats-in-a-name"},"nextItem":{"title":"Using Gulp in Visual Studio instead of Web Optimization","permalink":"/using-gulp-in-visual-studio-instead-of-web-optimization"}},"content":"## Coded UI, IE 11 and the \\"runas\\" problem\\n\\n\x3c!--truncate--\x3e\\n\\n\\"I\'m not angry, I\'m just disappointed.\\"\\n\\nThat\'s kind of how I feel about Coded UI tests. It may well be that you\'ve never heard of them - in my experience very few people seem to be aware of them. What are they? Well, I\'ve never used [Selenium](http://www.seleniumhq.org/) but as best I understand Coded UI is Microsoft\'s own version of that. Namely it\'s a way to automate testing, in my case browser-based testing. You can write a suite of tests that will spin up your application and test it out, going from screen to screen, URL to URL and asserting all is as you would expect.\\n\\nThe project that I\'m currently working on has a pretty comprehensive set of tests covering the use of the application. Each night as the clock strikes midnight a lonely computer in the West End of London whirrs into life and runs the full suite. It takes about 8 hours and at the end a report slips into your inbox letting you know of any failures.\\n\\n## Sounds brilliant right? How could someone not love this?\\n\\nWell a number of reasons. First of all, _it takes 8 hours_!!!! That\'s a long time; I\'d rather learn what I broke today rather than tomorrow.\\n\\nAlso, and this is probably more significant, Coded UI tests are pretty flaky. Let me qualify that. For a test to be particularly useful it has to be quick, repeatable and reliable. As I\'ve said, Coded UI tests are not quick.\\n\\nBy their very nature integration tests (of which Coded UI tests are a type) can never be entirely reliably repeatable. They test your app in it\'s entirety. So, for example, if a 3rd party service goes down for 5 minutes then you will get failed tests. You\'ll burn time investigating these false positives.\\n\\nFurther to that, Coded UI tests are repeatable, except when they\'re not. I\'ve seen colleagues reduced to near tears by incredible sensitivity of Coded UI tests. Out of the box Coded UI tests appear to ship with the [\\"Works on my machine\\"](http://blog.codinghorror.com/the-works-on-my-machine-certification-program/) guarantee. It requires far more effort that you\'d expect to come up with tests that can be reliably expected to pass. They will fail for surprising reasons. For instance, did you know that using the 2.x branch of jQuery won\'t work with Coded UI? [Neither did I.](https://connect.microsoft.com/VisualStudio/Feedback/Details/794841) I\'ve lost track of the time that has been wasted running the same test in multiple different environments trying to identify what exactly is upsetting Coded UI about the environment this time.\\n\\nIt is sad but true that with Coded UI tests you can spend an _enormous_ amount of time maintaining the test pack on a day to day basis. As infrastructure and project dependencies are upgraded you will sadly discover Coded UI has once again gone into the foetal position and has to tempted back to normal functioning by whispering sweet nothings in it\'s ear. (_\\"It\'s not true that they\'ve ended support for Windows XP\\" / \\"IE 6 will live forever\\"_ and so on)\\n\\nCoded UI also appears to be badly supported by Microsoft. Documentation is pretty sparse and, as we\'ll come back to in a minute, Coded UI is sometimes broken or damaged by other products shipped by Microsoft. This makes it hard to have faith in Coded UI. Indeed, if you\'re thinking of automating your QA testing my advice would be \\"look into Selenium\\". Not because I\'ve used it (I haven\'t) but those I\'ve met who have used Selenium and Coded UI say Selenium wins hands down.\\n\\n## And yet, and yet...\\n\\nAll of the above said, if you have a Coded UI test suite it can still pay dividends. Significant dividends. As I mentioned, my current project has a significant coverage of Coded UI tests. We\'ve crawled over a lot of broken glass to put these together. But now they\'re there it is undeniably useful.\\n\\nEvery now and then we\'ll do a significant refactor of part of the application. For instance, we\'ve entirely changed our persistence strategy in the app but been able to check the code in with a high degree of confidence gleaned from running our test suite using the refactored codebase.\\n\\nLet me be clear: Coded UI tests can be useful.\\n\\n## The \\"runas\\" Problem\\n\\nLong preamble over, this post is about how to work around the latest issue Coded UI has thrown in our direction. I call it the \\"runas\\" problem. Our application is a Knockout / ASP.Net MVC web app built to be used in an intranet environment. By that I mean that identity is handled by Active Directory / [Windows Authentication](http://en.wikipedia.org/wiki/Integrated_Windows_Authentication). When someone logs into our app we know who they are without them having to directly supply us with a username and password. No, by logging into their computer they have announced just who they are and Internet Explorer (for it is he) will pass along the credentials. (The app can be used with pretty much any browser but we\'re only mandated to support IE 9+.)\\n\\nIn order that we can test the app we have a number of test accounts set up in Active Directory. These test accounts have been assigned various roles (viewer / editor / administrator etc). Our tests are designed to run using these accounts in order that all scenarios can be adequately tested.\\n\\nTo achieve this lofty goal the following code (or something very like it) is executed as the first step in any Coded UI test:\\n\\n```cs\\nstring browserLocation = \\"C:\\\\\\\\Program Files\\\\\\\\Internet Explorer\\\\\\\\iexplore.exe\\";\\nstring url = \\"http://localhost:12345/\\";\\nstring username = \\"test.editor\\";\\nstring domain = \\"theDomain\\";\\nvar password = new SecureString();\\nforeach (char c in \\"test.editor.password\\")\\n{\\n    password.AppendChar(c);\\n}\\n\\nApplicationUnderTest.Launch(browserLocation, null, url, username, password, domain);\\n```\\n\\nWhat this does is fire up Internet Explorer as the supplied user of theDomain\\\\test.editor, and navigate to the home page. With that as our starting place we could dependably then run a test as this test user. This was a solution not without quirks (on occasion Coded UI tests would \\"stutter\\" - repeating each keypress 3 times with calamitous effects). But generally, this worked.\\n\\nUntil that is either Visual Studio 2013 Update 3 or Internet Explorer 11 was installed. One of these (and it appears to be hotly contested) broke the ability to run the above code successfully. After these were installed running the above code resulted in the following error message:\\n\\n> \\"The application cannot be started. This could be due to one of the following reasons:\\n>\\n> 1.  Another instance of the application is already running and only one instance can be running at a time.\\n> 2.  The application started another process and has now stopped. You may need to launch the process directly.\\n> 3.  You do not have sufficient privileges for this application.\\" File: C:\\\\Program Files\\\\Internet Explorer\\\\iexplore.exe.\\"\\n\\nLamentably, this was pretty much unresolvable and [logging it with Microsoft yielded nothing helpful](https://connect.microsoft.com/VisualStudio/feedbackdetail/view/949049/coded-ui-cannot-run-as-a-different-user-with-visual-studio-2013-update-3). This is what I mean about Coded UI being badly supported by Microsoft. Despite my best efforts to report this issue both to Connect and [elsewhere](http://social.msdn.microsoft.com/Forums/vstudio/en-US/f48665e4-569a-4b67-9bdb-5522b2adffb2/cannot-run-coded-ui-tests-as-different-user-on-windows-81?forum=vsmantest#28c9decb-b579-4848-a7a9-f41c57584d59) and in the end nothing useful happened.\\n\\nSo what to do? I still have Coded UI tests, I still need to be able to run them. And crucially I need to be able to run them impersonating a different user. What to do indeed....\\n\\n## The <strike>hack</strike>\\n\\nworkaround\\n\\nAfter IE 11 / Visual Studio Update 3 / whatev\'s was installed I was left with a setup that allowed me to run Coded UI tests, <u>but only</u>\\n\\nas the current user. On that basis I started looking into a little MVC jiggery pokery. All my controllers inherit from a single base controller. Inside there I placed the following extra override:\\n\\n```cs\\npublic abstract class BaseController : System.Web.Mvc.Controller\\n{\\n  //...\\n\\n  protected override void OnAuthorization(AuthorizationContext filterContext)\\n  {\\n#if DEBUG\\n    if (filterContext.HttpContext.IsDebuggingEnabled)// Is compilation debug=\\"true\\" set in the web.config?\\n    {\\n      var userToImpersonate = Session[\\"UserToImpersonate\\"] as string;\\n      if (!string.IsNullOrEmpty(userToImpersonate))\\n      {\\n        // userToImpersonate example: \\"test.editor@theDomain.com\\"\\n        filterContext.HttpContext.User = new RolePrincipal(new WindowsIdentity(userToImpersonate));\\n      }\\n    }\\n#endif\\n      base.OnAuthorization(filterContext);\\n  }\\n\\n  //...\\n}\\n```\\n\\nEach request will trigger this method as one of the first steps in the MVC pipeline. What it does is checks the `Session` for a user to impersonate. (Yes I\'m as wary of Session as the next chap - but in this case it\'s the right tool for the job.) If a user has been specified then it replaces the current user with the `Session` user. From this point forwards the app is effectively running as that user. That\'s great!\\n\\nIn order that Coded UI can make use of this mechanism we need to introduce a \\"hook\\". This is going to look a bit hacky - bear with me. Inside `Global.asax.cs` we\'re going to add a `Session_Start` method:\\n\\n```cs\\nprotected void Session_Start(object sender, EventArgs eventArgs)\\n{\\n#if DEBUG\\n    // If a user to impersonate has been supplied then add this user to the session\\n    // Impersonation will happen in the OnAuthorization method of our base MVC controller\\n    // Note, this is only allowed in debug mode - not in release mode\\n    // This exists purely to support coded ui tests\\n    if (Context.IsDebuggingEnabled)  // Is compilation debug=\\"true\\" set in the web.config?\\n    {\\n        var userToImpersonate = Request.QueryString[\\"UserToImpersonate\\"] as string;\\n        if (!string.IsNullOrEmpty(userToImpersonate))\\n        {\\n            Session.Add(\\"UserToImpersonate\\", userToImpersonate);\\n        }\\n    }\\n#endif\\n}\\n```\\n\\nFor the first Request in a Session this checks the `QueryString` for a parameter called `UserToImpersonate`. If it\'s found then it\'s placed into `Session`. With this hook exposed we can now amend the first step that all our Coded UI tests follow:\\n\\n```cs\\n// Various lines commented out as doesn\'t work with IE 11 - left as an example of how it could be done in the past\\n//string browserLocation = \\"C:\\\\\\\\Program Files\\\\\\\\Internet Explorer\\\\\\\\iexplore.exe\\";\\nstring url = \\"http://localhost:12345/\\";\\nstring username = \\"test.editor\\";\\nstring domain = \\"theDomain.com\\";\\n//var password = new SecureString();\\n//foreach (char c in \\"test.editor.password\\")\\n//{\\n//    password.AppendChar(c);\\n//}\\n\\n//ApplicationUnderTest.Launch(browserLocation, null, url, username, password, domain);\\n\\n// Suffixing url with UrlToImpersonate which will be picked up in Session_Start and used to impersonate\\n// in OnAuthorization in BaseController.  Also no longer using ApplicationUnderTest.Launch; switched to\\n// BrowserWindow.Launch\\n// No longer used parameters: browserLocation, password\\nvar userToImpersonate = username + \\"@\\" + domain; // eg \\"test.editor@theDomain.com\\"\\nvar urlWithUser = url + \\"?UserToImpersonate=\\" + HttpUtility.UrlEncode(userToImpersonate);\\nvar browser = BrowserWindow.Launch(urlWithUser, \\"-nomerge\\"); // \\"-nomerge\\" flag forces a new session\\n```\\n\\nAs you can see we actually need less when we\'re using this approach. We no longer need to directly specify the password or the browser location. And the user to impersonate is now passed in as the part of the initial URL used to launch the test.\\n\\nPay careful attention to the \\"-nomerge\\" flag that is passed in. This ensures that when another browser instance is opened a new session will be started. This is essential for \\"multi-user\\" tests that run tests for _different_ users as part of the same test. It ensures that \\"test.editor\\" and \\"test.different.editor\\" can co-exist happily.\\n\\n## What do I think of the workaround?\\n\\nThis approach works reliably and dependably. More so than the original approach which on occasion wouldn\'t work or would \\"stutter\\" keypresses. That\'s the good news.\\n\\nThe not so good news is that this approach is, in my view, a bit of hack. I want you to know that this isn\'t my ideal.\\n\\nI _really_ don\'t like having to change the actual system code to facilitate the impersonation requirement. Naturally we only ship the release and not the debug builds to Production so the \\"back door\\" that this approach provides will not exist in our Production builds. It will only be accessible in our development environments and on our Coded UI test server. But it feels oh so wrong that there is an effective potential back door in the system now. Well, only if the stars were to align in a really terrible (and admittedly rather unlikely) way. But still, you take my point. Caveat emptor and all that. This is something of a cutdown example to illustrate the point. If anyone else intends to use this then I\'d suggest doing more to safeguard your approach. Implementing impersonation allowlists so \\"any\\" user cannot be impersonated would be a sensible precaution to start with.\\n\\nPerhaps this is just one more reason that I\'m not that enamoured of Coded UI. Once again it is useful but I\'ve had to compromise more than I\'d like to keep it\'s use. If anyone out there has a better solution I would _love_ to hear from you."},{"id":"using-gulp-in-visual-studio-instead-of-web-optimization","metadata":{"permalink":"/using-gulp-in-visual-studio-instead-of-web-optimization","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2014-11-04-using-gulp-in-visual-studio-instead-of-web-optimization/index.md","source":"@site/blog/2014-11-04-using-gulp-in-visual-studio-instead-of-web-optimization/index.md","title":"Using Gulp in Visual Studio instead of Web Optimization","description":"The ASP.NET team may replace Web Optimization with Grunt or Gulp. John Reilly tried out Gulp, which concatenates, minimises & version-numbers files.","date":"2014-11-04T00:00:00.000Z","tags":[{"inline":false,"label":"Visual Studio","permalink":"/tags/visual-studio","description":"The Visual Studio IDE."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."},{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."},{"inline":false,"label":"Node.js","permalink":"/tags/node-js","description":"The Node.js runtime."}],"readingTime":15.435,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"using-gulp-in-visual-studio-instead-of-web-optimization","title":"Using Gulp in Visual Studio instead of Web Optimization","authors":"johnnyreilly","tags":["visual studio","typescript","javascript","node.js"],"hide_table_of_contents":false,"description":"The ASP.NET team may replace Web Optimization with Grunt or Gulp. John Reilly tried out Gulp, which concatenates, minimises & version-numbers files."},"unlisted":false,"prevItem":{"title":"Pretending to be someone you\'re not and the dark pit of despair","permalink":"/Coded-UI-IE-11-and-the-runas-problem"},"nextItem":{"title":"Caching and Cache-Busting in AngularJS with HTTP interceptors","permalink":"/caching-and-cache-busting-in-angularjs-with-http-interceptors"}},"content":"### Updated 17/02/2015: I\'ve taken the approach discussed in this post a little further - you can see [here](../2012-10-05-using-web-optimization-with-mvc-3/index.md)\\n\\n\x3c!--truncate--\x3e\\n\\nI\'ve used a number of tools to package up JavaScript and CSS in my web apps. [Andrew Davey\'s tremendous Cassette](http://getcassette.net/) has been really useful. Also good (although less powerful/magical) has been Microsoft\'s very own [Microsoft.AspNet.Web.Optimization](https://www.nuget.org/packages/Microsoft.AspNet.Web.Optimization/) that ships with MVC.\\n\\nI was watching the [ASP.NET Community Standup from October 7th, 2014](http://youtu.be/NgbA2BxNweE?list=PL0M0zPgJ3HSftTAAHttA3JQU4vOjXFquF) and learned that the ASP.Net team is not planning to migrate [Microsoft.AspNet.Web.Optimization](https://www.nuget.org/packages/Microsoft.AspNet.Web.Optimization/) to the next version of ASP.Net. Instead they\'re looking to make use of JavaScript task runners like [Grunt](http://gruntjs.com/) and maybe [Gulp](http://gulpjs.com/). Perhaps you\'re even dimly aware that they\'ve been taking steps to make these runners more of a first class citizen in Visual Studio, hence the recent release of the new and groovy [Task Runner Explorer](http://visualstudiogallery.msdn.microsoft.com/8e1b4368-4afb-467a-bc13-9650572db708).\\n\\nGulp has been on my radar for a while now as has Grunt. By \\"on my radar\\" what I really mean is \\"Hmmmm, I really need to learn this..... perhaps I could wait until the [Betamax vs VHS battles](http://en.wikipedia.org/wiki/Videotape_format_war) are done? Oh never mind, here we go...\\".\\n\\nMy understanding is that Grunt and Gulp essentially do the same thing (run tasks in JavaScript) but have different approaches. Grunt is more about configuration, Gulp is more about code. At present Gulp also has a performance advantage as it does less IO than Grunt - though I understand that\'s due to change in the future. But generally my preference is code over configuration. On that basis I decided that I was going to give Gulp first crack.\\n\\n## Bub bye Web Optimization\\n\\nI already had a project that used [Web Optimization](https://github.com/johnnyreilly/Proverb) to bundle JavaScript and CSS files. When debugging on my own machine Web Optimization served up the full JavaScript and CSS files. Thanks to the magic of source maps I was able to debug the TypeScript that created the JavaScript files too. Which was nice. When I deployed to production, Web Optimization minified and concatenated the JavaScript and CSS files. This meant I had a single HTTP request for JavaScript and a single HTTP request for CSS. This was also... nooice!\\n\\nI took a copy of my existing project and created a [new repo for it on GitHub](https://github.com/johnnyreilly/Proverb-gulp). It was very simple in terms of bundling. It had a `BundleConfig` that created 2 bundles; 1 for JavaScript and 1 for CSS:\\n\\n```cs\\nusing System.Web;\\nusing System.Web.Optimization;\\n\\nnamespace Proverb.Web\\n{\\n    public class BundleConfig\\n    {\\n        // For more information on bundling, visit http://go.microsoft.com/fwlink/?LinkId=301862\\n        public static void RegisterBundles(BundleCollection bundles)\\n        {\\n            var angularApp = new ScriptBundle(\\"~/angularApp\\").Include(\\n\\n                // Vendor Scripts\\n                \\"~/scripts/jquery-{version}.js\\",\\n                \\"~/scripts/angular.js\\",\\n                \\"~/scripts/angular-animate.js\\",\\n                \\"~/scripts/angular-route.js\\",\\n                \\"~/scripts/angular-sanitize.js\\",\\n                \\"~/scripts/angular-ui/ui-bootstrap-tpls.js\\",\\n\\n                \\"~/scripts/toastr.js\\",\\n                \\"~/scripts/moment.js\\",\\n                \\"~/scripts/spin.js\\",\\n                \\"~/scripts/underscore.js\\",\\n\\n                // Bootstrapping\\n                \\"~/app/app.js\\",\\n                \\"~/app/config.route.js\\",\\n\\n                // common Modules\\n                \\"~/app/common/common.js\\",\\n                \\"~/app/common/logger.js\\",\\n                \\"~/app/common/spinner.js\\",\\n\\n                // common.bootstrap Modules\\n                \\"~/app/common/bootstrap/bootstrap.dialog.js\\"\\n                );\\n\\n            // directives\\n            angularApp.IncludeDirectory(\\"~/app/directives\\", \\"*.js\\", true);\\n\\n            // services\\n            angularApp.IncludeDirectory(\\"~/app/services\\", \\"*.js\\", true);\\n\\n            // controllers\\n            angularApp.IncludeDirectory(\\"~/app/admin\\", \\"*.js\\", true);\\n            angularApp.IncludeDirectory(\\"~/app/about\\", \\"*.js\\", true);\\n            angularApp.IncludeDirectory(\\"~/app/dashboard\\", \\"*.js\\", true);\\n            angularApp.IncludeDirectory(\\"~/app/layout\\", \\"*.js\\", true);\\n            angularApp.IncludeDirectory(\\"~/app/sayings\\", \\"*.js\\", true);\\n            angularApp.IncludeDirectory(\\"~/app/sages\\", \\"*.js\\", true);\\n\\n            bundles.Add(angularApp);\\n\\n            bundles.Add(new StyleBundle(\\"~/Content/css\\").Include(\\n                \\"~/content/ie10mobile.css\\",\\n                \\"~/content/bootstrap.css\\",\\n                \\"~/content/font-awesome.css\\",\\n                \\"~/content/toastr.css\\",\\n                \\"~/content/styles.css\\"\\n            ));\\n        }\\n    }\\n}\\n```\\n\\nI set myself a task. I wanted to be able to work in \\\\***exactly**\\\\* the way I was working now. But using Gulp instead of Web Optimization. I wanted to lose the BundleConfig above and remove Web Optimization from my application, secure in the knowledge that I had lost nothing. Could it be done? Read on!\\n\\n## Installing Gulp (and Associates)\\n\\nI fired up Visual Studio and looked for an excuse to use the Task Runner Explorer. The first thing I needed was Gulp. My machine already had Node and NPM installed so I went to the command line to install Gulp globally:\\n\\n```ps\\nnpm install gulp -g\\n```\\n\\nNow to start to plug Gulp into my web project. It was time to make the introductions: Visual Studio meet NPM. At the root of the web project I created a `package.json` file by executing the following command and accepting all the defaults:\\n\\n```ps\\nnpm init\\n```\\n\\nI wanted to add Gulp as a development dependency of my project: (\\"Development\\" because I only need to run tasks at development time. My app has no dependency on Gulp at runtime - at that point it\'s just about serving up static files.)\\n\\n```ps\\nnpm install gulp --save-dev\\n```\\n\\nThis installs gulp local to the project as a development dependency. As a result we now have a \\"node_modules\\" folder sat in our root which contains our node packages. Currently, as our `package.json` reveals, this is only gulp:\\n\\n```json\\n\\"devDependencies\\": {\\n    \\"gulp\\": \\"^3.8.8\\"\\n  }\\n```\\n\\nIt\'s time to go to town. Let\'s install all the packages we\'re going to need to bundle and minify JavaScript and CSS:\\n\\n```sh\\nnpm install gulp-concat gulp-uglify gulp-rev del path gulp-ignore gulp-asset-manifest gulp-minify-css --save-dev\\n```\\n\\nThis installs the packages as dev dependencies (as you\'ve probably guessed) and leaves us with a list of dev dependencies like this:\\n\\n```json\\n\\"devDependencies\\": {\\n    \\"del\\": \\"^0.1.3\\",\\n    \\"gulp\\": \\"^3.8.8\\",\\n    \\"gulp-asset-manifest\\": \\"0.0.5\\",\\n    \\"gulp-concat\\": \\"^2.4.1\\",\\n    \\"gulp-ignore\\": \\"^1.2.1\\",\\n    \\"gulp-minify-css\\": \\"^0.3.10\\",\\n    \\"gulp-rev\\": \\"^1.1.0\\",\\n    \\"gulp-uglify\\": \\"^1.0.1\\",\\n    \\"path\\": \\"^0.4.9\\"\\n  }\\n```\\n\\n## Making `gulpfile.js`\\n\\nSo now I was ready. I had everything I needed to replace my `BundleConfig.cs`. I created a new file called `gulpfile.js` in the root of my web project that looked like this:\\n\\n```js\\n/// <vs AfterBuild=\'default\' />\\nvar gulp = require(\'gulp\');\\n\\n// Include Our Plugins\\nvar concat = require(\'gulp-concat\');\\nvar ignore = require(\'gulp-ignore\');\\nvar manifest = require(\'gulp-asset-manifest\');\\nvar minifyCss = require(\'gulp-minify-css\');\\nvar uglify = require(\'gulp-uglify\');\\nvar rev = require(\'gulp-rev\');\\nvar del = require(\'del\');\\nvar path = require(\'path\');\\n\\nvar tsjsmapjsSuffix = \'.{ts,js.map,js}\';\\nvar excludetsjsmap = \'**/*.{ts,js.map}\';\\n\\nvar bundleNames = { scripts: \'scripts\', styles: \'styles\' };\\n\\nvar filesAndFolders = {\\n  base: \'.\',\\n  buildBaseFolder: \'./build/\',\\n  debug: \'debug\',\\n  release: \'release\',\\n  css: \'css\',\\n\\n  // The fonts we want Gulp to process\\n  fonts: [\'./fonts/*.*\'],\\n\\n  // The scripts we want Gulp to process - adapted from BundleConfig\\n  scripts: [\\n    // Vendor Scripts\\n    \'./scripts/angular.js\',\\n    \'./scripts/angular-animate.js\',\\n    \'./scripts/angular-route.js\',\\n    \'./scripts/angular-sanitize.js\',\\n    \'./scripts/angular-ui/ui-bootstrap-tpls.js\',\\n\\n    \'./scripts/toastr.js\',\\n    \'./scripts/moment.js\',\\n    \'./scripts/spin.js\',\\n    \'./scripts/underscore.js\',\\n\\n    // Bootstrapping\\n    \'./app/app\' + tsjsmapjsSuffix,\\n    \'./app/config.route\' + tsjsmapjsSuffix,\\n\\n    // common Modules\\n    \'./app/common/common\' + tsjsmapjsSuffix,\\n    \'./app/common/logger\' + tsjsmapjsSuffix,\\n    \'./app/common/spinner\' + tsjsmapjsSuffix,\\n\\n    // common.bootstrap Modules\\n    \'./app/common/bootstrap/bootstrap.dialog\' + tsjsmapjsSuffix,\\n\\n    // directives\\n    \'./app/directives/**/*\' + tsjsmapjsSuffix,\\n\\n    // services\\n    \'./app/services/**/*\' + tsjsmapjsSuffix,\\n\\n    // controllers\\n    \'./app/about/**/*\' + tsjsmapjsSuffix,\\n    \'./app/admin/**/*\' + tsjsmapjsSuffix,\\n    \'./app/dashboard/**/*\' + tsjsmapjsSuffix,\\n    \'./app/layout/**/*\' + tsjsmapjsSuffix,\\n    \'./app/sages/**/*\' + tsjsmapjsSuffix,\\n    \'./app/sayings/**/*\' + tsjsmapjsSuffix,\\n  ],\\n\\n  // The styles we want Gulp to process - adapted from BundleConfig\\n  styles: [\\n    \'./content/ie10mobile.css\',\\n    \'./content/bootstrap.css\',\\n    \'./content/font-awesome.css\',\\n    \'./content/toastr.css\',\\n    \'./content/styles.css\',\\n  ],\\n};\\n\\nfilesAndFolders.debugFolder =\\n  filesAndFolders.buildBaseFolder + \'/\' + filesAndFolders.debug + \'/\';\\nfilesAndFolders.releaseFolder =\\n  filesAndFolders.buildBaseFolder + \'/\' + filesAndFolders.release + \'/\';\\n\\n/**\\n * Create a manifest depending upon the supplied arguments\\n *\\n * @param {string} manifestName\\n * @param {string} bundleName\\n * @param {boolean} includeRelativePath\\n * @param {string} pathPrepend\\n */\\nfunction getManifest(\\n  manifestName,\\n  bundleName,\\n  includeRelativePath,\\n  pathPrepend,\\n) {\\n  // Determine filename (\\"./build/manifest-debug.json\\" or \\"./build/manifest-release.json\\"\\n  var manifestFile =\\n    filesAndFolders.buildBaseFolder + \'manifest-\' + manifestName + \'.json\';\\n\\n  return manifest({\\n    bundleName: bundleName,\\n    includeRelativePath: includeRelativePath,\\n    manifestFile: manifestFile,\\n    log: true,\\n    pathPrepend: pathPrepend,\\n    pathSeparator: \'/\',\\n  });\\n}\\n\\n// Delete the build folder\\ngulp.task(\'clean\', function (cb) {\\n  del([filesAndFolders.buildBaseFolder], cb);\\n});\\n\\n// Copy across all files in filesAndFolders.scripts to build/debug\\ngulp.task(\'scripts-debug\', [\'clean\'], function () {\\n  return gulp\\n    .src(filesAndFolders.scripts, { base: filesAndFolders.base })\\n    .pipe(gulp.dest(filesAndFolders.debugFolder));\\n});\\n\\n// Create a manifest.json for the debug build - this should have lots of script files in\\ngulp.task(\'manifest-scripts-debug\', [\'scripts-debug\'], function () {\\n  return gulp\\n    .src(filesAndFolders.scripts, { base: filesAndFolders.base })\\n    .pipe(ignore.exclude(\'**/*.{ts,js.map}\')) // Exclude ts and js.map files from the manifest (as they won\'t become script tags)\\n    .pipe(getManifest(filesAndFolders.debug, bundleNames.scripts, true));\\n});\\n\\n// Copy across all files in filesAndFolders.styles to build/debug\\ngulp.task(\'styles-debug\', [\'clean\'], function () {\\n  return gulp\\n    .src(filesAndFolders.styles, { base: filesAndFolders.base })\\n    .pipe(gulp.dest(filesAndFolders.debugFolder));\\n});\\n\\n// Create a manifest.json for the debug build - this should have lots of style files in\\ngulp.task(\\n  \'manifest-styles-debug\',\\n  [\'styles-debug\', \'manifest-scripts-debug\'],\\n  function () {\\n    return (\\n      gulp\\n        .src(filesAndFolders.styles, { base: filesAndFolders.base })\\n        //.pipe(ignore.exclude(\\"**/*.{ts,js.map}\\")) // Exclude ts and js.map files from the manifest (as they won\'t become script tags)\\n        .pipe(getManifest(filesAndFolders.debug, bundleNames.styles, true))\\n    );\\n  },\\n);\\n\\n// Concatenate & Minify JS for release into a single file\\ngulp.task(\'scripts-release\', [\'clean\'], function () {\\n  return (\\n    gulp\\n      .src(filesAndFolders.scripts)\\n      .pipe(ignore.exclude(\'**/*.{ts,js.map}\')) // Exclude ts and js.map files - not needed in release mode\\n\\n      .pipe(concat(\'app.js\')) // Make a single file - if you want to see the contents then include the line below\\n      //.pipe(gulp.dest(releaseFolder))\\n\\n      .pipe(uglify()) // Make the file titchy tiny small\\n      .pipe(rev()) // Suffix a version number to it\\n      .pipe(gulp.dest(filesAndFolders.releaseFolder))\\n  ); // Write single versioned file to build/release folder\\n});\\n\\n// Create a manifest.json for the release build - this should just have a single file for scripts\\ngulp.task(\'manifest-scripts-release\', [\'scripts-release\'], function () {\\n  return gulp\\n    .src(filesAndFolders.buildBaseFolder + filesAndFolders.release + \'/*.js\')\\n    .pipe(getManifest(filesAndFolders.release, bundleNames.scripts, false));\\n});\\n\\n// Copy across all files in filesAndFolders.styles to build/debug\\ngulp.task(\'styles-release\', [\'clean\'], function () {\\n  return (\\n    gulp\\n      .src(filesAndFolders.styles)\\n      .pipe(concat(\'app.css\')) // Make a single file - if you want to see the contents then include the line below\\n      //.pipe(gulp.dest(releaseFolder))\\n\\n      .pipe(minifyCss()) // Make the file titchy tiny small\\n      .pipe(rev()) // Suffix a version number to it\\n      .pipe(\\n        gulp.dest(filesAndFolders.releaseFolder + \'/\' + filesAndFolders.css),\\n      )\\n  ); // Write single versioned file to build/release folder\\n});\\n\\n// Create a manifest.json for the debug build - this should have a single style files in\\ngulp.task(\\n  \'manifest-styles-release\',\\n  [\'styles-release\', \'manifest-scripts-release\'],\\n  function () {\\n    return gulp\\n      .src(filesAndFolders.releaseFolder + \'**/*.css\')\\n      .pipe(\\n        getManifest(\\n          filesAndFolders.release,\\n          bundleNames.styles,\\n          false,\\n          filesAndFolders.css + \'/\',\\n        ),\\n      );\\n  },\\n);\\n\\n// Copy across all fonts in filesAndFolders.fonts to both release and debug locations\\ngulp.task(\'fonts\', [\'clean\'], function () {\\n  return gulp\\n    .src(filesAndFolders.fonts, { base: filesAndFolders.base })\\n    .pipe(gulp.dest(filesAndFolders.debugFolder))\\n    .pipe(gulp.dest(filesAndFolders.releaseFolder));\\n});\\n\\n// Default Task\\ngulp.task(\'default\', [\\n  \'scripts-debug\',\\n  \'manifest-scripts-debug\',\\n  \'styles-debug\',\\n  \'manifest-styles-debug\',\\n  \'scripts-release\',\\n  \'manifest-scripts-release\',\\n  \'styles-release\',\\n  \'manifest-styles-release\',\\n  \'fonts\',\\n]);\\n```\\n\\n## What `gulpfile.js` does\\n\\nThis file does a number of things each time it is run. First of all it deletes any `build` folder in the root of the web project so we\'re ready to build anew. Then it packages up files both for debug and for release mode. For debug it does the following:\\n\\n1. It copies the `ts`, `js.map` and `js` files declared in `filesAndFolders.scripts` to the `build/debug` folder preserving their original folder structure. (So, for example, `app/app.ts`, `app/app.js.map` and `app/app.js` will all end up at `build/debug/app/app.ts`, `build/debug/app/app.js.map` and `build/debug/app/app.js` respectively.) This is done to allow the continued debugging of the original TypeScript files when running in debug mode.\\n2. It copies the `css` files declared in `filesAndFolders.styles` to the `build/debug` folder preserving their original folder structure. (So `content/bootstrap.css` will end up at `build/debug/content/bootstrap.css`.)\\n3. It creates a `build/manifest-debug.json` file which contains details of all the script and style files that have been packaged up:\\n\\n```json\\n{\\n  \\"scripts\\": [\\n    \\"scripts/angular.js\\",\\n    \\"scripts/angular-animate.js\\",\\n    \\"scripts/angular-route.js\\",\\n    \\"scripts/angular-sanitize.js\\",\\n    \\"scripts/angular-ui/ui-bootstrap-tpls.js\\",\\n    \\"scripts/toastr.js\\",\\n    \\"scripts/moment.js\\",\\n    \\"scripts/spin.js\\",\\n    \\"scripts/underscore.js\\",\\n    \\"app/app.js\\",\\n    \\"app/config.route.js\\",\\n    \\"app/common/common.js\\",\\n    \\"app/common/logger.js\\",\\n    \\"app/common/spinner.js\\",\\n    \\"app/common/bootstrap/bootstrap.dialog.js\\",\\n    \\"app/directives/imgPerson.js\\",\\n    \\"app/directives/serverError.js\\",\\n    \\"app/directives/sidebar.js\\",\\n    \\"app/directives/spinner.js\\",\\n    \\"app/directives/waiter.js\\",\\n    \\"app/directives/widgetClose.js\\",\\n    \\"app/directives/widgetHeader.js\\",\\n    \\"app/directives/widgetMinimize.js\\",\\n    \\"app/services/datacontext.js\\",\\n    \\"app/services/repositories.js\\",\\n    \\"app/services/repository.sage.js\\",\\n    \\"app/services/repository.saying.js\\",\\n    \\"app/about/about.js\\",\\n    \\"app/admin/admin.js\\",\\n    \\"app/dashboard/dashboard.js\\",\\n    \\"app/layout/shell.js\\",\\n    \\"app/layout/sidebar.js\\",\\n    \\"app/layout/topnav.js\\",\\n    \\"app/sages/sageDetail.js\\",\\n    \\"app/sages/sageEdit.js\\",\\n    \\"app/sages/sages.js\\",\\n    \\"app/sayings/sayingEdit.js\\",\\n    \\"app/sayings/sayings.js\\"\\n  ],\\n  \\"styles\\": [\\n    \\"content/ie10mobile.css\\",\\n    \\"content/bootstrap.css\\",\\n    \\"content/font-awesome.css\\",\\n    \\"content/toastr.css\\",\\n    \\"content/styles.css\\"\\n  ]\\n}\\n```\\n\\nFor release our gulpfile works with the same resources but has a different aim. Namely to minimise the the number of HTTP requests, obfuscate the code and version the files produced to prevent caching issues. To achieve those lofty aims it does the following:\\n\\n1. It concatenates together all the `js` files declared in `filesAndFolders.scripts`, minifies them and writes them to a single `build/release/app-{xxxxx}.js` file (where `-{xxxxx}` represents a version created by gulp-rev).\\n2. It concatenates together all the `css` files declared in `filesAndFolders.styles`, minifies them and writes them to a single `build/release/css/app-{xxxxx}.css` file. The file is placed in a css subfolder because of relative paths specified in the CSS file.\\n3. It creates a `build/manifest-release.json` file which contains details of all the script and style files that have been packaged up:\\n\\n```json\\n{\\n  \\"scripts\\": [\\"app-95d1e06d.js\\"],\\n  \\"styles\\": [\\"css/app-1a6256ea.css\\"]\\n}\\n```\\n\\nAs you can see, the number of files included are reduced down to 2; 1 for JavaScript and 1 for CSS.\\n\\nFinally, for both the debug and release packages the contents of the `fonts` folder is copied across wholesale, preserving the original folder structure. This is because the CSS files contain relative references that point to the font files. If I had image files which were referenced by my CSS I\'d similarly need to include these in the build process.\\n\\n## Task Runner Explorer gets in on the action\\n\\nThe eagle eyed amongst you will also have noticed a peculiar first line to our `gulpfile.js`:\\n\\n```js\\n/// <vs AfterBuild=\'default\' />\\n```\\n\\nThis mysterious comment is actually how the Task Runner Explorer hooks our `gulpfile.js` into the Visual Studio build process. Our \\"magic comment\\" ensures that on the `AfterBuild` event, Task Runner Explorer runs the `default` task in our `gulpfile.js`. The reason we\'re using the `AfterBuild` event rather than the `BeforeBuild` event is because our project contains TypeScript and we need the transpiled JavaScript to be created before we can usefully run our package tasks. If we were using JavaScript alone then that wouldn\'t be an issue and either build event would do.\\n\\n![](Screenshot-2014-10-21-17.02.11.webp)\\n\\n## How do I use this in my HTML?\\n\\nWell this is magnificent - we have a gulpfile that builds our debug and release packages. The question now is, how do we use it?\\n\\nWeb Optimization made our lives really easy. Up in my head I had a `@Styles.Render(\\"~/Content/css\\")` which pushed out my CSS and down at the foot of the body tag I had a `@Scripts.Render(\\"~/angularApp\\")` which pushed out my script tags. `Styles` and `Scripts` are server-side utilities. It would be very easy to write equivalent utility classes that, depending on whether we were in debug or not, read the appropriate `build/manifest-xxxxxx.json` file and served up either debug or release `style` / `script` tags.\\n\\nThat would be pretty simple - and for what it\'s worth \\\\*\\\\*simple is <u>good</u>\\n\\n\\\\*\\\\*. But today I felt like a challenge. What say server side rendering had been outlawed? A draconian ruling had been passed and all you had to play with was HTML / JavaScript and a server API that served up JSON? What would you do then? (All fantasy I know... But go with me on this - it\'s a journey.) Or more sensibly, what if you just want to remove some of the work your app is doing server-side to bundle and minify. Just serve up static assets instead. Spend less money in Azure why not?\\n\\nBefore I make all the changes let\'s review where we were. I had a single MVC view which, in terms of bundles, CSS and JavaScript pretty much looked like this:\\n\\n```html\\n<!doctype html>\\n<html>\\n  <head>\\n    \x3c!-- ... --\x3e\\n    @Styles.Render(\\"~/Content/css\\")\\n  </head>\\n  <body>\\n    \x3c!-- ... --\x3e\\n\\n    @Scripts.Render(\\"~/angularApp\\")\\n    <script>\\n      (function () {\\n        $.getJSON(\'@Url.Content(\\"~/Home/StartApp\\")\').done(\\n          function (startUpData) {\\n            var appConfig = $.extend({}, startUpData, {\\n              appRoot: \'@Url.Content(\\"~/\\")\',\\n              remoteServiceRoot: \'@Url.Content(\\"~/api/\\")\',\\n            });\\n\\n            angularApp.start({\\n              thirdPartyLibs: {\\n                moment: window.moment,\\n                toastr: window.toastr,\\n                underscore: window._,\\n              },\\n              appConfig: appConfig,\\n            });\\n          },\\n        );\\n      })();\\n    <\/script>\\n  </body>\\n</html>\\n```\\n\\nThis is already more a complicated example than most peoples use cases. Essentially what\'s happening here is both bundles are written out as part of the HTML and then, once the scripts have loaded the Angular app is bootstrapped with some configuration loaded from the server by a good old jQuery AJAX call.\\n\\nAfter reading [an article about script loading by the magnificently funny Jake Archibald](http://www.html5rocks.com/en/tutorials/speed/script-loading/) I felt ready. I cast my MVC view to the four winds and created myself a straight HTML file:\\n\\n```html\\n<!doctype html>\\n<html>\\n  <head>\\n    \x3c!-- ... --\x3e\\n  </head>\\n  <body>\\n    \x3c!-- ... --\x3e\\n\\n    <script src=\\"Scripts/jquery-2.1.1.min.js\\"><\/script>\\n    <script>\\n      (function () {\\n        var appConfig = {};\\n        var scriptsToLoad;\\n\\n        /**\\n         * Handler which fires as each script loads\\n         */\\n        function onScriptLoad(event) {\\n          scriptsToLoad -= 1;\\n\\n          // Now all the scripts are present start the app\\n          if (scriptsToLoad === 0) {\\n            angularApp.start({\\n              thirdPartyLibs: {\\n                moment: window.moment,\\n                toastr: window.toastr,\\n                underscore: window._,\\n              },\\n              appConfig: appConfig,\\n            });\\n          }\\n        }\\n\\n        // Load startup data from the server\\n        $.getJSON(\'api/Startup\').done(function (startUpData) {\\n          appConfig = startUpData;\\n\\n          // Determine the assets folder depending upon whether in debug mode or not\\n          var buildFolder = appConfig.appRoot + \'build/\';\\n          var debugOrRelease = appConfig.inDebug ? \'debug\' : \'release\';\\n          var manifestFile =\\n            buildFolder + \'manifest-\' + debugOrRelease + \'.json\';\\n          var outputFolder = buildFolder + debugOrRelease + \'/\';\\n\\n          // Load JavaScript and CSS listed in manifest file\\n          $.getJSON(manifestFile).done(function (manifest) {\\n            manifest.styles.forEach(function (href) {\\n              var link = document.createElement(\'link\');\\n\\n              link.rel = \'stylesheet\';\\n              link.media = \'all\';\\n              link.href = outputFolder + href;\\n\\n              document.head.appendChild(link);\\n            });\\n\\n            scriptsToLoad = manifest.scripts.length;\\n            manifest.scripts.forEach(function (src) {\\n              var script = document.createElement(\'script\');\\n\\n              script.onload = onScriptLoad;\\n              script.src = outputFolder + src;\\n              script.async = false;\\n\\n              document.head.appendChild(script);\\n            });\\n          });\\n        });\\n      })();\\n    <\/script>\\n  </body>\\n</html>\\n```\\n\\nIf you very carefully compare the HTML above the MVC view that it replaces you can see the commonalities. They are doing pretty much the same thing - the only real difference is the bootstrapping API. Previously it was an MVC endpoint at `/Home/StartApp`. Now it\'s a Web API endpoint at `api/Startup`. Here\'s how it works:\\n\\n1. A jQuery AJAX call kicks off a call to load the bootstrapping / app config data. Importantly this data includes whether the app is running in debug or not.\\n2. Depending on the `isDebug` flag the app either loads the `build/manifest-debug.json` or `build/manifest-release.json` manifest.\\n3. For each CSS file in the styles bundle a `link` element is created and added to the page.\\n4. For each JavaScript file in the scripts bundle a `script` element is created and added to the page.\\n\\nIt\'s worth pointing out that this also has a performance edge over Web Optimization as the assets are loaded asynchronously! (Yes I know it says `script.async = false` but that\'s not what you think it is... Go read Jake\'s article!)\\n\\nTo finish off I had to make a few tweaks to my `web.config`:\\n\\n```xml\\n\x3c!-- Allow ASP.Net to serve up JSON files --\x3e\\n    <system.webServer>\\n        <staticContent>\\n            <mimeMap fileExtension=\\".json\\" mimeType=\\"application/json\\"/>\\n        </staticContent>\\n    </system.webServer>\\n\\n    \x3c!-- The build folder (and it\'s child folder \\"debug\\") will not be cached.\\n         When people are debugging they don\'t want to cache --\x3e\\n    <location path=\\"build\\">\\n        <system.webServer>\\n            <staticContent>\\n                <clientCache cacheControlMode=\\"DisableCache\\"/>\\n            </staticContent>\\n        </system.webServer>\\n    </location>\\n\\n    \x3c!-- The release folder will be cached for a loooooong time\\n         When you\'re in Production caching is your friend --\x3e\\n    <location path=\\"build/release\\">\\n        <system.webServer>\\n            <staticContent>\\n                <clientCache cacheControlMode=\\"UseMaxAge\\"/>\\n            </staticContent>\\n        </system.webServer>\\n    </location>\\n```\\n\\n## I want to publish, how do I include my assets?\\n\\nIt\'s time for some `csproj` trickery. I must say I think I\'ll be glad to see the back of project files when ASP.Net vNext ships. This is what you need:\\n\\n```xml\\n<Target Name=\\"AfterBuild\\">\\n    <ItemGroup>\\n      \x3c!-- what ever is in the build folder should be included in the project --\x3e\\n      <Content Include=\\"build\\\\**\\\\*.*\\" />\\n    </ItemGroup>\\n  </Target>\\n```\\n\\nWhat\'s happening here is that \\\\*_after_\\\\* a build Visual Studio considers the complete contents of the build folder to part of the project. It\'s after the build because the folder will be deleted and reconstructed as part of the build."},{"id":"caching-and-cache-busting-in-angularjs-with-http-interceptors","metadata":{"permalink":"/caching-and-cache-busting-in-angularjs-with-http-interceptors","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2014-10-06-caching-and-cache-busting-in-angularjs-with-http-interceptors/index.md","source":"@site/blog/2014-10-06-caching-and-cache-busting-in-angularjs-with-http-interceptors/index.md","title":"Caching and Cache-Busting in AngularJS with HTTP interceptors","description":"Learn how to modify GET request URLs for static resources and AngularJS views with HTTP interceptors using version numbers and unique querystrings.","date":"2014-10-06T00:00:00.000Z","tags":[{"inline":false,"label":"Angular JS","permalink":"/tags/angularjs","description":"The original Angular JS framework."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":4.075,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"caching-and-cache-busting-in-angularjs-with-http-interceptors","title":"Caching and Cache-Busting in AngularJS with HTTP interceptors","authors":"johnnyreilly","tags":["angularjs","typescript"],"hide_table_of_contents":false,"description":"Learn how to modify GET request URLs for static resources and AngularJS views with HTTP interceptors using version numbers and unique querystrings."},"unlisted":false,"prevItem":{"title":"Using Gulp in Visual Studio instead of Web Optimization","permalink":"/using-gulp-in-visual-studio-instead-of-web-optimization"},"nextItem":{"title":"He tasks me; he heaps me.... I will wreak that MOQ upon him.","permalink":"/he-tasks-me-he-heaps-me-i-will-wreak"}},"content":"## Loading On-Demand and Caching\\n\\n\x3c!--truncate--\x3e\\n\\n[I\'ve written before about my own needs for caching and cache-busting when using RequireJS.](../2014-03-05-caching-and-cache-busting-with-requirejs/index.md) Long story short, when I\'m loading _static_ resources (scripts / views etc) on demand from the server I want to do a little URL fiddling along the way. I want to do that to cater for these 2 scenarios:\\n\\n1. _In Development_ \\\\- I want my URLs for static resources to have a unique querystring with each request to ensure that resources are loaded afresh each time. (eg so a GET request URL might look like this: \\"/app/layout/sidebar.html?v=IAmRandomYesRandomRandomIsWhatIAm58965782\\")\\n2. _In Production_ \\\\- I want my URLs for static resources to have a querystring with that is driven by the application version number. This means that static resources can potentially be cached with a given querystring - subsequent requests should result in a 304 status code (indicating \u201CNot Modified\u201D) and local cache should be used. But when a new version of the app is rolled out and the app version is incremented then the querystring will change and resources will be loaded anew. (eg a GET request URL might look like this: \\"/app/layout/sidebar.html?v=1.0.5389.16180\\")\\n\\n## Loading Views in AngularJS Using this Approach\\n\\nI have exactly the same use cases when I\'m using AngularJS for views. Out of the box with AngularJS 1.x views are loaded lazily (unlike controllers, services etc). For that reason I want to use the same approach I\'ve outlined above to load my views. Also, I want to prepend my URLs with the root of my application - this allows me to cater for my app being deployed in a virtual folder.\\n\\nIt turns out that\'s pretty easy thanks to [HTTP interceptors](https://docs.angularjs.org/api/ng/service/$http#interceptors). They allow you to step into the pipeline and access and modify requests and responses made by your application. When AngularJS loads a view it\'s the HTTP service doing the heavy lifting. So to deal with my own use case, I just need to add in an HTTP interceptor that amends the get request. This is handled in the example that follows in the `configureHttpProvider` function: (The example that follows is TypeScript - though if you just chopped out the interface and the type declarations you\'d find this is pretty much idiomatic JavaScript)\\n\\n```js\\ninterface config {\\n  appRoot: string; // eg \\"/\\"\\n  inDebug: boolean; // eg true or false\\n  urlCacheBusterSuffix: string; // if in debug this might look like this: \\"v=1412608547047\\",\\n  // if not in debug this might look like this: \\"v=1.0.5389.16180\\"\\n}\\n\\nfunction configureHttpProvider() {\\n  // This is the name of our HTTP interceptor\\n  var serviceId = \'urlInterceptor\';\\n\\n  // We\'re going to create a service factory which will be our HTTP interceptor\\n  // It will be injected with a config object which is represented by the config interface above\\n  app.factory(serviceId, [\\n    \'$templateCache\',\\n    \'config\',\\n    function ($templateCache: ng.ITemplateCacheService, config: config) {\\n      // We\'re returning an object literal with a single function; the \\"request\\" function\\n      var service = {\\n        request: request,\\n      };\\n\\n      return service;\\n\\n      // Request will be called with a request config object which includes the URL which we will amend\\n      function request(requestConfig: ng.IRequestConfig) {\\n        // For the loading of HTML templates we want the appRoot to be prefixed to the path\\n        // and we want a suffix to either allow caching or prevent caching\\n        // (depending on whether in debug mode or not)\\n        if (\\n          requestConfig.method === \'GET\' &&\\n          endsWith(requestConfig.url, \'.html\')\\n        ) {\\n          // If this has already been placed into a primed template cache then we should leave the URL as is\\n          // so that the version in templateCache is served.  If we tweak the URL then it will not be found\\n          var cachedAlready = $templateCache.get(requestConfig.url);\\n          if (!cachedAlready) {\\n            // THIS IS THE MAGIC!!!!!!!!!!!!!!!\\n\\n            requestConfig.url =\\n              config.appRoot + requestConfig.url + config.urlCacheBusterSuffix;\\n\\n            // WE NOW HAVE A URL WHICH IS CACHE-FRIENDLY FOR OUR PURPOSES - REJOICE!!!!!!!!!!!\\n          }\\n        }\\n\\n        return requestConfig;\\n      }\\n\\n      // <a href=\\"http://stackoverflow.com/a/2548133/761388\\">a simple JavaScript string \\"endswith\\" implementation</a>\\n      function endsWith(str: string, suffix: string) {\\n        return str.indexOf(suffix, str.length - suffix.length) !== -1;\\n      }\\n    },\\n  ]);\\n\\n  // This adds our service factory interceptor into the pipeline\\n  app.config([\\n    \'$httpProvider\',\\n    function ($httpProvider: ng.IHttpProvider) {\\n      $httpProvider.interceptors.push(serviceId);\\n    },\\n  ]);\\n}\\n```\\n\\nThis interceptor steps in and amends each ajax request when all the following conditions hold true:\\n\\n1. It\'s a GET request.\\n2. It\'s requesting a file that ends \\".html\\" - a template basically.\\n3. The template cache does not already contain the template. I left this out at first and got bitten when I found that the contents of the template cache were being ignored for pre-primed templates. Ugly.\\n\\n## Interesting technique.... How do I apply it?\\n\\nIsn\'t it always much more helpful when you can see an example of code in the context of which it is actually used? Course it is! If you want that then take a look at [`app.ts`](https://github.com/johnnyreilly/Proverb/blob/master/Proverb.Web/app/app.ts) on GitHub. And if you\'d like the naked JavaScript well [that\'s there too](https://github.com/johnnyreilly/Proverb/blob/master/Proverb.Web/app/app.js)."},{"id":"he-tasks-me-he-heaps-me-i-will-wreak","metadata":{"permalink":"/he-tasks-me-he-heaps-me-i-will-wreak","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2014-10-03-he-tasks-me-he-heaps-me-i-will-wreak/index.md","source":"@site/blog/2014-10-03-he-tasks-me-he-heaps-me-i-will-wreak/index.md","title":"He tasks me; he heaps me.... I will wreak that MOQ upon him.","description":"Use Moq to simplify async testing, with ReturnAsync method. For testing a class that consumes async API, mock it using Task.Delay with Moqs Returns.","date":"2014-10-03T00:00:00.000Z","tags":[{"inline":false,"label":"Automated Testing","permalink":"/tags/automated-testing","description":"How to perform the automation of tests."}],"readingTime":3.045,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"he-tasks-me-he-heaps-me-i-will-wreak","title":"He tasks me; he heaps me.... I will wreak that MOQ upon him.","authors":"johnnyreilly","tags":["automated testing"],"hide_table_of_contents":false,"description":"Use Moq to simplify async testing, with ReturnAsync method. For testing a class that consumes async API, mock it using Task.Delay with Moqs Returns."},"unlisted":false,"prevItem":{"title":"Caching and Cache-Busting in AngularJS with HTTP interceptors","permalink":"/caching-and-cache-busting-in-angularjs-with-http-interceptors"},"nextItem":{"title":"Journalling the Migration of Jasmine Tests to TypeScript","permalink":"/migrating-jasmine-tests-to-typescript"}},"content":"Enough with the horrific misquotes - this is about Moq and async (that\'s my slight justification for robbing Herman Melville).\\n\\n\x3c!--truncate--\x3e\\n\\nIt\'s pretty straightforward to use Moq to do async testing thanks to it\'s marvellous `ReturnsAsync` method. That means it\'s really easy to test a class that consumes an async API. Below is an example of a class that does just that: (it so happens that this class is a Web API controller but that\'s pretty irrelevant to be honest)\\n\\n```cs\\nnamespace Proverb.Web.Controllers\\n{\\n    // ISageService included inline for ease of explanation\\n    public interface ISageService\\n    {\\n        Task<int> DeleteAsync(int id);\\n    }\\n\\n    public class SageController : ApiController\\n    {\\n        ISageService _sageService;\\n\\n        public SageController(ISageService userService)\\n        {\\n            _sageService = userService;\\n        }\\n\\n        public async Task<IHttpActionResult> Delete(int id)\\n        {\\n            int deleteCount = await _sageService.DeleteAsync(id);\\n\\n            if (deleteCount == 0)\\n                return NotFound();\\n            else\\n                return Ok();\\n        }\\n   }\\n}\\n```\\n\\nTo mock the `_sageService.DeleteAsync` method it\'s as easy as this:\\n\\n```cs\\nnamespace Proverb.Web.Tests.ASPNet.Controllers\\n{\\n    [TestClass]\\n    public class SageControllerTests\\n    {\\n        private Mock<ISageService> _sageServiceMock;\\n        private SageController _controller;\\n\\n        [TestInitialize]\\n        public void Initialise()\\n        {\\n            _sageServiceMock = new Mock<ISageService>();\\n\\n            _controller = new SageController(_sageServiceMock.Object);\\n        }\\n\\n        [TestMethod]\\n        public async Task Delete_returns_a_NotFound()\\n        {\\n            _sageServiceMock\\n                .Setup(x => x.DeleteAsync(_sage.Id))\\n                .ReturnsAsync(0); // This makes me *so* happy...\\n\\n            IHttpActionResult result = await _controller.Delete(_sage.Id);\\n\\n            var notFound = result as NotFoundResult;\\n            Assert.IsNotNull(notFound);\\n            _sageServiceMock.Verify(x => x.DeleteAsync(_sage.Id));\\n        }\\n\\n        [TestMethod]\\n        public async Task Delete_returns_an_Ok()\\n        {\\n            _sageServiceMock\\n                .Setup(x => x.DeleteAsync(_sage.Id))\\n                .ReturnsAsync(1); // I\'m still excited now!\\n\\n            IHttpActionResult result = await _controller.Delete(_sage.Id);\\n\\n            var ok = result as OkResult;\\n            Assert.IsNotNull(ok);\\n            _sageServiceMock.Verify(x => x.DeleteAsync(_sage.Id));\\n        }\\n    }\\n}\\n```\\n\\n## But wait.... What if there\'s like... Nothing?\\n\\nNope, I\'m not getting into metaphysics. Something more simple. What if the `async` API you\'re consuming returns just a `Task`? Not a `Task` of `int` but a simple old humble `Task`.\\n\\nSo to take our example we\'re going from this:\\n\\n```cs\\npublic interface ISageService\\n    {\\n        Task<int> DeleteAsync(int id);\\n    }\\n```\\n\\nTo this:\\n\\n```ts\\npublic interface ISageService\\n    {\\n        Task DeleteAsync(int id);\\n    }\\n```\\n\\nYour initial thought might be \\"well that\'s okay, I\'ll just lop off the `ReturnsAsync` statements and I\'m home free\\". That\'s what I thought anyway.... And I was \\\\***WRONG**\\\\*! A moments thought and you realise that there\'s still a return type - it\'s just `Task` now. What you want to do is something like this:\\n\\n```cs\\n_sageServiceMock\\n                .Setup(x => x.DeleteAsync(_sage.Id))\\n                .ReturnsAsync(void); // This\'ll definitely work... Probably\\n```\\n\\nNo it won\'t - `void` is not a real type and much as you might like it to, this is not going to work.\\n\\nSo right now you\'re thinking, well Moq probably has my back - it\'ll have something like `ReturnsTask`, right? Wrong! It\'s intentional it turns out - there\'s a discussion on [GitHub about the issue](https://github.com/Moq/moq4/issues/117). And in that discussion there\'s just what we need. We can use `Task.Delay` or `Task.FromResult` alongside Moq\'s good old `Returns` method and we\'re home free!\\n\\n## Here\'s one I made earlier...\\n\\n```cs\\nnamespace Proverb.Web.Controllers\\n{\\n    // ISageService again included inline for ease of explanation\\n    public interface ISageService\\n    {\\n        Task DeleteAsync(int id);\\n    }\\n\\n    public class SageController : ApiController\\n    {\\n        ISageService _sageService;\\n\\n        public SageController(ISageService userService)\\n        {\\n            _sageService = userService;\\n        }\\n\\n        public async Task<IHttpActionResult> Delete(int id)\\n        {\\n            await _sageService.DeleteAsync(id);\\n\\n            return Ok();\\n        }\\n   }\\n}\\n```\\n\\n```cs\\nnamespace Proverb.Web.Tests.ASPNet.Controllers\\n{\\n    [TestClass]\\n    public class SageControllerTests\\n    {\\n        private Mock<ISageService> _sageServiceMock;\\n        private SageController _controller;\\n\\n        readonly Task TaskOfNowt = Task.Delay(0);\\n        // Or you could use this equally valid but slightly more verbose approach:\\n        //readonly Task TaskOfNowt = Task.FromResult<object>(null);\\n\\n        [TestInitialize]\\n        public void Initialise()\\n        {\\n            _sageServiceMock = new Mock<ISageService>();\\n\\n            _controller = new SageController(_sageServiceMock.Object);\\n        }\\n\\n        [TestMethod]\\n        public async Task Delete_returns_an_Ok()\\n        {\\n            _sageServiceMock\\n                .Setup(x => x.DeleteAsync(_sage.Id))\\n                .Returns(TaskOfNowt); // Feels good doesn\'t it?\\n\\n            IHttpActionResult result = await _controller.Delete(_sage.Id);\\n\\n            var ok = result as OkResult;\\n            Assert.IsNotNull(ok);\\n            _sageServiceMock.Verify(x => x.DeleteAsync(_sage.Id));\\n        }\\n    }\\n}\\n```"},{"id":"migrating-jasmine-tests-to-typescript","metadata":{"permalink":"/migrating-jasmine-tests-to-typescript","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2014-09-13-migrating-jasmine-tests-to-typescript/index.md","source":"@site/blog/2014-09-13-migrating-jasmine-tests-to-typescript/index.md","title":"Journalling the Migration of Jasmine Tests to TypeScript","description":"John describes issues migrating Jasmine tests from JS to TypeScript, including tooling, typings, and missing dependencies.","date":"2014-09-13T00:00:00.000Z","tags":[{"inline":false,"label":"Automated Testing","permalink":"/tags/automated-testing","description":"How to perform the automation of tests."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."},{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."}],"readingTime":9.5,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"migrating-jasmine-tests-to-typescript","title":"Journalling the Migration of Jasmine Tests to TypeScript","authors":"johnnyreilly","tags":["automated testing","typescript","javascript"],"hide_table_of_contents":false,"description":"John describes issues migrating Jasmine tests from JS to TypeScript, including tooling, typings, and missing dependencies."},"unlisted":false,"prevItem":{"title":"He tasks me; he heaps me.... I will wreak that MOQ upon him.","permalink":"/he-tasks-me-he-heaps-me-i-will-wreak"},"nextItem":{"title":"Unit Testing an Angular Controller with Jasmine","permalink":"/unit-testing-angular-controller-with"}},"content":"I previously attempted to migrate my Jasmine tests from JavaScript to TypeScript. The last time I tried it didn\'t go so well and I bailed. Thank the Lord for source control. But feeling I shouldn\'t be deterred I decided to have another crack at it.\\n\\n\x3c!--truncate--\x3e\\n\\nI did manage it this time... Sort of. Unfortunately there was a problem which I discovered right at the end. An issue with the TypeScript / Visual Studio tooling. So, just to be clear, this is not a blog post of \\"do this and it will work perfectly\\". On this occasion there will be some rough edges. This post exists, as much as anything else, as a record of the problems I experienced - I hope it will prove useful. Here we go:\\n\\n## What to Migrate?\\n\\nI\'m going to use one of the test files in my my side project [Proverb](https://github.com/johnnyreilly/Proverb). It\'s the tests for an AngularJS controller called `sageDetail` \\\\- I\'ve written about it [before](../2014-09-10-unit-testing-angular-controller-with/index.md). Here it is in all it\'s JavaScript-y glory:\\n\\n```ts\\ndescribe(\'Proverb.Web -> app-> controllers ->\', function () {\\n  beforeEach(function () {\\n    module(\'app\');\\n  });\\n\\n  describe(\'sageDetail ->\', function () {\\n    var $rootScope,\\n      getById_deferred, // deferred used for promises\\n      $location,\\n      $routeParams_stub,\\n      common,\\n      datacontext, // controller dependencies\\n      sageDetailController; // the controller\\n\\n    beforeEach(inject(function (\\n      _$controller_,\\n      _$rootScope_,\\n      _$q_,\\n      _$location_,\\n      _common_,\\n      _datacontext_,\\n    ) {\\n      $rootScope = _$rootScope_;\\n      $q = _$q_;\\n\\n      $location = _$location_;\\n      common = _common_;\\n      datacontext = _datacontext_;\\n\\n      $routeParams_stub = { id: \'10\' };\\n      getById_deferred = $q.defer();\\n\\n      spyOn(datacontext.sage, \'getById\').and.returnValue(\\n        getById_deferred.promise,\\n      );\\n      spyOn(common, \'activateController\').and.callThrough();\\n      spyOn(common.logger, \'getLogFn\').and.returnValue(\\n        jasmine.createSpy(\'log\'),\\n      );\\n      spyOn($location, \'path\').and.returnValue(jasmine.createSpy(\'path\'));\\n\\n      sageDetailController = _$controller_(\'sageDetail\', {\\n        $location: $location,\\n        $routeParams: $routeParams_stub,\\n        common: common,\\n        datacontext: datacontext,\\n      });\\n    }));\\n\\n    describe(\'on creation ->\', function () {\\n      it(\\"controller should have a title of \'Sage Details\'\\", function () {\\n        expect(sageDetailController.title).toBe(\'Sage Details\');\\n      });\\n\\n      it(\'controller should have no sage\', function () {\\n        expect(sageDetailController.sage).toBeUndefined();\\n      });\\n\\n      it(\'datacontext.sage.getById should be called\', function () {\\n        expect(datacontext.sage.getById).toHaveBeenCalledWith(10, true);\\n      });\\n    });\\n\\n    describe(\'activateController ->\', function () {\\n      var sage_stub;\\n      beforeEach(function () {\\n        sage_stub = { name: \'John\' };\\n      });\\n\\n      it(\'should set sages to be the resolved promise values\', function () {\\n        getById_deferred.resolve(sage_stub);\\n        $rootScope.$digest(); // So Angular processes the resolved promise\\n\\n        expect(sageDetailController.sage).toBe(sage_stub);\\n      });\\n\\n      it(\\"should log \'Activated Sage Details View\' and set title with name\\", function () {\\n        getById_deferred.resolve(sage_stub);\\n        $rootScope.$digest(); // So Angular processes the resolved promise\\n\\n        expect(sageDetailController.log).toHaveBeenCalledWith(\\n          \'Activated Sage Details View\',\\n        );\\n        expect(sageDetailController.title).toBe(\\n          \'Sage Details: \' + sage_stub.name,\\n        );\\n      });\\n    });\\n\\n    describe(\'gotoEdit ->\', function () {\\n      var sage_stub;\\n      beforeEach(function () {\\n        sage_stub = { id: 20 };\\n      });\\n\\n      it(\'should set $location.path to edit URL\', function () {\\n        getById_deferred.resolve(sage_stub);\\n        $rootScope.$digest(); // So Angular processes the resolved promise\\n\\n        sageDetailController.gotoEdit();\\n\\n        expect($location.path).toHaveBeenCalledWith(\\n          \'/sages/edit/\' + sage_stub.id,\\n        );\\n      });\\n    });\\n  });\\n});\\n```\\n\\n## Off we go\\n\\nRighteo. Let\'s flip the switch. `sageDetail.js` you shall go to the ball! One wave of my magic wand and `sageDetail.js` becomes `sageDetail.ts`... Alakazam!! Of course we\'ve got to do the fiddling with the `csproj` file to include the dependent JavaScript files. (I\'ll be very pleased when ASP.Net vNext ships and I don\'t have to do this anymore....) So find this:\\n\\n```xml\\n<TypeScriptCompile Include=\\"app\\\\sages\\\\sageDetail.ts\\" />\\n```\\n\\nAnd add this:\\n\\n```xml\\n<Content Include=\\"app\\\\sages\\\\sageDetail.js\\">\\n  <DependentUpon>sageDetail.ts</DependentUpon>\\n</Content>\\n<Content Include=\\"app\\\\sages\\\\sageDetail.js.map\\">\\n  <DependentUpon>sageDetail.ts</DependentUpon>\\n</Content>\\n```\\n\\nWhat next? I\'ve a million red squigglies in my code. It\'s \\"could not find symbol\\" city. Why? Typings! We need typings! So let\'s begin - I\'m needing the Jasmine typings for starters. So let\'s hit NuGet and it looks like we need [this](http://www.nuget.org/packages/jasmine.TypeScript.DefinitelyTyped/):\\n\\n`Install-Package jasmine.TypeScript.DefinitelyTyped`That did no good at all. Still red squigglies. I\'m going to hazard a guess that this is something to do with the fact my JavaScript Unit Test project doesn\'t contain the various TypeScript artefacts that Visual Studio kindly puts into the web csproj for you. This is because I\'m keeping my JavaScript tests in a separate project from the code being tested. Also, the Visual Studio TypeScript tooling seems to work on the assumption that TypeScript will only be used within a web project; not a test project. Well I won\'t let that hold me back... Time to port the TypeScript artefacts in the web csproj over by hand. I\'ll take this:\\n\\n```xml\\n<Import Project=\\"$(MSBuildExtensionsPath32)\\\\Microsoft\\\\VisualStudio\\\\v$(VisualStudioVersion)\\\\TypeScript\\\\Microsoft.TypeScript.Default.props\\" Condition=\\"Exists(\'$(MSBuildExtensionsPath32)\\\\Microsoft\\\\VisualStudio\\\\v$(VisualStudioVersion)\\\\TypeScript\\\\Microsoft.TypeScript.Default.props\')\\" />\\n```\\n\\nAnd I\'ll also take this\\n\\n```xml\\n<PropertyGroup Condition=\\"\'$(Configuration)\' == \'Debug\'\\">\\n  <TypeScriptNoImplicitAny>True</TypeScriptNoImplicitAny>\\n</PropertyGroup>\\n<Import Project=\\"$(MSBuildExtensionsPath32)\\\\Microsoft\\\\VisualStudio\\\\v$(VisualStudioVersion)\\\\TypeScript\\\\Microsoft.TypeScript.targets\\" Condition=\\"Exists(\'$(MSBuildExtensionsPath32)\\\\Microsoft\\\\VisualStudio\\\\v$(VisualStudioVersion)\\\\TypeScript\\\\Microsoft.TypeScript.targets\')\\" />\\n```\\n\\nBingo bango - a difference. I no longer have red squigglies under the Jasmine statements (`describe`, `it` etc). But alas, I do everywhere else. One in particular draws my eye...\\n\\n## Could not find symbol \'$q\'\\n\\nOnce again TypeScript picks up the hidden bugs in my JavaScript:\\n\\n```ts\\n$q = _$q_;\\n```\\n\\nThat\'s right it\'s an implicit global. Quickly fixed:\\n\\n```ts\\nvar $q = _$q_;\\n```\\n\\n## Typings? Where we\'re going, we need typings...\\n\\nWe need more types. We\'re going to need the types created by our application; our controllers / services / directives etc. As well that we need the types used in the creation of the app. So the Angular typings etc. Since we\'re going to need to use `reference` statements to pull in the types created by our application I might as well use them to pull in the required definition files as well (eg `angular.d.ts`):\\n\\n```xml\\n/// <reference path=\\"../../../proverb.web/scripts/typings/angularjs/angular.d.ts\\" />\\n/// <reference path=\\"../../../proverb.web/scripts/typings/angularjs/angular-mocks.d.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/sages/sagedetail.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/common/common.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/services/datacontext.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/services/repository.sage.ts\\" />\\n```\\n\\nNow we need to work our way through the \\"variable \'x\' implicitly has an \'any\' type\\" messages. One thing we need to do is to amend our original sageDetails.ts file so that the `sageDetailRouteParams` interface and `SageDetail` class are exported from the controllers module. We can\'t use the types otherwise. Now we can add typings to our file - once finished it looks like this:\\n\\n```ts\\n/// <reference path=\\"../../../proverb.web/scripts/typings/angularjs/angular.d.ts\\" />\\n/// <reference path=\\"../../../proverb.web/scripts/typings/angularjs/angular-mocks.d.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/sages/sagedetail.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/common/common.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/services/datacontext.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/services/repository.sage.ts\\" />\\ndescribe(\'Proverb.Web -> app-> controllers ->\', function () {\\n  beforeEach(function () {\\n    module(\'app\');\\n  });\\n\\n  describe(\'sageDetail ->\', function () {\\n    var $rootScope: ng.IRootScopeService,\\n      // deferred used for promises\\n      getById_deferred: ng.IDeferred<sage>,\\n      // controller dependencies\\n      $location: ng.ILocationService,\\n      $routeParams_stub: controllers.sageDetailRouteParams,\\n      common: common,\\n      datacontext: datacontext,\\n      sageDetailController: controllers.SageDetail; // the controller\\n\\n    beforeEach(inject(function (\\n      _$controller_: any,\\n      _$rootScope_: ng.IRootScopeService,\\n      _$q_: ng.IQService,\\n      _$location_: ng.ILocationService,\\n      _common_: common,\\n      _datacontext_: datacontext,\\n    ) {\\n      $rootScope = _$rootScope_;\\n      var $q = _$q_;\\n\\n      $location = _$location_;\\n      common = _common_;\\n      datacontext = _datacontext_;\\n\\n      $routeParams_stub = { id: \'10\' };\\n      getById_deferred = $q.defer();\\n\\n      spyOn(datacontext.sage, \'getById\').and.returnValue(\\n        getById_deferred.promise,\\n      );\\n      spyOn(common, \'activateController\').and.callThrough();\\n      spyOn(common.logger, \'getLogFn\').and.returnValue(\\n        jasmine.createSpy(\'log\'),\\n      );\\n      spyOn($location, \'path\').and.returnValue(jasmine.createSpy(\'path\'));\\n\\n      sageDetailController = _$controller_(\'sageDetail\', {\\n        $location: $location,\\n        $routeParams: $routeParams_stub,\\n        common: common,\\n        datacontext: datacontext,\\n      });\\n    }));\\n\\n    describe(\'on creation ->\', function () {\\n      it(\\"controller should have a title of \'Sage Details\'\\", function () {\\n        expect(sageDetailController.title).toBe(\'Sage Details\');\\n      });\\n\\n      it(\'controller should have no sage\', function () {\\n        expect(sageDetailController.sage).toBeUndefined();\\n      });\\n\\n      it(\'datacontext.sage.getById should be called\', function () {\\n        expect(datacontext.sage.getById).toHaveBeenCalledWith(10, true);\\n      });\\n    });\\n\\n    describe(\'activateController ->\', function () {\\n      var sage_stub: sage;\\n      beforeEach(function () {\\n        sage_stub = {\\n          name: \'John\',\\n          id: 10,\\n          username: \'John\',\\n          email: \'john@\',\\n          dateOfBirth: new Date(),\\n        };\\n      });\\n\\n      it(\'should set sages to be the resolved promise values\', function () {\\n        getById_deferred.resolve(sage_stub);\\n        $rootScope.$digest(); // So Angular processes the resolved promise\\n\\n        expect(sageDetailController.sage).toBe(sage_stub);\\n      });\\n\\n      it(\\"should log \'Activated Sage Details View\' and set title with name\\", function () {\\n        getById_deferred.resolve(sage_stub);\\n        $rootScope.$digest(); // So Angular processes the resolved promise\\n\\n        expect(sageDetailController.log).toHaveBeenCalledWith(\\n          \'Activated Sage Details View\',\\n        );\\n        expect(sageDetailController.title).toBe(\\n          \'Sage Details: \' + sage_stub.name,\\n        );\\n      });\\n    });\\n\\n    describe(\'gotoEdit ->\', function () {\\n      var sage_stub: sage;\\n      beforeEach(function () {\\n        sage_stub = {\\n          name: \'John\',\\n          id: 20,\\n          username: \'John\',\\n          email: \'john@\',\\n          dateOfBirth: new Date(),\\n        };\\n      });\\n\\n      it(\'should set $location.path to edit URL\', function () {\\n        getById_deferred.resolve(sage_stub);\\n        $rootScope.$digest(); // So Angular processes the resolved promise\\n\\n        sageDetailController.gotoEdit();\\n\\n        expect($location.path).toHaveBeenCalledWith(\\n          \'/sages/edit/\' + sage_stub.id,\\n        );\\n      });\\n    });\\n  });\\n});\\n```\\n\\n## So That\'s All Good...\\n\\nExcept it\'s not. When I run the tests using Chutzpah my `sageDetail` controller tests aren\'t found. My spider sense is tingling. This is something to do with the `reference` statements. They\'re throwing Chutzpah off. No bother, I can fix that with a quick tweak of the project file:\\n\\n```xml\\n<PropertyGroup Condition=\\"\'$(Configuration)\' == \'Debug\'\\">\\n    <TypeScriptNoImplicitAny>True</TypeScriptNoImplicitAny>\\n    <TypeScriptRemoveComments>True</TypeScriptRemoveComments>\\n  </PropertyGroup>\\n```\\n\\nThe TypeScript compiler will now strip comments; which includes the `reference` statements. Now my tests are detected \\\\***and**\\\\* they run. Yay!\\n\\n## Who Killed the TypeScript Language Service?\\n\\nYup it\'s dead. Whilst the compilation itself has no issues, take a look at the errors being presented for just one of the files back in the original web project:\\n\\n![](Screenshot-2014-09-12-23.15.22.webp)\\n\\nIt looks like having one TypeScript project in a solution which uses `reference` comments somehow breaks the implicit referencing behaviour built into Visual Studio for other TypeScript projects in the solution. I can say this with some confidence as if I pull out the `reference` comments from the top of the test file that we\'ve converted then it\'s business as usual - the TypeScript Language Service lives once more. I\'m sure you can see the problem here though: the TypeScript test file doesn\'t compile. All rather unsatisfactory.\\n\\nI suspect that if I added `reference` comments throughout the web project the TypeScript Language Service would be just fine. But I rather like the implicit referencing functionality so I\'m not inclined to do that. After reaching something of a brick wall and thinking I had encountered a bug in the TypeScript Language service I [raised an issue on GitHub](https://github.com/Microsoft/TypeScript/issues/673).\\n\\n## Solutions....\\n\\nThanks to the help of [Mohamed Hegazy](https://github.com/mhegazy) it emerged that the problem was down to missing `reference` comments in my `sageDetail` controller tests. One thing I had not considered was the 2 different ways each of my TypeScript projects were working:\\n\\n- Proverb.Web uses the Visual Studio implicit referencing functionality. This means that I do not need to use `reference` comments in the TypeScript files in Proverb.Web.\\n- Proverb.Web.JavaScript does \\\\***not**\\\\* uses the implicit referencing functionality. It needs `reference` comments to resolve references.\\n\\nThe important thing to take away from this (and the thing I had overlooked) was that Proverb.Web.JavaScript uses `reference` comments to pull in Proverb.Web TypeScript files. Those files have dependencies which are \\\\***not**\\\\* stated using `reference` comments. So the compiler trips up when it tries to walk the dependency tree - there are no `reference` comments to be followed! So for example, `common.ts` has a dependency upon `logger.ts`. Fixing the TypeScript Language Service involves ensuring that the full dependency list is included in the `sageDetail` controller tests file, like so:\\n\\n```ts\\n/// <reference path=\\"../../../proverb.web/scripts/typings/angularjs/angular.d.ts\\" />\\n/// <reference path=\\"../../../proverb.web/scripts/typings/angularjs/angular-mocks.d.ts\\" />\\n/// <reference path=\\"../../../proverb.web/scripts/typings/angularjs/angular-route.d.ts\\" />\\n/// <reference path=\\"../../../proverb.web/scripts/typings/toastr/toastr.d.ts\\" />\\n/// <reference path=\\"../../../proverb.web/scripts/typings/underscore/underscore.d.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/sages/sagedetail.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/common/logger.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/common/common.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/services/datacontext.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/services/repositories.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/services/repository.sage.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/services/repository.saying.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/app.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/config.route.ts\\" />\\n```\\n\\nWith this in place you have a working solution, albeit one that is a little flaky. [An alternative solution was suggested by Noel Abrahams](https://github.com/Microsoft/TypeScript/issues/673#issuecomment-56024348) which I quote here:\\n\\n> Why not do the following?\\n>\\n> - Compile Proverb.Web with --declarations and the option for combining output into a single file. This should create a Proverb.Web.d.ts in your output directory.\\n> - In Proverb.Web.Tests.JavaScript add a reference to this file.\\n> - Right-click Proverb.Web.Tests.JavaScript select \\"Build Dependencies\\" > \\"Project Dependencies\\" and add a reference to Proverb.Web.\\n>\\n> I don\'t think directly referencing TypeScript source files is a good idea, because it causes the file to be rebuilt every time the dependant project is compiled.\\n\\nMohamed rather liked this solution. It looks like some more work is due to be done on the TypeScript tooling to make this less headache-y in future."},{"id":"unit-testing-angular-controller-with","metadata":{"permalink":"/unit-testing-angular-controller-with","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2014-09-10-unit-testing-angular-controller-with/index.md","source":"@site/blog/2014-09-10-unit-testing-angular-controller-with/index.md","title":"Unit Testing an Angular Controller with Jasmine","description":"John shares how they wrote unit tests for an Angular controller in Proverb using Jasmine 2.0, with heavily annotated JavaScript tests.","date":"2014-09-10T00:00:00.000Z","tags":[{"inline":false,"label":"Angular JS","permalink":"/tags/angularjs","description":"The original Angular JS framework."},{"inline":false,"label":"Automated Testing","permalink":"/tags/automated-testing","description":"How to perform the automation of tests."}],"readingTime":7.71,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"unit-testing-angular-controller-with","title":"Unit Testing an Angular Controller with Jasmine","authors":"johnnyreilly","tags":["angularjs","automated testing"],"hide_table_of_contents":false,"description":"John shares how they wrote unit tests for an Angular controller in Proverb using Jasmine 2.0, with heavily annotated JavaScript tests."},"unlisted":false,"prevItem":{"title":"Journalling the Migration of Jasmine Tests to TypeScript","permalink":"/migrating-jasmine-tests-to-typescript"},"nextItem":{"title":"Running JavaScript Unit Tests in AppVeyor","permalink":"/running-javascript-unit-tests-in-appveyor"}},"content":"Anyone who reads my blog will know that I have been long in the habit of writing unit tests for my C# code. I\'m cool like that. However, it took me a while to get up and running writing unit tests for my JavaScript code. I finally [got there](../2014-03-17-the-surprisingly-happy-tale-of-visual/index.md) using a combination of Jasmine 2.0 and Chutzpah. (Jasmine being my test framework and Chutzpah being my test runner.)\\n\\n\x3c!--truncate--\x3e\\n\\nI\'m getting properly into the habit of testing my JavaScript. I won\'t pretend it\'s been particularly fun but I firmly believe it will end up being useful... That\'s what I tell myself during the long dark tea-times of the soul anyway.\\n\\nI have a side project called [Proverb](https://github.com/johnnyreilly/Proverb). It doesn\'t do anything in particular - for the most part it\'s a simple application that displays the collected wise sayings of a team that I used to be part of. There\'s not much to it - a bit of CRUD, a dashboard. Not much more. Because of the project\'s simplicity it\'s ideal to use Proverb\'s underlying idea when trying out new technologies / frameworks. [The best way to learn is to do](http://en.wikipedia.org/wiki/Paul_Halmos). So if I want to learn \\"X\\", then building Proverb using \\"X\\" is a good way to go.\\n\\nI digress already. I had a version of Proverb built using a combination of [AngularJS and TypeScript](https://github.com/johnnyreilly/Proverb/tree/master/AngularTypeScript). I had written the Angular side of Proverb without any tests. Now I was able to write JavaScript tests for my Angular code that\'s just what I set out to do. It should prove something of a of [Code Kata](<http://en.wikipedia.org/wiki/Kata_(programming)>) too.\\n\\nWhilst I\'m at it I thought it might prove helpful if I wrote up how I approached writing unit tests for a single Angular controller. So here goes.\\n\\n## What I\'m Testing\\n\\nI have an Angular controller called `sagesDetail`. It powers this screen:\\n\\n![](sageDetailScreen.webp)\\n\\n`sagesDetail` is a very simple controller. It does these things:\\n\\n1. Load the \\"sage\\" (think of it as just a \\"user\\") and make it available on the controller so it can be bound to the view.\\n2. Set the view title.\\n3. Log view activation.\\n4. Expose a `gotoEdit` method which, when called, redirects the user to the edit screen.\\n\\nThe controller is written in TypeScript and looks like this:\\n\\n### sagesDetail.ts\\n\\n```ts\\nmodule controllers {\\n  \'use strict\';\\n\\n  var controllerId = \'sageDetail\';\\n\\n  interface sageDetailRouteParams extends ng.route.IRouteParamsService {\\n    id: string;\\n  }\\n\\n  class SageDetail {\\n    log: loggerFunction;\\n    sage: sage;\\n    title: string;\\n\\n    static $inject = [\'$location\', \'$routeParams\', \'common\', \'datacontext\'];\\n    constructor(\\n      private $location: ng.ILocationService,\\n      private $routeParams: sageDetailRouteParams,\\n      private common: common,\\n      private datacontext: datacontext,\\n    ) {\\n      this.sage = undefined;\\n      this.title = \'Sage Details\';\\n\\n      this.log = common.logger.getLogFn(controllerId);\\n\\n      this.activate();\\n    }\\n\\n    // Prototype methods\\n\\n    activate() {\\n      var id = parseInt(this.$routeParams.id, 10);\\n      var dataPromises: ng.IPromise<any>[] = [\\n        this.datacontext.sage\\n          .getById(id, true)\\n          .then((data) => (this.sage = data)),\\n      ];\\n\\n      this.common\\n        .activateController(dataPromises, controllerId, this.title)\\n        .then(() => {\\n          this.log(\'Activated Sage Details View\');\\n          this.title = \'Sage Details: \' + this.sage.name;\\n        });\\n    }\\n\\n    gotoEdit() {\\n      this.$location.path(\'/sages/edit/\' + this.sage.id);\\n    }\\n  }\\n\\n  angular.module(\'app\').controller(controllerId, SageDetail);\\n}\\n```\\n\\nWhen compiled to JavaScript it looks like this:\\n\\n### sageDetail.js\\n\\n```js\\nvar controllers;\\n(function (controllers) {\\n  \'use strict\';\\n\\n  var controllerId = \'sageDetail\';\\n\\n  var SageDetail = (function () {\\n    function SageDetail($location, $routeParams, common, datacontext) {\\n      this.$location = $location;\\n      this.$routeParams = $routeParams;\\n      this.common = common;\\n      this.datacontext = datacontext;\\n      this.sage = undefined;\\n      this.title = \'Sage Details\';\\n\\n      this.log = common.logger.getLogFn(controllerId);\\n\\n      this.activate();\\n    }\\n    // Prototype methods\\n    SageDetail.prototype.activate = function () {\\n      var _this = this;\\n      var id = parseInt(this.$routeParams.id, 10);\\n      var dataPromises = [\\n        this.datacontext.sage.getById(id, true).then(function (data) {\\n          return (_this.sage = data);\\n        }),\\n      ];\\n\\n      this.common\\n        .activateController(dataPromises, controllerId, this.title)\\n        .then(function () {\\n          _this.log(\'Activated Sage Details View\');\\n          _this.title = \'Sage Details: \' + _this.sage.name;\\n        });\\n    };\\n\\n    SageDetail.prototype.gotoEdit = function () {\\n      this.$location.path(\'/sages/edit/\' + this.sage.id);\\n    };\\n    SageDetail.$inject = [\'$location\', \'$routeParams\', \'common\', \'datacontext\'];\\n    return SageDetail;\\n  })();\\n\\n  angular.module(\'app\').controller(controllerId, SageDetail);\\n})(controllers || (controllers = {}));\\n//# sourceMappingURL=sageDetail.js.map\\n```\\n\\n## Now for the Tests\\n\\nI haven\'t yet made the move of switching over my Jasmine tests from JavaScript to TypeScript. (It\'s on my list but there\'s only so many things you can do at once...) For that reason the tests you\'ll see here are straight JavaScript. Below you will see the tests for the `sageDetail` controller.\\n\\nI have put very comments in the test code to make clear the intent to you, dear reader. Annotated the life out of them. Naturally I wouldn\'t expect a test to be so heavily annotated in a typical test suite - and you can be sure mine normally aren\'t!\\n\\n### Jasmine tests for sageDetail.js\\n\\n```js\\ndescribe(\'Proverb.Web -> app-> controllers ->\', function () {\\n  // Before each test runs we\'re going to need ourselves an Angular App to test - go fetch!\\n  beforeEach(function () {\\n    module(\'app\'); // module is an alias for <a href=\\"https://docs.angularjs.org/api/ngMock/function/angular.mock.module\\">angular.mock.module</a>\\n  });\\n\\n  // Tests for the sageDetail controller\\n  describe(\'sageDetail ->\', function () {\\n    // Declare describe-scoped variables\\n    var $rootScope,\\n      getById_deferred, // deferred used for promises\\n      $location,\\n      $routeParams_stub,\\n      common,\\n      datacontext, // controller dependencies\\n      sageDetailController; // the controller\\n\\n    // Before each test runs set up the controller using inject - an alias for <a href=\\"https://docs.angularjs.org/api/ngMock/function/angular.mock.inject\\">angular.mock.inject</a>\\n    beforeEach(inject(function (\\n      _$controller_,\\n      _$rootScope_,\\n      _$q_,\\n      _$location_,\\n      _common_,\\n      _datacontext_,\\n    ) {\\n      // Note how each parameter is prefixed and suffixed with \\"_\\" - this an Angular nicety\\n      // which allows you to have variables in your tests with the original reference name.\\n      // So here we assign the injected parameters to the describe-scoped variables:\\n      $rootScope = _$rootScope_;\\n      $q = _$q_;\\n      $location = _$location_;\\n      common = _common_;\\n      datacontext = _datacontext_;\\n\\n      // Our controller has a dependency on an \\"id\\" property passed on the $routeParams\\n      // We\'re going to stub this out with a JavaScript object literal\\n      $routeParams_stub = { id: \'10\' };\\n\\n      // Our controller depends on a promise returned from this function: datacontext.sage.getById\\n      // Well strictly speaking it also uses a promise for activateController but since the activateController\\n      // promise just wraps the getById promise it will be resolved when the getById promise is.\\n      // Here we create a deferred representing the getById promise which we can resolve as we need to\\n      getById_deferred = $q.defer();\\n\\n      // set up a spy on datacontext.sage.getById and set it to return the promise of getById_deferred\\n      // this allows us to #1 detect that getById has been called\\n      // and #2 resolve / reject our promise as our test requires using getById_deferred\\n      spyOn(datacontext.sage, \'getById\').and.returnValue(\\n        getById_deferred.promise,\\n      );\\n\\n      // set up a spy on common.activateController and set it to call through\\n      // this allows us to detect that activateController has been called whilst\\n      // maintaining existing controller functionality\\n      spyOn(common, \'activateController\').and.callThrough();\\n\\n      // set up spys on common.logger.getLogFn and $location.path so we can detect they have been called\\n      spyOn(common.logger, \'getLogFn\').and.returnValue(\\n        jasmine.createSpy(\'log\'),\\n      );\\n      spyOn($location, \'path\').and.returnValue(jasmine.createSpy(\'path\'));\\n\\n      // create a sageDetail controller and inject the dependencies we have set up\\n      sageDetailController = _$controller_(\'sageDetail\', {\\n        $location: $location,\\n        $routeParams: $routeParams_stub,\\n        common: common,\\n        datacontext: datacontext,\\n      });\\n    }));\\n\\n    // Tests for the controller state at the point of the sageDetail controller\'s creation\\n    // ie before the getById / activateController promises have been resolved\\n    // So this tests the constructor (function) and the activate function up to the point\\n    // of the promise calls\\n    describe(\'on creation ->\', function () {\\n      it(\\"controller should have a title of \'Sage Details\'\\", function () {\\n        // tests this code has executed:\\n        // this.title = \\"Sage Details\\";\\n        expect(sageDetailController.title).toBe(\'Sage Details\');\\n      });\\n\\n      it(\'controller should have no sage\', function () {\\n        // tests this code has executed:\\n        // this.sage = undefined;\\n        expect(sageDetailController.sage).toBeUndefined();\\n      });\\n\\n      it(\'datacontext.sage.getById should be called\', function () {\\n        // tests this code has executed:\\n        // this.datacontext.sage.getById(id, true)\\n        expect(datacontext.sage.getById).toHaveBeenCalledWith(10, true);\\n      });\\n    });\\n\\n    // Tests for the controller state at the point of the resolution of the getById promise\\n    // ie after the getById / activateController promises have been resolved\\n    // So this tests the constructor (function) and the activate function after the point\\n    // of the promise calls\\n    describe(\'activateController ->\', function () {\\n      var sage_stub;\\n      beforeEach(function () {\\n        // Create a sage stub which will be used when resolving the getById promise\\n        sage_stub = { name: \'John\' };\\n      });\\n\\n      it(\'should set sages to be the resolved promise values\', function () {\\n        // Resolve the getById promise with the sage stub\\n        getById_deferred.resolve(sage_stub);\\n        $rootScope.$digest(); // So Angular processes the resolved promise\\n\\n        // tests this code has executed:\\n        // this.sage = data\\n        expect(sageDetailController.sage).toBe(sage_stub);\\n      });\\n\\n      it(\\"should log \'Activated Sage Details View\' and set title with name\\", function () {\\n        // Resolve the getById promise with the sage stub\\n        getById_deferred.resolve(sage_stub);\\n        $rootScope.$digest(); // So Angular processes the resolved promise\\n\\n        // tests this code has executed:\\n        // this.log(\\"Activated Sage Details View\\");\\n        // this.title = \\"Sage Details: \\" + this.sage.name;\\n        expect(sageDetailController.log).toHaveBeenCalledWith(\\n          \'Activated Sage Details View\',\\n        );\\n        expect(sageDetailController.title).toBe(\\n          \'Sage Details: \' + sage_stub.name,\\n        );\\n      });\\n    });\\n\\n    // Tests for the gotoEdit function on the controller\\n    // Note that this will only be called *after* a controller has been created\\n    // and it depends upon a sage having first been loaded\\n    describe(\'gotoEdit ->\', function () {\\n      var sage_stub;\\n      beforeEach(function () {\\n        // Create a sage stub which will be used when resolving the getById promise\\n        sage_stub = { id: 20 };\\n      });\\n\\n      it(\'should set $location.path to edit URL\', function () {\\n        // Resolve the getById promise with the sage stub\\n        getById_deferred.resolve(sage_stub);\\n        $rootScope.$digest(); // So Angular processes the resolved promise\\n\\n        sageDetailController.gotoEdit();\\n\\n        // tests this code has executed:\\n        // this.$location.path(\\"/sages/edit/\\" + this.sage.id);\\n        expect($location.path).toHaveBeenCalledWith(\\n          \'/sages/edit/\' + sage_stub.id,\\n        );\\n      });\\n    });\\n  });\\n});\\n```"},{"id":"running-javascript-unit-tests-in-appveyor","metadata":{"permalink":"/running-javascript-unit-tests-in-appveyor","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2014-09-06-running-javascript-unit-tests-in-appveyor/index.md","source":"@site/blog/2014-09-06-running-javascript-unit-tests-in-appveyor/index.md","title":"Running JavaScript Unit Tests in AppVeyor","description":"AppVeyor and Chutzpah were integrated to run C# and JavaScript unit tests in a single PowerShell script for CI purposes.","date":"2014-09-06T00:00:00.000Z","tags":[{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."},{"inline":false,"label":"Automated Testing","permalink":"/tags/automated-testing","description":"How to perform the automation of tests."}],"readingTime":2.96,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"running-javascript-unit-tests-in-appveyor","title":"Running JavaScript Unit Tests in AppVeyor","authors":"johnnyreilly","tags":["javascript","automated testing"],"hide_table_of_contents":false,"description":"AppVeyor and Chutzpah were integrated to run C# and JavaScript unit tests in a single PowerShell script for CI purposes."},"unlisted":false,"prevItem":{"title":"Unit Testing an Angular Controller with Jasmine","permalink":"/unit-testing-angular-controller-with"},"nextItem":{"title":"My Unrequited Love for Isolate Scope","permalink":"/my-unrequited-love-for-isolate-scope"}},"content":"## With a little help from Chutzpah...\\n\\n\x3c!--truncate--\x3e\\n\\n[AppVeyor](http://www.appveyor.com) (if you\'re not aware of it) is a Continuous Integration provider. If you like, it\'s plug-and-play CI for .NET developers. It\'s lovely. And what\'s more it\'s [\\"free for open-source projects with public repositories hosted on GitHub and BitBucket\\"](http://www.appveyor.com/pricing). Boom! I recently hooked up 2 of my GitHub projects with AppVeyor. It took me all of... 10 minutes. If that? It really is \\\\***that**\\\\* good.\\n\\nBut.... There had to be a \\"but\\" otherwise I wouldn\'t have been writing the post you\'re reading. For a little side project of mine called [Proverb](https://github.com/johnnyreilly/Proverb) there were C# unit tests and there were JavaScript unit tests. And the JavaScript unit tests weren\'t being run... No fair!!!\\n\\n[Chutzpah](https://chutzpah.codeplex.com/) is a JavaScript test runner which at this point runs QUnit, Jasmine and Mocha JavaScript tests. I use the [Visual Studio extension](http://visualstudiogallery.msdn.microsoft.com/f8741f04-bae4-4900-81c7-7c9bfb9ed1fe) to run Jasmine tests on my machine during development. I\'ve also been able to use [Chutzpah for CI purposes with Visual Studio Online / Team Foundation Server](../2014-03-17-the-surprisingly-happy-tale-of-visual/index.md). So what say we try and do the triple and make it work with AppVeyor too?\\n\\n## NuGet me?\\n\\nIn order that I could run Chutzpah I needed Chutzpah to be installed on the build machine. So I had 2 choices:\\n\\n1. Add Chutzpah direct to the repo\\n2. Add the [Chutzpah Nuget package](http://www.nuget.org/packages/chutzpah) to the solution\\n\\nUnsurprisingly I chose #2 - much cleaner.\\n\\n## Now to use Chutzpah\\n\\nTime to dust down the PowerShell. I created myself a \\"before tests script\\" and added it to my build. It looked a little something like this:\\n\\n```ps\\n# Locate Chutzpah\\n\\n$ChutzpahDir = get-childitem chutzpah.console.exe -recurse | select-object -first 1 | select -expand Directory\\n\\n# Run tests using Chutzpah and export results as JUnit format to chutzpah-results.xml\\n\\n$ChutzpahCmd = \\"$($ChutzpahDir)\\\\chutzpah.console.exe $($env:APPVEYOR_BUILD_FOLDER)\\\\AngularTypeScript\\\\Proverb.Web.Tests.JavaScript /junit .\\\\chutzpah-results.xml\\"\\nWrite-Host $ChutzpahCmd\\nInvoke-Expression $ChutzpahCmd\\n\\n# Upload results to AppVeyor one by one\\n\\n$testsuites = [xml](get-content .\\\\chutzpah-results.xml)\\n\\n$anyFailures = $FALSE\\nforeach ($testsuite in $testsuites.testsuites.testsuite) {\\n    write-host \\" $($testsuite.name)\\"\\n    foreach ($testcase in $testsuite.testcase){\\n        $failed = $testcase.failure\\n        $time = $testsuite.time\\n        if ($testcase.time) { $time = $testcase.time }\\n        if ($failed) {\\n            write-host \\"Failed   $($testcase.name) $($testcase.failure.message)\\"\\n            Add-AppveyorTest $testcase.name -Outcome Failed -FileName $testsuite.name -ErrorMessage $testcase.failure.message -Duration $time\\n            $anyFailures = $TRUE\\n        }\\n        else {\\n            write-host \\"Passed   $($testcase.name)\\"\\n            Add-AppveyorTest $testcase.name -Outcome Passed -FileName $testsuite.name -Duration $time\\n        }\\n\\n    }\\n}\\n\\nif ($anyFailures -eq $TRUE){\\n    write-host \\"Failing build as there are broken tests\\"\\n    $host.SetShouldExit(1)\\n}\\n```\\n\\nWhat this does is:\\n\\n1. Run Chutzpah from the installed NuGet package location, passing in the location of my Jasmine unit tests. In the case of my project there is a `chutzpah.json` file in the project which dictates how Chutzpah should run the tests. Also, [the JUnit flag is also passed](https://chutzpah.codeplex.com/wikipage?title=Command%20Line%20Options&referringTitle=Documentation) in order that Chutzpah creates a `chutzpah-results.xml` file of test results in the JUnit format.\\n2. We iterate through test results and tell AppVeyor about the the test passes and failures using the [Build Worker API](http://www.appveyor.com/docs/build-worker-api).\\n3. If there have been any failed tests then we fail the build. If you look [here](https://ci.appveyor.com/project/JohnReilly/proverb/build/1.0.17) you can see a deliberately failed build which demo\'s that this works as it should.\\n\\nThat\'s a wrap - We now have CI which includes our JavaScript tests! That\'s right we get to see beautiful screens like these:\\n\\n![](Screenshot-2014-09-06-21.43.15.webp)\\n\\n![](Screenshot-2014-09-06-21.49.38.webp)\\n\\n## Thanks to...\\n\\nThanks to Dan Jones, whose comments on [this discussion](http://help.appveyor.com/discussions/questions/390-running-jasmine-on-appveyor#comment_34433599) provided a number of useful pointers which moved me in the right direction. And thanks to Feador Fitzner who has generously [said AppVeyor will support JUnit in the future](http://help.appveyor.com/discussions/questions/495-integrating-chutzpah-into-appveyor#comment_34447202) which may simplify use of Chutzpah with AppVeyor even further."},{"id":"my-unrequited-love-for-isolate-scope","metadata":{"permalink":"/my-unrequited-love-for-isolate-scope","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2014-08-12-my-unrequited-love-for-isolate-scope/index.md","source":"@site/blog/2014-08-12-my-unrequited-love-for-isolate-scope/index.md","title":"My Unrequited Love for Isolate Scope","description":"A new version of the serverError directive is presented without isolated scope after discovering directives can only create one isolated scope.","date":"2014-08-12T00:00:00.000Z","tags":[{"inline":false,"label":"Angular JS","permalink":"/tags/angularjs","description":"The original Angular JS framework."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."},{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."}],"readingTime":4.515,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"my-unrequited-love-for-isolate-scope","title":"My Unrequited Love for Isolate Scope","authors":"johnnyreilly","tags":["angularjs","typescript","javascript"],"hide_table_of_contents":false,"description":"A new version of the serverError directive is presented without isolated scope after discovering directives can only create one isolated scope."},"unlisted":false,"prevItem":{"title":"Running JavaScript Unit Tests in AppVeyor","permalink":"/running-javascript-unit-tests-in-appveyor"},"nextItem":{"title":"Getting more RESTful with Web API and IHttpActionResult","permalink":"/getting-more-RESTful-with-Web-API"}},"content":"[I wrote a little while ago about creating a directive to present server errors on the screen in an Angular application](../2014-08-01-angularjs-meet-aspnet-server-validation/index.md). In my own (not so humble opinion), it was really quite nice. I was particularly proud of my usage of isolate scope. However, pride comes before a fall.\\n\\n\x3c!--truncate--\x3e\\n\\nIt turns out that using isolate scope in a directive is not always wise. Or rather \u2013 not always possible. And this is why:\\n\\n```\\nError: [$compile:multidir] Multiple directives [datepickerPopup, serverError] asking for new/isolated scope on: <input name=\\"sage.dateOfBirth\\" class=\\"col-xs-12 col-sm-9\\" type=\\"text\\" value=\\"\\" ng-click=\\"vm.dateOfBirthDatePickerOpen()\\" server-error=\\"vm.errors\\" ng-model=\\"vm.sage.dateOfBirth\\" is-open=\\"vm.dateOfBirthDatePickerIsOpen\\" datepicker-popup=\\"dd MMM yyyy\\">\\n```\\n\\nUg. What happened here? Well, I had a date field that I was using my serverError directive on. Nothing too controversial there. The problem came when I tried to plug in [UI Bootstrap\u2019s datepicker](http://angular-ui.github.io/bootstrap/) as well. That\u2019s right the directives are fighting. Sad face.\\n\\nTo be more precise, it turns out that only one directive on an element is allowed to create an isolated scope. So if I want to use UI Bootstrap\u2019s datepicker (and I do) \u2013 well my serverError directive is toast.\\n\\n## A New Hope\\n\\nSo ladies and gentlemen, let me present serverError 2.0 \u2013 this time without isolated scope:\\n\\n### serverError.ts\\n\\n```ts\\n(function () {\\n  \'use strict\';\\n\\n  var app = angular.module(\'app\');\\n\\n  // Plant a validation message to the right of the element when it is declared invalid by the server\\n  app.directive(\'serverError\', [\\n    function () {\\n      // Usage:\\n      // <input class=\\"col-xs-12 col-sm-9\\"\\n      //        name=\\"sage.name\\" ng-model=\\"vm.sage.name\\" server-error=\\"vm.errors\\" />\\n\\n      var directive = {\\n        link: link,\\n        restrict: \'A\',\\n        require: \'ngModel\', // supply the ngModel controller as the 4th parameter in the link function\\n      };\\n      return directive;\\n\\n      function link(\\n        scope: ng.IScope,\\n        element: ng.IAugmentedJQuery,\\n        attrs: ng.IAttributes,\\n        ngModelController: ng.INgModelController,\\n      ) {\\n        // Extract values from attributes (deliberately not using isolated scope)\\n        var errorKey: string = attrs[\'name\']; // eg \\"sage.name\\"\\n        var errorDictionaryExpression: string = attrs[\'serverError\']; // eg \\"vm.errors\\"\\n\\n        // Bootstrap alert template for error\\n        var template =\\n          \'<div class=\\"alert alert-danger col-xs-9 col-xs-offset-2\\" role=\\"alert\\"><i class=\\"glyphicon glyphicon-warning-sign larger\\"></i> %error%</div>\';\\n\\n        // Create an element to hold the validation message\\n        var decorator = angular.element(\'<div></div>\');\\n        element.after(decorator);\\n\\n        // Watch ngModelController.$error.server & show/hide validation accordingly\\n        scope.$watch(\\n          safeWatch(() => ngModelController.$error.server),\\n          showHideValidation,\\n        );\\n\\n        function showHideValidation(serverError: boolean) {\\n          // Display an error if serverError is true otherwise clear the element\\n          var errorHtml = \'\';\\n          if (serverError) {\\n            var errorDictionary: { [field: string]: string } = scope.$eval(\\n              errorDictionaryExpression,\\n            );\\n            errorHtml = template.replace(\\n              /%error%/,\\n              errorDictionary[errorKey] || \'Unknown error occurred...\',\\n            );\\n          }\\n          decorator.html(errorHtml);\\n        }\\n\\n        // wipe the server error message upon keyup or change events so can revalidate with server\\n        element.on(\'keyup change\', (event) => {\\n          scope.$apply(() => {\\n            ngModelController.$setValidity(\'server\', true);\\n          });\\n        });\\n      }\\n    },\\n  ]);\\n\\n  // Thanks @Basarat! http://stackoverflow.com/a/24863256/761388\\n  function safeWatch<T extends Function>(expression: T) {\\n    return () => {\\n      try {\\n        return expression();\\n      } catch (e) {\\n        return null;\\n      }\\n    };\\n  }\\n})();\\n```\\n\\n### serverError.js\\n\\n```js\\n(function () {\\n  \'use strict\';\\n\\n  var app = angular.module(\'app\');\\n\\n  // Plant a validation message to the right of the element when it is declared invalid by the server\\n  app.directive(\'serverError\', [\\n    function () {\\n      // Usage:\\n      // <input class=\\"col-xs-12 col-sm-9\\"\\n      //        name=\\"sage.name\\" ng-model=\\"vm.sage.name\\" server-error=\\"vm.errors\\" />\\n      var directive = {\\n        link: link,\\n        restrict: \'A\',\\n        require: \'ngModel\',\\n      };\\n      return directive;\\n\\n      function link(scope, element, attrs, ngModelController) {\\n        // Extract values from attributes (deliberately not using isolated scope)\\n        var errorKey = attrs[\'name\'];\\n        var errorDictionaryExpression = attrs[\'serverError\'];\\n\\n        // Bootstrap alert template for error\\n        var template =\\n          \'<div class=\\"alert alert-danger col-xs-9 col-xs-offset-2\\" role=\\"alert\\"><i class=\\"glyphicon glyphicon-warning-sign larger\\"></i> %error%</div>\';\\n\\n        // Create an element to hold the validation message\\n        var decorator = angular.element(\'<div></div>\');\\n        element.after(decorator);\\n\\n        // Watch ngModelController.$error.server & show/hide validation accordingly\\n        scope.$watch(\\n          safeWatch(function () {\\n            return ngModelController.$error.server;\\n          }),\\n          showHideValidation,\\n        );\\n\\n        function showHideValidation(serverError) {\\n          // Display an error if serverError is true otherwise clear the element\\n          var errorHtml = \'\';\\n          if (serverError) {\\n            var errorDictionary = scope.$eval(errorDictionaryExpression);\\n            errorHtml = template.replace(\\n              /%error%/,\\n              errorDictionary[errorKey] || \'Unknown error occurred...\',\\n            );\\n          }\\n          decorator.html(errorHtml);\\n        }\\n\\n        // wipe the server error message upon keyup or change events so can revalidate with server\\n        element.on(\'keyup change\', function (event) {\\n          scope.$apply(function () {\\n            ngModelController.$setValidity(\'server\', true);\\n          });\\n        });\\n      }\\n    },\\n  ]);\\n\\n  // Thanks @Basarat! http://stackoverflow.com/a/24863256/761388\\n  function safeWatch(expression) {\\n    return function () {\\n      try {\\n        return expression();\\n      } catch (e) {\\n        return null;\\n      }\\n    };\\n  }\\n})();\\n```\\n\\nThis version of the serverError directive is from a users perspective identical to the previous version. But it doesn\u2019t use isolated scope \u2013 this means it can be used in concert with other directives which do.\\n\\nIt works by pulling the `name` and `serverError` values off the attrs parameter. `name` is just a string - the value of which never changes so it can be used as is. `serverError` is an expression that represents the error dictionary that is used to store the server error messages. This is accessed through use of `scope.$eval` as an when it needs to.\\n\\n## My Plea\\n\\nWhat I\u2019ve outlined here works. I\u2019ll admit that usage of `$eval` makes me feel a little bit dirty (I\u2019ve got [\u201Ceval is evil\u201D](http://www.jslint.com/lint.html#evil) running through my head). Whilst it works, I\u2019m not sure what I\u2019ve done is necessarily best practice. After all [the Angular docs themselves say](https://docs.angularjs.org/guide/directive):\\n\\n> **\\\\*Best Practice:** Use the scope option to create isolate scopes when making components that you want to reuse throughout your app. \\\\*\\n\\nBut as we\u2019ve seen this isn\u2019t always an option. I\u2019ve written this post to document my own particular struggle and ask the question \u201Cis there a better way?\u201D If you know then please tell me!"},{"id":"getting-more-RESTful-with-Web-API","metadata":{"permalink":"/getting-more-RESTful-with-Web-API","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2014-08-08-getting-more-RESTful-with-Web-API/index.md","source":"@site/blog/2014-08-08-getting-more-RESTful-with-Web-API/index.md","title":"Getting more RESTful with Web API and IHttpActionResult","description":"Learn how to use HTTP status codes in Web API methods to return successful or failed requests without wrapping the outcomes.","date":"2014-08-08T00:00:00.000Z","tags":[{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":2.685,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"getting-more-RESTful-with-Web-API","title":"Getting more RESTful with Web API and IHttpActionResult","authors":"johnnyreilly","tags":["asp.net"],"hide_table_of_contents":false,"description":"Learn how to use HTTP status codes in Web API methods to return successful or failed requests without wrapping the outcomes."},"unlisted":false,"prevItem":{"title":"My Unrequited Love for Isolate Scope","permalink":"/my-unrequited-love-for-isolate-scope"},"nextItem":{"title":"AngularJS meet ASP.Net Server Validation","permalink":"/angularjs-meet-aspnet-server-validation"}},"content":"Up until, well yesterday really, I tended to have my Web API action methods all returning [200](http://en.wikipedia.org/wiki/HTTP_200#2xx_Success)\'s no matter what. Successful request? 200 for you sir! Some validation error in the model? 200 for you too ma\'am - but I\'ll wrap up the validation errors and send them back too. Database error? 200 and and an error message.\\n\\n\x3c!--truncate--\x3e\\n\\nIt kind of looked like this (this example taken from a [previous post](../2014-08-01-angularjs-meet-aspnet-server-validation/index.md)):\\n\\n```cs\\npublic class SageController : ApiController\\n{\\n  // ...\\n\\n  public IHttpActionResult Post(User sage)\\n  {\\n    if (!ModelState.IsValid) {\\n\\n      return Ok(new {\\n        Success = false,\\n        Errors = ModelState.ToErrorDictionary()\\n      });\\n    }\\n\\n    sage = _userService.Save(sage);\\n\\n    return Ok(new {\\n      Success = true,\\n      Entity = sage\\n    });\\n  }\\n\\n  // ...\\n}\\n```\\n\\nWell I\'m no RESTafarian but this felt a little... wrong. Like I wasn\'t fully embracing the web. I didn\'t want to have to include my own `Success` flag to indicate whether the request was good or not. I decided that I\'d rather have it at least a little more webby. To that end, I decided I\'d like to have 2xx success status codes for genuine success only and 4xx client error status codes for failures.\\n\\nLose the wrapper - embrace the web. This post is about doing just that.\\n\\n## Web API 2 - Bad Job on on the BadRequest Helper\\n\\nWeb API 2 ships with a whole host of API helper methods. Things like `Ok` (which you can see me using above) and `BadRequest`. `BadRequest` was what I had in mind to use in place of `Ok` where I had some kind of error I wanted to report to the client like so:\\n\\n```cs\\npublic class SageController : ApiController\\n{\\n  // ...\\n\\n  public IHttpActionResult Post(User sage)\\n  {\\n    if (!ModelState.IsValid) {\\n\\n      return BadRequest(new  {\\n        Errors = ModelState.ToErrorDictionary()\\n      });\\n    }\\n\\n    sage = _userService.Save(sage);\\n\\n    return Ok(new {\\n      Entity = sage\\n    });\\n  }\\n\\n  // ...\\n}\\n```\\n\\nLooks good right? No more need for my `Success` flag. Terser. Less code is better code. Unfortunately the built in `BadRequest` helper method doesn\'t have the flexibility of the `Ok` helper method - it doesn\'t allow you to send anything back you want. Fortunately this is easily remedied with a short extension method for `ApiController`:\\n\\n```cs\\nusing System.Net;\\nusing System.Web.Http;\\nusing System.Web.Http.Results;\\n\\nnamespace System.Web.Http\\n{\\n    public static class ControllerExtensions\\n    {\\n        public static IHttpActionResult BadRequest<T>(this ApiController controller, T obj)\\n        {\\n            return new NegotiatedContentResult<T>(HttpStatusCode.BadRequest, obj, controller);\\n        }\\n    }\\n}\\n```\\n\\nWith this in place I can then tweak my implementation to hook into the extension method:\\n\\n```cs\\npublic class SageController : ApiController\\n{\\n  // ...\\n\\n  public IHttpActionResult Post(User sage)\\n  {\\n    if (!ModelState.IsValid) {\\n      // See how we have \\"this.\\" before BadRequest so the Extension method is invoked\\n      return this.BadRequest(new  {\\n        Errors = ModelState.ToErrorDictionary()\\n      });\\n    }\\n\\n    sage = _userService.Save(sage);\\n\\n    return Ok(new {\\n      Entity = sage\\n    });\\n  }\\n\\n  // ...\\n}\\n```\\n\\nAnd now we have have an endpoint that serves up 2xx status codes or 4xx status codes just as I\'d hoped. Obviously this change in the way my action methods are returning will have implications for the consuming client (in my case an app built using AngularJS and $q). Essentially I can now use my `then` to handle the successes and my `catch` to handle the errors."},{"id":"angularjs-meet-aspnet-server-validation","metadata":{"permalink":"/angularjs-meet-aspnet-server-validation","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2014-08-01-angularjs-meet-aspnet-server-validation/index.md","source":"@site/blog/2014-08-01-angularjs-meet-aspnet-server-validation/index.md","title":"AngularJS meet ASP.Net Server Validation","description":"Learn how to perform server-side validation in your AngularJS and ASP.Net project using a `serverError` directive and server response error messages.","date":"2014-08-01T00:00:00.000Z","tags":[{"inline":false,"label":"Angular JS","permalink":"/tags/angularjs","description":"The original Angular JS framework."},{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":10.405,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"angularjs-meet-aspnet-server-validation","title":"AngularJS meet ASP.Net Server Validation","authors":"johnnyreilly","tags":["angularjs","asp.net","typescript"],"hide_table_of_contents":false,"description":"Learn how to perform server-side validation in your AngularJS and ASP.Net project using a `serverError` directive and server response error messages."},"unlisted":false,"prevItem":{"title":"Getting more RESTful with Web API and IHttpActionResult","permalink":"/getting-more-RESTful-with-Web-API"},"nextItem":{"title":"HotTowel-Angular meet TypeScript","permalink":"/hottowel-angular-meet-typescript"}},"content":"So. You\'re using AngularJS to build your front end with ASP.Net running on the server side. You\'re a trustworthy dev - you know that validation on the client will only get you so far. You need to validate on the server.\\n\\n\x3c!--truncate--\x3e\\n\\nMy particular scenario is where you have a form which you are saving. Angular serves you well when it comes to hooking in your own client side validation. But it doesn\'t really ship with anything that supports **nicely** presenting server side validation on the client. Invariably when you look around you find people duplicating their server side validation on the client and presenting all their server side validation in a `&lt;div&gt;` at the top of the screen.\\n\\nThis works but it\'s not as helpful to the user as it might be. It groups together all the validation from the server into one place. What I want is field level validation from the server that\'s presented on a field level basis on the screen.\\n\\nLet us travel together to this promised land...\\n\\n## What do we need client side?\\n\\nWell, let\'s start with a directive which I\'ll call `serverError`. This plants a validation message just _after_ the element being validated which is displayed when that element is declared invalid by the server. (That is to say when the `ngModel` has a `$error.server` set.) When the element is changed then the `$error.server` is unset in order that validation can be hidden and the form can be revalidated against the server.\\n\\nI\'m using TypeScript with Angular so for my JavaScript examples I\'ll give you both the TypeScript which I originally wrote and the generated JavaScript as well.\\n\\n### TypeScript\\n\\n```ts\\ninterface serverErrorScope extends ng.IScope {\\n    name: string;\\n    serverError: { [field: string]: string };\\n}\\n\\napp.directive(\\"serverError\\", [function () {\\n\\n  // Usage:\\n  // <input class=\\"col-xs-12 col-sm-9\\"\\n  //        name=\\"sage.name\\" ng-model=\\"vm.sage.name\\" server-error=\\"vm.errors\\" />\\n  var directive = {\\n    link: link,\\n    restrict: \\"A\\",\\n    require: \\"ngModel\\", // supply the ngModel controller as the 4th parameter in the link function\\n    scope: { // Pass in name and serverError to the scope\\n      name: \\"@\\",\\n      serverError: \\"=\\"\\n    }\\n  };\\n  return directive;\\n\\n  function link(scope: serverErrorScope, element: ng.IAugmentedJQuery, attrs: ng.IAttributes, ngModelController: ng.INgModelController) {\\n\\n    // Bootstrap alert template for error\\n    var template = \'<div class=\\"alert alert-danger\\" role=\\"alert\\">\' +\\n                               \'<i class=\\"glyphicon glyphicon-warning-sign\\"></i> \' +\\n                               \'%error%</div>\';\\n\\n    // Create an element to hold the validation message\\n    var decorator = angular.element(\'<div></div>\');\\n    element.after(decorator);\\n\\n    // Watch ngModelController.$error.server & show/hide validation accordingly\\n    scope.$watch(safeWatch(() => ngModelController.$error.server), showHideValidation);\\n\\n    function showHideValidation(serverError: boolean) {\\n\\n      // Display an error if serverError is true otherwise clear the element\\n      var errorHtml = \\"\\";\\n      if (serverError) {\\n        // Aliasing serverError and name to make it more obvious what their purpose is\\n        var errorDictionary = scope.serverError;\\n        var errorKey = scope.name;\\n        errorHtml = template.replace(/%error%/, errorDictionary[errorKey] || \\"Unknown error occurred...\\");\\n      }\\n      decorator.html(errorHtml);\\n    }\\n\\n    // wipe the server error message upon keyup or change events so can revalidate with server\\n    element.on(\\"keyup change\\", (event) => {\\n      scope.$apply(() => { ngModelController.$setValidity(\\"server\\", true); });\\n    });\\n  }\\n}]);\\n\\n// Thanks @Basarat! http://stackoverflow.com/a/24863256/761388\\nfunction safeWatch<t extends=\\"\\" function=\\"\\">(expression: T) {\\n  return () => {\\n    try {\\n      return expression();\\n    }\\n    catch (e) {\\n      return null;\\n    }\\n  };\\n}\\n</t>\\n```\\n\\n### JavaScript\\n\\n```js\\napp.directive(\'serverError\', [\\n  function () {\\n    // Usage:\\n    // <input class=\\"col-xs-12 col-sm-9\\"\\n    //        name=\\"sage.name\\" ng-model=\\"vm.sage.name\\" server-error=\\"vm.errors\\" />\\n    var directive = {\\n      link: link,\\n      restrict: \'A\',\\n      require: \'ngModel\',\\n      scope: {\\n        name: \'@\',\\n        serverError: \'=\',\\n      },\\n    };\\n    return directive;\\n\\n    function link(scope, element, attrs, ngModelController) {\\n      // Bootstrap alert template for error\\n      var template =\\n        \'<div class=\\"alert alert-danger\\" role=\\"alert\\">\' +\\n        \'<i class=\\"glyphicon glyphicon-warning-sign\\"></i> \' +\\n        \'%error%</div>\';\\n\\n      // Create an element to hold the validation message\\n      var decorator = angular.element(\'<div></div>\');\\n      element.after(decorator);\\n\\n      // Watch ngModelController.$error.server & show/hide validation accordingly\\n      scope.$watch(\\n        safeWatch(function () {\\n          return ngModelController.$error.server;\\n        }),\\n        showHideValidation,\\n      );\\n\\n      function showHideValidation(serverError) {\\n        // Display an error if serverError is true otherwise clear the element\\n        var errorHtml = \'\';\\n        if (serverError) {\\n          // Aliasing serverError and name to make it more obvious what their purpose is\\n          var errorDictionary = scope.serverError;\\n          var errorKey = scope.name;\\n          errorHtml = template.replace(\\n            /%error%/,\\n            errorDictionary[errorKey] || \'Unknown error occurred...\',\\n          );\\n        }\\n        decorator.html(errorHtml);\\n      }\\n\\n      // wipe the server error message upon keyup or change events so can revalidate with server\\n      element.on(\'keyup change\', function (event) {\\n        scope.$apply(function () {\\n          ngModelController.$setValidity(\'server\', true);\\n        });\\n      });\\n    }\\n  },\\n]);\\n\\n// Thanks @Basarat! http://stackoverflow.com/a/24863256/761388\\nfunction safeWatch(expression) {\\n  return function () {\\n    try {\\n      return expression();\\n    } catch (e) {\\n      return null;\\n    }\\n  };\\n}\\n```\\n\\nIf you look closely at this directive you\'ll see it is restricted to be used as an attribute and it depends on 2 things:\\n\\n1. The value that the `server-error` attribute is set to should be an object which will contain key / values where the keys represent fields that are being validated.\\n2. The element being validated must have a name property (which will be used to look up the validation message in the `server-error` error \\"dictionary\\".\\n\\nTotally not clear, right? Let\'s have an example. Here is my \\"sageEdit\\" screen which you saw the screenshot of earlier:\\n\\n```html\\n<section class=\\"mainbar\\" ng-controller=\\"sageEdit as vm\\">\\n  <section class=\\"matter\\">\\n    <div class=\\"container-fluid\\">\\n      <form name=\\"form\\" novalidate role=\\"form\\">\\n        <div>\\n          <button\\n            class=\\"btn btn-info\\"\\n            ng-click=\\"vm.save()\\"\\n            ng-disabled=\\"!vm.canSave\\"\\n          >\\n            <i class=\\"glyphicon glyphicon-save\\"></i>Save\\n          </button>\\n          <span ng-show=\\"vm.hasChanges\\" class=\\"dissolve-animation ng-hide\\">\\n            <i class=\\"glyphicon glyphicon-asterisk orange\\"></i>\\n          </span>\\n        </div>\\n        <div class=\\"widget wblue\\">\\n          <div data-cc-widget-header title=\\"{{vm.title}}\\"></div>\\n          <div class=\\"widget-content form-horizontal\\">\\n            <div class=\\"form-group\\">\\n              <label class=\\"col-xs-12 col-sm-2\\">Name</label>\\n              <input\\n                class=\\"col-xs-12 col-sm-9\\"\\n                name=\\"sage.name\\"\\n                ng-model=\\"vm.sage.name\\"\\n                server-error=\\"vm.errors\\"\\n              />\\n            </div>\\n            <div class=\\"form-group\\">\\n              <label class=\\"col-xs-12 col-sm-2\\">Username</label>\\n              <input\\n                class=\\"col-xs-12 col-sm-9\\"\\n                name=\\"sage.userName\\"\\n                ng-model=\\"vm.sage.userName\\"\\n                server-error=\\"vm.errors\\"\\n              />\\n            </div>\\n            <div class=\\"form-group\\">\\n              <label class=\\"col-xs-12 col-sm-2\\">Email</label>\\n              <input\\n                class=\\"col-xs-12 col-sm-9\\"\\n                type=\\"email\\"\\n                name=\\"sage.email\\"\\n                ng-model=\\"vm.sage.email\\"\\n                server-error=\\"vm.errors\\"\\n              />\\n            </div>\\n          </div>\\n        </div>\\n      </form>\\n    </div>\\n  </section>\\n</section>\\n```\\n\\nIf you look closely at where `server-error` is used we have a name attribute set (eg \\"sage.email\\") and we\'re passing in something called `<em>vm.</em>errors` as the `server-error` attribute value. That\'s because we\'re using the \\"controller as\\" syntax and our controller is called `vm`.\\n\\nOn that controller we\'re going to have a dictionary style object called `errors`. If you wanted to you could put that object on the scope instead and omit the \\"vm.\\" prefix. You could call it `wrongThingsWhatISpottedWithYourModel` or `barry` \\\\- whatever floats your boat really. You get my point; it\'s flexible.\\n\\nLet\'s take a look at our sageEdit Angular controller:\\n\\n### TypeScript\\n\\n```ts\\nmodule controllers {\\n  \'use strict\';\\n\\n  interface sageEditRouteParams extends ng.route.IRouteParamsService {\\n    id: number;\\n  }\\n\\n  interface sageEditScope extends ng.IScope {\\n    form: ng.IFormController;\\n  }\\n\\n  class SageEdit {\\n    errors: { [field: string]: string };\\n    log: loggerFunction;\\n    logError: loggerFunction;\\n    logSuccess: loggerFunction;\\n    sage: sage;\\n    title: string;\\n\\n    private _isSaving: boolean;\\n\\n    static $inject = [\\n      \'$location\',\\n      \'$routeParams\',\\n      \'$scope\',\\n      \'common\',\\n      \'datacontext\',\\n    ];\\n    constructor(\\n      private $location: ng.ILocationService,\\n      private $routeParams: sageEditRouteParams,\\n      private $scope: sageEditScope,\\n      private common: common,\\n      private datacontext: datacontext,\\n    ) {\\n      this.errors = {};\\n      this.log = common.logger.getLogFn(controllerId);\\n      this.logError = common.logger.getLogFn(controllerId, \'error\');\\n      this.logSuccess = common.logger.getLogFn(controllerId, \'success\');\\n      this.sage = undefined;\\n      this.title = \'Sage Edit\';\\n\\n      this._isSaving = false;\\n\\n      this.activate();\\n    }\\n\\n    // Prototype methods\\n\\n    activate() {\\n      var id = this.$routeParams.id;\\n      var dataPromises: ng.IPromise<any>[] = [this.getSage(id)];\\n\\n      this.common\\n        .activateController(dataPromises, controllerId, this.title)\\n        .then(() => {\\n          this.log(\'Activated Sage Edit View\');\\n          this.title = \'Sage Edit: \' + this.sage.name;\\n        });\\n    }\\n\\n    getSage(id: number) {\\n      return this.datacontext.sage.getById(id).then((sage) => {\\n        this.sage = sage;\\n      });\\n    }\\n\\n    save() {\\n      this.errors = {}; //Wipe server errors\\n      this._isSaving = true;\\n      this.datacontext.sage.save(this.sage).then((response) => {\\n        if (response.success) {\\n          this.sage = response.entity;\\n          this.logSuccess(\\n            \'Saved \' + this.sage.name + \' [\' + this.sage.id + \']\',\\n          );\\n          this.$location.path(\'/sages/detail/\' + this.sage.id);\\n        } else {\\n          this.logError(\'Failed to save\', response.errors);\\n\\n          angular.forEach(response.errors, (errors, field) => {\\n            (<ng.INgModelController>this.$scope.form[field]).$setValidity(\\n              \'server\',\\n              false,\\n            );\\n            this.errors[field] = errors.join(\',\');\\n          });\\n        }\\n\\n        this._isSaving = false;\\n      });\\n    }\\n\\n    // Properties\\n\\n    get hasChanges(): boolean {\\n      return this.$scope.form.$dirty;\\n    }\\n\\n    get canSave(): boolean {\\n      return this.hasChanges && !this._isSaving && this.$scope.form.$valid;\\n    }\\n  }\\n\\n  var controllerId = \'sageEdit\';\\n  angular.module(\'app\').controller(controllerId, SageEdit);\\n}\\n```\\n\\n### JavaScript\\n\\n```js\\nvar controllers;\\n(function (controllers) {\\n  \'use strict\';\\n\\n  var SageEdit = (function () {\\n    function SageEdit($location, $routeParams, $scope, common, datacontext) {\\n      this.$location = $location;\\n      this.$routeParams = $routeParams;\\n      this.$scope = $scope;\\n      this.common = common;\\n      this.datacontext = datacontext;\\n      this.errors = {};\\n      this.log = common.logger.getLogFn(controllerId);\\n      this.logError = common.logger.getLogFn(controllerId, \'error\');\\n      this.logSuccess = common.logger.getLogFn(controllerId, \'success\');\\n      this.sage = undefined;\\n      this.title = \'Sage Edit\';\\n\\n      this._isSaving = false;\\n\\n      this.activate();\\n    }\\n    // Prototype methods\\n    SageEdit.prototype.activate = function () {\\n      var _this = this;\\n      var id = this.$routeParams.id;\\n      var dataPromises = [this.getSage(id)];\\n\\n      this.common\\n        .activateController(dataPromises, controllerId, this.title)\\n        .then(function () {\\n          _this.log(\'Activated Sage Edit View\');\\n          _this.title = \'Sage Edit: \' + _this.sage.name;\\n        });\\n    };\\n\\n    SageEdit.prototype.getSage = function (id) {\\n      var _this = this;\\n      return this.datacontext.sage.getById(id).then(function (sage) {\\n        _this.sage = sage;\\n      });\\n    };\\n\\n    SageEdit.prototype.save = function () {\\n      var _this = this;\\n      this.errors = {}; //Wipe server errors\\n      this._isSaving = true;\\n      this.datacontext.sage.save(this.sage).then(function (response) {\\n        if (response.success) {\\n          _this.sage = response.entity;\\n          _this.logSuccess(\\n            \'Saved \' + _this.sage.name + \' [\' + _this.sage.id + \']\',\\n          );\\n\\n          _this.$location.path(\'/sages/detail/\' + _this.sage.id);\\n        } else {\\n          _this.logError(\'Failed to save\', response.errors);\\n\\n          angular.forEach(response.errors, function (errors, field) {\\n            _this.$scope.form[field].$setValidity(\'server\', false);\\n            _this.errors[field] = errors.join(\',\');\\n          });\\n        }\\n\\n        _this._isSaving = false;\\n      });\\n    };\\n\\n    Object.defineProperty(SageEdit.prototype, \'hasChanges\', {\\n      // Properties\\n      get: function () {\\n        return this.$scope.form.$dirty;\\n      },\\n      enumerable: true,\\n      configurable: true,\\n    });\\n\\n    Object.defineProperty(SageEdit.prototype, \'canSave\', {\\n      get: function () {\\n        return this.hasChanges && !this._isSaving && this.$scope.form.$valid;\\n      },\\n      enumerable: true,\\n      configurable: true,\\n    });\\n    SageEdit.$inject = [\\n      \'$location\',\\n      \'$routeParams\',\\n      \'$scope\',\\n      \'common\',\\n      \'datacontext\',\\n    ];\\n    return SageEdit;\\n  })();\\n\\n  var controllerId = \'sageEdit\';\\n  angular.module(\'app\').controller(controllerId, SageEdit);\\n})(controllers || (controllers = {}));\\n```\\n\\nOkay - this is a shedload of code and most of it isn\'t relevant to you. I share it as I like to see things in context. Let\'s focus in on the important bits that you should take away. Firstly, our controller has a property called `errors`.\\n\\nSecondly, when we attempt to save our server sends back a JSON payload which, given a validation failure, looks something like this:\\n\\n```json\\n{\\n  \\"success\\": false,\\n  \\"errors\\": {\\n    \\"sage.name\\": [\\"The Name field is required.\\"],\\n    \\"sage.userName\\": [\\n      \\"The UserName field is required.\\",\\n      \\"The field UserName must be a string with a minimum length of 3 and a maximum length of 30.\\"\\n    ],\\n    \\"sage.email\\": [\\"The Email field is not a valid e-mail address.\\"]\\n  }\\n}\\n```\\n\\nSo let\'s pare back our `save` function to the bare necessities (those simple bare necessities, forget about your worries and your strife...):\\n\\n### TypeScript\\n\\n```ts\\nsave() {\\n\\n      this.errors = {}; //Wipe server errors\\n\\n      this.datacontext.sage.save(this.sage).then(response => {\\n\\n        if (response.success) {\\n          this.sage = response.entity;\\n        }\\n        else {\\n          angular.forEach(response.errors, (errors, field) => {\\n            (<ng.INgModelController>this.$scope.form[field]).$setValidity(\\"server\\", false);\\n            this.errors[field] = errors.join(\\",\\");\\n          });\\n        }\\n      });\\n    }\\n```\\n\\n### JavaScript\\n\\n```js\\nSageEdit.prototype.save = function () {\\n  var _this = this;\\n  this.errors = {}; //Wipe server errors\\n  this.datacontext.sage.save(this.sage).then(function (response) {\\n    if (response.success) {\\n      _this.sage = response.entity;\\n    } else {\\n      angular.forEach(response.errors, function (errors, field) {\\n        _this.$scope.form[field].$setValidity(\'server\', false);\\n        _this.errors[field] = errors.join(\',\');\\n      });\\n    }\\n  });\\n};\\n```\\n\\nAt the point of save we wipe any server error messages that might be stored on the client. Then, if we receive back a payload with errors we store those errors and set the validity of the relevant form element to false. This will trigger the display of the message by our directive.\\n\\nThat\'s us done for the client side. You\'re no doubt now asking yourself this question:\\n\\n## How can I get ASP.Net to send me this information?\\n\\nSo glad you asked. We\'ve a simple model that looks like this which has a number of data annotations:\\n\\n```cs\\npublic class Sage\\n{\\n  public int Id { get; set; }\\n\\n  [Required]\\n  public string Name { get; set; }\\n\\n  [Required]\\n  [StringLength(30, MinimumLength = 3)]\\n  public string UserName { get; set; }\\n\\n  [EmailAddress]\\n  public string Email { get; set; }\\n}\\n```\\n\\nWhen we save we post back to a Web API controller that looks like this:\\n\\n```cs\\npublic class SageController : ApiController\\n{\\n  // ...\\n\\n  public IHttpActionResult Post(User sage)\\n  {\\n    if (!ModelState.IsValid) {\\n\\n      return Ok(new\\n      {\\n        Success = false,\\n        Errors = ModelState.ToErrorDictionary()\\n      });\\n    }\\n\\n    sage = _userService.Save(sage);\\n\\n    return Ok(new\\n    {\\n      Success = true,\\n      Entity = sage\\n    });\\n  }\\n\\n  // ...\\n}\\n```\\n\\nAs you can see, when `ModelState` is not valid we send back a dictionary of the `ModelState` error messages keyed by property name. We generate this with an extension method I wrote called `ToErrorDictionary`:\\n\\n```cs\\npublic static class ModelStateExtensions\\n{\\n  public static Dictionary<string, IEnumerable<string>> ToErrorDictionary(\\n    this System.Web.Http.ModelBinding.ModelStateDictionary modelState, bool camelCaseKeyName = true)\\n  {\\n    var errors = modelState\\n      .Where(x => x.Value.Errors.Any())\\n      .ToDictionary(\\n        kvp => CamelCasePropNames(kvp.Key),\\n        kvp => kvp.Value.Errors.Select(e => e.ErrorMessage)\\n      );\\n\\n    return errors;\\n  }\\n\\n  private static string CamelCasePropNames(string propName)\\n  {\\n    var array = propName.Split(\'.\');\\n    var camelCaseList = new string[array.Length];\\n    for (var i = 0; i < array.Length; i++)\\n    {\\n      var prop = array[i];\\n      camelCaseList[i] = prop.Substring(0, 1).ToLower() + prop.Substring(1, prop.Length - 1);\\n    }\\n    return string.Join(\\".\\", camelCaseList);\\n  }\\n}\\n```\\n\\nThat\'s it - your solution front to back. It would be quite easy to hook other types of validation in server-side (database level checks etc). I hope you find this useful."},{"id":"hottowel-angular-meet-typescript","metadata":{"permalink":"/hottowel-angular-meet-typescript","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2014-07-03-hottowel-angular-meet-typescript/index.md","source":"@site/blog/2014-07-03-hottowel-angular-meet-typescript/index.md","title":"HotTowel-Angular meet TypeScript","description":"Johnny Reilly creates a bare-bones port of the Hot Towel Angular SPA Template to TypeScript in order to demonstrate the ease of transition.","date":"2014-07-03T00:00:00.000Z","tags":[{"inline":false,"label":"Angular JS","permalink":"/tags/angularjs","description":"The original Angular JS framework."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":2.72,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"hottowel-angular-meet-typescript","title":"HotTowel-Angular meet TypeScript","authors":"johnnyreilly","tags":["angularjs","typescript"],"hide_table_of_contents":false,"description":"Johnny Reilly creates a bare-bones port of the Hot Towel Angular SPA Template to TypeScript in order to demonstrate the ease of transition."},"unlisted":false,"prevItem":{"title":"AngularJS meet ASP.Net Server Validation","permalink":"/angularjs-meet-aspnet-server-validation"},"nextItem":{"title":"A folk story wherein we shall find dates, DataAnnotations & data impedance mismatch","permalink":"/dates-DataAnnotations-and-data-impedance-mismatch"}},"content":"I\'ve recently ported John Papa\'s popular [Hot Towel Angular SPA Template](https://github.com/johnpapa/HotTowel-Angular) to TypeScript. Why? [Because it was there.](http://en.wikipedia.org/wiki/George_Mallory)\\n\\n\x3c!--truncate--\x3e\\n\\nIf you\'d like to read more about HotTowel-Angular then have a read of [John Papa\'s post](http://www.johnpapa.net/hot-towel-angular/). You can find my port on GitHub [here](https://github.com/johnnyreilly/HotTowel-Angular-TypeScript).\\n\\n## What is this port you speak of?\\n\\nIt is **intentionally** a \\"bare bones\\" port of the HotTowel-Angular JavaScript code across to TypeScript. It\'s essentially the same code as John\'s - just with added type annotations (and yes it is `noImplicitAny` compliant).\\n\\nYou could, if you wanted to, go much further. You could start using a whole host of TypeScripts functionality: modules / classes / arrow functions... the whole shebang. But my port is deliberately not that; I didn\'t want to scare your horses... I wanted you to see how easy it is to move from JS to TS. And I\'m standing on the shoulders of that great giant [John Papa](https://twitter.com/john_papa) for that purpose.\\n\\nIf you wanted an example of how you might go further in an Angular port to TypeScript then you could take a look at my [previous post](../2014-06-01-migrating-from-angularjs-to-angularts/index.md) on the topic.\\n\\n## What\'s in the repo?\\n\\nThe repo contains the contents of HotTowel-Angular\'s app folder, with each JavaScript file converted over to TypeScript. The compiled JavaScript files are also included so that you can compare just how similar the compiled JavaScript is to John\'s original code.\\n\\nIn fact there are only 2 differences in the end:\\n\\n### 1\\\\. sidebar.js\'s `getNavRoutes`\\n\\n...had the filtering changed from this:\\n\\n```ts\\nreturn r.config.settings && r.config.settings.nav;\\n```\\n\\nto this:\\n\\n```ts\\nreturn r.config.settings && r.config.settings.nav ? true : false;\\n```\\n\\nThis was necessary as TypeScript insists that the array `filter` predicate returns a `boolean`. John\'s original method returns a number (`nav`\'s value to be clear) which actually seems to work fine. My assumption is that JavaScript\'s filter method is happy with a truth-y / false-y test which John\'s implementation would satisfy.\\n\\n### 2\\\\. common.js\'s `$broadcast`\\n\\n...had to be given a rest parameter to satisfy the TS compiler. John\'s original method exposed no parameters as it just forwards on whatever arguments are passed to it. This means that `$broadcast` has a bit of unused code in the head of the generated method:\\n\\n```js\\nvar args = [];\\nfor (var _i = 0; _i < arguments.length - 0; _i++) {\\n  args[_i] = arguments[_i + 0];\\n}\\n```\\n\\n## If you want to use this\\n\\nThen simply follow the instructions for installing [HotTowel-Angular](https://github.com/johnpapa/HotTowel-Angular) and then drop this repo\'s app folder over the one just created when HotTowel-Angular was installed. If you\'re using Visual Studio then make sure that you include the new TS files into your project and give them the `BuildAction` of `TypeScriptCompile`.\\n\\nYou\'ll need the following NuGet packages for the relevant DefinitelyTyped Typings:\\n\\n```ps\\nInstall-Package angularjs.TypeScript.DefinitelyTyped\\n    Install-Package angular-ui-bootstrap.TypeScript.DefinitelyTyped\\n    Install-Package jquery.TypeScript.DefinitelyTyped\\n    Install-Package spin.TypeScript.DefinitelyTyped\\n    Install-Package toastr.TypeScript.DefinitelyTyped\\n```\\n\\nAnd you\'re good to go. If you\'re not using Visual Studio then you may need to add in some `&lt;reference path=\\"angular.d.ts\\" /&gt;` etc. statements to the TypeScript files as well.\\n\\nIf you\'re interested in the specific versions of the typings that I used then you can find them in the `packages.config` of the repo.\\n\\n## Thanks\\n\\nTo John Papa for creating HotTowel-Angular. Much love.\\n\\nAnd my mum too... Just because."},{"id":"dates-DataAnnotations-and-data-impedance-mismatch","metadata":{"permalink":"/dates-DataAnnotations-and-data-impedance-mismatch","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2014-06-20-dates-DataAnnotations-and-data-impedance-mismatch/index.md","source":"@site/blog/2014-06-20-dates-DataAnnotations-and-data-impedance-mismatch/index.md","title":"A folk story wherein we shall find dates, DataAnnotations & data impedance mismatch","description":"This article offers developers an attribute-based solution to prevent datetime errors, ensuring that DateTime properties only include dates.","date":"2014-06-20T00:00:00.000Z","tags":[],"readingTime":4.04,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"dates-DataAnnotations-and-data-impedance-mismatch","title":"A folk story wherein we shall find dates, DataAnnotations & data impedance mismatch","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false,"description":"This article offers developers an attribute-based solution to prevent datetime errors, ensuring that DateTime properties only include dates."},"unlisted":false,"prevItem":{"title":"HotTowel-Angular meet TypeScript","permalink":"/hottowel-angular-meet-typescript"},"nextItem":{"title":"Migrating from AngularJS to AngularTS - a walkthrough","permalink":"/migrating-from-angularjs-to-angularts"}},"content":"If you ever take a step back from what you\'re doing it can sometimes seem pretty abstract. Here\'s an example. I was looking at an issue in an app that I was supporting. The problem concerned a field which was to store a date value. Let\'s call it, for the sake of argument, `valuation_date`. (Clearly in reality the field name was entirely different... Probably.) This field was supposed to represent a specific date, like June 15th 2012 or 19th August 2014. To be clear, a date and \\\\***not**\\\\* in any way, a time.\\n\\n\x3c!--truncate--\x3e\\n\\n`valuation_date` was stored in a SQL database as a [`datetime`](http://msdn.microsoft.com/en-gb/library/ms187819.aspx). That\'s right a date with a time portion. I\'ve encountered this sort of scenario many times on systems I\'ve inherited. Although there is a [`date`](http://msdn.microsoft.com/en-gb/library/bb630352.aspx) type in SQL it\'s pretty rarely used. I think it only shipped in SQL Server with 2008 which may go some way to explaining this. Anyway, I digress...\\n\\n`valuation_date` was read into a field in a C# application called `ValuationDate` which was of type [`DateTime`](http://msdn.microsoft.com/en-us/library/system.datetime.aspx). As the name suggests this is also a date with a time portion. After a travelling through various layers of application this ended up being serialized as JSON and sent across the wire where it became a JavaScript variable by the name of `valuationDate` which had the type [`Date`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Date). Despite the deceptive name this is also, you guessed it, a date with a time portion. (Fine naming work there JavaScript!)\\n\\nYou can probably guess where I\'m going with this... Despite our (cough) rock solid naming convention, the situation had arisen where actual datetimes had snuck in. That\'s right, in the wilds of production, records with `valuation_date`s with time components had been spotted. My mission was to hunt them, kill them and stop them reproducing...\\n\\n## A Primitive Problem\\n\\nDates is a sticky topic in many languages. As I mentioned, SQL Server has a [`date`](http://msdn.microsoft.com/en-gb/library/bb630352.aspx) data type. C# has [`DateTime`](http://msdn.microsoft.com/en-gb/library/system.datetime.aspx). If you want to operate on Dates alone then you\'re best off talking looking at Jon Skeet\'s [NodaTime](http://nodatime.org/) \\\\- though most people start with `DateTime` and stick with it. (After all, it\'s native.) As to JavaScript, well primitive-wise there\'s no alternative to `Date` \\\\- but [`Moment.js`](http://momentjs.com/) may help.\\n\\nMy point is that it is a long standing issue in the development world. We represent data in types that aren\'t entirely meant for the purpose that they are used. It\'s not just restricted to dates, numbers have a comparable story around the issue of [decimals and doubles](http://csharpindepth.com/Articles/General/Decimal.aspx). As a result of data type issues, developers experience problems. Like the one I was facing.\\n\\n## An Attribute Solution\\n\\nThe source of the problem turned out to be the string JavaScript [`Date constructor`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Date) in an earlier version of Internet Explorer. The fix was switching away from using the JavaScript Date constructor in favour of using Moment.js\'s more dependable ability to parse strings into dates. Happy days we\'re working once more! Some quick work to put together a SQL script to fix up the data and we have ourselves our patch!\\n\\nBut we didn\'t want to get bitten again. We wanted ourselves a little [belts and braces](http://dictionary.cambridge.org/dictionary/british/belt-and-braces). What do do? Hang on a minute, lads \u2013 I\'ve got a great idea... It\'s `<a href=\\"http://msdn.microsoft.com/en-us/library/system.componentmodel.dataannotations.validationattribute(v=vs.110).aspx\\">ValidationAttribute</a>` time!\\n\\nWe whipped ourselves up an attribute that looked like this:\\n\\n```cs\\nusing System;\\nusing System.ComponentModel.DataAnnotations;\\nusing System.Globalization;\\n\\nnamespace My.Attributes\\n{\\n    [AttributeUsage(AttributeTargets.Property | AttributeTargets.Field, Inherited = false, AllowMultiple = false)]\\n    public class DateOnlyAttribute: ValidationAttribute\\n    {\\n        protected override ValidationResult IsValid(object value, ValidationContext validationContext)\\n        {\\n            if (value != null)\\n            {\\n                if (value is DateTime)\\n                {\\n                    // Date but not Time check\\n                    var date = (DateTime) value;\\n                    if (date.TimeOfDay != TimeSpan.Zero)\\n                    {\\n                        return new ValidationResult(date.ToString(\\"O\\", CultureInfo.InvariantCulture) + \\" is not a date - it is a date with a time\\", new[] { validationContext.MemberName });\\n                    }\\n                }\\n                else\\n                {\\n                    return new ValidationResult(\\"DateOnlyAttribute can only be used on DateTime? and DateTime\\", new[] { validationContext.MemberName });\\n                }\\n            }\\n\\n            return ValidationResult.Success;\\n        }\\n    }\\n}\\n```\\n\\nThis attribute does 2 things:\\n\\n1. Most importantly it fails validation for any `DateTime` or `DateTime?` that includes a time portion. It only allows through DateTimes where the clock strikes midnight. It\'s optimised for Cinderella.\\n2. It fails validation if the attribute is applied to any property which is not a `DateTime` or `DateTime?`.\\n\\nYou can decorate `DateTime` or `DateTime?` properties on your model with this attribute like so:\\n\\n```cs\\nnamespace My.Models\\n{\\n    public class ImAModelYouKnowWhatIMean\\n    {\\n        public int Id { get; set; }\\n\\n        [DateOnlyAttribute]\\n        public DateTime ValuationDate { get; set; }\\n\\n        // Other properties...\\n    }\\n}\\n```\\n\\nAnd if you\'re using MVC (or anything that makes use of the validation data annotations) then you\'ll now find that you are nicely protected from DateTimes masquerading as dates. Should they show up you\'ll find that `ModelState.IsValid` is false and you can kick them to the curb with alacrity!"},{"id":"migrating-from-angularjs-to-angularts","metadata":{"permalink":"/migrating-from-angularjs-to-angularts","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2014-06-01-migrating-from-angularjs-to-angularts/index.md","source":"@site/blog/2014-06-01-migrating-from-angularjs-to-angularts/index.md","title":"Migrating from AngularJS to AngularTS - a walkthrough","description":"Learn how to migrate an AngularJS app from JavaScript to TypeScript in this walkthrough on a simple website/app for sending prayer requests.","date":"2014-06-01T00:00:00.000Z","tags":[{"inline":false,"label":"Angular JS","permalink":"/tags/angularjs","description":"The original Angular JS framework."},{"inline":false,"label":"Automated Testing","permalink":"/tags/automated-testing","description":"How to perform the automation of tests."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":12.55,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"migrating-from-angularjs-to-angularts","title":"Migrating from AngularJS to AngularTS - a walkthrough","authors":"johnnyreilly","tags":["angularjs","automated testing","typescript"],"hide_table_of_contents":false,"description":"Learn how to migrate an AngularJS app from JavaScript to TypeScript in this walkthrough on a simple website/app for sending prayer requests."},"unlisted":false,"prevItem":{"title":"A folk story wherein we shall find dates, DataAnnotations & data impedance mismatch","permalink":"/dates-DataAnnotations-and-data-impedance-mismatch"},"nextItem":{"title":"Team Foundation Server, Continuous Integration and separate projects for JavaScript unit tests","permalink":"/team-foundation-server-continuous-integration-and-javascript-unit-tests-in-unit-test-project"}},"content":"It started with nuns. Don\'t all good stories start that way? One of my (many) aunts is a Poor Clare nun. At some point in the distant past I was cajoled into putting together a simple website for her convent. This post is a walkthrough of how to migrate from AngularJS using JavaScript to AngularJS using TypeScript. It just so happens that the AngularJS app in question is the one that belongs to my mother\'s sister\'s convent.\\n\\n\x3c!--truncate--\x3e\\n\\n## TL;DR - grab what you need\\n\\nFor reference the complete \\"before\\" and \\"after\\" projects can be found on GitHub [here](https://github.com/johnnyreilly/AngularJS2AngularTS). This is available so people can see clearly what changes have been made in the migration.\\n\\nThe content of the site is available for <u>reference only</u>\\n\\n. (Not that I can really imagine people creating their own \\"Poor Clares\\" site and hawking it to convents around the globe but I thought I\'d make the point.)\\n\\n## Background\\n\\nI\'ve been quietly maintaining this website / app for quite a while now. It\'s a very simple site; 95% of it is static content about the convent. The one piece of actual functionality is a page which allows the user of the website to send a prayer request to the nuns at the convent.\\n\\nBehind the scenes this sends 2 emails:\\n\\n- The first back to the person who submitted the prayer request assuring them that they will be prayed for.\\n- The second to the convent telling them the details of what the person would like prayer for.\\n\\n<aside><em>It\'s not accidental that I am not sharing the location of my aunt\'s website in this post. Given the inherent mischievousness of most developers (I should know, I am one) I harbour a fear that readers of this post might go away and submit many an insincere prayer request (or worse) to the convent. If that\'s you I don\'t intend to help you. You\'re clever, you\'ll find the site if you are so minded. But please know that the nuns who read any of your prayer requests are wonderful people (nuns get a bad rep) and that they love you. They *<strong>will</strong>* pray for you. They\'re good like that. I appeal to your better nature on this.</em></aside>\\n\\nRight now you are probably thinking this is an unusual post. Perhaps it is, but bear with me.\\n\\nOver time the website has had many incarnations. It\'s been table-based layout, it\'s used Kendo UI, it\'s used Bootstrap. It\'s been static HTML, it\'s been ASP.Net WebForms, it\'s been ASP.Net MVC and it\'s currently built using **AngularJS** with **MVC** on the back-end to handle bundling / minification and dispatching of emails.\\n\\nI decided to migrate this AngularJS app to use TypeScript. As I did that I thought I\'d document the process for anyone else who might be considering doing something similar. As it happens this is a particularly good candidate for migration as there\'s a full unit test suite for the app (written with Jasmine). Once I\'ve finished the migration these unit tests should pass, just as they do currently.\\n\\nYou are probably thinking to yourself \\"but TypeScript is just about adding compile-time annotations right? How could the unit tests not pass after migration?\\" Fair point, well made. Well that is generally true but I have something slightly different planned when we get to the controllers - you\'ll see what I mean...\\n\\nIt\'s also a good candidate for documenting a walkthrough as it\'s a particularly small and simple Angular app. It consists of just **3 controllers**, **2 services** and **1 app**.\\n\\nBefore I kick off I thought I\'d list a couple of guidelines / caveats on this post:\\n\\n- I don\'t intend to say much about the architecture of this application - I want to focus on the migration from JavaScript to TypeScript.\\n- The choices that I make for the migration path do not necessarily reflect the \\"one true way\\". Rather, they are pragmatic choices that I am making - there may be alternatives approaches here and there that could be used instead.\\n- I love Visual Studio - it\'s my IDE of choice and the one I am using as I perform the migration. Some of the points that I will make are Visual Studio specific - I will try and highlight that when appropriate.\\n\\n## Typings\\n\\nThe first thing we\'re going to need to get going are the Angular typing files which can be found on Definitely Typed [here](https://github.com/borisyankov/DefinitelyTyped/tree/master/angularjs). Since these typings are made available over [NuGet](https://www.nuget.org/packages/angularjs.TypeScript.DefinitelyTyped/) I\'m going to pull them in with a wave of my magic `Install-Package angularjs.TypeScript.DefinitelyTyped`.\\n\\nAs well as pulling in the typing files Visual Studio 2013 has also made some tweaks to my `PoorClaresAngular.csproj` file which it tells me about.\\n\\nAnd these are the TypeScript specific additions that Visual Studio has made to `PoorClaresAngular.csproj`:\\n\\n```xml\\n<Import\\n   Project=\\"$(MSBuildExtensionsPath32)\\\\Microsoft\\\\VisualStudio\\\\v$(VisualStudioVersion)\\\\TypeScript\\\\Microsoft.TypeScript.Default.props\\"\\n   Condition=\\"Exists(\'$(MSBuildExtensionsPath32)\\\\Microsoft\\\\VisualStudio\\\\v$(VisualStudioVersion)\\\\TypeScript\\\\Microsoft.TypeScript.Default.props\')\\" />\\n\\n  <TypeScriptToolsVersion>1.0</TypeScriptToolsVersion>\\n\\n  <Import\\n   Project=\\"$(MSBuildExtensionsPath32)\\\\Microsoft\\\\VisualStudio\\\\v$(VisualStudioVersion)\\\\TypeScript\\\\Microsoft.TypeScript.targets\\"\\n   Condition=\\"Exists(\'$(MSBuildExtensionsPath32)\\\\Microsoft\\\\VisualStudio\\\\v$(VisualStudioVersion)\\\\TypeScript\\\\Microsoft.TypeScript.targets\')\\" />\\n```\\n\\nI\'m going to add one extra of my own:\\n\\n```xml\\n<TypeScriptNoImplicitAny>True</TypeScriptNoImplicitAny>\\n```\\n\\nThis prevents you having variables of type `any` in your TypeScript codebase without you implicitly specifying the type. You can live without this but I\'ve found it\'s useful to catch where you\'re missing out on the benefit of static typing. Further to that, this option can be particularly useful when performing a migration. It will become obvious why this is the case as we go on.\\n\\nI decline the kind opportunity to further search NuGet as I\'m already on my way typing-wise. So let\'s review what has happened. Below you can see the typing files that have been pulled in and that the project and packages files were amended.\\n\\n## Changing JS files to TS files\\n\\nThis really should be as simple as changing all the JavaScript files underneath the `js` directory to have the suffix `ts`.\\n\\nAnd if you\'re not using Visual Studio it is. But if you are using Visual Studio there\'s a certain amount of fiddling required to include the generated `.js` and `.js.map` files associated with each `.ts` file. The easiest (hah!) thing to do is to crack open the project and wherever you find a `&lt;TypeScriptCompile Include=\\"js\\\\somePath.ts\\" /&gt;` to add in 2 `Content` statements, one for each generated file which states the dependency on the TypeScript file. For example:\\n\\n```xml\\n<TypeScriptCompile Include=\\"js\\\\services\\\\siteSectionService.ts\\" />\\n    <Content Include=\\"js\\\\services\\\\siteSectionService.js\\">\\n      <DependentUpon>siteSectionService.ts</DependentUpon>\\n    </Content>\\n    <Content Include=\\"js\\\\services\\\\siteSectionService.js.map\\">\\n      <DependentUpon>siteSectionService.ts</DependentUpon>\\n    </Content>\\n```\\n\\nIt\'s a bit of a pain to have to do this at the moment. Hopefully the Visual Studio tooling will catch up so this sort of tweaking becomes unnecessary.\\n\\n## Recap\\n\\nSo, where are we? Well, we\'ve got our project ready for TypeScript, we\'ve pulled in the Angular typings from Definitely Typed and we\'ve turned all our JavaScript files in the `js` directory into TypeScript files.\\n\\nNow we can actually start working through our TypeScript files and ensuring we\'re all typed correctly. Please note that because I\'m working in Visual Studio I get the benefit of implicit referencing; I don\'t have to explicitly state the typing files each TypeScript file relies on at the head of the file (eg `/// &lt;reference path=\\"angularjs/angular.d.ts\\" /&gt;`). If you aren\'t working in Visual Studio then you\'d need to add these yourself.\\n\\n## TypeScriptify `app.ts`\\n\\nOpening up `app.ts` we\'re presented with a few red squigglies.\\n\\nThese red squigglies are the direct result of my earlier opting in to `NoImplicitAny`. So in my view it\'s already paid for itself as it\'s telling me where I could start using typings. So to get things working nicely I\'ll give `$routeProvider` the type of `ng.route.IRouteProvider` and I\'ll explicitly specify the type of `any` for the 2 `params` parameters:\\n\\n```ts\\n// ...\\n    function ($routeProvider: ng.route.IRouteProvider) {\\n\\n        function getTheConventTemplateUrl(params: any) {\\n            var view = params.view || \\"home\\";\\n            return \\"partials/theConvent/\\" + view + \\".html\\";\\n        }\\n\\n        function getMainTemplateUrl(params: any) {\\n            var view = params.view || \\"home\\";\\n            return \\"partials/main/\\" + view + \\".html\\";\\n        }\\n\\n        // ...\\n    }\\n    // ...\\n```\\n\\n## TypeScriptify `siteSectionService.ts`\\n\\nOpening up `siteSectionService.ts` we\'re only presented with a single squiggly, and for the same reason as last time.\\n\\nThis error is easily remedied by giving `path` the type of `string`.\\n\\nWhat\'s more interesting / challenging is thinking about how we want to enforce the definition of `siteSectionService`. Remember, this is a service and as such it will be re-used elsewhere in the application (in both `navController` and `mainController`). What we need is an interface that describes what our (revealing module pattern) service exposes:\\n\\n```ts\\n\'use strict\';\\n\\ninterface ISiteSectionService {\\n  getSiteSection: () => string;\\n  determineSiteSection: (path: string) => void;\\n}\\n\\nangular.module(\'poorClaresApp.services\').factory(\\n  \'siteSectionService\',\\n\\n  [\\n    // No dependencies at present\\n    function (): ISiteSectionService {\\n      var siteSection = \'home\';\\n\\n      function getSiteSection() {\\n        return siteSection;\\n      }\\n\\n      function determineSiteSection(path: string) {\\n        var newSiteSection = \'home\';\\n\\n        if (path.indexOf(\'/theConvent/\') !== -1) {\\n          newSiteSection = \'theConvent\';\\n        } else if (path !== \'/\') {\\n          newSiteSection = \'main\';\\n        }\\n\\n        siteSection = newSiteSection;\\n      }\\n\\n      return {\\n        getSiteSection: getSiteSection,\\n        determineSiteSection: determineSiteSection,\\n      };\\n    },\\n  ],\\n);\\n```\\n\\nAs you can see the `ISiteSectionService ` interface is marked as the return type of the function. This ensures that what we return from the function satisfies that definition. Also, it allows us to re-use that interface elsewhere (as we will do later).\\n\\n## TypeScriptify `prayerRequestService.ts`\\n\\nOpening up `prayerRequestService.ts` we\'re again in `NoImplicitAny` country.\\n\\nThis is fixed up by defining `$http` as `ng.IHttpService` and `email` and `prayFor` as `string`.\\n\\nAs with `siteSectionService` we need to create an interface to define what `prayerRequestService` exposes. This leaves us with this:\\n\\n```ts\\n\'use strict\';\\n\\ninterface IPrayerRequestService {\\n  sendPrayerRequest: (\\n    email: string,\\n    prayFor: string,\\n  ) => ng.IPromise<{\\n    success: boolean;\\n    text: string;\\n  }>;\\n}\\n\\nangular.module(\'poorClaresApp.services\').factory(\\n  \'prayerRequestService\',\\n\\n  [\\n    \'$http\',\\n    function ($http: ng.IHttpService): IPrayerRequestService {\\n      var url = \'/PrayerRequest\';\\n\\n      function sendPrayerRequest(email: string, prayFor: string) {\\n        var params = { email: email, prayFor: prayFor };\\n\\n        return $http.post(url, params).then(function (response) {\\n          return {\\n            success: response.data.success,\\n            text: response.data.text,\\n          };\\n        });\\n      }\\n\\n      return {\\n        sendPrayerRequest: sendPrayerRequest,\\n      };\\n    },\\n  ],\\n);\\n```\\n\\n## TypeScriptify `prayerRequestController.ts`\\n\\nOpening up `prayerRequestController.ts` leads me to the conclusion that I have **no interesting way left** of telling you that we once more need to supply types for our parameters. Let\'s take it as read that the same will happen on all remaining files as well eh? Hopefully by now it\'s fairly clear that this option is useful, even if only for a migration. I say this because using it forces you to think about what typings should be applied to your code.\\n\\nWe\'ll define `$scope` as `ng.IScope`, `prayerRequestService` as `IPrayerRequestService` (which we created just now) and `prayerRequest` as `{ email: string; prayFor: string }`. Which leaves me with this:\\n\\n```ts\\n\'use strict\';\\n\\nangular.module(\'poorClaresApp.controllers\').controller(\\n  \'PrayerRequestController\',\\n\\n  [\\n    \'$scope\',\\n    \'prayerRequestService\',\\n    function ($scope: ng.IScope, prayerRequestService: IPrayerRequestService) {\\n      var vm = this;\\n\\n      vm.send = function (prayerRequest: { email: string; prayFor: string }) {\\n        vm.message = {\\n          success: true,\\n          text: \'Sending...\',\\n        };\\n\\n        prayerRequestService\\n          .sendPrayerRequest(prayerRequest.email, prayerRequest.prayFor)\\n          .then(function (response) {\\n            vm.message = {\\n              success: response.success,\\n              text: response.text,\\n            };\\n          })\\n          .then(null, function (error) {\\n            // IE 8 friendly alias for catch\\n            vm.message = {\\n              success: false,\\n              text: \'Sorry your email was not sent\',\\n            };\\n          });\\n      };\\n    },\\n  ],\\n);\\n```\\n\\nI could move on but let\'s go for bonus points (and now you\'ll see why the unit test suite is so handy). To quote the Angular documentation:\\n\\n> In Angular, a Controller is a JavaScript constructor function that is used to augment the Angular Scope.\\n\\nSo let\'s see if we can swap over our vanilla contructor function for a TypeScript class. This will (in my view) better express the intention of the code. To do this I am essentially following the example laid down by my Definitely Typed colleague [Basarat](https://twitter.com/basarat). I highly recommend his [screencast on the topic](https://www.youtube.com/watch?v=WdtVn_8K17E). Also kudos to [Andrew Davey](https://twitter.com/andrewdavey) whose [post on the topic](http://aboutcode.net/2013/10/20/typescript-angularjs-controller-classes.html) also fed into this.\\n\\n```ts\\n\'use strict\';\\n\\nmodule poorClaresApp.controllers {\\n  class PrayerRequestController {\\n    static $inject = [\'$scope\', \'prayerRequestService\'];\\n    constructor(\\n      private $scope: ng.IScope,\\n      private prayerRequestService: IPrayerRequestService,\\n    ) {}\\n\\n    message: { success: boolean; text: string };\\n\\n    send(prayerRequest: { email: string; prayFor: string }) {\\n      this.message = {\\n        success: true,\\n        text: \'Sending...\',\\n      };\\n\\n      this.prayerRequestService\\n        .sendPrayerRequest(prayerRequest.email, prayerRequest.prayFor)\\n        .then((response) => {\\n          this.message = {\\n            success: response.success,\\n            text: response.text,\\n          };\\n        })\\n        .then(null, (error) => {\\n          // IE 8 friendly alias for catch\\n          this.message = {\\n            success: false,\\n            text: \'Sorry your email was not sent\',\\n          };\\n        });\\n    }\\n  }\\n\\n  angular\\n    .module(\'poorClaresApp.controllers\')\\n    .controller(\'PrayerRequestController\', PrayerRequestController);\\n}\\n```\\n\\nMy only reservation with this approach is that we have to declare the TypeScript class outside the `angular.module...` statement. To avoid cluttering up global scope I\'ve placed our class in a module called `poorClaresApp.controllers` which maps nicely to our Angular module name. It would be nice if I could place the class definition in an [IIFE](http://en.wikipedia.org/wiki/Immediately-invoked_function_expression) to completely keep this completely isolated but TypeScript doesn\'t allow for that syntax (for reasons I\'m unclear about - the output would be legal JavaScript).\\n\\nFor a small class this seems to add a little noise but as classes grow in complexity I think this approach will quickly start to pay dividends. There are a few things worth noting about the above approach:\\n\\n- The required injectable parameters have moved into the class definition in the form of the `static $inject` statement. I personally like that this no longer sits outside the code it relates to.\\n- Because we\'re using TypeScript arrow functions (which preserve the outer \\"this\\" context) we are now free to dispose of the `var vm = this;` mechanism we\'re were previously using for the same purpose. Much more intuitive code to my mind.\\n- We are not actually using `$scope` at all in this controller - maybe it should be removed entirely in the long run.\\n\\n## TypeScriptify `navController.ts`\\n\\n`navController` can be simply converted like so:\\n\\n```ts\\n\'use strict\';\\n\\ninterface INavControllerScope extends ng.IScope {\\n  isCollapsed: boolean;\\n  siteSection: string;\\n}\\n\\nangular.module(\'poorClaresApp.controllers\').controller(\\n  \'NavController\',\\n\\n  [\\n    \'$scope\',\\n    \'siteSectionService\',\\n    function (\\n      $scope: INavControllerScope,\\n      siteSectionService: ISiteSectionService,\\n    ) {\\n      $scope.isCollapsed = true;\\n      $scope.siteSection = siteSectionService.getSiteSection();\\n\\n      $scope.$watch(\\n        siteSectionService.getSiteSection,\\n        function (newValue, oldValue) {\\n          $scope.siteSection = newValue;\\n        },\\n      );\\n    },\\n  ],\\n);\\n```\\n\\nI\'d draw your attention to the creation of a the `INavControllerScope` interface that extends the default Angular $scope of `ng.IScope` with 2 extra properties.\\n\\nLet\'s also switch this over to the class based approach (there is less of a reason to on this occasion just looking at the size of the codebase but I\'m all about consistency of approach):\\n\\n```ts\\n\'use strict\';\\n\\nmodule poorClaresApp.controllers {\\n  interface INavControllerScope extends ng.IScope {\\n    isCollapsed: boolean;\\n    siteSection: string;\\n  }\\n\\n  class NavController {\\n    static $inject = [\'$scope\', \'siteSectionService\'];\\n    constructor(\\n      private $scope: INavControllerScope,\\n      private siteSectionService: ISiteSectionService,\\n    ) {\\n      $scope.isCollapsed = true;\\n      $scope.siteSection = siteSectionService.getSiteSection();\\n\\n      $scope.$watch(\\n        siteSectionService.getSiteSection,\\n        function (newValue, oldValue) {\\n          $scope.siteSection = newValue;\\n        },\\n      );\\n    }\\n  }\\n\\n  angular\\n    .module(\'poorClaresApp.controllers\')\\n    .controller(\'NavController\', NavController);\\n}\\n```\\n\\n## TypeScriptify `mainController.ts`\\n\\nFinally, `mainController` can be converted as follows:\\n\\n```ts\\n\'use strict\';\\n\\nangular.module(\'poorClaresApp.controllers\').controller(\\n  \'MainController\',\\n\\n  [\\n    \'$location\',\\n    \'siteSectionService\',\\n    function (\\n      $location: ng.ILocationService,\\n      siteSectionService: ISiteSectionService,\\n    ) {\\n      siteSectionService.determineSiteSection($location.path());\\n    },\\n  ],\\n);\\n```\\n\\nAgain it\'s just a case of assigning the undeclared types. For completeness lets also switch this over to the class based approach:\\n\\n```ts\\n\'use strict\';\\n\\nmodule poorClaresApp.controllers {\\n  class MainController {\\n    static $inject = [\'$location\', \'siteSectionService\'];\\n    constructor(\\n      private $location: ng.ILocationService,\\n      private siteSectionService: ISiteSectionService,\\n    ) {\\n      siteSectionService.determineSiteSection($location.path());\\n    }\\n  }\\n\\n  angular\\n    .module(\'poorClaresApp.controllers\')\\n    .controller(\'MainController\', MainController);\\n}\\n```"},{"id":"team-foundation-server-continuous-integration-and-javascript-unit-tests-in-unit-test-project","metadata":{"permalink":"/team-foundation-server-continuous-integration-and-javascript-unit-tests-in-unit-test-project","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2014-05-15-team-foundation-server-continuous-integration-and-javascript-unit-tests-in-unit-test-project/index.md","source":"@site/blog/2014-05-15-team-foundation-server-continuous-integration-and-javascript-unit-tests-in-unit-test-project/index.md","title":"Team Foundation Server, Continuous Integration and separate projects for JavaScript unit tests","description":"Learn how to run JavaScript tests on TFS/VSO by creating a separate unit test project to house tests, and installing Chutzpah on TFS/VSO.","date":"2014-05-15T00:00:00.000Z","tags":[{"inline":false,"label":"Automated Testing","permalink":"/tags/automated-testing","description":"How to perform the automation of tests."},{"inline":false,"label":"Visual Studio","permalink":"/tags/visual-studio","description":"The Visual Studio IDE."}],"readingTime":2.65,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"team-foundation-server-continuous-integration-and-javascript-unit-tests-in-unit-test-project","title":"Team Foundation Server, Continuous Integration and separate projects for JavaScript unit tests","authors":"johnnyreilly","tags":["automated testing","visual studio"],"hide_table_of_contents":false,"description":"Learn how to run JavaScript tests on TFS/VSO by creating a separate unit test project to house tests, and installing Chutzpah on TFS/VSO."},"unlisted":false,"prevItem":{"title":"Migrating from AngularJS to AngularTS - a walkthrough","permalink":"/migrating-from-angularjs-to-angularts"},"nextItem":{"title":"TypeScript, JSDoc and Intellisense","permalink":"/typescript-jsdoc-and-intellisense"}},"content":"Do you like to separate out your unit tests from the project you are testing? I imagine so. My own practice when creating a new project in Visual Studio is to create a separate unit test project alongside whose responsibility is to house unit tests for that new project.\\n\\n\x3c!--truncate--\x3e\\n\\nWhen I check in code for that project I expect the continuous integration build to kick off and, as part of that, the unit tests to be run. When it comes to running .NET tests then Team Foundation Server (and it\'s cloud counterpart Visual Studio Online) has your back. When it comes to running JavaScript tests then... not so much.\\n\\nThis post will set out:\\n\\n1. How to get JavaScript tests to run on TFS / VSO in a continuous integration scenario.\\n2. How to achieve this \\\\***without**\\\\* having to include your tests as part of web project.\\n\\nTo do this I will lean heavily (that\'s fancy language for \\"rip off entirely\\") on an [excellent blog post by Mathew Aniyan](https://blogs.msdn.com/b/visualstudioalm/archive/2012/07/09/javascript-unit-tests-on-team-foundation-service-with-chutzpah.aspx) which covers point #1. My contribution is point #2.\\n\\n## Points #1 and #2 in short order\\n\\nFirst of all, install Chutzpah on TFS / VSO. You can do this by following [Steps 1 - 6 from Mathew Aniyan\'s post](https://blogs.msdn.com/b/visualstudioalm/archive/2012/07/09/javascript-unit-tests-on-team-foundation-service-with-chutzpah.aspx). Instead of following steps 7 and 8 create a new unit test project in your solution.\\n\\n<aside>This unit test project will effectively be a C# project that hosts no real C# code at all. Instead we\'re going to use it to house JavaScript tests. If there is another way to have a separate project which TFS / VSO can pick up on and run tests in then please let me know. As far as I\'m aware though, this is the only game in town.</aside>\\n\\n**Edit 29/05/2014:** Matthew Manela (creator of Chutzpah) has confirmed that this is the correct approach - thanks chap!\\n\\n> [@johnny_reilly](https://twitter.com/johnny_reilly) Nope that is pretty much what you need to do.\\n>\\n> \u2014 Matthew Manela (@mmanela) [May 15, 2014](https://twitter.com/mmanela/statuses/466962743400996864)\\n\\n<script async=\\"\\" src=\\"//platform.twitter.com/widgets.js\\" charSet=\\"utf-8\\"><\/script>\\n\\nTo our unit test project add your JavaScript unit tests. These should be marked in Visual Studio with a Build Action of \\"Content\\" and a Copy to Output Directory of \\"Do not copy\\". You should be able to run these tests locally using the Visual Studio Chutzpah extension - or indeed in some other JavaScript test runner. Then, and this is the important part, edit the csproj file of your unit test project and add this `Import Project` statement:\\n\\n```xml\\n<Import Project=\\"$(VSToolsPath)\\\\WebApplications\\\\Microsoft.WebApplication.targets\\" Condition=\\"\'$(VSToolsPath)\' != \'\'\\" />\\n```\\n\\nOrdering is important in this case. It matters that this new statement sits after the other `Import Project` statements. So you should end up with a csproj file that looks in part like this: (comments added by me for clarity)\\n\\n```xml\\n\x3c!-- Pre-existing Import Project statements start --\x3e\\n  <Import Project=\\"$(VSToolsPath)\\\\TeamTest\\\\Microsoft.TestTools.targets\\" Condition=\\"Exists(\'$(VSToolsPath)\\\\TeamTest\\\\Microsoft.TestTools.targets\')\\" />\\n  <Import Project=\\"$(MSBuildToolsPath)\\\\Microsoft.CSharp.targets\\" />\\n  \x3c!-- Pre-existing Import Project statements end --\x3e\\n\\n  \x3c!-- New addition start --\x3e\\n  <Import Project=\\"$(VSToolsPath)\\\\WebApplications\\\\Microsoft.WebApplication.targets\\" Condition=\\"\'$(VSToolsPath)\' != \'\'\\" />\\n  \x3c!-- New addition end --\x3e\\n```\\n\\nCheck in your amended csproj and the unit tests to TFS / VSO. You should see the JavaScript unit tests being run as part of the build."},{"id":"typescript-jsdoc-and-intellisense","metadata":{"permalink":"/typescript-jsdoc-and-intellisense","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2014-05-05-typescript-jsdoc-and-intellisense/index.md","source":"@site/blog/2014-05-05-typescript-jsdoc-and-intellisense/index.md","title":"TypeScript, JSDoc and Intellisense","description":"Transforming API documentation into JSDoc for TypeScript: author explains how he enriched popular `jquery.d.ts` file with comments.","date":"2014-05-05T00:00:00.000Z","tags":[{"inline":false,"label":"jQuery","permalink":"/tags/jquery","description":"The jQuery library."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":14.355,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"typescript-jsdoc-and-intellisense","title":"TypeScript, JSDoc and Intellisense","authors":"johnnyreilly","tags":["jquery","typescript"],"hide_table_of_contents":false,"description":"Transforming API documentation into JSDoc for TypeScript: author explains how he enriched popular `jquery.d.ts` file with comments."},"unlisted":false,"prevItem":{"title":"Team Foundation Server, Continuous Integration and separate projects for JavaScript unit tests","permalink":"/team-foundation-server-continuous-integration-and-javascript-unit-tests-in-unit-test-project"},"nextItem":{"title":"TypeScript this is what I want! (the unfortunate neglect of Instance Methods / callback functions)","permalink":"/typescript-instance-methods"}},"content":"## Days of Yore\\n\\n\x3c!--truncate--\x3e\\n\\nIt was my first job. The web was alive and well at this point but still very much in it\'s infancy. Newspapers had only recently moved on from calling it \\"the information superhighway\\". No-one was doing _real_ programming for the web - the desktop was where it was at.\\n\\nAs for me, I was writing call centre software. It was all very exciting. Here was the idea: the phone on your desk would start ringing and through the magic of [TAPI](http://en.wikipedia.org/wiki/Telephony_Application_Programming_Interface) our app would be presented with the telephone number of the dialer. It would then look up that telephone number in the appropriate CRM application and pop the callers details on the screen. You\'d pick up the phone and bellow \\"why hello Mr Jones!\\" and either impress the caller with your incredible fore-knowledge of who had rung you or perhaps terrify them with our [Brave New Orwellian World](http://en.wikipedia.org/wiki/Nineteen_Eighty-Four).\\n\\nMy job was to work out how to call into the APIs of the various CRM applications / databases being used and extract the relevant information. So it goes without saying that I have spent a lot of time with badly documented APIs. Or in fact _undocumented_ APIs. I know pain my friend...\\n\\nHours and days were spent debugging and walking APIs just to find out what they could do and what information they exposed. This, I need hardly say, was dull and tedious work. Having spent longer than I care to remember with no more information on an API than method names has left its mark on me. I am consequently keener than your average dev on documentation and intellisense. When you\'ve stared at the coalface of the [Lotus Notes](http://en.wikipedia.org/wiki/IBM_Notes) API for 2 weeks with only Dephi 3 as your constant companion you\'d feel the same way too. (This was [before the days of Google](http://en.wikipedia.org/wiki/AltaVista) and actually being able to find stuff on the internet.)\\n\\nIf you can convey information about the API that you\'re building then I\'d say you\'re duty-bound to do so. Or at least that it\'s good manners.\\n\\n## Definitely Intellisensed\\n\\nWhen I started getting involved with the [Definitely Typed project](https://github.com/DefinitelyTyped) my focus was on giving good Intellisense. Where there was documentation for an API I wanted to get that popping in front of users when they hit the \\".\\" key:\\n\\n![screenshot of intellisense in visual studio](screenshot-jsdoc-in-visual-studio-intellisense.png)\\n\\nAs the above screenshot demonstrates [TypeScript supports Intellisense](https://devblogs.microsoft.com/typescript/announcing-typescript-0-8-2/) through a slightly tweaked implementation of [JSDoc](http://en.wikipedia.org/wiki/JSDoc):\\n\\n> With 0.8.2, the TypeScript compiler and tools now support JSDoc comments.\\n>\\n> In the TypeScript implementation, because types are already part of the system, we allow the JSDoc type annotation to be elided, as in the example above.\\n>\\n> You can now document a variety of language constructs (including classes, modules, interfaces, and functions) with comments that become part of the information displayed to the user. We\u2019ve also started extending lib.d.ts, the default JS and DOM API library, with JSDoc comments.\\n\\nPartly as an exercise in getting better acquainted with TypeScript and partly responding to my instinctive need to have nicely documented APIs I decided to start adding JSDoc comments to the world\'s most popular typings file [`jquery.d.ts`](https://github.com/borisyankov/DefinitelyTyped/blob/master/jquery/jquery.d.ts).\\n\\n## Why `jquery.d.ts`?\\n\\nWell a number of reasons:\\n\\n1. I used `jquery.d.ts` already myself and I\'m a firm believer in [eating your own dogfood](https://en.wikipedia.org/wiki/Eating_your_own_dog_food)\\n2. jQuery is well documented. I needed a source of information to power my JSDoc and <a href=\\"//api.jquery.com\\">api.jquery.com</a> had my back.\\n3. `jquery.d.ts` was widely used. Given how ubiquitous jQuery has become this typing file was unsurprisingly the most popular in the world. That was key for me as I wanted feedback - if I was making a mess of the typings I wanted someone to pitch in and tell me.\\n\\nJust to digress once more, points #2 and #3 turned out to be of particular note.\\n\\nConcerning point #2, I did find the occasional [error](https://github.com/borisyankov/DefinitelyTyped/pull/1471#issuecomment-31204115) or [inconsistency](https://github.com/borisyankov/DefinitelyTyped/pull/1835#issuecomment-37533088) in the jQuery API documentation. These were definitely the exception rather than the rule though. And thanks to the very helpful [Dave Methvin](https://github.com/dmethvin) these actually lead to [minor improvements to the jQuery API documentation](https://github.com/jquery/api.jquery.com/pull/460).\\n\\n![Tweet by @basarat at 8:47 PM on Dec 26, 2013 reading \\"#TypeScript definitions pointing out errors in JavaScript docs of a project #Jquery : https://github.com/borisyankov/DefinitelyTyped/pull/1471#issuecomment-31204115 caught by @johnny_reilly\\" original tweet here: https://twitter.com/basarat/status/416309213430689792](jquery-type-definition-tweet.webp)\\n\\nConcerning point #3 I did indeed get feedback. As well as enriching `jquery.d.ts` with JSDoc goodness I also found myself fixing slight errors in the typings. Here and there I would find examples where `jquery.d.ts` was out of line the with API documentation. Where this was the case I would amend the typings to bring them into line - trying to make `jquery.d.ts` entirely API-compliant. This was <a href=\\"https://github.com/borisyankov/DefinitelyTyped/issues/1499\\">not always popular</a>. But despite the heat it generated I think it ended up leading to a better typing file. I\'m again grateful for Dave Methvin\'s thoughtful contributions.\\n\\n## Turning API documentation into JSDoc\\n\\nI wanted to take an example of API documentation and demonstrate how that can be applied to a typing file with particular focus on how JSDoc comments can be created to drive Intellisense. So let\'s take everyone\'s favourite jQuery method: `val`. The documentation of `val` can be found here: [api.jquery.com/val](http://api.jquery.com/val)\\n\\nBy the way, check out the \\\\*_entirely_\\\\* intuitive URL. Now you\'ve clocked just how straightforward that is you\'ve probably a fair idea how you could find pretty much any jQuery documentation you might need without recourse to Google. Brilliant!\\n\\nLet\'s take a look at what `val` looked like [before JSDoc](https://github.com/borisyankov/DefinitelyTyped/blob/c98eebb13724b5156f12318b68fc2d875ca6e4a3/jquery/jquery.d.ts#L364-L368) in the first version of the typing available on GitHub. (By the way, remember the original `jquery.d.ts`[ came out of the TypeScript team](https://typescript.codeplex.com/sourcecontrol/latest#samples/jquery/jquery.d.ts)):\\n\\n```ts\\nval(): any;\\nval(value: string[]): JQuery;\\nval(value: string): JQuery;\\nval(value: number): JQuery;\\nval(func: (index: any, value: any) => any): JQuery;\\n```\\n\\nAnd now let\'s look at `jquery.d.ts`[after JSDoc](https://github.com/borisyankov/DefinitelyTyped/blob/c259dba094121a389b41c573d5000dda7bdf2092/jquery/jquery.d.ts#L1494-L1545):\\n\\n```ts\\n/**\\n * Get the current value of the first element in the set of matched elements.\\n */\\nval(): any;\\n/**\\n * Set the value of each element in the set of matched elements.\\n *\\n * @param value A string of text or an array of strings corresponding to the value of each matched element to set as selected/checked.\\n */\\nval(value: string): JQuery;\\n/**\\n * Set the value of each element in the set of matched elements.\\n *\\n * @param value A string of text or an array of strings corresponding to the value of each matched element to set as selected/checked.\\n */\\nval(value: string[]): JQuery;\\n/**\\n * Set the value of each element in the set of matched elements.\\n *\\n * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.\\n */\\nval(func: (index: number, value: string) => string): JQuery;\\n/**\\n * Set the value of each element in the set of matched elements.\\n *\\n * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.\\n */\\nval(func: (index: number, value: string[]) => string): JQuery;\\n/**\\n * Set the value of each element in the set of matched elements.\\n *\\n * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.\\n */\\nval(func: (index: number, value: number) => string): JQuery;\\n/**\\n * Set the value of each element in the set of matched elements.\\n *\\n * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.\\n */\\nval(func: (index: number, value: string) => string[]): JQuery;\\n/**\\n * Set the value of each element in the set of matched elements.\\n *\\n * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.\\n */\\nval(func: (index: number, value: string[]) => string[]): JQuery;\\n/**\\n * Set the value of each element in the set of matched elements.\\n *\\n * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.\\n */\\nval(func: (index: number, value: number) => string[]): JQuery;\\n```\\n\\nMany changes yes? Let\'s break it down a little.\\n\\n## 1. You have 20 seconds to comply (with the API)\\n\\nThe first thing to note is the `number` setter method:\\n\\n```ts\\nval(value: number): JQuery;\\n```\\n\\nLet\'s have a look at the jQuery documentation for the simple setter:\\n\\n> ## [`.val( value )`](http://api.jquery.com/val/#val-value)\\n>\\n> <div><strong>value</strong></div>\\n>\\n> <div>Type: <a href=\\"http://api.jquery.com/Types/#String\\">String</a> or <a href=\\"http://api.jquery.com/Types/#Array\\">Array</a></div>\\n>\\n> <div>A string of text or an array of strings corresponding to the value of each matched element to set as selected/checked.</div>\\n\\nSee the problem? There is \\\\*_no_\\\\* `number` setter. The typings are wrong. So let\'s remedy this:\\n\\n```ts\\n<strike>val(value: number): JQuery;</strike>\\n```\\n\\n## 2. `String` and `Array of String` setters\\n\\nThe documentation states that we have setters which accept `String` and `Array of String`. These are already modeled in the existing typings by the `string` and `string[]` overloads:\\n\\n```ts\\nval(value: string[]): JQuery;\\n    val(value: string): JQuery;\\n```\\n\\nSo let\'s enrich these typings with some JSDoc:\\n\\n```ts\\n/**\\n * Set the value of each element in the set of matched elements.\\n *\\n * @param value A string of text or an array of strings corresponding to the value of each matched element to set as selected/checked.\\n */\\nval(value: string): JQuery;\\n/**\\n * Set the value of each element in the set of matched elements.\\n *\\n * @param value A string of text or an array of strings corresponding to the value of each matched element to set as selected/checked.\\n */\\nval(value: string[]): JQuery;\\n```\\n\\nIf you look you can see we\'ve added a related JSDoc style comment block prior to each overload. The first part of the comment (_\\"Set the value of...\\"_) is the overarching Intellisense that is displayed. Each of the `@param` statements represents each of the parameters and it\'s associated comment. By comparing the [API documentation](http://api.jquery.com/val/#val-value) to the JSDoc it\'s pretty clear how the API has been transformed into useful JSDoc.\\n\\nIt\'s worth noting that I could have taken the choice to customise the `@param value` comments based on the overload I was JSDoc-ing. Arguably it would have been more useful to have something like this instead:\\n\\n```ts\\n/**\\n     * Set the value of each element in the set of matched elements.\\n     *\\n     * @param value A string of text <strike>or an array of strings</strike> corresponding to the value of each matched element to set as selected/checked.\\n     */\\n    val(value: string): JQuery;\\n    /**\\n     * Set the value of each element in the set of matched elements.\\n     *\\n     * @param value <strike>A string of text or</strike> an array of strings corresponding to the value of each matched element to set as selected/checked.\\n     */\\n    val(value: string[]): JQuery;\\n```\\n\\nAfter some pondering I decided not to take this approach, just to maintain that close relationship between `jquery.d.ts` and [api.jquery.com](http://api.jquery.com/). It\'s open to debate how useful that relationship actually is so I thought I\'d just highlight this as a choice I made.\\n\\n## 3. Getter\\n\\nThe jQuery documentation for the getter looks like this:\\n\\n> [`.val()`](http://api.jquery.com/val/#val)\\n>\\n> Returns: <a href=\\"http://api.jquery.com/Types/#String\\">String</a> or <a href=\\"http://api.jquery.com/Types/#Number\\">Number</a> or <a href=\\"http://api.jquery.com/Types/#Array\\">Array</a>\\n>\\n> **Description: **Get the current value of the first element in the set of matched elements.\\n\\nSo the `val()` overload can return a `string`, a `number` or a `string[]`. Unfortunately there is no real way to model that in TypeScript at present due to the absence of [\\"union types\\"](https://typescript.codeplex.com/workitem/1364). Union types are being [discussed at present](https://typescript.codeplex.com/discussions/543598#PostDetailsCell_1239340) but in TypeScript v1.0 world the only viable approach is returning the `any` type. This implies `val()` returns any possible JavaScript value from `boolean` to `Function` and straight on \'til morning. So clearly this isn\'t accurate but importantly it also allows for the possibility of `val()` returning `string`, `number` or `string[]`.\\n\\nThe final getter typing with JSDoc applied ends up looking like this:\\n\\n```ts\\n/**\\n     * Get the current value of the first element in the set of matched elements.\\n     */\\n    val(): any;\\n```\\n\\nAs you can see the _\\"Get the current value...\\"_ from the API docs has been used as the overarching Intellisense that is displayed for the getter.\\n\\n## 4. The `Function` setter\\n\\nFinally we\'re going to take a look at the `Function` setter which is documented as follows:\\n\\n> [`.val( function(index, value) )`](http://api.jquery.com/val/#val-functionindex--value)\\n>\\n> `function(index, value)`\\n>\\n> <div>Type: <a href=\\"http://api.jquery.com/Types/#Function\\">Function</a>()</div>\\n>\\n> <div>A function returning the value to set. <code>this</code> is the current element. Receives the index position of the element in the set and the old value as arguments.</div>\\n\\nIf you cast your eyes back to the original typings for the `Function` setter you\'ll see they look like this:\\n\\n```ts\\nval(func: (index: any, value: any) => any): JQuery;\\n```\\n\\nThis is a good start but it\'s less accurate than it could be in a number of ways:\\n\\n1. `index` is a `number` \\\\- we needn\'t keep it as an `any`\\n2. `value` is the old value - we know from our getter that this can be a `string`, `number` or `string[]`. So we can lose the `any` in favour of overloads which specify different types for `value` in each.\\n3. The return value of the function is the value that should be set. We know from our other setters that the possible types allowed here are `string` and `string[]`. (And yes I\'m as puzzled as you are that the getter can return a `number` but the setter can\'t set one.) That being the case it makes sense for us to have overloads with functions that return both `string` and `string[]`\\n\\nSo, we\'ve got a little tidy up to do for #1 and extra overloads to add for #2 and #3. We\'re going to replace the single `Function` setter with 3 overloads to cater for #2. Then for #3 we\'re going to take each of the 3 overloads we\'ve just created and make 2 overloads place of each to handle the different return types. This will lead us with the grand total of 6 overloads to model our `Function` setter!\\n\\n```ts\\n/**\\n     * Set the value of each element in the set of matched elements.\\n     *\\n     * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.\\n     */\\n    val(func: (index: number, value: string) => string): JQuery;\\n    /**\\n     * Set the value of each element in the set of matched elements.\\n     *\\n     * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.\\n     */\\n    val(func: (index: number, value: string[]) => string): JQuery;\\n    /**\\n     * Set the value of each element in the set of matched elements.\\n     *\\n     * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.\\n     */\\n    val(func: (index: number, value: number) => string): JQuery;\\n    /**\\n     * Set the value of each element in the set of matched elements.\\n     *\\n     * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.\\n     */\\n    val(func: (index: number, value: string) => string[]): JQuery;\\n    /**\\n     * Set the value of each element in the set of matched elements.\\n     *\\n     * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.\\n     */\\n    val(func: (index: number, value: string[]) => string[]): JQuery;\\n    /**\\n     * Set the value of each element in the set of matched elements.\\n     *\\n     * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.\\n     */\\n    val(func: (index: number, value: number) => string[]): JQuery;\\n```\\n\\nA cursory glance shows that each of the overloads above shares the same JSDoc. Each has the _\\"Set the value...\\"_ from the API docs as the overarching Intellisense that is displayed for the `Function` setter. And each has the same `@param func` comment as well.\\n\\n## It could be you...\\n\\nThis post is much longer than I ever intended it to be. But I wanted to show how easy it is to create typings with JSDoc to drive Intellisense. For no obvious reason people generally don\'t make a great deal of use of JSDoc when creating typings. Perhaps the creators have no good source of documentation (a common problem). Or perhaps people are not even aware it\'s a possibility - they don\'t know about the TypeScript support of JSDoc. In case it\'s the latter I think this post was worth writing."},{"id":"typescript-instance-methods","metadata":{"permalink":"/typescript-instance-methods","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2014-04-01-typescript-instance-methods/index.md","source":"@site/blog/2014-04-01-typescript-instance-methods/index.md","title":"TypeScript this is what I want! (the unfortunate neglect of Instance Methods / callback functions)","description":"TypeScripts \\"Instance Methods\\" feature solves the `this` keyword issues in classes, unlike prototype methods. It suggests using a combination of the two.","date":"2014-04-01T00:00:00.000Z","tags":[{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":4.97,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"typescript-instance-methods","title":"TypeScript this is what I want! (the unfortunate neglect of Instance Methods / callback functions)","authors":"johnnyreilly","tags":["typescript"],"hide_table_of_contents":false,"description":"TypeScripts \\"Instance Methods\\" feature solves the `this` keyword issues in classes, unlike prototype methods. It suggests using a combination of the two."},"unlisted":false,"prevItem":{"title":"TypeScript, JSDoc and Intellisense","permalink":"/typescript-jsdoc-and-intellisense"},"nextItem":{"title":"The Surprisingly Happy Tale of Visual Studio Online, Continous Integration and Chutzpah","permalink":"/the-surprisingly-happy-tale-of-visual"}},"content":"I was recently reading [Jeff Walker\'s blog post \\"Why TypeScript Isn\'t the Answer\\"](http://www.walkercoderanger.com/blog/2014/02/typescript-isnt-the-answer/). This is part of series in which Jeff goes through various compile-to-JavaScript technologies including TypeScript, CoffeeScript and Dart and explains his view of why he feels they don\'t quite hit the mark.\\n\\n\x3c!--truncate--\x3e\\n\\nAs a user (and big fan) of TypeScript I read the post with interest and picked up on one particular issue that Jeff mentions:\\n\\n> Classes make the unchanged behaviour of the `this` keyword more confusing. For example, in a class like `Greeter` from the [TypeScript playground](http://www.typescriptlang.org/Playground), the use of `this` is confusing:\\n>\\n> ```ts\\n> class Greeter {\\n>   greeting: string;\\n>   constructor(message: string) {\\n>     this.greeting = message;\\n>   }\\n>   greet() {\\n>     return \'Hello, \' + this.greeting;\\n>   }\\n> }\\n> ```\\n>\\n> One can\u2019t help but feel the `this` keyword in the methods of `Greeter` should always reference a `Greeter` instance. However, the semantics of this are unchanged from JavaScript:\\n>\\n> ```js\\n> var greeter = new Greeter(\'world\');\\n> var unbound = greeter.greet;\\n> alert(unbound());\\n> ```\\n>\\n> The above code displays \u201CHello, undefined\u201D instead of the naively expected \u201CHello, world\u201D.\\n\\nNow Jeff is quite correct in everything he says above. However, he\'s also missing a trick. Or rather, he\'s missing out on a very useful feature of TypeScript.\\n\\n## Instance Methods to the Rescue!\\n\\nStill in the early days of TypeScript, the issue Jeff raises had already been identified. (And for what it\'s worth, this issue wasn\'t there by mistake - remember TypeScript is quite deliberately a \\"superset of JavaScript\\".) Happily with the [release of TypeScript 0.9.1](https://blogs.msdn.com/b/typescript/archive/2013/08/06/announcing-0-9-1.aspx) a nice remedy was included in the language in the form of \\"Instance Methods\\".\\n\\nInstance Methods are lexically scoped; bound to a specific instance of a JavaScript object. i.e. These methods are \\\\***not**\\\\* vulnerable to the \u201CHello, undefined\u201D issue Jeff raises. To quote the blog post:\\n\\n> We\'ve relaxed the restrictions on field initializers to now allow `\'this\'`. This means that classes can now contain both methods on the prototype, and **callback functions on the instance**. The latter are particularly useful when you want to use a member on the class as a callback function, as in the code above. This lets you mix-n-match between \u2018closure\u2019 style and \u2018prototype\u2019 style class member patterns easily.\\n\\n## `Greeter` with Instance Methods\\n\\nSo, if we take the `Greeter` example, how do we apply Instance Methods to it? Well, like this:\\n\\n```ts\\nclass Greeter {\\n  greeting: string;\\n  constructor(message: string) {\\n    this.greeting = message;\\n  }\\n  greet = () => {\\n    return \'Hello, \' + this.greeting;\\n  };\\n}\\n```\\n\\nCan you tell the difference? It\'s subtle. That\'s right; the mere swapping out of `()` with `= () =&gt;` on the `greet` method takes us from a `prototype` method to an Instance Method.\\n\\nObservant readers will have noticed that we are using TypeScript / [ES6\'s Arrow Function syntax](https://developer.mozilla.org/en/docs/Web/JavaScript/Reference/arrow_functions). In fact with that in mind I could actually have gone super-terse if I was so inclined:\\n\\n```ts\\nclass Greeter {\\n  greeting: string;\\n  constructor(message: string) {\\n    this.greeting = message;\\n  }\\n  greet = () => \'Hello, \' + this.greeting;\\n}\\n```\\n\\nBut either way, both of the above class declarations compile down to the following JavaScript:\\n\\n```js\\nvar Greeter = (function () {\\n  function Greeter(message) {\\n    var _this = this;\\n    this.greet = function () {\\n      return \'Hello, \' + _this.greeting;\\n    };\\n    this.greeting = message;\\n  }\\n  return Greeter;\\n})();\\n```\\n\\nWhich differs from the pre-Instance Methods generated JavaScript:\\n\\n```js\\nvar Greeter = (function () {\\n  function Greeter(message) {\\n    this.greeting = message;\\n  }\\n  Greeter.prototype.greet = function () {\\n    return \'Hello, \' + this.greeting;\\n  };\\n  return Greeter;\\n})();\\n```\\n\\nAs you can see the Instance Methods approach does \\\\***not**\\\\* make use of the `prototype` on `Greeter` to add the method. (As the pre-Instance Methods `greet()` declaration did.) Instead it creates a function directly on the created object and internally uses the `_this` variable inside the Instance Methods. (`_this` being a previously captured instance of `this`.)\\n\\nSo with Instance Methods we can repeat Jeff\'s experiment from earlier:\\n\\n```js\\nvar greeter = new Greeter(\'world\');\\nvar bound = greeter.greet;\\nalert(bound());\\n```\\n\\nBut this time round the code displays \u201CHello, world\u201D and no longer \u201CHello, undefined\u201D.\\n\\n## Updated 02/04/2014 - mixing and matching `prototype` and Instance Methods\\n\\n[Bart Verkoeijen](https://twitter.com/bgever) made an excellent comment concerning the extra memory that Instance Methods require as opposed to `prototype` methods. Not everyone reads the comments and so I thought I\'d add a little suffix to my post.\\n\\nWhat I\u2019ve come to realise is that it comes down to problem that you\u2019re trying to solve. Instance methods are bulletproof in terms of relying on a specific instance of `this` regardless of how a method is invoked. But for many of my use cases that\u2019s overkill. Let\u2019s take the original (`prototype` methods) `Greeter` example:\\n\\n```js\\nvar Greeter = (function () {\\n  function Greeter(message) {\\n    this.greeting = message;\\n  }\\n  Greeter.prototype.greet = function () {\\n    return \'Hello, \' + this.greeting;\\n  };\\n  return Greeter;\\n})();\\n\\nvar greeter = new Greeter(\'world\');\\nvar greeter2 = new Greeter(\'universe\');\\n\\nconsole.log(greeter.greet()); // Logs \\"Hello, world\\"\\nconsole.log(greeter2.greet()); // Logs \\"Hello, universe\\"\\n```\\n\\nAs you can see above, provided I invoke my `greet` method in the context of my created object then I can rely on `this` being what I would hope.\\n\\nThat being the case my general practice has not been to use exclusively Instance methods \\\\***or**\\\\* `prototype` methods. What I tend to do is start out only with `prototype` methods on my classes and switch them over to be an Instance method if there is an actual need to ensure context. So my TypeScript classes tend to be a combination of `prototype` methods and Instance methods.\\n\\nMore often than not the `prototype` methods are just fine. It tends to be where an object is interacting with some kind of presentation framework (Knockout / Angular etc) or being invoked as part of a callback (eg AJAX scenarios) where I need Instance methods."},{"id":"the-surprisingly-happy-tale-of-visual","metadata":{"permalink":"/the-surprisingly-happy-tale-of-visual","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2014-03-17-the-surprisingly-happy-tale-of-visual/index.md","source":"@site/blog/2014-03-17-the-surprisingly-happy-tale-of-visual/index.md","title":"The Surprisingly Happy Tale of Visual Studio Online, Continous Integration and Chutzpah","description":"John recounts his experience with JavaScript unit testing using Jasmine and Chutzpah for integration with Visual Studio and Team Foundation Service.","date":"2014-03-17T00:00:00.000Z","tags":[{"inline":false,"label":"Automated Testing","permalink":"/tags/automated-testing","description":"How to perform the automation of tests."},{"inline":false,"label":"Azure DevOps","permalink":"/tags/azure-devops","description":"The Azure DevOps suite of tools."},{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."}],"readingTime":5.87,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"the-surprisingly-happy-tale-of-visual","title":"The Surprisingly Happy Tale of Visual Studio Online, Continous Integration and Chutzpah","authors":"johnnyreilly","tags":["automated testing","azure devops","javascript"],"hide_table_of_contents":false,"description":"John recounts his experience with JavaScript unit testing using Jasmine and Chutzpah for integration with Visual Studio and Team Foundation Service."},"unlisted":false,"prevItem":{"title":"TypeScript this is what I want! (the unfortunate neglect of Instance Methods / callback functions)","permalink":"/typescript-instance-methods"},"nextItem":{"title":"Knockout + Globalize = valueNumber Binding Handler","permalink":"/knockout-globalize-valuenumber-binding"}},"content":"## Going off piste\\n\\n\x3c!--truncate--\x3e\\n\\nThe post that follows is a slightly rambly affair which is pretty much my journal of the first steps of getting up and running with JavaScript unit testing. I will not claim that much of this blog is down to me. In fact in large part is me working my way through [Mathew Aniyan\'s excellent blog post on integrating Chutzpah with TFS](https://blogs.msdn.com/b/visualstudioalm/archive/2012/07/09/javascript-unit-tests-on-team-foundation-service-with-chutzpah.aspx). But a few deviations from this post have made me think it worth keeping hold of this record for my benefit (if no-one else\'s).\\n\\nThat\'s the disclaimers out of the way now...\\n\\n## ...Try, try, try again...\\n\\nGetting started with JavaScript unit testing has not been the breeze I\u2019d expected. Frankly I\u2019ve found the docs out there not particularly helpful. But if at first you don\'t succeed then try, try, try again.\\n\\nSo after a number of failed attempts I\u2019m going to give it another go. [Rushaine McBean](http://www.hanselminutes.com/412/getting-started-with-javascript-unit-testing-with-jasmine-and-rushaine-mcbean) says Jasmine is easiest so I\'m going to follow her lead and give it a go.\\n\\nLet\u2019s new up a new (empty) ASP.NET project. Yes, I know ASP.NET has nothing to do with JavaScript unit testing but my end goal is to be able to run JS unit tests in Visual Studio and as part of Continuous Integration. Further to that, I\'m anticipating a future where I have a solution that contains JavaScript unit tests and C# unit tests as well. It is indeed a bold and visionary Brave New World. We\'ll see how far we get.\\n\\nFirst up, download Jasmine from [GitHub](http://jasmine.github.io/) \\\\- I\'ll use [v2.0](https://github.com/pivotal/jasmine/blob/master/dist/jasmine-standalone-2.0.0.zip). Unzip it and fire up SpecRunner.html and whaddya know... It works!\\n\\nAs well it might. I\u2019d be worried if it didn\u2019t. So I\u2019ll move the contents of the release package into my empty project. Now let\u2019s see if we can get those tests running inside Visual Studio. I\u2019d heard of [Chutzpah](https://chutzpah.codeplex.com/) which describes itself thusly:\\n\\n> _\u201CChutzpah is an open source JavaScript test runner which enables you to run unit tests using QUnit, Jasmine, Mocha, CoffeeScript and TypeScript.\u201D _\\n\\nWhat I\u2019m after is the Chutzpah test adapter for Visual Studio 2012/2013 which can be found [here](http://visualstudiogallery.msdn.microsoft.com/f8741f04-bae4-4900-81c7-7c9bfb9ed1fe). I download the VSIX and install. Pretty painless. Once I restart Visual Studio I can see my unit tests in the test explorer. Nice! Run them and...\\n\\nAll fail. This makes me sad. All the errors say \u201CCan\u2019t find variable: Player in file\u201D. Hmmm. Why? Dammit I\u2019m actually going to have to read the [documentation](https://chutzpah.codeplex.com/wikipage?title=Chutzpah%20File%20References&referringTitle=Documentation)... It turns out the issue can be happily resolved by adding these 3 references to the top of PlayerSpec.js:\\n\\n```js\\n/// <reference path=\\"../src/Player.js\\" />\\n/// <reference path=\\"../src/Song.js\\" />\\n/// <reference path=\\"SpecHelper.js\\" />\\n```\\n\\nNow the tests pass.\\n\\nThe question is: can we get this working with Visual Studio Online?\\n\\nFortunately another has gone before me. Mathew Aniyan has written a [superb blog post called \\"Javascript Unit Tests on Team Foundation Service with Chutzpah\\"](https://blogs.msdn.com/b/visualstudioalm/archive/2012/07/09/javascript-unit-tests-on-team-foundation-service-with-chutzpah.aspx). Using this post as a guide (it was written 18 months ago which is frankly aeons in the world of the web) I\'m hoping that I\'ll be able to, without too many tweaks, get Javascript unit tests running on Team Foundation Service / Visual Studio Online ( / insert this weeks rebranding here).\\n\\nFirst of all in Visual Studio Online I\u2019ll create a new project called \\"GettingStartedWithJavaScriptUnitTesting\\" (using all the default options). Apparently _\u201CYour project is created and your team is going to absolutely love this.\u201D_ Hmmmm... I think I\u2019ll be judge of that.\\n\\nLet\'s navigate to the project. I\'ll fire up Visual Studio by clicking on the \u201COpen in Visual Studio\u201D link. Once fired up and all the workspace mapping is sorted I\u2019ll move my project into the GettingStartedWithJavaScriptUnitTesting folder that now exists on my machine and add this to source control.\\n\\nBack to Mathew\'s blog. It suggests renaming Chutzpah.VS2012.vsix to Chutzpah.VS2012.zip and checking certain files into TFS. I think Chutzpah has changed a certain amount since this was written. To be on the safe side I\u2019ll create a new folder in the root of my project called Chutzpah.VS2012 and put the contents of Chutzpah.VS2012.zip in there and add it to TFS (being careful to ensure that no dll\u2019s are excluded).\\n\\nThen I\'ll follow steps 3 and 4 from the blog post:\\n\\n> \\\\*3\\\\. In Visual Studio, Open Team Explorer & connect to Team Foundation Service. Bring up the Manage Build Controllers dialog. [Build \u2013> Manage Build Controllers] Select Hosted Build Controller Click on Properties button to bring up the Build Controller Properties dialog.\\n>\\n> 4\\\\. Change Version Control Path to custom Assemblies to refer to the folder where you checked in the binaries in step 2.\\n>\\n> -\\n\\nIn step 5 the blog tells me to edit my build definition. Well I don\u2019t have one in this new project so let\u2019s click on \u201CNew Build Definition\u201D, create one and then follow step 5:\\n\\n> \\\\*5\\\\. In Team Explorer, go to the Builds section and Edit your Build Definition which will run the javascript tests. Click on the Process tab Select the row named Automated Tests. Click on \u2026 button next to the value.\\n>\\n> -\\n\\nRather than following step 6 (which essentially nukes the running of any .NET tests you might have) I chose to add another row by clicking \\"Add\\". In the dialog presented I changed the Test assembly specification to \\\\*\\\\*\\\\\\\\\\\\*.js and checked the \\"Fail build on test failure\\" checkbox.\\n\\nStep 7 says:\\n\\n> \\\\*7\\\\. Create your Web application in Visual Studio and add your Qunit or Jasmine unit tests to them. <u>Make sure that the js files (that contain the tests) are getting copied to the build output directory.</u>\\n>\\n> -\\n\\nThe picture below step 7 suggests that you should be setting your test / spec files to have a `Copy to Output Directory` setting of `Copy always`. **This did not work for me!!!** Instead, setting a `Build Action` of `Content` and a `Copy to Output Directory` setting of `Do not copy` did work.\\n\\nFinally I checked everything into source control and queued a build. I honestly did not expect this to work. It couldn\u2019t be this easy could it? And...\\n\\nWow! It did! Here\u2019s me cynically expecting some kind of \u201Cpermission denied\u201D error and it actually works! Brilliant! Look up in the cloud it says the same thing!\\n\\nFantastic!\\n\\nI realise that I haven\u2019t yet written a single JavaScript unit test of my own and so I\u2019ve still a way to go. What I have done is quietened those voices in my head that said \u201Cthere\u2019s not too much point having a unit test suite that isn\u2019t plugged into continuous integration\u201D. Although it\'s not documented here I\'m happy to be able to report that I have been able to follow the self-same instructions for Team Foundation Service / Visual Studio Online and get CI working with TFS 2012 on our build server as well.\\n\\nHaving got up and running off the back of other peoples hard work I best try and write some of my own tests now...."},{"id":"knockout-globalize-valuenumber-binding","metadata":{"permalink":"/knockout-globalize-valuenumber-binding","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2014-03-11-knockout-globalize-valuenumber-binding/index.md","source":"@site/blog/2014-03-11-knockout-globalize-valuenumber-binding/index.md","title":"Knockout + Globalize = valueNumber Binding Handler","description":"Learn how to use Globalize and Knockout to create a \\"valueNumber\\" binding handler that makes numeric validation and localization easy.","date":"2014-03-11T00:00:00.000Z","tags":[{"inline":false,"label":"Globalize","permalink":"/tags/globalize","description":"The Globalize library."}],"readingTime":3.885,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"knockout-globalize-valuenumber-binding","title":"Knockout + Globalize = valueNumber Binding Handler","authors":"johnnyreilly","tags":["globalize"],"hide_table_of_contents":false,"description":"Learn how to use Globalize and Knockout to create a \\"valueNumber\\" binding handler that makes numeric validation and localization easy."},"unlisted":false,"prevItem":{"title":"The Surprisingly Happy Tale of Visual Studio Online, Continous Integration and Chutzpah","permalink":"/the-surprisingly-happy-tale-of-visual"},"nextItem":{"title":"Caching and cache-busting with RequireJS","permalink":"/caching-and-cache-busting-with-requirejs"}},"content":"I\u2019ve long used [Globalize](https://github.com/jquery/globalize/) for my JavaScript number formatting / parsing needs. In a current project I\u2019m using Knockout for the UI. When it came to data-binding numeric values none of the default binding handlers seemed appropriate. What I wanted was a binding handler that:\\n\\n\x3c!--truncate--\x3e\\n\\n1. Was specifically purposed for dealing with numeric values\\n2. Handled the parsing / formatting for the current locale (and I naturally intended to use Globalize for this purpose)\\n\\nLike so much development we start by standing on the shoulders of giants. In this case it\u2019s the fantastic [Ryan Niemeyer](https://twitter.com/RPNiemeyer) who put up a [post on StackOverflow](http://stackoverflow.com/a/12647270/761388) that got me on the right track.\\n\\nEssentially his approach provides an \u201Cinterceptor\u201D mechanism that allows you to validate numeric data entry on input and format numeric data going out as well. Very nice. Into this I plugged Globalize to handle the parsing and formatting. I ended up with the \u201CvalueNumber\u201D binding handler:\\n\\n```js\\nko.bindingHandlers.valueNumber = {\\n  init: function (\\n    element,\\n    valueAccessor,\\n    allBindingsAccessor,\\n    viewModel,\\n    bindingContext,\\n  ) {\\n    /**\\n     * Adapted from the KO hasfocus handleElementFocusChange function\\n     */\\n    function elementIsFocused() {\\n      var isFocused = false,\\n        ownerDoc = element.ownerDocument;\\n      if (\'activeElement\' in ownerDoc) {\\n        var active;\\n        try {\\n          active = ownerDoc.activeElement;\\n        } catch (e) {\\n          // IE9 throws if you access activeElement during page load\\n          active = ownerDoc.body;\\n        }\\n        isFocused = active === element;\\n      }\\n\\n      return isFocused;\\n    }\\n\\n    /**\\n     * Adapted from the KO hasfocus handleElementFocusChange function\\n     *\\n     * @param {boolean} isFocused whether the current element has focus\\n     */\\n    function handleElementFocusChange(isFocused) {\\n      elementHasFocus(isFocused);\\n    }\\n\\n    var observable = valueAccessor(),\\n      properties = allBindingsAccessor(),\\n      elementHasFocus = ko.observable(elementIsFocused()),\\n      handleElementFocusIn = handleElementFocusChange.bind(null, true),\\n      handleElementFocusOut = handleElementFocusChange.bind(null, false);\\n\\n    var interceptor = ko.computed({\\n      read: function () {\\n        var currentValue = ko.utils.unwrapObservable(observable);\\n        if (elementHasFocus()) {\\n          return !isNaN(currentValue) &&\\n            currentValue !== null &&\\n            currentValue !== undefined\\n            ? currentValue\\n                .toString()\\n                .replace(\'.\', Globalize.findClosestCulture().numberFormat[\'.\']) // Displays correct decimal separator for the current culture (so de-DE would format 1.234 as \\"1,234\\")\\n            : null;\\n        } else {\\n          var format = properties.numberFormat || \'n2\',\\n            formattedNumber = Globalize.format(currentValue, format);\\n\\n          return formattedNumber;\\n        }\\n      },\\n      write: function (newValue) {\\n        var currentValue = ko.utils.unwrapObservable(observable),\\n          numberValue = Globalize.parseFloat(newValue);\\n\\n        if (!isNaN(numberValue)) {\\n          if (numberValue !== currentValue) {\\n            // The value has changed so update the observable\\n            observable(numberValue);\\n          }\\n        } else if (newValue.length === 0) {\\n          if (properties.isNullable) {\\n            // If newValue is a blank string and the isNullable property has been set then nullify the observable\\n            observable(null);\\n          } else {\\n            // If newValue is a blank string and the isNullable property has not been set then set the observable to 0\\n            observable(0);\\n          }\\n        }\\n      },\\n    });\\n\\n    ko.utils.registerEventHandler(element, \'focus\', handleElementFocusIn);\\n    ko.utils.registerEventHandler(element, \'focusin\', handleElementFocusIn); // For IE\\n    ko.utils.registerEventHandler(element, \'blur\', handleElementFocusOut);\\n    ko.utils.registerEventHandler(element, \'focusout\', handleElementFocusOut); // For IE\\n\\n    if (element.tagName.toLowerCase() === \'input\') {\\n      ko.applyBindingsToNode(element, { value: interceptor });\\n    } else {\\n      ko.applyBindingsToNode(element, { text: interceptor });\\n    }\\n  },\\n};\\n```\\n\\nUsing this binding handler you just need to drop in a `valueNumber` into your `data-bind` statement where you might previously have used a `value` binding. The binding also has a couple of nice hooks in place which you might find useful:\\n\\n<dl><dt>numberFormat (defaults to \\"n2\\")</dt><dd>allows you to specify a format to display your number with. Eg, \\"c2\\" would display your number as a currency to 2 decimal places, \\"p1\\" would display your number as a percentage to 1 decimal place etc</dd><dt>isNullable (defaults to false)</dt><dd>specifies whether your number should be treated as nullable. If it\'s not then clearing the elements value will set the underlying observable to 0.</dd></dl>\\n\\nFinally when the element gains focus / becomes active the full underlying value is displayed. (Kind of like Excel - like many an app, the one I\'m working on started life as Excel and the users want to keep some of the nice aspects of Excel\'s UI.) To take a scenario, let\'s imagine we have an input element which is applying the \\"n1\\" format. The underlying value backing this is 1.234. The valueNumber binding displays this as \\"1.2\\" when the input does not have focus and when the element gains focus the full \\"1.234\\" is displayed. Credit where it\u2019s due, this is thanks to [Robert Westerlund](http://stackoverflow.com/users/1105996/robert-westerlund) who was kind enough to respond to a [question of mine on StackOverflow](http://stackoverflow.com/a/22313546/761388).\\n\\nFinally, here\u2019s a demo using the \\"de-DE\\" locale:\\n\\n<iframe width=\\"100%\\" height=\\"400\\" src=\\"https://jsfiddle.net/johnny_reilly/jRt3k/embedded/result,js,html\\" allowFullScreen=\\"allowFullScreen\\" frameBorder=\\"0\\"></iframe>\\n\\n## PS Globalize is a-changing\\n\\nThe version of Globalize used in the binding handler is Globalize v0.1.1. This has been available in various forms for quite some time but as I write this the Globalize plugin is in the process of being ported to the [CLDR](http://cldr.unicode.org/). As part of that work it looks like the Globalize API will change. When that gets finalized I\u2019ll try and come back and update this."},{"id":"caching-and-cache-busting-with-requirejs","metadata":{"permalink":"/caching-and-cache-busting-with-requirejs","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2014-03-05-caching-and-cache-busting-with-requirejs/index.md","source":"@site/blog/2014-03-05-caching-and-cache-busting-with-requirejs/index.md","title":"Caching and cache-busting with RequireJS","description":"Learn how to use \\"urlArgs\\" in RequireJS to manage caching and offer a reusable solution for both development and production environments.","date":"2014-03-05T00:00:00.000Z","tags":[{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":8.925,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"caching-and-cache-busting-with-requirejs","title":"Caching and cache-busting with RequireJS","authors":"johnnyreilly","tags":["asp.net"],"hide_table_of_contents":false,"description":"Learn how to use \\"urlArgs\\" in RequireJS to manage caching and offer a reusable solution for both development and production environments."},"unlisted":false,"prevItem":{"title":"Knockout + Globalize = valueNumber Binding Handler","permalink":"/knockout-globalize-valuenumber-binding"},"nextItem":{"title":"TypeScript and RequireJS (Keep It Simple)","permalink":"/typescript-and-requirejs-keep-it-simple"}},"content":"Having put together a demo of using TypeScript with RequireJS my attention turned quickly to caching. Or rather, IE forced me to think about caching.\\n\\n\x3c!--truncate--\x3e\\n\\nEveryone has their own workflow, their own tools. The things they like to use as they put things together. And for me I\u2019m a Visual Studio man \u2013 it\u2019s not everyone\u2019s bag but I really like it. I find the JavaScript tooling is now really solid combined with IE and it (generally) makes me more productive. I want to use it. But, as you know, nothing is perfect...\\n\\nSo there I was, delighted with the TypeScript / RequireJS demo. It was working just lovely. I started investigating the debugging story. What would happen if I change a script file on the fly? When I refresh IE does it pick up the tweaks?\\n\\nLet\u2019s find out. I\'ll open up alerter.ts and change this:\\n\\n```ts\\nvar name = \'John\';\\n```\\n\\nto this:\\n\\n```js\\nvar name = \'Bobby\';\\n```\\n\\nAnd \\\\***boom**\\\\*! Nothing. I\u2019ve refreshed IE and I\u2019m expecting to see \u201CWelcome to Code Camp, Bobby\u201D. But I\u2019m still reading \u201CWelcome to Code Camp, John\u201D... I bet Chrome wouldn\u2019t do this to me... And I\u2019m right! It doesn\u2019t. I don\u2019t want to get too much into the details of this but it looks like it comes down to Chrome sending an \\"If-Modified-Since\\" request header where IE does not. I\u2019m pretty sure that IE could be configured to behave likewise but I\u2019d rather not have to remember that. (And furthermore I don\u2019t want to have to remind every person that works on the app to do that as well.)\\n\\nThis raises a number of issues but essentially it gets me to think about the sort of caching I want. Like most of you I have 2 significant use cases:\\n\\n1. Development\\n2. Production\\n\\nFor Development I want any changes to JavaScript files to be picked up \u2013 I do \\\\***not**\\\\* want caching. For Production I want caching in order that users have better performance / faster loading. If I ship a new version of the app to Production I also want users to pick up the new versions of a file and cache those.\\n\\n## Research\\n\\nI did a little digging. The most useful information I found was [a StackOverflow post on RequireJS and caching](http://stackoverflow.com/q/8315088/761388). Actually I\u2019d recommend anyone reading this to head over and read that from top to bottom. Read the question and all of the answers as well \u2013 pretty much everything will add to your understanding of RequireJS.\\n\\nAs with any set of answers there are different and conflicting views. [Phil McCull\u2019s (accepted) answer](http://stackoverflow.com/a/8479953/761388) was for my money the most useful. It pointed [back to the RequireJS documentation](http://requirejs.org/docs/api.html#config-urlArgs).\\n\\n> \\\\*\\"urlArgs: Extra query string arguments appended to URLs that RequireJS uses to fetch resources. Most useful to cache bust when the browser or server is not configured correctly. Example cache bust setting for urlArgs:\\n>\\n> ```js\\n> urlArgs: \'bust=\' + new Date().getTime();\\n> ```\\n>\\n> During development it can be useful to use this, however be sure to remove it before deploying your code.\\"\\n>\\n> -\\n\\nPhil\u2019s answer suggests using urlArgs \\\\***both**\\\\* for Production and for Development in 2 different ways. Using what amounts to a random number in the Development environment (as in the official docs) for cache-busting. For the Production environment he suggests using a specific version number which allows for client-side caching between different build versions.\\n\\nFull disclosure, this is not the approach favoured by James Burke (author of RequireJS). He doesn\u2019t go into why in the RequireJS docs but has [elsewhere expounded on this](https://groups.google.com/forum/#!msg/requirejs/3E9dP_BSQoY/36ut2Gtko7cJ):\\n\\n> _For deployed assets, I prefer to put the version or hash for the whole build as a build directory, then just modify the baseUrl config used for the project to use that versioned directory as the baseUrl. Then no other files change, and it helps avoid some proxy issues where they may not cache an URL with a query string on it. _\\n\\nI\u2019m not so worried about the proxy caching issue. My users tend to be people who use the application repeatedly and so the caching I most care about is their local machine caching. From what I understand urlArgs will work fine in this scenario. Yes the downside of this approach is that some proxy servers may not cache these assets. That\u2019s a shame but it\u2019s not a dealbreaker for me. As I said, I still have client side caching.\\n\\nIf you want to go a little deeper I recommend reading [Steve Souders post](http://www.stevesouders.com/blog/2008/08/23/revving-filenames-dont-use-querystring/) on the topic (in case you\u2019re not aware Steve is Google\u2019s Mr Performance). Interestingly, looking at the comments on the post it sounds like the lack of support for proxy caching with querystrings may that may be starting to change.\\n\\nBut either way, I\u2019m happy with this approach. As I always say, if it\u2019s good enough for Stack Overflow then it\u2019s good enough for me.\\n\\n## Implementation\\n\\nI\u2019m going to start off using the demo from [my last blog post](../2014-02-27-typescript-and-requirejs-keep-it-simple/index.md) as a basis. Let\u2019s take that and evolve it. As a result my solution is going to work with TypeScript and RequireJS (since the previous demo was about that) but the implementation I\u2019m going to come up with would work as well with vanilla JS as it would with TypeScript compiled JS.\\n\\nLet\u2019s take a look at our index.html. First we\u2019ll drop our usage of `main.ts` / `main.js` (our bootstrapper file that defines config and kicks off the \\"app\\"). We\u2019ll pull out the use of `data-main` and instead, just after the reference to require we\u2019ll add the contents of `main.js` much in [the style of the RequireJS docs](http://requirejs.org/docs/api.html#config). We\u2019ll also include a urlArgs that as a cache-buster that uses the approach outlined [in the RequireJS docs](http://requirejs.org/docs/api.html#config-urlArgs):\\n\\n```html\\n<script src=\\"/scripts/require.js\\"><\/script>\\n<script>\\n  require.config({\\n    baseUrl: \'/scripts\',\\n    paths: {\\n      jquery: \'jquery-2.1.0\',\\n    },\\n    urlArgs: \'v=\' + new Date().getTime(),\\n  });\\n\\n  require([\'alerter\'], function (alerter) {\\n    alerter.showMessage();\\n  });\\n<\/script>\\n```\\n\\nSpinning up the site all runs as you would expect. The question is: does this work as a cache-buster? Let\u2019s tweak `alerter.ts` / `alerter.js`.\\n\\nOh yeah! We\u2019re cache-busting like gangbusters!\\n\\nSo now let\u2019s comment out our existing urlArgs (which represents the Development solution from Phil\u2019s answer) and replace it with a fixed value like this:\\n\\n```js\\n//urlArgs: \\"v=\\" +  (new Date()).getTime()\\nurlArgs: \'v=1\';\\n```\\n\\nThis represents the Production solution from Phil\u2019s answer. Now let\u2019s run, refresh a couple of times and ensure that our fixed querystring value results in a 304 status code (indicating \u201CNot Modified\u201D and cached item used).\\n\\nIt does! Now let\u2019s increment the value:\\n\\n```js\\nurlArgs: \'v=2\';\\n```\\n\\nWhen we refresh the browser this should result in 200 status codes (indicating the cached version has not been used and the client has picked up a new version from the server).\\n\\nSuccess! That\u2019s our premise tested \u2013 both Development and Production scenarios. Now we want to turn this into a slightly more sophisticated reusable solution like this:\\n\\n```html\\n<script src=\\"/scripts/require.js\\"><\/script>\\n<script>\\n  var inDevelopment = true,\\n    version = \'1\';\\n\\n  require.config({\\n    baseUrl: \'/scripts\',\\n    paths: {\\n      jquery: \'jquery-2.1.0\',\\n    },\\n    urlArgs: \'v=\' + (inDevelopment ? new Date().getTime() : version),\\n  });\\n\\n  require([\'alerter\'], function (alerter) {\\n    alerter.showMessage();\\n  });\\n<\/script>\\n```\\n\\nIn the tweaked script above 2 variables are defined. The first is `inDevelopment` which models whether you are in the Development scenario (true) or the Production scenario (false). The second is `version` which represents the application version number. With this in place I can simply flip between the Development and Production scenario by changing the value of `inDevelopment`. And when a new version ships I can change the version number to force a cache refresh on the users.\\n\\nWhat drives the values of `inDevelopment` / `version` is down to you. You could load the `inDevelopment` / `version` values from some application endpoint. You could hardcode them in your screen. The choices are yours. I\u2019m going to finish off with a simple approach that I\'ve found useful.\\n\\n## Let\u2019s get the server involved!\\n\\nI want the server to drive my urlArgs value. Why? Well this project happens to be an ASP.NET project which handily has the concept of Development / Production scenarios nicely modelled by the [web.config\u2019s compilation debug flag](<http://msdn.microsoft.com/en-us/library/s10awwz0(v=vs.85).aspx>).\\n\\n```xml\\n<configuration>\\n  <system.web>\\n    <compilation debug=\\"true\\" targetFramework=\\"4.5\\" />\\n    <httpRuntime targetFramework=\\"4.5\\" />\\n  </system.web>\\n</configuration>\\n```\\n\\nIf debug is `true` then that reflects the Development scenario. If debug is `false` then that reflects the Production scenario.\\n\\nSo bearing that in mind I want to use the value of debug to drive my `urlArgs`. If I have my debug flag set to `true` I want to cache-bust all the way. Likewise, if debug is set to `false` then I want to serve up the version number so that caching is used until the version number changes. Time to break out the C#:\\n\\n```cs\\nnamespace RequireJSandCaching\\n{\\n    public static class RequireJSHelpers\\n    {\\n        private static readonly bool _inDebug;\\n        private static readonly string _version;\\n\\n        static RequireJSHelpers()\\n        {\\n            _inDebug = System.Web.HttpContext.Current.IsDebuggingEnabled;\\n            _version = (_inDebug)\\n                ? \\"InDebug\\"\\n                : System.Reflection.Assembly.GetExecutingAssembly().GetName().Version.ToString();\\n        }\\n\\n        public static string Version\\n        {\\n            get\\n            {\\n                return (_inDebug)\\n                    ? System.DateTime.Now.Ticks.ToString()\\n                    : _version;\\n            }\\n        }\\n    }\\n}\\n```\\n\\nThis is a static helper class called `RequireJSHelpers`. It has a static constructor which initialises 2 fields. `_inDebug` is taken from `System.Web.HttpContext.Current.IsDebuggingEnabled` which exposes the compilation debug value. `_version` is initialised, when debug is `false`, to the version number of the dll (driven by this `AssemblyInfo.cs [assembly: AssemblyVersion(\\"1.0.*\\")]` attribute)\\n\\nThere\u2019s 1 property on this helper class called version. Depending on whether the app is in debug mode or not this attribute either exposes the application version or effectively the C# equivalent to JavaScript\u2019s `(new Date()).getTime()`. (Well strictly speaking they have a different starting point in history but that\u2019s by-the-by... Both are of equal value as cache-busters.)\\n\\nYou probably see where this is all going.\\n\\nLet\u2019s clone our `index.html` page and call it `serverUrlArgs.cshtml` (note the suffix). Let\u2019s replace the script section with this:\\n\\n```html\\n<script>\\n  require.config({\\n    baseUrl: \'/scripts\',\\n    paths: {\\n      jquery: \'jquery-2.1.0\',\\n    },\\n    urlArgs: \'v=@RequireJSandCaching.RequireJSHelpers.Version\',\\n  });\\n\\n  require([\'alerter\'], function (alerter) {\\n    alerter.showMessage();\\n  });\\n<\/script>\\n```\\n\\nWhich drives `urlArgs` from the `RequireJSHelpers.Version` property. If we fire it up now (with debug set to true in our web.config) then we see requests like this:\\n\\nAnd if we set debug to false in our web.config then (after the initial requests have been cached) we see requests like this:\\n\\nThis leaves us with a simple mechanism to drive our RequireJS caching. If debug is set to `true` in our `web.config` then Require will perform cache-busting. If debug is set to `false` then RequireJS will perform only version-changing cache-busting and will, whilst the version remains constant, support client-side caching.\\n\\nFinished. In case it helps I\u2019ve put the code for this [up on GitHub](https://github.com/johnnyreilly/RequireJSandCaching)."},{"id":"typescript-and-requirejs-keep-it-simple","metadata":{"permalink":"/typescript-and-requirejs-keep-it-simple","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2014-02-27-typescript-and-requirejs-keep-it-simple/index.md","source":"@site/blog/2014-02-27-typescript-and-requirejs-keep-it-simple/index.md","title":"TypeScript and RequireJS (Keep It Simple)","description":"This article explains how to mix TypeScript and RequireJS, gives examples of the code changes needed, and shows how to create a demo.","date":"2014-02-27T00:00:00.000Z","tags":[{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":4.06,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"typescript-and-requirejs-keep-it-simple","title":"TypeScript and RequireJS (Keep It Simple)","authors":"johnnyreilly","tags":["typescript"],"hide_table_of_contents":false,"description":"This article explains how to mix TypeScript and RequireJS, gives examples of the code changes needed, and shows how to create a demo."},"unlisted":false,"prevItem":{"title":"Caching and cache-busting with RequireJS","permalink":"/caching-and-cache-busting-with-requirejs"},"nextItem":{"title":"WPF and Mystic Meg or Playing Futurologist","permalink":"/wpf-and-mystic-meg-or-playing"}},"content":"I\'m not the first to take a look at mixing TypeScript and RequireJS but I wanted to get it clear in my head. Also, I\'ve always felt the best way to learn is to do. So here we go. I\'m going to create a TypeScript and RequireJS demo based on [John Papa\'s \\"Keep It Simple RequireJS Demo\\"](https://github.com/johnpapa/kis-requirejs-demo/).\\n\\n\x3c!--truncate--\x3e\\n\\nSo let\'s fire up Visual Studio 2013 and create a new ASP.NET Web Application called \u201CRequireJSandTypeScript\u201D (the empty project template is fine).\\n\\nAdd a new HTML file to the root called \u201Cindex.html\u201D and base it on \u201Cindex3.html\u201D from [John Papa\u2019s demo](https://github.com/johnpapa/kis-requirejs-demo/blob/master/ModularDemo/index3.html):\\n\\n```html\\n<!doctype html>\\n<html>\\n  <head>\\n    <title>TypeScript with RequireJS</title>\\n  </head>\\n  <body>\\n    <div>\\n      <h1>TypeScript with RequireJS loading jQuery in Visual Studio land</h1>\\n    </div>\\n\\n    \x3c!-- use jquery to load this message--\x3e\\n    <p id=\\"message\\"></p>\\n\\n    \x3c!-- Shortcut to load require and then load main--\x3e\\n    <script\\n      src=\\"/scripts/require.js\\"\\n      data-main=\\"/scripts/main\\"\\n      type=\\"text/javascript\\"\\n    ><\/script>\\n  </body>\\n</html>\\n```\\n\\nJohn\u2019s demo depends on jQuery and RequireJS (not too surprisingly) so let\u2019s fire up Nuget and get them:\\n\\n```ps\\nInstall-Package RequireJS\\nInstall-Package jQuery\\n```\\n\\nWhilst we\u2019re at it, let\u2019s get the Definitely Typed typings as well:\\n\\n```ps\\nInstall-Package jQuery.TypeScript.DefinitelyTyped\\n```\\n\\nTo my surprise this popped up a dialog.\\n\\nBy \\"Your project has been configured to support TypeScript.\\" it means that the csproj file has had the following entries added:\\n\\n```xml\\n<Project ToolsVersion=\\"12.0\\" DefaultTargets=\\"Build\\" xmlns=\\"http://schemas.microsoft.com/developer/msbuild/2003\\">\\n  <Import Project=\\"$(MSBuildExtensionsPath32)\\\\Microsoft\\\\VisualStudio\\\\v$(VisualStudioVersion)\\\\TypeScript\\\\Microsoft.TypeScript.Default.props\\" Condition=\\"Exists(\'$(MSBuildExtensionsPath32)\\\\Microsoft\\\\VisualStudio\\\\v$(VisualStudioVersion)\\\\TypeScript\\\\Microsoft.TypeScript.Default.props\')\\" />\\n  ...\\n  <PropertyGroup>\\n    ...\\n    <TypeScriptToolsVersion>0.9</TypeScriptToolsVersion>\\n  </PropertyGroup>\\n  ...\\n  <Import Project=\\"$(MSBuildExtensionsPath32)\\\\Microsoft\\\\VisualStudio\\\\v$(VisualStudioVersion)\\\\TypeScript\\\\Microsoft.TypeScript.targets\\" Condition=\\"Exists(\'$(MSBuildExtensionsPath32)\\\\Microsoft\\\\VisualStudio\\\\v$(VisualStudioVersion)\\\\TypeScript\\\\Microsoft.TypeScript.targets\')\\" />\\n  ...\\n</Project>\\n```\\n\\nI\u2019m not sure when this tweak to the Visual Studio tooling was added was added. Perhaps it\'s part of the [TypeScript 1.0 RC release](https://blogs.msdn.com/b/typescript/archive/2014/02/25/announcing-typescript-1-0rc.aspx); either way it\u2019s pretty nice. Let\'s press on.\\n\\nWhilst we\u2019re at it let\u2019s make sure that we\u2019re compiling to AMD (to be RequireJS friendly) by adding in the following csproj tweaks just before the Microsoft.TypeScript.targets Project import statement:\\n\\n```xml\\n<PropertyGroup Condition=\\"\'$(Configuration)\' == \'Debug\'\\">\\n    <TypeScriptModuleKind>amd</TypeScriptModuleKind>\\n  </PropertyGroup>\\n  <PropertyGroup Condition=\\"\'$(Configuration)\' == \'Release\'\\">\\n    <TypeScriptModuleKind>amd</TypeScriptModuleKind>\\n  </PropertyGroup>\\n```\\n\\nWhere was I? Oh yes, typings. So let\u2019s get the RequireJS typings too:\\n\\n```ps\\nInstall-Package requirejs.TypeScript.DefinitelyTyped\\n```\\n\\nRight \u2013 looking at index.html we can see from the data-main tag that the first file loaded by RequireJS, our bootstrapper if you will, is main.js. So let\u2019s add ourselves a main.ts based on [John\'s example](https://github.com/johnpapa/kis-requirejs-demo/blob/master/ModularDemo/Scripts3/main.js) (which will in turn generate a main.js):\\n\\n```ts\\n(function () {\\n  requirejs.config({\\n    baseUrl: \'scripts\',\\n    paths: {\\n      jquery: \'jquery-2.1.0\',\\n    },\\n  });\\n\\n  require([\'alerter\'], (alerter) => {\\n    alerter.showMessage();\\n  });\\n})();\\n```\\n\\nmain.ts depends upon [alerter](https://github.com/johnpapa/kis-requirejs-demo/blob/master/ModularDemo/Scripts3/alerter.js) so let\u2019s add ourselves an alerter.ts as well:\\n\\n```ts\\ndefine(\'alerter\', [\'jquery\', \'dataservice\'], function ($, dataservice) {\\n  var name = \'John\',\\n    showMessage = function () {\\n      var msg = dataservice.getMessage();\\n\\n      $(\'#message\').text(msg + \', \' + name);\\n    };\\n\\n  return {\\n    showMessage: showMessage,\\n  };\\n});\\n```\\n\\nAnd a [dataservice.ts](https://github.com/johnpapa/kis-requirejs-demo/blob/master/ModularDemo/Scripts3/dataservice.js):\\n\\n```ts\\ndefine(\'dataservice\', [], function () {\\n  var msg = \'Welcome to Code Camp\',\\n    getMessage = function () {\\n      return msg;\\n    };\\n\\n  return {\\n    getMessage: getMessage,\\n  };\\n});\\n```\\n\\nThat all compiles fine. But we\u2019re missing a trick. We\u2019re supposed to be using TypeScripts AMD support so let\u2019s change the code to do just that. First dataservice.ts:\\n\\n```ts\\nvar msg = \'Welcome to Code Camp\';\\n\\nexport function getMessage() {\\n  return msg;\\n}\\n```\\n\\nThen alerter.ts:\\n\\n```ts\\nimport $ = require(\'jquery\');\\nimport dataservice = require(\'dataservice\');\\n\\nvar name = \'John\';\\n\\nexport function showMessage() {\\n  var msg = dataservice.getMessage();\\n\\n  $(\'#message\').text(msg + \', \' + name);\\n}\\n```\\n\\nI know both of the above look slightly different but if you look close you\'ll see it\'s really only boilerplate changes. The actual application code is unaffected. Finally, main.ts remains as it is and that\'s us done; we have ourselves a working demo... Yay!\\n\\nThanks to John Papa for creating such a simple demo I could use as the basis for my own demo.\\n\\n## Closing Thoughts\\n\\nUnfortunately there is no typing on the alerter reference within main.ts. To my knowledge there is no way to implicitly import the typings here \u2013 the only thing you can do is specify them manually. (By the way, if I\'m wrong about this then please do set me straight!) That said, this is not so bad really since this main.ts file is essentially just a bootstrapper that kicks things off. All the other files contain the real application code and they have have typings a-go-go. So we\'re happy.\\n\\n## Finally for bonus points....\\n\\nI\u2019ve included the js and js.map files in the project file as they don\'t seem to be added into the project by Visual Studio when the TS file is created or when it is compiled for the first time. I\'ve also ensured that these files are dependent upon the typescript files they were generated from.\\n\\n```xml\\n<TypeScriptCompile Include=\\"Scripts\\\\alerter.ts\\" />\\n    <Content Include=\\"Scripts\\\\alerter.js\\">\\n        <DependentUpon>alerter.ts</DependentUpon>\\n    </Content>\\n    <Content Include=\\"Scripts\\\\alerter.js.map\\">\\n        <DependentUpon>alerter.ts</DependentUpon>\\n    </Content>\\n    <TypeScriptCompile Include=\\"Scripts\\\\dataservice.ts\\" />\\n    <Content Include=\\"Scripts\\\\dataservice.js\\">\\n        <DependentUpon>dataservice.ts</DependentUpon>\\n    </Content>\\n    <Content Include=\\"Scripts\\\\dataservice.js.map\\">\\n        <DependentUpon>dataservice.ts</DependentUpon>\\n    </Content>\\n    <TypeScriptCompile Include=\\"Scripts\\\\main.ts\\" />\\n    <Content Include=\\"Scripts\\\\main.js\\">\\n        <DependentUpon>main.ts</DependentUpon>\\n    </Content>\\n    <Content Include=\\"Scripts\\\\main.js.map\\">\\n        <DependentUpon>main.ts</DependentUpon>\\n    </Content>\\n```\\n\\n## Want the code for your very own?\\n\\nWell you can grab it from [GitHub](https://github.com/johnnyreilly/RequireJSandTypeScript)."},{"id":"wpf-and-mystic-meg-or-playing","metadata":{"permalink":"/wpf-and-mystic-meg-or-playing","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2014-02-12-wpf-and-mystic-meg-or-playing/index.md","source":"@site/blog/2014-02-12-wpf-and-mystic-meg-or-playing/index.md","title":"WPF and Mystic Meg or Playing Futurologist","description":"Native client apps will eventually be replaced by rich web apps/SPAs. WPF will become more niche, but wont die, predicts John.","date":"2014-02-12T00:00:00.000Z","tags":[],"readingTime":2.92,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"wpf-and-mystic-meg-or-playing","title":"WPF and Mystic Meg or Playing Futurologist","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false,"description":"Native client apps will eventually be replaced by rich web apps/SPAs. WPF will become more niche, but wont die, predicts John."},"unlisted":false,"prevItem":{"title":"TypeScript and RequireJS (Keep It Simple)","permalink":"/typescript-and-requirejs-keep-it-simple"},"nextItem":{"title":"Integration Testing with Entity Framework and Snapshot Backups","permalink":"/integration-testing-with-entity"}},"content":"Time for an unusual post. Most of what gets put down here is technical \\"how-to\'s\\". It\'s usually prompted by something I\'ve been working on and serves, as much as anything else, as an aide-memoire. Not this time.\\n\\n\x3c!--truncate--\x3e\\n\\nI\u2019ve been watching the changes in the world of development of the last couple of years and I\u2019ve come to a controversial conclusion... So I wanted to write about it. Hopefully I\'ll be able to return to this in 5 years and say \\"wow - I\'m so insightful - almost visionary really\\". Or not. Either way, let\'s put it out there - it\'s sink or swim time. Ready for it? Here\u2019s my bet: WPF will die.\\n\\nSounds dramatic right? OK - I\'ve overstated my case just to get you to read on (I should work for the tabloids). Let me flesh this out a little. First of all, I think WPF is a fine technology - great apps are built with it. My personal favourite being the fantastic [GitHub for Windows](https://github.com/blog/1151-designing-github-for-windows). And actually I don\'t think WPF will die at all. What I think will happen is that it will become a more niche way to build applications.\\n\\nMore broadly, I think that native client apps (be they Windows, Mac, iOS, Android etc) will eventually come to be replaced by [rich web apps / SPAs](http://en.wikipedia.org/wiki/Single-page_application) of the Angular / Ember / Durandal ilk. I realise that at the moment that seems like a ludicrous statement \u2013 native apps are heavily used throughout enterprises worldwide and certainly will continue to be used and actively developed for at least the next 5 years.\\n\\nBut as the web comes to [perform like native](http://arstechnica.com/information-technology/2013/05/native-level-performance-on-the-web-a-brief-examination-of-asm-js/), as [JavaScript become a compile target](https://github.com/jashkenas/coffee-script/wiki/List-of-languages-that-compile-to-JS), as [HTML 5 provides rich UI](http://davidwalsh.name/canvas-demos) and as [interactive communication becomes possible](https://developer.mozilla.org/en/docs/WebSockets) I reckon this is a fairly probable scenario. Particularly when you consider the [API work Firefox is doing around Firefox OS](https://developer.mozilla.org/en-US/Apps/Reference). I could be wrong, but my expectation is that the day will come when people will have apps that they can access from anywhere, on any platform and those apps can be deployed without infrastructure having to push out new versions to each client machine.\\n\\nThe web undeniably has issues but I think it will likely win out. And the cost case for a single client app is pretty compelling to anyone funding a system.\\n\\nAs a dev I\u2019m always working with an ever-evolving grab bag of technology whether it be front end, middle tier, database or services. In fact that will likely always be the case (change being the only constant in the world of software). But on the basis of my expectations I\u2019m planning to always keep at least a toe in the world of web apps as a form of \u201Ccareer future-proofing\u201D.\\n\\nGoing less broad again when I look at the Microsoft stack, I think XAML will live on for some time. Obviously Silverlight is no longer being actively developed but MS are using it in Windows 8 (Phone and WinJS) as well as WPF. But I do kind of wonder if it will become like a bit like VB.Net, still around, still in use, but slowly dropping off in terms of popularity. Particularly as you can write WinJS apps in HTML / CSS / JavaScript.\\n\\nAs I say, I could very much be wrong about all of this. I don\u2019t know what your view of the future of the development landscape is? You may have a different insight? I\u2019d be intrigued to know!"},{"id":"integration-testing-with-entity","metadata":{"permalink":"/integration-testing-with-entity","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2014-01-24-integration-testing-with-entity/index.md","source":"@site/blog/2014-01-24-integration-testing-with-entity/index.md","title":"Integration Testing with Entity Framework and Snapshot Backups","description":"The article shows how to use SQL Servers snapshot backups for creating effective integration tests that dont affect production data.","date":"2014-01-24T00:00:00.000Z","tags":[{"inline":false,"label":"SQL Server","permalink":"/tags/sql-server","description":"The SQL Server database."},{"inline":false,"label":"Automated Testing","permalink":"/tags/automated-testing","description":"How to perform the automation of tests."}],"readingTime":14.39,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"integration-testing-with-entity","title":"Integration Testing with Entity Framework and Snapshot Backups","authors":"johnnyreilly","tags":["sql server","automated testing"],"hide_table_of_contents":false,"description":"The article shows how to use SQL Servers snapshot backups for creating effective integration tests that dont affect production data."},"unlisted":false,"prevItem":{"title":"WPF and Mystic Meg or Playing Futurologist","permalink":"/wpf-and-mystic-meg-or-playing"},"nextItem":{"title":"Upgrading to TypeScript 0.9.5 - A Personal Memoir","permalink":"/upgrading-to-typescript-095-personal"}},"content":"I\'ve written before about how unit testing [Entity Framework is a contentious and sometimes pointless activity](../2012-10-03-unit-testing-and-entity-framework-filth/index.md). The TL;DR is that LINQ-to-Objects != Linq-to-Entities and so if you want some useful tests around your data tier then integration tests that actually hit a database are what you want.\\n\\n\x3c!--truncate--\x3e\\n\\nHowever hitting an actual database is has serious implications. For a start you need a database server and you need a database. But the real issue lies around cleanup. When you write a test that amends data in the database you need the test to clean up after itself. If it doesn\'t then the next test that runs may trip over the amended data and that\'s your test pack instantly useless.\\n\\nWhat you want is a way to wipe the slate clean - to return the database back to the state that it was in before your test ran. Kind of like a database restore - except that would be slow. And this is where [SQL Server\'s snapshot backups](<http://technet.microsoft.com/en-us/library/ms189548(v=sql.105).aspx>) have got your back. To quote MSDN:\\n\\n> \\\\*Snapshot backups have the following primary benefits:\\n>\\n> - A backup can be created quickly, typically measured in seconds, with little or no effect on the server.\\n> - A restore operation can be accomplished from a disk backup just as quickly.\\n> - Backup to tape can be accomplished by another host without an effect on the production system.\\n> - **A copy of a production database can be created instantly for reporting or testing.**\\n>\\n> *\\n\\nJust the ticket.\\n\\n## Our Mission\\n\\nIn this post I want to go through the process of taking an existing database, pointing Entity Framework at it, setting up some repositories and then creating an integration test pack that uses snapshot backups to cleanup after each test runs. The code detailed in this post is available in this [GitHub repo](https://github.com/johnnyreilly/SnapshotBackupsIntegrationTesting) if you want to have a go yourself.\\n\\n## We need a database\\n\\nYou can find a whole assortment of databases [here](https://msftdbprodsamples.codeplex.com/releases). I\'m going to use [AdventureWorksLT](https://msftdbprodsamples.codeplex.com/wikipage?title=AWLTDocs) as it\'s small and simple. So I\'ll download [this](https://msftdbprodsamples.codeplex.com/downloads/get/478217) and unzip it. I\'ll drop `AdventureWorksLT2008R2_Data/index.mdf` and `AdventureWorksLT2008R2_log.LDF` in my data folder and attach AdventureWorksLT2008R2 to my database server. And now I have a database.\\n\\n## Assemble me your finest DbContext\\n\\nOr in English: we want to point Entity Framework at our new shiny database. So let\'s fire up Visual Studio (I\'m using 2013) and create a new solution called \\"AdventureWorks\\".\\n\\nTo our solution let\'s add a new class library project called \\"AdventureWorks.EntityFramework\\". And to that we\'ll add an ADO.NET Entity Data Model which we\'ll call \\"AdventureWorks.edmx\\". When the wizard fires up we\'ll use the \\"Generate from database\\" option, click Next and select \\"New Connection\\". In the dialog we\'ll select our newly attached AdventureWorksLT2008R2 database. We\'ll leave the \\"save entity connection settings in App.Config\\" option selected and click Next. I\'m going to use Entity Framework 6.0 - though I think that any version would do. I\'m going to pull in all tables / store procs and views. And now Entity Framework is pointing at my database.\\n\\n## Let There be Repositories!\\n\\nIn the name of testability let\'s create a new project to house repositories called \\"AdventureWorks.Repositories\\". I\'m going to use [K. Scott Allen](http://odetocode.com/about/scott-allen)\'s fine [article on MSDN](http://msdn.microsoft.com/en-us/library/ff714955.aspx) to create a very basic set of repositories wrapped in a unit of work.\\n\\nIn my new project I\'ll add a reference to the `AdventureWorks.EntityFramework` project and create a new `IRepository` interface that looks like this:\\n\\n```cs\\nusing System;\\nusing System.Linq;\\nusing System.Linq.Expressions;\\n\\nnamespace AdventureWorks.Repositories\\n{\\n    public interface IRepository<T> where T : class\\n    {\\n        IQueryable<T> FindAll();\\n        IQueryable<T> FindWhere(Expression<Func<T, bool>> predicate);\\n        T Add(T newEntity);\\n        T Remove(T entity);\\n    }\\n}\\n```\\n\\nAnd a new `IUnitOfWork` interface that looks like this:\\n\\n```cs\\nusing AdventureWorks.EntityFramework;\\n\\nnamespace AdventureWorks.Repositories\\n{\\n    public interface IUnitOfWork\\n    {\\n        public IRepository<ErrorLog> ErrorLogs { get; }\\n        public IRepository<Address> Addresses { get; }\\n        public IRepository<Customer> Customers { get; }\\n        public IRepository<CustomerAddress> CustomerAddresses { get; }\\n        public IRepository<Product> Products { get; }\\n        public IRepository<ProductCategory> ProductCategories { get; }\\n        public IRepository<ProductDescription> ProductDescriptions { get; }\\n        public IRepository<ProductModel> ProductModels { get; }\\n        public IRepository<ProductModelProductDescription> ProductModelProductDescriptions { get; }\\n        public IRepository<SalesOrderDetail> SalesOrderDetails { get; }\\n        public IRepository<SalesOrderHeader> SalesOrderHeaders { get; }\\n        public IRepository<BuildVersion> BuildVersions { get; }\\n\\n        void Commit();\\n    }\\n}\\n```\\n\\nNow for the implementation of `IRepository`. For this we\'ll need a reference to Entity Framework in our project. Then we\'ll create a class called `SqlRepository`:\\n\\n```cs\\nusing System;\\nusing System.Data.Entity;\\nusing System.Linq;\\nusing System.Linq.Expressions;\\n\\nnamespace AdventureWorks.Repositories\\n{\\n    public class SqlRepository<T> : IRepository<T> where T : class\\n    {\\n        public SqlRepository(DbContext context)\\n        {\\n            _dbSet = context.Set<T>();\\n        }\\n\\n        public IQueryable<T> FindAll()\\n        {\\n            return _dbSet;\\n        }\\n\\n        public IQueryable<T> FindWhere(Expression<Func<T, bool>> predicate)\\n        {\\n            return _dbSet.Where(predicate);\\n        }\\n\\n        public T Add(T newEntity)\\n        {\\n            return _dbSet.Add(newEntity);\\n        }\\n\\n        public T Remove(T entity)\\n        {\\n            return _dbSet.Remove(entity);\\n        }\\n\\n        protected DbSet<T> _dbSet;\\n    }\\n}\\n```\\n\\nAnd we also need the implementation of `IUnitOfWork`. So we\'ll create a class called `SqlUnitOfWork`:\\n\\n```cs\\nusing System;\\nusing System.Linq;\\nusing System.Data.Entity;\\nusing AdventureWorks.EntityFramework;\\n\\nnamespace AdventureWorks.Repositories\\n{\\n    public class SqlUnitOfWork : IUnitOfWork\\n    {\\n        public SqlUnitOfWork()\\n        {\\n            _context = new AdventureWorksLT2008R2Entities();\\n        }\\n\\n        public IRepository<ErrorLog> ErrorLogs\\n        {\\n            get\\n            {\\n                if (_errorLogs == null) _errorLogs = new SqlRepository<ErrorLog>(_context);\\n                return _errorLogs;\\n            }\\n        }\\n\\n        public IRepository<Address> Addresses\\n        {\\n            get\\n            {\\n                if (_addresses == null) _addresses = new SqlRepository<Address>(_context);\\n                return _addresses;\\n            }\\n        }\\n\\n        public IRepository<Customer> Customers\\n        {\\n            get\\n            {\\n                if (_customers == null) _customers = new SqlRepository<Customer>(_context);\\n                return _customers;\\n            }\\n        }\\n\\n        public IRepository<CustomerAddress> CustomerAddresses\\n        {\\n            get\\n            {\\n                if (_customerAddresses == null) _customerAddresses = new SqlRepository<CustomerAddress>(_context);\\n                return _customerAddresses;\\n            }\\n        }\\n\\n        public IRepository<Product> Products\\n        {\\n            get\\n            {\\n                if (_products == null) _products = new SqlRepository<Product>(_context);\\n                return _products;\\n            }\\n        }\\n\\n        public IRepository<ProductCategory> ProductCategories\\n        {\\n            get\\n            {\\n                if (_productCategories == null) _productCategories = new SqlRepository<ProductCategory>(_context);\\n                return _productCategories;\\n            }\\n        }\\n\\n        public IRepository<ProductDescription> ProductDescriptions\\n        {\\n            get\\n            {\\n                if (_productDescriptions == null) _productDescriptions = new SqlRepository<ProductDescription>(_context);\\n                return _productDescriptions;\\n            }\\n        }\\n\\n        public IRepository<ProductModel> ProductModels\\n        {\\n            get\\n            {\\n                if (_productModels == null) _productModels = new SqlRepository<ProductModel>(_context);\\n                return _productModels;\\n            }\\n        }\\n\\n        public IRepository<ProductModelProductDescription> ProductModelProductDescriptions\\n        {\\n            get\\n            {\\n                if (_productModelProductDescriptions == null) _productModelProductDescriptions = new SqlRepository<ProductModelProductDescription>(_context);\\n                return _productModelProductDescriptions;\\n            }\\n        }\\n\\n        public IRepository<SalesOrderDetail> SalesOrderDetails\\n        {\\n            get\\n            {\\n                if (_salesOrderDetails == null) _salesOrderDetails = new SqlRepository<SalesOrderDetail>(_context);\\n                return _salesOrderDetails;\\n            }\\n        }\\n\\n        public IRepository<SalesOrderHeader> SalesOrderHeaders\\n        {\\n            get\\n            {\\n                if (_salesOrderHeaders == null) _salesOrderHeaders = new SqlRepository<SalesOrderHeader>(_context);\\n                return _salesOrderHeaders;\\n            }\\n        }\\n\\n        public IRepository<BuildVersion> BuildVersions\\n        {\\n            get\\n            {\\n                if (_buildVersions == null) _buildVersions = new SqlRepository<BuildVersion>(_context);\\n                return _buildVersions;\\n            }\\n        }\\n\\n        public void Commit()\\n        {\\n            _context.SaveChanges();\\n        }\\n\\n        SqlRepository<ErrorLog> _errorLogs = null;\\n        SqlRepository<Address> _addresses = null;\\n        SqlRepository<Customer> _customers = null;\\n        SqlRepository<CustomerAddress> _customerAddresses = null;\\n        SqlRepository<Product> _products = null;\\n        SqlRepository<ProductCategory> _productCategories = null;\\n        SqlRepository<ProductDescription> _productDescriptions = null;\\n        SqlRepository<ProductModel> _productModels = null;\\n        SqlRepository<ProductModelProductDescription> _productModelProductDescriptions = null;\\n        SqlRepository<SalesOrderDetail> _salesOrderDetails = null;\\n        SqlRepository<SalesOrderHeader> _salesOrderHeaders = null;\\n        SqlRepository<BuildVersion> _buildVersions = null;\\n\\n        readonly DbContext _context;\\n    }\\n}\\n```\\n\\n## And Now Let\'s Start Integration Testing!\\n\\nLet\'s create a new Unit Test project called \\"AdventureWorks.Repositories.IntegrationTests\\". (And just to be clear: this is \\\\*not\\\\* a unit test project - it is an **_integration_** test project.) We\'ll add a reference back to our `AdventureWorks.Repositories` project for the repositories and one back to `AdventureWorks.EntityFramework` for our domain models. And finally you\'ll need a reference to Entity Framework in your IntegrationTest project as well as well.\\n\\nWe\'ll copy across the `app.config` from `AdventureWorks.EntityFramework` to `AdventureWorks.Repositories.IntegrationTests` as it contains the database connection details. It\'ll look something like this:\\n\\n```xml\\n<?xml version=\\"1.0\\" encoding=\\"utf-8\\"?>\\n<configuration>\\n    <configSections>\\n        <section name=\\"entityFramework\\" type=\\"System.Data.Entity.Internal.ConfigFile.EntityFrameworkSection, EntityFramework, Version=6.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089\\" requirePermission=\\"false\\" />\\n        \x3c!-- For more information on Entity Framework configuration, visit http://go.microsoft.com/fwlink/?LinkID=237468 --\x3e\\n    </configSections>\\n    <connectionStrings>\\n        <add name=\\"AdventureWorksLT2008R2Entities\\"\\n             connectionString=\\"metadata=res://*/AdventureWorks.csdl|res://*/AdventureWorks.ssdl|res://*/AdventureWorks.msl;provider=System.Data.SqlClient;provider connection string=&quot;data source=.;initial catalog=AdventureWorksLT2008R2;integrated security=True;MultipleActiveResultSets=True;App=EntityFramework&quot;\\"\\n             providerName=\\"System.Data.EntityClient\\" />\\n    </connectionStrings>\\n    <entityFramework>\\n        <defaultConnectionFactory type=\\"System.Data.Entity.Infrastructure.SqlConnectionFactory, EntityFramework\\" />\\n        <providers>\\n            <provider invariantName=\\"System.Data.SqlClient\\" type=\\"System.Data.Entity.SqlServer.SqlProviderServices, EntityFramework.SqlServer\\" />\\n        </providers>\\n    </entityFramework>\\n</configuration>\\n```\\n\\nNow we\'re ready for a test. We\'ll add ourselves a class called `BuildVersionTests`:\\n\\n```cs\\nusing System;\\nusing System.Linq;\\nusing System.Linq.Expressions;\\nusing Microsoft.VisualStudio.TestTools.UnitTesting;\\n\\nnamespace AdventureWorks.Repositories.IntegrationTests\\n{\\n    [TestClass]\\n    public class BuildVersionTests\\n    {\\n        [TestMethod]\\n        public void BuildVersions_should_return_the_correct_version_information()\\n        {\\n            // Arrange\\n            var uow = new SqlUnitOfWork();\\n\\n            // Act\\n            var buildVersions = uow.BuildVersions.FindAll().ToList();\\n\\n            // Assert\\n            Assert.AreEqual(1, buildVersions.Count);\\n            Assert.AreEqual(\\"10.00.80404.00\\", buildVersions[0].Database_Version);\\n            Assert.AreEqual(new DateTime(2008, 4, 4), buildVersions[0].ModifiedDate);\\n            Assert.AreEqual(1, buildVersions[0].SystemInformationID);\\n            Assert.AreEqual(new DateTime(2008, 4, 4), buildVersions[0].VersionDate);\\n        }\\n    }\\n}\\n```\\n\\nThis is as simple as it gets - our test creates a new unit of work and queries the `BuildVersions` table to see what we can see. All it\'s really doing is demonstrating that we can now hit our database through our repositories. As a side note, we could have the exact same test operating directly on the `DbContext` like this:\\n\\n```cs\\n[TestMethod]\\n        public void DbContext_BuildVersions_should_return_the_correct_version_information()\\n        {\\n            // Arrange\\n            var dbContext = new AdventureWorks.EntityFramework.AdventureWorksLT2008R2Entities();\\n\\n            // Act\\n            var buildVersions = dbContext.BuildVersions.ToList();\\n\\n            // Assert\\n            Assert.AreEqual(1, buildVersions.Count);\\n            Assert.AreEqual(\\"10.00.80404.00\\", buildVersions[0].Database_Version);\\n            Assert.AreEqual(new DateTime(2008, 4, 4), buildVersions[0].ModifiedDate);\\n            Assert.AreEqual(1, buildVersions[0].SystemInformationID);\\n            Assert.AreEqual(new DateTime(2008, 4, 4), buildVersions[0].VersionDate);\\n        }\\n```\\n\\nFor the most part we won\'t be doing this but I wanted to be clear that full power of Entity Framework is available to you as you\'re putting together your integration tests.\\n\\n## Database Snapshotting Time\\n\\nUp until this point we\'ve essentially been laying our infrastructure and doing our plumbing. We now have a database, domain models and data access courtesy of Entity Framework, a testable repository layer and finally an integration test pack. What we want now is to get our database snapshot / backup and restore mechanism set up and integrated into the test pack.\\n\\nLet\'s add references to the `System.Data` and `System.Configuration` assemblies to our integration testing project and then add a new class called `DatabaseSnapshot`:\\n\\n```cs\\nusing System.Configuration;\\nusing System.Data;\\nusing System.Data.SqlClient;\\n\\nnamespace AdventureWorks.Repositories.IntegrationTests\\n{\\n    public static class DatabaseSnapshot\\n    {\\n        private const string SpCreateSnapShotName = \\"SnapshotBackup_Create\\";\\n        private const string SpCreateSnapShot =\\n@\\"CREATE PROCEDURE [dbo].[\\" + SpCreateSnapShotName + @\\"]\\n    @databaseName        varchar(512),\\n    @databaseLogicalName varchar(512),\\n    @snapshotBackupPath  varchar(512),\\n    @snapshotBackupName  varchar(512)\\nAS\\nBEGIN\\n    SET NOCOUNT ON;\\n\\n    DECLARE @sql varchar(500)\\n    SELECT @sql = \'CREATE DATABASE \' + @snapshotBackupName +\\n                  \' ON (NAME=[\' + @databaseLogicalName +\\n                  \'], FILENAME=\'\'\' + @snapshotBackupPath + @snapshotBackupName +\\n                  \'\'\') AS SNAPSHOT OF [\' + @databaseName + \']\'\\n    EXEC(@sql)\\nEND\\";\\n\\n        private const string SpRestoreSnapShotName = \\"SnapshotBackup_Restore\\";\\n        private const string SpRestoreSnapShot =\\n@\\"CREATE PROCEDURE [dbo].[\\" + SpRestoreSnapShotName + @\\"]\\n    @databaseName varchar(512),\\n    @snapshotBackupName varchar(512)\\nAS\\nBEGIN\\n    SET NOCOUNT ON;\\n\\n    DECLARE @sql varchar(500)\\n    SET @sql  = \'ALTER DATABASE [\' + @databaseName + \'] SET SINGLE_USER WITH ROLLBACK IMMEDIATE\'\\n    EXEC (@sql)\\n\\n    RESTORE DATABASE @databaseName\\n    FROM DATABASE_SNAPSHOT = @snapshotBackupName\\n\\n    SET @sql = \'ALTER DATABASE [\' + @databaseName + \'] SET MULTI_USER\'\\n    EXEC (@sql)\\nEND\\";\\n\\n        private const string SpDeleteSnapShotName = \\"SnapshotBackup_Delete\\";\\n        private const string SpDeleteSnapShot =\\n@\\"CREATE PROCEDURE [dbo].[\\" + SpDeleteSnapShotName + @\\"]\\n    @snapshotBackupName varchar(512)\\nAS\\nBEGIN\\n    SET NOCOUNT ON;\\n\\n    DECLARE @sql varchar(500)\\n\\n    SELECT @sql = \'DROP DATABASE \' + @snapshotBackupName\\n    EXEC(@sql)\\nEND\\";\\n\\n        private static string _masterDbConnectionString;\\n        private static string _dbName;\\n        private static ConnectionStringSettings _dbConnectionStringSettings;\\n\\n        private static ConnectionStringSettings DbConnectionStringSettings\\n        {\\n            get\\n            {\\n                if (_dbConnectionStringSettings == null)\\n                    _dbConnectionStringSettings = ConfigurationManager.ConnectionStrings[\\"SnapshotBackup\\"];\\n\\n                return _dbConnectionStringSettings;\\n            }\\n        }\\n\\n        /// <summary>\\n        /// Stored procedures should be executed against master database\\n        /// </summary>\\n        private static string MasterDbConnectionString\\n        {\\n            get\\n            {\\n                if (string.IsNullOrEmpty(_masterDbConnectionString))\\n                {\\n                    var sqlConnection = new SqlConnection(DbConnectionStringSettings.ConnectionString);\\n                    _masterDbConnectionString = DbConnectionStringSettings.ConnectionString.Replace(sqlConnection.Database, \\"master\\");\\n                }\\n                return _masterDbConnectionString;\\n            }\\n        }\\n\\n        private static string DbName\\n        {\\n            get\\n            {\\n                if (string.IsNullOrEmpty(_dbName))\\n                    _dbName = new SqlConnection(DbConnectionStringSettings.ConnectionString).Database.TrimStart(\'[\').TrimEnd(\']\');\\n\\n                return _dbName;\\n            }\\n        }\\n\\n        public static void SetupStoredProcedures()\\n        {\\n            using (var conn = new SqlConnection(MasterDbConnectionString))\\n            {\\n                conn.Open();\\n\\n                // Drop the existing stored procedures\\n                SqlCommand cmd;\\n                const string dropProcSql = \\"IF EXISTS (SELECT * FROM sys.objects WHERE object_id = OBJECT_ID(N\'[dbo].[{0}]\') AND type in (N\'P\', N\'PC\')) DROP PROCEDURE [dbo].[{0}]\\";\\n                foreach (var spName in new[] { SpCreateSnapShotName, SpDeleteSnapShotName, SpRestoreSnapShotName })\\n                {\\n                    cmd = new SqlCommand(string.Format(dropProcSql, spName), conn);\\n                    cmd.ExecuteNonQuery();\\n                }\\n\\n                // Create the stored procedures anew\\n                foreach (var createProcSql in new[] { SpCreateSnapShot, SpDeleteSnapShot, SpRestoreSnapShot })\\n                {\\n                    cmd = new SqlCommand(createProcSql, conn);\\n                    cmd.ExecuteNonQuery();\\n                }\\n\\n                conn.Close();\\n            }\\n        }\\n\\n        public static void CreateSnapShot()\\n        {\\n            var databaseName = new SqlParameter { ParameterName = \\"@databaseName\\", SqlValue = SqlDbType.VarChar, Value = DbName };\\n            var databaseLogicalName = new SqlParameter { ParameterName = \\"@databaseLogicalName\\", SqlValue = SqlDbType.VarChar, Value = ConfigurationManager.AppSettings[\\"DatabaseLogicalName\\"] };\\n            var snapshotBackupPath = new SqlParameter { ParameterName = \\"@snapshotBackupPath\\", SqlValue = SqlDbType.VarChar, Value = ConfigurationManager.AppSettings[\\"SnapshotBackupPath\\"] };\\n            var snapshotBackupName = new SqlParameter { ParameterName = \\"@snapshotBackupName\\", SqlValue = SqlDbType.VarChar, Value = ConfigurationManager.AppSettings[\\"SnapshotBackupName\\"] };\\n\\n            ExecuteStoredProcAgainstMaster(SpCreateSnapShotName, new[] { databaseName, databaseLogicalName, snapshotBackupPath, snapshotBackupName });\\n        }\\n\\n        public static void DeleteSnapShot()\\n        {\\n            var snapshotBackupName = new SqlParameter { ParameterName = \\"@snapshotBackupName\\", SqlValue = SqlDbType.VarChar, Value = ConfigurationManager.AppSettings[\\"SnapshotBackupName\\"] };\\n\\n            ExecuteStoredProcAgainstMaster(SpDeleteSnapShotName, new[] { snapshotBackupName });\\n        }\\n\\n        public static void RestoreSnapShot()\\n        {\\n            var databaseName = new SqlParameter { ParameterName = \\"@databaseName\\", SqlValue = SqlDbType.VarChar, Value = DbName };\\n            var snapshotBackupName = new SqlParameter { ParameterName = \\"@snapshotBackupName\\", SqlValue = SqlDbType.VarChar, Value = ConfigurationManager.AppSettings[\\"SnapshotBackupName\\"] };\\n\\n            ExecuteStoredProcAgainstMaster(SpRestoreSnapShotName, new[] { databaseName, snapshotBackupName });\\n        }\\n\\n        private static void ExecuteStoredProcAgainstMaster(string storedProc, SqlParameter[] parameters)\\n        {\\n            using (var conn = new SqlConnection(MasterDbConnectionString))\\n            {\\n                conn.Open();\\n                var cmd = new SqlCommand(storedProc, conn) { CommandType = CommandType.StoredProcedure };\\n                cmd.Parameters.AddRange(parameters);\\n                cmd.ExecuteNonQuery();\\n                conn.Close();\\n            }\\n        }\\n    }\\n}\\n```\\n\\nThe `DatabaseSnapshot` class exposes 4 methods:\\n\\n<dl><dt>SetupStoredProcedures</dt><dd>This method creates 3 stored procedures on the master database: <code>SnapshotBackup_Create</code>, <code>SnapshotBackup_Restore</code> and <code>SnapshotBackup_Delete</code>. These procs do pretty much what their names suggest and the other 3 methods call these stored procedures when creating, restoring and deleting snapshot backups respectively. You can see the (fairly minimal) SQL for these stored procs at the top of the<code>DatabaseSnapshot</code> class.</dd><dt>CreateSnapShot</dt><dd>This method creates a snapshot backup of the database at this point in time.</dd><dt>RestoreSnapShot</dt><dd>This method restores the database back to state it was in when the snapshot backup was created.</dd><dt>DeleteSnapShot</dt><dd>This method attempts to delete the existing snapshot backup.</dd></dl>\\n\\nIn order that we can use the `DatabaseSnapshot` class we need to add the following entries to our `app.config`:\\n\\n```xml\\n<configuration>\\n\\n    <connectionStrings>\\n\\n        <add name=\\"SnapshotBackup\\"\\n             connectionString=\\"data source=.;initial catalog=AdventureWorksLT2008R2;Trusted_Connection=true;Connection Timeout=200\\" />\\n\\n    </connectionStrings>\\n\\n    <appSettings>\\n        <add key=\\"DatabaseLogicalName\\" value=\\"AdventureWorksLT2008_Data\\" />\\n        <add key=\\"SnapshotBackupPath\\" value=\\"C:\\\\DbSnapshots\\\\\\" />\\n        <add key=\\"SnapshotBackupName\\" value=\\"AdventureWorksLT2008R2_Snapshot\\" />\\n    </appSettings>\\n</configuration>\\n```\\n\\nThese settings allow have the following purposes:\\n\\n<dl><dt>SnapshotBackup</dt><dd>A connection string that allows <code>DatabaseSnapshot</code> to connect to the database.</dd><dt>DatabaseLogicalName</dt><dd>The logical name of the database you want to backup. (This can be found on the Files tab of the Database Properties in SSMS)</dd><dt>SnapshotBackupPath</dt><dd>The location where the snapshot backup is to be stored. You need to make sure that this exists on your machine.</dd><dt>SnapshotBackupName</dt><dd>The name of the snapshot backup that will be created.</dd></dl>\\n\\nNow to make use of `DatabaseSnapshot`. Let\'s add a new class called `SetUpTearDown`:\\n\\n```cs\\nusing Microsoft.VisualStudio.TestTools.UnitTesting;\\n\\nnamespace AdventureWorks.Repositories.IntegrationTests\\n{\\n    [TestClass]\\n    public static class SetUpTearDown\\n    {\\n        [AssemblyInitialize]\\n        public static void TestRunInitialize(TestContext context)\\n        {\\n            try\\n            {\\n                // Try to delete the snapshot in case it was left over from aborted test runs\\n                DatabaseSnapshot.DeleteSnapShot();\\n            }\\n            catch { /* this should fail with snapshot does not exist */ }\\n\\n            DatabaseSnapshot.SetupStoredProcedures();\\n            DatabaseSnapshot.CreateSnapShot();\\n        }\\n\\n\\n        [AssemblyCleanup]\\n        public static void TestRunCleanup()\\n        {\\n            DatabaseSnapshot.DeleteSnapShot();\\n        }\\n    }\\n}\\n```\\n\\nAt the start of the test run this will create a snapshot in case one doesn\'t exist already. And at the end of the test run it will be a good citizen and delete the snapshot. We\'ll also add an extra method to our `BuildVersionTests` class:\\n\\n```cs\\nnamespace AdventureWorks.Repositories.IntegrationTests\\n{\\n    [TestClass]\\n    public class BuildVersionTests\\n    {\\n        // ...\\n\\n        [TestCleanup]\\n        public void TestCleanup()\\n        {\\n            DatabaseSnapshot.RestoreSnapShot();\\n        }\\n    }\\n}\\n```\\n\\nThis will ensure that after each test runs the database will be restored back to the snapshot created in `SetUpTearDown`. Now if you re-run your tests, in between each test the restore back to the snapshot is taking place.\\n\\n## Prove it!\\n\\nOf course the tests we have in place at present don\'t actually change the data at all. So I could be lying. I\'m not. Let\'s prove it by adding one more class called `CustomerTests`:\\n\\n```cs\\nusing System;\\nusing System.Linq;\\nusing System.Linq.Expressions;\\nusing Microsoft.VisualStudio.TestTools.UnitTesting;\\nusing AdventureWorks.EntityFramework;\\n\\nnamespace AdventureWorks.Repositories.IntegrationTests\\n{\\n    [TestClass]\\n    public class CustomerTests\\n    {\\n        [TestMethod]\\n        public void Should_change_a_customers_first_and_last_name()\\n        {\\n            // Arrange\\n            var uow = new SqlUnitOfWork();\\n\\n            // Act\\n            var customer = uow.Customers.FindWhere(x => x.FirstName == \\"Jay\\" && x.LastName == \\"Adams\\").First();\\n            var customerId = customer.CustomerID;\\n            customer.FirstName = \\"John\\";\\n            customer.LastName = \\"Reilly\\";\\n            uow.Commit();\\n\\n            // Assert\\n            Assert.IsNotNull(uow.Customers.FindWhere(x => x.FirstName == \\"John\\" && x.LastName == \\"Reilly\\" && x.CustomerID == customerId).SingleOrDefault());\\n        }\\n\\n        [TestCleanup]\\n        public void TestCleanup()\\n        {\\n            DatabaseSnapshot.RestoreSnapShot();\\n        }\\n    }\\n}\\n```\\n\\nThe above test checks that you can look up an existing customer, Mr Jay Adams, and change his name to my name - to John Reilly. If I execute the test above and there was no restore in place then subsequently when I came to exercise this test it should start to fail as it no longer has a Mr Jay Adams to lookup. But with this restore mechanism in place I can execute this test repeatedly without worrying.\\n\\n## Rounding off\\n\\nAnd that\'s us finished - we now have a database snapshot restore mechanism in place. With this we can develop integration tests that thoroughly change the data in our database secure in the knowledge that once the test is complete our database will be restored back to it\'s initial state.\\n\\nObviously there are other alternative approaches for integration testing available to that which I\'ve laid out in this post. But I can imagine that this approach is very useful for applying to legacy applications that you might inherit and need to continue supporting. Also, this approach should fit in well with a continuous integration setup. It would be pretty straightforward to have database that existed purely for testing purposes against which all the integration tests could be set to run at the point of each check in.\\n\\nThanks to Marc Talary, Sandeep Deo and Tishul Vadher who all contributed to `DatabaseSnapshot`. Credit is also due to Google due to the hundreds of articles the team ended up reading on snapshot backups."},{"id":"upgrading-to-typescript-095-personal","metadata":{"permalink":"/upgrading-to-typescript-095-personal","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2014-01-09-upgrading-to-typescript-095-personal/index.md","source":"@site/blog/2014-01-09-upgrading-to-typescript-095-personal/index.md","title":"Upgrading to TypeScript 0.9.5 - A Personal Memoir","description":"Upgrade to TypeScript 0.9.5 worth it despite Visual Studio issues. Declaration merging glitches resolved by interface-driven approach.","date":"2014-01-09T00:00:00.000Z","tags":[{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":7.725,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"upgrading-to-typescript-095-personal","title":"Upgrading to TypeScript 0.9.5 - A Personal Memoir","authors":"johnnyreilly","tags":["typescript"],"hide_table_of_contents":false,"description":"Upgrade to TypeScript 0.9.5 worth it despite Visual Studio issues. Declaration merging glitches resolved by interface-driven approach."},"unlisted":false,"prevItem":{"title":"Integration Testing with Entity Framework and Snapshot Backups","permalink":"/integration-testing-with-entity"},"nextItem":{"title":"NuGet and WebMatrix: How to install a specific version of a package","permalink":"/nuget-and-webmatrix-how-to-install"}},"content":"I recently made the step to upgrade from TypeScript 0.9.1.1 to 0.9.5. To my surprise this process was rather painful and certainly not an unalloyed pleasure. Since I\'m now on the other side, so to speak, I thought I\'d share my experience and cast back a rope bridge to those about to journey over the abyss.\\n\\n\x3c!--truncate--\x3e\\n\\n## TL;DR\\n\\nTypeScript 0.9.5 is worth making the jump to. However, if you are using Visual Studio (as I would guess many are) then you should be aware of a number of problems with the TypeScript Visual Studio tooling for TS 0.9.5. These problems can be worked around if you follow the instructions in this post.\\n\\n## Upgrading the Plugin\\n\\nAt home I upgraded the moment TS 0.9.5 was released. This allowed me to help with migrating the [Definitely Typed typings](https://github.com/borisyankov/DefinitelyTyped) over from 0.9.1.1. And allowed me to give TS 0.9.5 a little test drive. However, I deliberately held off performing the upgrade at work until I knew that all the Definitely Typed typings had been upgraded. This was completed [by the end of 2013](https://github.com/borisyankov/DefinitelyTyped/pull/1385). So in the new year it seemed a good time to make the move.\\n\\nIf, like me, you are using TypeScript inside Visual Studio then you\'d imagine it\'s as simple as closing down VS, uninstalling TypeScript 0.9.1.1 from Programs and Features and then installing the [new plugin](http://www.typescriptlang.org/#Download). And it is if you are running IE 10 or IE 11 on your Windows machine. If you are running a lower IE version then there is a problem.\\n\\nRegrettably, the TypeScript 0.9.5 plugin installer has a dependency on IE 10. Fortunately TypeScript itself has no dependency on IE 10 at all (and why would it?). This dependency appears to have been a mistake. I [raised it as an issue](https://typescript.codeplex.com/workitem/1975) and the TS team have said that this will be resolved in the next major release.\\n\\nHappily there is a workaround if you\'re running IE 9 or lower which has been noted in the [comments underneath the TS 0.9.5 release blog post](https://blogs.msdn.com/b/typescript/archive/2013/12/05/announcing-typescript-0-9-5.aspx). All you do is set the `HKEY_LOCAL_MACHINE\\\\SOFTWARE\\\\Wow6432Node\\\\Microsoft\\\\Internet Explorer\\\\svcVersion` registry key value to `10.0.9200.16384` for the duration of the install.\\n\\nFirst hurdle jumped, the upgrade continues simple enough. Then the fun starts...\\n\\n## Declaration Merging is dead... Sort of\\n\\nHaving upgraded my plugin I opened up the project I\'m working on in Visual Studio. I used NuGet to upgrade all the Definitely Typed packages to the latest (TS 0.9.5) versions. Then I tried, and failed, to compile. It was the the most obscure error I\'ve seen in a while:\\n\\n```ts\\nVSTSC : tsc.js(37574, 25) Microsoft JScript runtime error : Unable to get value of the property \'wrapsSomeTypeParameter\': object is null or undefined\\n```\\n\\nAs you can see there was no indication where in my code the problem was being caused. Fortunately someone had already suffered this particular problem and logged an issue [here](https://typescript.codeplex.com/workitem/1995). Digging through the comments I found a common theme; everyone experiencing the problem was using the [Q typings](https://github.com/borisyankov/DefinitelyTyped/blob/master/q/Q.d.ts). So what\'s up with that?\\n\\nStrangely, if you directly referenced the Q typings everything was okay - which is how the Definitely Typed tests came to pass in the first place. But if you wanted to make use of these typings with implicit referencing (in Visual Studio since TS 0.9.1, all TypeScript files in a project are considered to be referencing each other) - well it doesn\'t work.\\n\\nI decided to take a look at the Q typings at this point to see what was so upsetting about them. The one thing that was obvious was that these typings make use of [Declaration Merging](https://blogs.msdn.com/b/typescript/archive/2013/06/18/announcing-typescript-0-9.aspx). And this made them slightly different to most of the other typing libraries that I was using. So I decided to refactor the Q typings to use the more interface driven approach the other typing libraries used in the hope that might resolve the issue.\\n\\nRoughly speaking I went from:\\n\\n```ts\\ndeclare function Q<T>(promise: Q.IPromise<T>): Q.Promise<T>;\\ndeclare function Q<T>(promise: JQueryPromise<T>): Q.Promise<T>;\\ndeclare function Q<T>(value: T): Q.Promise<T>;\\n\\ndeclare module Q {\\n  //\u2026 functions etc in here\\n}\\n\\ndeclare module \'q\' {\\n  export = Q;\\n}\\n```\\n\\nTo:\\n\\n```ts\\ninterface QIPromise<T> {\\n    //\u2026 functions etc in here\\n}\\n\\ninterface QDeferred<T> {\\n    //\u2026 functions etc in here\\n}\\n\\ninterface QPromise<T> {\\n    //\u2026 functions etc in here\\n}\\n\\ninterface QPromiseState<T> {\\n    //\u2026 functions etc in here\\n}\\n\\ninterface QStatic {\\n\\n    <t>(promise: QIPromise<T>): QPromise<T>;\\n    <t>(promise: JQueryPromise<T>): QPromise<T>;\\n    <t>(value: T): QPromise<T>;\\n\\n    //\u2026 other functions etc continue here\\n}\\n\\ndeclare module \\"q\\" {\\n    export = Q;\\n}\\ndeclare var Q: QStatic;\\n</t></t></t>\\n```\\n\\nAnd that fixed the obscure \'wrapsSomeTypeParameter\' error. The full source code of these amended typings can be found as a GitHub Repo [here](https://github.com/johnnyreilly/Q-TS-0.9.5-WorkAround) in case you want to use it yourself. (I did originally consider adding this to Definitely Typed but opted not to in the end - [see discussion on GitHub](https://github.com/borisyankov/DefinitelyTyped/pull/1497).)\\n\\n\x3c!-- <h4>TypeScript Language Service</h4> <p>At this point I could compile - which was fantastic.  However, the strangest thing: all the typings from other files were undetected.  Despite having the jQuery, Q, Knockout etc typings within my project the TypeScript Language Service was not detecting them.  The TypeScript Language Service (if you\'re not aware of it) is the supplier of Intellisense and all that good stuff which Visual Studio uses to give you a rich IDE.  This lead to the odd experience of being able to compile my TypeScript successfully (the compiler could detect my typings) but having a code editor that was a sea of red squiggly lines.</p> <p>There\'s a happy ending here - for although TypeScript 0.9.5 had delivered the problem it had also delivered a solution.  <a href=\\"https://blogs.msdn.com/b/typescript/archive/2013/12/05/announcing-typescript-0-9-5.aspx\\" target=\\"_blank\\">With TypeScript 0.9.5 you can now make use of a <code>_references.ts</code> file</a>:</p> <blockquote cite=\\"https://blogs.msdn.com/b/typescript/archive/2013/12/05/announcing-typescript-0-9-5.aspx\\"><em><p>\\"With the previous improvements to the Visual Studio experience, we\'ve moved to projects implicitly referencing the .ts files contained in the project.  This cut down on having to explicitly reference your files in the project, bringing the experience much closer to C#.  Unfortunately, it also did not work well when using the option to concatenate your output .js file.</p> <p>We\'re continuing to improve this experience.  Starting with 0.9.5, you can now add an <code>_references.ts</code> file to your project.  This file will be the first passed to the compiler, allowing you more control over the order the generated .js file when used in combination with the Combine JavaScript output into file option (the equivalent of using the --out commandline option).\\"</p></em></blockquote> <p>By adding an <code>_references.ts</code> file to our project we able to get the TypeScript Language Service functioning once more.  There were a couple of \\"gotchas\\" that you should be aware of:</p> <ul><li>You may already have a <code>_references.<strong>js</strong></code> file in your project.  It drives the JavaScript Intellisense Visual Studio provides.  So if you have parts of your application that are just straight JavaScript (we do) and you still want your Intellisense to persist then be certain to place your <code>_references.ts</code> file where it doesn\'t compile and delete your a <code>_references.<strong>js</strong></code> file.</li><li>Make sure your <code>_references.ts</code> contains <em>all</em> TypeScript files in your project.  Without this you don\'t have a functioning TypeScript Language Service.</li><li>Occasionally the problem will re-occur; the TypeScript Language Service will stop working again.  This can generally be righted by opening your <code>_references.ts</code> inside Visual Studio.  A little flaky I know.</li></ul> <p>In the end <a href=\\"https://typescript.codeplex.com/workitem/2071\\" target=\\"_blank\\">I logged the issue on CodePlex</a> and I\'m hopeful it will be resolved in subsequent versions of TypeScript.</p>--\x3e\\n\\n## The Promised Land\\n\\nYou\'re there. You\'ve upgraded to the new plugin and the new typings. All is compiling as it should and the language service is working as well. Was it worth it? I think yes, for the following reasons:\\n\\n1. TS 0.9.5 compiles faster, and hogs less memory.\\n2. When we compiled with TS 0.9.5 we found there were a couple of bugs in our codebase which the tightened up compiler was now detecting. Essentially where we\'d assumed types were flowing through to functions there were a couple of occasions with TS 0.9.1.1 where they weren\'t. Where we\'d assumed we had a type of `T` available in a function whereas it was actually a type of `any`. I was really surprised that this was the case since we were already making use of `noImplicitAny` compiler flag in our project. So where a type had changed and a retired property was being referenced TS 0.9.5 picked up an error that TS 0.9.1.1 had not. Good catch!\\n3. And finally (and I know these are really minor), the compiled JS is a little different now. Firstly, the compiled JS features all of TypeScript comments in the positions that you might hope for. Previously it seemed that about 75% came along for the ride and ended up in some strange locations sometimes. Secondly, enums are treated differently during compilation now - where it makes sense the actual backing value of an enum is used rather than going through the JavaScript construct. So it\'s a bit like a `const` I guess - presumably this allows JavaScript engines to optimise a little more.\\n\\nI hope I haven\'t put you off with this post. I think TypeScript 0.9.5 is well worth making the leap for - and hopefully by reading this you\'ll have saved yourself from a few of the rough edges."},{"id":"nuget-and-webmatrix-how-to-install","metadata":{"permalink":"/nuget-and-webmatrix-how-to-install","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2013-12-13-nuget-and-webmatrix-how-to-install/index.md","source":"@site/blog/2013-12-13-nuget-and-webmatrix-how-to-install/index.md","title":"NuGet and WebMatrix: How to install a specific version of a package","description":"WebMatrix lacks NuGet command line, but users can still install a specific version manually by following the necessary steps - a bit of a challenge.","date":"2013-12-13T00:00:00.000Z","tags":[{"inline":false,"label":"jQuery","permalink":"/tags/jquery","description":"The jQuery library."}],"readingTime":2.38,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"nuget-and-webmatrix-how-to-install","title":"NuGet and WebMatrix: How to install a specific version of a package","authors":"johnnyreilly","tags":["jquery"],"hide_table_of_contents":false,"description":"WebMatrix lacks NuGet command line, but users can still install a specific version manually by following the necessary steps - a bit of a challenge."},"unlisted":false,"prevItem":{"title":"Upgrading to TypeScript 0.9.5 - A Personal Memoir","permalink":"/upgrading-to-typescript-095-personal"},"nextItem":{"title":"Simple fading in and out using CSS transitions and classes","permalink":"/simple-fading-in-and-out-using-css-transitions"}},"content":"I\'ve recently been experimenting with WebMatrix. If you haven\'t heard of it, WebMatrix is Microsoft\'s _[\\"free, lightweight, cloud-connected web development tool\\"](http://www.microsoft.com/web/webmatrix/)_. All marketing aside, it\'s pretty cool. You can whip up a site in next to no time, it has source control, publishing abilities, intellisense. Much good stuff. And one thing it has, that I genuinely hadn\'t expected is [NuGet](https://www.nuget.org/). Brilliant!\\n\\n\x3c!--truncate--\x3e\\n\\nBut like any free product there are disadvantages. As a long time Visual Studio user I\'ve become very used to the power of the NuGet command line. I\'ve been spoiled. You don\'t have this in WebMatrix. You have a nice UI.\\n\\nLooks great right? However, if you want to install a specific version of a NuGet package... well let\'s see what happens...\\n\\nAs you\'re probably aware jQuery currently exists in 2 branches; the 1.10.x branch which supports IE 6-8 and the 2.0.x branch which doesn\'t. However there is only 1 jQuery inside NuGet. Let\'s click on install and see if we can select a specific version.\\n\\nHmmm.... As you can see it\'s 2.0.3 or bust. We can\'t select a specific version; we\'re forced to go with the latest and greatest which is a problem if you need to support IE 6-8. So the obvious strategy if you\'re in this particular camp is to forego NuGet entirely. Go old school. And we could. But let\'s say we want to keep using NuGet, mindful that a little while down the road we\'ll be ready to do that upgrade. Can it be done? Let\'s find out.\\n\\n## NuGet, by hook or by crook\\n\\nI\'ve created a new site in WebMatrix using the Empty Site template.\\n\\nLovely.\\n\\nNow to get me some jQuery 1.10.2 goodness. To the console Batman! We\'ve already got the NuGet command line installed (if you haven\'t you could get it from [here](http://nuget.org/nuget.exe)) and so we follow these steps:\\n\\n- At the `C:\\\\` prompt we enter `nuget install jQuery -Version 1.10.2` and down comes jQuery 1.10.2.\\n- We move `C:\\\\jQuery.1.10.2` to `C:\\\\Users\\\\me\\\\Documents\\\\My Web Sites\\\\Empty Site\\\\App_Data\\\\packages\\\\jQuery.1.10.2`.\\n- Then we delete the `C:\\\\Users\\\\me\\\\Documents\\\\My Web Sites\\\\Empty Site\\\\App_Data\\\\packages\\\\jQuery.1.10.2\\\\Tools` subfolder.\\n- We move `C:\\\\Users\\\\me\\\\Documents\\\\My Web Sites\\\\Empty Site\\\\App_Data\\\\packages\\\\jQuery.1.10.2\\\\Content\\\\Scripts` to `C:\\\\Users\\\\me\\\\Documents\\\\My Web Sites\\\\Empty Site\\\\Scripts`.\\n- And finally we delete the `C:\\\\Users\\\\me\\\\Documents\\\\My Web Sites\\\\Empty Site\\\\App_Data\\\\packages\\\\jQuery.1.10.2\\\\Content` folder.\\n\\nIf we go to NuGet and select updates you\'ll see that jQuery is now considered \\"installed\\" and an update is available. So, in short, our plan worked - yay!\\n\\n## Now for bonus points\\n\\nJust to prove that you can upgrade using the WebMatrix tooling following our manual install let\'s do it. Click \\"Update\\", then \\"Yes\\" and finally \\"I Accept\\" to the EULA. You\'ll now see we\'re now on jQuery 2.0.3.\\n\\n## Rounding off\\n\\nIn my example I\'m only looking at a simple JavaScript library. But the same principal should be able to be applied to any NuGet package as far as I\'m aware. Hope that helps!"},{"id":"simple-fading-in-and-out-using-css-transitions","metadata":{"permalink":"/simple-fading-in-and-out-using-css-transitions","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2013-12-04-simple-fading-in-and-out-using-css-transitions/index.md","source":"@site/blog/2013-12-04-simple-fading-in-and-out-using-css-transitions/index.md","title":"Simple fading in and out using CSS transitions and classes","description":"Learn to create a fade effect with CSS transitions for improved animation and battery life. Warning: display: none behaves differently than jQuery.","date":"2013-12-04T00:00:00.000Z","tags":[],"readingTime":3.64,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"simple-fading-in-and-out-using-css-transitions","title":"Simple fading in and out using CSS transitions and classes","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false,"description":"Learn to create a fade effect with CSS transitions for improved animation and battery life. Warning: display: none behaves differently than jQuery."},"unlisted":false,"prevItem":{"title":"NuGet and WebMatrix: How to install a specific version of a package","permalink":"/nuget-and-webmatrix-how-to-install"},"nextItem":{"title":"Rolling your own confirm mechanism using Promises and jQuery UI","permalink":"/rolling-your-own-confirm-mechanism"}},"content":"Caveat emptor folks... Let me start off by putting my hands up and saying I am no expert on CSS. And furthermore let me say that this blog post is essentially the distillation of a heady session of googling on the topic of CSS transitions. The credit for the technique detailed here belongs to many others, I\'m just documenting it for my own benefit (and for anyone who stumbles upon this).\\n\\n\x3c!--truncate--\x3e\\n\\n## What do we want to do?\\n\\nMost web developers have likely reached at some point for jQuery\'s [`fadeIn`](http://api.jquery.com/fadeIn/) and [`fadeOut`](http://api.jquery.com/fadeOut/) awesomeness. What could be cooler than fading in or out your UI, right?\\n\\nBehind the scenes of `fadeIn` and `fadeOut` JavaScript is doing an awful lot of work to create that animation. And in our modern world we simply don\'t need to do that work anymore; it\'s gone native and is covered by [CSS transitions](https://developer.mozilla.org/en-US/docs/Web/Guide/CSS/Using_CSS_transitions).\\n\\nAdded to the \\"[because it\'s there](http://en.wikipedia.org/wiki/George_Mallory)\\" reason for using CSS transitions to do fading there is a more important reason; let me quote [HTML5 rocks](http://www.html5rocks.com/en/tutorials/speed/html5/#toc-css3-transitions):\\n\\n> \\"_CSS Transitions make style animation trivial for everyone, but they also are a smart performance feature. Because a CSS transition is managed by the browser, the fidelity of its animation can be greatly improved, and in many cases hardware accelerated. Currently WebKit (Chrome, Safari, iOS) have hardware accelerated CSS transforms, but it\'s coming quickly to other browsers and platforms._\\"\\n\\nAdded to this, if you have mobile users then the usage of native functionality (as opposed to doing it manually in JavaScript) actually saves battery life.\\n\\n## I\'m sold - let\'s do it!\\n\\nThis is the CSS we\'ll need:\\n\\n```css\\n.fader {\\n  -moz-transition: opacity 0.7s linear;\\n  -o-transition: opacity 0.7s linear;\\n  -webkit-transition: opacity 0.7s linear;\\n  transition: opacity 0.7s linear;\\n}\\n\\n.fader.fadedOut {\\n  opacity: 0;\\n}\\n```\\n\\nNote we have 2 CSS classes:\\n\\n- `fader` \\\\- if this class is applied to an element then when the opacity of that element is changed it will be an animated change. The duration of the transition and the timing function used are customisable - in this case it takes 0.7 seconds and is linear.\\n- `fadedOut` \\\\- when used in conjunction with `fader` this class creates a fading in or fading out effect as it is removed or applied respectively. (This relies upon the default value of opacity being 1.)\\n\\nLet\'s see it in action:\\n\\n<iframe width=\\"100%\\" height=\\"200\\" src=\\"https://jsfiddle.net/johnny_reilly/86amq/embedded/result,js,html,css\\" allowFullScreen=\\"allowFullScreen\\" frameBorder=\\"0\\"></iframe>\\n\\nIt goes without saying that one day in the not too distant future (I hope) we\'ll be able to leave behind the horrible world of vendor prefixes. Then we\'ll be down to just the single `transition` statement. One day...\\n\\n## Now, a warning...\\n\\nUnfortunately the technique detailed above differs from [`fadeIn`](http://api.jquery.com/fadeIn/) and [`fadeOut`](http://api.jquery.com/fadeOut/) in one important way. When the `fadeOut` animation completes it sets removes the element from the flow of the DOM using `display: none`. However, display is not a property that can be animated and so you can\'t include this in your CSS transition. If removing the element from the flow of the DOM is something you need then you\'ll need to bear this in mind. If anyone has any suggestions for an nice way to approach this I\'d love to hear from you.\\n\\n## A halfway there solution to the `display: none`\\n\\nAndrew Davey tweeted me the suggestion below:\\n\\n> [@johnny_reilly](https://twitter.com/johnny_reilly) Yep, transitions are sweet. You could use the transitionend event to remove the element from the DOM [http://t.co/Q1oWy3g8Lp](http://t.co/Q1oWy3g8Lp)\\n>\\n> \u2014 Andrew Davey (@andrewdavey) [December 5, 2013](https://twitter.com/andrewdavey/statuses/408545283606212608)\\n\\n<script async=\\"\\" src=\\"//platform.twitter.com/widgets.js\\" charSet=\\"utf-8\\"><\/script>\\n\\nSo I thought I\'d give it a go. However, whilst we\'ve a `transitionend` event to play with we don\'t have a corresponding `transitionstart` or `transitionbegin`. So I tried this:\\n\\n```js\\n$(\'#showHideButton\').click(function () {\\n  var $alertDiv = $(\'#alertDiv\');\\n  if ($alertDiv.hasClass(\'fadedOut\')) {\\n    $alertDiv.removeClass(\'fadedOut\').css(\'display\', \'\');\\n  } else {\\n    $(\'#alertDiv\').addClass(\'fadedOut\');\\n  }\\n});\\n\\n$(document).on(\\n  \'webkitTransitionEnd transitionend oTransitionEnd\',\\n  \'.fader\',\\n  function (evnt) {\\n    var $faded = $(evnt.target);\\n    if ($faded.hasClass(\'fadedOut\')) {\\n      $faded.css(\'display\', \'none\');\\n    }\\n  },\\n);\\n```\\n\\nEssentially, on the `transitionend` event `display: none` is applied to the element in question. Groovy. In the absence of a `transitionstart` or `transitionbegin`, when removing the `fadeOut` class I\'m first manually clearing out the `display: none`. Whilst this works in terms of adding it back into the flow of the DOM it takes away all the `fadeIn` gorgeousness. So it\'s not quite the fully featured solution you might hope for. But it\'s a start."},{"id":"rolling-your-own-confirm-mechanism","metadata":{"permalink":"/rolling-your-own-confirm-mechanism","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2013-11-26-rolling-your-own-confirm-mechanism/index.md","source":"@site/blog/2013-11-26-rolling-your-own-confirm-mechanism/index.md","title":"Rolling your own confirm mechanism using Promises and jQuery UI","description":"Learn how to create a custom confirm dialog using jQuery UI\u2019s dialog and promises. The custom dialog is more configurable than the default\xa0`window.confirm`.","date":"2013-11-26T00:00:00.000Z","tags":[{"inline":false,"label":"jQuery","permalink":"/tags/jquery","description":"The jQuery library."}],"readingTime":4.33,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"rolling-your-own-confirm-mechanism","title":"Rolling your own confirm mechanism using Promises and jQuery UI","authors":"johnnyreilly","tags":["jquery"],"hide_table_of_contents":false,"description":"Learn how to create a custom confirm dialog using jQuery UI\u2019s dialog and promises. The custom dialog is more configurable than the default\xa0`window.confirm`."},"unlisted":false,"prevItem":{"title":"Simple fading in and out using CSS transitions and classes","permalink":"/simple-fading-in-and-out-using-css-transitions"},"nextItem":{"title":"TypeScript: Don\'t forget Build Action for Implicit Referencing...","permalink":"/typescript-dont-forget-build-action-for-implicit-referencing"}},"content":"We\'re here to talk about the [confirm](https://developer.mozilla.org/en-US/docs/Web/API/Window.confirm) dialog. Or, more specifically, how we can make our own confirm dialog.\\n\\n\x3c!--truncate--\x3e\\n\\nJavaScript in the browser has had the `window.confirm` method for the longest time. This method takes a string as an argument and displays it in the form of a dialog, giving the user the option to click on either an \\"OK\\" or a \\"Cancel\\" button. If the user clicks \\"OK\\" the method returns `true`, if the user clicks \\"Cancel\\" the method returns `false`.\\n\\n`window.confirm` is wonderful in one way - it has a simple API which is easy to grok. But regardless of the browser, `window.confirm` is always as ugly as sin. Look at the first picture in this blog post; hideous. Or, put more dispassionately, it\'s not terribly configurable; want to change the button text? You can\'t. Want to change the styling of the dialog? You can\'t. You get the picture.\\n\\n## Making confirm 2.0\\n\\n[jQuery UI\'s dialog](http://jqueryui.com/dialog/#modal-confirmation) has been around for a long time. I\'ve been using it for a long time. But, if you look at the API, you\'ll see it works in a very different way to `window.confirm` \\\\- basically it\'s all about the callbacks. My intention was to create a mechanism which allowed me to prompt the user with jQuery UI\'s tried and tested dialog, but to expose it in a way that embraced the simplicity of the `window.confirm` API.\\n\\nHow to do this? Promises! To quote [Martin Fowler](http://martinfowler.com/bliki/JavascriptPromise.html) (makes you look smart when you do that):\\n\\n> _\\"In Javascript, promises are objects which represent the pending result of an asynchronous operation. You can use these to schedule further activity after the asynchronous operation has completed by supplying a callback.\\"_\\n\\nWhen we show our dialog we are in asynchronous land; waiting for the user to click \\"OK\\" or \\"Cancel\\". When they do, we need to act on their response. So if our custom confirm dialog returns a promise of a boolean (`true` when the users click \\"OK\\", `false` otherwise) then that should be exactly what we need. I\'m going to use [Q](https://github.com/kriskowal/q) for promises. (Nothing particularly special about Q - it\'s one of many [Promises / A+](https://github.com/promises-aplus/promises-spec/blob/master/implementations/index.md) compliant implementations available.)\\n\\nHere\'s my custom confirm dialog:\\n\\n```js\\n/**\\n * Show a \\"confirm\\" dialog to the user (using jQuery UI\'s dialog)\\n *\\n * @param {string} message The message to display to the user\\n * @param {string} okButtonText OPTIONAL - The OK button text, defaults to \\"Yes\\"\\n * @param {string} cancelButtonText OPTIONAL - The Cancel button text, defaults to \\"No\\"\\n * @param {string} title OPTIONAL - The title of the dialog box, defaults to \\"Confirm...\\"\\n * @returns {Q.Promise<boolean>} A promise of a boolean value\\n */\\nfunction confirmDialog(message, okButtonText, cancelButtonText, title) {\\n  okButtonText = okButtonText || \'Yes\';\\n  cancelButtonText = cancelButtonText || \'No\';\\n  title = title || \'Confirm...\';\\n\\n  var deferred = Q.defer();\\n  $(\'<div title=\\"\' + title + \'\\">\' + message + \'</div>\').dialog({\\n    modal: true,\\n    buttons: [\\n      {\\n        // The OK button\\n        text: okButtonText,\\n        click: function () {\\n          // Resolve the promise as true indicating the user clicked \\"OK\\"\\n          deferred.resolve(true);\\n          $(this).dialog(\'close\');\\n        },\\n      },\\n      {\\n        // The Cancel button\\n        text: cancelButtonText,\\n        click: function () {\\n          $(this).dialog(\'close\');\\n        },\\n      },\\n    ],\\n    close: function (event, ui) {\\n      // Destroy the jQuery UI dialog and remove it from the DOM\\n      $(this).dialog(\'destroy\').remove();\\n\\n      // If the promise has not yet been resolved (eg the user clicked the close icon)\\n      // then resolve the promise as false indicating the user did *not* click \\"OK\\"\\n      if (deferred.promise.isPending()) {\\n        deferred.resolve(false);\\n      }\\n    },\\n  });\\n\\n  return deferred.promise;\\n}\\n```\\n\\nWhat\'s happening here? Well first of all, if `okButtonText`, `cancelButtonText` or `title` have false-y values then they are initialised to defaults. Next, we create a deferred object with Q. Then we create our modal dialog using jQuery UI. There\'s a few things worth noting about this:\\n\\n- We\'re not dependent on the dialog markup being in our HTML from the off. We create a brand new element which gets added to the DOM when the dialog is created. (I draw attention to this as the jQuery UI dialog documentation doesn\'t mention that you can use this approach - and frankly I prefer it.)\\n- The \\"OK\\" and \\"Cancel\\" buttons are initialised with the string values stored in `okButtonText` and `cancelButtonText`. So by default, \\"Yes\\" and \\"No\\".\\n- If the user clicks the \\"OK\\" button then the promise is resolved with a value of `true`.\\n- If the dialog closes and the promise has not been resolved then the promise is resolved with a value of `false`. This covers people clicking on the \\"Cancel\\" button as well as closing the dialog through other means.\\n\\nFinally we return the promise from our deferred object.\\n\\n## Going from `window.confirm` to `confirmDialog`\\n\\nIt\'s very simple to move from using `window.confirm` to `confirmDialog`. Take this example:\\n\\n```js\\nif (window.confirm(\'Are you sure?\')) {\\n  // Do something\\n}\\n```\\n\\nBecomes:\\n\\n```js\\nconfirmDialog(\'Are you sure?\').then(function (confirmed) {\\n  if (confirmed) {\\n    // Do something\\n  }\\n});\\n```\\n\\nThere\'s no more to it than that.\\n\\n## And finally a demo...\\n\\nWith the JSFiddle below you can create your own custom dialogs and see the result of clicking on either the \\"OK\\" or \\"Cancel\\" buttons.\\n\\n<iframe width=\\"100%\\" height=\\"500\\" src=\\"https://jsfiddle.net/johnny_reilly/ARWL5/embedded/result,js,html,css\\" allowFullScreen=\\"allowFullScreen\\" frameBorder=\\"0\\"></iframe>"},{"id":"typescript-dont-forget-build-action-for-implicit-referencing","metadata":{"permalink":"/typescript-dont-forget-build-action-for-implicit-referencing","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2013-11-04-typescript-dont-forget-build-action-for-implicit-referencing/index.md","source":"@site/blog/2013-11-04-typescript-dont-forget-build-action-for-implicit-referencing/index.md","title":"TypeScript: Don\'t forget Build Action for Implicit Referencing...","description":"TypeScript files in Visual Studio now implicitly reference each other. This caused problems for some projects and its important to check file settings.","date":"2013-11-04T00:00:00.000Z","tags":[{"inline":false,"label":"Definitely Typed","permalink":"/tags/definitely-typed","description":"The Definitely Typed project for TypeScript type definitions"},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":1.96,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"typescript-dont-forget-build-action-for-implicit-referencing","title":"TypeScript: Don\'t forget Build Action for Implicit Referencing...","authors":"johnnyreilly","tags":["definitely typed","typescript"],"hide_table_of_contents":false,"description":"TypeScript files in Visual Studio now implicitly reference each other. This caused problems for some projects and its important to check file settings."},"unlisted":false,"prevItem":{"title":"Rolling your own confirm mechanism using Promises and jQuery UI","permalink":"/rolling-your-own-confirm-mechanism"},"nextItem":{"title":"Getting TypeScript Compile-on-Save and Continuous Integration to play nice","permalink":"/getting-typescript-compile-on-save-and-continous-integration-to-play-nice"}},"content":"As part of the [known breaking changes between 0.9 and 0.9.1](https://typescript.codeplex.com/wikipage?title=Known%20breaking%20changes%20between%200.8%20and%200.9&referringTitle=Documentation) there was this subtle but significant switch:\\n\\n\x3c!--truncate--\x3e\\n\\n> In Visual Studio, all TypeScript files in a project are considered to be referencing each other\\n>\\n> _Description:_ Previously, all TypeScript files in a project had to reference each other explicitly. With 0.9.1, they now implicitly reference all other TypeScript files in the project. For existing projects that fit multiple projects into a single projects, these will now have to be separate projects.\\n>\\n> _Reason:_ This greatly simplifies using TypeScript in the project context.\\n\\nHaving been [initially resistant](https://typescript.codeplex.com/workitem/1471) to this change I recently decided to give it a try. That is to say I started pulling out the `/// &lt;reference`\'s from my TypeScript files. However, to my surprise, pulling out these references stopped my TypeScript from compiling and killed my Intellisense. After wrestling with this for a couple of hours I finally [filed an issue on the TypeScript CodePlex site](https://typescript.codeplex.com/workitem/1855). (Because clearly the problem was with TypeScript and not how I was using it, right?)\\n\\n## Wrong!\\n\\nWhen I looked through my typing files (\\\\*.d.ts) I found that, pretty much without exception, all had a Build Action of \\"Content\\" and not \\"TypeScriptCompile\\". I went through the project and switched the files over to being \\"TypeScriptCompile\\". This resolved the issue and I was then able to pull out the remaining `/// &lt;reference` comments from the codebase (though I did have to restart Visual Studio to get the Intellisense working).\\n\\nMost, if not all, of the typing files had been pulled in from NuGet and are part of the [DefinitelyTyped](https://github.com/borisyankov/DefinitelyTyped) project on GitHub. Unfortunately, at present, when TypeScript NuGet packages are added they are added without the \\"TypeScriptCompile\\" Build Action. I was going to post an issue there and ask if it\'s possible for NuGet packages to pull in typings files as \\"TypeScriptCompile\\" from the off - fortunately a chap called Natan Vivo [already has](https://github.com/borisyankov/DefinitelyTyped/issues/1138).\\n\\nSo until this issue is resolved it\'s probably a good idea to check that your TypeScript files are set to the correct Build Action in your project. And every time you upgrade your TypeScript NuGet packages double check that you still have the correct Build Action afterwards (and to get Intellisense working in VS 2012 at least you\'ll need to close and re-open the solution as well)."},{"id":"getting-typescript-compile-on-save-and-continous-integration-to-play-nice","metadata":{"permalink":"/getting-typescript-compile-on-save-and-continous-integration-to-play-nice","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2013-10-30-getting-typescript-compile-on-save-and-continous-integration-to-play-nice/index.md","source":"@site/blog/2013-10-30-getting-typescript-compile-on-save-and-continous-integration-to-play-nice/index.md","title":"Getting TypeScript Compile-on-Save and Continuous Integration to play nice","description":"Learn how to compile TypeScript in Visual Studio without making TypeScript compilation part of the build process on the server.","date":"2013-10-30T00:00:00.000Z","tags":[{"inline":false,"label":"Azure DevOps","permalink":"/tags/azure-devops","description":"The Azure DevOps suite of tools."},{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."}],"readingTime":3.93,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"getting-typescript-compile-on-save-and-continous-integration-to-play-nice","title":"Getting TypeScript Compile-on-Save and Continuous Integration to play nice","authors":"johnnyreilly","tags":["azure devops","typescript"],"hide_table_of_contents":false,"description":"Learn how to compile TypeScript in Visual Studio without making TypeScript compilation part of the build process on the server."},"unlisted":false,"prevItem":{"title":"TypeScript: Don\'t forget Build Action for Implicit Referencing...","permalink":"/typescript-dont-forget-build-action-for-implicit-referencing"},"nextItem":{"title":"Migrating from jquery.validate.unobtrusive.js to jQuery.Validation.Unobtrusive.Native","permalink":"/migrating-from-jquery.validate.unobtrusive.js-to-jQuery.Validation.Unobtrusive.Native"}},"content":"Well sort of... Perhaps this post should more accurately called \\"How to get CI to ignore your TypeScript whilst Visual Studio still compiles it...\\"\\n\\n\x3c!--truncate--\x3e\\n\\n## Once there was Web Essentials\\n\\nWhen I first started using TypeScript, I was using it in combination with Web Essentials. Those were happy days. I saved my TS file and Web Essentials would kick off TypeScript compilation. Ah bliss. But the good times couldn\'t last forever and sure enough when version 3.0 of Web Essentials shipped it [pulled support for TypeScript](http://madskristensen.net/post/Web-Essentials-2013-Where-is-the-TypeScript-support).\\n\\nThis made me, [and others](https://typescript.codeplex.com/workitem/1616), very sad. Essentially we were given the choice between sticking with an old version of Web Essentials (2.9 - the last release before 3.0) and keeping our Compile-on-Save \\\\***or**\\\\* keeping with the latest version of Web Essentials and losing it. And since I understood that newer versions of TypeScript had differences in the compiler flags which slightly broke compatibility with WE 2.9 the latter choice seemed the most sensible...\\n\\n## But there is still Compile on Save hope!\\n\\nThe information was that we need not lose our Compile on Save. We just need to follow the instructions [here](https://typescript.codeplex.com/wikipage?title=Compile-on-Save). Or to quote them:\\n\\n> Then additionally add (or replace if you had an older PreBuild action for TypeScript) the following at the end of your project file to include TypeScript compilation in your project.\\n>\\n> ...\\n>\\n> For C#-style projects (.csproj):\\n>\\n> ```xml\\n> <PropertyGroup Condition=\\"\'$(Configuration)\' == \'Debug\'\\">\\n>     <TypeScriptTarget>ES5</TypeScriptTarget>\\n>     <TypeScriptIncludeComments>true</TypeScriptIncludeComments>\\n>     <TypeScriptSourceMap>true</TypeScriptSourceMap>\\n>   </PropertyGroup>\\n>   <PropertyGroup Condition=\\"\'$(Configuration)\' == \'Release\'\\">\\n>     <TypeScriptTarget>ES5</TypeScriptTarget>\\n>     <TypeScriptIncludeComments>false</TypeScriptIncludeComments>\\n>     <TypeScriptSourceMap>false</TypeScriptSourceMap>\\n>   </PropertyGroup>\\n>   <Import Project=\\"$(MSBuildExtensionsPath32)\\\\Microsoft\\\\VisualStudio\\\\v$(VisualStudioVersion)\\\\TypeScript\\\\Microsoft.TypeScript.targets\\" />\\n> ```\\n\\nI followed these instructions (well I had to tweak the `Import Project` location) and I was in business again. But I when I came to check my code into TFS I came unstuck. The automated build kicked off and then, in short order, kicked me:\\n\\n> ```\\n> C:\\\\Builds\\\\1\\\\MyApp\\\\MyApp Continuous Integration\\\\src\\\\MyApp\\\\MyApp.csproj (1520): The imported project \\"C:\\\\Program Files (x86)\\\\MSBuild\\\\Microsoft\\\\VisualStudio\\\\v11.0\\\\TypeScript\\\\Microsoft.TypeScript.targets\\" was not found. Confirm that the path in the <import> declaration is correct, and that the file exists on disk.\\n> C:\\\\Builds\\\\1\\\\MyApp\\\\MyApp Continuous Integration\\\\src\\\\MyApp\\\\MyApp.csproj (1520): The imported project \\"C:\\\\Program Files (x86)\\\\MSBuild\\\\Microsoft\\\\VisualStudio\\\\v11.0\\\\TypeScript\\\\Microsoft.TypeScript.targets\\" was not found. Confirm that the path in the <import> declaration is correct, and that the file exists on disk.\\n> </import></import>\\n> ```\\n\\nThat\'s right, TypeScript wasn\'t installed on the build server. And since TypeScript was now part of the build process my builds were now failing. Ouch.\\n\\n## So what now?\\n\\nI did a little digging and found [this issue report on the TypeScript CodePlex site](https://typescript.codeplex.com/workitem/1518). To quote the issue, it seemed there were 2 possible solutions to get continuous integration and typescript playing nice:\\n\\n1. Install TypeScript on the build server\\n2. Copy the required files for Microsoft.TypeScript.targets to a different source-controlled folder and change the path references in the csproj file to this folder.\\n\\n\\\\#1 wasn\'t an option for us - we couldn\'t install on the build server. And covering both #1 and #2, I wasn\'t particularly inclined to kick off builds on the build server since I was wary of [reported problems with memory leaks](https://typescript.codeplex.com/workitem/1432) etc with the TS compiler. I may feel differently later when TS is no longer in Alpha and has stabilised but it didn\'t seem like the right time.\\n\\n## A solution\\n\\nSo, to sum up, what I wanted was to be able to compile TypeScript in Visual Studio on my machine, and indeed in VS on the machine of anyone else working on the project. But I \\\\***didn\'t**\\\\* want TypeScript compilation to be part of the build process on the server.\\n\\nThe solution in the end was pretty simple - I replaced the `.csproj` changes with the code below:\\n\\n```xml\\n<PropertyGroup Condition=\\"\'$(Configuration)\' == \'Debug\'\\">\\n    <TypeScriptTarget>ES5</TypeScriptTarget>\\n    <TypeScriptRemoveComments>false</TypeScriptRemoveComments>\\n    <TypeScriptSourceMap>false</TypeScriptSourceMap>\\n    <TypeScriptModuleKind>AMD</TypeScriptModuleKind>\\n    <TypeScriptNoImplicitAny>true</TypeScriptNoImplicitAny>\\n  </PropertyGroup>\\n  <PropertyGroup Condition=\\"\'$(Configuration)\' == \'Release\'\\">\\n    <TypeScriptTarget>ES5</TypeScriptTarget>\\n    <TypeScriptRemoveComments>false</TypeScriptRemoveComments>\\n    <TypeScriptSourceMap>false</TypeScriptSourceMap>\\n    <TypeScriptModuleKind>AMD</TypeScriptModuleKind>\\n    <TypeScriptNoImplicitAny>true</TypeScriptNoImplicitAny>\\n  </PropertyGroup>\\n  <Import Project=\\"$(VSToolsPath)\\\\TypeScript\\\\Microsoft.TypeScript.targets\\" Condition=\\"Exists(\'$(VSToolsPath)\\\\TypeScript\\\\Microsoft.TypeScript.targets\')\\" />\\n```\\n\\nWhat this does is enable TypeScript compilation \\\\***only**\\\\* if TypeScript is installed. So when I\'m busy developing with Visual Studio on my machine with the plugin installed I can compile TypeScript. But when I check in the TypeScript compilation is \\\\***not**\\\\* performed on the build server. This is because TypeScript is not installed on the build server and we are only compiling if it is installed. (Just to completely labour the point.)\\n\\n## Final thoughts\\n\\nI do consider this an interim solution. As I mentioned earlier, when TypeScript has stabilised I think I\'d like TS compilation to be part of the build process. Like with any other code I think compiling on check-in to catch bugs early is an excellent idea. But I think I\'ll wait until there\'s some clearer guidance on the topic from the TypeScript team before I take this step."},{"id":"migrating-from-jquery.validate.unobtrusive.js-to-jQuery.Validation.Unobtrusive.Native","metadata":{"permalink":"/migrating-from-jquery.validate.unobtrusive.js-to-jQuery.Validation.Unobtrusive.Native","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2013-10-04-migrating-from-jquery.validate.unobtrusive.js-to-jQuery.Validation.Unobtrusive.Native/index.md","source":"@site/blog/2013-10-04-migrating-from-jquery.validate.unobtrusive.js-to-jQuery.Validation.Unobtrusive.Native/index.md","title":"Migrating from jquery.validate.unobtrusive.js to jQuery.Validation.Unobtrusive.Native","description":"Migrating from jquery.validation.unobtrusive.js to jQuery.Validation.Unobtrusive.Native is easy, with only minor tweaks to HTML and JS needed.","date":"2013-10-04T00:00:00.000Z","tags":[{"inline":false,"label":"jQuery","permalink":"/tags/jquery","description":"The jQuery library."}],"readingTime":3.715,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"migrating-from-jquery.validate.unobtrusive.js-to-jQuery.Validation.Unobtrusive.Native","title":"Migrating from jquery.validate.unobtrusive.js to jQuery.Validation.Unobtrusive.Native","authors":"johnnyreilly","tags":["jquery"],"hide_table_of_contents":false,"description":"Migrating from jquery.validation.unobtrusive.js to jQuery.Validation.Unobtrusive.Native is easy, with only minor tweaks to HTML and JS needed."},"unlisted":false,"prevItem":{"title":"Getting TypeScript Compile-on-Save and Continuous Integration to play nice","permalink":"/getting-typescript-compile-on-save-and-continous-integration-to-play-nice"},"nextItem":{"title":"Using Bootstrap Tooltips to display jQuery Validation error messages","permalink":"/using-bootstrap-tooltips-to-display"}},"content":"So, you\'re looking at [jQuery.Validation.Unobtrusive.Native](https://github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native). You\'re thinking to yourself \\"Yeah, I\'d really like to use the native unobtrusive support in jQuery Validation. But I\'ve already got this app which is using [jquery.validate.unobtrusive.js](https://www.nuget.org/packages/jQuery.Validation.Unobtrusive/) \\\\- actually how easy is switching over?\\" Well I\'m here to tell you that it\'s pretty straightforward - here\'s a walkthrough of how it might be done.\\n\\n\x3c!--truncate--\x3e\\n\\n## I need something to migrate\\n\\nSo let\'s File > New Project ourselves a new MVC 4 application using the Internet Application template. I\'ve picked this template as I know it ships with account registration / login screens in place which make use of jquery.validate.unobtrusive.js. To demo this just run the project, click the \\"Log in\\" link and then click the \\"Log in\\" button.\\n\\nWhat you\'ve just witnessed is jquery.validate.unobtrusive.js doing its thing. Both the `UserName` and `Password` properties on the `LoginModel` are decorated with the `Required` data annotation which, in the above scenario, causes the validation to be triggered on the client thanks to MVC rendering data attributes in the HTML which jquery.validate.unobtrusive.js picks up on. The question is, how can we take the log in screen above and migrate it across to to using jQuery.Validation.Unobtrusive.Native?\\n\\n## Hit me up NuGet!\\n\\nTime to dive into NuGet and install jQuery.Validation.Unobtrusive.Native. We\'ll install the MVC 4 version using this command:\\n\\n```shell\\nInstall-Package jQuery.Validation.Unobtrusive.Native.MVC4\\n```\\n\\nWhat has this done to my project? Well 2 things\\n\\n1. It\'s upgraded jQuery Validation ([jquery.validate.js](http://jqueryvalidation.org/)) from v1.10.0 (the version that is currently part of the MVC 4 template) to v1.11.1 (the latest and greatest jQuery Validation as of the time of writing)\\n2. It\'s added a reference to the jQuery.Validation.Unobtrusive.Native.MVC4 assembly, like so:\\n\\nIn case you were wondering, doing this hasn\'t broken the existing jquery.validate.unobtrusive.js - if you head back to the Log in screen you\'ll still see the same behaviour as before.\\n\\n## Migrating...\\n\\nWe need to switch our TextBox and Password helpers over to using jQuery.Validation.Unobtrusive.Native, which we achieve by simply passing a second argument of `true` to `useNativeUnobtrusiveAttributes`. So we go from this:\\n\\n```cs\\n// ...\\n@Html.TextBoxFor(m => m.UserName)\\n// ...\\n@Html.PasswordFor(m => m.Password)\\n// ...\\n```\\n\\nTo this:\\n\\n```cs\\n// ...\\n@Html.TextBoxFor(m => m.UserName, true)\\n// ...\\n@Html.PasswordFor(m => m.Password, true)\\n// ...\\n```\\n\\nWith these minor tweaks in place the natively supported jQuery Validation data attributes will be rendered into the textbox / password elements instead of the jquery.validate.unobtrusive.js ones.\\n\\nNext lets do the JavaScript. If you take a look at the bottom of the `Login.cshtml` view you\'ll see this:\\n\\n```cs\\n@section Scripts {\\n    @Scripts.Render(\\"~/bundles/jqueryval\\")\\n}\\n```\\n\\nWhich renders the following scripts:\\n\\n```html\\n<script src=\\"/Scripts/jquery.unobtrusive-ajax.js\\"><\/script>\\n<script src=\\"/Scripts/jquery.validate.js\\"><\/script>\\n<script src=\\"/Scripts/jquery.validate.unobtrusive.js\\"><\/script>\\n```\\n\\nIn our brave new world we\'re only going to need jquery.validate.js - so let\'s create ourselves a new bundle in `BundleConfig.cs` which only contains that single file:\\n\\n```cs\\nbundles.Add(new ScriptBundle(\\"~/bundles/jqueryvalnative\\")\\n    .Include(\\"~/Scripts/jquery.validate.js\\"));\\n```\\n\\nTo finish off our migrated screen we need to do 2 things. First we need to switch over the `Login.cshtml` view to only render the jquery.validate.js script (in the form of our new bundle). Secondly, the other thing that jquery.validate.unobtrusive.js did was to trigger validation for the current form. So we need to do that ourselves now. So our finished Scripts section looks like this:\\n\\n```html\\n@section Scripts { @Scripts.Render(\\"~/bundles/jqueryvalnative\\")\\n<script>\\n  $(\'form\').validate();\\n<\/script>\\n}\\n```\\n\\nWhich renders the following script:\\n\\n```html\\n<script src=\\"/Scripts/jquery.validate.js\\"><\/script>\\n<script>\\n  $(\'form\').validate();\\n<\/script>\\n```\\n\\nAnd, pretty much, that\'s it. If you run the app now and go to the Log in screen and try to log in without credentials.\\n\\nWhich is functionally exactly the same as previously. The eagle eyed will notice some styling differences but that\'s all it comes down to really; style. And if you were so inclined you could easily style this up as you liked using CSS and the options you can pass to jQuery Validation (in fact a quick rummage through jquery.validate.unobtrusive.js should give you everything you need).\\n\\n## Rounding off\\n\\nBefore I sign off I\'d like to illustrate how little we\'ve had to change the code to start using jQuery.Validation.Unobtrusive.Native.\\n\\nAs you see, it takes very little effort to migrate from one approach to the other. And it\'s \\\\***your**\\\\* choice. If you want to have one screen that uses jQuery.Validation.Unobtrusive.Native and one screen that uses jquery.validation.unobtrusive.js then you can! Including jQuery.Validation.Unobtrusive.Native in your project gives you the **option** to use it. It doesn\'t force you to, you can do so as you need to and when you want to. It\'s down to you."},{"id":"using-bootstrap-tooltips-to-display","metadata":{"permalink":"/using-bootstrap-tooltips-to-display","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2013-08-17-using-bootstrap-tooltips-to-display/index.md","source":"@site/blog/2013-08-17-using-bootstrap-tooltips-to-display/index.md","title":"Using Bootstrap Tooltips to display jQuery Validation error messages","description":"Using tooltips can be a better approach than displaying validation messages next to the element being validated in jQuery Validation.","date":"2013-08-17T00:00:00.000Z","tags":[{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."},{"inline":false,"label":"jQuery","permalink":"/tags/jquery","description":"The jQuery library."}],"readingTime":2.855,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"using-bootstrap-tooltips-to-display","title":"Using Bootstrap Tooltips to display jQuery Validation error messages","authors":"johnnyreilly","tags":["javascript","jquery"],"hide_table_of_contents":false,"description":"Using tooltips can be a better approach than displaying validation messages next to the element being validated in jQuery Validation."},"unlisted":false,"prevItem":{"title":"Migrating from jquery.validate.unobtrusive.js to jQuery.Validation.Unobtrusive.Native","permalink":"/migrating-from-jquery.validate.unobtrusive.js-to-jQuery.Validation.Unobtrusive.Native"},"nextItem":{"title":"Announcing jQuery Validation Unobtrusive Native...","permalink":"/announcing-jquery-validation"}},"content":"I love jQuery Validation. I was recently putting together a screen which had a lot of different bits of validation going on. And the default jQuery Validation approach of displaying the validation messages next to the element being validated wasn\'t working for me. That is to say, because of the amount of elements on the form, the appearance of validation messages was really making a mess of the presentation. So what to do?\\n\\n\x3c!--truncate--\x3e\\n\\n## Tooltips to the rescue!\\n\\nI was chatting to [Marc Talary](https://plus.google.com/u/0/116859810359377785616/posts) about this and he had the bright idea of using tooltips to display the error messages. Tooltips would allow the existing presentation of the form to remain as is whilst still displaying the messages to the users. Brilliant idea!\\n\\nAfter a certain amount of fiddling I came up with a fairly solid mechanism for getting jQuery Validation to display error messages as tooltips which I\'ll share here. It\'s worth saying that for the application that Marc and I were working on we already had [jQuery UI](http://jqueryui.com/) in place and so we decided to use the [jQuery UI tooltip](http://jqueryui.com/tooltip/). This example will use the [Bootstrap tooltip](http://getbootstrap.com/javascript/#tooltips) instead. As much as anything else this demonstrates that you could swap out the tooltip mechanism here with any of your choosing.\\n\\n<iframe src=\\"https://htmlpreview.github.io/?https://gist.github.com/johnnyreilly/5867188/raw/2543a12fbd5c0aaad1da6793b7a7437492be3baf/DemoTooltip.html\\" width=\\"100%\\" height=\\"350\\"></iframe>\\n\\nBeautiful isn\'t it? Now look at the source:\\n\\n```html\\n<!doctype html>\\n<html lang=\\"en\\">\\n  <head>\\n    <meta charset=\\"utf-8\\" />\\n    <link\\n      href=\\"//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.min.css\\"\\n      rel=\\"stylesheet\\"\\n    />\\n    <style>\\n      form {\\n        padding: 10px;\\n      }\\n      .error {\\n        border: 1px solid #b94a48 !important;\\n        background-color: #fee !important;\\n      }\\n    </style>\\n  </head>\\n  <body>\\n    <form>\\n      <div class=\\"row\\">\\n        <label for=\\"RequiredDateDemo\\"\\n          >A date is required (eg \\"15 June 2012\\"):</label\\n        >\\n        <input\\n          data-msg-date=\\"The field RequiredDateDemo must be a date.\\"\\n          data-msg-required=\\"The RequiredDateDemo field is required.\\"\\n          data-rule-date=\\"true\\"\\n          data-rule-required=\\"true\\"\\n          id=\\"RequiredDateDemo\\"\\n          name=\\"RequiredDateDemo\\"\\n          type=\\"text\\"\\n          value=\\"\\"\\n        />\\n      </div>\\n\\n      <div class=\\"row\\">\\n        <label for=\\"StringLengthAndRequiredDemo\\"\\n          >A string is required between 5 and 10 characters long:</label\\n        >\\n        <input\\n          data-msg-maxlength=\\"The field StringLengthAndRequiredDemo must be a string with a minimum length of 5 and a maximum length of 10.\\"\\n          data-msg-minlength=\\"The field StringLengthAndRequiredDemo must be a string with a minimum length of 5 and a maximum length of 10.\\"\\n          data-msg-required=\\"The StringLengthAndRequiredDemo field is required.\\"\\n          data-rule-maxlength=\\"10\\"\\n          data-rule-minlength=\\"5\\"\\n          data-rule-required=\\"true\\"\\n          id=\\"StringLengthAndRequiredDemo\\"\\n          name=\\"StringLengthAndRequiredDemo\\"\\n          type=\\"text\\"\\n          value=\\"\\"\\n        />\\n      </div>\\n\\n      <div class=\\"row\\">\\n        <label for=\\"RangeAndNumberDemo\\"\\n          >Must be a number between -20 and 40:</label\\n        >\\n        <input\\n          data-msg-number=\\"The field RangeAndNumberDemo must be a number.\\"\\n          data-msg-range=\\"The field RangeAndNumberDemo must be between -20 and 40.\\"\\n          data-rule-number=\\"true\\"\\n          data-rule-range=\\"[-20,40]\\"\\n          id=\\"RangeAndNumberDemo\\"\\n          name=\\"RangeAndNumberDemo\\"\\n          type=\\"text\\"\\n          value=\\"-21\\"\\n        />\\n      </div>\\n\\n      <div class=\\"row\\">\\n        <label for=\\"RangeAndNumberDemo\\">An option must be selected:</label>\\n        <select\\n          data-msg-required=\\"The DropDownRequiredDemo field is required.\\"\\n          data-rule-required=\\"true\\"\\n          id=\\"DropDownRequiredDemo\\"\\n          name=\\"DropDownRequiredDemo\\"\\n        >\\n          <option value=\\"\\">Please select</option>\\n          <option value=\\"An Option\\">An Option</option>\\n        </select>\\n      </div>\\n\\n      <div class=\\"row\\">\\n        <button type=\\"submit\\">Validate</button>\\n      </div>\\n    </form>\\n\\n    <script\\n      src=\\"//ajax.aspnetcdn.com/ajax/jQuery/jquery-1.9.1.js\\"\\n      type=\\"text/javascript\\"\\n    ><\/script>\\n    <script\\n      src=\\"//ajax.aspnetcdn.com/ajax/jQuery.validate/1.11.1/jquery.validate.js\\"\\n      type=\\"text/javascript\\"\\n    ><\/script>\\n    <script src=\\"//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js\\"><\/script>\\n    <script type=\\"text/javascript\\">\\n      $(\'form\').validate({\\n        showErrors: function (errorMap, errorList) {\\n          // Clean up any tooltips for valid elements\\n          $.each(this.validElements(), function (index, element) {\\n            var $element = $(element);\\n\\n            $element\\n              .data(\'title\', \'\') // Clear the title - there is no error associated anymore\\n              .removeClass(\'error\')\\n              .tooltip(\'destroy\');\\n          });\\n\\n          // Create new tooltips for invalid elements\\n          $.each(errorList, function (index, error) {\\n            var $element = $(error.element);\\n\\n            $element\\n              .tooltip(\'destroy\') // Destroy any pre-existing tooltip so we can repopulate with new tooltip content\\n              .data(\'title\', error.message)\\n              .addClass(\'error\')\\n              .tooltip(); // Create a new tooltip based on the error messsage we just set in the title\\n          });\\n        },\\n\\n        submitHandler: function (form) {\\n          alert(\'This is a valid form!\');\\n        },\\n      });\\n    <\/script>\\n  </body>\\n</html>\\n```\\n\\nAll the magic is in the JavaScript, specifically the `showErrors` function that\'s passed as an option to jQuery Validation. Enjoy!"},{"id":"announcing-jquery-validation","metadata":{"permalink":"/announcing-jquery-validation","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2013-08-08-announcing-jquery-validation/index.md","source":"@site/blog/2013-08-08-announcing-jquery-validation/index.md","title":"Announcing jQuery Validation Unobtrusive Native...","description":"jQuery Validation Unobtrusive Native bridges data attributes and jQuery Validations native support. The ASP.Net MVC HTML extension is available on GitHub.","date":"2013-08-08T00:00:00.000Z","tags":[],"readingTime":2.295,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"announcing-jquery-validation","title":"Announcing jQuery Validation Unobtrusive Native...","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false,"description":"jQuery Validation Unobtrusive Native bridges data attributes and jQuery Validations native support. The ASP.Net MVC HTML extension is available on GitHub."},"unlisted":false,"prevItem":{"title":"Using Bootstrap Tooltips to display jQuery Validation error messages","permalink":"/using-bootstrap-tooltips-to-display"},"nextItem":{"title":"How I\'m Using Cassette part 3:Cassette and TypeScript Integration","permalink":"/how-im-using-cassette-part-3-typescript"}},"content":"I\'ve been busy working on an open source project called **[jQuery Validation Unobtrusive Native](https://github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native)**. [To see it in action take a look here](https://johnnyreilly.github.io/jQuery.Validation.Unobtrusive.Native/).\\n\\n\x3c!--truncate--\x3e\\n\\n## A Little Background\\n\\nI noticed a little while ago that jQuery Validation was now providing native support for validation driven by HTML 5 data attributes. As you may be aware, Microsoft shipped [jquery.validate.unobtrusive.js](http://bradwilson.typepad.com/blog/2010/10/mvc3-unobtrusive-validation.html) back with MVC 3. ([I have written about it before.](../2012-08-06-jquery-unobtrusive-validation/index.md)) It provided a way to apply data model validations to the client side using a combination of jQuery Validation and HTML 5 data attributes.\\n\\nThe principal of this was and is fantastic. But since that time the jQuery Validation project has implemented its own support for driving validation unobtrusively (shipping with [jQuery Validation 1.11.0](http://jquery.bassistance.de/validate/changelog.txt)). I\'ve been looking at a way to directly use the native support instead of jquery.validate.unobtrusive.js.\\n\\n## So... What is jQuery Validation Unobtrusive Native?\\n\\njQuery Validation Unobtrusive Native is a collection of ASP.Net MVC HTML helper extensions. These make use of jQuery Validation\'s native support for validation driven by HTML 5 data attributes. The advantages of the native support over jquery.validate.unobtrusive.js are:\\n\\n- Dynamically created form elements are parsed automatically. jquery.validate.unobtrusive.js does not support this whilst jQuery Validation does. [Take a look at a demo using Knockout.](http://johnnyreilly.github.io/jQuery.Validation.Unobtrusive.Native/AdvancedDemo/Knockout.html)\\n- jquery.validate.unobtrusive.js restricts how you use jQuery Validation. If you want to use showErrors or something similar then you may find that you need to go native (or at least you may find that significantly easier than working with the jquery.validate.unobtrusive.js defaults)...\\n- Send less code to your browser, make your browser to do less work and even get a (marginal) performance benefit .\\n\\nThis project intends to be a bridge between MVC\'s inbuilt support for driving validation from data attributes and jQuery Validation\'s native support for the same. This is achieved by hooking into the MVC data attribute creation mechanism and using it to generate the data attributes natively supported by jQuery Validation.\\n\\n## Future Plans\\n\\nSo far the basic set of the HtmlHelpers and their associated unobtrusive mappings have been implemented. If any have been missed then let me know. As time goes by I intend to:\\n\\n- fill in any missing gaps there may be\\n- maintain MVC 3, 4 (and when the time comes 5+) versions of this on Nuget\\n- not all data annotations generate client data attributes - if it makes sense I may look to implement some of these where it seems sensible. (eg the [MinLengthAttribute](http://msdn.microsoft.com/en-us/library/system.componentmodel.dataannotations.minlengthattribute.aspx) annotation could be mapped to [minlength](http://jqueryvalidation.org/minlength-method/) validation...)\\n- get the unit test coverage to a good level and finally (and perhaps most importantly)\\n- create some really useful [demos and documentation](http://johnnyreilly.github.io/jQuery.Validation.Unobtrusive.Native/Demo.html).\\n\\nHelp is appreciated so feel free to pitch in! You can find the project on GitHub [here](http://github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native)..."},{"id":"how-im-using-cassette-part-3-typescript","metadata":{"permalink":"/how-im-using-cassette-part-3-typescript","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2013-07-06-how-im-using-cassette-part-3-typescript/index.md","source":"@site/blog/2013-07-06-how-im-using-cassette-part-3-typescript/index.md","title":"How I\'m Using Cassette part 3:Cassette and TypeScript Integration","description":"The modern web is JavaScript. There\'s no two ways about it. HTML 5 has new CSS, new HTML but the most important aspect of it from an application development point of view is JavaScript. It\'s the engine. Without it HTML 5 wouldn\'t be the exciting application platform that it is. Half the posts on Hacker News would vanish.","date":"2013-07-06T00:00:00.000Z","tags":[{"inline":false,"label":"TypeScript","permalink":"/tags/typescript","description":"The TypeScript programming language."},{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."},{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":6.065,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"how-im-using-cassette-part-3-typescript","title":"How I\'m Using Cassette part 3:Cassette and TypeScript Integration","authors":"johnnyreilly","tags":["typescript","javascript","asp.net"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Announcing jQuery Validation Unobtrusive Native...","permalink":"/announcing-jquery-validation"},"nextItem":{"title":"jQuery Validation - Native Unobtrusive Validation Support!","permalink":"/jquery-validate-native-unobtrusive-validation"}},"content":"The modern web is JavaScript. There\'s no two ways about it. HTML 5 has new CSS, new HTML but the most important aspect of it from an application development point of view is JavaScript. It\'s the engine. Without it HTML 5 wouldn\'t be the exciting application platform that it is. Half the posts on [Hacker News](https://news.ycombinator.com/) would vanish.\\n\\n\x3c!--truncate--\x3e\\n\\nIt\'s easy to break a JavaScript application. One false keypress and you can mysteriously turn a fully functioning app into toast. And not know why. There\'s tools you can use to help yourself - [JSHint / JSLint](../2012-04-23-jshint-customising-your-hurt-feelings/index.md) but whilst these make error detection a little easier it remains very easy to shoot yourself in the foot with JavaScript. Because of this I\'ve come to really rather love [TypeScript](http://www.typescriptlang.org/). If you didn\'t already know, TypeScript can be summed up as JavaScript with optional static typing. It\'s a **_superset_** of JavaScript - JavaScript with go-faster stripes. When run through the compiler TypeScript is [transpiled](https://en.wikipedia.org/wiki/Source-to-source_compiler) into JavaScript. And importantly, if you have bugs in your code, the compiler should catch them at this point and let you know.\\n\\nNow very few of us are working on greenfield applications. Most of us have existing applications to maintain and support. Happily, TypeScript fits very well with this purely because TypeScript is a superset of JavaScript. That is to say: all JavaScript is valid TypeScript in the same way that all CSS is valid [LESS](http://lesscss.org/). This means that you can take an existing `.js` file, rename it to have a `.ts` suffix, run the TypeScript compiler over it and out will pop your JavaScript file just as it was before. You\'re then free to enrich your TypeScript file with the relevant type annotations at your own pace. Increasing the robustness of your codebase is a choice left to you.\\n\\nThe project I am working on has recently started to incorporate TypeScript. It\'s an ASP.Net MVC 4 application which makes use of [Knockout](http://knockoutjs.com/). The reason we started to incorporate TypeScript is because certain parts of the app, particularly the Knockout parts, were becoming more complex. This complexity wasn\'t really an issue when we were writing the relevant JavaScript. However, when it came to refactoring and handing files from one team member to another we realised it was very easy to introduce bugs into the codebase, particularly around the JavaScript. Hence TypeScript.\\n\\n## Cassette and TypeScript\\n\\nEnough of the pre-amble. The project was making use of Cassette for serving up its CSS and JavaScript. Because Cassette rocks. One of the reasons we use it is that we\'re making extensive use of [Cassette\'s ability to serve scripts in dependency order](../2013-06-06-how-im-using-cassette-part-2/index.md). So if we were to move to using TypeScript it was important that TypeScript and Cassette would play well together.\\n\\nI\'m happy to report that Cassettes and TypeScript do work well together, but there are a few things that you need to get up and running. Or, to be a little clearer, if you want to make use of Cassette\'s in-file Asset Referencing then you\'ll need to follow these steps. If you don\'t need Asset Referencing then you\'ll be fine using Cassette with TypeScript generated JavaScript as is \\\\***provided**\\\\* you ensure the TypeScript compiler is not preserving comments in the generated JavaScript.\\n\\n## The Fly in the Ointment: Asset References\\n\\nTypeScript is designed to allow you to break up your application into modules. However, the referencing mechanism which allows you to reference one TypeScript file / module from another is exactly the same as the existing Visual Studio XML reference comments mechanism that was originally introduced to drive JavaScript Intellisense in Visual Studio. To quote the [TypeScript spec](http://www.typescriptlang.org/Content/TypeScript%20Language%20Specification.pdf):\\n\\n- _A comment of the form /// <reference path=\\"\u2026\\"/> adds a dependency on the source file specified in the path argument. The path is resolved relative to the directory of the containing source file._\\n- _An external import declaration that specifies a relative external module name (section 11.2.1) resolves the name relative to the directory of the containing source file. If a source file with the resulting path and file extension \u2018.ts\u2019 exists, that file is added as a dependency. Otherwise, if a source file with the resulting path and file extension \u2018.d.ts\u2019 exists, that file is added as a dependency._\\n\\nThe problem is that [Cassette \\\\***also**\\\\* supports Visual Studio XML reference comments to drive Asset References](http://getcassette.net/documentation/v1/AssetReferences). The upshot of this is, that Cassette will parse the `/// &lt;reference path=\\"*.ts\\"/&gt;`s and will attempt to serve up the TypeScript files in the browser... Calamity!\\n\\n## Pulling the Fly from the Ointment\\n\\nAgain I\'m going to take the demo from last time ([the References branch of my CassetteDemo project](https://github.com/johnnyreilly/CassetteDemo/tree/References)) and build on top of it. First of all, we need to update the Cassette package. This is because to get Cassette working with TypeScript you need to be running at least Cassette 2.1. So let\'s let NuGet do it\'s thing:\\n\\n`Update-Package Cassette.Aspnet`\\n\\nAnd whilst we\'re at it let\'s grab the jQuery TypeScript typings - we\'ll need them later:\\n\\n`Install-Package jquery.TypeScript.DefinitelyTyped`\\n\\nNow we need to add a couple of classes to the project. First of all this:\\n\\n```cs\\nusing System;\\nusing Cassette.Scripts;\\n\\nnamespace CassetteDemo\\n{\\n    public class ParseJavaScriptNotTypeScriptReferences : ParseJavaScriptReferences\\n    {\\n        protected override bool ShouldAddReference(string referencePath)\\n        {\\n            return !referencePath.EndsWith(\\".ts\\", StringComparison.OrdinalIgnoreCase); // Will exclude TypeScript files from being served\\n        }\\n    }\\n}\\n```\\n\\nWhich subclasses `ParseJavaScriptReferences` and ensures TypeScript files are excluded when JavaScript references are being parsed. And to make sure that Cassette makes use of `ParseJavaScriptNotTypeScriptReferences` in place of `ParseJavaScriptReferences` we need this:\\n\\n```cs\\nusing Cassette.BundleProcessing;\\nusing Cassette.Scripts;\\n\\nnamespace CassetteDemo\\n{\\n    public class InsertIntoPipelineParseJavaScriptNotTypeScriptReferences : IBundlePipelineModifier<ScriptBundle>\\n    {\\n        public IBundlePipeline<ScriptBundle> Modify(IBundlePipeline<ScriptBundle> pipeline)\\n        {\\n            var positionOfJavaScriptReferenceParser = pipeline.IndexOf<ParseJavaScriptReferences>();\\n\\n            pipeline.RemoveAt(positionOfJavaScriptReferenceParser);\\n            pipeline.Insert<ParseJavaScriptNotTypeScriptReferences>(positionOfJavaScriptReferenceParser);\\n            return pipeline;\\n        }\\n    }\\n}\\n```\\n\\nNow we\'re in a position to use TypeScript with Cassette. To demonstrate this let\'s take the `Index.js` and rename it to `Index.ts`. And now it\'s TypeScript. However before it can compile it needs to know what jQuery is - so we drag in the jQuery typings from [Definitely Typed](http://github.com/borisyankov/DefinitelyTyped). And now it can compile from this:\\n\\n```ts\\n/// <reference path=\\"../../typings/jquery/jquery.d.ts\\" />\\n// @reference ~/bundles/core\\n\\n$(document).ready(function () {\\n  var $body = $(\'#body\');\\n\\n  $body.fadeOut(1000, function () {\\n    $body\\n      .html(\\n        \'<div style=\\"width: 150px; margin: 0 auto;\\">I made it all go away...</div>\',\\n      )\\n      .fadeIn();\\n  });\\n});\\n```\\n\\nTo this: (Please note that I get the TypeScript compiler to preserve my comments in order that I can continue to use Cassettes Asset Referencing)\\n\\n```js\\n/// <reference path=\\"../../typings/jquery/jquery.d.ts\\" />\\n// @reference ~/bundles/core\\n$(document).ready(function () {\\n  var $body = $(\'#body\');\\n\\n  $body.fadeOut(1000, function () {\\n    $body\\n      .html(\\n        \'<div style=\\"width: 150px; margin: 0 auto;\\">I made it all go away...</div>\',\\n      )\\n      .fadeIn();\\n  });\\n});\\n//@ sourceMappingURL=Index.js.map\\n```\\n\\nAs you can see the output JavaScript has both the TypeScript and the Cassette references in place. However thanks to `ParseJavaScriptNotTypeScriptReferences` those TypeScript references will be ignored by Cassette.\\n\\nSo that\'s it - we\'re home free. Before I finish off I\'d like to say thanks to Cassette\'s [Andrew Davey](http://twitter.com/andrewdavey) who [set me on the right path](https://groups.google.com/forum/?fromgroups=#!topic/cassette/SM3Rxh48D7Q) when trying to work out how to do this. A thousand thank yous Andrew!\\n\\nAnd finally, again as last time you can see what I\'ve done in this post by just looking at the repository on [GitHub](https://github.com/johnnyreilly/CassetteDemo/tree/TypeScript). The changes I made are on the TypeScript branch of that particular repository."},{"id":"jquery-validate-native-unobtrusive-validation","metadata":{"permalink":"/jquery-validate-native-unobtrusive-validation","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2013-06-26-jquery-validate-native-unobtrusive-validation/index.md","source":"@site/blog/2013-06-26-jquery-validate-native-unobtrusive-validation/index.md","title":"jQuery Validation - Native Unobtrusive Validation Support!","description":"Use HTML5 data attributes with jQuery Validation to simplify code and achieve validation unobtrusively. Ideal for dynamically added DOM elements.","date":"2013-06-26T00:00:00.000Z","tags":[{"inline":false,"label":"jQuery","permalink":"/tags/jquery","description":"The jQuery library."}],"readingTime":3.39,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"jquery-validate-native-unobtrusive-validation","title":"jQuery Validation - Native Unobtrusive Validation Support!","authors":"johnnyreilly","tags":["jquery"],"hide_table_of_contents":false,"description":"Use HTML5 data attributes with jQuery Validation to simplify code and achieve validation unobtrusively. Ideal for dynamically added DOM elements."},"unlisted":false,"prevItem":{"title":"How I\'m Using Cassette part 3:Cassette and TypeScript Integration","permalink":"/how-im-using-cassette-part-3-typescript"},"nextItem":{"title":"How I\'m Using Cassette part 2:Get Cassette to Serve Scripts in Dependency Order","permalink":"/how-im-using-cassette-part-2"}},"content":"Did you know that jQuery Validation natively supports the use of [HTML 5 data attributes](http://ejohn.org/blog/html-5-data-attributes/) to drive validation unobtrusively? Neither did I - I haven\'t seen any documentation for it. However, I was reading the [jQuery Validation test suite](https://github.com/jzaefferer/jquery-validation/blob/master/test/index.html) and that\'s what I spotted being used in some of the tests.\\n\\n\x3c!--truncate--\x3e\\n\\nI was quite keen to give it a try as I\'ve found the Microsoft produced [unobtrusive extensions](http://nuget.org/packages/jQuery.Validation.Unobtrusive/) both fantastic and frustrating in nearly equal measure. Fantastic because they work and they\'re [integrated nicely with MVC](../2012-08-06-jquery-unobtrusive-validation/index.md). Frustrating, because they don\'t allow you do all the things that jQuery Validate in the raw does.\\n\\nSo when I realised that there was native alternative available I was delighted. Enough with the fine words - what we want is a demo:\\n\\n<iframe src=\\"https://htmlpreview.github.io/?http://gist.github.com/johnnyreilly/5867188/raw/272b1b42f4773fe6df843550b3e3d457013522a8/Demo.html\\" width=\\"100%\\" height=\\"575\\"></iframe>\\n\\nNot particularly exciting? Not noticably different to any other jQuery Validate demo you\'ve ever seen? Fair enough. Now look at the source:\\n\\n```html\\n<!doctype html>\\n<html lang=\\"en\\">\\n  <head>\\n    <meta charset=\\"utf-8\\" />\\n    <link\\n      href=\\"http://netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.min.css\\"\\n      rel=\\"stylesheet\\"\\n    />\\n    <style>\\n      form {\\n        padding: 10px;\\n      }\\n      .error {\\n        color: red;\\n      }\\n    </style>\\n  </head>\\n  <body>\\n    <form>\\n      <label for=\\"RequiredDateDemo\\"\\n        >A date is required (eg \\"15 June 2012\\"):</label\\n      >\\n      <input\\n        data-msg-date=\\"The field RequiredDateDemo must be a date.\\"\\n        data-msg-required=\\"The RequiredDateDemo field is required.\\"\\n        data-rule-date=\\"true\\"\\n        data-rule-required=\\"true\\"\\n        id=\\"RequiredDateDemo\\"\\n        name=\\"RequiredDateDemo\\"\\n        type=\\"text\\"\\n        value=\\"\\"\\n      />\\n\\n      <hr />\\n\\n      <label for=\\"StringLengthAndRequiredDemo\\"\\n        >A string is required between 5 and 10 characters long:</label\\n      >\\n      <input\\n        data-msg-maxlength=\\"The field StringLengthAndRequiredDemo must be a string with a minimum length of 5 and a maximum length of 10.\\"\\n        data-msg-minlength=\\"The field StringLengthAndRequiredDemo must be a string with a minimum length of 5 and a maximum length of 10.\\"\\n        data-msg-required=\\"The StringLengthAndRequiredDemo field is required.\\"\\n        data-rule-maxlength=\\"10\\"\\n        data-rule-minlength=\\"5\\"\\n        data-rule-required=\\"true\\"\\n        id=\\"StringLengthAndRequiredDemo\\"\\n        name=\\"StringLengthAndRequiredDemo\\"\\n        type=\\"text\\"\\n        value=\\"\\"\\n      />\\n\\n      <hr />\\n\\n      <label for=\\"RangeAndNumberDemo\\"\\n        >Must be a number between -20 and 40:</label\\n      >\\n      <input\\n        data-msg-number=\\"The field RangeAndNumberDemo must be a number.\\"\\n        data-msg-range=\\"The field RangeAndNumberDemo must be between -20 and 40.\\"\\n        data-rule-number=\\"true\\"\\n        data-rule-range=\\"[-20,40]\\"\\n        id=\\"RangeAndNumberDemo\\"\\n        name=\\"RangeAndNumberDemo\\"\\n        type=\\"text\\"\\n        value=\\"-21\\"\\n      />\\n\\n      <hr />\\n\\n      <label for=\\"RangeAndNumberDemo\\">An option must be selected:</label>\\n      <select\\n        data-msg-required=\\"The DropDownRequiredDemo field is required.\\"\\n        data-rule-required=\\"true\\"\\n        id=\\"DropDownRequiredDemo\\"\\n        name=\\"DropDownRequiredDemo\\"\\n      >\\n        <option value=\\"\\">Please select</option>\\n        <option value=\\"An Option\\">An Option</option>\\n      </select>\\n\\n      <hr />\\n\\n      <button type=\\"submit\\">Validate</button>\\n    </form>\\n\\n    <script\\n      src=\\"http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.9.1.js\\"\\n      type=\\"text/javascript\\"\\n    ><\/script>\\n    <script\\n      src=\\"http://ajax.aspnetcdn.com/ajax/jQuery.validate/1.11.1/jquery.validate.js\\"\\n      type=\\"text/javascript\\"\\n    ><\/script>\\n    <script type=\\"text/javascript\\">\\n      var $form = $(\'form\');\\n      $form.validate();\\n      $form.submit(function (event) {\\n        if ($form.validate().valid()) {\\n          event.preventDefault();\\n\\n          alert(\'Valid!\');\\n        }\\n      });\\n    <\/script>\\n  </body>\\n</html>\\n```\\n\\nDo you see what I see? Data attributes (both `data-rule-*` and `data-msg-*`s) being used to drive the validation unobtrusively! And if you look at the JavaScript files referenced you will see \\\\***no sign**\\\\* of `jquery.validate.unobtrusive.js` \\\\- this is all raw jQuery Validate. Nothing else.\\n\\n## Why is this useful?\\n\\nFirst of all, I\'m of the opinion that it makes intuitive sense to have the validation information relevant to various DOM elements stored directly with those DOM elements. There will be occasions where you may not want to use this approach but, in the main, I think it\'s very sensible. It saves you bouncing back and forth between your HTML and your JavaScript and it means when you read the HTML you know there and then what validation applies to your form.\\n\\nI think this particularly applies when it comes to adding elements to the DOM dynamically. If I use data attributes to drive my validation and I dynamically add elements then jQuery Validate will parse the validation rules for me. I won\'t have to subsequently apply validation to those new elements once they\'ve been added to the DOM. 1 step instead of 2. It makes for simpler code and that\'s always a win.\\n\\n## Wrapping up\\n\\nFor myself I\'m in the early stages of experimenting with this but I thought it might be good to get something out there to show how this works. If anyone knows of any official documentation for this please do let me know - I\'d love to have a read of it. Maybe it\'s been out there all along and it\'s just my Googling powers are inadequate.\\n\\n## Updated 09/08/2012\\n\\nIf you\'re using ASP.Net MVC 3+ and this post has been of interest to you then you might want to take a look at [this](../2013-08-08-announcing-jquery-validation/index.md)."},{"id":"how-im-using-cassette-part-2","metadata":{"permalink":"/how-im-using-cassette-part-2","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2013-06-06-how-im-using-cassette-part-2/index.md","source":"@site/blog/2013-06-06-how-im-using-cassette-part-2/index.md","title":"How I\'m Using Cassette part 2:Get Cassette to Serve Scripts in Dependency Order","description":"Cassettes script dependency order feature is the most useful, managing script order manually is tedious. Use server-side or JavaScript asset references.","date":"2013-06-06T00:00:00.000Z","tags":[{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":7.445,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"how-im-using-cassette-part-2","title":"How I\'m Using Cassette part 2:Get Cassette to Serve Scripts in Dependency Order","authors":"johnnyreilly","tags":["asp.net"],"hide_table_of_contents":false,"description":"Cassettes script dependency order feature is the most useful, managing script order manually is tedious. Use server-side or JavaScript asset references."},"unlisted":false,"prevItem":{"title":"jQuery Validation - Native Unobtrusive Validation Support!","permalink":"/jquery-validate-native-unobtrusive-validation"},"nextItem":{"title":"How I\'m Using Cassette part 1:Getting Up and Running","permalink":"/how-im-using-cassette"}},"content":"[Last time](../2013-05-04-how-im-using-cassette/index.md) I wrote about Cassette I was talking about how to generally get up and running. How to use Cassette within an ASP.Net MVC project. What I want to write about now is (in my eyes) the most useful feature of Cassette by a country mile. This is Cassettes ability to ensure scripts are served in dependency order.\\n\\n\x3c!--truncate--\x3e\\n\\n## Why does this matter?\\n\\nYou might well ask. If we go back 10 years or so then really this wasn\'t a problem. No-one was doing a great deal with JavaScript. And if they did anything it tended to be code snippets in amongst the HTML; nothing adventurous. But unless you\'ve had your head in the sand for the last 3 years then you will have clearly noticed that JavaScript is in rude health and being used for all kinds of things you\'d never have imagined. In fact some would have it that it\'s the [assembly language of the web](http://www.hanselman.com/blog/JavaScriptisAssemblyLanguagefortheWebPart2MadnessorjustInsanity.aspx).\\n\\nFor my part, I\'ve been doing more and more with JavaScript. And as I do more and more with it I seek to modularise my code; ([like any good developer would](http://en.wikipedia.org/wiki/Separation_of_concerns)) breaking it up into discrete areas of functionality. I aim to only serve up the JavaScript that I need on a given page. And that would be all well and good but for one of the languages shortcomings. Modules. JavaScript doesn\'t yet have a good module loading story to tell. (Apparently one\'s coming in [EcmaScript 6](http://wiki.ecmascript.org/doku.php?id=harmony:modules)). (I don\'t want to get diverted into this topic as it\'s a big area. But if you\'re interested then you can read up a little on different approaches being used [here](http://requirejs.org/docs/whyamd.html#today). The ongoing contest between RequireJS and CommonJS frankly makes me want to keep my distance for now.)\\n\\n## It Depends\\n\\nBack to my point, JavaScripts native handling of script dependencies is non-existent. It\'s real \\"here be dragons\\" territory. If you serve up, for example, Slave.js that depends on things set up in Master.js before you\'ve actually served up Master.js, well it\'s not a delightful debugging experience. The errors tend be obscure and it\'s not always obvious what the correct ordering should be.\\n\\nNaturally this creates something of a headache around my own JavaScript modules. A certain amount of jiggery-pokery is required to ensure that scripts are served in the correct order so that they run as expected. And as your application becomes more complicated / modular, the number of problems around this area increase exponentially. It\'s **really** tedious. I don\'t want to be thinking about managing that as I\'m developing - I want to be focused on solving the problem at hand.\\n\\nIn short, what I want to do is reference a script file somewhere in my server-side pipeline. I could be in a view, a layout, a controller, a partial view, a HTML helper... - I just want to know that that script is going to turn up at the client in the right place in the HTML so it works. Always. And I don\'t want to have to think about it any further than that.\\n\\n## Enter Cassette, riding a white horse\\n\\nAnd this is where Cassette takes the pain away. To quote the documentation:\\n\\n> \\"_Some assets must be included in a page before others. For example, your code may use jQuery, so the jQuery script must be included first. Cassette will sort all assets based on references they declare._\\"\\n\\nJust the ticket!\\n\\n## Declaring References Server-Side\\n\\nWhat does this look like in reality? Let\'s build on what I did last time to demonstrate how I make use of Asset References to ensure my scripts turn up in the order I require.\\n\\nIn my `_Layout.cshtml` file I\'m going to remove the following reference from the head of the file:\\n\\n`Bundles.Reference(\\"~/bundles/core\\");`\\n\\nI\'m pulling this out of my layout page because it\'s presence means that **every** page MVC serves up is also serving up jQuery and jQuery UI (which is what `~/bundles/core` is). If a page doesn\'t actually make use of jQuery and / or jQuery UI then there\'s no point in doing this.\\n\\n\\"_But wait!_\\", I hear you cry, \\"_Haven\'t you just caused a bug with your reckless action? I distinctly recall that the `Login.cshtml` page has the following code in place:_\\"\\n\\n`Bundles.Reference(\\"~/bundles/validate\\");`\\n\\n\\"_And now with your foolhardy, nay, reckless attitude to the `~/bundles/core` bundle you\'ve broken your Login screen. How can jQuery Validation be expected to work if there\'s no jQuery there to extend?_\\"\\n\\nWell, I understand your concerns but really you needn\'t worry - Cassette\'s got my back. Look closely at the code below:\\n\\n```cs\\n// A bundle of the core scripts that will likely be used on every page of the app\\nbundles.Add<ScriptBundle>(\\"~/bundles/core\\",\\n                          new[]\\n                              {\\n                                  \\"~/Scripts/jquery-1.8.2.js\\",\\n                                  \\"~/Scripts/jquery-ui-1.8.24.js\\"\\n                              });\\n\\n// Validation scripts; only likely necessary on data entry screens\\nbundles.Add<ScriptBundle>(\\"~/bundles/validate\\",\\n                          new[]\\n                              {\\n                                  \\"~/Scripts/jquery.validate.js\\",\\n                                  \\"~/Scripts/jquery.validate.unobtrusive.js\\"\\n                              },\\n                          bundle => bundle.AddReference(\\"~/bundles/core\\")\\n    );\\n```\\n\\nSee it? The `~/bundles/validate` bundle declares a reference to the `~/bundles/core` bundle. The upshot of this is, that if you tell Cassette to reference `~/bundles/validate` it will ensure that before it renders that bundle it first renders any bundles that bundle depends on (in this case the `~/bundles/core` bundle).\\n\\nThis is a very simple demonstration of the feature but I can\'t underplay just how useful I find this.\\n\\n## Declaring References in your JavaScript itself\\n\\nAnd the good news doesn\'t stop there. Let\'s say you **don\'t** want to maintain your references in a separate file. You\'d rather declare references inside your JavaScript files themselves. Well - you can. Cassette caters for this through the usage of [Asset References](http://getcassette.net/documentation/v1/AssetReferences).\\n\\nLet\'s demo this. First of all add the following file at this location in the project: `~/Scripts/Views/Home/Index.js`\\n\\n```js\\n// @reference ~/bundles/core\\n\\n$(document).ready(function () {\\n  var $body = $(\'#body\');\\n\\n  $body.fadeOut(1000, function () {\\n    $body\\n      .html(\\n        \'<div style=\\"width: 150px; margin: 0 auto;\\">\' +\\n          \'I made it all go away...</div>\',\\n      )\\n      .fadeIn();\\n  });\\n});\\n```\\n\\nThe eagle-eyed amongst you will have noticed\\n\\n1. I\'m mirroring the MVC folder structure inside the Scripts directory. (There\'s nothing special about that by the way - it\'s just a file structure I\'ve come to find useful. It\'s very easy to find the script associated with a View if the scripts share the same organisational approach as the Views.).\\n2. The purpose of the script is very simple, it fades out the main body of the screen, re-writes the HTML in that tag and then fades back in. It\'s purpose is just to do something that is obvious to the user - so they can see the evidence of JavaScript executing.\\n3. Lastly and most importantly, do you notice that `// @reference ~/bundles/core` is the first line of the file? This is our script reference. It\'s this that Cassette will be reading to pick up references.\\n\\nTo make sure Cassette is picking up our brand new file let\'s take a look at `CassetteConfiguration.cs` and uncomment the line of code below:\\n\\n`bundles.AddPerIndividualFile<scriptbundle>(\\"~/Scripts/Views\\");</scriptbundle>`\\n\\nWith this in place Cassette will render out a bundle for each script in the Views subdirectory. Let\'s see if it works. Add the following reference to our new JavaScript file in `~/Views/Home/Index.cshtml`:\\n\\n`Bundles.Reference(\\"~/Scripts/Views/Home/Index.js\\");`\\n\\nNow `Index.js` is served up by Cassette. And more importantly before `Index.js` was served the referenced `~/bundles/core` was served too.\\n\\n## Avoiding the Gotcha\\n\\nThere is a gotcha which I\'ve discovered whilst using Cassette\'s Asset References. Strictly speaking it\'s a Visual Studio gotcha rather than a Cassette gotcha. It concerns Cassette\'s support for Visual Studio XML style reference comments. In the example above I could have written this:\\n\\n`/// &lt;reference path=\\"~/bundles/core\\" /&gt;`\\n\\nInstead of this:\\n\\n`// @reference ~/bundles/core`\\n\\nIt would fulfil exactly the same purpose and would work identically. But there\'s a problem. Using Visual Studio XML style reference comments to refer to Cassette bundles appears to trash the Visual Studios JavaScript Intellisense. You\'ll lose the Intellisense that\'s driven by `~/Scripts/_references.js` in VS 2012. So if you value your Intellisense (and I do) my advice is to stick to using the standard Cassette references style instead.\\n\\n## Go Forth and Reference\\n\\nThere is also support in Cassette for CSS referencing (as well as other types of referencing relating to LESS and even CoffeeScript). I haven\'t made use of CSS referencing myself as, in stark contrast to my JS, my CSS is generally one bundle of styles which I\'m happy to be rendered on each page. But it\'s nice to know the option is there if I wanted it.\\n\\nFinally, as last time you can see what I\'ve done in this post by just looking at the repository on [GitHub](https://github.com/johnnyreilly/CassetteDemo/tree/References). The changes I made are on the References branch of that particular repository.\\n\\n\x3c!-- I don\'t want to serve up a monster JavaScript payload with each screen refresh.  Quite besides anything else, if I did that each screen refresh would be slower as more JavaScript was served up and parsed - the UX would suffer.  I don\'t want that.  I want *<strong>performance</strong>*! --\x3e"},{"id":"how-im-using-cassette","metadata":{"permalink":"/how-im-using-cassette","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2013-05-04-how-im-using-cassette/index.md","source":"@site/blog/2013-05-04-how-im-using-cassette/index.md","title":"How I\'m Using Cassette part 1:Getting Up and Running","description":"Learn how to serve JavaScript assets efficiently in ASP.Net MVC with Cassette to avoid duplicate scripts and ensure speedy loading.","date":"2013-05-04T00:00:00.000Z","tags":[{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":8.81,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"how-im-using-cassette","title":"How I\'m Using Cassette part 1:Getting Up and Running","authors":"johnnyreilly","tags":["asp.net"],"hide_table_of_contents":false,"description":"Learn how to serve JavaScript assets efficiently in ASP.Net MVC with Cassette to avoid duplicate scripts and ensure speedy loading."},"unlisted":false,"prevItem":{"title":"How I\'m Using Cassette part 2:Get Cassette to Serve Scripts in Dependency Order","permalink":"/how-im-using-cassette-part-2"},"nextItem":{"title":"A navigation animation (for your users delectation)","permalink":"/a-navigation-animation-for-your-users"}},"content":"## Backing into the light\\n\\n\x3c!--truncate--\x3e\\n\\nFor a while now, I\'ve been seeking a bulletproof way to handle the following scenarios... all at the same time in the context of an ASP.Net MVC application:\\n\\n1. How to serve full-fat JavaScript in debug mode and minified in release mode\\n2. When debugging, ensure that the full-fat JS being served is definitely the latest version; and \\\\***not**\\\\* from the cache. (The time I\'ve wasted due to [304\'s](http://en.wikipedia.org/wiki/List_of_HTTP_status_codes#304)...)\\n3. How to add Javascript assets that need to be served up from any point in an ASP.Net MVC application (including views, layouts, partial views... even controllers if so desired) whilst preventing duplicate scripts from being served.\\n4. How to ensure that Javascript files are served up last to any web page to ensure a speedy feel to users (don\'t want JS blocking rendering).\\n5. And last but certainly not least the need to load Javascript files in dependency order. If `myView.js` depends on jQuery then clearly `jQuery-latest.js` needs to be served before `myView.js`.\\n\\nNow the best, most comprehensive and solid looking solution to this problem has for some time seemed to me to be [Andrew Davey\'s](http://aboutcode.net/)[Cassette](http://getcassette.net/). This addresses all my issues in one way or another, as well as bringing in a raft of other features (support for Coffeescript etc).\\n\\nHowever, up until now I\'ve slightly shied away from using Cassette as I was under the impression it had a large number of dependencies. That doesn\'t appear to be the case at all. I also had some vague notion that I could quite simply build my own solution to these problems making use of Microsoft\'s [Web Optimization](http://nuget.org/packages/Microsoft.AspNet.Web.Optimization/1.0.0) which nicely handles my #1 problem above. However, looking again at the documentation Cassette was promising to handle scenarios #1 - #5 without breaking sweat. How could I ignore that? I figured I should do the sensible thing and take another look at it. And, lo and behold, when I started evaluating it again it seemed to be just what I needed.\\n\\nWith the minumum of fuss I was able to get an ASP.Net MVC 4 solution up and running, integrated with Cassette, which dealt with all my scenarios very nicely indeed. I thought it might be good to write this up over a short series of posts and share what my finished code looks like. If you follow the steps I go through below it\'ll get you started using Cassette. Or you could skip to the end of this post and look at the repo on GitHub. Here we go...\\n\\n## Adding Cassette to a raw MVC 4 project\\n\\nFire up Visual Studio and create a new MVC 4 project (I used the internet template to have some content in place).\\n\\nGo to the Package Manager Console and key in \\"`Install-Package Cassette.Aspnet`\\". Cassette will install itself.\\n\\nNow you\'ve got Cassette in place you may as well pull out usage of Web Optimization as you\'re not going to need it any more.Be ruthless, delete App_Start/BundleConfig.cs and delete the line of code that references it in Global.asax.cs. If you take the time to run the app now you\'ll see you\'ve miraculously lost your CSS and your JavaScript. The code referencing it is still in place but there\'s nothing for it to serve up. Don\'t worry about that - we\'re going to come back and Cassette-ify things later on...\\n\\nYou\'ll also notice you now have a CassetteConfiguration.cs file in your project. Open it. Replace the contents with this (I\'ve just commented out the default code and implemented my own CSS and Script bundles based on what is available in the default template of an MVC 4 app):\\n\\n```cs\\nusing Cassette;\\nusing Cassette.Scripts;\\nusing Cassette.Stylesheets;\\n\\nnamespace CassetteDemo\\n{\\n    /// <summary>\\n    /// Configures the Cassette asset bundles for the web application.\\n    /// </summary>\\n    public class CassetteBundleConfiguration : IConfiguration<BundleCollection>\\n    {\\n        public void Configure(BundleCollection bundles)\\n        {\\n            // TODO: Configure your bundles here...\\n            // Please read http://getcassette.net/documentation/configuration\\n\\n            // This default configuration treats each file as a separate \'bundle\'.\\n            // In production the content will be minified, but the files are not combined.\\n            // So you probably want to tweak these defaults!\\n            //bundles.AddPerIndividualFile<StylesheetBundle>(\\"Content\\");\\n            //bundles.AddPerIndividualFile<ScriptBundle>(\\"Scripts\\");\\n\\n            // To combine files, try something like this instead:\\n            //   bundles.Add<StylesheetBundle>(\\"Content\\");\\n            // In production mode, all of ~/Content will be combined into a single bundle.\\n\\n            // If you want a bundle per folder, try this:\\n            //   bundles.AddPerSubDirectory<ScriptBundle>(\\"Scripts\\");\\n            // Each immediate sub-directory of ~/Scripts will be combined into its own bundle.\\n            // This is useful when there are lots of scripts for different areas of the website.\\n\\n            AddStylesheetBundles(bundles);\\n            AddScriptBundles(bundles);\\n        }\\n\\n        private static void AddStylesheetBundles(BundleCollection bundles)\\n        {\\n            bundles.Add<StylesheetBundle>(\\"~/bundles/css\\",\\n                                          \\"~/Content/Site.css\\",\\n                                          \\"~/Content/themes/base/jquery-ui.css\\"\\n                );\\n        }\\n\\n        private static void AddScriptBundles(BundleCollection bundles)\\n        {\\n            // A bundle of the scripts that will need to be added to the head (likely only ever to be Modernizr but you never know)\\n            bundles.Add<ScriptBundle>(\\"~/bundles/head\\",\\n                                      new[] {\\"~/Scripts/modernizr-2.6.2.js\\"},\\n                                      bundle => bundle.PageLocation = \\"head\\"\\n                );\\n\\n            // A bundle of the core scripts that will likely be used on every page of the app\\n            bundles.Add<ScriptBundle>(\\"~/bundles/core\\",\\n                                      new[]\\n                                          {\\n                                              \\"~/Scripts/jquery-1.8.2.js\\",\\n                                              \\"~/Scripts/jquery-ui-1.8.24.js\\"\\n                                          });\\n\\n            // Validation scripts; only likely necessary on date entry screens\\n            bundles.Add<ScriptBundle>(\\"~/bundles/validate\\",\\n                                      new[]\\n                                          {\\n                                              \\"~/Scripts/jquery.validate.js\\",\\n                                              \\"~/Scripts/jquery.validate.unobtrusive.js\\"\\n                                          },\\n                                      bundle => bundle.AddReference(\\"~/bundles/core\\")\\n                );\\n\\n            // Create a per file bundle for all areas / views\\n            //bundles.AddPerIndividualFile<ScriptBundle>(\\"~/Scripts/Views\\");\\n        }\\n    }\\n}\\n```\\n\\nIn the script above I\'ve created 4 bundles, 1 stylesheet bundle and 3 JavaScript bundles - each of these is roughly equivalent to Web Optimization bundles that are part of the MVC 4 template:\\n\\n#### `~/bundles/css`\\n\\nOur site CSS - this includes both our own CSS and the jQuery UI CSS as well. This is the rough equivalent of the Web Optimization bundles `~/Content/css` and `~/Content/themes/base/css` brought together.\\n\\n#### `~/bundles/head`\\n\\nWhat scripts we want served in the head tag - Modernizr basically. Do note the setting of the `PageLocation` property - the purpose of this will become apparent later. This is the direct equivalent of the Web Optimization bundle: `~/bundles/modernizr`.\\n\\n#### `~/bundles/core`\\n\\nThe scripts we want served on every page. For this example project I\'ve picked jQuery and jQuery UI. This is the rough equivalent of the Web Optimization bundles `~/bundles/jquery` and `~/bundles/jqueryui` brought together.\\n\\n#### `~/bundles/validate`\\n\\nThe validation scripts (that are dependent on the core scripts). This is the rough equivalent of the Web Optimization bundle: `~/bundles/jqueryval`.\\n\\nAt this point we\'ve set up Cassette in our project - although we\'re not making use of it yet. If you want to double check that everything is working properly then you can fire up your project and browse to \\"Cassette.axd\\" in the root.\\n\\n## How Web Optimization and Cassette Differ\\n\\nIf you\'re more familiar with the workings of Web Optimization than Cassette then it\'s probably worth taking a moment to appreciate an important distinction between the slightly different ways each works.\\n\\n**Web Optimization**\\n\\n1. Create bundles as desired.\\n2. Serve up bundles and / or straight JavaScript files as you like within your MVC views / partial views / layouts.\\n\\n**Cassette**\\n\\n1. Create bundles for \\\\***all**\\\\* JavaScript files you wish to serve up. You may wish to create some bundles which consist of a number of a number of JavaScript files pushed together. But for each individual file you wish to serve you also need to create an individual bundle. (Failure to do so may mean you fall prey to the \\"_Cannot find an asset bundle containing the path \\"\\\\~/Scripts/somePath.js\\"._\\")\\n2. Reference bundles and / or individual JavaScript files in their individual bundles as you like within your MVC views / partial views / layouts / controllers / HTML helpers... the list goes on!\\n3. Render the referenced scripts to the page (typically just before the closing `body` tag)\\n\\n## Making use of our Bundles\\n\\nNow we\'ve created our bundles let\'s get the project serving up CSS and JavaScript using Cassette. First the layout file. Take the `_Layout.cshtml` file from this:\\n\\n```html\\n<!doctype html>\\n<html lang=\\"en\\">\\n  <head>\\n    <meta charset=\\"utf-8\\" />\\n    <title>@ViewBag.Title - My ASP.NET MVC Application</title>\\n    <link href=\\"~/favicon.ico\\" rel=\\"shortcut icon\\" type=\\"image/x-icon\\" />\\n    <meta name=\\"viewport\\" content=\\"width=device-width\\" />\\n    @Styles.Render(\\"~/Content/css\\") @Scripts.Render(\\"~/bundles/modernizr\\")\\n  </head>\\n  <body>\\n    <header>\\n      <div class=\\"content-wrapper\\">\\n        <div class=\\"float-left\\">\\n          <p class=\\"site-title\\">\\n            @Html.ActionLink(\\"your logo here\\", \\"Index\\", \\"Home\\")\\n          </p>\\n        </div>\\n        <div class=\\"float-right\\">\\n          <section id=\\"login\\">@Html.Partial(\\"_LoginPartial\\")</section>\\n          <nav>\\n            <ul id=\\"menu\\">\\n              <li>@Html.ActionLink(\\"Home\\", \\"Index\\", \\"Home\\")</li>\\n              <li>@Html.ActionLink(\\"About\\", \\"About\\", \\"Home\\")</li>\\n              <li>@Html.ActionLink(\\"Contact\\", \\"Contact\\", \\"Home\\")</li>\\n            </ul>\\n          </nav>\\n        </div>\\n      </div>\\n    </header>\\n    <div id=\\"body\\">\\n      @RenderSection(\\"featured\\", required: false)\\n      <section class=\\"content-wrapper main-content clear-fix\\">\\n        @RenderBody()\\n      </section>\\n    </div>\\n    <footer>\\n      <div class=\\"content-wrapper\\">\\n        <div class=\\"float-left\\">\\n          <p>&copy; @DateTime.Now.Year - My ASP.NET MVC Application</p>\\n        </div>\\n      </div>\\n    </footer>\\n\\n    @Scripts.Render(\\"~/bundles/jquery\\") @RenderSection(\\"scripts\\", required:\\n    false)\\n  </body>\\n</html>\\n```\\n\\nTo this:\\n\\n```html\\n@{ Bundles.Reference(\\"~/bundles/css\\"); Bundles.Reference(\\"~/bundles/head\\");\\nBundles.Reference(\\"~/bundles/core\\"); }\\n<!doctype html>\\n<html lang=\\"en\\">\\n  <head>\\n    <meta charset=\\"utf-8\\" />\\n    <title>@ViewBag.Title - My ASP.NET MVC Application</title>\\n    <link href=\\"~/favicon.ico\\" rel=\\"shortcut icon\\" type=\\"image/x-icon\\" />\\n    <meta name=\\"viewport\\" content=\\"width=device-width\\" />\\n    @Bundles.RenderStylesheets() @Bundles.RenderScripts(\\"head\\")\\n  </head>\\n  <body>\\n    <header>\\n      <div class=\\"content-wrapper\\">\\n        <div class=\\"float-left\\">\\n          <p class=\\"site-title\\">\\n            @Html.ActionLink(\\"your logo here\\", \\"Index\\", \\"Home\\")\\n          </p>\\n        </div>\\n        <div class=\\"float-right\\">\\n          <section id=\\"login\\">@Html.Partial(\\"_LoginPartial\\")</section>\\n          <nav>\\n            <ul id=\\"menu\\">\\n              <li>@Html.ActionLink(\\"Home\\", \\"Index\\", \\"Home\\")</li>\\n              <li>@Html.ActionLink(\\"About\\", \\"About\\", \\"Home\\")</li>\\n              <li>@Html.ActionLink(\\"Contact\\", \\"Contact\\", \\"Home\\")</li>\\n            </ul>\\n          </nav>\\n        </div>\\n      </div>\\n    </header>\\n    <div id=\\"body\\">\\n      @RenderSection(\\"featured\\", required: false)\\n      <section class=\\"content-wrapper main-content clear-fix\\">\\n        @RenderBody()\\n      </section>\\n    </div>\\n    <footer>\\n      <div class=\\"content-wrapper\\">\\n        <div class=\\"float-left\\">\\n          <p>&copy; @DateTime.Now.Year - My ASP.NET MVC Application</p>\\n        </div>\\n      </div>\\n    </footer>\\n\\n    @Bundles.RenderScripts()\\n  </body>\\n</html>\\n```\\n\\nAnd now let\'s take one of the views, `Login.cshtml` and take it from this:\\n\\n```html\\n@model CassetteDemo.Models.LoginModel @{ ViewBag.Title = \\"Log in\\"; }\\n\\n<hgroup class=\\"title\\">\\n  <h1>@ViewBag.Title.</h1>\\n</hgroup>\\n\\n<section id=\\"loginForm\\">\\n  <h2>Use a local account to log in.</h2>\\n  @using (Html.BeginForm(new { ReturnUrl = ViewBag.ReturnUrl })) {\\n  @Html.AntiForgeryToken() @Html.ValidationSummary(true)\\n\\n  <fieldset>\\n    <legend>Log in Form</legend>\\n    <ol>\\n      <li>\\n        @Html.LabelFor(m => m.UserName) @Html.TextBoxFor(m => m.UserName)\\n        @Html.ValidationMessageFor(m => m.UserName)\\n      </li>\\n      <li>\\n        @Html.LabelFor(m => m.Password) @Html.PasswordFor(m => m.Password)\\n        @Html.ValidationMessageFor(m => m.Password)\\n      </li>\\n      <li>\\n        @Html.CheckBoxFor(m => m.RememberMe) @Html.LabelFor(m => m.RememberMe,\\n        new { @class = \\"checkbox\\" })\\n      </li>\\n    </ol>\\n    <input type=\\"submit\\" value=\\"Log in\\" />\\n  </fieldset>\\n  <p>@Html.ActionLink(\\"Register\\", \\"Register\\") if you don\'t have an account.</p>\\n  }\\n</section>\\n\\n<section class=\\"social\\" id=\\"socialLoginForm\\">\\n  <h2>Use another service to log in.</h2>\\n  @Html.Action(\\"ExternalLoginsList\\", new { ReturnUrl = ViewBag.ReturnUrl })\\n</section>\\n\\n@section Scripts { @Scripts.Render(\\"~/bundles/jqueryval\\") }\\n```\\n\\nTo this:\\n\\n```html\\n@model CassetteDemo.Models.LoginModel @{ ViewBag.Title = \\"Log in\\";\\nBundles.Reference(\\"~/bundles/validate\\"); }\\n\\n<hgroup class=\\"title\\">\\n  <h1>@ViewBag.Title.</h1>\\n</hgroup>\\n\\n<section id=\\"loginForm\\">\\n  <h2>Use a local account to log in.</h2>\\n  @using (Html.BeginForm(new { ReturnUrl = ViewBag.ReturnUrl })) {\\n  @Html.AntiForgeryToken() @Html.ValidationSummary(true)\\n\\n  <fieldset>\\n    <legend>Log in Form</legend>\\n    <ol>\\n      <li>\\n        @Html.LabelFor(m => m.UserName) @Html.TextBoxFor(m => m.UserName)\\n        @Html.ValidationMessageFor(m => m.UserName)\\n      </li>\\n      <li>\\n        @Html.LabelFor(m => m.Password) @Html.PasswordFor(m => m.Password)\\n        @Html.ValidationMessageFor(m => m.Password)\\n      </li>\\n      <li>\\n        @Html.CheckBoxFor(m => m.RememberMe) @Html.LabelFor(m => m.RememberMe,\\n        new { @class = \\"checkbox\\" })\\n      </li>\\n    </ol>\\n    <input type=\\"submit\\" value=\\"Log in\\" />\\n  </fieldset>\\n  <p>@Html.ActionLink(\\"Register\\", \\"Register\\") if you don\'t have an account.</p>\\n  }\\n</section>\\n\\n<section class=\\"social\\" id=\\"socialLoginForm\\">\\n  <h2>Use another service to log in.</h2>\\n  @Html.Action(\\"ExternalLoginsList\\", new { ReturnUrl = ViewBag.ReturnUrl })\\n</section>\\n```\\n\\nSo now you should be up and running with Cassette. If you want the code behind this then take I\'ve put it on GitHub [here](https://github.com/johnnyreilly/CassetteDemo)."},{"id":"a-navigation-animation-for-your-users","metadata":{"permalink":"/a-navigation-animation-for-your-users","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2013-04-26-a-navigation-animation-for-your-users/index.md","source":"@site/blog/2013-04-26-a-navigation-animation-for-your-users/index.md","title":"A navigation animation (for your users delectation)","description":"Adding a CSS animation or GIF can help users navigating an app in an iframe get visual feedback despite the lack of browser feedback tics.","date":"2013-04-26T00:00:00.000Z","tags":[],"readingTime":6.88,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"a-navigation-animation-for-your-users","title":"A navigation animation (for your users delectation)","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false,"description":"Adding a CSS animation or GIF can help users navigating an app in an iframe get visual feedback despite the lack of browser feedback tics."},"unlisted":false,"prevItem":{"title":"How I\'m Using Cassette part 1:Getting Up and Running","permalink":"/how-im-using-cassette"},"nextItem":{"title":"IE 10 Install Torches JavaScript Debugging in Visual Studio 2012 Through Auto Update (Probably)","permalink":"/ie-10-install-torches-javascript"}},"content":"## The Vexation\\n\\n\x3c!--truncate--\x3e\\n\\nThe current application I\'m working on lives within an `iframe`. A side effect of that is that my users no longer get the visual feedback that they\'re used to as they navigate around the site. By \\"visual feedback\\" what I mean are the little visual tics that are displayed in the browser when you\'re in the process of navigating from one screen to the next.\\n\\nWhen an application is nested in an `iframe` it seems that these visual tics aren\'t propogated up to the top frame of the browser as the user navigates around. Clicking on links results in a short lag whilst nothing appears to be happening and then, BANG!, a new page is rendered. This is not a great user experience. There\'s nothing to indicate that the link has been clicked on and the browser is doing something. Well, not in Internet Explorer at least - Chrome (my browser of choice) appears to do just that. But that\'s really by the by, the people using my app will be using the corporate browser, IE; so I need to think about them.\\n\\nNow I\'m fully aware that this is more in the region of nice-to-have rather than absolute necessity. That said, my experience is that when users think an application isn\'t responding fast enough their action point is usually \\"click it again, and maybe once more for luck\\". To prevent this from happening, I wanted to give the users back some kind of steer when they were in the process of navigation, `iframe` or no `iframe`.\\n\\n## The Agreeable Resolution\\n\\nTo that end, I\'ve come up with something that I feel does the job, and does it well. I\'ve taken a CSS animation courtesy of the good folk at [CSS Load](http://cssload.net/) and embedded it in the layout of my application. This animation is hidden from view until the user navigates to another page. At that point, the CSS animation appears in the header of the screen and remains in place until the new screen is rendered.\\n\\n\x3c!-- <p>And because they haven\'t yet invented the animated screenshot here\'s what the CSS animation looks like in full flight:</p> <style>#navigationAnimation {   margin: 7px;   clear: both; }   #circleG {   width: 46.666666666666664px;   height: 20px; }   .circleG {   background-color: #ffffff;   float: left;   height: 10px;   margin-left: 5px;   width: 10px;   -moz-border-radius: 7px;   -webkit-border-radius: 7px;   border-radius: 7px;   -moz-animation-name: bounce_circleG;   -moz-animation-duration: 0.6000000000000001s;   -moz-animation-iteration-count: infinite;   -moz-animation-direction: linear;   -webkit-animation-name: bounce_circleG;   -webkit-animation-duration: 0.6000000000000001s;   -webkit-animation-iteration-count: infinite;   -webkit-animation-direction: linear;   -ms-animation-name: bounce_circleG;   -ms-animation-duration: 0.6000000000000001s;   -ms-animation-iteration-count: infinite;   -ms-animation-direction: linear;   animation-name: bounce_circleG;   animation-duration: 0.6000000000000001s;   animation-iteration-count: infinite;   animation-direction: linear; }   #circleG_1 {   -moz-animation-delay: 0.12000000000000002s;   -webkit-animation-delay: 0.12000000000000002s;   -ms-animation-delay: 0.12000000000000002s;   animation-delay: 0.12000000000000002s; }   #circleG_2 {   -moz-animation-delay: 0.28s;   -webkit-animation-delay: 0.28s;   -ms-animation-delay: 0.28s;   animation-delay: 0.28s; }   #circleG_3 {   -moz-animation-delay: 0.36s;   -webkit-animation-delay: 0.36s;   -ms-animation-delay: 0.36s;   animation-delay: 0.36s; }   @-moz-keyframes bounce_circleG {   50% {     background-color: #000000;   } }   @-webkit-keyframes bounce_circleG {   50% {     background-color: #000000;   } }   @-ms-keyframes bounce_circleG {   50% {     background-color: #000000;   } }   @keyframes bounce_circleG {   50% {     background-color: #000000;   } }   </style> <div id=\\"navigationAnimation\\">    <div id=\\"circleG\\">        <div id=\\"circleG_1\\" class=\\"circleG\\"></div>        <div id=\\"circleG_2\\" class=\\"circleG\\"></div>        <div id=\\"circleG_3\\" class=\\"circleG\\"></div>    </div></div> <p>Beautiful don\'t you think?</p>--\x3e\\n\\n## How\'s that work then guv?\\n\\nYou\'re no doubt dazzled by the glory of it all. How was it accomplished? Well, it was actually a great deal easier than you might think. First of all we have the html:\\n\\n```html\\n<div class=\\"float-right hidden\\" id=\\"navigationAnimation\\">\\n  <div id=\\"circleG\\">\\n    <div id=\\"circleG_1\\" class=\\"circleG\\"></div>\\n    <div id=\\"circleG_2\\" class=\\"circleG\\"></div>\\n    <div id=\\"circleG_3\\" class=\\"circleG\\"></div>\\n  </div>\\n</div>\\n```\\n\\nApart from the outer `div` tag (#navigationAnimation) all of this is the HTML taken from [CSS Load](http://cssload.net/). If you wanted to use a different navigation animation you could easily replace the inner HTML with something else instead. Next up is the CSS, again courtesy of CSS Load (and it\'s this that turns this simple HTML into sumptuous animated goodness):\\n\\n```css\\n#navigationAnimation {\\n  margin-top: 7px;\\n}\\n\\n#circleG {\\n  width: 46.666666666666664px;\\n}\\n\\n.circleG {\\n  background-color: #ffffff;\\n  float: left;\\n  height: 10px;\\n  margin-left: 5px;\\n  width: 10px;\\n  -moz-border-radius: 7px;\\n  -webkit-border-radius: 7px;\\n  border-radius: 7px;\\n  -moz-animation-name: bounce_circleG;\\n  -moz-animation-duration: 0.6000000000000001s;\\n  -moz-animation-iteration-count: infinite;\\n  -moz-animation-direction: linear;\\n  -webkit-animation-name: bounce_circleG;\\n  -webkit-animation-duration: 0.6000000000000001s;\\n  -webkit-animation-iteration-count: infinite;\\n  -webkit-animation-direction: linear;\\n  -ms-animation-name: bounce_circleG;\\n  -ms-animation-duration: 0.6000000000000001s;\\n  -ms-animation-iteration-count: infinite;\\n  -ms-animation-direction: linear;\\n  animation-name: bounce_circleG;\\n  animation-duration: 0.6000000000000001s;\\n  animation-iteration-count: infinite;\\n  animation-direction: linear;\\n}\\n\\n#circleG_1 {\\n  -moz-animation-delay: 0.12000000000000002s;\\n  -webkit-animation-delay: 0.12000000000000002s;\\n  -ms-animation-delay: 0.12000000000000002s;\\n  animation-delay: 0.12000000000000002s;\\n}\\n\\n#circleG_2 {\\n  -moz-animation-delay: 0.28s;\\n  -webkit-animation-delay: 0.28s;\\n  -ms-animation-delay: 0.28s;\\n  animation-delay: 0.28s;\\n}\\n\\n#circleG_3 {\\n  -moz-animation-delay: 0.36s;\\n  -webkit-animation-delay: 0.36s;\\n  -ms-animation-delay: 0.36s;\\n  animation-delay: 0.36s;\\n}\\n\\n@-moz-keyframes bounce_circleG {\\n  50% {\\n    background-color: #000000;\\n  }\\n}\\n\\n@-webkit-keyframes bounce_circleG {\\n  50% {\\n    background-color: #000000;\\n  }\\n}\\n\\n@-ms-keyframes bounce_circleG {\\n  50% {\\n    background-color: #000000;\\n  }\\n}\\n\\n@keyframes bounce_circleG {\\n  50% {\\n    background-color: #000000;\\n  }\\n}\\n\\n/* classes below are not part of CSS animation */\\n\\n.hidden {\\n  display: none;\\n}\\n\\n.float-right {\\n  float: right;\\n  margin-left: 1em;\\n}\\n```\\n\\nAnd finally we have the JavaScript which is responsible for showing animation when the user starts navigating:\\n\\n```js\\n/*!\\n * Initialise the navigation animation\\n */\\n$(document).ready(function () {\\n  var navigationAnimationVisible, navigationFallback, $navigationAnimation;\\n\\n  // initialises the navigation animation (including fallback for browsers without CSS animations)\\n  function initialiseNavigationAnimation() {\\n    navigationAnimationVisible = false;\\n    $navigationAnimation = $(\'#navigationAnimation\');\\n    navigationFallback =\\n      \'<img src=\\"/images/navigationAnimation.gif\\" width=\\"43\\" height=\\"11\\" />\';\\n\\n    // fallback - initial call to ensure the image is cached before subsequent re-use (present flash to users of unloaded gif)\\n    if (!Modernizr.cssanimations) {\\n      $navigationAnimation.html(navigationFallback);\\n    }\\n  }\\n\\n  // Show or hide the navigation animation\\n  function showNavigating(makeVisible) {\\n    if (makeVisible && !navigationAnimationVisible) {\\n      // Show\\n      $navigationAnimation.removeClass(\'hidden\');\\n      navigationAnimationVisible = true;\\n    } else if (!makeVisible && navigationAnimationVisible) {\\n      // Hide\\n      $navigationAnimation.addClass(\'hidden\');\\n      navigationAnimationVisible = false;\\n    }\\n  }\\n\\n  // Initialise\\n  initialiseNavigationAnimation();\\n\\n  // Show navigation animation on screen change\\n  $(window).on(\'beforeunload\', function () {\\n    // fallback\\n    if (!Modernizr.cssanimations) {\\n      $navigationAnimation.html(navigationFallback);\\n    }\\n\\n    showNavigating(true);\\n  });\\n});\\n```\\n\\nIt\'s helped along with a little jQuery here but this could easily be accomplished with vanilla JS if you fancied. The approach works by hooking into the [beforeunload](https://developer.mozilla.org/en-US/docs/DOM/Mozilla_event_reference/beforeunload) event that fires when \\"_the window, the document and its resources are about to be unloaded_\\". There\'s a little bit more to the functionality in the JavaScript abover which I go into in the PPS below. Essentially that covers backwards compatibility with earlier versions of IE.\\n\\nI\'ve coded this up in a manner that lends itself to re-use. I can imagine that you might also want to make use of the navigation animation if, for example, you had an expensive AJAX operation on a page and you didn\'t want the users to despair. So the navigation animation could become a kind of a generic \\"I am doing something\\" animation instead - I leave it to your disgression.\\n\\n## Oh, and a final PS\\n\\nI had initially planned to use an old school animated GIF instead of a CSS animation. The thing that stopped me taking this course of action is that, to quote an [answer on Stack Overflow](http://stackoverflow.com/a/780617/761388) \\"_IE assumes that the clicking of a link heralds a new navigation where the current page contents will be replaced. As part of the process for perparing for that it halts the code that animates the GIFs._\\". So I needed animation that stayed animated. And lo, there were CSS animations...\\n\\n## Better make that a PPS - catering for IE 9 and earlier\\n\\nI spoke a touch too soon when I expounded on how CSS animations were going to get me out of a hole. Unfortunately, and to my lasting regret, they aren\'t supported in IE 9. And yes, at least for now that is what the users have. To get round this I\'ve delved a little bit further and discovered a frankly hacky way to make animated gifs stay animated after beforeunload has fired. It works by rendering an animated gif to the screen when beforeunload is fired. Why this works I couldn\'t say - but if you\'re interested to research more then take a look at [this answer on Stack Overflow](http://stackoverflow.com/a/1904931/761388). In my case I\'ve found an animated gif on [AjaxLoad](http://www.ajaxload.info/) which looks pretty similar to the CSS animation:\\n\\nThis is now saved away as `navigationAnimation.gif` in the application. The JavaScript uses Modernizr to detect if CSS animations are in play. If they\'re not then the animated gif is rendered to the screen in place of the CSS animation HTML. Ugly, but it seems to work well; I think this will work on IE 6 - 9. The CSS animations will work on IE 10+."},{"id":"ie-10-install-torches-javascript","metadata":{"permalink":"/ie-10-install-torches-javascript","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2013-04-17-ie-10-install-torches-javascript/index.md","source":"@site/blog/2013-04-17-ie-10-install-torches-javascript/index.md","title":"IE 10 Install Torches JavaScript Debugging in Visual Studio 2012 Through Auto Update (Probably)","description":"Learn how to fix missing Script Documents when debugging JavaScript in Visual Studio 2012, likely caused by auto-updating from IE9 to IE10.","date":"2013-04-17T00:00:00.000Z","tags":[{"inline":false,"label":"Visual Studio","permalink":"/tags/visual-studio","description":"The Visual Studio IDE."},{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."}],"readingTime":1.175,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"ie-10-install-torches-javascript","title":"IE 10 Install Torches JavaScript Debugging in Visual Studio 2012 Through Auto Update (Probably)","authors":"johnnyreilly","tags":["visual studio","javascript"],"hide_table_of_contents":false,"description":"Learn how to fix missing Script Documents when debugging JavaScript in Visual Studio 2012, likely caused by auto-updating from IE9 to IE10."},"unlisted":false,"prevItem":{"title":"A navigation animation (for your users delectation)","permalink":"/a-navigation-animation-for-your-users"},"nextItem":{"title":"Making IE 10\'s clear field (X) button and jQuery UI autocomplete play nice","permalink":"/making-ie-10s-clear-field-x-button-and"}},"content":"OK the title of this post is a little verbose. I\'ve just wasted a morning of my life trying to discover what happened to my ability to debug JavaScript in Visual Studio 2012. If you don\'t want to experience the same pain then read on...\\n\\n\x3c!--truncate--\x3e\\n\\n## The Symptoms\\n\\n1. I\'m not hitting my JavaScript breakpoints when I hit F5 in Visual Studio.\\n2. [Script Documents](http://msdn.microsoft.com/en-us/library/bb385621.aspx) is missing from the Solution Explorer when I\'m debugging in Visual Studio.\\n\\n## The Cure\\n\\nIn the end, after a great deal of frustration, I happened upon [this answer](http://stackoverflow.com/a/15908391/761388) on Stack Overflow. It set me in the right direction.\\n\\nI was seeing exactly the same as this list but with **TWO** instances of Internet Explorer in the list instead of one. Odd, I know.\\n\\nI fixed this up by selecting Google Chrome as my target instead of IE, running it and then setting it back to IE. And interestingly, when I went to set it back to IE there was only one instance of Internet Explorer in the list again.\\n\\n## The Probable Cause\\n\\nMy machine was auto updated from IE 9 to IE 10 just the other day. I \\\\***think**\\\\* my JavaScript debugging issue appeared at the same time. This would explain to me why I had two instances of \\"Internet Explorer\\" in my list. Not certain but I\'d say the evidence is fairly compelling.\\n\\nPainful Microsoft. Painful"},{"id":"making-ie-10s-clear-field-x-button-and","metadata":{"permalink":"/making-ie-10s-clear-field-x-button-and","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2013-04-09-making-ie-10s-clear-field-x-button-and/index.md","source":"@site/blog/2013-04-09-making-ie-10s-clear-field-x-button-and/index.md","title":"Making IE 10\'s clear field (X) button and jQuery UI autocomplete play nice","description":"IE 10 installed w/o notice on Johns machine, causing issues with jQuery UI auto-complete loading gifs which have been resolved with a CSS fix.","date":"2013-04-09T00:00:00.000Z","tags":[{"inline":false,"label":"jQuery","permalink":"/tags/jquery","description":"The jQuery library."}],"readingTime":1.69,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"making-ie-10s-clear-field-x-button-and","title":"Making IE 10\'s clear field (X) button and jQuery UI autocomplete play nice","authors":"johnnyreilly","tags":["jquery"],"hide_table_of_contents":false,"description":"IE 10 installed w/o notice on Johns machine, causing issues with jQuery UI auto-complete loading gifs which have been resolved with a CSS fix."},"unlisted":false,"prevItem":{"title":"IE 10 Install Torches JavaScript Debugging in Visual Studio 2012 Through Auto Update (Probably)","permalink":"/ie-10-install-torches-javascript"},"nextItem":{"title":"Death to compatibility mode","permalink":"/death-to-compatibility-mode"}},"content":"This morning when I logged on I was surprised to discover IE 10 had been installed onto my machine. I hadn\'t taken any action to trigger this myself and so I\u2019m assuming that this was part of the general Windows Update mechanism. I know [Microsoft had planned to push IE 10 out through this mechanism](http://technet.microsoft.com/en-us/ie/jj898508.aspx).\\n\\n\x3c!--truncate--\x3e\\n\\nI was a little surprised that my work desktop had been upgraded without any notice. And I was initially rather concerned given that most of my users have IE 9 and now I didn\'t have a test harness on my development machine any more. (I\'ve generally found that having the majority users browser on your own machine is a good idea.) However, I wasn\'t too concerned as I didn\u2019t think it would makes much of a difference to my development experience. I say that because IE10, as far as I understand, is basically IE 9 + more advanced CSS 3 and extra HTML 5 features. The rendering of my existing content developed for the IE 9 target should look pixel for pixel identical in IE 10. That\u2019s the theory anyway.\\n\\nHowever, I have found one exception to this rule already. IE 10 provides clear field buttons in text boxes.\\n\\nUnhappily I found these were clashing with our jQuery UI auto complete loading gif.\\n\\nI know; ugly isn\'t it? Happily I was able to resolve this with a CSS ~~hack~~\\n\\nfix which looks like this:\\n\\n```css\\n/* jQuery auto completes add the class below when loading */\\n.ui-autocomplete-loading {\\n  background: url(\'/images/ajax_loader.gif\') no-repeat right 0.5em center;\\n}\\n\\n/* How\'d you like them apples IE 10? */\\n.ui-autocomplete-loading::-ms-clear {\\n  display: none;\\n}\\n```\\n\\nAnd now the jQuery UI autocomplete looks like we expect during the loading phase.\\n\\nBut happily when the autocomplete is not in the loading phase we still have access to the IE 10 clear field button. This works because the CSS selector above only applies to the _ui-autocomplete-loading_ class (which is only applied to the textbox when the loading is taking place)."},{"id":"death-to-compatibility-mode","metadata":{"permalink":"/death-to-compatibility-mode","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2013-04-01-death-to-compatibility-mode/index.md","source":"@site/blog/2013-04-01-death-to-compatibility-mode/index.md","title":"Death to compatibility mode","description":"John discusses compatibility mode in Internet Explorer and suggests using custom HTTP headers or meta tags to prevent rendering and CSS issues.","date":"2013-04-01T00:00:00.000Z","tags":[],"readingTime":6.35,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"death-to-compatibility-mode","title":"Death to compatibility mode","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false,"description":"John discusses compatibility mode in Internet Explorer and suggests using custom HTTP headers or meta tags to prevent rendering and CSS issues."},"unlisted":false,"prevItem":{"title":"Making IE 10\'s clear field (X) button and jQuery UI autocomplete play nice","permalink":"/making-ie-10s-clear-field-x-button-and"},"nextItem":{"title":"DecimalModelBinder for nullable Decimals","permalink":"/decimalmodelbinder-for-nullable-decimals"}},"content":"For just over 10 years my bread and butter has been the development and maintenance of line of business apps. More particularly, web apps built on the Microsoft stack of love ([\xa9 Scott Hanselman](https://channel9.msdn.com/Events/MIX/MIX11/FRM02)). These sort of apps are typically accessed via the company intranet and since \\"bring your own device\\" is still a relatively new innovation these apps are invariably built for everyones favourite browser: Internet Explorer. As we all know, enterprises are generally not that speedy when it comes to upgrades. So we\'re basically talking IE 9 at best, but more often than not, IE 8.\\n\\n\x3c!--truncate--\x3e\\n\\nNow, unlike many people, I don\'t regard IE as a work of evil. I spent a fair number of years working for an organization which had IE 6 as the only installed browser on company desktops. (In fact, this was still the case as late as 2012!) Now, because JavaScript is so marvellously flexible I was still able to do a great deal with the help of a number of [shivs / shims](http://paulirish.com/2011/the-history-of-the-html5-shiv/).\\n\\nBut rendering and CSS - well that\'s another matter. Because here we\'re at the mercy of \\"compatibility mode\\". Perhaps a quick history lesson is in order. What is this \\"compatibility mode\\" of which you speak?\\n\\n## A Brief History\\n\\nWell it all started when Microsoft released IE 8. To quote them:\\n\\n> _A fundamental problem discussed during each and every Internet Explorer release is balancing new features and functionality with site compatibility for the existing Web. On the one hand, new features and functionality push the Web forward. On the other hand, the Web is a large expanse; requiring every legacy page to support the \\"latest and greatest\\" browser version immediately at product launch just isn\'t feasible. Internet Explorer 8 addresses this challenge by introducing compatibility modes which gives a way to introduce new features and stricter compliance to standards while enabling it to be backward compliant._ \\\\- excerpted from [understanding compatibility modes in Internet Explorer 8](https://blogs.msdn.com/b/askie/archive/2009/03/23/understanding-compatibility-modes-in-internet-explorer-8.aspx).\\n\\n## There\'s the rub\\n\\nSounds fair enough? Of course it does. Microsoft have generally bent over backwards to facilitate backwards compatibility. Quite right too - good business sense and all that. However, one of the choices made around backwards compatibility I\'ve come to regard as somewhat irksome. Later down in the article you\'ll find this doozy: (emphasis mine)\\n\\n> _\\"**for Intranet pages, 7 (IE 7 Standards) rendering mode is used by default** and can be changed.\\"_\\n\\nFor whatever reason, this decision was not particularly well promoted. As a result, a fair number of devs I\'ve encountered have little or no knowledge of compatibility mode. Certainly it came as a surprise to me. Here was I, developing away on my desktop. I\'d fire up the app hosted on my machine and test on my local install of IE 8. All would look new and shiny (well non-anchor tags would have `:hover` support). Happy and content, I\'d push to our test system and browse to it. Wait, what\'s happened? Where\'s the new style rendering? What\'s up with my CSS? This is a bug right?\\n\\nObviously I know now it\'s not a bug it\'s a \\"feature\\". And I have learned how to get round the intranet default of compatibility mode through cunning deployment of meta tags and custom http headers. Recently compatibility mode has come to bite me for the second time (in this case I was building for IE 9 and was left wondering where all my rounded corners had vanished to when I deployed...).\\n\\nFor my own sanity I thought it might be good to document the various ways that exist to solve this particular problem. Just to clarify terms, \\"solve\\" in this context means \\"force IE to render in the most standards compliant / like other browsers fashion it can muster\\". You can use compatibility mode to do more than just that and if you\'re interested in more about this then I recommend [this Stack Overflow answer](http://stackoverflow.com/a/6771584/761388).\\n\\n## Solution 1: Custom HTTP Header through web.config\\n\\nIf you\'re running IIS7 or greater then, for my money, this is the simplest and most pain free solution. All you need do is include the following snippet in your web config file:\\n\\n```xml\\n<?xml version=\\"1.0\\"?>\\n<configuration>\\n\\n  \x3c!-- ... --\x3e\\n\\n  <system.webServer>\\n    <httpProtocol>\\n      <customHeaders>\\n        <add name=\\"X-UA-Compatible\\" value=\\"IE=edge\\" />\\n      </customHeaders>\\n    </httpProtocol>\\n  </system.webServer>\\n\\n  \x3c!-- ... --\x3e\\n\\n<configuration>\\n```\\n\\nThis will make IIS serve up the above custom response HTTP header with each page.\\n\\n## Solution 2: Custom HTTP Header the hard way\\n\\nMaybe you\'re running II6 and so you making a change to the web.config won\'t make a difference. That\'s fine, you can still get the same behaviour by going to the HTTP headers tab in IIS (see below) and adding the `X-UA-Compatible: IE=edge` header by hand.\\n\\nOr, if you don\'t have access to IIS (don\'t laugh - it happens) you can fall back to doing this in code like this:\\n\\n```cs\\nResponse.AppendHeader(\\"X-UA-Compatible\\", \\"IE=edge\\");\\n```\\n\\nObviously there\'s a whole raft of ways you could get this in, using `Application_BeginRequest` in `Global.asax.cs` would probably as good an approach as any.\\n\\n## Solution 3: Meta Tags are go!\\n\\nThe final approach uses meta tags. And, in my experience it is the most quirky approach - it doesn\'t always seem to work. First up, what do we do? Well, in each page served we include the following meta tag like this:\\n\\n```html\\n<!doctype html>\\n<html>\\n  <head>\\n    <meta http-equiv=\\"X-UA-Compatible\\" content=\\"IE=edge\\" />\\n    \x3c!-- See how the meta tag is the first inside the head?  That\'s *important* --\x3e\\n  </head>\\n  <body></body>\\n</html>\\n```\\n\\nHaving crawled over the WWW equivalent of broken glass I now know why this \\\\***sometimes**\\\\* doesn\'t work. (And credit where it\'s due the answer came from [here](http://stackoverflow.com/a/3960197/761388).) It\'s all down to the positioning of the meta tag:\\n\\n> _The X-UA-compatible header is not case sensitive; however, it must appear in the Web page\'s header (the HEAD section) before all other elements, except for the title element and other meta elements._ \\\\- excerpted from [specifying legacy document modes](<http://msdn.microsoft.com/en-gb/library/jj676915(v=vs.85).aspx>)\\n\\nThat\'s right, get your meta tag in the wrong place and things won\'t work. And you won\'t know why. Lovely. But get it right and it\'s all gravy. This remains the most unsatisfactory approach in my book though.\\n\\n## And for bonus points: `IFRAME`s!\\n\\nBefore I finish off I thought it worth sharing a little known feature of `IFRAME`s. If page is running in compatibility mode and it contains an `IFRAME` then the page loaded in that `IFRAME` will **also run in compatibility mode**. No ifs, no buts.\\n\\nIn the case that I encountered this behaviour, the application was being hosted in an `IFRAME` inside Sharepoint. Because of the way our Sharepoint was configured it ended up that the only real game in town for us was the meta tags approach - which happily worked once we\'d correctly placed our meta tag.\\n\\nAgain, it\'s lamentable that this behaviour isn\'t better documented - hopefully the act of writing this here will mean that it becomes a little better known. There\'s probably a good reason for this behaviour, though I\'m frankly, I don\'t know what it is. If anyone does, I\'d be interested.\\n\\n## That\'s it\\n\\nArmed with the above I hope you have less compatibility mode pain than I have. The following blog entry is worth a read by the way:\\n\\n[https://blogs.msdn.com/b/ie/archive/2009/02/16/just-the-facts-recap-of-compatibility-view.aspx](https://blogs.msdn.com/b/ie/archive/2009/02/16/just-the-facts-recap-of-compatibility-view.aspx)\\n\\nFinally, I have an open question about compatibility mode. I _think_ (but I don\'t know) that even in compatibility mode IE runs using the same JavaScript engine. However I suspect it has a different DOM to play with. If anyone knows a little more about this and wants to let me know that\'d be fantastic."},{"id":"decimalmodelbinder-for-nullable-decimals","metadata":{"permalink":"/decimalmodelbinder-for-nullable-decimals","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2013-03-11-decimalmodelbinder-for-nullable-decimals/index.md","source":"@site/blog/2013-03-11-decimalmodelbinder-for-nullable-decimals/index.md","title":"DecimalModelBinder for nullable Decimals","description":"John forgot that MVCs ModelBinding doesnt handle regionalised numbers well. Provides solution found on Phil Haacks post.","date":"2013-03-11T00:00:00.000Z","tags":[{"inline":false,"label":"Globalize","permalink":"/tags/globalize","description":"The Globalize library."},{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":1.78,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"decimalmodelbinder-for-nullable-decimals","title":"DecimalModelBinder for nullable Decimals","authors":"johnnyreilly","tags":["globalize","asp.net"],"hide_table_of_contents":false,"description":"John forgot that MVCs ModelBinding doesnt handle regionalised numbers well. Provides solution found on Phil Haacks post."},"unlisted":false,"prevItem":{"title":"Death to compatibility mode","permalink":"/death-to-compatibility-mode"},"nextItem":{"title":"Unit testing ModelState","permalink":"/unit-testing-modelstate"}},"content":"My memory appears to be a sieve. Twice in the last year I\'ve forgotten that MVCs ModelBinding doesn\'t handle regionalised numbers terribly well. Each time I\'ve thought \\"hmmmm.... best Google that\\" and lo and behold come upon this post on the issue by the fantastic Phil Haack:\\n\\n\x3c!--truncate--\x3e\\n\\n[http://haacked.com/archive/2011/03/19/fixing-binding-to-decimals.aspx](http://haacked.com/archive/2011/03/19/fixing-binding-to-decimals.aspx)\\n\\nThis post has got me 90% of the way there, the last 10% being me tweaking it so the model binder can handle nullable decimals as well.\\n\\nIn the expectation I that I may forget this again I thought I\'d note down my tweaks now and hopefully save myself sometime when I\'m next looking at this next...\\n\\n```cs\\nusing System;\\nusing System.Globalization;\\nusing System.Web.Mvc;\\n\\nnamespace My.ModelBinders\\n{\\n    /// <summary>\\n    /// Thank you Phil Haack: used to model bind multiple culture decimals\\n    /// http://haacked.com/archive/2011/03/19/fixing-binding-to-decimals.aspx\\n    ///\\n    /// Use by adding these 2 lines to Application_Start in Global.asax.cs:\\n    ///\\n    /// System.Web.Mvc.ModelBinders.Binders.Add(typeof(decimal), new ModelBinders.DecimalModelBinder());\\n    /// System.Web.Mvc.ModelBinders.Binders.Add(typeof(decimal?), new ModelBinders.DecimalModelBinder());\\n    /// </summary>\\n    public class DecimalModelBinder : IModelBinder\\n    {\\n        public object BindModel(ControllerContext controllerContext,\\n            ModelBindingContext bindingContext)\\n        {\\n            ValueProviderResult valueResult = bindingContext.ValueProvider\\n                .GetValue(bindingContext.ModelName);\\n            ModelState modelState = new ModelState { Value = valueResult };\\n            object actualValue = null;\\n            try\\n            {\\n                //Check if this is a nullable decimal and a null or empty string has been passed\\n                var isNullableAndNull = (bindingContext.ModelMetadata.IsNullableValueType &&\\n                                         string.IsNullOrEmpty(valueResult.AttemptedValue));\\n\\n                //If not nullable and null then we should try and parse the decimal\\n                if (!isNullableAndNull)\\n                {\\n                    actualValue = decimal.Parse(valueResult.AttemptedValue, NumberStyles.Any, CultureInfo.CurrentCulture);\\n                }\\n            }\\n            catch (FormatException e)\\n            {\\n                modelState.Errors.Add(e);\\n            }\\n\\n            bindingContext.ModelState.Add(bindingContext.ModelName, modelState);\\n            return actualValue;\\n        }\\n    }\\n}\\n```\\n\\n## And now a question...\\n\\nWhy hasn\'t MVC got an out-of-the-box model binder that does this anyway? In Phil Haack\'s original post it looks like they were considering putting this into MVC itself at some point:\\n\\n\\"_... In that case, the DefaultModelBinder chokes on the value. This is unfortunate because jQuery Validate allows that value just fine. I\u2019ll talk to the rest of my team about whether we should fix this in the next version of ASP.NET MVC, but for now it\u2019s good to know there\u2019s a workaround..._\\"\\n\\nIf anyone knows the reason this never made it into core I\'d love to know. Maybe there\'s a good reason?"},{"id":"unit-testing-modelstate","metadata":{"permalink":"/unit-testing-modelstate","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2013-03-03-unit-testing-modelstate/index.md","source":"@site/blog/2013-03-03-unit-testing-modelstate/index.md","title":"Unit testing ModelState","description":"Testing Model validation in ASP.NET MVC can be accomplished by making use of ModelStateTestController class which simulates the functional tests.","date":"2013-03-03T00:00:00.000Z","tags":[{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."},{"inline":false,"label":"Automated Testing","permalink":"/tags/automated-testing","description":"How to perform the automation of tests."}],"readingTime":5.27,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"unit-testing-modelstate","title":"Unit testing ModelState","authors":"johnnyreilly","tags":["asp.net","automated testing"],"hide_table_of_contents":false,"description":"Testing Model validation in ASP.NET MVC can be accomplished by making use of ModelStateTestController class which simulates the functional tests."},"unlisted":false,"prevItem":{"title":"DecimalModelBinder for nullable Decimals","permalink":"/decimalmodelbinder-for-nullable-decimals"},"nextItem":{"title":"Unit testing MVC controllers / Mocking UrlHelper","permalink":"/unit-testing-mvc-controllers-mocking"}},"content":"- Me: \\"It can\'t be done\\"\\n- Him: \\"Yes it can\\"\\n- Me: \\"No it can\'t\\"\\n- Him: \\"Yes it can, I\'ve just done it\\"\\n- Me: \\"Ooooh! Show me ...\\"\\n\\n\x3c!--truncate--\x3e\\n\\nThe above conversation (or one much like it) took place between my colleague Marc Talary and myself a couple of weeks ago. It was one of those faintly embarrassing situations where you state your case with absolute certainty only to subsequently discover that you were \\\\***completely**\\\\* wrong. Ah arrogance, thy name is Reilly...\\n\\nThe disputed situation in this case was ModelState validation in ASP.Net MVC. How can you unit test a models validation driven by `DataAnnotations`? If at all. Well it can be done, and here\'s how.\\n\\n## Simple scenario\\n\\nLet\'s start with a simple model:\\n\\n```cs\\nusing System;\\nusing System.ComponentModel.DataAnnotations;\\n\\nnamespace MyNamespace.Model\\n{\\n    public class CarModel\\n    {\\n        [Required,\\n         Display(Name = \\"Purchased\\"),\\n         DisplayFormat(DataFormatString = \\"{0:d}\\", ApplyFormatInEditMode = true)]\\n        public DateTime Purchased { get; set; }\\n\\n        [Required,\\n         Display(Name = \\"Colour\\")]\\n        public string Colour{ get; set; }\\n    }\\n}\\n```\\n\\nAnd let\'s have a controller which makes use of that model:\\n\\n```cs\\nusing System.Web.Mvc;\\n\\nnamespace MyApp\\n{\\n    public class CarController : Controller\\n    {\\n        //...\\n\\n        public ActionResult Edit(CarModel model)\\n        {\\n            if (ModelState.IsValid) {\\n              //Save the model\\n              return View(\\"Details\\", model);\\n            }\\n\\n            return View(model);\\n        }\\n\\n        //...\\n    }\\n}\\n```\\n\\nWhen I was first looking at unit testing this I was slightly baffled by the behaviour I witnessed. I took an invalid model (where the properties set on the model were violating the model\'s validation `DataAnnotations`):\\n\\n```cs\\nvar car = new CarModel\\n{\\n    Puchased = null, //This is a required property and so this value is invalid\\n    Colour = null //This is a required property and so this value is invalid\\n};\\n```\\n\\nI passed the invalid model to the `Edit` controller action inside a unit test. My expectation was that the `ModelState.IsValid` code path would \\\\***not**\\\\* be followed as this was \\\\***not**\\\\* a valid model. So `ModelState.IsValid` should evaluate to `false`, right? Wrong!\\n\\nContrary to my expectation the validity of `ModelState` is not evaluated on the fly inside the controller. Rather it is determined during the model binding that takes place \\\\***before**\\\\* the actual controller action method is called. And that completely explains why during my unit test with an invalid model we find we\'re following the `ModelState.IsValid` code path.\\n\\n## Back to the dispute\\n\\nAs this blog post started off I was slightly missing Marc\'s point. I thought he was saying we should be testing the `ModelState.IsValid == false` code path. And given that `ModelState` is determined before we reach the controller my view was that the only way to achieve this was through making use of `ModelState.AddModelError` in our unit test (you can read a good explanation of that [here](http://stackoverflow.com/a/3816143/761388)). And indeed we were already testing for this; we were surfacing errors via a `JsonResult` and so had a test in place to ensure that `ModelState` errors were transformed in the manner we would expect.\\n\\nHowever, Marc\'s point was actually that we should have unit tests that enforced our design. That is to say, if we\'d decided a certain property on a model was mandatory we should have a test that checked that this was indeed the case. If someone came along later and removed the `Required` data annotation then we wanted that test to fail.\\n\\nIt\'s worth saying, we didn\'t want a unit test to ensure that ASP.Net MVC worked as expected. Rather, where we had used DataAnnotations against our models to drive validation, we wanted to ensure the validation didn\'t disappear further down the track. Just to be clear: we wanted to test our code, not Microsoft\'s.\\n\\n## Now I get to learn something\\n\\nWhen I grasped Marc\'s point I thought that the the only way to write these tests would be to make use of reflection. And whilst we could certainly do that I wasn\'t entirely happy with that as a solution. To my mind it was kind of testing \\"at one remove\\", if you see what I mean. What I really wanted was to see that MVC was surfacing validations in the manner I might have hoped. And you can!\\n\\n.... Drum roll... Ladies and gents may I present Marc\'s `ModelStateTestController`:\\n\\n```cs\\nusing System.Web.Mvc;\\nusing Moq;\\n\\nnamespace UnitTests.TestUtilities\\n{\\n    /// <summary>\\n    /// Instance of a controller for testing things that use controller methods i.e. controller.TryValidateModel(model)\\n    /// </summary>\\n    public class ModelStateTestController : Controller\\n    {\\n        public ModelStateTestController()\\n        {\\n            ControllerContext = (new Mock<ControllerContext>()).Object;\\n        }\\n\\n        public bool TestTryValidateModel(object model)\\n        {\\n            return TryValidateModel(model);\\n        }\\n    }\\n}\\n```\\n\\nThis class is, as you can see, incredibly simple. It is a controller, it inherits from `System.Web.Mvc.Controller` and establishes a mock context in the constructor using MOQ. This controller exposes a single method: `TestTryValidateModel`. This method internally determines the controller\'s `ModelState` given the supplied object by calling off to Mvc\'s (protected) `TryValidateModel` method (`TryValidateModel` evaluates `ModelState`).\\n\\nThis simple class allows us to test the validations on a model in a simple fashion that stays close to the way our models will actually be used in the wild. It\'s pragmatic and it\'s useful.\\n\\n## An example\\n\\nLet me wrap up with an example unit test. The test below makes use of the `ModelStateTestController` to check the application of the DataAnnotations on our model:\\n\\n```cs\\n[TestMethod]\\npublic void Unit_Test_CarModel_ModelState_validations_are_thrown()\\n{\\n    // Arrange\\n    var controller = new ModelStateTestController();\\n    var car = new CarModel\\n    {\\n        Puchased = null, //This is a required property and so this value is invalid\\n        Colour = null //This is a required property and so this value is invalid\\n    };\\n\\n    // Act\\n    var result = controller.TestTryValidateModel(company);\\n\\n    // Assert\\n    Assert.IsFalse(result);\\n\\n    var modelState = controller.ModelState;\\n\\n    Assert.AreEqual(2, modelState.Keys.Count);\\n\\n    Assert.IsTrue(modelState.Keys.Contains(\\"Purchased\\"));\\n    Assert.IsTrue(modelState[\\"Purchased\\"].Errors.Count == 1);\\n    Assert.AreEqual(\\"The Purchased field is required.\\", modelState[\\"Purchased\\"].Errors[0].ErrorMessage);\\n\\n    Assert.IsTrue(modelState.Keys.Contains(\\"Colour\\"));\\n    Assert.IsTrue(modelState[\\"Colour\\"].Errors.Count == 1);\\n    Assert.AreEqual(\\"The Colour field is required.\\", modelState[\\"Colour\\"].Errors[0].ErrorMessage);\\n}\\n```\\n\\n## Wrapping up\\n\\nIn a way I think it\'s a shame that `TryValidateModel` is a protected method. If it weren\'t it would be simplicity to write a unit test which tested the ModelState directly in context of the action method. It would be possible to get round this by establishing a base controller class which all our controllers would inherit from which implemented the `TestTryValidateModel` method from above. On the other hand maybe it\'s good to have clarity of the difference between testing model validations and testing controller actions. Something to ponder..."},{"id":"unit-testing-mvc-controllers-mocking","metadata":{"permalink":"/unit-testing-mvc-controllers-mocking","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2013-02-18-unit-testing-mvc-controllers-mocking/index.md","source":"@site/blog/2013-02-18-unit-testing-mvc-controllers-mocking/index.md","title":"Unit testing MVC controllers / Mocking UrlHelper","description":"This article presents a solution for testing ASP.net MVC controllers, including how to test controllers using UrlHelper.","date":"2013-02-18T00:00:00.000Z","tags":[{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":5.72,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"unit-testing-mvc-controllers-mocking","title":"Unit testing MVC controllers / Mocking UrlHelper","authors":"johnnyreilly","tags":["asp.net"],"hide_table_of_contents":false,"description":"This article presents a solution for testing ASP.net MVC controllers, including how to test controllers using UrlHelper."},"unlisted":false,"prevItem":{"title":"Unit testing ModelState","permalink":"/unit-testing-modelstate"},"nextItem":{"title":"Using Expressions with Constructors","permalink":"/using-expressions-with-constructors"}},"content":"## I have put a name to my pain...\\n\\n\x3c!--truncate--\x3e\\n\\nAnd it is unit testing ASP.Net MVC controllers.\\n\\nWell perhaps that\'s unfair. I have no problem unit testing MVC controllers.... **until** it comes to making use of the \\"innards\\" of MVC. Let me be more specific. This week I had a controller action that I needed to test. It looked a little like this:\\n\\n```cs\\nusing System.Collections.Generic;\\nusing System.Linq;\\nusing System.Web.Mvc;\\n\\nnamespace DemoApp.Areas.Demo.Controllers\\n{\\n    public class DemoController : System.Web.Mvc.Controller\\n    {\\n        //....\\n\\n        public JsonResult Edit(AnObject anObject)\\n        {\\n            //Indicate to the client we have saved and pass back the redirect URL\\n            return Json(new {\\n                Saved = true,\\n                RedirectUrl = Url.Action(\\"Details\\", anObject.AnotherTypeOfId)\\n                });\\n        }\\n\\n        //....\\n    }\\n}\\n```\\n\\nLooks fine right? It\'s an action that takes a simple object as an argument. That\'s ok. It returns a JsonResult. No worries. The JsonResult consists of an anonymous class. De nada. The anonymous class has one property that is driven by the controllers `UrlHelper`. Yeah that shouldn\'t be an issue... **Hold your horses sunshine - you\'re going nowhere!**\\n\\n## Getting disillusioned\\n\\nYup, the minute you start pumping in asserts around that `UrlHelper` driven property you\'re going to be mighty disappointed. What, you didn\'t expect the result to be `null`? Damn shame.\\n\\nDespite [articles](http://msdn.microsoft.com/en-us/magazine/dd942838.aspx) on MSDN about how the intention is for MVC to be deliberately testable the sad fact of the matter is that there is a yawning hole around the testing support for controllers in ASP.Net MVC. Whenever you try to test something that makes use of controller \\"gubbins\\" you have **serious** problems. And unfortunately I didn\'t find anyone out there who could offer the whole solution.\\n\\nAfter what I can best describe as a day of pain I found a way to scratch my particular itch. I found a way to write unit tests for controllers that made use of UrlHelper. As a bonus I managed to include the unit testing of Routes and Areas (well kind of) too.\\n\\n## MvcMockControllers updated\\n\\nThis solution is heavily based on the work of Scott Hanselman who [wrote and blogged about `MvcMockHelpers`](http://www.hanselman.com/blog/ASPNETMVCSessionAtMix08TDDAndMvcMockHelpers.aspx) back in 2008. Essentially I\'ve taken this and tweaked it so I could achieve my ends. My version of `MvcMockHelpers` looks a little like this:\\n\\n```cs\\nusing Moq;\\nusing System;\\nusing System.Collections.Specialized;\\nusing System.Web;\\nusing System.Web.Mvc;\\nusing System.Web.Routing;\\n\\nnamespace UnitTest.TestUtilities\\n{\\n    /// <summary>\\n    /// This class of MVC Mock helpers is originally based on Scott Hanselman\'s 2008 post:\\n    /// http://www.hanselman.com/blog/ASPNETMVCSessionAtMix08TDDAndMvcMockHelpers.aspx\\n    ///\\n    /// This has been updated and tweaked to work with MVC 3 / 4 projects (it hasn\'t been tested with MVC\\n    /// 1 / 2 but may work there) and also based my use cases\\n    /// </summary>\\n    public static class MvcMockHelpers\\n    {\\n        #region Mock HttpContext factories\\n\\n        public static HttpContextBase MockHttpContext()\\n        {\\n            var context = new Mock<HttpContextBase>();\\n            var request = new Mock<HttpRequestBase>();\\n            var response = new Mock<HttpResponseBase>();\\n            var session = new Mock<HttpSessionStateBase>();\\n            var server = new Mock<HttpServerUtilityBase>();\\n\\n            request.Setup(r => r.AppRelativeCurrentExecutionFilePath).Returns(\\"/\\");\\n            request.Setup(r => r.ApplicationPath).Returns(\\"/\\");\\n\\n            response.Setup(s => s.ApplyAppPathModifier(It.IsAny<string>())).Returns<string>(s => s);\\n            response.SetupProperty(res => res.StatusCode, (int)System.Net.HttpStatusCode.OK);\\n\\n            context.Setup(h => h.Request).Returns(request.Object);\\n            context.Setup(h => h.Response).Returns(response.Object);\\n\\n            context.Setup(ctx => ctx.Request).Returns(request.Object);\\n            context.Setup(ctx => ctx.Response).Returns(response.Object);\\n            context.Setup(ctx => ctx.Session).Returns(session.Object);\\n            context.Setup(ctx => ctx.Server).Returns(server.Object);\\n\\n            return context.Object;\\n        }\\n\\n        public static HttpContextBase MockHttpContext(string url)\\n        {\\n            var context = MockHttpContext();\\n            context.Request.SetupRequestUrl(url);\\n            return context;\\n        }\\n\\n        #endregion\\n\\n        #region Extension methods\\n\\n        public static void SetMockControllerContext(this Controller controller,\\n            HttpContextBase httpContext = null,\\n            RouteData routeData = null,\\n            RouteCollection routes = null)\\n        {\\n            //If values not passed then initialise\\n            routeData = routeData ?? new RouteData();\\n            routes = routes ?? RouteTable.Routes;\\n            httpContext = httpContext ?? MockHttpContext();\\n\\n            var requestContext = new RequestContext(httpContext, routeData);\\n            var context = new ControllerContext(requestContext, controller);\\n\\n            //Modify controller\\n            controller.Url = new UrlHelper(requestContext, routes);\\n            controller.ControllerContext = context;\\n        }\\n\\n        public static void SetHttpMethodResult(this HttpRequestBase request, string httpMethod)\\n        {\\n            Mock.Get(request).Setup(req => req.HttpMethod).Returns(httpMethod);\\n        }\\n\\n        public static void SetupRequestUrl(this HttpRequestBase request, string url)\\n        {\\n            if (url == null)\\n                throw new ArgumentNullException(\\"url\\");\\n\\n            if (!url.StartsWith(\\"~/\\"))\\n                throw new ArgumentException(\\"Sorry, we expect a virtual url starting with \\\\\\"~/\\\\\\".\\");\\n\\n            var mock = Mock.Get(request);\\n\\n            mock.Setup(req => req.QueryString).Returns(GetQueryStringParameters(url));\\n            mock.Setup(req => req.AppRelativeCurrentExecutionFilePath).Returns(GetUrlFileName(url));\\n            mock.Setup(req => req.PathInfo).Returns(string.Empty);\\n        }\\n\\n\\n        /// <summary>\\n        /// Facilitates unit testing of anonymouse types - taken from here:\\n        /// http://stackoverflow.com/a/5012105/761388\\n        /// </summary>\\n        public static object GetReflectedProperty(this object obj, string propertyName)\\n        {\\n            obj.ThrowIfNull(\\"obj\\");\\n            propertyName.ThrowIfNull(\\"propertyName\\");\\n\\n            var property = obj.GetType().GetProperty(propertyName);\\n\\n            if (property == null)\\n                return null;\\n\\n            return property.GetValue(obj, null);\\n        }\\n\\n        public static T ThrowIfNull<T>(this T value, string variableName) where T : class\\n        {\\n            if (value == null)\\n                throw new NullReferenceException(\\n                    string.Format(\\"Value is Null: {0}\\", variableName));\\n\\n            return value;\\n        }\\n\\n        #endregion\\n\\n        #region Private\\n\\n        static string GetUrlFileName(string url)\\n        {\\n            return (url.Contains(\\"?\\"))\\n                ? url.Substring(0, url.IndexOf(\\"?\\"))\\n                : url;\\n        }\\n\\n        static NameValueCollection GetQueryStringParameters(string url)\\n        {\\n            if (url.Contains(\\"?\\"))\\n            {\\n                var parameters = new NameValueCollection();\\n\\n                var parts = url.Split(\\"?\\".ToCharArray());\\n                var keys = parts[1].Split(\\"&\\".ToCharArray());\\n\\n                foreach (var key in keys)\\n                {\\n                    var part = key.Split(\\"=\\".ToCharArray());\\n                    parameters.Add(part[0], part[1]);\\n                }\\n\\n                return parameters;\\n            }\\n\\n            return null;\\n        }\\n\\n        #endregion\\n    }\\n}\\n```\\n\\n## What I want to test\\n\\nI want to be able to unit test the controller `Edit` method I mentioned earlier. This method calls the `Action` method on the controllers `Url` member (which is, in turn, a `UrlHelper`) to generate a URL for passing pack to the client. The URL generated should fit with the routing mechanism I have set up. In this case the route we expect a URL for was mapped by the following area registration:\\n\\n```cs\\nusing System.Web.Mvc;\\n\\nnamespace DemoApp.Areas.Demo\\n{\\n    public class DemoAreaRegistration : AreaRegistration\\n    {\\n        public override string AreaName\\n        {\\n            get\\n            {\\n                return \\"DemoArea\\";\\n            }\\n        }\\n\\n        public override void RegisterArea(AreaRegistrationContext context)\\n        {\\n            context.MapRoute(\\n                \\"DemoArea_default\\",\\n                \\"Demo/{oneTypeOfId}/{anotherTypeOfId}/{controller}/{action}/{id}\\",\\n                new { oneTypeOfId = 0, anotherTypeOfId = 0, action = \\"Index\\", id = UrlParameter.Optional }\\n            );\\n        }\\n    }\\n}\\n```\\n\\n## Enough of the waffle - show me a unit test\\n\\nNow to the meat; here\'s a unit test which demonstrates how this is used:\\n\\n```cs\\nusing System.Collections.Generic;\\nusing System.Linq;\\nusing System.Web.Mvc;\\nusing System.Web.Routing;\\nusing Microsoft.VisualStudio.TestTools.UnitTesting;\\nusing Moq;\\n\\nnamespace UnitTest.Areas.Demo.Controllers\\n{\\n    [TestClass]\\n    public class UnitTestingAnAreaUsingUrlHelper\\n    {\\n        private DemoController _controller;\\n\\n        [TestInitialize]\\n        public void InitializeTest()\\n        {\\n            _controller = new DemoController();\\n        }\\n\\n        [TestMethod]\\n        public void Edit_updates_the_object_and_returns_a_JsonResult_containing_the_redirect_URL()\\n        {\\n            // Arrange\\n            int anotherTypeOfId = 5332;\\n\\n            //Register the area as well as standard routes\\n            RouteTable.Routes.Clear();\\n            var areaRegistration = new DemoAreaRegistration();\\n            var areaRegistrationContext = new AreaRegistrationContext(\\n                areaRegistration.AreaName, RouteTable.Routes);\\n            areaRegistration.RegisterArea(areaRegistrationContext);\\n\\n            RouteConfig.RegisterRoutes(RouteTable.Routes);\\n\\n            //Initialise the controller and setup the context so MVC can pick up the relevant route data\\n            var httpContext = MvcMockHelpers.MockHttpContext(\\n                \\"~/Demo/77969/\\" + anotherTypeOfId + \\"/Company/Edit\\");\\n            var routeData = RouteTable.Routes.GetRouteData(httpContext);\\n            _controller.SetMockControllerContext(\\n                httpContext, routeData, RouteTable.Routes);\\n\\n            // Act\\n            var result = _controller.Edit(\\n                new AnObject{\\n                    WithAProperty = \\"Something\\",\\n                    AnotherTypeOfId = anotherTypeOfId });\\n\\n            // Assert\\n            Assert.AreEqual(\\"DemoArea\\", areaRegistration.AreaName);\\n\\n            Assert.IsInstanceOfType(result, typeof(JsonResult));\\n\\n            Assert.IsNotNull(result.Data,\\n                \\"There should be some data for the JsonResult\\");\\n            Assert.AreEqual(true,\\n                result.Data.GetReflectedProperty(\\"Saved\\"));\\n            Assert.AreEqual(\\"/Demo/77969/\\" + anotherTypeOfId + \\"/Company/Details\\",\\n                result.Data.GetReflectedProperty(\\"RedirectUrl\\"));\\n        }\\n\\n    }\\n}\\n```\\n\\nLet\'s go through this unit test and breakdown what\'s happening:\\n\\n1. Arrange\\n2. Act\\n3. Assert\\n\\nThe most interesting thing you\'ll note is the controller\'s UrlHelper is now generating a URL as we might have hoped. The URL is generated making use of our routing, yay! Finally we\'re also managing to unit test a route registered by our area."},{"id":"using-expressions-with-constructors","metadata":{"permalink":"/using-expressions-with-constructors","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2013-02-13-using-expressions-with-constructors/index.md","source":"@site/blog/2013-02-13-using-expressions-with-constructors/index.md","title":"Using Expressions with Constructors","description":"This article explains how John used LINQs expression to extend a validation class and automatically change the property name.","date":"2013-02-13T00:00:00.000Z","tags":[{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":2.835,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"using-expressions-with-constructors","title":"Using Expressions with Constructors","authors":"johnnyreilly","tags":["asp.net"],"hide_table_of_contents":false,"description":"This article explains how John used LINQs expression to extend a validation class and automatically change the property name."},"unlisted":false,"prevItem":{"title":"Unit testing MVC controllers / Mocking UrlHelper","permalink":"/unit-testing-mvc-controllers-mocking"},"nextItem":{"title":"Twitter.Bootstrap.MVC4 meet Bootstrap Datepicker *and* get your Internationalization on...","permalink":"/twitterbootstrapmvc4-meet-bootstrap_14"}},"content":"Every now and then you think \\"x should be easy\\" - and it isn\'t. I had one of those situations this morning. Something I thought would take 5 minutes had me still pondering 30 minutes later. I finally cracked it (with the help of a colleague - thanks Marc!) and I wanted to note down what I did since I\'m sure to forget this.\\n\\n\x3c!--truncate--\x3e\\n\\n## So what\'s the problem?\\n\\nIn our project we had a very simple validation class. It looked a bit like this:\\n\\n```cs\\n    public class FieldValidation\\n    {\\n        public FieldValidation(string fieldName, string message)\\n        {\\n            FieldName = fieldName;\\n            Message = message;\\n        }\\n\\n        public string FieldName { get; set; }\\n        public string Message { get; set; }\\n    }\\n```\\n\\nI wanted to take this class and extend it to have a constructor which allowed me to specify a Type and subsequently an Expression of that Type that allowed me to specify a property. 10 points if you read the last sentence and understood it without reading it a second time.\\n\\nCode is a better illustration; take a look below. I wanted to go from #1 to #2:\\n\\n```cs\\n//#1 Specify field name up front - how we currently use this\\nvar oldSchoolValidation = new FieldValidation(\\n  \\"WithAProperty\\", \\"Message of some kind...\\");\\n\\n//#2 Field name driven directly by property - how we want to use this\\nvar newSchoolValidation = new FieldValidation<AnObject>(\\n  x => x.WithAProperty, \\"Message of some kind...\\");\\n\\n/// <summary>\\n/// Example class for demo\\n/// </summary>\\npublic class AnObject\\n{\\n  public bool WithAProperty { get; set; }\\n}\\n```\\n\\n\\"Why?\\" I hear you ask. Well we had a swathe of statements in the code which test each property for a problem and would create a `FieldValidation` with the very same property name if one was found. There\'s no real problem with that but I\'m a man that likes to refactor. Property names change and I didn\'t want to have to remember to manually go through each `FieldValidation` keeping these in line. If I was using the actual property name to drive the creation of my `FieldValidations` then that problem disappears. And I like that.\\n\\n## So what\'s the solution?\\n\\nWell it\'s this:\\n\\n```cs\\n    public class FieldValidation\\n    {\\n        public FieldValidation(string fieldName, string message)\\n        {\\n            FieldName = fieldName;\\n            Message = message;\\n        }\\n\\n        public string FieldName { get; set; }\\n        public string Message { get; set; }\\n    }\\n\\n    public class FieldValidation<T> : FieldValidation where T : class\\n    {\\n        public FieldValidation(\\n            Expression<Func<T, object>> expression,\\n            string message)\\n        {\\n            //Will work for reference types\\n            var body = expression.Body as MemberExpression;\\n\\n            if (body == null)\\n            {\\n                //Will work for value types\\n                var uBody = (UnaryExpression)expression.Body;\\n                body = uBody.Operand as MemberExpression;\\n            }\\n\\n\\n            if (body == null)\\n                throw new ArgumentException(\\"Invalid property expression\\");\\n\\n            FieldName = body.Member.Name;\\n            Message = message;\\n        }\\n    }\\n```\\n\\nAs you can see we have taken the original FieldValidation class and added in a generic constructor which instead of taking `string fieldName` as a first argument it takes `Expression&lt;Func&lt;T, object&gt;&gt; expression`. LINQ\'s Expression magic is used to determine the supplied property name which is smashing. If you were wondering, the first `MemberExpression` code is used for _reference_ types. The `UnaryExpression` wrapping a `MemberExpression` code is used for _value_ types. A good explanation of this can be found [here](http://stackoverflow.com/a/12975480/761388).\\n\\nMy colleague directed me to [this crucial StackOverflow answer](http://stackoverflow.com/a/2916344) which provided some much needed direction when I was thrashing. And that\'s it; we\'re done, home free."},{"id":"twitterbootstrapmvc4-meet-bootstrap_14","metadata":{"permalink":"/twitterbootstrapmvc4-meet-bootstrap_14","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2013-01-14-twitterbootstrapmvc4-meet-bootstrap_14/index.md","source":"@site/blog/2013-01-14-twitterbootstrapmvc4-meet-bootstrap_14/index.md","title":"Twitter.Bootstrap.MVC4 meet Bootstrap Datepicker *and* get your Internationalization on...","description":"Learn how to internationalize ASP.NET web apps using Globalize and Bootstrap Datepicker in this developers comprehensive step by step guide.","date":"2013-01-14T00:00:00.000Z","tags":[{"inline":false,"label":"Globalize","permalink":"/tags/globalize","description":"The Globalize library."},{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."}],"readingTime":5.565,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"twitterbootstrapmvc4-meet-bootstrap_14","title":"Twitter.Bootstrap.MVC4 meet Bootstrap Datepicker *and* get your Internationalization on...","authors":"johnnyreilly","tags":["globalize","javascript"],"hide_table_of_contents":false,"description":"Learn how to internationalize ASP.NET web apps using Globalize and Bootstrap Datepicker in this developers comprehensive step by step guide."},"unlisted":false,"prevItem":{"title":"Using Expressions with Constructors","permalink":"/using-expressions-with-constructors"},"nextItem":{"title":"Twitter.Bootstrap.MVC4 meet Bootstrap Datepicker","permalink":"/twitterbootstrapmvc4-meet-bootstrap"}},"content":"[Last time](../2013-01-09-twitterbootstrapmvc4-meet-bootstrap/index.md) I wrote about marrying up Twitter.Bootstrap.MVC4 and Bootstrap Datepicker. It came together quite nicely but when I took a more in depth look at what I\'d done I discovered a problem. The brief work on regionalisation / internationalisation / localisation / globalisation / whatever it\'s called this week... wasn\'t really working. We had problems with the validation.\\n\\n\x3c!--truncate--\x3e\\n\\nI also discovered that Stefan Petre\'s Bootstrap Datepicker appears to have been abandoned. Andrew Rowls has taken it on and created a GitHub repository for it [here](https://github.com/eternicode/bootstrap-datepicker). Besides bug fixes he\'s also introduced the ability for the Bootstrap Datepicker to customised for different cultures.\\n\\nSince these 2 subjects are linked I tackled them together and thought it might be worth writing up here. You can find the conclusion of my work in a GitHub repository I created [here](https://github.com/johnnyreilly/BootstrapMvcSample).\\n\\n## Going global down in Acapulco\\n\\nFirst step in internationalising any ASP.Net web app is adding the following to the `web.config`:\\n\\n```xml\\n<?xml version=\\"1.0\\" encoding=\\"utf-8\\"?>\\n<configuration>\\n  <system.web>\\n\\n    \x3c!-- Other stuff here... --\x3e\\n\\n    <globalization\\n      culture=\\"auto\\"\\n      uiCulture=\\"auto\\"\\n      enableClientBasedCulture=\\"true\\" />\\n  </system.web>\\n\\n  \x3c!-- ...and here --\x3e\\n\\n</configuration>\\n```\\n\\nThen I pulled [Globalize](https://github.com/jquery/globalize) and the [Andrew Rowls fork of Bootstrap Datepicker](https://github.com/eternicode/bootstrap-datepicker) into the project (replacing Stefan\'s original assets). As well as this I pulled in the `jQuery.validate.globalize.js` extension [I wrote about here](../2012-09-06-globalize-and-jquery-validate/index.md). (This replaces some of the default jQuery Validate functionality for culture-specific functionality based on Globalize.) This extension depends on a meta tag that is generated using the following file (which also handles the serving up of the relevant JavaScript culture bundles, more of which shortly):\\n\\n```cs\\nusing System;\\nusing System.Collections.Generic;\\nusing System.IO;\\nusing System.Globalization;\\nusing System.Linq;\\nusing System.Web;\\n\\nnamespace System.Web.Mvc\\n{\\n    public static class GlobalizationHelpers\\n    {\\n        /// <summary>\\n        /// Taken from Scott Hanselman\'s blog post: http://www.hanselman.com/blog/GlobalizationInternationalizationAndLocalizationInASPNETMVC3JavaScriptAndJQueryPart1.aspx\\n        /// </summary>\\n        /// <typeparam name=\\"t\\"></typeparam>\\n        /// <param name=\\"htmlHelper\\"></param>\\n        /// <returns></returns>\\n        public static IHtmlString MetaAcceptLanguage<t>(this HtmlHelper<t> htmlHelper)\\n        {\\n            var acceptLanguage = HttpUtility.HtmlAttributeEncode(CultureInfo.CurrentUICulture.ToString());\\n            return new HtmlString(string.Format(\\"<meta name=\\\\\\"accept-language\\\\\\" content=\\\\\\"{0}\\\\\\" />\\", acceptLanguage));\\n        }\\n\\n        /// <summary>\\n        /// Return the JavaScript bundle for this users culture\\n        /// </summary>\\n        /// <typeparam name=\\"t\\"></typeparam>\\n        /// <param name=\\"htmlHelper\\"></param>\\n        /// <returns>a culture bundle that looks something like this: \\"~/js-culture.en-GB\\"</returns>\\n        public static string JsCultureBundle<t>(this HtmlHelper<t> htmlHelper)\\n        {\\n            return \\"~/js-culture.\\" + CultureInfo.CurrentUICulture.ToString();\\n        }\\n    }\\n}\\n```\\n\\n## Culture-specific script bundles\\n\\nWith all of my dependancies in place I was now ready to press on. Since both Globalize and the new Bootstrap Datepicker come with their own culture-specific JavaScript files it seemed a good idea to make use of ASP.Nets new bundling functionality. This I did here:\\n\\n```cs\\nusing System;\\nusing System.Web;\\nusing System.Web.Mvc;\\nusing System.Web.Optimization;\\nusing System.Globalization;\\nusing System.IO;\\nusing System.Linq;\\n\\nnamespace BootstrapSupport\\n{\\n    public class BootstrapBundleConfig\\n    {\\n        public static void RegisterBundles(BundleCollection bundles)\\n        {\\n            bundles.Add(new ScriptBundle(\\"~/js\\").Include(\\n                \\"~/Scripts/jquery-*\\",\\n                \\"~/Scripts/globalize.js\\", //The Globalize library\\n                \\"~/Scripts/bootstrap.js\\",\\n                \\"~/Scripts/bootstrap-datepicker.js\\", //This is the brand new internationalised Bootstrap Datepicker\\n                \\"~/Scripts/jquery.validate.js\\",\\n                \\"~/Scripts/jquery.validate.unobtrusive.js\\",\\n                \\"~/Scripts/jquery.validate.unobtrusive-custom-for-bootstrap.js\\",\\n                \\"~/Scripts/jquery.validate.globalize.js\\" //My jQuery Validate extension which depends on Globalize\\n                ));\\n\\n            //Create culture specific bundles which contain the JavaScript files that should be served for each culture\\n            foreach (var culture in CultureInfo.GetCultures(CultureTypes.AllCultures))\\n            {\\n                bundles.Add(new ScriptBundle(\\"~/js-culture.\\" + culture.Name).Include( //example bundle name would be \\"~/js-culture.en-GB\\"\\n                    DetermineCultureFile(culture, \\"~/Scripts/globalize-cultures/globalize.culture.{0}.js\\"),             //The Globalize locale-specific JavaScript file\\n                    DetermineCultureFile(culture, \\"~/Scripts/bootstrap-datepicker-locales/bootstrap-datepicker.{0}.js\\") //The Bootstrap Datepicker locale-specific JavaScript file\\n                ));\\n            }\\n\\n            bundles.Add(new StyleBundle(\\"~/content/css\\").Include(\\n                \\"~/Content/bootstrap.css\\",\\n                \\"~/Content/bootstrap-datepicker.css\\"\\n                ));\\n\\n            bundles.Add(new StyleBundle(\\"~/content/css-responsive\\").Include(\\n                \\"~/Content/bootstrap-responsive.css\\"\\n                ));\\n        }\\n\\n        /// <summary>\\n        /// Given the supplied culture, determine the most appropriate Globalize culture script file that should be served up\\n        /// </summary>\\n        /// <param name=\\"culture\\"></param>\\n        /// <param name=\\"filePattern\\">a file pattern, eg \\"~/Scripts/globalize-cultures/globalize.culture.{0}.js\\"</param>\\n        /// <param name=\\"defaultCulture\\">Default culture string to use (eg \\"en-GB\\") if one cannot be found for the supplied culture</param>\\n        /// <returns></returns>\\n        private static string DetermineCultureFile(CultureInfo culture,\\n            string filePattern,\\n            string defaultCulture = \\"en-GB\\" // I\'m a Brit and this is my default\\n            )\\n        {\\n            //Determine culture - GUI culture for preference, user selected culture as fallback\\n            var regionalisedFileToUse = string.Format(filePattern, defaultCulture);\\n\\n            //Try to pick a more appropriate regionalisation if there is one\\n            if (File.Exists(HttpContext.Current.Server.MapPath(string.Format(filePattern, culture.Name)))) //First try for a globalize.culture.en-GB.js style file\\n                regionalisedFileToUse = string.Format(filePattern, culture.Name);\\n            else if (File.Exists(HttpContext.Current.Server.MapPath(string.Format(filePattern, culture.TwoLetterISOLanguageName)))) //That failed; now try for a globalize.culture.en.js style file\\n                regionalisedFileToUse = string.Format(filePattern, culture.TwoLetterISOLanguageName);\\n\\n            return regionalisedFileToUse;\\n        }\\n\\n    }\\n}\\n```\\n\\nThe code above creates a script bundle for each culture when the application starts up. This script bundle contains the culture-specific Globalize and Bootstrap Datepicker JavaScript files. If further culture-specific components were added to the application it would make sense to include these here as well.\\n\\n`_BootstrapLayout.basic.cshtml` has been amended to make use of the new bundles and also to include a meta tag that will used to drive regionalisation:\\n\\n```html\\n<!doctype html>\\n<html lang=\\"en\\">\\n  <head>\\n    \x3c!-- Existing head content goes here --\x3e\\n\\n    \x3c!-- Added to the head to serve a meta tag like this: <meta name=\\"accept-language\\" content=\\"en-GB\\" /> --\x3e\\n    @Html.MetaAcceptLanguage()\\n\\n    \x3c!-- Existing head content continues here --\x3e\\n  </head>\\n  <body>\\n    \x3c!-- Existing body content goes here --\x3e\\n\\n    \x3c!-- Replaces the existing @Scripts.Render --\x3e\\n    @Scripts.Render( \\"~/js\\", Html.JsCultureBundle() //Serves up the\\n    \\"~/js-culture.de-DE\\" bundle for example )\\n\\n    \x3c!-- Existing body content continues here --\x3e\\n  </body>\\n</html>\\n```\\n\\nTo illustrate how this works, a German user running a machine with the de-DE culture would be served up the following 2 files:\\n\\n- `globalize.culture.de-DE.js`\\n- `bootstrap-datepicker.de.js`\\n\\n## Where have we got to?\\n\\nWith all this done we have now fixed the validation issues we were experiencing previously. This was done by including the Globalize library, the accept-language meta tag and the jQuery Validate Globalize extensions.\\n\\nBesides this we\'ve laid the groundwork for introducing internationalised datepickers by introducing Andrew Rowls fork of the Bootstrap Datepicker. That\'s what we\'ll do next...\\n\\n## International Bootstrap Datepicker\\n\\nThe final steps of switching over to using a culture-specific date picker are achieved by making a change to the Scripts section in the `Create.cshtml` file. The existing (and very simple) section should be replaced with this:\\n\\n```cs\\n@section Scripts {\\n<script type=\\"text/javascript\\">\\n    var currentCulture = $(\\"meta[name=\'accept-language\']\\").prop(\\"content\\"),\\n        language;\\n    // Set Globalize to the current culture driven by the meta tag (if any)\\n    if (currentCulture) {\\n        language = (currentCulture in $.fn.datepicker.dates)\\n            ? currentCulture //a language exists which looks like \\"zh-CN\\" so we\'ll use it\\n            : currentCulture.split(\\"-\\")[0]; //we\'ll try for a language that looks like \\"de\\" and use it if it exists (otherwise it will fall back to the default)\\n    }\\n    //Initialise any date pickers\\n    $(\'.datepicker\').datepicker({ language: language });\\n<\/script>\\n}\\n```\\n\\nThe script above takes the region from the accept-language meta tag and attempts to look up an associated \\"language\\" for the Bootstrap Datepicker. If it finds one it uses it, if not then the default language of \\"en\\" / English will be used.\\n\\n## Summary\\n\\nIn this post we:\\n\\n1. fixed the validation issues we\'d introduced by marrying up Twitter.Bootstrap.MVC4 and the Bootstrap Datepicker\\n2. switched over to using the Andrew Rowls fork of Bootstrap Datepicker and made use of the internationalisation functionality it exposes."},{"id":"twitterbootstrapmvc4-meet-bootstrap","metadata":{"permalink":"/twitterbootstrapmvc4-meet-bootstrap","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2013-01-09-twitterbootstrapmvc4-meet-bootstrap/index.md","source":"@site/blog/2013-01-09-twitterbootstrapmvc4-meet-bootstrap/index.md","title":"Twitter.Bootstrap.MVC4 meet Bootstrap Datepicker","description":"Learn about responsive web design and how to incorporate Twitter Bootstrap and Bootstrap Datepicker into ASP.Net MVC projects in this beginner\u2019s guide.","date":"2013-01-09T00:00:00.000Z","tags":[{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."},{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."}],"readingTime":3.81,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"twitterbootstrapmvc4-meet-bootstrap","title":"Twitter.Bootstrap.MVC4 meet Bootstrap Datepicker","authors":"johnnyreilly","tags":["asp.net","javascript"],"hide_table_of_contents":false,"description":"Learn about responsive web design and how to incorporate Twitter Bootstrap and Bootstrap Datepicker into ASP.Net MVC projects in this beginner\u2019s guide."},"unlisted":false,"prevItem":{"title":"Twitter.Bootstrap.MVC4 meet Bootstrap Datepicker *and* get your Internationalization on...","permalink":"/twitterbootstrapmvc4-meet-bootstrap_14"},"nextItem":{"title":"HTML to PDF using a WCF Service","permalink":"/html-to-pdf-using-wcf-service"}},"content":"## Updated 14/01/2013\\n\\nSince I wrote this I\'ve taken things on a little further - to read about that go [here](../2013-01-14-twitterbootstrapmvc4-meet-bootstrap_14/index.md).\\n\\n\x3c!--truncate--\x3e\\n\\n## Getting Responsive\\n\\nIt\'s the new year, it\'s time for new things. Long on my list of \\"things to do\\" was getting up to speed with [Responsive web design](http://en.wikipedia.org/wiki/Responsive_web_design). No doubt like everyone else I\'ve been hearing more and more about this over the last year (by the way there was a [good article on Mashable](http://mashable.com/2012/12/11/responsive-web-design/) about this last month). RWD (in case you don\'t already know) is pretty much about having web interfaces that format their presentation based on the device they\'re running to provide a good user experience. (I kind of think of it as a [write once, run anywhere](http://en.wikipedia.org/wiki/Write_once,_run_anywhere) approach - though hopefully without the negative connotations...)\\n\\nRather than diving straight in myself I\'d heard at a user group that it might be worth taking [Twitter Bootstrap](http://twitter.github.com/bootstrap/) as a baseline. I\'m a <strike>lazy</strike>\\n\\nbusy fellow so this sounded ideal.\\n\\n## I like ASP.Net MVC...\\n\\n... and this flavoured my investigations. I quickly stumbled on an [article written by Eric Hexter](http://lostechies.com/erichexter/2012/11/20/twitter-bootstrap-mvc4-the-template-nuget-package-for-asp-net-mvc4-projects/). Eric had brought together Twitter Bootstrap and ASP.Net MVC 4 in a [NuGet package](http://nuget.org/packages/twitter.bootstrap.mvc4). Excellent work chap!\\n\\nTo get up and running with Eric\'s work was a straightforward proposition. I...\\n\\n1. Created new MVC 4 application in Visual Studio called \u201CBootstrapMvcSample\u201D using the \u201CEmpty\u201D Project Template.\\n2. Executed the following commands at the NuGet Package Manager Console: - `Install-Package twitter.bootstrap.mvc4`\\n   - `Install-Package twitter.bootstrap.mvc4.sample`\\n\\nThis is just 1 page, with `@media` queries doing the heavy lifting.\\n\\n## Bootstrap Datepicker\\n\\nThe eagle-eyed amongst you will have noticed that the edit screen above features a date field. I\'ve long been a fan of datepickers to allow users to enter a date in an application in an intuitive fashion. Until native browser datepickers become the norm we\'ll be relying on some kind of component. Up until now my datepicker of choice has been the [jQuery UI one](http://jqueryui.com/datepicker/). Based on a quick Google it seemed that jQuery UI and Twitter Bootstrap were not necessarily natural bedfellows. (Though [Addy Osmani\'s jQuery UI Bootstrap](http://addyosmani.github.com/jquery-ui-bootstrap/) shows some promise...)\\n\\nSince I feared ending up down a blind alley I found myself casting around for a Twitter Bootstrap datepicker. I quickly happened upon [Stefan Petre\'s Bootstrap Datepicker](http://www.eyecon.ro/bootstrap-datepicker/) which looked just the ticket.\\n\\n## Shake hands and play nice...\\n\\nIncorporating the Bootstrap Datepicker into Twitter.Bootstrap.MVC4 was actually a pretty straightforward affair. I added the following datepicker assets to the ASP.Net MVC project as follows:\\n\\n- `bootstrap-datepicker.js` was added to `~\\\\Scripts`.\\n- `datepicker.css` was added to `~\\\\Content`. I renamed this file to `bootstrap-datepicker.css` to stay in line with the other css files.\\n\\nOnce this was done I amended the `BootstrapBundleConfig.cs` bundles to include these assets. Once this was done the bundle file looked like this:\\n\\n```cs\\nusing System.Web;\\nusing System.Web.Mvc;\\nusing System.Web.Optimization;\\n\\nnamespace BootstrapSupport\\n{\\n    public class BootstrapBundleConfig\\n    {\\n        public static void RegisterBundles(BundleCollection bundles)\\n        {\\n            bundles.Add(new ScriptBundle(\\"~/js\\").Include(\\n                \\"~/Scripts/jquery-1.*\\",\\n                \\"~/Scripts/bootstrap.js\\",\\n                \\"~/Scripts/bootstrap-datepicker.js\\", // ** NEW for Bootstrap Datepicker\\n                \\"~/Scripts/jquery.validate.js\\",\\n                \\"~/scripts/jquery.validate.unobtrusive.js\\",\\n                \\"~/Scripts/jquery.validate.unobtrusive-custom-for-bootstrap.js\\"\\n                ));\\n\\n            bundles.Add(new StyleBundle(\\"~/content/css\\").Include(\\n                \\"~/Content/bootstrap.css\\",\\n                \\"~/Content/bootstrap-datepicker.css\\" // ** NEW for Bootstrap Datepicker\\n                ));\\n\\n            bundles.Add(new StyleBundle(\\"~/content/css-responsive\\").Include(\\n                \\"~/Content/bootstrap-responsive.css\\"\\n                ));\\n        }\\n    }\\n}\\n```\\n\\nI then created this folder:`~\\\\Views\\\\Shared\\\\EditorTemplates`. To this folder I added the following `Date.cshtml` Partial to hold the datepicker EditorTemplate: (Having this in place meant that properties with the `[DataType(DataType.Date)]` attribute would automatically use this EditorTemplate when rendering an editor - I understand `[UIHint]` attributes can be used to the same end.)\\n\\n```cs\\n@model DateTime?\\n@Html.TextBox(\\"\\", (Model.HasValue ? Model.Value.ToShortDateString() : string.Empty), new {\\n    @class = \\"datepicker\\",\\n    data_date_format = System.Globalization.CultureInfo.CurrentCulture.DateTimeFormat.ShortDatePattern.ToLower()\\n})\\n```\\n\\nAnd finally I amended the `Create.cshtml` View (which perhaps more accurately might be called the Edit View?) to include a bit of JavaScript at the bottom to initialise any datepickers on the screen.\\n\\n```cs\\n@using BootstrapSupport\\n@model Object\\n@using (Html.BeginForm())\\n{\\n    @Html.ValidationSummary(true)\\n    <fieldset class=\\"form-horizontal\\">\\n        <legend>@Model.GetLabel() <small>Details</small></legend>\\n        @foreach (var property in Model.VisibleProperties())\\n        {\\n            @Html.BeginControlGroupFor(property.Name)\\n                @Html.Label(property.Name.ToSeparatedWords(), new { @class = \\"control-label\\" })\\n                <div class=\\"controls\\">\\n                    @Html.Editor(property.Name, new { @class = \\"input-xlarge\\" })\\n                    @Html.ValidationMessage(property.Name, null, new { @class = \\"help-inline\\" })\\n  \\t        </div>\\n            @Html.EndControlGroup()\\n        }\\n\\t\\t<div class=\\"form-actions\\">\\n            <button type=\\"submit\\" class=\\"btn btn-primary\\">Save changes</button>\\n            @Html.ActionLink(\\"Cancel\\",  \\"Index\\", null, new {@class = \\"btn \\"})\\n          </div>\\n    </fieldset>\\n}\\n<div>\\n    @Html.ActionLink(\\"Back to List\\", \\"Index\\")\\n</div>\\n\\n@section Scripts {\\n<script type=\\"text/javascript\\">\\n    $(\'.datepicker\').datepicker(); //Initialise any date pickers\\n<\/script>\\n}\\n```\\n\\nEt voil\xe0 - it works!\\n\\nMy thanks to [Eric Hexter](https://twitter.com/ehexter) and Stefan Petre for doing all the hard work!\\n\\n## Still to do\\n\\nI haven\'t really tested how this all fits together (if at all) with browsers running a non-English culture. There may still be a little tinkering require to get that working..."},{"id":"html-to-pdf-using-wcf-service","metadata":{"permalink":"/html-to-pdf-using-wcf-service","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2013-01-03-html-to-pdf-using-wcf-service/index.md","source":"@site/blog/2013-01-03-html-to-pdf-using-wcf-service/index.md","title":"HTML to PDF using a WCF Service","description":"This ASP.NET WCF service creates PDFs from HTML and is remotely fired with wkhtmltopdf, using `webHttpBinding` for simple service calls.","date":"2013-01-03T00:00:00.000Z","tags":[],"readingTime":3.13,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"html-to-pdf-using-wcf-service","title":"HTML to PDF using a WCF Service","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false,"description":"This ASP.NET WCF service creates PDFs from HTML and is remotely fired with wkhtmltopdf, using `webHttpBinding` for simple service calls."},"unlisted":false,"prevItem":{"title":"Twitter.Bootstrap.MVC4 meet Bootstrap Datepicker","permalink":"/twitterbootstrapmvc4-meet-bootstrap"},"nextItem":{"title":"Getting up to speed with Bloomberg\'s Open API...","permalink":"/a-nicer-net-api-for-bloombergs-open-api"}},"content":"## TL; DR - \\"Talk is cheap. Show me the code.\\"\\n\\n\x3c!--truncate--\x3e\\n\\nSome time ago I wrote a [post which demonstrated how you could make PDFs from HTML](../2012-04-05-making-pdfs-from-html-in-c-using/index.md) using C# and [wkhtmltopdf](http://code.google.com/p/wkhtmltopdf/). To my lasting surprise this has been the most popular post I\'ve written. I recently put together an ASP.NET WCF service which exposed this functionality which I thought might be worth sharing. The code can be found on GitHub [here](https://github.com/johnnyreilly/PdfMakerWcfService).\\n\\n## A little more detail\\n\\nI should say up front that I\'m still a little ambivalent about how sensible an idea this is. Behind the scenes this WCF service is remotely firing up wkhtmltopdf using `System.Diagnostics.Process`. I feel a little wary about recommending this as a solution for a variety of not particularly defined reasons. However, I have to say I\'ve found this pretty stable and reliable. Bottom line it seems to work and work consistently. But I though I should include a caveat emptor; there is probably a better approach than this available. Anyway...\\n\\nThere isn\'t actually a great deal to say about this WCF service. It should (hopefully) just do what it says on the tin. Putting it together didn\'t involve a great deal of work; essentially it takes the code from the initial blog post and just wraps it in a WCF service called `PdfMaker`. The service exposes 2 methods:\\n\\n1. `GetPdf` \\\\- given a supplied URL this method creates a PDF and then returns it as a Stream to the client\\n2. `GetPdfUrl` \\\\- given a supplied URL this method creates a PDF and then returns the location of it to the client\\n\\nBoth of these methods also set a Location header in the response indicating the location of the created PDF.\\n\\n## That which binds us\\n\\nThe service uses `webHttpBinding`. This is commonly employed when people want to expose a RESTful WCF service. The reason I\'ve used this binding is I wanted a simple \\"in\\" when calling the service. I wanted to be able to call the service via AJAX as well as directly by browsing to the service and supplying a URL-encoded URL like this:\\n\\n`http://localhost:59002/PdfMaker.svc/GetPdf?url=http%3A%2F%2Fnews.ycombinator.com/`You may wonder why I\'m using [http://news.ycombinator.com](http://news.ycombinator.com) for the example above. I chose this as Hacker News is a very simple site; very few resources and a small page size. This means the service has less work to do when creating the PDF; it\'s a quick demo.\\n\\nI should say that this service is arguably \\\\*\\\\*not\\\\*\\\\* completely RESTful as each GET operation behind the scenes attempts to create a new PDF (arguably a side-effect). These should probably be POST operations as they create a new resource each time they\'re hit. However, if they were I wouldn\'t be able to just enter a URL into a browser for testing and that\'s really useful. So tough, I shake my fist at the devotees of pure REST on this occasion. (If I should be attacked in the street shortly after this blog is posted then the police should be advised this is good line of inquiry...)\\n\\n## Good behaviour\\n\\nIt\'s worth noting that `automaticFormatSelectionEnabled` set to true on the behaviour so that content negotiation is enabled. Obviously for the `GetPdf` action this is rather meaningless as it\'s a stream that\'s passed back. However, for the `GetPdfUrl` action the returned string can either be JSON or XML. The Fiddler screenshots below demonstrate this in action.\\n\\n## Test Harness\\n\\nAs a final touch I added in a test harness in the form of `Demo.aspx`. Here\'s an example of the output generated when pointing at Hacker News:\\n\\n<iframe src=\\"https://docs.google.com/file/d/0B87K8-qxOZGFMGNCUWRneUFsVFU/preview\\" width=\\"500\\" height=\\"500\\"></iframe>\\n\\nAnd that\'s it. If there was a need this service could be easily extended to leverage the [various options](http://madalgo.au.dk/~jakobt/wkhtmltoxdoc/wkhtmltopdf-0.9.9-doc.html) that wkhtmltopdf makes available. Hope people find it useful."},{"id":"a-nicer-net-api-for-bloombergs-open-api","metadata":{"permalink":"/a-nicer-net-api-for-bloombergs-open-api","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2012-11-13-a-nicer-net-api-for-bloombergs-open-api/index.md","source":"@site/blog/2012-11-13-a-nicer-net-api-for-bloombergs-open-api/index.md","title":"Getting up to speed with Bloomberg\'s Open API...","description":"John documents his experience investigating Bloombergs Open API. He includes a simple C# console application wrapper for the API.","date":"2012-11-13T00:00:00.000Z","tags":[{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."},{"inline":false,"label":"C#","permalink":"/tags/csharp","description":"The C# programming language."}],"readingTime":11.39,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"a-nicer-net-api-for-bloombergs-open-api","title":"Getting up to speed with Bloomberg\'s Open API...","authors":"johnnyreilly","tags":["asp.net","c#"],"hide_table_of_contents":false,"description":"John documents his experience investigating Bloombergs Open API. He includes a simple C# console application wrapper for the API."},"unlisted":false,"prevItem":{"title":"HTML to PDF using a WCF Service","permalink":"/html-to-pdf-using-wcf-service"},"nextItem":{"title":"XSD/XML Schema Generator + Xsd.exe:Taking the pain out of manual XML","permalink":"/xsdxml-schema-generator-xsdexe-taking"}},"content":"A good portion of any devs life is usually spent playing with APIs. If you need to integrate some other system into the system you\'re working on (and it\'s rare to come upon a situation where this doesn\'t happen at some point) then it\'s API time.\\n\\n\x3c!--truncate--\x3e\\n\\nSome APIs are well documented and nice to use. Some aren\'t. I recently spent a goodly period of time investigating [Bloomberg\'s Open API](http://www.openbloomberg.com/open-api/) and it was a slightly painful experience. So much so that I thought it best to write up my own experiences and maybe I can save others time and a bit of pain.\\n\\nAlso, as I investigated the Bloomberg Open API I found myself coming up with my own little mini-C#-API. (It\'s generally a sure sign you\'ve found an API you don\'t love if you end up writing your own wrapper.) This mini API did the heavy lifting for me and just handed back nicely structured data to deal with. I have included this wrapper here as well.\\n\\n## Research\\n\\nThe initial plan was to, through code, extract Libor and Euribor rates from Bloomberg. I had access to a Bloomberg terminal and I had access to the internet - what could stop me? After digging around for a little while I found some useful resources that could be accessed from the Bloomberg terminal:\\n\\n1. Typing \u201C`WAPI&lt;GO&gt;`\u201D into Bloomberg lead me to the Bloomberg API documentation.\\n2. Typing \u201C`DOCS 2055451&lt;GO&gt;`\u201D into Bloomberg (I know - it\'s a bit cryptic) provided me with samples of how to use the Bloomberg API in VBA\\n\\nTo go with this I found some useful documentation of the Bloomberg Open API [here](http://www.openbloomberg.com/files/2012/10/blpapi-developers-guide.pdf) and I found the .NET Bloomberg Open API itself [here](http://www.openbloomberg.com/open-api/).\\n\\n## Hello World?\\n\\nThe first goal when getting up to speed with an API is getting it to do something. Anything. Just stick a fork into it and see if it croaks. Sticking a fork into Open API was achieved by taking the 30-odd example apps included in the Bloomberg Open API and running each in turn on the Bloomberg box until I had my \\"he\'s alive!!\\" moment. (I did find it surprising that not all of the examples worked - I don\'t know if there\'s a good reason for this...)\\n\\nHowever, when I tried to write my own C# console application to interrogate the Open API it wasn\'t as plain sailing as I\'d hoped. I\'d write something that looked correct, compiled successfully and deploy it onto the Bloomberg terminal only to have it die a sad death whenever I tried to fire it off.\\n\\nI generally find the fastest way to get up and running with an API is to debug it. To make calls to the API and then examine, field by field and method by method, what is actually there. This wasn\'t really an option with my console app though. I was using a shared Bloomberg terminal with very limited access. No Visual Studio on the box and no remote debugging enabled.\\n\\nIt was then that I had something of a eureka moment. I realised that the code in the VBA samples I\'d downloaded from Bloomberg looked quite similar to the C# code samples that shipped with Open API. Hmmmm.... Shortly after this I found myself sat at the Bloomberg machine debugging the Bloomberg API using the VBA IDE in Excel. (For the record, these debugging tools are aren\'t too bad at all - they\'re nowhere near as slick as their VS counterparts but they do the job.) This was my [Rosetta Stone](http://en.wikipedia.org/wiki/Rosetta_Stone) \\\\- I could take what I\'d learned from the VBA samples and translate that into equivalent C# / .NET code (bearing in mind what I\'d learned from debugging in Excel and in fact sometimes bringing along the VBA comments themselves if they provided some useful insight).\\n\\n## He\'s the Bloomberg, I\'m the Wrapper\\n\\nSo I\'m off and romping... I have something that works. Hallelujah! Now that that hurdle had been crossed I found myself examining the actual Bloomberg API code itself. It functioned just fine but it did a couple of things that I wasn\'t too keen on:\\n\\n1. The Bloomberg API came with custom data types. I didn\'t want to use these unless it was absolutely necessary - I just wanted to stick to the standard .NET types. This way if I needed to hand data onto another application I wouldn\'t be making each of these applications dependant on the Bloomberg Open API.\\n2. To get the data out of the Bloomberg API there was an awful lot of boilerplate. Code which handled the possibilities of very large responses that might be split into several packages. Code which walked the element tree returned from Bloomberg parsing out the data. It wasn\'t a beacon of simplicity.\\n\\nI wanted an API that I could simply invoke with security codes and required fields. And in return I wanted to be passed nicely structured data. As I\'ve already mentioned a desire to not introduce unnecessary dependencies I thought it might well suit to make use of nested Dictionaries. I came up with a simple C# Console project / application which had a reference to the Bloomberg Open API. It contained the following class; essentially my wrapper for Open API operations: (please note this is deliberately a very \\"bare-bones\\" implementation)\\n\\n```cs\\nusing System;\\nusing System.Collections.Generic;\\nusing System.Linq;\\nusing System.Text;\\n\\nusing Bloomberglp.Blpapi;\\n\\nnamespace BloombergConsole\\n{\\n    class BloombergApi\\n    {\\n        #region Members / Contructors\\n\\n        private Session _session = null;\\n        private Service _refDataService = null;\\n\\n        private readonly string _serverHost;\\n        private readonly int _serverPort;\\n\\n        public BloombergApi(string serverHost = \\"localhost\\", int serverPort = 8194)\\n        {\\n            _serverHost = serverHost;\\n            _serverPort = serverPort;\\n        }\\n\\n        /// <summary>\\n        /// Initialise the Session and the Service\\n        /// </summary>\\n        internal void InitialiseSessionAndService()\\n        {\\n            if (_session == null)\\n            {\\n                var sessionOptions = new SessionOptions\\n                {\\n                    ServerHost = _serverHost,\\n                    ServerPort = _serverPort\\n                };\\n\\n                //Console.WriteLine(\\"Connecting to {0}:{1}\\", sessionOptions.ServerHost, sessionOptions.ServerPort);\\n\\n                _session = new Session(sessionOptions);\\n\\n                if (!_session.Start())\\n                    throw new Exception(\\"Failed to connect!\\");\\n\\n                if (!_session.OpenService(\\"//blp/refdata\\"))\\n                {\\n                    _session.Stop();\\n                    _session = null;\\n\\n                    throw new Exception(\\"Failed to open //blp/refdata\\");\\n                }\\n\\n                _refDataService = _session.GetService(\\"//blp/refdata\\");\\n            }\\n        }\\n\\n        /// <summary>\\n        /// Dispose the Session and the Service\\n        /// </summary>\\n        internal void DisposeSessionAndService()\\n        {\\n            _refDataService = null;\\n\\n            //Stop the session\\n            if (_session != null)\\n            {\\n                _session.Stop();\\n                _session = null;\\n            }\\n        }\\n\\n        #endregion\\n\\n        #region Methods\\n\\n        internal Dictionary<string,    //Security\\n            Dictionary<string, object> //Fields and values\\n            > GetSecuritiesFields(string[] securities, string[] fields)\\n        {\\n            var securitiesFields = new Dictionary<string, Dictionary<string, object>>();\\n\\n            //Create request\\n            var referenceDataRequest = _refDataService.CreateRequest(\\"ReferenceDataRequest\\");\\n\\n            //Securities\\n            var securitiesElement = referenceDataRequest.GetElement(\\"securities\\");\\n            foreach (var security in securities)\\n                securitiesElement.AppendValue(security);\\n\\n            //Fields\\n            var fieldsElement = referenceDataRequest.GetElement(\\"fields\\");\\n            foreach (var field in fields)\\n                fieldsElement.AppendValue(field);\\n\\n            //   Send off request\\n            _session.SendRequest(referenceDataRequest, null);\\n\\n            //   Start with our flag set to False for not done\\n            var done = false;\\n\\n            //   Continue as long as we are not done\\n            while (!done)\\n            {\\n                //   Retrieve next event from the server\\n                var eventObj = _session.NextEvent();\\n\\n                //   As long as we have a partial or final response, start to process data\\n                if (eventObj.Type == Event.EventType.RESPONSE ||\\n                    eventObj.Type == Event.EventType.PARTIAL_RESPONSE)\\n                {\\n                    //  Loop through messages\\n                    foreach (Message msg in eventObj)\\n                    {\\n                        //   Error handler in case of problem which throws meaningful exception\\n                        if (msg.AsElement.HasElement(\\"responseError\\"))\\n                            throw new Exception(\\"Response error:  \\" + msg.GetElement(\\"responseError\\").GetElement(\\"message\\"));\\n\\n                        //   Extract the securityData top layer and the field data\\n                        //   History comes back on a single security basis so no looping there\\n                        var securityDataArray = msg.GetElement(\\"securityData\\");\\n\\n                        //   Loop through each security\\n                        for (var i = 0; i < securityDataArray.NumValues; i++)\\n                        {\\n                            //   First take out the security object...\\n                            var security = securityDataArray.GetValueAsElement(i);\\n\\n                            var securityName = security.GetElementAsString(\\"security\\");\\n\\n                            //   ... then extract the fieldData object\\n                            var fieldData = security.GetElement(\\"fieldData\\");\\n\\n                            //If we need to add a new security to the securitiesFields dictionary then do so\\n                            Dictionary<string, object> results = null;\\n                            if (!securitiesFields.ContainsKey(securityName))\\n                                securitiesFields.Add(securityName, new Dictionary<string, object>());\\n\\n                            //Get the fieldsByDate dictionary from the securitiesFields dictionary\\n                            results = securitiesFields[securityName];\\n\\n                            //Extract results and store in results dictionary\\n                            foreach (var dataElement in fieldData.Elements)\\n                            {\\n                                var dataElementName = dataElement.Name.ToString();\\n\\n                                //Not using this at present - just demonstrating that we can\\n                                switch (dataElement.Datatype)\\n                                {\\n                                    //Special handling to co-erce bloomberg datetimes back to standard .NET datetimes\\n                                    case Schema.Datatype.DATE:\\n                                        results.Add(dataElementName, dataElement.GetValueAsDate().ToSystemDateTime());\\n                                        break;\\n                                    case Schema.Datatype.DATETIME:\\n                                        results.Add(dataElementName, dataElement.GetValueAsDatetime().ToSystemDateTime());\\n                                        break;\\n                                    case Schema.Datatype.TIME:\\n                                        results.Add(dataElementName, dataElement.GetValueAsDatetime().ToSystemDateTime());\\n                                        break;\\n\\n                                    //Standard handling\\n                                    default:\\n                                        results.Add(dataElementName, dataElement.GetValue());\\n                                        break;\\n                                }\\n                            }\\n                        }\\n                    }\\n\\n                    //   Once we have a response we are done\\n                    if (eventObj.Type == Event.EventType.RESPONSE) done = true;\\n                }\\n            }\\n\\n            return securitiesFields;\\n        }\\n\\n        internal Dictionary<string,     //Security\\n            Dictionary<DateTime,        //DateTime of security\\n            Dictionary<string, object>> //Fields and values\\n            > GetSecuritiesFieldsByDate(string[] securities, string[] fields, DateTime startDate, DateTime endDate)\\n        {\\n            var securitiesFieldsByDate = new Dictionary<string, Dictionary<DateTime, Dictionary<string, object>>>();\\n\\n            //Create request\\n            var historyDataRequest = _refDataService.CreateRequest(\\"HistoricalDataRequest\\");\\n\\n            //Securities\\n            var securitiesElement = historyDataRequest.GetElement(\\"securities\\");\\n            foreach (var security in securities)\\n                securitiesElement.AppendValue(security);\\n\\n            //Fields\\n            var fieldsElement = historyDataRequest.GetElement(\\"fields\\");\\n            foreach (var field in fields)\\n                fieldsElement.AppendValue(field);\\n\\n            //   Set the start date and end date as YYYYMMDD strings\\n            historyDataRequest.Set(\\"startDate\\", startDate.ToString(\\"yyyyMMdd\\"));\\n            historyDataRequest.Set(\\"endDate\\", endDate.ToString(\\"yyyyMMdd\\"));\\n\\n            //   Send off request\\n            _session.SendRequest(historyDataRequest, null);\\n\\n            //   Start with our flag set to False for not done\\n            var done = false;\\n\\n            //   Continue as long as we are not done\\n            while (!done)\\n            {\\n                //   Retrieve next event from the server\\n                var eventObj = _session.NextEvent();\\n\\n                //   As long as we have a partial or final response, start to process data\\n                if (eventObj.Type == Event.EventType.RESPONSE ||\\n                    eventObj.Type == Event.EventType.PARTIAL_RESPONSE)\\n                {\\n                    //  Loop through messages\\n                    foreach (Message msg in eventObj)\\n                    {\\n                        //   Error handler in case of problem which throws meaningful exception\\n                        if (msg.AsElement.HasElement(\\"responseError\\"))\\n                            throw new Exception(\\"Response error:  \\" + msg.GetElement(\\"responseError\\").GetElement(\\"message\\"));\\n\\n                        //   Extract the securityData top layer and the field data\\n                        //   History comes back on a single security basis so no looping there\\n                        var security = msg.GetElement(\\"securityData\\");\\n                        var securityName = security.GetElementAsString(\\"security\\");\\n                        var fieldData = security.GetElement(\\"fieldData\\");\\n\\n                        //   Extract the data for each requested field\\n                        for (var i = 0; i < fieldData.NumValues; i++)\\n                        {\\n                            var data = fieldData.GetValueAsElement(i);\\n\\n                            //   First get the date - this is our key\\n                            var date = data.GetElementAsDate(\\"date\\").ToSystemDateTime();\\n\\n                            //If we need to add a new security to the securitiesFieldsByDate dictionary then do so\\n                            Dictionary<DateTime, Dictionary<string, object>> fieldsByDate = null;\\n                            if (!securitiesFieldsByDate.ContainsKey(securityName))\\n                                securitiesFieldsByDate.Add(securityName, new Dictionary<DateTime, Dictionary<string, object>>());\\n\\n                            //Get the fieldsByDate dictionary from the securitiesFieldsByDate dictionary\\n                            fieldsByDate = securitiesFieldsByDate[securityName];\\n\\n                            //Extract results and store in results dictionary\\n                            var results = new Dictionary<string, object>();\\n                            foreach (var dataElement in data.Elements)\\n                            {\\n                                var dataElementName = dataElement.Name.ToString();\\n\\n                                //Not using this at present - just demonstrating that we can\\n                                switch (dataElement.Datatype)\\n                                {\\n                                    //Special handling to co-erce bloomberg datetimes back to standard .NET datetimes\\n                                    case Schema.Datatype.DATE:\\n                                        results.Add(dataElementName, dataElement.GetValueAsDate().ToSystemDateTime());\\n                                        break;\\n                                    case Schema.Datatype.DATETIME:\\n                                        results.Add(dataElementName, dataElement.GetValueAsDatetime().ToSystemDateTime());\\n                                        break;\\n                                    case Schema.Datatype.TIME:\\n                                        results.Add(dataElementName, dataElement.GetValueAsDatetime().ToSystemDateTime());\\n                                        break;\\n\\n                                    //Standard handling\\n                                    default:\\n                                        results.Add(dataElementName, dataElement.GetValue());\\n                                        break;\\n                                }\\n                            }\\n\\n                            //Save results dictionary to fieldsByDate dictionary\\n                            fieldsByDate.Add(date, results);\\n                        }\\n                    }\\n\\n                    //   Once we have a response we are done\\n                    if (eventObj.Type == Event.EventType.RESPONSE) done = true;\\n                }\\n            }\\n\\n            return securitiesFieldsByDate;\\n        }\\n\\n        #endregion\\n    }\\n}\\n```\\n\\nThe project also contained this class which demonstrates how I made use of my wrapper:\\n\\n```cs\\nusing System;\\nusing System.Collections.Generic;\\nusing System.Linq;\\nusing System.Text;\\n\\n\\nnamespace BloombergConsole\\n{\\n    class NicerBloombergApiDemo\\n    {\\n        internal const string PX_LAST = \\"PX_LAST\\";\\n        internal const string PREV_CLOSE_VALUE_REALTIME = \\"PREV_CLOSE_VALUE_REALTIME\\";\\n        internal const string CHG_NET_1D = \\"CHG_NET_1D\\";\\n        internal const string CHG_PCT_1D = \\"CHG_PCT_1D\\";\\n        internal const string CHG_PCT_YTD = \\"CHG_PCT_YTD\\";\\n\\n        internal const string LIBOR_1_MONTH = \\"BP0001M Index\\";\\n        internal const string LIBOR_3_MONTH = \\"BP0003M Index\\";\\n        internal const string LIBOR_6_MONTH = \\"BP0006M Index\\";\\n        internal const string EURIBOR_1_MONTH = \\"EUR001M Index\\";\\n        internal const string EURIBOR_3_MONTH = \\"EUR003M Index\\";\\n        internal const string EURIBOR_6_MONTH = \\"EUR006M Index\\";\\n\\n        /// <summary>\\n        /// A demo of the nicer client\\n        /// </summary>\\n        /// <param name=\\"args\\"></param>\\n        public static void Main(String[] args)\\n        {\\n            System.Console.WriteLine(\\"Bloomberg console demo...\\");\\n\\n            var startDateTime = DateTime.Today.AddDays(-3);\\n            var endDateTime = DateTime.Today;\\n\\n            var nicerApi = new BloombergApi();\\n            nicerApi.InitialiseSessionAndService();\\n            try\\n            {\\n                //Get fields for given dates\\n                var securitiesFieldsByDate = nicerApi.GetSecuritiesFieldsByDate(\\n                    new string[] { LIBOR_1_MONTH, LIBOR_3_MONTH, LIBOR_6_MONTH, EURIBOR_1_MONTH, EURIBOR_3_MONTH, EURIBOR_6_MONTH\\n                    },\\n                    new string[] { PREV_CLOSE_VALUE_REALTIME, PX_LAST, CHG_NET_1D, CHG_PCT_1D, CHG_PCT_YTD\\n                    },\\n                    startDateTime, endDateTime);\\n                Console.WriteLine(\\"\\\\r\\\\nGetSecuritiesFieldsByDate\\");\\n\\n                //Loop by security\\n                foreach (var security in securitiesFieldsByDate)\\n                {\\n                    Console.WriteLine(\\"Security: {0}\\", security.Key);\\n\\n                    //Loop by date\\n                    foreach (var dateAndFields in security.Value.OrderBy(d => d.Key))\\n                    {\\n                        Console.WriteLine(dateAndFields.Key.ToString(\\"yyyy-MM-dd\\"));\\n\\n                        //Loop by field\\n                        foreach (var keyValue in dateAndFields.Value)\\n                        {\\n                            Console.WriteLine(\\"{0}: {1} ({2})\\", keyValue.Key, keyValue.Value, keyValue.Value.GetType());\\n                        }\\n                    }\\n                }\\n                Console.WriteLine();\\n\\n                //Get current rates\\n                var flashRates = nicerApi.GetSecuritiesFields(\\n                    new string[] { LIBOR_1_MONTH, LIBOR_3_MONTH, LIBOR_6_MONTH, EURIBOR_1_MONTH, EURIBOR_3_MONTH, EURIBOR_6_MONTH\\n                    },\\n                    new string[] { PREV_CLOSE_VALUE_REALTIME, PX_LAST, CHG_NET_1D, CHG_PCT_1D, CHG_PCT_YTD\\n                    });\\n                foreach (var securityAndFields in flashRates.OrderBy(d => d.Key))\\n                {\\n                    Console.WriteLine(securityAndFields.Key);\\n                    foreach (var keyValue in securityAndFields.Value)\\n                        Console.WriteLine(\\"{0}: {1} ({2})\\", keyValue.Key, keyValue.Value, keyValue.Value.GetType());\\n                }\\n                Console.WriteLine();\\n            }\\n            catch (Exception e)\\n            {\\n                System.Console.WriteLine(e.ToString());\\n            }\\n\\n            System.Console.WriteLine(\\"Press ENTER to quit\\");\\n            try\\n            {\\n                System.Console.Read();\\n            }\\n            catch (System.IO.IOException)\\n            {\\n            }\\n        }\\n    }\\n}\\n```\\n\\nThis covered my bases. It was simple, it was easy to consume and it didn\'t require any custom types. My mini-API is only really catering for my own needs (unsurprisingly). However, there\'s lots more to the Bloomberg Open API and I may end up taking this further in the future if I encounter use cases that my current API doesn\'t cover.\\n\\n## Updated (07/12/2012)\\n\\nFinally, a PS. I found in the [Open API FAQs](http://www.openbloomberg.com/faq/) that _\\"Testing any of that functionality currently requires a valid Bloomberg Desktop API (DAPI), Server API (SAPI) or Managed B-Pipe subscription. Bloomberg is planning on releasing a stand-alone simulator which will not require a subscription.\\"_ There isn\'t any word yet on this stand-alone simulator. I emailed Bloomberg at [open-tech@bloomberg.net](mailto:open-tech@bloomberg.net) to ask about this. They kindly replied that _\\"Unfortunately it is not yet available. We understand that this makes testing API applications somewhat impractical, so we\'re continuing to work on this tool.\\"_ Fingers crossed for something we can test soon!\\n\\n## Note to self (because I keep forgetting)\\n\\nIf you\'re looking to investigate what data is available about a security in Bloomberg it\'s worth typing \u201C`FLDS&lt;GO&gt;`\u201D into Bloomberg. This is the Bloomberg Fields Finder. Likewise, if you\'re trying to find a security you could try typing \u201C`SECF&lt;GO&gt;`\u201D into Bloomberg as this is the Security Finder."},{"id":"xsdxml-schema-generator-xsdexe-taking","metadata":{"permalink":"/xsdxml-schema-generator-xsdexe-taking","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2012-11-02-xsdxml-schema-generator-xsdexe-taking/index.md","source":"@site/blog/2012-11-02-xsdxml-schema-generator-xsdexe-taking/index.md","title":"XSD/XML Schema Generator + Xsd.exe:Taking the pain out of manual XML","description":"Discover how to use XSD for validating XML and generating C# classes from XSD files, including an online tool to simplify the task.","date":"2012-11-02T00:00:00.000Z","tags":[],"readingTime":7.815,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"xsdxml-schema-generator-xsdexe-taking","title":"XSD/XML Schema Generator + Xsd.exe:Taking the pain out of manual XML","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false,"description":"Discover how to use XSD for validating XML and generating C# classes from XSD files, including an online tool to simplify the task."},"unlisted":false,"prevItem":{"title":"Getting up to speed with Bloomberg\'s Open API...","permalink":"/a-nicer-net-api-for-bloombergs-open-api"},"nextItem":{"title":"MVC 3 meet Dictionary","permalink":"/mvc-3-meet-dictionary"}},"content":"## Is it 2003 again?!?\\n\\nI\'ve just discovered Xsd.exe. It\'s not new. Or shiny. And in fact it\'s been around since .NET 1.1. Truth be told, I\'ve been aware of it for years but up until now I\'ve not had need of it. But now now I\'ve investigated it a bit I\'ve found that it, combined with the XSD/XML Schema Generator can make for a nice tool to add to the utility belt.\\n\\nGranted XML has long since stopped being sexy. But if you need it, as I did recently, then this is for you.\\n\\n\x3c!--truncate--\x3e\\n\\n## To the XML Batman!\\n\\nNow XML is nothing new to me (or I imagine anyone who\'s been developing within the last 10 years). But most of the time when I use XML I\'m barely aware that it\'s going on - by and large it\'s XML doing the heavy lifting underneath my web services. But the glory of this situation is, I never have to think about it. It just works. All I have to deal with are nice strongly typed objects which makes writing robust code a doddle.\\n\\nI recently came upon a situation where I was working with XML in the raw; that is to say strings. I was going to be supplied with strings of XML which would represent various objects. It would be my job to take the supplied XML, extract out the data I needed and proceed accordingly.\\n\\n## We Don\'t Need No Validation...\\n\\nI lied!\\n\\nIn order to write something reliable I needed to be able to validate that the supplied XML was as I expected. So, [XSD](<http://en.wikipedia.org/wiki/XML_Schema_(W3C)>) time. If you\'re familiar with XML then you\'re probably equally familar with XSD which, to quote Wikipedia _\\"can be used to express a set of rules to which an XML document must conform in order to be considered \'valid\'\\"_.\\n\\nNow I\'ve written my fair share of XSDs over the years and I\'ve generally found it a slightly tedious exercise. So I was delighted to discover an online tool to simplify the task. It\'s called the [XSD/XML Schema Generator](http://www.freeformatter.com/xsd-generator.html). What this marvellous tool does is allow you to enter an example of your XML which it then uses to reverse engineer an XSD.\\n\\nHere\'s an example. I plugged in this:\\n\\n```xml\\n<?xml version=\\"1.0\\" encoding=\\"utf-8\\" ?>\\n<contact type=\\"personal\\">\\n  <firstName>John</firstName>\\n  <lastName>Reilly</lastName>\\n  <heightInInches>76</heightInInches>\\n</contact>\\n```\\n\\nAnd pulled out this:\\n\\n```xsd\\n<xs:schema attributeFormDefault=\\"unqualified\\" elementFormDefault=\\"qualified\\" xmlns:xs=\\"http://www.w3.org/2001/XMLSchema\\">\\n  <xs:element name=\\"contact\\">\\n    <xs:complexType>\\n      <xs:sequence>\\n        <xs:element type=\\"xs:string\\" name=\\"firstName\\"/>\\n        <xs:element type=\\"xs:string\\" name=\\"lastName\\"/>\\n        <xs:element type=\\"xs:byte\\" name=\\"heightInInches\\"/>\\n      </xs:sequence>\\n      <xs:attribute type=\\"xs:string\\" name=\\"type\\"/>\\n    </xs:complexType>\\n  </xs:element>\\n</xs:schema>\\n```\\n\\nFantastic! It doesn\'t matter if the tool gets something slightly wrong; you can tweak the generated XSD to your hearts content. This is great because it does the hard work for you, allowing you to step back, mop your brow and then heartily approve the results. This tool is a labour saving device. Put simply, it\'s a dishwasher.\\n\\n## Tools of the Trade\\n\\nHow to get to the actual data? I was initially planning to break out the [`XDocument`](<http://msdn.microsoft.com/en-us/library/system.xml.linq.xdocument(v=vs.100).aspx>), plug in my XSD and use the `Validate` method. Which would do the job just dandy.\\n\\nHowever I resisted. As much as I like LINQ to XML I turned to use [Xsd.exe](<http://msdn.microsoft.com/en-us/library/x6c1kb0s(v=vs.100).aspx>) instead. As I\'ve mentioned, this tool is as old as the hills. But there\'s gold in them thar hills, listen: _\\"The XML Schema Definition (Xsd.exe) tool generates XML schema or common language runtime classes from XDR, XML, and XSD files, or from classes in a runtime assembly.\\"_\\n\\nExcited? Thought not. But what this means is we can hurl our XSD at this tool and it will toss back a nicely formatted C# class for me to use. Good stuff! So how\'s it done? Well MSDN is roughly as informative as it ever is (which is to say, not terribly) but fortunately there\'s not a great deal to it. You fire up the Visual Studio Command Prompt (and I advise doing this in Administrator mode to escape permissions pain). Then you enter a command to generate your class. Here\'s an example using the Contact.xsd file we generated earlier:\\n\\n`xsd.exe \\"C:\\\\\\\\Contact.xsd\\" /classes /out:\\"C:\\\\\\\\\\" /namespace:\\"MyNameSpace\\"`\\n\\nAnd you\'re left with the lovely Contact.cs class:\\n\\n```cs\\n//------------------------------------------------------------------------------\\n// <auto-generated>\\n//     This code was generated by a tool.\\n//     Runtime Version:4.0.30319.239\\n//\\n//     Changes to this file may cause incorrect behavior and will be lost if\\n//     the code is regenerated.\\n// </auto-generated>\\n//------------------------------------------------------------------------------\\n\\n//\\n// This source code was auto-generated by xsd, Version=4.0.30319.1.\\n//\\nnamespace MyNameSpace {\\n    using System.Xml.Serialization;\\n\\n\\n    /// <remarks/>\\n    [System.CodeDom.Compiler.GeneratedCodeAttribute(\\"xsd\\", \\"4.0.30319.1\\")]\\n    [System.SerializableAttribute()]\\n    [System.Diagnostics.DebuggerStepThroughAttribute()]\\n    [System.ComponentModel.DesignerCategoryAttribute(\\"code\\")]\\n    [System.Xml.Serialization.XmlTypeAttribute(AnonymousType=true)]\\n    [System.Xml.Serialization.XmlRootAttribute(Namespace=\\"\\", IsNullable=false)]\\n    public partial class contact {\\n\\n        private string firstNameField;\\n\\n        private string lastNameField;\\n\\n        private sbyte heightInInchesField;\\n\\n        private string typeField;\\n\\n        /// <remarks/>\\n        public string firstName {\\n            get {\\n                return this.firstNameField;\\n            }\\n            set {\\n                this.firstNameField = value;\\n            }\\n        }\\n\\n        /// <remarks/>\\n        public string lastName {\\n            get {\\n                return this.lastNameField;\\n            }\\n            set {\\n                this.lastNameField = value;\\n            }\\n        }\\n\\n        /// <remarks/>\\n        public sbyte heightInInches {\\n            get {\\n                return this.heightInInchesField;\\n            }\\n            set {\\n                this.heightInInchesField = value;\\n            }\\n        }\\n\\n        /// <remarks/>\\n        [System.Xml.Serialization.XmlAttributeAttribute()]\\n        public string type {\\n            get {\\n                return this.typeField;\\n            }\\n            set {\\n                this.typeField = value;\\n            }\\n        }\\n    }\\n}\\n```\\n\\n## Justify Your Actions\\n\\nBut why is this good stuff? Indeed why is this more interesting than the newer, and hence obviously cooler, LINQ to XML? Well for my money it\'s the following reasons that are important:\\n\\n1. Intellisense - I have always loved this. Call me lazy but I think intellisense frees up the mind to think about what problem you\'re actually trying to solve. Xsd.exe\'s generated classes give me that; I don\'t need to hold the whole data structure in my head as I code.\\n2. Terse code - I\'m passionate about less code. I think that a noble aim in software development is to write as little code as possible in order to achieve your aims. I say this as generally I have found that writing a minimal amount of code expresses the intention of the code in a far clearer fashion. In service of that aim Xsd.exe\'s generated classes allow me to write less code than would be required with LINQ to XML.\\n3. To quote Scott Hanselman \\"[successful compilation is just the first unit test](http://www.hanselman.com/blog/NuGetPackageOfTheWeek6DynamicMalleableEnjoyableExpandoObjectsWithClay.aspx)\\". That it is but it\'s a doozy. If I\'m making changes to the code and I\'ve been using LINQ to XML I\'m not going to see the benefits of strong typing that I would with Xsd.exe\'s generated classes. I like learning if I\'ve broken the build sooner rather than later; strong typing gives me that safety net.\\n\\n## Serialization / Deserialization Helper\\n\\nAs you read this you\'re no doubt thinking \\"but wait he\'s shown us how to create XSDs from XML and classes from XSDs but how do we take XML and turn it into objects? And how do we turn those objects back into XML?\\"\\n\\nSee how I read your mind just there? It\'s a gift. Well, I\'ve written a little static helper class for the very purpose:\\n\\n```cs\\nusing System.IO;\\nusing System.Linq;\\nusing System.Text;\\nusing System.Xml.Serialization;\\n\\nnamespace My.Helpers\\n{\\n    public static class XmlConverter<T>\\n    {\\n        private static XmlSerializer _serializer = null;\\n\\n        #region Static Constructor\\n\\n        /// <summary>\\n        /// Static constructor that initialises the serializer for this type\\n        /// </summary>\\n        static XmlConverter()\\n        {\\n            _serializer = new XmlSerializer(typeof(T));\\n        }\\n\\n        #endregion\\n\\n        #region Public\\n\\n        /// <summary>\\n        /// Deserialize the supplied XML into an object\\n        /// </summary>\\n        /// <param name=\\"xml\\"></param>\\n        /// <returns></returns>\\n        public static T ToObject(string xml)\\n        {\\n            return (T)_serializer.Deserialize(new StringReader(xml));\\n        }\\n\\n        /// <summary>\\n        /// Serialize the supplied object into XML\\n        /// </summary>\\n        /// <param name=\\"obj\\"></param>\\n        /// <returns></returns>\\n        public static string ToXML(T obj)\\n        {\\n            using (var memoryStream = new MemoryStream())\\n            {\\n                _serializer.Serialize(memoryStream, obj);\\n\\n                return Encoding.UTF8.GetString(memoryStream.ToArray());\\n            }\\n        }\\n\\n        #endregion\\n    }\\n}\\n```\\n\\nAnd here\'s an example of how to use it:\\n\\n```cs\\nusing MyNameSpace;\\n\\n//Make a new contact\\ncontact myContact = new contact();\\n\\n//Serialize the contact to XML\\nstring myContactXML = XmlConverter<contact>.ToXML(myContact);\\n\\n//Deserialize the XML back into an object\\ncontact myContactAgain = XmlConverter<contact>.ToObject(myContactXML);\\n```\\n\\nI was tempted to name my methods in tribute to Crockford\'s JSON (namely `ToXML` becoming `stringify` and `ToObject` becoming `parse`). Maybe later.\\n\\nAnd that\'s us done. Whilst it\'s no doubt unfashionable I think that this is a very useful approach indeed and I commend it to the interweb!\\n\\n## Updated - using Xsd.exe to generate XSD from XML\\n\\nI was chatting to a friend about this blog post and he mentioned that you can actually use Xsd.exe to generate XSD files from XML as well. He\'s quite right - this feature does exist. To go back to our example from earlier we can execute the following command:\\n\\n`xsd.exe \\"C:\\\\\\\\Contact.xml\\" /out:\\"C:\\\\\\\\\\"`\\n\\nAnd this will generate the following file:\\n\\n```xsd\\n<?xml version=\\"1.0\\" encoding=\\"utf-8\\"?>\\n<xs:schema id=\\"NewDataSet\\" xmlns=\\"\\" xmlns:xs=\\"http://www.w3.org/2001/XMLSchema\\" xmlns:msdata=\\"urn:schemas-microsoft-com:xml-msdata\\">\\n  <xs:element name=\\"contact\\">\\n    <xs:complexType>\\n      <xs:sequence>\\n        <xs:element name=\\"firstName\\" type=\\"xs:string\\" minOccurs=\\"0\\" msdata:Ordinal=\\"0\\" />\\n        <xs:element name=\\"lastName\\" type=\\"xs:string\\" minOccurs=\\"0\\" msdata:Ordinal=\\"1\\" />\\n        <xs:element name=\\"heightInInches\\" type=\\"xs:string\\" minOccurs=\\"0\\" msdata:Ordinal=\\"2\\" />\\n      </xs:sequence>\\n      <xs:attribute name=\\"type\\" type=\\"xs:string\\" />\\n    </xs:complexType>\\n  </xs:element>\\n  <xs:element name=\\"NewDataSet\\" msdata:IsDataSet=\\"true\\" msdata:UseCurrentLocale=\\"true\\">\\n    <xs:complexType>\\n      <xs:choice minOccurs=\\"0\\" maxOccurs=\\"unbounded\\">\\n        <xs:element ref=\\"contact\\" />\\n      </xs:choice>\\n    </xs:complexType>\\n  </xs:element>\\n</xs:schema>\\n```\\n\\nHowever, the XSD generated above is very much a \\"Microsoft XSD\\"; it\'s an XSD which features MS properties and so on. It\'s fine but I think that generally I prefer my XSDs to be as vanilla as possible. To that end I\'m likely to stick to using the XSD/XML Schema Generator as it doesn\'t appear to be possible to get Xsd.exe to generate \\"vanilla XSD\\".\\n\\nThanks to Ajay for bringing it to my attention though."},{"id":"mvc-3-meet-dictionary","metadata":{"permalink":"/mvc-3-meet-dictionary","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2012-10-22-mvc-3-meet-dictionary/index.md","source":"@site/blog/2012-10-22-mvc-3-meet-dictionary/index.md","title":"MVC 3 meet Dictionary","description":"MVC 3 has a Dictionary deserialization bug resolved in MVC 4. Workaround includes using JSON stringify and manual deserialization.","date":"2012-10-22T00:00:00.000Z","tags":[{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":3.07,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"mvc-3-meet-dictionary","title":"MVC 3 meet Dictionary","authors":"johnnyreilly","tags":["asp.net"],"hide_table_of_contents":false,"description":"MVC 3 has a Dictionary deserialization bug resolved in MVC 4. Workaround includes using JSON stringify and manual deserialization."},"unlisted":false,"prevItem":{"title":"XSD/XML Schema Generator + Xsd.exe:Taking the pain out of manual XML","permalink":"/xsdxml-schema-generator-xsdexe-taking"},"nextItem":{"title":"Using Web Optimization with MVC 3","permalink":"/using-web-optimization-with-mvc-3"}},"content":"## Documenting a JsonValueProviderFactory Gotcha\\n\\nAbout a year ago I was involved in the migration of an ASP.NET WebForms application over to MVC 3. We\'d been doing a lot of AJAX-y / Single Page Application-y things in the project and had come to the conclusion that MVC might be a slightly better fit since we intended to continue down this path.\\n\\nDuring the migration we encountered a bug in MVC 3 concerning Dictionary deserialization. This bug has subsequently tripped me up a few more times as I failed to remember the nature of the problem correctly. So I\'ve written the issue up here as an aide to my own lamentable memory.\\n\\n\x3c!--truncate--\x3e\\n\\nBefore I begin I should say that the problem \\\\*<u>has been resolved in MVC 4</u>\\n\\n\\\\*. However given that I imagine many MVC 3 projects will not upgrade instantly there\'s probably some value in documenting the issue (and how to work around it). By the way, you can see my initial plea for assistance in [this StackOverflow question](http://stackoverflow.com/q/6881440/761388).\\n\\n## The Problem\\n\\nThe problem is that deserialization of Dictionary objects does not behave in the expected and desired fashion. When you fire off a dictionary it arrives at your endpoint as the enormously unhelpful `null`. To see this for yourself you can try using this JavaScript:\\n\\n```js\\n$.ajax(\'PostDictionary\', {\\n  type: \'POST\',\\n  contentType: \'application/json\',\\n  data: JSON.stringify({\\n    myDictionary: {\\n      This: \'is\',\\n      a: \'dictionary\',\\n    },\\n  }),\\n  success: function (result) {\\n    alert(JSON.stringify(result));\\n  },\\n});\\n```\\n\\nWith this C#:\\n\\n```cs\\n        //...\\n\\n        [HttpPost]\\n        public ActionResult PostDictionary(Dictionary<string, string> myDictionary)\\n        {\\n            return Json(myDictionary);\\n        }\\n\\n        //...\\n```\\n\\nYou get a null `null` dictionary.\\n\\nAfter a long time googling around on the topic I eventually discovered, much to my surprise, that I was actually tripping over a bug in MVC 3. It was filed by [Darin Dimitrov](http://stackoverflow.com/users/29407/darin-dimitrov) of Stack Overflow fame and I found details about it filed as an official bug [here](http://connect.microsoft.com/VisualStudio/feedback/details/636647/make-jsonvalueproviderfactory-work-with-dictionary-types-in-asp-net-mvc). To quote Darin:\\n\\n\\"_The System.Web.Mvc.JsonValueProviderFactory introduced in ASP.NET MVC 3 enables action methods to send and receive JSON-formatted text and to model-bind the JSON text to parameters of action methods. Unfortunately it doesn\'t work with dictionaries_\\"\\n\\n## The Workaround\\n\\nMy colleague found a workaround for the issue [here](http://stackoverflow.com/a/5397743/761388). There are 2 parts to this:\\n\\n1. Dictionaries in JavaScript are simple JavaScript Object Literals. In order to workaround this issue it is necessary to `JSON.stringify` our Dictionary / JOL before sending it to the endpoint. This is done so a string can be picked up at the endpoint.\\n2. The signature of your action is switched over from a Dictionary reference to a string reference. Deserialization is then manually performed back from the string to a Dictionary within the Action itself.\\n\\nI\'ve adapted my example from earlier to demonstrate this; first the JavaScript:\\n\\n```js\\n$.ajax(\'PostDictionary\', {\\n  type: \'POST\',\\n  contentType: \'application/json\',\\n  data: JSON.stringify({\\n    myDictionary: JSON.stringify({\\n      //Note the deliberate double JSON.stringify\\n      This: \'is\',\\n      a: \'dictionary\',\\n    }),\\n  }),\\n  success: function (result) {\\n    alert(JSON.stringify(result));\\n  },\\n});\\n```\\n\\nThen the C#:\\n\\n```cs\\n        //...\\n\\n        [HttpPost]\\n        public ActionResult PostDictionary(string myDictionary)\\n        {\\n            var actualDictionary = new System.Web.Script.Serialization.JavaScriptSerializer()\\n                .Deserialize<Dictionary<string, string>>(myDictionary);\\n\\n            return Json(actualDictionary);\\n        }\\n\\n        //...\\n```\\n\\nAnd now we\'re able to get a dictionary.\\n\\n## Summary and a PS\\n\\nSo that\'s it; a little unglamourous but this works. I\'m slightly surprised that that wasn\'t picked up before MVC 3 was released but at least it\'s been fixed for MVC 4. I look forward to this blog post being irrelevant and out of date \u263A.\\n\\nFor what it\'s worth in my example above we\'re using the trusty old `System.Web.Script.Serialization.JavaScriptSerializer` to perform deserialization. My preference is actually to use [JSON.Nets](http://james.newtonking.com/projects/json-net.aspx) implementation but for the sake of simplicity I went with .NETs internal one here. To be honest, either is fine to my knowledge."},{"id":"using-web-optimization-with-mvc-3","metadata":{"permalink":"/using-web-optimization-with-mvc-3","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2012-10-05-using-web-optimization-with-mvc-3/index.md","source":"@site/blog/2012-10-05-using-web-optimization-with-mvc-3/index.md","title":"Using Web Optimization with MVC 3","description":"Optimize JavaScript/CSS in MVC 3 through Microsofts NuGet package, bundling jQuery, jQuery UI, jQuery Validate and Modernizr.","date":"2012-10-05T00:00:00.000Z","tags":[{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":5.845,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"using-web-optimization-with-mvc-3","title":"Using Web Optimization with MVC 3","authors":"johnnyreilly","tags":["asp.net"],"hide_table_of_contents":false,"description":"Optimize JavaScript/CSS in MVC 3 through Microsofts NuGet package, bundling jQuery, jQuery UI, jQuery Validate and Modernizr."},"unlisted":false,"prevItem":{"title":"MVC 3 meet Dictionary","permalink":"/mvc-3-meet-dictionary"},"nextItem":{"title":"Unit Testing and Entity Framework: The Filth and the Fury","permalink":"/unit-testing-and-entity-framework-filth"}},"content":"A while ago I [wrote](http://icanmakethiswork.blogspot.com/2012/06/how-im-structuring-my-javascript-in-web.html#WebOptimization) about optimally serving up JavaScript in web applications. I mentioned that Microsoft had come up with a NuGet package called [Microsoft ASP.NET Web Optimization](http://nuget.org/packages/Microsoft.AspNet.Web.Optimization) which could help with that by minifying and bundling CSS and JavaScript. At the time I was wondering if I would be able to to use this package with pre-existing MVC 3 projects (given that the package had been released together with MVC 4). Happily it turns out you can. But it\'s not quite as straightforward as I might have liked so I\'ve documented how to get going with this here...\\n\\n\x3c!--truncate--\x3e\\n\\n## Getting the Basics in Place\\n\\nTo keep it simple I\'m going to go through taking a \\"vanilla\\" MVC 3 app and enhancing it to work with Web Optimization. To start, follow these basic steps:\\n\\n1. Open Visual Studio (bet you didn\'t see that coming!)\\n2. Create a new MVC 3 application (I called mine \\"WebOptimizationWithMvc3\\" to demonstrate my imaginative flair). It doesn\'t really matter which sort of MVC 3 project you create - I chose an Intranet application but really that\'s by the by.\\n3. Update pre-existing NuGet packages\\n4. At the NuGet console type: \\"`Install-Package Microsoft.AspNet.Web.Optimization`\\"\\n\\nWhilst the NuGet package adds the necessary references to your MVC 3 project it doesn\'t add the corresponding namespaces to the web.configs. To fix this manually add the following child XML element to the `&lt;namespaces&gt;` element in your root and Views web.config files:\\n\\n`&lt;add namespace=\\"System.Web.Optimization\\" /&gt;`\\n\\nThis gives you access to `Scripts` and `Styles` in your views without needing the fully qualified namespace. For reasons best known to Microsoft I had to close down and restart Visual Studio before intellisense started working. You may need to do likewise.\\n\\nNext up we want to get some JavaScript / CSS bundles in place. To do this, create a folder in the root of your project called \\"App_Start\\". There\'s nothing magical about this to my knowledge; this is just a convention that\'s been adopted to store all the bits of startup in one place and avoid clutterage. (I think this grew out of Nuget; see [David Ebbo talking about this here](http://blog.davidebbo.com/2011/02/appstart-folder-convention-for-nuget.html).) Inside your new folder you should add a new class called `BundleConfig.cs` which looks like this:\\n\\n```cs\\nusing System.Web;\\nusing System.Web.Optimization;\\n\\nnamespace WebOptimizationWithMvc3.App_Start\\n{\\n    public class BundleConfig\\n    {\\n        // For more information on Bundling, visit http://go.microsoft.com/fwlink/?LinkId=254725\\n        public static void RegisterBundles(BundleCollection bundles)\\n        {\\n            bundles.Add(new ScriptBundle(\\"~/bundles/jquery\\").Include(\\n                        \\"~/Scripts/jquery-{version}.js\\"));\\n\\n            bundles.Add(new ScriptBundle(\\"~/bundles/jqueryui\\").Include(\\n                        \\"~/Scripts/jquery-ui-{version}.js\\"));\\n\\n            bundles.Add(new ScriptBundle(\\"~/bundles/jqueryval\\").Include(\\n                        \\"~/Scripts/jquery.unobtrusive*\\",\\n                        \\"~/Scripts/jquery.validate*\\"));\\n\\n            // Use the development version of Modernizr to develop with and learn from. Then, when you\'re\\n            // ready for production, use the build tool at http://modernizr.com to pick only the tests you need.\\n            bundles.Add(new ScriptBundle(\\"~/bundles/modernizr\\").Include(\\n                        \\"~/Scripts/modernizr-*\\"));\\n\\n            bundles.Add(new StyleBundle(\\"~/Content/css\\").Include(\\"~/Content/site.css\\"));\\n\\n            bundles.Add(new StyleBundle(\\"~/Content/themes/base/css\\").Include(\\n                        \\"~/Content/themes/base/jquery.ui.core.css\\",\\n                        \\"~/Content/themes/base/jquery.ui.resizable.css\\",\\n                        \\"~/Content/themes/base/jquery.ui.selectable.css\\",\\n                        \\"~/Content/themes/base/jquery.ui.accordion.css\\",\\n                        \\"~/Content/themes/base/jquery.ui.autocomplete.css\\",\\n                        \\"~/Content/themes/base/jquery.ui.button.css\\",\\n                        \\"~/Content/themes/base/jquery.ui.dialog.css\\",\\n                        \\"~/Content/themes/base/jquery.ui.slider.css\\",\\n                        \\"~/Content/themes/base/jquery.ui.tabs.css\\",\\n                        \\"~/Content/themes/base/jquery.ui.datepicker.css\\",\\n                        \\"~/Content/themes/base/jquery.ui.progressbar.css\\",\\n                        \\"~/Content/themes/base/jquery.ui.theme.css\\"));\\n        }\\n    }\\n}\\n```\\n\\nThe above is what you get when you create a new MVC 4 project (as it includes Web Optimization out of the box). All it does is create some JavaScript and CSS bundles relating to jQuery, jQuery UI, jQuery Validate, Modernizr and the standard site CSS. Nothing radical here but this example should give you an idea of how bundling can be configured and used. To make use of `BundleConfig.cs` you should modify your `Global.asax.cs` so it looks like this:\\n\\n```cs\\nusing System;\\nusing System.Collections.Generic;\\nusing System.Linq;\\nusing System.Web;\\nusing System.Web.Mvc;\\nusing System.Web.Routing;\\nusing WebOptimizationWithMvc3.App_Start; //NEW MAGIC GOODNESS\\nusing System.Web.Optimization;           //NEW MAGIC GOODNESS\\n\\nnamespace WebOptimizationWithMvc3\\n{\\n    // Note: For instructions on enabling IIS6 or IIS7 classic mode,\\n    // visit http://go.microsoft.com/?LinkId=9394801\\n\\n    public class MvcApplication : System.Web.HttpApplication\\n    {\\n        public static void RegisterGlobalFilters(GlobalFilterCollection filters)\\n        {\\n            filters.Add(new HandleErrorAttribute());\\n        }\\n\\n        public static void RegisterRoutes(RouteCollection routes)\\n        {\\n            routes.IgnoreRoute(\\"{resource}.axd/{*pathInfo}\\");\\n\\n            routes.MapRoute(\\n                \\"Default\\", // Route name\\n                \\"{controller}/{action}/{id}\\", // URL with parameters\\n                new { controller = \\"Home\\", action = \\"Index\\", id = UrlParameter.Optional } // Parameter defaults\\n            );\\n\\n        }\\n\\n        protected void Application_Start()\\n        {\\n            AreaRegistration.RegisterAllAreas();\\n\\n            RegisterGlobalFilters(GlobalFilters.Filters);\\n            RegisterRoutes(RouteTable.Routes);\\n\\n            //NEW MAGIC GOODNESS START\\n            BundleConfig.RegisterBundles(BundleTable.Bundles);\\n            //NEW MAGIC GOODNESS END\\n        }\\n    }\\n}\\n```\\n\\nOnce you\'ve done this you\'re ready to start using Web Optimization in your MVC 3 application.\\n\\n## Switching over \\\\_Layout.cshtml to use Web Optimization\\n\\nWith a \\"vanilla\\" MVC 3 app the only use of CSS and JavaScript files is found in `_Layout.cshtml`. To switch over to using Web Optimization you should replace the existing `_Layout.cshtml` with this: (you\'ll see that the few differences that there are between the 2 are solely around the replacement of link / script tags with references to `Scripts` and `Styles` instead)\\n\\n```html\\n<!doctype html>\\n<html>\\n  <head>\\n    <title>@ViewBag.Title</title>\\n    @Styles.Render(\\"~/Content/css\\", \\"~/Content/themes/base/css\\")\\n    @Scripts.Render(\\"~/bundles/modernizr\\")\\n  </head>\\n  <body>\\n    <div class=\\"page\\">\\n      <div id=\\"header\\">\\n        <div id=\\"title\\">\\n          <h1>My MVC Application</h1>\\n        </div>\\n        <div id=\\"logindisplay\\">\\n          Welcome <strong>@User.Identity.Name</strong>!\\n        </div>\\n        <div id=\\"menucontainer\\">\\n          <ul id=\\"menu\\">\\n            <li>@Html.ActionLink(\\"Home\\", \\"Index\\", \\"Home\\")</li>\\n            <li>@Html.ActionLink(\\"About\\", \\"About\\", \\"Home\\")</li>\\n          </ul>\\n        </div>\\n      </div>\\n      <div id=\\"main\\">@RenderBody()</div>\\n      <div id=\\"footer\\"></div>\\n    </div>\\n    @Scripts.Render(\\"~/bundles/jquery\\", \\"~/bundles/jqueryui\\",\\n    \\"~/bundles/jqueryval\\") @RenderSection(\\"scripts\\", required: false)\\n  </body>\\n</html>\\n```\\n\\nDo note that in the above `Scripts.Render` call we\'re rendering out 3 bundles; jQuery, jQuery UI and jQuery Validate. We\'re not using any of these in `_Layout.cshtml` but rendering these (and their associated link tags) gives us a chance to demonstrate that everything is working as expected.\\n\\nIn your root web.config file make sure that the following tag is in place: `&lt;compilation debug=\\"<b>true</b>\\" targetFramework=\\"4.0\\"&gt;`. Then run, the generated HTML should look something like this:\\n\\n```html\\n<!doctype html>\\n<html>\\n  <head>\\n    <title>Home Page</title>\\n    <link href=\\"/Content/site.css\\" rel=\\"stylesheet\\" />\\n    <link href=\\"/Content/themes/base/jquery.ui.core.css\\" rel=\\"stylesheet\\" />\\n    <link\\n      href=\\"/Content/themes/base/jquery.ui.resizable.css\\"\\n      rel=\\"stylesheet\\"\\n    />\\n    <link\\n      href=\\"/Content/themes/base/jquery.ui.selectable.css\\"\\n      rel=\\"stylesheet\\"\\n    />\\n    <link\\n      href=\\"/Content/themes/base/jquery.ui.accordion.css\\"\\n      rel=\\"stylesheet\\"\\n    />\\n    <link\\n      href=\\"/Content/themes/base/jquery.ui.autocomplete.css\\"\\n      rel=\\"stylesheet\\"\\n    />\\n    <link href=\\"/Content/themes/base/jquery.ui.button.css\\" rel=\\"stylesheet\\" />\\n    <link href=\\"/Content/themes/base/jquery.ui.dialog.css\\" rel=\\"stylesheet\\" />\\n    <link href=\\"/Content/themes/base/jquery.ui.slider.css\\" rel=\\"stylesheet\\" />\\n    <link href=\\"/Content/themes/base/jquery.ui.tabs.css\\" rel=\\"stylesheet\\" />\\n    <link\\n      href=\\"/Content/themes/base/jquery.ui.datepicker.css\\"\\n      rel=\\"stylesheet\\"\\n    />\\n    <link\\n      href=\\"/Content/themes/base/jquery.ui.progressbar.css\\"\\n      rel=\\"stylesheet\\"\\n    />\\n    <link href=\\"/Content/themes/base/jquery.ui.theme.css\\" rel=\\"stylesheet\\" />\\n\\n    <script src=\\"/Scripts/modernizr-2.6.2.js\\"><\/script>\\n  </head>\\n  <body>\\n    <div class=\\"page\\">\\n      <div id=\\"header\\">\\n        <div id=\\"title\\">\\n          <h1>My MVC Application</h1>\\n        </div>\\n        <div id=\\"logindisplay\\">Welcome <strong>LNR\\\\jreilly</strong>!</div>\\n        <div id=\\"menucontainer\\">\\n          <ul id=\\"menu\\">\\n            <li><a href=\\"/\\">Home</a></li>\\n            <li><a href=\\"/Home/About\\">About</a></li>\\n          </ul>\\n        </div>\\n      </div>\\n      <div id=\\"main\\">\\n        <h2>Welcome to ASP.NET MVC!</h2>\\n        <p>\\n          To learn more about ASP.NET MVC visit\\n          <a href=\\"http://asp.net/mvc\\" title=\\"ASP.NET MVC Website\\"\\n            >http://asp.net/mvc</a\\n          >.\\n        </p>\\n      </div>\\n      <div id=\\"footer\\"></div>\\n    </div>\\n    <script src=\\"/Scripts/jquery-1.8.2.js\\"><\/script>\\n    <script src=\\"/Scripts/jquery-ui-1.8.24.js\\"><\/script>\\n    <script src=\\"/Scripts/jquery.unobtrusive-ajax.js\\"><\/script>\\n    <script src=\\"/Scripts/jquery.validate.js\\"><\/script>\\n    <script src=\\"/Scripts/jquery.validate.unobtrusive.js\\"><\/script>\\n  </body>\\n</html>\\n```\\n\\nThis demonstrates that when the application has debug set to true you see the full scripts / links being rendered out as you would hope (to make your debugging less painful).\\n\\nNow go back to your root `web.config` file and chance the debug tag to false: `&lt;compilation debug=\\"<b>false</b>\\" targetFramework=\\"4.0\\"&gt;`. This time when you run, the generated HTML should look something like this:\\n\\n```html\\n<!doctype html>\\n<html>\\n  <head>\\n    <title>Home Page</title>\\n    <link\\n      href=\\"/Content/css?v=zA21MEZPkFOTy3OUxWWonyifZGPNxI-SSbBOWkDhsHk1\\"\\n      rel=\\"stylesheet\\"\\n    />\\n    <link\\n      href=\\"/Content/themes/base/css?v=myqT7npwmF2ABsuSaHqt8SCvK8UFWpRv7T4M8r3kiK01\\"\\n      rel=\\"stylesheet\\"\\n    />\\n\\n    <script src=\\"/bundles/modernizr?v=QZTpgFA-zRi28FHInjPOp9lXJl6mFGrWHlv3QhMpqSw1\\"><\/script>\\n  </head>\\n  <body>\\n    <div class=\\"page\\">\\n      <div id=\\"header\\">\\n        <div id=\\"title\\">\\n          <h1>My MVC Application</h1>\\n        </div>\\n        <div id=\\"logindisplay\\">Welcome <strong>LNR\\\\jreilly</strong>!</div>\\n        <div id=\\"menucontainer\\">\\n          <ul id=\\"menu\\">\\n            <li><a href=\\"/\\">Home</a></li>\\n            <li><a href=\\"/Home/About\\">About</a></li>\\n          </ul>\\n        </div>\\n      </div>\\n      <div id=\\"main\\">\\n        <h2>Welcome to ASP.NET MVC!</h2>\\n        <p>\\n          To learn more about ASP.NET MVC visit\\n          <a href=\\"http://asp.net/mvc\\" title=\\"ASP.NET MVC Website\\"\\n            >http://asp.net/mvc</a\\n          >.\\n        </p>\\n      </div>\\n      <div id=\\"footer\\"></div>\\n    </div>\\n    <script src=\\"/bundles/jquery?v=-3plyJYF8LQ0YVYbKtEZnEbkML7BIL0Iul_dNlwGXq41\\"><\/script>\\n    <script src=\\"/bundles/jqueryui?v=RuyxWjtbiK02VYPQGF4OyBZcxNB-W9FsvN6HJTZj4NA1\\"><\/script>\\n    <script src=\\"/bundles/jqueryval?v=E3jxQivD8ilGcNEk6JrH6Jx2wDop7sWW2YKDc6Kq8gY1\\"><\/script>\\n  </body>\\n</html>\\n```\\n\\nThis time you can see that in non-debug mode (ie how it would run in Production) minified bundles of scripts and css files are being served up instead of the raw files. And that\'s it; done."},{"id":"unit-testing-and-entity-framework-filth","metadata":{"permalink":"/unit-testing-and-entity-framework-filth","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2012-10-03-unit-testing-and-entity-framework-filth/index.md","source":"@site/blog/2012-10-03-unit-testing-and-entity-framework-filth/index.md","title":"Unit Testing and Entity Framework: The Filth and the Fury","description":"Controversy arises over Unit Testing with Entity Framework & MOQ. A simple class could be used to wrap all Entity Framework code.","date":"2012-10-03T00:00:00.000Z","tags":[{"inline":false,"label":"Automated Testing","permalink":"/tags/automated-testing","description":"How to perform the automation of tests."},{"inline":false,"label":"SQL Server","permalink":"/tags/sql-server","description":"The SQL Server database."}],"readingTime":7.32,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"unit-testing-and-entity-framework-filth","title":"Unit Testing and Entity Framework: The Filth and the Fury","authors":"johnnyreilly","tags":["automated testing","sql server"],"hide_table_of_contents":false,"description":"Controversy arises over Unit Testing with Entity Framework & MOQ. A simple class could be used to wrap all Entity Framework code."},"unlisted":false,"prevItem":{"title":"Using Web Optimization with MVC 3","permalink":"/using-web-optimization-with-mvc-3"},"nextItem":{"title":"Giving OData to CRM 4.0","permalink":"/giving-odata-to-crm-40"}},"content":"Just recently I\'ve noticed that there appears to be something of a controversy around Unit Testing and Entity Framework. I first came across it as I was Googling around for useful posts on using MOQ in conjunction with EF. I\'ve started to notice the topic more and more and as I have mixed feelings on the subject (that is to say I don\'t have a settled opinion) I thought I\'d write about this and see if I came to any kind of conclusion...\\n\\n\x3c!--truncate--\x3e\\n\\n## The Setup\\n\\nIt started as I was working on a new project. We were using ASP.NET MVC 3 and Entity Framework with DbContext as our persistence layer. Rather than crowbarring the tests in afterwards the intention was to write tests to support the ongoing development. Not quite test driven development but certainly [test supported development](http://blog.troyd.net/Test+Supported+Development+TSD+Is+NOT+Test+Driven+Development+TDD.aspx). (Let\'s not get into the internecine conflict as to whether this is black belt testable code or not - it isn\'t but he who pays the piper etc.) Oh and we were planning to use MOQ as our mocking library.\\n\\nIt was the first time I\'d used DbContext rather than ObjectContext and so I thought I\'d do a little research on how people were using DbContext with regards to testability. I had expected to find that there was some kind of consensus and an advised way forwards. I didn\'t get that at all. Instead I found a number of conflicting opinions.\\n\\n## Using the Repository / Unit of Work Patterns\\n\\nOne thread of advice that came out was that people advised using the Repository / Unit of Work patterns as wrappers when it came to making testable code. This is kind of interesting in itself as to the best of my understanding ObjectSet / ObjectContext and DbSet / DbContext are both in themselves implementations of the Repository / Unit of Work patterns. So the advice was to build a Repository / Unit of Work pattern to wrap an existing Repository / Unit of Work pattern.\\n\\nNot as mad as it sounds. The reason for the extra abstraction is that ObjectContext / DbContext in the raw are not MOQ-able.\\n\\n## Or maybe I\'m wrong, maybe you can MOQ DbContext?\\n\\nNo you can\'t. Well that\'s not true. You can and it\'s documented [here](http://romiller.com/2012/02/14/testing-with-a-fake-dbcontext/) but there\'s a \\"but\\". You need to be using Entity Frameworks Code First approach; actually coding up your DbContext yourself. Before I\'d got on board the project had already begun and we were already some way down the road of using the Database First approach. So this didn\'t seem to be a go-er really.\\n\\nThe best article I found on testability and Entity Framework was [this one](http://msdn.microsoft.com/en-us/library/ff714955.aspx) by [K. Scott Allen](http://odetocode.com/) which essentially detailed how you could implement the Repository / Unit of Work patterns on top of ObjectSet / ObjectContext. In the end I adapted this to do the same thing sat on top of DbSet / DbContext instead.\\n\\nWith this in place I had me my testable code. I was quite happy with this as it seemed quite intelligible. My new approach looked similar to the existing DbSet / DbContext code and so there wasn\'t a great deal of re-writing to do. Sorted, right?\\n\\n## Here come the nagging doubts...\\n\\nI did wonder, given that I found a number of articles about applying the Repository / Unit of Work patterns on top of ObjectSet / ObjectContext that there didn\'t seem to be many examples to do the same for DbSet / DbContext. (I did find a few examples of this but none that felt satisfactory to me for a variety of reasons.) This puzzled me.\\n\\nI also started to notice that a 1 man war was being waged against the approach I was using by [Ladislav Mrnka](http://www.ladislavmrnka.com/about/). Here are a couple of examples of his crusade:\\n\\n- [An answer on StackOverflow](http://stackoverflow.com/a/6904479/761388) (there\'s quite a few similar answers around on StackOverflow saying similar)\\n- [A comment on Rowan Millers post about fake DbContexts](http://romiller.com/2012/02/14/testing-with-a-fake-dbcontext/#div-comment-1620)\\n\\nLadislav is quite strongly of the opinion that wrapping DbSet / DbContext (and I presume ObjectSet / ObjectContext too) in a further Repository / Unit of Work is an antipattern. To quote him: _\\"The reason why I don\u2019t like it is leaky abstraction in Linq-to-entities queries ... In your test you have Linq-to-Objects which is superset of Linq-to-entities and only subset of queries written in L2O is translatable to L2E\\"_. It\'s worth looking at [Jon Skeets explanation of \\"leaky abstractions\\"](http://www.youtube.com/watch?v=gNeSZYke-_Q) which he did for TekPub.\\n\\nAs much as I didn\'t want to admit it - I have come to the conclusion Ladislav probably has a point for a number of reasons:\\n\\n### 1\\\\. Just because it compiles and passes unit tests don\'t imagine that means it works...\\n\\nUnfortunately, a LINQ query that looks right, compiles and has passing unit tests written for it doesn\'t necessarily work. You can take a query that fails when executed against Entity Framework and come up with test data that will pass that unit test. As Ladislav rightly points out: `LINQ-to-Objects != LINQ-to-Entities`.\\n\\nSo in this case unit tests of this sort don\'t provide you with any security. What you need are \\\\*\\\\*<u>integration</u>\\n\\n\\\\*\\\\* tests. Tests that run against an instance of the database and demonstrate that LINQ will actually translate queries / operations into valid SQL.\\n\\n### 2\\\\. Complex queries\\n\\nYou can write some pretty complex LINQ queries if you want. This is made particularly easy if you\'re using [comprehension syntax](https://blogs.msdn.com/b/ericlippert/archive/2009/12/07/query-transformations-are-syntactic.aspx). Whilst these queries may be simple to write it can be uphill work to generate test data to satisfy this. So much so that at times it can feel you\'ve made a rod for your own back using this approach.\\n\\n### 3\\\\. Lazy Loading\\n\\nBy default Entity Framework employs lazy loading. This a useful approach which reduces the amount of data that is transported. Sometimes this approach forces you to specify up front if you require a particular entity through use of `Include` statements. This again doesn\'t lend itself to testing particularly well.\\n\\n## Where does this leave us?\\n\\nHaving considered all of the above for a while and tried out various different approaches I think I\'m coming to the conclusion that Ladislav is probably right. Implementing the Repository / Unit of Work patterns on top of ObjectSet / ObjectContext or DbSet / DbContext doesn\'t seem a worthwhile effort in the end.\\n\\nSo what\'s a better idea? I think that in the name of simplicity you might as well have a simple class which wraps all of your Entity Framework code. This class could implement an interface and hence be straightforwardly MOQ-able (or alternatively all methods could be virtual and you could forego the interface). Along with this you should have integration tests in place which test the execution of the actual Entity Framework code against a test database.\\n\\nNow I should say this approach is not necessarily my final opinion. It seems sensible and practical. I think it is likely to simplify the tests that are written around a project. It will certainly be more reliable than just having unit tests in place.\\n\\nIn terms of the project I\'m working on at the moment we\'re kind of doing this in a halfway house sense. That is to say, we\'re still using our Repository / Unit of Work wrappers for DbSet / DbContext but where things move away from simple operations we\'re adding extra methods to our Unit of Work class or Repository classes which wrap this functionality and then testing it using our integration tests.\\n\\nI\'m open to the possibility that my opinion may be modified further. And I\'d be very interested to know what other people think on the subject.\\n\\n## Update\\n\\nIt turns out that I\'m not alone in thinking about this issue and indeed others have expressed this rather better than me - take a look at Jimmy Bogard\'s post for an example: [http://lostechies.com/jimmybogard/2012/09/20/limiting-your-abstractions/](http://lostechies.com/jimmybogard/2012/09/20/limiting-your-abstractions/).\\n\\n## Updated 2\\n\\nI\'ve also recently watched the following Pluralsight course by Julie Lerman: [http://pluralsight.com/training/Courses/TableOfContents/efarchitecture#efarchitecture-m3-archrepo](http://pluralsight.com/training/Courses/TableOfContents/efarchitecture#efarchitecture-m3-archrepo). In this course Julie talks about different implementations of the Repository and Unit of Work patterns in conjunction with Entity Framework. Julie is in favour of using this approach but in this module she elaborates on different \\"flavours\\" of these patterns that you might want to use for different reasons (bounded contexts / reference contexts etc). She makes a compelling case and helpfully she is open enough to say that this a point of contention in the community. At the end of watching this I think I felt happy that our \\"halfway house\\" approach seems to fit and seems to work. More than anything else Julie made clear that there isn\'t one definitively \\"true\\" approach. Rather many different but similar approaches for achieving the same goal. Good stuff Julie!"},{"id":"giving-odata-to-crm-40","metadata":{"permalink":"/giving-odata-to-crm-40","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2012-09-24-giving-odata-to-crm-40/index.md","source":"@site/blog/2012-09-24-giving-odata-to-crm-40/index.md","title":"Giving OData to CRM 4.0","description":"The article explains how to create an OData service to access Dynamics CRM 4.0 by using LINQ to CRM provider and WCF Data Services.","date":"2012-09-24T00:00:00.000Z","tags":[{"inline":false,"label":"C#","permalink":"/tags/csharp","description":"The C# programming language."}],"readingTime":6.27,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"giving-odata-to-crm-40","title":"Giving OData to CRM 4.0","authors":"johnnyreilly","tags":["c#"],"hide_table_of_contents":false,"description":"The article explains how to create an OData service to access Dynamics CRM 4.0 by using LINQ to CRM provider and WCF Data Services."},"unlisted":false,"prevItem":{"title":"Unit Testing and Entity Framework: The Filth and the Fury","permalink":"/unit-testing-and-entity-framework-filth"},"nextItem":{"title":"Globalize and jQuery Validation","permalink":"/globalize-and-jquery-validate"}},"content":"Just recently I was tasked with seeing if we could provide a way to access our Dynamics CRM instance via OData. My initial investigations made it seem like there was nothing for me to do; [CRM 2011 provides OData support out of the box](http://msdn.microsoft.com/en-us/library/gg309461.aspx). Small problem. We were running CRM 4.0.\\n\\n\x3c!--truncate--\x3e\\n\\nIt could well have ended there apart from the fact that Microsoft makes it astonishingly easy to to create your own OData service using WCF Data Services. Because it\'s so straightforward I was able to get an OData solution for CRM 4.0 up and running with very little heavy lifting at all. Want to know how it\'s done?\\n\\n## LINQ to CRM\\n\\nTo start with you\'re going to need the [CRM SDK 4.0](http://www.microsoft.com/en-us/download/details.aspx?id=38). This contains a \\"vanilla\\" LINQ to CRM client which is used in each of the example applications that can be found in `microsoft.xrm\\\\samples`. We want this client (or something very like it) to use as the basis for our OData service.\\n\\nIn order to get a LINQ to CRM provider that caters for your own customised CRM instance you need to use the `crmsvcutil` utility from the CRM SDK (found in the `microsoft.xrm\\\\tools\\\\` directory). Detailed instructions on how to use this can be found in this Word document: `microsoft.xrm\\\\advanced_developer_extensions_-_developers_guide.docx`. Extra information around the topic can be found using these links:\\n\\n- [MSDN docs on xRM](http://msdn.microsoft.com/en-us/library/ff681559)\\n- [MSDN examples of LINQ queries](http://msdn.microsoft.com/en-us/library/ff681573)\\n- [CRM blog site](http://www.dynamicscrmtrickbag.com/)\\n- [Another site listing examples of LINQ to CRM](http://community.adxstudio.com/products/adxstudio-portals/developers-guide/archive/linq-to-crm-22/)\\n\\nYou should end up with custom generated data context classes which look not dissimilar to similar classes that you may already have in place for Entity Framework etc. With your `Xrm.DataContext` in hand (a subclass of `Microsoft.Xrm.Client.Data.Services.CrmDataContext`) you\'ll be ready to move forwards.\\n\\n## Make me an OData Service\\n\\nAs I said, Microsoft makes it fantastically easy to get an OData service up and running. [In this example](http://msdn.microsoft.com/en-US/library/dd728275) an entity context model is created from the Northwind database and then exposed as an OData service. To create my CRM OData service I followed a similar process. But rather than creating an entity context model using a database I plugged in the `Xrm.DataContext` instance of CRM that we created a moment ago. These are the steps I followed to make my service:\\n\\n1. Create a new ASP.NET Web Application called \\"CrmOData\\" (in case it\'s relevant I was using Visual Studio 2010 to do this).\\n2. Remove all ASPXs / JavaScript / CSS files etc leaving you with an essentially empty project.\\n3. Add references to the following DLLs that come with the SDK: - microsoft.crm.sdk.dll\\n\\n   - microsoft.crm.sdktypeproxy.dll\\n   - microsoft.crm.sdktypeproxy.xmlserializers.dll\\n   - microsoft.xrm.client.dll\\n   - microsoft.xrm.portal.dll\\n   - microsoft.xrm.portal.files.dll\\n\\n4. Add the `&lt;microsoft.xrm.client&gt;` config section to your web.config (not forgetting the associated Xrm connection string)\\n5. Add this new file below to the root of the project:\\n\\n```cs\\nusing System;\\nusing System.Collections.Generic;\\nusing System.Data.Services;\\nusing System.Data.Services.Common;\\nusing System.Linq;\\nusing System.Web;\\nusing System.ServiceModel.Web;\\nusing Microsoft.Xrm.Client;\\nusing log4net;\\n\\nnamespace CrmOData\\n{\\n\\n    /// <summary>\\n    /// Exposes an OData service providing access to CRM\\n    ///\\n    /// Examples of how to use service.\\n    ///\\n    /// URI     : http://myWebServer/CrmOData/Crm.svc\\n    /// Purpose : Demonstrates exposed endpoints\\n    ///\\n    /// URI     : http://myWebServer/CrmOData/Crm.svc/myCustomer\\n    /// Purpose : Demonstrates how to retrieve all customers\\n    ///\\n    /// URI     : http://myWebServer/CrmOData/Crm.svc/myCustomer?$filter=lastName eq \'Reilly\'\\n    /// Purpose : Demonstrates how to retrieve all customers with the Surname \\"Reilly\\"\\n    ///\\n    /// URI     : http://myWebServer/CrmOData/Crm.svc/myCustomer?$select=firstName,lastName\\n    /// Output  : Does not work.\\n    ///\\n    /// \\"$select statements are not supported. This problem is being discussed\\n    ///  here http://social.msdn.microsoft.com/Forums/en/adodotnetdataservices/thread/366086ee-dcef-496a-ad15-f461788ae678\\n    ///  and is caused by the fact that CrmDataContext implements the IExpandProvider interface which in turn causes\\n    ///  the DataService to lose support for $select projections\\"\\n    ///\\n    ///  See http://social.microsoft.com/Forums/en/crmdevelopment/thread/31daedb4-3d75-483a-8d7f-269af3375d74 for original post discussing this\\n    ///\\n    ///  URI     : http://myWebServer/CrmOData/Crm.svc/myCustomer(guid\'783323a1-b1f1-4910-b5be-a2f37e62d0ba\')/currentBalance\\n    ///  Purpose : Retrieves the current balance of the customers account\\n    ///\\n    ///  URI     : http://myWebServer/CrmOData/Crm.svc/myCustomer(guid\'783323a1-b1f1-4910-b5be-a2f37e62d0ba\')/currentBalance/$value\\n    ///  Output  : 321186905.8600\\n    ///  Purpose : The raw value\\n    ///\\n    ///  URI     : http://myWebServer/CrmOData/Crm.svc/myCustomer(guid\'783323a1-b1f1-4910-b5be-a2f37e62d0ba\')?$expand=transactions\\n    ///  Purpose : Retrieves a customer by their guid\'783323a1-b1f1-4910-b5be-a2f37e62d0ba\', with the transactions property expanded (the equivalent of Include in Entity Framework I guess)\\n    /// </summary>\\n    public class Crm : DataService< Xrm.DataContext >\\n    {\\n        private static ILog _log;\\n\\n        /// <summary>\\n        /// Initialise the service (this method is called only once to initialize service-wide policies.)\\n        /// </summary>\\n        /// <param name=\\"config\\"></param>\\n        public static void InitializeService(DataServiceConfiguration config)\\n        {\\n            //Allows access to everything\\n            config.SetEntitySetAccessRule(\\"*\\", EntitySetRights.AllRead);\\n            config.SetEntitySetPageSize(\\"*\\", 10); //Only allow access to 10 items at a time - don\'t want to bring down CRM\\n            config.SetServiceOperationAccessRule(\\"*\\", ServiceOperationRights.AllRead);\\n\\n            config.DataServiceBehavior.MaxProtocolVersion = DataServiceProtocolVersion.V2;\\n\\n            // set cache policy to this page\\n            HttpContext context = HttpContext.Current;\\n            HttpCachePolicy cachePolicy = HttpContext.Current.Response.Cache;\\n\\n            // server&private: server and client side cache only - not at proxy servers\\n            cachePolicy.SetCacheability(HttpCacheability.ServerAndPrivate);\\n\\n            // default cache expire: 60 seconds\\n            cachePolicy.SetExpires(HttpContext.Current.Timestamp.AddSeconds(60));\\n\\n            // cached output depends on: accept, charset, encoding, and all parameters (like $filter, etc)\\n            cachePolicy.VaryByHeaders[\\"Accept\\"] = true;\\n            cachePolicy.VaryByHeaders[\\"Accept-Charset\\"] = true;\\n            cachePolicy.VaryByHeaders[\\"Accept-Encoding\\"] = true;\\n            cachePolicy.VaryByParams[\\"*\\"] = true;\\n\\n            //allow client to send Cache-Control: nocache headers to invalidate cache\\n            cachePolicy.SetValidUntilExpires(false);\\n\\n            //Log service startup initialisation\\n            _log = log4net.LogManager.GetLogger(\\"Crm.svc\\");\\n            _log.Info(\\"Crm.svc initialising...\\");\\n        }\\n\\n        /// <summary>\\n        /// Allows the user to get the id of a specific CrmEntity given a supplied entity name\\n        /// and a supplied predicate which consists of a propertyName and a string propertyValue (eg \\"112001-S\\").\\n        ///\\n        /// If there is a need for a predicate with different type of value (eg int / datetime / decimal)\\n        /// then it could be introduced\\n        ///\\n        /// Example URI : http://myWebServer/CrmOData/Crm.svc/GetId?entityName=\'myCustomer\'&propertyName=\'customerNumber\'&propertyValue=\'23456KL-P\'\\n        /// </summary>\\n        /// <param name=\\"entityName\\">eg \\"myCustomer\\"</param>\\n        /// <param name=\\"propertyName\\">eg \\"customerNumber\\"</param>\\n        /// <param name=\\"propertyValue\\">eg \\"23456KL-P\\"</param>\\n        /// <returns></returns>\\n        [WebGet]\\n        public Guid? GetEntityId(string entityName, string propertyName, string propertyValue)\\n        {\\n            var entities = CurrentDataSource.GetEntities(entityName);\\n\\n            var entitiesWhere = entities.Where(x => (x.GetPropertyValue(propertyName) as string) == propertyValue);\\n\\n            var guid = entitiesWhere.Select(x => x.Id)\\n                                    .SingleOrDefault();\\n\\n            return guid;\\n        }\\n\\n        /// <summary>\\n        /// Handle exceptions\\n        /// </summary>\\n        /// <param name=\\"args\\"></param>\\n        protected override void HandleException(HandleExceptionArgs args)\\n        {\\n            base.HandleException(args);\\n\\n            //Log all exceptions\\n            _log.Error(string.Format(\\"\\\\r\\\\nResponseContentType: {0}\\\\r\\\\nResponseStatusCode: {1}\\\\r\\\\nResponseWritten: {2}\\\\r\\\\nUser: {3}{4}\\",\\n                args.ResponseContentType, args.ResponseStatusCode, args.ResponseWritten, HttpContext.Current.User.Identity.Name, args.Exception.GetExceptionDetails()),\\n                args.Exception);\\n        }\\n    }\\n}\\n```\\n\\nAnd that\'s it - done. When you run this web application you will find an OData service exposed at `http://localhost:12345/Crm.svc`. You could have it even simpler if you wanted - you could pull out the logging that\'s in place and leave only the `InitializeService` there. That\'s all you need. (The `GetEntityById` method is a helper method of my own for identifying the GUIDs of CRM.)\\n\\nYou may have noticed that I have made use of caching for my OData service following the steps I found [here](https://blogs.msdn.com/b/peter_qian/archive/2010/11/17/using-asp-net-output-caching-with-wcf-data-services.aspx). Again you may or may not want to use this.\\n\\n## Now, a warning...\\n\\nOkay - not so much a warning as a limitation. Whilst most aspects of the OData service work as you would hope there is no support for the $select operator. I had a frustrating time trying to discover why and then came upon this explanation:\\n\\n_\\"$select statements are not supported. This problem is being discussed here [http://social.msdn.microsoft.com/Forums/en/adodotnetdataservices/thread/366086ee-dcef-496a-ad15-f461788ae678](http://social.msdn.microsoft.com/Forums/en/adodotnetdataservices/thread/366086ee-dcef-496a-ad15-f461788ae678) and is caused by the fact that CrmDataContext implements the IExpandProvider interface which in turn causes the DataService to lose support for $select projections\\"_\\n\\nYou can also see [here](http://social.microsoft.com/Forums/en/crmdevelopment/thread/31daedb4-3d75-483a-8d7f-269af3375d74) for the original post discussing this.\\n\\n## Finishing off\\n\\nIn the example I set out here I used the version of WCF Data Services that shipped with Visual Studio 2010. WCF Data Services now ships separately from the .NET Framework and you can [pick up the latest and greatest from Nuget](http://nuget.org/packages?q=wcf+data+services). I understand that you could easily switch over to using the latest versions but since I didn\'t see any feature that I needed on this occasion I haven\'t.\\n\\nI hope you find this useful."},{"id":"globalize-and-jquery-validate","metadata":{"permalink":"/globalize-and-jquery-validate","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2012-09-06-globalize-and-jquery-validate/index.md","source":"@site/blog/2012-09-06-globalize-and-jquery-validate/index.md","title":"Globalize and jQuery Validation","description":"A jQuery plugin has been replaced by Globalize and makes locale specific number and date formatting easy with Javascript; a tutorial on how to use it.","date":"2012-09-06T00:00:00.000Z","tags":[{"inline":false,"label":"Globalize","permalink":"/tags/globalize","description":"The Globalize library."},{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."},{"inline":false,"label":"jQuery","permalink":"/tags/jquery","description":"The jQuery library."}],"readingTime":5.49,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"globalize-and-jquery-validate","title":"Globalize and jQuery Validation","authors":"johnnyreilly","tags":["globalize","asp.net","jquery"],"hide_table_of_contents":false,"description":"A jQuery plugin has been replaced by Globalize and makes locale specific number and date formatting easy with Javascript; a tutorial on how to use it."},"unlisted":false,"prevItem":{"title":"Giving OData to CRM 4.0","permalink":"/giving-odata-to-crm-40"},"nextItem":{"title":"How to attribute encode a PartialView in MVC (Razor)","permalink":"/how-to-attribute-encode-partialview-in"}},"content":"## Updated 05/10/2015\\n\\nIf you\'re after a version of this that works with Globalize 1.x then take a look [here](../2015-10-05-jquery-validation-globalize-hits-10/index.md).\\n\\n## Updated 27/08/2013\\n\\nTo make it easier for people to use the approach detailed in this post I have created a repository for `jquery.validate.globalize.js` on GitHub [here](https://github.com/johnnyreilly/jquery-validation-globalize).\\n\\nThis is also available as a nuget package [here](https://www.nuget.org/packages/jQuery.Validation.Globalize/).\\n\\nTo see a good demo take a look [here](http://jqueryvalidationunobtrusivenative.azurewebsites.net/AdvancedDemo/Globalize).\\n\\n\x3c!--truncate--\x3e\\n\\n## Background\\n\\n[I\'ve written before about a great little library called Globalize](../2012-05-07-globalizejs-number-and-date/index.md) which makes locale specific number / date formatting simple within JavaScript. And I\'ve just stumbled upon an [old post written by Scott Hanselman about the business of Globalisation / Internationalisation / Localisation within ASP.NET](http://www.hanselman.com/blog/GlobalizationInternationalizationAndLocalizationInASPNETMVC3JavaScriptAndJQueryPart1.aspx). It\'s a great post and I recommend reading it (I\'m using many of the approaches he discusses).\\n\\n## jQuery Global is dead... Long live Globalize!\\n\\nHowever, there\'s one tweak I would make to Scotts suggestions and that\'s to use Globalize in place of the jQuery Global plugin. The jQuery Global plugin has now effectively been reborn as Globalize (with no dependancy on jQuery). As far as I can tell jQuery Global is now disappearing from the web - certainly the link in Scotts post is dead now at least. I\'ve ~~ripped off~~ been inspired by the \\"Globalized jQuery Unobtrusive Validation\\" section of Scotts article and made `jquery.validate.globalize.js`.\\n\\nAnd for what it\'s worth `jquery.validate.globalize.js` applies equally to standard jQuery Validation as well as to jQuery Unobtrusive Validation. I say that as the above JavaScript is effectively a monkey patch to the number / date / range / min / max methods of jQuery.validate.js which forces these methods to use Globalize\'s parsing support instead.\\n\\nHere\'s the JavaScript:\\n\\n```js\\n(function ($, Globalize) {\\n  // Clone original methods we want to call into\\n  var originalMethods = {\\n    min: $.validator.methods.min,\\n    max: $.validator.methods.max,\\n    range: $.validator.methods.range,\\n  };\\n\\n  // Tell the validator that we want numbers parsed using Globalize\\n\\n  $.validator.methods.number = function (value, element) {\\n    var val = Globalize.parseFloat(value);\\n    return this.optional(element) || $.isNumeric(val);\\n  };\\n\\n  // Tell the validator that we want dates parsed using Globalize\\n\\n  $.validator.methods.date = function (value, element) {\\n    var val = Globalize.parseDate(value);\\n    return this.optional(element) || val;\\n  };\\n\\n  // Tell the validator that we want numbers parsed using Globalize,\\n  // then call into original implementation with parsed value\\n\\n  $.validator.methods.min = function (value, element, param) {\\n    var val = Globalize.parseFloat(value);\\n    return originalMethods.min.call(this, val, element, param);\\n  };\\n\\n  $.validator.methods.max = function (value, element, param) {\\n    var val = Globalize.parseFloat(value);\\n    return originalMethods.max.call(this, val, element, param);\\n  };\\n\\n  $.validator.methods.range = function (value, element, param) {\\n    var val = Globalize.parseFloat(value);\\n    return originalMethods.range.call(this, val, element, param);\\n  };\\n})(jQuery, Globalize);\\n\\n$(document).ready(function () {\\n  // Set Globalize to the current culture driven by the html lang property\\n  var currentCulture = $(\'html\').prop(\'lang\');\\n  if (currentCulture) {\\n    Globalize.culture(currentCulture);\\n  }\\n});\\n```\\n\\nThe above script does 2 things. Firstly it monkey patches jquery.validate.js to make use of Globalize.js number and date parsing in place of the defaults. Secondly it initialises Globalize to relevant current culture driven by the `html lang` property. So if the html tag looked like this:\\n\\n```html\\n<html lang=\\"de-DE\\">\\n  ...\\n</html>\\n```\\n\\nThen Globalize would be initialised with the \\"de-DE\\" culture assuming that culture was available and had been served up to the client. (By the way, the Globalize initialisation logic has only been placed in the code above to demonstrate that Globalize needs to be initialised to the culture. It\'s more likely that this initialisation step would sit elsewhere in a \\"proper\\" app.)\\n\\n## Wait, where\'s `html lang` getting set?\\n\\nIn Scott\'s article he created a `MetaAcceptLanguage` helper to generate a META tag like this: `&lt;meta name=\\"accept-language\\" content=\\"en-GB\\" /&gt;` which he used to drive Globalizes specified culture.\\n\\nRather than generating a meta tag I\'ve chosen to use the `lang` attribute of the `html` tag to specify the culture. I\'ve chosen to do this as it\'s more in line with the [W3C spec](http://www.w3.org/TR/i18n-html-tech-lang/#ri20030510.102829377). But it should be noted this is just a different way of achieving exactly the same end.\\n\\nSo how\'s it getting set? Well, it\'s no great shakes; in my `_Layout.cshtml` file my html tag looks like this:\\n\\n```html\\n<html lang=\\"@System.Globalization.CultureInfo.CurrentUICulture.Name\\"></html>\\n```\\n\\nAnd in my `web.config` I have following setting set:\\n\\n```xml\\n<configuration>\\n  <system.web>\\n    <globalization culture=\\"auto\\" uiCulture=\\"auto\\" />\\n    \x3c!--- Other stuff.... --\x3e\\n  </system.web>\\n</configuration>\\n```\\n\\nWith both of these set this means I get `&lt;html lang=\\"de-DE\\"&gt;` or `&lt;html lang=\\"en-GB\\"&gt;` etc. depending on a users culture.\\n\\n## Serving up the right Globalize culture files\\n\\nIn order that I send the correct Globalize culture to the client I\'ve come up with this static class which provides the user with the relevant culture URL (falling back to the en-GB culture if it can\'t find one based your culture):\\n\\n```cs\\nusing System;\\nusing System.Collections.Generic;\\nusing System.Linq;\\nusing System.Web;\\nusing System.Web.Hosting;\\nusing System.IO;\\nusing System.Globalization;\\n\\nnamespace My.Helpers\\n{\\n    /// <summary>\\n    /// Static class that is a store for commonly used filenames\\n    /// (so if the files are updated they only need to be amended in a single place)\\n    /// </summary>\\n    public static class GlobalizeUrls\\n    {\\n\\n        /// <summary>\\n        /// URL for Globalize: https://github.com/jquery/globalize\\n        /// </summary>\\n        public static string Globalize { get { return \\"~/Scripts/globalize.js\\"; } }\\n\\n        /// <summary>\\n        /// URL for the specific Globalize culture\\n        /// </summary>\\n        public static string GlobalizeCulture\\n        {\\n            get\\n            {\\n                //Determine culture - GUI culture for preference, user selected culture as fallback\\n                var currentCulture = CultureInfo.CurrentCulture;\\n                var filePattern = \\"~/scripts/globalize/globalize.culture.{0}.js\\";\\n                var regionalisedFileToUse = string.Format(filePattern, \\"en-GB\\"); //Default localisation to use\\n\\n                //Try to pick a more appropriate regionalisation\\n                if (File.Exists(HostingEnvironment.MapPath(string.Format(filePattern, currentCulture.Name)))) //First try for a globalize.culture.en-GB.js style file\\n                    regionalisedFileToUse = string.Format(filePattern, currentCulture.Name);\\n                else if (File.Exists(HostingEnvironment.MapPath(string.Format(filePattern, currentCulture.TwoLetterISOLanguageName)))) //That failed; now try for a globalize.culture.en.js style file\\n                    regionalisedFileToUse = string.Format(filePattern, currentCulture.TwoLetterISOLanguageName);\\n\\n                return regionalisedFileToUse;\\n            }\\n        }\\n    }\\n}\\n```\\n\\n## Putting it all together\\n\\nTo make use of all of this together you\'ll need to have the `html lang` attribute set as described earlier and some scripts output in your layout page like this:\\n\\n```html\\n<script src=\\"@Url.Content(\\"~/Scripts/jquery.js\\")\\" type=\\"text/javascript\\"><\/script>\\n<script src=\\"@Url.Content(GlobalizeUrls.Globalize)\\" type=\\"text/javascript\\"><\/script>\\n<script src=\\"@Url.Content(GlobalizeUrls.GlobalizeCulture)\\" type=\\"text/javascript\\"><\/script>\\n<script src=\\"@Url.Content(\\"~/Scripts/jquery.validate.js\\")\\" type=\\"text/javascript\\"><\/script>\\n<script src=\\"@Url.Content(\\"~/scripts/jquery.validate.globalize.js\\")\\" type=\\"text/javascript\\"><\/script>\\n\\n@* Only serve the following script if you need it: *@\\n<script src=\\"@Url.Content(\\"~/scripts/jquery.validate.unobtrusive.js\\")\\" type=\\"text/javascript\\"><\/script>\\n```\\n\\nWhich will render something like this:\\n\\n```html\\n<script src=\\"/Scripts/jquery.js\\" type=\\"text/javascript\\"><\/script>\\n<script src=\\"/Scripts/globalize.js\\" type=\\"text/javascript\\"><\/script>\\n<script\\n  src=\\"/scripts/globalize/globalize.culture.en-GB.js\\"\\n  type=\\"text/javascript\\"\\n><\/script>\\n<script src=\\"/Scripts/jquery.validate.js\\" type=\\"text/javascript\\"><\/script>\\n<script\\n  src=\\"/Scripts/jquery.validate.globalize.js\\"\\n  type=\\"text/javascript\\"\\n><\/script>\\n<script\\n  src=\\"/Scripts/jquery.validate.unobtrusive.js\\"\\n  type=\\"text/javascript\\"\\n><\/script>\\n```\\n\\nThis will load up jQuery, Globalize, your Globalize culture, jQuery Validate, jQuery Validates unobtrusive extensions (which you don\'t need if you\'re not using them) and the jQuery Validate Globalize script which will set up culture aware validation.\\n\\nFinally and just to re-iterate, it\'s highly worthwhile to give [Scott Hanselman\'s original article a look](http://www.hanselman.com/blog/GlobalizationInternationalizationAndLocalizationInASPNETMVC3JavaScriptAndJQueryPart1.aspx). Most all the ideas in here were taken wholesale from him!"},{"id":"how-to-attribute-encode-partialview-in","metadata":{"permalink":"/how-to-attribute-encode-partialview-in","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2012-08-24-how-to-attribute-encode-partialview-in/index.md","source":"@site/blog/2012-08-24-how-to-attribute-encode-partialview-in/index.md","title":"How to attribute encode a PartialView in MVC (Razor)","description":"Find out how to attribute encode PartialView HTML in Razor/ASP.Net MVC with the HTML helper method `PartialAttributeEncoded`.","date":"2012-08-24T00:00:00.000Z","tags":[{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":3.19,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"how-to-attribute-encode-partialview-in","title":"How to attribute encode a PartialView in MVC (Razor)","authors":"johnnyreilly","tags":["asp.net"],"hide_table_of_contents":false,"description":"Find out how to attribute encode PartialView HTML in Razor/ASP.Net MVC with the HTML helper method `PartialAttributeEncoded`."},"unlisted":false,"prevItem":{"title":"Globalize and jQuery Validation","permalink":"/globalize-and-jquery-validate"},"nextItem":{"title":"ClosedXML - the real SDK for Excel","permalink":"/closedxml-real-sdk-for-excel"}},"content":"This post is plagiarism. But I\'m plagiarising myself so I don\'t feel too bad.\\n\\n\x3c!--truncate--\x3e\\n\\nI posted a [question](http://stackoverflow.com/q/12093005/761388) on StackOverflow recently asking if there was a simple way to attribute encode a PartialView in Razor / ASP.NET MVC. I ended up answering my own question and since I thought it was a useful solution it might be worth sharing.\\n\\n## The Question\\n\\nIn the project I was working on I was using PartialViews to store the HTML that would be rendered in a tooltip in my ASP.NET MVC application. (In case you\'re curious I was using the [jQuery Tools library for my tooltip](http://jquerytools.org/demos/tooltip/index.html) effect.)\\n\\nI had thought that Razor, clever beast that it is, would automatically attribute encode anything sat between quotes in my HTML. Unfortunately this doesn\'t appear to be the case. In the short term I was able to workaround this by using single quotation marks to encapsulate my PartialViews HTML. See below for an example:\\n\\n```xml\\n<div class=\\"tooltip\\"\\n     title=\'@Html.Partial(\\"_MyTooltipInAPartial\\")\'>\\n    Some content\\n</div>\\n```\\n\\nNow this worked just fine but I was aware that if any PartialView needed to use single quotation marks I would have a problem. Let\'s say for a moment that `_MyTooltipInAPartial.cshtml` contained this:\\n\\n```xml\\n<span style=\\"color:green\\">fjkdsjf\'lksdjdlks</span>\\n```\\n\\nWell when I used my handy little single quote workaround, the following would result:\\n\\n```xml\\n<div class=\\"tooltip\\"\\n     title=\'<span style=\\"color:green\\">fjkdsjf\'lksdjdlks</span>\'>\\n    Some content\\n</div>\\n```\\n\\nWhich although it doesn\'t show up so well in the code sample above is definite _\\"does not compute, does not compute, does not compute \\\\*LOUD EXPLOSION\\\\*\\"_ territory.\\n\\n## The Answer\\n\\nThis took me back to my original intent which was to encapsulate the HTML in double quotes like this:\\n\\n```xml\\n<div class=\\"tooltip\\"\\n     title=\\"@Html.Partial(\\"_MyTooltipInAPartial\\")\\">\\n    Some content\\n</div>\\n```\\n\\nThough with the example discussed above we clearly had a problem whether we used single or double quotes. What to do?\\n\\nWell the answer wasn\'t too complicated. After a little pondering I ended up scratching my own itch by writing an HTML helper method called `PartialAttributeEncoded` which made use of `HttpUtility.HtmlAttributeEncode` to HTML attribute encode a PartialView.\\n\\nHere\'s the code:\\n\\n```cs\\nusing System.Web;\\nusing System.Web.Mvc;\\nusing System.Web.Mvc.Html;\\n\\nnamespace My.Helpers\\n{\\n    /// <summary>\\n    /// MVC HtmlHelper extension methods - html element extensions\\n    /// </summary>\\n    public static class PartialExtensions\\n    {\\n        /// <summary>\\n        /// Allows a partial to be rendered within quotation marks.\\n        /// I use this with jQuery tooltips where we store the tooltip HMTL within a partial.\\n        /// See example usage below:\\n        /// <div class=\\"tooltip\\" title=\\"@Html.PartialAttributeEncoded(\\"_MyTooltipInAPartial\\")\\">Some content</div>\\n        /// </summary>\\n        /// <param name=\\"helper\\"></param>\\n        /// <param name=\\"partialViewName\\"></param>\\n        /// <param name=\\"model\\"></param>\\n        /// <returns></returns>\\n        public static MvcHtmlString PartialAttributeEncoded(\\n          this HtmlHelper helper,\\n          string partialViewName,\\n          object model = null\\n        )\\n        {\\n            //Create partial using the relevant overload (only implemented ones I used)\\n            var partialString = (model == null)\\n                ? helper.Partial(partialViewName)\\n                : helper.Partial(partialViewName, model);\\n\\n            //Attribute encode the partial string - note that we have to .ToString() this to get back from an MvcHtmlString\\n            var partialStringAttributeEncoded = HttpUtility.HtmlAttributeEncode(partialString.ToString());\\n\\n            //Turn this back into an MvcHtmlString\\n            var partialMvcStringAttributeEncoded = MvcHtmlString.Create(partialStringAttributeEncoded);\\n\\n            return partialMvcStringAttributeEncoded;\\n        }\\n    }\\n}\\n```\\n\\nUsing the above helper is simplicity itself:\\n\\n```xml\\n<div class=\\"tooltip\\"\\n     title=\\"@Html.PartialAttributeEncoded(\\"_MyTooltipInAPartial\\")\\">\\n    Some content\\n</div>\\n```\\n\\nAnd, given the example I\'ve been going through, it would provide you with this output:\\n\\n```xml\\n<div class=\\"tooltip\\"\\n     title=\\"&lt;span style=&quot;color:green&quot;>fjkdsjf&#39;lksdjdlks</span>\\">\\n    Some content\\n</div>\\n```\\n\\nNow the HTML in the title attribute above might be an unreadable mess - but it\'s the unreadable mess you need. That\'s what the HTML we\'ve been discussing looks like when it\'s been encoded.\\n\\n## Final thoughts\\n\\nI was surprised that Razor didn\'t handle this out of the box. I wonder if this is something that will come along with a later version? It\'s worth saying that I experienced this issue when working on an MVC 3 application. It\'s possible that this issue may actually have been solved with MVC 4 already; I haven\'t had chance to check yet though."},{"id":"closedxml-real-sdk-for-excel","metadata":{"permalink":"/closedxml-real-sdk-for-excel","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2012-08-16-closedxml-real-sdk-for-excel/index.md","source":"@site/blog/2012-08-16-closedxml-real-sdk-for-excel/index.md","title":"ClosedXML - the real SDK for Excel","description":"Closed XML simplifies Excel document creation for developers with its straightforward API, sitting on top of Open XML. A frustration-solver for many!","date":"2012-08-16T00:00:00.000Z","tags":[{"inline":false,"label":"C#","permalink":"/tags/csharp","description":"The C# programming language."}],"readingTime":3.75,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"closedxml-real-sdk-for-excel","title":"ClosedXML - the real SDK for Excel","authors":"johnnyreilly","tags":["c#"],"hide_table_of_contents":false,"description":"Closed XML simplifies Excel document creation for developers with its straightforward API, sitting on top of Open XML. A frustration-solver for many!"},"unlisted":false,"prevItem":{"title":"How to attribute encode a PartialView in MVC (Razor)","permalink":"/how-to-attribute-encode-partialview-in"},"nextItem":{"title":"jQuery Unobtrusive Validation (+ associated gotchas)","permalink":"/jquery-unobtrusive-validation"}},"content":"Simplicity appeals to me. It always has. Something that is simple is straightforward to comprehend and is consequently easy to use. It\'s clarity.\\n\\n\x3c!--truncate--\x3e\\n\\n## Open XML\\n\\nSo imagine my joy when I first encountered [Open XML](http://msdn.microsoft.com/en-us/office/bb265236.aspx). In Microsofts own words:\\n\\nECMA Office Open XML (\\"Open XML\\") is an international, open standard for word-processing documents, presentations, and spreadsheets that can be freely implemented by multiple applications on multiple platforms.\\n\\nWhat does that actually mean? Well, from my perspective in the work I was doing I needed to be able to programmatically interact with Excel documents from C#. I needed to be able to create spreadsheets, to use existing template spreadsheets which I could populate dynamically in code. I needed to do Excel. And according to Microsoft, the Open XML SDK was how I did this.\\n\\nWhat can I say about it? Open XML works. The API functions. You can use this to achieve your aims; and I did (initially). However, there\'s a but and it\'s this: it became quickly apparent just how hard Open XML makes you work to achieve relatively simple goals. Things that ought to be, in my head, a doddle require reams and reams of obscure code. Sadly, I feel that Open XML is probably the most frustrating API that I have yet encountered (and I\'ve coded against the old school Lotus Notes API).\\n\\n## Closed XML - Open XML\'s DbContext\\n\\nAs I\'ve intimated I found Open XML to be enormously frustrating. I\'d regularly find myself thinking I\'d achieved my goal. I may have written War and Peace code-wise but it compiled, it looked right - the end was in sight. More fool me. I\'d run, sit back watch my Excel doc get created / updated / whatever. Then I\'d open it and be presented with some obscure error about a corrupt file. Not great.\\n\\nAs I was Googling around looking for answers to my problem that I discovered an open source project on CodePlex called [Closed XML](http://closedxml.codeplex.com/). I wasn\'t alone in frustrations with Open XML - there were many of us sharing the same opinion. And some fantastic person had stepped into the breach to save us! In ClosedXMLs own words:\\n\\nClosedXML makes it easier for developers to create Excel 2007/2010 files. It provides a nice object oriented way to manipulate the files (similar to VBA) without dealing with the hassles of XML Documents. It can be used by any .NET language like C# and Visual Basic (VB).\\n\\nHallelujah!!!\\n\\nThe way it works (as far as I understand) is that ClosedXML sits on top of Open XML and exposes a really straightforward API for you to interact with. I haven\'t looked into the guts of it but my guess is that it internally uses Open XML to achieve this (as to use ClosedXML you must reference DocumentFormat.OpenXml.dll).\\n\\nI\'ve found myself thinking of ClosedXML\'s relationship to Open XML in the same way as I think about Entity Frameworks DbContexts relationship to ObjectContext. They do the same thing but the former in both cases offers a better API. They makes achieving the same goals \\\\***much**\\\\* easier. (Although in fairness to the EF team I should say that ObjectContext was not particularly problematic to use; just DbContext made life even easier.)\\n\\n## Support - This is how it should be done!\\n\\nShortly after I started using ClosedXML I was asked if we could use it to perform a certain task. I tested. We couldn\'t.\\n\\nWhen I discovered this [I raised a ticket](http://closedxml.codeplex.com/workitem/8174) against the project asking if the functionality was likely to be added at any point. I honestly didn\'t expect to hear back any time soon and was mentally working out ways to get round the issue for now.\\n\\nTo my surprise within _5 hours_ [MDeLeon](http://www.codeplex.com/site/users/view/MDeLeon) the developer behind ClosedXML had released a patch to the source code! By any stretch of the imagination that is fast! As it happened there were a few bugs that needed ironing out and over the course of the next 3 working days MDeLeon performed a number of fixes and left me quickly in the position of having a version of ClosedXML which allowed me to achieve my goal.\\n\\nSo this blog post exists in part to point anyone who is battling Open XML to ClosedXML. It\'s brilliant, well documented and I\'d advise anyone to use it. You won\'t be disappointed. And in part I wanted to say thanks and well done to MDeLeon who quite made my week! Thank you!\\n\\n[http://closedxml.codeplex.com/](http://closedxml.codeplex.com/)"},{"id":"jquery-unobtrusive-validation","metadata":{"permalink":"/jquery-unobtrusive-validation","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2012-08-06-jquery-unobtrusive-validation/index.md","source":"@site/blog/2012-08-06-jquery-unobtrusive-validation/index.md","title":"jQuery Unobtrusive Validation (+ associated gotchas)","description":"Implement unobtrusive jQuery validation in your MVC application using HTML 5 data attributes to simplify code maintenance and reduce mistakes.","date":"2012-08-06T00:00:00.000Z","tags":[{"inline":false,"label":"jQuery","permalink":"/tags/jquery","description":"The jQuery library."}],"readingTime":4.48,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"jquery-unobtrusive-validation","title":"jQuery Unobtrusive Validation (+ associated gotchas)","authors":"johnnyreilly","tags":["jquery"],"hide_table_of_contents":false,"description":"Implement unobtrusive jQuery validation in your MVC application using HTML 5 data attributes to simplify code maintenance and reduce mistakes."},"unlisted":false,"prevItem":{"title":"ClosedXML - the real SDK for Excel","permalink":"/closedxml-real-sdk-for-excel"},"nextItem":{"title":"Rendering Partial View to a String","permalink":"/rendering-partial-view-to-string"}},"content":"I was recently working on a project which had client side validation manually set up which essentially duplicated the same logic on the server. Like many things this had started out small and grown and grown until it became arduos and tedious to maintain.\\n\\n\x3c!--truncate--\x3e\\n\\nTime to break out the unobtrusive jQuery validation.\\n\\nIf you\u2019re not aware of this, as part of MVC 3 Microsoft leveraged the pre-existing [jQuery Validate library](http://bassistance.de/jquery-plugins/jquery-plugin-validation/) and introduced an \u201Cunobtrusive\u201D extension to this which allows the library to be driven by HTML 5 data attributes. I have mentioned this lovely extension before but I haven\'t been using it for the last 6 months or so. And coming back to it I realised that I had forgotten a few of the details / quirks.\\n\\nFirst up, \\"where do these HTML 5 data attributes come from?\\" I hear you cry. Why from the [Validation attributes that live in System.ComponentModel.DataAnnotations](http://msdn.microsoft.com/en-us/library/system.componentmodel.dataannotations.validationattribute.aspx).\\n\\nLet me illustrate. This decoration:\\n\\n```cs\\n[Required(),\\n   Range(0.01, Double.MaxValue, ErrorMessage = \\"A positive value is required for Price\\"),\\n   Display(Name = \\"My Price\\")]\\n  public double Price { get; set; }\\n```\\n\\nspecifies that the Price field on the model is required, that it requires a positive numeric value and that it\u2019s official name is \u201CMy Price\u201D. As a result of this decoration, when you use syntax like this in your view:\\n\\n```xml\\n@Html.LabelFor(x => x.Price)\\n  @Html.TextBoxFor(x => x.Price, new { id = \\"itsMyPrice\\", type = \\"number\\" })\\n```\\n\\nYou end up with this HTML:\\n\\n```xml\\n<label for=\\"Price\\">My Price</label>\\n  <input data-val=\\"true\\" data-val-number=\\"The field My Price must be a number.\\" data-val-range=\\"A positive value is required for My Price\\" data-val-range-max=\\"1.79769313486232E+308\\" data-val-range-min=\\"0.01\\" data-val-required=\\"The My Price field is required.\\" id=\\"itsMyPrice\\" name=\\"Price\\" type=\\"number\\" value=\\"\\">\\n```\\n\\nAs you can see MVC has done the hard work of translating these data annotations into HTML 5 data attributes so you don\u2019t have to. With this in place you can apply your validation in 1 place (the model) and 1 place only. This reduces the code you need to write exponentially. It also reduces duplication and therefore reduces the likelihood of mistakes.\\n\\nTo validate a form it\u2019s as simple as this:\\n\\n```js\\n$(\'form\').validate();\\n```\\n\\nOr if you wanted to validate a single element:\\n\\n```js\\n$(\'form\').validate().element(\'elementSelector\');\\n```\\n\\nOr if you wanted to prevent default form submission until validation was passed:\\n\\n```js\\n$(\'form\').submit(function (event) {\\n  var isValid = $(this).validate().valid();\\n\\n  return isValid; //True will allow submission, false will not\\n});\\n```\\n\\nSee what I mean? Simple!\\n\\nIf you want to read up on this further I recommend these links:\\n\\n- [The home of jQuery Validate](http://bassistance.de/jquery-plugins/jquery-plugin-validation/) \\\\- by the way it seems to be important to work with the latest version (1.9 at time of writing). I found some strange AJAX issues when using 1.7...\\n- [Brad Wilson\'s walkthrough of unobtrusive client validation](http://bradwilson.typepad.com/blog/2010/10/mvc3-unobtrusive-validation.html)\\n- [An example of how to implement your own custom validation both server side \\\\*and\\\\* client side](http://www.devtrends.co.uk/blog/the-complete-guide-to-validation-in-asp.net-mvc-3-part-2)\\n- [How to apply unobtrusive jQuery validation to dynamic content](http://xhalent.wordpress.com/2011/01/24/applying-unobtrusive-validation-to-dynamic-content/) \\\\- handy if you\'re creating HTML on the client which you want to be validated.\\n- And finally, a workaround for [a bug in MVC 3](http://aspnet.codeplex.com/workitem/7629) which means that data attributes aren\u2019t emitted when using DropDownListFor for nested objects: [http://forums.asp.net/t/1649193.aspx/1/10](http://forums.asp.net/t/1649193.aspx/1/10). In fact because I\'ve only seen this on a forum I\'ve copied and the pasted the code there to below because I feared it being lost: **Update: It turns out the self-same issue exists for TextAreaFor as well. Details of this and a workaround can be found [here](http://aspnet.codeplex.com/workitem/8576)... **\\n\\n```cs\\n/// <summary>\\n    /// MVC HtmlHelper extension methods - html element extensions\\n    /// These are drop down list extensions that work round a bug in MVC 3: http://aspnet.codeplex.com/workitem/7629\\n    /// These workarounds were taken from here: http://forums.asp.net/t/1649193.aspx/1/10\\n    /// </summary>\\n    public static class DropDownListExtensions\\n    {\\n        [SuppressMessage(\\"Microsoft.Design\\", \\"CA1006:DoNotNestGenericTypesInMemberSignatures\\", Justification = \\"This is an appropriate nesting of generic types\\")]\\n        public static MvcHtmlString SelectListFor<TModel, TProperty>(this HtmlHelper<TModel> htmlHelper, Expression<Func<TModel, TProperty>> expression, IEnumerable<SelectListItem> selectList)\\n        {\\n            return SelectListFor(htmlHelper, expression, selectList, null /* optionLabel */, null /* htmlAttributes */);\\n        }\\n\\n\\n        [SuppressMessage(\\"Microsoft.Design\\", \\"CA1006:DoNotNestGenericTypesInMemberSignatures\\", Justification = \\"This is an appropriate nesting of generic types\\")]\\n        public static MvcHtmlString SelectListFor<TModel, TProperty>(this HtmlHelper<TModel> htmlHelper, Expression<Func<TModel, TProperty>> expression, IEnumerable<SelectListItem> selectList, object htmlAttributes)\\n        {\\n            return SelectListFor(htmlHelper, expression, selectList, null /* optionLabel */, new RouteValueDictionary(htmlAttributes));\\n        }\\n\\n\\n        [SuppressMessage(\\"Microsoft.Design\\", \\"CA1006:DoNotNestGenericTypesInMemberSignatures\\", Justification = \\"This is an appropriate nesting of generic types\\")]\\n        public static MvcHtmlString SelectListFor<TModel, TProperty>(this HtmlHelper<TModel> htmlHelper, Expression<Func<TModel, TProperty>> expression, IEnumerable<SelectListItem> selectList, IDictionary<string, object> htmlAttributes)\\n        {\\n            return SelectListFor(htmlHelper, expression, selectList, null /* optionLabel */, htmlAttributes);\\n        }\\n\\n\\n        [SuppressMessage(\\"Microsoft.Design\\", \\"CA1006:DoNotNestGenericTypesInMemberSignatures\\", Justification = \\"This is an appropriate nesting of generic types\\")]\\n        public static MvcHtmlString SelectListFor<TModel, TProperty>(this HtmlHelper<TModel> htmlHelper, Expression<Func<TModel, TProperty>> expression, IEnumerable<SelectListItem> selectList, string optionLabel)\\n        {\\n            return SelectListFor(htmlHelper, expression, selectList, optionLabel, null /* htmlAttributes */);\\n        }\\n\\n\\n        [SuppressMessage(\\"Microsoft.Design\\", \\"CA1006:DoNotNestGenericTypesInMemberSignatures\\", Justification = \\"This is an appropriate nesting of generic types\\")]\\n        public static MvcHtmlString SelectListFor<TModel, TProperty>(this HtmlHelper<TModel> htmlHelper, Expression<Func<TModel, TProperty>> expression, IEnumerable<SelectListItem> selectList, string optionLabel, object htmlAttributes)\\n        {\\n            return SelectListFor(htmlHelper, expression, selectList, optionLabel, new RouteValueDictionary(htmlAttributes));\\n        }\\n\\n\\n        [SuppressMessage(\\"Microsoft.Design\\", \\"CA1011:ConsiderPassingBaseTypesAsParameters\\", Justification = \\"Users cannot use anonymous methods with the LambdaExpression type\\")]\\n        [SuppressMessage(\\"Microsoft.Design\\", \\"CA1006:DoNotNestGenericTypesInMemberSignatures\\", Justification = \\"This is an appropriate nesting of generic types\\")]\\n        public static MvcHtmlString SelectListFor<TModel, TProperty>(this HtmlHelper<TModel> htmlHelper, Expression<Func<TModel, TProperty>> expression, IEnumerable<SelectListItem> selectList, string optionLabel, IDictionary<string, object> htmlAttributes)\\n        {\\n            if (expression == null)\\n            {\\n                throw new ArgumentNullException(\\"expression\\");\\n            }\\n\\n\\n            ModelMetadata metadata = ModelMetadata.FromLambdaExpression(expression, htmlHelper.ViewData);\\n\\n\\n            IDictionary<string, object> validationAttributes = htmlHelper\\n                .GetUnobtrusiveValidationAttributes(ExpressionHelper.GetExpressionText(expression), metadata);\\n\\n\\n            if (htmlAttributes == null)\\n                htmlAttributes = validationAttributes;\\n            else\\n                htmlAttributes = htmlAttributes.Concat(validationAttributes).ToDictionary(k => k.Key, v => v.Value);\\n\\n\\n            return SelectExtensions.DropDownListFor(htmlHelper, expression, selectList, optionLabel, htmlAttributes);\\n        }\\n    }\\n```"},{"id":"rendering-partial-view-to-string","metadata":{"permalink":"/rendering-partial-view-to-string","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2012-07-16-rendering-partial-view-to-string/index.md","source":"@site/blog/2012-07-16-rendering-partial-view-to-string/index.md","title":"Rendering Partial View to a String","description":"John solves a problem with Partial Views in ASP.NET MVC, allowing simplified code and multiple view nesting.","date":"2012-07-16T00:00:00.000Z","tags":[{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":4.06,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"rendering-partial-view-to-string","title":"Rendering Partial View to a String","authors":"johnnyreilly","tags":["asp.net"],"hide_table_of_contents":false,"description":"John solves a problem with Partial Views in ASP.NET MVC, allowing simplified code and multiple view nesting."},"unlisted":false,"prevItem":{"title":"jQuery Unobtrusive Validation (+ associated gotchas)","permalink":"/jquery-unobtrusive-validation"},"nextItem":{"title":"Optimally Serving Up JavaScript","permalink":"/how-im-structuring-my-javascript-in-web"}},"content":"## Well done that man!\\n\\n\x3c!--truncate--\x3e\\n\\nEvery now and then I\'m thinking to myself \\"_wouldn\'t it be nice if you could do x..._\\" And then I discover that someone else has thought the self same thoughts and better yet they have the answer! I had this situation recently and discovered the wonderful Kevin Craft had been there, done that and made the T-shirt. Here\'s his blog: [http://craftycodeblog.com/2010/05/15/asp-net-mvc-render-partial-view-to-string/](http://craftycodeblog.com/2010/05/15/asp-net-mvc-render-partial-view-to-string/) I wanted to talk about how this simple post provided me with an elegant solution to something I\'ve found niggling and unsatisfactory for a while now... ## How it helped\\n\\nJust last week I was thinking about `Partial Views`. Some background. I\'m working on an ASP.NET MVC 3 project which provides users with a nice web interface to manage the workflow surrounding certain types of financial asset. The user is presented with a web page which shows a kind of grid to the user. As the user hovers over a row they are presented with a context menu which allows them to perform certain workflow actions. If they perform an action then that row will need to be updated to reflect this. Back in the day this would have been achieved by doing a full postback to the server. At the server the action would be taken, the persistent storage updated and then the whole page would be served up to the user again with the relevant row of HTML updated but everything else staying as is. Now there\'s nothing wrong with this approach as such. I mean it works just fine. But in my case since I knew that it was only that single row of HTML that was going to be updated and so I was loath to re-render the whole page. It seemed a waste to get so much data back from the server when only a marginal amount was due to change. And also I didn\'t want the user to experience the screen refresh flash. Looks ugly. Now in the past when I\'ve had a solution to this problem which from a UI perspective is good but from a development perspective slightly unsatisfactory. I would have my page call a controller method (via `jQuery.ajax`) to perform the action. This controller would return a `JsonResult` indicating success or failure and any data necessary to update the screen. Then in the `success` function I would manually update the HTML on the screen using the data provided. Now this solution works but there\'s a problem. [Can you tell what it is yet?](http://en.wikipedia.org/wiki/Rolf_Harris) It\'s not very DRY. I\'m repeating myself. When the page is initially rendered I have a `View` which renders (in this example) all the relevant HTML for the screen \\\\*including\\\\* the HTML for my rows of data. And likewise I have my JavaScript method for updating the screen too. So with this solution I have duplicated my GUI logic. If I update 1, I need to update the other. It\'s not a massive hardship but it is, as I say, unsatisfactory. I was recently thinking that it would be nice if I could refactor my row HTML into a `Partial View` which I could then use in 2 places: 1. In my standard `View` as I iterated through each element for display and 2. Nested inside a `JsonResult`...\\n\\nThe wonderful thing about approach 2 is that it allows me to massively simplify my `success` to this:\\n\\n```js\\n$(\'myRowSelector\').empty().html(data.RowHTML); //Where RowHTML is the property that\\n//contains my stringified PartialView\\n```\\n\\nand if I later make changes to the `Partial View` these changes will not require me to make any changes to my JavaScript at all. Brilliant! And entirely satisfactory. On the grounds that someone else might have had the same idea I did a little googling around. Sure enough I discovered [Kevin Craft\'s post](http://craftycodeblog.com/2010/05/15/asp-net-mvc-render-partial-view-to-string/) which was just the ticket. It does exactly what I\'d hoped. Besides being a nice and DRY solution this approach has a number of other advantages as well: - Given it\'s a `Partial View` the Visual Studio IDE provides a nice experience when coding it up with regards to intellisense / highlighting etc. Not something available when you\'re hand coding up a string which contains the HTML you\'d like passed back...\\n\\n- A wonderful debug experience. You can debug the rendering of a `Partial View` being rendered to a string in the same way as if the ASP.NET MVC framework was serving it up. I could have lived without this but it\'s fantastic to have it available.\\n- It\'s possible to nest \\\\***multiple**\\\\* `Partial Views` within your `JsonResult`. THIS IS WONDERFUL!!! This means that if several parts of your screen need to be updated (perhaps the row and a status panel as well) then as long as both are refactored into a `Partial View` you can generate them on the fly and pass them back.\\n\\nExcellent stuff!\\n\\n```\\n\\n```"},{"id":"how-im-structuring-my-javascript-in-web","metadata":{"permalink":"/how-im-structuring-my-javascript-in-web","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2012-07-01-how-im-structuring-my-javascript-in-web/index.md","source":"@site/blog/2012-07-01-how-im-structuring-my-javascript-in-web/index.md","title":"Optimally Serving Up JavaScript","description":"John describes his \\"JS second\\" approach to structuring JavaScript and using HtmlHelper to control the order of scripts in web apps for internal use.","date":"2012-07-01T00:00:00.000Z","tags":[{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."},{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."}],"readingTime":13.48,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"how-im-structuring-my-javascript-in-web","title":"Optimally Serving Up JavaScript","authors":"johnnyreilly","tags":["asp.net","javascript"],"hide_table_of_contents":false,"description":"John describes his \\"JS second\\" approach to structuring JavaScript and using HtmlHelper to control the order of scripts in web apps for internal use."},"unlisted":false,"prevItem":{"title":"Rendering Partial View to a String","permalink":"/rendering-partial-view-to-string"},"nextItem":{"title":"Reasons to be Cheerful (why now is a good time to be a dev)","permalink":"/reasons-to-be-cheerful-why-now-is-good"}},"content":"I have occasionally done some server-side JavaScript with Rhino and Node.js but this is the exception rather than the rule. Like most folk at the moment, almost all the JavaScript I write is in a web context.\\n\\n\x3c!--truncate--\x3e\\n\\nOver time I\'ve come to adopt a roughly standard approach to how I structure my JavaScript; both the JavaScript itself and how it is placed / rendered in the an HTML document. I wanted to write about the approach I\'m using. Partly just to document the approach but also because I often find writing about something crystalises my feelings on the subject in one way or another. I think that most of what I\'m doing is sensible and rational but maybe as I write about this I\'ll come to some firmer conclusions about my direction of travel.\\n\\n## What are you up to?\\n\\nBefore I get started it\'s probably worth mentioning the sort of web development I\'m generally called to do (as this has obviously influenced my decisions).\\n\\nMost of my work tends to be on web applications used internally within a company. That is to say, web applications accessible on a Company intranet. Consequently, the user base for my applications tends to be smaller than the Amazons and Googles of this world. It almost invariably sits on the ASP.NET stack in some way. Either classic WebForms or MVC.\\n\\n## \\"Render first. JS second.\\"\\n\\nI took 2 things away from [Steve Souder\'s article](http://www.stevesouders.com/blog/2010/09/30/render-first-js-second/):\\n\\n1. Async script loading is better than synchronous script loading\\n2. Get your screen rendered and \\\\***then**\\\\* execute your JavaScript\\n\\nI\'m not doing any async script loading as yet; although I am thinking of giving it a try at some point. In terms of choosing a loader I\'ll probably give RequireJS first crack of the whip (purely as it looks like most people are tending it\'s direction and that can\'t be without reason).\\n\\nHowever - it seems that the concept of async script loading is kind of conflict with one of the other tenets of web wisdom: script bundling. Script bundling, if you\'re not already aware, is the idea that you should combine all your scripts into a single file and then just serve that. This prevents multiple HTTP requests as each script loads in. Async script loading is obviously okay with multiple HTTP requests, presumably because of the asynchronous non-blocking pattern of loading. So. 2 different ideas. And there\'s further movement on this front right now as [Microsoft are baking in script bundling to .NET 4.5](http://www.hanselman.com/blog/VisualStudio2012RCIsReleasedTheBigWebRollup.aspx).\\n\\nRather than divide myself between these 2 horses I have at the moment tried to follow the \\"JS second\\" part of this advice in my own (perhaps slightly old fashioned) way...\\n\\n## I want to serve you...\\n\\nI have been making sure that scripts are the last thing served to the screen by using a customised version of [Michael J. Ryan\'s HtmlHelper](http://frugalcoder.us/post/2009/06/29/Handling-Scripts-in-ASPNet-MVC.aspx). This lovely helper allows you to add script references as required from a number of different sources (layout page, view, partial view etc - even the controller if you so desired). It\'s simple to control the ordering of scripts by allowing you to set a priority for each script which determines the render order.\\n\\nThen as a final step before rendering the `&lt;/body&gt;` tag the scripts can be rendered in one block. By this point the web page is rendered visually and a marginal amount of blocking is, in my view, acceptable.\\n\\nIf anyone is curious - the class below is my own version of Michael\'s helper. My contribution is the go faster stripes relating to the caching suffix and the ability to specify dependancies using script references rather than using numeric priority mechanism):\\n\\n```cs\\nusing System;\\nusing System.Collections.Generic;\\nusing System.Linq;\\nusing System.Web;\\nusing System.Web.Mvc;\\nusing System.IO;\\nusing System.Text;\\n\\nnamespace DummyMvc.Helpers\\n{\\n    /// <summary>\\n    /// This helper makes creation of script tags within a view straightforward.\\n    /// If the specified file is missing an error is raised at runtime.  A v= suffix\\n    /// is added to URLs using the VersionHelper to make caching behave as desired\\n    /// both during development (where we don\'t want to cache as we make changes to code)\\n    /// and during production (where we do want to cache to improve the user loading experience)\\n    ///\\n    /// There are 2 approaches available using this helper.\\n    ///\\n    /// 1. Straight script tag creation using the Script method.\\n    ///\\n    /// A usage of:\\n    ///\\n    /// @Html.Script(\\"~/Scripts/jquery.js\\")\\n    ///\\n    /// Would immediatedly render a tag as below: (please note the v= suffix to the URL)\\n    ///\\n    /// <script src=\\"/Scripts/jquery.tools.min.js?v=634765611410213405\\" type=\\"text/javascript\\"><\/script>\\n    ///\\n    /// 2. Script bundling\\n    ///\\n    /// This approach allows you to build up the scripts to be rendered across multiple\\n    /// views / partial views / layout etc and then have them rendered in one hit (ideally\\n    /// just prior to the body tag being rendered as at this point the screen will be visually\\n    /// set up for the user and any script loading that causes blocking should not be a presentational\\n    /// issue). This is heavily based on Michael Ryans work; see links below.\\n    ///\\n    /// [Links]\\n    /// http://frugalcoder.us/post/2009/06/29/Handling-Scripts-in-ASPNet-MVC.aspx\\n    /// http://stackoverflow.com/questions/5598167/mvc3-need-help-with-html-helper\\n    ///\\n    /// [Usage]\\n    /// --- From each view / partial view ---\\n    /// @Html.AddClientScript(\\"~/Scripts/jquery.js\\")\\n    /// @Html.AddClientScript(\\"~/Scripts/jquery-ui.js\\", dependancies: new string[] {\\"~/Scripts/jquery.js\\"})\\n    /// @Html.AddClientScript(\\"~/Scripts/site.js\\", dependancies: new string[] {\\"~/Scripts/jquery.js\\"})\\n    /// @Html.AddClientScript(\\"~/Scripts/views/myview.js\\", dependancies: new string[] {\\"~/Scripts/jquery.js\\"})\\n    ///\\n    /// --- From the main Master/View (just before last Body tag so all \\"registered\\" scripts are included) ---\\n    /// @Html.ClientScripts()\\n    /// </summary>\\n    public static class ScriptExtensions\\n    {\\n        #region Public\\n\\n        /// <summary>\\n        /// Render a Script element given a URL\\n        /// </summary>\\n        /// <param name=\\"helper\\"></param>\\n        /// <param name=\\"url\\">URL of script to render</param>\\n        /// <returns></returns>\\n        public static MvcHtmlString Script(\\n          this HtmlHelper helper,\\n          string url)\\n        {\\n            return MvcHtmlString.Create(MakeScriptTag(helper, url));\\n        }\\n\\n        /// <summary>\\n        /// Add client script files to list of script files that will eventually be added to the view\\n        /// </summary>\\n        /// <param name=\\"scriptPath\\">path to script file</param>\\n        /// <param name=\\"dependancies\\">OPTIONAL: a string array of any script dependancies</param>\\n        /// <returns>always MvcHtmlString.Empty</returns>\\n        public static MvcHtmlString AddClientScript(this HtmlHelper helper, string scriptPath, string[] dependancies = null)\\n        {\\n            //If script list does not already exist then initialise\\n            if (!helper.ViewContext.HttpContext.Items.Contains(\\"client-script-list\\"))\\n                helper.ViewContext.HttpContext.Items[\\"client-script-list\\"] = new Dictionary<string, KeyValuePair<string, string[]>>();\\n\\n            var scripts = helper.ViewContext.HttpContext.Items[\\"client-script-list\\"] as Dictionary<string, KeyValuePair<string, string[]>>;\\n\\n            //Ensure scripts are not added twice\\n            var scriptFilePath = helper.ViewContext.HttpContext.Server.MapPath(DetermineScriptToRender(helper, scriptPath));\\n            if (!scripts.ContainsKey(scriptFilePath))\\n                scripts.Add(scriptFilePath, new KeyValuePair<string, string[]>(scriptPath, dependancies));\\n\\n            return MvcHtmlString.Empty;\\n        }\\n\\n        /// <summary>\\n        /// Add a script tag for each \\"registered\\" script associated with the current view.\\n        /// Output script tags with order depending on script dependancies\\n        /// </summary>\\n        /// <returns>MvcHtmlString</returns>\\n        public static MvcHtmlString ClientScripts(this HtmlHelper helper)\\n        {\\n            var url = new UrlHelper(helper.ViewContext.RequestContext, helper.RouteCollection);\\n            var scripts = helper.ViewContext.HttpContext.Items[\\"client-script-list\\"] as Dictionary<string, KeyValuePair<string, string[]>> ?? new Dictionary<string, KeyValuePair<string, string[]>>();\\n\\n            //Build script tag block\\n            var scriptList = new List<string>();\\n            if (scripts.Count > 0)\\n            {\\n                //Check all script dependancies exist and throw an exception if any do not\\n                var distinctDependancies = scripts.Where(s => s.Value.Value != null)\\n                                                  .SelectMany(s => s.Value.Value)\\n                                                  .Distinct()\\n                                                  .Select(s => GetScriptFilePath(helper, s)) //Exception will be thrown here if file does not exist\\n                                                  .ToList();\\n\\n                var missingDependancies = distinctDependancies.Except(scripts.Select(s => s.Key)).ToList();\\n                if (missingDependancies.Count > 0)\\n                {\\n                    throw new KeyNotFoundException(\\"The following dependancies are missing: \\" + Environment.NewLine +\\n                        Environment.NewLine +\\n                        string.Join(Environment.NewLine, missingDependancies));\\n                }\\n\\n                //Serve scripts without dependancies first\\n                scriptList.AddRange(scripts.Where(s => s.Value.Value == null).Select(s => s.Value.Key));\\n\\n                //Get all scripts which have dependancies\\n                var scriptsAndDependancies = scripts.Where(s => s.Value.Value != null)\\n                                                    .OrderBy(s => s.Value.Value.Length)\\n                                                    .Select(s => s.Value)\\n                                                    .ToList();\\n\\n                //Loop round adding scripts to the scriptList until all are done\\n                do\\n                {\\n                    //Loop backwards through list so items can be removed mid loop\\n                    for (var i = scriptsAndDependancies.Count - 1; i >= 0; i--)\\n                    {\\n                        var script = scriptsAndDependancies[i].Key;\\n                        var dependancies = scriptsAndDependancies[i].Value;\\n\\n                        //Check if all the dependancies have been added to scriptList yet\\n                        bool currentScriptDependanciesAdded = !dependancies.Except(scriptList).Any();\\n                        if (currentScriptDependanciesAdded)\\n                        {\\n                            //Move script to scriptList\\n                            scriptList.Add(script);\\n                            scriptsAndDependancies.RemoveAt(i);\\n                        }\\n                    }\\n                } while (scriptsAndDependancies.Count > 0);\\n            }\\n\\n            //Generate a script tag for each script\\n            var scriptsToRender = scriptList.Select(s => MakeScriptTag(helper, s)).ToList();\\n#if DEBUG\\n            scriptsToRender.Insert(0, \\"\x3c!-- BEGIN - HtmlHelperScriptExtensions.ClientScripts() --\x3e\\");\\n            scriptsToRender.Insert(scriptsToRender.Count, \\"\x3c!-- END - HtmlHelperScriptExtensions.ClientScripts() --\x3e\\");\\n#endif\\n\\n            //Output script tag block at point in view where method is called (by returning an MvcHtmlString)\\n            return (scriptsToRender.Count > 0\\n                ? MvcHtmlString.Create(string.Join(Environment.NewLine, scriptsToRender))\\n                : MvcHtmlString.Empty);\\n        }\\n\\n        /// <summary>\\n        /// Add client script block to list of script blocks that will eventually be added to the view\\n        /// </summary>\\n        /// <param name=\\"key\\">unique identifier for script block</param>\\n        /// <param name=\\"scriptBlock\\">script block</param>\\n        /// <param name=\\"dependancies\\">OPTIONAL: a string array of any script block dependancies</param>\\n        /// <returns>always MvcHtmlString.Empty</returns>\\n        public static MvcHtmlString AddClientScriptBlock(this HtmlHelper helper, string key, string scriptBlock, string[] dependancies = null)\\n        {\\n            //If script list does not already exist then initialise\\n            if (!helper.ViewContext.HttpContext.Items.Contains(\\"client-script-block-list\\"))\\n                helper.ViewContext.HttpContext.Items[\\"client-script-block-list\\"] = new Dictionary<string, KeyValuePair<string, string[]>>();\\n\\n            var scriptBlocks = helper.ViewContext.HttpContext.Items[\\"client-script-block-list\\"] as Dictionary<string, KeyValuePair<string, string[]>>;\\n\\n            //Prevent duplication\\n            if (scriptBlocks.ContainsKey(key)) return MvcHtmlString.Empty;\\n\\n            scriptBlocks.Add(key, new KeyValuePair<string, string[]>(scriptBlock, dependancies));\\n\\n            return MvcHtmlString.Empty;\\n        }\\n\\n        /// <summary>\\n        /// Output all \\"registered\\" script blocks associated with the current view.\\n        /// Output script tags with order depending on script dependancies\\n        /// </summary>\\n        /// <returns>MvcHtmlString</returns>\\n        public static MvcHtmlString ClientScriptBlocks(this HtmlHelper helper)\\n        {\\n            var scriptBlocks = helper.ViewContext.HttpContext.Items[\\"client-script-block-list\\"] as Dictionary<string, KeyValuePair<string, string[]>> ?? new Dictionary<string, KeyValuePair<string, string[]>>();\\n\\n            //Build script tag block\\n            var scriptBlockList = new List<string>();\\n            if (scriptBlocks.Count > 0)\\n            {\\n                //Check all script dependancies exist and throw an exception if any do not\\n                var distinctDependancies = scriptBlocks.Where(s => s.Value.Value != null)\\n                                                       .SelectMany(s => s.Value.Value)\\n                                                       .Distinct()\\n                                                       .ToList();\\n\\n                var missingDependancies = distinctDependancies.Except(scriptBlocks.Select(s => s.Key)).ToList();\\n                if (missingDependancies.Count > 0)\\n                {\\n                    throw new KeyNotFoundException(\\"The following dependancies are missing: \\" + Environment.NewLine +\\n                        Environment.NewLine +\\n                        string.Join(Environment.NewLine, missingDependancies));\\n                }\\n\\n                //Serve script blocks without dependancies first\\n                scriptBlockList.AddRange(scriptBlocks.Where(s => s.Value.Value == null).Select(s => s.Value.Key));\\n\\n                //Get all script blocks which have dependancies\\n                var scriptBlocksAndDependancies = scriptBlocks.Where(s => s.Value.Value != null)\\n                                                              .OrderBy(s => s.Value.Value.Length)\\n                                                              .Select(s => s.Value)\\n                                                              .ToList();\\n\\n                //Loop round adding scripts to the scriptList until all are done\\n                do\\n                {\\n                    //Loop backwards through list so items can be removed mid loop\\n                    for (var i = scriptBlocksAndDependancies.Count - 1; i >= 0; i--)\\n                    {\\n                        var scriptBlock = scriptBlocksAndDependancies[i].Key;\\n                        var dependancies = scriptBlocksAndDependancies[i].Value;\\n\\n                        //Check if all the dependancies have been added to scriptList yet\\n                        bool currentScriptBlockDependanciesAdded = !dependancies.Except(scriptBlockList).Any();\\n                        if (currentScriptBlockDependanciesAdded)\\n                        {\\n                            //Move script to scriptList\\n                            scriptBlockList.Add(scriptBlock);\\n                            scriptBlocksAndDependancies.RemoveAt(i);\\n                        }\\n                    }\\n                } while (scriptBlocksAndDependancies.Count > 0);\\n            }\\n\\n            //Generate a script tag for each script\\n            var scriptBlocksToRender = scriptBlockList.Select(s => string.Format(\\"<script type=\\\\\\"text/javascript\\\\\\">{0}{1}<\/script>\\", Environment.NewLine, s)).ToList();\\n#if DEBUG\\n            scriptBlocksToRender.Insert(0, \\"\x3c!-- BEGIN - HtmlHelperScriptExtensions.ClientScriptBlocks() --\x3e\\");\\n            scriptBlocksToRender.Insert(scriptBlocksToRender.Count, \\"\x3c!-- END - HtmlHelperScriptExtensions.ClientScriptBlocks() --\x3e\\");\\n#endif\\n\\n            //Output script tag block at point in view where method is called (by returning an MvcHtmlString)\\n            return (scriptBlocksToRender.Count > 0\\n                ? MvcHtmlString.Create(string.Join(Environment.NewLine, scriptBlocksToRender))\\n                : MvcHtmlString.Empty);\\n        }\\n\\n        #endregion\\n\\n        #region Private\\n\\n        /// <summary>\\n        /// Take a URL, resolve it and a version suffix.  In Debug this will be based on DateTime.Now to prevent caching\\n        /// on a development machine.  In Production this will be based on the version number of the appplication.\\n        /// This means when the version number is incremented in subsequent releases script files should be recached automatically.\\n        /// </summary>\\n        /// <param name=\\"helper\\"></param>\\n        /// <param name=\\"url\\">URL to resolve and add suffix to</param>\\n        /// <returns></returns>\\n        private static string ResolveUrlWithVersion(HtmlHelper helper, string url)\\n        {\\n#if DEBUG\\n            var suffix = DateTime.Now.Ticks.ToString();\\n#else\\n            var suffix = System.Reflection.Assembly.GetExecutingAssembly().GetName().Version.ToString();\\n#endif\\n\\n            var urlWithVersionSuffix = string.Format(\\"{0}?v={1}\\", url, suffix);\\n            var urlResolved = new UrlHelper(helper.ViewContext.RequestContext, helper.RouteCollection).Content(urlWithVersionSuffix);\\n\\n            return urlResolved;\\n        }\\n\\n        /// <summary>\\n        /// Create the string that represents a script tag\\n        /// </summary>\\n        /// <param name=\\"helper\\"></param>\\n        /// <param name=\\"url\\"></param>\\n        /// <returns></returns>\\n        private static string MakeScriptTag(HtmlHelper helper, string url)\\n        {\\n            var scriptToRender = DetermineScriptToRender(helper, url);\\n\\n            //Render script tag\\n            var scriptTag = new TagBuilder(\\"script\\");\\n            scriptTag.Attributes[\\"type\\"] = \\"text/javascript\\"; //This isn\'t really required with HTML 5 as this is the default.  As it does no real harm so I have left it for now. http://stackoverflow.com/a/9659074/761388\\n            scriptTag.Attributes[\\"src\\"] = SharedExtensions.ResolveUrlWithVersion(helper, scriptToRender);\\n\\n            var scriptTagString = scriptTag.ToString();\\n            return scriptTagString;\\n        }\\n\\n        /// <summary>\\n        /// Author      : John Reilly\\n        /// Description : Get the script that should be served to the user - throw an exception if it doesn\'t exist and minify if in release mode\\n        /// </summary>\\n        /// <param name=\\"helper\\"></param>\\n        /// <param name=\\"url\\"></param>\\n        /// <param name=\\"minifiedSuffix\\">OPTIONAL - this allows you to directly specify the minified suffix if it differs from the standard\\n        /// of \\"min.js\\" - unlikely this will ever be used but possible</param>\\n        /// <returns></returns>\\n        private static string DetermineScriptToRender(HtmlHelper helper, string url, string minifiedSuffix = \\"min.js\\")\\n        {\\n            //Initialise a list that will contain potential scripts to render\\n            var possibleScriptsToRender = new List<string>() { url };\\n\\n#if DEBUG\\n            //Don\'t add minified scripts in debug mode\\n#else\\n            //Add minified path of script to list\\n            possibleScriptsToRender.Insert(0, Path.ChangeExtension(url, minifiedSuffix));\\n#endif\\n\\n            var validScriptsToRender = possibleScriptsToRender.Where(s => File.Exists(helper.ViewContext.HttpContext.Server.MapPath(s)));\\n            if (!validScriptsToRender.Any())\\n                throw new FileNotFoundException(\\"Unable to render \\" + url + \\" as none of the following scripts exist:\\" +\\n                    string.Join(Environment.NewLine, possibleScriptsToRender));\\n            else\\n                return validScriptsToRender.First(); //Return first existing file in list (minified file in release mode)\\n        }\\n\\n        #endregion\\n    }\\n}\\n```\\n\\n## Minification - I want to serve you less...\\n\\nAnother tweak I made to the script helper meant that when compiling either the debug or production (minified) versions of common JS files will be included if available. This means in a production environment the users get minified JS files so faster loading. And in a development environment we get the full JS files which make debugging more straightforward.\\n\\nWhat I haven\'t started doing is minifying my own JS files as yet. I know I\'m being somewhat inconsistent here by sometimes serving minified files and sometimes not. I\'m not proud. Part of my rationale for this that since most of my users use my apps on a daily basis they will for the most part be using cached JS files. Obviously there\'ll be slightly slower load times the first time they go to a page but nothing that significant I hope.\\n\\nI have thought of starting to do my own minification as a build step but have held off for now. Again this is something being baked into .NET 4.5; another reason why I have held off doing this a different way for now.\\n\\nUpdate\\n\\nIt now looks like this Microsofts optimisations have become [this Nuget package](http://nuget.org/packages/Microsoft.AspNet.Web.Optimization). It\'s early days (well it was released on 15th August 2012 and I\'m writing this on the 16th) but I think this looks not to be tied to MVC 4 or .NET 4.5 in which case I could use it in my current MVC 3 projects. I hope so...\\n\\nBy the way there\'s a [nice rundown of how to use this by K. Scott Allen of Pluralsight](http://www.pluralsight.com/training/Courses/TableOfContents/mvc4#mvc4-m3-optimization). It\'s fantastic. Recommended.\\n\\nUpdate 2\\n\\nHaving done a little asking around I now understand that this \\\\***can**\\\\* be used with MVC 3 / .NET 4.0. Excellent!\\n\\nOne rather nice alternative script serving mechanism I\'ve seen (but not yet used) is Andrew Davey\'s [Cassette](http://getcassette.net) which I mean to take for a test drive soon. This looks fantastic (and is available as a [Nuget package](http://nuget.org/packages/Cassette) \\\\- 10 points!).\\n\\n## CDNs (they want to serve you)\\n\\nI\'ve never professionally made use of CDNs at all. There are [clearly good reasons why you should](http://encosia.com/3-reasons-why-you-should-let-google-host-jquery-for-you/) but most of those good reasons relate most to public facing web apps.\\n\\nAs I\'ve said, the applications I tend to work on sit behind firewalls and it\'s not always guaranteed what my users can see from the grand old world of web beyond. (Indeed what they see can change on hour by hour basis sometimes...) Combined with that, because my apps are only accessible by a select few I don\'t face the pressure to reduce load on the server that public web apps can face.\\n\\nSo while CDN\'s are clearly a good thing. I don\'t use them at present. And that\'s unlikely to change in the short term.\\n\\n## TL:DR\\n\\n1. I don\'t use CDNs - they\'re clearly useful but they don\'t suit my particular needs\\n2. I serve each JavaScript file individually just before the body tag. I don\'t bundle.\\n3. I don\'t minify my own scripts (though clearly it wouldn\'t be hard) but I do serve the minified versions of 3rd party libraries (eg jQuery) in a Production environment.\\n4. I don\'t use async script loaders at present. I may in future; we shall see.\\n\\nI expect some of the above may change (well, possibly not point #1) but this general approach is working well for me at present.\\n\\nI haven\'t touched at all on how I\'m structuring my JavaScript code itself. Perhaps next time."},{"id":"reasons-to-be-cheerful-why-now-is-good","metadata":{"permalink":"/reasons-to-be-cheerful-why-now-is-good","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2012-06-04-reasons-to-be-cheerful-why-now-is-good/index.md","source":"@site/blog/2012-06-04-reasons-to-be-cheerful-why-now-is-good/index.md","title":"Reasons to be Cheerful (why now is a good time to be a dev)","description":"Easy access to information via Google and the rise of JavaScript, as well as blogs, screencasts, and podcasts have made learning coding easier.","date":"2012-06-04T00:00:00.000Z","tags":[],"readingTime":7.53,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"reasons-to-be-cheerful-why-now-is-good","title":"Reasons to be Cheerful (why now is a good time to be a dev)","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false,"description":"Easy access to information via Google and the rise of JavaScript, as well as blogs, screencasts, and podcasts have made learning coding easier."},"unlisted":false,"prevItem":{"title":"Optimally Serving Up JavaScript","permalink":"/how-im-structuring-my-javascript-in-web"},"nextItem":{"title":"Dad Didn\'t Buy Any Games","permalink":"/dad-didnt-buy-any-games"}},"content":"I\'ve been a working as a developer in some way, shape or form for just over 10 years now. And it occurred to me the other day that I can\'t think of a better time to be a software developer than <u>right now</u>\\n\\n\x3c!--truncate--\x3e\\n\\n. This year was better than last year. Last year was better than the year before. This is a happily recurring theme. So why? Well I guess there are a whole host of reasons; this is my effort to identify just some of them... ## Google and the World Wide Web (other search providers are available)\\n\\nWhen I first started out as a humble Delphi developer back in 1999 learning was not the straightforward proposition it is today. If you want to know how to do something these days a good place to start is firing up your browser and putting your question into Google. If I was to ask the question _\\"how do I use AJAX\\"_ of a search engine 10 years ago and now I would see very different things.\\n\\n![](AJAX-bleach.webp)\\n\\nOn the left the past, on the right the present. Do try not to let the presence of W3Schools in the search results detract... And also best ignore that the term AJAX wasn\'t coined until 2006... What I\'m getting at is that finding out information these days is can be done really quickly. Excellent search engines are now the norm. Back when I started out this was not the case and you were essentially reliant on what had been written down in books and the kindliness of more experienced developers. Google (and others like them) have done us a great service. They\'ve made it easier to learn. ## Blogs / Screencasts / Training websites\\n\\nSomething else that has made it easier to learn is the rise and rise of blogs, screencasts and training websites. Over the last 5 years the internet has been filling up with people writing blogs talking about tools, techniques and approaches they are using. When you\'re searching for advice on how to do something you can pretty much guarantee these days that some good soul will have written about it already. The most generous devs out there have gone a step further producing screencasts demonstrating them coding and sharing it with the world \\\\***for free**\\\\*. See an example from the ever awesome Rebecca Murphey below:\\n\\n<iframe src=\\"https://player.vimeo.com/video/20457625\\" width=\\"500\\" height=\\"281\\" frameBorder=\\"0\\" mozallowfullscreen=\\"\\" allowFullScreen=\\"\\"></iframe>\\n\\nSimilarly, there are now a number of commercially available screencasts which make it really easy to ramp up and learn. There\'s [TekPub](http://tekpub.com/), there\'s [Pluralsight](http://www.pluralsight-training.net) (who have massively improved my commute with their mobile app by the way). All of these help tug away the curtain away from the software development Wizard of Oz. All this is a very wonderful thing indeed! ## Podcasts\\n\\nIf you\'re a Boogie Down Productions fan then you may be aware of the concept of [Edutainment](<http://en.wikipedia.org/wiki/Edutainment_(album)>). That is to say, the bridge that can exist between entertainment and education. This is what I\'ve found podcasts to be. I listen to a lot. [Hanselminutes](http://www.hanselminutes.com/). [Herding Code](http://herdingcode.com/). [JavaScript Jabber](http://javascriptjabber.com/). [The JavaScript Show](http://javascriptshow.com/). [Yet Another Podcast](http://jesseliberty.com/podcast/). There\'s more. There\'s something wonderful about about listening to other developers who are passionate about what they are doing. Interested in their work. Enthusiastic about their projects. It\'s infectious. It makes you want to grab a keyboard and start trying things out. I can\'t imagine I\'m the only dev that feels this way. And of course I couldn\'t fail to mention my favourite podcast: [This Developer\'s Life](http://www.thisdeveloperslife.com/). Put together by Scott Hanselman and Rob Conery (I love these guys by the way), and inspired by [This American Life](http://www.thisamericanlife.org/), this show tells some of the stories experienced by developers. It gives an insight into what it\'s like to be a developer. This podcast is more entertaining than educational but it\'s absolutely \\\\***fantastic**\\\\*. ## JavaScript (and HTML and CSS too)\\n\\nAll of the above have eased the learning path of developers and made it easier to keep in touch with the latest and greatest happenings in the dev world. Along with this there has, in my opinion, also been something of a unifying of purpose in the developer community of late. I attribute this to JavaScript, HTML and CSS. Back when I started out it seemed much more the case that developers were split into different tribes. There was the Delphi tribe, the Visual Basic tribe, the C++ tribe, the Java tribe (very much the \\"hip young gunslingers\\" tribe back then - I guess these days it\'d be the Node.JS guys) as well as many others. And each tribe more or less seemed to keep themselves to themselves. This wasn\'t malicious that I could tell; that just seemed to be the way it was. But shortly after I started out the idea of the web application took off in a major way. I was involved in this coming from the position of being an early adopter of ASP.NET (which I used, and loved, since it was first in beta). Many other web application technologies were available; JSP, PHP, Perl and the like. But what they all had in common was this: they all pumped out HTML and CSS to the user. Suddenly all these developers from subtly different backgrounds were all targeting the same technology for their GUI. This unifying effect has been \\\\***massively**\\\\* reinforced by JavaScript. Whereas HTML is a markup language, JavaScript is a programming language. And more by accident than grand design JavaScript has kind of become the [VM of the web](http://www.hanselman.com/blog/JavaScriptIsAssemblyLanguageForTheWebPart2MadnessOrJustInsanity.aspx). Given the rise and rise of the rich web client (driven onwards and upwards by the popularity of AJAX, Backbone.JS etc) this has meant that devs of all creeds and colours have been forced to pitch a tent on the same patch of dirt. Pretty much all of the tribes now have an embassy in JavaScript land. So there are all these devs out there who are used to working with different server-side technologies from each other. But when it comes to the client, we are all sharing the common language of JavaScript. To a certain extent we\'re all creating data services that just pump out JSON to the client. Through forums like [StackOverflow](http://stackoverflow.com/) devs of all the tribes are helping each other with web client \\"stuff\\". They\'re all interacting in ways that they probably wouldn\'t otherwise if the web client environment was as diverse as the server-side environment... ## The Browser Wars Begin Again\\n\\nDidn\'t things seem a little dull around 2003/2004? IE 6 had come out 3 years previously and had vanquished all comers. Microsoft was really the only game in town browser-wise. Things had stopped changing; it seemed like browsers were \\"done\\". You know, good enough and there was no need to take things any further. Then came Firefox. This lone browser appeared as an alternative to might of IE. I must admit the thing that first attracted me to Firefox was the fact it had tabs. I mean technically I knew Firefox was more secure than IE but honestly it was the tabs that attracted me in the first place. (This may offer some insight as to why so many people still smoke...) And somehow Firefox managed to jolt Microsoft out of it\'s inertia on the web. Microsoft started caring about IE again. (Not enough until quite recently in my book but you\'ve got to start somewhere.) I\'m a firm believer that change for it\'s own sake can often be a good thing. Change makes you think about why you do what you do and wonder if there might be better approaches that could be used instead. And these changes kind of feed into... ## ...HTML 5!\\n\\nThat\'s right HTML 5 which is all about change. It\'s taking HTML as we know and love it and bolting on new stuff. New elements (canvas), new styling (CSS 3), new JavaScript APIs, faster JavaScript engines, support for JavaScript 5. The list goes on... And all this new stuff is exciting, whizzy, fun to play with. That which wasn\'t possible yesterday is possible now. Playing with new toys is half the fun of being a dev. There\'s a lot of new toys about right now. ## The Feeling of Possibilites\\n\\nThis is what it comes down to I think. It\'s so easy to learn these days and there\'s so much to learn about. Right now lots of things are happening above and beyond what I\'ve mentioned above. Open source has come of age and gone mainstream. Github is with us. Google are making contentious forays into new languages with Dart and Native Client. Microsoft aren\'t remotely evil empire-like these days; they\'ve made .NET like a Swiss army knife. You can even run Node.js on IIS these days! Signal-R, Websockets, Coffeescript, JS.Next, Backbone.JS, Entity Framework, LINQ, the mobile web, ASP.NET MVC, Razor, Knockout.JS, the cloud, Windows Azure... So much is happening right now. People are making things. It\'s a very interesting time to be a dev. There are many reasons to be cheerful."},{"id":"dad-didnt-buy-any-games","metadata":{"permalink":"/dad-didnt-buy-any-games","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2012-05-30-dad-didnt-buy-any-games/index.md","source":"@site/blog/2012-05-30-dad-didnt-buy-any-games/index.md","title":"Dad Didn\'t Buy Any Games","description":"Growing up in the 80s, John didnt have a computer until his father gave him one but with no games, encouraging him to learn programming.","date":"2012-05-30T00:00:00.000Z","tags":[],"readingTime":2.06,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"dad-didnt-buy-any-games","title":"Dad Didn\'t Buy Any Games","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false,"description":"Growing up in the 80s, John didnt have a computer until his father gave him one but with no games, encouraging him to learn programming."},"unlisted":false,"prevItem":{"title":"Reasons to be Cheerful (why now is a good time to be a dev)","permalink":"/reasons-to-be-cheerful-why-now-is-good"},"nextItem":{"title":"Globalize.js - number and date localisation made easy","permalink":"/globalizejs-number-and-date"}},"content":"Inspired by [Hanselmans post on how he got started in programming](http://www.hanselman.com/blog/SheLetMeTakeTheComputerHomeHowDidYouGetStartedInComputersAndProgramming.aspx) I thought I\'d shared my own tale about how it all began... I grew up the 80\'s just outside London. For those of you of a different vintage let me paint a picture. These were the days when \\"Personal Computers\\", as they were then styled, were taking the world by storm. Every house would be equipped with either a ZX Spectrum, a Commodore 64 or an Amstrad CPC. These were 8 bit computers which were generally plugged into the family television and spent a good portion of their time loading games like [Target: Renegade](http://en.wikipedia.org/wiki/Target:_Renegade) from an audio cassette. But not in our house; we didn\'t have a computer. I remember mournfully pedalling home from friends houses on a number of occasions, glum as I compared my lot with theirs. Whereas my friends would be spending their evenings gleefully battering their keyboards as they thrashed the life out of various end-of-level bosses I was reduced to \\\\***wasting**\\\\* my time reading. That\'s right Enid Blyton - you were second best in my head. Then one happy day (and it may have been a Christmas present although I\'m not certain) our family became the proud possessors of an [Amstrad CPC 6128](http://en.wikipedia.org/wiki/Amstrad_CPC):\\n\\n![](CPC6128.webp)\\n\\n\x3c!--truncate--\x3e\\n\\nGlory be! I was going to play so many games! I would have such larks! My evenings would be filled with pixelated keyboard related destruction! Hallelujah!! But I was wrong. I had reckoned without my father. For reasons that I\'ve never really got to the bottom of Dad had invested in the computer but not in the games. Whilst I was firmly of the opinion that these 2 went together like Lennon and McCartney he was having none of it. \\"You can write your own son\\" he intoned and handed over a manual which contained listings for games:\\n\\n![](6a0120a85dcdae970b0120a86ddeee970b.webp)\\n\\nAnd that\'s where it first began really. I would spend my evenings typing the Locomotive Basic listings for computer games into the family computer. Each time I started I would be filled with great hopes for what might result. Each time I tended to be rewarded with something that looked a bit like this:\\n\\n![](images.webp)\\n\\nI\'m not sure that it\'s possible to learn to program by osmosis but if it is I\'m definitely a viable test case. I didn\'t become an expert Locomotive Basic programmer (was there ever such a thing?) but I did undoubtedly begin my understanding of software.... Thanks Dad!"},{"id":"globalizejs-number-and-date","metadata":{"permalink":"/globalizejs-number-and-date","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2012-05-07-globalizejs-number-and-date/index.md","source":"@site/blog/2012-05-07-globalizejs-number-and-date/index.md","title":"Globalize.js - number and date localisation made easy","description":"Globalize.js is a JavaScript library allowing developers to format and parse numbers and dates in a culture specific fashion for better user experience.","date":"2012-05-07T00:00:00.000Z","tags":[{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."},{"inline":false,"label":"Globalize","permalink":"/tags/globalize","description":"The Globalize library."}],"readingTime":7.52,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"globalizejs-number-and-date","title":"Globalize.js - number and date localisation made easy","authors":"johnnyreilly","tags":["javascript","globalize"],"hide_table_of_contents":false,"description":"Globalize.js is a JavaScript library allowing developers to format and parse numbers and dates in a culture specific fashion for better user experience."},"unlisted":false,"prevItem":{"title":"Dad Didn\'t Buy Any Games","permalink":"/dad-didnt-buy-any-games"},"nextItem":{"title":"Beg, Steal or Borrow a Decent JavaScript DateTime Converter","permalink":"/beg-steal-or-borrow-decent-javascript"}},"content":"I wanted to write about a JavaScript library which seems to have had very little attention so far. And that surprises me as it\'s\\n\\n\x3c!--truncate--\x3e\\n\\n1. Brilliant!\\n2. Solves a common problem that faces many app developers who work in the wonderful world of web; myself included\\n\\nThe library is called Globalize.js and can be found on [GitHub here](https://github.com/jquery/globalize). Globalize.js is a simple JavaScript library that allows you to format and parse numbers and dates in culture specific fashion.\\n\\n## Why does this matter?\\n\\nBecause different countries and cultures do dates and numbers in different ways. Christmas Day this year in England will be `25/12/2012` (dd/MM/yyyy). But for American eyes this should be `12/25/2012` (M/d/yyyy). And for German `25.12.2012` (dd.MM.yyyy). Likewise, if I was to express numerically the value of \\"one thousand exactly - to 2 decimal places\\", as a UK citizen I would do it like so: `1,000.00`. But if I was French I\'d express it like this: `1.000,00`. You see my point?\\n\\n## Why does this matter to me?\\n\\nFor a number of years I\'ve been working on applications that are used globally, from London to Frankfurt to Shanghai to New York to Singapore and many other locations besides. The requirement has always been to serve up localised dates and numbers so users experience of the system is more natural. Since our applications are all ASP.NET we\'ve never really had a problem server-side. Microsoft have blessed us with all the goodness of [System.Globalization](http://msdn.microsoft.com/en-us/library/system.globalization.aspx) which covers hundreds of different cultures and localisations. It makes it frankly easy:\\n\\n```cs\\nusing System.Globalization;\\n\\n//Produces: \\"06.05.2012\\"\\nnew DateTime(2012,5,6).ToString(\\"d\\", new CultureInfo(\\"de-DE\\"));\\n\\n//Produces: \\"45,56\\"\\n45.56M.ToString(\\"n\\", new CultureInfo(\\"fr-FR\\"));\\n```\\n\\nThe problem has always been client-side. If you need to localise dates and numbers on the client what do you do?\\n\\n## JavaScript Date / Number Localisation - the Status Quo\\n\\nWell to be frank - it\'s a bit rubbish really. What\'s on offer natively at present basically amounts to this:\\n\\n- [Date.toLocaleDateString](https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/Date/toLocaleDateString)\\n- [Number.ToLocaleString](https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/Number/ToLocaleString)\\n\\nThis is better than nothing - but not by much. There\'s no real control or flexibility here. If you don\'t like the native localisation format or you want something slightly different then tough. This is all you\'ve got to play with.\\n\\nFor the longest time this didn\'t matter too much. Up until relatively recently the world of web was far more about the thin client and the fat server. It would be quite standard to have all HTML generated on the server. And, as we\'ve seen .NET (and many other back end enviroments as well) give you all the flexiblility you might desire given this approach.\\n\\n[But the times they are a-changing](http://www.youtube.com/watch?v=k2sYIIjS-cQ). And given the ongoing explosion of HTML 5 the rich client is very definitely with us. So we need tools.\\n\\n## Microsoft doing \\\\*good things\\\\*\\n\\nHands up who remembers when Microsoft first shipped it\'s [ASP.NET AJAX](http://msdn.microsoft.com/en-us/magazine/cc163300.aspx) library back in 2007?\\n\\nWell a small part of this was the extensions ASP.NET AJAX added to JavaScripts native Date and Number objects.... These extensions allowed the localisation of Dates and Numbers to the current UI culture and the subsequent string parsing of these back into Dates / Numbers. These extensions pretty much gave JavaScript the functionality that the server already had in `System.Globalization`. (not quite like-for-like but near enough the mark)\\n\\nI\'m not aware of a great fuss ever being made about this - a fact I find surprising since one would imagine this is a common need. There\'s good documentation about this on MSDN - here\'s some useful links:\\n\\n- [Ajax Script Globalization and Localization](http://msdn.microsoft.com/en-us/library/bb386572.aspx)\\n- [Walkthrough: Globalizing a Date by Using Client Script](http://msdn.microsoft.com/en-us/library/bb386581.aspx)\\n- [JavaScript Base Type Extensions](http://msdn.microsoft.com/en-us/library/bb397506.aspx)\\n- [Date.parseLocale](http://msdn.microsoft.com/en-us/library/bb397521.aspx)\\n- [Date.localeFormat](http://msdn.microsoft.com/en-us/library/bb383816.aspx)\\n- [Number.localeFormat](http://msdn.microsoft.com/en-us/library/bb310813.aspx)\\n- [Number.parseLocale](http://msdn.microsoft.com/en-us/library/bb310985.aspx)\\n\\nWhen our team became aware of this we started to make use of it in our web applications. I imagine we weren\'t alone...\\n\\n## Microsoft doing \\\\*even better things\\\\* (Scott Gu to the rescue!)\\n\\nI started to think about this again when MVC reared it\'s lovely head.\\n\\nLike many, I found I preferred the separation of concerns / testability etc that MVC allowed. As such, our team was planning to, over time, migrate our ASP.NET WebForms applications over to MVC. However, before we could even begin to do this we had a problem. Our JavaScript localisation was dependant on the ScriptManager. The [ScriptManager](http://msdn.microsoft.com/en-us/library/system.web.ui.scriptmanager.aspx) is very much a WebForms construct.\\n\\nWhat to do? To the users it wouldn\'t be acceptable to remove the localisation functionality from the web apps. The architecture of an application is, to a certain extent, meaningless from the users perspective - they\'re only interested in what directly impacts them. That makes sense, even if it was a problem for us.\\n\\nFortunately the Great Gu had it in hand. Lo and behold the [this post](http://forum.jquery.com/topic/proposal-for-a-globalization-plugin-jquery-glob-js) appeared on the jQuery forum and the following post appeared on Guthrie\'s blog:\\n\\n[http://weblogs.asp.net/scottgu/archive/2010/06/10/jquery-globalization-plugin-from-microsoft.aspx](http://weblogs.asp.net/scottgu/archive/2010/06/10/jquery-globalization-plugin-from-microsoft.aspx)\\n\\nYes that\'s right. Microsoft were giving back to the jQuery community by contributing a jQuery globalisation plug-in. They\'d basically taken the work done with ASP.NET AJAX Date / Number extensions, jQuery-plug-in-ified it and put it out there. Fantastic!\\n\\nUsing this we could localise / globalise dates and numbers whether we were working in WebForms or in MVC. Or anything else for that matter. If we were suddenly seized with a desire to re-write our apps in PHP we\'d \\\\***still**\\\\* be able to use Globalize.js on the client to handle our regionalisation of dates and numbers.\\n\\n## History takes a funny course...\\n\\nNow for my part I would have expected that this announcement to be followed in short order by dancing in the streets and widespread adoption. Surprisingly, not so. All went quiet on the globalisation front for some time and then out of the blue the following comment appeared on the jQuery forum by [Richard D. Worth](http://rdworth.org/blog/) (he of jQuery UI fame):\\n\\n[http://blog.jquery.com/2011/04/16/official-plugins-a-change-in-the-roadmap/#comment-527484](http://blog.jquery.com/2011/04/16/official-plugins-a-change-in-the-roadmap/#comment-527484)\\n\\nThe long and short of which was:\\n\\n- The jQuery UI team were now taking care of (the re-named) Globalize.js library as the grid control they were developing had a need for some of Globalize.js\'s goodness. Consequently a home for Globalize.js appeared on the jQuery UI website: [http://wiki.jqueryui.com/Globalize](http://wiki.jqueryui.com/Globalize)\\n- The source of Globalize.js moved to this location on GitHub: [https://github.com/jquery/globalize/](https://github.com/jquery/globalize/)\\n- Perhaps most significantly, the jQuery globalisation plug-in as developed by Microsoft had now been made a standalone JavaScript library. This was clearly brilliant news for Node.js developers as they would now be able to take advantage of this and perform localisation / globalisation server-side - they wouldn\'t need to have jQuery along for the ride. Also, this would be presumably be good news for users of other client side JavaScript libraries like Dojo / YUI etc.\\n\\nGlobalize.js clearly has a rosy future in front of it. Using the new Globalize.js library was still simplicity itself. Here\'s some examples of localising dates / numbers using the German culture:\\n\\n```js\\n<script\\n  src=\\"/Scripts/Globalize/globalize.js\\"\\n  type=\\"text/javascript\\"><\/script>\\n<script\\n  src=\\"/Scripts/Globalize/cultures/globalize.culture.de-DE.js\\"\\n  type=\\"text/javascript\\"><\/script>\\n\\nGlobalize.culture(\\"de-DE\\");\\n\\n//\\"2012-05-06\\" - ISO 8601 format\\nGlobalize.format(new Date(2012,4,6), \\"yyyy-MM-dd\\");\\n\\n//\\"06.05.2012\\" - standard German short date format of dd.MM.yyyy\\nGlobalize.format(new Date(2012,4,6), Globalize.culture().calendar.patterns.d);\\n\\n//\\"4.576,3\\" - a number rendered to 1 decimal place\\nGlobalize.format(4576.34, \\"n1\\");\\n```\\n\\n## Stick a fork in it - it\'s done\\n\\nThe entry for Globalize.js on the jQuery UI site reads as follows:\\n\\n> _\\"version: 0.1.0a1 (not a jQuery UI version number, as this is a standalone utility) status: in development (part of Grid project)\\"_\\n\\nI held back from making use of the library for some time, deterred by the \\"in development\\" status. However, I had a bit of dialog with one of the jQuery UI team (I forget exactly who) who advised that the API was unlikely to change further and that the codebase was actually pretty stable. Our team did some testing of Globalize.js and found this very much to be case. Everything worked just as we expected and hoped. We\'re now using Globalize.js in a production environment with no problems reported; it\'s been doing a grand job.\\n\\nIn my opinion, Number / Date localisation on the client is ready for primetime right now - it works! Unfortunately, because Globalize.js has been officially linked in with the jQuery UI grid project it seems unlikely that this will officially ship until the grid does. Looking at the jQuery UI [roadmap](http://wiki.jqueryui.com/Roadmap) the grid is currently slated to release with jQuery UI 2.1. There isn\'t yet a release date for jQuery UI 1.9 and so it could be a long time before the grid actually sees the light of day.\\n\\nI\'m hoping that the jQuery UI team will be persuaded to \\"officially\\" release Globalize.js long before the grid actually ships. Obviously people can use Globalize.js as is right now (as we are) but it seems a shame that many others will be missing out on using this excellent functionality, deterred by the \\"in development\\" status. Either way, [the campaign to release Globalise.js officially starts here!](http://www.youtube.com/watch?v=qEMytPF8YuY)\\n\\n## The Future?\\n\\nThere are plans to bake globalisation right into JavaScript natively with EcmaScript 5.1. There\'s a good post on the topic [here](http://generatedcontent.org/post/59403168016/esintlapi). And here\'s a couple of historical links worth reading too:\\n\\n[http://norbertlindenberg.com/2012/02/ecmascript-internationalization-api/](http://norbertlindenberg.com/2012/02/ecmascript-internationalization-api/)[http://wiki.ecmascript.org/doku.php?id=globalization:specification_drafts](http://wiki.ecmascript.org/doku.php?id=globalization:specification_drafts)"},{"id":"beg-steal-or-borrow-decent-javascript","metadata":{"permalink":"/beg-steal-or-borrow-decent-javascript","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2012-04-28-beg-steal-or-borrow-decent-javascript/index.md","source":"@site/blog/2012-04-28-beg-steal-or-borrow-decent-javascript/index.md","title":"Beg, Steal or Borrow a Decent JavaScript DateTime Converter","description":"ASP.NETs JavaScriptSerializer class is improved through ISO 8601 and a custom converter for better DateTime serialisation.","date":"2012-04-28T00:00:00.000Z","tags":[{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."},{"inline":false,"label":"ASP.NET","permalink":"/tags/asp-net","description":"The web framework built by Microsoft."}],"readingTime":9.47,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"beg-steal-or-borrow-decent-javascript","title":"Beg, Steal or Borrow a Decent JavaScript DateTime Converter","authors":"johnnyreilly","tags":["javascript","asp.net"],"hide_table_of_contents":false,"description":"ASP.NETs JavaScriptSerializer class is improved through ISO 8601 and a custom converter for better DateTime serialisation."},"unlisted":false,"prevItem":{"title":"Globalize.js - number and date localisation made easy","permalink":"/globalizejs-number-and-date"},"nextItem":{"title":"JSHint - Customising your hurt feelings","permalink":"/jshint-customising-your-hurt-feelings"}},"content":"I\'ve so named this blog post because it shamelessly borrows from the fine work of others: Sebastian Markb\xe5ge and Nathan Vonnahme. Sebastian wrote a blog post documenting a good solution to the ASP.NET JavaScriptSerializer DateTime problem at the tail end of last year. However, his solution didn\'t get me 100% of the way there when I tried to use it because of a need to support IE 8 which lead me to use Nathan Vonnahme\'s ISO 8601 JavaScript Date parser. I thought it was worth documenting this, hence this post, but just so I\'m clear; the hard work here was done by Sebastian Markb\xe5ge and Nathan Vonnahme and not me. Consider me just a curator in this case. The original blog posts that I am drawing upon can be found here: 1. [http://blog.calyptus.eu/seb/2011/12/custom-datetime-json-serialization/](http://blog.calyptus.eu/seb/2011/12/custom-datetime-json-serialization/) and here: 2. [http://n8v.enteuxis.org/2010/12/parsing-iso-8601-dates-in-javascript/](http://n8v.enteuxis.org/2010/12/parsing-iso-8601-dates-in-javascript/)\\n\\n\x3c!--truncate--\x3e\\n\\n## DateTime, JSON, JavaScript Dates....\\n\\nLike many, I\'ve long been frustrated with the quirky DateTime serialisation employed by the `System.Web.Script.Serialization.JavaScriptSerializer` class. When serialising DateTimes so they can be JSON.parsed on the client, this serialiser uses the following approach: (from MSDN) [_Date object, represented in JSON as \\"\\\\/Date(number of ticks)\\\\/\\". The number of ticks is a positive or negative long value that indicates the number of ticks (milliseconds) that have elapsed since midnight 01 January, 1970 UTC.\\"_](http://msdn.microsoft.com/en-us/library/system.web.script.serialization.javascriptserializer.aspx) Now this is not particularly helpful in my opinion because it\'s not human readable (at least not this human; perhaps [Jon Skeet](http://stackoverflow.com/users/22656/jon-skeet)...) When consuming your data from web services / PageMethods using [jQuery.ajax](http://api.jquery.com/jQuery.ajax/) you are landed with the extra task of having to convert what were DateTimes on the server from Microsofts string Date format (eg `\\"\\\\/Date(1293840000000)\\\\/\\"`) into actual JavaScript Dates. It\'s also unhelpful because it\'s divergent from the approach to DateTime / Date serialisation used by a native JSON serialisers:\\n\\n![](FireBug-Dates.webp)\\n\\nJust as an aside it\'s worth emphasising that one of the limitations of JSON is that the JSON.parsing of a JSON.stringified date will \\\\***not**\\\\* return you to a JavaScript Date but rather an ISO 8601 date string which will need to be subsequently converted into a Date. Not JSON\'s fault - essentially down to the absence of a Date literal within JavaScript. ## Making JavaScriptSerializer behave more JSON\'y\\n\\nAnyway, I didn\'t think there was anything I could really do about this in an ASP.NET classic / WebForms world because, to my knowledge, it is not possible to swap out the serialiser that is used. JavaScriptSerializer is the only game in town. (Though I am optimistic about the future; given the announcement that I first picked up on Rick Strahl\'s blog that [Json.NET was going to be adopted as the default JSON serializer for ASP.NET Web API](http://www.west-wind.com/weblog/posts/2012/Mar/09/Using-an-alternate-JSON-Serializer-in-ASPNET-Web-API); what with Json.NET having out-of-the-box [ISO 8601 support](http://james.newtonking.com/archive/2009/02/20/good-date-times-with-json-net.aspx). I digress...) Because it can make debugging a much more straightforward process I place a lot of value on being able to read the network traffic that web apps generate. It\'s much easier to drop into Fiddler / FireBug / Chrome dev tools etc and watch what\'s happening there and then instead of having to manually process the data separately first so that you can understand it. I think this is nicely aligned with the [KISS principle](http://en.wikipedia.org/wiki/KISS_principle). For that reason I\'ve been generally converting DateTimes to ISO 8601 strings on the server before returning them to the client. A bit of extra overhead but generally worth it for the gains in clarity in my opinion. So I was surprised and delighted when I happened upon [Sebastian Markb\xe5ge\'s blog post](http://blog.calyptus.eu/seb/2011/12/custom-datetime-json-serialization/) which provided a DateTime JavaScriptConverter that could be plugged into the JavaScriptSerializer. You can see the code below (or on Sebastian\'s original post with a good explanation of how it works):\\n\\n```cs\\nusing System;\\nusing System.Collections;\\nusing System.Collections.Generic;\\nusing System.Web.Script.Serialization;\\n\\nnamespace MyNamespace\\n{\\n  /// <summary>\\n  /// A custom DateTime JavaScriptConverter courtesy of these good folks: http://blog.calyptus.eu/seb/2011/12/custom-datetime-json-serialization/\\n  /// Using this forces DataTimes to be serialised as ISO 8601 rather \\"\\\\/Date(1249335477787)\\\\/\\" style\\n  /// </summary>\\n  public class DateTimeJavaScriptConverter : JavaScriptConverter\\n  {\\n    public override object Deserialize(IDictionary<string, object> dictionary, Type type, JavaScriptSerializer serializer)\\n    {\\n      return new JavaScriptSerializer().ConvertToType(dictionary, type);\\n    }\\n\\n    public override IDictionary<string, object> Serialize(object obj, JavaScriptSerializer serializer)\\n    {\\n      if (!(obj is DateTime)) return null;\\n      return new CustomString(((DateTime)obj).ToUniversalTime().ToString(\\"O\\"));\\n    }\\n\\n    public override IEnumerable<Type> SupportedTypes\\n    {\\n      get { return new[] { typeof(DateTime) }; }\\n    }\\n\\n    private class CustomString : Uri, IDictionary<string, object>\\n    {\\n      public CustomString(string str)\\n        : base(str, UriKind.Relative)\\n      {\\n      }\\n\\n      void IDictionary<string, object>.Add(string key, object value) { throw new NotImplementedException(); }\\n      bool IDictionary<string, object>.ContainsKey(string key) { throw new NotImplementedException(); }\\n      ICollection<string> IDictionary<string, object>.Keys { get { throw new NotImplementedException(); } }\\n      bool IDictionary<string, object>.Remove(string key) { throw new NotImplementedException(); }\\n      bool IDictionary<string, object>.TryGetValue(string key, out object value) { throw new NotImplementedException(); }\\n      ICollection<object> IDictionary<string, object>.Values { get { throw new NotImplementedException(); } }\\n      object IDictionary<string, object>.this[string key]\\n      {\\n        get { throw new NotImplementedException(); }\\n        set { throw new NotImplementedException(); }\\n      }\\n      void ICollection<KeyValuePair<string, object>>.Add(KeyValuePair<string, object> item) { throw new NotImplementedException(); }\\n      void ICollection<KeyValuePair<string, object>>.Clear() { throw new NotImplementedException(); }\\n      bool ICollection<KeyValuePair<string, object>>.Contains(KeyValuePair<string, object> item) { throw new NotImplementedException(); }\\n      void ICollection<KeyValuePair<string, object>>.CopyTo(KeyValuePair<string, object>[] array, int arrayIndex) { throw new NotImplementedException(); }\\n      int ICollection<KeyValuePair<string, object>>.Count { get { throw new NotImplementedException(); } }\\n      bool ICollection<KeyValuePair<string, object>>.IsReadOnly { get { throw new NotImplementedException(); } }\\n      bool ICollection<KeyValuePair<string, object>>.Remove(KeyValuePair<string, object> item) { throw new NotImplementedException(); }\\n      IEnumerator<KeyValuePair<string, object>> IEnumerable<KeyValuePair<string, object>>.GetEnumerator() { throw new NotImplementedException(); }\\n      IEnumerator IEnumerable.GetEnumerator() { throw new NotImplementedException(); }\\n    }\\n  }\\n}\\n```\\n\\nUsing this converter meant that a DateTime that previously would have been serialised as `\\"\\\\/Date(1293840000000)\\\\/\\"` would now be serialised as `\\"2011-01-01T00:00:00.0000000Z\\"` instead. This is entirely agreeable because 1. it\'s entirely clear what a `\\"2011-01-01T00:00:00.0000000Z\\"` style date represents and 2. this is more in line with native browser JSON implementations and `&lt;statingTheObvious&gt;`consistency is a good thing.`&lt;/statingTheObvious&gt;`\\n\\n## Getting your web services to use the ISO 8601 DateTime Converter\\n\\nSebastian alluded in his post to a `web.config` setting that could be used to get web services / pagemethods etc. implementing his custom DateTime serialiser. This is it:\\n\\n```xml\\n<configuration>\\n  <system.web.extensions>\\n    <scripting>\\n      <webServices>\\n        \x3c!--\\n          This line of config means that when a JavaScriptSerializer is used by a web service / page method\\n          it will automatically register the DateTimeJavaScriptConverter to use.  To use the converter directly in code you would need to enter the below:\\n\\n          var serializer = new System.Web.Script.Serialization.JavaScriptSerializer();\\n          serializer.RegisterConverters(new JavaScriptConverter[] { new DateTimeJavaScriptConverter() });\\n\\n        --\x3e\\n        <jsonSerialization>\\n          <converters>\\n            <add name=\\"DateTimeJavaScriptConverter\\" type=\\"MyNamespace.DateTimeJavaScriptConverter\\"/>\\n          </converters>\\n        </jsonSerialization>\\n\\n      </webServices>\\n      <scriptResourceHandler enableCompression=\\"false\\" enableCaching=\\"true\\"/>\\n    </scripting>\\n  </system.web.extensions>\\n</configuration>\\n```\\n\\nWith this in place your web services / page methods will happily be able to serialise / deserialise ISO style date strings to your hearts content. ## What no ISO 8601 date string Date constructor?\\n\\nAs I mentioned earlier, Sebastian\'s solution didn\'t get me 100% of the way there. There was still a fly in the ointment in the form of IE 8. Unfortunately IE 8 doesn\'t have JavaScript [Date constructor that takes ISO 8601 date strings](https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/Date/parse). This lead me to using Nathan Vonnahme\'s ISO 8601 JavaScript Date parser, the code of which is below (or see his original post [here](http://n8v.enteuxis.org/2010/12/parsing-iso-8601-dates-in-javascript/)):\\n\\n```js\\n//===============================================================================\\n// Parse ISO 8601 Date Format date string and return a date - equivalent to https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/Date/parse\\n// Found here: n8v.enteuxis.org/2010/12/parsing-iso-8601-dates-in-javascript/\\n//===============================================================================\\nfunction parseISO8601Date(s) {\\n  // parenthese matches:\\n  // year month day    hours minutes seconds\\n  // dotmilliseconds\\n  // tzstring plusminus hours minutes\\n  var re =\\n    /(\\\\d{4})-(\\\\d\\\\d)-(\\\\d\\\\d)T(\\\\d\\\\d):(\\\\d\\\\d):(\\\\d\\\\d)(\\\\.\\\\d+)?(Z|([+-])(\\\\d\\\\d):(\\\\d\\\\d))/;\\n\\n  var d = [];\\n  d = s.match(re);\\n\\n  // \\"2010-12-07T11:00:00.000-09:00\\" parses to:\\n  //  [\\"2010-12-07T11:00:00.000-09:00\\", \\"2010\\", \\"12\\", \\"07\\", \\"11\\",\\n  //     \\"00\\", \\"00\\", \\".000\\", \\"-09:00\\", \\"-\\", \\"09\\", \\"00\\"]\\n  // \\"2010-12-07T11:00:00.000Z\\" parses to:\\n  //  [\\"2010-12-07T11:00:00.000Z\\",      \\"2010\\", \\"12\\", \\"07\\", \\"11\\",\\n  //     \\"00\\", \\"00\\", \\".000\\", \\"Z\\", undefined, undefined, undefined]\\n\\n  if (!d) {\\n    throw \\"Couldn\'t parse ISO 8601 date string \'\\" + s + \\"\'\\";\\n  }\\n\\n  // parse strings, leading zeros into proper ints\\n  var a = [1, 2, 3, 4, 5, 6, 10, 11];\\n  for (var i in a) {\\n    d[a[i]] = parseInt(d[a[i]], 10);\\n  }\\n  d[7] = parseFloat(d[7]);\\n\\n  // Date.UTC(year, month[, date[, hrs[, min[, sec[, ms]]]]])\\n  // note that month is 0-11, not 1-12\\n  // see https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/Date/UTC\\n  var ms = Date.UTC(d[1], d[2] - 1, d[3], d[4], d[5], d[6]);\\n\\n  // if there are milliseconds, add them\\n  if (d[7] > 0) {\\n    ms += Math.round(d[7] * 1000);\\n  }\\n\\n  // if there\'s a timezone, calculate it\\n  if (d[8] != \'Z\' && d[10]) {\\n    var offset = d[10] * 60 * 60 * 1000;\\n    if (d[11]) {\\n      offset += d[11] * 60 * 1000;\\n    }\\n    if (d[9] == \'-\') {\\n      ms -= offset;\\n    } else {\\n      ms += offset;\\n    }\\n  }\\n\\n  return new Date(ms);\\n}\\n```\\n\\nWith this in place I could parse ISO 8601 Dates just like anyone else. Great stuff. `parseISO8601Date(\\"2011-01-01T00:00:00.0000000Z\\")` would give me a JavaScript Date of `Sat Jan 1 00:00:00 UTC 2011`. Obviously in the fullness of time the parseISO8601Date solution should no longer be necessary because [EcmaScript 5 specifies an ISO 8601 date string constructor](http://es5.github.com/#x15.9.3.2). However, in the interim Nathan\'s solution is a lifesaver. Thanks again both to Sebastian Markb\xe5ge and Nathan Vonnahme who have both generously allowed me use their work as the basis for this post. ## PS And it would have worked if it wasn\'t for that pesky IE 9...\\n\\nSubsequent to writing this post I thought I\'d check that IE 9 had implemented a JavaScript Date constructor that would process an ISO 8601 date string like this: `new Date(\\"2011-01-01T00:00:00.0000000Z\\")`. It hasn\'t. Take a look:\\n\\n![](IE9-screenshot.webp)\\n\\nThis is slightly galling as the above code works dandy in Firefox and Chrome. As you can see from the screenshot you can get the JavaScript IE 9 Date constructor to play nice by trimming off the final 4 \\"0\\"\'s from the string. Frustrating. Obviously we can still use Nathan\'s solution but it\'s a shame that we can\'t use the native support. Based on what I\'ve read [here](http://msdn.microsoft.com/en-us/library/az4se3k1.aspx#Roundtrip) I think it would be possible to amend Sebastians serializer to fall in line with IE 9\'s pendantry by changing this:\\n\\n```cs\\nreturn new CustomString(((DateTime)obj).ToUniversalTime()\\n  .ToString(<b>\\"O\\"</b>)\\n);\\n```\\n\\nTo this:\\n\\n```cs\\nreturn new CustomString(((DateTime)obj).ToUniversalTime()\\n  .ToString(<b>\\"yyyy\'-\'MM\'-\'dd\'T\'HH\':\'mm\':\'ss\'.\'fffzzz\\"</b>)\\n);\\n```\\n\\nI\'ve held off from doing this myself as I rather like Sebastian\'s idea of being able to use Microsoft\'s Round-trip (\\"O\\", \\"o\\") Format Specifier. And it seems perverse that we should have to move away from using Microsoft\'s Round-trip Format Specifier purely because of (Microsoft\'s) IE! But it\'s a possibility to consider and so I put it out there. I would hope that MS will improve their JavaScript Date constructor with IE 10. A missed opportunity if they don\'t I think. ## PPS Just when you thought is over... IE 9 was right!\\n\\nSebastian got in contact after I first published this post and generously pointed out that, contrary to my expectation, IE 9 technically had the correct implementation. According to the [EMCAScript standard](http://es5.github.com/#x15.9.1.15) the Date constructor should not allow more than millisecond precision. In this case, Chrome and Firefox are being less strict - not more correct. On reflection this does rather make sense as the result of a `JSON.stringify(new Date())` never results in an ISO date string to the 10 millionths of a second detail. Sebastian has himself stopped using Microsoft\'s Round-trip (\\"O\\", \\"o\\") Format Specifier in favour of this format string: ```cs\\nreturn new CustomString(((DateTime)obj).ToUniversalTime()\\n\\n.ToString(<b>\\"yyyy-MM-ddTHH:mm:ss.fffZ\\"</b>)\\n\\n);\\n\\n```\\n\\n This results in date strings that comply perfectly with the ECMAScript spec. I suspect I\'ll switch to using this also now. Though I\'ll probably leave the first part of the post intact as I think the background remains interesting. Thanks again Sebastian!\\n```"},{"id":"jshint-customising-your-hurt-feelings","metadata":{"permalink":"/jshint-customising-your-hurt-feelings","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2012-04-23-jshint-customising-your-hurt-feelings/index.md","source":"@site/blog/2012-04-23-jshint-customising-your-hurt-feelings/index.md","title":"JSHint - Customising your hurt feelings","description":"JSHint is a tool for evaluating JavaScript code quality. Its configurable, has an extension for Visual Studio and is better than JSLint.","date":"2012-04-23T00:00:00.000Z","tags":[{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."}],"readingTime":4.505,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"jshint-customising-your-hurt-feelings","title":"JSHint - Customising your hurt feelings","authors":"johnnyreilly","tags":["javascript"],"hide_table_of_contents":false,"description":"JSHint is a tool for evaluating JavaScript code quality. Its configurable, has an extension for Visual Studio and is better than JSLint."},"unlisted":false,"prevItem":{"title":"Beg, Steal or Borrow a Decent JavaScript DateTime Converter","permalink":"/beg-steal-or-borrow-decent-javascript"},"nextItem":{"title":"A Simple Technique for Initialising Properties with Internal Setters for Unit Testing","permalink":"/simple-technique-for-initialising"}},"content":"As I\'ve started making greater use of JavaScript to give a richer GUI experience the amount of JS in my ASP.NET apps has unsurprisingly ballooned. If I\'m honest, I hadn\'t given much consideration to the code quality of my JavaScript in the past. However, if I was going to make increasing use of it (and given the way the web is going at the moment I\'d say that\'s a given) I didn\'t think this was tenable position to maintain. A friend of mine works for [Coverity](http://www.coverity.com/) which is a company that provides tools for analysing code quality. I understand, from conversations with him, that their tools provide static analysis for compiled languages such as C++ / C# / Java etc. I was looking for something similar for JavaScript. Like many, I have read and loved [Douglas Crockford\'s \\"JavaScript: The Good Parts\\"](http://www.amazon.com/JavaScript-Good-Parts-Douglas-Crockford/dp/0596517742); it is by some margin the most useful and interesting software related book I have read.So I was aware that Crockford had come up with his own JavaScript code quality tool called [JSLint](http://www.jslint.com/). JSLint is quite striking when you first encounter it:\\n\\n![](JSLint.webp)\\n\\n\x3c!--truncate--\x3e\\n\\nIt\'s the \\"Warning! JSLint will hurt your feelings.\\" that grabs you. And it\'s not wrong. I\'ve copied and pasted code that I\'ve written into JSLint and then gasped at the reams of errors JSLint would produce. I subsequently tried JSLint-ing various well known JS libraries (jQuery etc) and saw that JSLint considered they were thoroughly problematic as well. This made me feel slightly better. It was when I started examining some of the \\"errors\\" JSLint reported that I took exception. Yes, I took exception to exceptions! (I\'m \\\\***very**\\\\* pleased with that!) Here\'s a few of the errors generated by JSLint when inspecting [jquery-1.7.2.js](http://ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.js): - `Problem at line 16 character 10: Expected exactly one space between \'function\' and \'(\'.`\\n\\n- `Problem at line 25 character 1: Expected \'var\' at column 13, not column 1.`\\n- `Problem at line 31 character 5: Unexpected dangling \'_\' in \'_jQuery\'.`\\n\\nJSLint is, much like it\'s creator, quite opinionated. Which is no bad thing. Many of Crockfords opinions are clearly worth their salt. It\'s just I didn\'t want all of them enforced upon me. As you can see above most of these \\"problems\\" are essentially complaints about a different style rather than bugs or potential issues. Now there are options in JSLint that you can turn on and off which looked quite promising. But before I got to investigating them I heard about [JSHint](http://www.jshint.com), brainchild of Anton Kovalyov and Paul Irish. In their own words: _JSHint is a fork of JSLint, the tool written and maintained by Douglas Crockford. The project originally started as an effort to make a more configurable version of JSLint\u2014one that doesn\'t enforce one particular coding style on its users\u2014but then transformed into a separate static analysis tool with its own goals and ideals._ This sounded right up my alley! So I thought I\'d repeat my jQuery test. Here\'s a sample of what JSHint threw back at me, with its default settings in place: - `Line 230: return num == null ? Expected \'===\' and instead saw \'==\'. `\\n\\n- `Line 352: if ( (options = arguments[ i ]) != null ) { Expected \'!==\' and instead saw \'!=\'. `\\n- `Line 354: for ( name in options ) { The body of a for in should be wrapped in an if statement to filter unwanted properties from the prototype. `\\n\\nThese were much more the sort of \\"issues\\" I was interested in. Plus it seemed there was plenty of scope to tweak my options. Excellent. This was good. The icing on my cake would have been a plug-in for Visual Studio which would allow me to evaluate my JS files from within my IDE. Happily the world seems to be full of developers doing good turns for one another. I discovered an extension for VS called [JSLint for Visual Studio 2010](http://jslint4vs2010.codeplex.com/):\\n\\n![](Extensions.webp)\\n\\nThis was an extension that provided either JSLint \\\\***or**\\\\* JSHint evaluation as you preferred from within Visual Studio. Fantastic! With this extension in play you could add JavaScript static code analysis to your compilation process and so learn of all the issues in your code at the same time, whether they lay in C# or JS or [insert language here]. You could control how JS problems were reported; as warnings, errors etc. You could straightforwardly exclude files from evaluation (essential if you\'re reliant on a number of 3rd party JS libraries which you are not responsible for maintaining). You could cater for predefined variables; allow for jQuery or DOJO. You could simply evaluate a single file in your solution by right clicking it and hitting the \\"JS Lint\\" option in the context menu. And it was simplicity itself to activate and deactivate the JSHint / JSLint extension as required. For a more exhaustive round up of the options available I advise taking a look here: [http://jslint4vs2010.codeplex.com](http://jslint4vs2010.codeplex.com/). I would heartily recommend using JSHint if you\'re looking to improve your JS code quality. I\'m grateful to Crockford for making JSHint possible by first writing JSLint. For my part though I think JSHint is the more pragmatic and useful tool and likely to be the one I stick with. For interest (and frankly sheer entertainment value at the crotchetiness of Crockford) it\'s definitely worth having a read up on how JSHint came to pass: - [http://anton.kovalyov.net/2011/02/20/why-i-forked-jslint-to-jshint/](http://anton.kovalyov.net/2011/02/20/why-i-forked-jslint-to-jshint/)\\n\\n- [http://badassjs.com/post/3364925033/jshint-an-community-driven-fork-of-jslint](http://badassjs.com/post/3364925033/jshint-an-community-driven-fork-of-jslint)"},{"id":"simple-technique-for-initialising","metadata":{"permalink":"/simple-technique-for-initialising","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2012-04-16-simple-technique-for-initialising/index.md","source":"@site/blog/2012-04-16-simple-technique-for-initialising/index.md","title":"A Simple Technique for Initialising Properties with Internal Setters for Unit Testing","description":"Refactoring a legacy app includes adding unit tests, but properties with internal setters pose a problem. John explores various approaches.","date":"2012-04-16T00:00:00.000Z","tags":[{"inline":false,"label":"Automated Testing","permalink":"/tags/automated-testing","description":"How to perform the automation of tests."}],"readingTime":5.7,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"simple-technique-for-initialising","title":"A Simple Technique for Initialising Properties with Internal Setters for Unit Testing","authors":"johnnyreilly","tags":["automated testing"],"hide_table_of_contents":false,"description":"Refactoring a legacy app includes adding unit tests, but properties with internal setters pose a problem. John explores various approaches."},"unlisted":false,"prevItem":{"title":"JSHint - Customising your hurt feelings","permalink":"/jshint-customising-your-hurt-feelings"},"nextItem":{"title":"Making PDFs from HTML in C# using WKHTMLtoPDF","permalink":"/making-pdfs-from-html-in-c-using"}},"content":"I was recently working with my colleagues on refactoring a legacy application. We didn\'t have an immense amount of time available for this but the plan was to try and improve what was there as much as possible. In its initial state the application had no unit tests in place at all and so the plan was to refactor the code base in such a way as to make testing it a realistic proposition. To that end the [domain layer](http://en.wikipedia.org/wiki/Domain_layer) was being heavily adjusted and the GUI was being migrated from WebForms to MVC 3. The intention was to build up a pretty solid collection of unit tests. However, as we were working on this we realised we had a problem with properties on our models with [`internal`](<http://msdn.microsoft.com/en-us/library/7c5ka91b(v=vs.80).aspx>) setters...\\n\\n\x3c!--truncate--\x3e\\n\\n## Background\\n\\nThe entities of the project in question used an approach which would store pertinent bits of [normalised](http://en.wikipedia.org/wiki/Database_normalization) data for read-only purposes in related entities. I\'ve re-read that sentence and realise it\'s as clear as mud. Here is an example to clarify:\\n\\n```cs\\npublic class Person\\n{\\n  public int Id { get; set; }\\n  public string FirstName { get; set; }\\n  public string LastName { get; set; }\\n  public string Address { get; set; }\\n  public DateTime DateOfBirth { get; set; }\\n  /* Other fascinating properties... */\\n}\\n\\npublic class Order\\n{\\n  public int Id { get; set; }\\n  public string ProductOrdered { get; set; }\\n  public string OrderedById { get; set; }\\n  public string OrderedByFirstName { get; internal set; }\\n  public string OrderedByLastName { get; internal set; }\\n}\\n```\\n\\nIn the example above you have 2 types of entity: `Person` and `Order`. The `Order` entity makes use of the the `Id`, `FirstName` and `LastName` properties of the `Person` entity in the properties `OrderedById`, `OrderedByFirstName` and `OrderedByLastName`. For persistence (ie saving to the database) purposes the only necessary `Person` property is `OrderedById` identity. `OrderedByFirstName` and `OrderedByLastName` are just \\"nice to haves\\" - essentially present to make implementing the GUI more straightforward.\\n\\nTo express this behaviour / intention in the object model the setters for `OrderedByFirstName` and `OrderedByLastName` are marked as `internal`. The implication of this is that properties like this can only be initialised within the current assembly - or any explicitly associated \\"friend\\" assemblies. In practice this meant that internally set properties were only populated when an object was read in from the database. It wasn\'t possible to set these properties in other assemblies which meant less code was written (<u>a good thing</u>\\n\\n) - after all, why set a property when you don\'t need to?\\n\\nBackground explanation over. It may still be a little unclear but I hope you get the gist.\\n\\n## What\'s our problem?\\n\\nI was writing unit tests for the controllers in our main web application and was having problems with my arrangements. I was mocking the database calls in my controllers much in the manner that you might expect:\\n\\n```ts\\n// Arrange\\n  var orderDb = new Mock<IOrderDb>();\\n  orderDb\\n    .Setup(x => x.GetOrder(It.IsAny<int>()))\\n    .Returns(new Order{\\n      Id = 123,\\n      ProductOrdered = \\"Packet of coffee\\",\\n      OrderedById = 987456,\\n      OrderedByFirstName = \\"John\\",\\n      OrderedByLastName = \\"Reilly\\"\\n    });\\n}\\n```\\n\\nAll looks fine doesn\'t it? It\'s not. Because `OrderedByFirstName` and `OrderedByLastName` have internal setters we are <u>unable</u>\\n\\nto initialise them from within the context of our test project. So what to do?\\n\\nWe toyed with 3 approaches and since each has merits I thought it worth going through each of them:\\n\\n1. To the MOQumentation Batman!: [http://code.google.com/p/moq/wiki/QuickStart](http://code.google.com/p/moq/wiki/QuickStart)! Looking at the MOQ documentation it states the following:\\n\\n   _Mocking internal types of another project: add the following assembly attributes (typically to the AssemblyInfo.cs) to the project containing the internal types:_\\n\\n   ```cs\\n   // This assembly is the default dynamic assembly generated Castle DynamicProxy,\\n   // used by Moq. Paste in a single line.\\n   [assembly:InternalsVisibleTo(\\"DynamicProxyGenAssembly2,PublicKey=0024000004800000940000000602000000240000525341310004000001000100c547cac37abd99c8db225ef2f6c8a3602f3b3606cc9891605d02baa56104f4cfc0734aa39b93bf7852f7d9266654753cc297e7d2edfe0bac1cdcf9f717241550e0a7b191195b7667bb4f64bcb8e2121380fd1d9d46ad2d92d2d15605093924cceaf74c4861eff62abf69b9291ed0a340e113be11e6a7d3113e92484cf7045cc7\\")]\\n   [assembly: InternalsVisibleTo(\\"The.NameSpace.Of.Your.Unit.Test\\")] //I\'d hope it was shorter than that...\\n   ```\\n\\n   This looked to be exactly what we needed and in most situations it would make sense to go with this. Unfortunately for us there was a gotcha. Certain core shared parts of our application platform were [GAC](http://en.wikipedia.org/wiki/Global_Assembly_Cache)\'d. A requirement for GAC-ing an assembly is that it is [signed](http://msdn.microsoft.com/en-us/library/xc31ft41.aspx).\\n\\n   The upshot of this was that if we wanted to use the `InternalsVisibleTo` approach then we would need to sign our web application test project. We weren\'t particularly averse to that and initially did so without much thought. It was then we remembered that every assembly referenced by a signed assembly must also be signed as well. We didn\'t really want to sign our main web application purely for testing purposes. We could and if there weren\'t viable alternatives we well might have. But it just seemed like the wrong reason to be taking that decision. Like using a sledgehammer to crack a nut.\\n\\n2. The next approach we took was using mock objects. Instead of using our objects straight we would mock them as below:\\n\\n   ```cs\\n   //Create mock and set internal properties\\n         var orderMock = new Mock<Order>();\\n         orderMock.SetupGet(x => x.OrderedByFirstName).Returns(\\"John\\");\\n         orderMock.SetupGet(x => x.OrderedByLastName).Returns(\\"Reilly\\");\\n\\n         //Set up standard properties\\n         orderMock.SetupAllProperties();\\n         var orderStub = orderMock.Object;\\n         orderStub.Id = 123;\\n         orderStub.ProductOrdered = \\"Packet of coffee\\";\\n         orderStub.OrderedById = 987456;\\n   ```\\n\\n   Now this approach worked fine but had a couple of snags:\\n\\n   - As you can see it\'s pretty verbose and much less clear to read than it was previously.\\n   - It required that we add the `virtual` keyword to all our internally set properties like so:\\n\\n     ```cs\\n     public class Order\\n     {\\n       // ....\\n       public virtual string OrderedByFirstName { get; internal set; }\\n       public virtual string OrderedByLastName { get; internal set; }\\n       // ...\\n     }\\n     ```\\n\\n   - Our standard constructor already initialised the value of our internally set properties. So adding `virtual` to the internally set properties generated [ReSharper](http://www.jetbrains.com/resharper/) warnings aplenty about virtual properties being initialised in the constructor. Fair enough.\\n\\n   Because of the snags it still felt like we were in nutcracking territory...\\n\\n3. ... and this took us to the approach that we ended up adopting: a special mocking constructor for each class we wanted to test, for example:\\n\\n   ```cs\\n   /// <summary>\\n   /// Mocking constructor used to initialise internal properties\\n   /// </summary>\\n   public Order(string orderedByFirstName = null, string orderedByLastName = null)\\n   : this()\\n   {\\n   OrderedByFirstName = orderedByFirstName;\\n   OrderedByLastName = orderedByLastName;\\n   }\\n\\n   ```\\n\\n   Thanks to the ever lovely [Named and Optional Arguments](http://msdn.microsoft.com/en-us/library/dd264739.aspx) feature of C# combined with [Object Initializers](http://msdn.microsoft.com/en-us/library/bb397680.aspx) it meant it was possible to write quite expressive, succinct code using this approach; for example:\\n\\n   ```cs\\n   var order = new Order(\\n           orderedByFirstName: \\"John\\",\\n           orderedByLastName: \\"Reilly\\"\\n         )\\n         {\\n           Id = 123,\\n           ProductOrdered = \\"Packet of coffee\\",\\n           OrderedById = 987456\\n         };\\n   ```\\n\\n   Here we\'re calling the mocking constructor to set the internally set properties and subsequently initialising the other properties using the object initialiser mechanism.\\n\\n   Implementing these custom constructors wasn\'t a massive piece of work and so we ended up settling on this technique for initialising internal properties."},{"id":"making-pdfs-from-html-in-c-using","metadata":{"permalink":"/making-pdfs-from-html-in-c-using","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2012-04-05-making-pdfs-from-html-in-c-using/index.md","source":"@site/blog/2012-04-05-making-pdfs-from-html-in-c-using/index.md","title":"Making PDFs from HTML in C# using WKHTMLtoPDF","description":"Create PDF reports from HTML with WKHTMLtoPDF, an open source tool that renders web pages to PDF, using a simple wrapper class.","date":"2012-04-05T00:00:00.000Z","tags":[{"inline":false,"label":"C#","permalink":"/tags/csharp","description":"The C# programming language."}],"readingTime":8.735,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"making-pdfs-from-html-in-c-using","title":"Making PDFs from HTML in C# using WKHTMLtoPDF","authors":"johnnyreilly","tags":["c#"],"hide_table_of_contents":false,"description":"Create PDF reports from HTML with WKHTMLtoPDF, an open source tool that renders web pages to PDF, using a simple wrapper class."},"unlisted":false,"prevItem":{"title":"A Simple Technique for Initialising Properties with Internal Setters for Unit Testing","permalink":"/simple-technique-for-initialising"},"nextItem":{"title":"WCF - moving from Config to Code, a simple WCF service harness (plus implementing your own Authorization)","permalink":"/wcf-moving-from-config-to-code-simple"}},"content":"## Updated 03/01/2013\\n\\nI\'ve written a subsequent post which builds on the work of this original post. The new post exposes this functionality via a WCF service and can be found [here](../2013-01-03-html-to-pdf-using-wcf-service/index.md).\\n\\n## Making PDFs from HTML\\n\\nI wanted to talk about an approach I\'ve discovered for making PDFs directly from HTML. I realise that in these wild and crazy days of [PDF.js](http://mozilla.github.com/pdf.js/) and the like that techniques like this must seem very old hat. That said, this technique works and more importantly it solves a problem I was faced with but without forcing the users to move the \\"newest hottest version of X\\". Much as many of would love to solve problems this way, alas many corporations move slower than that and in the meantime we still have to deliver - we still have to meet requirements. Rather than just say \\"I did this\\" I thought I\'d record how I got to this point in the first place. I don\'t know about you but I find the reasoning behind why different technical decisions get made quite an interesting topic...\\n\\n\x3c!--truncate--\x3e\\n\\nFor some time I\'ve been developing / supporting an application which is used in an intranet environment where the company mandated browser is still IE 6. It was a requirement that there be \\"print\\" functionality in this application. As is well known (even by Microsoft themselves) the print functionality in IE 6 was never fantastic. But the requirement for usable printouts remained.\\n\\nThe developers working on the system before me decided to leverage Crystal Reports (remember that?). Essentially there was a reporting component to the application at the time which created custom reports using Crystal and rendered them to the user in the form of PDFs (which have been eminently printable for as long as I care to remember). One of the developers working on the system realised that it would be perfectly possible to create some \\"reports\\" within Crystal which were really \\"print to PDF\\" screens for the app.\\n\\nIt worked well and this solution stayed in place for a very long time. However, some years down the line the Crystal Reports was discarded as the reporting mechanism for the app. But we were unable to decommission Crystal entirely because we still needed it for printing.\\n\\nI\'d never really liked the Crystal solution for a number of reasons:\\n\\n1. We needed custom stored procs to drive the Crystal print screens which were near duplicates of the main app procs. This duplication of effort never felt right.\\n2. We had to switch IDEs whenever we were maintaining our print screens. And the Crystal IDE is not a joy to use.\\n3. Perhaps most importantly, for certain users we needed to hide bits of information from the print. The version of Crystal we were using did not make the dynamic customisation of our print screens a straightforward proposition. (In its defence we weren\'t really using it for what it was designed for.) As a result the developers before me had ended up creating various versions of each print screen revealing different levels of information. As you can imagine, this meant that the effort involved in making changes to the print screens had increased exponentially\\n\\nIt occurred to me that it would be good if we could find some way of generating our own PDF reports without using Crystal that would be a step forward. It was shortly after this that I happened upon [WKHTMLtoPDF](http://code.google.com/p/wkhtmltopdf/). This is an open source project which describes itself as a _\\"Simple shell utility to convert html to pdf using the webkit rendering engine, and qt.\\"_ I tested it out on various websites and it worked. It wasn\'t by any stretch of the imagination a perfect HTML to PDF tool but the quality it produced greatly outstripped the presentation currently in place via Crystal.\\n\\nThis was just the ticket. Using WKHTMLtoPDF I could have simple web pages in the application which could be piped into WKHTMLtoPDF to make a PDF as needed. It could be dynamic - because ASP.NET is dynamic. We wouldn\'t need to write and maintain custom stored procs anymore. And happily we would no longer need to use Crystal.\\n\\nBefore we could rid ourselves of Crystal though, I needed a way that I could generate these PDFs on the fly within the website. For this I ended up writing a simple wrapper class for WKHTMLtoPDF which could be used to invoke it on the fly. In fact a good portion of this was derived from various contributions on [a post on StackOverflow](http://stackoverflow.com/q/1331926). It ended up looking like this:\\n\\n```cs\\nusing System;\\nusing System.Collections.Generic;\\nusing System.Linq;\\nusing System.Security;\\nusing System.Web;\\nusing System.Web.Hosting;\\n\\nnamespace PdfGenerator\\n{\\n    public class PdfGenerator\\n    {\\n        /// <summary>\\n        /// Convert Html page at a given URL to a PDF file using open-source tool wkhtml2pdf\\n        ///   wkhtml2pdf can be found at: http://code.google.com/p/wkhtmltopdf/\\n        ///   Useful code used in the creation of this I love the good folk of StackOverflow!: http://stackoverflow.com/questions/1331926/calling-wkhtmltopdf-to-generate-pdf-from-html/1698839\\n        ///   An online manual can be found here: http://madalgo.au.dk/~jakobt/wkhtmltoxdoc/wkhtmltopdf-0.9.9-doc.html\\n        ///\\n        /// Ensure that the output folder specified is writeable by the ASP.NET process of IIS running on your server\\n        ///\\n        /// This code requires that the Windows installer is installed on the relevant server / client.  This can either be found at:\\n        ///   http://code.google.com/p/wkhtmltopdf/downloads/list - download wkhtmltopdf-0.9.9-installer.exe\\n        /// </summary>\\n        /// <param name=\\"pdfOutputLocation\\"></param>\\n        /// <param name=\\"outputFilenamePrefix\\"></param>\\n        /// <param name=\\"urls\\"></param>\\n        /// <param name=\\"options\\"></param>\\n        /// <param name=\\"pdfHtmlToPdfExePath\\"></param>\\n        /// <returns>the URL of the generated PDF</returns>\\n        public static string HtmlToPdf(string pdfOutputLocation, string outputFilenamePrefix, string[] urls,\\n            string[] options = null,\\n            string pdfHtmlToPdfExePath = \\"C:\\\\\\\\Program Files (x86)\\\\\\\\wkhtmltopdf\\\\\\\\wkhtmltopdf.exe\\")\\n        {\\n            string urlsSeparatedBySpaces = string.Empty;\\n            try\\n            {\\n                //Determine inputs\\n                if ((urls == null) || (urls.Length == 0))\\n                    throw new Exception(\\"No input URLs provided for HtmlToPdf\\");\\n                else\\n                    urlsSeparatedBySpaces = String.Join(\\" \\", urls); //Concatenate URLs\\n\\n                string outputFolder = pdfOutputLocation;\\n                string outputFilename = outputFilenamePrefix + \\"_\\" + DateTime.Now.ToString(\\"yyyy-MM-dd-hh-mm-ss-fff\\") + \\".PDF\\"; // assemble destination PDF file name\\n\\n                var p = new System.Diagnostics.Process()\\n                {\\n                    StartInfo =\\n                    {\\n                        FileName = pdfHtmlToPdfExePath,\\n                        Arguments = ((options == null) ? \\"\\" : String.Join(\\" \\", options)) + \\" \\" + urlsSeparatedBySpaces + \\" \\" + outputFilename,\\n                        UseShellExecute = false, // needs to be false in order to redirect output\\n                        RedirectStandardOutput = true,\\n                        RedirectStandardError = true,\\n                        RedirectStandardInput = true, // redirect all 3, as it should be all 3 or none\\n                        WorkingDirectory = HttpContext.Current.Server.MapPath(outputFolder)\\n                    }\\n                };\\n\\n                p.Start();\\n\\n                // read the output here...\\n                var output = p.StandardOutput.ReadToEnd();\\n                var errorOutput = p.StandardError.ReadToEnd();\\n\\n                // ...then wait n milliseconds for exit (as after exit, it can\'t read the output)\\n                p.WaitForExit(60000);\\n\\n                // read the exit code, close process\\n                int returnCode = p.ExitCode;\\n                p.Close();\\n\\n                // if 0 or 2, it worked so return path of pdf\\n                if ((returnCode == 0) || (returnCode == 2))\\n                    return outputFolder + outputFilename;\\n                else\\n                    throw new Exception(errorOutput);\\n            }\\n            catch (Exception exc)\\n            {\\n                throw new Exception(\\"Problem generating PDF from HTML, URLs: \\" + urlsSeparatedBySpaces + \\", outputFilename: \\" + outputFilenamePrefix, exc);\\n            }\\n        }\\n    }\\n}\\n```\\n\\nWith this wrapper I could pass in URLs and extract out PDFs. Here\'s a couple of examples of me doing just that:\\n\\n```cs\\n//Create PDF from a single URL\\n    var pdfUrl = PdfGenerator.HtmlToPdf(pdfOutputLocation: \\"~/PDFs/\\",\\n        outputFilenamePrefix: \\"GeneratedPDF\\",\\n        urls: new string[] { \\"http://news.bbc.co.uk\\" });\\n\\n    //Create PDF from multiple URLs\\n    var pdfUrl = PdfGenerator.HtmlToPdf(pdfOutputLocation: \\"~/PDFs/\\",\\n        outputFilenamePrefix: \\"GeneratedPDF\\",\\n        urls: new string[] { \\"http://www.google.co.uk\\", \\"http://news.bbc.co.uk\\" });\\n```\\n\\nAs you can see from the second example above it\'s possible to pipe a number of URLs into the wrapper all to be rendered to a single PDF. Most of the time this was surplus to our requirements but it\'s good to know it\'s possible. Take a look at the BBC website PDF generated by the first example:\\n\\n<iframe src=\\"https://docs.google.com/file/d/0B87K8-qxOZGFYktEWGtXRXJSSS1ZWFR4emFfMmVxZw/preview\\" width=\\"500\\" height=\\"500\\"></iframe>\\n\\nPretty good, no? As you can see it\'s not perfect from looking at the titles (bit squashed) but I deliberately picked a more complicated page to show what WKHTMLtoPDF was capable of. The print screens I had in mind to build would be significantly simpler than this.\\n\\nOnce this was in place I was able to scrap the Crystal solution. It was replaced with a couple of \\"print to PDF\\" ASPXs in the main web app which would be customised when rendering to hide the relevant bits of data from the user. These ASPXs would be piped into the HtmlToPdf method as needed and then the user would be redirected to that PDF. If for some reason the PDF failed to render the users would see the straight \\"print to PDF\\" ASPX - just not as a PDF if you see what I mean. I should say that it was pretty rare for a PDF to not render but this was my failsafe.\\n\\nThis new solution had a number of upsides from our perspective:\\n\\n- Development maintenance time (and consequently cost for our customers) for print screens was significantly reduced. This was due to the print screens being part of the main web app. This meant they shared styling etc with all the other web screens and the dynamic nature of ASP.NET made customising a screen on the fly simplicity itself.\\n- We were now able to regionalise our print screens for the users in the same way as we did with our main web app. This just wasn\'t realistic with the Crystal solution because of the amount of work involved.\\n- I guess this is kind of a [DRY](http://en.wikipedia.org/wiki/Don%27t_repeat_yourself) solution :-)\\n\\nYou can easily make use of the above approach yourself. All you need do is download and install [WKHTMLtoPDF](http://code.google.com/p/wkhtmltopdf/) on your machine. I advise using version 0.9.9 as the later release candidates appear slightly buggy at present.\\n\\nCouple of gotchas:\\n\\n1. Make sure that you pass the correct installation path to the HtmlToPdf method if you installed it anywhere other than the default location. You\'ll see that the class assumes the default if it wasn\'t passed\\n2. Ensure that Read and Execute rights are granted to the wkhtmltopdf folder for the relevant process\\n3. Ensure that Write rights are granted for the location you want to create your PDFs for the relevant process\\n\\nIn our situation we are are invoking this directly in our web application on demand. I have no idea how this would scale - perhaps not well. This is not really an issue for us as our user base is fairly small and this functionality isn\'t called excessively. I think if this was used much more than it is I\'d be tempted to hive off this functionality into a separate app. But this works just dandy for now."},{"id":"wcf-moving-from-config-to-code-simple","metadata":{"permalink":"/wcf-moving-from-config-to-code-simple","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2012-03-22-wcf-moving-from-config-to-code-simple/index.md","source":"@site/blog/2012-03-22-wcf-moving-from-config-to-code-simple/index.md","title":"WCF - moving from Config to Code, a simple WCF service harness (plus implementing your own Authorization)","description":"John describes his approach to developing a Windows Service-hosted WCF service/client harness, including locking down WCF authorization.","date":"2012-03-22T00:00:00.000Z","tags":[{"inline":false,"label":"Authentication and Authorization","permalink":"/tags/auth","description":"Matters related to identity and permissioning"}],"readingTime":10.655,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"wcf-moving-from-config-to-code-simple","title":"WCF - moving from Config to Code, a simple WCF service harness (plus implementing your own Authorization)","authors":"johnnyreilly","tags":["auth"],"hide_table_of_contents":false,"description":"John describes his approach to developing a Windows Service-hosted WCF service/client harness, including locking down WCF authorization."},"unlisted":false,"prevItem":{"title":"Making PDFs from HTML in C# using WKHTMLtoPDF","permalink":"/making-pdfs-from-html-in-c-using"},"nextItem":{"title":"Using the PubSub / Observer pattern to emulate constructor chaining without cluttering up global scope","permalink":"/using-pubsub-observer-pattern-to"}},"content":"Last time I wrote about WCF I was getting up and running with [WCF Transport Windows authentication using NetTcpBinding in an Intranet environment](../2012-02-15-wcf-transport-windows-authentication/index.md). I ended up with a WCF service hosted in a Windows Service which did pretty much what the previous post name implies.\\n\\n\x3c!--truncate--\x3e\\n\\nSince writing that I\'ve taken things on a bit further and I thought it worth recording my approach whilst it\'s still fresh in my mind. There\'s 3 things I want to go over:\\n\\n1. I\'ve moved away from the standard config driven WCF approach to a more \\"code-first\\" style\\n2. I\'ve established a basic Windows Service hosted WCF service / client harness which is useful if you\'re trying to get up and running with a WCF service quickly\\n3. I\'ve locked down the WCF authorization to a single Windows account through the use of my own [ServiceAuthorizationManager](http://msdn.microsoft.com/en-us/library/ms731774.aspx)\\n\\n## Moving from Config to Code\\n\\nSo, originally I was doing what all the cool kids are doing and driving the configuration of my WCF service and all its clients through config files. And why not? I\'m in good company.\\n\\nHere\'s why not: it gets \\\\***very**\\\\* verbose \\\\***very**\\\\* quickly....\\n\\nOkay - that\'s not the end of the world. My problem was that I had \\\\~10 Windows Services and 3 Web applications that needed to call into my WCF Service. I didn\'t want to have to separately tweak 15 or so configs each time I wanted to make one standard change to WCF configuration settings. I wanted everything in one place.\\n\\nNow there\'s newer (and probably hipper) ways of achieving this. [Here\'s one possibility I happened upon on StackOverflow that looks perfectly fine.](http://stackoverflow.com/a/2814286)\\n\\nWell I didn\'t use a hip new approach - no I went Old School with my old friend the [appSettings file attribute](http://msdn.microsoft.com/en-us/library/ms228154.aspx). Remember that? It\'s just a simple way to have all your common appSettings configuration settings in a single file which can be linked to from as many other apps as you like. It\'s wonderful and I\'ve been using it for a long time now. Unfortunately it\'s pretty basic in that it\'s only the appSettings section that can be shared out; no `&lt;system.serviceModel&gt;` or similar.\\n\\nBut that wasn\'t really a problem from my perspective. I realised that there were actually very few things that needed to be configurable for my WCF service. Really I wanted a basic WCF harness that could be initialised in code which implicitly set all the basic configuration with settings that worked (ie it was set up with defaults like maximum message size which were sufficiently sized). On top of that I would allow myself to configure just those things that I needed to through the use of my own custom WCF config settings in the shared appSettings.config file.\\n\\nOnce done I massively reduced the size of my configs from frankly gazillions of entries to just these appSettings.config entries which were shared across each of my WCF service clients and by my Windows Service harness:\\n\\n```xml\\n<appSettings>\\n  <add key=\\"WcfBaseAddressForClient\\" value=\\"net.tcp://localhost:9700/\\"/>\\n  <add key=\\"WcfWindowsSecurityApplied\\" value=\\"true\\" />\\n  <add key=\\"WcfCredentialsUserName\\" value=\\"myUserName\\" />\\n  <add key=\\"WcfCredentialsPassword\\" value=\\"myPassword\\" />\\n  <add key=\\"WcfCredentialsDomain\\" value=\\"myDomain\\" />\\n  </appSettings>\\n```\\n\\nAnd these config settings used only by my Windows Service harness:\\n\\n```xml\\n<appSettings file=\\"../Shared/AppSettings.config\\">\\n    <add key=\\"WcfBaseAddressForService\\" value=\\"net.tcp://localhost:9700/\\"/>\\n  </appSettings>\\n```\\n\\n## Show me your harness\\n\\nI ended up with a quite a nice basic \\"vanilla\\" framework that allowed me to quickly set up Windows Service hosted WCF services. The framework also provided me with a simple way to consume these WCF services with a minimum of code an configuration. No muss. No fuss. :-) So pleased with it was I that I thought I\'d go through it here much in the manner of a chef baking a cake...\\n\\nTo start with I created myself a Windows Service in Visual Studio which I grandly called \\"WcfWindowsService\\". The main service class looked like this:\\n\\n```cs\\npublic class WcfWindowsService: ServiceBase\\n  {\\n    public static string WindowsServiceName = \\"WCF Windows Service\\";\\n    public static string WindowsServiceDescription = \\"Windows service that hosts a WCF service.\\";\\n\\n    private static readonly log4net.ILog _logger = log4net.LogManager.GetLogger(System.Reflection.MethodBase.GetCurrentMethod().DeclaringType);\\n\\n    public List<ServiceHost> _serviceHosts = null;\\n\\n    public WcfWindowsService()\\n    {\\n      ServiceName = WindowsServiceName;\\n    }\\n\\n    public static void Main()\\n    {\\n      ServiceBase.Run(new WcfWindowsService());\\n    }\\n\\n    /// <summary>\\n    /// The Windows Service is starting\\n    /// </summary>\\n    /// <param name=\\"args\\"></param>\\n    protected override void OnStart(string[] args)\\n    {\\n      try\\n      {\\n        CloseAndClearServiceHosts();\\n\\n        //Make log4net startup\\n        XmlConfigurator.Configure();\\n        _logger.Warn(\\"WCF Windows Service starting...\\");\\n        _logger.Info(\\"Global.WcfWindowsSecurityApplied = \\" + Global.WcfWindowsSecurityApplied.ToString().ToLower());\\n\\n        if (Global.WcfWindowsSecurityApplied)\\n        {\\n          _logger.Info(\\"Global.WcfOnlyAuthorizedForWcfCredentials = \\" + Global.WcfOnlyAuthorizedForWcfCredentials.ToString().ToLower());\\n\\n          if (Global.WcfOnlyAuthorizedForWcfCredentials)\\n          {\\n            _logger.Info(\\"Global.WcfCredentialsDomain = \\" + Global.WcfCredentialsDomain);\\n            _logger.Info(\\"Global.WcfCredentialsUserName = \\" + Global.WcfCredentialsUserName);\\n          }\\n        }\\n\\n        //Create binding\\n        var wcfBinding = WcfHelper.CreateBinding(Global.WcfWindowsSecurityApplied);\\n\\n        // Create a servicehost and endpoints for each service and open each\\n        _serviceHosts = new List<ServiceHost>();\\n        _serviceHosts.Add(WcfServiceFactory<IHello>.CreateAndOpenServiceHost(typeof(HelloService), wcfBinding));\\n        _serviceHosts.Add(WcfServiceFactory<IGoodbye>.CreateAndOpenServiceHost(typeof(GoodbyeService), wcfBinding));\\n\\n        _logger.Warn(\\"WCF Windows Service started.\\");\\n      }\\n      catch (Exception exc)\\n      {\\n        _logger.Error(\\"Problem starting up\\", exc);\\n\\n        throw exc;\\n      }\\n    }\\n\\n    /// <summary>\\n    /// The Windows Service is stopping\\n    /// </summary>\\n    protected override void OnStop()\\n    {\\n      CloseAndClearServiceHosts();\\n\\n      _logger.Warn(\\"WCF Windows Service stopped\\");\\n    }\\n\\n    /// <summary>\\n    /// Close and clear service hosts in list and clear it down\\n    /// </summary>\\n    private void CloseAndClearServiceHosts()\\n    {\\n      if (_serviceHosts != null)\\n      {\\n        foreach (var serviceHost in _serviceHosts)\\n        {\\n          CloseAndClearServiceHost(serviceHost);\\n        }\\n\\n        _serviceHosts.Clear();\\n      }\\n    }\\n\\n    /// <summary>\\n    /// Close and clear the passed service host\\n    /// </summary>\\n    /// <param name=\\"serviceHost\\"></param>\\n    private void CloseAndClearServiceHost(ServiceHost serviceHost)\\n    {\\n      if (serviceHost != null)\\n      {\\n        _logger.Info(string.Join(\\", \\", serviceHost.BaseAddresses) + \\" is closing...\\");\\n\\n        serviceHost.Close();\\n\\n        _logger.Info(string.Join(\\", \\", serviceHost.BaseAddresses) + \\" is closed\\");\\n      }\\n    }\\n  }\\n```\\n\\nAs you\'ve no doubt noticed this makes use of [Log4Net](http://logging.apache.org/log4net/) for logging purposes (I\'ll assume you\'re aware of it). My Windows Service implements such fantastic WCF services as HelloService and GoodbyeService. Each revolutionary in their own little way. To give you a taste of the joie de vivre that these services exemplify take a look at this:\\n\\n```cs\\n// Implement the IHello service contract in a service class.\\n  public class HelloService : WcfServiceAuthorizationManager, IHello\\n  {\\n    // Implement the IHello methods.\\n    public string GreetMe(string thePersonToGreet)\\n    {\\n      return \\"well hello there \\" + thePersonToGreet;\\n    }\\n  }\\n```\\n\\nExciting! WcfWindowsService also references another class called \\"Global\\" which is a helper class - to be honest not much more than a wrapper for my config settings. It looks like this:\\n\\n```cs\\nstatic public class Global\\n  {\\n    #region Properties\\n\\n    // eg \\"net.tcp://localhost:9700/\\"\\n    public static string WcfBaseAddressForService { get { return ConfigurationManager.AppSettings[\\"WcfBaseAddressForService\\"]; } }\\n\\n    // eg true\\n    public static bool WcfWindowsSecurityApplied { get { return bool.Parse(ConfigurationManager.AppSettings[\\"WcfWindowsSecurityApplied\\"]); } }\\n\\n    // eg true\\n    public static bool WcfOnlyAuthorizedForWcfCredentials { get { return bool.Parse(ConfigurationManager.AppSettings[\\"WcfOnlyAuthorizedForWcfCredentials\\"]); } }\\n\\n    // eg \\"myDomain\\"\\n    public static string WcfCredentialsDomain { get { return ConfigurationManager.AppSettings[\\"WcfCredentialsDomain\\"]; } }\\n\\n    // eg \\"myUserName\\"\\n    public static string WcfCredentialsUserName { get { return ConfigurationManager.AppSettings[\\"WcfCredentialsUserName\\"]; } }\\n\\n    // eg \\"myPassword\\" - this should *never* be stored unencrypted and is only ever used by clients that are not already running with the approved Windows credentials\\n    public static string WcfCredentialsPassword { get { return ConfigurationManager.AppSettings[\\"WcfCredentialsPassword\\"]; } }\\n\\n    #endregion\\n  }\\n```\\n\\nWcfWindowsService creates and hosts a HelloService and a GoodbyeService when it starts up. It does this using my handy WcfServiceFactory:\\n\\n```cs\\npublic class WcfServiceFactory<TInterface>\\n  {\\n    private static readonly log4net.ILog _logger = log4net.LogManager.GetLogger(System.Reflection.MethodBase.GetCurrentMethod().DeclaringType);\\n\\n    public static ServiceHost CreateAndOpenServiceHost(Type serviceType, NetTcpBinding wcfBinding)\\n    {\\n      var serviceHost = new ServiceHost(serviceType, new Uri(Global.WcfBaseAddressForService + ServiceHelper<TInterface>.GetServiceName()));\\n      serviceHost.AddServiceEndpoint(typeof(TInterface), wcfBinding, \\"\\");\\n      serviceHost.Authorization.ServiceAuthorizationManager = new WcfServiceAuthorizationManager(); // This allows us to control authorisation within WcfServiceAuthorizationManager\\n      serviceHost.Open();\\n\\n      _logger.Info(string.Join(\\", \\", serviceHost.BaseAddresses) + \\" is now listening.\\");\\n\\n      return serviceHost;\\n    }\\n  }\\n```\\n\\nTo do this it also uses my equally handy WcfHelper class:\\n\\n```cs\\nstatic public class WcfHelper\\n  {\\n    /// <summary>\\n    /// Create a NetTcpBinding\\n    /// </summary>\\n    /// <param name=\\"useWindowsSecurity\\"></param>\\n    /// <returns></returns>\\n    public static NetTcpBinding CreateBinding(bool useWindowsSecurity)\\n    {\\n      var wcfBinding = new NetTcpBinding();\\n      if (useWindowsSecurity)\\n      {\\n        wcfBinding.Security.Mode = SecurityMode.Transport;\\n        wcfBinding.Security.Transport.ClientCredentialType = TcpClientCredentialType.Windows;\\n      }\\n      else\\n        wcfBinding.Security.Mode = SecurityMode.None;\\n\\n      wcfBinding.MaxBufferSize = int.MaxValue;\\n      wcfBinding.MaxReceivedMessageSize = int.MaxValue;\\n      wcfBinding.ReaderQuotas.MaxArrayLength = int.MaxValue;\\n      wcfBinding.ReaderQuotas.MaxDepth = int.MaxValue;\\n      wcfBinding.ReaderQuotas.MaxStringContentLength = int.MaxValue;\\n      wcfBinding.ReaderQuotas.MaxBytesPerRead = int.MaxValue;\\n\\n      return wcfBinding;\\n    }\\n  }\\n\\n  /// <summary>\\n  /// Create a WCF Client for use anywhere (be it Windows Service or ASP.Net web application)\\n  /// nb Credential fields are optional and only likely to be needed by web applications\\n  /// </summary>\\n  /// <typeparam name=\\"TInterface\\"></typeparam>\\n  public class WcfClientFactory<TInterface>\\n  {\\n    public static TInterface CreateChannel(bool useWindowsSecurity, string wcfBaseAddress, string wcfCredentialsUserName = null, string wcfCredentialsPassword = null, string wcfCredentialsDomain = null)\\n    {\\n      //Create NetTcpBinding using universally\\n      var wcfBinding = WcfHelper.CreateBinding(useWindowsSecurity);\\n\\n      //Get Service name from examining the ServiceNameAttribute decorating the interface\\n      var serviceName = ServiceHelper<TInterface>.GetServiceName();\\n\\n      //Create the factory for creating your channel\\n      var factory = new ChannelFactory<TInterface>(\\n        wcfBinding,\\n        new EndpointAddress(wcfBaseAddress + serviceName)\\n        );\\n\\n      //if credentials have been supplied then use them\\n      if (!string.IsNullOrEmpty(wcfCredentialsUserName))\\n      {\\n        factory.Credentials.Windows.ClientCredential = new System.Net.NetworkCredential(wcfCredentialsUserName, wcfCredentialsPassword, wcfCredentialsDomain);\\n      }\\n\\n      //Create the channel\\n      var channel = factory.CreateChannel();\\n\\n      return channel;\\n    }\\n  }\\n```\\n\\nNow the above WcfHelper class and it\'s comrade-in-arms the WcfClientFactory don\'t live in the WcfWindowsService project with the other classes. No. They live in a separate project called the WcfWindowsServiceContracts project with their old mucker the ServiceHelper:\\n\\n```cs\\npublic class ServiceHelper<T>\\n  {\\n    public static string GetServiceName()\\n    {\\n      var customAttributes = typeof(T).GetCustomAttributes(false);\\n      if (customAttributes.Length > 0)\\n      {\\n        foreach (var customAttribute in customAttributes)\\n        {\\n          if (customAttribute is ServiceNameAttribute)\\n          {\\n            return ((ServiceNameAttribute)customAttribute).ServiceName;\\n          }\\n        }\\n      }\\n\\n      throw new ArgumentException(\\"Interface is missing ServiceNameAttribute\\");\\n    }\\n  }\\n\\n  [AttributeUsage(AttributeTargets.Interface, AllowMultiple = false)]\\n  public class ServiceNameAttribute : System.Attribute\\n  {\\n    public ServiceNameAttribute(string serviceName)\\n    {\\n      this.ServiceName = serviceName;\\n    }\\n\\n    public string ServiceName { get; set; }\\n  }\\n```\\n\\nNow can you guess what the WcfWindowsServiceContracts project might contain? Yes; contracts for your services (oh the excitement)! What might one of these contracts look like I hear you ask... Well, like this:\\n\\n```cs\\n[ServiceContract()]\\n  [ServiceName(\\"HelloService\\")]\\n  public interface IHello\\n  {\\n    [OperationContract]\\n    string GreetMe(string thePersonToGreet);\\n  }\\n```\\n\\nThe WcfWindowsServiceContracts project is included in \\\\***any**\\\\* WCF client solution that wants to call your WCF services. It is also included in the WCF service solution. It facilitates the calling of services. What you\'re no doubt wondering is how this might be achieved. Well here\'s how, it uses our old friend the `WcfClientFactory`:\\n\\n```cs\\nvar helloClient = WcfClientFactory<IHello>\\n    .CreateChannel(\\n      useWindowsSecurity:     Global.WcfWindowsSecurityApplied,  // eg true\\n      wcfBaseAddress:         Global.WcfBaseAddressForClient,    // eg \\"net.tcp://localhost:9700/\\"\\n      wcfCredentialsUserName: Global.WcfCredentialsUserName,     // eg \\"myUserName\\" - Optional parameter - only passed by web applications that need to impersonate the valid user\\n      wcfCredentialsPassword: Global.WcfCredentialsPassword,     // eg \\"myPassword\\" - Optional parameter - only passed by web applications that need to impersonate the valid user\\n      wcfCredentialsDomain:   Global.WcfCredentialsDomain        // eg \\"myDomain\\" - Optional parameter - only passed by web applications that need to impersonate the valid user\\n    );\\n  var greeting = helloClient.GreetMe(\\"John\\"); //\\"well hello there John\\"\\n```\\n\\nSee? Simple as simple. The eagle eyed amongst you will have noticed that client example above is using \\"`Global`\\" which is essentially a copy of the `Global` class mentioned above that is part of the WcfWindowsService project.\\n\\n## Locking down Authorization to a single Windows account\\n\\nI can tell you think i\'ve forgotten something. \\"Tell me about this locking down to the single Windows account / what is this mysterious `WcfServiceAuthorizationManager` class that all your WCF services inherit from? Don\'t you fob me off now.... etc\\"\\n\\nWell ensuring that only a single Windows account is authorised (yes dammit the original English spelling) to access our WCF services is achieved by implementing our own `ServiceAuthorizationManager` class. This implementation is used for authorisation by your `ServiceHost` and the logic sits in the overridden `CheckAccessCore` method. All of our WCF service classes will inherit from our `ServiceAuthorizationManager` class and so trigger the `CheckAccessCore` authorisation each time they are called.\\n\\nAs you can see from the code below, depending on our configuration, we lock down access to all our WCF services to a specific Windows account. This is far from the only approach that you might want to take to authorisation; it\'s simply the one that we\'ve been using. However the power of being able to implement your own authorisation in the `CheckAccessCore` method allows you the flexibility to do pretty much anything you want:\\n\\n```cs\\npublic class WcfServiceAuthorizationManager : ServiceAuthorizationManager\\n  {\\n    protected static readonly log4net.ILog _logger = log4net.LogManager.GetLogger(System.Reflection.MethodBase.GetCurrentMethod().DeclaringType);\\n\\n    protected override bool CheckAccessCore(OperationContext operationContext)\\n    {\\n      if (Global.WcfWindowsSecurityApplied)\\n      {\\n        if ((operationContext.ServiceSecurityContext.IsAnonymous) ||\\n          (operationContext.ServiceSecurityContext.PrimaryIdentity == null))\\n        {\\n          _logger.Error(\\"WcfWindowsSecurityApplied = true but no credentials have been supplied\\");\\n          return false;\\n        }\\n\\n        if (Global.WcfOnlyAuthorizedForWcfCredentials)\\n        {\\n          if (operationContext.ServiceSecurityContext.PrimaryIdentity.Name.ToLower() == Global.WcfCredentialsDomain.ToLower() + \\"\\\\\\\\\\" + Global.WcfCredentialsUserName.ToLower())\\n          {\\n            _logger.Debug(\\"WcfOnlyAuthorizedForWcfCredentials = true and the valid user (\\" + operationContext.ServiceSecurityContext.PrimaryIdentity.Name + \\") has been supplied and access allowed\\");\\n            return true;\\n          }\\n          else\\n          {\\n            _logger.Error(\\"WcfOnlyAuthorizedForWcfCredentials = true and an invalid user (\\" + operationContext.ServiceSecurityContext.PrimaryIdentity.Name + \\") has been supplied and access denied\\");\\n            return false;\\n          }\\n        }\\n        else\\n        {\\n          _logger.Debug(\\"WcfOnlyAuthorizedForWcfCredentials = false, credentials were supplied (\\" + operationContext.ServiceSecurityContext.PrimaryIdentity.Name + \\") so access allowed\\");\\n          return true;\\n        }\\n      }\\n      else\\n      {\\n        _logger.Info(\\"WcfWindowsSecurityApplied = false so we are allowing unfettered access\\");\\n        return true;\\n      }\\n    }\\n  }\\n```\\n\\nPhewwww... I know this has ended up as a bit of a brain dump but hopefully people will find it useful. At some point I\'ll try to put up the above solution on GitHub so people can grab it easily for themselves."},{"id":"using-pubsub-observer-pattern-to","metadata":{"permalink":"/using-pubsub-observer-pattern-to","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2012-03-17-using-pubsub-observer-pattern-to/index.md","source":"@site/blog/2012-03-17-using-pubsub-observer-pattern-to/index.md","title":"Using the PubSub / Observer pattern to emulate constructor chaining without cluttering up global scope","description":"Pass objects between JavaScript files using PubSub interface to achieve code reusability without global scope pollution. No prototypes needed.","date":"2012-03-17T00:00:00.000Z","tags":[{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."}],"readingTime":5.48,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"using-pubsub-observer-pattern-to","title":"Using the PubSub / Observer pattern to emulate constructor chaining without cluttering up global scope","authors":"johnnyreilly","tags":["javascript"],"hide_table_of_contents":false,"description":"Pass objects between JavaScript files using PubSub interface to achieve code reusability without global scope pollution. No prototypes needed."},"unlisted":false,"prevItem":{"title":"WCF - moving from Config to Code, a simple WCF service harness (plus implementing your own Authorization)","permalink":"/wcf-moving-from-config-to-code-simple"},"nextItem":{"title":"Striving for (JavaScript) Convention","permalink":"/striving-for-javascript-convention"}},"content":"Yes the title of this post is \\\\***painfully**\\\\* verbose. Sorry about that. Couple of questions for you: - Have you ever liked the way you can have base classes in C# which can then be inherited and subclassed <u>in a different file / class</u>\\n\\n\x3c!--truncate--\x3e\\n\\n?\\n\\n- Have you ever thought; gosh it\'d be nice to do something like that in JavaScript...\\n- Have you then looked at JavaScripts prototypical inheritance and thought \\"right.... I\'m sure it\'s possible but this going to end up like [War and Peace](http://en.wikipedia.org/wiki/War_and_Peace)\\"\\n- Have you then subsequently thought \\"and hold on a minute... even if I did implement this using the prototype and split things between different files / modules wouldn\'t I have to pollute the global scope to achieve that? And wouldn\'t that mean that my code was exposed to the vagaries of any other scripts on the page? Hmmm...\\"\\n- [Men! Are you skinny? Do bullies kick sand in your face?](http://www.thrillingdetective.com/eyes/oxford.html) (Just wanted to see if you were still paying attention...)\\n\\n## The Problem\\n\\nWell, the above thoughts occurred to me just recently. I had a situation where I was working on an MVC project and needed to build up quite large objects within JavaScript representing various models. The models in question were already implemented on the server side using classes and made extensive use of inheritance because many of the properties were shared between the various models. That is to say we would have models which were implemented through the use of a class inheriting a base class which in turn inherits a further base class. With me? Good. Perhaps I can make it a little clearer with an example. Here are my 3 classes. First BaseReilly.cs:\\n\\n```cs\\npublic class BaseReilly\\n{\\n    public string LastName { get; set; }\\n\\n        public BaseReilly()\\n        {\\n            LastName = \\"Reilly\\";\\n        }\\n    }\\n```\\n\\nNext BoyReilly.cs (which inherits from BaseReilly):\\n\\n```cs\\npublic class BoyReilly : BaseReilly\\n{\\n    public string Sex { get; set; }\\n\\n    public BoyReilly()\\n        : base()\\n    {\\n        Sex = \\"It is a manchild\\";\\n    }\\n}\\n```\\n\\nAnd finally JohnReilly.cs (which inherits from BoyReilly which in turn inherits from BaseReilly):\\n\\n```cs\\npublic class JohnReilly : BoyReilly\\n{\\n    public string FirstName { get; set; }\\n\\n    public JohnReilly()\\n        : base()\\n    {\\n        FirstName = \\"John\\";\\n    }\\n}\\n```\\n\\nUsing the above I can create myself my very own \\"JohnReilly\\" like so:\\n\\n```cs\\nvar johnReilly = new JohnReilly();\\n```\\n\\nAnd it will look like this:\\n\\n![](CSharp-version-of-JohnReilly.webp)\\n\\nI was looking to implement something similar on the client and within JavaScript. I was keen to ensure [code reuse](http://en.wikipedia.org/wiki/Code_reuse). And my inclination to keep things simple made me wary of making use of the [prototype](http://bonsaiden.github.com/JavaScript-Garden/#object.prototype). It is undoubtedly powerful but I don\'t think even the mighty [Crockford](http://javascript.crockford.com/prototypal.html) would consider it \\"simple\\". Also I had the reservation of exposing my object to the global scope. So what to do? I had an idea.... ## The Big Idea\\n\\nFor a while I\'ve been making use explicit use of the [Observer pattern](http://en.wikipedia.org/wiki/Observer_pattern) in my JavaScript, better known by most as the publish/subscribe (or \\"PubSub\\") pattern. There\'s a million JavaScript libraries that facilitate this and after some experimentation I finally settled on [higgins](https://github.com/phiggins42/bloody-jquery-plugins/blob/master/pubsub.js) implementation as it\'s simple and I saw a [JSPerf](http://jsperf.com/pubsubjs-vs-jquery-custom-events/11) which demonstrated it as either the fastest or second fastest in class. Up until now my main use for it had been to facilitate loosely coupled GUI interactions. If I wanted one component on the screen to influence anothers behaviour I simply needed to get the first component to publish out the relevant events and the second to subscribe to these self-same events. One of the handy things about publishing out events this way is that with them you can also include data. This data can be useful when driving the response in the subscribers. However, it occurred to me that it would be equally possible to pass an object when publishing an event. \\\\*\\\\*<u>And the subscribers could enrich that object with data as they saw fit.</u>\\n\\n\\\\*\\\\* Now this struck me as a pretty useful approach. It\'s not rock solid secure as it\'s always possible that someone could subscribe to your events and get access to your object as you published out. However, that\'s pretty unlikely to happen accidentally; certainly far less likely than someone else\'s global object clashing with your global object. ## What might this look like in practice?\\n\\nSo this is what it ended up looking like when I turned my 3 classes into JavaScript files / modules. First BaseReilly.js:\\n\\n```js\\n$(function () {\\n  $.subscribe(\'PubSub.Inheritance.Emulation\', function (obj) {\\n    obj.LastName = \'Reilly\';\\n  });\\n});\\n```\\n\\nNext BoyReilly.js:\\n\\n```js\\n$(function () {\\n  $.subscribe(\'PubSub.Inheritance.Emulation\', function (obj) {\\n    obj.Sex = \'It is a manchild\';\\n  });\\n});\\n```\\n\\nAnd finally JohnReilly.js:\\n\\n```js\\n$(function () {\\n  $.subscribe(\'PubSub.Inheritance.Emulation\', function (obj) {\\n    obj.FirstName = \'John\';\\n  });\\n});\\n```\\n\\nIf the above scripts have been included in a page I can create myself my very own \\"JohnReilly\\" in JavaScript like so:\\n\\n```js\\nvar oJohnReilly = {}; //Empty object\\n\\n$.publish(\'PubSub.Inheritance.Emulation\', [oJohnReilly]); //Empty object \\"published\\" so it can be enriched by subscribers\\n\\nconsole.log(JSON.stringify(oJohnReilly)); //Show me this thing you call \\"JohnReilly\\"\\n```\\n\\nAnd it will look like this:\\n\\n![](JavaScript-version-of-JohnReilly.webp)\\n\\nAnd it works. Obviously the example I\'ve given above it somewhat naive - in reality my object properties are driven by GUI components rather than hard-coded. But I hope this illustrates the point. This technique allows you to simply share functionality between different JavaScript files and so keep your codebase tight. I certainly wouldn\'t recommend it for all circumstances but when you\'re doing something as simple as building up an object to be used to pass data around (as I am) then it works very well indeed. ## A Final Thought on Script Ordering\\n\\nA final thing that maybe worth mentioning is script ordering. The order in which functions are called is driven by the order in which subscriptions are made. In my example I was registering the scripts in this order:\\n\\n```html\\n<script src=\\"/Scripts/PubSubInheritanceDemo/BaseReilly.js\\"><\/script>\\n<script src=\\"/Scripts/PubSubInheritanceDemo/BoyReilly.js\\"><\/script>\\n<script src=\\"/Scripts/PubSubInheritanceDemo/JohnReilly.js\\"<>/script>\\n```\\n\\nSo when my event was published out the functions in the above JS files would be called in this order: 1. BaseReilly.js 2. BoyReilly.js 3. JohnReilly.js\\n\\nIf you were so inclined you could use this to emulate inheritance in behaviour. Eg you could set a property in `BaseReilly.js` which was subsequently overridden in `JohnReilly.js` or `BoyReilly.js` if you so desired. I\'m not doing that myself but it occurred as a possibility. ## PS\\n\\nIf you\'re interested in learning more about JavaScript stabs at inheritance you could do far worse than look at Bob Inces in depth StackOverflow [answer](http://stackoverflow.com/a/1598077/761388).\\n\\n```\\n\\n```"},{"id":"striving-for-javascript-convention","metadata":{"permalink":"/striving-for-javascript-convention","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2012-03-12-striving-for-javascript-convention/index.md","source":"@site/blog/2012-03-12-striving-for-javascript-convention/index.md","title":"Striving for (JavaScript) Convention","description":"Visual Studio 11 beta resolved issues. John has moved away from Hungarian Notation but retained using \\"$\\" as a prefix for jQuery objects.","date":"2012-03-12T00:00:00.000Z","tags":[{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."}],"readingTime":9.625,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"striving-for-javascript-convention","title":"Striving for (JavaScript) Convention","authors":"johnnyreilly","tags":["javascript"],"hide_table_of_contents":false,"description":"Visual Studio 11 beta resolved issues. John has moved away from Hungarian Notation but retained using \\"$\\" as a prefix for jQuery objects."},"unlisted":false,"prevItem":{"title":"Using the PubSub / Observer pattern to emulate constructor chaining without cluttering up global scope","permalink":"/using-pubsub-observer-pattern-to"},"nextItem":{"title":"jQuery Unobtrusive Remote Validation","permalink":"/jquery-unobtrusive-remote-validation"}},"content":"## Update\\n\\nThe speed of change makes fools of us all. Since I originally wrote this post all of 3 weeks ago Visual Studio 11 beta has been released and the issues I was seeking to solve have pretty much been resolved by the new innovations found therein. It\'s nicely detailed in [@carlbergenhem](http://www.twitter.com/carlbergenhem)\'s blog post: [My Top 5 Visual Studio 11 Designer Improvements for ASP.NET 4.5 Development](https://blogs.telerik.com/blogs/posts/12-03-26/my-top-5-visual-studio-11-designer-improvements-for-asp-net-4-5-development.aspx). I\'ve left the post in place below but much of what I said (particularly with regard to Hungarian Notation) I\'ve now moved away from. That was originally my intention anyway so that\'s no bad thing. The one HN artefact that I\'ve held onto is using \\"$\\" as a prefix for jQuery objects. I think that still makes sense. I would have written my first line of JavaScript in probably 2000. It probably looked something like this: `alert(\'hello world\')`. I know. Classy. As I\'ve mentioned before it was around 2010 before I took JavaScript in any way seriously. Certainly it was then when I started to actively learn the language. Because up until this point I\'d been studiously avoiding writing any JavaScript at all I\'d never really given thought to forms and conventions. When I wrote any JavaScript I just used the same style and approaches as I used in my main development language (of C#). By and large I have been following the .net naming conventions which are ably explained by Pete Brown [here](http://10rem.net/articles/net-naming-conventions-and-programming-standards---best-practices). Over time I have started to move away from this approach. Without a deliberate intention to do so I have found myself adopting a different style for my JavaScript code as compared with anything else I write. I wouldn\'t go so far as to say I\'m completely happy with the style I\'m currently using. But I find it more helpful than not and thought it might be worth talking about. It was really 2 things that started me down the road of \\"rolling my own\\" convention: dynamic typing and the lack of safety nets. Let\'s take each in turn....\\n\\n\x3c!--truncate--\x3e\\n\\n### 1\\\\. Dynamic typing\\n\\nHaving grown up (in a development sense) using compiled and strongly-typed languages I was used to the IDE making it pretty clear what was what through friendly tooltips and the like:\\n\\n![](IDE.png)\\n\\nJavaScript is loosely / dynamically typed ([occasionally called \\"untyped\\" but let\'s not go there](http://stackoverflow.com/questions/9154388/does-untyped-also-mean-dynamically-typed-in-the-academic-cs-world)). This means that the IDE can\'t easily determine what\'s what. So no tooltips for you sunshine. ### 2\\\\. The lack of safety nets / running with scissors\\n\\nNow I\'ve come to love it but what I realised pretty quickly when getting into JavaScript was this: you are running with scissors. If you\'re not careful and you don\'t take precautions it can bloody quickly. If I\'m writing C# I have a lot of safety nets. Not the least of which is \\"does it compile\\"? If I declare an integer and then subsequently try to assign a string value to it <u>it won\'t let me</u>\\n\\n. But JavaScript is forgiving. Some would say too forgiving. Let\'s do something mad:\\n\\n```js\\nvar iAmANumber = 77;\\n\\nconsole.log(iAmANumber); //Logs a number\\n\\niAmANumber = \\"It\'s a string\\";\\n\\nconsole.log(iAmANumber); //Logs a string\\n\\niAmANumber = {\\n  description: \'I am an object\',\\n};\\n\\nconsole.log(iAmANumber); //Logs an object\\n\\niAmANumber = function (myVariable) {\\n  console.log(myVariable);\\n};\\n\\nconsole.log(iAmANumber); //Logs a function\\niAmANumber(\'I am not a number, I am a free man!\'); //Calls a function which performs a log\\n```\\n\\nNow if I were to attempt something similar in C# fuggedaboudit but JavaScript; no I\'m romping home free:\\n\\n![](Mad-Stuff.webp)\\n\\nNow I\'m not saying that you should ever do the above, and thinking about it I can\'t think of a situation where you\'d want to (suggestions on a postcard). But the point is it\'s possible. And because it\'s possible to do this deliberately, it\'s doubly possible to do this accidentally. My point is this: it\'s easy to make bugs in JavaScript. ## What ~~Katy~~ Johnny Did Next\\n\\nI\'d started making more and more extensive use of JavaScript. I was beginning to move in the direction of using the [single-page application](http://en.wikipedia.org/wiki/Single-page_application) approach (_although more in the sense of giving application style complexity to individual pages rather than ensuring that entire applications ended up in a single page_). This meant that whereas in the past I\'d had the occasional 2 lines of JavaScript I now had a multitude of functions which were all interacting in response to user input. All these functions would contain a number of different variables. As well as this I was making use of jQuery for both Ajax purposes and to smooth out the DOM inconsistencies between various browsers. This only added to the mix as variables in one of my functions could be any one of the following: - a number\\n\\n- a string\\n- a boolean\\n- a date\\n- an object\\n- an array\\n- a function\\n- a jQuery object - not strictly a distinct JavaScript type obviously but treated pretty much as one in the sense that it has a particular functions / properties etc associated with it\\n\\nAs I started doing this sort of work I made no changes to my coding style. Wherever possible I did \\\\***exactly**\\\\* what I would have been doing in C# in JavaScript. And it worked fine. Until.... Okay there is no \\"until\\" as such, it did work fine. But what I found was that I would do a piece of work, check it into source control, get users to test it, release the work into Production and promptly move onto the next thing. However, a little way down the line there would be a request to add a new feature or perhaps a bug was reported and I\'d find myself back looking at the code. And, as is often the case, despite the comments I would realise that it wasn\'t particularly clear why something worked in the way it did. (Happily it\'s not just me that has this experience, paranoia has lead me to ask many a fellow developer and they have confessed to similar) When it came to bug hunting in particular I found myself cursing the lack of friendly tooltips and the like. Each time I wanted to look at a variable I\'d find myself tracking back through the function, looking for the initial use of the variable to determine the type. Then I\'d be tracking forward through the function for each subsequent use to ensure that it conformed. Distressingly, I would find examples of where it looked like I\'d forgotten the type of the variable towards the end of a function (for which I can only, regrettably, blame myself). Most commonly I would have a situation like this:\\n\\n```js\\nvar tableCell = $(\'#ItIsMostDefinitelyATableCell\'); //I jest ;-)\\n\\n/* ...THERE WOULD BE SOME CODE DOING SOMETHING HERE... */\\n\\ntableCell.className = \'makeMeProminent\'; //Oh dear - not good.\\n```\\n\\nYou see what happened above? I forgot I had a jQuery object and instead treated it like it was a standard DOM element. Oh dear. ## Spinning my own safety net; Hungarian style\\n\\nAfter I\'d experienced a few of the situations described above I decided that steps needed to be taken to minimise the risk of this. In this case, I decided that \\"steps\\" meant [Hungarian notation](http://en.wikipedia.org/wiki/Hungarian_notation). I know. I bet you\'re wincing right now. For those of you that don\'t remember HN was pretty much the standard way of coding at one point (although at the point that I started coding professionally it had already started to decline). It was adopted in simpler times long before the modern IDE\'s that tell you what each variable is became the norm. Back when you couldn\'t be sure of the types you were dealing with. In short, kind of like my situation with JavaScript right now. There\'s not much to it. By and large HN simply means having a lowercase prefix of 1-3 characters on all your variables indicating type. It doesn\'t solve all your problems. It doesn\'t guarantee to stop bugs. But because each instance of the variables use implicitly indicates it\'s type it makes bugs more glaringly obvious. This means when writing code I\'m less likely to misuse a variable (eg `iNum = \\"JIKJ\\"`) because part of my brain would be bellowing: \\"that just looks wrong... pay better attention lad!\\". Likewise, if I\'m scanning through some JavaScript and searching for a bug then this can make it more obvious. Here\'s some examples of different types of variables declared using the style I have adopted:\\n\\n```js\\nvar iInteger = 4;\\nvar dDecimal = 10.5;\\nvar sString = \'I am a string\';\\nvar bBoolean = true;\\nvar dteDate = new Date();\\nvar oObject = {\\n  description: \'I am an object\',\\n};\\nvar aArray = [34, 77];\\nvar fnFunction = function () {\\n  //Do something\\n};\\nvar $jQueryObject = $(\'#ItIsMostDefinitelyATableCell\');\\n```\\n\\nSome of you have read this and thought \\"hold on a minute... JavaScript doesn\'t have integers / decimals etc\\". You\'re quite right. My style is not specifically stating the type of a variable. More it is seeking to provide a guide on how a variable should be used. JavaScript does not have integers. But oftentimes I\'ll be using a number variable which i will only ever want to treat as an integer. And so I\'ll name it accordingly. ## Spinning a better safety net; DOJO style\\n\\nI would be the first to say that alternative approaches are available. And here\'s one I recently happened upon that I rather like the look of: look 2/3rds down at the parameters section of [the DOJO styleguide](http://dojotoolkit.org/community/styleGuide) Essentially they advise specifying parameter types through the use of prefixed comments. See the examples below:\\n\\n```js\\nfunction(/*String*/ foo, /*int*/ bar)...\\n```\\n\\nor\\n\\n```js\\nfunction(/_String?_/ foo, /_int_/ bar, /_String[]?_/ baz)...\\n```\\n\\nI really rather like this approach and I\'m thinking about starting to adopt it. It\'s not possible in Hungarian Notation to be so clear about the purpose of a variable. At least not without starting to adopt all kinds of kooky conventions that take in all the possible permutations of variable types. And if you did that you\'d really be defeating yourself anyway as it would simply reduce the clarity of your code and make bugs more likely. ## Spinning a better safety net; unit tests\\n\\nDespite being quite used to writing unit tests for all my server-side code I have not yet fully embraced unit testing on the client. Partly I\'ve been holding back because of the variety of JavaScript testing frameworks available. I wasn\'t sure which to start with. But given that it is so easy to introduce bugs into JavaScript I have come to the conclusion that it\'s better to have some tests in place rather than none. Time to embrace the new. ## Conclusion\\n\\nI\'ve found using Hungarian Notation useful whilst working in JavaScript. Not everyone will feel the same and I think that\'s fair enough; within reason I think it\'s generally a good idea to go with what you find useful. However, I am giving genuine consideration to moving to the DOJO style and moving back to my more standard camel-cased variable names instead of Hungarian Notation. Particularly since I strive to keep my functions short with the view that ideally each should 1 thing well. Keep it simple etc... And so in a perfect world the situation of forgetting a variables purpose shouldn\'t really arise... I think once I\'ve got up and running with JavaScript unit tests I may make that move. Hungarian Notation may have proved to be just a stop-gap measure until better techniques were employed..."},{"id":"jquery-unobtrusive-remote-validation","metadata":{"permalink":"/jquery-unobtrusive-remote-validation","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2012-03-03-jquery-unobtrusive-remote-validation/index.md","source":"@site/blog/2012-03-03-jquery-unobtrusive-remote-validation/index.md","title":"jQuery Unobtrusive Remote Validation","description":"Learn how to do remote validation with unobtrusive data attributes in ASP.NET MVC. Block validation and re-evaluate it with this hacky solution.","date":"2012-03-03T00:00:00.000Z","tags":[{"inline":false,"label":"jQuery","permalink":"/tags/jquery","description":"The jQuery library."}],"readingTime":9,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"jquery-unobtrusive-remote-validation","title":"jQuery Unobtrusive Remote Validation","authors":"johnnyreilly","tags":["jquery"],"hide_table_of_contents":false,"description":"Learn how to do remote validation with unobtrusive data attributes in ASP.NET MVC. Block validation and re-evaluate it with this hacky solution."},"unlisted":false,"prevItem":{"title":"Striving for (JavaScript) Convention","permalink":"/striving-for-javascript-convention"},"nextItem":{"title":"The Joy of JSON","permalink":"/joy-of-json"}},"content":"Just recently I have been particularly needing to make use of remote / server-side validation in my ASP.NET MVC application and found that the unobtrusive way of using this seemed to be rather inadequately documented (of course it\'s possible that it\'s well documented and I just didn\'t find the resources). Anyway I\'ve rambled on much longer than I intended to in this post so here\'s the TL;DR:\\n\\n\x3c!--truncate--\x3e\\n\\n- You \\\\***can**\\\\* use remote validation driven by unobtrusive data attributes\\n- Using remote validation you can supply \\\\***multiple**\\\\* parameters to be evaluated\\n- It is possible to block validation and force it to be re-evaluted - although using a slightly hacky method which I document here. For what it\'s worth I acknowledge up front that this is \\\\***not**\\\\* an ideal solution but it does seem to work. I really hope there is a better solution out there and if anyone knows about it then please get in contact and let me know.\\n\\nOff we go... So, jQuery unobtrusive validation; clearly the new cool right?\\n\\nI\'d never been particularly happy with the validation that I had traditionally been using with ASP.NET classic. It worked... but it always seemed a little... clunky? I realise that\'s not the most well expressed concern. For basic scenarios it seemed fine, but I have recollections of going through some pain as soon as I stepped outside of the basic form validation. Certainly when it came to validating custom controls that we had developed it never seemed entirely straightforward to get validation to play nice.\\n\\nBased on this I was keen to try something new and the opportunity presented itself when we started integrating MVC into our classic WebForms app. (By the way if you didn\'t know that MVC and ASP.NET could live together in perfect harmony, well, they can! And a good explanation on how to achieve it is offered by Colin Farr [here](http://www.britishdeveloper.co.uk/2011/05/convert-web-forms-mvc3-how-to.html).)\\n\\nJ\xf6rn Zaefferer came out with the [jQuery validation plug-in](http://bassistance.de/jquery-plugins/jquery-plugin-validation/) way back in 2006. And mighty fine it is too. Microsoft (gor\' bless \'em) really brought something new to the jQuery validation party when they came out with their unobtrusive javascript validation library along with MVC 3. What this library does, in short, is allows for jQuery validation to be driven by `data-val-*` attributes alone as long as the [jquery.validate.js](http://ajax.aspnetcdn.com/ajax/jquery.validate/1.9/jquery.validate.js) and [jquery.validate.unobtrusive.js](http://ajax.aspnetcdn.com/ajax/mvc/3.0/jquery.validate.unobtrusive.js) libraries are included in the screen (I have assumed you are already including jQuery). I know; powerful stuff!\\n\\nA good explanation of unobtrusive validation is given by Brad Wilson [here](http://bradwilson.typepad.com/blog/2010/10/mvc3-unobtrusive-validation.html).\\n\\nAnyway, to my point: what about remote validation? That is to say, what about validation which needs to go back to the server to perform the necessary tests? Well I struggled to find decent examples of how to use this. Those that I did find seemed to universally be php examples; not so useful for an ASP.NET user. Also, when I did root out an ASP.NET example there seemed to be a fundamental flaw. Namely, if remote validation hadn\'t been triggered and completed successfully then the submit could fire anyway. This seems to be down to the asynchronous nature of the test; ie because it is \\\\***not**\\\\* synchronous there is no \\"block\\" to the submit. And out of the box with unobtrusive validation there seems no way to make this synchronous. I could of course wire this up manually and simply side-step the restrictions of unobtrusive validation but that wasn\'t what I wanted.\\n\\n\\\\*\\\\*\\\\*Your mission John, should you decide to accept it, is this: <u>block the submit until remote validation has completed successfully</u>\\n\\n. As always, should you or any of your I.M. Force be caught or killed, the Secretary will disavow any knowledge of your actions.\\\\*\\\\*\\\\*\\n\\nSo that\'s what I wanted to do. Make it act like it\'s synchronous even though it\'s asynchronous. Bit horrible but I had a deadline to meet and so this is my pragmatic solution. There may be better alternatives but this worked for me.\\n\\nFirst of all the HTML:\\n\\n```html\\n<form\\n  action=\\"/Dummy/ValidationDemo.mvc/SaveUser\\"\\n  id=\\"ValidationForm\\"\\n  method=\\"post\\"\\n>\\n  First name:\\n  <input\\n    data-val=\\"true\\"\\n    data-val-required=\\"First Name required\\"\\n    id=\\"FirstName\\"\\n    name=\\"FirstName\\"\\n    type=\\"text\\"\\n    value=\\"\\"\\n  />\\n\\n  Last name:\\n  <input\\n    data-val=\\"true\\"\\n    data-val-required=\\"Last Name required\\"\\n    id=\\"LastName\\"\\n    name=\\"LastName\\"\\n    type=\\"text\\"\\n    value=\\"\\"\\n  />\\n\\n  User name:\\n  <input\\n    id=\\"UserName\\"\\n    name=\\"UserName\\"\\n    type=\\"text\\"\\n    value=\\"\\"\\n    data-val=\\"true\\"\\n    data-val-required=\\"You must enter a user name before we can validate it remotely\\"\\n    data-val-remote=\\"&amp;#39;UserNameInput&amp;#39; is invalid.\\"\\n    data-val-remote-additionalfields=\\"*.FirstName,*.LastName\\"\\n    data-val-remote-url=\\"/Dummy/ValidationDemo/IsUserNameValid\\"\\n  />\\n\\n  <input\\n    id=\\"SaveMyDataButton\\"\\n    name=\\"SaveMyDataButton\\"\\n    type=\\"button\\"\\n    value=\\"Click to Save\\"\\n  />\\n</form>\\n```\\n\\nI should mention that on my actual page (a cshtml partial view) the HTML for the inputs is generated by the use of the [InputExtensions.TextBoxFor](http://msdn.microsoft.com/en-us/library/system.web.mvc.html.inputextensions.textboxfor.aspx) method which is lovely. It takes your model and using the validation attributes that decorate your models properties it generates the relevant jQuery unobtrusive validation data attributes so you don\'t have to do it manually.\\n\\nBut for the purposes of seeing what\'s \\"under the bonnet\\" I thought it would be more useful to post the raw HTML so it\'s entirely clear what is being used. Also there doesn\'t appear to be a good way (that I\'ve yet seen) for automatically generating Remote validation data attributes in the way that I\'ve found works. So I\'m manually specifying the `data-val-remote-*` attributes using the htmlAttributes parameter of the TextBoxFor ([using \\"\\\\_\\" to replace \\"-\\"](http://stackoverflow.com/questions/4844001/html5-data-with-asp-net-mvc-textboxfor-html-attributes) obviously).\\n\\nNext the JavaScript that performs the validation:\\n\\n```js\\n$(document).ready(function () {\\n  var intervalId = null,\\n    //\\n    // DECLARE FUNCTION EXPRESSIONS\\n    //\\n\\n    //======================================================\\n    // function that triggers update when remote validation\\n    // completes successfully\\n    //======================================================\\n    pendingValidationComplete = function () {\\n      var i, errorList, errorListForUsers;\\n      var $ValidationForm = $(\'#ValidationForm\');\\n      if ($ValidationForm.data(\'validator\').pendingRequest === 0) {\\n        clearInterval(intervalId);\\n\\n        //Force validation to present to user\\n        //(this will *not* retrigger remote validation)\\n        if ($ValidationForm.valid()) {\\n          alert(\'Validation has succeeded - you can now submit\');\\n        } else {\\n          //Validation failed!\\n          errorList = $ValidationForm.data(\'validator\').errorList;\\n          errorListForUsers = [];\\n          for (i = 0; i < errorList.length; i++) {\\n            errorListForUsers.push(errorList[i].message);\\n          }\\n\\n          alert(errorListForUsers.join(\'\\\\r\\\\n\'));\\n        }\\n      }\\n    },\\n    //======================================================\\n    // Trigger validation\\n    //======================================================\\n    triggerValidation = function (evt) {\\n      //Removed cached values where remote is concerned\\n      // so remote validation is retriggered\\n      $(\'#UserName\').removeData(\'previousValue\');\\n\\n      //Trigger validation\\n      $(\'#ValidationForm\').valid();\\n\\n      //Setup interval which will evaluate validation\\n      //(this approach because of remote validation)\\n      intervalId = setInterval(pendingValidationComplete, 50);\\n    };\\n\\n  //\\n  //ASSIGN EVENT HANDLERS\\n  //\\n  $(\'#SaveMyDataButton\').click(triggerValidation);\\n});\\n```\\n\\nAnd finally the Controller:\\n\\n```cs\\npublic JsonResult IsUserNameValid(string UserName,\\n                                  string FirstName,\\n                                  string LastName)\\n{\\n  var userNameIsUnique = IsUserNameUnique(UserName);\\n  if (userNameIsUnique)\\n    return Json(true, JsonRequestBehavior.AllowGet);\\n  else\\n    return Json(string.Format(\\n                  \\"{0} is already taken I\'m afraid {1} {2}\\",\\n                  UserName, FirstName, LastName),\\n                JsonRequestBehavior.AllowGet);\\n}\\n\\nprivate bool IsUserNameUnique(string potentialUserName)\\n{\\n  return false;\\n}\\n```\\n\\nSo what happens here exactly? Well it\'s like this:\\n\\n1. The user enters their first name, last name and desired user name and hits the \\"Click to Save\\" button.\\n2. This forces validation by first removing any cached validation values stored in `previousValue` data attribute and then triggering the `valid` method. Disclaimer: I KNOW THIS IS A LITTLE HACKY. I would have expected there would be some way in the API to manually re-force validation. Unless I\'ve missed something there doesn\'t appear to be. ([And the good citizens of Stack Overflow would seem to concur.](http://stackoverflow.com/a/3797712/761388)) I would guess that the underlying assumption is that if nothing has changed on the client then that\'s all that matters. Clearly that\'s invalid for our remote example given that a username could be \\"claimed\\" at any time; eg in between people first entering their username (when validation should have fired automatically) and actually submitting the form. Anyway - this approach seems to get us round the problem.\\n3. When validation takes place the IsUserNameValid action / method on our controller will be called. It\'s important to note that I have set up a method that takes 3 inputs; UserName, which is supplied by default as the UserName input is the one which is decorated with remote validation attributes as well as the 2 extra inputs of FirstName and LastName. In the example I\'ve given I don\'t actually need these extra attributes. I\'m doing this because I know that I have situations in remote validation where I \\\\***need**\\\\* to supply multiple inputs and so essentially I did it here as a proof of concept. The addition of these 2 extra inputs was achieved through the use of the `data-val-remote-additionalfields` attribute. When searching for documentation about this I found absolutely <u>none</u>\\n\\n. I assume there is some out there - if anyone knows then I\'d very pleased to learn about it. I only learned about it in the end by finding an example of someone using this out in the great wide world and understanding how to use it based on their example. To understand how the `data-val-remote-additionalfields` attribute works you can look at jquery.validate.unobtrusive.js. If you\'re just looking to get up and running then I found that the following works: `data-val-remote-additionalfields=\\"*.FirstName,*.LastName\\"` You will notice that: - Each parameter is supplied in the format _\\\\*.[InputName]_ and inputs are delimited by \\",\\"\'s - Name is a <u>required</u>\\n\\nattribute for an input if you wish it to be evaluated with unobtrusive validation. (Completely obvious statement I realise; I\'m writing that sentence more for my benefit than yours) - Finally, our validation always fails. That\'s deliberate - I just wanted to be clear on the approach used to get remote unobtrusive validation with extra parameters up and running. 4. Using `setInterval` we intend to trigger the `pendingValidationComplete` function to check if remote validation has completed every 50ms - again I try to avoid setInterval wherever possible but this seems to be the most sensible solution in this case. 5. When the remote request finally completes (ie when `pendingRequest` has a value of 0) then we can safely proceed on the basis of our validation results. In the example above I\'m simply alerting to the screen based on my results; this is \\\\***not**\\\\* advised for any finished work; I\'m just using this mechanism here to demonstrate the principle.\\n\\nValidation in action:\\n\\n![](validation-screenshot2.webp)\\n\\nWell I\'ve gone on for far too long but I am happy to have an approach that does what I need. It does feel like a slightly hacky solution and I expect that there is a better approach for this that I\'m not aware of. As much as anything else I\'ve written this post in the hope that someone who knows this better approach will set me straight. In summary, this works. But if you\'re aware of a better solution then please do get in contact - I\'d love to know!\\n\\n**PS:**Just in case you\'re in the process of initially getting up and running with unobtrusive validation I\'ve listed below a couple of general helpful bits of config etc:\\n\\nThe following setting is essential for Application_Start in Global.asax.cs:\\n\\n```cs\\nDataAnnotationsModelValidatorProvider.AddImplicitRequiredAttributeForValueTypes = false;\\n```\\n\\nThe following settings should be used in your Web.Config:\\n\\n```xml\\n<appSettings>\\n  <add key=\\"ClientValidationEnabled\\" value=\\"true\\" />\\n  <add key=\\"UnobtrusiveJavaScriptEnabled\\" value=\\"true \\"/>\\n</appSettings>\\n```\\n\\nMy example used the following scripts:\\n\\n```html\\n<script src=\\"Scripts/jquery-1.7.1.js\\"><\/script>\\n<script src=\\"Scripts/jquery.validate.js\\"><\/script>\\n<script src=\\"Scripts/jquery.validate.unobtrusive.js\\"><\/script>\\n<script src=\\"Scripts/ValidationDemo.js\\"><\/script>\\n```"},{"id":"joy-of-json","metadata":{"permalink":"/joy-of-json","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2012-02-23-joy-of-json/index.md","source":"@site/blog/2012-02-23-joy-of-json/index.md","title":"The Joy of JSON","description":"JSON is a lightweight data format that helps create and read JavaScript objects. This article traces its discovery and explains its usefulness.","date":"2012-02-23T00:00:00.000Z","tags":[{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."}],"readingTime":3.55,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"joy-of-json","title":"The Joy of JSON","authors":"johnnyreilly","tags":["javascript"],"hide_table_of_contents":false,"description":"JSON is a lightweight data format that helps create and read JavaScript objects. This article traces its discovery and explains its usefulness."},"unlisted":false,"prevItem":{"title":"jQuery Unobtrusive Remote Validation","permalink":"/jquery-unobtrusive-remote-validation"},"nextItem":{"title":"WCF Transport Windows authentication using NetTcpBinding in an Intranet environment","permalink":"/wcf-transport-windows-authentication"}},"content":"So back to JSON. For those of you that don\'t know JSON stands for JavaScript Object Notation and is lightweight text based data interchange format. Rather than quote other people verbatim you can find thorough explanations of JSON here: - [Introducing JSON](http://www.json.org/)\\n\\n\x3c!--truncate--\x3e\\n\\n- [JSON in Javascript](http://www.json.org/js.html)\\n\\nAs mentioned in my previous [post on Ajax](../2012-02-05-potted-history-of-using-ajax-on/index.md) I came upon JSON quite by accident and was actually using it for some time without having any idea. But let\'s pull back a bit. Let\'s start with the JavaScript Object Literal. Some years ago I came upon this article by Christan Heilmann about the JavaScript Object Literal which had been published all the way back in 2006: [Show love to the JavaScript Object Literal](http://christianheilmann.com/2006/02/16/show-love-to-the-object-literal/) Now when I read this it was a revelation to me. I hadn\'t really used JavaScript objects a great deal at this point (yes I am one of those people that started using JavaScript without actually learning the thing) and when I had used them is was through the `var obj = new Object()` pattern (as that\'s the only approach I knew). So it was wonderful to discover that instead of the needlessly verbose:\\n\\n```js\\nvar myCar = new Object();\\nmyCar.wheels = 4;\\nmyCar.colour = \'blue\';\\n```\\n\\nI could simply use the much more concise object literal syntax to declare an object instead:\\n\\n```js\\nvar myCar = { wheels: 4, colour: \'blue\' };\\n```\\n\\nLovely. Henceforth I adopted this approach in my code as I\'m generally a believer that brevity is best. It was sometime later that I happened upon JSON (when I started looking into [jqGrid](../2012-01-14-jqgrid-its-just-far-better-grid/index.md)). Basically I was looking to pass complex data structures backward and forward to the server and, as far as I knew, there was no way to achieve this simply in JavaScript. I was expecting that I would have to manually serialise and deserialise (yes dammit I will use the English spellings!) objects when ever I wanted to do this sort of thing. However, I was reading the the fantastic Dave Ward\'s [Encosia](http://encosia.com/) blog which on this occasion was talking about the [troubles of UpdatePanels](http://encosia.com/why-aspnet-ajax-updatepanels-are-dangerous/) (a subject close to my heart by the way) and more interestingly the use of PageMethods in ASP.NET. This is what he said that made me prick up my ears: _\\"Page methods allow ASP.NET AJAX pages to directly execute a page\u2019s static methods, using JSON (JavaScript Object Notation). JSON is basically a minimalistic version of SOAP, which is perfectly suited for light weight communication between client and server.\\"_ JSON is a lightweight SOAP eh? I\'ve used SOAP. I wonder if I could use this.... To my complete surprise, and may I say delight, I discovered that a wonderful fellow called Douglas Crockford, he of [JavaScript, The Good Parts](http://www.amazon.co.uk/JavaScript-Good-Parts-Douglas-Crockford/dp/0596517742) fame had quietly come up with JSON some time ago. JSON, from my perspective, turned out to be a simple way to turn an object into a string and then from a string back into an object. So simple that it consists of 2 methods on a JSON object: - JSON.stringify(myObject) - take an object and make me a JSON string. (and by the way isn\'t \\"stringify\\" just the loveliest method name ever?)\\n\\n- JSON.parse(myJSONString) - take a JSON string and make me an object\\n\\nLet me illustrate the above method names using the myCar example from earlier:\\n\\n```js\\nvar myCar = { wheels: 4, colour: \'blue\' };\\n// myCar is an object\\n\\nvar myCarJSON = JSON.stringify(myCar);\\n//myCarJSON will look like this: \'{\\"wheels\\":4,\\"colour\\":\\"blue\\"}\'\\n\\nvar anotherCarMadeFromMyJSON = JSON.parse(myCarJSON);\\n//anotherCarMadeFromMyJSON will be a brand new \\"car\\" object\\n```\\n\\nI\'ve also demonstrated this using the Chrome Console:\\n\\n![](Using-JSON.webp)\\n\\nCrockford initially invented/discovered JSON himself and wrote a little helper library which provided a JSON object to be used by all and sundry. This can be found here: [JSON on GitHub](https://github.com/douglascrockford/JSON-js) Because JSON was so clearly wonderful, glorious and useful it ended up becoming a part of the EcmaScript 5 spec (in fact it\'s worth reading the brilliant [John Resig\'s blog post](http://ejohn.org/blog/ecmascript-5-strict-mode-json-and-more/) on this). This has lead to JSON being offered [natively in browsers](http://en.wikipedia.org/wiki/JSON#Native_encoding_and_decoding_in_browsers) for quite some time. However, for those of us (and I am one alas) still supporting IE 6 and the like we still have Crockfords JSON2.js to fall back on.\\n\\n```\\n\\n```"},{"id":"wcf-transport-windows-authentication","metadata":{"permalink":"/wcf-transport-windows-authentication","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2012-02-15-wcf-transport-windows-authentication/index.md","source":"@site/blog/2012-02-15-wcf-transport-windows-authentication/index.md","title":"WCF Transport Windows authentication using NetTcpBinding in an Intranet environment","description":"John explains authentication issues when migrating from .NET Remoting to WCF. The post highlights a security feature and suggests solutions.","date":"2012-02-15T00:00:00.000Z","tags":[{"inline":false,"label":"Authentication and Authorization","permalink":"/tags/auth","description":"Matters related to identity and permissioning"}],"readingTime":4.545,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"wcf-transport-windows-authentication","title":"WCF Transport Windows authentication using NetTcpBinding in an Intranet environment","authors":"johnnyreilly","tags":["auth"],"hide_table_of_contents":false,"description":"John explains authentication issues when migrating from .NET Remoting to WCF. The post highlights a security feature and suggests solutions."},"unlisted":false,"prevItem":{"title":"The Joy of JSON","permalink":"/joy-of-json"},"nextItem":{"title":"A Potted History of using Ajax (on the Microsoft Stack of Love)","permalink":"/potted-history-of-using-ajax-on"}},"content":"## Update\\n\\n\x3c!--truncate--\x3e\\n\\nSince I wrote this initial post I\'ve taken thinks on a bit further. [Take a look at this post to see what I mean.](../2012-03-22-wcf-moving-from-config-to-code-simple/index.md) I know I said I\'d write about JSON this time. I will get to that but not this time. This time WCF authentication quirks. I\'ve been working on a project that uses .NET Remoting to have a single central point to which web applications and Windows services can call into. This is used in an intranet environment and all the websites and Windows services were hosted on the same single server along with our .NET Remoting Windows service. (They could quite easily have been on different servers but there was no need in this case.) It was decided to \\"embrace the new\\" by migrating this .NET Remoting project over to WCF. The plan wasn\'t to do anything revolutionary, just to move from one approach to the other as easily as possible. I found the following useful article on MSDN: [http://msdn.microsoft.com/en-us/library/aa730857%28v=vs.80%29.aspx](http://msdn.microsoft.com/en-us/library/aa730857%28v=vs.80%29.aspx) This particular article was helpful and following the steps enclosed I was quickly up and running with a basic WCF service hosted in a Windows service. It was at this point I started thinking about security. The existing .NET Remoting approach had no security in place. This wasn\'t ideal but also probably wasn\'t the worry you might think. It was hosted in an intranet environment and hence not so exposed to the rigours of the Wild Wild Web. However, since I was looking at WCF I thought it would be a good opportunity to get some basic security in place. This generally pleases auditors. I opted to use [Windows Transport authentication](http://msdn.microsoft.com/en-us/library/ms733089.aspx) as this seemed pretty appropriate for an intranet environment. The idea being that we\'d authenticate with Windows for an account in our domain. After headbutting Windows for some time I managed to get a successful client call going from the website running on my development machine to the (separate) development server that was hosting our WCF Window service using Transport Windows authentication. However, when deploying the website to the development server I discovered we would experience the following error when the website attempted to call the WCF service (on the same server).\\n\\n```\\nEvent Type: Failure Audit\\nEvent Source: Security\\nEvent Category: Logon/Logoff\\nEvent ID: 537\\nDate: 15/02/2012\\nTime: 16:32:04\\nUser: NT AUTHORITY\\\\SYSTEM\\nComputer: MINE999\\nDescription:\\nLogon Failure:\\nReason: An error occurred during logon\\nLogon Type: 3\\nLogon Process: ^\\nAuthentication Package: NTLM\\nStatus code: 0xC000006D\\n```\\n\\nNot terribly helpful. At the end of the day it seemed we were suffering from a security \\"feature\\" introduced by Microsoft to prevent services calling services on the same box with a fully qualified name. An explanation of this can be found here: [http://developers.de/blogs/damir_dobric/archive/2009/08/28/authentication-problems-by-using-of-ntlm.aspx](http://developers.de/blogs/damir_dobric/archive/2009/08/28/authentication-problems-by-using-of-ntlm.aspx) Using method 1 in the enclosed link I initially worked round this by amending the registry and rebooting the server: [http://support.microsoft.com/kb/887993](http://support.microsoft.com/kb/887993) This was not a fantastic solution. Fortunately I subsequently found a better one but since the resources on the web are \\\\***ATROCIOUS**\\\\* on this point I thought I should take the time to note down the full explanation since otherwise it\'ll be lost in the mists of time. Here we go: The equivalent security to the previous .NET Remoting solution in WCF was to use this config setting on client and service:\\n\\n```xml\\n<security mode=\\"None\\" />\\n```\\n\\nAs I\'ve said, this is an intranet environment and so having this \\"none\\" security setting in place is made less worrying by the fact that the network itself is secured. But obviously this is not ideal and unlikely to be audit compliant. To use Windows security you need this netTcpBinding config setting on client and service:\\n\\n```xml\\n<security mode=\\"Transport\\">\\n<transport clientCredentialType=\\"Windows\\" />\\n</security>\\n```\\n\\nTo call the service with this setting in place you will need to be an authenticated Windows user. (Or at the very least impersonating one - but you knew that.) **NOW FOR THE MOST IMPORTANT BIT.....** The endpoint addresses \\\\***must**\\\\* be \\"localhost\\" for _both_ client and service when both are deployed to the same server. If this is not the case then you will suffer from the aforementioned security \\"feature\\" which will provide you with unhelpful \\"the server has rejected the client credentials\\" messages and \\\\***nothing**\\\\* else. **OK FINISHED - MOVE ALONG NOW... NOTHING MORE TO SEE HERE** With WCF Windows Transport authentication in place you can interrogate the calling user id within the service methods by simply evaluating ServiceSecurityContext.Current.PrimaryIdentity.Name (which will be something like \\"myDomain\\\\myUserName\\"). So we you wanted to, we could have a simple step which evaluated if the calling user is on the \\"approved\\" / \\"authorised\\" list. I\'m sure this could be made more sophisticated by using groups etc I guess - though I haven\'t investigated it further as yet. In fact, I suspect Microsoft may have something even more sophisticated still available for use which I\'m unaware of - if anyone knows a simple explanation of this then please do let me know! In closing, I do think Microsoft could work on providing more helpful error messages than \\"the server has rejected the client credentials\\". Going by what I read as I researched this error many people seem to have struggled much as I did before eventually bailing out and ended up chancing it by turning security off in their applications. Clearly it is not desirable to have people so confused by errors that they give up and settle for a less secure solution."},{"id":"potted-history-of-using-ajax-on","metadata":{"permalink":"/potted-history-of-using-ajax-on","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2012-02-05-potted-history-of-using-ajax-on/index.md","source":"@site/blog/2012-02-05-potted-history-of-using-ajax-on/index.md","title":"A Potted History of using Ajax (on the Microsoft Stack of Love)","description":"Discovering Ajax and JSON transformed Johns approach to programming by lifting limitations and improving performance.","date":"2012-02-05T00:00:00.000Z","tags":[{"inline":false,"label":"jQuery","permalink":"/tags/jquery","description":"The jQuery library."},{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."}],"readingTime":7.24,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"potted-history-of-using-ajax-on","title":"A Potted History of using Ajax (on the Microsoft Stack of Love)","authors":"johnnyreilly","tags":["jquery","javascript"],"hide_table_of_contents":false,"description":"Discovering Ajax and JSON transformed Johns approach to programming by lifting limitations and improving performance."},"unlisted":false,"prevItem":{"title":"WCF Transport Windows authentication using NetTcpBinding in an Intranet environment","permalink":"/wcf-transport-windows-authentication"},"nextItem":{"title":"JavaScript - getting to know the beast...","permalink":"/javascript-getting-to-know-beast"}},"content":"This post originally started out as an explanation of JSON. However as I wrote this quickly got abandoned in favour of writing about how I came to use JSON in the first place - which was through the use of Ajax. Having written a goodly amount I\'ve now decided to move the actual JSON stuff into another post since I think Ajax is probably worth thinking about by itself rather than as an aside. So let me start at the beginning and explain how I came to use Ajax in the first place (this may take some time so please bear with me). In late 2004 I first started working on a project which I was to remain involved with (on and off) for a very long time indeed. The project was part financial reporting system and part sales incentivisation tool; it was used internally in the investment bank in which I was working. The project had been in existence for a number of years and had a web front end which at that point would been built in a combination of HTML, JavaScript, classic ASP and with a Visual Basic 6.0 back end. One of the reasons I had been brought on to the project was to help \\".Net-ify\\" the thing and migrate it to ASP.NET and C#. I digress. The interesting thing about this app was that there were actually some quite advanced things being done with it (despite the classic ASP / VB). The users could enter trades into the system which represented actual trades that had been entered into a trading system elsewhere in the organisation. These trades would be assigned a reporting value which would be based on their various attributes. (Stay with me people this will get more interesting I \\\\***promise**\\\\*.) The calculation of the reporting value was quite an in depth process and needed to be performed server-side. However, the users had decreed that it wasn\'t acceptable to do a full postback to the server to perform this calculation; they wanted it done \\"on-the-fly\\". Now if you asked me at the time I\'d have said \\"can\'t be done\\". Fortunately the other people working on the project then weren\'t nearly so defeatist. Instead they went away and found Microsoft\'s [webservice.htc](http://msdn.microsoft.com/en-us/library/ie/ms531033%28v=vs.85%29.aspx) library. For those of you that don\'t know this was a JavaScript library that Microsoft came up with to enable the access of Web Services on the client. Given that it was designed to work with IE 5 I suspect it was created between 1999-2001 (but I\'m not certain about that). Now it came as a revelation to me but this was a JavaScript library that talked to our web services through the medium of XML. In short it was my first encounter with anything remotely [Ajax](<http://en.wikipedia.org/wiki/Ajax_(programming)>)\\\\-y. It was exciting! However, the possibilities of what we could do didn\'t actually become apparent to me for some years. It\'s worth saying that the way we were using webservice.htc was exceedingly simplistic and rather than investigating further I took the limited ways we were using it as indications of the limitations of Ajax and / or webservice.htc. So for a long time I thought the following: - The only way to pass multiple arguments to a web service was to package up arguments into a single string with delimiters which you could [split](<http://en.wikipedia.org/wiki/Comparison_of_programming_languages_(string_functions)#split>) and unpackage as your first step on the server.\\n\\n\x3c!--truncate--\x3e\\n\\n- The only valid return type was a single string. And so if you wanted to return a number of numeric values (as we did) the only way to do this was to package up return values into a very long string with delimiters in and (you guessed it!) [split](<http://en.wikipedia.org/wiki/Comparison_of_programming_languages_(string_functions)#split>) and unpackage as your first step on the client.\\n- The only thing that you could (or would want to) send back and forth between client and server was XML\\n\\nSo to recap, I\'m now aware that it\'s possible for JavaScript to interact with the server through the use of web services. It\'s possible, but ugly, not that quick and requires an awful lot of manual serialization / deserialization operations. It\'s clearly powerful but not much fun at all. And that\'s where I left it for a number of years. Let\'s fade to black... It\'s now 2007 and Microsoft have released ASP.NET Ajax, the details of which are well explained in this [article](http://msdn.microsoft.com/en-us/magazine/cc163499.aspx) (which I have only recently discovered). Now I\'m always interested in \\"the new\\" and so I was naturally interested in this. Just to be completely upfront about this I should confess that when I first discovered ASP.NET Ajax I didn\'t clock the power of it at all. Initially I just switched over from using webservice.htc to ASP.NET Ajax. This alone gave us a \\\\***massive**\\\\* performance improvement (I know it was massive since we actually received a \\"well done\\" email from our users which is testament to the difference it was making to their experience of the system). But we were still performing our manual serialisation / deserialisation of values on the client and the server. ie. Using Ajax was now much faster but still not too much fun. Let\'s jump forward in time again to around 2010 to the point in time when I was discovering jQuery and that JavaScript wasn\'t actually evil. It\'s not unusual for me to play around with \\"what if\\" scenarios in my code, just to see what might might be possible. Sometimes I discover things. So it was with JSON. We had a web service in the system that allowed us to look up a counterparty (ie a bank account) with an identifier. Once we looked it up we packaged up the counterparty details (eg name, location etc) into a big long string with delimiters and sent it back to client. One day I decided to change the return type on the web service from a string to the actual counterparty class. So we went from something like this:\\n\\n```cs\\n[WebService(Namespace = \\"http://tempuri.org/\\")]\\n[WebServiceBinding(ConformsTo = WsiProfiles.BasicProfile1_1)]\\n[ScriptService]\\npublic class CounterpartyWebService : System.Web.Services.WebService\\n{\\n  [WebMethod]\\n  public string GetCounterparty(string parameters)\\n  {\\n    string[] aParameters = parameters.Split(\\"|\\");\\n    int counterpartyId = int.Parse(aParameters[0]);\\n    bool includeLocation = (aParameters[1] == \\"1\\");\\n    Counterparty counterparty = \\\\_counterpartyDb\\n    .GetCounterparty(counterpartyId);\\n\\n        string returnValue = counterparty.Id +\\n                          \\"|\\" + counterparty.Name +\\n                          (includeLocation\\n                            ? \\"|\\" + counterparty.Location\\n                            : \\"\\");\\n\\n        return returnValue;\\n  }\\n}\\n```\\n\\nTo something like this:\\n\\n```cs\\n[WebMethod]\\npublic Counterparty GetCounterparty(string parameters)\\n{\\n  string[] aParameters = parameters.Split(\\"|\\");\\n  int counterpartyId = int.Parse(aParameters[0]);\\n  bool includeLocation = (aParameters[1] == \\"1\\");\\n  Counterparty counterparty = _counterpartyDb\\n    .GetCounterparty(counterpartyId);\\n\\n  return counterparty;\\n}\\n```\\n\\nI genuinely expected that this was just going to break. It didn\'t. Suddenly on the client I\'m sat there with a full blown object that looks just like the object I had on the server.\\n\\n**WHAT STRANGE MAGIC COULD THIS BE??????????** Certain that I\'d discovered witchcraft I decided to try something else. What would happen if I changed the signature on the method so it received individual parameters and passed my individual parameters to the web service instead of packaging them up into a string? I tried this:\\n\\n```cs\\n[WebMethod]\\npublic Counterparty GetCounterparty(int counterpartyId, bool includeLocation)\\n{\\n  Counterparty counterparty = \\\\_counterpartyDb\\n  .GetCounterparty(counterpartyId);\\n\\n  return counterparty;\\n}\\n```\\n\\nAnd it worked! **[IT WORKED!!!!!!!!!!!!!!!!!!!!!](http://www.youtube.com/watch?v=N_dWpCy8rdc&feature=related)** (And yes I know I wasn\'t actually using the includeLocation parameter - but the point was it was being passed to the server and I could have used it if I\'d wanted to.) I couldn\'t believe it. For **years** I\'d been using Ajax and without **any** idea of the power available to me. The ignorance! The stupidity of the man! To my complete surprise it turned out that: - Ajax could be quick! ASP.NET Ajax was lightening fast when compared to webservice.htc\\n\\n- You could send multiple arguments to a web service without all that packaging nonsense\\n- You could return complex objects without the need for packaging it all up yourself.\\n\\nEssentially the source of all this goodness was the magic of JSON. I wouldn\'t really come to comprehend this until I moved away from using the ASP.NET Ajax client libraries in favour of using the [jQuery.ajax](http://api.jquery.com/jQuery.ajax/) functionality. (Yes, having mostly rattled on about using webservice.htc and ASP.NET Ajax I should clarify that I have now forsaken both for jQuery as I find it more powerful and more configurable - but it\'s the journey that counts I guess!) It\'s abysmal that I didn\'t discover the power of Ajax sooner but the difference this discovery made to me was immense. Approaches that I would have dismissed or shied away from previously because of the amount of \\"plumbing\\" involved now became easy. This massively contributed to my [programmer joy](http://www.hanselman.com/blog/HanselminutesPodcast260NETAPIDesignThatOptimizesForProgrammerJoyWithJonathanCarter.aspx)! Next time I promise I\'ll aim to actually get onto JSON."},{"id":"javascript-getting-to-know-beast","metadata":{"permalink":"/javascript-getting-to-know-beast","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2012-01-30-javascript-getting-to-know-beast/index.md","source":"@site/blog/2012-01-30-javascript-getting-to-know-beast/index.md","title":"JavaScript - getting to know the beast...","description":"Discovering jQuery and advice from Elijah Manor and Douglas Crockford changes Johns initial dislike of JavaScripts quirks.","date":"2012-01-30T00:00:00.000Z","tags":[{"inline":false,"label":"JavaScript","permalink":"/tags/javascript","description":"The JavaScript programming language."},{"inline":false,"label":"C#","permalink":"/tags/csharp","description":"The C# programming language."}],"readingTime":5.81,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"javascript-getting-to-know-beast","title":"JavaScript - getting to know the beast...","authors":"johnnyreilly","tags":["javascript","c#"],"hide_table_of_contents":false,"description":"Discovering jQuery and advice from Elijah Manor and Douglas Crockford changes Johns initial dislike of JavaScripts quirks."},"unlisted":false,"prevItem":{"title":"A Potted History of using Ajax (on the Microsoft Stack of Love)","permalink":"/potted-history-of-using-ajax-on"},"nextItem":{"title":"What on earth is jQuery?  And why should I care?","permalink":"/what-on-earth-is-jquery-and-why-should"}},"content":"So it\'s 2010 and I\'ve started using jQuery. jQuery is a JavaScript library. This means that I\'m writing JavaScript... Gulp! I should say that at this point in time I **hated** JavaScript (I have mentioned this previously). But what I know now is that I barely understood the language at all. All the JavaScript I knew was the result of copying and pasting after I\'d hit \\"view source\\". I don\'t feel too bad about this - not because my ignorance was laudable but because I certainly wasn\'t alone in this. It seems that up until recently hardly anyone knew anything about JavaScript. It puzzles me now that I thought this was okay. I suppose like many people I didn\'t think JavaScript was capable of much and hence felt time spent researching it would be wasted. Just to illustrate where I was then, here is 2009 John\'s idea of some pretty \\"advanced\\" JavaScript:\\n\\n\x3c!--truncate--\x3e\\n\\n```html\\nfunction GiveMeASum(iNum1, iNum2) { var dteDate = new Date(); var iTotal = iNum1\\n+ iNum2; return \\"This is your total: \\" + iTotal + \\", at this time: \\" +\\ndteDate.toString(); }\\n\\n<input type=\\"text\\" id=\\"Number1\\" value=\\"4\\" />\\n<input type=\\"text\\" id=\\"Number2\\" value=\\"6\\" />\\n<input\\n  type=\\"button\\"\\n  value=\\"Click Me To Add\\"\\n  onclick=\\"alert(GiveMeASum(parseInt(document.getElementById(Number1).value, 10), parseInt(document.getElementById(Number2).value, 10)))\\"\\n/>\\n```\\n\\nI know - I\'m not to proud of it... Certainly if it was a horse you\'d shoot it. Basically, at that point I knew the following: - JavaScript had functions (but I knew only one way to use them - see above)\\n\\n- It had some concept of numbers (but I had no idea of the type of numbers I was dealing with; integer / float / decimal / who knows?)\\n- It had some concept of strings\\n- It had a date object\\n\\nThis was about the limit of my knowledge. If I was right, and that\'s all there was to JavaScript then my evaluation of it as utter rubbish would have been accurate. I was wrong. SOOOOOOOOOOOO WRONG! I first realised how wrong I was when I opened up the jQuery source to have a read. Put simply I had \\\\***no**\\\\* idea what I was looking at. For a while I wondered if I was actually looking at JavaScript; the code was so different to what I was expecting that for a goodly period I considered jQuery to be some kind of strange black magic; written in a language I did not understand. I was half right. jQuery wasn\'t black magic. But it was written in a language I didn\'t understand; namely JavaScript. :-( Here beginneth the lessons... I started casting around looking for information about JavaScript. Before very long I discovered one [Elijah Manor](http://www.elijahmanor.com/) who had helpfully done a number of talks and blog posts directed at C# developers (which I was) about JavaScript. My man! - [How good C# habits can encourage bad JavaScript habits part 1](http://enterprisejquery.com/2010/10/how-good-c-habits-can-encourage-bad-javascript-habits-part-1/)\\n\\n- [How good C# habits can encourage bad JavaScript habits part 2](http://enterprisejquery.com/2010/10/how-good-c-habits-can-encourage-bad-javascript-habits-part-2/)\\n- [How good C# habits can encourage bad JavaScript habits part 3](http://enterprisejquery.com/2010/10/how-good-c-habits-can-encourage-bad-javascript-habits-part-3/)\\n- [Video of Elijah Manor talking through the above material](https://blogs.msdn.com/b/ukmsdn/archive/2011/06/10/javascript-for-the-c-developer.aspx)\\n\\nFor me this was all massively helpful. In my development life so far I had only ever dealt with strongly typed, compiled \\"classical\\" languages. I had little to no experience of functional, dynamic and loosely typed languages (essentially what JavaScript is). Elijahs work opened up my eyes to some of the massive differences that exist. He also pointed me in the direction of the (never boring) Doug Crockford, author of the best programming book I have ever purchased: [JavaScript: The Good Parts](http://www.amazon.co.uk/JavaScript-Good-Parts-Douglas-Crockford/dp/0596517742). Who could not like a book about JavaScript which starts each chapter with a quote from Shakespeare and still comes in at only a 100 pages? It\'s also worth watching the man in person as he\'s a thoroughly engaging presence. There\'s loads of videos of him out there but this one is pretty good: [Douglas Crockford: The JavaScript Programming Language](http://www.youtube.com/watch?v=v2ifWcnQs6M). I don\'t want to waste your time by attempting to rehash what these guys have done already. I think it\'s always best to go to the source so I\'d advise you to check them out for yourselves. That said it\'s probably worth summarising some of the main points I took away from them (you can find better explanations of all of these through looking at their posts): 1. JavaScript has objects but has no classes. Instead it has (what I still consider to be) the weirdest type of inheritance going: prototypical inheritance. 2. JavaScript has the simplest and loveliest way of creating a new object out there; the \\"JavaScript Object Literal\\". Using this we can simply `var myCar = { wheels: 4, colour: \\"blue\\" }` and ladies and gents we have ourselves a car! (object) 3. In JavaScript functions are [first class objects](http://en.wikipedia.org/wiki/First-class_function). This means functions can be assigned to variables (as easily as you\'d assign a string to a variable) and crucially you can pass them as parameters to a function and pass them back as a return type. Herein lies power! 4. JavaScript has 6 possible values (false, null, undefined, empty strings, 0 and NaN) which it evaluates as false. These are known as the \\"false-y\\" values. It\'s a bit weird but on the plus side this can lead to some nicely terse code. 5. To perform comparisons in JavaScript you should avoid == and != and instead use === and !==. Before I discovered this I had been using == and != and then regularly puzzling over some truly odd behaviour. Small though it may sound, this may be the most important discovery of the lot as it was this that lead to me actually \\\\***trusting**\\\\* the language. Prior to this I vaguely thought I was picking up on some kind of bug in the JavaScript language which I plain didn\'t understand. (After all, in any sane universe should this really evaluate to true?: `0 == \\"\\"`) 6. Finally JavaScript has function scope rather than block scope. Interestingly it \\"hoists\\" variable declaration to the top of each function which can lead to some very surprising behaviour if you don\'t realise what is happening.\\n\\nI now realise that JavaScript is a fantastic language because of it\'s flexibility. It is also a deeply flawed language; in part due to it\'s unreasonably forgiving nature (you haven\'t finished your line with a semi-colon; that\'s okay - I can see you meant to so I\'ll stick one in / you haven\'t declared your variable; not a problem I won\'t tell you but I\'ll create a new variable stick it in global scope and off we go etc). It is without question the easiest language with which to create a proper dogs breakfast. To get the best out of JavaScript we need to understand the quirks of the language and we need good patterns. If you\'re interested in getting to grips with it I really advise you to check out the Elijah and Dougs work - it really helped me."},{"id":"what-on-earth-is-jquery-and-why-should","metadata":{"permalink":"/what-on-earth-is-jquery-and-why-should","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2012-01-24-what-on-earth-is-jquery-and-why-should/index.md","source":"@site/blog/2012-01-24-what-on-earth-is-jquery-and-why-should/index.md","title":"What on earth is jQuery?  And why should I care?","description":"What is jQuery? Discover the truth about the JavaScript library thats taking the web development world by storm - its simply brilliant!","date":"2012-01-24T00:00:00.000Z","tags":[{"inline":false,"label":"jQuery","permalink":"/tags/jquery","description":"The jQuery library."}],"readingTime":4.53,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"what-on-earth-is-jquery-and-why-should","title":"What on earth is jQuery?  And why should I care?","authors":"johnnyreilly","tags":["jquery"],"hide_table_of_contents":false,"description":"What is jQuery? Discover the truth about the JavaScript library thats taking the web development world by storm - its simply brilliant!"},"unlisted":false,"prevItem":{"title":"JavaScript - getting to know the beast...","permalink":"/javascript-getting-to-know-beast"},"nextItem":{"title":"jqGrid - it\'s just a far better grid","permalink":"/jqgrid-its-just-far-better-grid"}},"content":"What on earth is jQuery? What\'s a jQuery plugin?\\n\\n\x3c!--truncate--\x3e\\n\\nThese were the questions I was asking myself shortly after discovering that jqGrid was a \\"jQuery plugin\\". I\'d been vaguely aware of the phrase \\"jQuery\\" being increasingly mentioned on various techical websites since about 2009. But for some reason I\'d felt no urge to find out what it was. I seem to remember that I read the name \\"jQuery\\" and jumped to the perfectly logical (in my head) conclusion that this must be a Java SQL engine of some sort. (After all \\"j\\" as a prefix to anything so far had generally been Java and \\"Query\\" just rang of databases to me.) Clearly I was wrong - life\'s full of surprises.\\n\\nI soon discovered that, contrary to expectations, jQuery had nothing to do with Java \\\\***and**\\\\* nothing to do with databases either. It was in fact a JavaScript library written by the amazing [John Resig](http://ejohn.org/about/). At the time I had no love for JavaScript. I now realise I knew nearly nothing about it but my feeling was that JavaScript was awful - evil even. However, given JavaScripts ubiquity in the world of web it seemed to be a necessary evil.\\n\\nI took a look at the [jQuery website](http://jquery.com/) and after reading round a bit I noticed that it could be used for [Ajax](http://en.wikipedia.org/wiki/Ajax_%28programming%29) operations. This lead to me reaching the (incorrect) conclusion that jQuery was basically an alternative to the [Microsoft Ajax library](http://en.wikipedia.org/wiki/ASP.NET_AJAX#Microsoft_Ajax_Library) which we were already using to call various Web Services. But I remained frankly suspicious of jQuery. What was the point of this library? Why did it exist?\\n\\nI read the the [blog](http://weblogs.asp.net/scottgu/archive/2008/09/28/jquery-and-microsoft.aspx) by Scott Gu announcing Microsoft was going to start shipping jQuery with Visual Studio. The Great Gu trusted it. Therefore, I figured, it must be okay... Right?\\n\\nThe thing was, I was quite happy with the Microsoft Ajax library. I was familiar with it. It worked. Why switch? I saw the various operations Scott Gu was doing to divs on the screen using jQuery. I didn\'t want to do anything like that at all. As I said; I had no love for JavaScript - I viewed it as C#\'s simple-minded idiot cousin. My unofficial motto when doing web stuff was \\"wherever possible, do it on the server\\".\\n\\nI think I would have ignored jQuery entirely but for the fact of jqGrid. If I wanted to use jqGrid I had to use jQuery as well. In the end I decided I\'d allow it house room just for the sake of jqGrid and I\'d just ignore it apart from that. And that\'s how it was for a while.\\n\\nThen I had an epiphany. Okay - that\'s overplaying it. What actually happened was I realised that something we were doing elsewhere could be done faster and easier with jQuery. It\'s something so ridiculously feeble that I feel vaguely embarrassed sharing it. Anyway.\\n\\nSo, you know the css hover behaviour is only implemented for anchor tags in IE6? No? Well read this [Stack Overflow](http://stackoverflow.com/questions/36605/ie-6-css-hover-non-anchor-tag) entry - it\'ll clarify. Well, the app that I was working on was an internal web application only used by people with the corporate installation of IE 6 on their desktops. And it was \\"terribly important\\" that buttons had hover behaviour. For reasons that now escape me we were doing this by manually adding inline onmouseover / onmouseout event handlers to each input button on the screen in turn in every page in the [Page_Load](http://msdn.microsoft.com/en-us/library/ms178472.aspx) event server side. I think we were aware it wasn\'t fantastic to have to wire up each button in turn. But it worked and as with so many development situations we had other pressures, other requirements to fulfil and other fish to fry - so we left it at that.\\n\\nAnd then it occurred to me... What about using the [jQuery class selector](http://api.jquery.com/class-selector/) in conjunction with the [jQuery hover event](http://api.jquery.com/hover/)? I could have one method that I called on a page which would wire up all of my hover behaviours in one fell swoop. I wouldn\'t need to do input-by-input wireups anymore! Hallelujah! This is what I did:\\n\\nThe buttons I would like to style:\\n\\n```html\\n<input type=\\"button\\" value=\\"I am a button\\" class=\\"itIsAButton\\" />\\n<input type=\\"button\\" value=\\"So am I\\" class=\\"itIsAButton\\" />\\n<input type=\\"button\\" value=\\"Me too\\" class=\\"itIsAButton\\" />\\n```\\n\\nMy CSS (filter, by the way, is just linear gradients in IE 6-9):\\n\\n```css\\n.itIsAButton {\\n  filter: progid:DXImageTransform.Microsoft.Gradient (GradientType=0,StartColorStr=\'#ededed\',EndColorStr=\'#cdcdcd\');\\n}\\n\\n.itIsAButton:hover, .itIsAButton_hover /* \\"_hover\\" is for IE6 */ {\\n  filter: progid:DXImageTransform.Microsoft.Gradient (GradientType=0,StartColorStr=\'#f6f6f6\',EndColorStr=\'#efefef\');\\n}\\n```\\n\\nMy jQuery:\\n\\n```js\\n$(document).ready(function () {\\n  //Add hover behaviour on picker buttons for IE6\\n  if ($.browser.msie && parseInt($.browser.version, 10) < 7) {\\n    var fnButtonHover = function (handlerInOut) {\\n      var $btn = $(this);\\n      var sOriginalClass = $btn.prop(\'class\');\\n\\n      if (handlerInOut.type === \'mouseenter\') {\\n        //If not already hovering class then apply it\\n        if (sOriginalClass.indexOf(\'_hover\') === -1) {\\n          $btn.prop(\'class\', sOriginalClass + \'_hover\');\\n        }\\n      } else if (handlerInOut.type === \'mouseleave\') {\\n        //If not already non-hovering class then apply it\\n        if (sOriginalClass.indexOf(\'_hover\') !== -1) {\\n          $btn.prop(\'class\', sOriginalClass.split(\'_\')[0]);\\n        }\\n      }\\n    };\\n\\n    $(\'.itIsAButton\').hover(fnButtonHover);\\n  }\\n});\\n```\\n\\nAnd it worked. I didn\'t really understand this much about this jQuery \\"thing\\" at that point but I could now see that it clearly had at least one use. I\'ve come to appreciate that jQuery is one of the best pieces of software I\'ve ever encountered. Over time I may go further into some of the good stuff of jQuery. It is, quite simply, brilliant."},{"id":"jqgrid-its-just-far-better-grid","metadata":{"permalink":"/jqgrid-its-just-far-better-grid","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2012-01-14-jqgrid-its-just-far-better-grid/index.md","source":"@site/blog/2012-01-14-jqgrid-its-just-far-better-grid/index.md","title":"jqGrid - it\'s just a far better grid","description":"jqGrid is a sleek & efficient grid component for ASP.NET projects. Its minimal data exchange between client and server impressed John no end.","date":"2012-01-14T00:00:00.000Z","tags":[{"inline":false,"label":"jQuery","permalink":"/tags/jquery","description":"The jQuery library."}],"readingTime":5.41,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"jqgrid-its-just-far-better-grid","title":"jqGrid - it\'s just a far better grid","authors":"johnnyreilly","tags":["jquery"],"hide_table_of_contents":false,"description":"jqGrid is a sleek & efficient grid component for ASP.NET projects. Its minimal data exchange between client and server impressed John no end."},"unlisted":false,"prevItem":{"title":"What on earth is jQuery?  And why should I care?","permalink":"/what-on-earth-is-jquery-and-why-should"},"nextItem":{"title":"Standing on the Shoulders of Giants...","permalink":"/standing-on-shoulders-of-giants"}},"content":"The year was 2010 (not really that long ago I know) and the project that I was working on was sorely in need of a new grid component. It was an [ASP.NET WebForms](http://www.asp.net/web-forms) project and for some time we\'d been using what was essentially a glorified [datagrid](http://msdn.microsoft.com/en-us/library/system.web.ui.webcontrols.datagrid.aspx) which had a few extra features implemented to allow us to change column order / columns displayed / copy contents to clipboard etc. Our grid worked perfectly fine - it gave us the functionality we needed. However, it looked pretty terrible, and had some \\"quirky\\" approaches in place for supporting IE and Firefox side by side. Also, at the time we were attempting to make our app seem new and exciting again for the users. The surprising truth is that users seem to be more impressed with a visual revamp than with new or amended functionality. So I was looking for something which would make them sit up and say \\"oooh - isn\'t it pretty!\\". Unfortunately the nature of the organisation I was working for was not one that lended itself to paying for components. They were occasionally willing to do that but the hoops that would have to be jumped through first, the forms that would need to be signed in triplicate by people that had nearly nothing to do with the project made that an unattractive prospect. So I began my search initially looking at the various open source offerings that were around. As a minimum I was looking for something that would do what our home-grown component did already (change column order / columns displayed / copy contents to clipboard etc) but hopefully in a \\"nicer\\" way. Also, I had long been unhappy with the fact that to get our current grid to render results we did a \\\\***full postback**\\\\* to the server and re-rendered the whole page. Pointless! Why should you need to do all this each time when you only wanted to refresh the data? Instead I was thinking about using an [Ajax](http://en.wikipedia.org/wiki/Ajax_%28programming%29) approach; a grid that could just get the data that it needed and render it to the client. This seemed to me a vastly \\"cleaner\\" solution - why update a whole screen when you only want to update a small part of it? Why not save yourself the trouble of having to ensure that all other screen controls are persisted just as you\'d like them after the postback? I also thought it was probably something that would scale better as it would massively reduce the amount of data moving backwards and forwards between client and server. No need for a full page life cycle on the server each time the grid refreshes. Just simple data travelling down the pipes of web. With the above criteria in mind I set out on my Google quest for a grid. Quite soon I found that there was a component out there which seemed to do all that I wanted and far more besides. It was called [jqGrid](http://www.trirand.com/blog/):\\n\\n![](jqgrid-in-all-its-glory.webp)\\n\\n\x3c!--truncate--\x3e\\n\\nOooh look at the goodness! It had both column re-ordering and column choosing built in!: This was a \\\\***very promising sign**\\\\*! Now it\'s time for me to demonstrate my ignorance. According to the website this grid component was a \\"jQuery plugin\\". At the time I read this I had no idea what jQuery was at all - let alone what a plugin for it was. Anyway, I don\'t want to get diverted so let\'s just say that reading this lead to me getting an urgent education about some of the client side aspects of the modern web that I had been previously unaware of. I digress. This component did exactly what I wanted in terms of just sending data down the pipe. jqGrid worked with a whole number of possible data sources; XML, Array but the most exciting for me was obviously [JSON](http://www.json.org/). Take a look a the grid rendered below and the JSON that powered it (all from a simple [GET](http://www.trirand.com/blog/jqgrid/server.php?q=2&_search=false&nd=1326531357333&rows=10&page=1&sidx=id&sord=desc) request):\\n\\n![](Check-out-the-JSON.webp)\\n\\nAs you can see from the above screenshot, the grid has populated itself using the results of a web request. The only information that has gone to the server are the relevant criteria to drive the search results. The only information that has come back from the server is the data needed to drive the grid. Simple. Beautiful. I loved it and I wanted to use it. So I did! I had to take a few steps that most people thinking about using a grid component probably wont need to. First of all I had to write an ASP.Net WebForms wrapper for jqGrid which could be implemented in a similar way to our current custom datagrid. This was because, until the users were convinced that the new grid was better than the old both had to co-exist in the project and the user would have the option to switch between the two. This WebForms wrapper plugged into our old school XML column definition files and translated them into JSON for the grid. It also took [datasets](http://msdn.microsoft.com/en-us/library/system.data.dataset.aspx) (which drove our old grid) and translated them into jqGrid-friendly JSON. I wanted to power the jqGrid using WebMethods on ASPX\'s. After a little digging I found [Dave Ward of Encosia\'s post](http://encosia.com/using-jquery-to-directly-call-aspnet-ajax-page-methods/) which made it very simple (and in line with this I switched over from [GET](http://en.wikipedia.org/wiki/GET_%28HTTP%29#Request_methods) requests to [POSTs](http://en.wikipedia.org/wiki/POST_%28HTTP%29)). Finally I wrote some custom javascript which added a button to jqGrid which, if clicked, would copy the contents of the jqGrid to the clipboard (this was the only bit of functionality that didn\'t appear to be implemented out of the box with jqGrid). I think I\'m going to leave it there for now but I just wanted to say that I think jqGrid is a fantastic component and it\'s certainly made my life better! It\'s: - well supported, there is lots on [StackOverflow](http://stackoverflow.com/questions/tagged/jqgrid) and the like about it\\n\\n- there are regular [releases / upgrades](http://www.trirand.com/blog/)\\n- there are good online [demonstrations](http://trirand.com/blog/jqgrid/jqgrid.html) and [documentation](http://www.trirand.com/jqgridwiki/doku.php)\\n\\nI think Tony Tomov (the man behind jqGrid) has come up with something truly brilliant. It\'s worth saying that the equally brilliant jQueryUI team are in the process of writing an official [jQuery UI grid component](http://wiki.jqueryui.com/w/page/34246941/Grid) which uses jqGrid as one of its inspirations. However, this is still a long way from even a \\"zero feature\\" release. In the meantime jqGrid is continuing to go from strength to strength and as such I heartily recommend it. Finally, you can take a look at jqGrid\'s source on [GitHub](https://github.com/tonytomov/jqGrid)."},{"id":"standing-on-shoulders-of-giants","metadata":{"permalink":"/standing-on-shoulders-of-giants","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2012-01-07-standing-on-shoulders-of-giants/index.md","source":"@site/blog/2012-01-07-standing-on-shoulders-of-giants/index.md","title":"Standing on the Shoulders of Giants...","description":"John starts a blog inspired by Scott Hanselman to share useful tools and techniques for web development.","date":"2012-01-07T00:00:00.000Z","tags":[],"readingTime":3.23,"hasTruncateMarker":true,"authors":[{"name":"John Reilly","title":"OSS Engineer - TypeScript, Azure, React, Node.js, .NET","url":"https://johnnyreilly.com/about","imageURL":"https://johnnyreilly.com/img/profile.jpg","key":"johnnyreilly","page":null}],"frontMatter":{"slug":"standing-on-shoulders-of-giants","title":"Standing on the Shoulders of Giants...","authors":"johnnyreilly","hide_table_of_contents":false,"description":"John starts a blog inspired by Scott Hanselman to share useful tools and techniques for web development."},"unlisted":false,"prevItem":{"title":"jqGrid - it\'s just a far better grid","permalink":"/jqgrid-its-just-far-better-grid"}},"content":"It started with Scott Hanselman. I had no particular plans to start a blog at all. However, I was reading Scott Hanselman\'s turn of the year [post](http://www.hanselman.com/blog/YourBlogIsTheEngineOfCommunity.aspx) and I was struck with an idea.\\n\\n\x3c!--truncate--\x3e\\n\\nFirst, let me give a little background about myself. I\'m a software developer. I\'ve been in the industry for coming up to 15 years. I started out professionally writing call centre software. I moved on to code in a variety of different industries from straight IT to marketing and, for the last 7 years, finance.\\n\\nThough I initially started out writing in Delphi I fast found myself moving toward the Microsoft \\"stack of love\\". I should say that this move was not because I instinctively liked Microsofts stuff (in fact in the beginning I actively disliked it - moving from Delphi 3.0 to Visual Studio 5 left me finding Microsoft\'s offering very much wanting). Rather it was pragmatic. I needed a job and at the time VB was a far more transferable skill than Delphi. What with the all encompassing [dot-com bubble](http://en.wikipedia.org/wiki/Dot-com_bubble) of the late 90\'s I soon found myself working in the webtastic world of classic ASP (weep) and VB server components (remember them?).\\n\\nThough things can improve - and in my opinion they really did when Microsoft coughed up the first furball of ASP.NET Beta in (I think) 2001. I grabbed on with both hands. Since that point I\'ve been earning my bread pretty much, though not exclusively, in the ASP.NET universe.\\n\\nThe one thing that might not be clear from the above curriculum vitae is this: **I AM A COMPLETE AMATEUR.** I mean this in both senses of the word:\\n\\n1. I have no formal training to speak of - I didn\'t do a computer sciences degree. In fact my first real coding experience was writing a program in [Locomotive Basic](http://en.wikipedia.org/wiki/Locomotive_BASIC) for my father on our humble Amstrad CPC.\\n2. That said, I love it. I find writing code an intellectually, emotionally, creatively satisfying act. And whilst I undoubtedly have less of the theoretical knowledge which most professional developers seem to have, I probably counter-balance that with a hunger to keep learning and keep trying new things. And since software never sits still that\'s probably just as well. Keep watching the horizon - there will be something coming over it! And it\'s worth saying, I have an instinct for developing which serves me pretty well. I\'m good at coming up with elegant and pragmatic solutions. Put simply: I\'m good at making code work.\\n\\nSo back to the point. In my daily work life, like any other developer, I am repeatedly called on to turn someones requirement into a reality. Very rarely do I achieve this on my own. Like most of us I\'m a dwarf standing on the shoulders of giants. There\'s a lot of people out there who come up with useful tools / components / plug-ins that make it possible for me to deliver much more than I would given my own abilities.\\n\\nSo that\'s what I want to do: I want to talk about the tools, components and techniques that I have found useful in the everyday working life of a developer. It\'s likely to be quite a \\"webby\\" blog as I probably find that the most interesting area of development at the moment.\\n\\nI don\'t know how often I will write but my plan is that when I do, each time I\'ll talk about something I\'ve found useful - why I found it useful, what problems it solved, what issues it still presented me with and so on. This is probably not going to be a \\"techie techie\\" blog. Rather a blog that deals with the situations that can confront a developer and how I\'ve responded to them. I hope you find it interesting. And if you don\'t; please keep it to yourself :-)"}]}}')}}]);