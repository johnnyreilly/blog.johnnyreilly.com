"use strict";(self.webpackChunkblog_johnnyreilly_com=self.webpackChunkblog_johnnyreilly_com||[]).push([[89450],{16029:function(e){e.exports=JSON.parse('{"blogPosts":[{"id":"/2022/04/16/type-annotations-strong-types-weakly-held","metadata":{"permalink":"/2022/04/16/type-annotations-strong-types-weakly-held","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2022-04-16-type-annotations-strong-types-weakly-held/index.md","source":"@site/blog/2022-04-16-type-annotations-strong-types-weakly-held/index.md","title":"Type annotations: strong types, weakly held","description":"Recently, a new ECMAScript proposal called \\"Type Annotations\\" (originally named \\"Types as Comments\\") was revealed. The purpose is to allow type annotations to be valid JavaScript syntax. Albeit syntax that is ignored by JavaScript engines. The proposal is being worked on by Gil Tayar, Daniel Rosenwasser, Romulo Cintra, Rob Palmer, and others. Many of these people are from TypeScript community - however this proposal intentionally does not exist to benefit TypeScript alone.","date":"2022-04-16T00:00:00.000Z","formattedDate":"April 16, 2022","tags":[{"label":"type-annotations","permalink":"/tags/type-annotations"},{"label":"types-as-comments","permalink":"/tags/types-as-comments"},{"label":"JSDoc","permalink":"/tags/js-doc"},{"label":"ECMAScript","permalink":"/tags/ecma-script"},{"label":"types","permalink":"/tags/types"},{"label":"proposal","permalink":"/tags/proposal"}],"readingTime":8.61,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Type annotations: strong types, weakly held","authors":"johnnyreilly","tags":["type-annotations","types-as-comments","JSDoc","ECMAScript","types","proposal"],"image":"./title-image.png","hide_table_of_contents":false},"nextItem":{"title":"ESLint your C# in VS Code with Roslyn Analyzers","permalink":"/2022/04/06/eslint-your-csharp-in-vs-code-with-roslyn-analyzers"}},"content":"Recently, a new ECMAScript proposal called [\\"Type Annotations\\"](https://github.com/tc39/proposal-type-annotations) (originally named [\\"Types as Comments\\"](https://github.com/giltayar/proposal-types-as-comments)) was revealed. The purpose is to allow type annotations to be valid JavaScript syntax. Albeit syntax that is ignored by JavaScript engines. The proposal is being worked on by Gil Tayar, Daniel Rosenwasser, Romulo Cintra, Rob Palmer, and others. Many of these people are from TypeScript community - however this proposal intentionally does not exist to benefit TypeScript alone.\\n\\nIt\'s a contentious topic. As a regular (and longtime) TypeScript user, here\'s a description of the proposal and some thoughts.\\n\\n![title image reading \\"Type annotations: strong types, weakly held\\" with the JavaScript logo](title-image.png)\\n\\n## What is the proposal?\\n\\nTypes annotations is a proposal which would allow for the inclusion of types in JavaScript code. Consider the following piece of TypeScript:\\n\\n```ts\\nconst theAnswer: number = 42;\\n```\\n\\nAt present, this is not valid JavaScript. If you try and run the above in a JavaScript engine you\'ll get an error. Types are not part of JavaScript syntax.\\n\\n![screenshot of `const theAnswer: number = 42;` entered into the Chrome devtools and responding with an error that says `Uncaught SyntaxError: Missing initializer in const declaration`](screenshot-types-in-the-chrome-console.png)\\n\\nInterestingly, it\'s already possible to store types within JavaScript through a standard known as JSDoc. [I\'ve written about how TypeScript and JSDoc connect before.](https://blog.logrocket.com/typescript-vs-jsdoc-javascript/), essentially the thing to note is that JSDoc amounts to storing type declarations in the context of JavaScript comments.\\n\\nIt\'s already possible to write our code sample in valid JavaScript expressing the types within JSDoc. It looks like this:\\n\\n```ts\\n/** @type {number} */\\nconst theAnswer = 42;\\n```\\n\\nThis works, but it took two lines of code instead of one. The proposal allows for types to be directly expressed; not written as comments. So rather than writing the JSDoc equivalent, imagine if JavaScript was happy with the following instead:\\n\\n```ts\\nconst theAnswer: number = 42;\\n```\\n\\nThat\'s what the proposal amounts to.\\n\\n## What isn\'t it?\\n\\nNow that we understand what the proposal is, let\'s consider what it isn\'t.\\n\\nTypes annotations isn\'t an endorsement of a particular type system. Furthermore, it is not type checking in the browser or type checking in Node.js.\\n\\nLet\'s consider each of these. There\'s a number of languages which allow us to type check JavaScript. TypeScript, Flow, Hegel and others all play in this space. They are all similar, but different. They have different syntax and they do different things.\\n\\nWhat they have in common, is the space where types live in their syntax or grammar. The proposal essentially says \\"hey we might have different approaches to describing types, but we agree about where the types ought to live - let\'s standardise that\\".\\n\\nThis is why the original proposal name of \\"types as comments\\" is instructive; these types would be ignored by JavaScript runtimes. The fact they would be ignored is an indication that no existing type system would be \\"anointed\\" by this proposal.\\n\\nConsider the following:\\n\\n```ts\\nconst theAnswer: gibberish = 42;\\n```\\n\\nThis is neither TypeScript or Flow. Both would complain about the above. JavaScript, if this proposal were adopted, would be entirely untroubled.\\n\\nTo reiterate: the proposal is not an endorsement of any given type system and it follows that there is no runtime type checking being introduced to JavaScript.\\n\\n## Why do this at all?\\n\\nIt\'s worth taking a look at [Daniel Rosenwasser](https://devblogs.microsoft.com/typescript/a-proposal-for-type-syntax-in-javascript/)\'s post where he announces the proposal. Daniel is part of the TypeScript team and one of champions of this proposal, along with [Rob Palmer](https://twitter.com/robpalmer2) at Bloomberg and [Romulo Cintra](https://twitter.com/romulocintra) at Igalia.\\n\\nHe says:\\n\\n> Today, you can create a .js file in your editor and start sprinkling in types in the form of JSDoc comments.\\n>\\n> ```js\\n> /**\\n>  * @param a {number}\\n>  * @param b {number}\\n>  */\\n> function add(a, b) {\\n>   return a + b;\\n> }\\n> ```\\n>\\n> Because these are just comments, they don\u2019t change how your code runs at all \u2013 they\u2019re just a form of documentation, but TypeScript uses them to give you a better JavaScript editing experience ... This feature makes it incredibly convenient to get some of the TypeScript experience without a build step, and you can use it for small scripts, basic web pages, server code in Node.js, etc.\\n>\\n> Still, you\u2019ll notice that this is a little verbose \u2013 we love how lightweight the inner-loop is for writing JavaScript, but we\u2019re missing how convenient TypeScript makes it to just write types.\\n>\\n> _So what if we had both?_\\n>\\n> What if we could have something like TypeScript syntax which was totally ignored \u2013 sort of like comments \u2013 in JavaScript.\\n>\\n> ```ts\\n> function add(a: number, b: number) {\\n>   return a + b;\\n> }\\n> ```\\n\\nWhat I take from this, is that JavaScript with types annotations, would be a more developer friendly JSDoc.\\n\\n## \\"It\'s the JSDoc I always wanted!\\"\\n\\nThis idea really resonates with me. I\'m a longtime user of JSDoc. Let me articulate why I find it useful.\\n\\nWhat I wanted, way back before TypeScript existed, was JavaScript with static typing. TypeScript _mostly_ is that. At least in the way I choose to use it.\\n\\nI don\'t use `enum`s, `namespace`s, `decorator`s etc. This is significant as each of those features steps has an emit aspect; using one of these will require transpilation to create special JavaScript to represent a custom TypeScript implemented feature. All other TypeScript features are _erased_ by transpilation; there\'s no execution characteristics.\\n\\nSo by subsetting the features of TypeScript, we can choose to use only those features that do not have an emit aspect. By making that choice, it\'s possible to use just JavaScript, if we\'re willing to commit to using JSDoc syntax within JavaScript _instead_ of TypeScript. There\'s many in the community who are doing this on sizeable projects like [webpack](https://github.com/webpack/webpack) already. We don\'t lose type checking, we don\'t lose refactoring possibilities thanks to editors like VS Code.\\n\\nJSDoc is great, but it\'s undeniably more verbose than writing TypeScript. If types annotations was to be adopted, we\'d able to write TypeScript in our JavaScript files. We\'d be able to use TypeScript to type check that **if we wanted to**. But we wouldn\'t need to transpile our code prior to running. We could run our source code directly. Brilliant!\\n\\n## Controversy and Compromise\\n\\nUp until now, as we\'ve looked at the proposal, the story has been one of JavaScript becoming \\"types tolerant\\". And as a consequence, the syntax of Flow / TypeScript / Hegel et al would in future being considered valid JavaScript.\\n\\nThis paints a picture of JavaScript, a dynamic language, being changed to accomodate the sensibilities of those who favour static typing. If you should glance at the discussions on Hacker News and in the issues of the proposal it\'s clear there\'s a very vocal section of JavaScript developers who consider this proposal to be thoroughly unwanted.\\n\\nWhilst it\'s unlikely that the most fervent dynamic language advocate will change their mind, it\'s worth considering the nuance of this proposal. In actual fact, the proposal is a two way street; to comply with types becoming JavaScript native, languages like TypeScript would likely make changes to accomodate.\\n\\n## Generic invocations and TypeScript\\n\\nThere\'s a few cases which apply, the one that seems most significant is that of generic invocation. [To quote the proposal](https://github.com/giltayar/proposal-types-as-comments#generic-invocations):\\n\\n> One can explicitly specify the type arguments of a generic function invocation or generic class instantiation [in TypeScript](https://www.typescriptlang.org/docs/handbook/2/functions.html#specifying-type-arguments).\\n>\\n> ```ts\\n> // TypeScript\\n> add<number>(4, 5);\\n> new Point<bigint>(4n, 5n);\\n> ```\\n>\\n> The above syntax is already valid JavaScript that users may rely on, so we cannot use this syntax as-is.\\n\\nSo if this proposal was to land, writing today\'s style TypeScript in JavaScript would _not_ work in the case of generic invocations.\\n\\nIf we read on in the proposal it says;\\n\\n> We expect some form of new syntax that could be used to resolve this ambiguity.\\n> No specific solution is proposed at this point of time, but one example option is to use a syntactic prefix such as `::`\\n>\\n> ```ts\\n> // Types as Comments - example syntax solution\\n> add::<number>(4, 5)\\n> new Point::<bigint>(4n, 5n)\\n> ```\\n>\\n> These type arguments (`::<type>`) would be ignored by the JavaScript runtime.\\n> It would be reasonable for this non-ambiguous syntax to be adopted in TypeScript as well.\\n\\nThis last sentence is significant. Let\'s read it again:\\n\\n> It would be reasonable for this non-ambiguous syntax to be adopted in TypeScript as well\\n\\nWhilst not being an absolute commitment, this certainly suggests that TypeScript would be willing to change its own syntax to align with something that was standardised as typed JavaScript.\\n\\nSpeaking personally, I don\'t love the proposed new syntax; but I understand the rationale. Certainly a new generic invocation syntax is something I could come to terms with. It\'s good of the TypeScript team to be open to the idea of making changes to the language to align with the proposal. This is not zero cost to them. This demonstrates that to allow this proposal to land, there will be compromises on many sides. It\'s likely that Flow will be similarly affected also.\\n\\n## Conclusion\\n\\nWhen you see the various discussions on this topic online, it\'s clear there are many strong feelings. The proposal hasn\'t even reached stage 1 (of the potential 4 stages required for adoption). This may be a feature that doesn\'t make it. Or perhaps takes a long time to land on a mutually agreed design.\\n\\nSpeaking personally I\'m hopeful that this does end up being part of the language. Not only do I like running raw JS, I see the benefits of being able to onboard people from JavaScript to TypeScript by allowing types to live directly in JavaScript.\\n\\nIt\'s said that prediction is very difficult, especially if it\'s about the future. So it is hard to know for sure what the long term effects on the language and the ecosystem of this proposal might be. It would certainly lower the barrier to entry for using static typing with JavaScript, and as consequence, would likely lead to greater adoption and hence less bugs in userland. Time will tell.\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/types-as-comments-strong-types-weakly-held/)"},{"id":"/2022/04/06/eslint-your-csharp-in-vs-code-with-roslyn-analyzers","metadata":{"permalink":"/2022/04/06/eslint-your-csharp-in-vs-code-with-roslyn-analyzers","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2022-04-06-eslint-your-csharp-in-vs-code-with-roslyn-analyzers/index.md","source":"@site/blog/2022-04-06-eslint-your-csharp-in-vs-code-with-roslyn-analyzers/index.md","title":"ESLint your C# in VS Code with Roslyn Analyzers","description":"ESLint provides a great linting experience for TypeScript and JavaScript in VS Code. The suggestions, fixes and ignore options make creating clean code a joy. A similar experience is available for C# in VS Code through Roslyn Analyzers - this post tells us more.","date":"2022-04-06T00:00:00.000Z","formattedDate":"April 6, 2022","tags":[{"label":"Roslyn Analyzers","permalink":"/tags/roslyn-analyzers"},{"label":"C#","permalink":"/tags/c"},{"label":"VS Code","permalink":"/tags/vs-code"},{"label":"Lint","permalink":"/tags/lint"},{"label":"ESLint","permalink":"/tags/es-lint"}],"readingTime":10.335,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"ESLint your C# in VS Code with Roslyn Analyzers","authors":"johnnyreilly","tags":["Roslyn Analyzers","C#","VS Code","Lint","ESLint"],"image":"./title-image.png","hide_table_of_contents":false},"prevItem":{"title":"Type annotations: strong types, weakly held","permalink":"/2022/04/16/type-annotations-strong-types-weakly-held"},"nextItem":{"title":"Azure DevOps: consume a private artifact feed","permalink":"/2022/03/30/azure-devops-consume-private-nuget-artifact-feed"}},"content":"ESLint provides a great linting experience for TypeScript and JavaScript in VS Code. The suggestions, fixes and ignore options make creating clean code a joy. A similar experience is available for C# in VS Code through Roslyn Analyzers - this post tells us more.\\n\\n![title image reading \\"ESLint your C# in VS Code with Roslyn Analyzers\\" with the C# and VS Code logos`](title-image.png)\\n\\n## Linting and C#\\n\\nJavaScript and TypeScript benefit from a tremendous tooling ecosystem which allows us to simply format and lint our codebases as we\'re editing. Similar tooling exists for C#. [Previously I wrote about using `dotnet-format` to have a Prettier-like experience for formatting our C#](../2020-12-22-prettier-your-csharp-with-dotnet-format-and-lint-staged/index.md). If that last post focussed on formatting C#; looking through the lens of [Prettier](https://prettier.io/), this post focusses on linting; looking through the lens of [ESLint](https://eslint.org/).\\n\\n## Roslyn Analyzers\\n\\nThere\'s often overlap between linting and formatting tooling; and so it goes with C# as well. Linting and formatting in the .NET space make use of the [Roslyn Analyzers](https://github.com/dotnet/roslyn-analyzers):\\n\\n> Roslyn analyzers analyze your code for style, quality and maintainability, design and other issues. The documentation for Roslyn Analyzers can be found at docs.microsoft.com/dotnet/fundamentals/code-analysis/overview.\\n\\nTo learn more about them, it\'s worth reading [the excellent piece on the topic](https://endjin.com/blog/2022/01/raising-coding-standard-dotnet-analyzers) by [Ian Griffiths](https://twitter.com/idg10).\\n\\n## \\"Analyse `this`\\"\\n\\nIn order that we can see what the linting experience is like in VS Code, we\'re going to need a project to work on. We have the .NET 6 SDK installed, so we\'ll create ourselves a project:\\n\\n```shell\\ndotnet new webapi -o AnalyseThis\\n```\\n\\nWe have the [C# extension](https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.csharp) installed already, but we\'re getting no feedback on the code. Maybe it\'s already beautiful?\\n\\nOr maybe not. We\'re going to need an `.editorconfig` file to control all the code style settings. You can create this directly using the `dotnet` CLI like so;\\n\\n```shell\\ndotnet new editorconfig\\n```\\n\\nOnce this runs, it creates a file with all of the settings in with their default values. Alongside that, we need to wake VS Code up to our brave new world by setting the following in our `settings.json`:\\n\\n```json\\n{\\n  \\"omnisharp.enableRoslynAnalyzers\\": true,\\n  \\"omnisharp.enableEditorConfigSupport\\": true\\n}\\n```\\n\\nOr alternatively, use the GUI in VS Code to set these settings directly:\\n\\n![screenshot of the VS Code settings screen](screenshot-vs-code-settings-enable.png)\\n\\nIt\'s then a good idea to turn OmniSharp off and on again, so it picks up these changes:\\n\\n![screenshot of the VS Code \\"restart OmniSharp\\"](screenshot-vs-code-restart-omnisharp.png)\\n\\nThen, excitingly, we start to see code analysis, or linting, messages in the problems pane of VS Code:\\n\\n![screenshot of a first linting message and the code to which it applies](screenshot-initial-problems.png)\\n\\nIt\'s possible to use the `dotnet-format` command to surface this information:\\n\\n```shell\\ndotnet format style -v detailed --severity info --verify-no-changes\\n  The dotnet runtime version is \'6.0.2\'.\\n  Formatting code files in workspace \'/workspaces/AnalyseThis.csproj\'.\\n    Determining projects to restore...\\n  All projects are up-to-date for restore.\\n  Project AnalyseThis is using configuration from \'/workspaces/.editorconfig\'.\\n  Project AnalyseThis is using configuration from \'/workspaces/obj/Debug/net6.0/AnalyseThis.GeneratedMSBuildEditorConfig.editorconfig\'.\\n  Project AnalyseThis is using configuration from \'/usr/share/dotnet/sdk/6.0.200/Sdks/Microsoft.NET.Sdk/analyzers/build/config/analysislevel_6_default.editorconfig\'.\\n  Running 45 analyzers on AnalyseThis.\\n/workspaces/Controllers/WeatherForecastController.cs(14,57): info IDE0052: Private member \'WeatherForecastController._logger\' can be removed as the value assigned to it is never read [/workspaces/AnalyseThis.csproj]\\n  Formatted code file \'/workspaces/Controllers/WeatherForecastController.cs\'.\\n  Formatted 1 of 6 files.\\n  Format complete in 7993ms.\\n```\\n\\nNote the `IDE0052: Private member \'WeatherForecastController._logger\' can be removed as the value assigned to it is never read` message above.\\n\\n## Now fail my build!\\n\\nThis is all very exciting - we\'ve a world of extra linting at our fingertips! But what\'s a touch disappointing, is that the above information isn\'t surfaced in my build. What if as a team we commit to a particular code style? If I can\'t enforce that in the build, it\'s likely not going to happen.\\n\\nSo what do I do? Well, the information is out there on how to do this, but it\'s easy to miss. [You can find the details here](https://docs.microsoft.com/en-us/dotnet/fundamentals/code-analysis/overview#enable-on-build). We update our `AnalyseThis.csproj` to include an `EnforceCodeStyleInBuild` setting like so:\\n\\n```xml\\n  <PropertyGroup>\\n    <TargetFramework>net6.0</TargetFramework>\\n    <Nullable>enable</Nullable>\\n    <ImplicitUsings>enable</ImplicitUsings>\\n\\n    <EnforceCodeStyleInBuild>true</EnforceCodeStyleInBuild>\\n  </PropertyGroup>\\n```\\n\\nWe\'re going to replace our exhaustive `.editorconfig` file with a much simpler one:\\n\\n```ini\\n# Remove the line below if you want to inherit .editorconfig settings from higher directories\\nroot = true\\n\\n[*.cs]\\n# Default severity for analyzer diagnostics with category \'Style\' (escalated to build warnings)\\ndotnet_analyzer_diagnostic.category-Style.severity = warning\\n```\\n\\nDo you see what we did here? We told our build to treat `Style` diagnostics (lints) as warnings. Once OmniSharp picks this up, more linting messages start to appear in the problems pane of VS Code:\\n\\n![screenshot of more linting messages](screenshot-extra-problems.png)\\n\\nAnd what\'s more, if we attempt to build, guess what?\\n\\n```shell\\ndotnet build\\nMicrosoft (R) Build Engine version 17.1.0+ae57d105c for .NET\\nCopyright (C) Microsoft Corporation. All rights reserved.\\n\\n  Determining projects to restore...\\n  All projects are up-to-date for restore.\\n/workspaces/AnalyseThis/Controllers/WeatherForecastController.cs(3,1): warning IDE0160: Convert to block scoped namespace [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/WeatherForecast.cs(1,1): warning IDE0160: Convert to block scoped namespace [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Program.cs(1,1): warning IDE0008: Use explicit type instead of \'var\' [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Program.cs(10,1): warning IDE0008: Use explicit type instead of \'var\' [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Program.cs(15,5): warning IDE0058: Expression value is never used [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Program.cs(16,5): warning IDE0058: Expression value is never used [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n  AnalyseThis -> /workspaces/AnalyseThis/bin/Debug/net6.0/AnalyseThis.dll\\n\\nBuild succeeded.\\n\\n/workspaces/AnalyseThis/Controllers/WeatherForecastController.cs(3,1): warning IDE0160: Convert to block scoped namespace [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/WeatherForecast.cs(1,1): warning IDE0160: Convert to block scoped namespace [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Program.cs(1,1): warning IDE0008: Use explicit type instead of \'var\' [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Program.cs(10,1): warning IDE0008: Use explicit type instead of \'var\' [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Program.cs(15,5): warning IDE0058: Expression value is never used [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Program.cs(16,5): warning IDE0058: Expression value is never used [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n    6 Warning(s)\\n    0 Error(s)\\n\\nTime Elapsed 00:00:06.53\\n```\\n\\nThat\'s right! The same messages from the problems pane are now surfaced in our build as warnings. And we can kick it up a notch too; let\'s make them errors:\\n\\n```ini\\n# Remove the line below if you want to inherit .editorconfig settings from higher directories\\nroot = true\\n\\n[*.cs]\\n# Default severity for analyzer diagnostics with category \'Style\' (escalated to build errors)\\ndotnet_analyzer_diagnostic.category-Style.severity = error\\n```\\n\\nOnce OmniSharp catches up we see our warnings transform into errors:\\n\\n![screenshot of a more linting messages](screenshot-extra-problems-as-errors.png)\\n\\nAnd if we build...\\n\\n```shell\\ndotnet build\\nMicrosoft (R) Build Engine version 17.1.0+ae57d105c for .NET\\nCopyright (C) Microsoft Corporation. All rights reserved.\\n\\n  Determining projects to restore...\\n  All projects are up-to-date for restore.\\n/workspaces/AnalyseThis/WeatherForecast.cs(1,1): error IDE0160: Convert to block scoped namespace [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Controllers/WeatherForecastController.cs(3,1): error IDE0160: Convert to block scoped namespace [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Program.cs(1,1): error IDE0008: Use explicit type instead of \'var\' [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Program.cs(10,1): error IDE0008: Use explicit type instead of \'var\' [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Program.cs(15,5): error IDE0058: Expression value is never used [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Program.cs(16,5): error IDE0058: Expression value is never used [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n\\nBuild FAILED.\\n\\n/workspaces/AnalyseThis/WeatherForecast.cs(1,1): error IDE0160: Convert to block scoped namespace [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Controllers/WeatherForecastController.cs(3,1): error IDE0160: Convert to block scoped namespace [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Program.cs(1,1): error IDE0008: Use explicit type instead of \'var\' [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Program.cs(10,1): error IDE0008: Use explicit type instead of \'var\' [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Program.cs(15,5): error IDE0058: Expression value is never used [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n/workspaces/AnalyseThis/Program.cs(16,5): error IDE0058: Expression value is never used [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n    0 Warning(s)\\n    6 Error(s)\\n\\nTime Elapsed 00:00:04.22\\n```\\n\\nYes! Our style diagnostics are now failing the build. This is terrific!\\n\\n## Categories\\n\\nIt\'s worth pausing a second and considering the category upgrade we did here:\\n\\n```ini\\ndotnet_analyzer_diagnostic.category-Style.severity = error\\n```\\n\\nThere\'s a number of different categories that encapsulate groups of rules, [they\'re documented here](https://docs.microsoft.com/en-us/dotnet/fundamentals/code-analysis/categories). Taken from there you can see the wealth of different categories that exist:\\n\\n| Category                               | Description                                                                                                                                                                              | EditorConfig value                                              |\\n| -------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------- |\\n| Design rules                           | Design rules support adherence to the .NET Framework Design Guidelines.                                                                                                                  | `dotnet_analyzer_diagnostic.category-Design.severity`           |\\n| Documentation rules                    | Documentation rules support writing well-documented libraries through the correct use of XML documentation comments for externally visible APIs.                                         | `dotnet_analyzer_diagnostic.category-Documentation.severity`    |\\n| Globalization rules                    | Globalization rules support world-ready libraries and applications.                                                                                                                      | `dotnet_analyzer_diagnostic.category-Globalization.severity`    |\\n| Portability and interoperability rules | Portability rules support portability across different platforms. Interoperability rules support interaction with COM clients.                                                           | `dotnet_analyzer_diagnostic.category-Interoperability.severity` |\\n| Maintainability rules                  | Maintainability rules support library and application maintenance.                                                                                                                       | `dotnet_analyzer_diagnostic.category-Maintainability.severity`  |\\n| Naming rules                           | Naming rules support adherence to the naming conventions of the .NET design guidelines.                                                                                                  | `dotnet_analyzer_diagnostic.category-Naming.severity`           |\\n| Performance rules                      | Performance rules support high-performance libraries and applications.                                                                                                                   | `dotnet_analyzer_diagnostic.category-Performance.severity`      |\\n| SingleFile rules                       | Single-file rules support single-file applications.                                                                                                                                      | `dotnet_analyzer_diagnostic.category-SingleFile.severity`       |\\n| Reliability rules                      | Reliability rules support library and application reliability, such as correct memory and thread usage.                                                                                  | `dotnet_analyzer_diagnostic.category-Reliability.severity`      |\\n| Security rules                         | Security rules support safer libraries and applications. These rules help prevent security flaws in your program.                                                                        | `dotnet_analyzer_diagnostic.category-Security.severity`         |\\n| Style rules                            | Style rules support consistent code style in your codebase. These rules start with the \\"IDE\\" prefix.                                                                                     | `dotnet_analyzer_diagnostic.category-Style.severity`            |\\n| Usage rules                            | Usage rules support proper usage of .NET.                                                                                                                                                | `dotnet_analyzer_diagnostic.category-Usage.severity`            |\\n| N/A                                    | You can use this EditorConfig value to enable the following rules: IDE0051, IDE0064, IDE0076. While these rules start with \\"IDE\\", they are not technically part of the `Style` category. | `dotnet_analyzer_diagnostic.category-CodeQuality.severity`      |\\n\\nThe `IDE0052` information we saw when we used `dotnet format` earlier is technically part of the `CodeQuality` category. If we wanted to, we we could dial that up that category to an error like so:\\n\\n```ini\\n# Default severity for analyzer diagnostics with category \'CodeQuality\' (escalated to build errors)\\ndotnet_analyzer_diagnostic.category-CodeQuality.severity = error\\n```\\n\\n## Opt out of rules\\n\\nAs it turns out, I disagree with the complaints I\'m getting on the codebase right now, so I\'d like to dial those down to ignore. To do that globally, you simply put configuration in the `.editorconfig` to reflect that:\\n\\n```ini\\n# Remove the line below if you want to inherit .editorconfig settings from higher directories\\nroot = true\\n\\n[*.cs]\\n# Default severity for analyzer diagnostics with category \'Style\' (escalated to build warnings)\\ndotnet_analyzer_diagnostic.category-Style.severity = error\\n\\ndotnet_diagnostic.IDE0008.severity = none\\ndotnet_diagnostic.IDE0058.severity = none\\ndotnet_diagnostic.IDE0160.severity = none\\n```\\n\\nWhat we\'re doing here is saying \\"upgrade all [style rules](https://docs.microsoft.com/en-us/dotnet/fundamentals/code-analysis/style-rules/) to be errors, but `IDE0008`, `IDE0058` and `IDE0160` (which are style rules) - ignore those; don\'t tell me about them\\".\\n\\nNow I\'m not going to be bothered by those errors in future. Great.\\n\\n## Dial up information to warning\\n\\nIf we look again at our problems pane in VS Code, we can see there\'s an entry there. It\'s not an error, it\'s not a warning. It\'s information:\\n\\n![screenshot of a first linting message and the code to which it applies](screenshot-initial-problems.png)\\n\\nLet\'s say we want to take that and dial it up to be a warning, such that it surfaces in the build too. We can with a simple addition to our `.editorconfig`:\\n\\n```ini\\n# Remove the line below if you want to inherit .editorconfig settings from higher directories\\nroot = true\\n\\n[*.cs]\\n# Default severity for analyzer diagnostics with category \'Style\' (escalated to build warnings)\\ndotnet_analyzer_diagnostic.category-Style.severity = error\\n\\ndotnet_diagnostic.IDE0008.severity = none\\ndotnet_diagnostic.IDE0058.severity = none\\ndotnet_diagnostic.IDE0160.severity = none\\n\\n# Roslyn analzer surfaces this as information - we\'ll dial it up to a warning\\ndotnet_diagnostic.IDE0052.severity = warning\\n```\\n\\nOnce OmniSharp notices:\\n\\n![screenshot of our information now a warning](screenshot-information-as-warning.png)\\n\\nAnd if we run the build, there it is!\\n\\n```\\ndotnet build\\nMicrosoft (R) Build Engine version 17.1.0+ae57d105c for .NET\\nCopyright (C) Microsoft Corporation. All rights reserved.\\n\\n  Determining projects to restore...\\n  All projects are up-to-date for restore.\\n/workspaces/AnalyseThis/Controllers/WeatherForecastController.cs(14,57): warning IDE0052: Private member \'WeatherForecastController._logger\' can be removed as the value assigned to it is never read [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n  AnalyseThis -> /workspaces/AnalyseThis/bin/Debug/net6.0/AnalyseThis.dll\\n\\nBuild succeeded.\\n\\n/workspaces/AnalyseThis/Controllers/WeatherForecastController.cs(14,57): warning IDE0052: Private member \'WeatherForecastController._logger\' can be removed as the value assigned to it is never read [/workspaces/AnalyseThis/AnalyseThis.csproj]\\n    1 Warning(s)\\n    0 Error(s)\\n\\nTime Elapsed 00:00:02.21\\n```\\n\\n## Deactivate linting partially\\n\\nLet\'s say we want to ignore that one warning. We\'d like the equivalent functionality to `// eslint-disable-next-line`. That doesn\'t exist alas. However, what does is the equivalent to this:\\n\\n```js\\n/* eslint-disable */\\n\\nalert(\'foo\');\\n\\n/* eslint-enable */\\n```\\n\\nIn our case what we\'d do is this:\\n\\n```cs\\n#pragma warning disable\\n    private readonly ILogger<WeatherForecastController> _logger;\\n#pragma warning restore\\n```\\n\\nOr to be more specific:\\n\\n```cs\\n#pragma warning disable IDE0052\\n    private readonly ILogger<WeatherForecastController> _logger;\\n#pragma warning restore IDE0052\\n```\\n\\nAnd now we can opt out of that rule in this specific place - whilst maintaining it more generally.\\n\\n## Conclusion\\n\\nThere\'s powerful linting tools in C#, hopefully this guide has made it easier for you to surface them, control them and apply them both to VS Code and to your build.\\n\\nThanks to [Joey Robichaud](https://twitter.com/JoeyRobichaud), [Tim Heuer](https://twitter.com/timheuer) and [Youssef Victor](https://twitter.com/YoussefV1313) for some excellent pointers that fed into the writing of this post. [You can see the help they provided here](https://github.com/dotnet/roslyn/issues/60620)."},{"id":"/2022/03/30/azure-devops-consume-private-nuget-artifact-feed","metadata":{"permalink":"/2022/03/30/azure-devops-consume-private-nuget-artifact-feed","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2022-03-30-azure-devops-consume-private-nuget-artifact-feed/index.md","source":"@site/blog/2022-03-30-azure-devops-consume-private-nuget-artifact-feed/index.md","title":"Azure DevOps: consume a private artifact feed","description":"Private Azure Artifact feeds in in Azure DevOps can be used to serve NuGet packages. To build applications both locally and in an Azure Pipeline using those packages, there are a few steps to follow which this post will demonstrate.","date":"2022-03-30T00:00:00.000Z","formattedDate":"March 30, 2022","tags":[{"label":"Azure DevOps","permalink":"/tags/azure-dev-ops"},{"label":"NuGet","permalink":"/tags/nu-get"},{"label":"Azure Artifacts","permalink":"/tags/azure-artifacts"},{"label":".NET","permalink":"/tags/net"}],"readingTime":3.39,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Azure DevOps: consume a private artifact feed","authors":"johnnyreilly","tags":["Azure DevOps","NuGet","Azure Artifacts",".NET"],"image":"./title-image.png","hide_table_of_contents":false},"prevItem":{"title":"ESLint your C# in VS Code with Roslyn Analyzers","permalink":"/2022/04/06/eslint-your-csharp-in-vs-code-with-roslyn-analyzers"},"nextItem":{"title":"Lighthouse meet GitHub Actions","permalink":"/2022/03/20/lighthouse-meet-github-actions"}},"content":"Private Azure Artifact feeds in in Azure DevOps can be used to serve NuGet packages. To build applications both locally and in an Azure Pipeline using those packages, there are a few steps to follow which this post will demonstrate.\\n\\n![title image reading \\"Azure DevOps: consume a private artifact feed\\" with the Azure DevOps and Azure Pipelines logos`](title-image.png)\\n\\n## Make a `nuget.config`\\n\\nTo consume a private feed, you\'ll likely want to create a `nuget.config` file in the root of your repo. Here you list the package sources you want to consume, typically the NuGet official package source _as well_ as your private feed. See the example below:\\n\\n```xml\\n<?xml version=\\"1.0\\" encoding=\\"utf-8\\"?>\\n  <configuration>\\n    <packageSources>\\n      <add key=\\"NuGet official package source\\" value=\\"https://api.nuget.org/v3/index.json\\" />\\n      <add key=\\"my-nuget-packages\\" value=\\"https://pkgs.dev.azure.com/my-org/_packaging/my-nuget-packages/nuget/v3/index.json\\" />\\n    </packageSources>\\n  </configuration>\\n```\\n\\n## Consuming a private feed locally with the Azure Artifacts Credential Provider\\n\\nWith our `nuget.config` in place, can we build locally? Yes, once we\'ve authenticated. If you\'re using Rider or Visual Studio, these may take care of this for you. However, if you\'re using VS Code you might need to do something else.\\n\\nIf you experience 401\'s when you run `dotnet restore` like so:\\n\\n```shell\\nerror : Unable to load the service index for source https://pkgs.dev.azure.com/my-org/_packaging/not-there/nuget/v3/index.json. [/dev.azure.com/project/repo/src/App.csproj]\\nerror : Response status code does not indicate success: 401\\n```\\n\\nThen it\'s probably a sign you need to install the [Azure Artifacts Credential Provider](https://github.com/Microsoft/artifacts-credprovider). With that you should be able to restore nuget packages. See instructions [here](https://github.com/Microsoft/artifacts-credprovider#setup).\\n\\nOn Linux and Mac this is as simple as running `sh -c \\"$(curl -fsSL https://aka.ms/install-artifacts-credprovider.sh)\\"` in your terminal.\\n\\nSubsequently, running `dotnet restore --interactive` should trigger an authentication flow in the terminal, and subject to successful authentication, restore packages from the private feed.\\n\\n## Consuming a private feed in Azure Pipelines\\n\\nYou will need to authenticate within your pipeline before you can acquire your private feed packages. This is as simple as this:\\n\\n```yml\\n- task: NuGetAuthenticate@0\\n```\\n\\nBefore building / publishing or running tests, you must first explicitly `dotnet restore` and provide the path to the `nuget.config`. You can do this with the dedicated [.NET Core CLI task](https://docs.microsoft.com/en-us/azure/devops/pipelines/tasks/build/dotnet-core-cli) task like so:\\n\\n```yml\\n- task: DotNetCoreCLI@2\\n  displayName: \'dotnet restore\'\\n  inputs:\\n    command: \'restore\'\\n    projects: \'src/App/App.csproj\'\\n    nugetConfigPath: \'../../nuget.config\'\\n    feedsToUse: config\\n```\\n\\n## The publish gotcha\\n\\nOn occasion, it can happen that Azure Pipelines doesn\'t seem to be happy running a publish task with private feeds. Consider, a task like this:\\n\\n```yml\\n- task: DotNetCoreCLI@2\\n  displayName: \'dotnet publish\'\\n  inputs:\\n    command: publish\\n    arguments: \'--configuration Release --output $(Build.ArtifactStagingDirectory)/${{parameters.artifactName}} /p:SourceRevisionId=$(Build.SourceVersion)\'\\n    zipAfterPublish: true\\n    publishWebProjects: false\\n    workingDirectory: src/App\\n```\\n\\nThis can result in non-actionable errors like this:\\n\\n> `##[error]Error: There was an error when attempting to execute the process \'/opt/hostedtoolcache/dotnet/dotnet\'. This may indicate the process failed to start. Error: spawn /opt/hostedtoolcache/dotnet/dotnet ENOENT`\\n\\nA workaround in this situation is to invoke .NET through a bash script directly like so:\\n\\n```yml\\n- bash: |\\n    cd src/App\\n    dotnet --list-sdks\\n    echo \\"\\"\\n    echo \\"**************\\"\\n    echo \\"dotnet restore --configfile ../../nuget.config\\"\\n    dotnet restore --configfile ../../nuget.config\\n    echo \\"\\"\\n    echo \\"**************\\"\\n    echo \\"dotnet build --configuration Release --no-restore\\"\\n    dotnet build --configuration Release\\n    echo \\"\\"\\n    echo \\"**************\\"\\n    echo \\"dotnet publish --configuration Release --no-restore --output $(Build.ArtifactStagingDirectory)/App /p:SourceRevisionId=$(Build.SourceVersion)\\"\\n    dotnet publish --configuration Release --no-restore --output $(Build.ArtifactStagingDirectory)/App /p:SourceRevisionId=$(Build.SourceVersion)\\n  displayName: \'dotnet publish\'\\n\\n- task: ArchiveFiles@2\\n  displayName: \\"Create $(Build.ArtifactStagingDirectory)/App.zip\\"\\n  inputs:\\n    rootFolderOrFile: \\"$(Build.ArtifactStagingDirectory)/App\\"\\n    includeRootFolder: false\\n    archiveFile: \\"$(Build.ArtifactStagingDirectory)/App.zip\\"\\n```\\n\\nAnd note that after publishing we use the [Archive Files task](https://docs.microsoft.com/en-us/azure/devops/pipelines/tasks/utility/archive-files) to zip up the output of our publishing.\\n\\nYou may be tempted to use the zip command line utility to make your zip. Do not do this. I did this. I learned, through no small amount of suffering, that there is a problem with this. Whilst you can make a zip this way that will be consumed happily by Mac and OSX, when it comes to being deployed to Azure (even if you\'re deploying to Linux within Azure) via zip deploy it will not work. I can\'t tell you why, just that it won\'t. So use the dedicated task.\\n\\n## Summing up\\n\\nAnd that\'s it; with these approaches in place you should be able to build applications consuming privage NuGet feeds with ease."},{"id":"/2022/03/20/lighthouse-meet-github-actions","metadata":{"permalink":"/2022/03/20/lighthouse-meet-github-actions","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2022-03-20-lighthouse-meet-github-actions/index.md","source":"@site/blog/2022-03-20-lighthouse-meet-github-actions/index.md","title":"Lighthouse meet GitHub Actions","description":"Lighthouse is a tremendous tool for auditing the performance and usability of websites. Rather than having to perform these audits manually, it\'s helpful to be able to plug it into your CI pipeline. This post illustrates how to integrate Lighthouse into a GitHub Actions workflow for an Azure Static Web App, and report findings directly in pull requests that are raised.","date":"2022-03-20T00:00:00.000Z","formattedDate":"March 20, 2022","tags":[{"label":"Azure Static Web Apps","permalink":"/tags/azure-static-web-apps"},{"label":"GitHub Actions","permalink":"/tags/git-hub-actions"},{"label":"Docusaurus","permalink":"/tags/docusaurus"}],"readingTime":11.4,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Lighthouse meet GitHub Actions","authors":"johnnyreilly","tags":["Azure Static Web Apps","GitHub Actions","Docusaurus"],"image":"./title-image.png","hide_table_of_contents":false},"prevItem":{"title":"Azure DevOps: consume a private artifact feed","permalink":"/2022/03/30/azure-devops-consume-private-nuget-artifact-feed"},"nextItem":{"title":"Swashbuckle & inheritance: Give. Me. The. Types","permalink":"/2022/03/06/swashbuckle-inheritance-multiple-return-types"}},"content":"Lighthouse is a tremendous tool for auditing the performance and usability of websites. Rather than having to perform these audits manually, it\'s helpful to be able to plug it into your CI pipeline. This post illustrates how to integrate Lighthouse into a GitHub Actions workflow for an Azure Static Web App, and report findings directly in pull requests that are raised.\\n\\n![title image reading \\"Lighthouse meet GitHub Actions\\" with the Lighthouse logo and a screenshot of the results in a GitHub comment`](title-image.png)\\n\\n## What we\'ll do\\n\\nThis post isn\'t a walkthrough of how to use Lighthouse effectively. There is already [great guidance out there on this topic](https://blog.logrocket.com/lighthouse-and-how-to-use-it-more-effectively/).\\n\\nInstead, we\'re going build a simple web application, in the context of a GitHub repo. We\'ll wire it up to deploy via GitHub Actions to Azure Static Web Apps. Static Web Apps is a free hosting option for static websites and it comes with [staging environments](https://docs.microsoft.com/en-us/azure/static-web-apps/review-publish-pull-requests) or deployment previews built in. This feature deploys a fully functional version of a site each time a pull request is raised, built upon the changes implemented in that pull request.\\n\\nThe staging environment is a perfect place to implement our Lighthouse checks. If a pull request impacts usability or performance, seeing those details in the context of our pull request is exactly where we\'d like to learn this. This kind of check gives us the opportunity to ensure we only merge when we\'re happy that the changes do not negatively impact our Lighthouse scores.\\n\\nIn this post we\'ll start from the point of an empty GitHub repo and build up from there.\\n\\n## Create our application\\n\\nInside the root of our repository we\'re going to create a [Docusaurus site](https://docusaurus.io/). Docusaurus is a good example of a static site, the kind of which is a natural fit for Jamstack. We could equally use something else like [Hugo](https://gohugo.io/) for instance.\\n\\nAt the command line we\'ll enter:\\n\\n```shell\\nnpx create-docusaurus@latest website classic\\n```\\n\\nAnd Docusaurus will create a new site in the `website` directory. Let\'s commit and push this and turn our attention to Azure.\\n\\n## Creating a Static Web App in Azure\\n\\nThere\'s a number of ways to create a Static Web App in Azure. It\'s possible to use [infrastructure as code with a language like Bicep](https://blog.johnnyreilly.com/2021/08/15/bicep-azure-static-web-apps-azure-devops#bicep-template). But for this post let\'s use the [Azure Portal](https://portal.azure.com) instead. If you don\'t have an account already, you can set one up for free very quickly.\\n\\nOnce you\'ve logged in, click \\"Create a resource\\" and look up Static Web App:\\n\\n![Screenshot of the Azure Portal, \\"Create a resource\\" Azure Static Web Apps section](screenshot-azure-portal-create-a-resource.png)\\n\\nClick on \\"Create\\" and you\'ll be take to the creation dialog:\\n\\n![Screenshot of the Azure Portal, \\"Create a resource\\" Azure Static Web Apps dialog](screenshot-azure-portal-create-a-resource-dialog.png)\\n\\nYou\'ll need to create a resource group for your SWA to live in, give the app a name, the \\"Free\\" plan and a deployment source of GitHub.\\n\\nClick on the \\"Sign in with GitHub\\" button and authorize Azure to access your GitHub account for Static Web Apps.\\n\\n![Screenshot of the Azure Portal, \\"Create a resource\\" Azure Static Web Apps dialog - repository settings](screenshot-azure-portal-create-a-resource-dialog-repo.png)\\n\\nAt this point Azure will query GitHub on your behalf and look up the organisations and repositories you have access to. Select the repository that you\'d like to deploy to your Static Web App and select the branch you\'d like to deploy.\\n\\nYou also need to provide Azure with some build details that help it understand how your app is built. We\'ll provide a preset of \\"Custom\\". We\'ll set the \\"App location\\" (the root of our front end app) to be `\\"/website\\"` to tally up with the application we just created. We\'ll leave \\"Api location\\" blank and we\'ll set the output location to be `\\"build\\"` - this is the directory under `website` where Docusaurus will create our site.\\n\\nFinally click \\"Review + create\\" and then \\"Create\\".\\n\\nAzure will now:\\n\\n- Create an Azure Static Web app resource in Azure\\n- Update your repository to add a GitHub Actions workflow to deploy your static web app\\n- Kick off a first run of the GitHub Actions workflow to deploy your SWA.\\n\\nPretty amazing, right?\\n\\nWhen you look at the resource in Azure it will look something like this:\\n\\n![Screenshot of the Azure Portal, your Azure Static Web Apps resource](screenshot-azure-portal-static-web-app-resource.png)\\n\\nIf you click on the GitHub Action runs you\'ll be presented with your GitHub Action:\\n\\n![Screenshot of the GitHub Action](screenshot-github-action.png)\\n\\nAnd when that finishes running you\'ll be able to see your deployed Static Web App by clicking on the URL in the Azure Portal:\\n\\n![Screenshot of your Static Web App running in a browser](screenshot-static-web-app.png)\\n\\nWe now have:\\n\\n- a GitHub repo\\n- which contains a simple web application\\n- and a GitHub Actions workflow which:\\n  - deploys to an Azure Static Web App\\n  - spins up a staging environment for pull requests\\n\\n## Preparing to plug in Lighthouse\\n\\nWith this groundwork in place we\'re ready to add Lighthouse into the mix. If you look in the `/.github/workflows` folder of your repo, you\'ll find a workflow file with contents along these lines:\\n\\n```yml\\nname: Azure Static Web Apps CI/CD\\n\\non:\\n  push:\\n    branches:\\n      - main\\n  pull_request:\\n    types: [opened, synchronize, reopened, closed]\\n    branches:\\n      - main\\n\\njobs:\\n  build_and_deploy_job:\\n    if: github.event_name == \'push\' || (github.event_name == \'pull_request\' && github.event.action != \'closed\')\\n    runs-on: ubuntu-latest\\n    name: Build and Deploy Job\\n    steps:\\n      - uses: actions/checkout@v2\\n        with:\\n          submodules: true\\n      - name: Build And Deploy\\n        id: builddeploy\\n        uses: Azure/static-web-apps-deploy@v1\\n        with:\\n          azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN_AGREEABLE_ROCK_039A51810 }}\\n          repo_token: ${{ secrets.GITHUB_TOKEN }} # Used for Github integrations (i.e. PR comments)\\n          action: \'upload\'\\n          ###### Repository/Build Configurations - These values can be configured to match your app requirements. ######\\n          # For more information regarding Static Web App workflow configurations, please visit: https://aka.ms/swaworkflowconfig\\n          app_location: \'/website\' # App source code path\\n          api_location: \'\' # Api source code path - optional\\n          output_location: \'build\' # Built app content directory - optional\\n          ###### End of Repository/Build Configurations ######\\n\\n  close_pull_request_job:\\n    if: github.event_name == \'pull_request\' && github.event.action == \'closed\'\\n    runs-on: ubuntu-latest\\n    name: Close Pull Request Job\\n    steps:\\n      - name: Close Pull Request\\n        id: closepullrequest\\n        uses: Azure/static-web-apps-deploy@v1\\n        with:\\n          azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN_AGREEABLE_ROCK_039A51810 }}\\n          action: \'close\'\\n```\\n\\nThis was created for us when we set up our SWA in Azure. We\'re now going to update the contents to add some Lighthouse jobs.\\n\\nBefore we do that, we need to acquire two things:\\n\\n1. the custom domain of our static web app\\n2. the location of the resource group where the SWA resides\\n\\nThese two pieces of information are required such that we can determine the URL of our staging environments.\\n\\n### Custom domain\\n\\nWe acquire the custom domain of our static web app in the \\"Custom Domains\\" screen of the Azure Portal:\\n\\n![screenshot of the custom domain screen in the Azure Portal](screenshot-azure-portal-static-web-app-custom-domain.png)\\n\\nThe custom domain is the auto-generated custom domain - it\'s highlighted in the screenshot above. For the SWA we\'re building here the custom domain is `agreeable-rock-039a51810.1.azurestaticapps.net`.\\n\\n### Location\\n\\nWe acquire the location by looking at the resource group in the Azure Portal. For the SWA we\'ve been building the location is \\"Central US\\". However, rather than the \\"display name\\" variant of the location, what we want is the \\"code\\" which will be used in the URL. You can see what this is by clicking on the \\"JSON view\\" in the Azure Portal:\\n\\n![screenshot of the resource group JSON view with location highlighted](screenshot-azure-portal-static-web-app-rg-location.png)\\n\\nAs the screenshot above demonstrates, the code we need is `centralus`.\\n\\n## Plugging in Lighthouse\\n\\nWe now have all we need to plug in Lighthouse. Let\'s create a branch:\\n\\n```shell\\ngit checkout -b lighthouse\\n```\\n\\nWe\'re going to add a new \\"Lighthouse report\\" job to our GitHub Actions workflow file:\\n\\n```yml\\nlighthouse_report_job:\\n  name: Lighthouse report\\n  if: github.event_name == \'pull_request\' && github.event.action != \'closed\'\\n  runs-on: ubuntu-latest\\n  steps:\\n    - uses: actions/checkout@v2\\n\\n    - name: Static Web App - get preview URL\\n      id: static_web_app_preview_url\\n      uses: azure/CLI@v1\\n      with:\\n        inlineScript: |\\n          CUSTOM_DOMAIN=\'agreeable-rock-039a51810.1.azurestaticapps.net\'\\n          LOCATION=\'centralus\'\\n\\n          PREVIEW_URL=\\"https://${CUSTOM_DOMAIN/.[1-9]./-${{github.event.pull_request.number }}.$LOCATION.1.}\\"\\n\\n          echo \\"::set-output name=PREVIEW_URL::$PREVIEW_URL\\"\\n\\n    - name: Static Web App - wait for preview\\n      id: static_web_app_wait_for_preview\\n      uses: nev7n/wait_for_response@v1\\n      with:\\n        url: \'${{ steps.static_web_app_preview_url.outputs.PREVIEW_URL }}\'\\n        responseCode: 200\\n        timeout: 600000\\n        interval: 1000\\n\\n    - name: Audit URLs using Lighthouse\\n      id: lighthouse_audit\\n      uses: treosh/lighthouse-ci-action@v8\\n      with:\\n        urls: |\\n          ${{ steps.static_web_app_preview_url.outputs.PREVIEW_URL }}\\n        configPath: ./.github/workflows/lighthousesrc.json\\n        uploadArtifacts: true\\n        temporaryPublicStorage: true\\n        runs: 5\\n\\n    - name: Format lighthouse score\\n      id: format_lighthouse_score\\n      uses: actions/github-script@v5\\n      with:\\n        script: |\\n          const lighthouseCommentMaker = require(\'./.github/workflows/lighthouseCommentMaker.js\');\\n\\n          const lighthouseOutputs = {\\n            manifest: ${{ steps.lighthouse_audit.outputs.manifest }},\\n            links: ${{ steps.lighthouse_audit.outputs.links }}\\n          };\\n\\n          const comment = lighthouseCommentMaker({ lighthouseOutputs });\\n          core.setOutput(\\"comment\\", comment);\\n\\n    - name: Add Lighthouse stats as comment\\n      id: comment_to_pr\\n      uses: marocchino/sticky-pull-request-comment@v2.0.0\\n      with:\\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\\n        number: ${{ github.event.pull_request.number }}\\n        header: lighthouse\\n        message: ${{ steps.format_lighthouse_score.outputs.comment }}\\n```\\n\\nThere\'s a number of things happening in this workflow. Let\'s walk through them.\\n\\n### Static Web App - get preview URL\\n\\nHere we construct the preview URL of our static web app using:\\n\\n- the custom domain\\n- the location\\n- the pull request number eg 123\\n\\nGiven a custom domain of `agreeable-rock-039a51810.1.azurestaticapps.net`, a location of `centralus` and a pull request number of `123`, the preview url would be `agreeable-rock-039a51810-123.centralus.1.azurestaticapps.net`. Using a little bash magic we create an output variable named `PREVIEW_URL` containing that value. We\'ll re-use it later in the workflow.\\n\\n### Static Web App - wait for preview\\n\\nWe don\'t want to run our test until the static web app is up and running. To cater for this we\'re going to pull in the [`wait_for_response`](https://github.com/nev7n/wait_for_response) GitHub Action. This polls until a website returns a `200`, we\'re going to point it at our SWA.\\n\\n### Audit URLs using Lighthouse\\n\\nThe big moment has arrived! We\'re going to plug Lighthouse into our workflow using the [`lighthouse-ci-action`](https://github.com/treosh/lighthouse-ci-action) GitHub Action.\\n\\nWe provide a `configPath: ./.github/workflows/lighthousesrc.json` which points to file that configures our Lighthouse configuration. We\'ll create that file as well and populate it with this:\\n\\n```json\\n{\\n  \\"ci\\": {\\n    \\"collect\\": {\\n      \\"settings\\": {\\n        \\"configPath\\": \\"./.github/workflows/lighthouse-config.js\\"\\n      }\\n    }\\n  }\\n}\\n```\\n\\nThis in turn hands off the specific configuration to a `lighthouse-config.js` file that we also need to create:\\n\\n```js\\n// see https://github.com/GoogleChrome/lighthouse/blob/master/docs/configuration.md\\nmodule.exports = {\\n  extends: \'lighthouse:default\',\\n  settings: {\\n    // audits can be found here:\\n    // https://github.com/GoogleChrome/lighthouse/blob/eba2a4d19c5786dc37e993858ff4b663181f81e5/lighthouse-core/config/default-config.js#L174\\n    skipAudits: [\\n      \'canonical\', // for staging sites this will always be incorrect\\n      \'maskable-icon\',\\n      \'valid-source-maps\',\\n      \'unsized-images\',\\n      \'offline-start-url\',\\n    ],\\n  },\\n};\\n```\\n\\nThe configuration above can be amended based upon the various links in the comments. Generally it\'s a good idea to roll with the defaults; however skipping the `canonical` audit is sensible as it will reliably be incorrect for staging sites.\\n\\nAlong side the Lighthouse configuration, there\'s config for the GitHub Action itself:\\n\\n- `uploadArtifacts: true` - will save results as an action artifacts\\n- `temporaryPublicStorage: true` - will upload lighthouse report to the temporary storage\\n- `runs: 5` - will run Lighthouse 5 times to get more reliable performance results\\n\\n### Format lighthouse score\\n\\nWe\'ve run Lighthouse at this point. What we want to do next is take the results of the run and build up some text that we can add to our pull request as a comment.\\n\\nFor this we\'re going to use the [`github-script`](https://github.com/actions/github-script) GitHub Action, grab the outputs of the previous step and call out to a `lighthouseCommentMaker.js` file we\'re going to write to make the comment we\'d like to publish to our PR:\\n\\n```js\\n// @ts-check\\n\\n/**\\n * @typedef {Object} Summary\\n * @prop {number} performance\\n * @prop {number} accessibility\\n * @prop {number} best-practices\\n * @prop {number} seo\\n * @prop {number} pwa\\n */\\n\\n/**\\n * @typedef {Object} Manifest\\n * @prop {string} url\\n * @prop {boolean} isRepresentativeRun\\n * @prop {string} htmlPath\\n * @prop {string} jsonPath\\n * @prop {Summary} summary\\n */\\n\\n/**\\n * @typedef {Object} LighthouseOutputs\\n * @prop {Record<string, string>} links\\n * @prop {Manifest[]} manifest\\n */\\n\\nconst formatScore = (/** @type { number } */ score) => Math.round(score * 100);\\nconst emojiScore = (/** @type { number } */ score) =>\\n  score >= 0.9 ? \'\ud83d\udfe2\' : score >= 0.5 ? \'\ud83d\udfe0\' : \'\ud83d\udd34\';\\n\\nconst scoreRow = (\\n  /** @type { string } */ label,\\n  /** @type { number } */ score\\n) => `| ${emojiScore(score)} ${label} | ${formatScore(score)} |`;\\n\\n/**\\n * @param {LighthouseOutputs} lighthouseOutputs\\n */\\nfunction makeComment(lighthouseOutputs) {\\n  const { summary } = lighthouseOutputs.manifest[0];\\n  const [[testedUrl, reportUrl]] = Object.entries(lighthouseOutputs.links);\\n\\n  const comment = `## \u26a1\ufe0f\ud83c\udfe0 Lighthouse report\\n\\nWe ran Lighthouse against the changes and produced this [report](${reportUrl}). Here\'s the summary:\\n\\n| Category | Score |\\n| -------- | ----- |\\n${scoreRow(\'Performance\', summary.performance)}\\n${scoreRow(\'Accessibility\', summary.accessibility)}\\n${scoreRow(\'Best practices\', summary[\'best-practices\'])}\\n${scoreRow(\'SEO\', summary.seo)}\\n${scoreRow(\'PWA\', summary.pwa)}\\n\\n*Lighthouse ran against [${testedUrl}](${testedUrl})*\\n`;\\n\\n  return comment;\\n}\\n\\nmodule.exports = ({ lighthouseOutputs }) => {\\n  return makeComment(lighthouseOutputs);\\n};\\n```\\n\\nThe above code takes the Lighthouse outputs and creates some Markdown to represent the results. It uses some nice emoji as well. Wonderfully, we\'re entirely free to customise this as much as we\'d like; it\'s just code! All that matters is that a string is pumped out at the end.\\n\\n### Add Lighthouse stats as comment\\n\\nFinally we\'re ready to add the comment to the PR. We\'ll do this using the [`sticky-pull-request-comment`](https://github.com/marocchino/sticky-pull-request-comment) GitHub Action. We pass in the comment we\'ve just made in the previous step, as well as some other parameters, and this will write the comment to the PR.\\n\\n## Putting it all together\\n\\nWhen we commit our changes and raise a pull request, we see our GitHub Action run, and then once it has we see a Lighthouse report being attached to our pull request:\\n\\n![screenshot of GitHub pull request showing the Lighthouse results as a comment](screenshot-lighthouse-github-comment.png)\\n\\nNote you can also click on a link in the comment to go directly to the full report.\\n\\n![screenshot of Lighthouse report](screenshot-lighthouse-report.png)\\n\\nNow, with each PR that is raised, any regressions in performance can be observed and resolved _before_ they make get in front of customer\'s eyes!\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/lighthouse-meets-github-actions-use-lighthouse-ci/)"},{"id":"/2022/03/06/swashbuckle-inheritance-multiple-return-types","metadata":{"permalink":"/2022/03/06/swashbuckle-inheritance-multiple-return-types","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2022-03-06-swashbuckle-inheritance-multiple-return-types/index.md","source":"@site/blog/2022-03-06-swashbuckle-inheritance-multiple-return-types/index.md","title":"Swashbuckle & inheritance: Give. Me. The. Types","description":"For API endpoints that return multiple types, you can use inheritance with Swashbuckle to get create a Swagger / Open API definition featuring the variety of available types. Serving all these types is not the default behaviour. This post shows you how to opt in.","date":"2022-03-06T00:00:00.000Z","formattedDate":"March 6, 2022","tags":[{"label":"Swashbuckle","permalink":"/tags/swashbuckle"},{"label":".NET","permalink":"/tags/net"},{"label":"inheritance","permalink":"/tags/inheritance"},{"label":"UseOneOfForPolymorphism","permalink":"/tags/use-one-of-for-polymorphism"},{"label":"multiple return types","permalink":"/tags/multiple-return-types"},{"label":"discriminated unions","permalink":"/tags/discriminated-unions"}],"readingTime":5.555,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Swashbuckle & inheritance: Give. Me. The. Types","authors":"johnnyreilly","tags":["Swashbuckle",".NET","inheritance","UseOneOfForPolymorphism","multiple return types","discriminated unions"],"image":"./title-image.png","hide_table_of_contents":false},"prevItem":{"title":"Lighthouse meet GitHub Actions","permalink":"/2022/03/20/lighthouse-meet-github-actions"},"nextItem":{"title":"Azure Static Web Apps - a Netlify alternative","permalink":"/2022/02/08/azure-static-web-apps-a-netlify-alternative"}},"content":"For API endpoints that return multiple types, you can use inheritance with Swashbuckle to get create a Swagger / Open API definition featuring the variety of available types. Serving all these types is not the default behaviour. This post shows you how to opt in.\\n\\n![title image reading \\"Swashbuckle and inheritance: Give. Me. The. Types\\" with Sid Swashbuckle the Pirate and Open API logos](title-image.png)\\n\\n## Making a simple API\\n\\nThe first thing we\'re going to need is an API, which we\'ll build with the .NET 6 SDK:\\n\\n```bash\\ndotnet new webapi\\ndotnet add package Swashbuckle.AspNetCore\\n```\\n\\nWhen we run this with `dotnet run` we find Swashbuckle living at http://localhost:5000/swagger/index.html defining our web api that serves up a WeatherForecast:\\n\\n![screenshot of swagger UI including `WeatherForecast`](screenshot-initial-swagger-ui.png)\\n\\nIf we look at the `swagger.json` created at our `http://localhost:5000/swagger/v1/swagger.json` endpoint we see the following definition:\\n\\n```json\\n{\\n  \\"openapi\\": \\"3.0.1\\",\\n  \\"info\\": {\\n    \\"title\\": \\"SwashbuckleInheritance\\",\\n    \\"version\\": \\"1.0\\"\\n  },\\n  \\"paths\\": {\\n    \\"/WeatherForecast\\": {\\n      \\"get\\": {\\n        \\"tags\\": [\\"WeatherForecast\\"],\\n        \\"operationId\\": \\"GetWeatherForecast\\",\\n        \\"responses\\": {\\n          \\"200\\": {\\n            \\"description\\": \\"Success\\",\\n            \\"content\\": {\\n              \\"text/plain\\": {\\n                \\"schema\\": {\\n                  \\"type\\": \\"array\\",\\n                  \\"items\\": {\\n                    \\"$ref\\": \\"#/components/schemas/WeatherForecast\\"\\n                  }\\n                }\\n              },\\n              \\"application/json\\": {\\n                \\"schema\\": {\\n                  \\"type\\": \\"array\\",\\n                  \\"items\\": {\\n                    \\"$ref\\": \\"#/components/schemas/WeatherForecast\\"\\n                  }\\n                }\\n              },\\n              \\"text/json\\": {\\n                \\"schema\\": {\\n                  \\"type\\": \\"array\\",\\n                  \\"items\\": {\\n                    \\"$ref\\": \\"#/components/schemas/WeatherForecast\\"\\n                  }\\n                }\\n              }\\n            }\\n          }\\n        }\\n      }\\n    }\\n  },\\n  \\"components\\": {\\n    \\"schemas\\": {\\n      \\"WeatherForecast\\": {\\n        \\"type\\": \\"object\\",\\n        \\"properties\\": {\\n          \\"date\\": {\\n            \\"type\\": \\"string\\",\\n            \\"format\\": \\"date-time\\"\\n          },\\n          \\"temperatureC\\": {\\n            \\"type\\": \\"integer\\",\\n            \\"format\\": \\"int32\\"\\n          },\\n          \\"temperatureF\\": {\\n            \\"type\\": \\"integer\\",\\n            \\"format\\": \\"int32\\",\\n            \\"readOnly\\": true\\n          },\\n          \\"summary\\": {\\n            \\"type\\": \\"string\\",\\n            \\"nullable\\": true\\n          }\\n        },\\n        \\"additionalProperties\\": false\\n      }\\n    }\\n  }\\n}\\n```\\n\\nOnly a single return type is defined: `WeatherForecast`.\\n\\n## Multiple return types\\n\\nNow we\'ve got our simple API, let\'s evolve it to serve up multiple types. We\'re going to do this by updating our `WeatherForecast.cs` as follows:\\n\\n```cs\\npublic class WeatherForecast\\n{\\n    public DateTime Date { get; set; }\\n\\n    public int TemperatureC { get; set; }\\n\\n    public int TemperatureF => 32 + (int)(TemperatureC / 0.5556);\\n\\n    public string? Summary { get; set; }\\n}\\n\\npublic class WeatherForecastWithLocation : WeatherForecast\\n{\\n    public string? Location { get; set; }\\n}\\n```\\n\\nWe now have both a `WeatherForecast` and a `WeatherForecastWithLocation` that inherits from `WeatherForecast` and adds in a `Location` property.\\n\\nWe\'ll also update the `GetWeatherForecast` endpoint to surface both these types:\\n\\n```cs\\n[HttpGet(Name = \\"GetWeatherForecast\\")]\\npublic IEnumerable<WeatherForecast> Get() =>\\n    DateTime.Now.Minute < 30\\n        ? Enumerable.Range(1, 5).Select(index => new WeatherForecast\\n        {\\n            Date = DateTime.Now.AddDays(index),\\n            TemperatureC = Random.Shared.Next(-20, 55),\\n            Summary = Summaries[Random.Shared.Next(Summaries.Length)]\\n        })\\n        : Enumerable.Range(1, 5).Select(index => new WeatherForecastWithLocation\\n        {\\n            Date = DateTime.Now.AddDays(index),\\n            TemperatureC = Random.Shared.Next(-20, 55),\\n            Summary = Summaries[Random.Shared.Next(Summaries.Length)],\\n            Location = \\"London\\"\\n        })\\n        .ToArray();\\n```\\n\\nWe\'ve amended the endpoint to return `WeatherForecast`s for the first thirty minutes of each hour, and `WeatherForecastWithLocation`s for the second thirty minutes. This is plainly a contrived example, but it demonstrates what it looks like to have an API endpoint with multiple return types.\\n\\nIncidentally, the reason we\'re able to achieve this without the compiler shouting at us is because our endpoint is saying it returns a `WeatherForecast` and that is the base type of `WeatherForecastWithLocation` as well.\\n\\nTo prove that it works, we wait for half past the hour and enter:\\n\\n```bash\\ncurl -X \'GET\' \'http://localhost:5000/WeatherForecast\'\\n```\\n\\nWe see back JSON that includes the `Location` property. Huzzah!\\n\\n```json\\n[\\n  {\\n    \\"location\\": \\"London\\",\\n    \\"date\\": \\"2022-03-07T08:51:02.0932353+00:00\\",\\n    \\"temperatureC\\": -4,\\n    \\"temperatureF\\": 25,\\n    \\"summary\\": \\"Bracing\\"\\n  },\\n  {\\n    \\"location\\": \\"London\\",\\n    \\"date\\": \\"2022-03-08T08:51:02.0938418+00:00\\",\\n    \\"temperatureC\\": -5,\\n    \\"temperatureF\\": 24,\\n    \\"summary\\": \\"Balmy\\"\\n  },\\n  {\\n    \\"location\\": \\"London\\",\\n    \\"date\\": \\"2022-03-09T08:51:02.0938513+00:00\\",\\n    \\"temperatureC\\": 51,\\n    \\"temperatureF\\": 123,\\n    \\"summary\\": \\"Warm\\"\\n  },\\n  {\\n    \\"location\\": \\"London\\",\\n    \\"date\\": \\"2022-03-10T08:51:02.0938518+00:00\\",\\n    \\"temperatureC\\": 35,\\n    \\"temperatureF\\": 94,\\n    \\"summary\\": \\"Warm\\"\\n  },\\n  {\\n    \\"location\\": \\"London\\",\\n    \\"date\\": \\"2022-03-11T08:51:02.0938537+00:00\\",\\n    \\"temperatureC\\": 2,\\n    \\"temperatureF\\": 35,\\n    \\"summary\\": \\"Cool\\"\\n  }\\n]\\n```\\n\\nWhilst we\'ve got behaviour that handles multiple return types, what we don\'t have is Swagger / Open API that represents that. Despite our tweaks, our Swagger / Open API definition remains unchanged.\\n\\n## Serving up subtypes\\n\\nIn a perfect world, C# would have support for discriminated unions, and we\'d be using [`oneOf`](https://swagger.io/docs/specification/data-models/oneof-anyof-allof-not/) to represent the multiple types being surfaced. [The day may come where C# supports discriminated unions](https://github.com/dotnet/csharplang/issues/113), but until that time we\'ll be achieving this behaviour with inheritance. We do this by having an endpoint that surfaces up a base type, and all our possible return types must either subclass that base type, or be that base type.\\n\\nTo be clearer: we want our served up Swagger / Open API definition to serve up the definitions of our subclasses. It needs to shout about `WeatherForecastWithLocation` in the same way it shouts about `WeatherForecast`.\\n\\nIt turns out that this is eminently achievable with Swashbuckle, but you do need to know where to look. [Look here](https://github.com/domaindrivendev/Swashbuckle.AspNetCore#describing-discriminators).\\n\\nTo apply this tweak to our own `Program.cs` we simply update the `AddSwaggerGen` as follows:\\n\\n```cs\\nbuilder.Services.AddSwaggerGen(swaggerGenOptions =>\\n{\\n    swaggerGenOptions.UseAllOfForInheritance();\\n    swaggerGenOptions.UseOneOfForPolymorphism();\\n\\n    swaggerGenOptions.SelectSubTypesUsing(baseType =>\\n        typeof(Program).Assembly.GetTypes().Where(type => type.IsSubclassOf(baseType))\\n    );\\n});\\n```\\n\\nThere\'s three things we\'re doing here:\\n\\n1. With [`UseAllOfForInheritance`](https://github.com/domaindrivendev/Swashbuckle.AspNetCore#enabling-inheritance) we\'re enabling inheritance - this allows us to maintain the inheritance hierarchy in any generated client models.\\n2. With [`UseOneOfForPolymorphism`](https://github.com/domaindrivendev/Swashbuckle.AspNetCore#enabling-polymorphism) we\'re listing the possible subtypes for an action that accepts/returns base types.\\n3. With [`SelectSubTypesUsing`](https://github.com/domaindrivendev/Swashbuckle.AspNetCore#detecting-subtypes) we\'re pointing Swashbuckle at the type hierarchies it exposes in the generated Swagger.\\n\\nThen next time we `dotnet run` we see that we\'re serving up both `WeatherForecast` and `WeatherForecastWithLocation`:\\n\\n![screenshot of swagger UI including `WeatherForecast` and `WeatherForecastWithLocation`](screenshot-swagger-ui-with-location.png)\\n\\nWe can also see this directly in the `swagger.json` created at our `http://localhost:5000/swagger/v1/swagger.json` endpoint:\\n\\n```json\\n{\\n  \\"openapi\\": \\"3.0.1\\",\\n  \\"info\\": {\\n    \\"title\\": \\"SwashbuckleInheritance\\",\\n    \\"version\\": \\"1.0\\"\\n  },\\n  \\"paths\\": {\\n    \\"/WeatherForecast\\": {\\n      \\"get\\": {\\n        \\"tags\\": [\\"WeatherForecast\\"],\\n        \\"operationId\\": \\"GetWeatherForecast\\",\\n        \\"responses\\": {\\n          \\"200\\": {\\n            \\"description\\": \\"Success\\",\\n            \\"content\\": {\\n              \\"text/plain\\": {\\n                \\"schema\\": {\\n                  \\"type\\": \\"array\\",\\n                  \\"items\\": {\\n                    \\"oneOf\\": [\\n                      {\\n                        \\"$ref\\": \\"#/components/schemas/WeatherForecast\\"\\n                      },\\n                      {\\n                        \\"$ref\\": \\"#/components/schemas/WeatherForecastWithLocation\\"\\n                      }\\n                    ]\\n                  }\\n                }\\n              },\\n              \\"application/json\\": {\\n                \\"schema\\": {\\n                  \\"type\\": \\"array\\",\\n                  \\"items\\": {\\n                    \\"oneOf\\": [\\n                      {\\n                        \\"$ref\\": \\"#/components/schemas/WeatherForecast\\"\\n                      },\\n                      {\\n                        \\"$ref\\": \\"#/components/schemas/WeatherForecastWithLocation\\"\\n                      }\\n                    ]\\n                  }\\n                }\\n              },\\n              \\"text/json\\": {\\n                \\"schema\\": {\\n                  \\"type\\": \\"array\\",\\n                  \\"items\\": {\\n                    \\"oneOf\\": [\\n                      {\\n                        \\"$ref\\": \\"#/components/schemas/WeatherForecast\\"\\n                      },\\n                      {\\n                        \\"$ref\\": \\"#/components/schemas/WeatherForecastWithLocation\\"\\n                      }\\n                    ]\\n                  }\\n                }\\n              }\\n            }\\n          }\\n        }\\n      }\\n    }\\n  },\\n  \\"components\\": {\\n    \\"schemas\\": {\\n      \\"WeatherForecast\\": {\\n        \\"type\\": \\"object\\",\\n        \\"properties\\": {\\n          \\"date\\": {\\n            \\"type\\": \\"string\\",\\n            \\"format\\": \\"date-time\\"\\n          },\\n          \\"temperatureC\\": {\\n            \\"type\\": \\"integer\\",\\n            \\"format\\": \\"int32\\"\\n          },\\n          \\"temperatureF\\": {\\n            \\"type\\": \\"integer\\",\\n            \\"format\\": \\"int32\\",\\n            \\"readOnly\\": true\\n          },\\n          \\"summary\\": {\\n            \\"type\\": \\"string\\",\\n            \\"nullable\\": true\\n          }\\n        },\\n        \\"additionalProperties\\": false\\n      },\\n      \\"WeatherForecastWithLocation\\": {\\n        \\"type\\": \\"object\\",\\n        \\"allOf\\": [\\n          {\\n            \\"$ref\\": \\"#/components/schemas/WeatherForecast\\"\\n          }\\n        ],\\n        \\"properties\\": {\\n          \\"location\\": {\\n            \\"type\\": \\"string\\",\\n            \\"nullable\\": true\\n          }\\n        },\\n        \\"additionalProperties\\": false\\n      }\\n    }\\n  }\\n}\\n```\\n\\nThere\'s two things to note about the new definition:\\n\\n1. The `WeatherForecastWithLocation` type is included in the `schemas`\\n2. The return type has widened to include `WeatherForecastWithLocation` as well using `oneOf`\\n\\n   ```json\\n   \\"oneOf\\": [\\n       {\\n           \\"$ref\\": \\"#/components/schemas/WeatherForecast\\"\\n       },\\n       {\\n           \\"$ref\\": \\"#/components/schemas/WeatherForecastWithLocation\\"\\n       }\\n   ]\\n   ```\\n\\nSuccess!"},{"id":"/2022/02/08/azure-static-web-apps-a-netlify-alternative","metadata":{"permalink":"/2022/02/08/azure-static-web-apps-a-netlify-alternative","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2022-02-08-azure-static-web-apps-a-netlify-alternative/index.md","source":"@site/blog/2022-02-08-azure-static-web-apps-a-netlify-alternative/index.md","title":"Azure Static Web Apps - a Netlify alternative","description":"Jamstack sites have taken the world by storm. There\'s currently fierce competition between offerings like Netlify and Cloudflare. A new player in this space is Azure Static Web Apps. This post will look at what working with SWAs is like and will demonstrate deploying one using GitHub Actions.","date":"2022-02-08T00:00:00.000Z","formattedDate":"February 8, 2022","tags":[{"label":"Azure Static Web Apps","permalink":"/tags/azure-static-web-apps"},{"label":"GitHub Actions","permalink":"/tags/git-hub-actions"},{"label":"Docusaurus","permalink":"/tags/docusaurus"}],"readingTime":7.65,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Azure Static Web Apps - a Netlify alternative","authors":"johnnyreilly","tags":["Azure Static Web Apps","GitHub Actions","Docusaurus"],"hide_table_of_contents":false},"prevItem":{"title":"Swashbuckle & inheritance: Give. Me. The. Types","permalink":"/2022/03/06/swashbuckle-inheritance-multiple-return-types"},"nextItem":{"title":"Lazy loading images with Docusaurus","permalink":"/2022/02/02/lazy-loading-images-with-docusaurus"}},"content":"Jamstack sites have taken the world by storm. There\'s currently fierce competition between offerings like [Netlify and Cloudflare](https://blog.logrocket.com/netlify-vs-cloudflare-pages/). A new player in this space is Azure Static Web Apps. This post will look at what working with SWAs is like and will demonstrate deploying one using GitHub Actions.\\n\\n## Jamstack and Azure Static Web Apps\\n\\n[Jamstack](https://en.m.wikipedia.org/wiki/Jamstack) stands for JavaScript, API and Markup In Jamstack websites, the application logic typically resides on the client side. Typically these clients are built as [single-page applications](https://en.m.wikipedia.org/wiki/Single-page_application) and often have HTML files statically generated for every possible path to support search engine optimization.\\n\\nAzure Static Web Apps were released for general use in [May 2021](https://azure.microsoft.com/en-us/updates/azure-static-web-apps-is-now-generally-available/) and offer features including:\\n\\n- Globally distributed content for production apps\\n- Auto-provisioned preview environments\\n- Custom domain configuration and free SSL certificates\\n- Built-in access to a variety of authentication providers\\n- Route-based authorization\\n- Custom routing\\n- Integration with serverless APIs powered by Azure Functions\\n- A custom Visual Studio Code developer extension\\n\\nSignificantly, [these features are available to use for free](https://azure.microsoft.com/en-gb/pricing/details/app-service/static/). With Netlify there is also a [free tier](https://www.netlify.com/pricing/), however it\'s quite easy to exceed the build limits of the free tier and land yourself with an unexpected bill. By combining Azure Static Web Apps with GitHub Actions we can build comparable experiences and save ourselves money!\\n\\nSo let\'s build ourselves a simple SWA and deploy it with GitHub Actions.\\n\\n## Create our application\\n\\nInside the root of our repository we\'re going to create a [Docusaurus site](https://docusaurus.io/). Docusaurus is a good example of a static site, the kind of which is a natural fit for Jamstack. We could equally use something else like [Hugo](https://gohugo.io/) for instance.\\n\\nAt the command line we\'ll enter:\\n\\n```shell\\nnpx create-docusaurus@latest website classic\\n```\\n\\nAnd Docusaurus will create a new site in the `website` directory. Let\'s commit and push this and turn our attention to Azure.\\n\\n## Creating a Static Web App in Azure\\n\\nThere\'s a number of ways to create a Static Web App in Azure. It\'s possible to use [infrastructure as code with a language like Bicep](https://blog.johnnyreilly.com/2021/08/15/bicep-azure-static-web-apps-azure-devops#bicep-template). But for this post let\'s use the [Azure Portal](https://portal.azure.com) instead. If you don\'t have an account already, you can set one up for free very quickly.\\n\\nOnce you\'ve logged in, click \\"Create a resource\\" and look up Static Web App:\\n\\n![Screenshot of the Azure Portal, \\"Create a resource\\" Azure Static Web Apps section](screenshot-azure-portal-create-a-resource.png)\\n\\nClick on \\"Create\\" and you\'ll be take to the creation dialog:\\n\\n![Screenshot of the Azure Portal, \\"Create a resource\\" Azure Static Web Apps dialog](screenshot-azure-portal-create-a-resource-dialog.png)\\n\\nYou\'ll need to create a resource group for your SWA to live in, give the app a name, the \\"Free\\" plan and a deployment source of GitHub.\\n\\nClick on the \\"Sign in with GitHub\\" button and authorize Azure to access your GitHub account for Static Web Apps.\\n\\n![Screenshot of the Azure Portal, \\"Create a resource\\" Azure Static Web Apps dialog - repository settings](screenshot-azure-portal-create-a-resource-dialog-repo.png)\\n\\nAt this point Azure will query GitHub on your behalf and look up the organisations and repositories you have access to. Select the repository that you\'d like to deploy to your Static Web App and select the branch you\'d like to deploy.\\n\\nYou also need to provide Azure with some build details that help it understand how your app is built. We\'ll provide a preset of \\"Custom\\". We\'ll set the \\"App location\\" (the root of our front end app) to be `\\"/website\\"` to tally up with the application we just created. We\'ll leave \\"Api location\\" blank and we\'ll set the output location to be `\\"build\\"` - this is the directory under `website` where Docusaurus will create our site.\\n\\nFinally click \\"Review + create\\" and then \\"Create\\".\\n\\nAzure will now:\\n\\n- Create an Azure Static Web app resource in Azure\\n- Update your repository to add a GitHub Actions workflow to deploy your static web app\\n- Kick off a first run of the GitHub Actions workflow to deploy your SWA.\\n\\nPretty amazing, right?\\n\\nWhen you look at the resource in Azure it will look something like this:\\n\\n![Screenshot of the Azure Portal, your Azure Static Web Apps resource](screenshot-azure-portal-static-web-app-resource.png)\\n\\nIf you click on the GitHub Action runs you\'ll be presented with your GitHub Action:\\n\\n![Screenshot of the GitHub Action](screenshot-github-action.png)\\n\\nAnd when that finishes running you\'ll be able to see your deployed Static Web App by clicking on the URL in the Azure Portal:\\n\\n![Screenshot of your Static Web App running in a browser](screenshot-static-web-app.png)\\n\\nWe\'ve gone from having nothing, to having a brand new website in Azure, shipped via continous deployment in GitHub Actions in a matter of minutes. This is low friction and high value!\\n\\n## Authentication\\n\\nNow we\'ve done our initial deployment, let\'s take it a stage further and add authentication.\\n\\nOne of the awesome features of Static Web Apps is the fact that [authentication is available straight out of the box](https://docs.microsoft.com/en-us/azure/static-web-apps/authentication-authorization?tabs=invitations#login). We can pick from GitHub, Azure Active Directory and Twitter as identity providers. Let\'s roll with GitHub and amend our `website/src/pages/index.js` to support authentication:\\n\\n```jsx\\nimport React, { useState, useEffect } from \'react\';\\nimport clsx from \'clsx\';\\nimport Layout from \'@theme/Layout\';\\nimport useDocusaurusContext from \'@docusaurus/useDocusaurusContext\';\\nimport styles from \'./index.module.css\';\\n\\n/**\\n * @typedef {object} UserInfo\\n * @prop {\\"github\\"} identityProvider\\n * @prop {string} userId\\n * @prop {string} userDetails\\n * @prop {string[]} userRoles\\n */\\n\\n/**\\n * @return {UserInfo | null}\\n */\\nfunction useUserInfo() {\\n  const [userInfo, setUserInfo] = useState(null);\\n\\n  useEffect(() => {\\n    async function getUserInfo() {\\n      const response = await fetch(\'/.auth/me\');\\n      const payload = await response.json();\\n      const { clientPrincipal } = payload;\\n      return clientPrincipal;\\n    }\\n\\n    getUserInfo().then((ui) => setUserInfo(ui));\\n  }, []);\\n\\n  return userInfo;\\n}\\n\\nexport default function Home() {\\n  const { siteConfig } = useDocusaurusContext();\\n  const userInfo = useUserInfo();\\n\\n  return (\\n    <Layout\\n      title={`Hello from ${siteConfig.title}`}\\n      description=\\"Description will go into a meta tag in <head />\\"\\n    >\\n      <header className={clsx(\'hero hero--primary\', styles.heroBanner)}>\\n        <div className=\\"container\\">\\n          <h1 className=\\"hero__title\\">{siteConfig.title}</h1>\\n          <p className=\\"hero__subtitle\\">{siteConfig.tagline}</p>\\n          <div className={styles.buttons}>\\n            {userInfo ? (\\n              <p>Hello {userInfo.userDetails}</p>\\n            ) : (\\n              <a\\n                className=\\"button button--secondary button--lg\\"\\n                href=\\"/.auth/login/github\\"\\n              >\\n                Click here to login\\n              </a>\\n            )}\\n          </div>\\n        </div>\\n      </header>\\n    </Layout>\\n  );\\n}\\n```\\n\\nThe above code does the following:\\n\\n- Implements a React hook named `useUserInfo` which calls the `/.auth/me` endpoint of your SWA. This returns `null` when not authenticated, and the `UserInfo` when authenticated.\\n- For users who are not authenticated, display a link button which takes people to `/.auth/login/github`, thus triggering the GitHub authentication flow.\\n- For users who are authenticated, display the users `userDetails`; the GitHub username.\\n\\nLet\'s commit and push this and (when our build has finished running) browse to our Static Web App once again:\\n\\n![Screenshot of Static Web App now featuring a login button](screenshot-static-web-app-login.png)\\n\\nIf we click to login, we\'re taken through the GitHub authentication flow:\\n\\n![Screenshot of Static Web App now featuring a login button](screenshot-static-web-app-login-github.png)\\n\\nOnce you\'ve authorised and granted consent you\'ll be redirected to your app and see that you\'re logged in:\\n\\n![Screenshot of Static Web App showing I\'m logged in](screenshot-static-web-app-logged-in.png)\\n\\nIf we pop open the devtools of Chrome we\'ll see what comes back from the `/.auth/me` endpoint:\\n\\n![Screenshot of Chrome devtools displaying a JSON structure](screenshot-static-web-app-devtools-me.png)\\n\\n```json\\n{\\n  \\"clientPrincipal\\": {\\n    \\"identityProvider\\": \\"github\\",\\n    \\"userId\\": \\"1f5b4b7de7d445e29dd6188bcc7ee052\\",\\n    \\"userDetails\\": \\"johnnyreilly\\",\\n    \\"userRoles\\": [\\"anonymous\\", \\"authenticated\\"]\\n  }\\n}\\n```\\n\\nWe\'ve now implemented and demonstrated authentication with Azure Static Web Apps with very little effort on our behalf. This is tremendous!\\n\\n## Staging Environments\\n\\nFinally, let\'s look at a super cool feature that Static Web Apps provides by default. If you take a look at the Environments tab of your SWA you\'ll see this:\\n\\n![Screenshot of the Azure Portal, your Azure Static Web Apps resource - featuring the phrase \\"Open pull requests against the linked repository to create a staging environment.\\"](screenshot-azure-portal-static-web-app-resource-environments.png)\\n\\n> ## Staging\\n>\\n> Open pull requests against the linked repository to create a staging environment.\\n\\nLet\'s try that out! We\'ll create a new branch:\\n\\n```shell\\ngit checkout -b feat/show-me-staging\\n```\\n\\nIn our `index.js` we\'ll add an arbitrary piece of text:\\n\\n```jsx\\n<p>I\'m a staging environment!</p>\\n```\\n\\nThen we\'ll commit and push our branch to GitHub and create a pull request. This triggers our GitHub Action to run once again. But this time, rather than publishing over our existing Static Web App, it\'s going to spin up a brand new one with our changes in. Not only that, it\'s going to put a link for us in our GitHub pull request so we can browse straight to it:\\n\\n![Screenshot of the pull request in GitHub including a comment from the GitHub Actions bot which says: \\"Azure Static Web Apps: Your stage site is ready! Visit it here: https://ambitious-island-05069ea10-2.centralus.azurestaticapps.net\\"](screenshot-github-pull-request-deploy-preview.png)\\n\\nThis is the equivalent of Netlify Deploy Previews, implemented with Azure Static Web Apps and GitHub Actions. Given the allowances for GitHub Actions currently sit at [2,000 free minutes per month](https://docs.github.com/en/billing/managing-billing-for-github-actions/about-billing-for-github-actions) as compared with Netlify\'s [300 free minutes per month](https://www.netlify.com/pricing/), you\'re less likely to receive a bill for using Static Web Apps.\\n\\nThis staging environment will last only until the pull request is closed. At that point the environment is torn down by the GitHub Action.\\n\\n## Conclusion\\n\\nIn this post we\'ve deployed a website to a Static Web App using GitHub Actions and implemented authentication. We\'ve also demonstrated Azure\'s equivalent of Netlify\'s deploy previews; staging environments.\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/azure-static-web-apps-netlify-alternative/)"},{"id":"/2022/02/02/lazy-loading-images-with-docusaurus","metadata":{"permalink":"/2022/02/02/lazy-loading-images-with-docusaurus","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2022-02-02-lazy-loading-images-with-docusaurus/index.md","source":"@site/blog/2022-02-02-lazy-loading-images-with-docusaurus/index.md","title":"Lazy loading images with Docusaurus","description":"If you\'d like to improve the performance of a Docusaurus website by implementing native lazy-loading of images, you can. This post shows you how you too can have <img loading=\\"lazy\\"  on your images by writing a Rehype plugin.","date":"2022-02-02T00:00:00.000Z","formattedDate":"February 2, 2022","tags":[{"label":"Docusaurus","permalink":"/tags/docusaurus"},{"label":"lazy load images","permalink":"/tags/lazy-load-images"},{"label":"rehype","permalink":"/tags/rehype"},{"label":"rehype plugin","permalink":"/tags/rehype-plugin"}],"readingTime":2.95,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Lazy loading images with Docusaurus","authors":"johnnyreilly","tags":["Docusaurus","lazy load images","rehype","rehype plugin"],"image":"./title-image.png","hide_table_of_contents":false},"prevItem":{"title":"Azure Static Web Apps - a Netlify alternative","permalink":"/2022/02/08/azure-static-web-apps-a-netlify-alternative"},"nextItem":{"title":"Migrating from GitHub Pages to Azure Static Web Apps","permalink":"/2022/02/01/migrating-from-github-pages-to-azure-static-web-apps"}},"content":"If you\'d like to improve the performance of a Docusaurus website by implementing native lazy-loading of images, you can. This post shows you how you too can have `<img loading=\\"lazy\\" ` on your images by writing a Rehype plugin.\\n\\n![title image reading \\"Lazy loading images with Docusaurus\\" with a Docusaurus logo and an image that reads `<img loading=\\"lazy\\" `](title-image.png)\\n\\n## Update 26/02/2022\\n\\nYou don\'t need this anymore. As of Docusaurus [v2.0.0-beta.16](https://github.com/facebook/docusaurus/releases/tag/v2.0.0-beta.16) Docusaurus lazy loads markdown images by default. You can see the commit where it was added [here](https://github.com/facebook/docusaurus/pull/6598). Isn\'t that wonderful?\\n\\n\u2705cumulative no of network requests for Docusaurus sites will go \ud83d\udc47\\n\u2705perceived performance will go \u261d\ufe0f\\n\u2705hosting costs will go \ud83d\udc47\\n\\n## Lazy loading images\\n\\nNative browser lazy loading for images is a relatively recent innovation. To read more on the topic, [do look at this post](https://web.dev/browser-level-image-lazy-loading/). The TL;DR is this though: by adding the `loading=\\"lazy\\"` attribute to an `img` element, modern browsers will delay loading the image until it is needed. This provides better performance to your users: when it comes to loading, less is more.\\n\\n## Docusaurus\\n\\nIf you\'re using Docusaurus then you\'re likely writing Markdown. I am. This blog is written using Markdown, and converted, using [MDX plugins](https://docusaurus.io/docs/next/markdown-features/plugins) into JSX. This handles images as well as we can [see here](https://github.com/facebook/docusaurus/blob/6ec0db4722cbf988fd5280a4442223637c2de8d7/packages/docusaurus-mdx-loader/src/remark/transformImage/index.ts#L79):\\n\\n```ts\\njsxNode.value = `<img ${alt}src={${src}}${title}${width}${height} />`;\\n```\\n\\nThe crucial thing to note about the above, is the lack of the `loading=\\"lazy\\"` attribute. Can we add that somehow? Yes we can!\\n\\n## Rehype plugin\\n\\nTo do this, we\'re going to write our own mini [rehype plugin](https://github.com/rehypejs) that will take the HTML being pumped out of Docusaurus and add the `loading=\\"lazy\\"` attribute.\\n\\nAlongside our `docusaurus.config.js` we\'re going to create a `image-lazy-remark-plugin.js` file:\\n\\n```js\\nconst visit = require(\'unist-util-visit\');\\n\\n/** @type {import(\'unified\').Plugin<[], import(\'hast\').Root>} */\\nfunction lazyLoadImagesPlugin() {\\n  return (tree) => {\\n    visit(tree, [\'element\', \'jsx\'], (node) => {\\n      if (node.type === \'element\' && node.tagName === \'img\') {\\n        // handles nodes like this:\\n\\n        // {\\n        //   type: \'element\',\\n        //   tagName: \'img\',\\n        //   properties: {\\n        //     src: \'https://some.website.com/cat.gif\',\\n        //     alt: null\\n        //   },\\n        //   ...\\n        // }\\n\\n        node.properties.loading = \'lazy\';\\n      } else if (node.type === \'jsx\' && node.value.includes(\'<img \')) {\\n        // handles nodes like this:\\n\\n        // {\\n        //   type: \'jsx\',\\n        //   value: \'<img src={require(\\"!/workspaces/blog.johnnyreilly.com/blog-website/node_modules/url-loader/dist/cjs.js?limit=10000&name=assets/images/[name]-[hash].[ext]&fallback=/workspaces/blog.johnnyreilly.com/blog-website/node_modules/file-loader/dist/cjs.js!./bower-with-the-long-paths.png\\").default} width=\\"640\\" height=\\"497\\" />\'\\n        // }\\n\\n        node.value = node.value.replace(/<img /g, \'<img loading=\\"lazy\\" \');\\n      }\\n    });\\n  };\\n}\\n\\nmodule.exports = lazyLoadImagesPlugin;\\n```\\n\\nAs the code above suggests, it looks for `img` elements, whether they be in HTML or JSX, and adds in the `loading=\\"lazy\\"` attribute.\\n\\nTo apply this to our blog, we simply tweak the `docusaurus.config.js` file to make use of our plugin:\\n\\n```js\\nconst imageLazyRemarkPlugin = require(\'./image-lazy-remark-plugin\');\\n\\n// ...\\n\\n/** @type {import(\'@docusaurus/types\').Config} */\\nconst config = {\\n  // ...\\n\\n  presets: [\\n    [\\n      \'@docusaurus/preset-classic\',\\n      /** @type {import(\'@docusaurus/preset-classic\').Options} */\\n      ({\\n        // ...\\n        blog: {\\n          // ...\\n          rehypePlugins: [imageLazyRemarkPlugin],\\n        },\\n        // ...\\n      }),\\n    ],\\n  ],\\n  // ...\\n};\\n```\\n\\n## What\'s the result?\\n\\nWith this in place, next time we run a build, we can see the attribute being applied to our image elements:\\n\\n![screenshot of an img element with the loading=\\"lazy\\" attribute set](screenshot-of-img-loading-lazy-element.png)\\n\\nConsequently, when we fire up devtools we can see that only the images onscreen are being loaded. In the example below we\'re _not_ seeing five other images being loaded because they\'re offscreen and haven\'t been scrolled to as yet:\\n\\n![screenshot of chrome devtools showing only two images being loaded - the ones that are on the screen](screenshot-of-chrome-devtools-showing-only-onscreen-images-loaded.png)\\n\\nAmazing! It works! It\'s possible that this could land directly in Docusaurus one day. [Go here to follow the discussion on this.](https://docusaurus.io/feature-requests/p/lazy-loading-images-in-blog-posts-by-default)"},{"id":"/2022/02/01/migrating-from-github-pages-to-azure-static-web-apps","metadata":{"permalink":"/2022/02/01/migrating-from-github-pages-to-azure-static-web-apps","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2022-02-01-migrating-from-github-pages-to-azure-static-web-apps/index.md","source":"@site/blog/2022-02-01-migrating-from-github-pages-to-azure-static-web-apps/index.md","title":"Migrating from GitHub Pages to Azure Static Web Apps","description":"You can use Bicep and GitHub Actions to build and deploy to a static website on Azure Static Web Apps. This post demonstrates how.","date":"2022-02-01T00:00:00.000Z","formattedDate":"February 1, 2022","tags":[{"label":"Azure Static Web Apps","permalink":"/tags/azure-static-web-apps"},{"label":"Bicep","permalink":"/tags/bicep"},{"label":"GitHub Actions","permalink":"/tags/git-hub-actions"},{"label":"GitHub Pages","permalink":"/tags/git-hub-pages"}],"readingTime":7.91,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Migrating from GitHub Pages to Azure Static Web Apps","authors":"johnnyreilly","tags":["Azure Static Web Apps","Bicep","GitHub Actions","GitHub Pages"],"image":"./title-image.png","hide_table_of_contents":false},"prevItem":{"title":"Lazy loading images with Docusaurus","permalink":"/2022/02/02/lazy-loading-images-with-docusaurus"},"nextItem":{"title":"Azure Container Apps: dapr, devcontainer, debug and deploy","permalink":"/2022/01/22/azure-container-apps-dapr-bicep-github-actions-debug-devcontainer"}},"content":"You can use Bicep and GitHub Actions to build and deploy to a static website on Azure Static Web Apps. This post demonstrates how.\\n\\n![title image reading \\"Migrating from GitHub Pages to Azure Static Web Apps\\" with GitHub and Azure Static Web Apps logos](title-image.png)\\n\\n## Why migrate?\\n\\nThis blog has been hosted on GitHub Pages for some time. It also makes use of Netlify for deployment previews. These are both great, but it\'s always niggled that there\'s two mechanisms in play; each separately configured. It\'s time to simplify.\\n\\n[Azure Static Web Apps](https://azure.microsoft.com/en-us/services/app-service/static/) supports both hosting static websites and deployment previews (known as \\"staging environments\\"). So we\'re going to migrate across to use Static Web Apps in place of both of GitHub Pages and Netlify. I\'m choosing to use Bicep to do this as I tend towards using infrastructure as code. If you wanted to roll with a more \\"point and click\\" approach in the Azure Portal, you could do that too. Simply ignore the Bicep related portions of the post.\\n\\n## Bicep\\n\\nThe first thing we\'re going to need is a Bicep template to deploy our SWA. In our GitHub repo we\'re going to add a `infra` folder, and in there we\'ll create a `main.bicep` file:\\n\\n```bicep\\nparam location string\\nparam branch string\\nparam name string\\nparam tags object\\n@secure()\\nparam repositoryToken string\\nparam customDomainName string\\n\\nresource staticWebApp \'Microsoft.Web/staticSites@2021-02-01\' = {\\n  name: name\\n  location: location\\n  tags: tags\\n  sku: {\\n    name: \'Free\'\\n    tier: \'Free\'\\n  }\\n  properties: {\\n    repositoryUrl: \'https://github.com/johnnyreilly/blog.johnnyreilly.com\'\\n    repositoryToken: repositoryToken\\n    branch: branch\\n    provider: \'GitHub\'\\n    stagingEnvironmentPolicy: \'Enabled\'\\n    allowConfigFileUpdates: true\\n    buildProperties:{\\n      skipGithubActionWorkflowGeneration: true\\n    }\\n  }\\n}\\n\\n// resource customDomain \'Microsoft.Web/staticSites/customDomains@2021-02-01\' = {\\n//   parent: staticWebApp\\n//   name: customDomainName\\n//   properties: {}\\n// }\\n\\noutput staticWebAppDefaultHostName string = staticWebApp.properties.defaultHostname // eg gentle-bush-0db02ce03.azurestaticapps.net\\noutput staticWebAppId string = staticWebApp.id\\noutput staticWebAppName string = staticWebApp.name\\n```\\n\\nMost of the Bicep template above is self-explanatory. There\'s a few things to highlight:\\n\\n- We\'re using the \\"Free\\" SKU which means we don\'t have to pay to run our website.\\n- We need to provide a `repositoryToken` - this is a little surprising as you\'ll see later in the template that we supply the `skipGithubActionWorkflowGeneration: true` which means we\'re _not_ requiring our SWA to interact with GitHub on our behalf - but it seems that there\'s a requirement for a GitHub token anyway. We\'ll roll with it.\\n- We\'re enabling deployment previews / staging environments with `stagingEnvironmentPolicy: \'Enabled\'`\\n- The `branch` is always set to `main` - we have to let Azure know this so it knows which branch is the primary branch and hence which other ones will have staging environments.\\n- It also includes a section for the custom domain which is commented out - we\'ll uncomment that later once we\'ve set up our custom domain / DNS.\\n\\n## Setting up a resource group\\n\\nWith our Bicep in place, we\'re going to need a resource group to send it to. We\'re going to create ourselves a resource group in West Europe:\\n\\n```shell\\naz group create -g rg-blog-johnnyreilly-com -l westeurope\\n```\\n\\n## Secrets for GitHub Actions\\n\\nWe\'re aiming to set up a GitHub Action to handle our deployment which depends upon some secrets.\\n\\n### `AZURE_CREDENTIALS` - GitHub logging into Azure\\n\\nFirst a `AZURE_CREDENTIALS` secret that facilitates GitHub logging into Azure. We\'ll use the Azure CLI to create this:\\n\\n```shell\\naz ad sp create-for-rbac --name \\"myApp\\" --role contributor \\\\\\n    --scopes /subscriptions/{subscription-id}/resourceGroups/{resource-group} \\\\\\n    --sdk-auth\\n```\\n\\nRemember to replace the `{subscription-id}` with your subscription id and `{resource-group}` with the name of your resource group (`rg-blog-johnnyreilly-com` if you\'re following along). This command will pump out a lump of JSON that looks something like this:\\n\\n```json\\n{\\n  \\"clientId\\": \\"a-client-id\\",\\n  \\"clientSecret\\": \\"a-client-secret\\",\\n  \\"subscriptionId\\": \\"a-subscription-id\\",\\n  \\"tenantId\\": \\"a-tenant-id\\",\\n  \\"activeDirectoryEndpointUrl\\": \\"https://login.microsoftonline.com\\",\\n  \\"resourceManagerEndpointUrl\\": \\"https://management.azure.com/\\",\\n  \\"activeDirectoryGraphResourceId\\": \\"https://graph.windows.net/\\",\\n  \\"sqlManagementEndpointUrl\\": \\"https://management.core.windows.net:8443/\\",\\n  \\"galleryEndpointUrl\\": \\"https://gallery.azure.com/\\",\\n  \\"managementEndpointUrl\\": \\"https://management.core.windows.net/\\"\\n}\\n```\\n\\nTake this and save it as the `AZURE_CREDENTIALS` secret in GitHub.\\n\\n### `WORKFLOW_TOKEN` - Azure accessing the GitHub container registry\\n\\nWe also need a secret for updating workflows from Azure. Azure Static Web Apps can update your workflow - they need access to do this when we\'re deploying. To facilitate this we\'ll set up a `WORKFLOW_TOKEN` secret. This is a GitHub personal access token with the `workflow` scope. [Follow the instructions here to create the token.](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token)\\n\\nIronically, we\'re not planning to use this functionality, but the validation for the Bicep template will fail if it isn\'t supplied.\\n\\n## Deploying with GitHub Actions\\n\\nWith our secrets configured, we\'re now well placed to update our GitHub Action. We\'ll tweak the content of `.github/workflows/build-and-deploy.yaml` file in our repository to the following:\\n\\n```yaml\\nname: Build and Deploy\\n\\non:\\n  push:\\n    branches:\\n      - main\\n  pull_request:\\n    types: [opened, synchronize, reopened, closed]\\n    branches:\\n      - main\\n\\nenv:\\n  RESOURCE_GROUP: rg-blog-johnnyreilly-com\\n  LOCATION: westeurope\\n  STATICWEBAPPNAME: blog.johnnyreilly.com\\n  TAGS: \'{\\"owner\\":\\"johnnyreilly\\", \\"email\\":\\"johnny_reilly@hotmail.com\\"}\'\\n\\njobs:\\n  build_and_deploy_swa_job:\\n    if: github.event_name == \'push\' || (github.event_name == \'pull_request\' && github.event.action != \'closed\')\\n    runs-on: ubuntu-latest\\n    name: Build and deploy\\n    steps:\\n      - uses: actions/checkout@v2\\n        with:\\n          submodules: true\\n\\n      - name: Azure Login\\n        uses: azure/login@v1\\n        with:\\n          creds: ${{ secrets.AZURE_CREDENTIALS }}\\n\\n      - name: Set Deployment Name\\n        id: deployment_name\\n        run: |\\n          REF_SHA=\'${{ github.ref }}.${{ github.sha }}\'\\n          DEPLOYMENT_NAME=\\"${REF_SHA////-}\\"\\n          echo \\"::set-output name=DEPLOYMENT_NAME::$DEPLOYMENT_NAME\\"\\n\\n      - name: Static Web App - change details\\n        id: static_web_app_what_if\\n        if: github.event_name == \'pull_request\'\\n        uses: azure/CLI@v1\\n        with:\\n          inlineScript: |\\n            az deployment group what-if \\\\\\n              --resource-group ${{ env.RESOURCE_GROUP }} \\\\\\n              --name \\"${{ steps.deployment_name.outputs.DEPLOYMENT_NAME }}\\" \\\\\\n              --template-file ./infra/main.bicep \\\\\\n              --parameters \\\\\\n                  branch=\'main\' \\\\\\n                  location=\'${{ env.LOCATION }}\' \\\\\\n                  name=\'${{ env.STATICWEBAPPNAME }}\' \\\\\\n                  tags=\'${{ env.TAGS }}\' \\\\\\n                  repositoryToken=\'${{ secrets.WORKFLOW_TOKEN }}\' \\\\\\n                  customDomainName=\'${{ env.STATICWEBAPPNAME }}\'\\n\\n      - name: Static Web App - deploy infra\\n        id: static_web_app_deploy\\n        if: github.event_name != \'pull_request\'\\n        uses: azure/CLI@v1\\n        with:\\n          inlineScript: |\\n            az deployment group create \\\\\\n              --resource-group ${{ env.RESOURCE_GROUP }} \\\\\\n              --name \\"${{ steps.deployment_name.outputs.DEPLOYMENT_NAME }}\\" \\\\\\n              --template-file ./infra/main.bicep \\\\\\n              --parameters \\\\\\n                  branch=\'main\' \\\\\\n                  location=\'${{ env.LOCATION }}\' \\\\\\n                  name=\'${{ env.STATICWEBAPPNAME }}\' \\\\\\n                  tags=\'${{ env.TAGS }}\' \\\\\\n                  repositoryToken=\'${{ secrets.WORKFLOW_TOKEN }}\' \\\\\\n                  customDomainName=\'${{ env.STATICWEBAPPNAME }}\'\\n\\n      - name: Static Web App - get API key for deployment\\n        id: static_web_app_apikey\\n        uses: azure/CLI@v1\\n        with:\\n          inlineScript: |\\n            APIKEY=$(az staticwebapp secrets list --name \'${{ env.STATICWEBAPPNAME }}\' | jq -r \'.properties.apiKey\')\\n            echo \\"::set-output name=APIKEY::$APIKEY\\"\\n\\n      - name: Static Web App - build and deploy\\n        id: static_web_app_build_and_deploy\\n        uses: Azure/static-web-apps-deploy@v1\\n        with:\\n          azure_static_web_apps_api_token: ${{ steps.static_web_app_apikey.outputs.APIKEY }}\\n          repo_token: ${{ secrets.GITHUB_TOKEN }} # Used for Github integrations (i.e. PR comments)\\n          action: \'upload\'\\n          ###### Repository/Build Configurations - These values can be configured to match your app requirements. ######\\n          # For more information regarding Static Web App workflow configurations, please visit: https://aka.ms/swaworkflowconfig\\n          app_location: \'/blog-website\' # App source code path\\n          api_location: \'\' # Api source code path - optional\\n          output_location: \'build\' # Built app content directory - optional\\n          ###### End of Repository/Build Configurations ######\\n\\n      - name: Static Web App - get preview URL\\n        id: static_web_app_preview_url\\n        uses: azure/CLI@v1\\n        with:\\n          inlineScript: |\\n            DEFAULTHOSTNAME=$(az staticwebapp show -n \'${{ env.STATICWEBAPPNAME }}\' | jq -r \'.defaultHostname\')\\n            echo $DEFAULTHOSTNAME\\n\\n            PREVIEW_URL=\\"https://${DEFAULTHOSTNAME/.[1-9]./-${{github.event.pull_request.number }}.${{ env.LOCATION }}.1.}\\"\\n            echo $PREVIEW_URL\\n\\n            echo \\"::set-output name=PREVIEW_URL::$PREVIEW_URL\\"\\n\\n    outputs:\\n      preview-url: ${{steps.static_web_app_preview_url.outputs.PREVIEW_URL}}\\n\\n  close_pull_request_job:\\n    if: github.event_name == \'pull_request\' && github.event.action == \'closed\'\\n    runs-on: ubuntu-latest\\n    name: Cleanup Pull Request staging environment\\n    steps:\\n      - name: Azure Login\\n        uses: azure/login@v1\\n        with:\\n          creds: ${{ secrets.AZURE_CREDENTIALS }}\\n\\n      - name: Get API key for deployment\\n        id: apikey\\n        uses: azure/CLI@v1\\n        with:\\n          inlineScript: |\\n            APIKEY=$(az staticwebapp secrets list --name \'${{ env.STATICWEBAPPNAME }}\' | jq -r \'.properties.apiKey\')\\n            echo \\"::set-output name=APIKEY::$APIKEY\\"\\n\\n      - name: Close Pull Request\\n        id: closepullrequest\\n        uses: Azure/static-web-apps-deploy@v1\\n        with:\\n          azure_static_web_apps_api_token: ${{ steps.apikey.outputs.APIKEY }}\\n          action: \'close\'\\n```\\n\\nThe above workflow does the following:\\n\\n- For main branch deployments it releases our static web app making use of Bicep. For pull requests it tells us if there\'s any changes that the current PR would make to our SWA as a consequence.\\n- It acquires an API Key from Azure which can then be used to perform a deployment.\\n- It deploys [using the dedicated GitHub Action for SWAs](https://github.com/Azure/static-web-apps-deploy)\\n- It calculates the preview URL for a given pull request (it isn\'t used as yet, but could be)\\n- When a pull request is closed it triggers the GitHub Action to clean up the preview environment.\\n\\n## DNS and custom domains\\n\\nOnce our GitHub Action has run for the first time on the main branch, we\'ll be deploying to Azure Static Web Apps.\\n\\nOnce we\'ve started deploying there, we want to get our custom domain set up to point to it. To do this, we\'re going to fire up the [Azure Portal](https://portal.azure.com) and go to add a custom domain:\\n\\n![screenshot of the Azure Portal Add Custom Domain screen](./custom-domain.png)\\n\\nWe\'re going to add a TXT record for my blog. Azure generates a code for us:\\n\\n![screenshot of the Azure Portal Add Custom Domain screen](./custom-domain-code.png)\\n\\nWe need to take that code and go a register it with our DNS provider. In my case that\'s Cloudflare, so we can go there and add it:\\n\\n![screenshot of Cloudflare](./cloudflare-dns.png)\\n\\nAfter a while (I think about twenty minutes in my case), this lead to the domain name being validated:\\n\\n![screenshot of the Azure Portal Add Custom Domain screen with domain validated](./custom-domain-code-validated.png)\\n\\nNow that we have a custom domain set up in Azure, we want to uncomment the `resource customDomain` portion of the Bicep template now as well:\\n\\n```bicep\\nresource customDomain \'Microsoft.Web/staticSites/customDomains@2021-02-01\' = {\\n  parent: staticWebApp\\n  name: customDomainName\\n  properties: {}\\n}\\n```\\n\\nThis will mean that subsequent deployments to Azure do _not_ wipe out our newly configured domain name.\\n\\nWe\'re now ready to start pointing our DNS to the Static Web Apps instance. We jump back across to Cloudflare and we amend the CNAME record that currently points to johnnyreilly.github.io, and switch it to point to the auto-generated domain in Azure:\\n\\n![screenshot of Cloudflare with the CNAME record set](./cloudflare-dns-cname.png)\\n\\nAnd just like that, we\'re hosted on Static Web Apps!"},{"id":"/2022/01/22/azure-container-apps-dapr-bicep-github-actions-debug-devcontainer","metadata":{"permalink":"/2022/01/22/azure-container-apps-dapr-bicep-github-actions-debug-devcontainer","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2022-01-22-azure-container-apps-dapr-bicep-github-actions-debug-devcontainer/index.md","source":"@site/blog/2022-01-22-azure-container-apps-dapr-bicep-github-actions-debug-devcontainer/index.md","title":"Azure Container Apps: dapr, devcontainer, debug and deploy","description":"This post shows how to build and deploy two Azure Container Apps using Bicep and GitHub Actions. These apps will communicate using dapr, be built in VS Code using a devcontainer. It will be possible to debug in VS Code and run with docker-compose.","date":"2022-01-22T00:00:00.000Z","formattedDate":"January 22, 2022","tags":[{"label":"Azure Container Apps","permalink":"/tags/azure-container-apps"},{"label":"Bicep","permalink":"/tags/bicep"},{"label":"GitHub Actions","permalink":"/tags/git-hub-actions"},{"label":"GitHub container registry","permalink":"/tags/git-hub-container-registry"},{"label":"devcontainer","permalink":"/tags/devcontainer"},{"label":"debug","permalink":"/tags/debug"}],"readingTime":21.49,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Azure Container Apps: dapr, devcontainer, debug and deploy","authors":"johnnyreilly","tags":["Azure Container Apps","Bicep","GitHub Actions","GitHub container registry","devcontainer","debug"],"image":"./title-image.png","hide_table_of_contents":false},"prevItem":{"title":"Migrating from GitHub Pages to Azure Static Web Apps","permalink":"/2022/02/01/migrating-from-github-pages-to-azure-static-web-apps"},"nextItem":{"title":"Preload fonts with Docusaurus","permalink":"/2021/12/29/preload-fonts-with-docusaurus"}},"content":"This post shows how to build and deploy two Azure Container Apps using Bicep and GitHub Actions. These apps will communicate using [dapr](https://docs.dapr.io/), be built in [VS Code using a devcontainer](https://code.visualstudio.com/docs/remote/containers). It will be possible to debug in VS Code and run with `docker-compose`.\\n\\nThis follows on from the [previous post](../2021-12-27-azure-container-apps-build-and-deploy-with-bicep-and-github-actions/index.md) which built and deployed a simple web application to Azure Container Apps using Bicep and GitHub Actions using the GitHub container registry.\\n\\n![title image reading \\"Azure Container Apps dapr, devcontainer, debug and deploy\\"  with the dapr, Bicep, Azure Container Apps and GitHub Actions logos](title-image.png)\\n\\n## What we\'re going to build\\n\\nAs an engineer, I\'m productive when:\\n\\n- Integrating different services together is a turnkey experience and\\n- I\'m able to easily debug my code\\n\\nI\'ve found that using dapr and VS Code I\'m able to achieve both of these goals. I can build an application made up of multiple services, compose them together using dapr and deploy them to Azure Container Apps with relative ease.\\n\\nIn this post we\'re going to build an example of that from scratch, with a [koa/node.js](https://koajs.com/) (built with TypeScript) front end that will communicate with a [dotnet](https://dotnet.microsoft.com/en-us/) service via dapr.\\n\\nAll the work done in this post can be found in the [`dapr-devcontainer-debug-and-deploy`](https://github.com/johnnyreilly/dapr-devcontainer-debug-and-deploy/tree/v1.0.0) repo. As a note, if you\'re interested in this topic it\'s also worth looking at the [`Azure-Samples/container-apps-store-api-microservice`](https://github.com/Azure-Samples/container-apps-store-api-microservice) repo.\\n\\n## Setting up our devcontainer\\n\\nThe first thing we\'ll do is set up our devcontainer. We\'re going to use a tweaked version of the [docker-in-docker](https://github.com/microsoft/vscode-dev-containers/tree/main/containers/docker-in-docker) image from the [vscode-dev-containers](https://github.com/microsoft/vscode-dev-containers) repo.\\n\\nIn the root of our project we\'ll create a `.devcontainer` folder, and within that a `library-scripts` folder. There\'s a number of communal scripts from the [`vscode-dev-containers`](https://github.com/microsoft/vscode-dev-containers) repo which we\'re going to lift and shift into in our `library-scripts` folder:\\n\\n- [docker-in-docker-debian.sh](https://github.com/microsoft/vscode-dev-containers/blob/d93de4632781372d4b4da1699e27ae3a2404c96c/script-library/docker-in-docker-debian.sh) - for installing Docker in Docker\\n- [azcli-debian.sh](https://github.com/microsoft/vscode-dev-containers/blob/d93de4632781372d4b4da1699e27ae3a2404c96c/script-library/azcli-debian.sh) - for installing the Azure CLI\\n\\nIn the `.devcontainer` folder we want to create a `Dockerfile`:\\n\\n```docker\\n# [Choice] .NET version: 6.0, 5.0, 3.1, 2.1\\nARG VARIANT=3.1\\nFROM mcr.microsoft.com/vscode/devcontainers/dotnet:0-${VARIANT}\\nRUN su vscode -c \\"umask 0002 && dotnet tool install -g Microsoft.Tye --version \\\\\\"0.10.0-alpha.21420.1\\\\\\" 2>&1\\"\\n\\n# [Choice] Node.js version: none, lts/*, 16, 14, 12, 10\\nARG NODE_VERSION=\\"14\\"\\nRUN if [ \\"${NODE_VERSION}\\" != \\"none\\" ]; then su vscode -c \\"umask 0002 && . /usr/local/share/nvm/nvm.sh && nvm install ${NODE_VERSION} 2>&1\\"; fi\\n\\n# [Option] Install Azure CLI\\nARG INSTALL_AZURE_CLI=\\"false\\"\\nCOPY library-scripts/azcli-debian.sh /tmp/library-scripts/\\nRUN if [ \\"$INSTALL_AZURE_CLI\\" = \\"true\\" ]; then bash /tmp/library-scripts/azcli-debian.sh; fi \\\\\\n    && apt-get clean -y && rm -rf /var/lib/apt/lists/* /tmp/library-scripts \\\\\\n    && az bicep install\\n\\n# [Option] Enable non-root Docker access in container\\nARG ENABLE_NONROOT_DOCKER=\\"true\\"\\n# [Option] Use the OSS Moby CLI instead of the licensed Docker CLI\\nARG USE_MOBY=\\"true\\"\\n# [Option] Engine/CLI Version\\nARG DOCKER_VERSION=\\"latest\\"\\n\\n# Enable new \\"BUILDKIT\\" mode for Docker CLI\\nENV DOCKER_BUILDKIT=1\\n\\nARG USERNAME=vscode\\n\\n# Install needed packages and setup non-root user. Use a separate RUN statement to add your\\n# own dependencies. A user of \\"automatic\\" attempts to reuse an user ID if one already exists.\\nCOPY library-scripts/docker-in-docker-debian.sh /tmp/library-scripts/\\nRUN apt-get update \\\\\\n    && apt-get install python3-pip -y \\\\\\n# Use Docker script from script library to set things up\\n    && /bin/bash /tmp/library-scripts/docker-in-docker-debian.sh \\"${ENABLE_NONROOT_DOCKER}\\" \\"${USERNAME}\\" \\"${USE_MOBY}\\" \\"${DOCKER_VERSION}\\"\\n\\n# Install Dapr\\nRUN wget -q https://raw.githubusercontent.com/dapr/cli/master/install/install.sh -O - | /bin/bash \\\\\\n    # Clean up\\n    && apt-get autoremove -y && apt-get clean -y && rm -rf /var/lib/apt/lists/* /tmp/library-scripts/\\n\\n# Add daprd to the path for the VS Code Dapr extension.\\nENV PATH=\\"${PATH}:/home/${USERNAME}/.dapr/bin\\"\\n\\n# Install Tye\\nENV PATH=/home/${USERNAME}/.dotnet/tools:$PATH\\n\\nVOLUME [ \\"/var/lib/docker\\" ]\\n\\n# Setting the ENTRYPOINT to docker-init.sh will configure non-root access\\n# to the Docker socket. The script will also execute CMD as needed.\\nENTRYPOINT [ \\"/usr/local/share/docker-init.sh\\" ]\\nCMD [ \\"sleep\\", \\"infinity\\" ]\\n\\n# [Optional] Uncomment this section to install additional OS packages.\\n# RUN apt-get update && export DEBIAN_FRONTEND=noninteractive \\\\\\n#     && apt-get -y install --no-install-recommends <your-package-list-here>\\n```\\n\\nThe above is a loose riff on the [docker-in-docker Dockerfile](https://github.com/microsoft/vscode-dev-containers/blob/main/containers/docker-in-docker/.devcontainer/Dockerfile), lovingly mixed with the [Azure-Samples container-apps Dockerfile](https://github.com/Azure-Samples/container-apps-store-api-microservice/blob/main/.devcontainer/Dockerfile).\\n\\nIt installs the following:\\n\\n- Dot Net\\n- Node.js\\n- the Azure CLI\\n- Docker\\n- Bicep\\n- Dapr\\n\\nNow we have our `Dockerfile`, we need a `devcontainer.json` to go with it:\\n\\n```json\\n// For format details, see https://aka.ms/devcontainer.json. For config options, see the README at:\\n// https://github.com/microsoft/vscode-dev-containers/tree/v0.205.0/containers/dapr-dotnet\\n{\\n  \\"name\\": \\"dapr\\",\\n  \\"build\\": {\\n    \\"dockerfile\\": \\"Dockerfile\\",\\n    \\"args\\": {\\n      // Update \'VARIANT\' to pick a .NET Core version: 3.1, 5.0, 6.0\\n      \\"VARIANT\\": \\"6.0\\",\\n      // Options\\n      \\"NODE_VERSION\\": \\"lts/*\\",\\n      \\"INSTALL_AZURE_CLI\\": \\"true\\"\\n    }\\n  },\\n  \\"runArgs\\": [\\"--init\\", \\"--privileged\\"],\\n  \\"mounts\\": [\\"source=dind-var-lib-docker,target=/var/lib/docker,type=volume\\"],\\n  \\"overrideCommand\\": false,\\n\\n  // Use this environment variable if you need to bind mount your local source code into a new container.\\n  \\"remoteEnv\\": {\\n    \\"LOCAL_WORKSPACE_FOLDER\\": \\"${localWorkspaceFolder}\\",\\n    \\"PATH\\": \\"/home/vscode/.dapr/bin/:/home/vscode/.dotnet/tools:$PATH${containerEnv:PATH}\\"\\n  },\\n\\n  // Set *default* container specific settings.json values on container create.\\n  \\"settings\\": {},\\n\\n  // Add the IDs of extensions you want installed when the container is created.\\n  \\"extensions\\": [\\n    \\"ms-azuretools.vscode-dapr\\",\\n    \\"ms-azuretools.vscode-docker\\",\\n    \\"ms-dotnettools.csharp\\",\\n    \\"ms-vscode.azurecli\\",\\n    \\"ms-azuretools.vscode-bicep\\"\\n  ],\\n\\n  // Use \'forwardPorts\' to make a list of ports inside the container available locally.\\n  // \\"forwardPorts\\": [],\\n\\n  // Ensure Dapr is running on opening the container\\n  \\"postCreateCommand\\": \\"dapr uninstall --all && dapr init\\",\\n\\n  // Comment out connect as root instead. More info: https://aka.ms/vscode-remote/containers/non-root.\\n  \\"remoteUser\\": \\"vscode\\",\\n  \\"features\\": {\\n    \\"azure-cli\\": \\"latest\\"\\n  }\\n}\\n```\\n\\nThe above will:\\n\\n- install Node 16 / dotnet 6 and the latest Azure CLI\\n- install a number of VS Code extensions related to dapr / Docker / Bicep / Azure / C#\\n- install dapr when the container starts\\n\\nWe\'re ready! Reopen your repo in a container (it will take a while first time out) and you\'ll be ready to go.\\n\\n## Create a dotnet service\\n\\nNow we\'re going to create a dotnet service. The aim of this post is not to build a specific application, but rather to demonstrate how simple service to service communication is with dapr. So we\'ll use the web api template that ships with dotnet 6. That arrives with a fake weather API included, so we\'ll name our service accordingly:\\n\\n```shell\\ndotnet new webapi -o WeatherService\\n```\\n\\nInside the created `Program.cs`, find the following line and delete it:\\n\\n```cs\\napp.UseHttpsRedirection();\\n```\\n\\nHTTPS is important, however Azure Container Apps are going to tackle that for us.\\n\\n## Create a Node.js service (with Koa)\\n\\nCreating our dotnet service was very simple. We\'re now going to create a web app with Node.js and Koa that calls our dotnet service. This will be a little more complicated - but still surprisingly simple thanks to the great API choices of dapr.\\n\\nLet\'s make that service:\\n\\n```shell\\nmkdir WebService\\ncd WebService\\nnpm init -y\\nnpm install koa axios --save\\nnpm install @types/koa @types/node @types/axios typescript --save-dev\\n```\\n\\nWe\'re installing the following:\\n\\n- [koa](https://koajs.com/) - the web framework we\'re going to use\\n- [axios](https://axios-http.com/) - to make calls to our dotnet service via HTTP / dapr\\n- [TypeScript](https://www.typescriptlang.org/) and associated type definitions, so we can take advantage of static typing. Admittedly since we\'re building a minimal example this is not super beneficial; but TS makes me happy and I\'d certainly want static typing in place if going beyond a simple example. Start as you mean to go on.\\n\\nWe\'ll create a `tsconfig.json`:\\n\\n```json\\n{\\n  \\"compilerOptions\\": {\\n    \\"esModuleInterop\\": true,\\n    \\"module\\": \\"commonjs\\",\\n    \\"target\\": \\"es2017\\",\\n    \\"noImplicitAny\\": true,\\n    \\"outDir\\": \\"./dist\\",\\n    \\"strict\\": true,\\n    \\"sourceMap\\": true\\n  }\\n}\\n```\\n\\nWe\'ll update the `scripts` section of our `package.json` like so:\\n\\n```json\\n  \\"scripts\\": {\\n    \\"build\\": \\"tsc\\",\\n    \\"start\\": \\"node dist/index.js\\"\\n  },\\n```\\n\\nSo we can build and start our web app. Now let\'s write it!\\n\\nWe\'re going to create an `index.ts` file:\\n\\n```ts\\nimport Koa from \'koa\';\\nimport axios from \'axios\';\\n\\n// How we connect to the dotnet service with dapr\\nconst daprSidecarBaseUrl = `http://localhost:${\\n  process.env.DAPR_HTTP_PORT || 3501\\n}`;\\n// app id header for service discovery\\nconst weatherServiceAppIdHeaders = {\\n  \'dapr-app-id\': process.env.WEATHER_SERVICE_NAME || \'dotnet-app\',\\n};\\n\\nconst app = new Koa();\\n\\napp.use(async (ctx) => {\\n  try {\\n    const data = await axios.get<WeatherForecast[]>(\\n      `${daprSidecarBaseUrl}/weatherForecast`,\\n      {\\n        headers: weatherServiceAppIdHeaders,\\n      }\\n    );\\n\\n    ctx.body = `And the weather today will be ${data.data[0].summary}`;\\n  } catch (exc) {\\n    console.error(\'Problem calling weather service\', exc);\\n    ctx.body = \'Something went wrong!\';\\n  }\\n});\\n\\nconst portNumber = 3000;\\napp.listen(portNumber);\\nconsole.log(`listening on port ${portNumber}`);\\n\\ninterface WeatherForecast {\\n  date: string;\\n  temperatureC: number;\\n  temperatureF: number;\\n  summary: string;\\n}\\n```\\n\\nThe above code is fairly simple but is achieving quite a lot. It:\\n\\n- uses various environment variables to construct the URLs / headers which allow connecting to the dapr sidecar running alongside the app, and consequently to the weather service through the dapr sidecar running alongside the weather service. We\'re going to set up the environment variables which this code relies upon later.\\n- spins up a web server with koa on port 3000\\n- that web server, when sent an HTTP request, will call the `weatherForecast` endpoint of the dotnet app. It will grab what comes back, take the first entry in there and surface that up as the weather forecast.\\n- We\'re also defining a `WeatherForecast` interface to represent the type of the data that comes back from the dotnet service\\n\\nIt\'s worth dwelling for a moment on the simplicity that dapr is affording us here. We\'re able to make HTTP requests to our dotnet service just like they were any other service running locally. What\'s actually happening is illustrated by the diagram below:\\n\\n![a diagram showing traffic going from the web service to the weather service and back again via dapr](./dapr-sidecar.drawio.svg)\\n\\nWe\'re making HTTP requests from the web service, which look like they\'re going directly to the weather service. But in actual fact, they\'re being routed through dapr sidecars until they reach their destination. Why is this fantastic? Well there\'s two things we aren\'t having to think about here:\\n\\n- certificates\\n- inter-service authentication\\n\\nBoth of these can be complex and burn a large amount of engineering time. Because we\'re using dapr it\'s not a problem we have to solve. Isn\'t that great?\\n\\n## Debugging dapr in VS Code\\n\\nWe want to be able to debug this code. We can achieve that in VS Code by setting a [`launch.json`](https://code.visualstudio.com/docs/editor/debugging#_launchjson-attributes) and a [`tasks.json`](https://code.visualstudio.com/docs/editor/tasks) file.\\n\\nFirst of all we\'ll create a `launch.json` file in the `.vscode` folder of our repo:\\n\\n```json\\n{\\n  // Use IntelliSense to learn about possible attributes.\\n  // Hover to view descriptions of existing attributes.\\n  // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387\\n  \\"version\\": \\"0.2.0\\",\\n  \\"compounds\\": [\\n    {\\n      \\"name\\": \\"All Container Apps\\",\\n      \\"configurations\\": [\\"WeatherService\\", \\"WebService\\"],\\n      \\"presentation\\": {\\n        \\"hidden\\": false,\\n        \\"group\\": \\"Containers\\",\\n        \\"order\\": 1\\n      }\\n    }\\n  ],\\n  \\"configurations\\": [\\n    {\\n      \\"name\\": \\"WeatherService\\",\\n      \\"type\\": \\"coreclr\\",\\n      \\"request\\": \\"launch\\",\\n      \\"preLaunchTask\\": \\"daprd-debug-dotnet\\",\\n      \\"postDebugTask\\": \\"daprd-down-dotnet\\",\\n      \\"program\\": \\"${workspaceFolder}/WeatherService/bin/Debug/net6.0/WeatherService.dll\\",\\n      \\"args\\": [],\\n      \\"cwd\\": \\"${workspaceFolder}\\",\\n      \\"stopAtEntry\\": false,\\n      \\"env\\": {\\n        \\"DOTNET_ENVIRONMENT\\": \\"Development\\",\\n        \\"DOTNET_URLS\\": \\"http://localhost:5000\\",\\n        \\"DAPR_HTTP_PORT\\": \\"3500\\",\\n        \\"DAPR_GRPC_PORT\\": \\"50000\\",\\n        \\"DAPR_METRICS_PORT\\": \\"9090\\"\\n      }\\n    },\\n\\n    {\\n      \\"name\\": \\"WebService\\",\\n      \\"type\\": \\"node\\",\\n      \\"request\\": \\"launch\\",\\n      \\"preLaunchTask\\": \\"daprd-debug-node\\",\\n      \\"postDebugTask\\": \\"daprd-down-node\\",\\n      \\"program\\": \\"${workspaceFolder}/WebService/index.ts\\",\\n      \\"cwd\\": \\"${workspaceFolder}\\",\\n      \\"env\\": {\\n        \\"NODE_ENV\\": \\"development\\",\\n        \\"PORT\\": \\"3000\\",\\n        \\"DAPR_HTTP_PORT\\": \\"3501\\",\\n        \\"DAPR_GRPC_PORT\\": \\"50001\\",\\n        \\"DAPR_METRICS_PORT\\": \\"9091\\",\\n        \\"WEATHER_SERVICE_NAME\\": \\"dotnet-app\\"\\n      },\\n      \\"protocol\\": \\"inspector\\",\\n      \\"outFiles\\": [\\"${workspaceFolder}/WebService/dist/**/*.js\\"],\\n      \\"serverReadyAction\\": {\\n        \\"action\\": \\"openExternally\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nThe things to note about this are:\\n\\n- we create a Node.js (\\"WebService\\") and a dotnet (\\"WeatherService\\") configuration. These are referenced by the `All Container Apps` compound. Kicking off that will start both the Node.js and the dotnet apps.\\n- The Node.js app runs a `daprd-debug-node` task prior to launch and a `daprd-down-node` task when debugging completes. Comparable tasks are run by the dotnet container - we\'ll look at these in a moment.\\n- Various environment variables are configured, most of which control the behaviour of dapr. When we\'re debugging locally we\'ll be using some non-typical ports to accomodate multiple dapr sidecars being in play at the same time. Note also the `\\"WEATHER_SERVICE_NAME\\": \\"dotnet-app\\"` - it\'s this that allows the WebService to communicate with the WeatherService - `dotnet-app` is the `appId` used to identify a service with dapr. We\'ll see that as we configure our `tasks.json`.\\n\\nHere\'s the `tasks.json` we must make:\\n\\n```json\\n{\\n  // See https://go.microsoft.com/fwlink/?LinkId=733558\\n  // for the documentation about the tasks.json format\\n  \\"version\\": \\"2.0.0\\",\\n  \\"tasks\\": [\\n    {\\n      \\"label\\": \\"dotnet-build\\",\\n      \\"command\\": \\"dotnet\\",\\n      \\"type\\": \\"process\\",\\n      \\"args\\": [\\n        \\"build\\",\\n        \\"${workspaceFolder}/WeatherService/WeatherService.csproj\\",\\n        \\"/property:GenerateFullPaths=true\\",\\n        \\"/consoleloggerparameters:NoSummary\\"\\n      ],\\n      \\"problemMatcher\\": \\"$msCompile\\"\\n    },\\n    {\\n      \\"label\\": \\"daprd-debug-dotnet\\",\\n      \\"appId\\": \\"dotnet-app\\",\\n      \\"appPort\\": 5000,\\n      \\"httpPort\\": 3500,\\n      \\"grpcPort\\": 50000,\\n      \\"metricsPort\\": 9090,\\n      \\"type\\": \\"daprd\\",\\n      \\"dependsOn\\": [\\"dotnet-build\\"]\\n    },\\n    {\\n      \\"label\\": \\"daprd-down-dotnet\\",\\n      \\"appId\\": \\"dotnet-app\\",\\n      \\"type\\": \\"daprd-down\\"\\n    },\\n\\n    {\\n      \\"label\\": \\"npm-install\\",\\n      \\"type\\": \\"shell\\",\\n      \\"command\\": \\"npm install\\",\\n      \\"options\\": {\\n        \\"cwd\\": \\"${workspaceFolder}/WebService\\"\\n      }\\n    },\\n    {\\n      \\"label\\": \\"webservice-build\\",\\n      \\"type\\": \\"typescript\\",\\n      \\"tsconfig\\": \\"WebService/tsconfig.json\\",\\n      \\"problemMatcher\\": [\\"$tsc\\"],\\n      \\"group\\": {\\n        \\"kind\\": \\"build\\",\\n        \\"isDefault\\": true\\n      },\\n      \\"dependsOn\\": [\\"npm-install\\"]\\n    },\\n    {\\n      \\"label\\": \\"daprd-debug-node\\",\\n      \\"appId\\": \\"node-app\\",\\n      \\"appPort\\": 3000,\\n      \\"httpPort\\": 3501,\\n      \\"grpcPort\\": 50001,\\n      \\"metricsPort\\": 9091,\\n      \\"type\\": \\"daprd\\",\\n      \\"dependsOn\\": [\\"webservice-build\\"]\\n    },\\n    {\\n      \\"label\\": \\"daprd-down-node\\",\\n      \\"appId\\": \\"node-app\\",\\n      \\"type\\": \\"daprd-down\\"\\n    }\\n  ]\\n}\\n```\\n\\nThere\'s two sets of tasks here; one for the WeatherService and one for the WebService. You\'ll see some commonalities here. For each service there\'s a `daprd` task that depends upon the relevant service being built and passes the various ports for the dapr sidecar to run on that runs just before debugging kicks off. To go with that, there\'s a `daprd-down` task for each service that runs when debugging finishes and shuts down dapr.\\n\\nWe\'re now ready to debug our app. Let\'s hit F5.\\n\\n![screenshot of debugging the index.ts file in VS Code](./debugging.png)\\n\\nAnd if we look at our browser:\\n\\n![screenshot of browsing Firefox at http://localhost:3000 and seeing \\"And the weather today will be Freezing\\" in the output](./app-running.png)\\n\\nIt works! We\'re running a Node.js WebService which, when called, is communicating with our dotnet WeatherService and surfacing up the results. Brilliant!\\n\\n## Containerising our services with Docker\\n\\nBefore we can deploy each of our services, they need to be containerised.\\n\\nFirst let\'s add a `Dockerfile` to the `WeatherService` folder:\\n\\n```docker\\nFROM mcr.microsoft.com/dotnet/sdk:6.0 as build\\nWORKDIR /app\\nCOPY . .\\nRUN dotnet restore\\nRUN dotnet publish -o /app/publish\\n\\nFROM mcr.microsoft.com/dotnet/aspnet:6.0 as runtime\\nWORKDIR /app\\nCOPY --from=build /app/publish /app\\n\\nENV DOTNET_ENVIRONMENT=Production\\nENV ASPNETCORE_URLS=\'http://+:5000\'\\nEXPOSE 5000\\nENTRYPOINT [ \\"dotnet\\", \\"/app/WeatherService.dll\\" ]\\n```\\n\\nThen we\'ll add a `Dockerfile` to the `WebService` folder:\\n\\n```docker\\nFROM node:16 AS build\\nWORKDIR /app\\nCOPY package.json ./\\nCOPY package-lock.json ./\\nRUN npm install\\n\\nCOPY . .\\nRUN npm run build\\n\\nFROM node:16 AS runtime\\nWORKDIR /app\\nCOPY --from=build /app/dist /app\\nCOPY --from=build /app/package.json /app\\nCOPY --from=build /app/package-lock.json /app\\nRUN npm install\\n\\nENV NODE_ENV production\\nEXPOSE 3000\\nENTRYPOINT [ \\"node\\", \\"/app/index.js\\" ]\\n```\\n\\nLikely these `Dockerfile`s could be optimised further; but we\'re not focussed on that just now. What we have now are two simple `Dockerfile`s that will give us images we can run. Given that one depends on the other it makes sense to bring them together with a `docker-compose.yml` file which we\'ll place in the root of the repo:\\n\\n```yml\\nversion: \'3.4\'\\n\\nservices:\\n  weatherservice:\\n    image: ${REGISTRY:-weatherservice}:${TAG:-latest}\\n    build:\\n      context: ./WeatherService\\n      dockerfile: Dockerfile\\n    ports:\\n      - \'50000:50000\' # Dapr instances communicate over gRPC so we need to expose the gRPC port\\n    environment:\\n      DOTNET_ENVIRONMENT: \'Development\'\\n      ASPNETCORE_URLS: \'http://+:5000\'\\n      DAPR_HTTP_PORT: 3500\\n      DAPR_GRPC_PORT: 50000\\n      DAPR_METRICS_PORT: 9090\\n\\n  weatherservice-dapr:\\n    image: \'daprio/daprd:latest\'\\n    command:\\n      [\\n        \'./daprd\',\\n        \'-app-id\',\\n        \'dotnet-app\',\\n        \'-app-port\',\\n        \'5000\',\\n        \'-dapr-http-port\',\\n        \'3500\',\\n        \'-placement-host-address\',\\n        \'placement:50006\',\\n      ]\\n    network_mode: \'service:weatherservice\'\\n    depends_on:\\n      - weatherservice\\n\\n  webservice:\\n    image: ${REGISTRY:-webservice}:${TAG:-latest}\\n    ports:\\n      - \'3000:3000\' # The web front end port\\n      - \'50001:50001\' # Dapr instances communicate over gRPC so we need to expose the gRPC port\\n    build:\\n      context: ./WebService\\n      dockerfile: Dockerfile\\n    environment:\\n      NODE_ENV: \'development\'\\n      PORT: \'3000\'\\n      DAPR_HTTP_PORT: 3501\\n      DAPR_GRPC_PORT: 50001\\n      DAPR_METRICS_PORT: 9091\\n      WEATHER_SERVICE_NAME: \'dotnet-app\'\\n\\n  webservice-dapr:\\n    image: \'daprio/daprd:latest\'\\n    command: [\\n        \'./daprd\',\\n        \'-app-id\',\\n        \'node-app\',\\n        \'-app-port\',\\n        \'3000\',\\n        \'-dapr-http-port\',\\n        \'3501\',\\n        \'-placement-host-address\',\\n        \'placement:50006\', # Dapr\'s placement service can be reach via the docker DNS entry\\n      ]\\n    network_mode: \'service:webservice\'\\n    depends_on:\\n      - webservice\\n\\n  dapr-placement:\\n    image: \'daprio/dapr:latest\'\\n    command: [\'./placement\', \'-port\', \'50006\']\\n    ports:\\n      - \'50006:50006\'\\n```\\n\\nWith this in place we can run `docker-compose up` and bring up our application locally.\\n\\nAnd now we have docker images built, we can look at deploying them.\\n\\n## Deploying to Azure\\n\\nAt this point we have pretty much everything we need in terms of application code and the ability to build and debug it. Now we\'d like to deploy it to Azure.\\n\\nLet\'s begin with the Bicep required to deploy our Azure Container Apps.\\n\\nIn our repository we\'ll create an `infra` directory, into which we\'ll place a `main.bicep` file which will contain our Bicep template:\\n\\n```bicep\\nparam branchName string\\n\\nparam webServiceImage string\\nparam webServicePort int\\nparam webServiceIsExternalIngress bool\\n\\nparam weatherServiceImage string\\nparam weatherServicePort int\\nparam weatherServiceIsExternalIngress bool\\n\\nparam containerRegistry string\\nparam containerRegistryUsername string\\n@secure()\\nparam containerRegistryPassword string\\n\\nparam tags object\\n\\nvar location = resourceGroup().location\\nvar minReplicas = 0\\nvar maxReplicas = 1\\n\\nvar branch = toLower(last(split(branchName, \'/\')))\\n\\nvar environmentName = \'${branch}-env\'\\nvar workspaceName = \'${branch}-log-analytics\'\\nvar appInsightsName = \'${branch}-app-insights\'\\nvar webServiceContainerAppName = \'${branch}-web\'\\nvar weatherServiceContainerAppName = \'${branch}-weather\'\\n\\nvar containerRegistryPasswordRef = \'container-registry-password\'\\n\\nresource workspace \'Microsoft.OperationalInsights/workspaces@2020-08-01\' = {\\n  name: workspaceName\\n  location: location\\n  tags: tags\\n  properties: {\\n    sku: {\\n      name: \'PerGB2018\'\\n    }\\n    retentionInDays: 30\\n    workspaceCapping: {}\\n  }\\n}\\n\\nresource appInsights \'Microsoft.Insights/components@2020-02-02-preview\' = {\\n  name: appInsightsName\\n  location: location\\n  tags: tags\\n  kind: \'web\'\\n  properties: {\\n    Application_Type: \'web\'\\n    Flow_Type: \'Bluefield\'\\n  }\\n}\\n\\nresource environment \'Microsoft.Web/kubeEnvironments@2021-02-01\' = {\\n  name: environmentName\\n  kind: \'containerenvironment\'\\n  location: location\\n  tags: tags\\n  properties: {\\n    type: \'managed\'\\n    internalLoadBalancerEnabled: false\\n    appLogsConfiguration: {\\n      destination: \'log-analytics\'\\n      logAnalyticsConfiguration: {\\n        customerId: workspace.properties.customerId\\n        sharedKey: listKeys(workspace.id, workspace.apiVersion).primarySharedKey\\n      }\\n    }\\n    containerAppsConfiguration: {\\n      daprAIInstrumentationKey: appInsights.properties.InstrumentationKey\\n    }\\n  }\\n}\\n\\nresource weatherServiceContainerApp \'Microsoft.Web/containerapps@2021-03-01\' = {\\n  name: weatherServiceContainerAppName\\n  kind: \'containerapps\'\\n  tags: tags\\n  location: location\\n  properties: {\\n    kubeEnvironmentId: environment.id\\n    configuration: {\\n      secrets: [\\n        {\\n          name: containerRegistryPasswordRef\\n          value: containerRegistryPassword\\n        }\\n      ]\\n      registries: [\\n        {\\n          server: containerRegistry\\n          username: containerRegistryUsername\\n          passwordSecretRef: containerRegistryPasswordRef\\n        }\\n      ]\\n      ingress: {\\n        external: weatherServiceIsExternalIngress\\n        targetPort: weatherServicePort\\n      }\\n    }\\n    template: {\\n      containers: [\\n        {\\n          image: weatherServiceImage\\n          name: weatherServiceContainerAppName\\n          transport: \'auto\'\\n        }\\n      ]\\n      scale: {\\n        minReplicas: minReplicas\\n        maxReplicas: maxReplicas\\n      }\\n      dapr: {\\n        enabled: true\\n        appPort: weatherServicePort\\n        appId: weatherServiceContainerAppName\\n      }\\n    }\\n  }\\n}\\n\\nresource webServiceContainerApp \'Microsoft.Web/containerapps@2021-03-01\' = {\\n  name: webServiceContainerAppName\\n  kind: \'containerapps\'\\n  tags: tags\\n  location: location\\n  properties: {\\n    kubeEnvironmentId: environment.id\\n    configuration: {\\n      secrets: [\\n        {\\n          name: containerRegistryPasswordRef\\n          value: containerRegistryPassword\\n        }\\n      ]\\n      registries: [\\n        {\\n          server: containerRegistry\\n          username: containerRegistryUsername\\n          passwordSecretRef: containerRegistryPasswordRef\\n        }\\n      ]\\n      ingress: {\\n        external: webServiceIsExternalIngress\\n        targetPort: webServicePort\\n      }\\n    }\\n    template: {\\n      containers: [\\n        {\\n          image: webServiceImage\\n          name: webServiceContainerAppName\\n          transport: \'auto\'\\n          env: [\\n            {\\n              name: \'WEATHER_SERVICE_NAME\'\\n              value: weatherServiceContainerAppName\\n            }\\n          ]\\n        }\\n      ]\\n      scale: {\\n        minReplicas: minReplicas\\n        maxReplicas: maxReplicas\\n      }\\n      dapr: {\\n        enabled: true\\n        appPort: webServicePort\\n        appId: webServiceContainerAppName\\n      }\\n    }\\n  }\\n}\\n\\noutput webServiceUrl string = webServiceContainerApp.properties.latestRevisionFqdn\\n```\\n\\nThis will deploy two container apps - one for our `WebService` and one for our `WeatherService`. Alongside that we\'ve resources for logging and environments.\\n\\n## Setting up a resource group\\n\\nWith our Bicep in place, we\'re going to need a resource group to send it to. Right now, Azure Container Apps aren\'t available everywhere. So we\'re going to create ourselves a resource group in North Europe which does support ACAs:\\n\\n```shell\\naz group create -g rg-aca -l northeurope\\n```\\n\\n## Secrets for GitHub Actions\\n\\nWe\'re aiming to set up a GitHub Action to handle our deployment. This will depend upon a number of secrets:\\n\\n![Screenshot of the secrets in the GitHub website that we need to create](screenshot-github-secrets.png)\\n\\nWe\'ll need to create each of these secrets.\\n\\n### `AZURE_CREDENTIALS` - GitHub logging into Azure\\n\\nSo GitHub Actions can interact with Azure on our behalf, we need to provide it with some credentials. We\'ll use the Azure CLI to create these:\\n\\n```shell\\naz ad sp create-for-rbac --name \\"myApp\\" --role contributor \\\\\\n    --scopes /subscriptions/{subscription-id}/resourceGroups/{resource-group} \\\\\\n    --sdk-auth\\n```\\n\\nRemember to replace the `{subscription-id}` with your subscription id and `{resource-group}` with the name of your resource group (`rg-aca` if you\'re following along). This command will pump out a lump of JSON that looks something like this:\\n\\n```json\\n{\\n  \\"clientId\\": \\"a-client-id\\",\\n  \\"clientSecret\\": \\"a-client-secret\\",\\n  \\"subscriptionId\\": \\"a-subscription-id\\",\\n  \\"tenantId\\": \\"a-tenant-id\\",\\n  \\"activeDirectoryEndpointUrl\\": \\"https://login.microsoftonline.com\\",\\n  \\"resourceManagerEndpointUrl\\": \\"https://management.azure.com/\\",\\n  \\"activeDirectoryGraphResourceId\\": \\"https://graph.windows.net/\\",\\n  \\"sqlManagementEndpointUrl\\": \\"https://management.core.windows.net:8443/\\",\\n  \\"galleryEndpointUrl\\": \\"https://gallery.azure.com/\\",\\n  \\"managementEndpointUrl\\": \\"https://management.core.windows.net/\\"\\n}\\n```\\n\\nTake this and save it as the `AZURE_CREDENTIALS` secret in Azure.\\n\\n### `PACKAGES_TOKEN` - Azure accessing the GitHub container registry\\n\\nWe also need a secret for accessing packages from Azure. We\'re going to be publishing packages to the GitHub container registry. Azure is going to need to be able to access this when we\'re deploying. ACA deployment works by telling Azure where to look for an image and providing any necessary credentials to do the acquisition. To facilitate this we\'ll set up a `PACKAGES_TOKEN` secret. This is a GitHub personal access token with the `read:packages` scope. [Follow the instructions here to create the token.](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token)\\n\\n## Deploying with GitHub Actions\\n\\nWith our secrets configured, we\'re now well placed to write our GitHub Action. We\'ll create a `.github/workflows/build-and-deploy.yaml` file in our repository and populate it thusly:\\n\\n```yaml\\n# yaml-language-server: $schema=./build.yaml\\nname: Build and Deploy\\non:\\n  # Trigger the workflow on push or pull request,\\n  # but only for the main branch\\n  push:\\n    branches:\\n      - main\\n  pull_request:\\n    branches:\\n      - main\\n    # Publish semver tags as releases.\\n    tags: [\'v*.*.*\']\\n  workflow_dispatch:\\n\\nenv:\\n  RESOURCE_GROUP: rg-aca\\n  REGISTRY: ghcr.io\\n  IMAGE_NAME: ${{ github.repository }}\\n\\njobs:\\n  build:\\n    runs-on: ubuntu-latest\\n    strategy:\\n      matrix:\\n        services:\\n          [\\n            { \'imageName\': \'node-service\', \'directory\': \'./WebService\' },\\n            { \'imageName\': \'dotnet-service\', \'directory\': \'./WeatherService\' },\\n          ]\\n    permissions:\\n      contents: read\\n      packages: write\\n    outputs:\\n      image-node: ${{ steps.image-tag.outputs.image-node-service }}\\n      image-dotnet: ${{ steps.image-tag.outputs.image-dotnet-service }}\\n    steps:\\n      - name: Checkout repository\\n        uses: actions/checkout@v2\\n\\n      # Login against a Docker registry except on PR\\n      # https://github.com/docker/login-action\\n      - name: Log into registry ${{ env.REGISTRY }}\\n        if: github.event_name != \'pull_request\'\\n        uses: docker/login-action@v1\\n        with:\\n          registry: ${{ env.REGISTRY }}\\n          username: ${{ github.actor }}\\n          password: ${{ secrets.GITHUB_TOKEN }}\\n\\n      # Extract metadata (tags, labels) for Docker\\n      # https://github.com/docker/metadata-action\\n      - name: Extract Docker metadata\\n        id: meta\\n        uses: docker/metadata-action@v3\\n        with:\\n          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/${{ matrix.services.imageName }}\\n          tags: |\\n            type=semver,pattern={{version}}\\n            type=semver,pattern={{major}}.{{minor}}\\n            type=semver,pattern={{major}}\\n            type=ref,event=branch\\n            type=sha\\n\\n      # Build and push Docker image with Buildx (don\'t push on PR)\\n      # https://github.com/docker/build-push-action\\n      - name: Build and push Docker image\\n        uses: docker/build-push-action@v2\\n        with:\\n          context: ${{ matrix.services.directory }}\\n          push: ${{ github.event_name != \'pull_request\' }}\\n          tags: ${{ steps.meta.outputs.tags }}\\n          labels: ${{ steps.meta.outputs.labels }}\\n\\n      - name: Output image tag\\n        id: image-tag\\n        run: echo \\"::set-output name=image-${{ matrix.services.imageName }}::${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/${{ matrix.services.imageName }}:sha-$(git rev-parse --short HEAD)\\" | tr \'[:upper:]\' \'[:lower:]\'\\n\\n  deploy:\\n    runs-on: ubuntu-latest\\n    needs: [build]\\n    steps:\\n      - name: Checkout repository\\n        uses: actions/checkout@v2\\n\\n      - name: Azure Login\\n        uses: azure/login@v1\\n        with:\\n          creds: ${{ secrets.AZURE_CREDENTIALS }}\\n\\n      - name: Deploy bicep\\n        uses: azure/CLI@v1\\n        if: github.event_name != \'pull_request\'\\n        with:\\n          inlineScript: |\\n            REF_SHA=\'${{ github.ref }}.${{ github.sha }}\'\\n            DEPLOYMENT_NAME=\\"${REF_SHA////-}\\"\\n            echo \\"DEPLOYMENT_NAME=$DEPLOYMENT_NAME\\"\\n\\n            TAGS=\'{\\"owner\\":\\"johnnyreilly\\", \\"email\\":\\"johnny_reilly@hotmail.com\\"}\'\\n            az deployment group create \\\\\\n              --resource-group ${{ env.RESOURCE_GROUP }} \\\\\\n              --name \\"$DEPLOYMENT_NAME\\" \\\\\\n              --template-file ./infra/main.bicep \\\\\\n              --parameters \\\\\\n                  branchName=\'${{ github.event.number == 0 && \'main\' ||  format(\'pr-{0}\', github.event.number) }}\' \\\\\\n                  webServiceImage=\'${{ needs.build.outputs.image-node }}\' \\\\\\n                  webServicePort=3000 \\\\\\n                  webServiceIsExternalIngress=true \\\\\\n                  weatherServiceImage=\'${{ needs.build.outputs.image-dotnet }}\' \\\\\\n                  weatherServicePort=5000 \\\\\\n                  weatherServiceIsExternalIngress=false \\\\\\n                  containerRegistry=${{ env.REGISTRY }} \\\\\\n                  containerRegistryUsername=${{ github.actor }} \\\\\\n                  containerRegistryPassword=${{ secrets.PACKAGES_TOKEN }} \\\\\\n                  tags=\\"$TAGS\\"\\n```\\n\\nThere\'s a lot in this workflow. Let\'s dig into the `build` and `deploy` jobs to see what\'s happening.\\n\\n### `build` - building our image\\n\\nThe `build` job is all about building our container images and pushing then to the GitHub registry. It\'s heavily inspired by [Jeff Hollan](https://twitter.com/jeffhollan)\'s [Azure sample app GHA](https://github.com/Azure-Samples/container-apps-store-api-microservice). When we look at the `strategy` we can see a `matrix` of `services` consisting of two services; our node app and our dotnet app:\\n\\n```yaml\\nstrategy:\\n  matrix:\\n    services:\\n      [\\n        { \'imageName\': \'node-service\', \'directory\': \'./WebService\' },\\n        { \'imageName\': \'dotnet-service\', \'directory\': \'./WeatherService\' },\\n      ]\\n```\\n\\nThis is a matrix because a typical use case of an Azure Container Apps will be multi-container - just as this is. The `outputs` pumps out the details of our `image-node` and `image-dotnet` images to be used later:\\n\\n```yaml\\noutputs:\\n  image-node: ${{ steps.image-tag.outputs.image-node-service }}\\n  image-dotnet: ${{ steps.image-tag.outputs.image-dotnet-service }}\\n```\\n\\nWith that understanding in place, let\'s examine what each of the steps in the `build` job does\\n\\n- `Log into registry` - logs into the GitHub container registry\\n- `Extract Docker metadata` - acquire tags which will be used for versioning\\n- `Build and push Docker image` - build the docker image and if this is not a PR: tag, label and push it to the registry\\n- `Output image tag` - write out the image tag for usage in deployment\\n\\n### `deploy` - shipping our image to Azure\\n\\nThe `deploy` job runs the [`az deployment group create`](https://docs.microsoft.com/en-us/cli/azure/deployment/group?view=azure-cli-latest#az_deployment_group_create) command which performs a deployment of our `main.bicep` file.\\n\\n```yaml\\n- name: Deploy bicep\\n  uses: azure/CLI@v1\\n  if: github.event_name != \'pull_request\'\\n  with:\\n    inlineScript: |\\n      REF_SHA=\'${{ github.ref }}.${{ github.sha }}\'\\n      DEPLOYMENT_NAME=\\"${REF_SHA////-}\\"\\n      echo \\"DEPLOYMENT_NAME=$DEPLOYMENT_NAME\\"\\n\\n      TAGS=\'{\\"owner\\":\\"johnnyreilly\\", \\"email\\":\\"johnny_reilly@hotmail.com\\"}\'\\n      az deployment group create \\\\\\n        --resource-group ${{ env.RESOURCE_GROUP }} \\\\\\n        --name \\"$DEPLOYMENT_NAME\\" \\\\\\n        --template-file ./infra/main.bicep \\\\\\n        --parameters \\\\\\n            branchName=\'${{ github.event.number == 0 && \'main\' ||  format(\'pr-{0}\', github.event.number) }}\' \\\\\\n            webServiceImage=\'${{ needs.build.outputs.image-node }}\' \\\\\\n            webServicePort=3000 \\\\\\n            webServiceIsExternalIngress=true \\\\\\n            weatherServiceImage=\'${{ needs.build.outputs.image-dotnet }}\' \\\\\\n            weatherServicePort=5000 \\\\\\n            weatherServiceIsExternalIngress=false \\\\\\n            containerRegistry=${{ env.REGISTRY }} \\\\\\n            containerRegistryUsername=${{ github.actor }} \\\\\\n            containerRegistryPassword=${{ secrets.PACKAGES_TOKEN }} \\\\\\n            tags=\\"$TAGS\\"\\n```\\n\\nIn either case we pass the same set of parameters:\\n\\n```shell\\nbranchName=\'${{ github.event.number == 0 && \'main\' ||  format(\'pr-{0}\', github.event.number) }}\' \\\\\\nwebServiceImage=\'${{ needs.build.outputs.image-node }}\' \\\\\\nwebServicePort=3000 \\\\\\nwebServiceIsExternalIngress=true \\\\\\nweatherServiceImage=\'${{ needs.build.outputs.image-dotnet }}\' \\\\\\nweatherServicePort=5000 \\\\\\nweatherServiceIsExternalIngress=true \\\\\\ncontainerRegistry=${{ env.REGISTRY }} \\\\\\ncontainerRegistryUsername=${{ github.actor }} \\\\\\ncontainerRegistryPassword=${{ secrets.PACKAGES_TOKEN }} \\\\\\ntags=\\"$tags\\"\\n```\\n\\nThese are either:\\n\\n- secrets we set up earlier\\n- [special github variables](https://docs.github.com/en/actions/learn-github-actions/contexts)\\n- environment variables declared at the start of the script or\\n- outputs from the build step - this is where we acquire our node and dotnet images\\n\\n## Running it\\n\\nWhen the GitHub Action has been run you\'ll find that Azure Container Apps are now showing up inside the Azure Portal in your resource group, alongside the other resources:\\n\\n![screenshot of the Azure Container App\'s resource group in the Azure Portal](screenshot-azure-portal-resource-group.png)\\n\\nIf we take a look at our web ACA we\'ll see\\n\\n![screenshot of the web Azure Container App\'s in the Azure Portal](screenshot-azure-portal-container-app.png)\\n\\nAnd when we take a closer look at the container app, we find a URL we can navigate to:\\n\\n![screenshot of the Azure Container App in the Azure Portal revealing it\'s URL](screenshot-working-app.png)\\n\\nCongratulations! You\'ve built and deployed a simple web app to Azure Container Apps with Bicep and GitHub Actions and secrets.\\n\\n## `The subscription \'***\' cannot have more than 2 environments.`\\n\\nBefore signing off, it\'s probably worth sharing this gotcha. If you\'ve been playing with Azure Container Apps you may have already deployed an \\"environment\\" (`Microsoft.Web/kubeEnvironments`). It\'s fairly common to have a limit of one environment per subscription, which is what this message is saying. So either delete other environments, share the one you have or arrange to raise the limit on your subscription."},{"id":"/2021/12/29/preload-fonts-with-docusaurus","metadata":{"permalink":"/2021/12/29/preload-fonts-with-docusaurus","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-12-29-preload-fonts-with-docusaurus/index.md","source":"@site/blog/2021-12-29-preload-fonts-with-docusaurus/index.md","title":"Preload fonts with Docusaurus","description":"When we\'re using custom fonts in our websites, it\'s good practice to preload the fonts to minimise the flash of unstyled text. This post shows how to achieve this with Docusaurus. It does so by building a Docusaurus plugin which makes use of Satyendra Singh\'s excellent webpack-font-preload-plugin","date":"2021-12-29T00:00:00.000Z","formattedDate":"December 29, 2021","tags":[{"label":"Docusaurus","permalink":"/tags/docusaurus"},{"label":"preload","permalink":"/tags/preload"},{"label":"webpack","permalink":"/tags/webpack"},{"label":"fonts","permalink":"/tags/fonts"},{"label":"plugin","permalink":"/tags/plugin"},{"label":"configureWebpack","permalink":"/tags/configure-webpack"}],"readingTime":2.27,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Preload fonts with Docusaurus","authors":"johnnyreilly","tags":["Docusaurus","preload","webpack","fonts","plugin","configureWebpack"],"image":"./title-image.png","hide_table_of_contents":false},"prevItem":{"title":"Azure Container Apps: dapr, devcontainer, debug and deploy","permalink":"/2022/01/22/azure-container-apps-dapr-bicep-github-actions-debug-devcontainer"},"nextItem":{"title":"Query deployment outputs with the Azure CLI","permalink":"/2021/12/28/azure-cli-show-query-output-properties"}},"content":"When we\'re using custom fonts in our websites, it\'s good practice to preload the fonts to minimise the [flash of unstyled text](https://css-tricks.com/fout-foit-foft/). This post shows how to achieve this with Docusaurus. It does so by building a Docusaurus plugin which makes use of [Satyendra Singh](https://github.com/sn-satyendra)\'s excellent [`webpack-font-preload-plugin`](https://github.com/sn-satyendra/webpack-font-preload-plugin)\\n\\n![title image reading \\"Preload fonts with Docusaurus\\" in a ridiculous font with the Docusaurus logo and a screenshot of a preload link HTML element](title-image.png)\\n\\n## Preload web fonts with Docusaurus\\n\\nTo quote the docs of the `webpack-font-preload-plugin`:\\n\\n> The [preload](https://developer.mozilla.org/en-US/docs/Web/HTML/Preloading_content) value of the `<link>` element\'s `rel` attribute lets you declare fetch requests in the HTML\'s `<head>`, specifying resources that your page will need very soon, which you want to start loading early in the page lifecycle, before browsers\' main rendering machinery kicks in. This ensures they are available earlier and are less likely to block the page\'s render, improving performance.\\n>\\n> This plugin specifically targets fonts used with the application which are bundled using webpack. The plugin would add `<link>` tags in the begining of `<head>` of your html:\\n>\\n> ```html\\n> <link rel=\\"preload\\" href=\\"/font1.woff\\" as=\\"font\\" crossorigin />\\n> <link rel=\\"preload\\" href=\\"/font2.woff\\" as=\\"font\\" crossorigin />\\n> ```\\n\\nIf you want to learn more about preloading web fonts, it\'s also worth [reading this excellent article](https://web.dev/codelab-preload-web-fonts/).\\n\\nThe blog you\'re reading is built with [Docusaurus](https://docusaurus.io/). Our mission: for the HTML our Docusaurus build pumps out to feature preload `link` elements. Something like this:\\n\\n```html\\n<link\\n  rel=\\"preload\\"\\n  href=\\"/assets/fonts/Poppins-Regular-8081832fc5cfbf634aa664a9eff0350e.ttf\\"\\n  as=\\"font\\"\\n  crossorigin=\\"\\"\\n/>\\n```\\n\\nThis `link` element has the `rel=\\"preload\\"` attribute set, which triggers font preloading.\\n\\nBut the thing to take from the above text is that the filename features a hash in the name. This demonstrates that the font is being pumped through the Docusaurus build, which is powered by webpack. So we need some webpack whispering to get font preloading going.\\n\\n## Making a plugin\\n\\nWe\'re going to make a minimal [Docusaurus plugin](https://docusaurus.io/docs/using-plugins#creating-plugins) using `webpack-font-preload-plugin`. Let\'s add it to our project:\\n\\n```shell\\nyarn add webpack-font-preload-plugin\\n```\\n\\nNow in the `docusaurus.config.js` we can create our minimal plugin:\\n\\n```js\\nconst FontPreloadPlugin = require(\'webpack-font-preload-plugin\');\\n\\n//...\\n/** @type {import(\'@docusaurus/types\').Config} */\\nconst config = {\\n  //...\\n  plugins: [\\n    function preloadFontPlugin(_context, _options) {\\n      return {\\n        name: \'preload-font-plugin\',\\n        configureWebpack(_config, _isServer) {\\n          return {\\n            plugins: [new FontPreloadPlugin()],\\n          };\\n        },\\n      };\\n    },\\n    // ...\\n  ],\\n  //...\\n};\\n```\\n\\nIt\'s a super simple plugin, it does nothing more than `new` up an instance of the webpack plugin, inside the context of the `configureWebpack` method. That\'s all that\'s required.\\n\\nWith this in place we\'re now seeing the `<link rel=\\"preload\\" ... />` elements being included in the HTML pumped out of our Docusaurus build. This means we have font preloading working:\\n\\n![screenshot of the Chrome devtools featuring link rel=\\"preload\\" elements](screenshot-preload-devtools.png)\\n\\nHuzzah!"},{"id":"/2021/12/28/azure-cli-show-query-output-properties","metadata":{"permalink":"/2021/12/28/azure-cli-show-query-output-properties","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-12-28-azure-cli-show-query-output-properties/index.md","source":"@site/blog/2021-12-28-azure-cli-show-query-output-properties/index.md","title":"Query deployment outputs with the Azure CLI","description":"It\'s often desirable to query the outputs of deployments to Azure. This post demonstrates how to do this using the Azure CLI, bash and jq. It also shows how to generically convert deployment outputs to GitHub Action job outputs.","date":"2021-12-28T00:00:00.000Z","formattedDate":"December 28, 2021","tags":[{"label":"Azure CLI","permalink":"/tags/azure-cli"},{"label":"deployment outputs","permalink":"/tags/deployment-outputs"},{"label":"bash","permalink":"/tags/bash"},{"label":"jq","permalink":"/tags/jq"},{"label":"GitHub Actions","permalink":"/tags/git-hub-actions"}],"readingTime":2.57,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Query deployment outputs with the Azure CLI","authors":"johnnyreilly","tags":["Azure CLI","deployment outputs","bash","jq","GitHub Actions"],"image":"./title-image.png","hide_table_of_contents":false},"prevItem":{"title":"Preload fonts with Docusaurus","permalink":"/2021/12/29/preload-fonts-with-docusaurus"},"nextItem":{"title":"Azure Container Apps: build and deploy with Bicep and GitHub Actions","permalink":"/2021/12/27/azure-container-apps-build-and-deploy-with-bicep-and-github-actions"}},"content":"It\'s often desirable to query the outputs of deployments to Azure. This post demonstrates how to do this using the Azure CLI, bash and jq. It also shows how to generically convert deployment outputs to GitHub Action job outputs.\\n\\n![title image reading \\"Query deployment outputs with the Azure CLI\\" with the Azure logo and the Azure Cloud Shell in the background](title-image.png)\\n\\n## Deployment outputs\\n\\nWhen we deploy something to Azure, we frequently have outputs which we want to use. Let\'s consider the canonical case, whereby a website is created and we want to use the URL of where it has been deployed. We can see these values in the Azure Portal:\\n\\n![a screenshot of the Azure portal demostrating deployment outputs, there is a single output of \\"nodeUrl\\"](./screenshot-azure-portal-deployment-outputs.png)\\n\\nThe above deployment has a single output of `nodeUrl`. Rather than logging into the portal to acquire this value, how can we do so using the Azure CLI and bash?\\n\\n## Acquire all outputs\\n\\nThe way to acquire outputs from the Azure CLI is using the [`az group deployment show`](https://docs.microsoft.com/en-us/cli/azure/group/deployment?view=azure-cli-latest#az_group_deployment_show) command:\\n\\n```bash\\naz deployment group show \\\\\\n  -g <resource-group-name> \\\\\\n  -n <deployment-name> \\\\\\n  --query properties.outputs\\n```\\n\\nRunning the above will produce a piece of JSON that contains all our outputs. In our case, we have a single deployment output: `nodeUrl`. So our JSON looks like this:\\n\\n```json\\n{\\n  \\"nodeUrl\\": {\\n    \\"type\\": \\"String\\",\\n    \\"value\\": \\"some.url.northeurope.azurecontainerapps.io\\"\\n  }\\n}\\n```\\n\\n## Acquire an individual output\\n\\nTo acquire an individual output, you can provide a targeted `--query` to pull out the value you care about. However, there\'s a slight issue:\\n\\n```bash\\njohn@Azure:~$ NODE_URL=$(az deployment group show -g rg-aca -n our-deployment --query properties.outputs.nodeUrl.value)\\njohn@Azure:~$ echo $NODE_URL\\n\\"some.url.northeurope.azurecontainerapps.io\\"\\n```\\n\\nThe value we capture in the `NODE_URL` variable above is surrounded by quotes. These will probably get in the way when we\'re scripting something with this. Rather than purging them with bash, I tend to use [`jq`\'s `--raw-output / -r` option](https://stedolan.github.io/jq/manual/) to grab the raw string.\\n\\n```bash\\njohn@Azure:~$ NODE_URL=$(az deployment group show -g rg-aca -n our-deployment --query properties.outputs | jq -r \'.nodeUrl.value\')\\njohn@Azure:~$ echo $NODE_URL\\nsome.url.northeurope.azurecontainerapps.io\\n```\\n\\nPerfect!\\n\\nThere\'s another approach you could use which [Aleksandar Nikoli\u0107 shared](https://twitter.com/alexandair/status/1476554234543890437), which means jq needn\'t be used at all; using the `tsv` output formatter:\\n\\n```bash\\njohn@Azure:~$ NODE_URL=$(az deployment group show -g rg-aca -n our-deployment --query properties.outputs.nodeUrl.value -o tsv)\\njohn@Azure:~$ echo $NODE_URL\\nsome.url.northeurope.azurecontainerapps.io\\n```\\n\\n## Convert deployment outputs to GitHub Action job outputs\\n\\nBefore wrapping up, here\'s one more useful script, if you find yourself automating in the context of GitHub Actions. It\'s often useful to take the deployment outputs, and convert them into GHA job outputs that can be used in other jobs.\\n\\nWith JSON and [jq](https://stedolan.github.io/jq/) in hand, it\'s possible to expose these like so:\\n\\n```bash\\nDEPLOYMENT_OUTPUTS=$(az deployment group show \\\\\\n  --resource-group ${{ env.RESOURCE_GROUP }} \\\\\\n  --name $DEPLOYMENT_NAME \\\\\\n  --query properties.outputs)\\n\\necho \'convert deployment outputs to outputs\'\\necho $DEPLOYMENT_OUTPUTS | jq -c \'. | to_entries[] | [.key, .value.value]\' |\\n  while IFS=$\\"\\\\n\\" read -r c; do\\n    OUTPUT_NAME=$(echo \\"$c\\" | jq -r \'.[0]\')\\n    OUTPUT_VALUE=$(echo \\"$c\\" | jq -r \'.[1]\')\\n    echo \\"setting output $OUTPUT_NAME=$OUTPUT_VALUE\\"\\n    echo \\"::set-output name=$OUTPUT_NAME::$OUTPUT_VALUE\\"\\n  done\\n```"},{"id":"/2021/12/27/azure-container-apps-build-and-deploy-with-bicep-and-github-actions","metadata":{"permalink":"/2021/12/27/azure-container-apps-build-and-deploy-with-bicep-and-github-actions","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-12-27-azure-container-apps-build-and-deploy-with-bicep-and-github-actions/index.md","source":"@site/blog/2021-12-27-azure-container-apps-build-and-deploy-with-bicep-and-github-actions/index.md","title":"Azure Container Apps: build and deploy with Bicep and GitHub Actions","description":"This post shows how to build and deploy a simple web application to Azure Container Apps using Bicep and GitHub Actions. This includes the configuration and deployment of secrets.","date":"2021-12-27T00:00:00.000Z","formattedDate":"December 27, 2021","tags":[{"label":"Azure Container Apps","permalink":"/tags/azure-container-apps"},{"label":"Bicep","permalink":"/tags/bicep"},{"label":"GitHub Actions","permalink":"/tags/git-hub-actions"},{"label":"GitHub container registry","permalink":"/tags/git-hub-container-registry"}],"readingTime":13.365,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Azure Container Apps: build and deploy with Bicep and GitHub Actions","authors":"johnnyreilly","tags":["Azure Container Apps","Bicep","GitHub Actions","GitHub container registry"],"image":"./title-image.png","hide_table_of_contents":false},"prevItem":{"title":"Query deployment outputs with the Azure CLI","permalink":"/2021/12/28/azure-cli-show-query-output-properties"},"nextItem":{"title":"Azure Container Apps, Bicep and GitHub Actions","permalink":"/2021/12/19/azure-container-apps-bicep-and-github-actions"}},"content":"This post shows how to build and deploy a simple web application to Azure Container Apps using Bicep and GitHub Actions. This includes the configuration and deployment of secrets.\\n\\nThis post follows on from the [previous post](../2021-12-19-azure-container-apps-bicep-and-github-actions/index.md) which deployed infrastructure and a \\"hello world\\" container, this time introducing the building of an image and storing it in the [GitHub container registry](https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry) so it can be deployed.\\n\\nIf you\'d like to learn more about using dapr with Azure Container Apps then you might want to read [this post](../2022-01-22-azure-container-apps-dapr-bicep-github-actions-debug-devcontainer/index.md).\\n\\n![title image reading \\"Azure Container Apps: build and deploy with Bicep and GitHub Actions\\" with the Bicep, Azure Container Apps and GitHub Actions logos](title-image.png)\\n\\n## The containerised convent\\n\\nI learn the most about a technology when I\'m using it to build something. It so happens that I have an aunt that\'s a nun, and long ago she persuaded me to build her convent a website. I\'m a good nephew and I complied. Since that time I\'ve been merrily overengineering it for fun and non-profit.\\n\\nMy aunts website is a pretty vanilla node app. Significantly it is already containerised and runs on [Azure App Service Web App for Containers](https://azure.microsoft.com/en-gb/services/app-service/containers/). Given it lives in the context of a container, this makes it a great candidate for porting to Azure Container Apps.\\n\\nSo that\'s what we\'ll do in this post. But where I\'m building and deploying my aunt\'s container, you could equally be substituting your own; with some minimal changes.\\n\\n## Bicep\\n\\nLet\'s begin with the Bicep required to deploy our Azure Container App.\\n\\nIn our repository we\'ll create an `infra` directory, into which we\'ll place a `main.bicep` file which will contain our Bicep template:\\n\\n```bicep\\nparam nodeImage string\\nparam nodePort int\\nparam nodeIsExternalIngress bool\\n\\nparam containerRegistry string\\nparam containerRegistryUsername string\\n@secure()\\nparam containerRegistryPassword string\\n\\nparam tags object\\n\\n@secure()\\nparam APPSETTINGS_API_KEY string\\nparam APPSETTINGS_DOMAIN string\\nparam APPSETTINGS_FROM_EMAIL string\\nparam APPSETTINGS_RECIPIENT_EMAIL string\\n\\nvar location = resourceGroup().location\\nvar environmentName = \'env-${uniqueString(resourceGroup().id)}\'\\nvar minReplicas = 0\\n\\nvar nodeServiceAppName = \'node-app\'\\nvar workspaceName = \'${nodeServiceAppName}-log-analytics\'\\nvar appInsightsName = \'${nodeServiceAppName}-app-insights\'\\n\\nvar containerRegistryPasswordRef = \'container-registry-password\'\\nvar mailgunApiKeyRef = \'mailgun-api-key\'\\n\\nresource workspace \'Microsoft.OperationalInsights/workspaces@2020-08-01\' = {\\n  name: workspaceName\\n  location: location\\n  tags: tags\\n  properties: {\\n    sku: {\\n      name: \'PerGB2018\'\\n    }\\n    retentionInDays: 30\\n    workspaceCapping: {}\\n  }\\n}\\n\\nresource appInsights \'Microsoft.Insights/components@2020-02-02-preview\' = {\\n  name: appInsightsName\\n  location: location\\n  tags: tags\\n  kind: \'web\'\\n  properties: {\\n    Application_Type: \'web\'\\n    Flow_Type: \'Bluefield\'\\n  }\\n}\\n\\nresource environment \'Microsoft.Web/kubeEnvironments@2021-03-01\' = {\\n  name: environmentName\\n  location: location\\n  tags: tags\\n  properties: {\\n    type: \'managed\'\\n    internalLoadBalancerEnabled: false\\n    appLogsConfiguration: {\\n      destination: \'log-analytics\'\\n      logAnalyticsConfiguration: {\\n        customerId: workspace.properties.customerId\\n        sharedKey: listKeys(workspace.id, workspace.apiVersion).primarySharedKey\\n      }\\n    }\\n    containerAppsConfiguration: {\\n      daprAIInstrumentationKey: appInsights.properties.InstrumentationKey\\n    }\\n  }\\n}\\n\\nresource containerApp \'Microsoft.Web/containerapps@2021-03-01\' = {\\n  name: nodeServiceAppName\\n  kind: \'containerapps\'\\n  tags: tags\\n  location: location\\n  properties: {\\n    kubeEnvironmentId: environment.id\\n    configuration: {\\n      secrets: [\\n        {\\n          name: containerRegistryPasswordRef\\n          value: containerRegistryPassword\\n        }\\n        {\\n          name: mailgunApiKeyRef\\n          value: APPSETTINGS_API_KEY\\n        }\\n      ]\\n      registries: [\\n        {\\n          server: containerRegistry\\n          username: containerRegistryUsername\\n          passwordSecretRef: containerRegistryPasswordRef\\n        }\\n      ]\\n      ingress: {\\n        \'external\': nodeIsExternalIngress\\n        \'targetPort\': nodePort\\n      }\\n    }\\n    template: {\\n      containers: [\\n        {\\n          image: nodeImage\\n          name: nodeServiceAppName\\n          transport: \'auto\'\\n          env: [\\n            {\\n              name: \'APPSETTINGS_API_KEY\'\\n              secretref: mailgunApiKeyRef\\n            }\\n            {\\n              name: \'APPSETTINGS_DOMAIN\'\\n              value: APPSETTINGS_DOMAIN\\n            }\\n            {\\n              name: \'APPSETTINGS_FROM_EMAIL\'\\n              value: APPSETTINGS_FROM_EMAIL\\n            }\\n            {\\n              name: \'APPSETTINGS_RECIPIENT_EMAIL\'\\n              value: APPSETTINGS_RECIPIENT_EMAIL\\n            }\\n          ]\\n        }\\n      ]\\n      scale: {\\n        minReplicas: minReplicas\\n      }\\n    }\\n  }\\n}\\n```\\n\\nLet\'s talk through this template. The environment, workspace and app insights resources are fairly self explanatory. The `containerApp` resource is where the action is. We\'ll drill into that resource and the parameters used to configure it.\\n\\n### The node container app\\n\\nWe\'re going to create a single container app for our node web application. This is configured with these parameters:\\n\\n```bicep\\nparam nodeImage string\\nparam nodePort int\\nparam nodeIsExternalIngress bool\\n```\\n\\nThe above parameters relate to the node application that represents the website. The `nodeImage` is the container image which should be deployed to a container app. The `nodePort` is the port from the app which should be exposed (`3000` in our case). `nodeIsExternalIngress` is [whether the container should be accessible on the internet](https://docs.microsoft.com/en-us/azure/container-apps/ingress?tabs=bash#configuration). (Always `true` incidentally.)\\n\\nWhen these parameters are applied to the `containerApp` resource, it looks like this:\\n\\n```bicep\\nvar nodeServiceAppName = \'node-app\'\\n\\nresource containerApp \'Microsoft.Web/containerapps@2021-03-01\' = {\\n  // ...\\n  properties: {\\n      // ...\\n      ingress: {\\n        \'external\': nodeIsExternalIngress\\n        \'targetPort\': nodePort\\n      }\\n    }\\n    template: {\\n      containers: [\\n        {\\n          image: nodeImage\\n          name: nodeServiceAppName\\n          // ...\\n        }\\n      ]\\n      // ...\\n    }\\n  }\\n}\\n```\\n\\n### Accessing the GitHub Container Registry\\n\\nGiven that we\'ve told Bicep to deploy an `image`, we\'re going to need to tell it what registry it can use to acquire that image. Our template takes these parameters:\\n\\n```bicep\\nparam containerRegistry string\\nparam containerRegistryUsername string\\n@secure()\\nparam containerRegistryPassword string\\n\\nparam tags object\\n```\\n\\nWith the exception of the `tags` object which is metadata to apply to resources, these parameters are related to the container registry where our images will be stored. GitHub\'s in our case. Remember, what we deploy to Azure Container Apps are container images. To get something running in an ACA, it first has to reside in a container registry. There\'s a multitude of container registries out there and we\'re using the one directly available in GitHub. As an alternative, we could use an [Azure Container Registry](https://azure.microsoft.com/en-us/services/container-registry/), or [Docker Hub](https://hub.docker.com/) - or something else entirely.\\n\\nDo note the [`@secure()`](https://docs.microsoft.com/en-us/azure/azure-resource-manager/bicep/parameters#secure-parameters) decorator. This marks the `containerRegistryPassword` parameter as secure. The value for a secure parameter isn\'t saved to the deployment history and isn\'t logged. Typically you\'ll want to mark secrets with the `@secure()` decorator for this very reason.\\n\\nWe use the parameters to configure the `registries` property of our container app. This tells the ACA where it can go to collect the image it needs. You can also see our first usage of secrets here. We declare the `containerRegistryPassword` as a secret which is stored against the ref `\'container-registry-password\'`; captured as the variable `containerRegistryPasswordRef`. That variable is then referenced in the `passwordSecretRef` property - thus telling ACA where it can find the password.\\n\\n```bicep\\nvar containerRegistryPasswordRef = \'container-registry-password\'\\n\\nresource containerApp \'Microsoft.Web/containerapps@2021-03-01\' = {\\n  // ...\\n  properties: {\\n    // ...\\n    configuration: {\\n      secrets: [\\n        {\\n          name: containerRegistryPasswordRef\\n          value: containerRegistryPassword\\n        }\\n        // ...\\n      ]\\n      registries: [\\n        {\\n          server: containerRegistry\\n          username: containerRegistryUsername\\n          passwordSecretRef: containerRegistryPasswordRef\\n        }\\n      ]\\n      // ...\\n    }\\n    // ...\\n  }\\n}\\n```\\n\\n### Secrets / Configuration\\n\\nThe final collection of parameters are unrelated to the infrastructure of deployment, rather they are the things required to configure our running application:\\n\\n```bicep\\n@secure()\\nparam APPSETTINGS_API_KEY string\\nparam APPSETTINGS_DOMAIN string\\nparam APPSETTINGS_FROM_EMAIL string\\nparam APPSETTINGS_RECIPIENT_EMAIL string\\n```\\n\\nAgain we\'ve got a secret marked with `@secure()` in the form of our `APPSETTINGS_API_KEY`. Just as we did with `containerRegistryPassword`, we declare `APPSETTINGS_API_KEY` to be a secret, which is stored against the ref `\'mailgun-api-key\'`; captured as the variable `mailgunApiKeyRef`.\\n\\nAll of our configuration is exposed to the running application through environment variables. By and large this is achieved through the mechanism of key / value pairs (well technically `name` / `value`) with a slight variation for secrets. Similar to the `passwordSecretRef` mechanism we used for the registry password, we use a `secretref` in place of `value` when passing a secret, and the value will be the ref that was set up in the `secrets` section; `mailgunApiKeyRef` in this case.\\n\\n```bicep\\nvar mailgunApiKeyRef = \'mailgun-api-key\'\\n\\nresource containerApp \'Microsoft.Web/containerapps@2021-03-01\' = {\\n  // ...\\n  properties: {\\n    // ...\\n    configuration: {\\n      secrets: [\\n        // ...\\n        {\\n          name: mailgunApiKeyRef\\n          value: APPSETTINGS_API_KEY\\n        }\\n      ]\\n      // ...\\n    }\\n    template: {\\n      containers: [\\n        {\\n          // ...\\n          env: [\\n            {\\n              name: \'APPSETTINGS_API_KEY\'\\n              secretref: mailgunApiKeyRef\\n            }\\n            {\\n              name: \'APPSETTINGS_DOMAIN\'\\n              value: APPSETTINGS_DOMAIN\\n            }\\n            {\\n              name: \'APPSETTINGS_FROM_EMAIL\'\\n              value: APPSETTINGS_FROM_EMAIL\\n            }\\n            {\\n              name: \'APPSETTINGS_RECIPIENT_EMAIL\'\\n              value: APPSETTINGS_RECIPIENT_EMAIL\\n            }\\n          ]\\n        }\\n      ]\\n      // ...\\n    }\\n  }\\n}\\n```\\n\\n## Setting up a resource group\\n\\nWith our Bicep in place, we\'re going to need a resource group to send it to. Right now, Azure Container Apps aren\'t available everywhere. So we\'re going to create ourselves a resource group in North Europe which does support ACAs:\\n\\n```shell\\naz group create -g rg-aca -l northeurope\\n```\\n\\n## Secrets for GitHub Actions\\n\\nWe\'re aiming to set up a GitHub Action to handle our deployment. This will depend upon a number of secrets:\\n\\n![Screenshot of the secrets in the GitHub website that we need to create](screenshot-github-secrets.png)\\n\\nWe\'ll need to create each of these secrets.\\n\\n### `AZURE_CREDENTIALS` - GitHub logging into Azure\\n\\nSo GitHub Actions can interact with Azure on our behalf, we need to provide it with some credentials. We\'ll use the Azure CLI to create these:\\n\\n```shell\\naz ad sp create-for-rbac --name \\"myApp\\" --role contributor \\\\\\n    --scopes /subscriptions/{subscription-id}/resourceGroups/{resource-group} \\\\\\n    --sdk-auth\\n```\\n\\nRemember to replace the `{subscription-id}` with your subscription id and `{resource-group}` with the name of your resource group (`rg-aca` if you\'re following along). This command will pump out a lump of JSON that looks something like this:\\n\\n```json\\n{\\n  \\"clientId\\": \\"a-client-id\\",\\n  \\"clientSecret\\": \\"a-client-secret\\",\\n  \\"subscriptionId\\": \\"a-subscription-id\\",\\n  \\"tenantId\\": \\"a-tenant-id\\",\\n  \\"activeDirectoryEndpointUrl\\": \\"https://login.microsoftonline.com\\",\\n  \\"resourceManagerEndpointUrl\\": \\"https://management.azure.com/\\",\\n  \\"activeDirectoryGraphResourceId\\": \\"https://graph.windows.net/\\",\\n  \\"sqlManagementEndpointUrl\\": \\"https://management.core.windows.net:8443/\\",\\n  \\"galleryEndpointUrl\\": \\"https://gallery.azure.com/\\",\\n  \\"managementEndpointUrl\\": \\"https://management.core.windows.net/\\"\\n}\\n```\\n\\nTake this and save it as the `AZURE_CREDENTIALS` secret in Azure.\\n\\n### `PACKAGES_TOKEN` - Azure accessing the GitHub container registry\\n\\nWe also need a secret for accessing packages from Azure. We\'re going to be publishing packages to the GitHub container registry. Azure is going to need to be able to access this when we\'re deploying. ACA deployment works by telling Azure where to look for an image and providing any necessary credentials to do the acquisition. To facilitate this we\'ll set up a `PACKAGES_TOKEN` secret. This is a GitHub personal access token with the `read:packages` scope. [Follow the instructions here to create the token.](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token)\\n\\n### Secrets for the app\\n\\nAlongside these infrastructure / deployment related secrets, we\'ll need ones to configure the app at runtime:\\n\\n- `APPSETTINGS_API_KEY` - an API key for Mailgun which will be used to send emails\\n- `APPSETTINGS_DOMAIN` - the domain for the email eg `mg.poorclaresarundel.org`\\n- `APPSETTINGS_FROM_EMAIL` - who automated emails should come from eg `noreply@mg.poorclaresarundel.org`\\n- `APPSETTINGS_RECIPIENT_EMAIL` - the email address emails should be sent to\\n\\nStrictly speaking, only the API key is a secret. But to simplify this post we\'ll configure all of these as secrets in GitHub.\\n\\n## Deploying with GitHub Actions\\n\\nWith our secrets configured, we\'re now well placed to write our GitHub Action. We\'ll create a `.github/workflows/deploy.yaml` file in our repository and populate it thusly:\\n\\n```yaml\\n# yaml-language-server: $schema=./build.yaml\\nname: Build and Deploy\\non:\\n  # Trigger the workflow on push or pull request,\\n  # but only for the main branch\\n  push:\\n    branches:\\n      - main\\n  pull_request:\\n    branches:\\n      - main\\n    # Publish semver tags as releases.\\n    tags: [\'v*.*.*\']\\n  workflow_dispatch:\\n\\nenv:\\n  RESOURCE_GROUP: rg-aca\\n  REGISTRY: ghcr.io\\n  IMAGE_NAME: ${{ github.repository }}\\n\\njobs:\\n  build:\\n    runs-on: ubuntu-latest\\n    strategy:\\n      matrix:\\n        services:\\n          [{ \'imageName\': \'node-service\', \'directory\': \'./node-service\' }]\\n    permissions:\\n      contents: read\\n      packages: write\\n    outputs:\\n      containerImage-node: ${{ steps.image-tag.outputs.image-node-service }}\\n    steps:\\n      - name: Checkout repository\\n        uses: actions/checkout@v2\\n\\n      # Login against a Docker registry except on PR\\n      # https://github.com/docker/login-action\\n      - name: Log into registry ${{ env.REGISTRY }}\\n        if: github.event_name != \'pull_request\'\\n        uses: docker/login-action@v1\\n        with:\\n          registry: ${{ env.REGISTRY }}\\n          username: ${{ github.actor }}\\n          password: ${{ secrets.GITHUB_TOKEN }}\\n\\n      # Extract metadata (tags, labels) for Docker\\n      # https://github.com/docker/metadata-action\\n      - name: Extract Docker metadata\\n        id: meta\\n        uses: docker/metadata-action@v3\\n        with:\\n          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/${{ matrix.services.imageName }}\\n          tags: |\\n            type=semver,pattern={{version}}\\n            type=semver,pattern={{major}}.{{minor}}\\n            type=semver,pattern={{major}}\\n            type=ref,event=branch\\n            type=sha\\n\\n      # Build and push Docker image with Buildx (don\'t push on PR)\\n      # https://github.com/docker/build-push-action\\n      - name: Build and push Docker image\\n        uses: docker/build-push-action@v2\\n        with:\\n          context: ${{ matrix.services.directory }}\\n          push: ${{ github.event_name != \'pull_request\' }}\\n          tags: ${{ steps.meta.outputs.tags }}\\n          labels: ${{ steps.meta.outputs.labels }}\\n\\n      - name: Output image tag\\n        id: image-tag\\n        run: echo \\"::set-output name=image-${{ matrix.services.imageName }}::${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/${{ matrix.services.imageName }}:sha-$(git rev-parse --short HEAD)\\" | tr \'[:upper:]\' \'[:lower:]\'\\n\\n  deploy:\\n    runs-on: ubuntu-latest\\n    needs: [build]\\n    steps:\\n      - name: Checkout repository\\n        uses: actions/checkout@v2\\n\\n      - name: Azure Login\\n        uses: azure/login@v1\\n        with:\\n          creds: ${{ secrets.AZURE_CREDENTIALS }}\\n\\n      - name: Deploy bicep\\n        uses: azure/CLI@v1\\n        if: github.event_name != \'pull_request\'\\n        with:\\n          inlineScript: |\\n            tags=\'{\\"owner\\":\\"johnnyreilly\\", \\"email\\":\\"johnny_reilly@hotmail.com\\"}\'\\n            az deployment group create \\\\\\n              --resource-group ${{ env.RESOURCE_GROUP }} \\\\\\n              --template-file ./infra/main.bicep \\\\\\n              --parameters \\\\\\n                  nodeImage=\'${{ needs.build.outputs.containerImage-node }}\' \\\\\\n                  nodePort=3000 \\\\\\n                  nodeIsExternalIngress=true \\\\\\n                  containerRegistry=${{ env.REGISTRY }} \\\\\\n                  containerRegistryUsername=${{ github.actor }} \\\\\\n                  containerRegistryPassword=${{ secrets.PACKAGES_TOKEN }} \\\\\\n                  tags=\\"$tags\\" \\\\\\n                  APPSETTINGS_API_KEY=\\"${{ secrets.APPSETTINGS_API_KEY }}\\" \\\\\\n                  APPSETTINGS_DOMAIN=\\"${{ secrets.APPSETTINGS_DOMAIN }}\\" \\\\\\n                  APPSETTINGS_FROM_EMAIL=\\"${{ secrets.APPSETTINGS_FROM_EMAIL }}\\" \\\\\\n                  APPSETTINGS_RECIPIENT_EMAIL=\\"${{ secrets.APPSETTINGS_RECIPIENT_EMAIL }}\\"\\n\\n      - name: What-if bicep\\n        uses: azure/CLI@v1\\n        if: github.event_name == \'pull_request\'\\n        with:\\n          inlineScript: |\\n            tags=\'{\\"owner\\":\\"johnnyreilly\\", \\"email\\":\\"johnny_reilly@hotmail.com\\"}\'\\n            az deployment group what-if \\\\\\n              --resource-group ${{ env.RESOURCE_GROUP }} \\\\\\n              --template-file ./infra/main.bicep \\\\\\n              --parameters \\\\\\n                  nodeImage=\'${{ needs.build.outputs.containerImage-node }}\' \\\\\\n                  nodePort=3000 \\\\\\n                  nodeIsExternalIngress=true \\\\\\n                  containerRegistry=${{ env.REGISTRY }} \\\\\\n                  containerRegistryUsername=${{ github.actor }} \\\\\\n                  containerRegistryPassword=${{ secrets.PACKAGES_TOKEN }} \\\\\\n                  tags=\\"$tags\\" \\\\\\n                  APPSETTINGS_API_KEY=\\"${{ secrets.APPSETTINGS_API_KEY }}\\" \\\\\\n                  APPSETTINGS_DOMAIN=\\"${{ secrets.APPSETTINGS_DOMAIN }}\\" \\\\\\n                  APPSETTINGS_FROM_EMAIL=\\"${{ secrets.APPSETTINGS_FROM_EMAIL }}\\" \\\\\\n                  APPSETTINGS_RECIPIENT_EMAIL=\\"${{ secrets.APPSETTINGS_RECIPIENT_EMAIL }}\\"\\n```\\n\\nThere\'s a lot in this workflow. Let\'s dig into the `build` and `deploy` jobs to see what\'s happening.\\n\\n### `build` - building our image\\n\\nThe `build` job is all about building our container images and pushing then to the GitHub registry. It\'s heavily inspired by [Jeff Hollan](https://twitter.com/jeffhollan)\'s [Azure sample app GHA](https://github.com/Azure-Samples/container-apps-store-api-microservice). When we look at the `strategy` we can see a `matrix` of `services` consisting of a single service; our node app:\\n\\n```yaml\\nstrategy:\\n  matrix:\\n    services: [{ \'imageName\': \'node-service\', \'directory\': \'./node-service\' }]\\n```\\n\\nThis is a matrix because a typical use case of an Azure Container App will be multi-container, so we\'re starting generic from the beginning. The `outputs` pumps out the details of our `containerImage-node` image to be used later:\\n\\n```yaml\\noutputs:\\n  containerImage-node: ${{ steps.image-tag.outputs.image-node-service }}\\n```\\n\\nWith that understanding in place, let\'s examine what each of the steps in the `build` job does\\n\\n- `Log into registry` - logs into the GitHub container registry\\n- `Extract Docker metadata` - acquire tags which will be used for versioning\\n- `Build and push Docker image` - build the docker image and if this is not a PR: tag, label and push it to the registry\\n- `Output image tag` - write out the image tag for usage in deployment\\n\\n### `deploy` - shipping our image to Azure\\n\\nThe `deploy` job does two possible things with our Bicep template; `main.bicep`.\\n\\nIn the case of a pull request, it runs the [`az deployment group what-if`](https://docs.microsoft.com/en-us/cli/azure/deployment/group?view=azure-cli-latest#az_deployment_group_what_if) - this allows us to see what the effect would be of applying a PR to our infrastructure.\\n\\n```yaml\\n- name: What-if bicep\\n  uses: azure/CLI@v1\\n  if: github.event_name == \'pull_request\'\\n  with:\\n    inlineScript: |\\n      tags=\'{\\"owner\\":\\"johnnyreilly\\", \\"email\\":\\"johnny_reilly@hotmail.com\\"}\'\\n      az deployment group what-if \\\\\\n        --resource-group ${{ env.RESOURCE_GROUP }} \\\\\\n        --template-file ./infra/main.bicep \\\\\\n        --parameters \\\\\\n            nodeImage=\'${{ needs.build.outputs.containerImage-node }}\' \\\\\\n            nodePort=3000 \\\\\\n            nodeIsExternalIngress=true \\\\\\n            containerRegistry=${{ env.REGISTRY }} \\\\\\n            containerRegistryUsername=${{ github.actor }} \\\\\\n            containerRegistryPassword=${{ secrets.PACKAGES_TOKEN }} \\\\\\n            tags=\\"$tags\\" \\\\\\n            APPSETTINGS_API_KEY=\\"${{ secrets.APPSETTINGS_API_KEY }}\\" \\\\\\n            APPSETTINGS_DOMAIN=\\"${{ secrets.APPSETTINGS_DOMAIN }}\\" \\\\\\n            APPSETTINGS_FROM_EMAIL=\\"${{ secrets.APPSETTINGS_FROM_EMAIL }}\\" \\\\\\n            APPSETTINGS_RECIPIENT_EMAIL=\\"${{ secrets.APPSETTINGS_RECIPIENT_EMAIL }}\\"\\n```\\n\\nWhen it\'s not a pull request, it runs the [`az deployment group create`](https://docs.microsoft.com/en-us/cli/azure/deployment/group?view=azure-cli-latest#az_deployment_group_create) command which performs a deployment of our `main.bicep` file.\\n\\n```yaml\\n- name: Deploy bicep\\n  uses: azure/CLI@v1\\n  if: github.event_name != \'pull_request\'\\n  with:\\n    inlineScript: |\\n      tags=\'{\\"owner\\":\\"johnnyreilly\\", \\"email\\":\\"johnny_reilly@hotmail.com\\"}\'\\n      az deployment group create \\\\\\n        --resource-group ${{ env.RESOURCE_GROUP }} \\\\\\n        --template-file ./infra/main.bicep \\\\\\n        --parameters \\\\\\n            nodeImage=\'${{ needs.build.outputs.containerImage-node }}\' \\\\\\n            nodePort=3000 \\\\\\n            nodeIsExternalIngress=true \\\\\\n            containerRegistry=${{ env.REGISTRY }} \\\\\\n            containerRegistryUsername=${{ github.actor }} \\\\\\n            containerRegistryPassword=${{ secrets.PACKAGES_TOKEN }} \\\\\\n            tags=\\"$tags\\" \\\\\\n            APPSETTINGS_API_KEY=\\"${{ secrets.APPSETTINGS_API_KEY }}\\" \\\\\\n            APPSETTINGS_DOMAIN=\\"${{ secrets.APPSETTINGS_DOMAIN }}\\" \\\\\\n            APPSETTINGS_FROM_EMAIL=\\"${{ secrets.APPSETTINGS_FROM_EMAIL }}\\" \\\\\\n            APPSETTINGS_RECIPIENT_EMAIL=\\"${{ secrets.APPSETTINGS_RECIPIENT_EMAIL }}\\"\\n```\\n\\nIn either case we pass the same set of parameters:\\n\\n```shell\\nnodeImage=\'${{ needs.build.outputs.containerImage-node }}\' \\\\\\nnodePort=3000 \\\\\\nnodeIsExternalIngress=true \\\\\\ncontainerRegistry=${{ env.REGISTRY }} \\\\\\ncontainerRegistryUsername=${{ github.actor }} \\\\\\ncontainerRegistryPassword=${{ secrets.PACKAGES_TOKEN }} \\\\\\ntags=\\"$tags\\" \\\\\\nAPPSETTINGS_API_KEY=\\"${{ secrets.APPSETTINGS_API_KEY }}\\" \\\\\\nAPPSETTINGS_DOMAIN=\\"${{ secrets.APPSETTINGS_DOMAIN }}\\" \\\\\\nAPPSETTINGS_FROM_EMAIL=\\"${{ secrets.APPSETTINGS_FROM_EMAIL }}\\" \\\\\\nAPPSETTINGS_RECIPIENT_EMAIL=\\"${{ secrets.APPSETTINGS_RECIPIENT_EMAIL }}\\"\\n```\\n\\nThese are either:\\n\\n- secrets we set up earlier\\n- environment variables declared at the start of the script or\\n- outputs from the build step - this is where we acquire our node image\\n\\n## Running it\\n\\nWhen the GitHub Action has been run you\'ll find that Azure Container App is now showing up inside the Azure Portal in your resource group, alongside the other resources:\\n\\n![screenshot of the Azure Container App\'s resource group in the Azure Portal](screenshot-azure-portal-container-app.png)\\n\\nAnd when we take a closer look at the container app, we find a URL we can navigate to:\\n\\n![screenshot of the Azure Container App in the Azure Portal revealing it\'s URL](screenshot-azure-portal-container-app-url.png)\\n\\nCongratulations! You\'ve built and deployed a simple web app to Azure Container Apps with Bicep and GitHub Actions and secrets."},{"id":"/2021/12/19/azure-container-apps-bicep-and-github-actions","metadata":{"permalink":"/2021/12/19/azure-container-apps-bicep-and-github-actions","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-12-19-azure-container-apps-bicep-and-github-actions/index.md","source":"@site/blog/2021-12-19-azure-container-apps-bicep-and-github-actions/index.md","title":"Azure Container Apps, Bicep and GitHub Actions","description":"Azure Container Apps are an exciting way to deploy containers to Azure. This post shows how to deploy the infrastructure for an Azure Container App to Azure using Bicep and GitHub Actions. The Azure Container App documentation features quickstarts for deploying your first container app using both the Azure Portal and the Azure CLI. These are great, but there\'s a gap if you prefer to deploy using Bicep and you\'d like to get your CI/CD setup right from the beginning. This post aims to fill that gap.","date":"2021-12-19T00:00:00.000Z","formattedDate":"December 19, 2021","tags":[{"label":"Azure Container Apps","permalink":"/tags/azure-container-apps"},{"label":"Bicep","permalink":"/tags/bicep"},{"label":"GitHub Actions","permalink":"/tags/git-hub-actions"}],"readingTime":3.595,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Azure Container Apps, Bicep and GitHub Actions","authors":"johnnyreilly","tags":["Azure Container Apps","Bicep","GitHub Actions"],"image":"./title-image.png","hide_table_of_contents":false},"prevItem":{"title":"Azure Container Apps: build and deploy with Bicep and GitHub Actions","permalink":"/2021/12/27/azure-container-apps-build-and-deploy-with-bicep-and-github-actions"},"nextItem":{"title":"Open Graph: a guide to sharable social media previews","permalink":"/2021/12/12/open-graph-sharing-previews-guide"}},"content":"Azure Container Apps are an exciting way to deploy containers to Azure. This post shows how to deploy the infrastructure for an Azure Container App to Azure using Bicep and GitHub Actions. The [Azure Container App documentation](https://docs.microsoft.com/en-us/azure/container-apps/) features quickstarts for deploying your first container app using both the Azure Portal and the Azure CLI. These are great, but there\'s a gap if you prefer to deploy using Bicep and you\'d like to get your CI/CD setup right from the beginning. This post aims to fill that gap.\\n\\nIf you\'re interested in building your own containers as well, it\'s worth looking at [this follow up post](../2021-12-27-azure-container-apps-build-and-deploy-with-bicep-and-github-actions/index.md).\\n\\n![title image reading \\"Azure Container Apps, Bicep and GitHub Actions\\" with the Bicep, Azure Container Apps and GitHub Actions logos](title-image.png)\\n\\n## Bicep\\n\\nLet\'s begin with the Bicep required to deploy an Azure Container App.\\n\\nIn our new repository we\'ll create an `infra` directory, into which we\'ll place a `main.bicep` file which will contain our Bicep template.\\n\\nI\'ve pared this down to the simplest Bicep template that I can; it only requires a name parameter:\\n\\n```bicep\\nparam name string\\nparam secrets array = []\\n\\nvar location = resourceGroup().location\\nvar environmentName = \'Production\'\\nvar workspaceName = \'${name}-log-analytics\'\\n\\nresource workspace \'Microsoft.OperationalInsights/workspaces@2020-08-01\' = {\\n  name: workspaceName\\n  location: location\\n  properties: {\\n    sku: {\\n      name: \'PerGB2018\'\\n    }\\n    retentionInDays: 30\\n    workspaceCapping: {}\\n  }\\n}\\n\\nresource environment \'Microsoft.Web/kubeEnvironments@2021-03-01\' = {\\n  name: environmentName\\n  location: location\\n  properties: {\\n    type: \'managed\'\\n    internalLoadBalancerEnabled: false\\n    appLogsConfiguration: {\\n      destination: \'log-analytics\'\\n      logAnalyticsConfiguration: {\\n        customerId: workspace.properties.customerId\\n        sharedKey: listKeys(workspace.id, workspace.apiVersion).primarySharedKey\\n      }\\n    }\\n  }\\n}\\n\\nresource containerApp \'Microsoft.Web/containerapps@2021-03-01\' = {\\n  name: name\\n  kind: \'containerapps\'\\n  location: location\\n  properties: {\\n    kubeEnvironmentId: environment.id\\n    configuration: {\\n      secrets: secrets\\n      registries: []\\n      ingress: {\\n        \'external\':true\\n        \'targetPort\':80\\n      }\\n    }\\n    template: {\\n      containers: [\\n        {\\n          \'name\':\'simple-hello-world-container\'\\n          \'image\':\'mcr.microsoft.com/azuredocs/containerapps-helloworld:latest\'\\n          \'command\':[]\\n          \'resources\':{\\n            \'cpu\':\'.25\'\\n            \'memory\':\'.5Gi\'\\n          }\\n        }\\n      ]\\n    }\\n  }\\n}\\n```\\n\\nSome things to note from the template:\\n\\n- We\'re deploying three resources; a container app, a kube environment and an operational insights.\\n- Just like the official quickstarts we\'re going to use the `containerapps-helloworld` image.\\n\\n## Setting up a resource group\\n\\nIn order that you can deploy your Bicep, we\'re going to need a resource group to send it to. Right now, Azure Container Apps aren\'t available everywhere. So we\'re going to create ourselves a resource group in North Europe which does support ACAs:\\n\\n```shell\\naz group create -g rg-aca -l northeurope\\n```\\n\\n## Deploying with the Azure CLI\\n\\nWith this resource group in place, we could simply deploy using the Azure CLI like so:\\n\\n```shell\\naz deployment group create \\\\\\n  --resource-group rg-aca \\\\\\n  --template-file ./infra/main.bicep \\\\\\n  --parameters \\\\\\n    name=\'container-app\'\\n```\\n\\n## Deploying with GitHub Actions\\n\\nHowever, we\'re aiming to set up a GitHub Action to do this for us. We\'ll create a `.github/workflows/deploy.yaml` file in our repository:\\n\\n```yaml\\nname: Deploy\\non:\\n  push:\\n    branches: [main]\\n  workflow_dispatch:\\n\\nenv:\\n  RESOURCE_GROUP: rg-aca\\n\\njobs:\\n  deploy:\\n    runs-on: ubuntu-latest\\n    steps:\\n      - name: Checkout repository\\n        uses: actions/checkout@v2\\n\\n      - name: Azure Login\\n        uses: azure/login@v1\\n        with:\\n          creds: ${{ secrets.AZURE_CREDENTIALS }}\\n\\n      - name: Deploy bicep\\n        uses: azure/CLI@v1\\n        with:\\n          inlineScript: |\\n            az deployment group create \\\\\\n              --resource-group ${{ env.RESOURCE_GROUP }} \\\\\\n              --template-file ./infra/main.bicep \\\\\\n              --parameters \\\\\\n                name=\'container-app\'\\n```\\n\\nThe above GitHub action is very simple. It:\\n\\n1. Logs into Azure using some `AZURE_CREDENTIALS` we\'ll set up in a moment.\\n2. Invokes the Azure CLI to deploy our Bicep template.\\n\\nLet\'s create that `AZURE_CREDENTIALS` secret in GitHub:\\n\\n![Screenshot of `AZURE_CREDENTIALS` secret in the GitHub website that we need to create](screenshot-github-secrets.png)\\n\\nWe\'ll use the Azure CLI once more:\\n\\n```shell\\naz ad sp create-for-rbac --name \\"myApp\\" --role contributor \\\\\\n    --scopes /subscriptions/{subscription-id}/resourceGroups/{resource-group} \\\\\\n    --sdk-auth\\n```\\n\\nRemember to replace the `{subscription-id}` with your subscription id and `{resource-group}` with the name of your resource group (`rg-aca` if you\'re following along). This command will pump out a lump of JSON that looks something like this:\\n\\n```json\\n{\\n  \\"clientId\\": \\"a-client-id\\",\\n  \\"clientSecret\\": \\"a-client-secret\\",\\n  \\"subscriptionId\\": \\"a-subscription-id\\",\\n  \\"tenantId\\": \\"a-tenant-id\\",\\n  \\"activeDirectoryEndpointUrl\\": \\"https://login.microsoftonline.com\\",\\n  \\"resourceManagerEndpointUrl\\": \\"https://management.azure.com/\\",\\n  \\"activeDirectoryGraphResourceId\\": \\"https://graph.windows.net/\\",\\n  \\"sqlManagementEndpointUrl\\": \\"https://management.core.windows.net:8443/\\",\\n  \\"galleryEndpointUrl\\": \\"https://gallery.azure.com/\\",\\n  \\"managementEndpointUrl\\": \\"https://management.core.windows.net/\\"\\n}\\n```\\n\\nTake this and save it as the `AZURE_CREDENTIALS` secret in Azure.\\n\\n## Running it\\n\\nWhen the GitHub Action has been run you\'ll find that Azure Container App is now showing up inside the Azure Portal:\\n\\n![screenshot of the Azure Container App in the Azure Portal](screenshot-azure-portal-container-app.png)\\n\\nYou\'ll see a URL is displayed, when you go that URL you\'ll find the hello world image is running!\\n\\n![screenshot of the running Azure Container App](screenshot-of-running-container-app.png)"},{"id":"/2021/12/12/open-graph-sharing-previews-guide","metadata":{"permalink":"/2021/12/12/open-graph-sharing-previews-guide","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-12-12-open-graph-sharing-previews-guide/index.md","source":"@site/blog/2021-12-12-open-graph-sharing-previews-guide/index.md","title":"Open Graph: a guide to sharable social media previews","description":"The Open Graph protocol has become the standard mechanism for sharing rich content on the web. This post looks at what implementing Open Graph tags for sharable previews (often called social media previews) looks like, the tools you can use and also an examines the different platform rendering issue.","date":"2021-12-12T00:00:00.000Z","formattedDate":"December 12, 2021","tags":[{"label":"Open Graph","permalink":"/tags/open-graph"}],"readingTime":6.65,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Open Graph: a guide to sharable social media previews","authors":"johnnyreilly","tags":["Open Graph"],"image":"./title-image.png","hide_table_of_contents":false},"prevItem":{"title":"Azure Container Apps, Bicep and GitHub Actions","permalink":"/2021/12/19/azure-container-apps-bicep-and-github-actions"},"nextItem":{"title":"Azure Static Web App Deploy Previews with Azure DevOps","permalink":"/2021/12/05/azure-static-web-app-deploy-previews-with-azure-devops"}},"content":"The Open Graph protocol has become the standard mechanism for sharing rich content on the web. This post looks at what implementing Open Graph tags for sharable previews (often called social media previews) looks like, the tools you can use and also an examines the different platform rendering issue.\\n\\n![title image reading \\"Open Graph: a guide to sharable social media previews\\" with the open graph logo and screenshots of twitter shared cards](title-image.png)\\n\\n## Open Graph protocol and sharing\\n\\nYou may have noticed, that when you share a URL, the platform on which you\'re sharing may display a kind of \\"preview\\" of the link. Here\'s an example of sharing a link to a blog on Twitter:\\n\\n[![screenshot of tweet demonstrating sharing](screenshot-of-tweet-demonstrating-sharing.png)](https://twitter.com/johnny_reilly/status/1454092877722800131)\\n\\nSharing a link has automagically generated a preview \\"card\\" at the bottom of the tweet. It contains an image, it has the title of the blog and it has a description of the post as well.\\n\\nThis looks pretty fabulous and it gives the reader of that tweet some fairly rich information about what might be in that post. It potentially saves readers a click if it\'s obvious that the post isn\'t particularly interesting to them. Conversely, it makes it more likely that the reader will click if it does seem intriguing. Sharing previews are an asset.\\n\\nTwitter made this card using a combination of Open Graph metatags (and some custom tags) which my blog surfaces.\\n\\n## Open Graph meta tags\\n\\nThe [Open Graph protocol](https://ogp.me/) came out of Facebook and it describes itself thusly:\\n\\n> The Open Graph protocol enables any web page to become a rich object in a social graph. For instance, this is used on Facebook to allow any web page to have the same functionality as any other object on Facebook.\\n\\nWhat Open Graph is all about, is meta tags. Adding meta tags to an HTML page to explicitly define pieces of standardised information. Now there\'s many purposes for this, and we\'re interested in just one: sharing.\\n\\nNow that we understand what sharing previews give us, let\'s understand how they work. The [Open Graph](https://ogp.me/#metadata) website has a great walkthrough of the minimum requirement for Open Graph:\\n\\n> - `og:title` - The title of your object as it should appear within the graph, e.g., \\"The Rock\\".\\n> - `og:type` - The type of your object, e.g., \\"video.movie\\". Depending on the type you specify, other properties may also be required.\\n> - `og:image` - An image URL which should represent your object within the graph.\\n> - `og:url` - The canonical URL of your object that will be used as its permanent ID in the graph, e.g., \\"https://www.imdb.com/title/tt0117500/\\".\\n>\\n> As an example, the following is the Open Graph protocol markup for The Rock on IMDB:\\n>\\n> ```html\\n> <html prefix=\\"og: https://ogp.me/ns#\\">\\n>   <head>\\n>     <title>The Rock (1996)</title>\\n>     <meta property=\\"og:title\\" content=\\"The Rock\\" />\\n>     <meta property=\\"og:type\\" content=\\"video.movie\\" />\\n>     <meta property=\\"og:url\\" content=\\"https://www.imdb.com/title/tt0117500/\\" />\\n>     <meta\\n>       property=\\"og:image\\"\\n>       content=\\"https://ia.media-imdb.com/images/rock.jpg\\"\\n>     />\\n>     ...\\n>   </head>\\n>   ...\\n> </html>\\n> ```\\n\\nSharing previews have very similar, but crucially slightly different, requirements. Five tags are required to generate a sharable preview:\\n\\n- `og:title` - The title of your page\\n- `og:description` - A description of the content of that page\\n- `og:image` - An image URL which should appear in the social media share.\\n- `og:url` - The canonical URL of your web page.\\n- `twitter:card` - A [custom tag which is only required by Twitter](https://developer.twitter.com/en/docs/twitter-for-websites/cards/guides/getting-started#started) indicating the type of share, be it `\\"summary\\"`, `\\"summary_large_image\\"`, `\\"app\\"`, or `\\"player\\"`. Probably `\\"summary\\"` or `\\"summary_large_image\\"` for most use cases.\\n\\nIf we implement these, then our page will offer sharable previews.\\n\\nWith this understanding in place; we can take a look at what it would look like to add sharable previews to a website. We\'ll make ourselves a React website with:\\n\\n```\\nnpx react-static create\\n```\\n\\nWhen prompted, name the site `demo` and select the `blank` template.\\n\\nPlease note, nothing that we\'re doing here is React specific; it\'s applicable to all websites regardless of the technology they\'re built with; this is just a straightforward way to demo a website.\\n\\nWe\'re using [`react-static`](https://github.com/react-static/react-static) for this demo because it is a static site generator. This is significant because, as a general rule, many platforms that support sharing, do not crawl dynamically generated meta tags. By this we mean, tags generated by JavaScript at runtime. Rather, these tags must be baked into the HTML that is served up, so a static site generator like `react-static` fits the brief well as it takes care of this.\\n\\nWe\'re going to replace the `App.js` that is scaffolded out with our own `App.js`:\\n\\n```jsx\\nimport * as React from \'react\';\\nimport { Head } from \'react-static\';\\nimport \'./app.css\';\\n\\nfunction App() {\\n  const openGraphData = {\\n    title: \'Open Graph: a guide to sharing previews\',\\n    description:\\n      \'This page features the Open Graph protocol markup for sharing previews.\',\\n    url: \'https://johnnyreilly.github.io/open-graph-sharing-previews/\',\\n    image:\\n      \'https://upload.wikimedia.org/wikipedia/commons/7/72/Open_Graph_protocol_logo.png\',\\n  };\\n  return (\\n    <div className=\\"App\\">\\n      <Head>\\n        <meta property=\\"og:title\\" content={openGraphData.title} />\\n        <meta property=\\"og:description\\" content={openGraphData.description} />\\n        <meta property=\\"og:url\\" content={openGraphData.url} />\\n        <meta property=\\"og:image\\" content={openGraphData.image} />\\n        <meta name=\\"twitter:card\\" content=\\"summary\\" />\\n      </Head>\\n      <h1>{openGraphData.title}</h1>\\n      <img src={openGraphData.image} alt=\\"The Open Graph protocol logo\\" />\\n      <h2>Share it and see!</h2>\\n    </div>\\n  );\\n}\\n\\nexport default App;\\n```\\n\\nThe code above renders the required meta tags for sharing previews. When we build and deploy this we can see they show up like so:\\n\\n![screenshot of demo with devtools open illustrating the meta tags](screenshot-of-demo-with-devtools-open.png)\\n\\n## Tools for testing sharing\\n\\nNow we have a demo, it would be tremendous to be able to test it out. There\'s various official tools to test your URLs:\\n\\n- [Twitter](https://cards-dev.twitter.com/validator)\\n- [Facebook](https://developers.facebook.com/tools/debug/)\\n- [LinkedIn](https://www.linkedin.com/post-inspector/inspect/)\\n\\nThere\'s also a number of unoffical \\"aggregator\\" tools that attempt to render the appearance of your social previews across multiple platforms to save you going to each tool in turn:\\n\\n- https://www.opengraph.xyz/\\n- https://metatags.io/\\n- https://socialsharepreview.com/\\n\\nLet\'s test out the Twitter validator:\\n\\n![screenshot of testing out our site using the twitter validator](screenshot-of-twitter-validator.png)\\n\\nTerrific! We have sharable previews enabled for the site we\'ve made.\\n\\n## Sharable preview rendering: not yet standard\\n\\nNow we have a sense of what sharing previews look like, what powers them and how to implement them. So far we\'ve looked just at Twitter for examples of sharing previews. However, support for Open Graph sharing previews is widespread. Examples of other places where you can use them include: Facebook, Polywork, Slack, Teams, Linked In, Outlook.com, Discord... The list is now very long indeed.\\n\\nHowever, each platform implements sharing previews according to their own standard. What does mean? Well, a link shared on Twitter will look different to one shared on Outlook.com. For example:\\n\\n![screenshot of an email being sent in outlook with a share preview card to the same blog showing the untruncated title](screenshot-of-email-demonstrating-sharing-with-a-non-cropped-image.png)\\n\\nAbove I\'m sharing a link to a blog post. The image is to the left, the title and description is to the right. Now let\'s look at the same link shared on Twitter:\\n\\n[![screenshot of a tweet where the image in the share preview card has been cropped making the title unreadable](screenshot-of-tweet-demonstrating-sharing-with-a-cropped-image.png)](https://twitter.com/AzureWeekly/status/1436733027489652743)\\n\\nHere the image is above the title and the description. More distressingly, the image has been cropped which renders the title slightly unreadable.\\n\\nSo whilst the mechanism for sharing is roughly standardised, the rendering is not. It\'s not dissimilar to the web in the year 2000. Back then, a single piece of HTML could be rendered in many different ways, depending upon the browser. The same statement is true now for Open Graph sharing. Sharing can look very different depending upon the platform which is displaying the preview. The only way to avoid this at present is to thoroughly on all the platforms where we want to share links; ensuring the sharable previews look acceptable.\\n\\n## Conclusion\\n\\nIn this post we\'ve understood what sharable previews are, how to add them to a website, how to test them and some of the rough edges to be aware of.\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/open-graph-sharable-social-media-previews/)"},{"id":"/2021/12/05/azure-static-web-app-deploy-previews-with-azure-devops","metadata":{"permalink":"/2021/12/05/azure-static-web-app-deploy-previews-with-azure-devops","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-12-05-azure-static-web-app-deploy-previews-with-azure-devops/index.md","source":"@site/blog/2021-12-05-azure-static-web-app-deploy-previews-with-azure-devops/index.md","title":"Azure Static Web App Deploy Previews with Azure DevOps","description":"I love Netlify deploy previews. This post implements a pull request deployment preview mechanism for Azure Static Web Apps in the context of Azure DevOps which is very much inspired by the Netlify offering.","date":"2021-12-05T00:00:00.000Z","formattedDate":"December 5, 2021","tags":[{"label":"Azure Static Web Apps","permalink":"/tags/azure-static-web-apps"},{"label":"Azure DevOps","permalink":"/tags/azure-dev-ops"},{"label":"Netlify deploy previews","permalink":"/tags/netlify-deploy-previews"}],"readingTime":10.76,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Azure Static Web App Deploy Previews with Azure DevOps","authors":"johnnyreilly","tags":["Azure Static Web Apps","Azure DevOps","Netlify deploy previews"],"image":"./title-image.png","hide_table_of_contents":false},"prevItem":{"title":"Open Graph: a guide to sharable social media previews","permalink":"/2021/12/12/open-graph-sharing-previews-guide"},"nextItem":{"title":"TypeScript vs JSDoc JavaScript","permalink":"/2021/11/22/typescript-vs-jsdoc-javascript"}},"content":"I love [Netlify deploy previews](https://www.netlify.com/products/deploy-previews/). This post implements a pull request deployment preview mechanism for Azure Static Web Apps in the context of Azure DevOps which is very much inspired by the Netlify offering.\\n\\n![title image reading \\"Azure Static Web App Deploy Previews with Azure DevOps\\" with a Azure, Bicep and Azure DevOps logos](title-image.png)\\n\\nHaving a build of your latest pull request which is deployed and clickable from the PR itself is a wonderful developer experience. It reduces friction for testing out changes by allowing you to see the impact from within the PR itself. No checking to see if an environment is free with the rest of the team, then manually running a pipeline and waiting whilst a deployment happens. No. It\'s all there without you having to lift a finger. I use Netlify deploy previews on my blog and have become accustomed to the delight that is this:\\n\\n![screenshot of a Netlify deploy preview on my latest blog post](screenshot-of-netlify-deploy-preview-in-pull-request.png)\\n\\nI love this and I wanted to implement the \\"browse the preview\\" mechanism in Azure DevOps as well, using Azure Static Web Apps. This blog post contains two things:\\n\\n1. A pull request deployment environment mechanism using Azure and Azure Pipelines with Bicep.\\n2. A mechanism for updating a pull request in Azure DevOps with a link to the deployment environment (the \\"browse the preview\\")\\n\\nIt\'s worth bearing in mind that there\'s a very similar feature to what we\'re going to build for **1.** in SWAs now called \\"staging environments\\" that is presently only available on GitHub and not Azure DevOps:\\n\\n[![screenshot of Anthony Chu at Microsoft saying \\"Unfortunately environments is not yet available for Azure DevOps.\\"](screenshot-of-staging-environments-not-available-yet.png)](https://docs.microsoft.com/en-us/answers/questions/574288/creating-environments-for-azure-static-web-app.html)\\n\\nIt\'s possible that in future the deployment environment aspect of this blog post may be rendered redundant by staging environments landing in Azure DevOps. However, the second part, which updates a PR in ADO with a link is probably generally useful. And it may be the case that the approach of provisioning an environment on demand and extracting a URL could be reworked to work with App Service and similar too.\\n\\nI wrote about using [SWAs with Azure DevOps earlier this year](./2021-08-15-bicep-azure-static-web-apps-azure-devops/index.md). This blog post will take the form of a [pull request on the code written in that post](https://dev.azure.com/johnnyreilly/azure-static-web-apps/_git/azure-static-web-apps/pullrequest/3).\\n\\n## Getting `defaultHostName` from Static Web Apps\\n\\nThe first thing we\'re going to do is take the Bicep from that post and tweak it to the following:\\n\\n```bicep\\nparam appName string\\nparam repositoryUrl string\\nparam repositoryBranch string\\n\\nparam skuName string = \'Free\'\\nparam skuTier string = \'Free\'\\n\\nresource staticWebApp \'Microsoft.Web/staticSites@2021-02-01\' = {\\n  name: repositoryBranch == \'main\' ? appName : \'${appName}-${repositoryBranch}\'\\n  location: resourceGroup().location\\n  sku: {\\n    name: skuName\\n    tier: skuTier\\n  }\\n  properties: {\\n    // The provider, repositoryUrl and branch fields are required for successive deployments to succeed\\n    // for more details see: https://github.com/Azure/static-web-apps/issues/516\\n    provider: \'DevOps\'\\n    repositoryUrl: repositoryUrl\\n    branch: repositoryBranch\\n    buildProperties: {\\n      skipGithubActionWorkflowGeneration: true\\n    }\\n  }\\n}\\n\\noutput staticWebAppDefaultHostName string = staticWebApp.properties.defaultHostname // eg gentle-bush-0db02ce03.azurestaticapps.net\\noutput staticWebAppId string = staticWebApp.id\\noutput staticWebAppName string = staticWebApp.name\\n```\\n\\nThere\'s some changes in here. First of all we\'re using a newer version of the `staticSites` resource in Azure. You\'ll also see that we name the resource conditionally now. If we\'re on the `main` branch we name it as we did before with `appName`. But if we aren\'t then we suffix the `name` with the `repositoryBranch`. It\'s worth knowing that [there are restrictions and conventions for Azure resource naming](https://docs.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-best-practices/resource-abbreviations#compute-and-web). If you have a branch name that is just alphanumerics and hyphens you\'ll be fine.\\n\\nYou\'ll see the output of the Bicep file has changed. Previously we were outputting the `apiKey` that we used for deployment. This isn\'t the securest of approaches as, by having this as a deployment output, this data can be accessed by people who share access with your Azure portal. So we\'re going to use a different (and more secure) approach to acquire this in our pipeline later.\\n\\nMore significantly, we are now outputting the `staticWebAppDefaultHostName` of our newly provisioned SWA. This is the location where people will be able to view the deployment preview. Since we want to pump that into our pull request description, so people can click on the link, we are going to need this. We\'re also pumping out the `staticWebAppId` and `staticWebAppName`. We\'ll use the `staticWebAppName` to acquire the `apiKey` in our pipeline.\\n\\n## Azure Pipelines tweaks\\n\\nNow to the pipeline. After the deployment, our updated pipeline is going to acquire the `apiKey` for deployment like so:\\n\\n```yml\\n- task: AzureCLI@2\\n  displayName: \'Acquire API key for deployment\'\\n  inputs:\\n    azureSubscription: $(serviceConnection)\\n    scriptType: bash\\n    scriptLocation: inlineScript\\n    inlineScript: |\\n      APIKEY=$(az staticwebapp secrets list --name $(staticWebAppName) | jq -r \'.properties.apiKey\')\\n      echo \\"##vso[task.setvariable variable=apiKey;issecret=true]$APIKEY\\"\\n```\\n\\nThe above uses the [Azure CLI task](https://docs.microsoft.com/en-us/azure/devops/pipelines/tasks/deploy/azure-cli?view=azure-devops) to acquire the `apiKey`. It uses [jq](https://stedolan.github.io/jq/) to pull out the required property from the JSON and writes it as a secret variable in the pipeline to be used in the deployment.\\n\\nAt the end of the pipeline, if we\'re not on the `main` branch, the the pipeline is going to run a custom script that will update the PR with the preview URL:\\n\\n```yml\\n- task: Npm@1\\n  displayName: \'Pull request preview install\'\\n  condition: and(succeeded(), ne(variables.isMain, \'true\'))\\n  inputs:\\n    command: \'install\'\\n    workingDir: pull-request-preview\\n\\n- task: Npm@1\\n  displayName: \'Pull request preview\'\\n  condition: and(succeeded(), ne(variables.isMain, \'true\'))\\n  inputs:\\n    command: \'custom\'\\n    customCommand: \'run pull-request-preview -- --sat \\"$(System.AccessToken)\\" --project \\"$(System.TeamProject)\\" --repository \\"$(Build.Repository.Name)\\" --systemCollectionUri \\"$(System.CollectionUri)\\" --pullRequestId $(System.PullRequest.PullRequestId) --previewUrl \\"https://$(staticWebAppDefaultHostName)\\"\'\\n    workingDir: pull-request-preview\\n```\\n\\nWe haven\'t written that script yet; we will in a moment.\\n\\nThe complete `azure-piplines.yml` is below, and you\'ll notice we\'ve moved all variables save for the `subscriptionId` into the `azure-pipelines.yml` and we\'re using a `westeurope` location / resource group as at present `staticSites` is not available everywhere:\\n\\n```yml\\npool:\\n  vmImage: ubuntu-latest\\n\\nvariables:\\n  # subscriptionId is a variable defined on the pipeline itself\\n  - name: appName\\n    value: \'our-static-web-app\'\\n  - name: location\\n    value: \'westeurope\' #\xa0at time of writing static sites are available in limited locations such as westeurope\\n  - name: serviceConnection\\n    value: \'azureRMWestEurope\'\\n  - name: azureResourceGroup # this resource group lives in westeurope\\n    value: \'johnnyreilly\'\\n\\nsteps:\\n  - checkout: self\\n    submodules: true\\n\\n  - bash: az bicep build --file infra/static-web-app/main.bicep\\n    displayName: \'Compile Bicep to ARM\'\\n\\n  - task: AzureResourceManagerTemplateDeployment@3\\n    name: DeployStaticWebAppInfra\\n    displayName: Deploy Static Web App infra\\n    inputs:\\n      deploymentScope: Resource Group\\n      azureResourceManagerConnection: $(serviceConnection)\\n      subscriptionId: $(subscriptionId)\\n      action: Create Or Update Resource Group\\n      resourceGroupName: $(azureResourceGroup)\\n      location: $(location)\\n      templateLocation: Linked artifact\\n      csmFile: \'infra/static-web-app/main.json\' # created by bash script\\n      overrideParameters: >-\\n        -repositoryUrl $(Build.Repository.Uri)\\n        -repositoryBranch $(Build.SourceBranchName)\\n        -appName $(appName)\\n      deploymentMode: Incremental\\n      deploymentOutputs: deploymentOutputs\\n\\n  - task: PowerShell@2\\n    name: \'SetDeploymentOutputVariables\'\\n    displayName: \'Set Deployment Output Variables\'\\n    inputs:\\n      targetType: inline\\n      script: |\\n        $armOutputObj = \'$(deploymentOutputs)\' | ConvertFrom-Json\\n        $armOutputObj.PSObject.Properties | ForEach-Object {\\n          $keyname = $_.Name\\n          $value = $_.Value.value\\n\\n          # Creates a standard pipeline variable\\n          Write-Output \\"##vso[task.setvariable variable=$keyName;]$value\\"\\n\\n          # Display keys and values in pipeline\\n          Write-Output \\"output variable: $keyName $value\\"\\n        }\\n      pwsh: true\\n\\n  - task: AzureCLI@2\\n    displayName: \'Acquire API key for deployment\'\\n    inputs:\\n      azureSubscription: $(serviceConnection)\\n      scriptType: bash\\n      scriptLocation: inlineScript\\n      inlineScript: |\\n        APIKEY=$(az staticwebapp secrets list --name $(staticWebAppName) | jq -r \'.properties.apiKey\')\\n        echo \\"##vso[task.setvariable variable=apiKey;issecret=true]$APIKEY\\"\\n\\n  - task: AzureStaticWebApp@0\\n    name: DeployStaticWebApp\\n    displayName: Deploy Static Web App\\n    inputs:\\n      app_location: \'static-web-app\'\\n      # api_location: \'api\'\\n      output_location: \'build\'\\n      azure_static_web_apps_api_token: $(apiKey)\\n\\n  - task: Npm@1\\n    displayName: \'Pull request preview install\'\\n    condition: and(succeeded(), ne(variables.isMain, \'true\'))\\n    inputs:\\n      command: \'install\'\\n      workingDir: pull-request-preview\\n\\n  - task: Npm@1\\n    displayName: \'Pull request preview\'\\n    condition: and(succeeded(), ne(variables.isMain, \'true\'))\\n    inputs:\\n      command: \'custom\'\\n      customCommand: \'run pull-request-preview -- --sat \\"$(System.AccessToken)\\" --project \\"$(System.TeamProject)\\" --repository \\"$(Build.Repository.Name)\\" --systemCollectionUri \\"$(System.CollectionUri)\\" --pullRequestId $(System.PullRequest.PullRequestId) --previewUrl \\"https://$(staticWebAppDefaultHostName)\\"\'\\n      workingDir: pull-request-preview\\n```\\n\\n## Updating the PR with a preview URL\\n\\nWe want to be able to update our pull request with our deploy URL. To make that happen, we\'re going to whiz up a little node app using TypeScript, ts-node and [the azure-devops-node-api package](https://github.com/microsoft/azure-devops-node-api).\\n\\nLet\'s create our app:\\n\\n```bash\\nmkdir pull-request-preview\\ncd pull-request-preview\\nnpm init --yes\\nnpm install @types/node @types/yargs ts-node typescript azure-devops-node-api yargs --save\\n```\\n\\nWe\'ll update our newly created `package.json` file with a `pull-request-preview` script which will be the entry point.\\n\\n```json\\n  \\"scripts\\": {\\n    \\"pull-request-preview\\": \\"ts-node ./index.ts\\"\\n  },\\n```\\n\\nWe\'ll add a `tsconfig.json` file that looks like this:\\n\\n```json\\n{\\n  \\"compilerOptions\\": {\\n    \\"target\\": \\"ES2015\\",\\n    \\"module\\": \\"CommonJS\\",\\n    \\"strict\\": true,\\n    \\"esModuleInterop\\": true,\\n    \\"moduleResolution\\": \\"node\\"\\n  }\\n}\\n```\\n\\nFinally we\'ll add our script in a new `index.ts` file:\\n\\n```ts\\n#!/usr/bin/env node\\nimport yargs from \'yargs/yargs\';\\nimport * as nodeApi from \'azure-devops-node-api\';\\nimport { IGitApi } from \'azure-devops-node-api/GitApi\';\\nimport { PullRequestStatus } from \'azure-devops-node-api/interfaces/GitInterfaces\';\\n\\nconst parser = yargs(process.argv.slice(2)).options({\\n  pat: { type: \'string\', default: \'\' },\\n  sat: { type: \'string\', default: \'\' },\\n  systemCollectionUri: { type: \'string\', demandOption: true },\\n  project: { type: \'string\', demandOption: true },\\n  repository: { type: \'string\', demandOption: true },\\n  pullRequestId: { type: \'number\' },\\n  previewUrl: { type: \'string\', demandOption: true },\\n});\\n\\n(async () => {\\n  await run(await parser.argv);\\n})();\\n\\nasync function run({\\n  pat,\\n  sat,\\n  project,\\n  repository,\\n  systemCollectionUri,\\n  pullRequestId,\\n  previewUrl,\\n}: {\\n  pat: string;\\n  sat: string;\\n  systemCollectionUri: string;\\n  project: string;\\n  repository: string;\\n  pullRequestId: number | undefined;\\n  previewUrl: string;\\n}) {\\n  const config: Config = { project, repository };\\n  const gitApi = await getGitApi({ pat, sat, systemCollectionUri });\\n\\n  if (!pullRequestId)\\n    console.log(\\n      \'No pull request id supplied, so will look up latest active PR\'\\n    );\\n\\n  const pullRequestIdToUpdate =\\n    pullRequestId || (await getActivePullRequestId({ gitApi, config }));\\n  if (!pullRequestIdToUpdate) {\\n    console.log(\'No pull request found\');\\n    return;\\n  }\\n\\n  console.log(\\n    `Updating ${systemCollectionUri}/${project}/_git/${repository}/pullrequest/${pullRequestIdToUpdate} with a preview URL of ${previewUrl}`\\n  );\\n\\n  const pullRequest = await getPullRequest({\\n    gitApi,\\n    config,\\n    pullRequestId: pullRequestIdToUpdate,\\n  });\\n\\n  await updatePullRequestDescription({\\n    gitApi,\\n    config,\\n    pullRequestId: pullRequestIdToUpdate,\\n    description: makePreviewDescriptionMarkdown(\\n      pullRequest.description!,\\n      previewUrl\\n    ),\\n  });\\n\\n  console.log(\\n    `Updated pull request description a preview URL of ${previewUrl}`\\n  );\\n}\\n\\ninterface Config {\\n  project: string;\\n  repository: string;\\n}\\n\\nasync function getGitApi({\\n  sat,\\n  pat,\\n  systemCollectionUri,\\n}: {\\n  pat: string;\\n  sat: string;\\n  systemCollectionUri: string;\\n}) {\\n  const authHandler = pat\\n    ? nodeApi.getPersonalAccessTokenHandler(\\n        pat,\\n        /** allowCrossOriginAuthentication */ true\\n      )\\n    : nodeApi.getHandlerFromToken(\\n        sat,\\n        /** allowCrossOriginAuthentication */ true\\n      );\\n\\n  const webApi = new nodeApi.WebApi(systemCollectionUri, authHandler);\\n  const gitApi = await webApi.getGitApi();\\n\\n  return gitApi;\\n}\\n\\nasync function getActivePullRequestId({\\n  gitApi,\\n  config,\\n}: {\\n  gitApi: IGitApi;\\n  config: Config;\\n}) {\\n  const topActivePullRequest = await gitApi.getPullRequests(\\n    config.repository, // repository.id!,\\n    { status: PullRequestStatus.Active },\\n    config.project,\\n    undefined,\\n    /** skip */ 0,\\n    /** top */ 1\\n  );\\n\\n  return topActivePullRequest.length > 0\\n    ? topActivePullRequest[0].pullRequestId\\n    : undefined;\\n}\\n\\nasync function getPullRequest({\\n  gitApi,\\n  config,\\n  pullRequestId,\\n}: {\\n  gitApi: IGitApi;\\n  config: Config;\\n  pullRequestId: number;\\n}) {\\n  const pullRequest = await gitApi.getPullRequest(\\n    config.repository, // repository.id!,\\n    pullRequestId,\\n    config.project,\\n    undefined,\\n    /** skip */ 0,\\n    /** top */ 1,\\n    /** includeCommits */ false,\\n    /** includeWorkItemRefs */ false\\n  );\\n  return pullRequest;\\n}\\n\\nasync function updatePullRequestDescription({\\n  gitApi,\\n  config,\\n  pullRequestId,\\n  description,\\n}: {\\n  gitApi: IGitApi;\\n  config: Config;\\n  pullRequestId: number;\\n  description: string;\\n}) {\\n  // To do an update with the API you must provide a new object with only the properties you are updating\\n  const updatePullRequest = {\\n    description,\\n  };\\n  await gitApi.updatePullRequest(\\n    updatePullRequest,\\n    config.repository,\\n    pullRequestId,\\n    config.project\\n  );\\n}\\n\\nfunction makePreviewDescriptionMarkdown(desc: string, previewUrl: string) {\\n  const previewRegex = /(> -*\\\\n> # Preview:\\\\n.*\\\\n>.*\\\\n> -*\\\\n)/;\\n\\n  const makePreview = (previewUrl: string) => `> ---\\n> # Preview:\\n> ${previewUrl}\\n> \\n> ---\\n`;\\n\\n  const alreadyHasPreview = desc.match(previewRegex);\\n  return alreadyHasPreview\\n    ? desc.replace(previewRegex, makePreview(previewUrl))\\n    : makePreview(previewUrl) + desc;\\n}\\n```\\n\\nThe above code does two things:\\n\\n1. Looks up the pull request, using the details supplied from the pipeline. It\'s worth noting that the `System.PullRequest.PullRequestId` variable is [initialized only if the build ran because of a Git PR affected by a branch policy](https://docs.microsoft.com/en-us/azure/devops/pipelines/build/variables?view=azure-devops&tabs=yaml). If you don\'t have that set up, the script falls back to using the latest active pull request. This is generally useful when you\'re getting set up in the first place; you won\'t want to rely on this behaviour.\\n2. Updates the pull request description with a prefix piece of markdown that provides the link to the preview URL. This is our \\"browse the preview\\":\\n   ![screenshot of rendered markdown with the preview link](screenshot-of-deploy-preview-small.png)\\n\\nThis script could be refactored into a dedicated Azure Pipelines custom task.\\n\\n## Permissions\\n\\nThe first time you run this you may encounter a permissions error of the form:\\n\\n```\\nError: TF401027: You need the Git \'PullRequestContribute\' permission to perform this action.\\n```\\n\\nTo remedy this you need to give your build service the relevant permissions to update a pull request. You can do that by going to the security settings of your repo and setting \\"Contribute to pull requests\\" to \\"Allow\\" for your build service:\\n\\n![Screenshot of \\"Contribute to pull requests\\" permission in Azure DevOps Git security being set to \\"Allow\\" ](screenshot-of-git-repository-security-settings.png)\\n\\n## Enjoy! (and keep Azure tidy)\\n\\nWhen the pipeline is now run you can see that a deployment preview link is now updated onto the PR description:\\n\\n![Screenshot of deployment preview on PR](screenshot-of-deploy-preview.png)\\n\\nThis will happen whenever a PR is raised which is tremendous.\\n\\nA thing to remember, is that there\'s nothing in this post that tears down the temporary deployment after the pull request has been merged. It will hang around. We happen to be using free resources in this post, but if we weren\'t there would be cost implications. Either way, you\'ll want to clean up unused environments as a matter of course. And I\'d advise automating that.\\n\\nSo be tidy and cost aware with this approach."},{"id":"/2021/11/22/typescript-vs-jsdoc-javascript","metadata":{"permalink":"/2021/11/22/typescript-vs-jsdoc-javascript","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-11-22-typescript-vs-jsdoc-javascript/index.md","source":"@site/blog/2021-11-22-typescript-vs-jsdoc-javascript/index.md","title":"TypeScript vs JSDoc JavaScript","description":"There\'s a debate to be had about whether using JavaScript or TypeScript leads to better outcomes when building a project. The introduction of using JSDoc annotations to type a JavaScript codebase introduces a new dynamic to this discussion. This post will investigate what that looks like, and come to an (opinionated) conclusion.","date":"2021-11-22T00:00:00.000Z","formattedDate":"November 22, 2021","tags":[{"label":"JavaScript","permalink":"/tags/java-script"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"JSDoc","permalink":"/tags/js-doc"}],"readingTime":5.28,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"TypeScript vs JSDoc JavaScript","authors":"johnnyreilly","tags":["JavaScript","TypeScript","JSDoc"],"image":"./title-image.png","hide_table_of_contents":false},"prevItem":{"title":"Azure Static Web App Deploy Previews with Azure DevOps","permalink":"/2021/12/05/azure-static-web-app-deploy-previews-with-azure-devops"},"nextItem":{"title":"Azure standard availability tests with Bicep","permalink":"/2021/11/18/azure-standard-tests-with-bicep"}},"content":"There\'s a debate to be had about whether using JavaScript or TypeScript leads to better outcomes when building a project. The introduction of using JSDoc annotations to type a JavaScript codebase introduces a new dynamic to this discussion. This post will investigate what that looks like, and come to an (opinionated) conclusion.\\n\\n![title image reading \\"JSDoc JavaScript vs TypeScript\\" with a JavaScript logo and TypeScript logo](title-image.png)\\n\\n## Updated 6th December 2021\\n\\nThis blog evolved to become a talk:\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/5MZoAcheyE4?start=240\\" title=\\"YouTube video player\\" frameBorder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowFullScreen></iframe>\\n\\nIf you\'d talked to me in 2018, I would have solidly recommended using TypeScript, and steering away from JavaScript. The rationale is simple: I\'m exceedingly convinced of the value that static typing provides in terms of productivity / avoiding bugs in production. I appreciate this can be a contentious issue, but that is my settled opinion on the subject. Other opinions are available.\\n\\nTypeScript has long had a good static typing story. JavaScript is dynamically typed and so historically has not. Thanks to TypeScript support for JSDoc, JavaScript can now be statically type checked.\\n\\n## What is JSDoc JavaScript?\\n\\nJSDoc itself actually dates way back to 1999. According to the [Wikipedia entry](https://en.wikipedia.org/wiki/JSDoc):\\n\\n> JSDoc is a markup language used to annotate JavaScript source code files. Using comments containing JSDoc, programmers can add documentation describing the application programming interface of the code they\'re creating.\\n\\nThe TypeScript team have taken JSDoc support and run with it. You can now use a [variant of JSDoc annotations](https://www.typescriptlang.org/docs/handbook/jsdoc-supported-types.html) to provide type information in JavaScript files.\\n\\nWhat does this look like? Well, to take a simple example, a TypeScript statement like so:\\n\\n```ts\\nlet myString: string;\\n```\\n\\nCould become the equivalent JavaScript statement with a JSDoc annotation:\\n\\n```ts\\n/** @type {string} */\\nlet myString;\\n```\\n\\nThis is type enhanced JavaScript which the TypeScript compiler can understand and type check.\\n\\n## Why use JSDoc JavaScript?\\n\\nWhy would you use JSDoc JavaScript instead of TypeScript? Well there\'s a number of possible use cases.\\n\\nPerhaps you\'re writing simple node scripts and you\'d like a little type safety to avoid mistakes. Or perhaps you want to dip your project\'s toe in the waters of static type checking but without fully committing. JSDoc allows for that. Or perhaps your team simply prefers not having a compile step.\\n\\nThat, in fact, was the rationale of the webpack team. A little bit of history: webpack has always been a JavaScript codebase. As the codebase grew and grew, there was often discussion about using static typing. However, having a compilation step wasn\'t desired.\\n\\nTypeScript had been quietly adding support for type checking JavaScript with the assistance of JSDoc for some time. Initial support arrived with the `--checkJs` compiler option in [TypeScript 2.3](https://www.typescriptlang.org/docs/handbook/release-notes/typescript-2-3.html#errors-in-js-files-with---checkjs).\\n\\nA community member by the name of [Mohsen Azimi](https://twitter.com/mohsen____) experimentally started out using this approach to type check the webpack codebase. [His PR](https://github.com/webpack/webpack/pull/6862) ended up being a test case that helped improve the type checking of JavaScript by TypeScript. TypeScript v2.9 shipped with a whole host of JSDoc improvements as a consequence of the webpack work. Being such a widely used project this also helped popularise the approach of using JSDoc to type check JavaScript codebases. It demonstrated that this approach could work on a significantly sized codebase.\\n\\nThese days, JSDoc type checking with TypeScript is extremely powerful. Whilst not quite on par with TypeScript (not all TypeScript syntax is supported in JSDoc) the gap in functionality is pretty small.\\n\\nIt\'s a completely legitimate choice to build a JavaScript codebase with all the benefits of static typing.\\n\\n## Why use TypeScript?\\n\\nSo if you were starting a project today, and you\'d decided you wanted to make use of static typing, how do you choose? TypeScript or JavaScript with JSDoc?\\n\\nWell, unless you\'ve a compelling need to avoid a compilation step, I\'m going to suggest that TypeScript may be the better choice for a number of reasons.\\n\\nFirstly, the tooling support for using TypeScript directly is better than that for JSDoc JavaScript. At the time of writing, things like refactoring tools etc in your editor work more effectively with TypeScript than with JSDoc JavaScript. (Although these are improving as time goes by.)\\n\\nSecondly, working with JSDoc is distinctly \\"noisier\\". It requires far more keystrokes to achieve the same level of type safety. Consider the following TypeScript:\\n\\n```ts\\nfunction stringsStringStrings(\\n  p1: string,\\n  p2?: string,\\n  p3?: string,\\n  p4 = \'test\'\\n): string {\\n  // ...\\n}\\n```\\n\\nAs compared to the equivalent JSDoc JavaScript:\\n\\n```ts\\n/**\\n * @param {string}  p1\\n * @param {string=} p2\\n * @param {string} [p3]\\n * @param {string} [p4=\\"test\\"]\\n * @return {string}\\n */\\nfunction stringsStringStrings(p1, p2, p3, p4) {\\n  // ...\\n}\\n```\\n\\nIt may be my own familiarity with TypeScript speaking, but I find that the TypeScript is easier to read and comprehend as compared to the JSDoc JavaScript alternative. The fact that all JSDoc annotations live in comments, rather than directly in syntax, makes it harder to follow. (It certainly doesn\'t help that many VS Code themes present comments in a very faint colour.)\\n\\nMy final reason for favouring TypeScript comes down to falling into the [\\"pit of success\\"](https://blog.codinghorror.com/falling-into-the-pit-of-success/). You\'re cutting _against_ the grain when it comes to static typing and JavaScript. You can have it, but you have to work that bit harder to ensure that you have statically typed code. On the other hand, you\'re cutting _with_ the grain when it comes to static typing and TypeScript. You have to work hard to opt out of static typing. The TypeScript defaults tend towards static typing, whilst the JavaScript defaults tend away.\\n\\nAs someone who very much favours static typing, you can imagine how this is compelling to me!\\n\\n## It\'s your choice!\\n\\nSo in a way, I don\'t feel super strongly whether people use JavaScript or TypeScript. But having static typing will likely be a benefit to new projects. Bottom line, I\'m keen that people fall into the \\"pit of success\\", so my recommendation for a new project would be TypeScript.\\n\\nI really like JSDoc myself, and will often use it on small projects. It\'s a fantastic addition to TypeScript\'s capabilities. For bigger projects, I\'ll likely go with TypeScript from the get go. But really, this is a choice - and either is great.\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/typescript-vs-jsdoc-javascript/)"},{"id":"/2021/11/18/azure-standard-tests-with-bicep","metadata":{"permalink":"/2021/11/18/azure-standard-tests-with-bicep","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-11-18-azure-standard-tests-with-bicep/index.md","source":"@site/blog/2021-11-18-azure-standard-tests-with-bicep/index.md","title":"Azure standard availability tests with Bicep","description":"Azure standard tests are a tremendous way to monitor the uptime of your services in Azure. Sometimes also called availability tests, web tests and ping tests, this post goes through how to deploy one using Bicep. It also looks at some of the gotchas that you may encounter as you\'re setting it up.","date":"2021-11-18T00:00:00.000Z","formattedDate":"November 18, 2021","tags":[{"label":"Azure","permalink":"/tags/azure"},{"label":"Bicep","permalink":"/tags/bicep"},{"label":"standard tests","permalink":"/tags/standard-tests"}],"readingTime":5.745,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Azure standard availability tests with Bicep","authors":"johnnyreilly","tags":["Azure","Bicep","standard tests"],"image":"./title-image.png","hide_table_of_contents":false},"prevItem":{"title":"TypeScript vs JSDoc JavaScript","permalink":"/2021/11/22/typescript-vs-jsdoc-javascript"},"nextItem":{"title":"NSwag generated C# client: Open API property name clashes and decimal types rather than double","permalink":"/2021/10/31/nswag-generated-c-sharp-client-property-name-clash"}},"content":"Azure standard tests are a tremendous way to monitor the uptime of your services in Azure. Sometimes also called availability tests, web tests and ping tests, this post goes through how to deploy one using Bicep. It also looks at some of the gotchas that you may encounter as you\'re setting it up.\\n\\n![title image reading \\"Azure standard availability tests with Bicep\\" with a Bicep logo and Azure logos](title-image.png)\\n\\n## What are standard tests?\\n\\nTo quote the [docs](https://docs.microsoft.com/en-us/azure/azure-monitor/app/availability-standard-tests):\\n\\n> Standard tests are a single request test that is similar to the URL ping test but more advanced. In addition to validating whether an endpoint is responding and measuring the performance, Standard tests also includes SSL certificate validity, proactive lifetime check, HTTP request verb (for example GET,HEAD,POST, etc.), custom headers, and custom data associated with your HTTP request.\\n\\nSo we can use these to:\\n\\n- send requests to a URL\\n- from a variety of geographic locations\\n- and determine if it is responding with a 200 status code\\n\\nThe URL may be one of our own service URLs, but it could be checking any kind of URL. It\'s web specific, not Azure specific.\\n\\n## Standard test Bicep\\n\\nNow we\'re going to write a Bicep module that provisions a standard test named `standard-test.bicep`:\\n\\n```bicep\\n@description(\'Tags that our resources need\')\\nparam tags object\\n\\n@description(\'The resource id of the app insights which the webtest will reference\')\\nparam appInsightsResourceId string\\n\\n@description(\'The name of the webtest to create\')\\nparam standardTestName string\\n\\n@description(\'URL to test\')\\nparam urlToTest string\\n\\n@description(\'Interval in seconds between test runs for this WebTest. Default value is 300.\')\\nparam frequency int = 300\\n\\n@description(\'Seconds until this WebTest will timeout and fail. Default value is 30.\')\\nparam timeout int = 30\\n\\n// useful reference:\\n// https://docs.microsoft.com/en-us/azure/azure-monitor/app/monitor-web-app-availability#azure\\n@allowed([\\n  \'emea-au-syd-edge\' // Australia\u202fEast\\n  \'latam-br-gru-edge\' // Brazil South\\n  \'us-fl-mia-edge\' // Central US\\n  \'apac-hk-hkn-azr\' // East Asia\\n  \'us-va-ash-azr\' // East US\\n  \'emea-ch-zrh-edge\' // France South (Formerly France Central)\\n  \'emea-fr-pra-edge\' // France Central\\n  \'apac-jp-kaw-edge\' // Japan East\\n  \'emea-gb-db3-azr\' // North Europe\\n  \'us-il-ch1-azr\' // North Central US\\n  \'us-tx-sn1-azr\' // South Central US\\n  \'apac-sg-sin-azr\' // Southeast Asia\\n  \'emea-se-sto-edge\' // UK West\\n  \'emea-nl-ams-azr\' // West Europe\\n  \'us-ca-sjc-azr\' // West US\\n  \'emea-ru-msa-edge\' // UK South\\n])\\n@description(\'The populations (locations) for the test\')\\nparam testPopulations array = [\\n  \'emea-se-sto-edge\' // UK West\\n  \'emea-ru-msa-edge\' // UK South\\n  \'emea-gb-db3-azr\' // North Europe\\n  \'us-va-ash-azr\' // East US\\n  \'apac-sg-sin-azr\' // Southeast Asia\\n]\\n\\nvar tagsWithHiddenLink = union({\\n  \'hidden-link:${appInsightsResourceId}\': \'Resource\'\\n}, tags)\\n\\nresource standardWebTest \'Microsoft.Insights/webtests@2018-05-01-preview\' = {\\n  name: standardTestName\\n  location: resourceGroup().location\\n  tags: tagsWithHiddenLink\\n  kind: \'ping\'\\n  properties: {\\n    SyntheticMonitorId: urlToTest\\n    Name: urlToTest\\n    Description: null\\n    Enabled: true\\n    Frequency: frequency\\n    Timeout: timeout\\n    Kind: \'standard\'\\n    RetryEnabled: true\\n    Locations: [for testPopulation in testPopulations: {\\n      Id: testPopulation\\n    }]\\n    Configuration: null\\n    Request: {\\n      RequestUrl: urlToTest\\n      Headers: null\\n      HttpVerb: \'GET\'\\n      RequestBody: null\\n      ParseDependentRequests: false\\n      FollowRedirects: null\\n    }\\n    ValidationRules: {\\n      ExpectedHttpStatusCode: 200\\n      IgnoreHttpsStatusCode: false\\n      ContentValidation: null\\n      SSLCheck: true\\n      SSLCertRemainingLifetimeCheck: 7\\n    }\\n  }\\n}\\n\\noutput standardWebTestName string = standardWebTest.name\\noutput standardWebTestId string = standardWebTest.id\\n```\\n\\n### Locations / populations\\n\\nYou\'ll note that a parameter to the Bicep module is `testPopulations`. These are the geographical places where requests will be sent from. You\'ll note we have a default value of five populations, but these could be any of the (presently) sixteen valid values. If you were wondering where those are sourced from, [here is the link to the Azure docs](https://docs.microsoft.com/en-us/azure/azure-monitor/app/availability-standard-tests#location-population-tags).\\n\\n### The `hidden-link` tag\\n\\nAnother significant call out should go to the `hidden-link` tag. The `hidden-link` tag is a mandatory tag that connects the test (known in Azure as a \\"webtest\\") to an app insights instance.\\n\\nIf you do not provide a `hidden-link` tag, or if you try to specify a resource group other than the app insights resource group, Azure will fail to deploy your test and you may find yourself presented with an error like this in the deployments section of the Azure Portal.\\n\\n> Resource should exist in the same resource group as the linked component\\n\\n![screenshot of the Azure Portal Deployments section saying \\"Resource should exist in the same resource group as the linked component\\"](screenshot-azure-portal-deployments-resource-should-exist-in-the-same-resource-group.png)\\n\\nIn our module we set both the `hidden-link` tag as well as the tags that have been supplied via the `tags` parameter.\\n\\n### App insights and standard tests share a resource group\\n\\nAnother thing that can cause issues is the deployment of your app insights resource. It\'s not unusual to spin up Azure resources on demand, for a given branch of your source code. Those resources will be named in relation to the branch and will depend upon one another. I\'ve never managed to successfully create an app insights resource, and reference it from a standard test within the same Bicep file. It appears to be necessary to separate the two actions, such that Azure recognises the existence of the app insights resource when the standard test is deployed.\\n\\nIf you are working with long-lived app insights it won\'t be an issue for you, but if you aren\'t it\'s worth being aware of.\\n\\n## Using `standard-test.bicep`\\n\\nOur Bicep module can be invoked from another Bicep module named `ping-them.bicep` like so:\\n\\n```bicep\\n@description(\'Tags that our resources need\')\\nparam tags object\\n\\n@description(\'The name of the app insights\')\\nparam appInsightsName string\\n\\n@description(\'An object where the keys are the name of the web test and the values are the URL eg {\\"my-standard-test\\": \\"https://status.azure.com/en-gb/status\\"} \')\\nparam standardTests object\\n\\nvar appInsightsResourceId = resourceId(\'Microsoft.Insights/components\', appInsightsName)\\n\\nmodule standardTestsToCreate \'standard-test.bicep\' = [for standardTest in items(standardTests): {\\n  name: standardTest.key\\n  params: {\\n    tags: tags\\n    appInsightsResourceId: appInsightsResourceId\\n    standardTestName: standardTest.key\\n    urlToTest: standardTest.value\\n  }\\n}]\\n```\\n\\nAs you can see, this module itself takes a number of parameters, and will typically be invoked from some kind of continuous integration mechanism such as Azure Pipelines or GitHub Actions.\\n\\nThis module is written in the expectation that multiple URLs will need to be pinged, and so it has a parameter named `standardTests` which is effectively a dictionary of key-value pairs, where the key is the name of the standard test, and the value is the URL to test.\\n\\nThe module makes use of the [`items`](https://docs.microsoft.com/en-us/azure/azure-resource-manager/bicep/bicep-functions-array#items) array helper in Bicep to convert the object into an array that can be iterated over.\\n\\n## Azure Pipelines test\\n\\nWe\'re going to use Azure Pipelines to test this out. Here\'s an `azure-pipelines.yml` file:\\n\\n```yml\\ntrigger:\\n  - main\\n\\npool:\\n  vmImage: ubuntu-latest\\n\\nsteps:\\n  - checkout: self\\n    submodules: true\\n\\n  - bash: az bicep build --file ping-them.bicep\\n    displayName: \'Compile Bicep to ARM\'\\n\\n  - task: AzureResourceManagerTemplateDeployment@3\\n    name: DeploySharedWebTests\\n    displayName: Deploy Shared Web Tests\\n    inputs:\\n      deploymentScope: Resource Group\\n      azureResourceManagerConnection: ${{ variables.serviceConnection }}\\n      subscriptionId: $(subscriptionId)\\n      action: Create Or Update Resource Group\\n      resourceGroupName: $(resourceGroup)\\n      location: $(location)\\n      templateLocation: Linked artifact\\n      csmFile: \'ping-them.json\' # created by bash script\\n      overrideParameters: >-\\n        -tags {\\"owner\\": \\"@johnny_reilly\\", \\"branch\\": \\"$(Build.SourceBranchName)\\"}\\n        -appInsightsName $(appInsightsName)\\n        -standardTests {\\"my-standard-test\\": \\"https://status.azure.com/en-gb/status\\"}\\n      deploymentMode: Incremental\\n```\\n\\nWhen run, it invokes our `ping-them.bicep` module, passing two URLs to test.\\n\\nWhen executed, you end up with a delightful \\"availability test\\" (which is your standard test) in Azure:\\n\\n![screenshot of an Availability test in the Azure Portal](screenshot-azure-portal-availability.png)"},{"id":"/2021/10/31/nswag-generated-c-sharp-client-property-name-clash","metadata":{"permalink":"/2021/10/31/nswag-generated-c-sharp-client-property-name-clash","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-10-31-nswag-generated-c-sharp-client-property-name-clash/index.md","source":"@site/blog/2021-10-31-nswag-generated-c-sharp-client-property-name-clash/index.md","title":"NSwag generated C# client: Open API property name clashes and decimal types rather than double","description":"NSwag is a great tool for generating client libraries in C# and TypeScript from Open API / Swagger definitions. You can face issues where Open API property names collide due to the nature of the C# language, and when you want to use decimal for your floating point numeric type over double. This post demonstrates how to get over both issues.","date":"2021-10-31T00:00:00.000Z","formattedDate":"October 31, 2021","tags":[{"label":"nswag","permalink":"/tags/nswag"},{"label":"CSharp","permalink":"/tags/c-sharp"}],"readingTime":10.575,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"NSwag generated C# client: Open API property name clashes and decimal types rather than double","authors":"johnnyreilly","tags":["nswag","CSharp"],"image":"./title-image.png","hide_table_of_contents":false},"prevItem":{"title":"Azure standard availability tests with Bicep","permalink":"/2021/11/18/azure-standard-tests-with-bicep"},"nextItem":{"title":"Docusaurus, meta tags and Google Discover","permalink":"/2021/10/18/docusaurus-meta-tags-and-google-discover"}},"content":"NSwag is a great tool for generating client libraries in C# and TypeScript from Open API / Swagger definitions. You can face issues where Open API property names collide due to the nature of the C# language, and when you want to use `decimal` for your floating point numeric type over `double`. This post demonstrates how to get over both issues.\\n\\n![title image reading \\"NSwag generated C# client: Open API property name clashes and decimal types rather than double\\" with a C# logo and Open API logos](title-image.png)\\n\\n## Make a C# Client Generator\\n\\nLet\'s get a console app set up that will allow us to generate a C# client using an Open API file:\\n\\n```sh\\ndotnet new console -o NSwag\\ncd NSwag\\ndotnet add package NSwag.CodeGeneration.CSharp\\n```\\n\\nWe\'ll also add a `petstore-simple.json` file to our project which we\'ll borrow from https://github.com/OAI/OpenAPI-Specification/blob/main/examples/v2.0/json/petstore-simple.json (home of the Open API specification):\\n\\n```json\\n{\\n  \\"swagger\\": \\"2.0\\",\\n  \\"info\\": {\\n    \\"version\\": \\"1.0.0\\",\\n    \\"title\\": \\"Swagger Petstore\\",\\n    \\"description\\": \\"A sample API that uses a petstore as an example to demonstrate features in the swagger-2.0 specification\\",\\n    \\"termsOfService\\": \\"http://swagger.io/terms/\\",\\n    \\"contact\\": {\\n      \\"name\\": \\"Swagger API Team\\"\\n    },\\n    \\"license\\": {\\n      \\"name\\": \\"MIT\\"\\n    }\\n  },\\n  \\"host\\": \\"petstore.swagger.io\\",\\n  \\"basePath\\": \\"/api\\",\\n  \\"schemes\\": [\\"http\\"],\\n  \\"consumes\\": [\\"application/json\\"],\\n  \\"produces\\": [\\"application/json\\"],\\n  \\"paths\\": {\\n    \\"/pets\\": {\\n      \\"get\\": {\\n        \\"description\\": \\"Returns all pets from the system that the user has access to\\",\\n        \\"operationId\\": \\"findPets\\",\\n        \\"produces\\": [\\n          \\"application/json\\",\\n          \\"application/xml\\",\\n          \\"text/xml\\",\\n          \\"text/html\\"\\n        ],\\n        \\"parameters\\": [\\n          {\\n            \\"name\\": \\"tags\\",\\n            \\"in\\": \\"query\\",\\n            \\"description\\": \\"tags to filter by\\",\\n            \\"required\\": false,\\n            \\"type\\": \\"array\\",\\n            \\"items\\": {\\n              \\"type\\": \\"string\\"\\n            },\\n            \\"collectionFormat\\": \\"csv\\"\\n          },\\n          {\\n            \\"name\\": \\"limit\\",\\n            \\"in\\": \\"query\\",\\n            \\"description\\": \\"maximum number of results to return\\",\\n            \\"required\\": false,\\n            \\"type\\": \\"integer\\",\\n            \\"format\\": \\"int32\\"\\n          }\\n        ],\\n        \\"responses\\": {\\n          \\"200\\": {\\n            \\"description\\": \\"pet response\\",\\n            \\"schema\\": {\\n              \\"type\\": \\"array\\",\\n              \\"items\\": {\\n                \\"$ref\\": \\"#/definitions/Pet\\"\\n              }\\n            }\\n          },\\n          \\"default\\": {\\n            \\"description\\": \\"unexpected error\\",\\n            \\"schema\\": {\\n              \\"$ref\\": \\"#/definitions/ErrorModel\\"\\n            }\\n          }\\n        }\\n      },\\n      \\"post\\": {\\n        \\"description\\": \\"Creates a new pet in the store.  Duplicates are allowed\\",\\n        \\"operationId\\": \\"addPet\\",\\n        \\"produces\\": [\\"application/json\\"],\\n        \\"parameters\\": [\\n          {\\n            \\"name\\": \\"pet\\",\\n            \\"in\\": \\"body\\",\\n            \\"description\\": \\"Pet to add to the store\\",\\n            \\"required\\": true,\\n            \\"schema\\": {\\n              \\"$ref\\": \\"#/definitions/NewPet\\"\\n            }\\n          }\\n        ],\\n        \\"responses\\": {\\n          \\"200\\": {\\n            \\"description\\": \\"pet response\\",\\n            \\"schema\\": {\\n              \\"$ref\\": \\"#/definitions/Pet\\"\\n            }\\n          },\\n          \\"default\\": {\\n            \\"description\\": \\"unexpected error\\",\\n            \\"schema\\": {\\n              \\"$ref\\": \\"#/definitions/ErrorModel\\"\\n            }\\n          }\\n        }\\n      }\\n    },\\n    \\"/pets/{id}\\": {\\n      \\"get\\": {\\n        \\"description\\": \\"Returns a user based on a single ID, if the user does not have access to the pet\\",\\n        \\"operationId\\": \\"findPetById\\",\\n        \\"produces\\": [\\n          \\"application/json\\",\\n          \\"application/xml\\",\\n          \\"text/xml\\",\\n          \\"text/html\\"\\n        ],\\n        \\"parameters\\": [\\n          {\\n            \\"name\\": \\"id\\",\\n            \\"in\\": \\"path\\",\\n            \\"description\\": \\"ID of pet to fetch\\",\\n            \\"required\\": true,\\n            \\"type\\": \\"integer\\",\\n            \\"format\\": \\"int64\\"\\n          }\\n        ],\\n        \\"responses\\": {\\n          \\"200\\": {\\n            \\"description\\": \\"pet response\\",\\n            \\"schema\\": {\\n              \\"$ref\\": \\"#/definitions/Pet\\"\\n            }\\n          },\\n          \\"default\\": {\\n            \\"description\\": \\"unexpected error\\",\\n            \\"schema\\": {\\n              \\"$ref\\": \\"#/definitions/ErrorModel\\"\\n            }\\n          }\\n        }\\n      },\\n      \\"delete\\": {\\n        \\"description\\": \\"deletes a single pet based on the ID supplied\\",\\n        \\"operationId\\": \\"deletePet\\",\\n        \\"parameters\\": [\\n          {\\n            \\"name\\": \\"id\\",\\n            \\"in\\": \\"path\\",\\n            \\"description\\": \\"ID of pet to delete\\",\\n            \\"required\\": true,\\n            \\"type\\": \\"integer\\",\\n            \\"format\\": \\"int64\\"\\n          }\\n        ],\\n        \\"responses\\": {\\n          \\"204\\": {\\n            \\"description\\": \\"pet deleted\\"\\n          },\\n          \\"default\\": {\\n            \\"description\\": \\"unexpected error\\",\\n            \\"schema\\": {\\n              \\"$ref\\": \\"#/definitions/ErrorModel\\"\\n            }\\n          }\\n        }\\n      }\\n    }\\n  },\\n  \\"definitions\\": {\\n    \\"Pet\\": {\\n      \\"type\\": \\"object\\",\\n      \\"allOf\\": [\\n        {\\n          \\"$ref\\": \\"#/definitions/NewPet\\"\\n        },\\n        {\\n          \\"required\\": [\\"id\\"],\\n          \\"properties\\": {\\n            \\"id\\": {\\n              \\"type\\": \\"integer\\",\\n              \\"format\\": \\"int64\\"\\n            }\\n          }\\n        }\\n      ]\\n    },\\n    \\"NewPet\\": {\\n      \\"type\\": \\"object\\",\\n      \\"required\\": [\\"name\\"],\\n      \\"properties\\": {\\n        \\"name\\": {\\n          \\"type\\": \\"string\\"\\n        },\\n        \\"tag\\": {\\n          \\"type\\": \\"string\\"\\n        }\\n      }\\n    },\\n    \\"ErrorModel\\": {\\n      \\"type\\": \\"object\\",\\n      \\"required\\": [\\"code\\", \\"message\\"],\\n      \\"properties\\": {\\n        \\"code\\": {\\n          \\"type\\": \\"integer\\",\\n          \\"format\\": \\"int32\\"\\n        },\\n        \\"message\\": {\\n          \\"type\\": \\"string\\"\\n        }\\n      }\\n    }\\n  }\\n}\\n```\\n\\nWe\'ll tweak our `NSwag.csproj` file to ensure that the `json` file is included in our build output:\\n\\n```xml\\n<Project Sdk=\\"Microsoft.NET.Sdk\\">\\n  \x3c!-- ... ---\x3e\\n  <ItemGroup>\\n    <Content Include=\\"**\\\\*.json\\">\\n      <CopyToOutputDirectory>PreserveNewest</CopyToOutputDirectory>\\n    </Content>\\n  </ItemGroup>\\n</Project>\\n```\\n\\nThis will give us a console app with a reference to NSwag. Now we\'ll flesh out the `Program.cs` file thusly:\\n\\n```cs\\nusing System;\\nusing System.IO;\\nusing System.Reflection;\\nusing System.Threading.Tasks;\\nusing NJsonSchema;\\nusing NJsonSchema.Visitors;\\nusing NSwag.CodeGeneration.CSharp;\\n\\nnamespace NSwag {\\n    class Program {\\n        static async Task Main(string[] args) {\\n            Console.WriteLine(\\"Generating client...\\");\\n            await ClientGenerator.GenerateCSharpClient();\\n            Console.WriteLine(\\"Generated client.\\");\\n        }\\n    }\\n\\n    public static class ClientGenerator {\\n\\n        public async static Task GenerateCSharpClient() =>\\n            GenerateClient(\\n                // https://github.com/OAI/OpenAPI-Specification/blob/main/examples/v2.0/json/petstore-simple.json\\n                document: await GetDocumentFromFile(\\"petstore-simple.json\\"),\\n                generatedLocation: \\"GeneratedClient.cs\\",\\n                generateCode: (OpenApiDocument document) => {\\n                    var settings = new CSharpClientGeneratorSettings();\\n\\n                    var generator = new CSharpClientGenerator(document, settings);\\n                    var code = generator.GenerateFile();\\n                    return code;\\n                }\\n            );\\n\\n        private static void GenerateClient(OpenApiDocument document, string generatedLocation, Func<OpenApiDocument, string> generateCode) {\\n            var root = Path.GetDirectoryName(Assembly.GetEntryAssembly().Location);\\n            var location = Path.GetFullPath(Path.Join(root, @\\"../../../\\", generatedLocation));\\n\\n            Console.WriteLine($\\"Generating {location}...\\");\\n\\n            var code = generateCode(document);\\n\\n            System.IO.File.WriteAllText(location, code);\\n        }\\n\\n        private static async Task<OpenApiDocument> GetDocumentFromFile(string swaggerJsonFilePath) {\\n            var root = Path.GetDirectoryName(Assembly.GetEntryAssembly().Location);\\n            var swaggerJson = await File.ReadAllTextAsync(Path.GetFullPath(Path.Join(root, swaggerJsonFilePath)));\\n            var document = await OpenApiDocument.FromJsonAsync(swaggerJson);\\n\\n            return document;\\n        }\\n    }\\n}\\n```\\n\\nIf we perform a `dotnet run` we now pump out a `GeneratedClient.cs` file which is a C# client library for the pet store. Fabulous.\\n\\nSo far so dandy. We\'re taking an Open API `json` file and generating a C# client library from it.\\n\\n## When properties collide\\n\\nIt\'s time to break things. We\'re presently generating a `Pet` class that looks like this:\\n\\n```cs\\n[System.CodeDom.Compiler.GeneratedCode(\\"NJsonSchema\\", \\"10.5.2.0 (Newtonsoft.Json v13.0.0.0)\\")]\\npublic partial class Pet : NewPet\\n{\\n    [Newtonsoft.Json.JsonProperty(\\"id\\", Required = Newtonsoft.Json.Required.Always)]\\n    public long Id { get; set; }\\n}\\n```\\n\\nWe\'re going to take our `Pet` definition in the `petstore-simple.json` file, and add a new `@id` property alongside the `id` property:\\n\\n```json\\n\\"Pet\\": {\\n    \\"type\\": \\"object\\",\\n    \\"allOf\\": [\\n        {\\n            \\"$ref\\": \\"#/definitions/NewPet\\"\\n        },\\n        {\\n            \\"required\\": [\\n                \\"id\\"\\n            ],\\n            \\"properties\\": {\\n                \\"id\\": {\\n                    \\"type\\": \\"integer\\",\\n                    \\"format\\": \\"int64\\"\\n                },\\n                \\"@id\\": {\\n                    \\"type\\": \\"integer\\",\\n                    \\"format\\": \\"int64\\"\\n                }\\n            }\\n        }\\n    ]\\n},\\n```\\n\\nFor why? Whilst this may seem esoteric, this is a scenario that can present. It\'s not unknown to encounter properties which are identical, save for an `@` prefix. This is often the case for meta-properties.\\n\\nWhat do we get if we run our generator over that?\\n\\n```cs\\n[System.CodeDom.Compiler.GeneratedCode(\\"NJsonSchema\\", \\"10.5.2.0 (Newtonsoft.Json v13.0.0.0)\\")]\\npublic partial class Pet : NewPet\\n{\\n    [Newtonsoft.Json.JsonProperty(\\"id\\", Required = Newtonsoft.Json.Required.Always)]\\n    public long Id { get; set; }\\n\\n    [Newtonsoft.Json.JsonProperty(\\"@id\\", Required = Newtonsoft.Json.Required.Always)]\\n    public long Id { get; set; }\\n}\\n```\\n\\nWe get code that doesn\'t compile. You can\'t have two properties in a C# class with the same name. You also cannot have `@` as a character in a C# property or variable name. To quote the [docs](https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/tokens/verbatim):\\n\\n> The @ special character serves as a verbatim identifier.\\n\\nIt so happens that, by default, NSwag purges `@` characters from property names. If there isn\'t another property which is named the same save for an `@` prefix, this is a fine strategy. If there is, as for us now, you\'re toast.\\n\\nThere\'s a workaround. We\'ll create a new `HandleAtCSharpPropertyNameGenerator` class:\\n\\n```cs\\n/// <summary>\\n/// Replace characters which will not comply with C# syntax with something that will\\n/// </summary>\\npublic class HandleAtCSharpPropertyNameGenerator : NJsonSchema.CodeGeneration.IPropertyNameGenerator {\\n    /// <summary>Generates the property name.</summary>\\n    /// <param name=\\"property\\">The property.</param>\\n    /// <returns>The new name.</returns>\\n    public virtual string Generate(JsonSchemaProperty property) =>\\n        ConversionUtilities.ConvertToUpperCamelCase(property.Name\\n            .Replace(\\"\\\\\\"\\", string.Empty)\\n            .Replace(\\"@\\", \\"__\\") // make \\"@\\" => \\"__\\", so \\"@type\\" => \\"__type\\"\\n            .Replace(\\"?\\", string.Empty)\\n            .Replace(\\"$\\", string.Empty)\\n            .Replace(\\"[\\", string.Empty)\\n            .Replace(\\"]\\", string.Empty)\\n            .Replace(\\"(\\", \\"_\\")\\n            .Replace(\\")\\", string.Empty)\\n            .Replace(\\".\\", \\"-\\")\\n            .Replace(\\"=\\", \\"-\\")\\n            .Replace(\\"+\\", \\"plus\\"), true)\\n            .Replace(\\"*\\", \\"Star\\")\\n            .Replace(\\":\\", \\"_\\")\\n            .Replace(\\"-\\", \\"_\\")\\n            .Replace(\\"#\\", \\"_\\");\\n}\\n```\\n\\nThis is a replacement for the `CSharpPropertyNameGenerator` that NSwag ships with. Rather than purging the `@` character, it replaces usage with a double underscore: `__`.\\n\\nWe\'ll make use of our new `PropertyNameGenerator`:\\n\\n```cs\\npublic async static Task GenerateCSharpClient() =>\\n    GenerateClient(\\n        // https://github.com/OAI/OpenAPI-Specification/blob/main/examples/v2.0/json/petstore-simple.json\\n        document: await GetDocumentFromFile(\\"petstore-simple.json\\"),\\n        generatedLocation: \\"GeneratedClient.cs\\",\\n        generateCode: (OpenApiDocument document) => {\\n            var settings = new CSharpClientGeneratorSettings {\\n                CSharpGeneratorSettings = {\\n                    PropertyNameGenerator = new HandleAtCSharpPropertyNameGenerator() // @ shouldn\'t cause us problems\\n                }\\n            };\\n\\n            var generator = new CSharpClientGenerator(document, settings);\\n            var code = generator.GenerateFile();\\n            return code;\\n        }\\n    );\\n```\\n\\nWith this in place, when we `dotnet run` we create a class that looks like this:\\n\\n```cs\\n[System.CodeDom.Compiler.GeneratedCode(\\"NJsonSchema\\", \\"10.5.2.0 (Newtonsoft.Json v13.0.0.0)\\")]\\npublic partial class Pet : NewPet\\n{\\n    [Newtonsoft.Json.JsonProperty(\\"id\\", Required = Newtonsoft.Json.Required.Always)]\\n    public long Id { get; set; }\\n\\n    [Newtonsoft.Json.JsonProperty(\\"@id\\", Required = Newtonsoft.Json.Required.Always)]\\n    public long __id { get; set; }\\n}\\n```\\n\\nSo the newly generated property name is `__id` rather than the clashing `Id`. Rather wonderfully, this works. It resolves the issue we faced. We\'ve chosen to use `__` as our prefix - we could choose something else if that worked better for us.\\n\\nKnowing that this hook exists is super useful.\\n\\n## Use `decimal` not `double` for floating point numbers\\n\\nAnother common problem with generated C# clients is the number type used to represent floating point numbers. The default for C# is `double`.\\n\\nThis is a reasonable choice when you consider the [official format](https://swagger.io/docs/specification/data-models/data-types/#numbers) for highly precise floating point numbers is `double`:\\n\\n> OpenAPI has two numeric types, `number` and `integer`, where `number` includes both integer and floating-point numbers. An optional `format` keyword serves as a hint for the tools to use a specific numeric type:\\n>\\n> `float` - Floating-point numbers.\\n> `double` - Floating-point numbers with double precision.\\n\\nLet\'s tweak our pet definition to reflect this:\\n\\n```json\\n\\"Pet\\": {\\n    \\"type\\": \\"object\\",\\n    \\"allOf\\": [\\n        {\\n            \\"$ref\\": \\"#/definitions/NewPet\\"\\n        },\\n        {\\n            \\"required\\": [\\n                \\"id\\"\\n            ],\\n            \\"properties\\": {\\n                \\"id\\": {\\n                    \\"type\\": \\"number\\",\\n                    \\"format\\": \\"double\\"\\n                },\\n                \\"@id\\": {\\n                    \\"type\\": \\"number\\",\\n                    \\"format\\": \\"double\\"\\n                }\\n            }\\n        }\\n    ]\\n},\\n```\\n\\nWith this in place, when we `dotnet run` we create a class that looks like this:\\n\\n```cs\\n[System.CodeDom.Compiler.GeneratedCode(\\"NJsonSchema\\", \\"10.5.2.0 (Newtonsoft.Json v13.0.0.0)\\")]\\npublic partial class Pet : NewPet\\n{\\n    [Newtonsoft.Json.JsonProperty(\\"id\\", Required = Newtonsoft.Json.Required.Always)]\\n    public double Id { get; set; }\\n\\n    [Newtonsoft.Json.JsonProperty(\\"@id\\", Required = Newtonsoft.Json.Required.Always)]\\n    public double __id { get; set; }\\n}\\n```\\n\\nC# developers may well rather work with a [`decimal`](https://docs.microsoft.com/en-us/dotnet/api/system.decimal?view=net-5.0) type which can handle \\"financial calculations that require large numbers of significant integral and fractional digits and no round-off errors\\".\\n\\nThere is a way to switch from using `double` to `decimal` in your generated clients. I\'ve been using the approach for some years, and I suspect I first adapted it from [a comment on GitHub](https://github.com/RicoSuter/NSwag/issues/1814#issuecomment-448752684).\\n\\nIt uses the [visitor pattern](https://en.m.wikipedia.org/wiki/Visitor_pattern) and looks like this:\\n\\n```cs\\n/// <summary>\\n/// By default the C# decimal number type used is double; this makes it decimal\\n/// </summary>\\npublic class DoubleToDecimalVisitor : JsonSchemaVisitorBase {\\n    protected override JsonSchema VisitSchema(JsonSchema schema, string path, string typeNameHint) {\\n        if (schema.Type == JsonObjectType.Number)\\n            schema.Format = JsonFormatStrings.Decimal;\\n\\n        return schema;\\n    }\\n}\\n```\\n\\nThe code above, when invoked upon our `OpenApiDocument`, changes the format of all number types to be `decimal`. Which results in code along these lines:\\n\\n```cs\\n[System.CodeDom.Compiler.GeneratedCode(\\"NJsonSchema\\", \\"10.5.2.0 (Newtonsoft.Json v13.0.0.0)\\")]\\npublic partial class Pet : NewPet\\n{\\n    [Newtonsoft.Json.JsonProperty(\\"id\\", Required = Newtonsoft.Json.Required.Always)]\\n    public decimal Id { get; set; }\\n\\n    [Newtonsoft.Json.JsonProperty(\\"@id\\", Required = Newtonsoft.Json.Required.Always)]\\n    public decimal __id { get; set; }\\n}\\n```\\n\\nIf we take all the code, and put it together, we end up with this:\\n\\n```cs\\nusing System;\\nusing System.IO;\\nusing System.Reflection;\\nusing System.Threading.Tasks;\\nusing NJsonSchema;\\nusing NJsonSchema.Visitors;\\nusing NSwag.CodeGeneration.CSharp;\\n\\nnamespace NSwag {\\n    class Program {\\n        static async Task Main(string[] args) {\\n            Console.WriteLine(\\"Generating client...\\");\\n            await ClientGenerator.GenerateCSharpClient();\\n            Console.WriteLine(\\"Generated client.\\");\\n        }\\n    }\\n\\n    public static class ClientGenerator {\\n\\n        public async static Task GenerateCSharpClient() =>\\n            GenerateClient(\\n                // https://github.com/OAI/OpenAPI-Specification/blob/main/examples/v2.0/json/petstore-simple.json\\n                document: await GetDocumentFromFile(\\"petstore-simple.json\\"),\\n                generatedLocation: \\"GeneratedClient.cs\\",\\n                generateCode: (OpenApiDocument document) => {\\n                    new DoubleToDecimalVisitor().Visit(document); // we want decimals not doubles\\n\\n                    var settings = new CSharpClientGeneratorSettings {\\n                        CSharpGeneratorSettings = {\\n                            PropertyNameGenerator = new HandleAtCSharpPropertyNameGenerator() // @ shouldn\'t cause us problems\\n                        }\\n                    };\\n\\n                    var generator = new CSharpClientGenerator(document, settings);\\n                    var code = generator.GenerateFile();\\n                    return code;\\n                }\\n            );\\n\\n        private static void GenerateClient(OpenApiDocument document, string generatedLocation, Func<OpenApiDocument, string> generateCode) {\\n            var root = Path.GetDirectoryName(Assembly.GetEntryAssembly().Location);\\n            var location = Path.GetFullPath(Path.Join(root, @\\"../../../\\", generatedLocation));\\n\\n            Console.WriteLine($\\"Generating {location}...\\");\\n\\n            var code = generateCode(document);\\n\\n            System.IO.File.WriteAllText(location, code);\\n        }\\n\\n        private static async Task<OpenApiDocument> GetDocumentFromFile(string swaggerJsonFilePath) {\\n            var root = Path.GetDirectoryName(Assembly.GetEntryAssembly().Location);\\n            var swaggerJson = await File.ReadAllTextAsync(Path.GetFullPath(Path.Join(root, swaggerJsonFilePath)));\\n            var document = await OpenApiDocument.FromJsonAsync(swaggerJson);\\n\\n            return document;\\n        }\\n    }\\n\\n    /// <summary>\\n    /// By default the C# decimal number type used is double; this makes it decimal\\n    /// </summary>\\n    public class DoubleToDecimalVisitor : JsonSchemaVisitorBase {\\n        protected override JsonSchema VisitSchema(JsonSchema schema, string path, string typeNameHint) {\\n            if (schema.Type == JsonObjectType.Number)\\n                schema.Format = JsonFormatStrings.Decimal;\\n\\n            return schema;\\n        }\\n    }\\n\\n    /// <summary>\\n    /// Replace characters which will not comply with C# syntax with something that will\\n    /// </summary>\\n    public class HandleAtCSharpPropertyNameGenerator : NJsonSchema.CodeGeneration.IPropertyNameGenerator {\\n        /// <summary>Generates the property name.</summary>\\n        /// <param name=\\"property\\">The property.</param>\\n        /// <returns>The new name.</returns>\\n        public virtual string Generate(JsonSchemaProperty property) =>\\n            ConversionUtilities.ConvertToUpperCamelCase(property.Name\\n                .Replace(\\"\\\\\\"\\", string.Empty)\\n                .Replace(\\"@\\", \\"__\\") // make \\"@\\" => \\"__\\", so \\"@type\\" => \\"__type\\"\\n                .Replace(\\"?\\", string.Empty)\\n                .Replace(\\"$\\", string.Empty)\\n                .Replace(\\"[\\", string.Empty)\\n                .Replace(\\"]\\", string.Empty)\\n                .Replace(\\"(\\", \\"_\\")\\n                .Replace(\\")\\", string.Empty)\\n                .Replace(\\".\\", \\"-\\")\\n                .Replace(\\"=\\", \\"-\\")\\n                .Replace(\\"+\\", \\"plus\\"), true)\\n                .Replace(\\"*\\", \\"Star\\")\\n                .Replace(\\":\\", \\"_\\")\\n                .Replace(\\"-\\", \\"_\\")\\n                .Replace(\\"#\\", \\"_\\");\\n    }\\n}\\n```\\n\\n## Conclusion\\n\\nThis post takes the tremendous NSwag, and demonstrates a mechanism for using it to create C# clients from an Open API / Swagger documents which:\\n\\n- can handle property names with an `@` prefix which might collide with the same property without the prefix\\n- use `decimal` as the preferred number type for floating point numbers"},{"id":"/2021/10/18/docusaurus-meta-tags-and-google-discover","metadata":{"permalink":"/2021/10/18/docusaurus-meta-tags-and-google-discover","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-10-18-docusaurus-meta-tags-and-google-discover/index.md","source":"@site/blog/2021-10-18-docusaurus-meta-tags-and-google-discover/index.md","title":"Docusaurus, meta tags and Google Discover","description":"Google Discover is a way that people can find your content. To make your content more attractive, Google encourage using high quality images which are enabled by setting the max-image-preview:large meta tag. This post shows you how to achieve that with Docusaurus.","date":"2021-10-18T00:00:00.000Z","formattedDate":"October 18, 2021","tags":[{"label":"Docusaurus","permalink":"/tags/docusaurus"},{"label":"meta tags","permalink":"/tags/meta-tags"},{"label":"max-image-preview","permalink":"/tags/max-image-preview"},{"label":"Google Discover","permalink":"/tags/google-discover"}],"readingTime":2.67,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Docusaurus, meta tags and Google Discover","authors":"johnnyreilly","tags":["Docusaurus","meta tags","max-image-preview","Google Discover"],"image":"./title-image.png","hide_table_of_contents":false},"prevItem":{"title":"NSwag generated C# client: Open API property name clashes and decimal types rather than double","permalink":"/2021/10/31/nswag-generated-c-sharp-client-property-name-clash"},"nextItem":{"title":"Structured data, SEO and React","permalink":"/2021/10/15/structured-data-seo-and-react"}},"content":"Google Discover is a way that people can find your content. To make your content more attractive, Google encourage using high quality images which are enabled by setting the `max-image-preview:large` meta tag. This post shows you how to achieve that with Docusaurus.\\n\\n![title image reading \\"Docusaurus, meta tags and Google Discover\\" with a Docusaurus logo and the Google Discover phone photo taken from https://developers.google.com/search/docs/advanced/mobile/google-discover](title-image.png)\\n\\n## Google Discover\\n\\nI\'m an Android user. Google Discover will present articles to me in various places on my phone. [According to the docs](https://developers.google.com/search/docs/advanced/mobile/google-discover):\\n\\n> With Discover, you can get updates for your interests, like your favorite sports team or news site, without searching for them. You can choose the types of updates you want to see in Discover in the Google app or when you\u2019re browsing the web on your phone.\\n\\nIt turns out that my own content is showing up in Discover. I (ahem) discovered this by looking at the Google search console and noticing a \\"Discover\\" tab:\\n\\n![screenshot of the Google search console featuring a \\"discover\\" image](screenshot-of-discover-in-search-console.png)\\n\\nAs I read up about Discover I noticed this:\\n\\n> To increase the likelihood of your content appearing in Discover, we recommend the following:\\n> ...\\n>\\n> - Include compelling, high-quality images in your content, especially large images that are more likely to generate visits from Discover. Large images need to be at least 1200 px wide and enabled by the `max-image-preview:large` setting...\\n\\nI was already trying to include images with my blog posts as described... But `max-image-preview:large` was news to me. [Reading up further](https://developers.google.com/search/docs/advanced/robots/robots_meta_tag#max-image-preview) revealed that the \\"setting\\" was simply a meta tag to be added to the HTML that looked like this:\\n\\n```html\\n<meta name=\\"robots\\" content=\\"max-image-preview:standard\\" />\\n```\\n\\nIncidentally, applying this setting will affect all forms of search results. So not just Discover, but Google web search, Google Images and Assistant as well. The result of having this meta tag will be that bigger images are displayed in search results, which should make the content more attractive.\\n\\n## Docusaurus let\'s get meta\\n\\nNow we understand what we want (an extra meta tag on all our pages), how do we apply this to Docusaurus?\\n\\nWell, it\'s remarkably simple. There\'s an optional [`metadata`](https://docusaurus.io/docs/api/themes/configuration#metadata) property in `docusaurus.config.js`. This property allows you to configure additional html metadata (and override existing ones). The property is an array of `Metadata`, each entry of which will be directly passed to the `<meta />` tag.\\n\\nSo in our case we\'d want to pass an object with `name: \'robots\'` and `content: \'max-image-preview:large\'` to render our desired meta tag. Which looks like this:\\n\\n```js\\n/** @type {import(\'@docusaurus/types\').DocusaurusConfig} */\\nmodule.exports = {\\n  //...\\n  themeConfig: {\\n    // <meta name=\\"robots\\" content=\\"max-image-preview:large\\">\\n    metadata: [{ name: \'robots\', content: \'max-image-preview:large\' }],\\n    //...\\n  },\\n  //...\\n};\\n```\\n\\nWith that in place, we find our expected `meta` tag is now part of our rendered HTML:\\n\\n![screenshot of the <meta name=\\"robots\\" content=\\"max-image-preview:large\\"> tag taken from Chrome Devtools](screenshot-of-meta-tag.png)\\n\\n## Meta meta\\n\\nWe should now have a more Google Discover-friendly website which is tremendous!\\n\\nBefore signing off, here\'s a fun fact: the PR that published this blog post is the _same_ PR that added `max-image-preview:standard` to my blog. [Peep it here](https://github.com/johnnyreilly/blog.johnnyreilly.com/pull/114) - meta in so many ways \ud83d\ude09"},{"id":"/2021/10/15/structured-data-seo-and-react","metadata":{"permalink":"/2021/10/15/structured-data-seo-and-react","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-10-15-structured-data-seo-and-react/index.md","source":"@site/blog/2021-10-15-structured-data-seo-and-react/index.md","title":"Structured data, SEO and React","description":"People being able to discover your website when they search is important. This post is about how you can add structured data to a site. Adding structured data will help search engines like Google understand your content, and get it in front of more eyeballs. We\'ll illustrate this by making a simple React app which incorporates structured data.","date":"2021-10-15T00:00:00.000Z","formattedDate":"October 15, 2021","tags":[{"label":"structured data","permalink":"/tags/structured-data"},{"label":"SEO","permalink":"/tags/seo"},{"label":"React","permalink":"/tags/react"}],"readingTime":5.95,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Structured data, SEO and React","authors":"johnnyreilly","tags":["structured data","SEO","React"],"image":"./structured-data-seo-and-react.png","hide_table_of_contents":false},"prevItem":{"title":"Docusaurus, meta tags and Google Discover","permalink":"/2021/10/18/docusaurus-meta-tags-and-google-discover"},"nextItem":{"title":"Permissioning Azure Pipelines with Bicep and Azure RBAC Role Assignments","permalink":"/2021/09/12/permissioning-azure-pipelines-bicep-role-assignments"}},"content":"People being able to discover your website when they search is important. This post is about how you can add structured data to a site. Adding structured data will help search engines like Google understand your content, and get it in front of more eyeballs. We\'ll illustrate this by making a simple React app which incorporates structured data.\\n\\n![title image reading \\"Structured data, SEO and React\\" with a screenshot of the rich results tool in the background](structured-data-seo-and-react.png)\\n\\n## Updated 28th October 2021\\n\\nThis blog evolved to become a talk:\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/zi1CHB-eVck?start=282\\" title=\\"YouTube video player\\" frameBorder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowFullScreen></iframe>\\n\\n## What is structured data?\\n\\nGoogle, DuckDuckGo and others are proficient at understanding the content of websites. However, scraping HTML is not a highly reliable way to categorise content. HTML is about presentation and it can have all manner of different structures. To make the life of search engines easier, there\'s a standardized format known as \\"structured data\\" which can be embedded within a page. That standardized format allows you to explicitly declare the type of content the page contains.\\n\\nSo let\'s say you\'ve written an article, you can reliably state in a language that Google understands \\"this page is an article, it has this title, this description and image and was published on this date\\". There are hundreds of types of structured data available, and you can read about all of them in depth at https://schema.org/ which is maintained by representatives of the search engine community.\\n\\nIt\'s worth knowing that whilst there are many types of structured data available to choose from, there are definitely more popular options and those that are more niche. So [Article](https://schema.org/Article) is likely to be used a great deal more than, perhaps, [MolecularEntity](https://schema.org/MolecularEntity).\\n\\nAs well as there being different types of structured data, there also a variety of formats which can be used to provide it; these include [JSON-LD](http://json-ld.org/), [Microdata](https://www.w3.org/TR/microdata/) and [RDFa](https://rdfa.info/). Google explicitly prefer JSON-LD and so that\'s what we\'ll focus on. JSON-LD is effectively a rending of a piece of JSON inside a `script` tag with the custom type of `application/ld+json`. For example:\\n\\n```html\\n<script type=\\"application/ld+json\\">\\n  {\\n    \\"@context\\": \\"https://schema.org/\\",\\n    \\"@type\\": \\"Recipe\\",\\n    \\"name\\": \\"Chocolate Brownie\\",\\n    \\"author\\": {\\n      \\"@type\\": \\"Person\\",\\n      \\"name\\": \\"John Reilly\\"\\n    },\\n    \\"datePublished\\": \\"2014-09-01\\",\\n    \\"description\\": \\"The most amazing chocolate brownie recipe\\",\\n    \\"prepTime\\": \\"PT60M\\"\\n  }\\n<\/script>\\n```\\n\\n## Structured data in action\\n\\nWhilst structured data is helpful for search engines in general, it can also make a difference to the way your content is rendered _inside_ search results. For instance, let\'s search for \\"best brownie recipe\\" in Google and see what shows up:\\n\\n![screenshot of google search results for \\"best brownie recipe\\" including a rich text results set at the top of the list showing recipes from various sources](screenshot-of-rich-text-results.png)\\n\\nWhen you look at the screenshot above, you\'ll notice that at the top of the list (before the main search results) there\'s a carousel which shows various brownie recipe links, with dedicated pictures, titles and descriptions. Where did this come from? The answer, unsurprisingly, is structured data.\\n\\nIf we click on the first link, we\'re taken to the recipe in question. Looking at the HTML of that page we find a number of JSON-LD sections:\\n\\n![screenshot of JSON-LD sections in the BBC Good Food website](structured-data-in-action.png)\\n\\nIf we grab the content of one JSON-LD section and paste it into the devtools console, it becomes much easier to read:\\n\\n![screenshot of JSON-LD section transformed into a JavaScript Object Literal](single-structured-data-as-JSON.png)\\n\\nIf we look at the `@type` property we can see it\'s a `\\"Recipe\\"`. This means it\'s an example of the https://schema.org/Recipe schema. If we look further at the `headline` property, it reads `\\"Best ever chocolate brownies recipe\\"`. That matches up with headline that was displayed in the search results.\\n\\nNow we have a sense of what the various search engines are using as they categorise the page, and we understand exactly what is powering the carousel in the Google search results.\\n\\nIncidentally, there\'s a special name for this \\"carousel\\"; it is a \\"rich result\\". A rich result is a search result singled out for special treatment when it is displayed. Google provide a [Rich Results Test tool](https://search.google.com/test/rich-results) which allows you to validate if a site provides structured data which is eligible to be featured in rich results. We\'ll make use of this later.\\n\\n## Adding structured data to a website\\n\\nNow we\'ll make ourselves a React app and add structured data to it. In the console we\'ll execute the following command:\\n\\n```\\nnpx create-react-app my-app\\n```\\n\\nWe now have a simple React app which consists of a single page. Let\'s replace the content of the existing `App.js` file with this:\\n\\n```jsx\\n//@ts-check\\nimport \'./App.css\';\\n\\nfunction App() {\\n  // https://schema.org/Article\\n  const articleStructuredData = {\\n    \'@context\': \'https://schema.org\',\\n    \'@type\': \'Article\',\\n    headline: \'Structured data for you\',\\n    description: \'This is an article that demonstrates structured data.\',\\n    image: ./\'https://upload.wikimedia.org/wikipedia/commons/4/40/JSON-LD.svg\',\\n    datePublished: new Date(\'2021-09-04T09:25:01.340Z\').toISOString(),\\n    author: {\\n      \'@type\': \'Person\',\\n      name: \'John Reilly\',\\n      url: \'https://twitter.com/johnny_reilly\',\\n    },\\n  };\\n\\n  return (\\n    <div className=\\"App\\">\\n      <script type=\\"application/ld+json\\">\\n        {JSON.stringify(articleStructuredData)}\\n      <\/script>\\n\\n      <h1>{articleStructuredData.headline}</h1>\\n      <h3>\\n        by{\' \'}\\n        <a href={articleStructuredData.author.url}>\\n          {articleStructuredData.author.name}\\n        </a>{\' \'}\\n        on {articleStructuredData.datePublished}\\n      </h3>\\n\\n      <img\\n        style={{ width: \'5em\' }}\\n        alt=\\"https://json-ld.org/ - Website content released under a Creative Commons CC0 Public Domain Dedication except where an alternate is specified., CC0, via Wikimedia Commons\\"\\n        src={articleStructuredData.image}\\n      />\\n\\n      <p>{articleStructuredData.description}</p>\\n\\n      <p>Take a look at the source of this page and find the JSON-LD!</p>\\n    </div>\\n  );\\n}\\n\\nexport default App;\\n```\\n\\nIf we look at the code above, we can see we\'re creating a JavaScript object literal named `articleStructuredData` which contains the data of an https://schema.org/Article. `articleStructuredData` is then used to do two things:\\n\\n1. to contribute to the content of the page\\n2. to render a JSON-LD script tag: `<script type=\\"application/ld+json\\">` which is populated by calling `JSON.stringify(articleStructuredData)`\\n\\nWhen we run our site locally with `npm start` we see a simple article site that looks like this:\\n\\n![screenshot of article page](screenshot-of-article.png)\\n\\nNow let\'s see if it supports structured data in the way we hope.\\n\\n## Using the Rich Results Test\\n\\nIf we go to https://search.google.com/test/rich-results we find the Rich Results Test tool. There\'s two ways you can test; providing a URL or providing code. In our case we don\'t have a public facing URL and so we\'re going to use the HTML that React is rendering.\\n\\nIn devtools we\'ll use the \\"copy outerHTML\\" feature to grab the HTML, then we\'ll paste it into Rich Results:\\n\\n![screenshot of rich results tool in code view](screenshot-of-rich-results-tool.png)\\n\\nWe hit the \\"TEST CODE\\" button and we see results that look like this:\\n\\n![screenshot of the results of testing our site using the rich results tool](screenshot-of-rich-results-tool-test.png)\\n\\nSo we\'ve been successful in building a website that renders structured data. More than that, we\'re doing it in a way that we know Google will recognise and can use to render rich results in search. That\'s a really useful way to drive traffic to our website.\\n\\nThis post has illustrated what it looks like to create an `Article`. Google has some [great resources](https://developers.google.com/search/docs/advanced/structured-data/search-gallery) on other types that it supports and prioritises for rich results which should help you build the structured data you need for your particular content.\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/react-structured-data-and-seo/)"},{"id":"/2021/09/12/permissioning-azure-pipelines-bicep-role-assignments","metadata":{"permalink":"/2021/09/12/permissioning-azure-pipelines-bicep-role-assignments","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-09-12-permissioning-azure-pipelines-bicep-role-assignments/index.md","source":"@site/blog/2021-09-12-permissioning-azure-pipelines-bicep-role-assignments/index.md","title":"Permissioning Azure Pipelines with Bicep and Azure RBAC Role Assignments","description":"How can we deploy resources to Azure, and then run an integration test through them in the context of an Azure Pipeline? This post will show how to do this by permissioning our Azure Pipeline to access these resources using Azure RBAC role assignments. It will also demonstrate a dotnet test that runs in the context of the pipeline and makes use of those role assignments.","date":"2021-09-12T00:00:00.000Z","formattedDate":"September 12, 2021","tags":[{"label":"Role Assignments","permalink":"/tags/role-assignments"},{"label":"Bicep","permalink":"/tags/bicep"},{"label":"Azure DevOps","permalink":"/tags/azure-dev-ops"},{"label":"Azure Pipelines","permalink":"/tags/azure-pipelines"}],"readingTime":8.715,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Permissioning Azure Pipelines with Bicep and Azure RBAC Role Assignments","authors":"johnnyreilly","tags":["Role Assignments","Bicep","Azure DevOps","Azure Pipelines"],"image":"./permissioning-azure-pipelines-with-bicep-and-role-assignments.png","hide_table_of_contents":false},"prevItem":{"title":"Structured data, SEO and React","permalink":"/2021/10/15/structured-data-seo-and-react"},"nextItem":{"title":"Google APIs: authentication with TypeScript","permalink":"/2021/09/10/google-apis-authentication-with-typescript"}},"content":"How can we deploy resources to Azure, and then run an integration test through them in the context of an Azure Pipeline? This post will show how to do this by permissioning our Azure Pipeline to access these resources using Azure RBAC role assignments. It will also demonstrate a dotnet test that runs in the context of the pipeline and makes use of those role assignments.\\n\\n![title image reading \\"Permissioning Azure Pipelines with Bicep and Role Assignments\\" and some Azure logos](permissioning-azure-pipelines-with-bicep-and-role-assignments.png)\\n\\nWe\'re following this approach as an alternative to [exporting connection strings](./2021-07-07-output-connection-strings-and-keys-from-azure-bicep/index.md), as these can be viewed in the Azure Portal; which may be an security issue if you have many people who are able to access the portal and view deployment outputs.\\n\\nWe\'re going to demonstrate this approach using Event Hubs. It\'s worth calling out that this is a generally useful approach which can be applied to any Azure resources that support Azure RBAC Role Assignments. So wherever in this post you read \\"Event Hubs\\", imagine substituting other Azure resources you\'re working with.\\n\\nThe post will do the following:\\n\\n- Add Event Hubs to our Azure subscription\\n- Permission our service connection / service principal\\n- Deploy to Azure with Bicep\\n- Write an integration test\\n- Write a pipeline to bring it all together\\n\\n## Add Event Hubs to your subscription\\n\\nFirst of all, we may need to add Event Hubs to our Azure subscription.\\n\\nWithout this in place, we may encounter errors of the type:\\n\\n> ##[error]MissingSubscriptionRegistration: The subscription is not registered to use namespace \'Microsoft.EventHub\'. See https://aka.ms/rps-not-found for how to register subscriptions.\\n\\nWe do this by going to \\"Resource Providers\\" in the [Azure Portal](https://portal.azure.com) and registering the resources you need. Lots are registered by default, but not all.\\n\\n![Screenshot of the Azure Portal, subscriptions -> resource providers section, showing that Event Hubs have been registered](screenshot-azure-portal-subscription-resource-providers.png)\\n\\n## Permission our service connection / service principal\\n\\nIn order that we can run pipelines related to Azure, we mostly need to have an Azure Resource Manager service connection set up in Azure DevOps. Once that exists, we also need to give it a role assignment to allow it to create role assignments of its own when pipelines are running.\\n\\nWithout this in place, we may encounter errors of the type:\\n\\n> ##[error]The template deployment failed with error: \'Authorization failed for template resource \'{GUID-THE-FIRST}\' of type \'Microsoft.Authorization/roleAssignments\'. The client \'{GUID-THE-SECOND}\' with object id \'{GUID-THE-SECOND}\' does not have permission to perform action \'Microsoft.Authorization/roleAssignments/write\' at scope \'/subscriptions/\\\\*\\\\*\\\\*/resourceGroups/johnnyreilly/providers/Microsoft.EventHub/namespaces/evhns-demo/providers/Microsoft.Authorization/roleAssignments/{GUID-THE-FIRST}\'.\'.\\n\\nEssentially, we want to be able to run pipelines that say \\"hey Azure, we want to give permissions to our service connection\\". We are doing this _with_ the self same service connection, so (chicken and egg) we first need to give it permission to give those commands in future. This is a little confusing; but let\'s role with it. (Pun most definitely intended. \ud83d\ude09)\\n\\nTo grant that permission / add that role assignment, we go to the service connection in Azure Devops:\\n\\n![Screenshot of the service connection in Azure DevOps](screenshot-azure-devops-service-connection.png)\\n\\nWe can see there\'s two links here; first we\'ll click on \\"Manage Service Principal\\", which will take us to the service principal in the Azure Portal:\\n\\n![Screenshot of the service principal in the Azure Portal](screenshot-azure-portal-service-principal.png)\\n\\nTake note of the display name of the service principal; we\'ll need that as we click on the \\"Manage service connection roles\\" link, which will take us to the resource groups IAM page in the Azure Portal:\\n\\n![Screenshot of the resource groups IAM page in the Azure Portal](screenshot-azure-portal-service-principal-access-control.png)\\n\\nHere we can click on \\"Add role assignment\\", select \\"Owner\\":\\n\\n![Screenshot of the add role assignment IAM page in the Azure Portal](screenshot-azure-portal-add-role-assignment.png)\\n\\nThen when selecting members we should be able to look up the service principal to assign it:\\n\\n![Screenshot of the add role assignment select member IAM page in the Azure Portal](screenshot-azure-portal-add-role-assignment-member.png)\\n\\nWe now have a service connection which we should be able to use for granting permissions / role assignments, which is what we need.\\n\\n## Event Hub and Role Assignment with Bicep\\n\\nNext we want a Bicep file that will, when run, provision an Event Hub and a role assignment which will allow our Azure Pipeline (via its service connection) to interact with it.\\n\\n```bicep\\n@description(\'Name of the eventhub namespace\')\\nparam eventHubNamespaceName string\\n\\n@description(\'Name of the eventhub name\')\\nparam eventHubName string\\n\\n@description(\'The service principal\')\\nparam principalId string\\n\\n// Create an Event Hub namespace\\nresource eventHubNamespace \'Microsoft.EventHub/namespaces@2021-01-01-preview\' = {\\n  name: eventHubNamespaceName\\n  location: resourceGroup().location\\n  sku: {\\n    name: \'Standard\'\\n    tier: \'Standard\'\\n    capacity: 1\\n  }\\n  properties: {\\n    zoneRedundant: true\\n  }\\n}\\n\\n// Create an Event Hub inside the namespace\\nresource eventHub \'Microsoft.EventHub/namespaces/eventhubs@2021-01-01-preview\' = {\\n  parent: eventHubNamespace\\n  name: eventHubName\\n  properties: {\\n    messageRetentionInDays: 7\\n    partitionCount: 1\\n  }\\n}\\n\\n// give Azure Pipelines Service Principal permissions against the Event Hub\\n\\nvar roleDefinitionAzureEventHubsDataOwner = subscriptionResourceId(\'Microsoft.Authorization/roleDefinitions\', \'f526a384-b230-433a-b45c-95f59c4a2dec\')\\n\\nresource integrationTestEventHubReceiverNamespaceRoleAssignment \'Microsoft.Authorization/roleAssignments@2018-01-01-preview\' = {\\n  name: guid(principalId, eventHub.id, roleDefinitionAzureEventHubsDataOwner)\\n  scope: eventHubNamespace\\n  properties: {\\n    roleDefinitionId: roleDefinitionAzureEventHubsDataOwner\\n    principalId: principalId\\n  }\\n}\\n```\\n\\nDo note that our bicep template takes the service principal id as a parameter. We\'re going to supply this later from our Azure Pipeline.\\n\\n## Our test\\n\\nWe\'re now going to write a dotnet integration test which will make use of the infrastructure deployed by our Bicep template. Let\'s create a new test project:\\n\\n```\\nmkdir src\\ncd src\\ndotnet new xunit -o IntegrationTests\\ncd IntegrationTests\\ndotnet add package Azure.Identity\\ndotnet add package Azure.Messaging.EventHubs\\ndotnet add package FluentAssertions\\ndotnet add package Microsoft.Extensions.Configuration.EnvironmentVariables\\n```\\n\\nWe\'ll create a test file called `EventHubTest.cs` with these contents:\\n\\n```cs\\nusing System;\\nusing System.Collections.Generic;\\nusing System.Linq;\\nusing System.Text;\\nusing System.Threading;\\nusing System.Threading.Tasks;\\nusing Azure.Identity;\\nusing Azure.Messaging.EventHubs;\\nusing Azure.Messaging.EventHubs.Consumer;\\nusing Azure.Messaging.EventHubs.Producer;\\nusing FluentAssertions;\\nusing Microsoft.Extensions.Configuration;\\nusing Newtonsoft.Json;\\nusing Xunit;\\nusing Xunit.Abstractions;\\n\\nnamespace IntegrationTests\\n{\\n    public record EchoMessage(string Id, string Message, DateTime Timestamp);\\n\\n    public class EventHubTest\\n    {\\n        private readonly ITestOutputHelper _output;\\n\\n        public EventHubTest(ITestOutputHelper output)\\n        {\\n            _output = output;\\n        }\\n\\n        [Fact]\\n        public async Task Can_post_message_to_event_hub_and_read_it_back()\\n        {\\n            // ARRANGE\\n            var configuration = new ConfigurationBuilder()\\n                .AddEnvironmentVariables()\\n                .Build();\\n\\n            // populated by variables specified in the Azure Pipeline\\n            var eventhubNamespaceName = configuration[\\"EVENTHUBNAMESPACENAME\\"];\\n            eventhubNamespaceName.Should().NotBeNull();\\n            var eventhubName = configuration[\\"EVENTHUBNAME\\"];\\n            eventhubName.Should().NotBeNull();\\n            var tenantId = configuration[\\"TENANTID\\"];\\n            tenantId.Should().NotBeNull();\\n\\n            // populated as a consequence of the addSpnToEnvironment in the azure-pipelines.yml\\n            var servicePrincipalId = configuration[\\"SERVICEPRINCIPALID\\"];\\n            servicePrincipalId.Should().NotBeNull();\\n            var servicePrincipalKey = configuration[\\"SERVICEPRINCIPALKEY\\"];\\n            servicePrincipalKey.Should().NotBeNull();\\n\\n            var fullyQualifiedNamespace = $\\"{eventhubNamespaceName}.servicebus.windows.net\\";\\n\\n            var clientCredential = new ClientSecretCredential(tenantId, servicePrincipalId, servicePrincipalKey);\\n            var eventHubClient = new EventHubProducerClient(\\n                fullyQualifiedNamespace: fullyQualifiedNamespace,\\n                eventHubName: eventhubName,\\n                credential: clientCredential\\n            );\\n            var ourGuid = Guid.NewGuid().ToString();\\n            var now = DateTime.UtcNow;\\n            var sentEchoMessage = new EchoMessage(Id: ourGuid, Message: $\\"Test message\\", Timestamp: now);\\n            var sentEventData = new EventData(\\n                Encoding.UTF8.GetBytes(JsonConvert.SerializeObject(sentEchoMessage))\\n            );\\n\\n            // ACT\\n            await eventHubClient.SendAsync(new List<EventData> { sentEventData }, CancellationToken.None);\\n\\n            var eventHubConsumerClient = new EventHubConsumerClient(\\n                consumerGroup: EventHubConsumerClient.DefaultConsumerGroupName,\\n                fullyQualifiedNamespace: fullyQualifiedNamespace,\\n                eventHubName: eventhubName,\\n                credential: clientCredential\\n            );\\n\\n            List<PartitionEvent> partitionEvents = new();\\n            await foreach (var partitionEvent in eventHubConsumerClient.ReadEventsAsync(new ReadEventOptions\\n            {\\n                MaximumWaitTime = TimeSpan.FromSeconds(10)\\n            }))\\n            {\\n                if (partitionEvent.Data == null) break;\\n                _output.WriteLine(Encoding.UTF8.GetString(partitionEvent.Data.EventBody.ToArray()));\\n                partitionEvents.Add(partitionEvent);\\n            }\\n\\n            // ASSERT\\n            partitionEvents.Count.Should().BeGreaterOrEqualTo(1);\\n            var firstOne = partitionEvents.FirstOrDefault(evnt =>\\n              ExtractTypeFromEventBody<EchoMessage>(evnt, _output)?.Id == ourGuid\\n            );\\n            var receivedEchoMessage = ExtractTypeFromEventBody<EchoMessage>(firstOne, _output);\\n            receivedEchoMessage.Should().BeEquivalentTo(sentEchoMessage, because: \\"the event body should be the same one posted to the message queue\\");\\n        }\\n\\n        private static T ExtractTypeFromEventBody<T>(PartitionEvent evnt, ITestOutputHelper _output)\\n        {\\n            try\\n            {\\n                return JsonConvert.DeserializeObject<T>(Encoding.UTF8.GetString(evnt.Data.EventBody.ToArray()));\\n            }\\n            catch (JsonException)\\n            {\\n                _output.WriteLine(\\"[\\" + Encoding.UTF8.GetString(evnt.Data.EventBody.ToArray()) + \\"] is probably not JSON\\");\\n                return default(T);\\n            }\\n        }\\n    }\\n}\\n```\\n\\nLet\'s talk through what happens in the test above:\\n\\n1. We read in Event Hub connection configuration for the test from environment variables. (These will be supplied by an Azure Pipeline that we will create shortly.)\\n2. We post a message to the Event Hub.\\n3. We read a message back from the Event Hub.\\n4. We confirm that the message we read back matches the one we posted.\\n\\nNow that we have our test, we want to be able to execute it. For that we need an Azure Pipeline!\\n\\n## Azure Pipeline\\n\\nWe\'re going to add an `azure-pipelines.yml` file which Azure DevOps can use to power a pipeline:\\n\\n```yml\\nvariables:\\n  - name: eventHubNamespaceName\\n    value: evhns-demo\\n  - name: eventHubName\\n    value: evh-demo\\n\\npool:\\n  vmImage: ubuntu-latest\\n\\nsteps:\\n  - task: AzureCLI@2\\n    displayName: Get Service Principal Id\\n    inputs:\\n      azureSubscription: $(serviceConnection)\\n      scriptType: bash\\n      scriptLocation: inlineScript\\n      addSpnToEnvironment: true\\n      inlineScript: |\\n        PRINCIPAL_ID=$(az ad sp show --id $servicePrincipalId --query objectId -o tsv)\\n        echo \\"##vso[task.setvariable variable=PIPELINE_PRINCIPAL_ID;]$PRINCIPAL_ID\\"\\n\\n  - bash: az bicep build --file infra/main.bicep\\n    displayName: \'Compile Bicep to ARM\'\\n\\n  - task: AzureResourceManagerTemplateDeployment@3\\n    name: DeployEventHubInfra\\n    displayName: Deploy Event Hub infra\\n    inputs:\\n      deploymentScope: Resource Group\\n      azureResourceManagerConnection: $(serviceConnection)\\n      subscriptionId: $(subscriptionId)\\n      action: Create Or Update Resource Group\\n      resourceGroupName: $(azureResourceGroup)\\n      location: $(location)\\n      templateLocation: Linked artifact\\n      csmFile: \'infra/main.json\' # created by bash script\\n      overrideParameters: >-\\n        -eventHubNamespaceName $(eventHubNamespaceName)\\n        -eventHubName $(eventHubName)\\n        -principalId $(PIPELINE_PRINCIPAL_ID)\\n      deploymentMode: Incremental\\n\\n  - task: UseDotNet@2\\n    displayName: \'Install .NET SDK 5.0.x\'\\n    inputs:\\n      packageType: \'sdk\'\\n      version: 5.0.x\\n\\n  - task: AzureCLI@2\\n    displayName: dotnet integration test\\n    inputs:\\n      azureSubscription: $(serviceConnection)\\n      scriptType: pscore\\n      scriptLocation: inlineScript\\n      addSpnToEnvironment: true # allows access to service principal details in script\\n      inlineScript: |\\n        cd $(Build.SourcesDirectory)/src/IntegrationTests\\n        dotnet test\\n```\\n\\nWhen the pipeline is run, it does the following:\\n\\n1. Gets the service principal id from the service connection.\\n2. Compiles our Bicep into an ARM template\\n3. Deploys the compiled ARM template to Azure\\n4. Installs the dotnet SDK\\n5. Uses the [Azure CLI task](https://docs.microsoft.com/en-us/azure/devops/pipelines/tasks/deploy/azure-cli?view=azure-devops) which allows us to access service principal details in the pipeline to run our dotnet test.\\n\\nWe\'ll create a pipeline in Azure DevOps pointing to this file, and we\'ll also create the variables that it depends upon:\\n\\n- `azureResourceGroup` - the name of your resource group in Azure where the app will be deployed\\n- `location` - where your app is deployed, eg `northeurope`\\n- `serviceConnection` - the name of your AzureRM service connection in Azure DevOps\\n- `subscriptionId` - your Azure subscription id from the [Azure Portal](https://portal.azure.com)\\n- `tenantId` - the Azure tenant id from the [Azure Portal](https://portal.azure.com)\\n\\n## Running the pipeline\\n\\nNow we\'re ready to run our pipeline:\\n\\n![screenshot of pipeline running successfully](screenshot-azure-pipelines-tests-passing.png)\\n\\nHere we can see that the pipeline runs and the test passes. That means we\'ve successfully provisioned the Event Hub and permissioned our pipeline to be able to access it using Azure RBAC role assignments. We then wrote a test which used the pipeline credentials to interact with the Event Hub. To see the repo that demostrates this, [look here](https://dev.azure.com/johnnyreilly/blog-demos/_git/permissioning-azure-pipelines-bicep-role-assignments).\\n\\nJust to reiterate: we\'ve demonstrated this approach using Event Hubs. This is a generally useful approach which can be applied to any Azure resources that support Azure RBAC Role Assignments.\\n\\nThanks to [Jamie McCrindle](https://twitter.com/foldr) for helping out with permissioning the service connection / service principal. [His post on rotating `AZURE_CREDENTIALS` in GitHub with Terraform](https://foldr.uk/rotating-azure-credentials-in-github-with-terraform) provides useful background for those who would like to do similar permissioning using Terraform."},{"id":"/2021/09/10/google-apis-authentication-with-typescript","metadata":{"permalink":"/2021/09/10/google-apis-authentication-with-typescript","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-09-10-google-apis-authentication-with-typescript/index.md","source":"@site/blog/2021-09-10-google-apis-authentication-with-typescript/index.md","title":"Google APIs: authentication with TypeScript","description":"Google has a wealth of APIs which we can interact with. At the time of writing, there\'s more than two hundred available; including YouTube, Google Calendar and GMail (alongside many others). To integrate with these APIs, it\'s necessary to authenticate and then use that credential with the API. This post will take you through how to do just that using TypeScript. It will also demonstrate how to use one of those APIs: the Google Calendar API.","date":"2021-09-10T00:00:00.000Z","formattedDate":"September 10, 2021","tags":[{"label":"Google APIs","permalink":"/tags/google-ap-is"},{"label":"TypeScript","permalink":"/tags/type-script"}],"readingTime":8.705,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Google APIs: authentication with TypeScript","authors":"johnnyreilly","tags":["Google APIs","TypeScript"],"image":"./app-registration.png","hide_table_of_contents":false},"prevItem":{"title":"Permissioning Azure Pipelines with Bicep and Azure RBAC Role Assignments","permalink":"/2021/09/12/permissioning-azure-pipelines-bicep-role-assignments"},"nextItem":{"title":"Bicep: syntax highlighting with PrismJS (and Docusaurus)","permalink":"/2021/08/19/bicep-syntax-highlighting-with-prismjs"}},"content":"Google has a wealth of APIs which we can interact with. At the time of writing, there\'s more than two hundred available; including YouTube, Google Calendar and GMail (alongside many others). To integrate with these APIs, it\'s necessary to authenticate and then use that credential with the API. This post will take you through how to do just that using TypeScript. It will also demonstrate how to use one of those APIs: the Google Calendar API.\\n\\n## Creating an OAuth 2.0 Client ID on the Google Cloud Platform\\n\\nThe first thing we need to do is go to the [Google Cloud Platform to create a project](https://console.cloud.google.com/projectcreate). The name of the project doesn\'t matter particularly; although it can be helpful to name the project to align with the API you\'re intending to consume. That\'s what we\'ll do here as we plan to integrate with the Google Calendar API:\\n\\n![Screenshot of the Create Project screen in the Google Cloud Platform](google-cloud-platform-create-project.png)\\n\\nThe project is the container in which the OAuth 2.0 Client ID will be housed. Now we\'ve created the project, let\'s go to the [credentials screen](https://console.cloud.google.com/apis/credentials) and create an OAuth Client ID using the Create Credentials dropdown:\\n\\n![Screenshot of the Create Credentials dropdown in the Google Cloud Platform](create-credentials.png)\\n\\nYou\'ll likely have to create an OAuth consent screen before you can create the OAuth Client ID. Going through the journey of doing that feels a little daunting as many questions have to be answered. This is because the consent screen can be used for a variety of purposes beyond the API authentication we\'re looking at today.\\n\\nWhen challenged, you can generally accept the defaults and proceed. The user type you\'ll require will be \\"External\\":\\n\\n![Screenshot of the OAuth consent screen in the Google Cloud Platform](oauth-consent-screen.png)\\n\\nYou\'ll also be required to create an app registration - all that\'s really required here is a name (which can be anything) and your email address:\\n\\n![Screenshot of the OAuth consent screen in the Google Cloud Platform](app-registration.png)\\n\\nYou don\'t need to worry about scopes. You can either plan to publish the app, or alternately set yourself up to be a test user - you\'ll need to do one of these in order that you can authenticate with the app. Continuing to the end of the journey should provide you with the OAuth consent screen which you need in order that you may then create the OAuth Client ID.\\n\\nCreating the OAuth Client ID is slightly confusing as the \\"Application type\\" required is \\"TVs and Limited Input devices\\".\\n\\n![Screenshot of the create OAuth Client ID screen in the Google Cloud Platform](create-oauth-client-id-type.png)\\n\\nWe\'re using this type of application as we want to acquire a [refresh token](https://oauth.net/2/grant-types/refresh-token/) which we\'ll be able to use in future to aquire access tokens which will be used to access the Google APIs.\\n\\nOnce it\'s created, you\'ll be able to download the Client ID from the Google Cloud Platform:\\n\\n![Screenshot of the create OAuth Client ID screen in the Google Cloud Platform](oauth-client-id.png)\\n\\nWhen you download it, it should look something like this:\\n\\n```json\\n{\\n  \\"installed\\": {\\n    \\"client_id\\": \\"CLIENT_ID\\",\\n    \\"project_id\\": \\"PROJECT_ID\\",\\n    \\"auth_uri\\": \\"https://accounts.google.com/o/oauth2/auth\\",\\n    \\"token_uri\\": \\"https://oauth2.googleapis.com/token\\",\\n    \\"auth_provider_x509_cert_url\\": \\"https://www.googleapis.com/oauth2/v1/certs\\",\\n    \\"client_secret\\": \\"CLIENT_SECRET\\",\\n    \\"redirect_uris\\": [\\"urn:ietf:wg:oauth:2.0:oob\\", \\"http://localhost\\"]\\n  }\\n}\\n```\\n\\nYou\'ll need the `client_id`, `client_secret` and `redirect_uris` - but keep them in a safe place and don\'t commit `client_id` and `client_secret` to source control!\\n\\n## Acquiring a refresh token\\n\\nNow we\'ve got our `client_id` and `client_secret`, we\'re ready to write a simple node command line application which we can use to obtain a refresh token. This is actually a multi-stage process that will end up looking like this:\\n\\n- Provide the Google authentication provider with the `client_id` and `client_secret`, in return it will provide an authentication URL.\\n- Open the authentication URL in the browser and grant consent, the provider will hand over a code.\\n- Provide the Google authentication provider with the `client_id`, `client_secret` and the code, it will acquire and provide users with a refresh token.\\n\\nLet\'s start coding. We\'ll initialise a TypeScript Node project like so:\\n\\n```bash\\nmkdir src\\ncd src\\nnpm init -y\\nnpm install googleapis ts-node typescript yargs @types/yargs @types/node\\nnpx tsc --init\\n```\\n\\nWe\'ve added a number of dependencies that will allow us to write a TypeScript Node command line application. We\'ve also added a dependency to the [`googleapis`](https://www.npmjs.com/package/googleapis) package which describes itself as:\\n\\n> Node.js client library for using Google APIs. Support for authorization and authentication with OAuth 2.0, API Keys and JWT tokens is included.\\n\\nWe\'re going to make use of the OAuth 2.0 part. We\'ll start our journey by creating a file called `google-api-auth.ts`:\\n\\n```ts\\nimport { getArgs, makeOAuth2Client } from \'./shared\';\\n\\nasync function getToken() {\\n  const { clientId, clientSecret, code } = await getArgs();\\n  const oauth2Client = makeOAuth2Client({ clientId, clientSecret });\\n\\n  if (code) await getRefreshToken(code);\\n  else getAuthUrl();\\n\\n  async function getAuthUrl() {\\n    const url = oauth2Client.generateAuthUrl({\\n      // \'online\' (default) or \'offline\' (gets refresh_token)\\n      access_type: \'offline\',\\n\\n      // scopes are documented here: https://developers.google.com/identity/protocols/oauth2/scopes#calendar\\n      scope: [\\n        \'https://www.googleapis.com/auth/calendar\',\\n        \'https://www.googleapis.com/auth/calendar.events\',\\n      ],\\n    });\\n\\n    console.log(`Go to this URL to acquire a refresh token:\\\\n\\\\n${url}\\\\n`);\\n  }\\n\\n  async function getRefreshToken(code: string) {\\n    const token = await oauth2Client.getToken(code);\\n    console.log(token);\\n  }\\n}\\n\\ngetToken();\\n```\\n\\nAnd a common file named `shared.ts` which `google-api-auth.ts` imports and which we\'ll re-use later:\\n\\n```ts\\nimport { google } from \'googleapis\';\\nimport yargs from \'yargs/yargs\';\\nconst { hideBin } = require(\'yargs/helpers\');\\n\\nexport async function getArgs() {\\n  const argv = await Promise.resolve(yargs(hideBin(process.argv)).argv);\\n\\n  const clientId = argv[\'clientId\'] as string;\\n  const clientSecret = argv[\'clientSecret\'] as string;\\n\\n  const code = argv.code as string | undefined;\\n  const refreshToken = argv.refreshToken as string | undefined;\\n  const test = argv.test as boolean;\\n\\n  if (!clientId) throw new Error(\'No clientId \');\\n  console.log(\'We have a clientId\');\\n\\n  if (!clientSecret) throw new Error(\'No clientSecret\');\\n  console.log(\'We have a clientSecret\');\\n\\n  if (code) console.log(\'We have a code\');\\n  if (refreshToken) console.log(\'We have a refreshToken\');\\n\\n  return { code, clientId, clientSecret, refreshToken, test };\\n}\\n\\nexport function makeOAuth2Client({\\n  clientId,\\n  clientSecret,\\n}: {\\n  clientId: string;\\n  clientSecret: string;\\n}) {\\n  return new google.auth.OAuth2(\\n    /* YOUR_CLIENT_ID */ clientId,\\n    /* YOUR_CLIENT_SECRET */ clientSecret,\\n    /* YOUR_REDIRECT_URL */ \'urn:ietf:wg:oauth:2.0:oob\'\\n  );\\n}\\n```\\n\\nThe `getToken` function above does these things:\\n\\n1. If given a `client_id` and `client_secret` it will obtain an authentication URL.\\n2. If given a `client_id`, `client_secret` and `code` it will obtain a refresh token (scoped to access the Google Calendar API).\\n\\nWe\'ll add an entry to our `package.json` which will allow us to run our console app:\\n\\n```json\\n    \\"google-api-auth\\": \\"ts-node google-api-auth.ts\\"\\n```\\n\\nNow we\'re ready to acquire the refresh token. We\'ll run the following command (substituting in the appropriate values):\\n\\n`npm run google-api-auth -- --clientId CLIENT_ID --clientSecret CLIENT_SECRET`\\n\\nClick on the URL that is generated in the console, it should open up a consent screen in the browser which looks like this:\\n\\n![Screenshot of the consent screen](grant-consent.png)\\n\\nAuthenticate and grant consent and you should get a code:\\n\\n![Screenshot of the generated code](auth-code.png)\\n\\nThen (quickly) paste the acquired code into the following command:\\n\\n`npm run google-api-auth -- --clientId CLIENT_ID --clientSecret CLIENT_SECRET --code THISISTHECODE`\\n\\nThe `refresh_token` (alongside much else) will be printed to the console. Grab it and put it somewhere secure. Again, no storing in source control!\\n\\nIt\'s worth taking a moment to reflect on what we\'ve done. We\'ve acquired a refresh token which involved a certain amount of human interaction. We\'ve had to run a console command, do some work in a browser and run another commmand. You wouldn\'t want to do this repeatedly because it involves human interaction. Intentionally it cannot be automated. However, once you\'ve acquired the refresh token, you can use it repeatedly until it expires (which may be never or at least years in the future). So once you have the refresh token, and you\'ve stored it securely, you have what you need to be able to automate an API interaction.\\n\\n## Accessing the Google Calendar API\\n\\nLet\'s test out our refresh token by attempting to access the Google Calendar API. We\'ll create a `calendar.ts` file\\n\\n```ts\\nimport { google } from \'googleapis\';\\nimport { getArgs, makeOAuth2Client } from \'./shared\';\\n\\nasync function makeCalendarClient() {\\n  const { clientId, clientSecret, refreshToken } = await getArgs();\\n  const oauth2Client = makeOAuth2Client({ clientId, clientSecret });\\n  oauth2Client.setCredentials({\\n    refresh_token: refreshToken,\\n  });\\n\\n  const calendarClient = google.calendar({\\n    version: \'v3\',\\n    auth: oauth2Client,\\n  });\\n  return calendarClient;\\n}\\n\\nasync function getCalendar() {\\n  const calendarClient = await makeCalendarClient();\\n\\n  const { data: calendars, status } = await calendarClient.calendarList.list();\\n\\n  if (status === 200) {\\n    console.log(\'calendars\', calendars);\\n  } else {\\n    console.log(\'there was an issue...\', status);\\n  }\\n}\\n\\ngetCalendar();\\n```\\n\\nThe `getCalendar` function above uses the `client_id`, `client_secret` and `refresh_token` to access the Google Calendar API and retrieve the list of calendars.\\n\\nWe\'ll add an entry to our `package.json` which will allow us to run this function:\\n\\n```json\\n    \\"calendar\\": \\"ts-node calendar.ts\\",\\n```\\n\\nNow we\'re ready to test `calendar.ts`. We\'ll run the following command (substituting in the appropriate values):\\n\\n`npm run calendar -- --clientId CLIENT_ID --clientSecret CLIENT_SECRET --refreshToken REFRESH_TOKEN`\\n\\nWhen we run for the first time, we may encounter a self explanatory message which tells us that we need enable the calendar API for our application:\\n\\n```\\n(node:31563) UnhandledPromiseRejectionWarning: Error: Google Calendar API has not been used in project 77777777777777 before or it is disabled. Enable it by visiting https://console.developers.google.com/apis/api/calendar-json.googleapis.com/overview?project=77777777777777 then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry.\\n```\\n\\nOnce enabled, we can run successfully for the first time. Consequently we should see something like this showing up in the console:\\n\\n![Screenshot of calendars list response in the console](calendars-response.png)\\n\\nThis demonstrates that we\'re successfully integrating with a Google API using our refresh token.\\n\\n## Today the Google Calendar API, tomorrow the (Google API) world!\\n\\nWhat we\'ve demonstrated here is integrating with the Google Calendar API. However, that is not the limit of what we can do. As we discussed earlier, Google has more than two hundred APIs we can interact with, and the key to that interaction is following the same steps for authentication that this post outlines.\\n\\nLet\'s imagine that we want to integrate with the YouTube API or the GMail API. We\'d be able to follow the steps in this post, using different [scopes for the refresh token appropriate to the API](https://developers.google.com/identity/protocols/oauth2/scopes#calendar), and build an integration against that API. [Take a look at the available APIs](https://developers.google.com/apis-explorer) here.\\n\\nThe approach outlined by this post is the key to integrating with a multitude of Google APIs. Happy integrating!\\n\\nThe idea of this was sparked by [Martin Fowler\'s post](https://martinfowler.com/articles/command-line-google.html) on the topic which comes from a Ruby angle.\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/how-to-authenticate-access-google-apis-using-oauth-2-0/)"},{"id":"/2021/08/19/bicep-syntax-highlighting-with-prismjs","metadata":{"permalink":"/2021/08/19/bicep-syntax-highlighting-with-prismjs","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-08-19-bicep-syntax-highlighting-with-prismjs/index.md","source":"@site/blog/2021-08-19-bicep-syntax-highlighting-with-prismjs/index.md","title":"Bicep: syntax highlighting with PrismJS (and Docusaurus)","description":"Bicep is an amazing language, it\'s also very new. If you want to write attractive code snippets about Bicep, you can by using PrismJS (and Docusaurus). This post shows you how.","date":"2021-08-19T00:00:00.000Z","formattedDate":"August 19, 2021","tags":[{"label":"Bicep","permalink":"/tags/bicep"},{"label":"PrismJS","permalink":"/tags/prism-js"}],"readingTime":2.21,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Bicep: syntax highlighting with PrismJS (and Docusaurus)","authors":"johnnyreilly","tags":["Bicep","PrismJS"],"image":"./bicep-syntax-highlighting-with-prismjs.png","hide_table_of_contents":false},"prevItem":{"title":"Google APIs: authentication with TypeScript","permalink":"/2021/09/10/google-apis-authentication-with-typescript"},"nextItem":{"title":"Publish Azure Static Web Apps with Bicep and Azure DevOps","permalink":"/2021/08/15/bicep-azure-static-web-apps-azure-devops"}},"content":"Bicep is an amazing language, it\'s also very new. If you want to write attractive code snippets about Bicep, you can by using PrismJS (and Docusaurus). This post shows you how.\\n\\n![title image reading \\"Publish Azure Static Web Apps with Bicep and Azure DevOps\\" and some Azure logos](bicep-syntax-highlighting-with-prismjs.png)\\n\\n## Syntax highlighting\\n\\nI\'ve been writing blog posts about Bicep for a little while. I was frustrated that the code snippets were entirely unhighlighted. I\'m keen my posts are as readable as possible, and so I [looked into adding support to PrismJS](https://github.com/PrismJS/prism/pull/3027) which is what [Docusaurus](https://docusaurus.io/) uses to power syntax highlighting.\\n\\nWhilst my regex fu is amateur at best, happily [Michael Schmidt](https://github.com/RunDevelopment) of the PrismJS family is considerably better. He took the support I added and [made it much better](https://github.com/PrismJS/prism/pull/3028).\\n\\n## Docusaurus meet Bicep\\n\\nIf you have any code snippets that start with three backticks and the word `bicep`...\\n\\n````\\n```bicep\\n// code goes here...\\n````\\n\\n... then ideally you\'d like to see some syntax highlighting in your post. Since Bicep isn\'t \\"in the box\\" for Docusaurus you need to [explicitly opt into support like so:](https://docusaurus.io/docs/next/markdown-features/code-blocks#supported-languages)\\n\\n```js\\n    prism: {\\n      additionalLanguages: [\\"powershell\\", \\"csharp\\", \\"docker\\", \\"bicep\\"],\\n    },\\n```\\n\\nAbove you can see a snippet from my own [`docusaurus.config.js`](https://github.com/johnnyreilly/blog.johnnyreilly.com/blob/b2df93efb72adc32d9f45de4f727e890e59a4919/blog-website/docusaurus.config.js#L185) which adds Bicep, alongside the other additional languages I use.\\n\\nWith this in place, you would typically get all the syntax highlighting support you need.\\n\\n## Early adoption workaround\\n\\nI\'m writing this post before the latest version of PrismJS has shipped. As such, Bicep support isn\'t available by default yet. But if you\'re an early adopter, you can get support right now. The secret is adding a `resolutions` section to your `package.json` which points to the GitHub Repo [where Prism lives](https://github.com/PrismJS/prism):\\n\\n```json\\n  \\"resolutions\\": {\\n    \\"prismjs\\": \\"PrismJS/prism\\"\\n  },\\n```\\n\\nThis will mean that Yarn (if you\'re using Docusaurus you\'re probably using Yarn) pulls `prismjs` directly from GitHub, as demonstrated by the `yarn.lock` file:\\n\\n```\\nprismjs@PrismJS/prism, prismjs@^1.23.0:\\n  version \\"1.24.1\\"\\n  resolved \\"https://codeload.github.com/PrismJS/prism/tar.gz/59f449d33dc9fd19302f21aad95fc0b5028ac830\\"\\n```\\n\\n## What does it look like?\\n\\nFinally, let\'s see if works. Here\'s a Bicep code snippet that I borrowed from [an earlier post](/2021/08/19/bicep-syntax-highlighting-with-prismjs):\\n\\n```bicep\\nparam repositoryUrl string\\nparam repositoryBranch string\\n\\nparam location string = \'westeurope\'\\nparam skuName string = \'Free\'\\nparam skuTier string = \'Free\'\\n\\nparam appName string\\n\\nresource staticWebApp \'Microsoft.Web/staticSites@2020-12-01\' = {\\n  name: appName\\n  location: location\\n  tags: tagsObj\\n  sku: {\\n    name: skuName\\n    tier: skuTier\\n  }\\n  properties: {\\n    // The provider, repositoryUrl and branch fields are required for successive deployments to succeed\\n    // for more details see: https://github.com/Azure/static-web-apps/issues/516\\n    provider: \'DevOps\'\\n    repositoryUrl: repositoryUrl\\n    branch: repositoryBranch\\n    buildProperties: {\\n      skipGithubActionWorkflowGeneration: true\\n    }\\n  }\\n}\\n\\noutput deployment_token string = listSecrets(staticWebApp.id, staticWebApp.apiVersion).properties.apiKey\\n```\\n\\nAs you can see, it\'s delightfully highlighted by PrismJS. Enjoy!"},{"id":"/2021/08/15/bicep-azure-static-web-apps-azure-devops","metadata":{"permalink":"/2021/08/15/bicep-azure-static-web-apps-azure-devops","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-08-15-bicep-azure-static-web-apps-azure-devops/index.md","source":"@site/blog/2021-08-15-bicep-azure-static-web-apps-azure-devops/index.md","title":"Publish Azure Static Web Apps with Bicep and Azure DevOps","description":"This post demonstrates how to deploy Azure Static Web Apps using Bicep and Azure DevOps. It includes a few workarounds for the \\"Provider is invalid. Cannot change the Provider. Please detach your static site first if you wish to use to another deployment provider.\\" issue.","date":"2021-08-15T00:00:00.000Z","formattedDate":"August 15, 2021","tags":[{"label":"Azure Static Web App","permalink":"/tags/azure-static-web-app"},{"label":"Bicep","permalink":"/tags/bicep"},{"label":"Azure DevOps","permalink":"/tags/azure-dev-ops"},{"label":"Azure Pipelines","permalink":"/tags/azure-pipelines"}],"readingTime":4.39,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Publish Azure Static Web Apps with Bicep and Azure DevOps","authors":"johnnyreilly","tags":["Azure Static Web App","Bicep","Azure DevOps","Azure Pipelines"],"image":"./title-image.png","hide_table_of_contents":false},"prevItem":{"title":"Bicep: syntax highlighting with PrismJS (and Docusaurus)","permalink":"/2021/08/19/bicep-syntax-highlighting-with-prismjs"},"nextItem":{"title":"TypeScript 4.4 and more readable code","permalink":"/2021/08/14/typescript-4-4-more-readable-code"}},"content":"This post demonstrates how to deploy [Azure Static Web Apps](https://docs.microsoft.com/en-us/azure/static-web-apps/overview) using Bicep and Azure DevOps. It includes a few workarounds for the [\\"Provider is invalid. Cannot change the Provider. Please detach your static site first if you wish to use to another deployment provider.\\" issue](https://github.com/Azure/static-web-apps/issues/516).\\n\\n![title image reading \\"Publish Azure Static Web Apps with Bicep and Azure DevOps\\" and some Azure logos](title-image.png)\\n\\n## Bicep template\\n\\nThe first thing we\'re going to do is create a folder where our Bicep file for deploying our Azure Static Web App will live:\\n\\n```bash\\nmkdir infra/static-web-app -p\\n```\\n\\nThen we\'ll create a `main.bicep` file:\\n\\n```bicep\\nparam repositoryUrl string\\nparam repositoryBranch string\\n\\nparam location string = \'westeurope\'\\nparam skuName string = \'Free\'\\nparam skuTier string = \'Free\'\\n\\nparam appName string\\n\\nresource staticWebApp \'Microsoft.Web/staticSites@2020-12-01\' = {\\n  name: appName\\n  location: location\\n  sku: {\\n    name: skuName\\n    tier: skuTier\\n  }\\n  properties: {\\n    // The provider, repositoryUrl and branch fields are required for successive deployments to succeed\\n    // for more details see: https://github.com/Azure/static-web-apps/issues/516\\n    provider: \'DevOps\'\\n    repositoryUrl: repositoryUrl\\n    branch: repositoryBranch\\n    buildProperties: {\\n      skipGithubActionWorkflowGeneration: true\\n    }\\n  }\\n}\\n\\noutput deployment_token string = listSecrets(staticWebApp.id, staticWebApp.apiVersion).properties.apiKey\\n```\\n\\nThere\'s some things to draw attention to in the code above:\\n\\n1. The `provider`, `repositoryUrl` and `branch` fields are required for successive deployments to succeed. In our case we\'re deploying via Azure DevOps and so our provider is `\'DevOps\'`. For more details, [look at this issue](https://github.com/Azure/static-web-apps/issues/516).\\n2. We\'re creating a `deployment_token` which we\'ll need in order that we can deploy into the Azure Static Web App resource.\\n\\n## Static Web App\\n\\nIn order that we can test out Azure Static Web Apps, what we need is a static web app. You could use pretty much anything here; we\'re going to use Docusaurus. We\'ll execute this single command:\\n\\n```bash\\nnpx @docusaurus/init@latest init static-web-app classic\\n```\\n\\nWhich will scaffold a Docusaurus site in a folder named `static-web-app`. We don\'t need to change it any further; let\'s just see if we can deploy it.\\n\\n## Azure Pipeline\\n\\nWe\'re going to add an `azure-pipelines.yml` file which Azure DevOps can use to power a pipeline:\\n\\n```yml\\ntrigger:\\n  - main\\n\\npool:\\n  vmImage: ubuntu-latest\\n\\nsteps:\\n  - checkout: self\\n    submodules: true\\n\\n  - bash: az bicep build --file infra/static-web-app/main.bicep\\n    displayName: \'Compile Bicep to ARM\'\\n\\n  - task: AzureResourceManagerTemplateDeployment@3\\n    name: DeployStaticWebAppInfra\\n    displayName: Deploy Static Web App infra\\n    inputs:\\n      deploymentScope: Resource Group\\n      azureResourceManagerConnection: $(serviceConnection)\\n      subscriptionId: $(subscriptionId)\\n      action: Create Or Update Resource Group\\n      resourceGroupName: $(azureResourceGroup)\\n      location: $(location)\\n      templateLocation: Linked artifact\\n      csmFile: \'infra/static-web-app/main.json\' # created by bash script\\n      overrideParameters: >-\\n        -repositoryUrl $(repo)\\n        -repositoryBranch $(Build.SourceBranchName)\\n        -appName $(staticWebAppName)\\n      deploymentMode: Incremental\\n      deploymentOutputs: deploymentOutputs\\n\\n  - task: PowerShell@2\\n    name: \'SetDeploymentOutputVariables\'\\n    displayName: \'Set Deployment Output Variables\'\\n    inputs:\\n      targetType: inline\\n      script: |\\n        $armOutputObj = \'$(deploymentOutputs)\' | ConvertFrom-Json\\n        $armOutputObj.PSObject.Properties | ForEach-Object {\\n          $keyname = $_.Name\\n          $value = $_.Value.value\\n\\n          # Creates a standard pipeline variable\\n          Write-Output \\"##vso[task.setvariable variable=$keyName;issecret=true]$value\\"\\n\\n          # Display keys in pipeline\\n          Write-Output \\"output variable: $keyName\\"\\n        }\\n      pwsh: true\\n\\n  - task: AzureStaticWebApp@0\\n    name: DeployStaticWebApp\\n    displayName: Deploy Static Web App\\n    inputs:\\n      app_location: \'static-web-app\'\\n      # api_location: \'api\' # we don\'t have an API\\n      output_location: \'build\'\\n      azure_static_web_apps_api_token: $(deployment_token) # captured from deploymentOutputs\\n```\\n\\nWhen the pipeline is run, it does the following:\\n\\n1. Compiles our Bicep into an ARM template\\n2. Deploys the compiled ARM template to Azure\\n3. Captures the deployment outputs (essentially the `deployment_token`) and converts them into variables to use in the pipeline\\n4. Deploys our Static Web App using the `deployment_token`\\n\\nThe pipeline depends upon a number of variables:\\n\\n- `azureResourceGroup` - the name of your resource group in Azure where the app will be deployed\\n- `location` - where your app is deployed, eg `northeurope`\\n- `repo` - the URL of your repository in Azure DevOps, eg https://dev.azure.com/johnnyreilly/_git/azure-static-web-apps\\n- `serviceConnection` - the name of your AzureRM service connection in Azure DevOps\\n- `staticWebAppName` - the name of your static web app, eg `azure-static-web-apps-johnnyreilly`\\n- `subscriptionId` - your Azure subscription id from the [Azure Portal](https://portal.azure.com)\\n\\nA successful pipeline looks something like this:\\n\\n![Screenshot of successfully running Azure Pipeline](successful-azure-pipelines-run-screenshot.png)\\n\\nWhat you might notice is that the `AzureStaticWebApp` is itself installing and building our application. This is handled by [Microsoft Oryx](https://github.com/Microsoft/Oryx). The upshot of this is that we don\'t need to manually run `npm install` and `npm build` ourselves; the `AzureStaticWebApp` task will take care of it for us.\\n\\nFinally, let\'s see if we\'ve deployed something successfully...\\n\\n![Screenshot of deployed Azure Static Web App](deployed-azure-static-web-app-screenshot.png)\\n\\nWe have! It\'s worth noting that you\'ll likely want to give your Azure Static Web App a lovelier URL, and perhaps even put it behind Azure Front Door as well.\\n\\n## `Provider is invalid` workaround 2\\n\\n[Shane Neff](https://www.linkedin.com/in/shaneneff/) was attempting to follow the instructions in this post and encountered issues. He shared his struggles with me as he encountered the [\\"Provider is invalid. Cannot change the Provider. Please detach your static site first if you wish to use to another deployment provider.\\" issue](https://github.com/Azure/static-web-apps/issues/516).\\n\\nHe was good enough to share his solution as well, which is inserting this task at the start of the pipeline (before the `az bicep build` step):\\n\\n```yml\\n- task: AzureCLI@2\\n  inputs:\\n    azureSubscription: \'<name of your service connection>\'\\n    scriptType: \'bash\'\\n    scriptLocation: \'inlineScript\'\\n    inlineScript: \'az staticwebapp disconnect -n <name of your app>\'\\n```\\n\\nI haven\'t had the problems that Shane has had myself, but I wanted to share his fix for the people out there who almost certainly are bumping on this."},{"id":"/2021/08/14/typescript-4-4-more-readable-code","metadata":{"permalink":"/2021/08/14/typescript-4-4-more-readable-code","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-08-14-typescript-4-4-more-readable-code/index.md","source":"@site/blog/2021-08-14-typescript-4-4-more-readable-code/index.md","title":"TypeScript 4.4 and more readable code","description":"An exciting feature is shipping with TypeScript 4.4. It has the name \\"Control Flow Analysis of Aliased Conditions\\" which is quite a mouthful. This post unpacks what this feature is, and demonstrates the contribution it makes to improving the readability of code.","date":"2021-08-14T00:00:00.000Z","formattedDate":"August 14, 2021","tags":[{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"Control Flow Analysis of Aliased Conditions","permalink":"/tags/control-flow-analysis-of-aliased-conditions"}],"readingTime":4.065,"truncated":true,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"TypeScript 4.4 and more readable code","authors":"johnnyreilly","tags":["TypeScript","Control Flow Analysis of Aliased Conditions"],"image":"./reactions-on-github.png","hide_table_of_contents":false},"prevItem":{"title":"Publish Azure Static Web Apps with Bicep and Azure DevOps","permalink":"/2021/08/15/bicep-azure-static-web-apps-azure-devops"},"nextItem":{"title":"TypeScript, abstract classes, and constructors","permalink":"/2021/08/01/typescript-abstract-classes-and-constructors"}},"content":"An exciting feature is shipping with TypeScript 4.4. It has the name [\\"Control Flow Analysis of Aliased Conditions\\"](https://devblogs.microsoft.com/typescript/announcing-typescript-4-4-beta/#cfa-aliased-conditions) which is quite a mouthful. This post unpacks what this feature is, and demonstrates the contribution it makes to improving the readability of code.\\n\\n## Updated 30th September 2021\\n\\nThis blog evolved to become a talk:\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/LxZx3ycrxI0\\" title=\\"YouTube video player\\" frameBorder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowFullScreen></iframe>\\n\\n\x3c!--truncate--\x3e\\n\\n## Indirect type narrowing via `const`\\n\\nOn June 24th 2021, an issue on the TypeScript GitHub repository with the title \\"Indirect type narrowing via `const`\\" was closed by [Anders Hejlsberg](https://www.twitter.com/ahejlsberg). The issue had been open since 2016 and it was closed as it was covered by [a pull request addressing control flow analysis of aliased conditional expressions and discriminants](https://github.com/microsoft/TypeScript/pull/44730).\\n\\nIt\'s fair to say that the TypeScript community was very excited about this, both judging from reactions on the issue:\\n\\n[![Screenshot of reactions on GitHub](reactions-on-github.png)](https://github.com/microsoft/TypeScript/issues/12184#issuecomment-867928408)\\n\\nAnd also the general delight on Twitter:\\n\\n[![Screenshot of reactions on Twitter](reactions-on-twitter.png)](https://www.twitter.com/johnny_reilly/status/1408162514504933378)\\n\\nWhat Zeh said is a great explanation of the significance of this feature:\\n\\n> Lack of type narrowing with consts made me repeat code, or avoid helpfully namef consts, too many times\\n\\nWith this feature we\'re going to have the possibility of more readable code, and less repetition. That\'s amazing!\\n\\n## The code we would like to write\\n\\nRather than starting with an explanation of what this new language feature is, let\'s instead start from the position of writing some code and seeing what\'s possible with TypeScript 4.4 that we couldn\'t tackle previously.\\n\\nHere\'s a simple function that adds all the parameters it receives and returns the total. It\'s a tolerant function and will allow people to supply numbers in the form of strings as well; so it would successfully process `\'2\'` as it would `2`. This is, of course, a slightly contrived example, but should be useful for demonstrating the new feature.\\n\\n```ts\\nfunction add(...thingsToAdd: (string | number)[]): number {\\n  let total = 0;\\n  for (const thingToAdd of thingsToAdd) {\\n    if (typeof thingToAdd === \'string\') {\\n      total += Number(thingToAdd);\\n    } else {\\n      total += thingToAdd;\\n    }\\n  }\\n  return total;\\n}\\n\\nconsole.log(add(1, \'7\', \'3\', 9));\\n```\\n\\n[Try it out in the TypeScript playground.](https://www.typescriptlang.org/play?ts=4.3.5#code/GYVwdgxgLglg9mABAQwCaoBQDodQBYxgDmAzgCpwCC6AXIhiVAE6FGIA+iYIAtgEYBTJgEoA2gF1hdbvyGIA3gChEKxABsBURFDhRkaxAF5EABgDcy1cDhN6EBI20FiFaqkRxgT1uSrphCpaqqjBeGFAAngAOAp7eLn7uhsmIAOSMLMSpAUrBeao6egYA1MYAcryCTOHORK7+FvkqAL6IAmokAoFNeYX6iKXxdYmNTc1BiOPBTJogTEh9ahbjivZgJHAaWGpwRBhomACMADRpAOypp6kAzJeIAJzCwkA)\\n\\nIf we look at this function, whilst it works, it\'s not super expressive. The `typeof thingToAdd === \'string\'` performs two purposes:\\n\\n1. It narrows the type from `string | number` to `string`\\n2. It branches the logic, such that the `string` can be coerced into a `number` and added to the total.\\n\\nYou can infer this from reading the code. However, what if we were to re-write it to capture intent? Let\'s try creating a `shouldCoerceToNumber` constant which expresses the action we need to take:\\n\\n```ts\\nfunction add(...thingsToAdd: (string | number)[]): number {\\n  let total = 0;\\n  for (const thingToAdd of thingsToAdd) {\\n    const shouldCoerceToNumber = typeof thingToAdd === \'string\';\\n    if (shouldCoerceToNumber) {\\n      total += Number(thingToAdd);\\n    } else {\\n      total += thingToAdd;\\n    }\\n  }\\n  return total;\\n}\\n\\nconsole.log(add(1, \'7\', \'3\', 9));\\n```\\n\\n[Try it out in the TypeScript playground.](https://www.typescriptlang.org/play?ts=4.3.5#code/GYVwdgxgLglg9mABAQwCaoBQDodQBYxgDmAzgCpwCC6AXIhiVAE6FGIA+iYIAtgEYBTJgEoA2gF1hdbvyGIA3gChEKxABsBURFDhRkaxAF5EABgDcy1cDhN6EBI20FiFaqkRxgT1uSrphCpaqqvZgjiR4cCBqqADCcEIQAhQAcryCtsZQAJ4ADgKe3i5+7oZliADkjCzEFRbBwTBeDJHRcQlMSanpQgFKDQPauvqIANTGabJMGPisrv71gwC+iAJqJAKBgw06egbjRUTzqIsDS0GI58FMmiBMSLv6FueKoSRwGlhqcEQYaJgARgANJUAOwVEEVADMEMQAE5hMIgA)\\n\\nThis is valid code; however TypeScript 4.3 is choking with an error:\\n\\n![Screenshot of the TypeScript playground running TypeScript 4.3 and throwing an error on our new code](doesnt-work-in-typescript-4-3.png)\\n\\nThe error being surfaced is:\\n\\n> `Operator \'+=\' cannot be applied to types \'number\' and \'string | number\'.(2365)`\\n\\nWhat\'s happening here, is TypeScript _does not remember_ that `shouldCoerceToNumber` represents a type narrowing of `thingToAdd` from `string | number` to `string`. So the type of `thingToAdd` remains unchanged from `string | number` when we write code that depends upon it.\\n\\nThis has terrible consequences. It means we can\'t write this more expressive code that we\'re interested in, and would be better for maintainers of our codebase. And this is what TypeScript 4.4, with our new feature, unlocks. Let\'s change the playground to use TypeScript 4.4 instead:\\n\\n![Screenshot of the TypeScript playground running TypeScript 4.4 and working with our new code - it shows the `thingToAdd` variable has been narrowed to a `string`](does-work-in-typescript-4-4.png)\\n\\n[Try it out in the TypeScript playground.](https://www.typescriptlang.org/play?ts=4.4.0-beta#code/GYVwdgxgLglg9mABAQwCaoBQDodQBYxgDmAzgCpwCC6AXIhiVAE6FGIA+iYIAtgEYBTJgEoA2gF1hdbvyGIA3gChEKxABsBURFDhRkaxAF5EABgDcy1cDhN6EBI20FiFaqkRxgT1uSrphCpaqqvZgjiR4cCBqqADCcEIQAhQAcryCtsZQAJ4ADgKe3i5+7oZliADkjCzEFRbBwTBeDJHRcQlMSanpQgFKDQPauvqIANTGabJMGPisrv71gwC+iAJqJAKBgw06egbjRUTzqIsDS0GI58FMmiBMSLv6FueKoSRwGlhqcEQYaJgARgANJUAOwVEEVADMEMQAE5hMIgA)\\n\\nDelightfully, we no longer have errors now we\'ve made the switch. And as the screenshot shows, the `thingToAdd` variable has been narrowed to a `string`. This is because Control Flow Analysis of Aliased Conditions is now in play.\\n\\nSo we\'re now writing more expressive code, and TypeScript is willing us on our way.\\n\\n## Read more\\n\\nThis feature is a tremendous addition to the TypeScript language. It should have a significant long-term positive impact on how people write code with TypeScript.\\n\\nTo read more, do check out the excellent [TypeScript 4.4 beta release notes](https://devblogs.microsoft.com/typescript/announcing-typescript-4-4-beta/#cfa-aliased-conditions). There\'s also some other exciting feature shipping with this release as well. Thanks very much to the TypeScript team for once again improving the language, and making a real contribution to people being able to write readable code.\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/typescript-4-4-and-more-readable-code/)"},{"id":"/2021/08/01/typescript-abstract-classes-and-constructors","metadata":{"permalink":"/2021/08/01/typescript-abstract-classes-and-constructors","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-08-01-typescript-abstract-classes-and-constructors/index.md","source":"@site/blog/2021-08-01-typescript-abstract-classes-and-constructors/index.md","title":"TypeScript, abstract classes, and constructors","description":"TypeScript has the ability to define classes as abstract. This means they cannot be instantiated directly, only non-abstract subclasses can be. Let\'s take a look at what this means when it comes to constructor usage.","date":"2021-08-01T00:00:00.000Z","formattedDate":"August 1, 2021","tags":[{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"abstract","permalink":"/tags/abstract"},{"label":"constructors","permalink":"/tags/constructors"},{"label":"classes","permalink":"/tags/classes"}],"readingTime":4.665,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"TypeScript, abstract classes, and constructors","authors":"johnnyreilly","tags":["TypeScript","abstract","constructors","classes"],"image":"./vs-code-abstract-screenshot.png","hide_table_of_contents":false},"prevItem":{"title":"TypeScript 4.4 and more readable code","permalink":"/2021/08/14/typescript-4-4-more-readable-code"},"nextItem":{"title":"Directory.Build.props: C# 9 for all your projects","permalink":"/2021/07/14/directory-build-props-c-sharp-9-for-all"}},"content":"TypeScript has the ability to define classes as abstract. This means they cannot be instantiated directly, only non-abstract subclasses can be. Let\'s take a look at what this means when it comes to constructor usage.\\n\\n## Making a scratchpad\\n\\nIn order that we can dig into this, let\'s create ourselves a scratchpad project to work with. We\'re going to create a node project and install TypeScript as a dependency.\\n\\n```bash\\nmkdir ts-abstract-constructors\\ncd ts-abstract-constructors\\nnpm init --yes\\nnpm install typescript @types/node --save-dev\\n```\\n\\nWe now have a `package.json` file set up. We need to initialise a TypeScript project as well:\\n\\n```\\nnpx tsc --init\\n```\\n\\nThis will give us a `tsconfig.json` file that will drive configuration of TypeScript. By default TypeScript transpiles to an older version of JavaScript that predates classes. So we\'ll update the config to target a newer version of the language that does include them:\\n\\n```json\\n    \\"target\\": \\"es2020\\",\\n    \\"lib\\": [\\"es2020\\"],\\n```\\n\\nLet\'s create ourselves a TypeScript file called `index.ts`. The name is not significant; we just need a file to develop in.\\n\\nFinally we\'ll add a script to our `package.json` that compiles our TypeScript to JavaScript, and then runs the JS with node:\\n\\n```json\\n\\"start\\": \\"tsc --project \\\\\\".\\\\\\" && node index.js\\"\\n```\\n\\n## Making an abstract class\\n\\nNow we\'re ready. Let\'s add an abstract class with a constructor to our `index.ts` file:\\n\\n```ts\\nabstract class ViewModel {\\n  id: string;\\n\\n  constructor(id: string) {\\n    this.id = id;\\n  }\\n}\\n```\\n\\nConsider the `ViewModel` class above. Let\'s say we\'re building some kind of CRUD app, we\'ll have different views. Each of those views will have a corresponding viewmodel which is a subclass of the `ViewModel` abstract class. The `ViewModel` class has a mandatory `id` parameter in the constructor. This is to ensure that every viewmodel has an `id` value. If this were a real app, `id` would likely be the value with which an entity was looked up in some kind of database.\\n\\nImportantly, all subclasses of `ViewModel` should either:\\n\\n- not implement a constructor at all, leaving the base class constructor to become the default constructor of the subclass _or_\\n\\n- implement their own constructor which invokes the `ViewModel` base class constructor.\\n\\n## Taking our abstract class for a spin\\n\\nNow we have it, let\'s see what we can do with our abstract class. First of all, can we instantiate our abstract class? We shouldn\'t be able to do this:\\n\\n```\\nconst viewModel = new ViewModel(\'my-id\');\\n\\nconsole.log(`the id is: ${viewModel.id}`);\\n```\\n\\nAnd sure enough, running `npm start` results in the following error (which is also being reported by our editor; VS Code).\\n\\n```shell\\nindex.ts:9:19 - error TS2511: Cannot create an instance of an abstract class.\\n\\nconst viewModel = new ViewModel(\'my-id\');\\n```\\n\\n![Screenshot of \\"Cannot create an instance of an abstract class.\\" error in VS Code](vs-code-abstract-screenshot.png)\\n\\nTremendous. However, it\'s worth remembering that `abstract` is a TypeScript concept. When we compile our TS, although it\'s throwing a compilation error, it still transpiles an `index.js` file that looks like this:\\n\\n```js\\n\'use strict\';\\nclass ViewModel {\\n  constructor(id) {\\n    this.id = id;\\n  }\\n}\\nconst viewModel = new ViewModel(\'my-id\');\\nconsole.log(`the id is: ${viewModel.id}`);\\n```\\n\\nAs we can see, there\'s no mention of `abstract`; it\'s just a straightforward `class`. In fact, if we directly execute the file with `node index.js` we can see an output of:\\n\\n```\\nthe id is: my-id\\n```\\n\\nSo the transpiled code is valid JavaScript even if the source code isn\'t valid TypeScript. This all reminds us that `abstract` is a TypeScript construct.\\n\\n## Subclassing without a new constructor\\n\\nLet\'s now create our first subclass of `ViewModel` and attempt to instantiate it:\\n\\n```ts\\nclass NoNewConstructorViewModel extends ViewModel {}\\n\\n// error TS2554: Expected 1 arguments, but got 0.\\nconst viewModel1 = new NoNewConstructorViewModel();\\n\\nconst viewModel2 = new NoNewConstructorViewModel(\'my-id\');\\n```\\n\\n![Screenshot of \\"error TS2554: Expected 1 arguments, but got 0.\\" error in VS Code](vs-code-no-new-constructor.png)\\n\\nAs the TypeScript compiler tells us, the second of these instantiations is legitimate as it relies upon the constructor from the base class as we\'d hope. The first is not as there is no parameterless constructor.\\n\\n## Subclassing with a new constructor\\n\\nHaving done that, let\'s try subclassing and implementing a new constructor which has two parameters (to differentiate from the constructor we\'re overriding):\\n\\n```ts\\nclass NewConstructorViewModel extends ViewModel {\\n  data: string;\\n  constructor(id: string, data: string) {\\n    super(id);\\n    this.data = data;\\n  }\\n}\\n\\n// error TS2554: Expected 2 arguments, but got 0.\\nconst viewModel3 = new NewConstructorViewModel();\\n\\n// error TS2554: Expected 2 arguments, but got 1.\\nconst viewModel4 = new NewConstructorViewModel(\'my-id\');\\n\\nconst viewModel5 = new NewConstructorViewModel(\'my-id\', \'important info\');\\n```\\n\\n![Screenshot of \\"error TS2554: Expected 1 arguments, but got 1.\\" error in VS Code](vs-code-new-constructor.png)\\n\\nAgain, only one of the attempted instantiations is legitimate. `viewModel3` is not as there is no parameterless constructor. `viewModel4` is not as we have overridden the base class constructor with our new one that has two parameters. Hence `viewModel5` is our \\"Goldilocks\\" instantiation; it\'s just right!\\n\\nIt\'s also worth noting that we\'re calling `super` in the `NewConstructorViewModel` constructor. This invokes the constructor of the `ViewModel` base (or \\"super\\") class. TypeScript enforces that we pass the appropriate arguments (in our case a single `string`).\\n\\n## Wrapping it up\\n\\nWe\'ve seen that TypeScript ensures correct usage of constructors when we have an abstract class. Importantly, all subclasses of abstract classes either:\\n\\n- do not implement a constructor at all, leaving the base class constructor (the abstract constructor) to become the default constructor of the subclass _or_\\n\\n- implement their own constructor which invokes the base (or \\"super\\") class constructor with the correct arguments.\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/typescript-abstract-classes-and-constructors/)"},{"id":"/2021/07/14/directory-build-props-c-sharp-9-for-all","metadata":{"permalink":"/2021/07/14/directory-build-props-c-sharp-9-for-all","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-07-14-directory-build-props-c-sharp-9-for-all/index.md","source":"@site/blog/2021-07-14-directory-build-props-c-sharp-9-for-all/index.md","title":"Directory.Build.props: C# 9 for all your projects","description":".NET Core can make use of C# 9 by making some changes to your .csproj files. There is a way to opt all projects in a solution into this behaviour in a single place, through using a Directory.Build.props file and / or a Directory.Build.targets file. Here\'s how to do it.","date":"2021-07-14T00:00:00.000Z","formattedDate":"July 14, 2021","tags":[{"label":"Directory.Build.props","permalink":"/tags/directory-build-props"},{"label":"C# 9","permalink":"/tags/c-9"},{"label":".NET Core","permalink":"/tags/net-core"}],"readingTime":1.935,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Directory.Build.props: C# 9 for all your projects","authors":"johnnyreilly","tags":["Directory.Build.props","C# 9",".NET Core"],"image":"./title-image.png","hide_table_of_contents":false},"prevItem":{"title":"TypeScript, abstract classes, and constructors","permalink":"/2021/08/01/typescript-abstract-classes-and-constructors"},"nextItem":{"title":"webpack? esbuild? Why not both?","permalink":"/2021/07/11/webpack-esbuild-why-not-both"}},"content":".NET Core can make use of C# 9 by making some changes to your `.csproj` files. There is a way to opt all projects in a solution into this behaviour in a _single_ place, through using a `Directory.Build.props` file and / or a `Directory.Build.targets` file. Here\'s how to do it.\\n\\n![title image showing name of post and the C# logo](title-image.png)\\n\\n## \\"have you the good news about `Directory.Build.props`\\"?\\n\\n[I wrote recently about using C# 9 with in-process Azure Functions.](./2021-07-01-c-sharp-9-azure-functions-in-process/index.md) What that amounted to, was using C# 9 with .NET Core.\\n\\nOne of the best things about blogging, is all that you get to learn along the way. After I put up that post, [Daniel Earwicker](https://twitter.com/danielearwicker) was kind enough to send this message:\\n\\n[![title image showing name of post and the C# logo](daniel-earwicker-tweet.png)](https://twitter.com/danielearwicker/status/1412678642203828226)\\n\\nI was intrigued that Daniel was able to configure all the projects in a solution to use the same approach using some strange incantations named `Directory.Build.props` and `Directory.Build.targets`. [Microsoft describes them thusly](https://docs.microsoft.com/en-us/visualstudio/msbuild/customize-your-build?view=vs-2019#directorybuildprops-and-directorybuildtargets):\\n\\n> Prior to MSBuild version 15, if you wanted to provide a new, custom property to projects in your solution, you had to manually add a reference to that property to every project file in the solution. Or, you had to define the property in a `.props` file and then explicitly import the `.props` file in every project in the solution, among other things.\\n>\\n> However, now you can add a new property to every project in one step by defining it in a single file called `Directory.Build.props` in the root folder that contains your source.\\n\\nLet\'s see if we can put it to use.\\n\\n## `Directory.Build.props`: C# 9 for all\\n\\nSo, rather than us updating each of our `.csproj` files, we should be able to create a `Directory.Build.props` file to sit alongside our `.sln` file in the root of our source code. We\'ll add this into the file:\\n\\n```xml\\n<Project>\\n <PropertyGroup>\\n    \x3c!-- use C# 9 --\x3e\\n    <LangVersion>9.0</LangVersion>\\n </PropertyGroup>\\n <ItemGroup>\\n    \x3c!-- allows some C# 9 support with .NET Core 3.1 https://github.com/manuelroemer/IsExternalInit --\x3e\\n    <PackageReference Include=\\"IsExternalInit\\" Version=\\"1.0.1\\">\\n      <IncludeAssets>runtime; build; native; contentfiles; analyzers; buildtransitive</IncludeAssets>\\n      <PrivateAssets>all</PrivateAssets>\\n    </PackageReference>\\n  </ItemGroup>\\n</Project>\\n```\\n\\nNow we\'re free to add projects into the solution, which will _already_ support C# 9 without us taking any further steps. It\'s as simple as that! Thanks to Daniel for sharing this super handy tip. \u2764\ufe0f\ud83c\udf3b"},{"id":"/2021/07/11/webpack-esbuild-why-not-both","metadata":{"permalink":"/2021/07/11/webpack-esbuild-why-not-both","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-07-11-webpack-esbuild-why-not-both/index.md","source":"@site/blog/2021-07-11-webpack-esbuild-why-not-both/index.md","title":"webpack? esbuild? Why not both?","description":"Builds can be made faster using tools like esbuild. However, if you\'re invested in webpack but would still like to take advantage of speedier builds, there is a way. This post takes us through using esbuild alongside webpack using esbuild-loader.","date":"2021-07-11T00:00:00.000Z","formattedDate":"July 11, 2021","tags":[{"label":"webpack","permalink":"/tags/webpack"},{"label":"esbuild","permalink":"/tags/esbuild"},{"label":"esbuild-loader","permalink":"/tags/esbuild-loader"},{"label":"ts-loader","permalink":"/tags/ts-loader"},{"label":"babel-loader","permalink":"/tags/babel-loader"}],"readingTime":7.055,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"webpack? esbuild? Why not both?","authors":"johnnyreilly","tags":["webpack","esbuild","esbuild-loader","ts-loader","babel-loader"],"image":"./webpack-esbuild-why-not-both.jpg","hide_table_of_contents":false},"prevItem":{"title":"Directory.Build.props: C# 9 for all your projects","permalink":"/2021/07/14/directory-build-props-c-sharp-9-for-all"},"nextItem":{"title":"Output connection strings and keys from Azure Bicep","permalink":"/2021/07/07/output-connection-strings-and-keys-from-azure-bicep"}},"content":"Builds can be made faster using tools like [esbuild](https://github.com/evanw/esbuild). However, if you\'re invested in [webpack](https://github.com/webpack/webpack) but would still like to take advantage of speedier builds, there is a way. This post takes us through using esbuild alongside webpack using [esbuild-loader](https://github.com/privatenumber/esbuild-loader).\\n\\n![A screenshot of the \\"why not both\\" meme adapted to include webpack and esbuild](webpack-esbuild-why-not-both.jpg)\\n\\n## Web development\\n\\nWith apologies to those suffering from JavaScript fatigue, once again the world of web development is evolving. It\'s long been common practice to run your JavaScript and TypeScript through some kind of Node.js based build tool, like webpack or rollup.js. These tools are written in the same language they compile to; JavaScript (or TypeScript). The new kids on the blog are tools like [esbuild](https://github.com/evanw/esbuild), [Vite](https://github.com/vitejs/vite) and [swc](https://github.com/swc-project/swc). The significant difference between these and their predecessors is that they are written in languages like Go and Rust. Go and Rust enjoy far greater performance than JavaScript. This translates into significantly faster builds. If you\'d like to read about esbuild directly there\'s a [great post](https://blog.logrocket.com/fast-javascript-bundling-with-esbuild/) about it.\\n\\nThese new tools are transformative and represent a likely future of build tooling for the web. In the long term, the likes of esbuild, Vite and friends may well come to displace the current standard build tools. So the webpacks, the rollups and so on.\\n\\nHowever, that\u2019s the long term. There\u2019s a lot of projects out there that are already heavily invested in their current build tooling. Mostly webpack. Migrating to a new build tool is no small piece of work. New projects might start with Vite, but existing ones are less likely to be ported. There\u2019s a reason webpack is so popular. It does a lot of things very well indeed. It\'s battle tested on large projects; it\'s mature and it handles many use cases.\\n\\nSo if you\u2019re a team that wants to have faster builds, but doesn\u2019t have the time to go through a big migration... Is there anything you can do? Yes. There\u2019s a middle ground to be explored. There\u2019s a relatively new project named [esbuild-loader](https://github.com/privatenumber/esbuild-loader) developed by [hiroki osame](https://twitter.com/privatenumbr). It\'s a webpack loader built on top of esbuild. It allows users to swap out `ts-loader` or `babel-loader` with itself, and massively improve build speeds.\\n\\nTo declare an interest here, I\'m the primary maintainer of [ts-loader](https://github.com/TypeStrong/ts-loader); a popular TypeScript loader that is commonly used with webpack. However, I feel strongly that the important thing here is developer productivity. As Node.js-based projects, `ts-loader` and `babel-loader` will never be able to compete with `esbuild-loader` in the same way. As a language, Go really, uh, goes!\\n\\nWhilst esbuild may not work for all use cases, it will for the majority. As such `esbuild-loader` represents a middle ground; and an early way to get access to the increased build speed that esbuild offers _without_ saying goodbye to webpack. This post will look at using `esbuild-loader` in your webpack setup.\\n\\n## Migrating an existing project to esbuild\\n\\nIt\'s very straightforward to migrate a project which uses either `babel-loader` or `ts-loader` to `esbuild-loader`. You install the dependency:\\n\\n```bash\\nnpm i -D esbuild-loader\\n```\\n\\nThen if we are currently using `babel-loader`, we make this change to our `webpack.config.js`:\\n\\n```diff\\n  module.exports = {\\n    module: {\\n      rules: [\\n-       {\\n-         test: /\\\\.js$/,\\n-         use: \'babel-loader\',\\n-       },\\n+       {\\n+         test: /\\\\.js$/,\\n+         loader: \'esbuild-loader\',\\n+         options: {\\n+           loader: \'jsx\',  // Remove this if you\'re not using JSX\\n+           target: \'es2015\'  // Syntax to compile to (see options below for possible values)\\n+         }\\n+       },\\n\\n        ...\\n      ],\\n    },\\n  }\\n```\\n\\nOr if we\'re using `ts-loader`, we make this change to our `webpack.config.js`:\\n\\n```diff\\n  module.exports = {\\n    module: {\\n      rules: [\\n-       {\\n-         test: /\\\\.tsx?$/,\\n-         use: \'ts-loader\'\\n-       },\\n+       {\\n+         test: /\\\\.tsx?$/,\\n+         loader: \'esbuild-loader\',\\n+         options: {\\n+           loader: \'tsx\',  // Or \'ts\' if you don\'t need tsx\\n+           target: \'es2015\'\\n+         }\\n+       },\\n\\n        ...\\n      ]\\n    },\\n  }\\n```\\n\\n## Creating a baseline application\\n\\nLet\'s try `esbuild-loader` out in practice. We\'re going to create a new React application using [Create React App](https://create-react-app.dev/):\\n\\n```bash\\nnpx create-react-app my-app --template typescript\\n```\\n\\nThis will scaffold out a new React application using TypeScript in the `my-app` directory. It\'s worth knowing that Create React App uses `babel-loader` behind the scenes.\\n\\nCRA also uses the [fork-ts-checker-webpack-plugin](https://github.com/TypeStrong/fork-ts-checker-webpack-plugin) to provide TypeScript type checking. This is very useful, as esbuild _just_ does transpilation and [does not intend to provide type checking support](https://esbuild.github.io/faq/#upcoming-roadmap). So it\'s tremendous we still have that plugin in place as otherwise we would lose type checking.\\n\\nSo we can understand the advantage of moving to esbuild, we first need a baseline to understand what performance looks like with babel-loader. We\'ll run `time npm run build` to execute a build of our simple app:\\n\\n![A screenshot of the completed build for Create React App](create-react-app-raw.png)\\n\\nOur complete build, TypeScript type checking, transpilation, minification and so on, all took 22.08 seconds. The question now is, what will happen if we drop esbuild into the mix?\\n\\n## CRACO\\n\\nOne way to customise a Create React App build is by running `npm run eject` and then customising the code that CRA pumps out. Doing so is fine, but it means you can\'t keep track with CRA\'s evolution. An alternative is to use a tool like [CRACO](https://github.com/gsoft-inc/craco) which allows us to tweak configuration in place. It describes itself this way:\\n\\n> *C*reate *R*eact *A*pp *C*onfiguration *O*verride is an easy and comprehensible configuration layer for create-react-app.\\n\\nWe\'re going to use CRACO, so we\'ll add `esbuild-loader` and CRACO as dependencies:\\n\\n```bash\\nnpm install @craco/craco esbuild-loader --save-dev\\n```\\n\\nThen we\'ll swap over our various `scripts` in our `package.json` to use `CRACO`:\\n\\n```json\\n\\"start\\": \\"craco start\\",\\n\\"build\\": \\"craco build\\",\\n\\"test\\": \\"craco test\\",\\n```\\n\\nOur app now uses CRACO, but we haven\'t yet configured it. So we\'ll add a `craco.config.js` file to the root of our project. This is where we swap out `babel-loader` for `esbuild-loader`:\\n\\n```js\\nconst {\\n  addAfterLoader,\\n  removeLoaders,\\n  loaderByName,\\n  getLoaders,\\n  throwUnexpectedConfigError,\\n} = require(\'@craco/craco\');\\nconst { ESBuildMinifyPlugin } = require(\'esbuild-loader\');\\n\\nconst throwError = (message) =>\\n  throwUnexpectedConfigError({\\n    packageName: \'craco\',\\n    githubRepo: \'gsoft-inc/craco\',\\n    message,\\n    githubIssueQuery: \'webpack\',\\n  });\\n\\nmodule.exports = {\\n  webpack: {\\n    configure: (webpackConfig, { paths }) => {\\n      const { hasFoundAny, matches } = getLoaders(\\n        webpackConfig,\\n        loaderByName(\'babel-loader\')\\n      );\\n      if (!hasFoundAny) throwError(\'failed to find babel-loader\');\\n\\n      console.log(\'removing babel-loader\');\\n      const { hasRemovedAny, removedCount } = removeLoaders(\\n        webpackConfig,\\n        loaderByName(\'babel-loader\')\\n      );\\n      if (!hasRemovedAny) throwError(\'no babel-loader to remove\');\\n      if (removedCount !== 2)\\n        throwError(\'had expected to remove 2 babel loader instances\');\\n\\n      console.log(\'adding esbuild-loader\');\\n\\n      const tsLoader = {\\n        test: /\\\\.(js|mjs|jsx|ts|tsx)$/,\\n        include: paths.appSrc,\\n        loader: require.resolve(\'esbuild-loader\'),\\n        options: {\\n          loader: \'tsx\',\\n          target: \'es2015\',\\n        },\\n      };\\n\\n      const { isAdded: tsLoaderIsAdded } = addAfterLoader(\\n        webpackConfig,\\n        loaderByName(\'url-loader\'),\\n        tsLoader\\n      );\\n      if (!tsLoaderIsAdded) throwError(\'failed to add esbuild-loader\');\\n      console.log(\'added esbuild-loader\');\\n\\n      console.log(\'adding non-application JS babel-loader back\');\\n      const { isAdded: babelLoaderIsAdded } = addAfterLoader(\\n        webpackConfig,\\n        loaderByName(\'esbuild-loader\'),\\n        matches[1].loader // babel-loader\\n      );\\n      if (!babelLoaderIsAdded)\\n        throwError(\'failed to add back babel-loader for non-application JS\');\\n      console.log(\'added non-application JS babel-loader back\');\\n\\n      console.log(\'replacing TerserPlugin with ESBuildMinifyPlugin\');\\n      webpackConfig.optimization.minimizer = [\\n        new ESBuildMinifyPlugin({\\n          target: \'es2015\',\\n        }),\\n      ];\\n\\n      return webpackConfig;\\n    },\\n  },\\n};\\n```\\n\\nSo what\'s happening here? The script looks for `babel-loader` usages in the default Create React App config. There will be two; one for TypeScript / JavaScript application code (we want to replace this) and one for non application JavaScript code. It\'s not too clear what non application JavaScript code there is or can be, so we\'ll leave it in place; it may be important. Significantly, the code we care about is the application code.\\n\\nYou cannot remove a _single_ loader using `CRACO`, so instead we\'ll remove both and we\'ll add back the non application JavaScript `babel-loader`. We\'ll also add `esbuild-loader` with the `{ loader: \'tsx\', target: \'es2015\' }` option set (to ensure we can process JSX/TSX).\\n\\nFinally we\'ll swap out using Terser for JavaScript minification for esbuild as well.\\n\\nOur migration is complete. The next time we build we\'ll have Create React App running using `esbuild-loader` _without_ having ejected. Once again we\'ll run `time npm run build` to execute a build of our simple app and determine how long it takes:\\n\\n![A screenshot of the completed build for Create React App with esbuild](create-react-app-esbuild.png)\\n\\nOur complete build, TypeScript type checking, transpilation, minification and so on, all took 13.85 seconds. By migrating to `esbuild-loader` we\'ve reduced our overall compilation time by approximately one third; this is a tremendous improvement!\\n\\nAs your codebase scales and your application grows, compilation time can skyrocket also. With `esbuild-loader` you should get ongoing benefits to your build time.\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/webpack-or-esbuild-why-not-both/)"},{"id":"/2021/07/07/output-connection-strings-and-keys-from-azure-bicep","metadata":{"permalink":"/2021/07/07/output-connection-strings-and-keys-from-azure-bicep","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-07-07-output-connection-strings-and-keys-from-azure-bicep/index.md","source":"@site/blog/2021-07-07-output-connection-strings-and-keys-from-azure-bicep/index.md","title":"Output connection strings and keys from Azure Bicep","description":"If we\'re provisioning resources in Azure with Bicep, we may have a need to acquire the connection strings and keys of our newly deployed infrastructure. For example, the connection strings of an event hub or the access keys of a storage account. Perhaps we\'d like to use them to run an end-to-end test, perhaps we\'d like to store these secrets somewhere for later consumption. This post shows how to do that using Bicep and the listKeys helper. Optionally it shows how we could consume this in Azure Pipelines.","date":"2021-07-07T00:00:00.000Z","formattedDate":"July 7, 2021","tags":[{"label":"Bicep","permalink":"/tags/bicep"},{"label":"Azure","permalink":"/tags/azure"},{"label":"connection string","permalink":"/tags/connection-string"},{"label":"keys","permalink":"/tags/keys"}],"readingTime":6.135,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Output connection strings and keys from Azure Bicep","authors":"johnnyreilly","tags":["Bicep","Azure","connection string","keys"],"image":"./title-image.jpg","hide_table_of_contents":false},"prevItem":{"title":"webpack? esbuild? Why not both?","permalink":"/2021/07/11/webpack-esbuild-why-not-both"},"nextItem":{"title":"C# 9 in-process Azure Functions","permalink":"/2021/07/01/c-sharp-9-azure-functions-in-process"}},"content":"If we\'re provisioning resources in Azure with Bicep, we may have a need to acquire the connection strings and keys of our newly deployed infrastructure. For example, the connection strings of an event hub or the access keys of a storage account. Perhaps we\'d like to use them to run an end-to-end test, perhaps we\'d like to store these secrets somewhere for later consumption. This post shows how to do that using Bicep and the `listKeys` helper. Optionally it shows how we could consume this in Azure Pipelines.\\n\\nPlease note that exporting keys / connection strings etc from Bicep / ARM templates is generally considered to be a less secure approach. This is because these values will be visible inside the deployments section of the Azure Portal. Anyone who has access to this will be able to see them. An alternative approach would be permissioning our pipeline to access the resources directly. You can read about that approach [here](2021-09-12-permissioning-azure-pipelines-bicep-role-assignments/index.md).\\n\\n![image which contains the blog title](title-image.jpg)\\n\\n## Event Hub connection string\\n\\nFirst of all, let\'s provision an Azure Event Hub. This involves deploying an event hub namespace, an event hub in that namespace and an authorization rule. The following Bicep will do this for us:\\n\\n```bicep\\n// Create an event hub namespace\\n\\nvar eventHubNamespaceName = \'evhns-demo\'\\n\\nresource eventHubNamespace \'Microsoft.EventHub/namespaces@2021-01-01-preview\' = {\\n  name: eventHubNamespaceName\\n  location: resourceGroup().location\\n  sku: {\\n    name: \'Standard\'\\n    tier: \'Standard\'\\n    capacity: 1\\n  }\\n  properties: {\\n    zoneRedundant: true\\n  }\\n}\\n\\n// Create an event hub inside the namespace\\n\\nvar eventHubName = \'evh-demo\'\\n\\nresource eventHubNamespaceName_eventHubName \'Microsoft.EventHub/namespaces/eventhubs@2021-01-01-preview\' = {\\n  parent: eventHubNamespace\\n  name: eventHubName\\n  properties: {\\n    messageRetentionInDays: 7\\n    partitionCount: 1\\n  }\\n}\\n\\n// Grant Listen and Send on our event hub\\n\\nresource eventHubNamespaceName_eventHubName_ListenSend \'Microsoft.EventHub/namespaces/eventhubs/authorizationRules@2021-01-01-preview\' = {\\n  parent: eventHubNamespaceName_eventHubName\\n  name: \'ListenSend\'\\n  properties: {\\n    rights: [\\n      \'Listen\'\\n      \'Send\'\\n    ]\\n  }\\n  dependsOn: [\\n    eventHubNamespace\\n  ]\\n}\\n```\\n\\nWhen this is deployed to Azure, it will result in creating something like this:\\n\\n![screenshot of event hub connection strings in the Azure Portal](event-hub-connection-string.png)\\n\\nAs we can see, there are connection strings available which can be used to access the event hub. How do we get a connection string that we can play with? It\'s easily achieved by appending the following to our Bicep:\\n\\n```bicep\\n// Determine our connection string\\n\\nvar eventHubNamespaceConnectionString = listKeys(eventHubNamespaceName_eventHubName_ListenSend.id, eventHubNamespaceName_eventHubName_ListenSend.apiVersion).primaryConnectionString\\n\\n// Output our variables\\n\\noutput eventHubNamespaceConnectionString string = eventHubNamespaceConnectionString\\noutput eventHubName string = eventHubName\\n```\\n\\nWhat we\'re doing here is using the [`listKeys`](https://docs.microsoft.com/en-us/azure/azure-resource-manager/bicep/bicep-functions-resource#list) helper on our authorization rule and retrieving the handy `primaryConnectionString`, which is then exposed as an output variable.\\n\\n## Storage Account connection string\\n\\nWe\'d like to obtain a connection string for a storage account also. Let\'s put together a Bicep file that creates a storage account and a container therein. (Incidentally, it\'s fairly common to have a storage account provisioned alongside an event hub to facilitate reading from an event hub.)\\n\\n```bicep\\n// Create a storage account\\n\\nvar storageAccountName = \'stdemo\'\\n\\nresource eventHubNamespaceName_storageAccount \'Microsoft.Storage/storageAccounts@2021-02-01\' = {\\n  name: storageAccountName\\n  location: resourceGroup().location\\n  sku: {\\n    name: \'Standard_LRS\'\\n    tier: \'Standard\'\\n  }\\n  kind: \'StorageV2\'\\n  properties: {\\n    networkAcls: {\\n      bypass: \'AzureServices\'\\n      defaultAction: \'Allow\'\\n    }\\n    accessTier: \'Hot\'\\n    allowBlobPublicAccess: false\\n    minimumTlsVersion: \'TLS1_2\'\\n    allowSharedKeyAccess: true\\n  }\\n}\\n\\n// create a container inside that storage account\\n\\nvar blobContainerName = \'test-container\'\\n\\nresource storageAccountName_default_containerName \'Microsoft.Storage/storageAccounts/blobServices/containers@2021-02-01\' = {\\n  name: \'${storageAccountName}/default/${blobContainerName}\'\\n  dependsOn: [\\n    eventHubNamespaceName_storageAccount\\n  ]\\n}\\n```\\n\\nWhen this is deployed to Azure, it will result in creating something like this:\\n\\n![screenshot of storage account access keys in the Azure Portal](storage-account-access-keys.png)\\n\\nAgain we can see, there are connection strings available in the Azure Portal, which can be used to access the storage account. However, things aren\'t quite as simple as previously; in that there doesn\'t seem to be a way to directly acquire a connection string. What we can do, is acquire a key; and construct ourselves a connection string with that. Here\'s how:\\n\\n```bicep\\n// Determine our connection string\\n\\nvar blobStorageConnectionString = \'DefaultEndpointsProtocol=https;AccountName=${eventHubNamespaceName_storageAccount.name};EndpointSuffix=${environment().suffixes.storage};AccountKey=${listKeys(eventHubNamespaceName_storageAccount.id, eventHubNamespaceName_storageAccount.apiVersion).keys[0].value}\'\\n\\n// Output our variable\\n\\noutput blobStorageConnectionString string = blobStorageConnectionString\\noutput blobContainerName string = blobContainerName\\n```\\n\\nIf you just wanted to know how to acquire connection strings from Bicep then you can stop now; we\'re done! But if you\'re curious on how the Bicep might connect to ~~the shoulder~~ Azure Pipelines... Read on.\\n\\n## From Bicep to Azure Pipelines\\n\\nIf we put together our snippets above into a single Bicep file it would look like this:\\n\\n```bicep\\n// Create an event hub namespace\\n\\nvar eventHubNamespaceName = \'evhns-demo\'\\n\\nresource eventHubNamespace \'Microsoft.EventHub/namespaces@2021-01-01-preview\' = {\\n  name: eventHubNamespaceName\\n  location: resourceGroup().location\\n  sku: {\\n    name: \'Standard\'\\n    tier: \'Standard\'\\n    capacity: 1\\n  }\\n  properties: {\\n    zoneRedundant: true\\n  }\\n}\\n\\n// Create an event hub inside the namespace\\n\\nvar eventHubName = \'evh-demo\'\\n\\nresource eventHubNamespaceName_eventHubName \'Microsoft.EventHub/namespaces/eventhubs@2021-01-01-preview\' = {\\n  parent: eventHubNamespace\\n  name: eventHubName\\n  properties: {\\n    messageRetentionInDays: 7\\n    partitionCount: 1\\n  }\\n}\\n\\n// Grant Listen and Send on our event hub\\n\\nresource eventHubNamespaceName_eventHubName_ListenSend \'Microsoft.EventHub/namespaces/eventhubs/authorizationRules@2021-01-01-preview\' = {\\n  parent: eventHubNamespaceName_eventHubName\\n  name: \'ListenSend\'\\n  properties: {\\n    rights: [\\n      \'Listen\'\\n      \'Send\'\\n    ]\\n  }\\n  dependsOn: [\\n    eventHubNamespace\\n  ]\\n}\\n\\n// Create a storage account\\n\\nvar storageAccountName = \'stdemo\'\\n\\nresource eventHubNamespaceName_storageAccount \'Microsoft.Storage/storageAccounts@2021-02-01\' = {\\n  name: storageAccountName\\n  location: resourceGroup().location\\n  sku: {\\n    name: \'Standard_LRS\'\\n    tier: \'Standard\'\\n  }\\n  kind: \'StorageV2\'\\n  properties: {\\n    networkAcls: {\\n      bypass: \'AzureServices\'\\n      defaultAction: \'Allow\'\\n    }\\n    accessTier: \'Hot\'\\n    allowBlobPublicAccess: false\\n    minimumTlsVersion: \'TLS1_2\'\\n    allowSharedKeyAccess: true\\n  }\\n}\\n\\n// create a container inside that storage account\\n\\nvar blobContainerName = \'test-container\'\\n\\nresource storageAccountName_default_containerName \'Microsoft.Storage/storageAccounts/blobServices/containers@2021-02-01\' = {\\n  name: \'${storageAccountName}/default/${blobContainerName}\'\\n  dependsOn: [\\n    eventHubNamespaceName_storageAccount\\n  ]\\n}\\n\\n// Determine our connection strings\\n\\nvar blobStorageConnectionString       = \'DefaultEndpointsProtocol=https;AccountName=${eventHubNamespaceName_storageAccount.name};EndpointSuffix=${environment().suffixes.storage};AccountKey=${listKeys(eventHubNamespaceName_storageAccount.id, eventHubNamespaceName_storageAccount.apiVersion).keys[0].value}\'\\nvar eventHubNamespaceConnectionString = listKeys(eventHubNamespaceName_eventHubName_ListenSend.id, eventHubNamespaceName_eventHubName_ListenSend.apiVersion).primaryConnectionString\\n\\n// Output our variables\\n\\noutput blobStorageConnectionString string = blobStorageConnectionString\\noutput blobContainerName string = blobContainerName\\noutput eventHubNamespaceConnectionString string = eventHubNamespaceConnectionString\\noutput eventHubName string = eventHubName\\n```\\n\\nThis might be consumed in an Azure Pipeline that looks like this:\\n\\n```yml\\n- bash: az bicep build --file infra/our-test-app/main.bicep\\n  displayName: \'Compile Bicep to ARM\'\\n\\n- task: AzureResourceManagerTemplateDeployment@3\\n  name: DeploySharedInfra\\n  displayName: Deploy Shared ARM Template\\n  inputs:\\n    deploymentScope: Resource Group\\n    azureResourceManagerConnection: ${{ parameters.serviceConnection }}\\n    subscriptionId: $(subscriptionId)\\n    action: Create Or Update Resource Group\\n    resourceGroupName: $(azureResourceGroup)\\n    location: $(location)\\n    templateLocation: Linked artifact\\n    csmFile: \'infra/our-test-app/main.json\' # created by bash script\\n    deploymentMode: Incremental\\n    deploymentOutputs: deployOutputs\\n\\n- task: PowerShell@2\\n  name: \'SetOutputVariables\'\\n  displayName: \'Set Output Variables\'\\n  inputs:\\n    targetType: inline\\n    script: |\\n      $armOutputObj = \'$(deployOutputs)\' | ConvertFrom-Json\\n      $armOutputObj.PSObject.Properties | ForEach-Object {\\n        $keyname = $_.Name\\n        $value = $_.Value.value\\n\\n        # Creates a standard pipeline variable\\n        Write-Output \\"##vso[task.setvariable variable=$keyName;]$value\\"\\n\\n        # Creates an output variable\\n        Write-Output \\"##vso[task.setvariable variable=$keyName;issecret=true;isOutput=true]$value\\"\\n\\n        # Display keys in pipeline\\n        Write-Output \\"output variable: $keyName\\"\\n      }\\n    pwsh: true\\n```\\n\\nAbove we can see:\\n\\n- the Bicep get compiled to ARM\\n- the ARM is deployed to Azure, with `deploymentOutputs` being passed out at the end\\n- the outputs are turned into secret output variables inside the pipeline (the names of which are printed to the console)\\n\\nWith the above in place, we now have all of our variables in place; `blobStorageConnectionString`, `blobContainerName`, `eventHubNamespaceConnectionString` and `eventHubName`. These could now be consumed in whatever way is useful. Consider the following:\\n\\n```yml\\n- task: UseDotNet@2\\n  displayName: \'Install .NET Core SDK 3.1.x\'\\n  inputs:\\n    packageType: \'sdk\'\\n    version: 3.1.x\\n\\n- task: DotNetCoreCLI@2\\n  displayName: \'dotnet run eventhub test\'\\n  inputs:\\n    command: \'run\'\\n    arguments: \'eventhub test --eventHubNamespaceConnectionString \\"$(eventHubNamespaceConnectionString)\\" --eventHubName \\"$(eventHubName)\\" --blobStorageConnectionString \\"$(blobStorageConnectionString)\\" --blobContainerName \\"$(blobContainerName)\\"\'\\n    workingDirectory: \'$(Build.SourcesDirectory)/OurTestApp\'\\n```\\n\\nHere we run a .NET application and pass it our connection strings. Please note, there\'s nothing .NET specific about what we\'re doing above - it could be any kind of application, bash script or similar that consumes our connection strings. The significant thing is that we can acquire connection strings in an automated fashion, for use in whichever manner pleases us."},{"id":"/2021/07/01/c-sharp-9-azure-functions-in-process","metadata":{"permalink":"/2021/07/01/c-sharp-9-azure-functions-in-process","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-07-01-c-sharp-9-azure-functions-in-process/index.md","source":"@site/blog/2021-07-01-c-sharp-9-azure-functions-in-process/index.md","title":"C# 9 in-process Azure Functions","description":"C# 9 has some amazing features. Azure Functions are have two modes: isolated and in-process. Whilst isolated supports .NET 5 (and hence C# 9), in-process supports .NET Core 3.1 (C# 8). This post shows how we can use C# 9 with in-process Azure Functions running on .NET Core 3.1.","date":"2021-07-01T00:00:00.000Z","formattedDate":"July 1, 2021","tags":[{"label":"C# 9","permalink":"/tags/c-9"},{"label":"Azure Functions","permalink":"/tags/azure-functions"},{"label":".NET","permalink":"/tags/net"},{"label":"in-process","permalink":"/tags/in-process"}],"readingTime":4.565,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"C# 9 in-process Azure Functions","authors":"johnnyreilly","tags":["C# 9","Azure Functions",".NET","in-process"],"image":"./title-image.png","hide_table_of_contents":false},"prevItem":{"title":"Output connection strings and keys from Azure Bicep","permalink":"/2021/07/07/output-connection-strings-and-keys-from-azure-bicep"},"nextItem":{"title":"React 18 and TypeScript","permalink":"/2021/06/30/react-18-and-typescript"}},"content":"C# 9 has some amazing features. Azure Functions are have two modes: isolated and in-process. Whilst isolated supports .NET 5 (and hence C# 9), in-process supports .NET Core 3.1 (C# 8). This post shows how we can use C# 9 with in-process Azure Functions running on .NET Core 3.1.\\n\\n![title image showing name of post and the Azure Functions logo](title-image.png)\\n\\n## Azure Functions: in-process and isolated\\n\\nHistorically .NET Azure Functions have been in-process. This changed with .NET 5 where a new model was introduced named \\"isolated\\". [To quote from the roadmap](https://techcommunity.microsoft.com/t5/apps-on-azure/net-on-azure-functions-roadmap/ba-p/2197916):\\n\\n> Running in an isolated process decouples .NET functions from the Azure Functions host\u2014allowing us to more easily support new .NET versions and address pain points associated with sharing a single process.\\n\\nHowever, the initial launch of isolated functions [does not have the full level of functionality enjoyed by in-process functions](https://docs.microsoft.com/en-us/azure/azure-functions/dotnet-isolated-process-guide#differences-with-net-class-library-functions). This will happen, according the roadmap:\\n\\n> Long term, our vision is to have full feature parity out of process, bringing many of the features that are currently exclusive to the in-process model to the isolated model. We plan to begin delivering improvements to the isolated model after the .NET 6 general availability release.\\n\\nIn the future, in-process functions will be retired in favour of isolated functions. However, it will be .NET 7 (scheduled to ship in November 2022) before that takes place:\\n\\n![the Azure Functions roadmap image illustrating the future of .NET functions taken from https://techcommunity.microsoft.com/t5/apps-on-azure/net-on-azure-functions-roadmap/ba-p/2197916](dotnet-functions-roadmap.png)\\n\\nAs the image taken from the roadmap shows, when .NET 5 shipped, it did not support in-process Azure Functions. When .NET 6 ships in November, it should.\\n\\nIn the meantime, we would like to use C# 9.\\n\\n## Setting up a C# 8 project\\n\\nWe\'re have the [Azure Functions Core Tools](https://docs.microsoft.com/en-us/azure/azure-functions/functions-run-local) installed, so let\'s create a new function project:\\n\\n```bash\\nfunc new --worker-runtime dotnet --template \\"Http Trigger\\" --name \\"HelloRecord\\"\\n```\\n\\nThe above command scaffolds out a .NET Core 3.1 Azure function project which contains a single Azure function. The `--worker-runtime dotnet` parameter is what causes an in-process .NET Core 3.1 function being created. You should have a `.csproj` file that looks like this:\\n\\n```xml\\n<Project Sdk=\\"Microsoft.NET.Sdk\\">\\n  <PropertyGroup>\\n    <TargetFramework>netcoreapp3.1</TargetFramework>\\n    <AzureFunctionsVersion>v3</AzureFunctionsVersion>\\n  </PropertyGroup>\\n  <ItemGroup>\\n    <PackageReference Include=\\"Microsoft.NET.Sdk.Functions\\" Version=\\"3.0.11\\" />\\n  </ItemGroup>\\n  <ItemGroup>\\n    <None Update=\\"host.json\\">\\n      <CopyToOutputDirectory>PreserveNewest</CopyToOutputDirectory>\\n    </None>\\n    <None Update=\\"local.settings.json\\">\\n      <CopyToOutputDirectory>PreserveNewest</CopyToOutputDirectory>\\n      <CopyToPublishDirectory>Never</CopyToPublishDirectory>\\n    </None>\\n  </ItemGroup>\\n</Project>\\n```\\n\\nWe\'re running with C# 8 and .NET Core 3.1 at this point. What does it take to get us to C# 9?\\n\\n## What does it take to get to C# 9?\\n\\nThere\'s a [great post on Reddit addressing using C# 9 with .NET Core 3.1 which says:](https://www.reddit.com/r/csharp/comments/kiplz8/can_i_use_c90_with_aspnet_core_31/)\\n\\n> You can use `<LangVersion>9.0</LangVersion>`, and VS even includes support for suggesting a language upgrade.\\n>\\n> However, there are three categories of features in C#:\\n>\\n> 1. features that are entirely part of the compiler. Those will work.\\n>\\n> 2. features that require BCL additions. Since you\'re on the older BCL, those will need to be backported. For example, to use init; and record, you can use https://github.com/manuelroemer/IsExternalInit.\\n>\\n> 3. features that require runtime additions. Those cannot be added at all. For example, default interface members in C# 8, and covariant return types in C# 9.\\n\\nOf the above, 1 and 2 add a tremendous amount of value. The features of 3 are great, but more niche. Speaking personally, I care a great deal about [Record types](https://docs.microsoft.com/en-us/dotnet/csharp/whats-new/csharp-9#record-types). So let\'s apply this.\\n\\n## Adding C# 9 to the in-process function\\n\\nTo get C# into the mix, we want to make two changes:\\n\\n- add a `<LangVersion>9.0</LangVersion>` to the `<PropertyGroup>` element of our `.csproj` file\\n- add a package reference to the [`IsExternalInit`](https://github.com/manuelroemer/IsExternalInit)\\n\\nThe applied changes look like this:\\n\\n```diff\\n<Project Sdk=\\"Microsoft.NET.Sdk\\">\\n  <PropertyGroup>\\n    <TargetFramework>netcoreapp3.1</TargetFramework>\\n+    <LangVersion>9.0</LangVersion>\\n    <AzureFunctionsVersion>v3</AzureFunctionsVersion>\\n  </PropertyGroup>\\n  <ItemGroup>\\n    <PackageReference Include=\\"Microsoft.NET.Sdk.Functions\\" Version=\\"3.0.11\\" />\\n+    <PackageReference Include=\\"IsExternalInit\\" Version=\\"1.0.1\\" PrivateAssets=\\"all\\" />\\n  </ItemGroup>\\n  <ItemGroup>\\n    <None Update=\\"host.json\\">\\n      <CopyToOutputDirectory>PreserveNewest</CopyToOutputDirectory>\\n    </None>\\n    <None Update=\\"local.settings.json\\">\\n      <CopyToOutputDirectory>PreserveNewest</CopyToOutputDirectory>\\n      <CopyToPublishDirectory>Never</CopyToPublishDirectory>\\n    </None>\\n  </ItemGroup>\\n</Project>\\n```\\n\\nIf we used `dotnet add package IsExternalInit`, we might be using a different syntax in the `.csproj`. Be not afeard - that won\'t affect usage.\\n\\n## Making a C# 9 program\\n\\nNow we can theoretically use C# 9.... Let\'s use C# 9. We\'ll tweak our `HelloRecord.cs` file, add in a simple `record` named `MessageRecord` and tweak the `Run` method to use it:\\n\\n```cs\\nusing System;\\nusing System.IO;\\nusing System.Threading.Tasks;\\nusing Microsoft.AspNetCore.Mvc;\\nusing Microsoft.Azure.WebJobs;\\nusing Microsoft.Azure.WebJobs.Extensions.Http;\\nusing Microsoft.AspNetCore.Http;\\nusing Microsoft.Extensions.Logging;\\nusing Newtonsoft.Json;\\n\\nnamespace tmp\\n{\\n    public record MessageRecord(string message);\\n\\n    public static class HelloRecord\\n    {\\n        [FunctionName(\\"HelloRecord\\")]\\n        public static async Task<IActionResult> Run(\\n            [HttpTrigger(AuthorizationLevel.Function, \\"get\\", \\"post\\", Route = null)] HttpRequest req,\\n            ILogger log)\\n        {\\n            log.LogInformation(\\"C# HTTP trigger function processed a request.\\");\\n\\n            string name = req.Query[\\"name\\"];\\n\\n            string requestBody = await new StreamReader(req.Body).ReadToEndAsync();\\n            dynamic data = JsonConvert.DeserializeObject(requestBody);\\n            name = name ?? data?.name;\\n\\n            var responseMessage = new MessageRecord(string.IsNullOrEmpty(name)\\n                ? \\"This HTTP triggered function executed successfully. Pass a name in the query string or in the request body for a personalized response.\\"\\n                : $\\"Hello, {name}. This HTTP triggered function executed successfully.\\");\\n\\n            return new OkObjectResult(responseMessage);\\n        }\\n    }\\n}\\n```\\n\\nIf we kick off our function with `func start`:\\n\\n![screenshot of the output of the HelloRecord function](calling-hello-record.png)\\n\\nWe can see we can compile, and output is as we might expect and hope. Likewise if we try and debug in VS Code, we can:\\n\\n![screenshot of the output of the HelloRecord function](debugging-hello-record.png)\\n\\n## Best before...\\n\\nSo, we\'ve now a way to use C# 9 (or most of it) with in-process .NET Core 3.1 apps. This should serve until .NET 6 ships in November 2021 and we\'re able to use C# 9 by default."},{"id":"/2021/06/30/react-18-and-typescript","metadata":{"permalink":"/2021/06/30/react-18-and-typescript","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-06-30-react-18-and-typescript/index.md","source":"@site/blog/2021-06-30-react-18-and-typescript/index.md","title":"React 18 and TypeScript","description":"React 18 alpha has been released; but can we use it with TypeScript? The answer is \\"yes\\", but you need to do a couple of things to make that happen. This post will show you what to do.","date":"2021-06-30T00:00:00.000Z","formattedDate":"June 30, 2021","tags":[{"label":"React","permalink":"/tags/react"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"React 18","permalink":"/tags/react-18"}],"readingTime":3.405,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"React 18 and TypeScript","authors":"johnnyreilly","tags":["React","TypeScript","React 18"],"image":"./createNode-error.png","hide_table_of_contents":false},"prevItem":{"title":"C# 9 in-process Azure Functions","permalink":"/2021/07/01/c-sharp-9-azure-functions-in-process"},"nextItem":{"title":"Azure Functions and .NET 5: Query params, Dependency Injection, Bicep & Build","permalink":"/2021/06/11/azure-functions-dotnet-5-query-params-di-bicep"}},"content":"[React 18 alpha has been released](https://reactjs.org/blog/2021/06/08/the-plan-for-react-18.html); but can we use it with TypeScript? The answer is \\"yes\\", but you need to do a couple of things to make that happen. This post will show you what to do.\\n\\n## Creating a React App with TypeScript\\n\\nLet\'s create ourselves a vanilla React TypeScript app with [Create React App](https://create-react-app.dev/):\\n\\n```shell\\nyarn create react-app my-app --template typescript\\n```\\n\\nNow let\'s upgrade the version of React to `@next`:\\n\\n```shell\\nyarn add react@next react-dom@next\\n```\\n\\nWhich will leave you with entries in the `package.json` which use React 18. It will likely look something like this:\\n\\n```json\\n    \\"react\\": \\"^18.0.0-alpha-e6be2d531\\",\\n    \\"react-dom\\": \\"^18.0.0-alpha-e6be2d531\\",\\n```\\n\\nIf we run `yarn start` we\'ll find ourselves running a React 18 app. Exciting!\\n\\n## Using the new APIs\\n\\nSo let\'s try using [`ReactDOM.createRoot`](https://github.com/reactwg/react-18/discussions/5) API. It\'s this API that opts our application into using new features of React 18. We\'ll open up `index.tsx` and make this change:\\n\\n```diff\\n-ReactDOM.render(\\n-  <React.StrictMode>\\n-    <App />\\n-  </React.StrictMode>,\\n-  document.getElementById(\'root\')\\n-);\\n+const root = ReactDOM.createRoot(document.getElementById(\'root\'));\\n+\\n+root.render(\\n+  <React.StrictMode>\\n+    <App />\\n+  </React.StrictMode>\\n+);\\n```\\n\\nIf we were running JavaScript alone, this would work. However, because we\'re using TypeScript as well, we\'re now confronted with an error:\\n\\n> `Property \'createRoot\' does not exist on type \'typeof import(\\"/code/my-app/node_modules/@types/react-dom/index\\")\'. TS2339`\\n\\n![a screenshot of the Property \'createRoot\' does not exist error](createNode-error.png)\\n\\nThis is the TypeScript compiler complaining that it doesn\'t know anything about `ReactDOM.createRoot`. This is because the type definitions that are currently in place in our application don\'t have that API defined.\\n\\nLet\'s upgrade our type definitions:\\n\\n```shell\\nyarn add @types/react @types/react-dom\\n```\\n\\nWe might reasonably hope that everything should work now. Alas it does not. The same error is presenting. TypeScript is not happy.\\n\\n## Telling TypeScript about the new APIs\\n\\nIf we take a look at the [PR that added support for the APIs](https://github.com/DefinitelyTyped/DefinitelyTyped/pull/53685), we\'ll find some tips. If you look at one of the [`next.d.ts`](https://github.com/DefinitelyTyped/DefinitelyTyped/blob/a07e9cfb005682fb6be0a2e85113eac131c3006f/types/react/next.d.ts) you\'ll find this info, courtesy of [Sebastian Silbermann](https://twitter.com/sebsilbermann):\\n\\n````ts\\n/**\\n * These are types for things that are present in the upcoming React 18 release.\\n *\\n * Once React 18 is released they can just be moved to the main index file.\\n *\\n * To load the types declared here in an actual project, there are three ways. The easiest one,\\n * if your `tsconfig.json` already has a `\\"types\\"` array in the `\\"compilerOptions\\"` section,\\n * is to add `\\"react/next\\"` to the `\\"types\\"` array.\\n *\\n * Alternatively, a specific import syntax can to be used from a typescript file.\\n * This module does not exist in reality, which is why the {} is important:\\n *\\n * ```ts\\n * import {} from \'react/next\'\\n * ```\\n *\\n * It is also possible to include it through a triple-slash reference:\\n *\\n * ```ts\\n * /// <reference types=\\"react/next\\" />\\n * ```\\n *\\n * Either the import or the reference only needs to appear once, anywhere in the project.\\n */\\n````\\n\\nLet\'s try the first item on the list. We\'ll edit our `tsconfig.json` and add a new entry to the `\\"compilerOptions\\"` section:\\n\\n```json\\n    \\"types\\": [\\"react/next\\", \\"react-dom/next\\"]\\n```\\n\\nIf we restart our build with `yarn start` we\'re now presented with a _different_ error:\\n\\n> `Argument of type \'HTMLElement | null\' is not assignable to parameter of type \'Element | Document | DocumentFragment | Comment\'. Type \'null\' is not assignable to type \'Element | Document | DocumentFragment | Comment\'. TS2345`\\n\\n![a screenshot of the null is not assignable error](null_is_not_assignable-error.png)\\n\\nNow this is actually nothing to do with issues with our new React type definitions. They are fine. This is TypeScript saying \\"it\'s not guaranteed that `document.getElementById(\'root\')` returns something that is not `null`... since we\'re in `strictNullChecks` mode you need to be sure `root` is not null\\".\\n\\nWe\'ll deal with that by testing we do have an element in play before invoking `ReactDOM.createRoot`:\\n\\n```diff\\n-const root = ReactDOM.createRoot(document.getElementById(\'root\'));\\n+const rootElement = document.getElementById(\'root\');\\n+if (!rootElement) throw new Error(\'Failed to find the root element\');\\n+const root = ReactDOM.createRoot(rootElement);\\n```\\n\\nNow that change is made, we have a working React 18 application, using TypeScript. Enjoy!\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/how-to-use-typescript-with-react-18-alpha/)"},{"id":"/2021/06/11/azure-functions-dotnet-5-query-params-di-bicep","metadata":{"permalink":"/2021/06/11/azure-functions-dotnet-5-query-params-di-bicep","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-06-11-azure-functions-dotnet-5-query-params-di-bicep/index.md","source":"@site/blog/2021-06-11-azure-functions-dotnet-5-query-params-di-bicep/index.md","title":"Azure Functions and .NET 5: Query params, Dependency Injection, Bicep & Build","description":"The upgrade of Azure Functions from .NET Core 3.1 to .NET 5 is significant. This post shows part of the upgrade: Query params, Dependency Injection, Bicep & Build","date":"2021-06-11T00:00:00.000Z","formattedDate":"June 11, 2021","tags":[{"label":"Azure Functions","permalink":"/tags/azure-functions"},{"label":".NET 5","permalink":"/tags/net-5"},{"label":"querystring","permalink":"/tags/querystring"},{"label":"query params","permalink":"/tags/query-params"},{"label":"dependency injection","permalink":"/tags/dependency-injection"},{"label":"Bicep","permalink":"/tags/bicep"}],"readingTime":3.38,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Azure Functions and .NET 5: Query params, Dependency Injection, Bicep & Build","authors":"johnnyreilly","tags":["Azure Functions",".NET 5","querystring","query params","dependency injection","Bicep"],"image":"./title-image.png","description":"The upgrade of Azure Functions from .NET Core 3.1 to .NET 5 is significant. This post shows part of the upgrade: Query params, Dependency Injection, Bicep & Build","hide_table_of_contents":false},"prevItem":{"title":"React 18 and TypeScript","permalink":"/2021/06/30/react-18-and-typescript"},"nextItem":{"title":"Azurite and Table Storage in a dev container","permalink":"/2021/05/15/azurite-and-table-storage-dev-container"}},"content":"The upgrade of Azure Functions from .NET Core 3.1 to .NET 5 is significant. There\'s an excellent [guide](https://codetraveler.io/2021/05/28/creating-azure-functions-using-net-5/) for the general steps required to perform the upgrade. However there\'s a number of (unrelated) items which are not covered by that post:\\n\\n- Query params\\n- Dependency Injection\\n- Bicep\\n- Build\\n\\nThis post will show how to tackle these.\\n\\n![title image showing name of post and the Azure Functions logo](title-image.png)\\n\\n## Query params\\n\\nAs part of the move to .NET 5 functions, we say goodbye to [`HttpRequest`](https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.http.httprequest?view=aspnetcore-5.0) and hello to [`HttpRequestData`](https://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.functions.worker.http.httprequestdata?view=azure-dotnet). Now `HttpRequest` had a useful [`Query`](https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.http.httprequest.query?view=aspnetcore-5.0#Microsoft_AspNetCore_Http_HttpRequest_Query) property which allowed for the simple extraction of query parameters like so.\\n\\n```cs\\nvar from = req.Query[\\"from\\"]\\n```\\n\\n`HttpRequestData` has no such property. However, it\'s straightforward to make our own. It\'s simply a matter of using [`System.Web.HttpUtility.ParseQueryString`](https://docs.microsoft.com/en-us/dotnet/api/system.web.httputility.parsequerystring?view=net-5.0) on `req.Url.Query` and using that:\\n\\n```cs\\nvar query = System.Web.HttpUtility.ParseQueryString(req.Url.Query);\\nvar from = query[\\"from\\"]\\n```\\n\\n## Dependency Injection, local development and Azure Application Settings\\n\\nDependency Injection is a much more familiar shape in .NET 5 if you\'re familiar with .NET Core web apps. Once again we have a `Program.cs` file. To get the configuration built in such a way to support both local development and when deployed to Azure, there\'s a few things to do. When deployed to Azure you\'ll likely want to read from Azure Application Settings:\\n\\n![screenshot of Azure Application Settings](application-settings.png)\\n\\nTo tackle both of these, you\'ll want to use `AddJsonFile` and `AddEnvironmentVariables` in `ConfigureAppConfiguration`. A final `Program.cs` might look something like this:\\n\\n```cs\\nusing System;\\nusing System.Threading.Tasks;\\nusing Microsoft.Extensions.Configuration;\\nusing Microsoft.Extensions.DependencyInjection;\\nusing Microsoft.Extensions.Hosting;\\n\\nnamespace MyApp\\n{\\n    public class Program\\n    {\\n        public static Task Main(string[] args)\\n        {\\n            var host = new HostBuilder()\\n                .ConfigureAppConfiguration(configurationBuilder =>\\n                    configurationBuilder\\n                        .AddCommandLine(args)\\n                        // below is for local development\\n                        .AddJsonFile(\\"local.settings.json\\", optional: true, reloadOnChange: true)\\n                        // below is what you need to read Application Settings in Azure\\n                        .AddEnvironmentVariables()\\n                )\\n                .ConfigureFunctionsWorkerDefaults()\\n                .ConfigureServices(services =>\\n                {\\n                    services.AddLogging();\\n                    services.AddHttpClient();\\n                })\\n                .Build();\\n\\n            return host.RunAsync();\\n        }\\n    }\\n}\\n```\\n\\nWith this approach in place, when the application runs, it should construct a configuration driven by all the providers required to run our application.\\n\\n## Bicep\\n\\nWhen it comes to deploying to Azure via [Bicep](https://github.com/Azure/bicep), there\'s some small tweaks required:\\n\\n- `appSettings.FUNCTIONS_WORKER_RUNTIME` becomes `dotnet-isolated`\\n- `linuxFxVersion` becomes `DOTNET-ISOLATED|5.0`\\n\\nApplied to the resource itself the diff looks like this:\\n\\n```diff\\nresource functionAppName_resource \'Microsoft.Web/sites@2018-11-01\' = {\\n  name: functionAppName\\n  location: location\\n  tags: tags_var\\n  kind: \'functionapp,linux\'\\n  identity: {\\n    type: \'SystemAssigned\'\\n  }\\n  properties: {\\n    serverFarmId: appServicePlanName_resource.id\\n    siteConfig: {\\n      http20Enabled: true\\n      remoteDebuggingEnabled: false\\n      minTlsVersion: \'1.2\'\\n      appSettings: [\\n        {\\n          name: \'FUNCTIONS_EXTENSION_VERSION\'\\n          value: \'~3\'\\n        }\\n        {\\n          name: \'FUNCTIONS_WORKER_RUNTIME\'\\n-          value: \'dotnet\'\\n+          value: \'dotnet-isolated\'\\n        }\\n        {\\n          name: \'AzureWebJobsStorage\'\\n          value: \'DefaultEndpointsProtocol=https;AccountName=${storageAccountName};AccountKey=${listKeys(resourceId(\'Microsoft.Storage/storageAccounts\', storageAccountName), \'2019-06-01\').keys[0].value};EndpointSuffix=${environment().suffixes.storage}\'\\n        }\\n      ]\\n      connectionStrings: [\\n        {\\n          name: \'TableStorageConnectionString\'\\n          connectionString: \'DefaultEndpointsProtocol=https;AccountName=${storageAccountName};AccountKey=${listKeys(resourceId(\'Microsoft.Storage/storageAccounts\', storageAccountName), \'2019-06-01\').keys[0].value};EndpointSuffix=${environment().suffixes.storage}\'\\n        }\\n      ]\\n-      linuxFxVersion: \'DOTNETCORE|LTS\'\\n+      linuxFxVersion: \'DOTNET-ISOLATED|5.0\'\\n      ftpsState: \'Disabled\'\\n      managedServiceIdentityId: 1\\n    }\\n    clientAffinityEnabled: false\\n    httpsOnly: true\\n  }\\n}\\n```\\n\\n## Building .NET 5 functions\\n\\nBefore signing off, there\'s one more thing to slip in. When attempting to build .NET 5 Azure Functions with the .NET SDK _alone_, you\'ll encounter this error:\\n\\n```\\nThe framework \'Microsoft.NETCore.App\', version \'3.1.0\' was not found.\\n```\\n\\nDocs on this seem to be pretty short. The closest I came to docs was [this comment on Stack Overflow](https://stackoverflow.com/questions/66938752/net-5-the-framework-microsoft-netcore-app-version-3-1-0-was-not-found/66938753#66938753):\\n\\n> To build .NET 5 functions, the .NET Core 3 SDK is required. So this must be installed alongside the 5.0.x sdk.\\n\\nSo with Azure Pipelines you might have have something that looks like this:\\n\\n```yml\\nstages:\\n  - stage: build\\n    displayName: build\\n    pool:\\n      vmImage: \'ubuntu-latest\'\\n    jobs:\\n      - job: BuildAndTest\\n        displayName: \'Build and Test\'\\n        steps:\\n          # we need .NET Core SDK 3.1 too!\\n          - task: UseDotNet@2\\n            displayName: \'Install .NET Core SDK 3.1\'\\n            inputs:\\n              packageType: \'sdk\'\\n              version: 3.1.x\\n\\n          - task: UseDotNet@2\\n            displayName: \'Install .NET SDK 5.0\'\\n            inputs:\\n              packageType: \'sdk\'\\n              version: 5.0.x\\n\\n          - task: DotNetCoreCLI@2\\n            displayName: \'function app test\'\\n            inputs:\\n              command: test\\n\\n          - task: DotNetCoreCLI@2\\n            displayName: \'function app build\'\\n            inputs:\\n              command: build\\n              arguments: \'--configuration Release --output $(Build.ArtifactStagingDirectory)/MyApp\'\\n\\n          - task: DotNetCoreCLI@2\\n            displayName: \'function app publish\'\\n            inputs:\\n              command: publish\\n              arguments: \'--configuration Release --output $(Build.ArtifactStagingDirectory)/MyApp /p:SourceRevisionId=$(Build.SourceVersion)\'\\n              publishWebProjects: false\\n              modifyOutputPath: false\\n              zipAfterPublish: true\\n\\n          - publish: $(Build.ArtifactStagingDirectory)/MyApp\\n            artifact: functionapp\\n```\\n\\nHave fun building .NET 5 functions!"},{"id":"/2021/05/15/azurite-and-table-storage-dev-container","metadata":{"permalink":"/2021/05/15/azurite-and-table-storage-dev-container","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-05-15-azurite-and-table-storage-dev-container/index.md","source":"@site/blog/2021-05-15-azurite-and-table-storage-dev-container/index.md","title":"Azurite and Table Storage in a dev container","description":"It\'s great to be able to develop locally without needing a \\"real\\" database to connect to. Azurite is an Azure Storage emulator which exists to support just that. This post demonstrates how to run Azurite v3 in a dev container, such that you can access the Table Storage API, which is currently in preview.","date":"2021-05-15T00:00:00.000Z","formattedDate":"May 15, 2021","tags":[{"label":"Azurite","permalink":"/tags/azurite"},{"label":"Azure Table Storage","permalink":"/tags/azure-table-storage"},{"label":"VS Code","permalink":"/tags/vs-code"},{"label":"dev container","permalink":"/tags/dev-container"},{"label":"devcontainer","permalink":"/tags/devcontainer"},{"label":"Docker","permalink":"/tags/docker"}],"readingTime":6.505,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Azurite and Table Storage in a dev container","authors":"johnnyreilly","tags":["Azurite","Azure Table Storage","VS Code","dev container","devcontainer","Docker"],"image":"./dev-container-start.gif","hide_table_of_contents":false},"prevItem":{"title":"Azure Functions and .NET 5: Query params, Dependency Injection, Bicep & Build","permalink":"/2021/06/11/azure-functions-dotnet-5-query-params-di-bicep"},"nextItem":{"title":"Create a Pipeline with the Azure DevOps API","permalink":"/2021/05/08/create-pipeline-with-azure-devops-api"}},"content":"It\'s great to be able to develop locally without needing a \\"real\\" database to connect to. [Azurite](https://github.com/Azure/Azurite) is an Azure Storage emulator which exists to support just that. This post demonstrates how to run Azurite v3 in a [dev container](https://code.visualstudio.com/docs/remote/containers), such that you can access the Table Storage API, which is currently in preview.\\n\\n## Azurite in VS Code\\n\\n[Azurite v3.12.0](https://github.com/Azure/Azurite/releases/tag/v3.12.0) recently shipped, and with it came:\\n\\n> Preview of Table Service in npm package and docker image. (Visual Studio Code extension doesn\'t support Table Service in this release)\\n\\nYou\'ll note that whilst there\'s a VS Code extension for Azurite, it doesn\'t have support for the Table Service yet. However, we do have it available in the form of a Docker image. So whilst we may not be able to directly use the Table APIs of Azurite in VS Code, what we could do instead is use a dev container.\\n\\nWe\'ll start by making ourselves a new directory and open VS Code in that location:\\n\\n```bash\\nmkdir azurite-devcontainer\\ncode azurite-devcontainer\\n```\\n\\nWe\'re going to initialise a dev container there for function apps based upon [the example Azure Functions & C# - .NET Core 3.1 container](https://github.com/microsoft/vscode-dev-containers/tree/master/containers/azure-functions-dotnetcore-3.1). We\'ll use it later to test our Azurite connectivity. To do that let\'s create ourselves a `.devcontainer` directory:\\n\\n```bash\\nmkdir .devcontainer\\n```\\n\\nAnd inside there we\'ll add a `devcontainer.json`:\\n\\n```json\\n// For format details, see https://aka.ms/devcontainer.json. For config options, see the README at:\\n// https://github.com/microsoft/vscode-dev-containers/tree/v0.177.0/containers/azure-functions-dotnetcore-3.1\\n{\\n  \\"name\\": \\"Azurite and Azure Functions & C# - .NET Core 3.1\\",\\n  \\"dockerComposeFile\\": \\"docker-compose.yml\\",\\n  \\"service\\": \\"app\\",\\n  \\"workspaceFolder\\": \\"/workspace\\",\\n  \\"forwardPorts\\": [7071],\\n\\n  // Set *default* container specific settings.json values on container create.\\n  \\"settings\\": {\\n    \\"terminal.integrated.defaultProfile.linux\\": \\"/bin/bash\\"\\n  },\\n\\n  // Add the IDs of extensions you want installed when the container is created.\\n  \\"extensions\\": [\\n    \\"ms-azuretools.vscode-azurefunctions\\",\\n    \\"ms-dotnettools.csharp\\"\\n  ],\\n\\n  // Use \'postCreateCommand\' to run commands after the container is created.\\n  // \\"postCreateCommand\\": \\"dotnet restore\\",\\n\\n  // Comment out connect as root instead. More info: https://aka.ms/vscode-remote/containers/non-root.\\n  \\"remoteUser\\": \\"vscode\\"\\n}\\n```\\n\\nAs we can see, we\'re referencing a `docker-compose.yml` file; let\'s add that:\\n\\n```yml\\nversion: \'3\'\\n\\nservices:\\n  app:\\n    build:\\n      context: .\\n      dockerfile: Dockerfile\\n      args:\\n        # On Linux, you may need to update USER_UID and USER_GID below if not your local UID is not 1000.\\n        USER_UID: 10000\\n        USER_GID: 10000\\n\\n    init: true\\n    volumes:\\n      - ..:/workspace:cached\\n\\n    # Overrides default command so things don\'t shut down after the process ends.\\n    command: sleep infinity\\n\\n    # Uncomment the next line to use a non-root user for all processes.\\n    user: vscode\\n\\n  # run azurite and expose the relevant ports\\n  azurite:\\n    image: ./\'mcr.microsoft.com/azure-storage/azurite\'\\n    ports:\\n      - \'10000:10000\'\\n      - \'10001:10001\'\\n      - \'10002:10002\'\\n```\\n\\nIt consists of two services; `app` and `azurite`. `azurite` is the Docker image of Azurite, which exposes the Azurite ports so `app` can access it. Note the name of `azurite`; that will turn out to be significant later. We\'re actually only going to use the Table Storage port of `10002`, but this would allow us to use Blobs and Queues also. The `azurite` service is effectively going to be executing this command for us when it runs:\\n\\n```bash\\ndocker run -p 10000:10000 -p 10001:10001 -p 10002:10002 mcr.microsoft.com/azure-storage/azurite\\n```\\n\\nNow let\'s look at `app`. This is our Azure Functions container. It references a `Dockerfile` which we need to add:\\n\\n```dockerfile\\n# Find the Dockerfile for mcr.microsoft.com/azure-functions/dotnet:3.0-dotnet3-core-tools at this URL\\n# https://github.com/Azure/azure-functions-docker/blob/master/host/3.0/buster/amd64/dotnet/dotnet-core-tools.Dockerfile\\nFROM mcr.microsoft.com/azure-functions/dotnet:3.0-dotnet3-core-tools\\n```\\n\\nWe now have ourselves a dev container! VS Code should prompt us to reopen inside the container:\\n\\n![The dev container starting](dev-container-start.gif)\\n\\n## Make a function app\\n\\nNow we\'re inside our container, we\'re going to make ourselves a function app that will use Azurite. Let\'s fire up the terminal in VS Code and create a function app containing a simple HTTP function:\\n\\n```bash\\nmkdir src\\ncd src\\nfunc init TableStorage --dotnet\\ncd TableStorage\\n```\\n\\nWe need to add a package for the APIs which interact with Table Storage:\\n\\n```bash\\ndotnet restore\\ndotnet add package Microsoft.Azure.Cosmos.Table --version 1.0.8\\n```\\n\\nThe name is somewhat misleading, as it\'s both for Cosmos _and_ for Table Storage. Famously, naming things is hard \ud83d\ude09.\\n\\nOur mission is to be able to write and read from Azurite Table Storage. We need something to read and write that we care about. I like to visit [Kew Gardens](https://www.kew.org/kew-gardens) and so let\'s imagine ourselves a system which tracks visitors to Kew.\\n\\nWe\'re going to add a class called `KewGardensVisit`:\\n\\n```cs\\nusing System;\\nusing Microsoft.Azure.Cosmos.Table;\\n\\nnamespace TableStorage\\n{\\n    public class KewGardenVisit : TableEntity\\n    {\\n        public KewGardenVisit() {}\\n        public KewGardenVisit(DateTime arrivedAt, string memberId)\\n        {\\n            PartitionKey = arrivedAt.ToString(\\"yyyy-MM-dd\\", System.Globalization.CultureInfo.InvariantCulture);\\n            RowKey = memberId;\\n\\n            ArrivedAt = arrivedAt;\\n        }\\n\\n        public DateTime ArrivedAt { get; set; }\\n    }\\n}\\n```\\n\\nNow we have our entity, let\'s add a class called `HelloAzuriteTableStorage` which will contain functions which interact with the storage:\\n\\n```cs\\nusing System;\\nusing System.Linq;\\nusing System.Threading.Tasks;\\nusing Microsoft.AspNetCore.Mvc;\\nusing Microsoft.Azure.WebJobs;\\nusing Microsoft.Azure.WebJobs.Extensions.Http;\\nusing Microsoft.AspNetCore.Http;\\nusing Microsoft.Extensions.Logging;\\nusing Microsoft.Azure.Cosmos.Table;\\n\\nnamespace TableStorage\\n{\\n    public static class HelloAzuriteTableStorage\\n    {\\n        // Note how we\'re addressing our azurite service\\n        const string AZURITE_TABLESTORAGE_CONNECTIONSTRING =\\n            \\"DefaultEndpointsProtocol=http;\\" +\\n            \\"AccountName=devstoreaccount1;\\" +\\n            \\"AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;\\" +\\n            \\"BlobEndpoint=http://azurite:10000/devstoreaccount1;\\" +\\n            \\"QueueEndpoint=http://azurite:10001/devstoreaccount1;\\" +\\n            \\"TableEndpoint=http://azurite:10002/devstoreaccount1;\\";\\n        const string TABLE_NAME = \\"KewGardenVisits\\";\\n\\n        [FunctionName(\\"SaveVisit\\")]\\n        public static async Task<IActionResult> RunSaveVisit(\\n            [HttpTrigger(AuthorizationLevel.Anonymous, \\"get\\", \\"post\\", Route = null)] HttpRequest req,\\n            ILogger log)\\n        {\\n            try\\n            {\\n                var table = await GetTable(log);\\n\\n                // Create the InsertOrReplace table operation\\n                var insertOrMergeOperation = TableOperation.InsertOrMerge(new KewGardenVisit(\\n                    arrivedAt: DateTime.UtcNow,\\n                    memberId: \\"JR123456743921\\"\\n                ));\\n\\n                // Execute the operation.\\n                TableResult result = await table.ExecuteAsync(insertOrMergeOperation);\\n                KewGardenVisit savedTelemetry = result.Result as KewGardenVisit;\\n\\n                if (result.RequestCharge.HasValue)\\n                    log.LogInformation(\\"Request Charge of InsertOrMerge Operation: \\" + result.RequestCharge);\\n\\n                return new OkObjectResult(savedTelemetry);\\n\\n            }\\n            catch (Exception e)\\n            {\\n                log.LogError(e, \\"Problem saving\\");\\n            }\\n\\n            return new BadRequestObjectResult(\\"There was an issue\\");\\n        }\\n\\n        [FunctionName(\\"GetTodaysVisits\\")]\\n        public static async Task<IActionResult> RunGetTodaysVisits(\\n            [HttpTrigger(AuthorizationLevel.Anonymous, \\"get\\", Route = null)] HttpRequest req,\\n            ILogger log)\\n        {\\n            try\\n            {\\n                var table = await GetTable(log);\\n\\n                var snowOnTheAdoTelemetries = table.CreateQuery<KewGardenVisit>()\\n                    .Where(x => x.PartitionKey == DateTime.UtcNow.ToString(\\"yyyy-MM-dd\\", System.Globalization.CultureInfo.InvariantCulture))\\n                    .ToArray();\\n\\n                return new OkObjectResult(snowOnTheAdoTelemetries);\\n\\n            }\\n            catch (Exception e)\\n            {\\n                log.LogError(e, \\"Problem loading\\");\\n                return new BadRequestObjectResult(\\"There was an issue\\");\\n            }\\n        }\\n\\n        private static async Task<CloudTable> GetTable(ILogger log)\\n        {\\n            // Construct a new TableClient using a TableSharedKeyCredential.\\n            var storageAccount = CloudStorageAccount.Parse(AZURITE_TABLESTORAGE_CONNECTIONSTRING); ;\\n\\n            // Create a table client for interacting with the table service\\n            CloudTableClient tableClient = storageAccount.CreateCloudTableClient(new TableClientConfiguration());\\n\\n            // Create a table client for interacting with the table service\\n            CloudTable table = tableClient.GetTableReference(TABLE_NAME);\\n            if (await table.CreateIfNotExistsAsync())\\n                log.LogInformation(\\"Created Table named: {0}\\", TABLE_NAME);\\n            else\\n                log.LogInformation(\\"Table {0} already exists\\", TABLE_NAME);\\n\\n            return table;\\n        }\\n    }\\n}\\n```\\n\\nThere\'s a couple of things to draw attention to here:\\n\\n- `AZURITE_TABLESTORAGE_CONNECTIONSTRING` - this mega string is based upon the [Azurite connection string docs](https://github.com/Azure/Azurite#connection-strings). The account name and key are the [Azurite default storage accounts](https://github.com/Azure/Azurite#default-storage-account). You\'ll note we target `TableEndpoint=http://azurite:10002/devstoreaccount1`. The `azurite` here is replacing the standard `127.0.0.1` where Azurite typically listens. This `azurite` name comes from the name of our service in the `docker-compose.yml` file.\\n\\n- We\'re creating two functions `SaveVisit` and `GetTodaysVisits`. `SaveVisit` creates an entry in our storage to represent someone\'s visit. It\'s a hardcoded value representing me, and we\'re exposing a write operation at a `GET` endpoint which is not very RESTful. But this is a demo and Roy Fielding would forgive us. `GetTodaysVisits` allows us to read back the visits that have happened today.\\n\\nLet\'s see if it works by entering `func start` and browsing to `http://localhost:7071/api/savevisit`\\n\\n![a screenshot of the response from the savevisits endpoint](savevisits.png)\\n\\nLooking good. Now let\'s see if we can query them at `http://localhost:7071/api/gettodaysvisits`:\\n\\n![a screenshot of the response from the gettodaysvisits endpoint](gettodaysvisits.png)\\n\\nDisco.\\n\\n## Can we swap out Azurite for The Real Thing\u2122\ufe0f?\\n\\nYou may be thinking _\\"This is great! But in the end I need to write to Azure Table Storage itself; not Azurite.\\"_\\n\\nThat\'s a fair point. Fortunately, it\'s only the connection string that determines where you read and write to. It would be fairly easy to dependency inject the appropriate connection string, or indeed a service that is connected to the storage you wish to target. If you want to make that happen, you can."},{"id":"/2021/05/08/create-pipeline-with-azure-devops-api","metadata":{"permalink":"/2021/05/08/create-pipeline-with-azure-devops-api","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-05-08-create-pipeline-with-azure-devops-api/index.md","source":"@site/blog/2021-05-08-create-pipeline-with-azure-devops-api/index.md","title":"Create a Pipeline with the Azure DevOps API","description":"Creating an Azure Pipeline using the Azure DevOps REST API is possible, but badly documented. This post goes through how to do this.","date":"2021-05-08T00:00:00.000Z","formattedDate":"May 8, 2021","tags":[{"label":"Azure Pipelines","permalink":"/tags/azure-pipelines"},{"label":"Azure DevOps API","permalink":"/tags/azure-dev-ops-api"}],"readingTime":1.745,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Create a Pipeline with the Azure DevOps API","authors":"johnnyreilly","tags":["Azure Pipelines","Azure DevOps API"],"image":"./new-pipeline.png","hide_table_of_contents":false},"prevItem":{"title":"Azurite and Table Storage in a dev container","permalink":"/2021/05/15/azurite-and-table-storage-dev-container"},"nextItem":{"title":"Blog Archive for Docusaurus","permalink":"/2021/05/01/blog-archive-for-docusaurus"}},"content":"Creating an Azure Pipeline using the Azure DevOps REST API is possible, but badly documented. This post goes through how to do this.\\n\\n## curling a pipeline\\n\\nThe [documentation](https://docs.microsoft.com/en-us/rest/api/azure/devops/pipelines/pipelines/create?view=azure-devops-rest-6.1) for creating an Azure Pipeline using the Azure DevOps API is somewhat lacking. However it isn\'t actually too hard, you just need the recipe.\\n\\nHere\'s a curl to make you a pipeline:\\n\\n```bash\\ncurl  --user \'\':\'PERSONAL_ACCESS_TOKEN\' --header \\"Content-Type: application/json\\" --header \\"Accept:application/json\\" https://dev.azure.com/organisation-name/sandbox/_apis/pipelines?api-version=6.1-preview.1 -d @makepipeline.json\\n```\\n\\nLooking at the above there\'s two things you need:\\n\\n1. A personal access token. You can make one of those here: https://dev.azure.com/organisation-name/_usersSettings/tokens (where `organisation-name` is the name of your organisation)\\n2. A `makepipeline.json` file, which contains the details of the pipeline you want to create:\\n\\n```json\\n{\\n  \\"folder\\": null,\\n  \\"name\\": \\"pipeline-made-by-api\\",\\n  \\"configuration\\": {\\n    \\"type\\": \\"yaml\\",\\n    \\"path\\": \\"/azure-pipelines.yml\\",\\n    \\"repository\\": {\\n      \\"id\\": \\"guid-of-repo-id\\",\\n      \\"name\\": \\"my-repo\\",\\n      \\"type\\": \\"azureReposGit\\"\\n    }\\n  }\\n}\\n```\\n\\nLet\'s talk through the significant properties above:\\n\\n- `folder` - can be `null` if you\'d like the pipeline to be created in the root of Pipelines; otherwise provide the folder name. Incidentally a `null` will be translated into a value of `\\\\\\\\` which appears to be the magic value which represents the root.\\n- `name` - your pipeline needs a name\\n- `path` - this is the path to the yaml pipelines file in the repo. Note we\'re creating the pipeline itself here; what\'s actually in the pipeline sits in that file.\\n- `repository.id` - this is the guid that represents the repo you\'re creating the pipeline for. You can find this out by going to your equivalent https://dev.azure.com/organisation-name/project-name/_settings/repositories (substituting in appropriate values) and looking up your repository there.\\n- `repository.name` - the name of your repo\\n\\nWhen you execute your curl you should be returned some JSON along these lines:\\n\\n```json\\n{\\n  \\"_links\\": {\\n    \\"self\\": {\\n      \\"href\\": \\"https://dev.azure.com/organisation-name/2184049d-8bc4-484a-91e6-00fca6b5b19f/_apis/pipelines/975?revision=1\\"\\n    },\\n    \\"web\\": {\\n      \\"href\\": \\"https://dev.azure.com/organisation-name/2184049d-8bc4-484a-91e6-00fca6b5b19f/_build/definition?definitionId=975\\"\\n    }\\n  },\\n  \\"configuration\\": {\\n    \\"path\\": \\"/azure-pipelines.yml\\",\\n    \\"repository\\": {\\n      \\"id\\": \\"9a72560d-1622-4016-93dd-32ac85b96d03\\",\\n      \\"type\\": \\"azureReposGit\\"\\n    },\\n    \\"type\\": \\"yaml\\"\\n  },\\n  \\"url\\": \\"https://dev.azure.com/organisation-name/2184049d-8bc4-484a-91e6-00fca6b5b19f/_apis/pipelines/975?revision=1\\",\\n  \\"id\\": 975,\\n  \\"revision\\": 1,\\n  \\"name\\": \\"pipeline-made-by-api\\",\\n  \\"folder\\": \\"\\\\\\\\\\"\\n}\\n```\\n\\nAnd inside Azure DevOps you\'ll now have a shiny new pipeline:\\n\\n![The new pipeline](new-pipeline.png)"},{"id":"/2021/05/01/blog-archive-for-docusaurus","metadata":{"permalink":"/2021/05/01/blog-archive-for-docusaurus","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-05-01-blog-archive-for-docusaurus/index.md","source":"@site/blog/2021-05-01-blog-archive-for-docusaurus/index.md","title":"Blog Archive for Docusaurus","description":"Docusaurus doesn\'t ship with \\"blog archive\\" functionality. By which I mean, something that allows you to look at an overview of your historic blog posts. It turns out it is fairly straightforward to implement your own. This post does just that.","date":"2021-05-01T00:00:00.000Z","formattedDate":"May 1, 2021","tags":[{"label":"Docusaurus","permalink":"/tags/docusaurus"},{"label":"blog archive","permalink":"/tags/blog-archive"},{"label":"webpack","permalink":"/tags/webpack"}],"readingTime":5.38,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Blog Archive for Docusaurus","authors":"johnnyreilly","tags":["Docusaurus","blog archive","webpack"],"image":"./docusaurus-blog-archive.png","hide_table_of_contents":false},"prevItem":{"title":"Create a Pipeline with the Azure DevOps API","permalink":"/2021/05/08/create-pipeline-with-azure-devops-api"},"nextItem":{"title":"The Service Now API and TypeScript Conditional Types","permalink":"/2021/04/24/service-now-api-and-typescript-conditional-types"}},"content":"Docusaurus doesn\'t ship with \\"blog archive\\" functionality. By which I mean, something that allows you to look at an overview of your historic blog posts. It turns out it is fairly straightforward to implement your own. This post does just that.\\n\\n![Docusaurus blog archive](docusaurus-blog-archive.png)\\n\\n## Update 2021-09-01\\n\\nAs of [v2.0.0-beta.6](https://github.com/facebook/docusaurus/releases/tag/v2.0.0-beta.6), Docusauras _does_ ship with blog archive functionality that lives at the `archive` route. This is down to the work of [Gabriel Csapo](https://github.com/gabrielcsapo) in [this PR](https://github.com/facebook/docusaurus/pull/5428).\\n\\nIf you\'d like to know how to build your own, read on... But you may not need to!\\n\\n## Blogger\'s blog archive\\n\\nI recently went through the exercise of [migrating my blog from Blogger to Docusaurus](./2021-03-15-from-blogger-to-docusaurus/index.md). I found that [Docusaurus](https://docusaurus.io/) was a tremendous platform upon which to build a blog, but it was missing a feature from Blogger that I valued highly; the blog archive:\\n\\n![Blogger blog archive](blogger-blog-archive-small.png)\\n\\nThe blog archive is a way by which you can browse through your historic blog posts. A place where you can see all that you\'ve written and when. I find this very helpful. I didn\'t really want to make the jump without having something like that around.\\n\\n## Handrolling a Docusaurus blog archive\\n\\nLet\'s create our own blog archive in the land of the Docusaurus.\\n\\nWe\'ll create a new page under the `pages` directory called `blog-archive.js` and we\'ll add a link to it in our `docusaurus.config.js`:\\n\\n```json\\n    navbar: {\\n      // ...\\n      items: [\\n        // ...\\n        { to: \\"blog-archive\\", label: \\"Blog Archive\\", position: \\"left\\" },\\n        // ...\\n      ],\\n    },\\n```\\n\\n## Obtaining the blog data\\n\\nThis page will be powered by webpack\'s [`require.context`](https://webpack.js.org/guides/dependency-management/#requirecontext) function. `require.context` allows us to use webpack to obtain all of the blog modules:\\n\\n```js\\nrequire.context(\'../../blog\', false, //index.md/);\\n```\\n\\nThe code snippet above looks in the `blog` directory for files / modules ending with the suffix `\\"/index.md\\"`. Each one of these represents a blog post. The function returns a `context` object, which contains all of the data about these modules.\\n\\nBy reducing over that data we can construct an array of objects called `allPosts` that could drive a blog archive screen. Let\'s do this below, and we\'ll use [TypeScripts JSDoc support](https://www.typescriptlang.org/docs/handbook/jsdoc-supported-types.html) to type our JavaScript:\\n\\n```tsx\\n/**\\n * @typedef {Object} BlogPost - creates a new type named \'BlogPost\'\\n * @property {string} date - eg \\"2021-04-24T00:00:00.000Z\\"\\n * @property {string} formattedDate - eg \\"April 24, 2021\\"\\n * @property {string} title - eg \\"The Service Now API and TypeScript Conditional Types\\"\\n * @property {string} permalink - eg \\"/2021/04/24/service-now-api-and-typescript-conditional-types\\"\\n */\\n\\n/** @type {BlogPost[]} */\\nconst allPosts = ((ctx) => {\\n  /** @type {string[]} */\\n  const blogpostNames = ctx.keys();\\n\\n  return blogpostNames.reduce(\\n    (blogposts, blogpostName, i) => {\\n      const module = ctx(blogpostName);\\n      const { date, formattedDate, title, permalink } = module.metadata;\\n      return [\\n        ...blogposts,\\n        {\\n          date,\\n          formattedDate,\\n          title,\\n          permalink,\\n        },\\n      ];\\n    },\\n    /** @type {string[]}>} */ []\\n  );\\n})(require.context(\'../../blog\', true, /index.md/));\\n```\\n\\nObserve the `metadata` property in the screenshot below:\\n\\n![require.context](require.context.png)\\n\\nThis gives us a flavour of the data available in the modules and shows how we pull out the bits that we need; `date`, `formattedDate`, `title` and `permalink`.\\n\\n## Presenting it\\n\\nNow we have our data in the form of `allPosts`, let\'s display it. We\'d like to break it up into posts by year, which we can do by reducing and looking at the `date` property which is an ISO-8601 style date string taking a format that begins `yyyy-mm-dd`:\\n\\n```tsx\\nconst postsByYear = allPosts.reduceRight((posts, post) => {\\n  const year = post.date.split(\'-\')[0];\\n  const yearPosts = posts.get(year) || [];\\n  return posts.set(year, [post, ...yearPosts]);\\n}, /** @type {Map<string, BlogPost[]>}>} */ new Map());\\n\\nconst yearsOfPosts = Array.from(postsByYear, ([year, posts]) => ({\\n  year,\\n  posts,\\n}));\\n```\\n\\nNow we\'re ready to blast it onto the screen. We\'ll create two components:\\n\\n- `Year` - which is a list of the posts for a given year and\\n- `BlogArchive` - which is the overall page and maps over `yearsOfPosts` to render `Year`s\\n\\n```tsx\\nfunction Year(\\n  /** @type {{ year: string; posts: BlogPost[]}} */ { year, posts }\\n) {\\n  return (\\n    <div className={clsx(\'col col--4\', styles.feature)}>\\n      <h3>{year}</h3>\\n      <ul>\\n        {posts.map((post) => (\\n          <li key={post.date}>\\n            <Link to={post.permalink}>\\n              {post.formattedDate} - {post.title}\\n            </Link>\\n          </li>\\n        ))}\\n      </ul>\\n    </div>\\n  );\\n}\\n\\nfunction BlogArchive() {\\n  return (\\n    <Layout title=\\"Blog Archive\\">\\n      <header className={clsx(\'hero hero--primary\', styles.heroBanner)}>\\n        <div className=\\"container\\">\\n          <h1 className=\\"hero__title\\">Blog Archive</h1>\\n          <p className=\\"hero__subtitle\\">Historic posts</p>\\n        </div>\\n      </header>\\n      <main>\\n        {yearsOfPosts && yearsOfPosts.length > 0 && (\\n          <section className={styles.features}>\\n            <div className=\\"container\\">\\n              <div className=\\"row\\">\\n                {yearsOfPosts.map((props, idx) => (\\n                  <Year key={idx} {...props} />\\n                ))}\\n              </div>\\n            </div>\\n          </section>\\n        )}\\n      </main>\\n    </Layout>\\n  );\\n}\\n```\\n\\n## Bringing it all together\\n\\nWe\'re finished! We have a delightful looking blog archive plumbed into our blog:\\n\\n![Docusaurus blog archive](docusaurus-blog-archive.png)\\n\\nIt is possible that a blog archive may become natively available in Docusaurus in future. If you\'re interested in this, you can track [this issue](https://github.com/facebook/docusaurus/issues/4431).\\n\\nHere\'s the final code - which you can see [powering this screen](https://blog.johnnyreilly.com/blog-archive). And you can see the code that backs it [here](https://github.com/johnnyreilly/blog.johnnyreilly.com/blob/main/blog-website/src/pages/blog-archive.js):\\n\\n```tsx\\nimport React from \'react\';\\nimport clsx from \'clsx\';\\nimport Layout from \'@theme/Layout\';\\nimport Link from \'@docusaurus/Link\';\\nimport styles from \'./styles.module.css\';\\n\\n/**\\n * @typedef {Object} BlogPost - creates a new type named \'BlogPost\'\\n * @property {string} date - eg \\"2021-04-24T00:00:00.000Z\\"\\n * @property {string} formattedDate - eg \\"April 24, 2021\\"\\n * @property {string} title - eg \\"The Service Now API and TypeScript Conditional Types\\"\\n * @property {string} permalink - eg \\"/2021/04/24/service-now-api-and-typescript-conditional-types\\"\\n */\\n\\n/** @type {BlogPost[]} */\\nconst allPosts = ((ctx) => {\\n  /** @type {string[]} */\\n  const blogpostNames = ctx.keys();\\n\\n  return blogpostNames.reduce(\\n    (blogposts, blogpostName, i) => {\\n      const module = ctx(blogpostName);\\n      const { date, formattedDate, title, permalink } = module.metadata;\\n      return [\\n        ...blogposts,\\n        {\\n          date,\\n          formattedDate,\\n          title,\\n          permalink,\\n        },\\n      ];\\n    },\\n    /** @type {BlogPost[]}>} */ []\\n  );\\n  // @ts-ignore\\n})(require.context(\'../../blog\', true, /index.md/));\\n\\nconst postsByYear = allPosts.reduceRight((posts, post) => {\\n  const year = post.date.split(\'-\')[0];\\n  const yearPosts = posts.get(year) || [];\\n  return posts.set(year, [post, ...yearPosts]);\\n}, /** @type {Map<string, BlogPost[]>}>} */ new Map());\\n\\nconst yearsOfPosts = Array.from(postsByYear, ([year, posts]) => ({\\n  year,\\n  posts,\\n}));\\n\\nfunction Year(\\n  /** @type {{ year: string; posts: BlogPost[]}} */ { year, posts }\\n) {\\n  return (\\n    <div className={clsx(\'col col--4\', styles.feature)}>\\n      <h3>{year}</h3>\\n      <ul>\\n        {posts.map((post) => (\\n          <li key={post.date}>\\n            <Link to={post.permalink}>\\n              {post.formattedDate} - {post.title}\\n            </Link>\\n          </li>\\n        ))}\\n      </ul>\\n    </div>\\n  );\\n}\\n\\nfunction BlogArchive() {\\n  return (\\n    <Layout title=\\"Blog Archive\\">\\n      <header className={clsx(\'hero hero--primary\', styles.heroBanner)}>\\n        <div className=\\"container\\">\\n          <h1 className=\\"hero__title\\">Blog Archive</h1>\\n          <p className=\\"hero__subtitle\\">Historic posts</p>\\n        </div>\\n      </header>\\n      <main>\\n        {yearsOfPosts && yearsOfPosts.length > 0 && (\\n          <section className={styles.features}>\\n            <div className=\\"container\\">\\n              <div className=\\"row\\">\\n                {yearsOfPosts.map((props, idx) => (\\n                  <Year key={idx} {...props} />\\n                ))}\\n              </div>\\n            </div>\\n          </section>\\n        )}\\n      </main>\\n    </Layout>\\n  );\\n}\\n\\nexport default BlogArchive;\\n```"},{"id":"/2021/04/24/service-now-api-and-typescript-conditional-types","metadata":{"permalink":"/2021/04/24/service-now-api-and-typescript-conditional-types","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-04-24-service-now-api-and-typescript-conditional-types/index.md","source":"@site/blog/2021-04-24-service-now-api-and-typescript-conditional-types/index.md","title":"The Service Now API and TypeScript Conditional Types","description":"The Service Now REST API is an API which allows you to interact with Service Now. It produces different shaped results based upon the sysparmdisplayvalue query parameter. This post looks at how we can model these API results with TypeScripts conditional types. The aim being to minimise repetition whilst remaining strongly typed. This post is specifically about the Service Now API, but the principles around conditional type usage are generally applicable.","date":"2021-04-24T00:00:00.000Z","formattedDate":"April 24, 2021","tags":[{"label":"Service Now","permalink":"/tags/service-now"},{"label":"Table API","permalink":"/tags/table-api"},{"label":"Change Request","permalink":"/tags/change-request"},{"label":"sysparm_display_value","permalink":"/tags/sysparm-display-value"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"conditional types","permalink":"/tags/conditional-types"}],"readingTime":7.75,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"The Service Now API and TypeScript Conditional Types","authors":"johnnyreilly","tags":["Service Now","Table API","Change Request","change_request","sysparm_display_value","TypeScript","conditional types"],"image":"./ts-ervice-now.png","hide_table_of_contents":false},"prevItem":{"title":"Blog Archive for Docusaurus","permalink":"/2021/05/01/blog-archive-for-docusaurus"},"nextItem":{"title":"ts-loader goes webpack 5","permalink":"/2021/04/20/ts-loader-goes-webpack-5"}},"content":"The [Service Now REST API](https://docs.servicenow.com/bundle/paris-application-development/page/build/applications/concept/api-rest.html) is an API which allows you to interact with Service Now. It produces different shaped results based upon the [`sysparm_display_value` query parameter](https://docs.servicenow.com/bundle/paris-application-development/page/integrate/inbound-rest/concept/c_TableAPI.html#c_TableAPI__table-GET). This post looks at how we can model these API results with TypeScripts conditional types. The aim being to minimise repetition whilst remaining strongly typed. This post is specifically about the Service Now API, but the principles around conditional type usage are generally applicable.\\n\\n![Service Now and TypeScript](ts-ervice-now.png)\\n\\n## The power of a query parameter\\n\\nThere is a query parameter which many endpoints in Service Nows Table API support named `sysparm_display_value`. The docs describe it thus:\\n\\n> Data retrieval operation for reference and choice fields.\\n> Based on this value, retrieves the display value and/or the actual value from the database.\\n>\\n> Valid values:\\n>\\n> - `true`: Returns the display values for all fields.\\n> - `false`: Returns the actual values from the database.\\n> - `all`: Returns both actual and display value\\n\\nLet\'s see what that looks like when it comes to loading a Change Request. Consider the following curls:\\n\\n```shell\\n# sysparm_display_value=all\\ncurl \\"https://ourcompanyinstance.service-now.com/api/now/table/change_request?sysparm_query=number=CHG0122585&sysparm_limit=1&sysparm_display_value=all\\" --request GET --header \\"Accept:application/json\\" --user \'API_USERNAME\':\'API_PASSWORD\' | jq \'.result[0] | { state, sys_id, number, requested_by, reason }\'\\n\\n# sysparm_display_value=true\\ncurl \\"https://ourcompanyinstance.service-now.com/api/now/table/change_request?sysparm_query=number=CHG0122585&sysparm_limit=1&sysparm_display_value=true\\" --request GET --header \\"Accept:application/json\\" --user \'API_USERNAME\':\'API_PASSWORD\' | jq \'.result[0] | { state, sys_id, number, requested_by, reason }\'\\n\\n# sysparm_display_value=false\\ncurl \\"https://ourcompanyinstance.service-now.com/api/now/table/change_request?sysparm_query=number=CHG0122585&sysparm_limit=1&sysparm_display_value=false\\" --request GET --header \\"Accept:application/json\\" --user \'API_USERNAME\':\'API_PASSWORD\' | jq \'.result[0] | { state, sys_id, number, requested_by, reason }\'\\n```\\n\\nWhen executed, they each load the same Change Request from Service Now with a different value for `sysparm_display_value`. You\'ll notice there\'s some [`jq`](https://stedolan.github.io/jq/) in the mix as well. This is because there\'s a _lot_ of data in a Change Request. Rather than display everything, we\'re displaying a subset of fields. The first curl has a `sysparm_display_value` value of `all`, the second `false` and the third `true`. What do the results look like?\\n\\n`sysparm_display_value=all`:\\n\\n```json\\n{\\n  \\"state\\": {\\n    \\"display_value\\": \\"Closed\\",\\n    \\"value\\": \\"3\\"\\n  },\\n  \\"sys_id\\": {\\n    \\"display_value\\": \\"4d54d7481b37e010d315cbb5464bcb95\\",\\n    \\"value\\": \\"4d54d7481b37e010d315cbb5464bcb95\\"\\n  },\\n  \\"number\\": {\\n    \\"display_value\\": \\"CHG0122595\\",\\n    \\"value\\": \\"CHG0122595\\"\\n  },\\n  \\"requested_by\\": {\\n    \\"display_value\\": \\"Sally Omer\\",\\n    \\"link\\": \\"https://ourcompanyinstance.service-now.com/api/now/table/sys_user/b15cf3ebdbe11300f196f3651d961999\\",\\n    \\"value\\": \\"b15cf3ebdbe11300f196f3651d961999\\"\\n  },\\n  \\"reason\\": {\\n    \\"display_value\\": null,\\n    \\"value\\": \\"\\"\\n  }\\n}\\n```\\n\\n`sysparm_display_value=true`:\\n\\n```json\\n{\\n  \\"state\\": \\"Closed\\",\\n  \\"sys_id\\": \\"4d54d7481b37e010d315cbb5464bcb95\\",\\n  \\"number\\": \\"CHG0122595\\",\\n  \\"requested_by\\": {\\n    \\"display_value\\": \\"Sally Omer\\",\\n    \\"link\\": \\"https://ourcompanyinstance.service-now.com/api/now/table/sys_user/b15cf3ebdbe11300f196f3651d961999\\"\\n  },\\n  \\"reason\\": null\\n}\\n```\\n\\n`sysparm_display_value=false`:\\n\\n```json\\n{\\n  \\"state\\": \\"3\\",\\n  \\"sys_id\\": \\"4d54d7481b37e010d315cbb5464bcb95\\",\\n  \\"number\\": \\"CHG0122595\\",\\n  \\"requested_by\\": {\\n    \\"link\\": \\"https://ourcompanyinstance.service-now.com/api/now/table/sys_user/b15cf3ebdbe11300f196f3651d961999\\",\\n    \\"value\\": \\"b15cf3ebdbe11300f196f3651d961999\\"\\n  },\\n  \\"reason\\": \\"\\"\\n}\\n```\\n\\nAs you can see, we have the same properties being returned each time, but with a different shape. Let\'s call out some interesting highlights:\\n\\n- `requested_by` is _always_ an object which contains `link`. It may also contain `value` and `display_value` depending upon `sysparm_display_value`\\n- `state`, `sys_id`, `number` and `reason` are objects containing `value` and `display_value` when `sysparm_display_value` is `all`. Otherwise, the value of `value` or `display_value` is surfaced up directly; not in an object.\\n- most values are strings, even if they represent another data type. So `state.value` is always a stringified number. The only exception to this rule is `reason.display_value` which can be `null`\\n\\n## Type Definition time\\n\\nWe want to create type definitions for these API results. We could of course create three different results, but that would involve duplication. Boo! It\'s worth bearing in mind we\'re looking at a subset of five properties in this example. In reality, there are many, many properties on a Change Request. Whilst this example is for a subset, if we wanted to go on to create the full type definition the duplication would become very impractical.\\n\\nWhat can we do? Well, if all of the underlying properties were of the same type, we could use a generic and be done. But given the underlying types can vary, that\'s not going to work. We can achieve this though through using a combination of generics and conditional types.\\n\\nLet\'s begin by creating a string literal type of the possible values of `sysparm_display_value`:\\n\\n```ts\\nexport type DisplayValue = \'all\' | \'true\' | \'false\';\\n```\\n\\n## Making a `PropertyValue` type\\n\\nNext we need to create a type that models the object with `display_value` and `value` properties.\\n\\n:::info a type for state, sys_id, number and reason\\n\\n- `state`, `sys_id`, `number` and `reason` are objects containing `value` and `display_value` when `sysparm_display_value` is `\'all\'`. Otherwise, the value of `value` or `display` is surfaced up directly; not in an object.\\n- most values are strings, even if they represent another data type. So `state.value` is always a stringified number. The only exception to this rule is `reason.display_value` which can be `null`\\n\\n:::\\n\\n```ts\\nexport interface ValueAndDisplayValue<TValue = string, TDisplayValue = string> {\\n  display_value: TDisplayValue;\\n  value: TValue;\\n}\\n```\\n\\nNote that this is a generic property with a default type of `string` for both `display_value` and `value`. Most of the time, `string` is the type in question so it\'s great that TypeScript allows us to cut down on the amount of syntax we use.\\n\\nNow we\'re going to create our first conditional type:\\n\\n```ts\\nexport type PropertyValue<\\n  TAllTrueFalse extends DisplayValue,\\n  TValue = string,\\n  TDisplayValue = string\\n> = TAllTrueFalse extends \'all\'\\n  ? ValueAndDisplayValue<TValue, TDisplayValue>\\n  : TAllTrueFalse extends \'true\'\\n  ? TDisplayValue\\n  : TValue;\\n```\\n\\nThe `PropertyValue` will either be a `ValueAndDisplayValue`, a `TDisplayValue` or a `TValue`, depending upon whether `PropertyValue` is `\'all\'`, `\'true\'` or `\'false\'` respectively. That\'s hard to grok. Let\'s look at an example of each of those cases using the `reason` property, which allows a `TValue` of `string` and a `TDisplayValue` of `string | null`:\\n\\n```ts\\nconst reasonAll: PropertyValue<\'all\', string, string | null> = {\\n  display_value: null,\\n  value: \'\',\\n};\\nconst reasonTrue: PropertyValue<\'true\', string, string | null> = null;\\nconst reasonFalse: PropertyValue<\'false\', string, string | null> = \'\';\\n```\\n\\nConsider the type on the left and the value on the right. We\'re successfully modelling our `PropertyValue`s. I\'ve deliberately picked an edge case example to push our conditional type to its limits.\\n\\n## Service Now Change Request States\\n\\nLet\'s look at another usage. We\'ll create a type that repesents the possible values of a Change Request\'s `state` in Service Now. Do take a moment to appreciate these values. Many engineers were lost in the numerous missions to obtain these rare and secret enums. Alas, the Service Now API docs have some significant gaps.\\n\\n```ts\\n/** represents the possible Change Request \\"State\\" values in Service Now */\\nexport const STATE = {\\n  NEW: \'-5\',\\n  ASSESS: \'-4\',\\n  SENT_FOR_APPROVAL: \'-3\',\\n  SCHEDULED: \'-2\',\\n  APPROVED: \'-1\',\\n  WAITING: \'1\',\\n  IN_PROGRESS: \'2\',\\n  COMPLETE: \'3\',\\n  ERROR: \'4\',\\n  CLOSED: \'7\',\\n} as const;\\n\\nexport type State = typeof STATE[keyof typeof STATE];\\n```\\n\\nBy combining `State` and `PropertyValue`, we can strongly type the `state` property of Change Requests. Consider:\\n\\n```ts\\nconst stateAll: PropertyValue<\'all\', State> = {\\n  display_value: \'Closed\',\\n  value: \'3\',\\n};\\nconst stateTrue: PropertyValue<\'true\', State> = \'Closed\';\\nconst stateFalse: PropertyValue<\'false\', State> = \'3\';\\n```\\n\\nWith that in place, let\'s turn our attention to our other natural type that the `requested_by` property demonstrates.\\n\\n## Making a `LinkValue` type\\n\\n:::info a type for requested_by\\n\\n`requested_by` is _always_ an object which contains `link`. It may also contain `value` and `display_value` depending upon `sysparm_display_value`\\n\\n:::\\n\\n```ts\\ninterface Link {\\n  link: string;\\n}\\n\\n/** when TAllTrueFalse is \'false\' */\\nexport interface LinkAndValue extends Link {\\n  value: string;\\n}\\n\\n/** when TAllTrueFalse is \'true\' */\\nexport interface LinkAndDisplayValue extends Link {\\n  display_value: string;\\n}\\n\\n/** when TAllTrueFalse is \'all\' */\\nexport interface LinkValueAndDisplayValue\\n  extends LinkAndValue,\\n    LinkAndDisplayValue {}\\n```\\n\\nThe three types above model the different scenarios. Now we need a conditional type to make use of them:\\n\\n```ts\\nexport type LinkValue<TAllTrueFalse extends DisplayValue> =\\n  TAllTrueFalse extends \'all\'\\n    ? LinkValueAndDisplayValue\\n    : TAllTrueFalse extends \'true\'\\n    ? LinkAndDisplayValue\\n    : LinkAndValue;\\n```\\n\\nThis is hopefully simpler to read than the `PropertyValue` type, and if you look at the examples below you can see what usage looks like:\\n\\n```ts\\nconst requested_byAll: LinkValue<\'all\'> = {\\n  display_value: \'Sally Omer\',\\n  link: \'https://ourcompanyinstance.service-now.com/api/now/table/sys_user/b15cf3ebdbe11300f196f3651d961999\',\\n  value: \'b15cf3ebdbe11300f196f3651d961999\',\\n};\\nconst requested_byTrue: LinkValue<\'true\'> = {\\n  display_value: \'Sally Omer\',\\n  link: \'https://ourcompanyinstance.service-now.com/api/now/table/sys_user/b15cf3ebdbe11300f196f3651d961999\',\\n};\\nconst requested_byFalse: LinkValue<\'false\'> = {\\n  link: \'https://ourcompanyinstance.service-now.com/api/now/table/sys_user/b15cf3ebdbe11300f196f3651d961999\',\\n  value: \'b15cf3ebdbe11300f196f3651d961999\',\\n};\\n```\\n\\n## Making our complete type\\n\\nWith these primitives in place, we can now build ourself a (cut-down) type that models a Change Request:\\n\\n```ts\\nexport interface ServiceNowChangeRequest<TAllTrueFalse extends DisplayValue> {\\n  state: PropertyValue<TAllTrueFalse, State>;\\n  sys_id: PropertyValue<TAllTrueFalse>;\\n  number: PropertyValue<TAllTrueFalse>;\\n  requested_by: LinkValue<TAllTrueFalse>;\\n  reason: PropertyValue<TAllTrueFalse, string, string | null>;\\n  // there are *way* more properties in reality\\n}\\n```\\n\\nThis is a generic type which will accept `\'all\'`, `\'true\'` or `\'false\'` and will use that type to drive the type of the properties _inside_ the object. And now we have successfully typed our Service Now Change Request, thanks to TypeScript\'s conditional types.\\n\\nTo test it out, let\'s take the JSON responses we got back from our curls at the start, and see if we can make `ServiceNowChangeRequest`s with them.\\n\\n```ts\\nconst changeRequestFalse: ServiceNowChangeRequest<\'false\'> = {\\n  state: \'3\',\\n  sys_id: \'4d54d7481b37e010d315cbb5464bcb95\',\\n  number: \'CHG0122595\',\\n  requested_by: {\\n    link: \'https://ourcompanyinstance.service-now.com/api/now/table/sys_user/b15cf3ebdbe11300f196f3651d961999\',\\n    value: \'b15cf3ebdbe11300f196f3651d961999\',\\n  },\\n  reason: \'\',\\n};\\n\\nconst changeRequestTrue: ServiceNowChangeRequest<\'true\'> = {\\n  state: \'Closed\',\\n  sys_id: \'4d54d7481b37e010d315cbb5464bcb95\',\\n  number: \'CHG0122595\',\\n  requested_by: {\\n    display_value: \'Sally Omer\',\\n    link: \'https://ourcompanyinstance.service-now.com/api/now/table/sys_user/b15cf3ebdbe11300f196f3651d961999\',\\n  },\\n  reason: null,\\n};\\n\\nconst changeRequestAll: ServiceNowChangeRequest<\'all\'> = {\\n  state: {\\n    display_value: \'Closed\',\\n    value: \'3\',\\n  },\\n  sys_id: {\\n    display_value: \'4d54d7481b37e010d315cbb5464bcb95\',\\n    value: \'4d54d7481b37e010d315cbb5464bcb95\',\\n  },\\n  number: {\\n    display_value: \'CHG0122595\',\\n    value: \'CHG0122595\',\\n  },\\n  requested_by: {\\n    display_value: \'Sally Omer\',\\n    link: \'https://ourcompanyinstance.service-now.com/api/now/table/sys_user/b15cf3ebdbe11300f196f3651d961999\',\\n    value: \'b15cf3ebdbe11300f196f3651d961999\',\\n  },\\n  reason: {\\n    display_value: null,\\n    value: \'\',\\n  },\\n};\\n```\\n\\nWe can! Do take a look at this in the [TypeScript playground](https://www.typescriptlang.org/play?#code/KYDwDg9gTgLgBDAnmYcAiBLAzmANgQ0QDV9cBXVAXjgHJTca4AfWmKCxlmgM1K2BoBuAFDDQkWHAwA7GMCi8AxqhLlgAQWkATTDgLFSFADwAVVRTjUsbGQHMANHBO68hc1TjWodgHxwA3sJwwXBa2K6IAPoAbobAAFxOLvruIiFwsWqJZnEiAL6i4tDwSChwAApQECiwBmpGQSEm6ri4JuzAAGJ8qKBy2ljo4Slx9o3BOWqWnjbSDuNJw25x0152wn7Uza3tFN24-HB9wAO09DQLAPxw7po6S3XGkxSOzg-uPgvZLW0d+4fHU40NgcK6LPTLNRfJypUSKCDSaxwKDAfBYBE-RKVaryJDuIx0Vo0RxrOYk2a2ZhwaRkVqbAILABEYQhUUyFEZiRprTG6UZ7OAnLgjMZwgK8MR8BRaIRuwSFSqNTxcQJIIE5O8ZJmmspLG5uHp+pEEqR0vR0n+8uxSsewAJvAO6u1dg1dip+vpNCEogA9AAqP3I4BgFH8WSDGAAC1QkCwWAwACNcKgAMKR-BzVAAJWAAEcKEjGQBlGD4OSMjJxQYyOBF+TRDDKOAAOQgAHc4H6fWJwMU4Cb4EXmiYAKLTQLpZsjgDqiRoAFoAKzEhbqItFkfrufzgAsK-SG+bJkinQA8lnIupyuUs6eiOoADLbgDM+5CRZTAAkR2gAKoPn9twAJjfYIrxvO9ANoecAEZQLgad1AASRMJDmwAcTnODeRCNDIgg9Cs03Is5xAnDghTU8AFlygA0c51fci4BHLNbyzOc9yYlMH1PDc0DnAB2Fc8jgNF+wRawRB7CQSmQVASzLDxSmACBuFrYcRwAbQAa2ARBVIQOSDKHdRRwAXSkgcZkUzEFRxWp8UJBhHAUuR6QnEJmQeGI4iFRkU1wCB+C0RkmP5XzEkZZ9RTyY0JPgaxFLlLFFVxW1VQ6Yla1LNzpn8wLgsZOLJWsuRLRS+zlXqHgeiy1zgHpKKitEGQ5AUfAmwfGRtIZdJcG6xJSVsfJfQDOA22jaQnB+OVLSkQYasdRgu2kvtWvkJRUC66RtLudwjhAfotEGbaeo84IBUGikRuEf1Awmk5pp2P4enm1hMs7bsikkdb2s67q7mSSELEBY64FO3qQhZCIfKyZ05huu7xsmp7fj2V7sDOIlPtWn7ZA2jqtu625tCB20DqOk6Ae0dxHFOwH3hWfwClx2SylO-FtjRrpXtBwYyY+aYudm3nDpOMGnIudJrg5uIGdZdxoWFl7HQp8WFrVKWQhl6n7gVuJoXpmncjheKg3zYBrGALRIgTRBbNl6rzncplof0WGOUiloMFrCAAFt5FCpl+p2vzIxgGAwCweIfR9GRoktuRFES2w7AAOn4KAG2UedpHbNP4T9n18DADAfTztsfVLJNgB9LBECwSIyEzn0ExgxdFG4Z9gATLQE2AGCYOfAAGYfuBggBOAA2Lup8XGCtGnyeJ-wIO+QFPy247rue77geh9H8fp9n+fF6n5fV7FYrTTzAs5Btu3kvB4mVWBTKXb5N3CA9wUvf632A5QDXp5EO2kw4RyjjHOO0gE5W2TqWVOcwM71kbMAXO+dC7F1LuXdsVd8A1zrg3JuLct6d27r3fug8R5j0njPZ8c8F5LwnivGKwgrIogtlbB+iByrPx2o5B0-AaAfxAd1cBkdo6x3jonYA8D8CINsMgrOqD0FtgLv7LBZcK54IIfXRuzd5Ct3bmQ3elCD40OPvQ0+TCWFhQ3pFUhO8KH72oUfOhDCz4XxilJb68BfqbVrCg5QrY2xpgzLYYAOZOEwFMDNFWAIxanAFnEPw51SpWlSg5FUyt0aOhcjlBqaQQh6MiBgLQFUbSczibk-gPginBBpH7fuUAKlpSqc9GphSFgcLvtbW2iBEiOyeNUnmjo6ndNROaVpWT6g5NGfwV0WohrulpAaG6VlFDpkzFEu+vC6zKOCe2MJ2zb6J3tLVERjJErlkitFHCVyiFlL8juLQi4XkCR3AADhggmZ8AlgDDxgsPLQz5jEJgTG8qeO4EyKATJ8xcwDGSNOaX5L86FAVASAoueFiKekyO4UKNJjJQHiMgVImBMi5EKKUdnNBFd1FFxLlo3B1dkyEP0SQ4xTi95UMPrQk+jDz7MNXnYiKwpHHkJ5eYtxArPHCtFMEPI9yzQIj8t402JVNnhMiac6wT99m0pCcciJOyzlvw4Jc65v9hQBSCtbRFJSnmRReW8rQHzvm-P+YC4FoKO7gshdC2FOL7nIsDpFNFGKsXBsaIyPFXD+mEtdt5exwpvYAMDmFElkVw4SKgdIuBKd06Z1paohlmicGV1ZbXEpBioBGO3pKsxrj+VWMFV4xoSqY0qukEKfUYoNVIi1Sc6JtkDWoKNVsk1uqYmS3pMIDyVyCmJs-smsV+U7UhVFWoPy0UO33MdSFRIRKv5sjXS695Xyfl-IBUCkFYKIU7ihTCuFCKt2e2FOet1l7PU3p9fegNz6cV7pjaGoBR6k2sh-qiz86KYKYuxa+pkKb-IwcjQhhVcBO3BFjdOvpdtl2eRPVBv+Psiz+wzcHMR2aIGSOgbApOhakHFpUfSzBTKK06LZTWzlDbTEuL5ZYjxNiRVIbXRKvjvKLHuOsUKlhwHsPdoI9hojKb9RvutSKDtYogA)."},{"id":"/2021/04/20/ts-loader-goes-webpack-5","metadata":{"permalink":"/2021/04/20/ts-loader-goes-webpack-5","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-04-20-ts-loader-goes-webpack-5/index.md","source":"@site/blog/2021-04-20-ts-loader-goes-webpack-5/index.md","title":"ts-loader goes webpack 5","description":"ts-loader has just released v9.0.0. This post goes through what this release is all about, and what it took to ship this version. For intrigue, it includes a brief scamper into my mental health along the way. Some upgrades go smoothly - this one had some hiccups. But we\'ll get into that.","date":"2021-04-20T00:00:00.000Z","formattedDate":"April 20, 2021","tags":[{"label":"webpack","permalink":"/tags/webpack"},{"label":"ts-loader","permalink":"/tags/ts-loader"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"webpack 5","permalink":"/tags/webpack-5"}],"readingTime":6.28,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"ts-loader goes webpack 5","authors":"johnnyreilly","tags":["webpack","ts-loader","TypeScript","webpack 5"],"image":"./ts-loader-9.png","hide_table_of_contents":false},"prevItem":{"title":"The Service Now API and TypeScript Conditional Types","permalink":"/2021/04/24/service-now-api-and-typescript-conditional-types"},"nextItem":{"title":"Hello World Bicep","permalink":"/2021/04/10/hello-world-bicep"}},"content":"`ts-loader` has just released [v9.0.0](https://github.com/TypeStrong/ts-loader/releases/tag/v9.0.0). This post goes through what this release is all about, and what it took to ship this version. For intrigue, it includes a brief scamper into my mental health along the way. Some upgrades go smoothly - this one had some hiccups. But we\'ll get into that.\\n\\n![hello world bicep](ts-loader-9.png)\\n\\n## One big pull request\\n\\nAs of v8, `ts-loader` supported webpack 4 and webpack 5. However the webpack 5 support was best efforts, and not protected by any automated tests. `ts-loader` has two test packs:\\n\\n1. A [comparison test pack](https://github.com/TypeStrong/ts-loader/tree/main/test/comparison-tests#readme) that compares transpilation and webpack compilation output with known outputs.\\n2. An [execution test pack](https://github.com/TypeStrong/ts-loader/tree/main/test/execution-tests#readme) that executes Karma test packs written in TypeScript using `ts-loader`.\\n\\nThe test packs were tightly coupled to webpack 4 (and in the case of the comparison test pack, that\'s unavoidable). The mission was to port `ts-loader` to be built against (and have an automated test pack that ran against) webpack 5.\\n\\nThis ended up being a [very big pull request](https://github.com/TypeStrong/ts-loader/pull/1251). Work on it started back in February 2021 and we\'re shipping now in April of 2021. I\'d initially expected it would take a couple of days at most. I had underestimated.\\n\\nA number of people collaborated on this PR, either with code, feedback, testing or even just responding to questions. So I\'d like to say thank you to:\\n\\n- [John Wallsten](https://github.com/JonWallsten) - who did a lot of the work swapping `ts-loader` over to webpack 5 APIs\\n- [Nick Excell](https://github.com/appzuka)\\n- [Andrew Branch](https://github.com/andrewbranch)\\n- [Alexander Akait](https://github.com/alexander-akait) - who provided webpack 5 expertise and ideas\\n- [Tobias Koppers](https://github.com/sokra) - who got me out of a hole - more on that later\\n\\n## What\'s changed\\n\\nLet\'s go through what\'s different in v9. There\'s two breaking changes:\\n\\n- The minimum webpack version supported is now webpack 5. This simplifies the codebase, which previously had to if/else the various API registrations based on the version of webpack being used.\\n- The minimum node version supported is now node 12. [Node 10 reaches end of life status at the end of April 2021.](https://nodejs.org/en/about/releases/)\\n\\nAn interesting aspect of migrating to building against webpack 5 was dropping the dependency upon [`@types/webpack`](https://www.npmjs.com/package/@types/webpack) in favour of the types that now ship with webpack 5 itself. This was a mostly great experience; however we discovered some missing pieces.\\n\\nMost notably, the `LoaderContext` [wasn\'t strongly typed](https://github.com/webpack/webpack/blob/03961f33912ab6735d470b870eacff678735a9ed/lib/NormalModule.js#L424). `LoaderContext` is the value of `this` in the context of a running loader function. So it is probably the most interesting and important type from the perspective of a loader author.\\n\\nHistorically we used our own definition which had been adapted from the one in `@types/webpack`. [I\'ve looked into the possibility of a type being exposed in webpack itself.](https://github.com/webpack/webpack/issues/13162) However, it turns out, [it\'s complicated - with the `LoaderContext` type being effectively created across two packages](https://github.com/webpack/webpack/pull/13164#issuecomment-821410359). The type is initially created in `webpack` and then augmented later in `loader-runner`, prior to being supplied to loaders. You can read more on that [here](https://github.com/webpack/webpack/pull/13164#issuecomment-821410359).\\n\\nFor now we\'ve opted to stick with keeping [an interface in `ts-loader`](https://github.com/TypeStrong/ts-loader/pull/1251/commits/acbc71feed91fe14ec065dd9d31081af7a492f47) that models what arrives in the loader when executed. We have freshened it up somewhat, to model the webpack 5 world.\\n\\nAlongside these changes, a [number of dependencies were upgraded](https://github.com/TypeStrong/ts-loader/pull/1251/files#diff-7ae45ad102eab3b6d7e7896acd08c427a9b25b346470d7bc6507b6481575d519).\\n\\n## The hole\\n\\nBy the 19th of February most of the work was done. However, [we were experiencing different behaviour between Linux and Windows in our comparison test pack](https://github.com/TypeStrong/ts-loader/pull/1251#issuecomment-781967959).\\n\\nAs far as I was aware, we were doing all the appropriate work to ensure `ts-loader` and our test packs worked cross platform. But we were still experiencing problems whenever we ran the test pack on Windows. I\'d done no end of tweaking but nothing worked. I couldn\'t explain it. I couldn\'t fix it. I was finding that tough to deal with.\\n\\nI really want to be transparent about the warts and all aspect of open source software development. It is like all other types of software development; sometimes things go wrong and it can be tough to work out why. Right then, I was really quite unhappy. Things weren\'t working code-wise and I was at a loss to say why. This is not something that I dig.\\n\\nI also wasn\'t sleeping amazingly at this point. It was winter and we\'d been in lockdown in the UK for three months; as the COVID-19 pandemic ground relentlessly on. I love my family dearly. I really do. With that said, having my children around whilst I attempted to work was remarkably tough. I love those guys but, woah, was it stressful.\\n\\nI was feeling at a low ebb. And I wasn\'t sure what to do next. So, feeling tired and pretty fed up, I took a break.\\n\\n## \\"Anybody down there?\\"\\n\\nTime passed. In March [Alexander Akait](https://github.com/alexander-akait) checked in to see how things were going and volunteered to help. He also [suggested what turned out to be the fix](https://github.com/TypeStrong/ts-loader/pull/1251#issuecomment-799531375); namely replacing usage of `\'\\\\\'` with `\'/\'` in the assets supplied back to webpack. But crucially I implemented this wrong. Observe [this commit](https://github.com/TypeStrong/ts-loader/pull/1251/commits/4bcc5c9623acfd7ffbaf028781a8353b37243804):\\n\\n```ts\\nconst assetPath = path\\n  .relative(compilation.compiler.outputPath, outputFile.name)\\n  // According to @alexander-akait we should always \'/\' https://github.com/TypeStrong/ts-loader/pull/1251#issuecomment-799606985\\n  .replace(/\\\\//g, \'/\');\\n```\\n\\nIf you look closely at the `replace` you\'ll see that I\'m globally replacing `\'/\'` with `\'/\'` _rather_ than globally replacing `\'\\\\\'` with `\'/\'`. The wasted time this caused... I could weep.\\n\\nI generally thrashed around for a bit after this. Going in circles, like a six year old swimming wearing one armband. Then [Tobias kindly volunteered to help](https://github.com/TypeStrong/ts-loader/pull/1251#issuecomment-805143890). This much I\'ve learned from a career in software: if talented people offer their assistance, grab it with both hands!\\n\\nI\'d been trying be as \\"learn in public\\" as possible about the issues I was facing on the pull request. The idea being, to surface the problems in a public forum where others can read and advise. And also to attempt a textual kind of [rubber duck debugging](https://en.wikipedia.org/wiki/Rubber_duck_debugging).\\n\\nWhen Tobias pitched in, I wanted to make it as easy as possible for him to help. So I wrote up [a full description of what had changed](https://github.com/TypeStrong/ts-loader/pull/1251#issuecomment-805181069). What the divergent behaviour in test packs looked like. I shared my speculation for what might be causing the issue (I was wrong by the way). Finally I provided a simple way to get up and running with the broken code. The easier I could make it for others to collaborate on this, I figured, the greater the likelihood of an answer. Tobias got to an answer quickly:\\n\\n> The problem is introduced due to some normalization logic in the test case: see [#1273](https://github.com/TypeStrong/ts-loader/pull/1273)\\n>\\n> While the PR fixes the problem, I think the paths should be normalized earlier in the pipeline to make this normalization code unnecessary. Note that asset names should have only `/` as they are filenames and not paths. Only absolute paths have `\\\\`.\\n\\nTobias had raised a PR which introduced a workaround to resolved things in the test pack. This made me happy. More than that, he also identified that the issue lay in `ts-loader` itself. This caused me to look again at the changes I\'d made, including my `replace` addition. [With fresh eyes, I now realised this was a bug](https://github.com/TypeStrong/ts-loader/pull/1251#issuecomment-805907212), and [fixed](https://github.com/TypeStrong/ts-loader/pull/1251/commits/427714e43519289bb5745ca078133d1ace8fc2c1) it.\\n\\nI found then that I could revert Tobias\' workaround and still have passing tests. Result!\\n\\n## Release details\\n\\nNow that we\'ve got there; we\'ve shipped. You can get the latest version of `ts-loader` on [npm](https://www.npmjs.com/package/ts-loader/v/9.0.0) and you can find the release details on [GitHub](https://github.com/TypeStrong/ts-loader/releases/tag/v9.0.0).\\n\\nThanks everyone - I couldn\'t have done it without your help. \ud83c\udf3b\u2764\ufe0f"},{"id":"/2021/04/10/hello-world-bicep","metadata":{"permalink":"/2021/04/10/hello-world-bicep","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-04-10-hello-world-bicep/index.md","source":"@site/blog/2021-04-10-hello-world-bicep/index.md","title":"Hello World Bicep","description":"Bicep makes Azure Resource Management a great deal simpler than ARM templates. The selling point here is grokkability. This post takes a look at the \\"Hello World\\" example recently added to the Bicep repo to appreciate quite what a difference it makes.","date":"2021-04-10T00:00:00.000Z","formattedDate":"April 10, 2021","tags":[{"label":"Bicep","permalink":"/tags/bicep"},{"label":"ARM templates","permalink":"/tags/arm-templates"}],"readingTime":2.675,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Hello World Bicep","authors":"johnnyreilly","tags":["Bicep","ARM templates"],"image":"./hello-world-bicep.png","hide_table_of_contents":false},"prevItem":{"title":"ts-loader goes webpack 5","permalink":"/2021/04/20/ts-loader-goes-webpack-5"},"nextItem":{"title":"Bicep meet Azure Pipelines 2","permalink":"/2021/03/23/bicep-meet-azure-pipelines-2"}},"content":"Bicep makes Azure Resource Management a great deal simpler than ARM templates. The selling point here is grokkability. This post takes a look at the [\\"Hello World\\" example recently added to the Bicep repo](https://github.com/Azure/bicep/pull/2011) to appreciate quite what a difference it makes.\\n\\n![hello world bicep](hello-world-bicep.png)\\n\\n## More than configuration\\n\\nThe [\\"Hello World\\"](https://github.com/Azure/bicep/tree/187d4d2047dc83c69695ba79761f552bcb00c319/docs/examples/000/01-hello-world) added to the Bicep repo by [Chris Lewis](https://github.com/ChristopherGLewis) illustrates the simplest usage of Bicep:\\n\\n> This bicep file takes a `yourName` parameter and adds that to a `hello` variable and returns the concatenated string as an ARM output.\\n\\nThis is, when you consider it, the very essence of a computer program. Taking an input, doing some computation and providing an output. When I think about ARM templates, (and because Bicep is transpiled into ARM templates I mentally bracket the two together) I tend to think about resources being deployed. I focus on _configuration_, not _computation_\\n\\nThis is an imperfect mental model. ARM templates can do so much more than deploy by slinging strings and numbers. Thanks to the wealth of [template functions](https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/template-functions) that exist they have much more power. They can do computation.\\n\\nThe Hello World example focuses just on computation.\\n\\n## From terse to verbose\\n\\nThe Hello World example is made up of two significant files:\\n\\n1. `main.bicep` - the bicep code\\n2. `main.json` - the ARM template compiled from the Bicep file\\n\\nThe [`main.bicep`](https://github.com/Azure/bicep/blob/187d4d2047dc83c69695ba79761f552bcb00c319/docs/examples/000/01-hello-world/main.bicep) file amounts to 3 lines of code (I have omitted the comment line):\\n\\n```bicep\\nparam yourName string\\nvar hello = \'Hello World! - Hi\'\\n\\noutput helloWorld string = \'${hello} ${yourName}\'\\n```\\n\\n- the first line takes the _input_ of `yourName`\\n- the second line declares a `hello` variable\\n- the third line _computes_ the new value of `helloWorld` based upon `hello` and `yourName`, then passes it as _output_\\n\\nGosh is it ever simple. It\'s easy to read and it\'s simple to understand. Even if you don\'t know Bicep, if you\'ve experience in another language you can likely guess what\'s happening.\\n\\nLet\'s compare this with the [`main.json`](https://github.com/Azure/bicep/blob/187d4d2047dc83c69695ba79761f552bcb00c319/docs/examples/000/01-hello-world/main.json) that `main.bicep` is transpiled into:\\n\\n```json\\n{\\n  \\"$schema\\": \\"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\\",\\n  \\"contentVersion\\": \\"1.0.0.0\\",\\n  \\"metadata\\": {\\n    \\"_generator\\": {\\n      \\"name\\": \\"bicep\\",\\n      \\"version\\": \\"dev\\",\\n      \\"templateHash\\": \\"6989941473549654446\\"\\n    }\\n  },\\n  \\"parameters\\": {\\n    \\"yourName\\": {\\n      \\"type\\": \\"string\\"\\n    }\\n  },\\n  \\"functions\\": [],\\n  \\"variables\\": {\\n    \\"hello\\": \\"Hello World! - Hi\\"\\n  },\\n  \\"resources\\": [],\\n  \\"outputs\\": {\\n    \\"helloWorld\\": {\\n      \\"type\\": \\"string\\",\\n      \\"value\\": \\"[format(\'{0} {1}\', variables(\'hello\'), parameters(\'yourName\'))]\\"\\n    }\\n  }\\n}\\n```\\n\\nThe above ARM template expresses exactly the same thing as the Bicep alternative. But that 3 lines of logic has become 27 lines of JSON. We\'ve lost something in the transition. Intent is no longer clear. We\'ve gone from something easy to reason about, to something that is hard to reason about. You need to think a lot less to write the Bicep alternative and that\'s a _good_ thing.\\n\\nI was chatting to someone recently who expressed it well by saying:\\n\\n> ARM is the format that the resource providers understand, so really it\u2019s the Azure equivalent of Assembler \u2013 and I don\u2019t know anyone who enjoys coding in Assembler.\\n\\nThis is a great example of the value that Bicep provides. If you\'d like to play with the Hello World a little, why not [take it for a spin in the Bicep playground](https://aka.ms/bicepdemo#eJzT1w9OzC3ISVXISM3JyVcozy/KSeEqSCxKzFWozC8t8kvMTVUoLinKzEvnKkssgqqyVVD3ADPCQcoVFXQVPDLVubjyS0sKSksgasAyUJ0g9SrVYOFaBZVqmLm16gCvlitr)."},{"id":"/2021/03/23/bicep-meet-azure-pipelines-2","metadata":{"permalink":"/2021/03/23/bicep-meet-azure-pipelines-2","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-03-23-bicep-meet-azure-pipelines-2/index.md","source":"@site/blog/2021-03-23-bicep-meet-azure-pipelines-2/index.md","title":"Bicep meet Azure Pipelines 2","description":"Last time I wrote about how to use the Azure CLI to run Bicep within the context of an Azure Pipeline. The solution was relatively straightforward, and involved using az deployment group create in a task. There\'s an easier way.","date":"2021-03-23T00:00:00.000Z","formattedDate":"March 23, 2021","tags":[{"label":"Bicep","permalink":"/tags/bicep"},{"label":"ARM templates","permalink":"/tags/arm-templates"},{"label":"Azure Pipelines","permalink":"/tags/azure-pipelines"},{"label":"Azure CLI","permalink":"/tags/azure-cli"}],"readingTime":1.665,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Bicep meet Azure Pipelines 2","authors":"johnnyreilly","tags":["Bicep","ARM templates","Azure Pipelines","Azure CLI"],"image":"./bicep-meet-azure-pipelines.png","hide_table_of_contents":false},"prevItem":{"title":"Hello World Bicep","permalink":"/2021/04/10/hello-world-bicep"},"nextItem":{"title":"Bicep meet Azure Pipelines","permalink":"/2021/03/20/bicep-meet-azure-pipelines"}},"content":"[Last time](./2021-03-20-bicep-meet-azure-pipelines/index.md) I wrote about how to use the Azure CLI to run Bicep within the context of an Azure Pipeline. The solution was relatively straightforward, and involved using `az deployment group create` in a task. There\'s an easier way.\\n\\n![Bicep meet Azure Pipelines](bicep-meet-azure-pipelines.png)\\n\\n## The easier way\\n\\nThe target reader of the previous post was someone who was already using `AzureResourceManagerTemplateDeployment@3` in an Azure Pipeline to deploy an ARM template. Rather than replacing your existing `AzureResourceManagerTemplateDeployment@3` tasks, all you need do is insert a prior `bash` step that compiles the Bicep to ARM, which your existing template can then process. It looks like this:\\n\\n```yml\\n- bash: az bicep build --file infra/app-service/azuredeploy.bicep\\n  displayName: \'Compile Bicep to ARM\'\\n```\\n\\nThis will take your Bicep template of `azuredeploy.bicep`, transpile it into an ARM template named `azuredeploy.json` which a subsequent `AzureResourceManagerTemplateDeployment@3` task can process. Since this is just exercising the Azure CLI, using `bash` is not required; powershell etc would also be fine; it\'s just required that the Azure CLI is available in a pipeline.\\n\\nIn fact this simple task could even be a one-liner if you didn\'t fancy using the `displayName`. (Though I say keep it; optimising for readability is generally a good shout.) A full pipeline could look like this:\\n\\n```yml\\n- bash: az bicep build --file infra/app-service/azuredeploy.bicep\\n  displayName: \'Compile Bicep to ARM\'\\n\\n- task: AzureResourceManagerTemplateDeployment@3\\n  displayName: \'Deploy Hello Azure ARM\'\\n  inputs:\\n    azureResourceManagerConnection: \'$(azureSubscription)\'\\n    action: Create Or Update Resource Group\\n    resourceGroupName: \'$(resourceGroupName)\'\\n    location: \'North Europe\'\\n    templateLocation: Linked artifact\\n    csmFile: \'infra/app-service/azuredeploy.json\' # created by bash script\\n    csmParametersFile: \'infra/app-service/azuredeploy.parameters.json\'\\n    deploymentMode: Incremental\\n    deploymentOutputs: resourceGroupDeploymentOutputs\\n    overrideParameters: -applicationName $(Build.Repository.Name)\\n\\n- pwsh: |\\n    $outputs = ConvertFrom-Json \'$(resourceGroupDeploymentOutputs)\'\\n    foreach ($output in $outputs.PSObject.Properties) {\\n        Write-Host \\"##vso[task.setvariable variable=RGDO_$($output.Name)]$($output.Value.value)\\"\\n    }\\n  displayName: \'Turn ARM outputs into variables\'\\n```\\n\\nAnd when it\'s run, it may result in something along these lines:\\n\\n![Bicep in an Azure Pipeline](azure-pipeline-with-bicep.png)\\n\\nSo if you want to get using Bicep right now with minimal effort, this an on ramp that could work for you! Props to [Jamie McCrindle](https://twitter.com/foldr) for suggesting this."},{"id":"/2021/03/20/bicep-meet-azure-pipelines","metadata":{"permalink":"/2021/03/20/bicep-meet-azure-pipelines","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-03-20-bicep-meet-azure-pipelines/index.md","source":"@site/blog/2021-03-20-bicep-meet-azure-pipelines/index.md","title":"Bicep meet Azure Pipelines","description":"Bicep is a terser and more readable alternative language to ARM templates. Running ARM templates in Azure Pipelines is straightforward. However, there isn\'t yet a first class experience for running Bicep in Azure Pipelines. This post demonstrates an approach that can be used until a Bicep task is available.","date":"2021-03-20T00:00:00.000Z","formattedDate":"March 20, 2021","tags":[{"label":"Bicep","permalink":"/tags/bicep"},{"label":"ARM templates","permalink":"/tags/arm-templates"},{"label":"Azure Pipelines","permalink":"/tags/azure-pipelines"},{"label":"Azure CLI","permalink":"/tags/azure-cli"}],"readingTime":4.9,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Bicep meet Azure Pipelines","authors":"johnnyreilly","tags":["Bicep","ARM templates","Azure Pipelines","Azure CLI"],"image":"./bicep-meet-azure-pipelines.png","hide_table_of_contents":false},"prevItem":{"title":"Bicep meet Azure Pipelines 2","permalink":"/2021/03/23/bicep-meet-azure-pipelines-2"},"nextItem":{"title":"RSS update; we moved to Docusaurus","permalink":"/2021/03/17/rss-update-we-moved-to-docusaurus"}},"content":"[Bicep](https://github.com/Azure/bicep) is a terser and more readable alternative language to ARM templates. Running ARM templates in Azure Pipelines is straightforward. However, there isn\'t yet a first class experience for running Bicep in Azure Pipelines. This post demonstrates an approach that can be used until a Bicep task is available.\\n\\n![Bicep meet Azure Pipelines](bicep-meet-azure-pipelines.png)\\n\\n## Bicep: mostly ARMless\\n\\nIf you\'ve been working with Azure and infrastructure as code, you\'ll likely have encountered [ARM templates](https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/overview). They\'re a domain specific language that lives inside JSON, used to define the infrastructure that is deployed to Azure; App Services, Key Vaults and the like.\\n\\nARM templates are quite verbose and not the easiest thing to read. This is a consequence of being effectively a language nestled inside another language. Bicep is an alternative language which is far more readable. Bicep transpiles down to ARM templates, in the same way that TypeScript transpiles down to JavaScript.\\n\\nBicep is quite new, but already it enjoys feature parity with ARM templates (as of [v0.3](https://github.com/Azure/bicep/releases/tag/v0.3.1)) and ships as part of the [Azure CLI](https://github.com/MicrosoftDocs/azure-docs-cli/blob/master/docs-ref-conceptual/release-notes-azure-cli/index.md#arm-1). However, as Bicep is new, it doesn\'t yet have a dedicated Azure Pipelines task for deployment. This should exist in future, perhaps as soon as the [v0.4 release](https://github.com/Azure/bicep/issues/1341). In the meantime there\'s an alternative way to achieve this which we\'ll go through.\\n\\n## App Service with Bicep\\n\\nLet\'s take a simple Bicep file, `azuredeploy.bicep`, which is designed to deploy an App Service resource to Azure. It looks like this:\\n\\n```bicep\\n@description(\'Tags that our resources need\')\\nparam tags object = {\\n  costCenter: \'todo: replace\'\\n  environment: \'todo: replace\'\\n  application: \'todo: replace with app name\'\\n  description: \'todo: replace\'\\n  managedBy: \'ARM\'\\n}\\n\\n@minLength(2)\\n@description(\'Base name of the resource such as web app name and app service plan\')\\nparam applicationName string\\n\\n@description(\'Location for all resources.\')\\nparam location string = resourceGroup().location\\n\\n@description(\'The SKU of App Service Plan\')\\nparam sku string\\n\\nvar appServicePlanName_var = \'plan-${applicationName}-${tags.environment}\'\\nvar linuxFxVersion = \'DOTNETCORE|5.0\'\\nvar fullApplicationName_var = \'app-${applicationName}-${uniqueString(applicationName)}\'\\n\\nresource appServicePlanName \'Microsoft.Web/serverfarms@2019-08-01\' = {\\n  name: appServicePlanName_var\\n  location: location\\n  sku: {\\n    name: sku\\n  }\\n  kind: \'linux\'\\n  tags: {\\n    CostCenter: tags.costCenter\\n    Environment: tags.environment\\n    Description: tags.description\\n    ManagedBy: tags.managedBy\\n  }\\n  properties: {\\n    reserved: true\\n  }\\n}\\n\\nresource fullApplicationName \'Microsoft.Web/sites@2018-11-01\' = {\\n  name: fullApplicationName_var\\n  location: location\\n  kind: \'app\'\\n  tags: {\\n    CostCenter: tags.costCenter\\n    Environment: tags.environment\\n    Description: tags.description\\n    ManagedBy: tags.managedBy\\n  }\\n  properties: {\\n    serverFarmId: appServicePlanName.id\\n    clientAffinityEnabled: true\\n    siteConfig: {\\n      appSettings: []\\n      linuxFxVersion: linuxFxVersion\\n      alwaysOn: false\\n      ftpsState: \'Disabled\'\\n      http20Enabled: true\\n      minTlsVersion: \'1.2\'\\n      remoteDebuggingEnabled: false\\n    }\\n    httpsOnly: true\\n  }\\n  identity: {\\n    type: \'SystemAssigned\'\\n  }\\n}\\n\\noutput fullApplicationName string = fullApplicationName_var\\n```\\n\\nWhen transpiled down to an ARM template, this Bicep file more than doubles in size:\\n\\n- `azuredeploy.bicep` - 1782 bytes\\n- `azuredeploy.json` - 3863 bytes\\n\\nThis tells you something of the advantage of Bicep. The template comes with an associated `azuredeploy.parameters.json` file:\\n\\n```json\\n{\\n  \\"$schema\\": \\"https://schema.management.azure.com/schemas/2019-04-01/deploymentParameters.json#\\",\\n  \\"contentVersion\\": \\"1.0.0.0\\",\\n  \\"parameters\\": {\\n    \\"tags\\": {\\n      \\"value\\": {\\n        \\"costCenter\\": \\"8888\\",\\n        \\"environment\\": \\"stg\\",\\n        \\"application\\": \\"hello-azure\\",\\n        \\"description\\": \\"App Service for hello-azure\\",\\n        \\"managedBy\\": \\"ARM\\"\\n      }\\n    },\\n    \\"sku\\": {\\n      \\"value\\": \\"B1\\"\\n    }\\n  }\\n}\\n```\\n\\nIt\'s worth remembering that you can use the same parameters files with Bicep that you can use with ARM templates. This is great for minimising friction when it comes to migrating.\\n\\n## Bicep in `azure-pipelines.yml`\\n\\nNow we have our Bicep file, we want to execute it from the context of an Azure Pipeline. If we were working directly with the ARM template we\'d likely have something like this in place:\\n\\n```yml\\n- task: AzureResourceManagerTemplateDeployment@3\\n  displayName: \'Deploy Hello Azure ARM\'\\n  inputs:\\n    azureResourceManagerConnection: \'$(azureSubscription)\'\\n    action: Create Or Update Resource Group\\n    resourceGroupName: \'$(resourceGroupName)\'\\n    location: \'North Europe\'\\n    templateLocation: Linked artifact\\n    csmFile: \'infra/app-service/azuredeploy.json\'\\n    csmParametersFile: \'infra/app-service/azuredeploy.parameters.json\'\\n    deploymentMode: Incremental\\n    deploymentOutputs: resourceGroupDeploymentOutputs\\n    overrideParameters: -applicationName $(Build.Repository.Name)\\n\\n- pwsh: |\\n    $outputs = ConvertFrom-Json \'$(resourceGroupDeploymentOutputs)\'\\n    foreach ($output in $outputs.PSObject.Properties) {\\n        Write-Host \\"##vso[task.setvariable variable=RGDO_$($output.Name)]$($output.Value.value)\\"\\n    }\\n  displayName: \'Turn ARM outputs into variables\'\\n```\\n\\nThere\'s two tasks above. The first is the native task for ARM deployments which takes our ARM template and our parameters and deploys them. The second task takes the output variables from the first task and converts them into Azure Pipeline variables such that they can be referenced later in the pipeline. In this case this variablifies our `fullApplicationName` output.\\n\\nThere is, as yet, no `BicepTemplateDeployment@1`. [Though it\'s coming](https://github.com/Azure/bicep/issues/1341). In the meantime, the marvellous [Alex Frankel](https://twitter.com/adotfrank) [advised](https://github.com/Azure/bicep/issues/1341#issuecomment-802010110):\\n\\n> I\'d recommend using the [Azure CLI task](https://docs.microsoft.com/azure/devops/pipelines/tasks/deploy/azure-cli?view=azure-devops) to deploy. As long as that task is updated to Az CLI version 2.20 or later, it will automatically install the bicep CLI when calling `az deployment group create -f main.bicep`.\\n\\nLet\'s give it a go!\\n\\n```yml\\n- task: AzureCLI@2\\n  displayName: \'Deploy Hello Azure Bicep\'\\n  inputs:\\n    azureSubscription: \'$(azureSubscription)\'\\n    scriptType: bash\\n    scriptLocation: inlineScript\\n    inlineScript: |\\n      az --version\\n\\n      echo \\"az deployment group create --resource-group \'$(resourceGroupName)\' --name appservicedeploy\\"\\n      az deployment group create --resource-group \'$(resourceGroupName)\' --name appservicedeploy \\\\\\n        --template-file infra/app-service/azuredeploy.bicep \\\\\\n        --parameters infra/app-service/azuredeploy.parameters.json \\\\\\n        --parameters applicationName=\'$(Build.Repository.Name)\'\\n\\n      echo \\"az deployment group show --resource-group \'$(resourceGroupName)\' --name appservicedeploy\\"\\n      deploymentoutputs=$(az deployment group show --resource-group \'$(resourceGroupName)\' --name appservicedeploy \\\\\\n        --query properties.outputs)\\n\\n      echo \'convert outputs to variables\'\\n      echo $deploymentoutputs | jq -c \'. | to_entries[] | [.key, .value.value]\' |\\n        while IFS=$\\"\\\\n\\" read -r c; do\\n          outputname=$(echo \\"$c\\" | jq -r \'.[0]\')\\n          outputvalue=$(echo \\"$c\\" | jq -r \'.[1]\')\\n          echo \\"setting variable RGDO_$outputname=$outputvalue\\"\\n          echo \\"##vso[task.setvariable variable=RGDO_$outputname]$outputvalue\\"\\n        done\\n```\\n\\nThe above is just a single Azure CLI task (as advised). It invokes `az deployment group create` passing the relevant parameters. It then acquires the output properties using `az deployment group show`. Finally it once again converts these outputs to Azure Pipeline variables with some [`jq`](https://stedolan.github.io/jq/) smarts.\\n\\nThis works right now, and running it results in something like the output below. So if you\'re excited about Bicep and don\'t want to wait for 0.4 to start moving on this, then this can get you going. To track the progress of the custom task, [keep an eye on this issue](https://github.com/Azure/bicep/issues/1341).\\n\\n![Bicep in an Azure Pipeline](bicep-in-a-pipeline.png)\\n\\n## Update: an even simpler alternative\\n\\nThere is even a simpler way to do this which I discovered subsequent to writing this. [Have a read](./2021-03-23-bicep-meet-azure-pipelines-2/index.md)."},{"id":"/2021/03/17/rss-update-we-moved-to-docusaurus","metadata":{"permalink":"/2021/03/17/rss-update-we-moved-to-docusaurus","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-03-17-rss-update-we-moved-to-docusaurus/index.md","source":"@site/blog/2021-03-17-rss-update-we-moved-to-docusaurus/index.md","title":"RSS update; we moved to Docusaurus","description":"My blog lived happily on Blogger for the past decade. It\'s now built with Docusaurus and hosted on GitHub Pages. To understand the why, read my last post. This post serves purely to share details of feed updates for RSS / Atom subscribers.","date":"2021-03-17T00:00:00.000Z","formattedDate":"March 17, 2021","tags":[{"label":"Blogger","permalink":"/tags/blogger"},{"label":"Docusaurus","permalink":"/tags/docusaurus"},{"label":"RSS","permalink":"/tags/rss"},{"label":"Atom","permalink":"/tags/atom"}],"readingTime":0.555,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"RSS update; we moved to Docusaurus","authors":"johnnyreilly","tags":["Blogger","Docusaurus","RSS","Atom"],"image":"./rss.png","hide_table_of_contents":false},"prevItem":{"title":"Bicep meet Azure Pipelines","permalink":"/2021/03/20/bicep-meet-azure-pipelines"},"nextItem":{"title":"From Blogger to Docusaurus","permalink":"/2021/03/15/from-blogger-to-docusaurus"}},"content":"My blog lived happily on [Blogger](https://icanmakethiswork.blogspot.com/) for the past decade. It\'s now built with [Docusaurus](https://v2.docusaurus.io/) and hosted on [GitHub Pages](https://pages.github.com/). To understand the why, [read my last post](./2021-03-15-from-blogger-to-docusaurus/index.md). This post serves purely to share details of feed updates for RSS / Atom subscribers.\\n\\nThe Atom feed at this location no longer exists: https://blog.johnnyreilly.com/feeds/posts/default\\n\\nThe following feeds are new and different:\\n\\n- RSS - https://blog.johnnyreilly.com/rss.xml\\n- Atom - https://blog.johnnyreilly.com/atom.xml\\n\\nThe new format might mess with any feed reader you have set up. I do apologise for the friction; hopefully it shouldn\'t cause you too much drama.\\n\\nFinally, all historic links should continue to work with the new site; redirects have been implemented."},{"id":"/2021/03/15/from-blogger-to-docusaurus","metadata":{"permalink":"/2021/03/15/from-blogger-to-docusaurus","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-03-15-from-blogger-to-docusaurus/index.md","source":"@site/blog/2021-03-15-from-blogger-to-docusaurus/index.md","title":"From Blogger to Docusaurus","description":"Docusaurus is, amongst other things, a Markdown powered blogging platform. My blog has lived happily on Blogger for the past decade. I\'m considering moving, but losing my historic content as part of the move was never an option. This post goes through what it would look like to move from Blogger to Docusaurus without losing your content.","date":"2021-03-15T00:00:00.000Z","formattedDate":"March 15, 2021","tags":[{"label":"Blogger","permalink":"/tags/blogger"},{"label":"Docusaurus","permalink":"/tags/docusaurus"}],"readingTime":8.05,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"From Blogger to Docusaurus","authors":"johnnyreilly","tags":["Blogger","Docusaurus"],"image":"./docusaurus.png","hide_table_of_contents":false},"prevItem":{"title":"RSS update; we moved to Docusaurus","permalink":"/2021/03/17/rss-update-we-moved-to-docusaurus"},"nextItem":{"title":"Managed Identity, Azure SQL and Entity Framework","permalink":"/2021/03/10/managed-identity-azure-sql-entity-framework"}},"content":"[Docusaurus](https://v2.docusaurus.io/) is, amongst other things, a Markdown powered blogging platform. My blog has lived happily on [Blogger](https://www.blogger.com/) for the past decade. I\'m considering moving, but losing my historic content as part of the move was never an option. This post goes through what it would look like to move from Blogger to Docusaurus _without_ losing your content.\\n\\nIt is imperative that the world never forgets what I was doing with jQuery in 2012.\\n\\n## Blog as code\\n\\nEverything is better when it\'s code. Infrastructure as code. Awesome right? So naturally \\"blog as code\\" must be better than just a blog. More seriously, [Markdown](https://en.wikipedia.org/wiki/Markdown) is a tremendous documentation format. Simple, straightforward and, like Goldilocks, \\"just right\\". For a long time I\'ve written everything as Markdown. My years of toil down the Open Source mines have preconditioned me to be very MD-disposed.\\n\\nI started out writing this blog a long time ago as pure HTML. Not the smoothest of writing formats. At some point I got into the habit of spinning up a new repo in GitHub for a new blogpost, writing it in Markdown and piping it through a variety of tools to convert it into HTML for publication on Blogger. As time passed I felt I\'d be a lot happier if I wasn\'t creating a repo each time. What if I did all my blogging in a single repo and used that as the code that represented my blog?\\n\\nJust having that thought laid the seeds for what was to follow:\\n\\n1. An investigation into importing my content from Blogger into a GitHub repo\\n2. An experimental port to Docusaurus\\n3. The automation of publication to Docusaurus and Blogger\\n\\nWe\'re going to go through 1 and 2 now. But before we do that, let\'s create ourselves a Docusaurus site for our blog:\\n\\n```\\nnpx @docusaurus/init@latest init blog-website classic\\n```\\n\\n## I want everything\\n\\nThe first thing to do, was obtain my blog content. This is a mass of HTML that lived inside Blogger\'s database. (One assumes they have a database; I haven\'t actually checked.) There\'s a \\"Back up content\\" option inside Blogger to allow this:\\n\\n![Download content from Blogger](blogger-back-up-your-content.png)\\n\\nIt provides you with an XML file with a dispiritingly small size. Ten years blogging? You\'ll get change out of 4Mb it turns out.\\n\\n## From HTML in XML to Markdown\\n\\nWe now want to take that XML and:\\n\\n- Extract each blog post (and it\'s associated metadata; title / tags and whatnot)\\n- Convert the HTML content of each blog post from HTML to Markdown and save it as a `/index.md` file\\n- Download the images used in the blogpost so they can be stored in the repo alongside\\n\\nTo do this we\'re going to whip up a smallish TypeScript console app. Let\'s initialise it with the packages we\'re going to need:\\n\\n```\\nmkdir from-blogger-to-docusaurus\\ncd from-blogger-to-docusaurus\\nnpx typescript --init\\nyarn init\\nyarn add @types/axios @types/he @types/jsdom @types/node @types/showdown axios fast-xml-parser he jsdom showdown ts-node typescript\\n```\\n\\nWe\'re using:\\n\\n- [`fast-xml-parser`](https://github.com/NaturalIntelligence/fast-xml-parser) to parse XML\\n- [`he`](https://github.com/mathiasbynens/he), [jsdom](https://github.com/jsdom/jsdom) and [showdown](https://github.com/showdownjs/showdown) to convert HTML to Markdown\\n- [`axios`](https://github.com/axios/axios) to download images\\n- [`typescript`](https://github.com/microsoft/TypeScript) to code in and [`ts-node`](https://github.com/TypeStrong/ts-node) to make our TypeScript Node.js console app.\\n\\nNow we have all the packages we need, it\'s time to write our script.\\n\\n```ts\\nimport fs from \'fs\';\\nimport path from \'path\';\\nimport showdown from \'showdown\';\\nimport he from \'he\';\\nimport jsdom from \'jsdom\';\\nimport axios from \'axios\';\\nimport fastXmlParser from \'fast-xml-parser\';\\n\\nconst bloggerXmlPath = \'./blog-03-13-2021.xml\';\\nconst docusaurusDirectory = \'../blog-website\';\\nconst notMarkdownable: string[] = [];\\n\\nasync function fromXmlToMarkDown() {\\n  const posts = await getPosts();\\n\\n  for (const post of posts) {\\n    await makePostIntoMarkDownAndDownloadImages(post);\\n  }\\n  if (notMarkdownable.length)\\n    console.log(\\n      \'These blog posts could not be turned into MarkDown - go find out why!\',\\n      notMarkdownable\\n    );\\n}\\n\\nasync function getPosts(): Promise<Post[]> {\\n  const xml = await fs.promises.readFile(bloggerXmlPath, \'utf-8\');\\n\\n  const options = {\\n    attributeNamePrefix: \'@_\',\\n    attrNodeName: \'attr\', //default is \'false\'\\n    textNodeName: \'#text\',\\n    ignoreAttributes: false,\\n    ignoreNameSpace: false,\\n    allowBooleanAttributes: true,\\n    parseNodeValue: true,\\n    parseAttributeValue: true,\\n    trimValues: true,\\n    cdataTagName: \'__cdata\', //default is \'false\'\\n    cdataPositionChar: \'\\\\\\\\c\',\\n    parseTrueNumberOnly: false,\\n    arrayMode: true, //\\"strict\\"\\n    attrValueProcessor: (val: string, attrName: string) =>\\n      he.decode(val, { isAttributeValue: true }), //default is a=>a\\n    tagValueProcessor: (val: string, tagName: string) => he.decode(val), //default is a=>a\\n  };\\n\\n  const traversalObj = fastXmlParser.getTraversalObj(xml, options);\\n  const blog = fastXmlParser.convertToJson(traversalObj, options);\\n\\n  const postsRaw = blog.feed[0].entry.filter(\\n    (entry: any) =>\\n      entry.category.some(\\n        (category: any) =>\\n          category.attr[\'@_term\'] ===\\n          \'http://schemas.google.com/blogger/2008/kind#post\'\\n      ) &&\\n      entry.link.some(\\n        (link: any) =>\\n          link.attr[\'@_href\'] && link.attr[\'@_type\'] === \'text/html\'\\n      ) &&\\n      entry.published < \'2021-03-07\'\\n  );\\n\\n  const posts: Post[] = postsRaw.map((entry: any) => {\\n    return {\\n      title: entry.title[0][\'#text\'],\\n      content: entry.content[0][\'#text\'],\\n      published: entry.published,\\n      link: entry.link.find(\\n        (link: any) =>\\n          link.attr[\'@_href\'] && link.attr[\'@_type\'] === \'text/html\'\\n      )\\n        ? entry.link.find(\\n            (link: any) =>\\n              link.attr[\'@_href\'] && link.attr[\'@_type\'] === \'text/html\'\\n          ).attr[\'@_href\']\\n        : undefined,\\n      tags:\\n        Array.isArray(entry.category) &&\\n        entry.category.some(\\n          (category: any) =>\\n            category.attr[\'@_scheme\'] === \'http://www.blogger.com/atom/ns#\'\\n        )\\n          ? entry.category\\n              .filter(\\n                (category: any) =>\\n                  category.attr[\'@_scheme\'] ===\\n                    \'http://www.blogger.com/atom/ns#\' &&\\n                  category.attr[\'@_term\'] !== \'constructor\'\\n              ) // \'constructor\' will make docusaurus choke\\n              .map((category: any) => category.attr[\'@_term\'])\\n          : [],\\n    };\\n  });\\n\\n  for (const post of posts) {\\n    const { content, ...others } = post;\\n    console.log(others, content.length);\\n    if (!content || !others.title || !others.published)\\n      throw new Error(\'No content\');\\n  }\\n\\n  return posts.filter((post) => post.link);\\n}\\n\\nasync function makePostIntoMarkDownAndDownloadImages(post: Post) {\\n  const converter = new showdown.Converter({\\n    ghCodeBlocks: true,\\n  });\\n  const linkSections = post.link.split(\'/\');\\n  const linkSlug = linkSections[linkSections.length - 1];\\n  const filename =\\n    post.published.substr(0, 10) + \'-\' + linkSlug.replace(\'.html\', \'/index.md\');\\n\\n  const contentProcessed = post.content\\n    // remove stray <br /> tags\\n    .replace(/<br\\\\s*\\\\/?>/gi, \'\\\\n\')\\n    // translate <code class=\\"lang-cs\\" into <code class=\\"language-cs\\"> to be showdown friendly\\n    .replace(/code class=\\"lang-/gi, \'code class=\\"language-\');\\n\\n  const images: string[] = [];\\n  const dom = new jsdom.JSDOM(contentProcessed);\\n  let markdown = \'\';\\n  try {\\n    markdown = converter\\n      .makeMarkdown(contentProcessed, dom.window.document)\\n      // bigger titles\\n      .replace(/#### /g, \'## \')\\n\\n      // <div style=\\"width:100%;height:0;padding-bottom:56%;position:relative;\\"><iframe src=\\"https://giphy.com/embed/l7JDTHpsXM26k\\" width=\\"100%\\" height=\\"100%\\" style=\\"position:absolute\\" frameBorder=\\"0\\" class=\\"giphy-embed\\" allowFullScreen=\\"\\"></iframe></div>\\n\\n      // The mechanism below extracts the underlying iframe\\n      .replace(/<div.*(<iframe.*\\">).*<\\\\/div>/g, (replacer) => {\\n        const dom = new jsdom.JSDOM(replacer);\\n        const iframe = dom?.window?.document?.querySelector(\'iframe\');\\n        return iframe?.outerHTML ?? \'\';\\n      })\\n\\n      // The mechanism below strips class and style attributes from iframes - react hates them\\n      .replace(/<iframe.*<\\\\/iframe>/g, (replacer) => {\\n        const dom = new jsdom.JSDOM(replacer);\\n        const iframe = dom?.window?.document?.querySelector(\'iframe\');\\n        iframe?.removeAttribute(\'class\');\\n        iframe?.removeAttribute(\'style\');\\n        return iframe?.outerHTML ?? \'\';\\n      })\\n\\n      // capitalise appropriately\\n      .replace(/frameBorder/g, \'frameBorder\')\\n      .replace(/allowFullScreen/g, \'allowFullScreen\')\\n      .replace(/charset/g, \'charSet\')\\n\\n      // Deals with these:\\n      // [![null](<https://4.bp.blogspot.com/-b9-GrL0IXaY/Xmqj4GRhKXI/AAAAAAAAT5s/ZoceUInSY5EWXeCr2LkGV9Zvea8S6-mUgCPcBGAYYCw/s640/hello_world_idb_keyval.png> =640x484)](<https://4.bp.blogspot.com/-b9-GrL0IXaY/Xmqj4GRhKXI/AAAAAAAAT5s/ZoceUInSY5EWXeCr2LkGV9Zvea8S6-mUgCPcBGAYYCw/s1600/hello_world_idb_keyval.png>)We successfully wrote something into IndexedDB, read it back and printed that value to the console. Amazing!\\n      .replace(\\n        /\\\\[!\\\\[null\\\\]\\\\(<(.*?)>\\\\)/g,\\n        (match) =>\\n          `![](${match.slice(match.indexOf(\'<\') + 1, match.indexOf(\'>\'))})\\\\n\\\\n`\\n      )\\n\\n      // Blogger tends to put images in HTML that looks like this:\\n      // <div class=\\"separator\\" style=\\"clear: both;\\"><a href=\\"https://1.bp.blogspot.com/-UwrtZigWg78/YDqN82KbjVI/AAAAAAAAZTE/Umezr1MGQicnxMMr5rQHD4xKINg9fasDACLcBGAsYHQ/s783/traffic-to-app-service.png\\" style=\\"display: block; padding: 1em 0; text-align: center; \\"><img alt=\\"traffic to app service\\" border=\\"0\\" width=\\"600\\" data-original-height=\\"753\\" data-original-width=\\"783\\" src=\\"https://1.bp.blogspot.com/-UwrtZigWg78/YDqN82KbjVI/AAAAAAAAZTE/Umezr1MGQicnxMMr5rQHD4xKINg9fasDACLcBGAsYHQ/s600/traffic-to-app-service.png\\"></a></div>\\n\\n      // The mechanism below extracts the underlying image path and it\'s alt text\\n      .replace(/<div.*(<img.*\\">).*<\\\\/div>/g, (replacer) => {\\n        const div = new jsdom.JSDOM(replacer);\\n        const img = div?.window?.document?.querySelector(\'img\');\\n        const alt = img?.getAttribute(\'alt\') ?? \'\';\\n        const src = img?.getAttribute(\'src\') ?? \'\';\\n\\n        if (src) images.push(src);\\n\\n        return `![${alt}](${src})`;\\n      });\\n  } catch (e) {\\n    console.log(post.link);\\n    console.log(e);\\n    notMarkdownable.push(post.link);\\n    return;\\n  }\\n\\n  const imageDirectory = filename.replace(\'/index.md\', \'\');\\n  for (const url of images) {\\n    try {\\n      const localUrl = await downloadImage(url, imageDirectory);\\n      markdown = markdown.replace(url, \'blog/\' + localUrl);\\n    } catch (e) {\\n      console.error(`Failed to download ${url}`);\\n    }\\n  }\\n\\n  const content = `---\\ntitle: \\"${post.title}\\"\\nauthor: John Reilly\\nauthor_url: https://github.com/johnnyreilly\\nauthor_image_url: https://avatars.githubusercontent.com/u/1010525?s=400&u=294033082cfecf8ad1645b4290e362583b33094a&v=4\\ntags: [${post.tags.join(\', \')}]\\nhide_table_of_contents: false\\n---\\n${markdown}\\n`;\\n\\n  await fs.promises.writeFile(\\n    path.resolve(docusaurusDirectory, \'blog\', filename),\\n    content\\n  );\\n}\\n\\nasync function downloadImage(url: string, directory: string) {\\n  console.log(`Downloading ${url}`);\\n  const pathParts = new URL(url).pathname.split(\'/\');\\n  const filename = pathParts[pathParts.length - 1];\\n  const directoryTo = path.resolve(\\n    docusaurusDirectory,\\n    \'static\',\\n    \'blog\',\\n    directory\\n  );\\n  const pathTo = path.resolve(\\n    docusaurusDirectory,\\n    \'static\',\\n    \'blog\',\\n    directory,\\n    filename\\n  );\\n\\n  if (!fs.existsSync(directoryTo)) {\\n    fs.mkdirSync(directoryTo);\\n  }\\n\\n  const writer = fs.createWriteStream(pathTo);\\n\\n  const response = await axios({\\n    url,\\n    method: \'GET\',\\n    responseType: \'stream\',\\n  });\\n\\n  response.data.pipe(writer);\\n\\n  return new Promise<string>((resolve, reject) => {\\n    writer.on(\'finish\', () => resolve(directory + \'/\' + filename));\\n    writer.on(\'error\', reject);\\n  });\\n}\\n\\ninterface Post {\\n  title: string;\\n  content: string;\\n  published: string;\\n  link: string;\\n  tags: string[];\\n}\\n\\n// do it!\\nfromXmlToMarkDown();\\n```\\n\\nTo summarise what the script does, it:\\n\\n- parses the blog XML into an array of `Post`s\\n- each post is then converted from HTML into Markdown, a Docusaurus header is created and prepended, then the file is saved to the `blog-website/blog` directory\\n- the images of each post are downloaded with Axios and saved to the `blog-website/static/blog/{POST NAME}` directory\\n\\n## Bringing it all together\\n\\nTo run the script, we add the following script to the `package.json`:\\n\\n```\\n  \\"scripts\\": {\\n    \\"start\\": \\"ts-node index.ts\\"\\n  },\\n```\\n\\nAnd have ourselves a merry little `yarn start` to kick off the process. In a very short period of time, if you crack open the `blogs` directory of your Docusaurus site you\'ll see a collection of Markdown files which represent your blog and are ready to power Docusaurus:\\n\\n![Markdown files](blogs-as-markdown.png)\\n\\nI have slightly papered over some details here. For my own case I discovered that I hadn\'t always written perfect HTML when blogging. I had to go in and fix the HTML in a number of historic blogs such that the mechanism would work. I also learned that a number of my screenshots that I use to illustrate posts have vanished from Blogger at some point. This makes me all the more convinced that storing your blog in a repo is a good idea. Things should not \\"go missing\\".\\n\\nCongratulations! We\'re now the proud owners of a Docusaurus blog site based upon our Blogger content that looks something like this:\\n\\n![Blog in Docusaurus](docusaurus.png)\\n\\n## Making the move?\\n\\nNow that I\'ve got the content, I\'m theoretically safe to migrate from Blogger to Docusaurus. I\'m pondering this now and I have come up with a checklist of criteria to satisfy before I do. You can have a read of the [criteria here](https://github.com/johnnyreilly/blog.johnnyreilly.com#migrating-to-docusauras).\\n\\nOdds are, I\'m likely to make the move; it\'s probably just a matter of time."},{"id":"/2021/03/10/managed-identity-azure-sql-entity-framework","metadata":{"permalink":"/2021/03/10/managed-identity-azure-sql-entity-framework","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-03-10-managed-identity-azure-sql-entity-framework/index.md","source":"@site/blog/2021-03-10-managed-identity-azure-sql-entity-framework/index.md","title":"Managed Identity, Azure SQL and Entity Framework","description":"Managed Identity offers a very secure way for applications running in Azure to connect to Azure SQL databases. It\'s an approach that does not require code changes; merely configuration of connection string and associated resources. Hence it has a good developer experience. Importantly, it allows us to avoid exposing our database to username / password authentication, and hence making it a tougher target for bad actors.","date":"2021-03-10T00:00:00.000Z","formattedDate":"March 10, 2021","tags":[{"label":"connection string","permalink":"/tags/connection-string"},{"label":"managed identity","permalink":"/tags/managed-identity"},{"label":"entity framework","permalink":"/tags/entity-framework"},{"label":"Microsoft.Data.SqlClient","permalink":"/tags/microsoft-data-sql-client"}],"readingTime":4.925,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Managed Identity, Azure SQL and Entity Framework","authors":"johnnyreilly","tags":["connection string","managed identity","entity framework","Microsoft.Data.SqlClient"],"image":"./entity-framework-core-nuget.png","hide_table_of_contents":false},"prevItem":{"title":"From Blogger to Docusaurus","permalink":"/2021/03/15/from-blogger-to-docusaurus"},"nextItem":{"title":"NSwag: TypeScript and CSharp client generation based on an API","permalink":"/2021/03/06/generate-typescript-and-csharp-clients-with-nswag"}},"content":"Managed Identity offers a very secure way for applications running in Azure to connect to Azure SQL databases. It\'s an approach that does not require code changes; merely configuration of connection string and associated resources. Hence it has a good developer experience. Importantly, it allows us to avoid exposing our database to username / password authentication, and hence making it a tougher target for bad actors.\\n\\nThis post talks us through using managed identity for connecting to Azure SQL.\\n\\n## `Integrated Security=true`\\n\\nEveryone is deploying to the cloud. Few are the organisations that view deployment to data centers they manage as the future. This is generally a good thing, however in the excitement of the new, it\'s possible to forget some of the good properties that \\"on premise\\" deployment afforded when it came to connectivity and authentication.\\n\\nI speak of course, of our old friend `Integrated Security=true`. When you seek to connect a web application to a database, you\'ll typically use some kind of database connection string. And back in the day, it may have looked something like this:\\n\\n```\\nData Source=myServer;Initial Catalog=myDB;Integrated Security=true;\\n```\\n\\nThe above provides a database server, a database and also `Integrated Security=true`. When you see `Integrated Security=true`, what you\'re essentially looking at is an instruction to use the identity that an application is running under (typically called a \\"service account\\") as the authentication credential to secure access to the database. Under the covers, this amounts to [Windows Authentication](https://docs.microsoft.com/en-us/dotnet/framework/data/adonet/sql/authentication-in-sql-server).\\n\\nThe significant thing about this approach is that it is more secure than using usernames and passwords in the connection string. If you have to use username and password to authenticate, then you need to persist them somewhere - so you need to make sure that\'s secure. Also, if someone manages to acquire that username and password, they\'re free to get access to the database and do malicious things.\\n\\nBottom line: the less you are sharing authentication credentials, the better your security. Integrated Security is a harder nut to crack than username and password. The thing to note about the above phrase is \\"Windows Authentication\\". Web Apps in Azure / AWS etc do not typically use Windows Authentication when it comes to connecting to the database. Connecting with username / password is far more common.\\n\\nWhat if there was a way to have the developer experience of `Integrated Security=true` without needing to use Windows Authentication? There is.\\n\\n## Managed Identity\\n\\nThe docs express the purpose of [managed identity](https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview) well:\\n\\n> A common challenge for developers is the management of secrets and credentials to secure communication between different services. On Azure, managed identities eliminate the need for developers having to manage credentials by providing an identity for the Azure resource in Azure AD and using it to obtain Azure Active Directory (Azure AD) tokens\\n\\nHistorically a certain amount of ceremony was required to use managed identity to connect to a database, and could involve augmenting a `DbContext` like so:\\n\\n```cs\\npublic MyDbContext(DbContextOptions options) : base(options) {\\n    var conn = (Microsoft.Data.SqlClient.SqlConnection)Database.GetDbConnection();\\n    var credential = new DefaultAzureCredential();\\n    var token = credential\\n        .GetToken(\\n            new Azure.Core.TokenRequestContext(new[] { \\"https://database.windows.net/.default\\" })\\n        );\\n    conn.AccessToken = token.Token;\\n}\\n```\\n\\nThis mechanism works, and has the tremendous upside of no longer requiring credentials be passed in a connection string. However, as you can see this isn\'t the simplest of setups. And also, what if you don\'t want to use managed identity when you\'re developing locally? This approach has baggage and forces us to make code changes.\\n\\n## Connection String alone\\n\\nThe wonderful aspect of the original `Integrated Security=true` approach, was that there were no code changes required; one need only supply the connection string. Just configuration.\\n\\nThis is now possible with Azure SQL thanks to [this PR](https://github.com/dotnet/SqlClient/pull/730) to the [Microsoft.Data.SqlClient](https://www.nuget.org/packages/Microsoft.Data.SqlClient/) nuget package. (Incidentally, [Microsoft.Data.SqlClient is the successor to System.Data.SqlClient.](https://devblogs.microsoft.com/dotnet/introducing-the-new-microsoftdatasqlclient/))\\n\\nSupport for connection string managed identities [shipped with v2.1](https://github.com/dotnet/SqlClient/blob/master/release-notes/2.1/2.1.0/index.md#Azure-Active-Directory-Managed-Identity-authentication). Connection strings can look slightly different depending on the type of managed identity you\'re using:\\n\\n```\\n// For System Assigned Managed Identity\\n\\"Server:{serverURL}; Authentication=Active Directory MSI; Initial Catalog={db};\\"\\n\\n// For System Assigned Managed Identity\\n\\"Server:{serverURL}; Authentication=Active Directory Managed Identity; Initial Catalog={db};\\"\\n\\n// For User Assigned Managed Identity\\n\\"Server:{serverURL}; Authentication=Active Directory MSI; User Id={ObjectIdOfManagedIdentity}; Initial Catalog={db};\\"\\n\\n// For User Assigned Managed Identity\\n\\"Server:{serverURL}; Authentication=Active Directory Managed Identity; User Id={ObjectIdOfManagedIdentity}; Initial Catalog={db};\\"\\n```\\n\\nRegardless of the approach, you can see that none of the connection strings have credentials in them. And that\'s special.\\n\\n## Usage with Entity Framework Core 5\\n\\nIf you\'re using Entity Framework Core, you might be struggling to get this working and encountering strange error messages. In my ASP.NET project I had a dependendency on\\n[Microsoft.EntityFrameworkCore.SqlServer@5](https://www.nuget.org/packages/Microsoft.EntityFrameworkCore.SqlServer/5.0.4).\\n\\n![Microsoft.EntityFrameworkCore.SqlServer@5 in NuGet](entity-framework-core-nuget.png)\\n\\nIf you look close above, you\'ll see that the package has a dependency on Microsoft.Data.SqlClient, but crucially on 2.0.1 or greater. So if `dotnet` has installed a version of Microsoft.Data.SqlClient which is _less_ than 2.1 then the functionality required will not be installed. The resolution is simple, ensure that the required version is installed:\\n\\n```\\ndotnet add package Microsoft.Data.SqlClient --version 2.1.2\\n```\\n\\nThe version which we want to use is 2.1 (or greater) and fortunately that is compatible with Entity Framework Core 5. Incidentally, when Entity Framework Core 6 ships it will no longer be necessary to manually specify this dependency as it already requires Microsoft.Data.SqlClient@2.1 as a minimum.\\n\\n## User Assigned Managed Identity\\n\\nIf you\'re using user assigned managed identity, you\'ll need to supply the object id of your managed identity, which you can find in the [Azure Portal](https://portal.azure.com/):\\n\\n![Managed Identity object id](managed-identity-object-id.png)\\n\\nYou can configure this in ARM as well, but cryptically, the object id goes by the nom de plume of `principalId` (thanks to my partner in crime [John McCormick](https://github.com/jmccor99) for puzzling that out):\\n\\n```json\\n\\"CONNECTIONSTRINGS__OURDBCONNECTION\\": \\"[concat(\'Server=tcp:\', parameters(\'sqlServerName\') , \'.database.windows.net,1433;Initial Catalog=\', parameters(\'sqlDatabaseName\'),\';Authentication=Active Directory MSI\',\';User Id=\', reference(resourceId(\'Microsoft.ManagedIdentity/userAssignedIdentities/\', parameters(\'managedIdentityName\')), \'2018-11-30\').principalId)]\\"\\n```\\n\\nThat\'s it! With managed identity handling your authentication you can sleep easy, knowing you should be in a better place security wise."},{"id":"/2021/03/06/generate-typescript-and-csharp-clients-with-nswag","metadata":{"permalink":"/2021/03/06/generate-typescript-and-csharp-clients-with-nswag","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-03-06-generate-typescript-and-csharp-clients-with-nswag/index.md","source":"@site/blog/2021-03-06-generate-typescript-and-csharp-clients-with-nswag/index.md","title":"NSwag: TypeScript and CSharp client generation based on an API","description":"Generating clients for APIs is a tremendous way to reduce the amount of work you have to do when you\'re building a project. Why handwrite that code when it can be auto-generated for you quickly and accurately by a tool like NSwag? To quote the docs:","date":"2021-03-06T00:00:00.000Z","formattedDate":"March 6, 2021","tags":[{"label":"NSwag","permalink":"/tags/n-swag"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"CSharp","permalink":"/tags/c-sharp"},{"label":"API","permalink":"/tags/api"}],"readingTime":8.505,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"NSwag: TypeScript and CSharp client generation based on an API","authors":"johnnyreilly","tags":["NSwag","TypeScript","CSharp","API"],"image":"./use-generated-client.gif","hide_table_of_contents":false},"prevItem":{"title":"Managed Identity, Azure SQL and Entity Framework","permalink":"/2021/03/10/managed-identity-azure-sql-entity-framework"},"nextItem":{"title":"Goodbye Client Affinity, Hello Data Protection with Azure","permalink":"/2021/02/27/goodbye-client-affinity-hello-data-protection-with-azure"}},"content":"Generating clients for APIs is a tremendous way to reduce the amount of work you have to do when you\'re building a project. Why handwrite that code when it can be auto-generated for you quickly and accurately by a tool like [NSwag](https://github.com/RicoSuter/NSwag)? To quote the docs:\\n\\n> The NSwag project provides tools to generate OpenAPI specifications from existing ASP.NET Web API controllers and client code from these OpenAPI specifications. The project combines the functionality of Swashbuckle (OpenAPI/Swagger generation) and AutoRest (client generation) in one toolchain.\\n\\nThere\'s some great posts out there that show you how to generate the clients with NSwag using an `nswag.json` file directly from a .NET project.\\n\\nHowever, what if you want to use NSwag purely for its client generation capabilities? You may have an API written with another language / platform that exposes a Swagger endpoint, that you simply wish to create a client for. How do you do that? Also, if you want to do some special customisation of the clients you\'re generating, you may find yourself struggling to configure that in `nswag.json`. In that case, it\'s possible to hook into NSwag directly to do this with a simple .NET console app.\\n\\nThis post will:\\n\\n- Create a .NET API which exposes a Swagger endpoint. (Alternatively, you could use any other Swagger endpoint; [for example an Express API](https://blog.logrocket.com/documenting-your-express-api-with-swagger/).)\\n- Create a .NET console app which can create both TypeScript and CSharp clients from a Swagger endpoint.\\n- Create a script which, when run, creates a TypeScript client.\\n- Consume the API using the generated client in a simple TypeScript application.\\n\\nYou will need both [Node.js](https://nodejs.org/en/) and the [.NET SDK](https://dotnet.microsoft.com/download) installed.\\n\\n## Create an API\\n\\nWe\'ll now create an API which exposes a [Swagger / Open API](https://swagger.io/resources/open-api/) endpoint. Whilst we\'re doing that we\'ll create a TypeScript React app which we\'ll use later on. We\'ll drop to the command line and enter the following commands which use the .NET SDK, node and the `create-react-app` package:\\n\\n```shell\\nmkdir src\\ncd src\\nnpx create-react-app client-app --template typescript\\nmkdir server-app\\ncd server-app\\ndotnet new api -o API\\ncd API\\ndotnet add package NSwag.AspNetCore\\n```\\n\\nWe now have a .NET API with a dependency on NSwag. We\'ll start to use it by replacing the `Startup.cs` that\'s been generated with the following:\\n\\n```cs\\nusing Microsoft.AspNetCore.Builder;\\nusing Microsoft.AspNetCore.Hosting;\\nusing Microsoft.Extensions.Configuration;\\nusing Microsoft.Extensions.DependencyInjection;\\nusing Microsoft.Extensions.Hosting;\\n\\nnamespace API\\n{\\n    public class Startup\\n    {\\n        const string ALLOW_DEVELOPMENT_CORS_ORIGINS_POLICY = \\"AllowDevelopmentSpecificOrigins\\";\\n        const string LOCAL_DEVELOPMENT_URL = \\"http://localhost:3000\\";\\n\\n        public Startup(IConfiguration configuration)\\n        {\\n            Configuration = configuration;\\n        }\\n\\n        public IConfiguration Configuration { get; }\\n\\n        // This method gets called by the runtime. Use this method to add services to the container.\\n        public void ConfigureServices(IServiceCollection services)\\n        {\\n\\n            services.AddControllers();\\n\\n            services.AddCors(options => {\\n                options.AddPolicy(name: ALLOW_DEVELOPMENT_CORS_ORIGINS_POLICY,\\n                    builder => {\\n                        builder.WithOrigins(LOCAL_DEVELOPMENT_URL)\\n                            .AllowAnyMethod()\\n                            .AllowAnyHeader()\\n                            .AllowCredentials();\\n                    });\\n            });\\n\\n            // Register the Swagger services\\n            services.AddSwaggerDocument();\\n        }\\n\\n        // This method gets called by the runtime. Use this method to configure the HTTP request pipeline.\\n        public void Configure (IApplicationBuilder app, IWebHostEnvironment env)\\n        {\\n            if (env.IsDevelopment())\\n            {\\n                app.UseDeveloperExceptionPage();\\n            }\\n            else\\n            {\\n                app.UseExceptionHandler(\\"/Error\\");\\n                app.UseHsts ();\\n                app.UseHttpsRedirection();\\n            }\\n\\n            app.UseDefaultFiles();\\n            app.UseStaticFiles();\\n\\n            app.UseRouting();\\n\\n            app.UseAuthorization();\\n\\n            // Register the Swagger generator and the Swagger UI middlewares\\n            app.UseOpenApi();\\n            app.UseSwaggerUi3();\\n\\n            if (env.IsDevelopment())\\n                app.UseCors(ALLOW_DEVELOPMENT_CORS_ORIGINS_POLICY);\\n\\n            app.UseEndpoints(endpoints =>\\n            {\\n                endpoints.MapControllers();\\n            });\\n        }\\n    }\\n}\\n```\\n\\nThe significant changes in the above `Startup.cs` are:\\n\\n1. Exposing a Swagger endpoint with `UseOpenApi` and `UseSwaggerUi3`. NSwag will automagically create Swagger endpoints in your application for all your controllers. The .NET template ships with a `WeatherForecastController`.\\n2. Allowing [Cross-Origin Requests (CORS)](https://docs.microsoft.com/en-us/aspnet/core/security/cors) which is useful during development (and will facilitate a demo later).\\n\\nBack in the root of our project we\'re going to initialise an npm project. We\'re going to use this to put in place a number of handy [`npm scripts`](https://docs.npmjs.com/cli/v6/using-npm/scripts) that will make our project easier to work with. So we\'ll `npm init` and accept all the defaults.\\n\\nNow we\'re going add some dependencies which our scripts will use: `npm install cpx cross-env npm-run-all start-server-and-test`\\n\\nWe\'ll also add ourselves some `scripts` to our `package.json`:\\n\\n```json\\n\\"scripts\\": {\\n    \\"postinstall\\": \\"npm run install:client-app && npm run install:server-app\\",\\n    \\"install:client-app\\": \\"cd src/client-app && npm install\\",\\n    \\"install:server-app\\": \\"cd src/server-app/API && dotnet restore\\",\\n    \\"build\\": \\"npm run build:client-app && npm run build:server-app\\",\\n    \\"build:client-app\\": \\"cd src/client-app && npm run build\\",\\n    \\"postbuild:client-app\\": \\"cpx \\\\\\"src/client-app/build/**/*.*\\\\\\" \\\\\\"src/server-app/API/wwwroot/\\\\\\"\\",\\n    \\"build:server-app\\": \\"cd src/server-app/API && dotnet build --configuration release\\",\\n    \\"start\\": \\"run-p start:client-app start:server-app\\",\\n    \\"start:client-app\\": \\"cd src/client-app && npm start\\",\\n    \\"start:server-app\\": \\"cross-env ASPNETCORE_URLS=http://*:5000 ASPNETCORE_ENVIRONMENT=Development dotnet watch --project src/server-app/API run --no-launch-profile\\"\\n  }\\n```\\n\\nLet\'s walk through what the above scripts provide us with:\\n\\n- Running `npm install` in the root of our project will not only install dependencies for our root `package.json`, thanks to our `postinstall`, `install:client-app` and `install:server-app` scripts it will install the React app and .NET app dependencies as well.\\n- Running `npm run build` will build our client and server apps.\\n- Running `npm run start` will start both our React app and our .NET app. Our React app will be started at [http://localhost:3000](http://localhost:3000). Our .NET app will be started at [http://localhost:5000](http://localhost:5000) (some environment variables are passed to it with [`cross-env`](https://github.com/kentcdodds/cross-env) ).\\n\\nOnce `npm run start` has been run, you will find a Swagger endpoint at [http://localhost:5000/swagger](http://localhost:5000/swagger):\\n\\n![swagger screenshot](swagger.png)\\n\\n## The client generator project\\n\\nNow we\'ve scaffolded our Swagger-ed API, we want to put together the console app that will generate our typed clients.\\n\\n```shell\\ncd src/server-app\\ndotnet new console -o APIClientGenerator\\ncd APIClientGenerator\\ndotnet add package NSwag.CodeGeneration.CSharp\\ndotnet add package NSwag.CodeGeneration.TypeScript\\ndotnet add package NSwag.Core\\n```\\n\\nWe now have a console app with dependencies on the code generation portions of NSwag. Now let\'s change up `Program.cs` to make use of this:\\n\\n```cs\\nusing System;\\nusing System.IO;\\nusing System.Threading.Tasks;\\nusing NJsonSchema;\\nusing NJsonSchema.CodeGeneration.TypeScript;\\nusing NJsonSchema.Visitors;\\nusing NSwag;\\nusing NSwag.CodeGeneration.CSharp;\\nusing NSwag.CodeGeneration.TypeScript;\\n\\nnamespace APIClientGenerator\\n{\\n    class Program\\n    {\\n        static async Task Main(string[] args)\\n        {\\n            if (args.Length != 3)\\n                throw new ArgumentException(\\"Expecting 3 arguments: URL, generatePath, language\\");\\n\\n            var url = args[0];\\n            var generatePath = Path.Combine(Directory.GetCurrentDirectory(), args[1]);\\n            var language = args[2];\\n\\n            if (language != \\"TypeScript\\" && language != \\"CSharp\\")\\n                throw new ArgumentException(\\"Invalid language parameter; valid values are TypeScript and CSharp\\");\\n\\n            if (language == \\"TypeScript\\")\\n                await GenerateTypeScriptClient(url, generatePath);\\n            else\\n                await GenerateCSharpClient(url, generatePath);\\n        }\\n\\n        async static Task GenerateTypeScriptClient(string url, string generatePath) =>\\n            await GenerateClient(\\n                document: await OpenApiDocument.FromUrlAsync(url),\\n                generatePath: generatePath,\\n                generateCode: (OpenApiDocument document) =>\\n                {\\n                    var settings = new TypeScriptClientGeneratorSettings();\\n\\n                    settings.TypeScriptGeneratorSettings.TypeStyle = TypeScriptTypeStyle.Interface;\\n                    settings.TypeScriptGeneratorSettings.TypeScriptVersion = 3.5M;\\n                    settings.TypeScriptGeneratorSettings.DateTimeType = TypeScriptDateTimeType.String;\\n\\n                    var generator = new TypeScriptClientGenerator(document, settings);\\n                    var code = generator.GenerateFile();\\n\\n                    return code;\\n                }\\n            );\\n\\n        async static Task GenerateCSharpClient(string url, string generatePath) =>\\n            await GenerateClient(\\n                document: await OpenApiDocument.FromUrlAsync(url),\\n                generatePath: generatePath,\\n                generateCode: (OpenApiDocument document) =>\\n                {\\n                    var settings = new CSharpClientGeneratorSettings\\n                    {\\n                        UseBaseUrl = false\\n                    };\\n\\n                    var generator = new CSharpClientGenerator(document, settings);\\n                    var code = generator.GenerateFile();\\n                    return code;\\n                }\\n            );\\n\\n        private async static Task GenerateClient(OpenApiDocument document, string generatePath, Func<OpenApiDocument, string> generateCode)\\n        {\\n            Console.WriteLine($\\"Generating {generatePath}...\\");\\n\\n            var code = generateCode(document);\\n\\n            await System.IO.File.WriteAllTextAsync(generatePath, code);\\n        }\\n    }\\n}\\n```\\n\\nWe\'ve created ourselves a simple .NET console application that creates TypeScript and CSharp clients for a given Swagger URL. It expects three arguments:\\n\\n- `url` \\\\- the url of the `swagger.json` file to generate a client for.\\n- `generatePath` \\\\- the path where the generated client file should be placed, relative to this project.\\n- `language` \\\\- the language of the client to generate; valid values are \\"TypeScript\\" and \\"CSharp\\".\\n\\nTo create a TypeScript client with it then we\'d use the following command:\\n\\n```shell\\ndotnet run --project src/server-app/APIClientGenerator http://localhost:5000/swagger/v1/swagger.json src/client-app/src/clients.ts TypeScript\\n```\\n\\nHowever, for this to run successfully, we\'ll first have to ensure the API is running. It would be great if we had a single command we could run that would:\\n\\n- bring up the API\\n- generate a client\\n- bring down the API\\n\\nLet\'s make that.\\n\\n## Building a \\"make a client\\" script\\n\\nIn the root of the project we\'re going to add the following `scripts`:\\n\\n```json\\n\\"generate-client:server-app\\": \\"start-server-and-test generate-client:server-app:serve http-get://localhost:5000/swagger/v1/swagger.json generate-client:server-app:generate\\",\\n    \\"generate-client:server-app:serve\\": \\"cross-env ASPNETCORE_URLS=http://*:5000 ASPNETCORE_ENVIRONMENT=Development dotnet run --project src/server-app/API --no-launch-profile\\",\\n    \\"generate-client:server-app:generate\\": \\"dotnet run --project src/server-app/APIClientGenerator http://localhost:5000/swagger/v1/swagger.json src/client-app/src/clients.ts TypeScript\\",\\n```\\n\\nLet\'s walk through what\'s happening here. Running `npm run generate-client:server-app` will:\\n\\n- Use the [`start-server-and-test`](https://github.com/bahmutov/start-server-and-test) package to spin up our server-app by running the `generate-client:server-app:serve` script.\\n- `start-server-and-test` waits for the Swagger endpoint to start responding to requests. When it does start responding, `start-server-and-test` runs the `generate-client:server-app:generate` script which runs our APIClientGenerator console app and provides it with the URL where our swagger can be found, the path of the file to generate and the language of \\"TypeScript\\"\\n\\nIf you were wanting to generate a C# client (say if you were writing a [Blazor](https://blog.logrocket.com/js-free-frontends-blazor/) app) then you could change the `generate-client:server-app:generate` script as follows:\\n\\n```json\\n\\"generate-client:server-app:generate\\": \\"dotnet run --project src/server-app/ApiClientGenerator http://localhost:5000/swagger/v1/swagger.json clients.cs CSharp\\",\\n```\\n\\n## Consume our generated API client\\n\\nLet\'s run the `npm run generate-client:server-app` command. It creates a `clients.ts` file which nestles nicely inside our `client-app`. We\'re going to exercise that in a moment. First of all, let\'s enable proxying from our `client-app` to our `server-app` following the instructions in the [Create React App docs](https://create-react-app.dev/docs/proxying-api-requests-in-development/) and adding the following to our `client-app/package.json`:\\n\\n```json\\n\\"proxy\\": \\"http://localhost:5000\\"\\n```\\n\\nNow let\'s start our apps with `npm run start`. We\'ll then replace the contents of `App.tsx` with:\\n\\n```jsx\\nimport React from \\"react\\";\\nimport \\"./App.css\\";\\nimport { WeatherForecast, WeatherForecastClient } from \\"./clients\\";\\n\\nfunction App() {\\n  const [weather, setWeather] = React.useState<WeatherForecast[] | null>();\\n  React.useEffect(() => {\\n    async function loadWeather() {\\n      const weatherClient = new WeatherForecastClient(/* baseUrl */ \\"\\");\\n      const forecast = await weatherClient.get();\\n      setWeather(forecast);\\n    }\\n    loadWeather();\\n  }, [setWeather]);\\n\\n  return (\\n    <div className=\\"App\\">\\n      <header className=\\"App-header\\">\\n        {weather ? (\\n          <table>\\n            <thead>\\n              <tr>\\n                <th>Date</th>\\n                <th>Summary</th>\\n                <th>Centigrade</th>\\n                <th>Fahrenheit</th>\\n              </tr>\\n            </thead>\\n            <tbody>\\n              {weather.map(({ date, summary, temperatureC, temperatureF }) => (\\n                <tr key={date}>\\n                  <td>{new Date(date).toLocaleDateString()}</td>\\n                  <td>{summary}</td>\\n                  <td>{temperatureC}</td>\\n                  <td>{temperatureF}</td>\\n                </tr>\\n              ))}\\n            </tbody>\\n          </table>\\n        ) : (\\n          <p>Loading weather...</p>\\n        )}\\n      </header>\\n    </div>\\n  );\\n}\\n\\nexport default App;\\n```\\n\\nInside the `React.useEffect` above you can see we create a new instance of the auto-generated `WeatherForecastClient`. We then call `weatherClient.get()` which sends the `GET` request to the server to acquire the data and provides it in a strongly typed fashion (`get()` returns an array of `WeatherForecast`). This is then displayed on the page like so:\\n\\n![load data from server](use-generated-client.gif)\\n\\nAs you an see we\'re loading data from the server using our auto-generated client. We\'re reducing the amount of code we have to write _and_ we\'re reducing the likelihood of errors.\\n\\n_This post was originally posted on [LogRocket](https://blog.logrocket.com/generate-typescript-csharp-clients-nswag-api/)._"},{"id":"/2021/02/27/goodbye-client-affinity-hello-data-protection-with-azure","metadata":{"permalink":"/2021/02/27/goodbye-client-affinity-hello-data-protection-with-azure","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-02-27-goodbye-client-affinity-hello-data-protection-with-azure/index.md","source":"@site/blog/2021-02-27-goodbye-client-affinity-hello-data-protection-with-azure/index.md","title":"Goodbye Client Affinity, Hello Data Protection with Azure","description":"How to use ASP.NET Data Protection to remove the need for sticky sessions with Client Affinity","date":"2021-02-27T00:00:00.000Z","formattedDate":"February 27, 2021","tags":[{"label":"Azure","permalink":"/tags/azure"},{"label":"Data Protection","permalink":"/tags/data-protection"},{"label":"Easy Auth","permalink":"/tags/easy-auth"},{"label":"ASP.NET","permalink":"/tags/asp-net"},{"label":"Client Affinity","permalink":"/tags/client-affinity"}],"readingTime":3.45,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Goodbye Client Affinity, Hello Data Protection with Azure","description":"How to use ASP.NET Data Protection to remove the need for sticky sessions with Client Affinity","authors":"johnnyreilly","tags":["Azure","Data Protection","Easy Auth","ASP.NET","Client Affinity"],"image":"./traffic-to-app-service.png","hide_table_of_contents":false},"prevItem":{"title":"NSwag: TypeScript and CSharp client generation based on an API","permalink":"/2021/03/06/generate-typescript-and-csharp-clients-with-nswag"},"nextItem":{"title":"Making Easy Auth tokens survive releases on Linux Azure App Service","permalink":"/2021/02/16/easy-auth-tokens-survive-releases-on-linux-azure-app-service"}},"content":"I\'ve written lately about [zero downtime releases with Azure App Service](./2021-02-11-azure-app-service-health-checks-and-zero-downtime-deployments/index.md). Zero downtime releases are only successful if your authentication mechanism survives a new deployment. We looked in my last post at [how to achieve this with Azure\'s in-built authentication mechanism; Easy Auth](./2021-02-16-easy-auth-tokens-survive-releases-on-linux-azure-app-service/index.md).\\n\\nWe\'re now going to look at how the same goal can be achieved if your ASP.NET application is authenticating another way. We achieve this through use of the [ASP.NET Data Protection](https://docs.microsoft.com/en-us/aspnet/core/security/data-protection/configuration/overview) system. Andrew Lock has written [an excellent walkthrough on the topic](https://andrewlock.net/an-introduction-to-the-data-protection-system-in-asp-net-core/) and I encourage you to read it.\\n\\nWe\'re interested in the ASP.NET data-protection system because it encrypts and decrypts sensitive data including the authentication cookie. It\'s wonderful that the data protection does this, but at the same time it presents a problem. We would like to route traffic to _multiple_ instances of our application\u2026 So traffic could go to instance 1, instance 2 of our app etc.\\n\\n![traffic to app service](traffic-to-app-service.png)\\n\\nHow can we ensure the different instances of our app can read the authentication cookies regardless of the instance that produced them? How can we ensure that instance 1 can read cookies produced by instance 2 and vice versa? And for that matter, we\'d like all instances to be able to read cookies whether they were produced by an instance in a production or staging slot.\\n\\nWe\'re aiming to avoid the use of \\"sticky sessions\\" and ARRAffinity cookies. These ensure that traffic is continually routed to the same instance. Routing to the same instance explicitly prevents us from stopping routing traffic to an old instance and starting routing to a new one.\\n\\nWith the data protection activated and multiple instances of your app service you immediately face the issue that different instances of the app will be unable to read cookies they did not create. This is the default behaviour of data protection. [To quote the docs:](https://docs.microsoft.com/en-us/aspnet/core/host-and-deploy/web-farm?view=aspnetcore-5.0#data-protection)\\n\\n> Data Protection relies upon a set of cryptographic keys stored in a key ring. When the Data Protection system is initialized, it applies default settings that store the key ring locally. Under the default configuration, a unique key ring is stored on each node of the web farm. Consequently, each web farm node can\'t decrypt data that\'s encrypted by an app on any other node.\\n\\nThe problem here is the data protection keys (the key ring) is being stored locally on each instance. What are the implications of this? Well, For example, instance 2 doesn\'t have access to the keys instance 1 is using and so can\'t decrypt instance 1 cookies.\\n\\n## Sharing is caring\\n\\nWhat we need to do is move away from storing keys locally, and to storing it in a _shared_ place instead. We\'re going to store data protection keys in Azure Blob Storage and protect the keys with Azure Key Vault:\\n\\n![persist keys to azure blob](data-protection-zero-downtime.png)\\n\\nAll instances of the application can access the key ring and consequently sharing cookies is enabled. [As the documentation attests](https://docs.microsoft.com/en-us/aspnet/core/security/data-protection/configuration/overview?view=aspnetcore-5.0#protectkeyswithazurekeyvault), enabling this is fairly simple. It amounts to adding the following packages to your ASP.NET app:\\n\\n- [`Azure.Extensions.AspNetCore.DataProtection.Blobs`](https://www.nuget.org/packages/Azure.Extensions.AspNetCore.DataProtection.Blobs)\\n- [`Azure.Extensions.AspNetCore.DataProtection.Keys`](https://www.nuget.org/packages/Azure.Extensions.AspNetCore.DataProtection.Keys)\\n\\nAnd adding the following to the `ConfigureServices` in your ASP.NET app:\\n\\n```cs\\nservices.AddDataProtection().SetApplicationName(\\"OurWebApp\\")\\n        // azure credentials require storage blob contributor role permissions\\n        // eg https://my-storage-account.blob.core.windows.net/keys/key\\n        .PersistKeysToAzureBlobStorage(new Uri($\\"https://{Configuration[\\"StorageAccountName\\"]}.blob.core.windows.net/keys/key\\"), new DefaultAzureCredential())\\n\\n        // azure credentials require key vault crypto role permissions\\n        // eg https://my-key-vault.vault.azure.net/keys/dataprotection\\n        .ProtectKeysWithAzureKeyVault(new Uri($\\"https://{Configuration[\\"KeyVaultName\\"]}.vault.azure.net/keys/dataprotection\\"), new DefaultAzureCredential());\\n```\\n\\nIn the above example you can see we\'re passing the name of our Storage account and Key Vault via configuration.\\n\\nThere\'s one more crucial piece of the puzzle here; and it\'s role assignments, better known as permissions. Your App Service needs to be able to read and write to Azure Key Vault and the Azure Blob Storage. The permissions of [Storage Blob Data Contributor](https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#storage-blob-data-contributor) and [Key Vault Crypto Officer](https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#key-vault-crypto-officer-preview) are sufficient to enable this. (If you\'d like to see what configuring that looks like via ARM templates then [check out this post](./2021-02-08-arm-templates-security-role-assignments/index.md).)\\n\\nWith this in place we\'re able to route traffic to any instance of our application, secure in the knowledge that it will be able to read the cookies. Furthermore, we\'ve enabled zero downtime releases as a direct consequence."},{"id":"/2021/02/16/easy-auth-tokens-survive-releases-on-linux-azure-app-service","metadata":{"permalink":"/2021/02/16/easy-auth-tokens-survive-releases-on-linux-azure-app-service","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-02-16-easy-auth-tokens-survive-releases-on-linux-azure-app-service/index.md","source":"@site/blog/2021-02-16-easy-auth-tokens-survive-releases-on-linux-azure-app-service/index.md","title":"Making Easy Auth tokens survive releases on Linux Azure App Service","description":"I wrote recently about zero downtime deployments on Azure App Service. Many applications require authentication, and ours is no exception. In our case we\'re using Azure Active Directory facilitated by \\"Easy Auth\\" which provides authentication to our App Service.","date":"2021-02-16T00:00:00.000Z","formattedDate":"February 16, 2021","tags":[{"label":"Azure","permalink":"/tags/azure"},{"label":"Easy Auth","permalink":"/tags/easy-auth"},{"label":"tokens","permalink":"/tags/tokens"},{"label":"SAS","permalink":"/tags/sas"},{"label":"Blob Storage","permalink":"/tags/blob-storage"}],"readingTime":3.91,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Making Easy Auth tokens survive releases on Linux Azure App Service","authors":"johnnyreilly","image":"./easy-auth-zero-downtime-deployment.png","tags":["Azure","Easy Auth","tokens","SAS","Blob Storage"],"hide_table_of_contents":false},"prevItem":{"title":"Goodbye Client Affinity, Hello Data Protection with Azure","permalink":"/2021/02/27/goodbye-client-affinity-hello-data-protection-with-azure"},"nextItem":{"title":"Azure App Service, Health checks and zero downtime deployments","permalink":"/2021/02/11/azure-app-service-health-checks-and-zero-downtime-deployments"}},"content":"I [wrote recently about zero downtime deployments on Azure App Service](./2021-02-11-azure-app-service-health-checks-and-zero-downtime-deployments/index.md). Many applications require authentication, and ours is no exception. In our case we\'re using Azure Active Directory facilitated by [\\"Easy Auth\\"](https://docs.microsoft.com/en-us/azure/app-service/overview-authentication-authorization) which provides authentication to our App Service.\\n\\nOur app uses a Linux App Service. It\'s worth knowing that Linux App Services run as a Docker container. As a consequence, Easy Auth works in a slightly different way; effectively as a middleware. [To quote the docs on Easy Auth](https://docs.microsoft.com/en-us/azure/app-service/overview-authentication-authorization#on-containers):\\n\\n> This module handles several things for your app:\\n>\\n> - Authenticates users with the specified provider\\n> - Validates, stores, and refreshes tokens\\n> - Manages the authenticated session\\n> - Injects identity information into request headers The module runs separately from your application code and is configured using app settings. No SDKs, specific languages, or changes to your application code are required.\\n>\\n> The authentication and authorization module runs in a separate container, isolated from your application code. Using what\'s known as the [Ambassador](https://docs.microsoft.com/en-us/azure/architecture/patterns/ambassador) pattern, it interacts with the incoming traffic to perform similar functionality as on Windows.\\n\\nHowever, [Microsoft have acknowledged there is a potential bug in Easy Auth support at present](https://social.msdn.microsoft.com/Forums/en-US/dde551b2-c86d-474b-b0a6-cc66163785a1/restarting-azure-app-service-on-linux-with-azure-active-directory-authentication-resets-authme#b59951ab-623a-4442-a221-80c157387bbe). When the app service is restarted, the stored tokens are removed, and **authentication begins to fail**. As you might well imagine, authentication similarly starts to fail when a new app service is introduced - as is the case during deployment.\\n\\nThis is really significant. You may well have \\"zero downtime deployment\\", but it doesn\'t amount to a hill of beans if the moment you\'ve deployed your users find they\'re effectively logged out. [The advice from Microsoft](https://social.msdn.microsoft.com/Forums/en-US/dde551b2-c86d-474b-b0a6-cc66163785a1/restarting-azure-app-service-on-linux-with-azure-active-directory-authentication-resets-authme#b59951ab-623a-4442-a221-80c157387bbe) is to use [Blob Storage for Token Cache](https://docs.microsoft.com/en-gb/archive/blogs/jpsanders/azure-app-service-authentication-using-a-blob-storage-for-token-cache):\\n\\n[Chris Gillum](https://twitter.com/cgillum) said in a [blog on the topic](https://cgillum.tech/2016/03/07/app-service-token-store/):\\n\\n> you can provision an Azure Blob Storage container and configure your web app with a SaS URL (with read/write/list access) pointing to that blob container. This SaS URL can then be saved to the `WEBSITE_AUTH_TOKEN_CONTAINER_SASURL` app setting. When this app setting is present, all tokens will be stored in and fetched from the specified blob container.\\n\\nTo turn that into something visual, what\'s suggested is this:\\n\\n![diagram of Easy Auth with blog storage](easy-auth-zero-downtime-deployment.png)\\n\\n## SaS-sy ARM Templates\\n\\nI have the good fortune to work with some very talented people. One of them, [John McCormick](https://github.com/jmccor99) turned his hand to putting this proposed solution into `azure-pipelines.yml` and ARM template-land. First of all, let\'s look at our `azure-pipelines.yml`. We add the following, prior to our deployment job:\\n\\n```yml\\n- job: SASGen\\n        displayName: Generate SAS Token\\n\\n        steps:\\n          - task: AzurePowerShell@4\\n            name: ObtainSasTokenTask\\n            inputs:\\n              azureSubscription: $(serviceConnection)\\n              ScriptType: inlineScript\\n              Inline: |\\n                $startTime = Get-Date\\n                $expiryTime = $startTime.AddDays(90)\\n                $storageAcc = Get-AzStorageAccount -ResourceGroupName $(azureResourceGroup) -Name $(storageAccountName)\\n                $ctx = $storageAcc.Context\\n                $sas = New-AzStorageContainerSASToken -Context $ctx -Name \\"tokens\\" -Permission \\"rwl\\" -Protocol HttpsOnly -StartTime $startTime -ExpiryTime $expiryTime -FullUri\\n                Write-Host \\"##vso[task.setvariable variable=sasToken;issecret=true;isOutput=true]$sas\\"\\n              azurePowerShellVersion: \'LatestVersion\'\\n\\n      - job: DeployAppARMTemplates\\n        variables:\\n          sasToken: $[dependencies.SASGen.outputs[\'ObtainSasTokenTask.sasToken\'] ]\\n        displayName: Deploy App ARM Templates\\n        dependsOn:\\n        - SASGen\\n\\n        steps:\\n          - task: AzureResourceManagerTemplateDeployment@3\\n            displayName: Deploy app-service ARM Template\\n            inputs:\\n              deploymentScope: Resource Group\\n              azureResourceManagerConnection: $(serviceConnection)\\n              subscriptionId: $(subscriptionId)\\n              action: Create Or Update Resource Group\\n              resourceGroupName: $(azureResourceGroup)\\n              location: $(location)\\n              templateLocation: Linked artifact\\n              csmFile: \'infra/app-service/azuredeploy.json\'\\n              csmParametersFile: \'infra/azuredeploy.parameters.json\'\\n              overrideParameters: >-\\n                -sasUrl $(sasToken)\\n              deploymentMode: Incremental\\n```\\n\\nThere\'s two notable things happening above:\\n\\n1. In the `SASGen` job, a PowerShell script runs that [generates a SaS token URL](https://docs.microsoft.com/en-us/powershell/module/az.storage/new-azstoragecontainersastoken?view=azps-5.5.0) with read, write and list permissions that will last for 90 days. (Incidentally, there is a way to do this via [ARM templates, and without PowerShell](https://stackoverflow.com/a/56127006/761388) \\\\- but alas it didn\'t seem to work when we experimented with it.)\\n2. The generated (secret) token URL (`sasUrl`) is passed as a parameter to our App Service ARM template. The ARM template sets an appsetting for the app service:\\n\\n```json\\n{\\n    \\"apiVersion\\": \\"2020-09-01\\",\\n    \\"name\\": \\"appsettings\\",\\n    \\"type\\": \\"config\\",\\n    \\"properties\\": {\\n        \\"WEBSITE_AUTH_TOKEN_CONTAINER_SASURL\\": \\"[parameters(\'sasUrl\')]\\"\\n    }\\n},\\n```\\n\\nIf you google `WEBSITE_AUTH_TOKEN_CONTAINER_SASURL` you will not find a geat deal. Documentation is short. What you will find is [Jeff Sanders excellent blog on the topic](http://jsandersblog.azurewebsites.net/2017/08/10/azure-app-service-authentication-using-a-blob-storage-for-token-cache/). It is, in terms of content, it has some commonality with this post; except in Jeff\'s example he\'s manually implementing the workaround in the Azure Portal.\\n\\n## What\'s actually happening?\\n\\nWith this in place, every time someone logs into your app a JSON token is written to the storage like so:\\n\\n![token in storage account](token.png)\\n\\nIf you take the trouble to look inside you\'ll find something like this tucked away:\\n\\n```json\\n{\\n  \\"encrypted\\": true,\\n  \\"tokens\\": {\\n    \\"aad\\": \\"herewith_a_very_very_long_encrypted_token\\"\\n  },\\n  \\"version\\": 1\\n}\\n```\\n\\nWith this in place, you can safely restart your app service and / or deploy a new one, safe in the knowledge that the tokens will live on in the storage account, and that consequently you will not be unauthenticating users."},{"id":"/2021/02/11/azure-app-service-health-checks-and-zero-downtime-deployments","metadata":{"permalink":"/2021/02/11/azure-app-service-health-checks-and-zero-downtime-deployments","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-02-11-azure-app-service-health-checks-and-zero-downtime-deployments/index.md","source":"@site/blog/2021-02-11-azure-app-service-health-checks-and-zero-downtime-deployments/index.md","title":"Azure App Service, Health checks and zero downtime deployments","description":"I\'ve been working recently on zero downtime deployments using Azure App Service. They\'re facilitated by a combination of Health checks and deployment slots. This post will talk about why this is important and how it works.","date":"2021-02-11T00:00:00.000Z","formattedDate":"February 11, 2021","tags":[{"label":"Azure App Service","permalink":"/tags/azure-app-service"},{"label":"Health checks","permalink":"/tags/health-checks"},{"label":"deployment slots","permalink":"/tags/deployment-slots"},{"label":"zero downtime deployments","permalink":"/tags/zero-downtime-deployments"}],"readingTime":7.485,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Azure App Service, Health checks and zero downtime deployments","authors":"johnnyreilly","tags":["Azure App Service","Health checks","deployment slots","zero downtime deployments"],"hide_table_of_contents":false},"prevItem":{"title":"Making Easy Auth tokens survive releases on Linux Azure App Service","permalink":"/2021/02/16/easy-auth-tokens-survive-releases-on-linux-azure-app-service"},"nextItem":{"title":"Azure RBAC: role assignments and ARM templates","permalink":"/2021/02/08/arm-templates-security-role-assignments"}},"content":"I\'ve been working recently on zero downtime deployments using Azure App Service. They\'re facilitated by a combination of [Health checks](https://docs.microsoft.com/en-us/azure/app-service/monitor-instances-health-check) and [deployment slots](https://docs.microsoft.com/en-us/azure/app-service/deploy-staging-slots). This post will talk about why this is important and how it works.\\n\\n## Why zero downtime deployments?\\n\\nHistorically (and for many applications, currently) deployment results in downtime. A period of time during the release where an application is not available to users whilst the new version is deployed. There are a number of downsides to releases with downtime:\\n\\n1. Your users cannot use your application. This will frustrate them and make them sad.\\n2. Because you\'re a kind person and you want your users to be happy, you\'ll optimise to make their lives better. You\'ll release when the fewest users are accessing your application. It will likely mean you\'ll end up working late, early or at weekends.\\n3. Again because you want to reduce impact on users, you\'ll release less often. This means that every release will bring with it a greater collection of changes. This is turn will often result in a large degree of focus on manually testing each release, to reduce the likelihood of bugs ending up in users hands. This is a noble aim, but it drags the teams focus away from shipping.\\n\\nPut simply: downtime in releases impacts customer happiness and leads to reduced pace for teams. It\'s a vicious circle.\\n\\nBut if we turn it around, what does it look like if releases have _no_ downtime at all?\\n\\n1. Your users can always use your application. This will please them.\\n2. Your team is now safe to release at any time, day or night. They will likely release more often as a consequence.\\n3. If your team has sufficient automated testing in place, they\'re now in a position where they can move to [Continuous Deployment](https://www.atlassian.com/continuous-delivery/principles/continuous-integration-vs-delivery-vs-deployment).\\n4. Releases become boring. This is good. They \\"just work\u2122\ufe0f\\" and so the team can focus instead on building the cool features that are going to make users lives even better.\\n\\n## Manual zero downtime releases with App Services\\n\\nApp Services have the ability to scale out. To [quote the docs](https://azure.microsoft.com/en-us/blog/scaling-up-and-scaling-out-in-windows-azure-web-sites/):\\n\\n> A scale out operation is the equivalent of creating multiple copies of your web site and adding a load balancer to distribute the demand between them. When you scale out ... there is no need to configure load balancing separately since this is already provided by the platform.\\n\\nAs you can see, scaling out works by having multiple instances of your app. Deployment slots are exactly this, but with an extra twist. If you add a deployment slot to your App Service, then you **no longer deploy to production**. Instead you deploy to your staging slot. Your staging slot is accessible in the same way your production slot is accessible. So whilst your users may go to [https://my-glorious-app.io](https://my-glorious-app.io), your staging slot may live at [https://my-glorious-app-stage.azurewebsites.net](https://my-glorious-app-stage.azurewebsites.net) instead. Because this is accessible, this is testable. You are in a position to test the deployed application before making it generally available.\\n\\n![diagram of network traffic going to various App Service Deployment Slots](app-service-with-slots.png)\\n\\nOnce you\'re happy that everything looks good, you can \\"swap slots\\". What this means, is the version of the app living in the staging slot, gets moved into the production slot. So that which lived at [https://my-glorious-app-stage.azurewebsites.net](https://my-glorious-app-stage.azurewebsites.net) moves to [https://my-glorious-app.io](https://my-glorious-app.io). For a more details on what that involves [read this](https://docs.microsoft.com/en-us/azure/app-service/deploy-staging-slots#what-happens-during-a-swap). The significant take home is this: there is no downtime. Traffic stops being routed to the old instance and starts being routed to the new one. It\'s as simple as that.\\n\\nI should mention at this point that there\'s a [number of zero downtime strategies out there](https://opensource.com/article/17/5/colorful-deployments) and slots can help support a number of these. This includes canary deployments, where a subset of traffic is routed to the new version prior to it being opened out more widely. In our case, we\'re looking at rolling deployments, where we replace the currently running instances of our application with the new ones; but it\'s worth being aware that there are other strategies that slots can facilitate.\\n\\nSo what does it look like when slots swap? Well, to test that out, we swapped slots on our two App Service instances. We repeatedly CURLed our apps [`api/build`](./2021-01-29-surfacing-azure-pipelines-build-info-in-an-aspnet-react-app/index.md) endpoint that exposes the build information; to get visibility around which version of our app we were routing traffic to. This is what we saw:\\n\\n```\\nThu Jan 21 11:51:51 GMT 2021\\n{\\"buildNumber\\":\\"20210121.5\\",\\"buildId\\":\\"17992\\",\\"commitHash\\":\\"c2122919df54bfa6a0d20bceb9f06890f822b26e\\"}\\nThu Jan 21 11:51:54 GMT 2021\\n{\\"buildNumber\\":\\"20210121.6\\",\\"buildId\\":\\"18015\\",\\"commitHash\\":\\"062ac1488fcf1737fe1dbab0d05c095786218f30\\"}\\nThu Jan 21 11:51:57 GMT 2021\\n{\\"buildNumber\\":\\"20210121.5\\",\\"buildId\\":\\"17992\\",\\"commitHash\\":\\"c2122919df54bfa6a0d20bceb9f06890f822b26e\\"}\\nThu Jan 21 11:52:00 GMT 2021\\n{\\"buildNumber\\":\\"20210121.6\\",\\"buildId\\":\\"18015\\",\\"commitHash\\":\\"062ac1488fcf1737fe1dbab0d05c095786218f30\\"}\\nThu Jan 21 11:52:03 GMT 2021\\n{\\"buildNumber\\":\\"20210121.6\\",\\"buildId\\":\\"18015\\",\\"commitHash\\":\\"062ac1488fcf1737fe1dbab0d05c095786218f30\\"}\\nThu Jan 21 11:52:05 GMT 2021\\n{\\"buildNumber\\":\\"20210121.6\\",\\"buildId\\":\\"18015\\",\\"commitHash\\":\\"062ac1488fcf1737fe1dbab0d05c095786218f30\\"}\\nThu Jan 21 11:52:08 GMT 2021\\n{\\"buildNumber\\":\\"20210121.5\\",\\"buildId\\":\\"17992\\",\\"commitHash\\":\\"c2122919df54bfa6a0d20bceb9f06890f822b26e\\"}\\nThu Jan 21 11:52:10 GMT 2021\\n{\\"buildNumber\\":\\"20210121.6\\",\\"buildId\\":\\"18015\\",\\"commitHash\\":\\"062ac1488fcf1737fe1dbab0d05c095786218f30\\"}\\nThu Jan 21 11:52:12 GMT 2021\\n{\\"buildNumber\\":\\"20210121.5\\",\\"buildId\\":\\"17992\\",\\"commitHash\\":\\"c2122919df54bfa6a0d20bceb9f06890f822b26e\\"}\\nThu Jan 21 11:52:15 GMT 2021\\n{\\"buildNumber\\":\\"20210121.6\\",\\"buildId\\":\\"18015\\",\\"commitHash\\":\\"062ac1488fcf1737fe1dbab0d05c095786218f30\\"}\\nThu Jan 21 11:52:17 GMT 2021\\n{\\"buildNumber\\":\\"20210121.6\\",\\"buildId\\":\\"18015\\",\\"commitHash\\":\\"062ac1488fcf1737fe1dbab0d05c095786218f30\\"}\\n```\\n\\nThe first new version of our application showed up in a production slot at 11:51:54, and the last old version showed up at 11:52:12. So it took a total of 15 seconds to complete the transition from hitting only instances of the old application to hitting only instances of the new application. During that 15 seconds either old or new versions of the application would be serving traffic. Significantly, there was always a version of the application returning responses.\\n\\nThis is _very_ exciting! We have zero downtime deployments!\\n\\n## Rollbacks for bonus points\\n\\nWe now have the new version of the app (`buildNumber: 20210121.6`) in the production slot, and the old version of the app (`buildNumber: 20210121.5`) in the staging slot.\\n\\nSlots have a tremendous rollback story. If it emerges that there was some uncaught issue in your release and you\'d like to revert to the previous version, you can! Just as we swapped just now to move `buildNumber: 20210121.6` from the staging slot to the production slot and `buildNumber: 20210121.5` the other way, we can swap right back and revert our release like so:\\n\\n![diagram of network traffic going to various App Service Deployment Slots exposing build number](app-service-with-slots-and-build-number.png)\\n\\nOnce again users going to [https://my-glorious-app.io](https://my-glorious-app.io) are hitting `buildNumber: 20210121.5`.\\n\\nThis is also _very_ exciting! We have zero downtime deployments _and_ rollbacks!\\n\\n## Automated zero downtime releases with Health checks\\n\\nThe final piece of the puzzle here automation. You\'re a sophisticated team, you\'ve put a great deal of energy into automating your tests. You don\'t want your release process to be manual for this very reason; you trust your test coverage. You want to move to Continuous Deployment.\\n\\nFortunately, automating swapping slots is a breeze with `azure-pipelines.yml`. Consider the following:\\n\\n```yml\\n- job: DeployApp\\n        displayName: Deploy app\\n        dependsOn:\\n        - DeployARMTemplates\\n\\n        steps:\\n        - download: current\\n          artifact: webapp\\n\\n        - task: AzureWebApp@1\\n          displayName: \'Deploy Web Application\'\\n          inputs:\\n            azureSubscription: $(serviceConnection)\\n            resourceGroupName: $(azureResourceGroup)\\n            appName: $(appServiceName)\\n            package: $(Pipeline.Workspace)/webapp/**/*.zip\\n            slotName: stage\\n            deployToSlotOrASE: true\\n            deploymentMethod: auto\\n\\n      - job: SwapSlots\\n        displayName: Swap Slots\\n        dependsOn:\\n        - DeployApp\\n\\n        steps:\\n          - task: AzureAppServiceManage@0\\n            displayName: Swap Slots\\n            inputs:\\n              action: \'Swap Slots\'\\n              azureSubscription: $(serviceConnection)\\n              resourceGroupName: $(azureResourceGroup)\\n              webAppName: $(appServiceName)\\n              SourceSlot: \'stage\'\\n```\\n\\nThe first job here, deploys our previously built `webapp` to the `stage` slot. The second job swaps the slot.\\n\\nWhen I first considered this, the question rattling around in the back of my mind was this: how does App Service know when it\'s safe to swap? What if we swap before our app has fully woken up and started serving responses?\\n\\nIt so happens that using [Health checks, App Service caters for this beautifully](https://docs.microsoft.com/en-us/azure/app-service/monitor-instances-health-check). A health check endpoint is a URL in your application which, when hit, checks the dependencies of your application. \\"Is the database accessible?\\" \\"Are the APIs I depend upon accessible?\\" The diagram from the docs expresses it very well:\\n\\n![diagram of traffic hitting the health check endpoint](health-check-failure-diagram.png)\\n\\nThis approach is very similar to [liveness, readiness and startup probes in Kubernetes](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/). To make use of Health checks, in our ARM template for our App Service we have configured a `healthCheckPath`:\\n\\n```json\\n\\"siteConfig\\": {\\n    \\"linuxFxVersion\\": \\"[parameters(\'linuxFxVersion\')]\\",\\n    \\"alwaysOn\\": true,\\n    \\"http20Enabled\\": true,\\n    \\"minTlsVersion\\": \\"1.2\\",\\n    \\"healthCheckPath\\": \\"/api/health\\",\\n    //...\\n}\\n```\\n\\nThis tells App Service where to look to check the health. The health check endpoint itself is provided by the `MapHealthChecks` in our `Startup.cs` of our .NET application:\\n\\n```cs\\napp.UseEndpoints(endpoints => {\\n    endpoints.MapControllerRoute(\\n        name: \\"default\\",\\n        pattern: \\"{controller}/{action=Index}/{id?}\\");\\n\\n    endpoints.MapHealthChecks(\\"/api/health\\");\\n});\\n```\\n\\nYou read a full list of all the ways App Service uses Health checks [here](https://docs.microsoft.com/en-us/azure/app-service/monitor-instances-health-check#what-app-service-does-with-health-checks). Pertinent for zero downtime deployments is this:\\n\\n> when scaling up or out, App Service pings the Health check path to ensure new instances are ready.\\n\\nThis is the magic sauce. App Service doesn\'t route traffic to an instance until it\'s given the thumbs up that it\'s ready in the form of passing health checks. This is excellent; it is this that makes automated zero downtime releases a reality.\\n\\nProps to the various Azure teams that have made this possible; I\'m very impressed by the way in which the Health checks and slots can be combined together to support some tremendous use cases."},{"id":"/2021/02/08/arm-templates-security-role-assignments","metadata":{"permalink":"/2021/02/08/arm-templates-security-role-assignments","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-02-08-arm-templates-security-role-assignments/index.md","source":"@site/blog/2021-02-08-arm-templates-security-role-assignments/index.md","title":"Azure RBAC: role assignments and ARM templates","description":"This post is about Azure\'s role assignments and ARM templates. Role assignments can be thought of as \\"permissions for Azure\\".","date":"2021-02-08T00:00:00.000Z","formattedDate":"February 8, 2021","tags":[{"label":"Azure","permalink":"/tags/azure"},{"label":"ARM templates","permalink":"/tags/arm-templates"},{"label":"role assignments","permalink":"/tags/role-assignments"},{"label":"permissions","permalink":"/tags/permissions"}],"readingTime":6.015,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Azure RBAC: role assignments and ARM templates","authors":"johnnyreilly","image":"./with-great-power-comes-great-responsibility.jpg","tags":["Azure","ARM templates","role assignments","permissions"],"hide_table_of_contents":false},"prevItem":{"title":"Azure App Service, Health checks and zero downtime deployments","permalink":"/2021/02/11/azure-app-service-health-checks-and-zero-downtime-deployments"},"nextItem":{"title":"ASP.NET, Serilog and Application Insights","permalink":"/2021/01/30/aspnet-serilog-and-application-insights"}},"content":"This post is about Azure\'s role assignments and ARM templates. Role assignments can be thought of as \\"permissions for Azure\\".\\n\\nIf you\'re deploying to Azure, there\'s a good chance you\'re using [ARM templates](https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/overview) to do so. Once you\'ve got past \\"Hello World\\", you\'ll probably find yourself in a situation when you\'re deploying multiple types of resource to make your solution. For instance, you may be deploying an [App Service](https://docs.microsoft.com/en-us/azure/app-service/quickstart-arm-template?pivots=platform-linux#review-the-template) alongside [Key Vault](https://docs.microsoft.com/en-us/azure/templates/microsoft.keyvault/vaults) and [Storage](https://docs.microsoft.com/en-us/azure/templates/microsoft.storage/storageaccounts).\\n\\nOne of the hardest things when it comes to deploying software and having it work, is permissions. Without adequate permissions configured, the most beautiful code can do _nothing_. Incidentally, this is a good thing. We\'re deploying to the web; many people are there, not all good. As a different kind of web-head once said:\\n\\n![Spider-man saying with great power, comes great responsibility](with-great-power-comes-great-responsibility.jpg)\\n\\nAzure has great power and [suggests you use it wisely](https://docs.microsoft.com/en-us/azure/security/fundamentals/identity-management-best-practices#use-role-based-access-control).\\n\\n> Access management for cloud resources is critical for any organization that uses the cloud. [Azure role-based access control (Azure RBAC)](https://docs.microsoft.com/en-us/azure/role-based-access-control/overview) helps you manage who has access to Azure resources, what they can do with those resources, and what areas they have access to.\\n>\\n> Designating groups or individual roles responsible for specific functions in Azure helps avoid confusion that can lead to human and automation errors that create security risks. Restricting access based on the [need to know](https://en.wikipedia.org/wiki/Need_to_know) and [least privilege](https://en.wikipedia.org/wiki/Principle_of_least_privilege) security principles is imperative for organizations that want to enforce security policies for data access.\\n\\nThis is good advice. With that in mind, how can we ensure that the different resources we\'re deploying to Azure can talk to one another?\\n\\n## Role (up for your) assignments\\n\\nThe answer is roles. There\'s a number of roles that exist in Azure that can be assigned to users, groups, service principals and managed identities. In our own case we\'re using managed identity for our resources. What we can do is use [\\"role assignments\\"](https://docs.microsoft.com/en-us/azure/role-based-access-control/overview#how-azure-rbac-works) to give our managed identity access to given resources. [Arturo Lucatero](https://twitter.com/ArLucaID) gives a great short explanation of this:\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/Dzhm-garKBM\\" frameBorder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowFullScreen=\\"\\"></iframe>\\n\\nWhilst this explanation is delightfully simple, the actual implementation when it comes to ARM templates is a little more involved. Because now it\'s time to talk \\"magic\\" GUIDs. Consider the following truncated ARM template, which gives our managed identity (and hence our App Service which uses this identity) access to Key Vault and Storage:\\n\\n```json\\n{\\n  \\"$schema\\": \\"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\\",\\n  // ...\\n  \\"variables\\": {\\n    // ...\\n    \\"managedIdentity\\": \\"[concat(\'mi-\', parameters(\'applicationName\'), \'-\', parameters(\'environment\'), \'-\', \'001\')]\\",\\n    \\"appInsightsName\\": \\"[concat(\'appi-\', parameters(\'applicationName\'), \'-\', parameters(\'environment\'), \'-\', \'001\')]\\",\\n    \\"keyVaultName\\": \\"[concat(\'kv-\', parameters(\'applicationName\'), \'-\', parameters(\'environment\'), \'-\', \'001\')]\\",\\n    \\"storageAccountName\\": \\"[concat(\'st\', parameters(\'applicationName\'), parameters(\'environment\'), \'001\')]\\",\\n    \\"storageBlobDataContributor\\": \\"[subscriptionResourceId(\'Microsoft.Authorization/roleDefinitions\', \'ba92f5b4-2d11-453d-a403-e96b0029c9fe\')]\\",\\n    \\"keyVaultSecretsOfficer\\": \\"[subscriptionResourceId(\'Microsoft.Authorization/roleDefinitions\', \'b86a8fe4-44ce-4948-aee5-eccb2c155cd7\')]\\",\\n    \\"keyVaultCryptoOfficer\\": \\"[subscriptionResourceId(\'Microsoft.Authorization/roleDefinitions\', \'14b46e9e-c2b7-41b4-b07b-48a6ebf60603\')]\\",\\n    \\"uniqueRoleGuidKeyVaultSecretsOfficer\\": \\"[guid(resourceId(\'Microsoft.KeyVault/vaults\',  variables(\'keyVaultName\')), variables(\'keyVaultSecretsOfficer\'), resourceId(\'Microsoft.KeyVault/vaults\', variables(\'keyVaultName\')))]\\",\\n    \\"uniqueRoleGuidKeyVaultCryptoOfficer\\": \\"[guid(resourceId(\'Microsoft.KeyVault/vaults\',  variables(\'keyVaultName\')), variables(\'keyVaultCryptoOfficer\'), resourceId(\'Microsoft.KeyVault/vaults\', variables(\'keyVaultName\')))]\\",\\n    \\"uniqueRoleGuidStorageAccount\\": \\"[guid(resourceId(\'Microsoft.Storage/storageAccounts\',  variables(\'storageAccountName\')), variables(\'storageBlobDataContributor\'), resourceId(\'Microsoft.Storage/storageAccounts\', variables(\'storageAccountName\')))]\\"\\n  },\\n  \\"resources\\": [\\n    {\\n      \\"type\\": \\"Microsoft.ManagedIdentity/userAssignedIdentities\\",\\n      \\"name\\": \\"[variables(\'managedIdentity\')]\\",\\n      \\"apiVersion\\": \\"2018-11-30\\",\\n      \\"location\\": \\"[parameters(\'location\')]\\"\\n    },\\n    // ...\\n    {\\n      \\"type\\": \\"Microsoft.Storage/storageAccounts/providers/roleAssignments\\",\\n      \\"apiVersion\\": \\"2020-04-01-preview\\",\\n      \\"name\\": \\"[concat(variables(\'storageAccountName\'), \'/Microsoft.Authorization/\', variables(\'uniqueRoleGuidStorageAccount\'))]\\",\\n      \\"dependsOn\\": [\\n        \\"[resourceId(\'Microsoft.ManagedIdentity/userAssignedIdentities\', variables(\'managedIdentity\'))]\\"\\n      ],\\n      \\"properties\\": {\\n        \\"roleDefinitionId\\": \\"[variables(\'storageBlobDataContributor\')]\\",\\n        \\"principalId\\": \\"[reference(resourceId(\'Microsoft.ManagedIdentity/userAssignedIdentities/\', variables(\'managedIdentity\')), \'2018-11-30\').principalId]\\",\\n        \\"scope\\": \\"[resourceId(\'Microsoft.Storage/storageAccounts\', variables(\'storageAccountName\'))]\\",\\n        \\"principalType\\": \\"ServicePrincipal\\"\\n      }\\n    },\\n    {\\n      \\"type\\": \\"Microsoft.KeyVault/vaults/providers/roleAssignments\\",\\n      \\"apiVersion\\": \\"2018-01-01-preview\\",\\n      \\"name\\": \\"[concat(variables(\'keyVaultName\'), \'/Microsoft.Authorization/\', variables(\'uniqueRoleGuidKeyVaultSecretsOfficer\'))]\\",\\n      \\"dependsOn\\": [\\n        \\"[resourceId(\'Microsoft.ManagedIdentity/userAssignedIdentities\', variables(\'managedIdentity\'))]\\"\\n      ],\\n      \\"properties\\": {\\n        \\"roleDefinitionId\\": \\"[variables(\'keyVaultSecretsOfficer\')]\\",\\n        \\"principalId\\": \\"[reference(resourceId(\'Microsoft.ManagedIdentity/userAssignedIdentities/\', variables(\'managedIdentity\')), \'2018-11-30\').principalId]\\",\\n        \\"scope\\": \\"[resourceId(\'Microsoft.KeyVault/vaults\', variables(\'keyVaultName\'))]\\",\\n        \\"principalType\\": \\"ServicePrincipal\\"\\n      }\\n    },\\n    {\\n      \\"type\\": \\"Microsoft.KeyVault/vaults/providers/roleAssignments\\",\\n      \\"apiVersion\\": \\"2018-01-01-preview\\",\\n      \\"name\\": \\"[concat(variables(\'keyVaultName\'), \'/Microsoft.Authorization/\', variables(\'uniqueRoleGuidKeyVaultCryptoOfficer\'))]\\",\\n      \\"dependsOn\\": [\\n        \\"[resourceId(\'Microsoft.ManagedIdentity/userAssignedIdentities\', variables(\'managedIdentity\'))]\\"\\n      ],\\n      \\"properties\\": {\\n        \\"roleDefinitionId\\": \\"[variables(\'keyVaultCryptoOfficer\')]\\",\\n        \\"principalId\\": \\"[reference(resourceId(\'Microsoft.ManagedIdentity/userAssignedIdentities/\', variables(\'managedIdentity\')), \'2018-11-30\').principalId]\\",\\n        \\"scope\\": \\"[resourceId(\'Microsoft.KeyVault/vaults\', variables(\'keyVaultName\'))]\\",\\n        \\"principalType\\": \\"ServicePrincipal\\"\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nLet\'s take a look at these three variables:\\n\\n```json\\n\\"storageBlobDataContributor\\": \\"[subscriptionResourceId(\'Microsoft.Authorization/roleDefinitions\', \'ba92f5b4-2d11-453d-a403-e96b0029c9fe\')]\\",\\n\\"keyVaultSecretsOfficer\\": \\"[subscriptionResourceId(\'Microsoft.Authorization/roleDefinitions\', \'b86a8fe4-44ce-4948-aee5-eccb2c155cd7\')]\\",\\n\\"keyVaultCryptoOfficer\\": \\"[subscriptionResourceId(\'Microsoft.Authorization/roleDefinitions\', \'14b46e9e-c2b7-41b4-b07b-48a6ebf60603\')]\\",\\n```\\n\\nThe three variables above contain the subscription resource ids for the roles [Storage Blob Data Contributor](https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#storage-blob-data-contributor), [Key Vault Secrets Officer](https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#key-vault-secrets-officer-preview) and [Key Vault Crypto Officer](https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#key-vault-crypto-officer-preview). The first question on your mind is likely: \\"what is `ba92f5b4-2d11-453d-a403-e96b0029c9fe` and where does it come from?\\" Great question! Well, each of these GUIDs represents a built-in role in Azure RBAC. The `ba92f5b4-2d11-453d-a403-e96b0029c9fe` represents the Storage Blob Data Contributor role.\\n\\nHow can I look these up? Well, there\'s two ways; [there\'s an article which documents them here](https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles) or you could crack open the [Cloud Shell](https://azure.microsoft.com/en-gb/features/cloud-shell/) and look up a role by GUID like so:\\n\\n```ps\\nGet-AzRoleDefinition | ? {$_.id -eq \\"ba92f5b4-2d11-453d-a403-e96b0029c9fe\\" }\\n\\nName             : Storage Blob Data Contributor\\nId               : ba92f5b4-2d11-453d-a403-e96b0029c9fe\\nIsCustom         : False\\nDescription      : Allows for read, write and delete access to Azure Storage blob containers and data\\nActions          : {Microsoft.Storage/storageAccounts/blobServices/containers/delete, Microsoft.Storage/storageAccounts/blobServices/containers/read,\\n                   Microsoft.Storage/storageAccounts/blobServices/containers/write, Microsoft.Storage/storageAccounts/blobServices/generateUserDelegationKey/action}\\nNotActions       : {}\\nDataActions      : {Microsoft.Storage/storageAccounts/blobServices/containers/blobs/delete, Microsoft.Storage/storageAccounts/blobServices/containers/blobs/read,\\n                   Microsoft.Storage/storageAccounts/blobServices/containers/blobs/write, Microsoft.Storage/storageAccounts/blobServices/containers/blobs/move/action\u2026}\\nNotDataActions   : {}\\nAssignableScopes : {/}\\n```\\n\\nOr by name like so:\\n\\n```ps\\nGet-AzRoleDefinition | ? {$_.name -like \\"*Crypto Officer*\\" }\\n\\nName             : Key Vault Crypto Officer\\nId               : 14b46e9e-c2b7-41b4-b07b-48a6ebf60603\\nIsCustom         : False\\nDescription      : Perform any action on the keys of a key vault, except manage permissions. Only works for key vaults that use the \'Azure role-based access control\' permission model.\\nActions          : {Microsoft.Authorization/*/read, Microsoft.Insights/alertRules/*, Microsoft.Resources/deployments/*, Microsoft.Resources/subscriptions/resourceGroups/read\u2026}\\nNotActions       : {}\\nDataActions      : {Microsoft.KeyVault/vaults/keys/*}\\nNotDataActions   : {}\\nAssignableScopes : {/}\\n```\\n\\nAs you can see, the `Actions` section of the output above (and in even more detail on the [linked article](https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles)) provides information about what the different roles can do. So if you\'re looking to enable one Azure resource to talk to another, you should be able to refer to these to identify a role that you might want to use.\\n\\n## Creating a role assignment\\n\\nSo now we understand how you identify the roles in question, let\'s take the final leap and look at assigning those roles to our managed identity. For each role assignment, you\'ll need a `roleAssignments` resource defined that looks like this:\\n\\n```json\\n{\\n  \\"type\\": \\"Microsoft.KeyVault/vaults/providers/roleAssignments\\",\\n  \\"apiVersion\\": \\"2018-01-01-preview\\",\\n  \\"name\\": \\"[concat(variables(\'keyVaultName\'), \'/Microsoft.Authorization/\', variables(\'uniqueRoleGuidKeyVaultCryptoOfficer\'))]\\",\\n  \\"dependsOn\\": [\\n    \\"[resourceId(\'Microsoft.ManagedIdentity/userAssignedIdentities\', variables(\'managedIdentity\'))]\\"\\n  ],\\n  \\"properties\\": {\\n    \\"roleDefinitionId\\": \\"[variables(\'keyVaultCryptoOfficer\')]\\",\\n    \\"principalId\\": \\"[reference(resourceId(\'Microsoft.ManagedIdentity/userAssignedIdentities/\', variables(\'managedIdentity\')), \'2018-11-30\').principalId]\\",\\n    \\"scope\\": \\"[resourceId(\'Microsoft.KeyVault/vaults\', variables(\'keyVaultName\'))]\\",\\n    \\"principalType\\": \\"ServicePrincipal\\"\\n  }\\n}\\n```\\n\\nLet\'s go through the above, significant property by significant property (it\'s also worth checking the official reference [here](https://docs.microsoft.com/en-us/azure/templates/microsoft.authorization/roleassignments)):\\n\\n- `type` \\\\- the type of role assignment we want to create, for a key vault it\'s `\\"Microsoft.KeyVault/vaults/providers/roleAssignments\\"`, for storage it\'s `\\"Microsoft.Storage/storageAccounts/providers/roleAssignments\\"`. The pattern is that it\'s the resource type, followed by `\\"/providers/roleAssignments\\"`.\\n- `dependsOn` \\\\- before we can create a role assignment, we need the service principal we desire to permission (in our case a managed identity) to exist\\n- `properties.roleDefinitionId` \\\\- the role that we\'re assigning, provided as an id. So for this example it\'s the `keyVaultCryptoOfficer` variable, which was earlier defined as `[subscriptionResourceId(\'Microsoft.Authorization/roleDefinitions\', \'ba92f5b4-2d11-453d-a403-e96b0029c9fe\')]`. (Note the use of the GUID)\\n- `properties.principalId` \\\\- the id of the principal we\'re adding permissions for. In our case this is a managed identity (a type of service principal).\\n- `properties.scope` \\\\- we\'re modifying another resource; our key vault isn\'t defined in this ARM template and we want to specify the resource we\'re granting permissions to.\\n- `properties.principalType` \\\\- the type of principal that we\'re creating an assignment for; in our this is `\\"ServicePrincipal\\"` \\\\- our managed identity.\\n\\nThere is an alternate approach that you can use where the `type` is `\\"Microsoft.Authorization/roleAssignments\\"`. Whilst this also works, it displayed errors in the [Azure tooling for VS Code](https://marketplace.visualstudio.com/items?itemName=msazurermtools.azurerm-vscode-tools). As such, we\'ve opted not to use that approach in our ARM templates.\\n\\nMany thanks to the awesome [John McCormick](https://github.com/jmccor99) who wrangled permissions with me until we bent Azure RBAC to our will."},{"id":"/2021/01/30/aspnet-serilog-and-application-insights","metadata":{"permalink":"/2021/01/30/aspnet-serilog-and-application-insights","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-01-30-aspnet-serilog-and-application-insights/index.md","source":"@site/blog/2021-01-30-aspnet-serilog-and-application-insights/index.md","title":"ASP.NET, Serilog and Application Insights","description":"If you\'re deploying an ASP.NET application to Azure App Services, there\'s a decent chance you\'ll also be using the fantastic Serilog and will want to plug it into Azure\'s Application Insights.","date":"2021-01-30T00:00:00.000Z","formattedDate":"January 30, 2021","tags":[{"label":"asp.net","permalink":"/tags/asp-net"},{"label":"Azure","permalink":"/tags/azure"},{"label":"Application Insights","permalink":"/tags/application-insights"},{"label":"Serilog","permalink":"/tags/serilog"}],"readingTime":3.75,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"ASP.NET, Serilog and Application Insights","authors":"johnnyreilly","image":"./application-insights-properties.png","tags":["asp.net","Azure","Application Insights","Serilog"],"hide_table_of_contents":false},"prevItem":{"title":"Azure RBAC: role assignments and ARM templates","permalink":"/2021/02/08/arm-templates-security-role-assignments"},"nextItem":{"title":"Azure Pipelines Build Info in an ASP.NET React app","permalink":"/2021/01/29/surfacing-azure-pipelines-build-info-in-an-aspnet-react-app"}},"content":"If you\'re deploying an ASP.NET application to Azure App Services, there\'s a decent chance you\'ll also be using the fantastic [Serilog](https://serilog.net/) and will want to plug it into Azure\'s [Application Insights](https://docs.microsoft.com/en-us/azure/azure-monitor/app/app-insights-overview).\\n\\nThis post will show you how it\'s done, and it\'ll also build upon the [build info work from our previous post](2021-01-29-surfacing-azure-pipelines-build-info-in-an-aspnet-react-app/index.md). In what way? Great question. Well logs are a tremendous diagnostic tool. If you have logs which display some curious behaviour, and you\'d like to replicate that in another environment, you really want to take exactly that version of the codebase out to play. Our last post introduced build info into our application in the form of our `AppVersionInfo` class that looks something like this:\\n\\n```json\\n{\\n  \\"buildNumber\\": \\"20210130.1\\",\\n  \\"buildId\\": \\"123456\\",\\n  \\"branchName\\": \\"main\\",\\n  \\"commitHash\\": \\"7089620222c30c1ad88e4b556c0a7908ddd34a8e\\"\\n}\\n```\\n\\nWe\'d initially exposed an endpoint in our application which surfaced up this information. Now we\'re going to take that self same information and bake it into our log messages by making use of [Serilog\'s enrichment functionality](https://github.com/serilog/serilog/wiki/Enrichment). Build info and Serilog\'s enrichment are the double act your logging has been waiting for.\\n\\n## Let\'s plug it together\\n\\nWe\'re going to need a number of Serilog dependencies added to our `.csproj`:\\n\\n```xml\\n<PackageReference Include=\\"Serilog.AspNetCore\\" Version=\\"3.4.0\\" />\\n<PackageReference Include=\\"Serilog.Enrichers.Environment\\" Version=\\"2.1.3\\" />\\n<PackageReference Include=\\"Serilog.Enrichers.Thread\\" Version=\\"3.1.0\\" />\\n<PackageReference Include=\\"Serilog.Sinks.ApplicationInsights\\" Version=\\"3.1.0\\" />\\n<PackageReference Include=\\"Serilog.Sinks.Async\\" Version=\\"1.4.0\\" />\\n```\\n\\nThe earlier in your application lifetime you get logging wired up, the happier you will be. Earlier, means more information when you\'re diagnosing issues. So we want to start in our `Program.cs`; `Startup.cs` would be just _way_ too late.\\n\\n```cs\\npublic class Program {\\n    const string APP_NAME = \\"MyAmazingApp\\";\\n\\n    public static int Main(string[] args) {\\n        AppVersionInfo.InitialiseBuildInfoGivenPath(Directory.GetCurrentDirectory());\\n        LoggerConfigurationExtensions.SetupLoggerConfiguration(APP_NAME, AppVersionInfo.GetBuildInfo());\\n\\n        try\\n        {\\n            Log.Information(\\"Starting web host\\");\\n            CreateHostBuilder(args).Build().Run();\\n            return 0;\\n        }\\n        catch (Exception ex)\\n        {\\n            Log.Fatal(ex, \\"Host terminated unexpectedly\\");\\n            return 1;\\n        }\\n        finally\\n        {\\n            Log.CloseAndFlush();\\n        }\\n    }\\n\\n    public static IHostBuilder CreateHostBuilder(string[] args) =>\\n        Host.CreateDefaultBuilder(args)\\n            .UseSerilog((hostBuilderContext, services, loggerConfiguration) => {\\n                loggerConfiguration.ConfigureBaseLogging(APP_NAME, AppVersionInfo.GetBuildInfo());\\n                loggerConfiguration.AddApplicationInsightsLogging(services, hostBuilderContext.Configuration);\\n            })\\n            .ConfigureWebHostDefaults(webBuilder => {\\n                webBuilder\\n                    .UseStartup<Startup>();\\n            });\\n}\\n```\\n\\nIf you look at the code above you\'ll see that the first line of code that executes is `AppVersionInfo.InitialiseBuildInfoGivenPath`. This initialises our `AppVersionInfo` so we have meaningful build info to pump into our logs. The next thing we do is to configure Serilog with `LoggerConfigurationExtensions.SetupLoggerConfiguration`. This provides us with a configured logger so we are free to log any issues that take place during startup. (Incidentally, after startup you\'ll likely inject an `ILogger` into your classes rather than using the static `Log` directly.)\\n\\nFinally, we call `CreateHostBuilder` which in turn calls `UseSerilog` to plug Serilog into ASP.NET. If you take a look inside the body of `UseSerilog` you\'ll see we configure the logging of ASP.NET (in the same way we did for Serilog) and we hook into Application Insights as well. There\'s been a number of references to `LoggerConfigurationExtensions`. Let\'s take a look at it:\\n\\n```cs\\ninternal static class LoggerConfigurationExtensions {\\n    internal static void SetupLoggerConfiguration(string appName, BuildInfo buildInfo) {\\n        Log.Logger = new LoggerConfiguration()\\n            .ConfigureBaseLogging(appName, buildInfo)\\n            .CreateLogger();\\n    }\\n\\n    internal static LoggerConfiguration ConfigureBaseLogging(\\n        this LoggerConfiguration loggerConfiguration,\\n        string appName,\\n        BuildInfo buildInfo\\n    ) {\\n        loggerConfiguration\\n            .MinimumLevel.Debug()\\n            .MinimumLevel.Override(\\"Microsoft\\", LogEventLevel.Information)\\n            // AMAZING COLOURS IN THE CONSOLE!!!!\\n            .WriteTo.Async(a => a.Console(theme: AnsiConsoleTheme.Code))\\n            .Enrich.FromLogContext()\\n            .Enrich.WithMachineName()\\n            .Enrich.WithThreadId()\\n            // Build information as custom properties\\n            .Enrich.WithProperty(nameof(buildInfo.BuildId), buildInfo.BuildId)\\n            .Enrich.WithProperty(nameof(buildInfo.BuildNumber), buildInfo.BuildNumber)\\n            .Enrich.WithProperty(nameof(buildInfo.BranchName), buildInfo.BranchName)\\n            .Enrich.WithProperty(nameof(buildInfo.CommitHash), buildInfo.CommitHash)\\n            .Enrich.WithProperty(\\"ApplicationName\\", appName);\\n\\n        return loggerConfiguration;\\n    }\\n\\n    internal static LoggerConfiguration AddApplicationInsightsLogging(this LoggerConfiguration loggerConfiguration, IServiceProvider services, IConfiguration configuration)\\n    {\\n        if (!string.IsNullOrWhiteSpace(configuration.GetValue<string>(\\"APPINSIGHTS_INSTRUMENTATIONKEY\\")))\\n        {\\n            loggerConfiguration.WriteTo.ApplicationInsights(\\n                services.GetRequiredService<TelemetryConfiguration>(),\\n                TelemetryConverter.Traces);\\n        }\\n\\n        return loggerConfiguration;\\n    }\\n}\\n```\\n\\nIf we take a look at the `ConfigureBaseLogging` method above, we can see that our logs are being enriched with the build info, property by property. We\'re also giving ourselves a beautifully coloured console thanks to Serilog\'s glorious [theme support](https://github.com/serilog/serilog-sinks-console#themes):\\n\\n![screenshot of the console featuring coloured output](coloured-console.png)\\n\\nTake a moment to admire the salmon pinks. Is it not lovely?\\n\\nFinally we come to the main act. Plugging in Application Insights is as simple as dropping in `loggerConfiguration.WriteTo.ApplicationInsights` into our configuration. You\'ll note that this depends upon the existence of an application setting of `APPINSIGHTS_INSTRUMENTATIONKEY` - this is the secret sauce that we need to be in place so we can pipe logs merrily to Application Insights. So you\'ll need this configuration in place so this works.\\n\\n![screenshot of application insights with our output](application-insights-properties.png)\\n\\nAs you can see, we now have the likes of `BuildNumber`, `CommitHash` and friends visible on each log. Happy diagnostic days!\\n\\nI\'m indebted to the marvellous [Marcel Michau](https://twitter.com/MarcelMichau) who showed me how to get the fiddlier parts of how to get Application Insights plugged in the right way. Thanks chap!"},{"id":"/2021/01/29/surfacing-azure-pipelines-build-info-in-an-aspnet-react-app","metadata":{"permalink":"/2021/01/29/surfacing-azure-pipelines-build-info-in-an-aspnet-react-app","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-01-29-surfacing-azure-pipelines-build-info-in-an-aspnet-react-app/index.md","source":"@site/blog/2021-01-29-surfacing-azure-pipelines-build-info-in-an-aspnet-react-app/index.md","title":"Azure Pipelines Build Info in an ASP.NET React app","description":"How do you answer the question: \\"what version of my application is running in Production right now?\\" This post demonstrates how to surface the build metadata that represents the version of your app, from your app using Azure Pipelines and ASP.NET.","date":"2021-01-29T00:00:00.000Z","formattedDate":"January 29, 2021","tags":[{"label":"build information","permalink":"/tags/build-information"},{"label":"azure pipelines","permalink":"/tags/azure-pipelines"}],"readingTime":6.535,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Azure Pipelines Build Info in an ASP.NET React app","authors":"johnnyreilly","image":"./about-page.png","tags":["build information","azure pipelines"],"hide_table_of_contents":false},"prevItem":{"title":"ASP.NET, Serilog and Application Insights","permalink":"/2021/01/30/aspnet-serilog-and-application-insights"},"nextItem":{"title":"Azure Easy Auth and Roles with .NET and Microsoft.Identity.Web","permalink":"/2021/01/17/azure-easy-auth-and-roles-with-net-and-microsoft-identity-web"}},"content":"How do you answer the question: \\"what version of my application is running in Production right now?\\" This post demonstrates how to surface the build metadata that represents the version of your app, from your app using Azure Pipelines and ASP.NET.\\n\\nMany is the time where I\'ve been pondering over why something isn\'t working as expected and burned a disappointing amount of time before realising that I\'m playing with an old version of an app. Wouldn\'t it be great give our app a way to say: \\"Hey! I\'m version 1.2.3.4 of your app; built from this commit hash, I was built on Wednesday, I was the nineth build that day and I was built from the `main` branch. And I\'m an Aries.\\" Or something like that.\\n\\nThis post was inspired by [Scott Hanselman\'s similar post on the topic](https://www.hanselman.com/blog/adding-a-git-commit-hash-and-azure-devops-build-number-and-build-id-to-an-aspnet-website). Ultimately this ended up going in a fairly different direction and so seemed worthy of a post of its own.\\n\\nA particular difference is that this is targeting SPAs. Famously, cache invalidation is hard. It\'s possible for the HTML/JS/CSS of your app to be stale due to aggressive caching. So we\'re going to make it possible to see build information for both when the SPA (or \\"client\\") is built, as well as when the .NET app (or \\"server\\") is built. We\'re using a specific type of SPA here; a [React](https://reactjs.org/) SPA built with [TypeScript](https://www.typescriptlang.org/) and [Material UI](https://material-ui.com/), however the principles here are general; you could surface this up any which way you choose.\\n\\n## Putting build info into `azure-pipelines.yml`\\n\\nThe first thing we\'re going to do is to inject our build details into two identical `buildinfo.json` files; one that sits in the server codebase and which will be used to drive the server build information, and one that sits in the client codebase to drive the client equivalent. They\'ll end up looking something like this:\\n\\n```json\\n{\\n  \\"buildNumber\\": \\"20210130.1\\",\\n  \\"buildId\\": \\"123456\\",\\n  \\"branchName\\": \\"main\\",\\n  \\"commitHash\\": \\"7089620222c30c1ad88e4b556c0a7908ddd34a8e\\"\\n}\\n```\\n\\nWe generate this by adding the following `yml` to the beginning of our `azure-pipelines.yml` (crucially before the client or server build take place):\\n\\n```yml\\n- script: |\\n      echo -e -n \\"{\\\\\\"buildNumber\\\\\\":\\\\\\"$(Build.BuildNumber)\\\\\\",\\\\\\"buildId\\\\\\":\\\\\\"$(Build.BuildId)\\\\\\",\\\\\\"branchName\\\\\\":\\\\\\"$(Build.SourceBranchName)\\\\\\",\\\\\\"commitHash\\\\\\":\\\\\\"$(Build.SourceVersion)\\\\\\"}\\" > \\"$(Build.SourcesDirectory)/src/client-app/src/buildinfo.json\\"\\n      echo -e -n \\"{\\\\\\"buildNumber\\\\\\":\\\\\\"$(Build.BuildNumber)\\\\\\",\\\\\\"buildId\\\\\\":\\\\\\"$(Build.BuildId)\\\\\\",\\\\\\"branchName\\\\\\":\\\\\\"$(Build.SourceBranchName)\\\\\\",\\\\\\"commitHash\\\\\\":\\\\\\"$(Build.SourceVersion)\\\\\\"}\\" > \\"$(Build.SourcesDirectory)/src/server-app/Server/buildinfo.json\\"\\n    displayName: \\"emit build details as JSON\\"\\n    failOnStderr: true\\n```\\n\\nAs you can see, we\'re placing the following variables that are available at build time in Azure Pipelines, into the `buildinfo.json`:\\n\\n- `BuildNumber` - The name of the completed build; which usually takes the form of a date in the `yyyyMMdd` format, suffixed by `.x` where `x` is a number that increments representing the number of builds that have taken place on the given day.\\n- `BuildId` - The ID of the record for the completed build.\\n- `SourceVersion` - This is the commit hash of the source code in Git\\n- `SourceBranchName` - The name of the branch in Git.\\n\\n[There\'s many variables available in Azure Pipelines that can be used](https://docs.microsoft.com/en-us/azure/devops/pipelines/build/variables?view=azure-devops&tabs=yaml#build-variables-devops-services) - we\'ve picked out the ones most interesting to us.\\n\\n## Surfacing the server build info\\n\\nOur pipeline is dropping the `buildinfo.json` over pre-existing stub `buildinfo.json` files in both our client and server codebases. The stub files look like this:\\n\\n```json\\n{\\n  \\"buildNumber\\": \\"yyyyMMdd.x\\",\\n  \\"buildId\\": \\"xxxxxx\\",\\n  \\"branchName\\": \\"\\",\\n  \\"commitHash\\": \\"LOCAL_BUILD\\"\\n}\\n```\\n\\nIn our .NET app, the `buildinfo.json` file has been dropped in the root of the app. And as luck would have it, all JSON files are automatically included in a .NET build and so it will be available at runtime. We want to surface this file through an API, and we also want to use it to stamp details into our logs.\\n\\nSo we need to parse the file, and for that we\'ll use this:\\n\\n```cs\\nusing System;\\nusing System.IO;\\nusing System.Text.Json;\\n\\nnamespace Server {\\n    public record BuildInfo(string BranchName, string BuildNumber, string BuildId, string CommitHash);\\n\\n    public static class AppVersionInfo {\\n        private const string _buildFileName = \\"buildinfo.json\\";\\n        private static BuildInfo _fileBuildInfo = new(\\n            BranchName: \\"\\",\\n            BuildNumber: DateTime.UtcNow.ToString(\\"yyyyMMdd\\") + \\".0\\",\\n            BuildId: \\"xxxxxx\\",\\n            CommitHash: $\\"Not yet initialised - call {nameof(InitialiseBuildInfoGivenPath)}\\"\\n        );\\n\\n        public static void InitialiseBuildInfoGivenPath(string path) {\\n            var buildFilePath = Path.Combine(path, _buildFileName);\\n            if (File.Exists(buildFilePath)) {\\n                try {\\n                    var buildInfoJson = File.ReadAllText(buildFilePath);\\n                    var buildInfo = JsonSerializer.Deserialize<BuildInfo>(buildInfoJson, new JsonSerializerOptions {\\n                        PropertyNamingPolicy = JsonNamingPolicy.CamelCase\\n                    });\\n                    if (buildInfo == null) throw new Exception($\\"Failed to deserialise {_buildFileName}\\");\\n\\n                    _fileBuildInfo = buildInfo;\\n                } catch (Exception) {\\n                    _fileBuildInfo = new BuildInfo(\\n                        BranchName: \\"\\",\\n                        BuildNumber: DateTime.UtcNow.ToString(\\"yyyyMMdd\\") + \\".0\\",\\n                        BuildId: \\"xxxxxx\\",\\n                        CommitHash: \\"Failed to load build info from buildinfo.json\\"\\n                    );\\n                }\\n            }\\n        }\\n\\n        public static BuildInfo GetBuildInfo() => _fileBuildInfo;\\n    }\\n}\\n```\\n\\nThe above code reads the `buildinfo.json` file and deserialises it into a `BuildInfo` record which is then surfaced up by the `GetBuildInfo` method. We initialise this at the start of our `Program.cs` like so:\\n\\n```cs\\npublic static int Main(string[] args) {\\n    AppVersionInfo.InitialiseBuildInfoGivenPath(Directory.GetCurrentDirectory());\\n    // Now we\'re free to call AppVersionInfo.GetBuildInfo()\\n    // ....\\n}\\n```\\n\\nNow we need a controller to surface this information up. We\'ll add ourselves a `BuildInfoController.cs`:\\n\\n```cs\\nusing Microsoft.AspNetCore.Authorization;\\nusing Microsoft.AspNetCore.Mvc;\\n\\nnamespace Server.Controllers {\\n    [ApiController]\\n    public class BuildInfoController : ControllerBase {\\n        [AllowAnonymous]\\n        [HttpGet(\\"api/build\\")]\\n        public BuildInfo GetBuild() => AppVersionInfo.GetBuildInfo();\\n    }\\n}\\n```\\n\\nThis exposes an `api/build` endpoint in our .NET app that, when hit, will display the following JSON:\\n\\n![screenshot of api/build output](api-build-screenshot.png)\\n\\n## Surfacing the client build info\\n\\nOur server now lets the world know which version it is running and this is tremendous. Now let\'s make our client do the same.\\n\\nVery little is required to achieve this. Again we have a `buildinfo.json` sat in the root of our codebase. We\'re able to import it as a module in TypeScript because we\'ve set the following property in our `tsconfig.json`:\\n\\n```json\\n\\"resolveJsonModule\\": true,\\n```\\n\\nAs a consequence, consumption is as simple as:\\n\\n```ts\\nimport clientBuildInfo from \'./buildinfo.json\';\\n```\\n\\nWhich provides us with a `clientBuildInfo` which TypeScript automatically derives as this type:\\n\\n```ts\\ntype ClientBuildInfo = {\\n  buildNumber: string;\\n  buildId: string;\\n  branchName: string;\\n  commitHash: string;\\n};\\n```\\n\\nHow you choose to use that information is entirely your choice. We\'re going to add ourselves an \\"about\\" screen in our app, which displays both client info (loaded using the mechanism above) and server info (`fetch`ed from the `/api/build` endpoint).\\n\\n```tsx\\nimport {\\n  Card,\\n  CardContent,\\n  CardHeader,\\n  createStyles,\\n  Grid,\\n  makeStyles,\\n  Theme,\\n  Typography,\\n  Zoom,\\n} from \'@material-ui/core\';\\nimport React from \'react\';\\nimport clientBuildInfo from \'../../buildinfo.json\';\\nimport { projectsPurple } from \'../shared/colors\';\\nimport { Loading } from \'../shared/Loading\';\\nimport { TransitionContainer } from \'../shared/TransitionContainer\';\\n\\nconst useStyles = (cardColor: string) =>\\n  makeStyles((theme: Theme) =>\\n    createStyles({\\n      card: {\\n        padding: theme.spacing(0),\\n        backgroundColor: cardColor,\\n        color: theme.palette.common.white,\\n        minHeight: theme.spacing(28),\\n      },\\n      avatar: {\\n        backgroundColor: theme.palette.getContrastText(cardColor),\\n        color: cardColor,\\n      },\\n      main: {\\n        padding: theme.spacing(2),\\n      },\\n    })\\n  )();\\n\\ntype Styles = ReturnType<typeof useStyles>;\\n\\nconst AboutPage: React.FC = () => {\\n  const [serverBuildInfo, setServerBuildInfo] =\\n    React.useState<typeof clientBuildInfo>();\\n\\n  React.useEffect(() => {\\n    fetch(\'/api/build\')\\n      .then((response) => response.json())\\n      .then(setServerBuildInfo);\\n  }, []);\\n\\n  const classes = useStyles(projectsPurple);\\n\\n  return (\\n    <TransitionContainer>\\n      <Grid container spacing={3}>\\n        <Grid item xs={12} sm={12} container alignItems=\\"center\\">\\n          <Grid item>\\n            <Typography variant=\\"h4\\" component=\\"h1\\">\\n              About\\n            </Typography>\\n          </Grid>\\n        </Grid>\\n      </Grid>\\n      <Grid container spacing={1}>\\n        <BuildInfo\\n          classes={classes}\\n          title=\\"Client Version\\"\\n          {...clientBuildInfo}\\n        />\\n      </Grid>\\n      <br />\\n      <Grid container spacing={1}>\\n        {serverBuildInfo ? (\\n          <BuildInfo\\n            classes={classes}\\n            title=\\"Server Version\\"\\n            {...serverBuildInfo}\\n          />\\n        ) : (\\n          <Loading />\\n        )}\\n      </Grid>\\n    </TransitionContainer>\\n  );\\n};\\n\\ninterface Props {\\n  classes: Styles;\\n  title: string;\\n  branchName: string;\\n  buildNumber: string;\\n  buildId: string;\\n  commitHash: string;\\n}\\n\\nconst BuildInfo: React.FC<Props> = ({\\n  classes,\\n  title,\\n  branchName,\\n  buildNumber,\\n  buildId,\\n  commitHash,\\n}) => (\\n  <Zoom mountOnEnter unmountOnExit in={true}>\\n    <Card className={classes.card}>\\n      <CardHeader title={title} />\\n      <CardContent className={classes.main}>\\n        <Typography variant=\\"body1\\" component=\\"p\\">\\n          <b>Build Number</b> {buildNumber}\\n        </Typography>\\n        <Typography variant=\\"body1\\" component=\\"p\\">\\n          <b>Build Id</b> {buildId}\\n        </Typography>\\n        <Typography variant=\\"body1\\" component=\\"p\\">\\n          <b>Branch Name</b> {branchName}\\n        </Typography>\\n        <Typography variant=\\"body1\\" component=\\"p\\">\\n          <b>Commit Hash</b> {commitHash}\\n        </Typography>\\n      </CardContent>\\n    </Card>\\n  </Zoom>\\n);\\n\\nexport default AboutPage;\\n```\\n\\nWhen the above page is viewed it looks like this:\\n\\n![screenshot of our web app surfacing up the build information](about-page.png)\\n\\nAnd that\'s it! Our app is clearly telling us what version is being run, both on the server and in the client. Thanks to Scott Hanselman for his work which inspired this."},{"id":"/2021/01/17/azure-easy-auth-and-roles-with-net-and-microsoft-identity-web","metadata":{"permalink":"/2021/01/17/azure-easy-auth-and-roles-with-net-and-microsoft-identity-web","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-01-17-azure-easy-auth-and-roles-with-net-and-microsoft-identity-web/index.md","source":"@site/blog/2021-01-17-azure-easy-auth-and-roles-with-net-and-microsoft-identity-web/index.md","title":"Azure Easy Auth and Roles with .NET and Microsoft.Identity.Web","description":"I wrote recently about how to get Azure Easy Auth to work with roles. This involved borrowing the approach used by MaximeRouiller.Azure.AppService.EasyAuth.","date":"2021-01-17T00:00:00.000Z","formattedDate":"January 17, 2021","tags":[{"label":"Azure","permalink":"/tags/azure"},{"label":"Easy Auth","permalink":"/tags/easy-auth"},{"label":"Roles","permalink":"/tags/roles"},{"label":"ASP.NET","permalink":"/tags/asp-net"},{"label":"Microsoft.Identity.Web","permalink":"/tags/microsoft-identity-web"}],"readingTime":2.35,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Azure Easy Auth and Roles with .NET and Microsoft.Identity.Web","authors":"johnnyreilly","tags":["Azure","Easy Auth","Roles","ASP.NET","Microsoft.Identity.Web"],"hide_table_of_contents":false},"prevItem":{"title":"Azure Pipelines Build Info in an ASP.NET React app","permalink":"/2021/01/29/surfacing-azure-pipelines-build-info-in-an-aspnet-react-app"},"nextItem":{"title":"Azure Easy Auth and Roles with .NET (and .NET Core)","permalink":"/2021/01/14/azure-easy-auth-and-roles-with-dotnet-and-core"}},"content":"[I wrote recently about how to get Azure Easy Auth to work with roles](./2021-01-14-azure-easy-auth-and-roles-with-dotnet-and-core/index.md). This involved borrowing the approach used by [MaximeRouiller.Azure.AppService.EasyAuth](https://github.com/MaximRouiller/MaximeRouiller.Azure.AppService.EasyAuth).\\n\\nAs a consequence of writing that post I came to learn that official support for Azure Easy Auth had landed in October 2020 in v1.2 of [Microsoft.Identity.Web](https://github.com/AzureAD/microsoft-identity-web/wiki/1.2.0#integration-with-azure-app-services-authentication-of-web-apps-running-with-microsoftidentityweb). This was great news; I was delighted.\\n\\nHowever, it turns out that the same authorization issue that `MaximeRouiller.Azure.AppService.EasyAuth` suffers from, is visited upon `Microsoft.Identity.Web` as well.\\n\\n## Getting set up\\n\\nWe\'re using a .NET 5 project, running in an Azure App Service (Linux). In our `.csproj` we have:\\n\\n```xml\\n<PackageReference Include=\\"Microsoft.Identity.Web\\" Version=\\"1.4.1\\" />\\n```\\n\\nIn our `Startup.cs` we\'re using:\\n\\n```cs\\npublic void ConfigureServices(IServiceCollection services) {\\n    //...\\n    services.AddMicrosoftIdentityWebAppAuthentication(Configuration);\\n    //...\\n}\\n\\npublic void Configure(IApplicationBuilder app, IWebHostEnvironment env) {\\n    //...\\n    app.UseAuthentication();\\n    app.UseAuthorization();\\n    //...\\n}\\n```\\n\\n## You gotta `roles` with it\\n\\nWhilst the authentication works, authorization does not. So whilst my app knows who I am - the authorization is not working with relation to **roles**.\\n\\nWhen directly using `Microsoft.Identity.Web` when running locally, we see these claims:\\n\\n```json\\n[\\n  // ...\\n  {\\n    \\"type\\": \\"http://schemas.microsoft.com/ws/2008/06/identity/claims/role\\",\\n    \\"value\\": \\"Administrator\\"\\n  },\\n  {\\n    \\"type\\": \\"http://schemas.microsoft.com/ws/2008/06/identity/claims/role\\",\\n    \\"value\\": \\"Reader\\"\\n  }\\n  // ...\\n]\\n```\\n\\nHowever, we get different behaviour with EasyAuth; it provides roles related claims with a **different type**:\\n\\n```json\\n[\\n  // ...\\n  {\\n    \\"type\\": \\"roles\\",\\n    \\"value\\": \\"Administrator\\"\\n  },\\n  {\\n    \\"type\\": \\"roles\\",\\n    \\"value\\": \\"Reader\\"\\n  }\\n  // ...\\n]\\n```\\n\\nThis means that roles related authorization _does not work_ with Easy Auth:\\n\\n```cs\\n[Authorize(Roles = \\"Reader\\")]\\n[HttpGet(\\"api/reader\\")]\\npublic string GetWithReader() =>\\n    \\"this is a secure endpoint that users with the Reader role can access\\";\\n```\\n\\nThis is because .NET is looking for claims with a `type` of `\\"http://schemas.microsoft.com/ws/2008/06/identity/claims/role\\"` and not finding them with Easy Auth.\\n\\n## Claims transformation FTW\\n\\nThere is a way to work around this issue .NET using `IClaimsTransformation`. This is a poorly documented feature, but fortunately [Gunnar Peipman\'s blog does a grand job of explaining it](https://gunnarpeipman.com/aspnet-core-adding-claims-to-existing-identity/).\\n\\nInside our `Startup.cs` I\'ve registered a claims transformer:\\n\\n```cs\\nservices.AddScoped<IClaimsTransformation, AddRolesClaimsTransformation>();\\n```\\n\\nAnd that claims transformer looks like this:\\n\\n```cs\\npublic class AddRolesClaimsTransformation : IClaimsTransformation {\\n    private readonly ILogger<AddRolesClaimsTransformation> _logger;\\n\\n    public AddRolesClaimsTransformation(ILogger<AddRolesClaimsTransformation> logger) {\\n        _logger = logger;\\n    }\\n\\n    public Task<ClaimsPrincipal> TransformAsync(ClaimsPrincipal principal) {\\n        var mappedRolesClaims = principal.Claims\\n            .Where(claim => claim.Type == \\"roles\\")\\n            .Select(claim => new Claim(ClaimTypes.Role, claim.Value))\\n            .ToList();\\n\\n        // Clone current identity\\n        var clone = principal.Clone();\\n\\n        if (clone.Identity is not ClaimsIdentity newIdentity) return Task.FromResult(principal);\\n\\n        // Add role claims to cloned identity\\n        foreach (var mappedRoleClaim in mappedRolesClaims)\\n            newIdentity.AddClaim(mappedRoleClaim);\\n\\n        if (mappedRolesClaims.Count > 0)\\n            _logger.LogInformation(\\"Added roles claims {mappedRolesClaims}\\", mappedRolesClaims);\\n        else\\n            _logger.LogInformation(\\"No roles claims added\\");\\n\\n        return Task.FromResult(clone);\\n    }\\n}\\n```\\n\\nThe class above creates a new principal with `\\"roles\\"` claims mapped across to `\\"http://schemas.microsoft.com/ws/2008/06/identity/claims/role\\"`. This is enough to get .NET treating roles the way you\'d hope.\\n\\n[I\'ve raised an issue against the `Microsoft.Identity.Web` repo](https://github.com/AzureAD/microsoft-identity-web/issues/881) about this. Perhaps one day this workaround will no longer be necessary."},{"id":"/2021/01/14/azure-easy-auth-and-roles-with-dotnet-and-core","metadata":{"permalink":"/2021/01/14/azure-easy-auth-and-roles-with-dotnet-and-core","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-01-14-azure-easy-auth-and-roles-with-dotnet-and-core/index.md","source":"@site/blog/2021-01-14-azure-easy-auth-and-roles-with-dotnet-and-core/index.md","title":"Azure Easy Auth and Roles with .NET (and .NET Core)","description":"If this post is interesting to you, you may also want to look at this one where we try to use Microsoft.Identity.Web for the same purpose.","date":"2021-01-14T00:00:00.000Z","formattedDate":"January 14, 2021","tags":[{"label":"Azure","permalink":"/tags/azure"},{"label":"App service","permalink":"/tags/app-service"},{"label":"authorisation","permalink":"/tags/authorisation"},{"label":"Authentication","permalink":"/tags/authentication"},{"label":"azure AD","permalink":"/tags/azure-ad"}],"readingTime":5.635,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Azure Easy Auth and Roles with .NET (and .NET Core)","authors":"johnnyreilly","tags":["Azure","App service","authorisation","Authentication","azure AD"],"hide_table_of_contents":false},"prevItem":{"title":"Azure Easy Auth and Roles with .NET and Microsoft.Identity.Web","permalink":"/2021/01/17/azure-easy-auth-and-roles-with-net-and-microsoft-identity-web"},"nextItem":{"title":"react-query: strongly typing useQueries","permalink":"/2021/01/03/strongly-typing-react-query-s-usequeries"}},"content":"_If this post is interesting to you, you may also want to [look at this one where we try to use Microsoft.Identity.Web for the same purpose.](./2021-01-17-azure-easy-auth-and-roles-with-net-and-microsoft-identity-web/index.md)_\\n\\nAzure has a feature which is intended to allow Authentication and Authorization to be applied outside of your application code. It\'s called [\\"Easy Auth\\"](https://docs.microsoft.com/en-us/azure/app-service/overview-authentication-authorization). Unfortunately, in the context of App Services it doesn\'t work with .NET Core and .NET. Perhaps it would be better to say: of the various .NETs, it supports .NET Framework. [To quote the docs](https://docs.microsoft.com/en-us/azure/app-service/overview-authentication-authorization#userapplication-claims):\\n\\n> At this time, ASP.NET Core does not currently support populating the current user with the Authentication/Authorization feature. However, some [3rd party, open source middleware components](https://github.com/MaximRouiller/MaximeRouiller.Azure.AppService.EasyAuth) do exist to help fill this gap.\\n\\nThanks to [Maxime Rouiller](https://twitter.com/MaximRouiller) there\'s a way forward here. However, as I was taking this for a spin today, I discovered another issue.\\n\\n## Where are our roles?\\n\\nConsider the following .NET controller:\\n\\n```cs\\n[Authorize(Roles = \\"Administrator,Reader\\")]\\n[HttpGet(\\"api/admin-reader\\")]\\npublic string GetWithAdminOrReader() =>\\n    \\"this is a secure endpoint that users with the Administrator or Reader role can access\\";\\n\\n[Authorize(Roles = \\"Administrator\\")]\\n[HttpGet(\\"api/admin\\")]\\npublic string GetWithAdmin() =>\\n    \\"this is a secure endpoint that users with the Administrator role can access\\";\\n\\n[Authorize(Roles = \\"Reader\\")]\\n[HttpGet(\\"api/reader\\")]\\npublic string GetWithReader() =>\\n    \\"this is a secure endpoint that users with the Reader role can access\\";\\n```\\n\\nThe three endpoints above restrict access based upon roles. However, even with Maxime\'s marvellous shim in the mix, authorization doesn\'t work when deployed to an Azure App Service. Why? Well, it comes down to how roles are mapped to claims.\\n\\nLet\'s back up a bit. First of all we\'ve added a dependency to our project:\\n\\n```shell\\ndotnet add package MaximeRouiller.Azure.AppService.EasyAuth\\n```\\n\\nNext we\'ve updated our `Startup.ConfigureServices` such that it looks like this:\\n\\n```cs\\nif (Env.IsDevelopment()) {\\n    services.AddMicrosoftIdentityWebAppAuthentication(Configuration);\\nelse\\n    services.AddAuthentication(\\"EasyAuth\\").AddEasyAuthAuthentication((o) => { });\\n```\\n\\nWith the above in place, either the Microsoft Identity platform will directly be used for authentication, or Maxime\'s package will be used as the default authentication scheme. The driver for this is `Env` which is an `IHostEnvironment` that was injected to the `Startup.cs`. Running locally, both authentication and authorization will work. However, deployed to an Azure App Service, only authentication will work.\\n\\nIt turns out that directly using the Microsoft Identity platform, we see roles claims coming through like so:\\n\\n```json\\n[\\n  // ...\\n  {\\n    \\"type\\": \\"http://schemas.microsoft.com/ws/2008/06/identity/claims/role\\",\\n    \\"value\\": \\"Administrator\\"\\n  },\\n  {\\n    \\"type\\": \\"http://schemas.microsoft.com/ws/2008/06/identity/claims/role\\",\\n    \\"value\\": \\"Reader\\"\\n  }\\n  // ...\\n]\\n```\\n\\nBut in Azure we see roles claims showing up with a different `type`:\\n\\n```json\\n[\\n  // ...\\n  {\\n    \\"type\\": \\"roles\\",\\n    \\"value\\": \\"Administrator\\"\\n  },\\n  {\\n    \\"type\\": \\"roles\\",\\n    \\"value\\": \\"Reader\\"\\n  }\\n  // ...\\n]\\n```\\n\\nThis is the crux of the problem; .NET and .NET Core are looking in a different place for roles.\\n\\n## Role up, role up!\\n\\nThere wasn\'t an obvious way to make this work with Maxime\'s package. So we ended up lifting the source code of Maxime\'s package and tweaking it. Take a look:\\n\\n```cs\\nusing Microsoft.AspNetCore.Authentication;\\nusing Microsoft.Extensions.DependencyInjection;\\nusing Microsoft.Extensions.Logging;\\nusing Microsoft.Extensions.Options;\\nusing System;\\nusing System.Collections.Generic;\\nusing System.Linq;\\nusing System.Security.Claims;\\nusing System.Text.Encodings.Web;\\nusing System.Text.Json;\\nusing System.Text.Json.Serialization;\\nusing System.Threading.Tasks;\\n\\n/// <summary>\\n/// Based on https://github.com/MaximRouiller/MaximeRouiller.Azure.AppService.EasyAuth\\n/// Essentially EasyAuth only supports .NET Framework: https://docs.microsoft.com/en-us/azure/app-service/app-service-authentication-how-to#access-user-claims\\n/// This allows us to get support for Authentication and Authorization (using roles) with .NET\\n/// </summary>\\nnamespace EasyAuth {\\n    public static class EasyAuthAuthenticationBuilderExtensions {\\n        public static AuthenticationBuilder AddEasyAuthAuthentication(\\n            this IServiceCollection services) =>\\n            services.AddAuthentication(\\"EasyAuth\\").AddEasyAuthAuthenticationScheme(o => { });\\n\\n        public static AuthenticationBuilder AddEasyAuthAuthenticationScheme(\\n            this AuthenticationBuilder builder,\\n            Action<EasyAuthAuthenticationOptions> configure) =>\\n                builder.AddScheme<EasyAuthAuthenticationOptions, EasyAuthAuthenticationHandler>(\\n                    \\"EasyAuth\\",\\n                    \\"EasyAuth\\",\\n                    configure);\\n    }\\n\\n    public class EasyAuthAuthenticationOptions : AuthenticationSchemeOptions {\\n        public EasyAuthAuthenticationOptions() {\\n            Events = new object();\\n        }\\n    }\\n\\n    public class EasyAuthAuthenticationHandler : AuthenticationHandler<EasyAuthAuthenticationOptions> {\\n        public EasyAuthAuthenticationHandler(\\n            IOptionsMonitor<EasyAuthAuthenticationOptions> options,\\n            ILoggerFactory logger,\\n            UrlEncoder encoder,\\n            ISystemClock clock)\\n            : base(options, logger, encoder, clock) {\\n        }\\n\\n        protected override Task<AuthenticateResult> HandleAuthenticateAsync() {\\n            try {\\n                var easyAuthEnabled = string.Equals(Environment.GetEnvironmentVariable(\\"WEBSITE_AUTH_ENABLED\\", EnvironmentVariableTarget.Process), \\"True\\", StringComparison.InvariantCultureIgnoreCase);\\n                if (!easyAuthEnabled) return Task.FromResult(AuthenticateResult.NoResult());\\n\\n                var easyAuthProvider = Context.Request.Headers[\\"X-MS-CLIENT-PRINCIPAL-IDP\\"].FirstOrDefault();\\n                var msClientPrincipalEncoded = Context.Request.Headers[\\"X-MS-CLIENT-PRINCIPAL\\"].FirstOrDefault();\\n                if (string.IsNullOrWhiteSpace(easyAuthProvider) ||\\n                    string.IsNullOrWhiteSpace(msClientPrincipalEncoded))\\n                    return Task.FromResult(AuthenticateResult.NoResult());\\n\\n                var decodedBytes = Convert.FromBase64String(msClientPrincipalEncoded);\\n                var msClientPrincipalDecoded = System.Text.Encoding.Default.GetString(decodedBytes);\\n                var clientPrincipal = JsonSerializer.Deserialize<MsClientPrincipal>(msClientPrincipalDecoded);\\n                if (clientPrincipal == null) return Task.FromResult(AuthenticateResult.NoResult());\\n\\n                var mappedRolesClaims = clientPrincipal.Claims\\n                    .Where(claim => claim.Type == \\"roles\\")\\n                    .Select(claim => new Claim(ClaimTypes.Role, claim.Value))\\n                    .ToList();\\n\\n                var claims = clientPrincipal.Claims.Select(claim => new Claim(claim.Type, claim.Value)).ToList();\\n                claims.AddRange(mappedRolesClaims);\\n\\n                var principal = new ClaimsPrincipal();\\n                principal.AddIdentity(new ClaimsIdentity(claims, clientPrincipal.AuthenticationType, clientPrincipal.NameType, clientPrincipal.RoleType));\\n\\n                var ticket = new AuthenticationTicket(principal, easyAuthProvider);\\n                var success = AuthenticateResult.Success(ticket);\\n                Context.User = principal;\\n\\n                return Task.FromResult(success);\\n            } catch (Exception ex) {\\n                return Task.FromResult(AuthenticateResult.Fail(ex));\\n            }\\n        }\\n    }\\n\\n    public class MsClientPrincipal {\\n        [JsonPropertyName(\\"auth_typ\\")]\\n        public string? AuthenticationType { get; set; }\\n        [JsonPropertyName(\\"claims\\")]\\n        public IEnumerable<UserClaim> Claims { get; set; } = Array.Empty<UserClaim>();\\n        [JsonPropertyName(\\"name_typ\\")]\\n        public string? NameType { get; set; }\\n        [JsonPropertyName(\\"role_typ\\")]\\n        public string? RoleType { get; set; }\\n    }\\n\\n    public class UserClaim {\\n        [JsonPropertyName(\\"typ\\")]\\n        public string Type { get; set; } = string.Empty;\\n        [JsonPropertyName(\\"val\\")]\\n        public string Value { get; set; } = string.Empty;\\n    }\\n}\\n```\\n\\nThere\'s a number of changes in the above code to Maxime\'s package. Three changes that are not significant and one that is. First the insignificant changes:\\n\\n1. It uses [`System.Text.Json`](https://docs.microsoft.com/en-us/dotnet/standard/serialization/system-text-json-how-to?pivots=dotnet-5-0) in place of JSON.NET\\n2. It uses [C#s nullable reference types](./2020-12-20-nullable-reference-types-csharp-strictnullchecks/index.md)\\n3. It changes the extension method signature such that instead of entering `services.AddAuthentication().AddEasyAuthAuthentication((o) => { })` we now need only enter `services.AddEasyAuthAuthentication()`\\n\\nNow the significant change:\\n\\nWhere the middleware encounters claims in the `X-MS-CLIENT-PRINCIPAL` header with the `Type` of `\\"roles\\"` it creates brand new claims for each, with the same `Value` but with the official `Type` supplied by `ClaimsTypes.Role` of `\\"http://schemas.microsoft.com/ws/2008/06/identity/claims/role\\"`. The upshot of this, is that when the processed claims are inspected in Azure they now look more like this:\\n\\n```json\\n[\\n  // ...\\n  {\\n    \\"type\\": \\"roles\\",\\n    \\"value\\": \\"Administrator\\"\\n  },\\n  {\\n    \\"type\\": \\"roles\\",\\n    \\"value\\": \\"Reader\\"\\n  },\\n  // ...\\n  {\\n    \\"type\\": \\"http://schemas.microsoft.com/ws/2008/06/identity/claims/role\\",\\n    \\"value\\": \\"Administrator\\"\\n  },\\n  {\\n    \\"type\\": \\"http://schemas.microsoft.com/ws/2008/06/identity/claims/role\\",\\n    \\"value\\": \\"Reader\\"\\n  }\\n]\\n```\\n\\nAs you can see, we now have both the originally supplied roles _as well_ as roles of the type that .NET and .NET Core expect. Consequently, roles based behaviour starts to work. Thanks to Maxime for his fine work on the initial solution. It would be tremendous if neither the code in this blog post nor Maxime\'s shim were required. Still, until that glorious day!\\n\\n## Update: Potential ways forward\\n\\nWhen I was tweeting this post, Maxime was good enough to respond and suggest that this may be resolved within Azure itself in future:\\n\\n> Oh, so that\'s why they removed the name? \ud83d\ude32\ud83d\ude1c Jokes aside, we hope that this package won\'t be necessary for the future. I know that [@mattchenderson](https://twitter.com/mattchenderson?ref_src=twsrc%5Etfw) is part of a working group to update Easy Auth. Might want to make sure you follow him as well. \ud83d\ude01\\n>\\n> \u2014 Maxime Rouiller (@MaximRouiller) [January 14, 2021](https://twitter.com/MaximRouiller/status/1349804324713615366?ref_src=twsrc%5Etfw)\\n\\nThere\'s a prospective PR that would add an event to Maxime\'s API. If something along these lines was merged, then my workaround would no longer be necessary. Follow the PR [here](https://github.com/MaximRouiller/MaximeRouiller.Azure.AppService.EasyAuth/pull/13)."},{"id":"/2021/01/03/strongly-typing-react-query-s-usequeries","metadata":{"permalink":"/2021/01/03/strongly-typing-react-query-s-usequeries","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-01-03-strongly-typing-react-query-s-usequeries/index.md","source":"@site/blog/2021-01-03-strongly-typing-react-query-s-usequeries/index.md","title":"react-query: strongly typing useQueries","description":"react-query has a weakly typed hook named useQueries. It\'s possible to turn that into a strong typed hook; this post shows you how.","date":"2021-01-03T00:00:00.000Z","formattedDate":"January 3, 2021","tags":[{"label":"useQueries","permalink":"/tags/use-queries"},{"label":"react-query","permalink":"/tags/react-query"}],"readingTime":7.795,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"react-query: strongly typing useQueries","authors":"johnnyreilly","image":"./strongly-typing-usequeries.png","tags":["useQueries","react-query"],"hide_table_of_contents":false},"prevItem":{"title":"Azure Easy Auth and Roles with .NET (and .NET Core)","permalink":"/2021/01/14/azure-easy-auth-and-roles-with-dotnet-and-core"},"nextItem":{"title":"Create React App with ts-loader and CRACO","permalink":"/2021/01/02/create-react-app-with-ts-loader-and-craco"}},"content":"`react-query` has a weakly typed hook named `useQueries`. It\'s possible to turn that into a strong typed hook; this post shows you how.\\n\\n![title image that says \\"react-query: strongly typings useQueries\\"](strongly-typing-usequeries.png)\\n\\n## Update April 2022\\n\\nYou don\'t need this blog post! Just use a `react-query@3.28.0` or greater; [artysidorenko](https://github.com/artysidorenko) [contributed a PR that moved this behaviour into the package](https://github.com/tannerlinsley/react-query/pull/2634).\\n\\n## What is `useQueries`?\\n\\nIf you haven\'t used [`react-query`](https://react-query.tanstack.com/) then I heartily recommend it. It provides (to quote the docs):\\n\\n> Hooks for fetching, caching and updating asynchronous data in React\\n\\nWith version 3 of `react-query`, a new hook was added: [`useQueries`](https://react-query.tanstack.com/reference/useQueries). This hook allows you fetch a variable number of queries at the same time. An example of what usage looks like is this ([borrowed from the excellent docs](https://react-query.tanstack.com/guides/parallel-queries#dynamic-parallel-queries-with-usequeries)):\\n\\n```tsx\\nfunction App({ users }) {\\n  const userQueries = useQueries(\\n    users.map((user) => {\\n      return {\\n        queryKey: [\'user\', user.id],\\n        queryFn: () => fetchUserById(user.id),\\n      };\\n    })\\n  );\\n}\\n```\\n\\nWhilst `react-query` is written in TypeScript, the way that `useQueries` is presently written strips the types that are supplied to it. Consider [the signature of the `useQueries`](https://github.com/tannerlinsley/react-query/blob/d25ab3ef8260ea1c02b52b7082c3ce4120596b31/src/react/useQueries.ts#L8):\\n\\n```ts\\nexport function useQueries(queries: UseQueryOptions[]): UseQueryResult[] {\\n```\\n\\nThis returns an array of [`UseQueryResult`](https://github.com/tannerlinsley/react-query/blob/d25ab3ef8260ea1c02b52b7082c3ce4120596b31/src/react/types.ts#L42):\\n\\n```ts\\nexport type UseQueryResult<\\n  TData = unknown,\\n  TError = unknown\\n> = UseBaseQueryResult<TData, TError>;\\n```\\n\\nAs you can see, no type parameters are passed to `UseQueryResult` in the `useQueries` signature and so it takes the default types of `unknown`. This forces the consumer to either assert the type that they believe to be there, or to use type narrowing to ensure the type. The former approach exposes a possibility of errors (the user can specify incorrect types) and the latter approach requires our code to perform type narrowing operations which are essentially unnecessary (the type hasn\'t changed since it was returned; it\'s simply been discarded).\\n\\nWhat if there was a way to strongly type `useQueries` so we neither risked specifying incorrect types, nor wasted precious lines of code and CPU cycles performing type narrowing? There is my friends, read on!\\n\\n## `useQueriesTyped` - a strongly typed wrapper for `useQueries`\\n\\nIt\'s possible to wrap the `useQueries` hook with our own `useQueriesTyped` hook which exposes a strongly typed API. It looks like this:\\n\\n```ts\\nimport { useQueries, UseQueryOptions, UseQueryResult } from \'react-query\';\\n\\ntype Awaited<T> = T extends PromiseLike<infer U> ? Awaited<U> : T;\\n\\nexport function useQueriesTyped<TQueries extends readonly UseQueryOptions[]>(\\n  queries: [...TQueries]\\n): {\\n  [ArrayElement in keyof TQueries]: UseQueryResult<\\n    TQueries[ArrayElement] extends { select: infer TSelect }\\n      ? TSelect extends (data: any) => any\\n        ? ReturnType<TSelect>\\n        : never\\n      : Awaited<\\n          ReturnType<\\n            NonNullable<\\n              Extract<TQueries[ArrayElement], UseQueryOptions>[\'queryFn\']\\n            >\\n          >\\n        >\\n  >;\\n} {\\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\\n  return useQueries(\\n    queries as UseQueryOptions<unknown, unknown, unknown>[]\\n  ) as any;\\n}\\n```\\n\\nLet\'s unpack this. The first and most significant thing to note here is that `queries` moves from being `UseQueryOptions[]` to being `TQueries extends readonly UseQueryOptions[]` \\\\- far more fancy! The reason for this change is we want the type parameters to flow through on an element by element basis in the supplied array. [TypeScript 4\'s variadic tuple types](https://www.typescriptlang.org/docs/handbook/release-notes/typescript-4-0.html#variadic-tuple-types) should allow us to support this. So the new array signature looks like this:\\n\\n```ts\\nqueries: [...TQueries];\\n```\\n\\nWhere `TQueries` is\\n\\n```ts\\nTQueries extends readonly UseQueryOptions[]\\n```\\n\\nWhat this means is, that each element of the rest parameters array must have a type of `readonly UseQueryOptions`. Otherwise the compiler will shout at us (and rightly so).\\n\\nSo that\'s what\'s coming in.... What\'s going out? Well the return type of `useQueriesTyped` is the tremendously verbose:\\n\\n```ts\\n{\\n  [ArrayElement in keyof TQueries]: UseQueryResult<\\n    TQueries[ArrayElement] extends { select: infer TSelect }\\n      ? TSelect extends (data: any) => any\\n        ? ReturnType<TSelect>\\n        : never\\n      : Awaited<\\n          ReturnType<\\n            NonNullable<\\n              Extract<TQueries[ArrayElement], UseQueryOptions>[\'queryFn\']\\n            >\\n          >\\n        >\\n  >\\n}\\n```\\n\\nLet\'s walk this through. First of all we\'ll look at this bit:\\n\\n```ts\\n{ [ArrayElement in keyof TQueries]: /* the type has been stripped to protect your eyes */ }\\n```\\n\\nOn the face of it, it looks like we\'re returning an `Object`, not an `Array`. There\'s nuance here; [JavaScript `Array`s are `Object`s](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array).\\n\\nMore specifically, by approaching the signature this way, we can acquire the `ArrayElement` type which represents each of the keys of the array. Consider this array:\\n\\n```ts\\n[1, \'two\', new Date()];\\n```\\n\\nFor the above, `ArrayElement` would take the values `0`, `1` and `2`. And this is going to prove useful in a moment as we\'re going to index into our `TQueries` object to surface up the return types for each element of our return array from there.\\n\\nNow let\'s look at the return type for each element. The signature of that looks like this:\\n\\n```ts\\nUseQueryResult<\\n    TQueries[ArrayElement] extends { select: infer TSelect }\\n      ? TSelect extends (data: any) => any\\n        ? ReturnType<TSelect>\\n        : never\\n      : Awaited<\\n          ReturnType<\\n            NonNullable<\\n              Extract<TQueries[ArrayElement], UseQueryOptions>[\'queryFn\']\\n            >\\n          >\\n        >\\n  >\\n```\\n\\nGosh... Well there\'s a lot going on here. Let\'s start in the middle and work our way out.\\n\\n```ts\\nTQueries[ArrayElement];\\n```\\n\\nThe above code indexes into our `TQueries` array for each element of our strongly typed indexer `ArrayElement`. So it might resolve the first element of an array to `{ queryKey: \'key1\', queryFn: () =&gt; 1 }`, for example. Next:\\n\\n```ts\\nExtract < TQueries[ArrayElement], UseQueryOptions > [\'queryFn\'];\\n```\\n\\nWe\'re now taking the type of each element provided, and grabbing the type of the `queryFn` property. It\'s this type which contains the type of the data that will be passed back, that we want to make use of. So for an examples of `[{ queryKey: \'key1\', queryFn: () =&gt; 1 }, { queryKey: \'key2\', queryFn: () =&gt; \'two\' }, { queryKey: \'key3\', queryFn: () =&gt; new Date() }]` we\'d have the type: `const result: [() =&gt; number, () =&gt; string, () =&gt; Date]`.\\n\\n```ts\\nNonNullable<Extract<TQueries[ArrayElement], UseQueryOptions>[\'queryFn\']>\\n```\\n\\nThe next stage is using `NonNullable` on our `queryFn`, given that on `UseQueryOptions` it\'s an optional type. In our use case it is not optional / nullable and so we need to enforce that.\\n\\n```ts\\nReturnType<NonNullable<Extract<TQueries[ArrayElement], UseQueryOptions>[\'queryFn\']>>\\n```\\n\\nNow we want to get the return type of our `queryFn` \\\\- as that\'s the data type we\'re interested. So we use TypeScript\'s `ReturnType` for that.\\n\\n```ts\\nReturnType<NonNullable<Extract<TQueries[ArrayElement], UseQueryOptions>[\'queryFn\']>>\\n```\\n\\nHere we\'re using [TypeScript 4.1\'s recursive conditional types](https://devblogs.microsoft.com/typescript/announcing-typescript-4-1/#recursive-conditional-types) to unwrap a `Promise` (or not) to the relevant type. This allows us to get the actual type we\'re interested in, as opposed to the `Promise` of that type. Finally we have the type we need! So we can do this:\\n\\n```ts\\ntype Awaited<T> = T extends PromiseLike<infer U> ? Awaited<U> : T;\\n\\nAwaited<ReturnType<NonNullable<Extract<TQueries[ArrayElement], UseQueryOptions>[\'queryFn\']>>>\\n```\\n\\nIt\'s at this point where we reach a conditional type in our type definition. Essentially, we have two different typing behaviours in play:\\n\\n1. Where we\'re inferring the return type of the query\\n2. Where we\'re inferring the return type of a `select`. A `select` option can be used to transform or select a part of the data returned by the query function. It has the signature: `select: (data: TData) => TSelect`\\n\\nWe\'ve been unpacking the first of these so far. Now we encounter the conditional type that chooses between them:\\n\\n```ts\\nTQueries[ArrayElement] extends { select: infer TSelect }\\n      ? TSelect extends (data: any) => any\\n        ? ReturnType<TSelect>\\n        : never\\n      : Awaited< /*...*/ >\\n  >\\n```\\n\\nWhat\'s happening here is:\\n\\n- if a query includes a `select` option, we infer what that is and then subsequently extract the return type of the `select`.\\n- otherwise we use the query return type (as we we\'ve previously examined)\\n\\nFinally, whichever type we end up with, we supply that type as a parameter to `UseQueryResult`. And that is what is going to surface up our types to our users.\\n\\n## Usage\\n\\nSo what does using our `useQueriesTyped` hook look like?\\n\\nWell, supplying `queryFn`s with different signatures looks like this:\\n\\n```ts\\nconst result = useQueriesTyped(\\n  { queryKey: \'key1\', queryFn: () => 1 },\\n  { queryKey: \'key2\', queryFn: () => \'two\' }\\n);\\n// const result: [QueryObserverResult<number, unknown>, QueryObserverResult<string, unknown>]\\n\\nif (result[0].data) {\\n  // number\\n}\\nif (result[1].data) {\\n  // string\\n}\\n```\\n\\nAs you can see, we\'re being returned a `Tuple` and the exact types are flowing through.\\n\\nNext let\'s look at a `.map` example with identical types in our supplied array:\\n\\n```ts\\nconst resultWithAllTheSameTypes = useQueriesTyped(\\n  ...[1, 2].map((x) => ({ queryKey: `${x}`, queryFn: () => x }))\\n);\\n// const resultWithAllTheSameTypes: QueryObserverResult<number, unknown>[]\\n\\nif (resultWithAllTheSameTypes[0].data) {\\n  // number\\n}\\n```\\n\\nThe return type of `number` is flowing through for each element.\\n\\nFinally let\'s look at how `.map` handles arrays with different types of elements:\\n\\n```ts\\nconst resultWithDifferentTypes = useQueriesTyped(\\n  ...[1, \'two\', new Date()].map((x) => ({ queryKey: `${x}`, queryFn: () => x }))\\n);\\n//const resultWithDifferentTypes: QueryObserverResult<string | number | Date, unknown>[]\\n\\nif (resultWithDifferentTypes[0].data) {\\n  // string | number | Date\\n}\\n\\nif (resultWithDifferentTypes[1].data) {\\n  // string | number | Date\\n}\\n\\nif (resultWithDifferentTypes[2].data) {\\n  // string | number | Date\\n}\\n```\\n\\nAdmittedly this last example is a somewhat unlikely scenario. But again we can see the types flowing through - though further narrowing would be required here to get to the exact type.\\n\\n## In the box?\\n\\nIt\'s great that we can wrap `useQueries` to get a strongly typed experience. It would be tremendous if this functionality was available by default. [There\'s a discussion going on around this](https://github.com/tannerlinsley/react-query/pull/1527). It\'s possible that this wrapper may no longer need to exist, and that would be amazing. In the meantime; enjoy!"},{"id":"/2021/01/02/create-react-app-with-ts-loader-and-craco","metadata":{"permalink":"/2021/01/02/create-react-app-with-ts-loader-and-craco","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2021-01-02-create-react-app-with-ts-loader-and-craco/index.md","source":"@site/blog/2021-01-02-create-react-app-with-ts-loader-and-craco/index.md","title":"Create React App with ts-loader and CRACO","description":"Create React App is a fantastic way to get up and running building a web app with React. It also supports using TypeScript with React. Simply entering the following:","date":"2021-01-02T00:00:00.000Z","formattedDate":"January 2, 2021","tags":[{"label":"CRACO","permalink":"/tags/craco"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"create react app","permalink":"/tags/create-react-app"},{"label":"fork-ts-checker-webpack-plugin","permalink":"/tags/fork-ts-checker-webpack-plugin"},{"label":"ts-loader","permalink":"/tags/ts-loader"}],"readingTime":3.405,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Create React App with ts-loader and CRACO","authors":"johnnyreilly","tags":["CRACO","TypeScript","create react app","fork-ts-checker-webpack-plugin","ts-loader"],"hide_table_of_contents":false},"prevItem":{"title":"react-query: strongly typing useQueries","permalink":"/2021/01/03/strongly-typing-react-query-s-usequeries"},"nextItem":{"title":"Azure Pipelines meet Jest","permalink":"/2020/12/30/azure-pipelines-meet-jest"}},"content":"[Create React App](https://create-react-app.dev/) is a fantastic way to get up and running building a web app with React. It also supports using TypeScript with React. Simply entering the following:\\n\\n```shell\\nnpx create-react-app my-app --template typescript\\n```\\n\\nWill give you a great TypeScript React project to get building with. There\'s two parts to the TypeScript support that exist:\\n\\n1. Transpilation AKA \\"turning our TypeScript into JavaScript\\". Back since [Babel 7 launched, Babel has enjoyed great support for transpiling TypeScript into JavaScript](https://devblogs.microsoft.com/typescript/typescript-and-babel-7/). Create React App leverages this; using the Babel webpack loader, [babel-loader](https://github.com/babel/babel-loader), for transpilation.\\n2. Type checking AKA \\"seeing if our code compiles\\". Create React App uses the [`fork-ts-checker-webpack-plugin`](https://github.com/TypeStrong/fork-ts-checker-webpack-plugin) to run the TypeScript type checker on a separate process and report any issues that may exist.\\n\\nThis is a great setup and works very well for the majority of use cases. However, what if we\'d like to tweak this setup? What if we\'d like to swap out `babel-loader` for `ts-loader` for compilation purposes? Can we do that?\\n\\nYes you can! And that\'s what we\'re going to do using a tool named [`CRACO`](https://github.com/gsoft-inc/craco) \\\\- the pithy shortening of \\"Create React App Configuration Override\\". This is a tool that allows us to:\\n\\n> Get all the benefits of create-react-app and customization without using \'eject\' by adding a single `craco.config.js` file at the root of your application and customize your eslint, babel, postcss configurations and many more.\\n\\n## ~~`babel-loader`~~ `ts-loader`\\n\\nSo let\'s do the swap. First of all we\'re going to need to add `CRACO` and `ts-loader` to our project:\\n\\n```shell\\nnpm install @craco/craco ts-loader --save-dev\\n```\\n\\nThen we\'ll swap over our various `scripts` in our `package.json` to use `CRACO`:\\n\\n```json\\n\\"start\\": \\"craco start\\",\\n\\"build\\": \\"craco build\\",\\n\\"test\\": \\"craco test\\",\\n```\\n\\nFinally we\'ll add a `craco.config.js` file to the root of our project. This is where we swap out `babel-loader` for `ts-loader`:\\n\\n```js\\nconst {\\n  addAfterLoader,\\n  removeLoaders,\\n  loaderByName,\\n  getLoaders,\\n  throwUnexpectedConfigError,\\n} = require(\'@craco/craco\');\\n\\nconst throwError = (message) =>\\n  throwUnexpectedConfigError({\\n    packageName: \'craco\',\\n    githubRepo: \'gsoft-inc/craco\',\\n    message,\\n    githubIssueQuery: \'webpack\',\\n  });\\n\\nmodule.exports = {\\n  webpack: {\\n    configure: (webpackConfig, { paths }) => {\\n      const { hasFoundAny, matches } = getLoaders(\\n        webpackConfig,\\n        loaderByName(\'babel-loader\')\\n      );\\n      if (!hasFoundAny) throwError(\'failed to find babel-loader\');\\n\\n      console.log(\'removing babel-loader\');\\n      const { hasRemovedAny, removedCount } = removeLoaders(\\n        webpackConfig,\\n        loaderByName(\'babel-loader\')\\n      );\\n      if (!hasRemovedAny) throwError(\'no babel-loader to remove\');\\n      if (removedCount !== 2)\\n        throwError(\'had expected to remove 2 babel loader instances\');\\n\\n      console.log(\'adding ts-loader\');\\n\\n      const tsLoader = {\\n        test: /\\\\.(js|mjs|jsx|ts|tsx)$/,\\n        include: paths.appSrc,\\n        loader: require.resolve(\'ts-loader\'),\\n        options: { transpileOnly: true },\\n      };\\n\\n      const { isAdded: tsLoaderIsAdded } = addAfterLoader(\\n        webpackConfig,\\n        loaderByName(\'url-loader\'),\\n        tsLoader\\n      );\\n      if (!tsLoaderIsAdded) throwError(\'failed to add ts-loader\');\\n      console.log(\'added ts-loader\');\\n\\n      console.log(\'adding non-application JS babel-loader back\');\\n      const { isAdded: babelLoaderIsAdded } = addAfterLoader(\\n        webpackConfig,\\n        loaderByName(\'ts-loader\'),\\n        matches[1].loader // babel-loader\\n      );\\n      if (!babelLoaderIsAdded)\\n        throwError(\'failed to add back babel-loader for non-application JS\');\\n      console.log(\'added non-application JS babel-loader back\');\\n\\n      return webpackConfig;\\n    },\\n  },\\n};\\n```\\n\\nSo what\'s happening here? The script looks for `babel-loader` usages in the default Create React App config. There will be two; one for TypeScript / JavaScript application code (we want to replace this) and one for non application JavaScript code. I\'m actually not too clear what non application JavaScript code there is or can be, but we\'ll leave it in place; it may be important.\\n\\nYou cannot remove a _single_ loader using `CRACO`, so instead we\'ll remove both and we\'ll add back the non application JavaScript `babel-loader`. We\'ll also add `ts-loader` with the `transpileOnly: true` option set (to ensure `ts-loader` doesn\'t do type checking).\\n\\nNow the next time we run `npm start` we\'ll have Create React App running using `ts-loader` and _without_ having ejected. If we want to adjust the options of `ts-loader` further then we\'re completely at liberty to do so, adjusting the `options` in our `craco.config.js`.\\n\\nIf you value debugging your original source code rather than the transpiled JavaScript, remember to set the `\\"sourceMap\\": true` property in your `tsconfig.json`.\\n\\nFinally, if we wanted to go even further, we could remove the `fork-ts-checker-webpack-plugin` and move `ts-loader` to use `transpileOnly: false` so it performs type checking also. However, generally it may be better to stay with the setup with post outlines for performance reasons."},{"id":"/2020/12/30/azure-pipelines-meet-jest","metadata":{"permalink":"/2020/12/30/azure-pipelines-meet-jest","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2020-12-30-azure-pipelines-meet-jest/index.md","source":"@site/blog/2020-12-30-azure-pipelines-meet-jest/index.md","title":"Azure Pipelines meet Jest","description":"This post explains how to integrate the tremendous test runner Jest with the continuous integration platform Azure Pipelines. Perhaps we\'re setting up a new project and we\'ve created a new React app with Create React App. This ships with Jest support out of the box. How do we get that plugged into Pipelines such that:","date":"2020-12-30T00:00:00.000Z","formattedDate":"December 30, 2020","tags":[{"label":"azure-pipelines","permalink":"/tags/azure-pipelines"},{"label":"jest","permalink":"/tags/jest"}],"readingTime":3.27,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Azure Pipelines meet Jest","authors":"johnnyreilly","image":"./test-results.png","tags":["azure-pipelines","jest"],"hide_table_of_contents":false},"prevItem":{"title":"Create React App with ts-loader and CRACO","permalink":"/2021/01/02/create-react-app-with-ts-loader-and-craco"},"nextItem":{"title":"dotnet-format: Prettier your C# with lint-staged & husky","permalink":"/2020/12/22/prettier-your-csharp-with-dotnet-format-and-lint-staged"}},"content":"This post explains how to integrate the tremendous test runner [Jest](https://jestjs.io/) with the continuous integration platform [Azure Pipelines](https://azure.microsoft.com/en-gb/services/devops/pipelines/?nav=min). Perhaps we\'re setting up a new project and we\'ve created a new React app with [Create React App](https://create-react-app.dev/). This ships with Jest support out of the box. How do we get that plugged into Pipelines such that:\\n\\n1. Tests run as part of our pipeline\\n2. A failing test fails the build\\n3. Test results are reported in Azure Pipelines UI?\\n\\n## Tests run as part of our pipeline\\n\\nFirst of all, lets get the tests running. Crack open your `azure-pipelines.yml` file and, in the appropriate place add the following:\\n\\n```yml\\n- task: Npm@1\\n  displayName: npm run test\\n  inputs:\\n    command: \'custom\'\\n    workingDir: \'src/client-app\'\\n    customCommand: \'run test\'\\n```\\n\\nThe above will, when run, trigger a `npm run test` in the `src/client-app` folder of my project (it\'s here where my React app lives). You\'d imagine this would just work\u2122\ufe0f - but life is not that simple. This is because Jest, by default, runs in watch mode. This is blocking and so not appropriate for CI.\\n\\nIn our `src/client-app/package.json` let\'s create a new script that runs the tests but _not_ in watch mode:\\n\\n```json\\n\\"test:ci\\": \\"npm run test -- --watchAll=false\\",\\n```\\n\\nand switch our `azure-pipelines.yml` to use it:\\n\\n```yml\\n- task: Npm@1\\n  displayName: npm run test\\n  inputs:\\n    command: \'custom\'\\n    workingDir: \'src/client-app\'\\n    customCommand: \'run test:ci\'\\n```\\n\\nBoom! We\'re now running tests as part of our pipeline. And also, failing tests will fail the build, because of Jest\'s default behaviour of exiting with status code 1 on failed tests.\\n\\n## Tests results are reported in Azure Pipelines UI\\n\\nPipelines has a really nice UI for reporting test results. If you\'re using something like .NET then you\'ll find that test results just magically show up there. We\'d like that for our Jest tests as well. And we can have it.\\n\\nThe way we achieve this is by:\\n\\n1. Producing test results in a format that can be subsequently processed\\n2. Using those test results to publish to Azure Pipelines\\n\\nThe way that you configure Jest test output is through usage of [`reporters`](https://jestjs.io/docs/en/cli#--reporters). However, Create React App doesn\'t support these. However that\'s not an issue, as the marvellous [Dan Abramov](https://twitter.com/dan_abramov) demonstrates [here](https://github.com/facebook/create-react-app/issues/2474#issuecomment-306340526).\\n\\nWe need to install the [`jest-junit`](https://github.com/jest-community/jest-junit) package to our `client-app`:\\n\\n```\\nnpm install jest-junit --save-dev\\n```\\n\\nAnd we\'ll tweak our `test:ci` script to use the `jest-junit` reporter as well:\\n\\n```json\\n\\"test:ci\\": \\"npm run test -- --watchAll=false --reporters=default --reporters=jest-junit\\",\\n```\\n\\nWe also need to add some configuration to our `package.json` in the form of a `jest-junit` element:\\n\\n```json\\n\\"jest-junit\\": {\\n        \\"suiteNameTemplate\\": \\"{filepath}\\",\\n        \\"outputDirectory\\": \\".\\",\\n        \\"outputName\\": \\"junit.xml\\"\\n    }\\n```\\n\\nThe above configuration will use the name of the test file as the suite name in the results, which should speed up the tracking down of the failing test. The other values specify where the test results should be published to, in this case the root of our `client-app` with the filename `junit.xml`.\\n\\nNow our CI is producing our test results, how do we get them into Pipelines? For that we need the [Publish test results task](https://docs.microsoft.com/en-us/azure/devops/pipelines/tasks/test/publish-test-results?view=azure-devops&tabs=trx%2Cyaml) and a new step in our `azure-pipelines.yml` _after_ our `npm run test` step:\\n\\n```yml\\n- task: Npm@1\\n  displayName: npm run test\\n  inputs:\\n    command: \'custom\'\\n    workingDir: \'src/client-app\'\\n    customCommand: \'run test:ci\'\\n\\n- task: PublishTestResults@2\\n  displayName: \'supply npm test results to pipelines\'\\n  condition: succeededOrFailed() # because otherwise we won\'t know what tests failed\\n  inputs:\\n    testResultsFiles: \'src/client-app/junit.xml\'\\n```\\n\\nThis will read the test results from our `src/client-app/junit.xml` file and pump them into Pipelines. Do note that we\'re _always_ running this step; so if the previous step failed (as it would in the case of a failing test) we still pump out the details of what that failure was. Like so:\\n\\n![screenshot of test results being published to Azure Pipelines regardless of passing or failing tests](test-and-publish-steps.png)\\n\\nAnd that\'s it! Azure Pipelines and Jest integrated.\\n\\n![screenshot of test results published to Azure Pipelines](test-results.png)"},{"id":"/2020/12/22/prettier-your-csharp-with-dotnet-format-and-lint-staged","metadata":{"permalink":"/2020/12/22/prettier-your-csharp-with-dotnet-format-and-lint-staged","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2020-12-22-prettier-your-csharp-with-dotnet-format-and-lint-staged/index.md","source":"@site/blog/2020-12-22-prettier-your-csharp-with-dotnet-format-and-lint-staged/index.md","title":"dotnet-format: Prettier your C# with lint-staged & husky","description":"Consistent formatting in a codebase is a good thing. We can achieve this in dotnet using dotnet format, used in combination with the npm packages husky and lint-staged. This post shows how.","date":"2020-12-22T00:00:00.000Z","formattedDate":"December 22, 2020","tags":[{"label":"Prettier","permalink":"/tags/prettier"},{"label":"dotnet-format","permalink":"/tags/dotnet-format"},{"label":"lint-staged","permalink":"/tags/lint-staged"},{"label":"husky","permalink":"/tags/husky"},{"label":"CSharpier","permalink":"/tags/c-sharpier"}],"readingTime":4.37,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"dotnet-format: Prettier your C# with lint-staged & husky","authors":"johnnyreilly","image":"./title-image.png","tags":["Prettier","dotnet-format","lint-staged","husky","CSharpier"],"hide_table_of_contents":false},"prevItem":{"title":"Azure Pipelines meet Jest","permalink":"/2020/12/30/azure-pipelines-meet-jest"},"nextItem":{"title":"Make Microsoft.Identity.Web respond with 403 forbidden instead of a 302 redirect","permalink":"/2020/12/21/how-to-make-azure-ad-403"}},"content":"Consistent formatting in a codebase is a good thing. We can achieve this in dotnet using `dotnet format`, used in combination with the npm packages `husky` and `lint-staged`. This post shows how.\\n\\n![title image reading \\"dotnet-format: Prettier your CSharp with lint-staged and husky\\" and the dotnet-format logo](title-image.png)\\n\\n## Update 17/09/2021\\n\\nThis has been updated to work with the latest versions of `lint-staged` and `husky`.\\n\\n## Update linting 07/04/2022\\n\\nIf you\'re interested in formatting, you might be interested in linting; formatting\'s big sister. C# has linting too; [read about it here](../2022-04-06-eslint-your-csharp-in-vs-code-with-roslyn-analyzers/index.md).\\n\\n## Why format?\\n\\nConsistent formatting makes code less confusing to newcomers and it allows whoever is working on the codebase to reliably focus on the task at hand. Not \\"fixing curly braces because Janice messed them up with her last commit\\". (A `git commit` message that would be tragic in so many ways.)\\n\\nOnce we\'ve agreed that we want to have consistent formatting, we want it to be enforced. Enter, stage left, [Prettier](https://prettier.io/), the fantastic tool for formatting code. It rocks; I\'ve been using on my JavaScript / TypeScript for the longest time. But what about C#? Well, there is a [Prettier plugin for C#](https://github.com/warrenseine/prettier-plugin-csharp).... Sort of. It appears to be abandoned and contains the worrying message in the `README/index.md`:\\n\\n> Please note that this plugin is under active development, and might not be ready to run on production code yet. It will break your code.\\n\\nNot a ringing endorsement.\\n\\n## `dotnet-format`: a new hope\\n\\n[Margarida Pereira](https://twitter.com/margaridagp) recently pointed me in the direction of [`dotnet-format`](https://github.com/dotnet/format) which is a formatter for .NET. It\'s a .NET tool which:\\n\\n> is a code formatter for dotnet that applies style preferences to a project or solution. Preferences will be read from an `.editorconfig` file, if present, otherwise a default set of preferences will be used.\\n\\nIt can be installed with:\\n\\n```shell\\ndotnet tool install -g dotnet-format\\n```\\n\\nThe [VS Code C# extension will make use of this formatter](https://github.com/dotnet/format/issues/648#issuecomment-614905524), we just need to set the following in our `settings.json`:\\n\\n```json\\n\\"omnisharp.enableRoslynAnalyzers\\": true,\\n\\"omnisharp.enableEditorConfigSupport\\": true\\n```\\n\\n## Customising our formatting\\n\\nIf we\'d like to deviate from the [default formatting options](https://docs.microsoft.com/en-us/dotnet/fundamentals/code-analysis/code-style-rule-options) then create ourselves an `.editorconfig` file in the root of our project. Let\'s say we prefer more of the [K & R style](https://en.wikipedia.org/wiki/Indentation_style#K&R_style) approach to braces instead of the C# default of [Allman style](https://en.wikipedia.org/wiki/Indentation_style#Allman_style). To make `dotnet-format` use that we\'d set the following:\\n\\n```ini\\n# Remove the line below if you want to inherit .editorconfig settings from higher directories\\nroot = true\\n\\n# See https://github.com/dotnet/format/blob/master/docs/Supported-.editorconfig-options/index.md for reference\\n[*.cs]\\ncsharp_new_line_before_open_brace = none\\ncsharp_new_line_before_catch = false\\ncsharp_new_line_before_else = false\\ncsharp_new_line_before_finally = false\\ncsharp_new_line_before_members_in_anonymous_types = false\\ncsharp_new_line_before_members_in_object_initializers = false\\ncsharp_new_line_between_query_expression_clauses = true\\n```\\n\\nWith this in place it\'s K & R all the way baby!\\n\\n## `lint-staged` / `husky` integration\\n\\nIt\'s become somewhat standard to use the marvellous [`husky`](https://github.com/typicode/husky) and [`lint-staged`](https://github.com/okonet/lint-staged) to enforce code quality. To quote the docs:\\n\\n> Run linters against staged git files and don\'t let \ud83d\udca9 slip into our code base!\\n\\nTo add this to our (otherwise C# codebase), we\'re going to need a `package.json` file:\\n\\n```sh\\nnpm init --yes\\n```\\n\\nWe\'ll install `husky` and `lint-staged`:\\n\\n```sh\\nnpx husky-init && npm install\\nnpm install lint-staged --save-dev\\n```\\n\\nWe should have a new file living at `.husky/pre-commit` which is our pre-commit hook.\\n\\nWithin that file we should replace `npm test` with `npx lint-staged --relative`. This is the command that will be run on commit. `lint-staged` will be run and we\'re specifying `relative` so that **relative** file paths will be used. This is important as `dotnet format`\'s `--include` accepts \\"a list of relative file or folder paths to include in formatting\\". **Absolute paths (the default) won\'t work - and if we pass them to `dotnet format`, it will not format the files.**\\n\\nFinally we add the following entry to the `package.json`:\\n\\n```json\\n  \\"lint-staged\\": {\\n    \\"*.cs\\": \\"dotnet format --include\\"\\n  }\\n```\\n\\nThis is the task that will be invoked by `lint-staged` against files with a `.cs` suffix on commit. When `lint-staged` runs, it will pass a list of relative file paths to `dotnet format`. So if we\'d staged two files it might end up executing a command like this:\\n\\n`dotnet format --include src/server-app/Server/Controllers/UserController.cs src/server-app/Server/Controllers/WeatherForecastController.cs`\\n\\nWe should end up with a `package.json` that looks something like this:\\n\\n```json\\n{\\n  \\"name\\": \\"app\\",\\n  \\"version\\": \\"1.0.0\\",\\n  \\"description\\": \\"[![Shared Build Status](https://dev.azure.com/investec/maas/_apis/build/status/shared?repoName=maas)](https://dev.azure.com/investec/maas/_build/latest?definitionId=1128&repoName=maas)\\",\\n  \\"main\\": \\"index.js\\",\\n  \\"dependencies\\": {\\n    \\"husky\\": \\"^7.0.2\\"\\n  },\\n  \\"devDependencies\\": {\\n    \\"husky\\": \\"^7.0.0\\",\\n    \\"lint-staged\\": \\"^11.1.2\\"\\n  },\\n  \\"scripts\\": {\\n    \\"test\\": \\"echo \\\\\\"Error: no test specified\\\\\\" && exit 1\\",\\n    \\"prepare\\": \\"husky install\\"\\n  },\\n  \\"lint-staged\\": {\\n    \\"*.cs\\": \\"dotnet format --include\\"\\n  },\\n  \\"repository\\": {\\n    \\"type\\": \\"git\\",\\n    \\"url\\": \\"https://investec@dev.azure.com/investec/maas/_git/maas\\"\\n  },\\n  \\"keywords\\": [],\\n  \\"author\\": \\"\\",\\n  \\"license\\": \\"ISC\\"\\n}\\n```\\n\\nBy and large we don\'t have to think about this; the important take home is that we\'re now enforcing standardised formatting for all C# files upon commit. Everything that goes into the codebase will be formatted in a consistent fashion.\\n\\n## CSharpier - update 16/05/2021\\n\\nThere is an alternative to the CSharp Prettier project. It\'s being worked on by\\n[Bela VanderVoort](https://github.com/belav) and it goes by the name of [csharpier](https://github.com/belav/csharpier). When comparing CSharpier and dotnet-format, Bela put it like this:\\n\\n> I could see CSharpier being the non-configurable super opinionated formatter and dotnet-format being for the people that do want to have options.\\n\\nCheck it out!"},{"id":"/2020/12/21/how-to-make-azure-ad-403","metadata":{"permalink":"/2020/12/21/how-to-make-azure-ad-403","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2020-12-21-how-to-make-azure-ad-403/index.md","source":"@site/blog/2020-12-21-how-to-make-azure-ad-403/index.md","title":"Make Microsoft.Identity.Web respond with 403 forbidden instead of a 302 redirect","description":"By default Microsoft.Identity.Web responds to unauthorized requests with a 302 (redirect). Do you want a 403 (forbidden) instead? Here\'s how.","date":"2020-12-21T00:00:00.000Z","formattedDate":"December 21, 2020","tags":[{"label":"Microsoft.Identity.Web","permalink":"/tags/microsoft-identity-web"},{"label":"OnRedirectToAccessDenied","permalink":"/tags/on-redirect-to-access-denied"},{"label":"Azure AD","permalink":"/tags/azure-ad"},{"label":"Azure Active Directory","permalink":"/tags/azure-active-directory"},{"label":"redirect","permalink":"/tags/redirect"},{"label":"ASP.NET","permalink":"/tags/asp-net"}],"readingTime":2.715,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Make Microsoft.Identity.Web respond with 403 forbidden instead of a 302 redirect","authors":"johnnyreilly","tags":["Microsoft.Identity.Web","OnRedirectToAccessDenied","Azure AD","Azure Active Directory","redirect","ASP.NET"],"image":"./Forbidden.png","hide_table_of_contents":false},"prevItem":{"title":"dotnet-format: Prettier your C# with lint-staged & husky","permalink":"/2020/12/22/prettier-your-csharp-with-dotnet-format-and-lint-staged"},"nextItem":{"title":"Nullable reference types; CSharp\'s very own strictNullChecks","permalink":"/2020/12/20/nullable-reference-types-csharp-strictnullchecks"}},"content":"By default `Microsoft.Identity.Web` responds to unauthorized requests with a 302 (redirect). Do you want a 403 (forbidden) instead? Here\'s how.\\n\\nIf you\'re using the tremendous [Azure Active Directory for authentication with ASP.NET](https://docs.microsoft.com/en-us/azure/active-directory/develop/scenario-web-app-sign-user-app-configuration?tabs=aspnetcore) then there\'s a good chance you\'re using the [`Microsoft.Identity.Web`](https://github.com/AzureAD/microsoft-identity-web) library. It\'s this that allows us to drop the following statement into the `ConfigureServices` method of our `Startup` class:\\n\\n```cs\\nservices.AddMicrosoftIdentityWebAppAuthentication(Configuration);\\n```\\n\\nWhich (combined with configuration in our `appsettings.json` files) hooks us up with Azure AD for authentication. This is 95% awesome. The 5% is what we\'re here for. Here\'s a screenshot of the scenario that troubles us:\\n\\n![a screenshot of Chrome Devtools showing a 302](AccessDenied.png)\\n\\nWe\'ve made a request to `/WeatherForecast`; a secured endpoint (a controller decorated with the `Authorize` attribute). We\'re authenticated; the app knows who we are. But we\'re not authorized / allowed to access this endpoint. We don\'t have permission. The HTTP specification caters directly for this scenario with [status code `403 Forbidden`](https://tools.ietf.org/html/rfc7231#section-6.5.3):\\n\\n> The 403 (Forbidden) status code indicates that the server understood the request but refuses to authorize it.\\n\\nHowever, `Microsoft.Identity.Web` is ploughing another furrow. Instead of returning `403`, it\'s returning `302 Found` and redirecting the browser to `https://localhost:5001/Account/AccessDenied?ReturnUrl=%2FWeatherForecast`. Now the intentions here are _great_. If you wanted to implement a page in your application at that endpoint that displayed some kind of useful message it would be really useful. However, what if you want the more HTTP-y behaviour instead? In the case of a HTTP request triggered by JavaScript (typical for Single Page Applications) then this redirect isn\'t that helpful. JavaScript doesn\'t really know what to do with the `302` and whilst you could code around this, it\'s not desirable.\\n\\nWe want `403` - we don\'t want `302`.\\n\\n## Give us `403`\\n\\nYou can have this behaviour by dropping the following code after your `services.AddMicrosoftIdentityWebAppAuthentication`:\\n\\n```cs\\nservices.Configure<CookieAuthenticationOptions>(CookieAuthenticationDefaults.AuthenticationScheme, options =>\\n{\\n    options.Events.OnRedirectToAccessDenied = new Func<RedirectContext<CookieAuthenticationOptions>, Task>(context =>\\n    {\\n        context.Response.StatusCode = StatusCodes.Status403Forbidden;\\n        return context.Response.CompleteAsync();\\n    });\\n});\\n```\\n\\nThis code hijacks the redirect to AccessDenied and transforms it into a `403` instead. Tremendous! What does this look like?\\n\\n![a screenshot of Chrome Devtools showing a 403](Forbidden.png)\\n\\nThis is the behaviour we want!\\n\\n## Extra customisation bonus points\\n\\nYou may want to have some nuance to the way you handle unauthorized requests. Because of the nature of `OnRedirectToAccessDenied` this is entirely possible; you have complete access to the requests coming in which you can use to direct behaviour. To take a single example, let\'s say we want to direct normal browsing behaviour (AKA humans clicking about in Chrome) which is not authorized to a given screen, otherwise provide `403`s. What would that look like?\\n\\n```cs\\nservices.Configure<CookieAuthenticationOptions>(CookieAuthenticationDefaults.AuthenticationScheme, options =>\\n{\\n    options.Events.OnRedirectToAccessDenied = new Func<RedirectContext<CookieAuthenticationOptions>, Task>(context =>\\n    {\\n        var isRequestForHtml = context.Request.Headers[\\"Accept\\"].ToString().Contains(\\"text/html\\");\\n        if (isRequestForHtml) {\\n            context.Response.StatusCode = StatusCodes.Status302Found;\\n            context.Response.Headers[\\"Location\\"] = \\"/unauthorized\\";\\n        }\\n        else {\\n            context.Response.StatusCode = StatusCodes.Status403Forbidden;\\n        }\\n\\n        return context.Response.CompleteAsync();\\n    });\\n});\\n```\\n\\nSo above, we check the request `Accept` headers and see if they contain `\\"text/html\\"`; which we\'re using as a signal that the request came from a users browsing. (This may not be bulletproof; better suggestions gratefully received.) If the request does contain a ` \\"text/html\\"``Accept ` header then we redirect the client to an `/unauthorized` screen, otherwise we return `403` as we did before. Super flexible and powerful!"},{"id":"/2020/12/20/nullable-reference-types-csharp-strictnullchecks","metadata":{"permalink":"/2020/12/20/nullable-reference-types-csharp-strictnullchecks","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2020-12-20-nullable-reference-types-csharp-strictnullchecks/index.md","source":"@site/blog/2020-12-20-nullable-reference-types-csharp-strictnullchecks/index.md","title":"Nullable reference types; CSharp\'s very own strictNullChecks","description":"\'Tis the season to play with new compiler settings! I\'m a very keen TypeScript user and have been merrily using strictNullChecks since it shipped. I was dimly aware that C# was also getting a similar feature by the name of nullable reference types.","date":"2020-12-20T00:00:00.000Z","formattedDate":"December 20, 2020","tags":[{"label":"CSharp","permalink":"/tags/c-sharp"},{"label":"Nullable reference types","permalink":"/tags/nullable-reference-types"}],"readingTime":3.85,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Nullable reference types; CSharp\'s very own strictNullChecks","authors":"johnnyreilly","tags":["CSharp","Nullable reference types"],"hide_table_of_contents":false},"prevItem":{"title":"Make Microsoft.Identity.Web respond with 403 forbidden instead of a 302 redirect","permalink":"/2020/12/21/how-to-make-azure-ad-403"},"nextItem":{"title":"azure-pipelines-task-lib and isOutput setVariable","permalink":"/2020/12/09/azure-pipelines-task-lib-and-isoutput-setvariable"}},"content":"\'Tis the season to play with new compiler settings! I\'m a very keen TypeScript user and have been merrily using [`strictNullChecks`](https://www.typescriptlang.org/docs/handbook/release-notes/typescript-2-0.html#--strictnullchecks) since it shipped. I was dimly aware that C# was also getting a similar feature by the name of [nullable reference types](https://docs.microsoft.com/en-us/dotnet/csharp/tutorials/nullable-reference-types).\\n\\nIt\'s only now that I\'ve got round to taking at look at this marvellous feature. I thought I\'d share what moving to nullable reference types looked like for me; and what code changes I found myself making as a consequence.\\n\\n## Turning on nullable reference types\\n\\nTo turn on nullable reference types in a C# project you should pop open the `.csproj` file and ensure it contains a `<Nullable>enable</Nullable>`. So if you had a .NET Core 3.1 codebase it might look like this:\\n\\n```xml\\n<PropertyGroup>\\n    <TargetFramework>netcoreapp3.1</TargetFramework>\\n    <Nullable>enable</Nullable>\\n</PropertyGroup>\\n```\\n\\nWhen you compile from this point forward, possible null reference types are reported as warnings. Consider this C#:\\n\\n```cs\\n[ApiController]\\npublic class UserController : ControllerBase\\n{\\n    private readonly ILogger<UserController> _logger;\\n\\n    public UserController(ILogger<UserController> logger)\\n    {\\n        _logger = logger;\\n    }\\n\\n    [AllowAnonymous]\\n    [HttpGet(\\"UserName\\")]\\n    public string GetUserName()\\n    {\\n        if (User.Identity.IsAuthenticated) {\\n            _logger.LogInformation(\\"{User} is getting their username\\", User.Identity.Name);\\n            return User.Identity.Name;\\n        }\\n\\n        _logger.LogInformation(\\"The user is not authenticated\\");\\n        return null;\\n    }\\n}\\n```\\n\\nA `dotnet build` results in this:\\n\\n```shell\\ndotnet build --configuration release\\n\\nMicrosoft (R) Build Engine version 16.7.1+52cd83677 for .NET\\nCopyright (C) Microsoft Corporation. All rights reserved.\\n\\n  Determining projects to restore...\\n  Restored /Users/jreilly/code/app/src/server-app/Server/app.csproj (in 471 ms).\\nControllers/UserController.cs(38,24): warning CS8603: Possible null reference return. [/Users/jreilly/code/app/src/server-app/Server/app.csproj]\\nControllers/UserController.cs(42,20): warning CS8603: Possible null reference return. [/Users/jreilly/code/app/src/server-app/Server/app.csproj]\\n  app -> /Users/jreilly/code/app/src/server-app/Server/bin/release/netcoreapp3.1/app.dll\\n  app -> /Users/jreilly/code/app/src/server-app/Server/bin/release/netcoreapp3.1/app.Views.dll\\n\\nBuild succeeded.\\n\\nControllers/UserController.cs(38,24): warning CS8603: Possible null reference return. [/Users/jreilly/code/app/src/server-app/Server/app.csproj]\\nControllers/UserController.cs(42,20): warning CS8603: Possible null reference return. [/Users/jreilly/code/app/src/server-app/Server/app.csproj]\\n    2 Warning(s)\\n    0 Error(s)\\n```\\n\\nYou see the two `\\"Possible null reference return.\\"` warnings? Bingo\\n\\n## Really make it hurt\\n\\nThis is good - information is being surfaced up. But it\'s a warning. I could ignore it. I like compilers to get really up in my face and force me to make a change. I\'m not into warnings; I\'m into errors. Know what works for you. If you\'re similarly minded, you can upgrade nullable reference warnings to errors by tweaking the `.csproj` a touch further. Add yourself a `<WarningsAsErrors>nullable</WarningsAsErrors>` element. So maybe your `.csproj` now looks like this:\\n\\n```xml\\n<PropertyGroup>\\n    <TargetFramework>netcoreapp3.1</TargetFramework>\\n    <Nullable>enable</Nullable>\\n    <WarningsAsErrors>nullable</WarningsAsErrors>\\n</PropertyGroup>\\n```\\n\\nAnd a `dotnet build` will result in this:\\n\\n```shell\\ndotnet build --configuration release\\n\\nMicrosoft (R) Build Engine version 16.7.1+52cd83677 for .NET\\nCopyright (C) Microsoft Corporation. All rights reserved.\\n\\n  Determining projects to restore...\\n  Restored /Users/jreilly/code/app/src/server-app/Server/app.csproj (in 405 ms).\\nControllers/UserController.cs(38,24): error CS8603: Possible null reference return. [/Users/jreilly/code/app/src/server-app/Server/app.csproj]\\nControllers/UserController.cs(42,20): error CS8603: Possible null reference return. [/Users/jreilly/code/app/src/server-app/Server/app.csproj]\\n\\nBuild FAILED.\\n\\nControllers/UserController.cs(38,24): error CS8603: Possible null reference return. [/Users/jreilly/code/app/src/server-app/Server/app.csproj]\\nControllers/UserController.cs(42,20): error CS8603: Possible null reference return. [/Users/jreilly/code/app/src/server-app/Server/app.csproj]\\n    0 Warning(s)\\n    2 Error(s)\\n```\\n\\nYay! Errors!\\n\\n## What do they mean?\\n\\n\\"`Possible null reference return`\\" isn\'t the clearest of errors. What does that actually amount to? Well, it amounts to the compiler saying \\"you\'re a liar! (maybe)\\". Let\'s look again at the code where this error is reported:\\n\\n```cs\\n[AllowAnonymous]\\n[HttpGet(\\"UserName\\")]\\npublic string GetUserName()\\n{\\n    if (User.Identity.IsAuthenticated) {\\n        _logger.LogInformation(\\"{User} is getting their username\\", User.Identity.Name);\\n        return User.Identity.Name;\\n    }\\n\\n    _logger.LogInformation(\\"The user is not authenticated\\");\\n    return null;\\n}\\n```\\n\\nWe\'re getting that error reported where we\'re returning `null` and where we\'re returning `User.Identity.Name` which _may_ be `null`. And we\'re getting that because as far as the compiler is concerned `string` has changed. Before we turned on nullable reference types the compiler considered `string` to mean `string` _OR_`null`. Now, `string` means `string`.\\n\\nThis is the same sort of behaviour as TypeScripts `strictNullChecks`. With TypeScript, before you turn on `strictNullChecks`, as far as the compiler is concerned, `string` means `string`_OR_`null`_OR_`undefined` (JavaScript didn\'t feel one null-ish value was enough and so has two - don\'t ask). Once `strictNullChecks` is on, `string` means `string`.\\n\\nIt\'s a lot clearer. And that\'s why the compiler is getting antsy. The method signature is `string`, but it can see `null` potentially being returned. It doesn\'t like it. By and large that\'s good. We want the compiler to notice this as that\'s the entire point. We want to catch accidental `null`s before they hit a user. This is _great_! However, what do you do if have a method (as we do) that legitimately returns a `string` or `null`?\\n\\n## Widening the type to include `null`\\n\\nWe change the signature from this:\\n\\n```cs\\npublic string GetUserName()\\n```\\n\\nTo this:\\n\\n```cs\\npublic string? GetUserName()\\n```\\n\\nThat\'s right, the simple addition of `?` marks a reference type (like a string) as potentially being `null`. Adding that means that we\'re potentially returning `null`, but we\'re sure about it; there\'s intention here - it\'s not accidental. Wonderful!"},{"id":"/2020/12/09/azure-pipelines-task-lib-and-isoutput-setvariable","metadata":{"permalink":"/2020/12/09/azure-pipelines-task-lib-and-isoutput-setvariable","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2020-12-09-azure-pipelines-task-lib-and-isoutput-setvariable/index.md","source":"@site/blog/2020-12-09-azure-pipelines-task-lib-and-isoutput-setvariable/index.md","title":"azure-pipelines-task-lib and isOutput setVariable","description":"Some blog posts are insightful treatises on the future of web development, some are \\"here\'s how I solved my problem\\". This is most assuredly the latter.","date":"2020-12-09T00:00:00.000Z","formattedDate":"December 9, 2020","tags":[{"label":"azure-pipelines-task-lib","permalink":"/tags/azure-pipelines-task-lib"},{"label":"Azure Pipelines","permalink":"/tags/azure-pipelines"},{"label":"custom task","permalink":"/tags/custom-task"}],"readingTime":1.615,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"azure-pipelines-task-lib and isOutput setVariable","authors":"johnnyreilly","tags":["azure-pipelines-task-lib","Azure Pipelines","custom task"],"hide_table_of_contents":false},"prevItem":{"title":"Nullable reference types; CSharp\'s very own strictNullChecks","permalink":"/2020/12/20/nullable-reference-types-csharp-strictnullchecks"},"nextItem":{"title":"Visual Studio Marketplace: images in Markdown!","permalink":"/2020/11/28/images-in-markdown-for-azure-devops-marketplace"}},"content":"Some blog posts are insightful treatises on the future of web development, some are \\"here\'s how I solved my problem\\". This is most assuredly the latter.\\n\\nI\'m writing an [custom pipelines task extension for Azure Pipelines](https://docs.microsoft.com/en-us/azure/devops/extend/develop/add-build-task?view=azure-devops). It\'s written with TypeScript and the [azure-pipelines-task-lib](https://github.com/microsoft/azure-pipelines-task-lib).\\n\\nThe pipeline needs to output a variable. Azure Pipelines does that using the `setvariable` command combined with [isOutput=true](https://docs.microsoft.com/en-us/azure/devops/pipelines/process/variables?view=azure-devops&tabs=yaml%2Cbatch#set-a-multi-job-output-variable). This looks something like this: `##vso[task.setvariable variable=myOutputVar;isOutput=true]this is the value\\"`.\\n\\nThe bad news is that the lib [doesn\'t presently support `isOutput=true`](https://github.com/microsoft/azure-pipelines-task-lib/issues/688). Gosh it makes me sad. Hopefully in future it will be resolved. But what now?\\n\\nFor now we can hack ourselves a workaround:\\n\\n```ts\\nimport * as tl from \'azure-pipelines-task-lib/task\';\\nimport * as tcm from \'azure-pipelines-task-lib/taskcommand\';\\nimport * as os from \'os\';\\n\\n/**\\n * Sets a variable which will be output as well.\\n *\\n * @param     name    name of the variable to set\\n * @param     val     value to set\\n * @param     secret  whether variable is secret.  Multi-line secrets are not allowed.  Optional, defaults to false\\n * @returns   void\\n */\\nexport function setOutputVariable(\\n  name: string,\\n  val: string,\\n  secret = false\\n): void {\\n  // use the implementation of setVariable to set all the internals,\\n  // then subsequently set the output variable manually\\n  tl.setVariable(name, val, secret);\\n\\n  const varValue = val || \'\';\\n\\n  // write the command\\n  // see https://docs.microsoft.com/en-us/azure/devops/pipelines/process/variables?view=azure-devops&tabs=yaml%2Cbatch#set-a-multi-job-output-variable\\n  _command(\\n    \'task.setvariable\',\\n    {\\n      variable: name || \'\',\\n      isOutput: \'true\',\\n      issecret: (secret || false).toString(),\\n    },\\n    varValue\\n  );\\n}\\n\\nconst _outStream = process.stdout;\\n\\nfunction _writeLine(str: string): void {\\n  _outStream.write(str + os.EOL);\\n}\\n\\nfunction _command(command: string, properties: any, message: string) {\\n  const taskCmd = new tcm.TaskCommand(command, properties, message);\\n  _writeLine(taskCmd.toString());\\n}\\n```\\n\\nThe above is effectively a wrapper for the existing [`setVariable`](https://github.com/microsoft/azure-pipelines-task-lib/blob/90e9cde0e509cba77185a80ef3af2fc898fb026c/node/task.ts#L162). However, once it\'s called into the initial implementation, `setOutputVariable` then writes out the same variable once more, but this time bolting on `isOutput=true`.\\n\\nFinally, I\'ve raised a PR to see if `isOutput` can be added directly to the library. [You can track progress on that here.](https://github.com/microsoft/azure-pipelines-task-lib/pull/691)"},{"id":"/2020/11/28/images-in-markdown-for-azure-devops-marketplace","metadata":{"permalink":"/2020/11/28/images-in-markdown-for-azure-devops-marketplace","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2020-11-28-images-in-markdown-for-azure-devops-marketplace/index.md","source":"@site/blog/2020-11-28-images-in-markdown-for-azure-devops-marketplace/index.md","title":"Visual Studio Marketplace: images in Markdown!","description":"Publish your README/index.md and associated images to Visual Studio Marketplace.","date":"2020-11-28T00:00:00.000Z","formattedDate":"November 28, 2020","tags":[{"label":"Azure DevOps Marketplace","permalink":"/tags/azure-dev-ops-marketplace"},{"label":"Visual Studio Marketplace","permalink":"/tags/visual-studio-marketplace"},{"label":"markdown","permalink":"/tags/markdown"},{"label":"images","permalink":"/tags/images"}],"readingTime":2.405,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Visual Studio Marketplace: images in Markdown!","authors":"johnnyreilly","tags":["Azure DevOps Marketplace","Visual Studio Marketplace","markdown","images"],"image":"./azure-devops-marketplace.png","description":"Publish your README/index.md and associated images to Visual Studio Marketplace.","hide_table_of_contents":false},"prevItem":{"title":"azure-pipelines-task-lib and isOutput setVariable","permalink":"/2020/12/09/azure-pipelines-task-lib-and-isoutput-setvariable"},"nextItem":{"title":"Bulletproof uniq with TypeScript generics (yay code reviews!)","permalink":"/2020/11/14/bulletproof-uniq-with-typescript"}},"content":"I\'ve recently found myself developing [custom pipelines task extensions for Azure DevOps](https://docs.microsoft.com/en-us/azure/devops/extend/develop/add-build-task?view=azure-devops). The extensions being developed end up in the [Azure DevOps Marketplace](https://marketplace.visualstudio.com/azuredevops). What you see there when you look at existing extensions is some pretty lovely documentation.\\n\\n![screenshot of a rich Markdown powered screen with images in Visual Studio Marketplace](azure-devops-marketplace.png)\\n\\n## How can our tasks look as lovely?\\n\\nThat, my friends, is the question to answer. Good documentation is key to success. Here\'s the ask: when a custom task is installed it becomes available in the marketplace, we want it to:\\n\\n- contain documentation\\n- that documentation should support images... For a picture, famously, speaks a thousand words\\n\\n## Mark(Down) our manifest\\n\\nTo get documentation showing up in the marketplace, we need to take a look at the `vss-extension.json` file which lies at the root of our extension folder. It\'s a kind of manifest file and is documented [here](https://docs.microsoft.com/en-us/azure/devops/extend/develop/manifest?view=azure-devops).\\n\\n[Tucked away in the docs, you\'ll find mention of a `content` property and the words:](https://docs.microsoft.com/en-us/azure/devops/extend/develop/manifest?view=azure-devops#discovery-attributes)\\n\\n> Dictionary of content files that describe your extension to users... Each file is assumed to be in [GitHub Flavored Markdown format](https://help.github.com/articles/github-flavored-markdown/). The path of each item is the path to the markdown file in the extension. Valid keys: `details`.\\n\\nThis means we can have a Markdown file in our repo which documents our task. To stay consistent with most projects, a solid choice is to use the `README/index.md` that sits in the root of the project to this end.\\n\\nSo the simple addition of this:\\n\\n```json\\n{\\n  //...\\n  \\"content\\": {\\n    \\"details\\": {\\n      \\"path\\": \\"README/index.md\\"\\n    }\\n  }\\n  //...\\n}\\n```\\n\\nGives us documentation in the marketplace. Yay!\\n\\n## Now the images...\\n\\nIf we are referencing images in our `README/index.md` then, as it stands right now, they won\'t show up in the marketplace. It\'ll be broken link city. Imagine some Markdown like this:\\n\\n```md\\n![alt text](images/screenshot.png)\\n```\\n\\nThis is entirely correct and supported, but won\'t work by default. This is because these images need to be specified in the [`files` property](https://docs.microsoft.com/en-us/azure/devops/extend/develop/manifest?view=azure-devops#files) of the `vss-extension.json`.\\n\\n```json\\n{\\n  //...\\n  \\"content\\": {\\n    \\"details\\": {\\n      \\"path\\": \\"README/index.md\\"\\n    }\\n  },\\n  \\"files\\": [\\n    {\\n      \\"path\\": \\"images\\",\\n      \\"addressable\\": true\\n    }\\n  ]\\n  //...\\n}\\n```\\n\\nConsider the above; the `path` of `images` includes everything inside the `images` folder in the task. However, it\'s crucial that the [`\\"addressable\\": true`](https://docs.microsoft.com/en-us/azure/devops/extend/develop/manifest?view=azure-devops#properties-1) is present as well. It\'s this that makes the files in this `path` URL-addressable. And without that, the images won\'t be displayed.\\n\\nThat\'s it! We\'re done! We can have rich, image inclusive, documentation in our custom tasks.\\n\\nA final note: it\'s possible to specify individual files rather than whole paths in the `files` directory and you might want to do that if you\'re being very careful around file size. There is a maximum size for a custom task and it\'s easy to breach it. But by and large I find that \\"allowlisting\\" a single directory is easier."},{"id":"/2020/11/14/bulletproof-uniq-with-typescript","metadata":{"permalink":"/2020/11/14/bulletproof-uniq-with-typescript","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2020-11-14-bulletproof-uniq-with-typescript/index.md","source":"@site/blog/2020-11-14-bulletproof-uniq-with-typescript/index.md","title":"Bulletproof uniq with TypeScript generics (yay code reviews!)","description":"Never neglect the possibilities of a code review. There are times when you raise a PR and all you want is for everyone to hit approve so you can merge, merge and ship, ship! This can be a missed opportunity. For as much as I\'d like to imagine my code is perfect, it\'s patently not. There\'s always scope for improvement.","date":"2020-11-14T00:00:00.000Z","formattedDate":"November 14, 2020","tags":[{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"unique","permalink":"/tags/unique"}],"readingTime":3.855,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Bulletproof uniq with TypeScript generics (yay code reviews!)","authors":"johnnyreilly","tags":["TypeScript","unique"],"hide_table_of_contents":false},"prevItem":{"title":"Visual Studio Marketplace: images in Markdown!","permalink":"/2020/11/28/images-in-markdown-for-azure-devops-marketplace"},"nextItem":{"title":"Throttling data requests with React Hooks","permalink":"/2020/11/10/throttle-data-requests-with-react-hooks"}},"content":"Never neglect the possibilities of a code review. There are times when you raise a PR and all you want is for everyone to hit approve so you can merge, merge and ship, ship! This can be a missed opportunity. For as much as I\'d like to imagine my code is perfect, it\'s patently not. There\'s always scope for improvement.\\n\\n## \\"What\'s this?\\"\\n\\nThis week afforded me that opportunity. I was walking through a somewhat complicated PR on a call and someone said \\"what\'s this?\\". They\'d spotted an expression much like this in my code:\\n\\n```ts\\nconst myValues = [...new Set(allTheValuesSupplied)];\\n```\\n\\nWhat is that? Well, it\'s a number of things:\\n\\n1. [It\'s a way to get the unique values in a collection.](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Set#Remove_duplicate_elements_from_the_array)\\n2. It\'s a pro-tip and a coding BMX trick.\\n\\nWhat do I mean? Well, this is indeed a technique for getting the unique values in a collection. But it relies upon you knowing a bunch of things:\\n\\n- [`Set`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Set) contains unique values. If you add multiple identical values, only a single value will be stored.\\n- The [`Set` constructor](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Set/Set) takes [iterable objects](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Iteration_protocols#The_iterable_protocol). This means we can `new` up a `Set` with an array that we want to \\"unique-ify\\" and we will have a `Set` that contains those unique values.\\n- If you want to go on to do filtering / mapping etc on your unique values, you\'ll need to get them out of the `Set`. This is because (regrettably) ECMAScript iterables don\'t implicitly support these operations and neither are methods such as these part of the `Set` API. The easiest way to do that is to [spread](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Spread_syntax) into a new array which you can then operate upon.\\n\\nI have this knowledge. Lots of people have this knowledge. But whilst this may be the case, using this technique goes against what I would generally consider to be a good tenet of programming: comprehensibility. When you read this code above, it doesn\'t immediately tell you what it\'s doing. This is a strike against it.\\n\\nFurther to that, it\'s \\"noisy\\". Even if the reader does have this knowledge, as they digest the code, they have to mentally unravel it. \\"Oh it\'s a `Set`, we\'re passing in values, then spreading it out, it\'s probably intended to get the unique values.... Right, cool, cool.... Continue!\\"\\n\\n <iframe src=\\"https://giphy.com/embed/4NnSe87mg3h25JYIDh\\" width=\\"100%\\" height=\\"100%\\" frameBorder=\\"0\\" allowFullScreen=\\"\\"></iframe>\\n\\n[Margarida Pereira](https://twitter.com/margaridagp) explicitly called this out and I found myself agreeing. Let\'s make a `uniq` function!\\n\\n## `uniq` v1\\n\\nI wrote a very simple `uniq` function which looked like this:\\n\\n```ts\\n/**\\n * Return the unique values found in the passed iterable\\n */\\nfunction uniq<TElement>(iterableToGetUniqueValuesOf: Iterable<TElement>) {\\n  return [...new Set(iterableToGetUniqueValuesOf)];\\n}\\n```\\n\\nUsage of this was simple:\\n\\n```ts\\nuniq([1, 1, 1, 3, 1, 1, 2]); // produces [1, 3, 2]\\nuniq([\'John\', \'Guida\', \'Ollie\', \'Divya\', \'John\']); // produces [\\"John\\", \\"Guida\\", \\"Ollie\\", \\"Divya\\"]\\n```\\n\\nAnd I thought this was tremendous. I committed and pushed. I assumed there was no more to be done. Guida (Margarida) then made this very helpful comment:\\n\\n> BTW, I found a big bold warning that `new Set()` compares objects by reference (unless they\'re primitives) so it might be worth adding a comment to warn people that uniq/distinct compares objects by reference: [https://codeburst.io/javascript-array-distinct-5edc93501dc4](https://codeburst.io/javascript-array-distinct-5edc93501dc4)\\n\\nShe was right! If a caller was to, say, pass a collection of objects to `uniq` then they\'d end up highly disappointed. Consider:\\n\\n```ts\\nuniq([{ name: \'John\' }, { name: \'John\' }]); // produces [{ name: \\"John\\" }, { name: \\"John\\" }]\\n```\\n\\nWe can do better!\\n\\n## `uniq` v2\\n\\nI like compilers shouting at me. Or more accurately, I like compilers telling me when something isn\'t valid / supported / correct. I wanted `uniq` to mirror the behaviour of `Set` \\\\- to only support primitives such as `string`, `number` etc. So I made a new version of `uniq` that hardened up the generic contraints:\\n\\n```ts\\n/**\\n * Return the unique values found in the passed iterable\\n */\\nfunction uniq<TElement extends string | number | bigint | boolean | symbol>(\\n  iterableToGetUniqueValuesOf: Iterable<TElement>\\n) {\\n  return [...new Set(iterableToGetUniqueValuesOf)];\\n}\\n```\\n\\nWith this in place, the compiler started shouting in the most helpful way. When I re-attemped `[{ name: \\"John\\" }, { name: \\"John\\" }]` the compiler hit me with:\\n\\n`Argument of type \'{ name: string; }[]\' is not assignable to parameter of type \'Iterable&lt;string | number | bigint | boolean | symbol&gt;\'.`\\n\\n[Take a look.](https://www.typescriptlang.org/play?#code/FAYw9gdgzmA2CmA6WYDmAKArhAlgR3QG0BvAAggEMBbeALlICIApMACwgdIF8AaUsyjXrM2HbgF0AlJNCQYCZGiy4ChEewZ91HKTOAB6AFSHgpQ6QBK8AC6YAThFLXW8UtnyZXANwqxPUUgAzMGwAE1IcR2dXAAcKKCh4cJxreDsKACMEU0N9YEDsEGscSDcVAB4AFQBRBBoIa1J4AA9UiFCAqGs7SNRSAB9yTCoMtIHSDJxUSMbBjLA4eApHQagATxG4AD50U1J9lLTMhEqwAHEbAFUVTwA1X38AeUD6AElU9Kz4Ktr4eustsBJPw9vs7DZ7I5CIgYRB4AB3UgAZRs6EOnxO5yuN3g9z88Cgz0k4gA3MAuMAgA)\\n\\nThis is good. This is descriptive code that only allows legitimate inputs. It should lead to less confusion and a reduced likelihood of issues in Production. It\'s also a nice example of how code review can result in demonstrably better code. Thanks Guida!"},{"id":"/2020/11/10/throttle-data-requests-with-react-hooks","metadata":{"permalink":"/2020/11/10/throttle-data-requests-with-react-hooks","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2020-11-10-throttle-data-requests-with-react-hooks/index.md","source":"@site/blog/2020-11-10-throttle-data-requests-with-react-hooks/index.md","title":"Throttling data requests with React Hooks","description":"When an application loads data, typically relatively few HTTP requests will be made. For example, if we imagine we\'re making a student administration application, then a \\"view\\" screen might make a single HTTP request to load that student\'s data before displaying it.","date":"2020-11-10T00:00:00.000Z","formattedDate":"November 10, 2020","tags":[{"label":"throttle","permalink":"/tags/throttle"},{"label":"React","permalink":"/tags/react"},{"label":"Hooks","permalink":"/tags/hooks"},{"label":"data","permalink":"/tags/data"}],"readingTime":13.145,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Throttling data requests with React Hooks","authors":"johnnyreilly","tags":["throttle","React","Hooks","data"],"hide_table_of_contents":false},"prevItem":{"title":"Bulletproof uniq with TypeScript generics (yay code reviews!)","permalink":"/2020/11/14/bulletproof-uniq-with-typescript"},"nextItem":{"title":"Azure DevOps Client for Node.js - working around limitations","permalink":"/2020/10/31/azure-devops-node-api-missing-episodes"}},"content":"When an application loads data, typically relatively few HTTP requests will be made. For example, if we imagine we\'re making a student administration application, then a \\"view\\" screen might make a single HTTP request to load that student\'s data before displaying it.\\n\\nOccasionally there\'s a need for an application to make a large number of HTTP requests. Consider a reporting application which loads data and then aggregates it for presentation purposes.\\n\\nThis need presents two interesting problems to solve:\\n\\n1. how do we load data gradually?\\n2. how do we present loading progress to users?\\n\\nThis post will talk about how we can tackle these and demonstrate using a custom React Hook.\\n\\n## Let\'s bring Chrome to its knees\\n\\nWe\'ll begin our journey by spinning up a TypeScript React app with [Create React App](https://create-react-app.dev/):\\n\\n```shell\\nnpx create-react-app throttle-requests-react-hook --template typescript\\n```\\n\\nBecause we\'re going to be making a number of asynchronous calls, we\'re going to simplify the code by leaning on the widely used [`react-use`](https://github.com/streamich/react-use) for a [`useAsync`](https://github.com/streamich/react-use/blob/master/docs/useAsync/index.md) hook.\\n\\n```shell\\ncd throttle-requests-react-hook\\nyarn add react-use\\n```\\n\\nWe\'ll replace the `App.css` file with this:\\n\\n```css\\n.App {\\n  text-align: center;\\n}\\n\\n.App-header {\\n  background-color: #282c34;\\n  min-height: 100vh;\\n  display: flex;\\n  flex-direction: column;\\n  align-items: center;\\n  justify-content: center;\\n  font-size: calc(10px + 2vmin);\\n  color: white;\\n}\\n\\n.App-labelinput > * {\\n  margin: 0.5em;\\n  font-size: 24px;\\n}\\n\\n.App-link {\\n  color: #61dafb;\\n}\\n\\n.App-button {\\n  font-size: calc(10px + 2vmin);\\n  margin-top: 0.5em;\\n  padding: 1em;\\n  background-color: cornflowerblue;\\n  color: #ffffff;\\n  text-align: center;\\n}\\n\\n.App-progress {\\n  padding: 1em;\\n  background-color: cadetblue;\\n  color: #ffffff;\\n}\\n\\n.App-results {\\n  display: flex;\\n  flex-wrap: wrap;\\n}\\n\\n.App-results > * {\\n  padding: 1em;\\n  margin: 0.5em;\\n  background-color: darkblue;\\n  flex: 1 1 300px;\\n}\\n```\\n\\nThen we\'ll replace the `App.tsx` contents with this:\\n\\n```tsx\\nimport React, { useState } from \'react\';\\nimport { useAsync } from \'react-use\';\\nimport \'./App.css\';\\n\\nfunction use10_000Requests(startedAt: string) {\\n  const responses = useAsync(async () => {\\n    if (!startedAt) return;\\n\\n    // make 10,000 unique HTTP requests\\n    const results = await Promise.all(\\n      Array.from(Array(10_000)).map(async (_, index) => {\\n        const response = await fetch(\\n          `/manifest.json?querystringValueToPreventCaching=${startedAt}_request-${index}`\\n        );\\n        const json = await response.json();\\n        return json;\\n      })\\n    );\\n\\n    return results;\\n  }, [startedAt]);\\n\\n  return responses;\\n}\\n\\nfunction App() {\\n  const [startedAt, setStartedAt] = useState(\'\');\\n  const responses = use10_000Requests(startedAt);\\n\\n  return (\\n    <div className=\\"App\\">\\n      <header className=\\"App-header\\">\\n        <h1>The HTTP request machine</h1>\\n        <button\\n          className=\\"App-button\\"\\n          onClick={(_) => setStartedAt(new Date().toISOString())}\\n        >\\n          Make 10,000 requests\\n        </button>\\n        {responses.loading && <div>{progressMessage}</div>}\\n        {responses.error && <div>Something went wrong</div>}\\n        {responses.value && (\\n          <div className=\\"App-results\\">\\n            {responses.value.length} requests completed successfully\\n          </div>\\n        )}\\n      </header>\\n    </div>\\n  );\\n}\\n\\nexport default App;\\n```\\n\\nThe app that we\'ve built is very simple; it\'s a button which, when you press it, fires 10,000 HTTP requests in parallel using the [Fetch API](https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API). The data being requested in this case is an arbitrary JSON file; the `manifest.json`. If you look closely you\'ll see we\'re doing some querystring tricks with our URL to avoid getting cached data.\\n\\nIn fact, for this demo we\'re not interested in the results of these HTTP requests; rather we\'re interested in how the browser copes with this approach. (Spoiler: not well!) It\'s worth considering that requesting a text file from a server running on the same machine as the browser should be fast.\\n\\nSo we\'ll run `yarn start` and go to [http://localhost:3000](http://localhost:3000) to get to the app. Running with Devtools open results in the following unhappy affair:\\n\\n![](i-want-it-all.gif)\\n\\nThe GIF above has been edited significantly for length. In reality it took 20 seconds for the first request to be fired, prior to that Chrome was unresponsive. When requests did start to fire, a significant number failed with `net::ERR_INSUFFICIENT_RESOURCES`. Further to that, those requests that were fired sat in \\"Stalled\\" state prior to being executed. This is a consequence of [Chrome limiting the number of connections - all browsers do this](https://developers.google.com/web/tools/chrome-devtools/network/reference#timing):\\n\\n> There are already six TCP connections open for this origin, which is the limit. Applies to HTTP/1.0 and HTTP/1.1 only.\\n\\nIn summary, the problems with the current approach are:\\n\\n1. the browser becoming unresponsive\\n2. failing HTTP requests due to insufficient resources\\n3. no information displayed to the user around progress\\n\\n## Throttle me this\\n\\nInstead of hammering the browser by firing all the requests at once, we could instead implement a throttle. A throttle is a mechanism which allows you to limit the rate at which operations are performed. In this case we want to limit the rate at which HTTP requests are made. A throttle will tackle problems 1 and 2 - essentially keeping the browser free and easy and ensuring that requests are all successfully sent. We also want to keep our users informed around how progress is going. It\'s time to unveil the `useThrottleRequests` hook:\\n\\n```ts\\nimport { useMemo, useReducer } from \'react\';\\nimport { AsyncState } from \'react-use/lib/useAsync\';\\n\\n/** Function which makes a request */\\nexport type RequestToMake = () => Promise<void>;\\n\\n/**\\n * Given an array of requestsToMake and a limit on the number of max parallel requests\\n * queue up those requests and start firing them\\n * - inspired by Rafael Xavier\'s approach here: https://stackoverflow.com/a/48007240/761388\\n *\\n * @param requestsToMake\\n * @param maxParallelRequests the maximum number of requests to make - defaults to 6\\n */\\nasync function throttleRequests(\\n  requestsToMake: RequestToMake[],\\n  maxParallelRequests = 6\\n) {\\n  // queue up simultaneous calls\\n  const queue: Promise<void>[] = [];\\n  for (let requestToMake of requestsToMake) {\\n    // fire the async function, add its promise to the queue,\\n    // and remove it from queue when complete\\n    const promise = requestToMake().then((res) => {\\n      queue.splice(queue.indexOf(promise), 1);\\n      return res;\\n    });\\n    queue.push(promise);\\n\\n    // if the number of queued requests matches our limit then\\n    // wait for one to finish before enqueueing more\\n    if (queue.length >= maxParallelRequests) {\\n      await Promise.race(queue);\\n    }\\n  }\\n  // wait for the rest of the calls to finish\\n  await Promise.all(queue);\\n}\\n\\n/**\\n * The state that represents the progress in processing throttled requests\\n */\\nexport type ThrottledProgress<TData> = {\\n  /** the number of requests that will be made */\\n  totalRequests: number;\\n  /** the errors that came from failed requests */\\n  errors: Error[];\\n  /** the responses that came from successful requests */\\n  values: TData[];\\n  /** a value between 0 and 100 which represents the percentage of requests that have been completed (whether successfully or not) */\\n  percentageLoaded: number;\\n  /** whether the throttle is currently processing requests */\\n  loading: boolean;\\n};\\n\\nfunction createThrottledProgress<TData>(\\n  totalRequests: number\\n): ThrottledProgress<TData> {\\n  return {\\n    totalRequests,\\n    percentageLoaded: 0,\\n    loading: false,\\n    errors: [],\\n    values: [],\\n  };\\n}\\n\\n/**\\n * A reducing function which takes the supplied `ThrottledProgress` and applies a new value to it\\n */\\nfunction updateThrottledProgress<TData>(\\n  currentProgress: ThrottledProgress<TData>,\\n  newData: AsyncState<TData>\\n): ThrottledProgress<TData> {\\n  const errors = newData.error\\n    ? [...currentProgress.errors, newData.error]\\n    : currentProgress.errors;\\n\\n  const values = newData.value\\n    ? [...currentProgress.values, newData.value]\\n    : currentProgress.values;\\n\\n  const percentageLoaded =\\n    currentProgress.totalRequests === 0\\n      ? 0\\n      : Math.round(\\n          ((errors.length + values.length) / currentProgress.totalRequests) *\\n            100\\n        );\\n\\n  const loading =\\n    currentProgress.totalRequests === 0\\n      ? false\\n      : errors.length + values.length < currentProgress.totalRequests;\\n\\n  return {\\n    totalRequests: currentProgress.totalRequests,\\n    loading,\\n    percentageLoaded,\\n    errors,\\n    values,\\n  };\\n}\\n\\ntype ThrottleActions<TValue> =\\n  | {\\n      type: \'initialise\';\\n      totalRequests: number;\\n    }\\n  | {\\n      type: \'requestSuccess\';\\n      value: TValue;\\n    }\\n  | {\\n      type: \'requestFailed\';\\n      error: Error;\\n    };\\n\\n/**\\n * Create a ThrottleRequests and an updater\\n */\\nexport function useThrottleRequests<TValue>() {\\n  function reducer(\\n    throttledProgressAndState: ThrottledProgress<TValue>,\\n    action: ThrottleActions<TValue>\\n  ): ThrottledProgress<TValue> {\\n    switch (action.type) {\\n      case \'initialise\':\\n        return createThrottledProgress(action.totalRequests);\\n\\n      case \'requestSuccess\':\\n        return updateThrottledProgress(throttledProgressAndState, {\\n          loading: false,\\n          value: action.value,\\n        });\\n\\n      case \'requestFailed\':\\n        return updateThrottledProgress(throttledProgressAndState, {\\n          loading: false,\\n          error: action.error,\\n        });\\n    }\\n  }\\n\\n  const [throttle, dispatch] = useReducer(\\n    reducer,\\n    createThrottledProgress<TValue>(/** totalRequests */ 0)\\n  );\\n\\n  const updateThrottle = useMemo(() => {\\n    /**\\n     * Update the throttle with a successful request\\n     * @param values from request\\n     */\\n    function requestSucceededWithData(value: TValue) {\\n      return dispatch({\\n        type: \'requestSuccess\',\\n        value,\\n      });\\n    }\\n\\n    /**\\n     * Update the throttle upon a failed request with an error message\\n     * @param error error\\n     */\\n    function requestFailedWithError(error: Error) {\\n      return dispatch({\\n        type: \'requestFailed\',\\n        error,\\n      });\\n    }\\n\\n    /**\\n     * Given an array of requestsToMake and a limit on the number of max parallel requests\\n     * queue up those requests and start firing them\\n     * - based upon https://stackoverflow.com/a/48007240/761388\\n     *\\n     * @param requestsToMake\\n     * @param maxParallelRequests the maximum number of requests to make - defaults to 6\\n     */\\n    function queueRequests(\\n      requestsToMake: RequestToMake[],\\n      maxParallelRequests = 6\\n    ) {\\n      dispatch({\\n        type: \'initialise\',\\n        totalRequests: requestsToMake.length,\\n      });\\n\\n      return throttleRequests(requestsToMake, maxParallelRequests);\\n    }\\n\\n    return {\\n      queueRequests,\\n      requestSucceededWithData,\\n      requestFailedWithError,\\n    };\\n  }, [dispatch]);\\n\\n  return {\\n    throttle,\\n    updateThrottle,\\n  };\\n}\\n```\\n\\nThe `useThrottleRequests` hook returns 2 properties:\\n\\n- `throttle` \\\\- a `ThrottledProgress&lt;TData&gt;` that contains the following data:\\n\\n  - `totalRequests` \\\\- the number of requests that will be made\\n  - `errors` \\\\- the errors that came from failed requests\\n  - `values` \\\\- the responses that came from successful requests\\n  - `percentageLoaded` \\\\- a value between 0 and 100 which represents the percentage of requests that have been completed (whether successfully or not)\\n  - `loading` \\\\- whether the throttle is currently processing requests\\n\\n- `updateThrottle` \\\\- an object which exposes 3 functions:\\n\\n  - `queueRequests` \\\\- the function to which you pass the requests that should be queued and executed in a throttled fashion\\n  - `requestSucceededWithData` \\\\- the function which is called if a request succeeds to provide the data\\n  - `requestFailedWithError` \\\\- the function which is called if a request fails to provide the error\\n\\nThat\'s a lot of words to describe our `useThrottleRequests` hook. Let\'s look at what it looks like by migrating our `use10_000Requests` hook to (no pun intended) use it. Here\'s a new implementation of `App.tsx`:\\n\\n```tsx\\nimport React, { useState } from \'react\';\\nimport { useAsync } from \'react-use\';\\nimport { useThrottleRequests } from \'./useThrottleRequests\';\\nimport \'./App.css\';\\n\\nfunction use10_000Requests(startedAt: string) {\\n  const { throttle, updateThrottle } = useThrottleRequests();\\n  const [progressMessage, setProgressMessage] = useState(\'not started\');\\n\\n  useAsync(async () => {\\n    if (!startedAt) return;\\n\\n    setProgressMessage(\'preparing\');\\n\\n    const requestsToMake = Array.from(Array(10_000)).map(\\n      (_, index) => async () => {\\n        try {\\n          setProgressMessage(`loading ${index}...`);\\n\\n          const response = await fetch(\\n            `/manifest.json?querystringValueToPreventCaching=${startedAt}_request-${index}`\\n          );\\n          const json = await response.json();\\n\\n          updateThrottle.requestSucceededWithData(json);\\n        } catch (error) {\\n          console.error(`failed to load ${index}`, error);\\n          updateThrottle.requestFailedWithError(error);\\n        }\\n      }\\n    );\\n\\n    await updateThrottle.queueRequests(requestsToMake);\\n  }, [startedAt, updateThrottle, setProgressMessage]);\\n\\n  return { throttle, progressMessage };\\n}\\n\\nfunction App() {\\n  const [startedAt, setStartedAt] = useState(\'\');\\n\\n  const { progressMessage, throttle } = use10_000Requests(startedAt);\\n\\n  return (\\n    <div className=\\"App\\">\\n      <header className=\\"App-header\\">\\n        <h1>The HTTP request machine</h1>\\n        <button\\n          className=\\"App-button\\"\\n          onClick={(_) => setStartedAt(new Date().toISOString())}\\n        >\\n          Make 10,000 requests\\n        </button>\\n        {throttle.loading && <div>{progressMessage}</div>}\\n        {throttle.values.length > 0 && (\\n          <div className=\\"App-results\\">\\n            {throttle.values.length} requests completed successfully\\n          </div>\\n        )}\\n        {throttle.errors.length > 0 && (\\n          <div className=\\"App-results\\">\\n            {throttle.errors.length} requests errored\\n          </div>\\n        )}\\n      </header>\\n    </div>\\n  );\\n}\\n\\nexport default App;\\n```\\n\\nLooking at the new `use10_000Requests` hook, there\'s a few subtle differences to our prior implementation. First of all, we\'re now exposing the `throttle`; a `ThrottleProgress&lt;TData&gt;`. Our updated hook also exposes a `progressMessage` which is a simple `string` stored with `useState` that we update as our throttle runs. In truth the information being surfaced here isn\'t that interesting. The `progressMessage` is in place just to illustrate that you could capture some data from your requests as they complete for display purposes; a running total for instance.\\n\\nSo, how does our new hook approach perform?\\n\\n![](i-want-it-all-with-hook.gif)\\n\\nVery well indeed! Please note that the above GIF has again been edited for brevity. If we look back at the problems we faced with the prior approach, how do we compare?\\n\\n1. ~~the browser becoming unresponsive~~ \\\\- the browser remains responsive.\\n2. ~~failing HTTP requests due to insufficient resources~~ \\\\- the browser does not experience failing HTTP requests.\\n3. ~~no information displayable to the user around progress~~ \\\\- details of progress are displayed to the user throughout.\\n\\nTremendous!\\n\\n## What shall we build?\\n\\nOur current example is definitely contrived. Let\'s try and apply our `useThrottleRequests` hook to a more realistic scenario. We\'re going to build an application which, given a repo on GitHub, lists all the contributors blogs. (You can specify a blog URL on your GitHub profile; many people use this to specify their Twitter profile.)\\n\\nWe can build this thanks to the excellent [GitHub REST API](https://docs.github.com/en/free-pro-team@latest/rest). It exposes two endpoints of interest given our goal.\\n\\n### 1\\\\. List repository contributors\\n\\n[List repository contributors](https://docs.github.com/en/free-pro-team@latest/rest/reference/repos#list-repository-contributors) lists contributors to the specified repository at this URL: `GET https://api.github.com/repos/{owner}/{repo}/contributors`. The response is an array of objects, crucially featuring a `url` property that points to the user in question\'s API endpoint:\\n\\n```js\\n[\\n  // ...\\n  {\\n    // ...\\n    url: \'https://api.github.com/users/octocat\',\\n    // ...\\n  },\\n  // ...\\n];\\n```\\n\\n### 2\\\\. Get a user\\n\\n[Get a user](https://docs.github.com/en/free-pro-team@latest/rest/reference/users#get-a-user) is the API that the `url` property above is referring to. When called it returns an object representing the publicly available information about a user:\\n\\n```js\\n{\\n  // ...\\n  \\"name\\": \\"The Octocat\\",\\n  // ...\\n  \\"blog\\": \\"https://github.blog\\",\\n  // ...\\n}\\n```\\n\\n## Blogging devs v1.0\\n\\nWe\'re now ready to build our blogging devs app; let\'s replace the existing `App.tsx` with:\\n\\n```tsx\\nimport React, { useCallback, useMemo, useState } from \'react\';\\nimport { useAsync } from \'react-use\';\\nimport { useThrottleRequests } from \'./useThrottleRequests\';\\nimport \'./App.css\';\\n\\ntype GitHubUser = { name: string; blog?: string };\\n\\nfunction timeout(ms: number) {\\n  return new Promise((resolve) => setTimeout(resolve, ms));\\n}\\n\\nfunction useContributors(contributorsUrlToLoad: string) {\\n  const { throttle, updateThrottle } = useThrottleRequests<GitHubUser>();\\n  const [progressMessage, setProgressMessage] = useState(\'\');\\n\\n  useAsync(async () => {\\n    if (!contributorsUrlToLoad) return;\\n\\n    setProgressMessage(\'loading contributors\');\\n\\n    // load contributors from GitHub\\n    const contributorsResponse = await fetch(contributorsUrlToLoad);\\n    const contributors: { url: string }[] = await contributorsResponse.json();\\n\\n    setProgressMessage(`loading ${contributors.length} contributors...`);\\n\\n    // For each entry in result, retrieve the given user from GitHub\\n    const requestsToMake = contributors.map(({ url }, index) => async () => {\\n      try {\\n        setProgressMessage(\\n          `loading ${index} / ${contributors.length}: ${url}...`\\n        );\\n\\n        const response = await fetch(url);\\n        const json: GitHubUser = await response.json();\\n\\n        // wait for 1 second before completing the request\\n        // - makes for better demos\\n        await timeout(1000);\\n\\n        updateThrottle.requestSucceededWithData(json);\\n      } catch (error) {\\n        console.error(`failed to load ${url}`, error);\\n        updateThrottle.requestFailedWithError(error);\\n      }\\n    });\\n\\n    await updateThrottle.queueRequests(requestsToMake);\\n\\n    setProgressMessage(\'\');\\n  }, [contributorsUrlToLoad, updateThrottle, setProgressMessage]);\\n\\n  return { throttle, progressMessage };\\n}\\n\\nfunction App() {\\n  // The owner and repo to query; we\'re going to default\\n  // to using DefinitelyTyped as an example repo as it\\n  // is one of the most contributed to repos on GitHub\\n  const [owner, setOwner] = useState(\'DefinitelyTyped\');\\n  const [repo, setRepo] = useState(\'DefinitelyTyped\');\\n  const handleOwnerChange = useCallback(\\n    (event: React.ChangeEvent<HTMLInputElement>) =>\\n      setOwner(event.target.value),\\n    [setOwner]\\n  );\\n  const handleRepoChange = useCallback(\\n    (event: React.ChangeEvent<HTMLInputElement>) => setRepo(event.target.value),\\n    [setRepo]\\n  );\\n\\n  const contributorsUrl = `https://api.github.com/repos/${owner}/${repo}/contributors`;\\n\\n  const [contributorsUrlToLoad, setUrlToLoad] = useState(\'\');\\n  const { progressMessage, throttle } = useContributors(contributorsUrlToLoad);\\n\\n  const bloggers = useMemo(\\n    () => throttle.values.filter((contributor) => contributor.blog),\\n    [throttle]\\n  );\\n\\n  return (\\n    <div className=\\"App\\">\\n      <header className=\\"App-header\\">\\n        <h1>Blogging devs</h1>\\n\\n        <p>\\n          Show me the{\' \'}\\n          <a\\n            className=\\"App-link\\"\\n            href={contributorsUrl}\\n            target=\\"_blank\\"\\n            rel=\\"noopener noreferrer\\"\\n          >\\n            contributors for {owner}/{repo}\\n          </a>{\' \'}\\n          who have blogs.\\n        </p>\\n\\n        <div className=\\"App-labelinput\\">\\n          <label htmlFor=\\"owner\\">GitHub Owner</label>\\n          <input\\n            id=\\"owner\\"\\n            type=\\"text\\"\\n            value={owner}\\n            onChange={handleOwnerChange}\\n          />\\n          <label htmlFor=\\"repo\\">GitHub Repo</label>\\n          <input\\n            id=\\"repo\\"\\n            type=\\"text\\"\\n            value={repo}\\n            onChange={handleRepoChange}\\n          />\\n        </div>\\n\\n        <button\\n          className=\\"App-button\\"\\n          onClick={(e) => setUrlToLoad(contributorsUrl)}\\n        >\\n          Load bloggers from GitHub\\n        </button>\\n\\n        {progressMessage && (\\n          <div className=\\"App-progress\\">{progressMessage}</div>\\n        )}\\n\\n        {throttle.percentageLoaded > 0 && (\\n          <>\\n            <h3>Behold {bloggers.length} bloggers:</h3>\\n            <div className=\\"App-results\\">\\n              {bloggers.map((blogger) => (\\n                <div key={blogger.name}>\\n                  <div>{blogger.name}</div>\\n                  <a\\n                    className=\\"App-link\\"\\n                    href={blogger.blog}\\n                    target=\\"_blank\\"\\n                    rel=\\"noopener noreferrer\\"\\n                  >\\n                    {blogger.blog}\\n                  </a>\\n                </div>\\n              ))}\\n            </div>\\n          </>\\n        )}\\n\\n        {throttle.errors.length > 0 && (\\n          <div className=\\"App-results\\">\\n            {throttle.errors.length} requests errored\\n          </div>\\n        )}\\n      </header>\\n    </div>\\n  );\\n}\\n\\nexport default App;\\n```\\n\\nThe application gives users the opportunity to enter the organisation and repository of a GitHub project. Then, when the button is clicked, it:\\n\\n- loads the contributors\\n- for each contributor it loads the individual user (separate HTTP request for each)\\n- as it loads it communicates how far through the loading progress it has got\\n- as users are loaded, it renders a tile for each user with a listed blog\\n\\nJust to make the demo a little clearer we\'ve artificially slowed the duration of each request by a second. What does it look like when you put it together? Well like this:\\n\\n![](blogging-devs.gif)\\n\\nWe have built a React Hook which allows us to:\\n\\n- gradually load data\\n- without blocking the UI of the browser\\n- and which provides progress data to keep users informed.\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/throttling-data-requests-with-react-hooks/)"},{"id":"/2020/10/31/azure-devops-node-api-missing-episodes","metadata":{"permalink":"/2020/10/31/azure-devops-node-api-missing-episodes","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2020-10-31-azure-devops-node-api-missing-episodes/index.md","source":"@site/blog/2020-10-31-azure-devops-node-api-missing-episodes/index.md","title":"Azure DevOps Client for Node.js - working around limitations","description":"The Azure DevOps Client library for Node.js has limitations and missing features, such as the ability to paginate git refs and create wiki posts. This post details some of these issues and illustrates a workaround using the Azure DevOps REST API.","date":"2020-10-31T00:00:00.000Z","formattedDate":"October 31, 2020","tags":[{"label":"azure devops api","permalink":"/tags/azure-devops-api"},{"label":"203","permalink":"/tags/203"},{"label":"node.js","permalink":"/tags/node-js"}],"readingTime":3.67,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Azure DevOps Client for Node.js - working around limitations","authors":"johnnyreilly","tags":["azure devops api","203","node.js"],"image":"./title-image.png","hide_table_of_contents":false},"prevItem":{"title":"Throttling data requests with React Hooks","permalink":"/2020/11/10/throttle-data-requests-with-react-hooks"},"nextItem":{"title":"Safari: The Mysterious Case of the Empty Download","permalink":"/2020/10/19/safari-empty-download-content-type"}},"content":"The Azure DevOps Client library for Node.js has limitations and missing features, such as the ability to paginate git refs and create wiki posts. This post details some of these issues and illustrates a workaround using the Azure DevOps REST API.\\n\\n![A title image that reads \\"Azure DevOps Client for Node.js - working around limitations\\"](title-image.png)\\n\\n## The Azure DevOps REST API and Client Libraries\\n\\nI\'ve been taking a good look at the [REST API for Azure DevOps](https://docs.microsoft.com/en-us/rest/api/azure/devops/?view=azure-devops-rest-6.1). I\'m delighted to say that it\'s a very full API. However, there\'s quirks.\\n\\nI\'m writing a tool that interrogates Azure DevOps in order that it can construct release documentation. That release documentation we would like to publish to the project wiki.\\n\\nTo make integration with Azure DevOps even easier, the ADO team have put a good amount of work into [client libraries](https://docs.microsoft.com/en-us/rest/api/azure/devops/?view=azure-devops-rest-6.1#client-libraries) that allow you to code in your language of choice. In my case I\'m writing a Node.js tool (using TypeScript) and happily the client lib for Node is written and published with TypeScript too. Tremendous! However, there is a \\"but\\" coming....\\n\\n## `GitApi` and `WikiApi` shortcomings\\n\\nAs I\'ve been using the Node client lib, I\'ve found minor quirks. Such as the [`GitApi.getRefs` missing the pagination parts of the API](https://github.com/microsoft/azure-devops-node-api/issues/415).\\n\\nWhilst the `GitApi` was missing some parameters on a method, the `WikiApi` was [missing whole endpoints, such as the Pages - Create Or Update](https://github.com/microsoft/azure-devops-node-api/issues/416) one. The various [client libraries are auto-generated](https://github.com/microsoft/azure-devops-node-api/blob/master/CONTRIBUTING/index.md#general-contribution-guide) which makes contribution a difficult game. The lovely [Matt Cooper](https://github.com/vtbassmatt) has [alerted the team](https://github.com/microsoft/azure-devops-node-api/issues/415#issuecomment-717991914)\\n\\n> These clients are generated from the server-side controllers, and at a glance, I don\'t understand why those two parameters weren\'t included. Full transparency, we don\'t dedicate a lot of cycles here, but I will get it on the team\'s radar to investigate/improve.\\n\\nIn the meantime, I still had a tool to write.\\n\\n## Handrolled Wiki API\\n\\nWhilst the Node.js client lib was missing some crucial pieces, there did seem to be a way forward. Using the API directly; not using the client lib to do our HTTP and using [axios](https://github.com/axios/axios) instead. Happily the types we needed were still available for be leveraged.\\n\\nLooking at the docs it seemed it ought to be simple:\\n\\n[https://docs.microsoft.com/en-us/rest/api/azure/devops/?view=azure-devops-rest-6.1#assemble-the-request](https://docs.microsoft.com/en-us/rest/api/azure/devops/?view=azure-devops-rest-6.1#assemble-the-request)\\n\\nBut when I attempted this I found my requests erroring out with 203 Non-Authoritative Informations. It didn\'t make sense. I couldn\'t get a single request to be successful, they all failed. It occurred to me that the answer was hiding in `node_modules`. I\'d managed to make successful requests to the API using the client lib. What was it doing that I wasn\'t?\\n\\nThe answer ended up being an authorization one-liner:\\n\\n```ts\\nconst request = await axios({\\n        url,\\n        headers: {\\n            Accept: \'application/json\',\\n            \'Content-Type\': \'application/json\',\\n            // This!\\n            Authorization: `Basic ${Buffer.from(`PAT:${adoPersonalAccessToken}`).toString(\'base64\')}`,\\n            \'X-TFS-FedAuthRedirect\': \'Suppress\',\\n        },\\n    });\\n}\\n```\\n\\nWith this in hand everything started to work and I found myself able to write my own clients to fill in the missing pieces from the client lib:\\n\\n```ts\\nimport axios from \'axios\';\\nimport {\\n  WikiPage,\\n  WikiPageCreateOrUpdateParameters,\\n  WikiType,\\n} from \'azure-devops-node-api/interfaces/WikiInterfaces\';\\nimport { IWikiApi } from \'azure-devops-node-api/WikiApi\';\\n\\nasync function getWikiPage({\\n  adoUrl,\\n  adoProject,\\n  adoPat,\\n  wikiId,\\n  path,\\n}: {\\n  adoUrl: string;\\n  adoProject: string;\\n  adoPat: string;\\n  wikiId: string;\\n  path: string;\\n}) {\\n  try {\\n    const url = `${makeBaseApiUrl({\\n      adoUrl,\\n      adoProject,\\n    })}/wiki/wikis/${wikiId}/pages?${apiVersion}&path=${path}&includeContent=True&recursionLevel=full`;\\n    const request = await axios({\\n      url,\\n      headers: makeHeaders(adoPat),\\n    });\\n\\n    const page: WikiPage = request.data;\\n    return page;\\n  } catch (error) {\\n    return undefined;\\n  }\\n}\\n\\nasync function createWikiPage({\\n  adoUrl,\\n  adoProject,\\n  adoPat,\\n  wikiId,\\n  path,\\n  data,\\n}: {\\n  adoUrl: string;\\n  adoProject: string;\\n  adoPat: string;\\n  wikiId: string;\\n  path: string;\\n  data: WikiPageCreateOrUpdateParameters;\\n}) {\\n  try {\\n    const url = `${makeBaseApiUrl({\\n      adoUrl,\\n      adoProject,\\n    })}/wiki/wikis/${wikiId}/pages?${apiVersion}&path=${path}`;\\n\\n    const request = await axios({\\n      method: \'PUT\',\\n      url,\\n      headers: makeHeaders(adoPat),\\n      data,\\n    });\\n\\n    const newPage: WikiPage = request.data;\\n    return newPage;\\n  } catch (error) {\\n    return undefined;\\n  }\\n}\\n\\nconst apiVersion = \'api-version=6.0\';\\n\\n/**\\n * Create the headers necessary to ake Azure DevOps happy\\n * @param adoPat Personal Access Token from ADO\\n */\\nfunction makeHeaders(adoPat: string) {\\n  return {\\n    Accept: \'application/json\',\\n    \'Content-Type\': \'application/json\',\\n    Authorization: `Basic ${Buffer.from(`PAT:${adoPat}`).toString(\'base64\')}`,\\n    \'X-TFS-FedAuthRedirect\': \'Suppress\',\\n  };\\n}\\n\\n/**\\n * eg https://dev.azure.com/{organization}/{project}/_apis\\n */\\nfunction makeBaseApiUrl({\\n  adoUrl,\\n  adoProject,\\n}: {\\n  adoUrl: string;\\n  adoProject: string;\\n}) {\\n  return `${adoUrl}/${adoProject}/_apis`;\\n}\\n```\\n\\nWith this I was able to write code like this:\\n\\n```ts\\nlet topLevelPage = await getWikiPage({\\n  adoUrl,\\n  adoProject,\\n  adoPat,\\n  wikiId,\\n  path: config.wikiTopLevelName,\\n});\\n\\nif (!topLevelPage)\\n  topLevelPage = await createWikiPage({\\n    adoUrl,\\n    adoProject,\\n    adoPat,\\n    wikiId,\\n    path: config.wikiTopLevelName,\\n    data: { content: \'\' },\\n  });\\n```\\n\\nand the wikis were ours!"},{"id":"/2020/10/19/safari-empty-download-content-type","metadata":{"permalink":"/2020/10/19/safari-empty-download-content-type","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2020-10-19-safari-empty-download-content-type/index.md","source":"@site/blog/2020-10-19-safari-empty-download-content-type/index.md","title":"Safari: The Mysterious Case of the Empty Download","description":"Safari wants a Content-Type header in responses. Even if the response is Content-Length: 0. Without this, Safari can attempt to trigger an empty download. Don\'t argue; just go with it; some browsers are strange.","date":"2020-10-19T00:00:00.000Z","formattedDate":"October 19, 2020","tags":[{"label":"Safari","permalink":"/tags/safari"},{"label":"Content-Type","permalink":"/tags/content-type"},{"label":"Content-Length","permalink":"/tags/content-length"}],"readingTime":2.215,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Safari: The Mysterious Case of the Empty Download","authors":"johnnyreilly","tags":["Safari","Content-Type","Content-Length"],"hide_table_of_contents":false},"prevItem":{"title":"Azure DevOps Client for Node.js - working around limitations","permalink":"/2020/10/31/azure-devops-node-api-missing-episodes"},"nextItem":{"title":"Autofac 6, integration tests and .NET generic hosting","permalink":"/2020/10/02/autofac-6-integration-tests-and-generic-hosting"}},"content":"Safari wants a `Content-Type` header in responses. Even if the response is `Content-Length: 0`. Without this, Safari can attempt to trigger an empty download. Don\'t argue; just go with it; some browsers are strange.\\n\\n## The longer version\\n\\nEvery now and then a mystery presents itself. A puzzle which just doesn\'t make sense and yet stubbornly continues to exist. I happened upon one of these the other day and to say it was frustrating does it no justice at all.\\n\\nIt all came back to the default iOS and Mac browser; Safari. When our users log into our application, they are redirected to a shared login provider which, upon successful authentication, hands over a cookie containing auth details and redirects back to our application. A middleware in our app reads what it needs from the cookie and then creates a cookie of its own which is to be used throughout the session. As soon as the cookie is set, the page refreshes and the app boots up in an authenticated state.\\n\\nThat\'s the background. This mechanism had long been working fine with Chrome (which the majority of our users browse with), Edge, Firefox and Internet Explorer. But we started to get reports from Safari users that, once they\'d supplied their credentials, they\'d not be authenticated and redirected back to our application. Instead they\'d be prompted to download an empty document and the redirect would not take place.\\n\\nAs a team we could not fathom why this should be the case; it just didn\'t make sense. There followed hours of experimentation before [Hennie](https://twitter.com/hennie_spies) noticed something. It was at the point when the redirect back to our app from the login provider took place. Specifically the initial response that came back which contained our custom cookie and a `Refresh: 0` header to trigger a refresh in the browser. There was no content in the response, save for headers. It was `Content-Length: 0` all the way.\\n\\nHennie noticed that there was no `Content-Type` set and wondered if that was significant. It didn\'t seem like it would be a necessary header given there was no content. But Safari reckons not with logic. As an experiment we tried setting the response header to `Content-Type: text/html`. It worked! No mystery download, no failed redirect (which it turned out was actually a successful redirect which wasn\'t being surfaced in Safari\'s network request tab).\\n\\nIt appears that always providing a `Content-Type` header in your responses is wise if only for the case of Safari. In fact, it\'s generally unlikely that this won\'t be set anyway, but it can happen as we have experienced. Hopefully we\'ve suffered so you don\'t have to."},{"id":"/2020/10/02/autofac-6-integration-tests-and-generic-hosting","metadata":{"permalink":"/2020/10/02/autofac-6-integration-tests-and-generic-hosting","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2020-10-02-autofac-6-integration-tests-and-generic-hosting/index.md","source":"@site/blog/2020-10-02-autofac-6-integration-tests-and-generic-hosting/index.md","title":"Autofac 6, integration tests and .NET generic hosting","description":"I blogged a little while ago around to support integration tests using Autofac. This was specific to Autofac but documented a workaround for a long standing issue with ConfigureTestContainer that was introduced into .NET core 3.0 which affects all third-party containers that use ConfigureTestContainer in their tests.","date":"2020-10-02T00:00:00.000Z","formattedDate":"October 2, 2020","tags":[{"label":"autofac","permalink":"/tags/autofac"},{"label":"asp.net","permalink":"/tags/asp-net"},{"label":"ConfigureTestContainer","permalink":"/tags/configure-test-container"},{"label":"Integration Testing","permalink":"/tags/integration-testing"}],"readingTime":2.23,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Autofac 6, integration tests and .NET generic hosting","authors":"johnnyreilly","tags":["autofac","asp.net","ConfigureTestContainer","Integration Testing"],"image":"./autofac-integration-tests.png","hide_table_of_contents":false},"prevItem":{"title":"Safari: The Mysterious Case of the Empty Download","permalink":"/2020/10/19/safari-empty-download-content-type"},"nextItem":{"title":"Why your team needs a newsfeed","permalink":"/2020/09/04/why-your-team-needs-newsfeed"}},"content":"I [blogged a little while ago around to support integration tests using Autofac](./2020-05-21-autofac-webapplicationfactory-integration-tests/index.md). This was specific to Autofac but documented a workaround for a [long standing issue with `ConfigureTestContainer` that was introduced into .NET core 3.0](https://github.com/dotnet/aspnetcore/issues/14907) which affects [all third-party containers](https://docs.microsoft.com/en-us/aspnet/core/fundamentals/dependency-injection?view=aspnetcore-3.1#default-service-container-replacement) that use `ConfigureTestContainer` in their tests.\\n\\n![A title image for the blog featuring the Autofac logo](autofac-integration-tests.png)\\n\\nI\'ll not repeat the contents of the previous post - it all still stands. However, with Autofac 6 the approach documented there will cease to work. This is because the previous approach relied upon `ContainerBuilder` not being sealed. [As of Autofac 6 it is.](https://github.com/autofac/Autofac/issues/1120)\\n\\nHappily the tremendous [Alistair Evans](https://twitter.com/evocationist) came up with an [alternative approach](https://github.com/autofac/Autofac/issues/1207#issuecomment-701961371) which is listed below:\\n\\n```cs\\n/// <summary>\\n/// Based upon https://github.com/dotnet/AspNetCore.Docs/tree/master/aspnetcore/test/integration-tests/samples/3.x/IntegrationTestsSample\\n/// </summary>\\n/// <typeparam name=\\"TStartup\\"></typeparam>\\npublic class AutofacWebApplicationFactory<TStartup> : WebApplicationFactory<TStartup> where TStartup : class\\n{\\n    protected override IHost CreateHost(IHostBuilder builder)\\n    {\\n        builder.UseServiceProviderFactory<ContainerBuilder>(new CustomServiceProviderFactory());\\n        return base.CreateHost(builder);\\n    }\\n}\\n\\n/// <summary>\\n/// Based upon https://github.com/dotnet/aspnetcore/issues/14907#issuecomment-620750841 - only necessary because of an issue in ASP.NET Core\\n/// </summary>\\npublic class CustomServiceProviderFactory : IServiceProviderFactory<ContainerBuilder>\\n{\\n    private AutofacServiceProviderFactory _wrapped;\\n    private IServiceCollection _services;\\n\\n    public CustomServiceProviderFactory()\\n    {\\n        _wrapped = new AutofacServiceProviderFactory();\\n    }\\n\\n    public ContainerBuilder CreateBuilder(IServiceCollection services)\\n    {\\n        // Store the services for later.\\n        _services = services;\\n\\n        return _wrapped.CreateBuilder(services);\\n    }\\n\\n    public IServiceProvider CreateServiceProvider(ContainerBuilder containerBuilder)\\n    {\\n        var sp = _services.BuildServiceProvider();\\n#pragma warning disable CS0612 // Type or member is obsolete\\n        var filters = sp.GetRequiredService<IEnumerable<IStartupConfigureContainerFilter<ContainerBuilder>>>();\\n#pragma warning restore CS0612 // Type or member is obsolete\\n\\n        foreach (var filter in filters)\\n        {\\n            filter.ConfigureContainer(b => { })(containerBuilder);\\n        }\\n\\n        return _wrapped.CreateServiceProvider(containerBuilder);\\n    }\\n}\\n```\\n\\nUsing this in place of the previous approach should allow you continue running your integration tests with Autofac 6. Thanks Alistair!\\n\\n## Concern for third-party containers\\n\\nWhilst this gets us back up and running, [Alistair pointed out that this approach depends upon a deprecated interface](https://github.com/autofac/Autofac/issues/1207#issuecomment-702250044). This is the [`IStartupConfigureContainerFilter`](https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.hosting.istartupconfigurecontainerfilter-1.configurecontainer?view=aspnetcore-3.1) which [has been marked as `Obsolete` since mid 2019](https://github.com/dotnet/aspnetcore/pull/11505). What this means is, at some point, this approach will stop working.\\n\\nThe marvellous David Fowler has said that [`ConfigureTestContainer` issue should be resolved in .NET](https://github.com/autofac/Autofac/issues/1207#issuecomment-702361608). However it\'s worth noting that this has been an issue since .NET Core 3 shipped and unfortunately the wonderful [Chris Ross has advised that it\'s not likely to be fixed for .NET 5](https://github.com/dotnet/aspnetcore/issues/14907#issuecomment-702287717).\\n\\nI\'m very keen this does get resolved in .NET. Building tests upon an `Obsolete` attribute doesn\'t fill me with confidence. I\'m a long time user of Autofac and I\'d like to continue to be. Here\'s hoping that\'s made possible by a fix landing in .NET. If this is something you care about, it may be worth upvoting / commenting on [the issue in GitHub](https://github.com/dotnet/aspnetcore/issues/14907) so the team are aware of desire around this being resolved."},{"id":"/2020/09/04/why-your-team-needs-newsfeed","metadata":{"permalink":"/2020/09/04/why-your-team-needs-newsfeed","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2020-09-04-why-your-team-needs-newsfeed/index.md","source":"@site/blog/2020-09-04-why-your-team-needs-newsfeed/index.md","title":"Why your team needs a newsfeed","description":"I\'m part of a team that builds an online platform. I\'m often preoccupied by how to narrow the gap between our users and \\"us\\" - the people that build the platform. It\'s important we understand how people use and interact with what we\'ve built. If we don\'t then we\'re liable to waste our time and energy building the wrong things. Or the wrong amount of the right things.","date":"2020-09-04T00:00:00.000Z","formattedDate":"September 4, 2020","tags":[{"label":"newsfeed","permalink":"/tags/newsfeed"}],"readingTime":4.99,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Why your team needs a newsfeed","authors":"johnnyreilly","tags":["newsfeed"],"hide_table_of_contents":false},"prevItem":{"title":"Autofac 6, integration tests and .NET generic hosting","permalink":"/2020/10/02/autofac-6-integration-tests-and-generic-hosting"},"nextItem":{"title":"Devcontainers AKA performance in a secure sandbox","permalink":"/2020/08/09/devcontainers-aka-performance-in-secure"}},"content":"I\'m part of a team that builds an online platform. I\'m often preoccupied by how to narrow the gap between our users and \\"us\\" - the people that build the platform. It\'s important we understand how people use and interact with what we\'ve built. If we don\'t then we\'re liable to waste our time and energy building the wrong things. Or the wrong amount of the right things.\\n\\nOn a recent holiday I spent a certain amount of time pondering how to narrow the gap between our user and us. We have lots of things that help us; we use various analytics tools like [mixpanel](https://mixpanel.com/), we\'ve got a mini analytics platform of our own, we have teams notifications that pop up client feedback and so on. They are all great, but they\'re somewhat disparate; they don\'t give us a clear insight as to who uses our platform and how they do so. The information is there, but it\'s tough to grok. It doesn\'t make for a joined up story.\\n\\nReaching around for how to solve this I had an idea: what if our platform had a newsfeed? The kind of thing that social media platforms the likes of Twitter and Facebook have used to great effect; a stream of mini-activities which show how the community interacts with the product. People logging in and browsing around, using features on the platform. If we could see this in near real time we\'d be brought closer to our users; we\'d have something that would help us have real empathy and understanding. We\'d see our product as the stories of users interacting with it.\\n\\n## How do you build a newsfeed?\\n\\nThis was an experiment that seemed worth pursuing. So I decided to build a proof of concept and see what happened. Now I intended to put the \\"M\\" into MVP with this; I went in with a number of intentional constraints:\\n\\n1. The news feed wouldn\'t auto update (users have the F5 key for that)\\n2. We\'d host the newsfeed in our own mini analytics platform (which is already used by the team to understand how people use the platform)\\n3. News stories wouldn\'t be stored anywhere; we\'d generate them on the fly by querying various databases / APIs. The cost of this would be that our news stories wouldn\'t be \\"persistent\\"; you wouldn\'t be able to address them with a URL; there\'d be no way to build \\"like\\" or \\"share\\" functionality.\\n\\nAll of the above constraints are, importantly, reversable decisions. If we want auto update it could be built later. If we want the newsfeed to live somewhere else we could move it. If we wanted news stories to be persisted then we could do that.\\n\\n## Implementation\\n\\nWith these constraints in mind, I turned my attention to the implementation. I built a `NewsFeedService` that would be queried for news stories. The interface I decided to build looked like this:\\n\\n```\\nNewsFeedService.getNewsFeed(from: Date, to: Date): NewsFeed\\n\\ntype NewsFeed {\\n    startedAt: Date;\\n    ended at: Date;\\n    stories: NewsStory[];\\n}\\n\\ntype NewsStory {\\n    /** When the story happened */\\n    happenedAt: Date;\\n    /** A code that represents the type of story this is; eg USER_SESSION */\\n    storyCode: string\\n    /** The story details in markdown format */\\n    story: string;\\n}\\n```\\n\\nEach query to `NewsFeedService.getNewsFeed` would query various databases / APIs related to our product, looking for interesting events. Whether it be users logging in, users performing some kind of action, whatever. For each interested event a news story like this would be produced:\\n\\n> Jane Smith logged in at 10:03am for 25 minutes. They placed [an order](https://my-glorious-platform.io/orders/janes-order) worth \xa33,000.\\n\\nNow the killer feature here is [Markdown](https://en.wikipedia.org/wiki/Markdown#:~:text=Markdown%20is%20a%20lightweight%20markup,using%20a%20plain%20text%20editor.). Our stories are written in Markdown. Why is Markdown cool? Well [to quote the creators of Markdown](https://web.archive.org/web/20040402182332/http://daringfireball.net/projects/markdown/):\\n\\n> Markdown allows you to write using an easy-to-read, easy-to-write plain text format, then convert it to structurally valid XHTML (or HTML).\\n\\nThis crucially includes the ability to include links. This was significant because I want us to be able to be able to click on pieces of information in the stories and be taken to the relevant place in the platform to see more details. Just as you see status updates on, for example, Twitter which lead you on to more details:\\n\\n> This is the history of [@DefinitelyTyped](https://twitter.com/DefinitelyTyped?ref_src=twsrc%5Etfw): [https://t.co/AY6s3bWnKP](https://t.co/AY6s3bWnKP) Thanks to [@SeaRyanC](https://twitter.com/SeaRyanC?ref_src=twsrc%5Etfw) & [@drosenwasser](https://twitter.com/drosenwasser?ref_src=twsrc%5Etfw) of the [@typescript](https://twitter.com/typescript?ref_src=twsrc%5Etfw) team, [@blakeembrey](https://twitter.com/blakeembrey?ref_src=twsrc%5Etfw) inventor of typings, [@vvakame](https://twitter.com/vvakame?ref_src=twsrc%5Etfw), [@\\\\_stevefenton](https://twitter.com/_stevefenton?ref_src=twsrc%5Etfw), [@basarat](https://twitter.com/basarat?ref_src=twsrc%5Etfw), and of course [@borisyankov](https://twitter.com/borisyankov?ref_src=twsrc%5Etfw) for telling me their parts of the story\u2764\ufe0f\ud83c\udf3b\\n>\\n> \u2014 John Reilly (@johnny_reilly) [October 8, 2019](https://twitter.com/johnny_reilly/status/1181542739994976256?ref_src=twsrc%5Etfw)\\n\\n<script async=\\"\\" src=\\"https://platform.twitter.com/widgets.js\\" charSet=\\"utf-8\\"><\/script>\\n\\nAgain consider this example news story:\\n\\n> Jane Smith logged in at 10:03am for 25 minutes. They placed [an order](https://my-glorious-platform.io/orders/janes-order) worth \xa33,000.\\n\\nConsider that story but without a link. It\'s not the same is it? A newsfeed without links would be missing a trick. Markdown gives us links. And happily due to my extensive work down the open source mines, I speak it like a native.\\n\\nThe first consumer of the newsfeed was to be our own mini analytics platform, which is a React app. Converting the markdown stories to React is a solved problem thanks to the wonderful [react-markdown](https://github.com/rexxars/react-markdown). You can simply sling Markdown at it and out comes HTML. Et voil\xe0 a news feed!\\n\\n## What\'s next?\\n\\nSo that\'s it! We\'ve built a (primitive) news feed. We can now see in real time how are users are getting on. We\'re closer to them, we understand them better as a consequence. If we want to take it further there\'s a number of things we could do:\\n\\n1. We could make the feed auto-update\\n2. We could push news stories to other destinations. Markdown is a gloriously portable format which can be used in a variety of environments. For instance the likes of Slack and [Teams](./2019-12-18-teams-notification-webhooks/index.md) accept it and apps like these are generally open on people\'s desktops and phones all the time anyway. Another way to narrow the gap between us and and our users.\\n\\nIt\'s very exciting!"},{"id":"/2020/08/09/devcontainers-aka-performance-in-secure","metadata":{"permalink":"/2020/08/09/devcontainers-aka-performance-in-secure","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2020-08-09-devcontainers-aka-performance-in-secure/index.md","source":"@site/blog/2020-08-09-devcontainers-aka-performance-in-secure/index.md","title":"Devcontainers AKA performance in a secure sandbox","description":"Many corporate machines arrive in engineers hands with a preponderance of pre-installed background tools; from virus checkers to backup utilities to port blockers; the list is long.","date":"2020-08-09T00:00:00.000Z","formattedDate":"August 9, 2020","tags":[{"label":"git clone","permalink":"/tags/git-clone"},{"label":"devcontainer","permalink":"/tags/devcontainer"},{"label":"performance","permalink":"/tags/performance"},{"label":"SSH","permalink":"/tags/ssh"}],"readingTime":6.54,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Devcontainers AKA performance in a secure sandbox","authors":"johnnyreilly","tags":["git clone","devcontainer","performance","SSH"],"hide_table_of_contents":false},"prevItem":{"title":"Why your team needs a newsfeed","permalink":"/2020/09/04/why-your-team-needs-newsfeed"},"nextItem":{"title":"Devcontainers and SSL interception","permalink":"/2020/07/11/devcontainers-and-ssl-interception"}},"content":"Many corporate machines arrive in engineers hands with a preponderance of pre-installed background tools; from virus checkers to backup utilities to port blockers; the list is long.\\n\\nThe reason that these tools are installed is generally noble. However, the implementation can often be problematic. The tools may be set up in such a way as they impact and interfere with one another. Really powerful machines with 8 CPUs and hardy SSDs can be slowed to a crawl. Put simply: the good people responsible for ensuring security are rarely encouraged to incentivise performance alongside it. And so don\'t.\\n\\nThe unfortunate consequence of considering the role of security without regard to performance is this: sluggish computers. The further consequence (and this is the one I want you to think long and hard about) is _low developer productivity_. And that sucks. It impacts what an organisation is able to do, how fast an organisation is able to move. Put simply: it can be the difference between success and failure.\\n\\nThe most secure computer is off. But you won\'t ship much with it. Encouraging your organisation to consider tackling security with performance in mind is worthwhile. It\'s a long game though. In the meantime what can we do?\\n\\n## \\"Hide from the virus checkers\\\\*\\\\*\\\\* in a devcontainer\\"\\n\\nDevcontainers, the infrastructure as code equivalent for developing software, have an underappreciated quality: unlocking your machine\'s performance.\\n\\nDevcontainers are isolated secure sandboxes in which you can build software. To quote the [docs](https://code.visualstudio.com/docs/remote/containers):\\n\\n> A `devcontainer.json` file in your project tells VS Code how to access (or create) a development container with a well-defined tool and runtime stack. This container can be used to run an application or to sandbox tools, libraries, or runtimes needed for working with a codebase.\\n>\\n> Workspace files are mounted from the local file system or copied or _cloned into the container_.\\n\\nWe\'re going to set up a devcontainer to code an ASP.NET Core application with a JavaScript (well TypeScript) front end. If there\'s one thing that\'s sure to catch a virus checkers beady eye, it\'s `node_modules`. `node_modules` contains more files than a black hole has mass. Consider a project with 5,000 source files. One trusty `yarn` later and the folder now has a tidy 250,000 files. The virus checker is now really sitting up and taking notice.\\n\\nOur project has a `git commit` hook set up with [Husky](https://github.com/typicode/husky) that formats our TypeScript files with [Prettier](https://prettier.io/). Every commit the files are formatted to align with the project standard. With all the virus checkers in place a `git commit` takes around 45 seconds. Inside a devcontainer we can drop this to 5 seconds. That\'s nine times faster. I\'ll repeat that: that\'s **nine times faster**!\\n\\nThe \\"cloned into a container\\" above is key to what we\'re going to do. We\'re _not_ going to mount our local file system into the devcontainer. Oh no. We\'re going to build a devcontainer with ASP.NET CORE and JavaScript in. Then, inside there, we\'re going to clone our repo. Then we can develop, build and debug all inside the container. It will feel like we\'re working on our own machine because VS Code does such a damn fine job. In reality, we\'re connecting to another computer (a Linux computer to boot) that is running in isolation to our own. In our case that machine is sharing our hardware; but that\'s just an implementation detail. It could be anywhere (and in the future may well be).\\n\\n## Make me a devcontainer...\\n\\nEnough talk... We\'re going to need a `.devcontainer/devcontainer.json`:\\n\\n```json\\n{\\n  \\"name\\": \\"my devcontainer\\",\\n  \\"dockerComposeFile\\": \\"../docker-compose.devcontainer.yml\\",\\n  \\"service\\": \\"my-devcontainer\\",\\n  \\"workspaceFolder\\": \\"/workspace\\",\\n\\n  // Set *default* container specific settings.json values on container create.\\n  \\"settings\\": {\\n    \\"terminal.integrated.shell.linux\\": \\"/bin/zsh\\"\\n  },\\n\\n  // Add the IDs of extensions you want installed when the container is created.\\n  \\"extensions\\": [\\n    \\"ms-dotnettools.csharp\\",\\n    \\"dbaeumer.vscode-eslint\\",\\n    \\"esbenp.prettier-vscode\\",\\n    \\"ms-mssql.mssql\\",\\n    \\"eamodio.gitlens\\",\\n    \\"ms-azuretools.vscode-docker\\",\\n    \\"k--kato.docomment\\",\\n    \\"Leopotam.csharpfixformat\\"\\n  ],\\n\\n  // Use \'postCreateCommand\' to clone the repo into the workspace folder when the devcontainer starts\\n  // and copy in the .env file\\n  \\"postCreateCommand\\": \\"git clone git@github.com:my-org/my-repo.git . && cp /.env /workspace/.env\\"\\n\\n  // \\"remoteUser\\": \\"vscode\\"\\n}\\n```\\n\\nNow the `docker-compose.devcontainer.yml` which lives in the root of the project. It provisions a SQL Server container (using the official image) and our devcontainer:\\n\\n```\\nversion: \\"3.7\\"\\nservices:\\n  my-devcontainer:\\n    image: my-devcontainer\\n    build:\\n      context: .\\n      dockerfile: Dockerfile.devcontainer\\n    command: /bin/zsh -c \\"while sleep 1000; do :; done\\"\\n    volumes:\\n      # mount .zshrc from home - make sure it doesn\'t contain Windows line endings\\n      - ~/.zshrc:/root/.zshrc\\n\\n    # user: vscode\\n    ports:\\n      - \\"5000:5000\\"\\n      - \\"8080:8080\\"\\n    environment:\\n      - CONNECTIONSTRINGS__MYDATABASECONNECTION\\n    depends_on:\\n      - db\\n  db:\\n    image: mcr.microsoft.com/mssql/server:2019-latest\\n    privileged: true\\n    ports:\\n      - 1433:1433\\n    environment:\\n      SA_PASSWORD: \\"Your_password123\\"\\n      ACCEPT_EULA: \\"Y\\"\\n```\\n\\nThe devcontainer will be built with the `Dockerfile.devcontainer` in the root of our repo. It relies upon your SSH keys and a `.env` file being available to be copied in:\\n\\n```\\n#-----------------------------------------------------------------------------------------------------------\\n# Based upon: https://github.com/microsoft/vscode-dev-containers/tree/master/containers/dotnetcore\\n#-----------------------------------------------------------------------------------------------------------\\nARG VARIANT=\\"3.1-bionic\\"\\nFROM mcr.microsoft.com/dotnet/core/sdk:${VARIANT}\\n\\n# Because MITM certificates\\nCOPY ./docker/certs/. /usr/local/share/ca-certificates/\\nENV NODE_EXTRA_CA_CERTS=/usr/local/share/ca-certificates/mitm.pem\\nRUN update-ca-certificates\\n\\n# This Dockerfile adds a non-root user with sudo access. Use the \\"remoteUser\\"\\n# property in devcontainer.json to use it. On Linux, the container user\'s GID/UIDs\\n# will be updated to match your local UID/GID (when using the dockerFile property).\\n# See https://aka.ms/vscode-remote/containers/non-root-user for details.\\nARG USERNAME=vscode\\nARG USER_UID=1000\\nARG USER_GID=$USER_UID\\n\\n# Options for common package install script\\nARG INSTALL_ZSH=\\"true\\"\\nARG UPGRADE_PACKAGES=\\"true\\"\\nARG COMMON_SCRIPT_SOURCE=\\"https://raw.githubusercontent.com/microsoft/vscode-dev-containers/master/script-library/common-debian.sh\\"\\nARG COMMON_SCRIPT_SHA=\\"dev-mode\\"\\n\\n# Settings for installing Node.js.\\nARG INSTALL_NODE=\\"true\\"\\nARG NODE_SCRIPT_SOURCE=\\"https://raw.githubusercontent.com/microsoft/vscode-dev-containers/master/script-library/node-debian.sh\\"\\nARG NODE_SCRIPT_SHA=\\"dev-mode\\"\\n\\n# ARG NODE_VERSION=\\"lts/*\\"\\nARG NODE_VERSION=\\"14\\"\\nENV NVM_DIR=/usr/local/share/nvm\\n\\n# Have nvm create a \\"current\\" symlink and add to path to work around https://github.com/microsoft/vscode-remote-release/issues/3224\\nENV NVM_SYMLINK_CURRENT=true\\nENV PATH=${NVM_DIR}/current/bin:${PATH}\\n\\n# Configure apt and install packages\\nRUN apt-get update \\\\\\n    && export DEBIAN_FRONTEND=noninteractive \\\\\\n    #\\n    # Verify git, common tools / libs installed, add/modify non-root user, optionally install zsh\\n    && apt-get -y install --no-install-recommends curl ca-certificates 2>&1 \\\\\\n    && curl -sSL ${COMMON_SCRIPT_SOURCE} -o /tmp/common-setup.sh \\\\\\n    && ([ \\"${COMMON_SCRIPT_SHA}\\" = \\"dev-mode\\" ] || (echo \\"${COMMON_SCRIPT_SHA} */tmp/common-setup.sh\\" | sha256sum -c -)) \\\\\\n    && /bin/bash /tmp/common-setup.sh \\"${INSTALL_ZSH}\\" \\"${USERNAME}\\" \\"${USER_UID}\\" \\"${USER_GID}\\" \\"${UPGRADE_PACKAGES}\\" \\\\\\n    #\\n    # Install Node.js\\n    && curl -sSL ${NODE_SCRIPT_SOURCE} -o /tmp/node-setup.sh \\\\\\n    && ([ \\"${NODE_SCRIPT_SHA}\\" = \\"dev-mode\\" ] || (echo \\"${COMMON_SCRIPT_SHA} */tmp/node-setup.sh\\" | sha256sum -c -)) \\\\\\n    && /bin/bash /tmp/node-setup.sh \\"${NVM_DIR}\\" \\"${NODE_VERSION}\\" \\"${USERNAME}\\" \\\\\\n    #\\n    # Clean up\\n    && apt-get autoremove -y \\\\\\n    && apt-get clean -y \\\\\\n    && rm -f /tmp/common-setup.sh /tmp/node-setup.sh \\\\\\n    && rm -rf /var/lib/apt/lists/* \\\\\\n    #\\n    # Workspace\\n    && mkdir workspace \\\\\\n    && chown -R ${NONROOT_USER}:root workspace\\n\\n\\n# Install Vim\\nRUN apt-get update && apt-get install -y \\\\\\n    vim \\\\\\n    && rm -rf /var/lib/apt/lists/*\\n\\n# Set up a timezone in the devcontainer - necessary for anything timezone dependent\\nENV TZ=Europe/London\\nRUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone \\\\\\n && apt-get update \\\\\\n && apt-get install --no-install-recommends -y \\\\\\n    apt-utils \\\\\\n    tzdata  \\\\\\n && apt-get autoremove -y \\\\\\n && apt-get clean -y \\\\\\n && rm -rf /var/lib/apt/lists/*\\n\\nENV DOTNET_RUNNING_IN_CONTAINER=true\\n\\n# Copy across SSH keys so you can git clone\\nRUN mkdir /root/.ssh\\nRUN chmod 700 /root/.ssh\\n\\nCOPY .ssh/id_rsa /root/.ssh\\nRUN chmod 600 /root/.ssh/id_rsa\\n\\nCOPY .ssh/id_rsa.pub /root/.ssh\\nRUN chmod 644 /root/.ssh/id_rsa.pub\\n\\nCOPY .ssh/known_hosts /root/.ssh\\nRUN chmod 644 /root/.ssh/known_hosts\\n\\n# Disable initial git clone prompt\\nRUN echo \\"StrictHostKeyChecking no\\" >> /etc/ssh/ssh_config\\n\\n# Copy across .env file so you can customise environment variables\\n# This will be copied into the root of the repo post git clone\\nCOPY .env /.env\\nRUN chmod 644 /.env\\n\\n# Install dotnet entity framework tools\\nRUN dotnet tool install dotnet-ef --tool-path /usr/local/bin --version 3.1.2\\n```\\n\\nWith this devcontainer you\'re good to go for an ASP.NET Core / JavaScript developer setup that is blazing fast! Remember to fire up Docker and give it goodly access to the resources of your host machine. All the CPUs, lots of memory and all the performance that there ought to be.\\n\\n_\\\\* \\"virus checkers\\" is a euphemism here for all the background tools that may be running. It was that or calling them \\"we are legion\\"_"},{"id":"/2020/07/11/devcontainers-and-ssl-interception","metadata":{"permalink":"/2020/07/11/devcontainers-and-ssl-interception","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2020-07-11-devcontainers-and-ssl-interception/index.md","source":"@site/blog/2020-07-11-devcontainers-and-ssl-interception/index.md","title":"Devcontainers and SSL interception","description":"Devcontainers are cool. They are the infrastructure as code equivalent for developing software.","date":"2020-07-11T00:00:00.000Z","formattedDate":"July 11, 2020","tags":[{"label":"devcontainer","permalink":"/tags/devcontainer"},{"label":"mitm certificate","permalink":"/tags/mitm-certificate"},{"label":"ssl interception","permalink":"/tags/ssl-interception"}],"readingTime":3.22,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Devcontainers and SSL interception","authors":"johnnyreilly","tags":["devcontainer","mitm certificate","ssl interception"],"hide_table_of_contents":false},"prevItem":{"title":"Devcontainers AKA performance in a secure sandbox","permalink":"/2020/08/09/devcontainers-aka-performance-in-secure"},"nextItem":{"title":"Task.WhenAll / Select is a footgun \ud83d\udc5f\ud83d\udd2b","permalink":"/2020/06/21/taskwhenall-select-is-footgun"}},"content":"[Devcontainers](https://code.visualstudio.com/docs/remote/containers) are cool. They are the infrastructure as code equivalent for developing software.\\n\\nImagine your new starter joins the team, you\'d like them to be contributing code on _day 1_. But if the first thing that happens is you hand them a sheaf of paper upon which are the instructions for how to get their machines set up for development, well, maybe it\'s going to be a while. But if your project has a devcontainer then you\'re off to the races. One trusty `git clone`, fire up VS Code and they can get going.\\n\\nThat\'s the dream right?\\n\\nI\'ve recently been doing some work getting a project I work on set up with a devcontainer. As I\'ve worked on that I\'ve become aware of some of the hurdles that might hamper your adoption of devcontainers in a corporate environment.\\n\\n## Certificates: I\'m starting with the man in the middle\\n\\nIt is a common practice in company networks to perform [SSL interception](https://docs.citrix.com/en-us/citrix-adc/13/forward-proxy/ssl-interception.html). Not SSL inception; that\'d be more fun.\\n\\n <iframe src=\\"https://giphy.com/embed/l7JDTHpsXM26k\\" width=\\"100%\\" height=\\"100%\\" frameBorder=\\"0\\" allowFullScreen=\\"\\"></iframe>\\n\\nSSL interception is the practice of installing a \\"man-in-the-middle\\" (MITM) CA certificate on users machines. When SSL traffic takes place from a users machine, it goes through a proxy. That proxy performs the SSL on behalf of that user and, if it\'s happy, supplies another certificate back to the users machine which satisfies the MITM CA certificate. So rather than seeing, for example, Google\'s certificate from [https://google.com](https://google.com) you\'d see the one resulting from the SSL interception. You can read more [here](https://security.stackexchange.com/questions/107542/is-it-common-practice-for-companies-to-mitm-https-traffic).\\n\\nNow this is a little known and less understood practice. I barely understand it myself. Certificates are _hard_. Even having read the above you may be none the wiser about why this is relevant. Let\'s get to the broken stuff.\\n\\n## \\"Devcontainers don\'t work at work!\\"\\n\\nSo, you\'re ready to get going with your first devcontainer. You fire up the [vscode-dev-containers](https://github.com/Microsoft/vscode-dev-containers) repo and find the container that\'s going to work for you. Copy pasta the `.devcontainer` into your repo, install the [Remote Development](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.vscode-remote-extensionpack) extension into VS Code and enter the `Remote-Containers: Reopen Folder in Container`. Here comes the future!\\n\\nBut when it comes to performing SSL inside the devcontainer, trouble awaits. Here\'s what a `yarn install` results in:\\n\\n```\\nyarn install v1.22.4\\n[1/4] Resolving packages...\\n[2/4] Fetching packages...\\nerror An unexpected error occurred: \\"https://registry.yarnpkg.com/@octokit/core/-/core-2.5.0.tgz: self signed certificate in certificate chain\\".\\n```\\n\\nOh no!\\n\\nGosh but it\'s okay - you\'re just bumping on the SSL interception. Why though? Well it\'s like this: when you fire up your devcontainer it builds a new Docker container. It\'s as well to imagine the container as a virtual operating system. So what\'s the difference between this operating system and the one our machine is running? Well a number of things, but crucially our host operating system has the MITM CA certificate installed. So when we SSL, we have the certificate that will match up with what the proxy sends back to us certificate-wise. And inside our trusty devcontainer we don\'t have that. Hence the sadness.\\n\\n## Devcontainer + MITM cert = working\\n\\nWe need to do two things to get this working:\\n\\n1. Acquire the requisite CA certificate(s) from your friendly neighbourhood networking team. Place them in a `certs` folder inside your repo, in the `.devcontainer` folder.\\n2. Add the following lines to your `.devcontainer/Dockerfile`, just after the initial `FROM` statement:\\n\\n```\\n# Because MITM certificates\\nCOPY certs/. /usr/local/share/ca-certificates/\\nENV NODE_EXTRA_CA_CERTS=/usr/local/share/ca-certificates/mitm.pem\\nRUN update-ca-certificates\\n```\\n\\nWhich does the following:\\n\\n- Copies the certs into the devcontainer\\n- This is a Node example and so we set an environment variable called [`NODE_EXTRA_CA_CERTS`](https://nodejs.org/api/cli.html#cli_node_extra_ca_certs_file) which points to the path of your MITM CA certificate file inside your devcontainer.\\n- updates the directory `/etc/ssl/certs` to hold SSL certificates and generates `ca-certificates.crt`\\n\\nWith these in place then you should be able to build your devcontainer with no SSL trauma. Enjoy!"},{"id":"/2020/06/21/taskwhenall-select-is-footgun","metadata":{"permalink":"/2020/06/21/taskwhenall-select-is-footgun","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2020-06-21-taskwhenall-select-is-footgun/index.md","source":"@site/blog/2020-06-21-taskwhenall-select-is-footgun/index.md","title":"Task.WhenAll / Select is a footgun \ud83d\udc5f\ud83d\udd2b","description":"This post differs from my typical fayre. Most often I write \\"here\'s how to do a thing\\". This is not that. It\'s more \\"don\'t do this thing I did\\". And maybe also, \\"how can we avoid a situation like this happening again in future?\\". On this topic I very much don\'t have all the answers - but by putting my thoughts down maybe I\'ll learn and maybe others will educate me. I would love that!","date":"2020-06-21T00:00:00.000Z","formattedDate":"June 21, 2020","tags":[{"label":"CSharp","permalink":"/tags/c-sharp"},{"label":"LINQ","permalink":"/tags/linq"},{"label":"Task.WhenAll","permalink":"/tags/task-when-all"},{"label":"Select","permalink":"/tags/select"}],"readingTime":5.97,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Task.WhenAll / Select is a footgun \ud83d\udc5f\ud83d\udd2b","authors":"johnnyreilly","tags":["CSharp","LINQ","Task.WhenAll","Select"],"hide_table_of_contents":false},"prevItem":{"title":"Devcontainers and SSL interception","permalink":"/2020/07/11/devcontainers-and-ssl-interception"},"nextItem":{"title":"Autofac, WebApplicationFactory and integration tests","permalink":"/2020/05/21/autofac-webapplicationfactory-integration-tests"}},"content":"This post differs from my typical fayre. Most often I write \\"here\'s how to do a thing\\". This is not that. It\'s more \\"don\'t do this thing I did\\". And maybe also, \\"how can we avoid a situation like this happening again in future?\\". On this topic I very much don\'t have all the answers - but by putting my thoughts down maybe I\'ll learn and maybe others will educate me. I would love that!\\n\\n## Doing things that don\'t scale\\n\\nThe platform that I work on once had zero users. We used to beg people to log in and see what we had built. Those days are (happily) but a memory. We\'re getting popular.\\n\\nAs our platform has grown in popularity it has revealed some bad choices we made. Approaches that look fine on the surface (and that work just dandy when you have no users) may start to cause problems as your number of users grows.\\n\\nI wanted to draw attention to one approach in particular that impacted us severely. In this case \\"impacted us severely\\" is a euphemism for \\"brought the site down and caused a critical incident\\".\\n\\nYou don\'t want this to happen to you. Trust me. So, what follows is a cautionary tale. The purpose of which is simply this: reader, do you have code of this ilk in your codebase? If you do: out, damn\'d spot! out, I say!\\n\\n## So cool, so terrible\\n\\nI love LINQ. I love a declarative / functional style of coding. It appeals to me on some gut level. I find it tremendously readable. Read any C# of mine and the odds are pretty good that you\'ll find some LINQ in the mix.\\n\\nImagine this scenario: you have a collection of user ids. You want to load the details of each user represented by their id from an API. You want to bag up all of those users into some kind of collection and send it back to the calling code.\\n\\nReading that, if you\'re like me, you\'re imagining some kind of map operation which loads the user details for each user id. Something like this:\\n\\n```cs\\nvar users = userIds.Select(userId => GetUserDetails(userId)).ToArray(); // users is User[]\\n```\\n\\nLovely. But you\'ll note that I\'m loading users from an API. Oftentimes, APIs are asynchronous. Certainly, in my case they were. So rather than calling a `GetUserDetails` function I found myself calling a `GetUserDetailsAsync` function, behind which an HTTP request is being sent and, later, a response is being returned.\\n\\nSo how do we deal with this? [`Task.WhenAll`](https://docs.microsoft.com/en-us/dotnet/api/system.threading.tasks.task.whenall?view=netcore-3.1#System_Threading_Tasks_Task_WhenAll__1_System_Collections_Generic_IEnumerable_System_Threading_Tasks_Task___0___) my friends!\\n\\n```cs\\nvar userTasks = userIds.Select(userId => GetUserDetailsAsync(userId));\\nvar users = await Task.WhenAll(tasks); // users is User[]\\n```\\n\\nIt worked great! Right up until to the point where it didn\'t. These sorts of shenanigans were fine when we had a minimal number of users... But there came a point where problems arose. It got to the point where that simple looking mapping operation became a cause of many, many, _many_ HTTP requests being fired concurrently. Then bad things started to happen. Not only did we realise we were launching a denial of service attack on the API we were consuming, we were bringing our own application to collapse.\\n\\nNot a proud day.\\n\\n## What is the problem?\\n\\nThrough log analysis, code reading and speculation, (with the help of the invaluable [Robski](https://www.linkedin.com/in/robert-grzankowski-53618114)) we came to realise that the cause of our woes was the `Task.WhenAll` / `Select` combination. Exercising that codepath was a surefire way to bring the application to its knees.\\n\\nAs I read around on the topic I happened upon [Mark Heath](https://www.twitter.com/mark_heath)\'s excellent list of [Async antipatterns](https://markheath.net/post/async-antipatterns). Number #6 on the list is \\"Excessive parallelization\\". It describes a nearly identical scenario to my own:\\n\\n> Now, this does \\"work\\", but what if there were 10,000 orders? We\'ve flooded the thread pool with thousands of tasks, potentially preventing other useful work from completing. If `ProcessOrderAsync` makes downstream calls to another service like a database or a microservice, we\'ll potentially overload that with too high a volume of calls.\\n\\nWe\'re definitely overloading the API we\'re consuming with too high a volume of calls. I have to admit that I\'m less clear on the direct reason that a `Task.WhenAll` / `Select` combination could prove fatal to our application. Mark suggests this approach will flood the thread pool with tasks. As I read around on `async` and `await` it\'s repeated again and again that a `Task` is not the same thing as a `Thread`. I have to hold my hands up here and say that I don\'t understand the implementation of `async` / `await` in C# well enough. [These docs are helpful but I still don\'t think the penny has fully dropped for me yet.](https://docs.microsoft.com/en-us/dotnet/standard/async-in-depth#deeper-dive-into-tasks-for-an-io-bound-operation) I will continue to read.\\n\\nOne thing we learned as we debugged the production k8s pod was that, prior to its collapse, our app appeared to be opening up 1 million connections to the API we were consuming. Which seemed a bit much. Worthy of investigation. It\'s worth saying that we\'re not certain this is exactly what is happening; we have less instrumentation in place than we\'d like. But some fancy wc grepping on Robski\'s behalf suggested this was the case.\\n\\n## What will we change in future?\\n\\nA learning that came out of this for us was this: we need more metrics exposed. We don\'t understand our application\'s behaviour under load as well as we\'d like. So we\'re planning to do some work with [App Metrics](https://www.app-metrics.io/) and [Grafana](https://grafana.com/) so we\'ve a better idea of how our application performs. If you want to improve something, first measure it.\\n\\nAnother fly in the ointment was that we were unable to reproduce the issue when running locally. It\'s worth saying here that I develop on a Windows machine and, when deployed, our application runs in a (Linux) Docker container. So there\'s a difference and a distance between our development experience and our running one.\\n\\nI\'m planning to migrate to developing in a [devcontainer](https://code.visualstudio.com/docs/remote/containers) where that\'s possible. That should narrow the gap between our production experience and our development one. Reducing the difference between the two is always useful as it means you\'re less likely to get different behaviour (ie \\"problems\\") in production as compared to development. I\'m curious as to whether I\'ll be able to replicate that behaviour in a devcontainer.\\n\\n## What did we do right now?\\n\\nTo solve the immediate issue we were able to pivot away to a completely different approach. We moved aggregation from our ASP.NET Core web application to our TypeScript / React client with a (pretty sweet) custom hook. The topic for a subsequent blog post.\\n\\nMoving to a different approach solved my immediate issue. But it left me puzzling. What was actually going wrong? Is it thread pool exhaustion? Is it something else? So many possibilities!\\n\\nIf anyone has any insights they\'d like to share that would be incredible! I\'ve also [asked a question on Stack Overflow](https://stackoverflow.com/questions/62490098/task-whenall-with-select-is-a-footgun-but-why/62490705) which has kindly had answers from generous souls. [James Skimming](https://twitter.com/jamesskimming)\'s answer lead me to [Steve Gordon\'s excellent post on connection pooling](https://www.stevejgordon.co.uk/httpclient-connection-pooling-in-dotnet-core) which I\'m still absorbing and seems like it could be relevant."},{"id":"/2020/05/21/autofac-webapplicationfactory-integration-tests","metadata":{"permalink":"/2020/05/21/autofac-webapplicationfactory-integration-tests","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2020-05-21-autofac-webapplicationfactory-integration-tests/index.md","source":"@site/blog/2020-05-21-autofac-webapplicationfactory-integration-tests/index.md","title":"Autofac, WebApplicationFactory and integration tests","description":"Updated 2nd Oct 2020: for an approach that works with Autofac 6 and ConfigureTestContainer see this post.","date":"2020-05-21T00:00:00.000Z","formattedDate":"May 21, 2020","tags":[{"label":"autofac","permalink":"/tags/autofac"},{"label":"WebApplicationFactory","permalink":"/tags/web-application-factory"},{"label":"ASP.Net Core","permalink":"/tags/asp-net-core"},{"label":"ConfigureTestContainer","permalink":"/tags/configure-test-container"},{"label":"Integration Testing","permalink":"/tags/integration-testing"}],"readingTime":3.56,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Autofac, WebApplicationFactory and integration tests","authors":"johnnyreilly","tags":["autofac","WebApplicationFactory","ASP.Net Core","ConfigureTestContainer","Integration Testing"],"image":"./autofac-webapplicationfactory-tests.png","hide_table_of_contents":false},"prevItem":{"title":"Task.WhenAll / Select is a footgun \ud83d\udc5f\ud83d\udd2b","permalink":"/2020/06/21/taskwhenall-select-is-footgun"},"nextItem":{"title":"From react-window to react-virtual","permalink":"/2020/05/10/from-react-window-to-react-virtual"}},"content":"**Updated 2nd Oct 2020:** _for an approach that works with Autofac 6 and `ConfigureTestContainer` see [this post](./2020-10-02-autofac-6-integration-tests-and-generic-hosting/index.md)._\\n\\n![A title image for the blog featuring the Autofac logo](autofac-webapplicationfactory-tests.png)\\n\\nThis is one of those occasions where I\'m not writing up my own work so much as my discovery after in depth googling.\\n\\nIntegration tests with ASP.NET Core are the best. They spin up an in memory version of your application and let you fire requests at it. They\'ve gone through a number of iterations since ASP.NET Core has been around. You may also be familiar with the `TestServer` approach of earlier versions. For some time, the advised approach has been using [`WebApplicationFactory`](https://docs.microsoft.com/en-us/aspnet/core/test/integration-tests?view=aspnetcore-3.1#basic-tests-with-the-default-webapplicationfactory).\\n\\nWhat makes this approach particularly useful / powerful is that you can swap out dependencies of your running app with fakes / stubs etc. Just like unit tests! But potentially more useful because they run your whole app and hence give you a greater degree of confidence. What does this mean? Well, imagine you changed a piece of middleware in your application; this could potentially break functionality. Unit tests would probably not reveal this. Integration tests would.\\n\\nThere is a fly in the ointment. A hair in the gazpacho. ASP.NET Core ships with dependency injection in the box. It has its own Inversion of Control container which is perfectly fine. However, many people are accustomed to using other IOC containers such as [Autofac](https://autofac.org/).\\n\\nWhat\'s the problem? Well, swapping out dependencies registered using ASP.NET Core\'s IOC requires using a hook called [`ConfigureTestServices`](https://docs.microsoft.com/en-us/aspnet/core/test/integration-tests?view=aspnetcore-3.1#inject-mock-services). There\'s an equivalent hook for swapping out services registered using a custom IOC container: [`ConfigureTestContainer`](https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.testhost.webhostbuilderextensions.configuretestcontainer?view=aspnetcore-3.0). Unfortunately, there is a bug in ASP.NET Core as of version 3.0: [When using GenericHost, in tests `ConfigureTestContainer` is not executed](https://github.com/dotnet/aspnetcore/issues/14907)\\n\\nThis means you cannot swap out dependencies that have been registered with Autofac and the like. According to the tremendous [David Fowler](https://www.twitter.com/davidfowl) of the ASP.NET team, [this will hopefully be resolved](https://github.com/dotnet/aspnetcore/issues/14907#issuecomment-592102145).\\n\\nIn the meantime, [there\'s a workaround thanks to various commenters on the thread](https://github.com/dotnet/aspnetcore/issues/14907#issuecomment-620750841). Instead of using `WebApplicationFactory` directly, subclass it and create a custom `AutofacWebApplicationFactory` (the name is not important). This custom class overrides the behavior of `ConfigureServices` and `CreateHost` with a `CustomServiceProviderFactory`:\\n\\n```cs\\nnamespace My.Web.Tests.Helpers {\\n    /// <summary>\\n    /// Based upon https://github.com/dotnet/AspNetCore.Docs/tree/master/aspnetcore/test/integration-tests/samples/3.x/IntegrationTestsSample\\n    /// </summary>\\n    /// <typeparam name=\\"TStartup\\"></typeparam>\\n    public class AutofacWebApplicationFactory<TStartup> : WebApplicationFactory<TStartup> where TStartup : class {\\n        protected override void ConfigureWebHost(IWebHostBuilder builder) {\\n            builder.ConfigureServices(services => {\\n                    services.AddSingleton<IAuthorizationHandler>(new PassThroughPermissionedRolesHandler());\\n                })\\n                .ConfigureTestServices(services => {\\n                }).ConfigureTestContainer<Autofac.ContainerBuilder>(builder => {\\n                    // called after Startup.ConfigureContainer\\n                });\\n        }\\n\\n        protected override IHost CreateHost(IHostBuilder builder) {\\n            builder.UseServiceProviderFactory(new CustomServiceProviderFactory());\\n            return base.CreateHost(builder);\\n        }\\n    }\\n\\n    /// <summary>\\n    /// Based upon https://github.com/dotnet/aspnetcore/issues/14907#issuecomment-620750841 - only necessary because of an issue in ASP.NET Core\\n    /// </summary>\\n    public class CustomServiceProviderFactory : IServiceProviderFactory<CustomContainerBuilder> {\\n        public CustomContainerBuilder CreateBuilder(IServiceCollection services) => new CustomContainerBuilder(services);\\n\\n        public IServiceProvider CreateServiceProvider(CustomContainerBuilder containerBuilder) =>\\n        new AutofacServiceProvider(containerBuilder.CustomBuild());\\n    }\\n\\n    public class CustomContainerBuilder : Autofac.ContainerBuilder {\\n        private readonly IServiceCollection services;\\n\\n        public CustomContainerBuilder(IServiceCollection services) {\\n            this.services = services;\\n            this.Populate(services);\\n        }\\n\\n        public Autofac.IContainer CustomBuild() {\\n            var sp = this.services.BuildServiceProvider();\\n#pragma warning disable CS0612 // Type or member is obsolete\\n            var filters = sp.GetRequiredService<IEnumerable<IStartupConfigureContainerFilter<Autofac.ContainerBuilder>>>();\\n#pragma warning restore CS0612 // Type or member is obsolete\\n\\n            foreach (var filter in filters) {\\n                filter.ConfigureContainer(b => { }) (this);\\n            }\\n\\n            return this.Build();\\n        }\\n    }\\n}\\n```\\n\\nI\'m going to level with you; I don\'t understand all of this code. I\'m not au fait with the inner workings of ASP.NET Core or Autofac but I can tell you what this allows. With this custom `WebApplicationFactory` in play you get `ConfigureTestContainer` back in the mix! You get to write code like this:\\n\\n```cs\\nusing System;\\nusing System.Net;\\nusing System.Net.Http.Headers;\\nusing System.Threading.Tasks;\\nusing FakeItEasy;\\nusing FluentAssertions;\\nusing Microsoft.AspNetCore.TestHost;\\nusing Microsoft.Extensions.DependencyInjection;\\nusing Xunit;\\nusing Microsoft.Extensions.Options;\\nusing Autofac;\\nusing System.Net.Http;\\nusing Newtonsoft.Json;\\n\\nnamespace My.Web.Tests.Controllers\\n{\\n    public class MyControllerTests : IClassFixture<AutofacWebApplicationFactory<My.Web.Startup>> {\\n        private readonly AutofacWebApplicationFactory<My.Web.Startup> _factory;\\n\\n        public MyControllerTests(\\n            AutofacWebApplicationFactory<My.Web.Startup> factory\\n        ) {\\n            _factory = factory;\\n        }\\n\\n        [Fact]\\n        public async Task My() {\\n            var fakeSomethingService = A.Fake<IMySomethingService>();\\n            var fakeConfig = Options.Create(new MyConfiguration {\\n                SomeConfig = \\"Important thing\\",\\n                OtherConfigMaybeAnEmailAddress = \\"johnny_reilly@hotmail.com\\"\\n            });\\n\\n            A.CallTo(() => fakeSomethingService.DoSomething(A<string>.Ignored))\\n                .Returns(Task.FromResult(true));\\n\\n            void ConfigureTestServices(IServiceCollection services) {\\n                services.AddSingleton(fakeConfig);\\n            }\\n\\n            void ConfigureTestContainer(ContainerBuilder builder) {\\n                builder.RegisterInstance(fakeSomethingService);\\n            }\\n\\n            var client = _factory\\n                .WithWebHostBuilder(builder => {\\n                    builder.ConfigureTestServices(ConfigureTestServices);\\n                    builder.ConfigureTestContainer<Autofac.ContainerBuilder>(ConfigureTestContainer);\\n                })\\n                .CreateClient();\\n\\n            // Act\\n            var request = StringContent(\\"{\\\\\\"sommat\\\\\\":\\\\\\"to see\\\\\\"}\\");\\n            request.Headers.ContentType = MediaTypeHeaderValue.Parse(\\"application/json\\");\\n            var response = await client.PostAsync(\\"/something/submit\\", request);\\n\\n            // Assert\\n            response.StatusCode.Should().Be(HttpStatusCode.OK);\\n\\n            A.CallTo(() => fakeSomethingService.DoSomething(A<string>.Ignored))\\n                .MustHaveHappened();\\n        }\\n\\n    }\\n}\\n```"},{"id":"/2020/05/10/from-react-window-to-react-virtual","metadata":{"permalink":"/2020/05/10/from-react-window-to-react-virtual","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2020-05-10-from-react-window-to-react-virtual/index.md","source":"@site/blog/2020-05-10-from-react-window-to-react-virtual/index.md","title":"From react-window to react-virtual","description":"The tremendous Tanner Linsley recently released react-virtual. react-virtual provides \\"hooks for virtualizing scrollable elements in React\\".","date":"2020-05-10T00:00:00.000Z","formattedDate":"May 10, 2020","tags":[{"label":"react-virtual","permalink":"/tags/react-virtual"},{"label":"react-window","permalink":"/tags/react-window"},{"label":"React","permalink":"/tags/react"}],"readingTime":2.55,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"From react-window to react-virtual","authors":"johnnyreilly","tags":["react-virtual","react-window","React"],"hide_table_of_contents":false},"prevItem":{"title":"Autofac, WebApplicationFactory and integration tests","permalink":"/2020/05/21/autofac-webapplicationfactory-integration-tests"},"nextItem":{"title":"Up to the clouds!","permalink":"/2020/04/04/up-to-clouds"}},"content":"The tremendous [Tanner Linsley](https://twitter.com/tannerlinsley) recently released [`react-virtual`](https://github.com/tannerlinsley/react-virtual). `react-virtual` provides \\"hooks for virtualizing scrollable elements in React\\".\\n\\nI was already using the (also excellent) [`react-window`](https://github.com/bvaughn/react-window) for this purpose. `react-window` does the virtualising job and does it very well indeed However, I was both intrigued by the lure of the new shiny thing. I\'ve also never been the biggest fan of `react-window`\'s API. So I tried switching over from `react-window` to `react-virtual` as an experiment. To my delight, the experiment went so well I didn\'t look back!\\n\\nWhat did I get out of the switch?\\n\\n- Simpler code / nicer developer ergonomics. The API for `react-virtual` allowed me to simplify my code and lose a layer of components.\\n- TypeScript support in the box\\n- Improved perceived performance. I didn\'t run any specific tests to quantify this, but I can say that the same functionality now feels snappier.\\n\\nI tweeted my delight at this and Tanner asked if there was commit diff I could share. I couldn\'t as it\'s a private codebase, but I thought it could form the basis of a blogpost.\\n\\n> Nice! Do you have a commit diff we could see?\\n>\\n> \u2014 Tanner Linsley \u269b\ufe0f (@tannerlinsley) [May 10, 2020](https://twitter.com/tannerlinsley/status/1259503283103608832?ref_src=twsrc%5Etfw)\\n\\n<script async=\\"\\" src=\\"https://platform.twitter.com/widgets.js\\" charSet=\\"utf-8\\"><\/script>\\n\\nIn case you hadn\'t guessed, this is that blog post...\\n\\n## Make that change\\n\\nSo what does the change look like? Well first remove `react-window` from your project:\\n\\n```\\nyarn remove react-window @types/react-window\\n```\\n\\nAdd the dependency to `react-virtual`:\\n\\n```\\nyarn add react-virtual\\n```\\n\\nChange your imports from:\\n\\n```ts\\nimport { FixedSizeList, ListChildComponentProps } from \'react-window\';\\n```\\n\\nto:\\n\\n```ts\\nimport { useVirtual } from \'react-virtual\';\\n```\\n\\nChange your component code from:\\n\\n```ts\\ntype ImportantDataListProps = {\\n  classes: ReturnType<typeof useStyles>;\\n  importants: ImportantData[];\\n};\\n\\nconst ImportantDataList: React.FC<ImportantDataListProps> = React.memo(\\n  (props) => (\\n    <FixedSizeList\\n      height={400}\\n      width={\'100%\'}\\n      itemSize={80}\\n      itemCount={props.importants.length}\\n      itemData={props}\\n    >\\n      {RenderRow}\\n    </FixedSizeList>\\n  )\\n);\\n\\ntype ListItemProps = {\\n  classes: ReturnType<typeof useStyles>;\\n  importants: ImportantData[];\\n};\\n\\nfunction RenderRow(props: ListChildComponentProps) {\\n  const { index, style } = props;\\n  const { importants, classes } = props.data as ListItemProps;\\n  const important = importants[index];\\n\\n  return (\\n    <ListItem button style={style} key={index}>\\n      <ImportantThing classes={classes} important={important} />\\n    </ListItem>\\n  );\\n}\\n```\\n\\nOf the above you can delete the `ListItemProps` type and the associate `RenderRow` function. You won\'t need them again! There\'s no longer a need to pass down data to the child element and then extract it for usage; it all comes down into a single simpler component.\\n\\nReplace the `ImportantDataList` component with this:\\n\\n```ts\\nconst ImportantDataList: React.FC<ImportantDataListProps> = React.memo(\\n  (props) => {\\n    const parentRef = React.useRef<HTMLDivElement>(null);\\n\\n    const rowVirtualizer = useVirtual({\\n      size: props.importants.length,\\n      parentRef,\\n      estimateSize: React.useCallback(() => 80, []), // This is just a best guess\\n      overscan: 5,\\n    });\\n\\n    return (\\n      <div\\n        ref={parentRef}\\n        style={{\\n          width: `100%`,\\n          height: `500px`,\\n          overflow: \'auto\',\\n        }}\\n      >\\n        <div\\n          style={{\\n            height: `${rowVirtualizer.totalSize}px`,\\n            width: \'100%\',\\n            position: \'relative\',\\n          }}\\n        >\\n          {rowVirtualizer.virtualItems.map((virtualRow) => (\\n            <div\\n              key={virtualRow.index}\\n              ref={virtualRow.measureRef}\\n              className={props.classes.hoverRow}\\n              style={{\\n                position: \'absolute\',\\n                top: 0,\\n                left: 0,\\n                width: \'100%\',\\n                height: `${virtualRow.size}px`,\\n                transform: `translateY(${virtualRow.start}px)`,\\n              }}\\n            >\\n              <ImportantThing\\n                classes={props.classes}\\n                important={props.importants[virtualRow.index]}\\n              />\\n            </div>\\n          ))}\\n        </div>\\n      </div>\\n    );\\n  }\\n);\\n```\\n\\nAnd you are done! Thanks Tanner for this tremendous library!"},{"id":"/2020/04/04/up-to-clouds","metadata":{"permalink":"/2020/04/04/up-to-clouds","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2020-04-04-up-to-clouds/index.md","source":"@site/blog/2020-04-04-up-to-clouds/index.md","title":"Up to the clouds!","description":"This last four months has been quite the departure for me. Most typically I find myself building applications; for this last period of time I\'ve been taking the platform that I work on, and been migrating it from running on our on premise servers to running in the cloud.","date":"2020-04-04T00:00:00.000Z","formattedDate":"April 4, 2020","tags":[{"label":"docker","permalink":"/tags/docker"},{"label":"kubernetes","permalink":"/tags/kubernetes"},{"label":"asp net core","permalink":"/tags/asp-net-core"},{"label":"aws","permalink":"/tags/aws"}],"readingTime":10.645,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Up to the clouds!","authors":"johnnyreilly","tags":["docker","kubernetes","asp net core","aws"],"hide_table_of_contents":false},"prevItem":{"title":"From react-window to react-virtual","permalink":"/2020/05/10/from-react-window-to-react-virtual"},"nextItem":{"title":"Offline storage in a PWA","permalink":"/2020/03/29/offline-storage-in-pwa"}},"content":"This last four months has been quite the departure for me. Most typically I find myself building applications; for this last period of time I\'ve been taking the platform that I work on, and been migrating it from running on our on premise servers to running in the cloud.\\n\\nThis turned out to be much more difficult than I\'d expected and for reasons that often surprised me. We knew where we wanted to get to, but not all of what we\'d need to do to get there. So many things you can only learn by doing. Whilst these experiences are still fresh in my mind I wanted to document some of the challenges we faced.\\n\\n## The mission\\n\\nAt the start of January, the team decided to make a concerted effort to take our humble ASP.NET Core application and migrate it to the cloud. We sat down with some friends from the DevOps team who are part of our organisation. We\'re fortunate in that these marvellous people are very talented engineers indeed. It was going to be a collaboration between our two teams of budding cloudmongers that would make this happen.\\n\\nNow our application is young. It is not much more than a year old. However it is growing _fast_. And as we did the migration from on premise to the cloud, that wasn\'t going to stop. Development of the application was to continue as is, shipping new versions daily. Without impeding that, we were to try and get the application migrated to the cloud.\\n\\nI would liken it to boarding a speeding train, fighting your way to the front, taking the driver hostage and then diverting the train onto a different track. It was challenging. Really, really challenging.\\n\\nSo many things had to change for us to get from on premise servers to the cloud, all the while keeping our application a going (and shipping) concern. Let\'s go through them one by one.\\n\\n## Kubernetes and Docker\\n\\nOur application was built using ASP.NET Core. A technology that is entirely cloud friendly (that\'s one of the reasons we picked it). We were running on a collection of hand installed, hand configured Windows servers. That had to change. We wanted to move our application to run on Kubernetes; so we didn\'t have to manually configure servers. Rather k8s would manage the provisioning and deployment of containers running our application. Worth saying now: I knew _nothing_ about Kubernetes. Or nearly nothing. I learned a bunch along the way, but, as I\'ve said, this was a collaboration between our team and the mighty site reliability engineers of the DevOps team. They knew a _lot_ about this k8s stuff and moreoften than not, our team stood back and let them work their magic.\\n\\nIn order that we could migrate to running in k8s, we first needed to containerise our application. We needed a `Dockerfile`. There followed a good amount of experimentation as we worked out how to build ourselves images. There\'s an art to building an optimal Docker image.\\n\\nSo that we can cover a lot of ground, this post will remain relatively high level. So here\'s a number of things that we encountered along the way that are worth considering:\\n\\n- Multi-stage builds were an absolute necessity for us. We\'d build the front end of our app (React / TypeScript) using one stage with a [Node base image](https://hub.docker.com/_/node). Then we\'d build our app using a [.NET Core SDK base image](https://hub.docker.com/_/microsoft-dotnet-core-sdk/). Finally, we\'d use a [ASP.Net](https://hub.docker.com/_/microsoft-dotnet-core-aspnet) image to run the app; copying in the output of previous stages.\\n- Our application accesses various SQL Server databases. We struggled to get our application to connect to them. The issue related to the SSL configuration of our runner image. The fix was simple but frustrating; use a `-bionic` image as it has the configuration you need. We found that gem [here](https://github.com/dotnet/SqlClient/issues/222#issuecomment-535802822).\\n- Tests. Automated tests. We want to run them in our build; but how? Once more multi-stage builds to the rescue. We\'d build our application, then in a separate stage we\'d run the tests; copying in the app from the build stage. If the tests failed, the build failed. If they passed then the intermediate stage containing the tests would be discarded by Docker. No unnecessary bloat of the image; all that testing goodness still; now in containerised form!\\n\\n## Jenkins\\n\\nOur on premise world used TeamCity for our continuous integration needs and Octopus for deployment. We liked these tools well enough; particularly Octopus. However, the DevOps team were very much of the mind that we should be use Jenkins instead. And [Pipeline](https://jenkins.io/doc/book/pipeline/). It was here that we initially struggled. To quote the docs:\\n\\n> Jenkins Pipeline (or simply \\"Pipeline\\" with a capital \\"P\\") is a suite of plugins which supports implementing and integrating continuous delivery pipelines into Jenkins.\\n\\nWhilst continuous delivery is super cool, and is something our team was interested in, we weren\'t ready for it yet. We didn\'t yet have the kind of automated testing in place that gave us the confidence that we\'d need to move to it. One day, but not today. For now there was still some manual testing done on each release, prior to shipping. Octopus suited us very well here as it allowed us to deploy, on demand, a build of our choice to a given environment. So the question was: what to do? Fortunately the immensely talented Aby Egea came up with a mechanism that supported that very notion. A pipeline that would, optionally, deploy our build to a specified environment. So we were good!\\n\\nOne thing we got to really appreciate about Jenkins was that the build is scripted with a [Jenkinsfile](https://jenkins.io/doc/book/pipeline/jenkinsfile/). This was in contrast to our TeamCity world where it was all manually configured. [Configuration as code](https://jenkins.io/projects/jcasc/) is truly a wonderful thing as your build pipeline becomes part of your codebase; open for everyone to see and understand. If anyone wants to change the build pipeline it has to get code reviewed like everything else. It was as code in our `Jenkinsfile` that the deployment mechanism lived.\\n\\n## Vault\\n\\nAnother thing that we used Octopus for was secrets. Applications run on configuration; these are settings that drive the behaviour of your application. A subset of configuration is \\"secrets\\". Secrets are configuration that can\'t be stored in source code; they would represent a risk if they did. For instance a database connection string. We\'d been merrily using Octopus for this; as Octopus deploys an application to a server it enriches the `appsettings.json` file with any required secrets.\\n\\nWithout Octopus in the mix, how were we to handle our secrets? The answer is with [Hashicorp Vault](https://www.vaultproject.io/). We\'d store our secrets in there and, thanks to clever work by [Robski](https://uk.linkedin.com/in/robert-grzankowski-53618114) of the DevOps team, when our container was brought up by Kubernetes, it would mount into the filesystem an `appsettings.Vault.json` file which we read thanks to our trusty friend [`.AddJsonFile`](https://docs.microsoft.com/en-us/aspnet/core/fundamentals/configuration/?view=aspnetcore-3.1#json-configuration-provider) with `optional: true`. (As the file didn\'t exist in our development environment.)\\n\\nHey presto! Safe secrets in k8s.\\n\\n## Networking\\n\\nOur on premise servers sat on the company network. They could see _everything_ that there was to see. All the other servers around them on the network, bleeping and blooping. The opposite was true in AWS. There was nothing to see. Nothing to access. As it should be. It\'s safer that way should a machine become compromised. For each database and each API our application depended upon, we needed to specifically allowlist access.\\n\\n## Kerberos\\n\\nThere\'s always a fly in the ointment. A nasty surprise on a dark night. Ours was realising that our application depended upon an API that was secured using [Windows Authentication](https://docs.microsoft.com/en-us/iis/configuration/system.webserver/security/authentication/windowsauthentication/). Our application was accessing it by running under a service account which had been permissioned to access it. However, in AWS, our application wasn\'t running as under a service account on the company network. Disappointingly, in the short term the API was not going to support an alternate authentication mechanism.\\n\\nWhat to do? Honestly it wasn\'t looking good. We were considering proxying through one of our Windows servers just to get access to that API. I was tremendously disappointed. At this point our hero arrived; one [JMac](https://twitter.com/foldr) hacked together a Kerberos sidecar approach one weekend. You can see a similar approach [here](https://github.com/edseymour/kinit-sidecar). This got us to a point that allowed us to access the API we needed to.\\n\\nI\'m kind of amazed that there isn\'t better documentation out there around have a Kerberos sidecar in a k8s setup. Tragically Windows Authentication is a widely used authentication mechanism. That being the case, having good docs to show how you can get a Kerberos sidecar in place would likely greatly advance the ability of enterprises to migrate to the cloud. The best docs I\'ve found are [here](https://blog.openshift.com/kerberos-sidecar-container/). It is super hard though. _So hard!_\\n\\n## Hangfire\\n\\nWe were using [Hosted Services](https://docs.microsoft.com/en-us/aspnet/core/fundamentals/host/hosted-services?view=aspnetcore-3.1&tabs=visual-studio) to perform background task running in our app. The nature of our background tasks meant that it was important to only run a single instance of a background task at a time. Or bad things would happen. This was going to become a problem since we had ambitions to be able to horizontally scale our application; to add new pods as running our app as demand determined.\\n\\nSo we started to use [Hangfire](https://www.hangfire.io/) to perform task running in our app. With Hangfire, when a job is picked up it gets locked so other servers can\'t pick it up. That\'s what we need.\\n\\nHangfire is pretty awesome. However it turns out that there\'s quirks when you move to a containerised environment. We have a number of recurring jobs that are scheduled to run at certain dates and times. In order that Hangfire can ascertain what time it is, it needs a timezone. It turns out that timezones on Windows != timezones in Docker / Linux.\\n\\nThis was a problem because, as we limbered up for the great migration, we were trying to run our cloud implementation side by side with our on premise one. And Windows picked a fight with Linux over timezones. You can see others bumping into this condition [here](https://github.com/HangfireIO/Hangfire/issues/1268). We learned this the hard way; jobs mysteriously stopping due to timezone related errors. Windows Hangfire not able to recognise Linux Hangfire timezones and vica versa.\\n\\nThe TL;DR is that we had to do a hard switch with Hangfire; it couldn\'t run side by side. Not the end of the world, but surprising.\\n\\n## Azure Active Directory Single Sign-On\\n\\nHistorically our application had used two modes of authentication; Windows Authentication and cookies. Windows Authentication doesn\'t generally play nicely with Docker. It\'s doable, but it\'s not the hill you want to die on. So we didn\'t; we swapped out Windows Authentication for [Azure AD SSO](https://docs.microsoft.com/en-us/azure/active-directory/manage-apps/what-is-single-sign-on) and didn\'t look back.\\n\\nWe also made some changes so our app would support cookies auth alongside Azure AD auth; [I\'ve written about this previously](https://blog.johnnyreilly.com/2020/03/dual-boot-authentication-with-aspnetcore.html).\\n\\n## Do the right thing and tell people about it\\n\\nWe\'re there now; we\'ve made the move. It was a difficult journey but one worth making; it sets up our platform for where we want to take it in the future. Having infrastructure as code makes all kinds of approaches possible that weren\'t before. Here\'s some things we\'re hoping to get out of the move:\\n\\n- blue green deployments - shipping without taking down our platform\\n- provision environments on demand - currently we have a highly contended situation when it comes to test environments. With k8s and AWS we can look at spinning up environments as we need them and throwing them away also\\n- autoscaling for need - we can start to look at spinning up new containers in times of high load and removing excessive containers in times of low load\\n\\nWe\'ve also become more efficient as a team. We are no longer maintaining servers, renewing certificates, installing software, RDPing onto boxes. All that time and effort we can plough back into making awesome experiences for our users.\\n\\nThere\'s a long list of other benefits and it\'s very exciting indeed! It\'s not enough for us to have done this though. It\'s important that we tell the story of what we\'ve done and how and why we\'ve done it. That way people have empathy for the work. Also they can start to think about how they could start to reap similar benefits themselves. By talking to others about the road we\'ve travelled, we can save them time and help them to travel a similar road. This is good for them and it\'s good for us; it helps our relationships and it helps us all to move forwards together.\\n\\nA rising tide lifts all boats. By telling others about our journey, we raise the water level. Up to the clouds!"},{"id":"/2020/03/29/offline-storage-in-pwa","metadata":{"permalink":"/2020/03/29/offline-storage-in-pwa","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2020-03-29-offline-storage-in-pwa/index.md","source":"@site/blog/2020-03-29-offline-storage-in-pwa/index.md","title":"Offline storage in a PWA","description":"When you are building any kind of application it\'s typical to want to store information which persists beyond a single user session. Sometimes that will be information that you\'ll want to live in some kind of centralised database, but not always.","date":"2020-03-29T00:00:00.000Z","formattedDate":"March 29, 2020","tags":[{"label":"PWA","permalink":"/tags/pwa"},{"label":"idb-keyval","permalink":"/tags/idb-keyval"},{"label":"IndexedDB","permalink":"/tags/indexed-db"},{"label":"localStorage","permalink":"/tags/local-storage"}],"readingTime":9.035,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Offline storage in a PWA","authors":"johnnyreilly","tags":["PWA","idb-keyval","IndexedDB","localStorage"],"hide_table_of_contents":false},"prevItem":{"title":"Up to the clouds!","permalink":"/2020/04/04/up-to-clouds"},"nextItem":{"title":"Dual boot authentication with ASP.NET","permalink":"/2020/03/22/dual-boot-authentication-with-aspnetcore"}},"content":"When you are building any kind of application it\'s typical to want to store information which persists beyond a single user session. Sometimes that will be information that you\'ll want to live in some kind of centralised database, but not always.\\n\\nAlso, you may want that data to still be available if your user is offline. Even if they can\'t connect to the network, the user may still be able to use the app to do meaningful tasks; but the app will likely require a certain amount of data to drive that.\\n\\nHow can we achieve this in the context of a PWA?\\n\\n## The problem with `localStorage`\\n\\nIf you were building a classic web app you\'d probably be reaching for [`Window.localStorage`](https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage) at this point. `Window.localStorage` is a long existing API that stores data beyond a single session. It has a simple API and is very easy to use. However, it has a couple of problems:\\n\\n1. `Window.localStorage` is synchronous. Not a tremendous problem for every app, but if you\'re building something that has significant performance needs then this could become an issue.\\n2. `Window.localStorage` cannot be used in the context of a `Worker` or a `ServiceWorker`. The APIs are not available there.\\n3. `Window.localStorage` stores only `string`s. Given [`JSON.stringify`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/JSON/stringify) and [`JSON.parse`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/JSON/parse) that\'s not a big problem. But it\'s an inconvenience.\\n\\nThe second point here is the significant one. If we\'ve a need to access our offline data in the context of a `ServiceWorker` (and if you\'re offline you\'ll be using a `ServiceWorker`) then what do you do?\\n\\n## IndexedDB to the rescue?\\n\\nFortunately, `localStorage` is not the only game in town. There\'s alternative offline storage mechanism available in browsers with the curious name of [IndexedDB](https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API). To quote the docs:\\n\\n> IndexedDB is a transactional database system, like an SQL-based RDBMS. However, unlike SQL-based RDBMSes, which use fixed-column tables, IndexedDB is a JavaScript-based object-oriented database. IndexedDB lets you store and retrieve objects that are indexed with a key; any objects supported by the structured clone algorithm can be stored. You need to specify the database schema, open a connection to your database, and then retrieve and update data within a series of transactions.\\n\\nIt\'s clear that IndexedDB is _very_ powerful. But it doesn\'t sound very simple. A further look at the [MDN example](https://github.com/mdn/to-do-notifications/blob/8b3e1708598e42062b0136608b1c5fbb66520f0a/scripts/todo.js#L48) of how to interact with IndexedDB does little to remove that thought.\\n\\nWe\'d like to be able to access data offline; but in a simple fashion. Like we could with `localStorage` which has a wonderfully straightforward API. If only someone would build an astraction on top of IndexedDB to make our lives easier...\\n\\nSomeone did.\\n\\n## IDB-Keyval to the rescue!\\n\\nThe excellent [Jake Archibald](https://twitter.com/jaffathecake) of Google has written [IDB-Keyval](https://github.com/jakearchibald/idb-keyval) which is:\\n\\n> A super-simple-small promise-based keyval store implemented with IndexedDB\\n\\nThe API is essentially equivalent to `localStorage` with a few lovely differences:\\n\\n1. The API is promise based; all functions return a `Promise`; this makes it a non-blocking API.\\n2. The API is not restricted to `string`s as `localStorage` is. To quote the docs: _this is IDB-backed, you can store anything structured-clonable (numbers, arrays, objects, dates, blobs etc)_\\n3. Because this is abstraction built on top of IndexedDB, it can be used both in the context of a typical web app and also in a `Worker` or a `ServiceWorker` if required.\\n\\n## Simple usage\\n\\nLet\'s take a look at what usage of `IDB-Keyval` might be like. For that we\'re going to need an application. It would be good to be able to demonstrate both simple usage and also how usage in the context of an application might look.\\n\\nLet\'s spin up a TypeScript React app with [Create React App](https://create-react-app.dev/):\\n\\n```shell\\nnpx create-react-app offline-storage-in-a-pwa --template typescript\\n```\\n\\nThis creates us a simple app. Now let\'s add IDB-Keyval to it:\\n\\n```shell\\nyarn add idb-keyval\\n```\\n\\nThen, let\'s update the `index.tsx` file to add a function that tests using IDB-Keyval:\\n\\n```tsx\\nimport React from \'react\';\\nimport ReactDOM from \'react-dom\';\\nimport { set, get } from \'idb-keyval\';\\nimport \'./index.css\';\\nimport App from \'./App\';\\nimport * as serviceWorker from \'./serviceWorker\';\\n\\nReactDOM.render(<App />, document.getElementById(\'root\'));\\n\\nserviceWorker.register();\\n\\nasync function testIDBKeyval() {\\n  await set(\'hello\', \'world\');\\n  const whatDoWeHave = await get(\'hello\');\\n  console.log(\\n    `When we queried idb-keyval for \'hello\', we found: ${whatDoWeHave}`\\n  );\\n}\\n\\ntestIDBKeyval();\\n```\\n\\nAs you can see, we\'ve added a `testIDBKeyval` function which does the following:\\n\\n1. Adds a value of `\'world\'` to IndexedDB using IDB-Keyval for the key of `\'hello\'`\\n2. Queries IndexedDB using IDB-Keyval for the key of `\'hello\'` and stores it in the variable `whatDoWeHave`\\n3. Logs out what we found.\\n\\nYou\'ll also note that `testIDBKeyval` is an `async` function. This is so that we can use `await` when we\'re interacting with IDB-Keyval. Given that its API is `Promise` based, it is `await` friendly. Where you\'re performing more than an a single asynchronous operation at a time, it\'s often valuable to use `async` / `await` to increase the readability of your codebase.\\n\\nWhat happens when we run our application with `yarn start`? Let\'s do that and take a look at the devtools:\\n\\n![](hello_world_idb_keyval.png)\\n\\nWe successfully wrote something into IndexedDB, read it back and printed that value to the console. Amazing!\\n\\n## Usage in React\\n\\nWhat we\'ve done so far is slightly abstract. It would be good to implement a real-world use case. Let\'s create an application which gives users the choice between using a \\"Dark mode\\" version of the app or not. To do that we\'ll replace our `App.tsx` with this:\\n\\n```tsx\\nimport React, { useState } from \'react\';\\nimport \'./App.css\';\\n\\nconst sharedStyles = {\\n  height: \'30rem\',\\n  fontSize: \'5rem\',\\n  textAlign: \'center\',\\n} as const;\\n\\nfunction App() {\\n  const [darkModeOn, setDarkModeOn] = useState(true);\\n  const handleOnChange = ({ target }: React.ChangeEvent<HTMLInputElement>) =>\\n    setDarkModeOn(target.checked);\\n\\n  const styles = {\\n    ...sharedStyles,\\n    ...(darkModeOn\\n      ? {\\n          backgroundColor: \'black\',\\n          color: \'white\',\\n        }\\n      : {\\n          backgroundColor: \'white\',\\n          color: \'black\',\\n        }),\\n  };\\n\\n  return (\\n    <div style={styles}>\\n      <input\\n        type=\\"checkbox\\"\\n        value=\\"darkMode\\"\\n        checked={darkModeOn}\\n        id=\\"darkModeOn\\"\\n        name=\\"darkModeOn\\"\\n        style={{ width: \'3rem\', height: \'3rem\' }}\\n        onChange={handleOnChange}\\n      />\\n      <label htmlFor=\\"darkModeOn\\">Use dark mode?</label>\\n    </div>\\n  );\\n}\\n\\nexport default App;\\n```\\n\\nWhen you run the app you can see how it works:\\n\\n![](use-dark-mode.gif)\\n\\nLooking at the code you\'ll be able to see that this is implemented using React\'s `useState` hook. So any user preference selected will be lost on a page refresh. Let\'s see if we can take this state and move it into IndexedDB using `IDB-Keyval`.\\n\\nWe\'ll change the code like so:\\n\\n```tsx\\nimport React, { useState, useEffect } from \'react\';\\nimport { set, get } from \'idb-keyval\';\\nimport \'./App.css\';\\n\\nconst sharedStyles = {\\n  height: \'30rem\',\\n  fontSize: \'5rem\',\\n  textAlign: \'center\',\\n} as const;\\n\\nfunction App() {\\n  const [darkModeOn, setDarkModeOn] = useState<boolean | undefined>(undefined);\\n\\n  useEffect(() => {\\n    get<boolean>(\'darkModeOn\').then((value) =>\\n      // If a value is retrieved then use it; otherwise default to true\\n      setDarkModeOn(value ?? true)\\n    );\\n  }, [setDarkModeOn]);\\n\\n  const handleOnChange = ({ target }: React.ChangeEvent<HTMLInputElement>) => {\\n    setDarkModeOn(target.checked);\\n\\n    set(\'darkModeOn\', target.checked);\\n  };\\n\\n  const styles = {\\n    ...sharedStyles,\\n    ...(darkModeOn\\n      ? {\\n          backgroundColor: \'black\',\\n          color: \'white\',\\n        }\\n      : {\\n          backgroundColor: \'white\',\\n          color: \'black\',\\n        }),\\n  };\\n\\n  return (\\n    <div style={styles}>\\n      {darkModeOn === undefined ? (\\n        <>Loading preferences...</>\\n      ) : (\\n        <>\\n          <input\\n            type=\\"checkbox\\"\\n            value=\\"darkMode\\"\\n            checked={darkModeOn}\\n            id=\\"darkModeOn\\"\\n            name=\\"darkModeOn\\"\\n            style={{ width: \'3rem\', height: \'3rem\' }}\\n            onChange={handleOnChange}\\n          />\\n          <label htmlFor=\\"darkModeOn\\">Use dark mode?</label>\\n        </>\\n      )}\\n    </div>\\n  );\\n}\\n\\nexport default App;\\n```\\n\\nThe changes here are:\\n\\n1. `darkModeOn` is now initialised to `undefined` and the app displays a loading message until `darkModeOn` has a value.\\n2. The app attempts to app load a value from IDB-Keyval with the key `\'darkModeOn\'` and set `darkModeOn` with the retrieved value. If no value is retrieved then it sets `darkModeOn` to `true`.\\n3. When the checkbox is changed, the corresponding value is both applied to `darkModeOn` and saved to IDB-Keyval with the key `\'darkModeOn\'`\\n\\nAs you can see, this means that we are persisting preferences beyond page refresh in a fashion that will work both online _and_ offline!\\n\\n![](use-dark-mode-with-idb-keyval.gif)\\n\\n## Usage as a React hook\\n\\nFinally it\'s time for bonus points. Wouldn\'t it be nice if we could move this functionality into a reusable React hook? Let\'s do it!\\n\\nLet\'s create a new `usePersistedState.ts` file:\\n\\n```ts\\nimport { useState, useEffect, useCallback } from \'react\';\\nimport { set, get } from \'idb-keyval\';\\n\\nexport function usePersistedState<TState>(\\n  keyToPersistWith: string,\\n  defaultState: TState\\n) {\\n  const [state, setState] = useState<TState | undefined>(undefined);\\n\\n  useEffect(() => {\\n    get<TState>(keyToPersistWith).then((retrievedState) =>\\n      // If a value is retrieved then use it; otherwise default to defaultValue\\n      setState(retrievedState ?? defaultState)\\n    );\\n  }, [keyToPersistWith, setState, defaultState]);\\n\\n  const setPersistedValue = useCallback(\\n    (newValue: TState) => {\\n      setState(newValue);\\n      set(keyToPersistWith, newValue);\\n    },\\n    [keyToPersistWith, setState]\\n  );\\n\\n  return [state, setPersistedValue] as const;\\n}\\n```\\n\\nThis new hook is modelled after the API of [`useState`](https://reactjs.org/docs/hooks-reference.html#usestate) and is named `usePersistentState`. It requires that a key be supplied which is the key that will be used to save the data. It also requires a default value to use in the case that nothing is found during the lookup.\\n\\nIt returns (just like `useState`) a stateful value, and a function to update it. Finally, let\'s switch over our `App.tsx` to use our shiny new hook:\\n\\n```tsx\\nimport React from \'react\';\\nimport \'./App.css\';\\nimport { usePersistedState } from \'./usePersistedState\';\\n\\nconst sharedStyles = {\\n  height: \'30rem\',\\n  fontSize: \'5rem\',\\n  textAlign: \'center\',\\n} as const;\\n\\nfunction App() {\\n  const [darkModeOn, setDarkModeOn] = usePersistedState<boolean>(\\n    \'darkModeOn\',\\n    true\\n  );\\n\\n  const handleOnChange = ({ target }: React.ChangeEvent<HTMLInputElement>) =>\\n    setDarkModeOn(target.checked);\\n\\n  const styles = {\\n    ...sharedStyles,\\n    ...(darkModeOn\\n      ? {\\n          backgroundColor: \'black\',\\n          color: \'white\',\\n        }\\n      : {\\n          backgroundColor: \'white\',\\n          color: \'black\',\\n        }),\\n  };\\n\\n  return (\\n    <div style={styles}>\\n      {darkModeOn === undefined ? (\\n        <>Loading preferences...</>\\n      ) : (\\n        <>\\n          <input\\n            type=\\"checkbox\\"\\n            value=\\"darkMode\\"\\n            checked={darkModeOn}\\n            id=\\"darkModeOn\\"\\n            name=\\"darkModeOn\\"\\n            style={{ width: \'3rem\', height: \'3rem\' }}\\n            onChange={handleOnChange}\\n          />\\n          <label htmlFor=\\"darkModeOn\\">Use dark mode?</label>\\n        </>\\n      )}\\n    </div>\\n  );\\n}\\n\\nexport default App;\\n```\\n\\n## Conclusion\\n\\nThis post has demonstrate how a web application or a PWA can safely store data that is persisted between sessions using native browser capabilities easily. IndexedDB powered the solution we\'ve built. We used used [IDB-Keyval](https://github.com/jakearchibald/idb-keyval) for the delightful and familiar abstraction it offers over IndexedDB. It\'s allowed us to come up with a solution with a similarly lovely API. It\'s worth knowing that there are alternatives to IDB-Keyval available such as [localForage](https://github.com/localForage/localForage). If you are building for older browsers which may lack good IndexedDB support then this would be a good choice. But be aware that with greater backwards compatibility comes greater download size. Do consider this and make the tradeoffs that make sense for you.\\n\\nFinally, I\'ve finished this post illustrating what usage would look like in a React context. Do be aware that there\'s nothing React specific about our offline storage mechanism. So if you\'re rolling with Vue, Angular or something else entirely: _this is for you too_! Offline storage is a feature that provide much greater user experiences. Please do consider making use of it in your applications.\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/offline-storage-for-pwas/)\\n\\n[The source code for this project can be found here.](https://github.com/johnnyreilly/offline-storage-in-a-pwa)"},{"id":"/2020/03/22/dual-boot-authentication-with-aspnetcore","metadata":{"permalink":"/2020/03/22/dual-boot-authentication-with-aspnetcore","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2020-03-22-dual-boot-authentication-with-aspnetcore/index.md","source":"@site/blog/2020-03-22-dual-boot-authentication-with-aspnetcore/index.md","title":"Dual boot authentication with ASP.NET","description":"This is a post about having two kinds of authentication working at the same time in ASP.Net Core. But choosing which authentication method to use dynamically at runtime; based upon the criteria of your choice.","date":"2020-03-22T00:00:00.000Z","formattedDate":"March 22, 2020","tags":[{"label":"Authentication","permalink":"/tags/authentication"},{"label":"dual authentication","permalink":"/tags/dual-authentication"},{"label":"Cookie","permalink":"/tags/cookie"},{"label":"Azure AD","permalink":"/tags/azure-ad"},{"label":"ForwardDefaultSelector","permalink":"/tags/forward-default-selector"},{"label":"ASP.NET","permalink":"/tags/asp-net"}],"readingTime":8.045,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Dual boot authentication with ASP.NET","authors":"johnnyreilly","tags":["Authentication","dual authentication","Cookie","Azure AD","ForwardDefaultSelector","ASP.NET"],"hide_table_of_contents":false},"prevItem":{"title":"Offline storage in a PWA","permalink":"/2020/03/29/offline-storage-in-pwa"},"nextItem":{"title":"Web Workers, comlink, TypeScript and React","permalink":"/2020/02/21/web-workers-comlink-typescript-and-react"}},"content":"This is a post about having two kinds of authentication working at the same time in ASP.Net Core. But choosing which authentication method to use dynamically at runtime; based upon the criteria of your choice.\\n\\nAlready this sounds complicated; let\'s fix that. Perhaps I should describe my situation to you. I\'ve an app which has two classes of user. One class, let\'s call them \\"customers\\" (because... uh... they\'re customers). The customers access our application via a public facing website. Traffic rolls through Cloudflare and into our application. The public facing URL is something fancy like [https://mega-app.com](https://mega-app.com). That\'s one class of user.\\n\\nThe other class of user we\'ll call \\"our peeps\\"; because they are _us_. We use the app that we build. Traffic from \\"us\\" comes from a different hostname; only addressable on our network. So URLs from requests that we make are more along the lines of [https://strictly4mypeeps.io](https://strictly4mypeeps.io).\\n\\nSo far, so uncontroversial. Now it starts to get interesting. Our customers log into our application using their super secret credentials. It\'s [cookie based authentication](https://docs.microsoft.com/en-us/aspnet/core/security/authentication/cookie?view=aspnetcore-3.1#create-an-authentication-cookie). But for our peeps we do something different. Having to enter your credentials each time you use the app is friction. It gets in the way. So for us we have [Azure AD](https://docs.microsoft.com/en-us/aspnet/core/security/authentication/azure-active-directory/?view=aspnetcore-3.1) in the mix. Azure AD is how we authenticate ourselves; and that means we don\'t spend 5% of each working day entering credentials.\\n\\n## Let us speak of the past\\n\\nNow our delightful little application grew up in a simpler time. A time where you went to the marketplace, picked out some healthy looking servers, installed software upon them, got them attached to the internet, deployed an app onto them and said \\"hey presto, we\'re live!\\".\\n\\nWay back when, we had some servers on the internet, that\'s how our customers got to our app. Our peeps, us, we went to other servers that lived on our network. So we had multiple instances of our app, deployed to different machines. The ones on the internet were configured to use cookie based auth, the ones on our internal network were Azure AD.\\n\\nAs I said, a simpler time.\\n\\n## A new hope\\n\\nWe\'ve been going through the process of cloudifying our app. Bye, bye servers, hello [Docker](https://www.docker.com/) and [Kubernetes](https://kubernetes.io/). So exciting! As we change the way our app is built and deployed; we\'ve been thinking about whether the choices we make still make sense.\\n\\nWhen it came to authentication, my initial thoughts were to continue the same road we\'re travelling; just in containers and pods. So where we had \\"internal\\" servers, we\'d have \\"internal\\" pods, and where we\'d have \\"external\\" servers we\'d have external pods. I had the good fortune to be working with the amazingly talented [Robski](https://uk.linkedin.com/in/robert-grzankowski-53618114). Robski knows far more about K8s and networking than I\'m ever likely to. He\'d regularly say things like \\"ingress\\" and \\"MTLS\\" whilst I stared blankly at him. He definitely knows stuff.\\n\\nRobski challenged my plans. \\"We don\'t need it. Have one pod that does both sorts of auth. If you do that, your implementation is simpler and scaling is more straightforward. You\'ll only need half the pods because you won\'t need internal _and_ external ones; one pod can handle both sets of traffic. You\'ll save money.\\"\\n\\nI loved the idea but I didn\'t think that ASP.Net Core supported it. \\"It\'s just not a thing Robski; ASP.Net Core doesn\'t suppport it.\\" Robski didn\'t believe me. That turned out to a _very good thing_. There followed a period of much googling and experimentation. One day of hunting in, I was still convinced there was no way to do it that would allow me to look in the mirror without self loathing. Then Robski sent me this:\\n\\n![screenshot of WhatsApp message with a link in it](robski-dynamic-auth.png)\\n\\nIt was a link to the amazing [David Fowler](https://twitter.com/davidfowl) talking about [some API I\'d never heard of called `SchemeSelector`](https://github.com/aspnet/Security/issues/1469#issuecomment-335027005). It turned out that this was the starting point for exactly what we needed; a way to dynamically select an authentication scheme at runtime.\\n\\n## Show me the code\\n\\nThis API did end up landing in ASP.Net Core, but with the name `ForwardDefaultSelector`. Not the most descriptive of names and I\'ve struggled to find any documentation on it at all. What I did discover was [an answer on StackOverflow by the marvellous Barbara Post](https://stackoverflow.com/a/51897159/761388). I was able to take the approach Barbara laid out and use it to my own ends. I ended up with this snippet of code added to my `Startup.ConfigureServices`:\\n\\n```cs\\nservices\\n    .AddAuthentication(sharedOptions => {\\n        sharedOptions.DefaultScheme = \\"WhichAuthDoWeUse\\";\\n        sharedOptions.DefaultAuthenticateScheme = \\"WhichAuthDoWeUse\\";\\n        sharedOptions.DefaultChallengeScheme = \\"WhichAuthDoWeUse\\";\\n    })\\n    .AddPolicyScheme(\\"WhichAuthDoWeUse\\", \\"Azure AD or Cookies\\", options => {\\n        options.ForwardDefaultSelector = context => {\\n            var (isExternalRequest, requestUrl) = context.Request.GetIsExternalRequestAndDomain();\\n            if (isExternalRequest) {\\n                _logger.LogInformation(\\n                    \\"Request ({RequestURL}) has come from external domain ({Domain}) so using Cookie Authentication\\",\\n                    requestUrl, ExternalBaseUrl);\\n\\n                return CookieAuthenticationDefaults.AuthenticationScheme;\\n           }\\n\\n           _logger.LogInformation(\\n               \\"Request ({RequestURL}) has not come from external domain ({Domain}) so using Azure AD Authentication\\",\\n               requestUrl, ExternalBaseUrl);\\n\\n            return AzureADDefaults.AuthenticationScheme;\\n        };\\n    })\\n    .AddAzureAD(options => {\\n        Configuration.Bind(\\"AzureAd\\", options);\\n    })\\n    .AddCookie(options => {\\n        options.Cookie.SecurePolicy = CookieSecurePolicy.Always;\\n        options.Cookie.SameSite = SameSiteMode.Strict;\\n        options.Cookie.HttpOnly = true;\\n        options.Events.OnRedirectToAccessDenied = (context) => {\\n            context.Response.StatusCode = Microsoft.AspNetCore.Http.StatusCodes.Status401Unauthorized;\\n            return Task.CompletedTask;\\n        };\\n\\n        options.Events.OnRedirectToLogin = (context) => {\\n            context.Response.StatusCode = Microsoft.AspNetCore.Http.StatusCodes.Status401Unauthorized;\\n            return Task.CompletedTask;\\n        };\\n    });\\n```\\n\\nIf you look at this code it\'s doing these things:\\n\\n1. Registering three types of authentication: Cookie, Azure AD and \\"WhichAuthDoWeUse\\"\\n2. Registers the default `Scheme` to be \\"WhichAuthDoWeUse\\".\\n\\n\\"WhichAuthDoWeUse\\" is effectively an `if` statement that says, _\\"if this is an external `Request` use Cookies authentication, otherwise use Azure AD\\"_. Given that \\"WhichAuthDoWeUse\\" is the default scheme, this code runs for each request, to determine which authentication method to use.\\n\\nAlongside this mechanism I added these extension methods:\\n\\n```cs\\nusing System;\\nusing Microsoft.AspNetCore.Http;\\nusing Microsoft.AspNetCore.Http.Extensions;\\n\\nnamespace My.App.Auth {\\n    public static class AuthExtensions {\\n        public const string ExternalBaseUrl = \\"https://mega-app.com\\";\\n        public const string InternalBaseUrl = \\"https://strictly4mypeeps.io\\";\\n\\n        /// <summary>\\n        /// Determines if a request is an \\"external\\" URL (eg begins \\"https://mega-app.com\\")\\n        /// or an \\"internal\\" URL (eg begins \\"https://strictly4mypeeps.io\\")\\n        /// </summary>\\n        public static (bool, string) GetIsExternalRequestAndDomain(this HttpRequest request) {\\n            var (requestUrl, domain) = GetRequestUrlAndDomain(request);\\n\\n            var isExternalUrl = domain == ExternalBaseUrl;\\n\\n            var isUnknownPath = domain == null; // This scenario is extremely unlikely but has been observed once during testing so we will cater for it\\n\\n            var isExternalRequest = isExternalUrl || isUnknownPath; // If unknown we\'ll treat as \\"external\\" for a safe fallback\\n\\n            return (isExternalRequest, requestUrl);\\n        }\\n\\n        /// <summary>\\n        /// Determines if a request is an \\"external\\" URL (eg begins \\"https://mega-app.com\\")\\n        /// or an \\"internal\\" URL (eg begins \\"https://strictly4mypeeps.io\\")\\n        /// </summary>\\n        public static (bool, string) GetIsInternalRequestAndDomain(this HttpRequest request) {\\n            var (requestUrl, domain) = GetRequestUrlAndDomain(request);\\n\\n            var isInternalRequest = domain == InternalBaseUrl;\\n\\n            return (isInternalRequest, requestUrl);\\n        }\\n\\n        private static (string, string) GetRequestUrlAndDomain(HttpRequest request) {\\n            string requestUrl = null;\\n            string domain = null;\\n            if (request.Host.HasValue) {\\n                requestUrl = request.GetEncodedUrl();\\n                domain = new Uri(requestUrl).GetLeftPart(UriPartial.Authority);\\n            }\\n\\n            return (requestUrl, domain);\\n        }\\n    }\\n}\\n```\\n\\nFinally, I updated the `SpaController.cs` (which serves initial requests to our Single Page Application) to cater for having two types of Auth in play:\\n\\n```cs\\n        /// <summary>\\n        /// ASP.NET will try and load the index.html using the FileServer if we don\'t have a route\\n        /// here to match `/`. These attributes can\'t be on Index or the spa fallback doesn\'t work\\n        /// Note: this is almost perfect except that if someone actually calls /index.html they\'ll get\\n        /// the FileServer one, not the one from this file.\\n        /// </summary>\\n        [HttpGet(\\"/\\")]\\n        [AllowAnonymous]\\n        public async Task<IActionResult> SpaFallback([FromQuery] string returnUrl) {\\n            var redirectUrlIfUserIsInternalAndNotAuthenticated = GetRedirectUrlIfUserIsInternalAndNotAuthenticated(returnUrl);\\n\\n            if (redirectUrlIfUserIsInternalAndNotAuthenticated != null)\\n                return LocalRedirect(redirectUrlIfUserIsInternalAndNotAuthenticated);\\n\\n            return await Index(); // Index just serves up our SPA index.html\\n        }\\n\\n        /// <summary>\\n        /// SPA landing with authorisation - this endpoint will typically not be directly navigated to by a user;\\n        /// rather it will be redirected to from the IndexWithoutAuthorisation and SpaFallback actions above\\n        /// in the case where a user is *not* authenticated but has come from an internal URL eg https://strictlyformypeeps.io\\n        /// </summary>\\n        [HttpGet(\\"/login-with-azure-ad\\")]\\n        [Authorize]\\n        public async Task<IActionResult> IndexWithAuthorisation()\\n        {\\n            return await Index(); // Index just serves up our SPA index.html\\n        }\\n\\n        /// <summary>\\n        /// This method returns a RedirectURL if a request is coming from an internal URL\\n        /// eg https://int.prd.our.cloud and is not authenticated.  In this case\\n        /// we likely want to trigger authentication by redirecting to an authorized endpoint\\n        /// </summary>\\n        string GetRedirectUrlIfUserIsInternalAndNotAuthenticated(string returnUrl)\\n        {\\n            // If a user is authenticated then we don\'t need to trigger authentication\\n            var isAuthenticated = User?.Identity?.Name != null;\\n            if (isAuthenticated)\\n                return null;\\n\\n            // This scenario is extremely unlikely but has been observed once during testing so we will cater for it\\n            var (isInternalRequest, requestUrl) = Request.GetIsInternalRequestAndDomain();\\n\\n            if (isInternalRequest) {\\n                var redirectUrl = $\\"/login-with-azure-ad{(string.IsNullOrEmpty(returnUrl) ? \\"\\" : \\"?returnUrl=\\" + WebUtility.UrlEncode(returnUrl))}\\";\\n                _logger.LogInformation(\\n                    \\"Request ({RequestURL}) has come from internal domain ({InternalDomain}) but is not authenticated; redirecting to {RedirectURL}\\",\\n                    requestUrl, AuthExtensions.InternalBaseUrl, redirectUrl);\\n\\n                return redirectUrl;\\n            }\\n\\n            return null;\\n        }\\n```\\n\\nThe code above allows anonymous requests to land in our app through the `AllowAnonymous` attribute. However, it checks the request when it comes in to see if:\\n\\n1. It\'s an internal request (i.e. the Request URL starts \\"[https://strictly4mypeeps.io/\\"](https://strictly4mypeeps.io/\\"))\\n2. The current user is _not_ authenticated.\\n\\nIn this case the user is redirected to the [https://strictly4mypeeps.io/login-with-azure-ad](https://strictly4mypeeps.io/login-with-azure-ad) route which is decorated with the `Authorize` attribute. This will trigger authentication for our unauthenticated internal users and drive them through the Azure AD login process.\\n\\n## The mystery of no documentation\\n\\nI\'m so surprised that this approach hasn\'t yet been better documented on the (generally superb) ASP.Net Core docs. It\'s such a potentially useful approach; and in our case, money saving too! I hope the official docs feature something on this in future. If they do, and I\'ve just missed it (possible!) then please hit me up in the comments."},{"id":"/2020/02/21/web-workers-comlink-typescript-and-react","metadata":{"permalink":"/2020/02/21/web-workers-comlink-typescript-and-react","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2020-02-21-web-workers-comlink-typescript-and-react/index.md","source":"@site/blog/2020-02-21-web-workers-comlink-typescript-and-react/index.md","title":"Web Workers, comlink, TypeScript and React","description":"JavaScript is famously single threaded. However, if you\'re developing for the web, you may well know that this is not quite accurate. There are Web Workers:","date":"2020-02-21T00:00:00.000Z","formattedDate":"February 21, 2020","tags":[{"label":"web workers","permalink":"/tags/web-workers"},{"label":"comlink","permalink":"/tags/comlink"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"React","permalink":"/tags/react"}],"readingTime":9.68,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Web Workers, comlink, TypeScript and React","authors":"johnnyreilly","tags":["web workers","comlink","TypeScript","React"],"hide_table_of_contents":false},"prevItem":{"title":"Dual boot authentication with ASP.NET","permalink":"/2020/03/22/dual-boot-authentication-with-aspnetcore"},"nextItem":{"title":"From create-react-app to PWA","permalink":"/2020/01/31/from-create-react-app-to-pwa"}},"content":"JavaScript is famously single threaded. However, if you\'re developing for the web, you may well know that this is not quite accurate. There are [`Web Workers`](https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Using_web_workers):\\n\\n> A worker is an object created using a constructor (e.g. `Worker()`) that runs a named JavaScript file \u2014 this file contains the code that will run in the worker thread; workers run in another global context that is different from the current window.\\n\\nGiven that there is a way to use other threads for background processing, why doesn\'t this happen all the time? Well there\'s a number of reasons; not the least of which is the ceremony involved in interacting with Web Workers. Consider the following example that illustrates moving a calculation into a worker:\\n\\n```js\\n// main.js\\nfunction add2NumbersUsingWebWorker() {\\n  const myWorker = new Worker(\'worker.js\');\\n\\n  myWorker.postMessage([42, 7]);\\n  console.log(\'Message posted to worker\');\\n\\n  myWorker.onmessage = function (e) {\\n    console.log(\'Message received from worker\', e.data);\\n  };\\n}\\n\\nadd2NumbersUsingWebWorker();\\n\\n// worker.js\\nonmessage = function (e) {\\n  console.log(\'Worker: Message received from main script\');\\n  const result = e.data[0] * e.data[1];\\n  if (isNaN(result)) {\\n    postMessage(\'Please write two numbers\');\\n  } else {\\n    const workerResult = \'Result: \' + result;\\n    console.log(\'Worker: Posting message back to main script\');\\n    postMessage(workerResult);\\n  }\\n};\\n```\\n\\n_This is not simple._ It\'s hard to understand what\'s happening. Also, this approach only supports a single method call. I\'d much rather write something that looked more like this:\\n\\n```js\\n// main.js\\nfunction add2NumbersUsingWebWorker() {\\n  const myWorker = new Worker(\'worker.js\');\\n\\n  const total = myWorker.add2Numbers([42, 7]);\\n  console.log(\'Message received from worker\', total);\\n}\\n\\nadd2NumbersUsingWebWorker();\\n\\n// worker.js\\nexport function add2Numbers(firstNumber, secondNumber) {\\n  const result = firstNumber + secondNumber;\\n  return isNaN(result) ? \'Please write two numbers\' : \'Result: \' + result;\\n}\\n```\\n\\nThere\'s a way to do this using a library made by Google called [comlink](https://github.com/GoogleChromeLabs/comlink). This post will demonstrate how we can use this. We\'ll use TypeScript and webpack. We\'ll also examine how to integrate this approach into a React app.\\n\\n## A use case for a Web Worker\\n\\nLet\'s make ourselves a TypeScript web app. We\'re going to use `create-react-app` for this:\\n\\n```shell\\nnpx create-react-app webworkers-comlink-typescript-react --template typescript\\n```\\n\\nCreate a `takeALongTimeToDoSomething.ts` file alongside `index.tsx`:\\n\\n```ts\\nexport function takeALongTimeToDoSomething() {\\n  console.log(\'Start our long running job...\');\\n  const seconds = 5;\\n  const start = new Date().getTime();\\n  const delay = seconds * 1000;\\n\\n  while (true) {\\n    if (new Date().getTime() - start > delay) {\\n      break;\\n    }\\n  }\\n  console.log(\'Finished our long running job\');\\n}\\n```\\n\\nTo `index.tsx` add this code:\\n\\n```ts\\nimport { takeALongTimeToDoSomething } from \'./takeALongTimeToDoSomething\';\\n\\n// ...\\n\\nconsole.log(\'Do something\');\\ntakeALongTimeToDoSomething();\\nconsole.log(\'Do another thing\');\\n```\\n\\nWhen our application runs we see this behaviour:\\n\\n![](blocking.gif)\\n\\nThe app starts and logs `Do something` and `Start our long running job...` to the console. It then blocks the UI until the `takeALongTimeToDoSomething` function has completed running. During this time the screen is empty and unresponsive. This is a poor user experience.\\n\\n## Hello `worker-plugin` and `comlink`\\n\\nTo start using comlink we\'re going to need to eject our `create-react-app` application. The way `create-react-app` works is by giving you a setup that handles a high percentage of the needs for a typical web app. When you encounter an unsupported use case, you can run the `yarn eject` command to get direct access to the configuration of your setup.\\n\\nWeb Workers are not that commonly used in day to day development at present. Consequently there isn\'t yet a \\"plug\'n\'play\\" solution for workers supported by `create-react-app`. There\'s a number of potential ways to support this use case and you can track the various discussions happening against `create-react-app` that covers this. For now, let\'s eject with:\\n\\n```\\nyarn eject\\n```\\n\\nThen let\'s install the packages we\'re going to be using:\\n\\n- [`worker-plugin`](https://github.com/GoogleChromeLabs/worker-plugin) \\\\- this webpack plugin automatically compiles modules loaded in Web Workers\\n- `comlink` \\\\- this library provides the RPC-like experience that we want from our workers\\n\\n```\\nyarn add comlink worker-plugin\\n```\\n\\nWe now need to tweak our `webpack.config.js` to use the `worker-plugin`:\\n\\n```js\\nconst WorkerPlugin = require(\'worker-plugin\');\\n\\n// ....\\n\\n    plugins: [\\n      new WorkerPlugin(),\\n\\n// ....\\n```\\n\\nDo note that there\'s a number of `plugins` statements in the `webpack.config.js`. You want the top level one; look out for the `new HtmlWebpackPlugin` statement and place your `new WorkerPlugin(),` before that.\\n\\n## Workerize our slow process\\n\\nNow we\'re ready to take our long running process and move it into a worker. Inside the `src` folder, create a new folder called `my-first-worker`. Our worker is going to live in here. Into this folder we\'re going to add a `tsconfig.json` file:\\n\\n```\\n{\\n  \\"compilerOptions\\": {\\n    \\"strict\\": true,\\n    \\"target\\": \\"esnext\\",\\n    \\"module\\": \\"esnext\\",\\n    \\"lib\\": [\\n      \\"webworker\\",\\n      \\"esnext\\"\\n    ],\\n    \\"moduleResolution\\": \\"node\\",\\n    \\"noUnusedLocals\\": true,\\n    \\"sourceMap\\": true,\\n    \\"allowJs\\": false,\\n    \\"baseUrl\\": \\".\\"\\n  }\\n}\\n```\\n\\nThis file exists to tell TypeScript that this is a Web Worker. Do note the `\\"lib\\": [ \\"webworker\\"` usage which does exactly that.\\n\\nAlongside the `tsconfig.json` file, let\'s create an `index.ts` file. This will be our worker:\\n\\n```ts\\nimport { expose } from \'comlink\';\\nimport { takeALongTimeToDoSomething } from \'../takeALongTimeToDoSomething\';\\n\\nconst exports = {\\n  takeALongTimeToDoSomething,\\n};\\nexport type MyFirstWorker = typeof exports;\\n\\nexpose(exports);\\n```\\n\\nThere\'s a number of things happening in our small worker file. Let\'s go through this statement by statement:\\n\\n```ts\\nimport { expose } from \'comlink\';\\n```\\n\\nHere we\'re importing the `expose` method from comlink. Comlink\u2019s goal is to make *expose*d values from one thread available in the other. The `expose` method can be viewed as the comlink equivalent of `export`. It is used to export the RPC style signature of our worker. We\'ll see it\'s use later.\\n\\n```ts\\nimport { takeALongTimeToDoSomething } from \'../takeALongTimeToDoSomething\';\\n```\\n\\nHere we\'re going to import our `takeALongTimeToDoSomething` function that we wrote previously, so we can use it in our worker.\\n\\n```ts\\nconst exports = {\\n  takeALongTimeToDoSomething,\\n};\\n```\\n\\nHere we\'re creating the public facing API that we\'re going to expose.\\n\\n```ts\\nexport type MyFirstWorker = typeof exports;\\n```\\n\\nWe\'re going to want our worker to be strongly typed. This line creates a type called `MyFirstWorker` which is derived from our `exports` object literal.\\n\\n```ts\\nexpose(exports);\\n```\\n\\nFinally we expose the `exports` using comlink. We\'re done; that\'s our worker finished. Now let\'s consume it. Let\'s change our `index.tsx` file to use it. Replace our import of `takeALongTimeToDoSomething`:\\n\\n```ts\\nimport { takeALongTimeToDoSomething } from \'./takeALongTimeToDoSomething\';\\n```\\n\\nWith an import of `wrap` from comlink that creates a local `takeALongTimeToDoSomething` function that wraps interacting with our worker:\\n\\n```ts\\nimport { wrap } from \'comlink\';\\n\\nfunction takeALongTimeToDoSomething() {\\n  const worker = new Worker(\'./my-first-worker\', {\\n    name: \'my-first-worker\',\\n    type: \'module\',\\n  });\\n  const workerApi = wrap<import(\'./my-first-worker\').MyFirstWorker>(worker);\\n  workerApi.takeALongTimeToDoSomething();\\n}\\n```\\n\\nNow we\'re ready to demo our application using our function offloaded into a Web Worker. It now behaves like this:\\n\\n![](non-blocking.gif)\\n\\nThere\'s a number of exciting things to note here:\\n\\n1. The application is now non-blocking. Our long running function is now not preventing the UI from updating\\n2. The functionality is lazily loaded via a `my-first-worker.chunk.worker.js` that has been created by the `worker-plugin` and `comlink`.\\n\\n## Using Web Workers in React\\n\\nThe example we\'ve showed so far demostrates how you could use Web Workers and why you might want to. However, it\'s a far cry from a real world use case. Let\'s take the next step and plug our Web Worker usage into our React application. What would that look like? Let\'s find out.\\n\\nWe\'ll return `index.tsx` back to it\'s initial state. Then we\'ll make a simple adder function that takes some values and returns their total. To our `takeALongTimeToDoSomething.ts` module let\'s add:\\n\\n```ts\\nexport function takeALongTimeToAddTwoNumbers(number1: number, number2: number) {\\n  console.log(\'Start to add...\');\\n  const seconds = 5;\\n  const start = new Date().getTime();\\n  const delay = seconds * 1000;\\n  while (true) {\\n    if (new Date().getTime() - start > delay) {\\n      break;\\n    }\\n  }\\n  const total = number1 + number2;\\n  console.log(\'Finished adding\');\\n  return total;\\n}\\n```\\n\\nLet\'s start using our long running calculator in a React component. We\'ll update our `App.tsx` to use this function and create a simple adder component:\\n\\n```tsx\\nimport React, { useState } from \'react\';\\nimport \'./App.css\';\\nimport { takeALongTimeToAddTwoNumbers } from \'./takeALongTimeToDoSomething\';\\n\\nconst App: React.FC = () => {\\n  const [number1, setNumber1] = useState(1);\\n  const [number2, setNumber2] = useState(2);\\n\\n  const total = takeALongTimeToAddTwoNumbers(number1, number2);\\n\\n  return (\\n    <div className=\\"App\\">\\n      <h1>Web Workers in action!</h1>\\n\\n      <div>\\n        <label>Number to add: </label>\\n        <input\\n          type=\\"number\\"\\n          onChange={(e) => setNumber1(parseInt(e.target.value))}\\n          value={number1}\\n        />\\n      </div>\\n      <div>\\n        <label>Number to add: </label>\\n        <input\\n          type=\\"number\\"\\n          onChange={(e) => setNumber2(parseInt(e.target.value))}\\n          value={number2}\\n        />\\n      </div>\\n      <h2>Total: {total}</h2>\\n    </div>\\n  );\\n};\\n\\nexport default App;\\n```\\n\\nWhen you try it out you\'ll notice that entering a single digit locks the UI for 5 seconds whilst it adds the numbers. From the moment the cursor stops blinking to the moment the screen updates the UI is non-responsive:\\n\\n![](blocking-react.gif)\\n\\nSo far, so classic. Let\'s Web Workerify this!\\n\\nWe\'ll update our `my-first-worker/index.ts` to import this new function:\\n\\n```ts\\nimport { expose } from \'comlink\';\\nimport {\\n  takeALongTimeToDoSomething,\\n  takeALongTimeToAddTwoNumbers,\\n} from \'../takeALongTimeToDoSomething\';\\n\\nconst exports = {\\n  takeALongTimeToDoSomething,\\n  takeALongTimeToAddTwoNumbers,\\n};\\nexport type MyFirstWorker = typeof exports;\\n\\nexpose(exports);\\n```\\n\\nAlongside our `App.tsx` file let\'s create an `App.hooks.ts` file.\\n\\n```ts\\nimport { wrap, releaseProxy } from \'comlink\';\\nimport { useEffect, useState, useMemo } from \'react\';\\n\\n/**\\n * Our hook that performs the calculation on the worker\\n */\\nexport function useTakeALongTimeToAddTwoNumbers(\\n  number1: number,\\n  number2: number\\n) {\\n  // We\'ll want to expose a wrapping object so we know when a calculation is in progress\\n  const [data, setData] = useState({\\n    isCalculating: false,\\n    total: undefined as number | undefined,\\n  });\\n\\n  // acquire our worker\\n  const { workerApi } = useWorker();\\n\\n  useEffect(() => {\\n    // We\'re starting the calculation here\\n    setData({ isCalculating: true, total: undefined });\\n\\n    workerApi\\n      .takeALongTimeToAddTwoNumbers(number1, number2)\\n      .then((total) => setData({ isCalculating: false, total })); // We receive the result here\\n  }, [workerApi, setData, number1, number2]);\\n\\n  return data;\\n}\\n\\nfunction useWorker() {\\n  // memoise a worker so it can be reused; create one worker up front\\n  // and then reuse it subsequently; no creating new workers each time\\n  const workerApiAndCleanup = useMemo(() => makeWorkerApiAndCleanup(), []);\\n\\n  useEffect(() => {\\n    const { cleanup } = workerApiAndCleanup;\\n\\n    // cleanup our worker when we\'re done with it\\n    return () => {\\n      cleanup();\\n    };\\n  }, [workerApiAndCleanup]);\\n\\n  return workerApiAndCleanup;\\n}\\n\\n/**\\n * Creates a worker, a cleanup function and returns it\\n */\\nfunction makeWorkerApiAndCleanup() {\\n  // Here we create our worker and wrap it with comlink so we can interact with it\\n  const worker = new Worker(\'./my-first-worker\', {\\n    name: \'my-first-worker\',\\n    type: \'module\',\\n  });\\n  const workerApi = wrap<import(\'./my-first-worker\').MyFirstWorker>(worker);\\n\\n  // A cleanup function that releases the comlink proxy and terminates the worker\\n  const cleanup = () => {\\n    workerApi[releaseProxy]();\\n    worker.terminate();\\n  };\\n\\n  const workerApiAndCleanup = { workerApi, cleanup };\\n\\n  return workerApiAndCleanup;\\n}\\n```\\n\\nThe `useWorker` and `makeWorkerApiAndCleanup` functions make up the basis of a shareable worker hooks approach. It would take very little work to paramaterise them so this could be used elsewhere. That\'s outside the scope of this post but would be extremely straightforward to accomplish.\\n\\nTime to test! We\'ll change our `App.tsx` to use the new `useTakeALongTimeToAddTwoNumbers` hook:\\n\\n```tsx\\nimport React, { useState } from \'react\';\\nimport \'./App.css\';\\nimport { useTakeALongTimeToAddTwoNumbers } from \'./App.hooks\';\\n\\nconst App: React.FC = () => {\\n  const [number1, setNumber1] = useState(1);\\n  const [number2, setNumber2] = useState(2);\\n\\n  const total = useTakeALongTimeToAddTwoNumbers(number1, number2);\\n\\n  return (\\n    <div className=\\"App\\">\\n      <h1>Web Workers in action!</h1>\\n\\n      <div>\\n        <label>Number to add: </label>\\n        <input\\n          type=\\"number\\"\\n          onChange={(e) => setNumber1(parseInt(e.target.value))}\\n          value={number1}\\n        />\\n      </div>\\n      <div>\\n        <label>Number to add: </label>\\n        <input\\n          type=\\"number\\"\\n          onChange={(e) => setNumber2(parseInt(e.target.value))}\\n          value={number2}\\n        />\\n      </div>\\n      <h2>\\n        Total:{\' \'}\\n        {total.isCalculating ? (\\n          <em>Calculating...</em>\\n        ) : (\\n          <strong>{total.total}</strong>\\n        )}\\n      </h2>\\n    </div>\\n  );\\n};\\n\\nexport default App;\\n```\\n\\nNow our calculation takes place off the main thread and the UI is no longer blocked!\\n\\n![](non-blocking-react.gif)\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/integrating-web-workers-in-a-react-app-with-comlink/)\\n\\n[The source code for this project can be found here.](https://github.com/johnnyreilly/webworkers-comlink-typescript-react)"},{"id":"/2020/01/31/from-create-react-app-to-pwa","metadata":{"permalink":"/2020/01/31/from-create-react-app-to-pwa","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2020-01-31-from-create-react-app-to-pwa/index.md","source":"@site/blog/2020-01-31-from-create-react-app-to-pwa/index.md","title":"From create-react-app to PWA","description":"Progressive Web Apps are a (terribly named) wonderful idea. You can build an app once using web technologies which serves all devices and form factors. It can be accessible over the web, but also surface on the home screen of your Android / iOS device. That app can work offline, have a splash screen when it launches and have notifications too.","date":"2020-01-31T00:00:00.000Z","formattedDate":"January 31, 2020","tags":[{"label":"create-react-app","permalink":"/tags/create-react-app"},{"label":"PWA","permalink":"/tags/pwa"}],"readingTime":10.595,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"From create-react-app to PWA","authors":"johnnyreilly","tags":["create-react-app","PWA"],"hide_table_of_contents":false},"prevItem":{"title":"Web Workers, comlink, TypeScript and React","permalink":"/2020/02/21/web-workers-comlink-typescript-and-react"},"nextItem":{"title":"LICENSE to kill your PWA","permalink":"/2020/01/21/license-to-kill-your-pwa"}},"content":"Progressive Web Apps are a (terribly named) wonderful idea. You can build an app _once_ using web technologies which serves all devices and form factors. It can be accessible over the web, but also surface on the home screen of your Android / iOS device. That app can work offline, have a splash screen when it launches and have notifications too.\\n\\nPWAs can be a money saver for your business. The alternative, should you want an app experience for your users, is building the same application using three different technologies (one for web, one for Android and one for iOS). When you take this path it\'s hard to avoid a multiplication of cost and complexity. It often leads to dividing up the team as each works on a different stack. It\'s common to lose a certain amount of focus as a consequence. PWAs can help here; they are a compelling alternative, not just from a developers standpoint, but from a resourcing one too.\\n\\nHowever, the downside of PWAs is that they are more complicated than normal web apps. Writing one from scratch is just less straightforward than a classic web app. There are easy onramps to building a PWA that help you fall into the pit of success. This post will highlight one of these. How you can travel from zero to a PWA of your very own using React and TypeScript.\\n\\nThis post presumes knowledge of:\\n\\n- React\\n- TypeScript\\n- Node\\n\\n## From console to web app\\n\\nTo create our PWA we\'re going to use [`create-react-app`](https://create-react-app.dev/). This excellent project has long had inbuilt support for making PWAs. In recent months that support has matured to a very satisfactory level. To create ourselves a TypeScript React app using `create-react-app` enter this `npx` command at the console:\\n\\n```shell\\nnpx create-react-app pwa-react-typescript --template typescript\\n```\\n\\nThis builds you a react web app built with TypeScript; it can be tested locally with:\\n\\n```shell\\ncd pwa-react-typescript\\nyarn start\\n```\\n\\n## From web app to PWA\\n\\nFrom web app to PWA is incredibly simple; it\u2019s just a question of opting in to offline behaviour. If you open up the `index.tsx` file in your newly created project you\'ll find this code:\\n\\n```ts\\n// If you want your app to work offline and load faster, you can change\\n// unregister() to register() below. Note this comes with some pitfalls.\\n// Learn more about service workers: https://bit.ly/CRA-PWA\\nserviceWorker.unregister();\\n```\\n\\nAs the hint suggests, swap `serviceWorker.unregister()` for `serviceWorker.register()` and you now have a PWA. Amazing! What does this mean? Well to [quote the docs](https://create-react-app.dev/docs/making-a-progressive-web-app/#why-opt-in):\\n\\n> - All static site assets are cached so that your page loads fast on subsequent visits, regardless of network connectivity (such as 2G or 3G). Updates are downloaded in the background.\\n> - Your app will work regardless of network state, even if offline. This means your users will be able to use your app at 10,000 feet and on the subway.\\n>\\n> ... it will take care of generating a service worker file that will automatically precache all of your local assets and keep them up to date as you deploy updates. The service worker will use a [cache-first strategy](https://developers.google.com/web/fundamentals/instant-and-offline/offline-cookbook/#cache-falling-back-to-network)for handling all requests for local assets, including [navigation requests](https://developers.google.com/web/fundamentals/primers/service-workers/high-performance-loading#first_what_are_navigation_requests) for your HTML, ensuring that your web app is consistently fast, even on a slow or unreliable network.\\n\\nUnder the bonnet, `create-react-app` is achieving this through the use of technology called [\\"Workbox\\"](https://developers.google.com/web/tools/workbox). Workbox describes itself as:\\n\\n> a set of libraries and Node modules that make it easy to cache assets and take full advantage of features used to build [Progressive Web Apps](https://developers.google.com/web/progressive-web-apps/).\\n\\nThe good folks of Google are aware that writing your own PWA can be tricky. There\'s much new behaviour to configure and be aware of; it\'s easy to make mistakes. Workbox is there to help ease the way forward by implementing default strategies for caching / offline behaviour which can be controlled through configuration.\\n\\nA downside of the usage of `Workbox` in `create-react-app` is that (as with most things `create-react-app`) there\'s little scope for configuration of your own if the defaults don\'t serve your purpose. This may change in the future, indeed [there\'s an open PR that adds this support](https://github.com/facebook/create-react-app/pull/5369).\\n\\n## Icons and splash screens and A2HS, oh my!\\n\\nBut it\'s not just an offline experience that makes this a PWA. Other important factors are:\\n\\n- That the app can be added to your home screen (A2HS AKA \\"installed\\").\\n- That the app has a name and an icon which can be customised.\\n- That there\'s a splash screen displayed to the user as the app starts up.\\n\\nAll of the above is \\"in the box\\" with `create-react-app`. Let\'s start customizing these.\\n\\nFirst of all, we\'ll give our app a name. Fire up `index.html` and replace `&lt;title&gt;React App&lt;/title&gt;` with `&lt;title&gt;My PWA&lt;/title&gt;`. (Feel free to concoct a more imaginative name than the one I\'ve suggested.) Next open up `manifest.json` and replace:\\n\\n```json\\n\\"short_name\\": \\"React App\\",\\n  \\"name\\": \\"Create React App Sample\\",\\n```\\n\\nwith:\\n\\n```json\\n\\"short_name\\": \\"My PWA\\",\\n  \\"name\\": \\"My PWA\\",\\n```\\n\\nYour app now has a name. The question you might be asking is: what is this `manifest.json` file? Well to [quote the good folks of Google](https://developers.google.com/web/fundamentals/web-app-manifest):\\n\\n> The [web app manifest](https://developer.mozilla.org/en-US/docs/Web/Manifest) is a simple JSON file that tells the browser about your web application and how it should behave when \'installed\' on the user\'s mobile device or desktop. Having a manifest is required by Chrome to show the [Add to Home Screen prompt](https://developers.google.com/web/fundamentals/app-install-banners/).\\n>\\n> A typical manifest file includes information about the app name, icons it should use, the start_url it should start at when launched, and more.\\n\\nSo the `manifest.json` is essentially metadata about your app. Here\'s what it should look like right now:\\n\\n```json\\n{\\n  \\"short_name\\": \\"My PWA\\",\\n  \\"name\\": \\"My PWA\\",\\n  \\"icons\\": [\\n    {\\n      \\"src\\": \\"favicon.ico\\",\\n      \\"sizes\\": \\"64x64 32x32 24x24 16x16\\",\\n      \\"type\\": \\"image/x-icon\\"\\n    },\\n    {\\n      \\"src\\": \\"logo192.png\\",\\n      \\"type\\": \\"image/png\\",\\n      \\"sizes\\": \\"192x192\\"\\n    },\\n    {\\n      \\"src\\": \\"logo512.png\\",\\n      \\"type\\": \\"image/png\\",\\n      \\"sizes\\": \\"512x512\\"\\n    }\\n  ],\\n  \\"start_url\\": \\".\\",\\n  \\"display\\": \\"standalone\\",\\n  \\"theme_color\\": \\"#000000\\",\\n  \\"background_color\\": \\"#ffffff\\"\\n}\\n```\\n\\nYou can use the above properties (and others not yet configured) to control how your app behaves. For instance, if you want to replace icons your app uses then it\'s a simple matter of:\\n\\n- placing new logo files in the `public` folder\\n- updating references to them in the `manifest.json`\\n- finally, for older Apple devices, updating the `&lt;link rel=\\"apple-touch-icon\\" ... /&gt;` in the `index.html`.\\n\\n## Where are we?\\n\\nSo far, we have a basic PWA in place. It\'s installable. You can run it locally and develop it with `yarn start`. You can build it for deployment with `yarn build`.\\n\\nWhat this isn\'t, is recognisably a web app. In the sense that it doesn\'t have support for different pages / URLs. We\'re typically going to want to break up our application this way. Let\'s do that now. We\'re going to use [`react-router`](https://github.com/ReactTraining/react-router); the de facto routing solution for React. To add it to our project (and the required type definitions for TypeScript) we use:\\n\\n```\\nyarn add react-router-dom @types/react-router-dom\\n```\\n\\nNow let\'s split up our app into a couple of pages. We\'ll replace the existing `App.tsx` with this:\\n\\n```tsx\\nimport React from \'react\';\\nimport { BrowserRouter as Router, Switch, Route, Link } from \'react-router-dom\';\\nimport About from \'./About\';\\nimport Home from \'./Home\';\\n\\nconst App: React.FC = () => (\\n  <Router>\\n    <nav>\\n      <ul>\\n        <li>\\n          <Link to=\\"/\\">Home</Link>\\n        </li>\\n        <li>\\n          <Link to=\\"/about\\">About</Link>\\n        </li>\\n      </ul>\\n    </nav>\\n    <Switch>\\n      <Route path=\\"/about\\">\\n        <About />\\n      </Route>\\n      <Route path=\\"/\\">\\n        <Home />\\n      </Route>\\n    </Switch>\\n  </Router>\\n);\\n\\nexport default App;\\n```\\n\\nThis will be our root page. It has the responsiblity of using `react-router` to render the pages we want to serve, and also to provide the links that allow users to navigate to those pages. In making our changes we\'ll have broken our test (which checked for a link we\'ve now deleted), so we\'ll fix it like so:\\n\\nReplace the `App.test.tsx` with this:\\n\\n```tsx\\nimport React from \'react\';\\nimport { render } from \'@testing-library/react\';\\nimport App from \'./App\';\\n\\ntest(\'renders about link\', () => {\\n  const { getByText } = render(<App />);\\n  const linkElement = getByText(/about/i);\\n  expect(linkElement).toBeInTheDocument();\\n});\\n```\\n\\nYou\'ll have noticed that in our new `App.tsx` we import two new components (or pages); `About` and `Home`. Let\'s create those. First `About.tsx`:\\n\\n```tsx\\nimport React from \'react\';\\n\\nconst About: React.FC = () => <h1>This is a PWA</h1>;\\n\\nexport default About;\\n```\\n\\nThen `Home.tsx`:\\n\\n```tsx\\nimport React from \'react\';\\n\\nconst Home: React.FC = () => <h1>Welcome to your PWA!</h1>;\\n\\nexport default Home;\\n```\\n\\n## Code splitting\\n\\nNow we\'ve split up our app into multiple sections, we\'re going to split the code too. A good way to improve loading times for PWAs is to ensure that the code is not built into big files. At the moment our app builds a `single-file.js`. If you run `yarn build` you\'ll see what this looks like:\\n\\n```\\n47.88 KB  build/static/js/2.89bc6648.chunk.js\\n  784 B     build/static/js/runtime-main.9c116153.js\\n  555 B     build/static/js/main.bc740179.chunk.js\\n  269 B     build/static/css/main.5ecd60fb.chunk.css\\n```\\n\\nNotice the `build/static/js/main.bc740179.chunk.js` file. This is our `single-file.js`. It represents the compiled output of building the TypeScript files that make up our app. It will grow and grow as our app grows, eventually becoming problematic from a user loading speed perspective.\\n\\n`create-react-app` is built upon webpack. There is excellent support for code splitting in webpack and hence [create-react-app supports it by default](https://reactjs.org/docs/code-splitting.html#code-splitting). Let\'s apply it to our app. Again we\'re going to change `App.tsx`.\\n\\nWhere we previously had:\\n\\n```tsx\\nimport About from \'./About\';\\nimport Home from \'./Home\';\\n```\\n\\nLet\'s replace with:\\n\\n```tsx\\nconst About = lazy(() => import(\'./About\'));\\nconst Home = lazy(() => import(\'./Home\'));\\n```\\n\\nThis is the syntax to lazily load components in React. You\'ll note that it internally uses the [dynamic `import()` syntax](https://github.com/tc39/proposal-dynamic-import) which webpack uses as a \\"split point\\".\\n\\nLet\'s also give React something to render whilst it waits for the dynamic imports to be resolved. Just inside our `&lt;Router&gt;` component we\'ll add a `&lt;Suspense&gt;` component too:\\n\\n```tsx\\n<Router>\\n  <Suspense fallback={<div>Loading...</div>}>{/*...*/}</Suspense>\\n</Router>\\n```\\n\\nThe `&lt;Suspense&gt;` component will render the `&lt;div&gt;Loading...&lt;/div&gt;` whilst it waits for a routes code to be dynamically loaded. So our final `App.tsx` component ends up looking like this:\\n\\n```tsx\\nimport React, { lazy, Suspense } from \'react\';\\nimport { BrowserRouter as Router, Switch, Route, Link } from \'react-router-dom\';\\nconst About = lazy(() => import(\'./About\'));\\nconst Home = lazy(() => import(\'./Home\'));\\n\\nconst App: React.FC = () => (\\n  <Router>\\n    <Suspense fallback={<div>Loading...</div>}>\\n      <nav>\\n        <ul>\\n          <li>\\n            <Link to=\\"/\\">Home</Link>\\n          </li>\\n          <li>\\n            <Link to=\\"/about\\">About</Link>\\n          </li>\\n        </ul>\\n      </nav>\\n      <Switch>\\n        <Route path=\\"/about\\">\\n          <About />\\n        </Route>\\n        <Route path=\\"/\\">\\n          <Home />\\n        </Route>\\n      </Switch>\\n    </Suspense>\\n  </Router>\\n);\\n\\nexport default App;\\n```\\n\\nThis is now a code split application. How can we tell? If we run `yarn build` again we\'ll see something like this:\\n\\n```\\n47.88 KB          build/static/js/2.89bc6648.chunk.js\\n  1.18 KB (+428 B)  build/static/js/runtime-main.415ab5ea.js\\n  596 B (+41 B)     build/static/js/main.e60948bb.chunk.js\\n  269 B             build/static/css/main.5ecd60fb.chunk.css\\n  233 B             build/static/js/4.0c85e1cb.chunk.js\\n  228 B             build/static/js/3.eed49094.chunk.js\\n```\\n\\nNote that we now have multiple `*.chunk.js` files. Our initial `main.*.chunk.js` and then `3.*.chunk.js` representing `Home.tsx` and `4.*.chunk.js` representing `About.tsx`.\\n\\nAs we continue to build out our app from this point we\'ll have a great approach in place to ensure that users load files as they need to and that those files should not be too large. Great performance which will scale.\\n\\n## Deploy your PWA\\n\\nNow that we have our basic PWA in place, let\'s deploy it so the outside world can appreciate it. We\'re going to use [Netlify](https://www.netlify.com/) for this.\\n\\nThe source code of our PWA lives on GitHub here: https://github.com/johnnyreilly/pwa-react-typescript\\n\\nWe\'re going to log into Netlify, click on the \\"Create a new site\\" option and select GitHub as the provider. We\'ll need to authorize Netlify to access our GitHub.\\n\\n![](netlify-auth.png)\\n\\nYou may need to click the \\"Configure Netlify on GitHub\\" button to grant permissions for Netlify to access your repo like so:\\n\\n![](netlify-repo-permissions.png)\\n\\nThen you can select your repo from within Netlify. All of the default settings that Netlify provides should work for our use case:\\n\\n![](netlify-deploy-settings.png)\\n\\nLet\'s hit the magic \\"Deploy site\\" button! In a matter of minutes you\'ll find that Netlify has deployed your PWA.\\n\\n![](netlify-deployed.png)\\n\\nIf we browse to the URL provided by Netlify we\'ll be able to see the deployed PWA in action. (You also have the opportunity to set up a custom domain name that you would typically want outside of a simple demo such as this.) Importantly this will be served over HTTPS which will allow our Service Worker to operate.\\n\\nNow that we know it\'s there, let\'s see how what we\'ve built holds up according to the professionals. We\'re going to run the Google Chrome Developer Tools Audit against our PWA:\\n\\n![](pwa-audit.png)\\n\\nThat is a good start for our PWA!\\n\\n[This post was originally published on LogRocket.](https://blog.logrocket.com/from-create-react-app-to-pwa/)\\n\\n[The source code for this project can be found here.](https://github.com/johnnyreilly/pwa-react-typescript)"},{"id":"/2020/01/21/license-to-kill-your-pwa","metadata":{"permalink":"/2020/01/21/license-to-kill-your-pwa","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2020-01-21-license-to-kill-your-pwa/index.md","source":"@site/blog/2020-01-21-license-to-kill-your-pwa/index.md","title":"LICENSE to kill your PWA","description":"Update: 26/01/2020 - LICENSE to kill revoked!","date":"2020-01-21T00:00:00.000Z","formattedDate":"January 21, 2020","tags":[],"readingTime":3.805,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"LICENSE to kill your PWA","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false},"prevItem":{"title":"From create-react-app to PWA","permalink":"/2020/01/31/from-create-react-app-to-pwa"},"nextItem":{"title":"EF Core 3.1 breaks left join with no navigation property","permalink":"/2020/01/02/ef-core-31-breaks-left-join-with-no-navigation-property"}},"content":"## Update: 26/01/2020 - LICENSE to kill revoked!\\n\\nFollowing the original publication of this post I received this tweet suggesting we should change the behaviour of the underlying `terser-webpack-plugin`:\\n\\n> Send a PR to change the name to .LICENSE.txt by default.\\n>\\n> \u2014 Tobias Koppers (@wSokra) [January 22, 2020](https://twitter.com/wSokra/status/1220069497660411904?ref_src=twsrc%5Etfw)\\n\\n<script async=\\"\\" src=\\"https://platform.twitter.com/widgets.js\\" charSet=\\"utf-8\\"><\/script>\\n\\nThat seemed like an excellent idea! I raised [this PR](https://github.com/webpack-contrib/terser-webpack-plugin/pull/210) which changes the behaviour such that instead of `.LICENSE` files being produced, `.LICENSE.txt` files are pumped out instead. Crucially they are IIS (and other servers) friendly. The great news is that future users of webpack / create-react-app etc will not face this problem at all; result!\\n\\n## The tragedy\\n\\nRecently my beloved PWA died. I didn\'t realise it at first. It wasn\'t until a week or so after the tragedy that I realised he\'d gone. In his place was the stale memory of service workers gone by. Last week\'s code; cached and repeatedly served up to a disappointed audience. Terrible news.\\n\\nWhat had happened? What indeed. The problem was quirky and (now that I know the answer) I\'m going to share it with you. Because it\'s entirely non-obvious.\\n\\n## The mystery\\n\\nOnce I realised that I was repeatedly being served up an old version of my PWA, I got to wondering.... Why? What\'s happening? What\'s wrong? What did I do? I felt bad. I stared at the ceiling. I sighed and opened my Chrome devtools. With no small amount of sadness I went to the `Application` tab, hit `Service Workers` and then `Unregister`.\\n\\nThen I hit refresh and took a look at console. I saw this:\\n\\n![](LICENSE-cannot-be-cached.png)\\n\\nWhat does this mean? Something about a \\"bad-precaching-response\\". And apparently this was happening whilst trying to load this resource: `/static/js/6.20102e99.chunk.js.LICENSE?__WB_REVISION__=e2fc36`\\n\\nThis `404` was preventing pre-caching from executing successfully. This was what was killing my PWA. This was the perpetrator. How to fix this? Read on!\\n\\n## The investigation\\n\\nTime to find out what\'s going on. I dropped that URL into my browser to see what would happen. `404` city man:\\n\\n![](LICENSE-file-screwing-me-over.png)\\n\\nSo, to disk. I took a look at what was actually on the server in that location. Sure enough, the file existed. When I opened it up I found this:\\n\\n```js\\n/**\\n * A better abstraction over CSS.\\n *\\n * @copyright Oleg Isonen (Slobodskoi) / Isonen 2014-present\\n * @website https://github.com/cssinjs/jss\\n * @license MIT\\n */\\n\\n/*\\nobject-assign\\n(c) Sindre Sorhus\\n@license MIT\\n*/\\n\\n/** @license React v16.12.0\\n * react.production.min.js\\n *\\n * Copyright (c) Facebook, Inc. and its affiliates.\\n *\\n * This source code is licensed under the MIT license found in the\\n * LICENSE file in the root directory of this source tree.\\n */\\n\\n/** @license React v16.12.0\\n * react-dom.production.min.js\\n *\\n * Copyright (c) Facebook, Inc. and its affiliates.\\n *\\n * This source code is licensed under the MIT license found in the\\n * LICENSE file in the root directory of this source tree.\\n */\\n\\n/** @license React v0.18.0\\n * scheduler.production.min.js\\n *\\n * Copyright (c) Facebook, Inc. and its affiliates.\\n *\\n * This source code is licensed under the MIT license found in the\\n * LICENSE file in the root directory of this source tree.\\n */\\n\\n/** @license React v16.12.0\\n * react-is.production.min.js\\n *\\n * Copyright (c) Facebook, Inc. and its affiliates.\\n *\\n * This source code is licensed under the MIT license found in the\\n * LICENSE file in the root directory of this source tree.\\n */\\n```\\n\\nWhat is this? Well, as the name of the file suggests, it\'s licenses. For some reason, my build was scraping the licenses from the head of some of my files and placing them in a separate `6.20102e99.chunk.js.LICENSE` file. Doing some more digging I happened upon [this discussion against the `create-react-app`](https://github.com/facebook/create-react-app/issues/6441) project. It\'s worth saying, that my PWA was an ejected `create-react-app` project.\\n\\nIt turned out the the issue was related to the [`terser-webpack-plugin`](https://github.com/webpack-contrib/terser-webpack-plugin). The default behaviour performs this kind of license file extraction. The app was being served by an IIS server and it wasn\'t configured to support the `.LICENSE` file type.\\n\\n## The resolution\\n\\nThe simplest solution was simply this: wave goodbye to `LICENSE` files. If you haven\'t ejected from your `create-react-app` then this might be a problem. But since I had, I was able to make this tweak to the terser settings in the `webpack.config.js`:\\n\\n```js\\nnew TerserPlugin({\\n    /* TURN OFF LICENSE FILES - SEE https://github.com/facebook/create-react-app/issues/6441 */\\n    extractComments: false,\\n    /* TURN OFF LICENSE FILES - Tweak by John Reilly */\\n    terserOptions: {\\n        // ....\\n```\\n\\nAnd with this we say goodbye to our `404`s and hello to a resurrected PWA!"},{"id":"/2020/01/02/ef-core-31-breaks-left-join-with-no-navigation-property","metadata":{"permalink":"/2020/01/02/ef-core-31-breaks-left-join-with-no-navigation-property","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2020-01-02-ef-core-31-breaks-left-join-with-no-navigation-property/index.md","source":"@site/blog/2020-01-02-ef-core-31-breaks-left-join-with-no-navigation-property/index.md","title":"EF Core 3.1 breaks left join with no navigation property","description":"Just recently my team took on the challenge of upgrading our codebase from .NET Core 2.2 to .NET Core 3.1. Along the way we encountered a quirky issue which caused us much befuddlement. Should you be befuddled too, then maybe this can help you.","date":"2020-01-02T00:00:00.000Z","formattedDate":"January 2, 2020","tags":[{"label":"Entity Framework","permalink":"/tags/entity-framework"},{"label":"left join","permalink":"/tags/left-join"},{"label":"navigation property","permalink":"/tags/navigation-property"},{"label":"broken","permalink":"/tags/broken"}],"readingTime":2.375,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"EF Core 3.1 breaks left join with no navigation property","authors":"johnnyreilly","tags":["Entity Framework","left join","navigation property","broken"],"hide_table_of_contents":false},"prevItem":{"title":"LICENSE to kill your PWA","permalink":"/2020/01/21/license-to-kill-your-pwa"},"nextItem":{"title":"Teams notification webhooks","permalink":"/2019/12/18/teams-notification-webhooks"}},"content":"Just recently my team took on the challenge of upgrading our codebase from .NET Core 2.2 to .NET Core 3.1. Along the way we encountered a quirky issue which caused us much befuddlement. Should you be befuddled too, then maybe this can help you.\\n\\nWhilst running our app, we started encountering an error with an Entity Framework Query that looked like this:\\n\\n```cs\\nvar stuffWeCareAbout = await context.Things\\n    .Include(thing => thing.ThisIsFine)\\n    .Include(thing => thing.Problematic)\\n    .Where(thing => thing.CreatedOn > startFromThisTime && thing.CreatedOn < endAtThisTime)\\n    .OrderByDescending(thing => thing.CreatedOn)\\n    .ToArrayAsync();\\n```\\n\\n## Join me!\\n\\nAs EF Core tried to join from the `Things` table to the `Problematic` table (some obfuscation in table names here), that which worked in .NET Core 2.2 was _not_ working in .NET Core 3.1. Digging into the issue, we discovered EF Core was generating an invalid `LEFT JOIN`:\\n\\n```sql\\nfail: Microsoft.EntityFrameworkCore.Database.Command[20102]\\n      Failed executing DbCommand (18ms) [Parameters=[@__startFromThisTime_0=\'?\' (DbType = DateTime2), @__endAtThisTime_1=\'?\' (DbType = DateTime2)], CommandType=\'Text\', CommandTimeout=\'30\']\\n      SELECT [o].[ThingId], [o].[AnonymousId], [o].[CreatedOn],  [o].[Status], [o].[UpdatedOn], [o0].[Id], [o0].[ThingId], [o0].[Name], [o1].[ThingId], [o1].[Status], [o1].[CreatedOn], [o1].[ThingThingId], [o1].[SentOn]\\n      FROM [Things] AS [o]\\n      LEFT JOIN [ThisIsFines] AS [o0] ON [o].[ThingId] = [o0].[ThingId]\\n      LEFT JOIN [Problematic] AS [o1] ON [o].[ThingId] = [o1].[ThingThingId]\\n      WHERE ([o].[CreatedOn] @__startFromThisTime_0) AND ([o].[CreatedOn] < @__endAtThisTime_1)\\n      ORDER BY [o].[CreatedOn] DESC, [o].[ThingId], [o1].[ThingId], [o1].[Status]\\nMicrosoft.EntityFrameworkCore.Database.Command: Error: Failed executing DbCommand (18ms) [Parameters=[@__startFromThisTime_0=\'?\' (DbType = DateTime2), @__endAtThisTime_1=\'?\' (DbType = DateTime2)], CommandType=\'Text\', CommandTimeout=\'30\']\\nSELECT [o].[ThingId], [o].[AnonymousId], [o].[CreatedOn], [o].[Status], [o].[UpdatedOn], [o0].[Id], [o0].[ThingId], [o0].[Name], [o1].[ThingId], [o1].[Status], [o1].[CreatedOn], [o1].[ThingThingId], [o1].[SentOn]\\nFROM [Things] AS [o]\\nLEFT JOIN [ThisIsFines] AS [o0] ON [o].[ThingId] = [o0].[ThingId]\\nLEFT JOIN [Problematic] AS [o1] ON [o].[ThingId] = [o1].[ThingThingId]\\nWHERE ([o].[CreatedOn] @__startFromThisTime_0) AND ([o].[CreatedOn] < @__endAtThisTime_1)\\nORDER BY [o].[CreatedOn] DESC, [o].[ThingId], [o1].[ThingId], [o1].[Status]\\n```\\n\\nDo you see it? Probably not; it took us a while too... The issue lay here:\\n\\n```sql\\nLEFT JOIN [Problematic] AS [o1] ON [o].[ThingId] = [o1].[ThingThingId]\\n```\\n\\nThis should actually have been:\\n\\n```sql\\nLEFT JOIN [Problematic] AS [o1] ON [o].[ThingId] = [o1].[ThingId]\\n```\\n\\nFor some reason EF Core was looking for `ThingThingId` where it should have looked for `ThingId`. But why?\\n\\n## Navigation properties to the rescue!\\n\\nThis was the `Problematic` class:\\n\\n```cs\\nusing System;\\nusing System.ComponentModel.DataAnnotations;\\nusing System.ComponentModel.DataAnnotations.Schema;\\n\\nnamespace Treasury.Data.Entities\\n{\\n    public class Problematic\\n    {\\n        [ForeignKey(\\"Thing\\")]\\n        [Required]\\n        public Guid ThingId { get; set; }\\n        [Required]\\n        public DateTime CreatedOn { get; set; }\\n        public DateTime SentOn { get; set; }\\n    }\\n}\\n```\\n\\nIf you look closely you\'ll see it has a `ForeignKey` but _no_ accompanying Navigation property. So let\'s add one:\\n\\n```cs\\nusing System;\\nusing System.ComponentModel.DataAnnotations;\\nusing System.ComponentModel.DataAnnotations.Schema;\\n\\nnamespace Our.App\\n{\\n    public class Problematic\\n    {\\n        [ForeignKey(\\"Thing\\")]\\n        [Required]\\n        public Guid ThingId { get; set; }\\n        [Required]\\n        public DateTime CreatedOn { get; set; }\\n        public DateTime SentOn { get; set; }\\n\\n        /* THIS NAVIGATION PROPERTY IS WHAT WE NEEDED!!! */\\n        public virtual Thing Thing { get; set; }\\n    }\\n}\\n```\\n\\nWith this in place our app starts generating the SQL we need."},{"id":"/2019/12/18/teams-notification-webhooks","metadata":{"permalink":"/2019/12/18/teams-notification-webhooks","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2019-12-18-teams-notification-webhooks/index.md","source":"@site/blog/2019-12-18-teams-notification-webhooks/index.md","title":"Teams notification webhooks","description":"Teams notifications are mighty useful. You can send them using Markdown via a webhook.","date":"2019-12-18T00:00:00.000Z","formattedDate":"December 18, 2019","tags":[{"label":"Microsoft Teams","permalink":"/tags/microsoft-teams"},{"label":"connectors","permalink":"/tags/connectors"},{"label":"notifications","permalink":"/tags/notifications"},{"label":"webhook","permalink":"/tags/webhook"}],"readingTime":3.19,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Teams notification webhooks","authors":"johnnyreilly","tags":["Microsoft Teams","connectors","notifications","webhook"],"image":"./teams-notification.gif","hide_table_of_contents":false},"prevItem":{"title":"EF Core 3.1 breaks left join with no navigation property","permalink":"/2020/01/02/ef-core-31-breaks-left-join-with-no-navigation-property"},"nextItem":{"title":"Definitely Typed: The Movie","permalink":"/2019/10/08/definitely-typed-movie"}},"content":"Teams notifications are mighty useful. You can send them using Markdown via a webhook.\\n\\nThis post will explain the following:\\n\\n1. How you can automate the sending of notifications using Teams.\\n2. How Teams supports Markdown in notifications.\\n3. How you can use ASP.Net Core to automate sending notifications.\\n\\n## Notifications via Webhooks\\n\\nNow, it\'s not obvious from Teams that there is a simple webhooks integration for Teams, but there is. It\'s tucked away under \\"Connectors\\". If you want to create a webhook of your own, find your team, your channel, click on the menu, then connectors and create a hook. Like so:\\n\\n![animation of setting up a webhook connector in Teams](teams-webhook-connector.gif)\\n\\nWith the URL you\'ve just obtained, you are now free to send notifications to that channel via a simple `curl`:\\n\\n```shell\\ncurl -H \\"Content-Type: application/json\\" -d \\"{\\\\\\"text\\\\\\": \\\\\\"Hello World\\\\\\"}\\" https://outlook.office.com/webhook/big-long-guid1/IncomingWebhook/big-long-guid2\\n```\\n\\n## Markdown\\n\\nLet\'s see if we can make this more interesting. It turns out that the the webhook can receive JSON as the body of the payload. And there\'s 3 properties we\'d like our JSON to contain:\\n\\n1. `title` - this is optional and is the title of your notification if supplied.\\n2. `textFormat` - provide the value `\\"markdown\\"` and then...\\n3. `text` - provide your markdown notification content!\\n\\nSo if we have a notification payload file called `down.json`:\\n\\n```json\\n{\\n  \\"title\\": \\"Your Notification Title\\",\\n  \\"textFormat\\": \\"markdown\\",\\n  \\"text\\": \\"*Wow*\\\\nThis is [markdown](https://en.wikipedia.org/wiki/Markdown)!\\\\n![do a little dance!](https://media.giphy.com/media/YJ5OlVLZ2QNl6/giphy.gif)\\\\n**Huzzah**!\\"\\n}\\n```\\n\\nWe can trigger it with this `curl`:\\n\\n```shell\\ncurl -H \\"Content-Type: application/json\\" -d @down.json https://outlook.office.com/webhook/big-long-guid1/IncomingWebhook/big-long-guid2\\n```\\n\\nAs you can see from the example above, you can use all the qualities of Markdown that you know and love. Text, bold text, italics, links and even images too. It\'s _great_!\\n\\n![animation of Teams notification](teams-notification.gif)\\n\\n## ASP.Net Core\\n\\nFinally, I wanted to illustrate just how simple the WebHooks API makes plugging notifications into an existing app. In our case we\'re going to use ASP.Net Core, but really there\'s nothing particular about how we\'re going to do this.\\n\\nHere\'s a class called `TeamsNotificationService`. It exposes 2 methods:\\n\\n- `SendNotification` which allows the consumer to just provide a `title` and a `message` - you could consume this from anywhere in your app and use it to publish the notification of your choice.\\n- `SendExcitingNotification` which actually uses `SendNotification` and illustrates how you might provide an exciting notification to publish out.\\n\\n```cs\\nusing System;\\nusing System.Collections.Generic;\\nusing System.Net.Http;\\nusing System.Net.Http.Headers;\\nusing System.Threading.Tasks;\\n\\nnamespace My.Services {\\n    public interface ITeamsNotificationService {\\n        Task SendNotification(string title, string message);\\n        Task SendExcitingNotification(Guid someAppId, string person);\\n    }\\n\\n    public class TeamsNotificationService : ITeamsNotificationService {\\n\\n        // in Startup.ConfigureServices you\'re going to want to add this line:\\n        // services.AddHttpClient(TeamsNotificationService.TEAMS_NOTIFIER_CLIENT);\\n\\n        public const string TEAMS_NOTIFIER_CLIENT = \\"TEAMS_NOTIFIER_CLIENT\\";\\n\\n        private readonly ILogger<TeamsNotificationService> logger;\\n        private readonly IHttpClientFactory _clientFactory;\\n\\n\\n        public TeamsNotificationService(\\n            ILogger<TeamsNotificationService> logger,\\n            IHttpClientFactory clientFactory\\n        ) {\\n            _logger = logger;\\n            _clientFactory = clientFactory;\\n        }\\n\\n        private HttpClient CreateClient() {\\n            var client = _clientFactory.CreateClient(TEAMS_NOTIFIER);\\n\\n            client.DefaultRequestHeaders.Clear();\\n            client.DefaultRequestHeaders.Accept.Clear();\\n            client.DefaultRequestHeaders.Accept.Add(new MediaTypeWithQualityHeaderValue(\\"application/json\\"));\\n\\n            return client;\\n        }\\n\\n        public async Task SendNotification(string title, string message) {\\n            try {\\n                var client = CreateClient();\\n\\n                var messageContents = string.IsNullOrEmpty(title)\\n                    ? new JsonContent(new { text = message, textFormat = \\"markdown\\" })\\n                    : new JsonContent(new { title = title, text = message, textFormat = \\"markdown\\" });\\n\\n                var webhookUrl = \\"https://outlook.office.com/webhook/big-long-guid1/IncomingWebhook/big-long-guid2\\";\\n                var response = await client.PostAsync(webhookUrl, messageContents);\\n\\n                _logger.LogInformation(\\"Sent {title} notification to Teams using {url}; received this response: {responseStatusCode}\\", title, url, response.StatusCode);\\n            }\\n            catch (Exception exc) {\\n                _logger.LogError(exc, $\\"Failed to send {title} notification to Teams\\");\\n            }\\n        }\\n\\n        public async Task SendExcitingNotification(Guid someAppId, string person) {\\n            var celebration = GetCelebration();\\n            await SendNotification(\\n                title: \\"Incredible Thing Alert!\\",\\n                message: $@\\"**{person}** has done something incredible! &#x1F44B;\\n\\n![celebration time!]({celebration})\\n\\n[Go see for yourself](https://my.app/some-page/{someAppId})\\"\\n            );\\n        }\\n\\n        string GetCelebration() => GetRandomItem(_celebrations);\\n        string GetRandomItem(string[] arrayOfStrings) => arrayOfStrings[new Random().Next(0, arrayOfStrings.Length)];\\n\\n        string[] _celebrations = new string[] {\\n            \\"https://media.giphy.com/media/KYElw07kzDspaBOwf9/giphy.gif\\",\\n            \\"https://media.giphy.com/media/GStLeae4F7VIs/giphy.gif\\",\\n            \\"https://media.giphy.com/media/NbXTwsoD7hvag/giphy.gif\\",\\n            \\"https://media.giphy.com/media/d86kftzaeizO8/giphy.gif\\",\\n            \\"https://media.giphy.com/media/YJ5OlVLZ2QNl6/giphy.gif\\",\\n            \\"https://media.giphy.com/media/kyLYXonQYYfwYDIeZl/giphy.gif\\",\\n            \\"https://media.giphy.com/media/KYElw07kzDspaBOwf9/giphy.gif\\",\\n            \\"https://media.giphy.com/media/6nuiJjOOQBBn2/giphy.gif\\",\\n            \\"https://media.giphy.com/media/hZj44bR9FVI3K/giphy.gif\\",\\n            \\"https://media.giphy.com/media/31lPv5L3aIvTi/giphy.gif\\"\\n        };\\n    }\\n}\\n```\\n\\nIt\'s as simple as that \ud83d\ude04"},{"id":"/2019/10/08/definitely-typed-movie","metadata":{"permalink":"/2019/10/08/definitely-typed-movie","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2019-10-08-definitely-typed-movie/index.md","source":"@site/blog/2019-10-08-definitely-typed-movie/index.md","title":"Definitely Typed: The Movie","description":"I\'d like to tell you a story. It\'s the tale of the ecosystem that grew up around a language: TypeScript. TypeScript is, for want of a better description, JavaScript after a trip to Saville Row. Essentially the same language, but a little more together, a little less wild west. JS with a decent haircut and a new suit. These days, the world seems to be written in TypeScript. And when you pause to consider just how young the language is, well, that\'s kind of amazing.","date":"2019-10-08T00:00:00.000Z","formattedDate":"October 8, 2019","tags":[{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"Definitely Typed","permalink":"/tags/definitely-typed"}],"readingTime":47.955,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Definitely Typed: The Movie","authors":"johnnyreilly","tags":["TypeScript","Definitely Typed"],"image":"./title-image.png","hide_table_of_contents":false},"prevItem":{"title":"Teams notification webhooks","permalink":"/2019/12/18/teams-notification-webhooks"},"nextItem":{"title":"Start Me Up: ts-loader meet .tsbuildinfo","permalink":"/2019/09/30/start-me-up-ts-loader-meet-tsbuildinfo"}},"content":"I\'d like to tell you a story. It\'s the tale of the ecosystem that grew up around a language: TypeScript. TypeScript is, for want of a better description, JavaScript after a trip to Saville Row. Essentially the same language, but a little more together, a little less wild west. JS with a decent haircut and a new suit. These days, the world seems to be written in TypeScript. And when you pause to consider just how young the language is, well, that\'s kind of amazing.\\n\\nWho could have predicted it would end up like this? When I was a boy I remember coming down the stairs in my childhood home. Shuffling to the edge of each step on my bottom before thumping down to the one beneath. When I look at those same stairs now they\'re so small. I barely notice the difference between one step and the next. But back then each step seemed giant, each one so far apart. Definitely Typed had any number of steps in its evolution. They all seemed so significant then; whereas now they\'re just a memory. Let\'s remember together\u2026\\n\\n![A title image that reads \\"Definitely Typed: The Movie\\"](title-image.png)\\n\\n## Prolog(ue)\\n\\nWhen it was first unveiled to the world by Anders Hejlsberg back in 2012, there was nothing to suggest TypeScript was going to be seismic in its effects. The language brought two important things to the table. First of all, the ability to write JavaScript with optional static typing (imagine this as \\"belts and braces\\" for JS). The second feature was interoperability with existing JavaScript.\\n\\nThe reason TypeScript has the traction that it does, is a consequence of the latter feature. The JavaScript ecosystem was already a roaring success by 2012. Many useful libraries were out there, authored in vanilla JavaScript. jQuery, Backbone, Knockout were all going concerns. People were building things.\\n\\nWisely, having TypeScript able to work with existing JavaScript libraries was a goal of the language right from the off. This made sense; otherwise it would have been like unveiling Netflix to the world whilst saying \\"sorry you can\'t use a television set to watch this\\". Remember, JS was great as is - people wanted static typing so they could be more productive and so they could sleep better at night. (\\"Oh wait, did I write that unit test to check all the properties? Dammit, it\'s 3am!\\") If TypeScript had hove onto the scene requiring that everything was written _in_ TypeScript then I would not be writing this. It didn\'t.\\n\\nInteroperability was made possible by the concept of \\"type definitions\\". Analogous to header files in C, these are TypeScript files with a `.d.ts` suffix that tell the compiler about an existing JavaScript library which is in scope. This means you can write TypeScript and use jQuery or [insert your favourite library name here]. Even though they are not written in TypeScript.\\n\\nAt the time of the initial TypeScript announcement (v0.8.1) there was no concept of a repository of type definitions. I mean, there was every chance that TypeScript wasn\'t going to be a big deal. Success wasn\'t guaranteed. But it happened. You\'re reading this in a world where Definitely Typed is one of the most popular repos on GitHub and where type definitions from it are published out to npm for consumption by developers greedy for static types. A world where the TypeScript team has pretty much achieved its goal of \\"types on every desk\\".\\n\\nI want to tell you the story of the history of type definitions in the TypeScript world. I\'m pretty well placed to do this since I\'ve been involved since the early days. Others involved have been kind enough to give me their time and tell me their stories. There\'s likely to be errors and omissions, and that\'s on me. It\'s an amazing tale though; I\'m fortunate to get to tell it.\\n\\n## The First Type Definition\\n\\nI was hanging out for something like TypeScript. I\'d been busily developing rich client applications in JS and, whilst I loved the language, I was dearly missing static typing. All the things broke all of the time and I wanted help. I wanted a compiler to take me by the hand and say \\"hey John, you just did a silly thing. Don\'t do it John; you\'ll only be filled with regret...\\". The TypeScript team wrote that compiler.\\n\\nWhen TypeScript was announced, it was important that the world could see that interop with JS was a first class citizen. Accordingly, a jQuery type definition was demonstrated as well. At the time, jQuery was the number one JavaScript library downloaded on the internet. So naturally it was the obvious choice for a demo. The type definition was fairly rough and ready but it worked. [You can see Anders Hejlsberg showing off the jQuery definition 45 minutes into this presentation introducing TypeScript.](https://channel9.msdn.com/posts/Anders-Hejlsberg-Introducing-TypeScript)\\n\\nConsumption was straightforward, if perhaps quirky. You took the `jquery.d.ts` file, copied it into your project location. Back then, to let the compiler know that a JS library had come to the party you had to use a kind of comment pragma in the header of your TypeScript files. For example: `/// <reference path=\\"jquery/jquery.d.ts\\" />`. This let TypeScript know that the type definition living at that path was relevant for the current script and it should scope it in.\\n\\nThere was no discussion of \u201chow do we type the world\u201d? Even if they wanted to, the TypeScript team didn\'t really have the resources at that point to support this. They\'d got as far as they had on the person power of four or five developers and some testers as well. There was a problem clearly waiting to be solved. As luck would have it, in Bulgaria a man named Boris Yankov had been watching the TypeScript announcement.\\n\\n## Boris Yankov\\n\\n![photograph of Boris Yankov looking mean, moody and magnificent](boris_yankov.jpeg)\\n\\nBoris Yankov was a handsome thirty year old man, living in the historic Bulgarian city of Plovdiv. He was swarthy with dark hair; like Ben Affleck if had been hanging out in Eastern Europe for a couple of years.\\n\\nBoris was a backend developer who\'d found himself doing more and more frontend. More JavaScript. He was accustomed to C# on the backend with static typing a-gogo. From his point of view JS was brittle. It was super easy to break things and have no idea until runtime that you\'d done so. It seemed so backward. He was ready for something TypeScript shaped.\\n\\n\\"What people forget is how different it was back then. Microsoft made this announcement, but probably most of the people that were listening were part of the MS ecosystem. I certainly was. Remember, back then if you had a Mac or did Linux you probably didn\'t think about MS too much.\\"\\n\\nBoris thought TypeScript just seemed like this interesting and weird thing that Microsoft were doing. He was excited by types; he was missing them and there was a real need there. A problem to solve. There were already people trying to address this. But the attempts so far had been underwhelming. Boris had encountered Google Closure Compiler; a tool built by Google which, amongst other things, introduces some measure of type safety to JavaScript by reading annotations in JSDoc format. Boris viewed GCC as a tentative first step. [One which lead the way for things like TypeScript and Flow to follow.](https://github.com/google/closure-compiler/wiki/Annotating-JavaScript-for-the-Closure-Compiler)\\n\\nThe other aspect of TypeScript that excited Boris was transpilation. Transpilation is the term coined to describe what TypeScript does when it comes to emit output. It takes in TypeScript and pumps out JavaScript. The question is: what sort of JavaScript? One choice the TypeScript team could have made was just having the compiler stripping out types from the codebase. If it worked that way then you\'d get out the JavaScript equivalent of the TypeScript you wrote. You wrote a `class`? TypeScript emits a `class`; just; one shorn of types and interfaces.\\n\\nThe TypeScript team made a different choice. They wrote the compiler such that the user could write ES6 style TypeScript syntax and have the TypeScript compiler transpile that down to ES5 or even ES3 syntax. This made TypeScript a much more interesting proposition than it already was, for a couple of reasons.\\n\\nES6 had been in the works for some time at this point. The release was shaping up to be the biggest incremental change to JavaScript that had so far happened. Or that would ever happen. Prior to this, JavaScript had experienced no small amount of tension and disagreement as it sought to evolve and develop. These played out in the form of the abandoned fourth edition of the language. There were arguments, harsh words, public disagreements and finally a failure to ship ECMAScript 4. In an alternate universe this was the end of the road for JavaScript. However, in our universe JavaScript got another throw of the dice.\\n\\nIt\'s telling that ES5 was for a long time known also as ES3.1; reflecting that it was initially planned to be the stepping stone between ES3 and ES4. In reality it ended up being the stepping stone between ES3 and ES6. As it turned out, it was a vital one too, [it allowed the TC39 to recalibrate after a very public shelving of plans.](https://en.m.wikipedia.org/wiki/ECMAScript)\\n\\nThe band was back together (albeit with a new rhythm section) and ES6 was going to be _massive_. JavaScript was going to get new constructs such as `Map`, `Set`, new scoping possibilities with `let` and `const`, `Promise`s which paved the way for new kinds of async programming, the contentious `class`es\u2026. And who can forget where they were when they first heard about \\"fat\\" arrow functions?\\n\\nPeople salivated at the idea of it all. Such new shiny toys! But how could we use them? Whilst all this new hotness was on the way, where could you actually run your new style code? Complete browser implementations of ES6 wouldn\'t start to materialise until 2018. Given the slowness of people to upgrade and the need to support the lowest common denominator of browser this could have meant that all the excitement was trapped in a never tomorrow situation.\\n\\nBack to TypeScript. The team had a solution for this issue. In their wisdom, the TypeScript team allowed us to write ES6 TypeScript and the compiler could (with some limitations) transpile it down to ES3 JavaScript. The audacity of this was immense. The TypeScript team brought the future back to the past. What\'s more, they made it work in Internet Explorer 6. Now that\'s rock\'n\'roll. It\'s nothing short of miraculous!\\n\\nThe significance of transpilation to TypeScript cannot be overstated.\\n\\nYou might be thinking to yourself, \\"that\'s just Babel, right?\\" Right. It\'s just that Babel didn\'t exist then. 6to5 was still an idea waiting for Sebastian McKenzie to think of. Even if you were kind of \\"meh\\" on types, the attraction of using a tool which allowed you to use new JavaScript constructs without breaking your customers was a significant draw. People may have come for types, but once they\'d experienced the joy of a lexically bound `this` in a fat arrow function they were _never_ going back.\\n\\nSuccess has many parents. TypeScript is a successful project. One reason for this is that it\'s an excellent product that fills a definite need. Another reason is one that can\'t be banked upon; timing. TypeScript has enjoyed phenomenal timing. Appearing just when JavaScript was going off like a rocket and having the twin benefits of types and future JS today when nothing else offered anything close, that\'s perfect timing. It got people\'s curiosity. Now it got Boris\'s attention.\\n\\n## Definitely Typed\\n\\n![The Definitely Typed logo](dt-logo-smallish.png)\\n\\nBoris had been feeling unproductive. He would build applications in JS and watch them unaccountably break as he made simple tweaks to them. He was constantly changing things, breaking them, fixing them and hoping he hadn\'t broken something else along the way. It was exhausting. He saw the promise in what TypeScript was offering and decided to give it a go.\\n\\nIt was great. He fired up Visual Studio and converted a `.js` file to end with the mystical TypeScript suffix of `.ts`. In front of his eyes, red squiggly lines started to appear here and there in his code. As he looked at the visual noise he could see this was TypeScript delivering on its promise. It was finding the bugs he hadn\'t spotted. These migrations were also addictive; the more information you could feed the compiler, the more problems it found. Boris felt it was time to start writing type definitions, whatever they were.\\n\\nBoris quickly learned how to write a type definition and set to work. Most libraries weren\'t well documented and so he found himself reading the source code of libraries he used in order that he could write the definitions. At first, the definitions were just files dropped in his ASP.NET MVC projects that he copied around. That wasn\'t going to scale; there needed to be somewhere he could go to grab type definitions when he needed them. And so on October 5th 2012 he created a repository under his profile at GitHub called \\"DefinitelyTyped\\": [https://github.com/DefinitelyTyped/DefinitelyTyped/commit/647369a322be470d84f8d226e297267a7d1a0796](https://github.com/DefinitelyTyped/DefinitelyTyped/commit/647369a322be470d84f8d226e297267a7d1a0796)\\n\\nBoris took his type definitions and put them into this repository. Were you ever curious what the first definition added was? Close your eyes and think... You might imagine it was the (then number one JavaScript library on the web) jQuery. In fact it was Modernizr. Then Underscore followed, and then jQuery. Take a look:\\n\\n[https://github.com/DefinitelyTyped/DefinitelyTyped/commits?after=4a4cf23ff4301835a45bb138bbb62bf5f0759255+699&author=borisyankov](https://github.com/DefinitelyTyped/DefinitelyTyped/commits?after=4a4cf23ff4301835a45bb138bbb62bf5f0759255+699&author=borisyankov)\\n\\n![A screenshot of the initial commits to Definitely Typed on GitHub](Initial-CommitsDefinitelyTyped.png)\\n\\nIt wasn\'t complicated; it was just a folder with subfolders underneath; each folder representing a project. One for jQuery, one for jQuery UI, one for Knockout.... You get the idea. It\'s not so different now.\\n\\nBoris had laid simple but dependable foundations. Definitely Typed had been born.\\n\\n## How Do You Test a Type Definition?\\n\\nBoris was careful too. Right from the first type definition he added tests alongside them. Now tests for a type definition were a conundrum. How do you write a test for interfaces that don\'t exist in the runtime environment? Code that is expunged as part of the compilation process. Well, the answer Boris came to was this: a compilation test.\\n\\nSomeone once said: compilation is the first unit test... But it\'s a doozy. They\'re right. The value you get from compilation, from a computer checking the assertions your code makes, is significant. Simply put, it takes a large amount of tests to get the same level of developer confidence. Computers are wonderful at attention to detail in a way that puts even the most anally retentive human being to shame.\\n\\nSo if Boris had written a definition called `mylib.d.ts`, he\'d write a file that exercises this type definition. A `mylib.tests.ts` if you will. This file would contain code that exercises the type definition in the way that it should correctly be used. This is code that will never be executed in the way that tests normally are; a test program is never actually run. Rather these tests exist solely for compilation time. (In much the same way that TypeScript types only exist for compilation time.) Boris\'s plan was this: no compilation errors in `mylib.tests.ts` represents passing tests. Compilation errors in `mylib.tests.ts` represents failing tests. It was functional, brutal and also beautiful in it\'s simplicity.\\n\\nSo, imagine your definition looked like this:\\n\\n```ts\\ndeclare function turnANumberIntoAString(\\n  numberToMakeStringOutOf: number\\n): string;\\n```\\n\\nYou might write a compilation test that looks like this:\\n\\n```ts\\nconst itIsAString: string = turnANumberIntoAString(42);\\n```\\n\\nThis test ensures that you can use your function in the way you\'d expect. It returns the types you\'d desire (a `string` in this case) and it accepts the parameters you\'d expect (a single `number` for this example). If someone changed the definition in future, such that a different type was returned or a different set of parameters was required it would break the test. The test code wouldn\'t compile anymore. That\'s the nature of our \\"test\\". It\'s blunt but effective.\\n\\nThis is the very first test committed to Definitely Typed; a test for Modernizr.\\n\\n[https://github.com/DefinitelyTyped/DefinitelyTyped/blob/a976315cacbcfc151d5f57b25f4325dc7deca6f2/Tests/modernizr.ts](https://github.com/DefinitelyTyped/DefinitelyTyped/blob/a976315cacbcfc151d5f57b25f4325dc7deca6f2/Tests/modernizr.ts)\\n\\nThis idea represents what tests look like throughout Definitely Typed today. They\'re now \\"run\\" as part of Continuous Integration and type definitions are tested in concert with one another to ensure that a change to one type definition doesn\'t break another. But there is nothing fundamentally different in place today to what Boris originally came up with.\\n\\n## Independence\\n\\nVery quickly, Definitely Typed became a known project. People like Steve Fenton (author of the first book about TypeScript) were vocal supporters of the project. The TypeScript team talked up the project and were entirely supportive of its existence. In fact, at every given opportunity Anders Hejlsberg would sing its praises. For a while you could guarantee that any TypeScript talk by Anders would include a variant of \\"this guy called Boris started a project called Definitely Typed\\". The impression he gave was that he was kind of amazed, and thoroughly delighted, the project existed.\\n\\nThe TypeScript team were completely uninvolved with Definitely Typed. That in itself is worth considering. The perception of Microsoft by developers generally in 2012 was at best, highly suspicious. \\"Embrace, extend, extinguish\\" - a strategy attributed to MS was very much a current perspective. This was born out in online comments and conversations at meetups. [The Hacker News comments on the TypeScript release were a mixed bag.](https://news.ycombinator.com/item?id=4597716) The reaction on social media was rather less generous. Certainly it was harsh enough to prompt Scott Hanselman to write something of [a defence of TypeScripts right to exist](https://www.hanselman.com/blog/WhyDoesTypeScriptHaveToBeTheAnswerToAnything.aspx).\\n\\nGiven that TypeScript had arrived with the promise of transforming the JavaScript developer experience, the developer community was understandably cautious. Was Microsoft doing a good or ill? Could they be trusted? There were already signs that MS was changing. For example, it had been shipping open source libraries such as jQuery with ASP.Net MVC for some time. Microsoft was starting to engage with the world of open source software.\\n\\nHow Microsoft interacted with the (very open source driven) JS community was going to be key to the success (or not) of TypeScript. What happened with the establishment of Definitely Typed very much indicated TypeScripts direction of travel.\\n\\nOn day one of its existence, Boris took type definitions written by Microsoft and made them available via Definitely Typed. A ballsy move. It would have been completely possible for MS to object to this. They didn\'t.\\n\\nPeople like Diullei Gomes started submitting pull requests to improve the existing definitions and add new ones. Diullei even wrote the first command line tooling which allowed people to install type definitions: TSD. Within a surprisingly short period, DT had become the default home of type definitions on the web. There were briefly alternative Definitely Typed styled collections of type definitions elsewhere on GitHub but they didn\'t last.\\n\\nThis all happened completely independently of the TypeScript team. Definitely Typed existing actually allowed TypeScript itself to prosper. It was worth persevering with this bleeding edge language because of the interoperability Definitely Typed was providing to the community. So the hands off attitude of MS was both surprising and encouraging. It showed trust of the community; something that hadn\'t hitherto been a commonly noted characteristic of MS.\\n\\nBoris started adding contributors to Definitely Typed to help him with the work. Definitely Typed was no longer a one man band, it had taken an important step. It was built and maintained by an increasing number of creative and generous people. All motivated by a simple aim: the best developer experience when working with TypeScript and existing JS libraries.\\n\\n## Basarat Ali Syed\\n\\n![A photograph of Basarat](basarat.jpg)\\n\\nBasarat Ali Syed was a 27 year old who had recently moved to Melbourne, Australia from Pakistan. You might know of him for a number of reasons, not least being the TypeScript equivalent of Jon Skeet. That, incidentally, is not a coincidence. Basarat had watched Jon Skeet\'s impressive work, being _the_ gold standard in C# answers and thought \\"there\'s something worth emulating here\\".\\n\\nBas was working for a startup who had a JS frontend. About six months before TypeScript was announced to the world he watched Anders Hejlsberg do a presentation on JavaScript which included Anders saying to the audience \\"don\'t you just wish you had type safety?\\" with a twinkle in his eye. TypeScript was of course well underway by this time; just not yet public. Bas remembered the comment and, when TypeScript was announced, he was ready. He made it his personal mission to be the goto person answering questions about TypeScript on Stack Overflow.\\n\\nIn those early days of TypeScript, if you put a question about TypeScript onto Stack Overflow there was a very good chance that Bas would answer it. And Bas was more helpful than your typical SO answerer. Not only would he provide helpful commentary and useful guidance, he would often find him answering \\"yeah, the problem isn\'t your code, it\'s the type definition. It needs improvement. In fact, I\'ve raised a PR to fix it here\u2026\\"\\n\\nBoris saw the drip, drip of Basarat PRs turning into a flood. So, very quickly, he invited Basarat join Definitely Typed. Now Bas could not just suggest changes, he could ensure they were made. Step by step the quality of type definitions improved.\\n\\nBasarat describes himself as a \\"serial OSS contributor and mover on-er\\". It\'s certainly true. As well as his Stack Overflow work, he\'s been someone involved in the early days of any number of open source projects. Not just Definitely Typed. Bas also worked on the TypeScript port of the JavaScript task runner; Grunt TS. He met up with Pete Hunt (he of React) at a Decompress conference and together they hacked together a POC webpack TypeScript loader. (That POC ultimately lead to James Brantly creating ts-loader which I maintain.) Bas wrote the atom-typescript plugin which offers first class support for TypeScript in Atom. Not content with that he went on to write a full blown editor of his own called alm-tools.\\n\\nThis is not an exhaustive list of his achievements and already I\'m tired. Besides this he wrote the TypeScript Deep Dive book and the VS Code TypeScript God extension. And more.\\n\\nBas had the level of self knowledge required to realise that getting others involved was key to the success of open source projects. Particularly given that he knew he had a predilection to eventually move on, to work on other things. So Bas kept his eyes open and welcomed in new maintainers for projects he was working on. Bas\' actions in particular were to be crucial. Bas grew the Definitely Typed team; he invited others in, he got people involved.\\n\\nOn December 28th 2013 Basarat decided that a regular contributor to Definitely Typed might be a potential team member. Bas opened up Twitter and sent a Direct Message to John Reilly.\\n\\n![A screenshot of direct message Basarat sent to John Reilly in Twitter](2019-10-02-21_51_58-basarat-_-Twitter.png)\\n\\n## John Reilly\\n\\n![A photograph of John Reilly](johnny_reilly.jpg)\\n\\nThat\'s me. Or [johnny_reilly on Twitter](https://twitter.com/johnny_reilly) and [johnnyreilly](https://github.com/johnnyreilly) on GitHub (as John Papa and I have learned to our chagrin; GitHub don\'t support the \\"\\\\_\\" character in usernames). Relatively few people call me Johnny. I\'m named that online because back when I applied for an email address, someone had already bagsied john\\\\_reilly@popularemailhotness.com. So rather than sully my handle with a number or a middle name I settled for johnny_reilly. I haven\'t looked back and have generally tried to keep that nom de plume wherever I lay my hat online.\\n\\nIn contrast to others I was a relatively late starter to TypeScript. I was intrigued right from the initial announcement, but held off from properly getting my hands dirty until generics was added to the language in 0.9. (This predisposition towards generics in a language perhaps explains why I didn\'t get too far with Golang.)\\n\\nAt that point I was working in London for a private equity house. It was based in the historic and affluent area of St James. St James is an interesting part of London, caught midway between the Government, Buckingham Palace and the heart of the West End. It\'s old fashioned, dripping with money and physically delightful. It\'s the sort of place film crews dash towards when they\'re called upon to show old fashioned London in all its pomp. It rocks.\\n\\nMy team hated JavaScript. Absolutely loathed it. I was the solo voice saying \\"but it\'s really cool!\\" whilst they all but burned effigies of Brendan Eich in each code review. However, to my delight (and their abject horror) the project we were working on could only be implemented using JS. Essentially the house wanted an application offering rich interactivity which had to be a web app. So\u2026 JS. We were coding then with a combination of jQuery and Knockout JS. And, in large part due to the majority of the team being unfamiliar with JS, we were shipping bugs. The kind of bugs that could be caught by a compiler. By static typing. Not to put too fine a point on it; by TypeScript.\\n\\nSo I proposed an experiment: \\"Let\'s take one screen and develop it with TypeScript. Let\'s leave the rest of the app as is; JavaScript as usual. And then once we\'re done with that screen let\'s see how we feel about it. TypeScript might not be that great. But that\'s fine, if it isn\'t we\'ll take the generated JS, keep that and throw away the TypeScript. Deal?\\"\\n\\nThe team were on board and, one sprint review later, we decided that all future JS functionality would be implemented with TypeScript. We were in!\\n\\nFrom day one of using TypeScript I was in love. I had the functionality of JavaScript, the future semantics of JavaScript and I was making less mistakes. Our team had become more productive. We were shipping faster and more reliably with fewer errors. People were noticing; our reputation as a team was improving, in part due to our usage of TypeScript. We had a jetpack.\\n\\nHowever. I wasn\'t satisfied. As I tapped away at my keyboard I found type definitions to be\u2026 imperfect. And that niggled. Did it ever niggle. By then [Jason Jarrett](https://github.com/staxmanade) had wired up Definitely Typed packages to be published out to Nuget. Devs using ASP.NET MVC 4 (as I then was) were busily installing type definitions alongside AutoFac and other dependencies. Whilst most of those dependencies arrived like polished diamonds, finished products ready to be plugged into the project and start adding value. The type definitions by contrast felt very beta. And of course, they were. TypeScript was beta. The definitions reflected the newness of the language.\\n\\nI could make it better.\\n\\nI started submitting pull requests. The first problem I decided to solve was IntelliSense. I wanted IntelliSense for jQuery. If you went to [https://api.jquery.com](https://api.jquery.com) there was rich documentation for every method jQuery exposed. I wanted to see that documentation inside Visual Studio as I coded. If I keyed in `$.appendTo(` I wanted VS to be filled with the content from [https://api.jquery.com/appendTo/](https://api.jquery.com/appendTo/) . That was my mission. For each overload of the method I\'d add something akin to this to the type definition file:\\n\\n```ts\\n/**\\n * Insert every element in the set of matched elements to the end of the target.\\n *\\n * @param value A selector, element, HTML string, array of elements, or jQuery\\n *              object; the matched set of elements will be inserted at the end\\n *              of the element(s) specified by this parameter.\\n */\\nappendTo(target: string): JQuery;\\n```\\n\\nIt was a tedious task plugging it all in, but the pleasure I got from having rich IntelliSense in VS more than made up for it to me. Along the way I added and fixed sections of the jQuery API that hadn\'t been implemented, or had been implemented incorrectly. It got to a point where jQuery was a good example of what a type definition should look like. That remains the case to this day; surprisingly few type definitions enjoy the JSDoc richness of jQuery. [I have tried to encourage more use of this with blog posts code reviews and the like, but it\'s never got the traction I\'d hoped.](https://blog.johnnyreilly.com/2014/05/typescript-jsdoc-and-intellisense.html)\\n\\nI\'m fairly relentless when I put my mind to something. I work very hard to make things come to pass. What this meant at one point was the Definitely Typed maintainers receiving multiple PRs a day. Which prompted Bas to wonder \\"I wonder if he\'d like to join us?\\"\\n\\nI happily accepted Bas\' invitation and soon found myself reading this email:\\n\\n> From: Bas\\n>\\n> Sent: 28 December 2013 11:47\\n>\\n> To: Boris Yankov; johnny\\\\_reilly@hotmail.com; Bas; vvakame; Bart van der Schoor; Diullei Gomes; steve fenton; Jason Jarret Subject: DefinitelyTyped team introduction\\n>\\n> Dear All,\\n>\\n> Meet John Reilly (github : https://github.com/johnnyreilly , twitter : https://twitter.com/johnny\\\\_reilly) who will be helping with Definitely Typed definitions.\\n>\\n> Boris manages the project and he can add you as a collaborator.\\n>\\n> Additional team member introductions:\\n>\\n> Admin : Boris Yankov\\n>\\n> TSD package manager : https://github.com/DefinitelyTyped/tsd : Diullei / Bart van der Schoor\\n>\\n> NUGET: https://github.com/DefinitelyTyped/NugetAutomation : Json Jarret\\n>\\n> Passionate TypeScript users like yourself: Wakame, Myself and SteveFenton .\\n>\\n> Cheers, Bas (Basarat)\\n\\nSome of those names you\'ll recognise; some perhaps not. Jason Jarrett wrote the Nuget distribution mechanism for type definitions that ended up existing for far longer than anyone (least of all Jason) anticipated. Steve Fenton was largely a cheerleader for Definitely Typed in its early days. Diullei and Bart, amongst other things, worked on the initial command line tooling for DT: TSD.\\n\\nAfter being powered up in Definitely Typed, my contributions only increased. Anything that I was using in my day to day work, I wanted to have an amazing TypeScript experience. I wanted the language to thrive and I was pretty sure I could help by trying to get users the best-in-class developer experience as they used JS libraries. I\'ve always found good developer experience a strong motivation; the idea being, if someone loves their tools, they\'ll do great work. The end customer (of whatever they\'re building) gets a better product sooner. Great developer experience is a force multiplier for building software.\\n\\n## Policy time\\n\\nTypeScript was now at version 0.9.1. Still very much beta. Back then every release was breaking. Breaking. Very much with a capital \\"B\\".\\n\\nTypeScript had, since the very early days, made a commitment to track the ECMAScript standard. All JavaScript is valid TypeScript. However, there was briefly a period where this might not have been so. One of the things people most remember from the initial release is that they could now write classes. These were already the standardised classes of ES6 but it almost wasn\'t to be. For a brief period there had been consideration of doing something subtly different. In fact Anders would describe the TypeScript team\'s journey towards embracing the standards as a tale tinged with regret. In doing so they\'d had to say goodbye to a different implementation of classes which he\'d preferred but which they\'d ditched because they weren\'t standard.\\n\\nAlongside differences like this there were other delineations. Types had different names in the past which, as time went by, were renamed to align with standards. `boolean` was originally `bool` for instance; likely a reflection of Anders involvement with C#.\\n\\nThese sorts of changes, alongside any number of others, meant that each release of TypeScript sometimes entirely broke the definitions in Definitely Typed. Most notable was the 0.9.1 -> 0.9.5 migration. This was both an exercise in serious pain endurance and also a testament to the already strong commitment to TypeScript that existed. The reason people were willing to put the effort in to keep these migrations going was because they believed it was worth it. They believed in TypeScript. This PR is testament to that: [https://github.com/DefinitelyTyped/DefinitelyTyped/pull/1385](https://github.com/DefinitelyTyped/DefinitelyTyped/pull/1385)\\n\\nA level of flux meant that for a long time Definitely Typed committed only to support the latest version of TypeScript and the latest version of packages therein. These days it\'s not so brutal, but then it had to be as a matter of necessity.\\n\\nThe compiler was changing too fast and there were too few people involved to allow for any realistic alternative. As is often the case in software development, it was \\"good enough\\". Any other choice would probably have increased the workload of maintainers to a point where the project would no longer be a going concern. It was a choice with downsides; trade-offs. But it was the choice that best served the future of Definitely Typed and TypeScript.\\n\\n## Masahiro Wakame\\n\\n![A photograph of Masahiro Wakame](masahiro_wakame.jpg)\\n\\nTime passed. Autumn turned into winter, winter into spring. TypeScript reached 1.0. It wasn\'t beta anymore. As each release came, the changes in the compiler became more gradual. This was a blessing for the Definitely Typed team. The projects popularity was ticking up and up. New definitions were added each day. The trickle of issues and PRs had become a stream, then a river. A river very much ready to burst its banks.\\n\\nIt was taking its toll. Inside Definitely Typed roles were shifting. Boris was starting to step back from day to day reviewing of PRs. New members were joining the project, like Igor Oleinikov. But the pace was insatiable.\\n\\nSome people left the project entirely, burned out by the never ending issues and PRs. Basarat started contributing less, beginning to turn his attention to one of his many sidejams. Fortunately, it turned out that before Basarat stepped back, he had done a very fine thing. In Tokyo, Japan was a 28 year old developer named Masahiro Wakame.\\n\\nMas was using JS to build the web applications he worked on. But ECMAScript 5 wasn\'t hitting the mark for him. For a time Masahiro used CoffeeScript (Jeremy Ashkenas Ruby style JS alternative). He liked it, but, as he put it: \\"I was shooting my foot everyday\\". Looking out for that elusive solution he landed on Dart. It looked amazing. But it wasn\'t ECMAScript. Masahiro worried he\'d be locked in. He\'d built some libraries and a testing framework using Dart. But he didn\'t feel he could suggest that his company adopted it; it was too different and only he knew it. He was left with the \\"what if I go under a bus?\\" problem. If he left the company, his colleagues would find it hard to move away from using Dart. This made him very hesitant. He didn\'t feel he could justify the choice.\\n\\nThen Masahiro heard about TypeScript. Like Goldilocks and the three bears, this third language sounded just right. He loved the type safety. It also had a compelling proposition: the transpiled JS that TypeScript generated was human readable and idiomatic. Generating idiomatic JS as opposed to some kind of strange byte code was a goal of the language from the early days, as Anders Hejlsberg would repeatedly explain. This generation of \\"real JS\\" made test driving TypeScript a low risk proposition. One that appealed to the likes of Masahiro. No lock-in. You decide TypeScript isn\'t for you? Fine. Take the generated JS files and shake the TypeScript dust off your sandals. Masahiro consequently went all in on TypeScript. This was his bet. And he was going to cover his bet by trying to make the ecosystem even stronger.\\n\\nMasahiro started out trying to improve the testing framework in DT; sending in pull requests. Before too long, Basarat messaged him to say \\"do you wanna become a committer?\\" [Masahiro became a committer.](https://github.com/DefinitelyTyped/DefinitelyTyped/pull/1358)\\n\\nIt turned out that MH had a special qualities that DT was going to sorely need: he was willing and able to review PR after PR, day after day. His stamina was incredible.\\n\\nWhilst it may not have been obvious from the outside, by now Definitely Typed was a slightly troubled project. The speed at which issues and PRs landed was relentless. Anyone who had once set GitHub to \\"watching\\" for Definitely Typed soon unsubscribed. It was becoming unmanageable. And whilst almost everyone else in the project was in the process of burning out / moving on / stepping back and similar, Masahiro kept going. He kept showing up. He kept reviewing. He kept merging. At his peek he was spending 2 hours a day, every day, glued to his screen in Tokyo and reviewing PRs for GitHub. The pulse of Definitely Typed may have slowed. But Masahiro kept the heart beating.\\n\\nAs Masahiro kept the lights on, in a hotel room in Buenos Aires an Australian named Blake Embrey was making plans...\\n\\n## Blake Embrey\\n\\n![A photograph of Blake Embrey](blake_embrey.jpeg)\\n\\nBlake was a 21 year old Australian. He was a nomadic developer, travelling around the world and working remotely. He travelled from country to country armed with a suitcase and his trusty MacBook in search of WiFi. He found himself dialing into standups from caf\xe9s in Vietnam at 1am to provide Jira updates, coding from airports as he criss-crossed the globe. It was an unusual life.\\n\\nA friend showed Blake TypeScript somewhere around the TypeScript 1.2 era. He was interested. He was mostly working on backend NodeJS at the time. He could see the potential that TypeScript had to help him. Around the TypeScript 1.5 era Blake started to take a really good look at what was possible. From his vantage point, there was good and there was also bad. And he thought he could help.\\n\\nAs a module author and developer, he loved TypeScript. It allowed him to write, publish and consume 100% type-safe JavaScript. Features like autocompletion, type validation and ES6 features became part of his typical workflow. It was so good!\\n\\nHowever, one step in this development lifecycle was broken to his mind. The problem was module shaped. Yeah, modules. Wade into the controversy!\\n\\nThanks again to Basarat, Blake soon became a DT contributor. Of all the contributors to Definitely Typed, Blake was the first one who was looking hard at the module problem. This was because whilst he wanted TypeScript to solve the same problems as everyone else, he wanted to solve them in a world of package dependencies. He wanted to solve for Node.\\n\\nNow it\'s worth taking a moment to draw a comparison between web development then, and web development now. Because it\'s changed. The phrase \\"web development fatigue\\" exists with good reason. Web development in 2014 as compared to web development in 2019 is a very different proposition. Historically, JavaScript has not had a good story around modularisation. The language meandered forwards without ever gaining an official approach to modularisation until ES6. So for twenty years, if you wanted to write a large JS application you had to think hard about how to solve this problem. And even when modules were nailed down it was longer still until module loading was standardised.\\n\\nBut that didn\'t stop us. JavaScript apps were still being built on the frontend and the backend. On the frontend an approach to modularisation emerged called the Asynchronous Module Definition. It had some adoption but in the main that wasn\'t how people rolled. The frontend was generally a sea of global variables. People would write programs that depended upon global variables and worked hard to make sure that they didn\u2019t collide. Everything did this. Everything. Underscore? It was a global variable called `_`. jQuery? It was two global variables: `$` and `jQuery`. That\'s just what people did. I\'m a person. I did that. If you were there you probably did too.\\n\\nOn the server side, in Node JS land, a different standard had emerged: CommonJS. Unlike AMD, CommonJS was simply how the Node JS community worked. Everything was a CommonJS module. Alongside Node, npm was growing and growing. Exposing Node developers to a rich ecosystem of modules or packages that they could drop into their apps with merely a tap tap tap of `npm install super-cool-package` and then `var scp = require(\'super-cool-package\')`.\\n\\nAnd therein, as the Bard would have it, lay the rub. You see, in the frontend it was simpler. Uglier but simpler. By and large, the global variables were fine. They weren\'t beautiful but they were functional. It may have impaired the development of frontend apps, but it certainly didn\'t stop it.\\n\\nAnd since a design goal of TypeScript was to meet JavaScript developers where they were and try and make their lives better, the initial focus of Definitely Typed was necessarily types that existed in the global namespace. So `jquery.d.ts` would declare global `$` and `jQuery` variables and underneath them all the jQuery methods and variables that were implemented. Alongside jQuery, maybe an application would have jQuery UI which would extend the `$` variable and add extra functionality. In addition maybe there\'d be a couple of jQuery plugins in play too. (It\'s worth saying that jQuery was the crack cocaine of web development back in the day. People just couldn\'t get enough.)\\n\\nTypeScript catered for this world by allowing type definitions to extend interfaces created by other definition files. The focus of most of the Definitely Typed contributors up to this point was frontend and hence DT was an ocean of global type definitions.\\n\\nOf course, this is not what the frontend world looks like these days. The frontend now is all about npm thanks to tools like Browserify, webpack, Rollup and the like. Client and server side development is mighty similar these days. Or at least, it\'s swimming in more of the same waters. There\'s a good TypeScript story to tell about this as well. But there wasn\'t always. Back to Blake.\\n\\nBlake had published a bunch of modules on npm. But no one had ever been able to consume the type definitions from them. Why was that? Well, without delving into great detail it comes down to type definitions of a package generally conflicting with type definitions that a user installs themselves.\\n\\nThis essentially came down to how TSD worked and what Definitely Typed contained. TSD was a pretty simple tool; by and large it worked by copying files from Definitely Typed into a users project. The files copied would contain type definitions which contained global types. So even though you cared solely about external modules, because of Definitely Typed you found yourself installing globals alongside which lead to conflicts between different type definitions. Different type definitions punching it out whilst the TypeScript compiler stood in between ineffectually shouting \\"leave it alone mate - it\'s not worth it!\\"\\n\\nHow could we have a world where external modules and global were treated distinctly? Blake had ideas\u2026 Plan one was to rewrite TSD to support external modules; the type of modules that were standard in Node land. After working hard on that for some time, Blake came to conclusion that solving global variables alongside external modules was a hard problem. A very hard problem. And perhaps that just running with external modules, [a new start if you will, represented the best way forwards](https://github.com/DefinitelyTyped/tsd/issues/150).\\n\\n## Typings\\n\\n![A screenshot of the Typings project](typings.jpg)\\n\\nBlake made [typings](https://github.com/typings/typings). Typings was a number of things; it was a new command line tool to replace TSD, it was a new approach to distributing type definitions and it was a registry. But Typings was a registry which pointed out to the web. Typings installation was entirely decentralized and the typings themselves could be downloaded from almost anywhere - GitHub, NPM, Bower and even over HTTP or the filesystem. Those type definitions could be external modules or globals.\\n\\nIt was radical. From centralisation to decentralisation. As Blake described it:\\n\\n> This decentralization solves the biggest pain point I see with maintaining DefinitelyTyped. How does an author of one typings package maintain their file in DefinitelyTyped when they get notifications on thousands of others? How do you make sure typings maintain quality when you have 1000s to review? The solution in typings is you don\u2019t, the community does. If typings are incorrect, I can just write and install my own from wherever I want, something that TSD doesn\u2019t really allow. There\u2019s no merge or review process you need to wait for (300+ open pull requests!).\\n>\\n> However, decentralization comes with the cost of discoverability. To solve this, a registry exists that maintains locations of where the best typing can currently be installed from, for any version. If there\u2019s a newer typing, patches, or the old typing author has somehow disappeared, you can replace the entry with your own so people will be directed to your typings from now on.\\n\\nThe world started to use Typings as the default CLI for type definitions. `typings.json` files started appearing in people\'s repos. Typings allowed consumption of types both from the Typings registry and from Definitely Typed and so there was an easy on ramp for people to start using Typings.\\n\\nLittle by little, people started consuming type definitions that came from the typings registry rather than from Definitely Typed. Typings began to thrive whilst DT continued to choke. The community was beginning to diverge.\\n\\n## The TypeScript Team\\n\\n![A photograph of the TypeScript team](TypeScriptTeam.jpg)\\n\\nOver in Seattle, the TypeScript team was thinking hard about the type definition ecosystem. About Definitely Typed and Typings. And about tooling and distribution.\\n\\nAt this point, there wasn\'t a dedicated registry for type definitions. There was GitHub. By and large, all type definitions lived in GitHub. Since GitHub is a git based source control provider it was possible for it to be used as a makeshift registry. So that\'s exactly what Definitely Typed and Typings were doing; piggy backing on GitHub and MacGyvering \\"infrastructure\\". It worked.\\n\\nThere wasn\'t a great versioning story. Definitely Typed just didn\'t do versioning. The latest and greatest was supported. Nothing else. The Typings approach was more nuanced. It did have an approach for versioning. It supported it by dint of allowing a version number in the registry to point to a specific git hash in a repo. It was an elegant and smart approach. Blake Embrey was one sharp cat.\\n\\nInnovative though it was, the decentralised Typings approach presented potential security risks as it pointed out to the web making auditing harder. Alongside this, The TypeScript team was pondering ways they could reduce friction for developers that wanted to use TypeScript.\\n\\nBy now, the JavaScript ecosystem had started to coalesce around npm as the registry du jour. Bower and jspm were starting to fade in popularity. NuGet (the .NET package manager) was no longer being encouraged as a place to house JS. npm was standard. TypeScript users found themselves using npm to install jQuery and reaching for tsd or Typings to install the associated type definitions. That\'s two commands. With two package managers. Each with subtly different syntax. And then perhaps you had to fiddle with the `tsconfig.json` to get the compiler looking in the right places. It worked. But it didn\'t feel \u2026. idiomatic. It didn\'t feel like TypeScript was meeting their users where they were.\\n\\nThe likes of Daniel Rosenwasser, Mohamed Hegazy and Ryan Cavanaugh found themselves pondering the problem. Alongside this, they were thinking more about what a first class module support experience in TypeScript would look like, motivated in part by the critical mass around npm, which was entirely module / package based.\\n\\nThat wasn\u2019t the only thing on their minds; there was also the testing story. Definitely Typed had a straightforward testing story due to being a real mega-repo. Everything lived together and could be tested together. Thanks to the hard work of the Definitely Typed team this was already in place; every PR spun up Travis and tested all the type definitions individually and in concert with one another. Typings didn\u2019t have this. What\u2019s more, it would be hard to build. The decentralised nature of Typings meant that you\u2019d need to build infrastructure to crawl the Typings registry, download the type definitions and then perform the tests. It was non-trivial and unlikely to be speedy.\\n\\nThere was one more factor in play. The TypeScript team were aware that for the longest time they\'d been working on the language. But they\'d become distant from one of the most significant aspects of how the language was used. They weren\u2019t well enough informed about the rough edges in the type definition space. They weren\u2019t feeling their users pain. They needed to address this and really there was only one thing to do... It was time for the TypeScript team to start eating their own dogfood.\\n\\n## A Plan Emerges\\n\\nThe TypeScript team reached out to Blake Embrey and started to talk about ways forward. They started collaborating over Slack.\\n\\n![A screenshot of the collaboration on Slack](typings_typescript_collaboration.jpg)\\n\\nThe TypeScript team had also been in contact with the Definitely Typed team. They were, at this point, aware that Definitely Typed was being kept going mainly due to the hard graft of Masahiro Wakame. As Daniel observed \u201cvvakame was a champ\u201d.\\n\\nAt this point I have to stick my own hand up and confess to thinking that Definitely Typed was not long for this world. Steve Ognibene (another DT member) and others were all feeling similarly. It seemed inevitable.\\n\\n![A photograph of Steve Ognibe](steveognibe.png)\\n\\nThe TypeScript team were about to change that. After talking, thinking, thinking and talking they put together a plan. It was going to change TypeScript and change Definitely Typed. It was also going to effectively end Typings.\\n\\nIt\u2019s worth saying at this point that the TypeScript team didn\u2019t enter into this lightly. They were hugely impressed by Typings. It was, to quote Daniel Rosenwasser, \u201can impressive piece of work\u201d. It also had the most amazing command line experience. Everyone on the team felt that it was an incredible endeavour and had their proverbial hats off to Blake Embrey. But Definitely Typed had critical mass and, whilst it had known problems, they were problems that could be likely solved (or ameliorated) through automation. The Typings approach was very innovative, but it presented other issues which seemed harder to solve. The TypeScript team made a bet. They placed their money on Definitely Typed.\\n\\nTo remove friction in the type acquisition space they decided to change the compiler. It would now look out for a special scoped namespace on npm named @types. Type definitions from Definitely Typed would be published out to @types. They would land as type definition packages that matched the non @types package. This meant that TypeScript was now sharing the same infrastructure as the rest of the JS ecosystem: npm. And consequently, installation of a package like jQuery in a TypeScript workflow now looked like this: `npm install jquery @types/jquery`. One command, one tool, one registry.\\n\\nThey published their plans here: [https://github.com/Microsoft/TypeScript/issues/9184](https://github.com/Microsoft/TypeScript/issues/9184)\\n\\nThere was more. The TypeScript team had really enjoyed knowing that this open source project which ran completely independently from the TypeScript team existed. And whilst they were focused directly on the language that was reasonable. But with the changes that were being planned, TypeScript was about to start explicitly depending upon Definitely Typed. It had been unofficially true up until that point. But now it was different; TypeScript were going to automate publishing Definitely Typed packages to the special @types scope in npm which the TypeScript compiler gave preference to. TypeScript and Definitely Typed were going from dating to being engaged.\\n\\nIt was time for the TypeScript team to get involved.\\n\\nThe team committed to doing weekly rotations of a TypeScript team member working on Definitely Typed. Reviewing PRs, merging them and, crucially, helping with automation and testing.\\n\\nTypeScript was now part of Definitely Typed. Definitely Typed was part of TypeScript.\\n\\n## TypeScript 2.0 / Definitely Typed 2.0\\n\\nBlake was immensely disappointed. He\'d put his heart and soul into Typings. It was a massive amount of work and he\'d not only started a project, he\'d started a community that he felt responsible for.\\n\\nAlthough that work had arguably kickstarted the discussion of what the future of type acquisition in TypeScript should look like, Typings wouldn\'t be coming along for the ride. It was a burner rocket, carrying the good ship TypeScript into outer orbit, dropping back to Earth once it\'s job was done.\\n\\nVery much, Blake had in mind all the people that had contributed to Typings. That all their work was going to be abandoned. He felt a sense of responsibility. It was both frustrating and heartbreaking.\\n\\nWhen TypeScript 2.0 shipped, in the release announcement was the following statement: [https://devblogs.microsoft.com/typescript/announcing-typescript-2-0/](https://devblogs.microsoft.com/typescript/announcing-typescript-2-0/)\\n\\n> We\u2019d like to thank Blake Embrey for his work on Typings and helping us bring this solution forward.\\n\\nBlake really appreciated the recognition. In years to come Blake would come to feel that the decisions made were the right ones. That they lead to TypeScripts continued success and served the community well. But he has regrets. He says now \\"I am disappointed we didn\'t get to integrate the two philosophies for managing types. It hurt Typings registry contributors without a story in place, I didn\u2019t want to let down and alienate potential contributors of type definitions.\\"\\n\\nA young Australian man had helped change the direction of TypeScript. It was time for him to take a well earned rest.\\n\\nIn the meantime, the TypeScript team was starting to get stuck into the work of giving Definitely Typed a make-over.\\n\\n![Screenshot of the rota for Definitely Typed work for the TypeScript team](rotation.png)\\n\\nAt this point, Definitely Typed had more than 500 open pull requests. Most of which had been open for a very long time. The most urgent and pressing problem was getting that down. The TypeScript team committed to, in perpetuity, a weekly rotation where one team member would review PRs. This would, in future, mean that PRs were handled in a timely fashion and that the number of open PRs was generally kept beneath 100.\\n\\nAlongside this, changes were being made to the TypeScript compiler. In large part these related to enabling automatic type acquisition through the @types scope. To make that work, the TypeScript team realised pretty quickly that many of the type definitions would not work as is. Ryan wrote up this report:\\n\\n![Ryan Cavanagh\'s report on what to do in Definitely Typed](RyansDefTypReport.png)\\n\\nAt this point in time there were around 1700 type definitions. Pretty much all of them required some massaging. Roughly speaking, with TS 2.0, the language was going to move from a name based type acquisition approach to a file based one. New features were added to TypeScript 2.0 such as the `export as namespace` syntax to support a type definition supporting both being used in modules (where there are `import` / `export`s) but also in script files (where there aren\'t)\\n\\nRyan Cavanaugh put together scripts that migrated 1200 of the type definitions to TypeScript 2.0 syntax. The remaining 500 were delicately transitioned by hand by diligent TypeScript team members. It was a task of utter drudgery that still sparks flickers of PTSD in those who were involved. It was like being in the digital equivalent of a Dickensian workhouse.\\n\\nThis was one of the reasons why going with the centralised approach of Definitely Typed instead of the decentralised one of Typings was necessary. Because the TypeScript team were involved in DT they could help make things happen. They could do the hard work. In a decentralised world that wouldn\'t be possible; everything would constantly be held up, waiting.\\n\\nIt took a long time to get the types 2.0 branch to a point where CI went green. All this time, merges we\'re taking place between the master branch and the future one. It was hard, unglamorous work. As Ryan put it, \\"I partied hard when CI went green for the first time on types 2.0.\\"\\n\\n![Screenshot of @types going green](types20goinggreen.png)\\n\\nThe first and most obvious addition was the automation of TypeScript definitions being published out to npm.\\n\\nNext came a solution for the \\"notification flood\\" issue. It was no longer feasible for a user to have Definitely Typed set up as \\"watching\\" in GitHub. That way lead an unstoppable deluge of information about issues and pull requests. The result of that was that users were generally unaware of changes / issues and so on. People, as much as they wanted to be, were becoming disconnected from the type definitions they were interested in in DT.\\n\\nThe solution for this problem was, as with so many problems, a bot. It would send notifications to the users who had historically worked on a type definition when someone sent a PR. This was hugely useful. It made it possible for people to become effective stewards of the type definitions they knew about. It meant people could effectively remain involved with DT; giving them targeted information. It was the solution to a communications problem.\\n\\nAs Ryan Cavanaugh put it when he looked back upon TypeScripts story, he had this to say: \u201cDefinitely Typed is the best thing that could exist from our perspective\u201d.\\n\\nHe was speaking from the perspective of a TypeScript team member. He could as well be speaking for the developer world at large. Definitely Typed is an organic monster of open source goodness; bringing types to the world thanks to nearly 10,000 contributors. Each person of which has donated at least an hour or their time for the greater good. Far more than that in many cases. It\u2019s incredible. I\u2019m glad I get to be part of it. I never would have guessed it would have turned out like this."},{"id":"/2019/09/30/start-me-up-ts-loader-meet-tsbuildinfo","metadata":{"permalink":"/2019/09/30/start-me-up-ts-loader-meet-tsbuildinfo","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2019-09-30-start-me-up-ts-loader-meet-tsbuildinfo/index.md","source":"@site/blog/2019-09-30-start-me-up-ts-loader-meet-tsbuildinfo/index.md","title":"Start Me Up: ts-loader meet .tsbuildinfo","description":"With TypeScript 3.4, a new behaviour landed and a magical new file type appeared; .tsbuildinfo","date":"2019-09-30T00:00:00.000Z","formattedDate":"September 30, 2019","tags":[{"label":".tsbuildinfo","permalink":"/tags/tsbuildinfo"},{"label":"TypeScript","permalink":"/tags/type-script"}],"readingTime":1.73,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Start Me Up: ts-loader meet .tsbuildinfo","authors":"johnnyreilly","tags":[".tsbuildinfo","TypeScript"],"hide_table_of_contents":false},"prevItem":{"title":"Definitely Typed: The Movie","permalink":"/2019/10/08/definitely-typed-movie"},"nextItem":{"title":"Coming Soon: Definitely Typed","permalink":"/2019/09/14/coming-soon-definitely-typed"}},"content":"With TypeScript 3.4, [a new behaviour landed and a magical new file type appeared; `.tsbuildinfo`](https://www.typescriptlang.org/docs/handbook/release-notes/typescript-3-4.html)\\n\\n> TypeScript 3.4 introduces a new flag called `--incremental` which tells TypeScript to save information about the project graph from the last compilation. The next time TypeScript is invoked with `--incremental`, it will use that information to detect the least costly way to type-check and emit changes to your project.\\n>\\n> ...\\n>\\n> These `.tsbuildinfo` files can be safely deleted and don\u2019t have any impact on our code at runtime - they\u2019re purely used to make compilations faster.\\n\\nThis was all very exciting, but until the release of TypeScript 3.6 there were no APIs available to allow third party tools like `ts-loader` to hook into them. The wait is over! Because with TypeScript 3.6 the APIs landed: [https://www.typescriptlang.org/docs/handbook/release-notes/typescript-3-6.html#apis-to-support---build-and---incremental](https://www.typescriptlang.org/docs/handbook/release-notes/typescript-3-6.html#apis-to-support---build-and---incremental)\\n\\nThis was the handiwork of the very excellent [@sheetalkamat](https://twitter.com/sheetalkamat) of the TypeScript team - you can see her PR here: [https://github.com/microsoft/TypeScript/pull/31432](https://github.com/microsoft/TypeScript/pull/31432)\\n\\nWhat\'s more, Sheetal took the PR for a test drive using `ts-loader`, and her hard work has just shipped with [`v6.2.0`](https://github.com/TypeStrong/ts-loader/releases/tag/v6.2.0):\\n\\n- [https://github.com/TypeStrong/ts-loader/pull/1012](https://github.com/TypeStrong/ts-loader/pull/1012)\\n- [https://github.com/TypeStrong/ts-loader/pull/1017](https://github.com/TypeStrong/ts-loader/pull/1017)\\n\\nIf you\'re a `ts-loader` user, and you\'re using TypeScript 3.6+ then you can get the benefit of this now. That is, if you make use of the `experimentalWatchApi: true` option. With this set:\\n\\n1. ts-loader will both emit and consume the `.tsbuildinfo` artefact.\\n\\n2. This applies both when a project has `tsconfig.json` options `composite` or `incremental` set to `true`.\\n\\n3. The net result of people using this should be faster cold starts in build time where a previous compilation has taken place.\\n\\n## `ts-loader v7.0.0`\\n\\nWe would love for you to take this new functionality for a spin. Partly because we think it will make your life better. And partly because we\'re planning to make using the watch API the default behaviour of `ts-loader` when we come to ship `v7.0.0`.\\n\\nIf you can take this for a spin before we make that change we\'d be so grateful. Thanks so much to Sheetal for persevering away on this feature. It\'s amazing work and so very appreciated."},{"id":"/2019/09/14/coming-soon-definitely-typed","metadata":{"permalink":"/2019/09/14/coming-soon-definitely-typed","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2019-09-14-coming-soon-definitely-typed/index.md","source":"@site/blog/2019-09-14-coming-soon-definitely-typed/index.md","title":"Coming Soon: Definitely Typed","description":"A long time ago (well, 2012) in a galaxy far, far away (okay; Plovdiv, Bulgaria)....","date":"2019-09-14T00:00:00.000Z","formattedDate":"September 14, 2019","tags":[{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"Definitely Typed","permalink":"/tags/definitely-typed"}],"readingTime":0.985,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Coming Soon: Definitely Typed","authors":"johnnyreilly","tags":["TypeScript","Definitely Typed"],"hide_table_of_contents":false},"prevItem":{"title":"Start Me Up: ts-loader meet .tsbuildinfo","permalink":"/2019/09/30/start-me-up-ts-loader-meet-tsbuildinfo"},"nextItem":{"title":"Symbiotic Definitely Typed","permalink":"/2019/08/17/symbiotic-definitely-typed"}},"content":"A long time ago (well, 2012) in a galaxy far, far away (okay; Plovdiv, Bulgaria)....\\n\\n[Definitely Typed](https://github.com/DefinitelyTyped/DefinitelyTyped) began!\\n\\nThis is a project that set out to provide type definitions for every JavaScript library that lacked them. An ambitious goal. Have you ever wondered what the story that lay behind it was?\\n\\nPerhaps you know that the project was started by a shadowy figure named \\"Boris Yankov\\". And maybe you know that the TypeScript team is now part of the Definitely Typed team. There\'s a lot more to tell.\\n\\nThis autumn, I\'d like to tell you the story of how Definitely Typed came to be what it is. From an individual commit in a repo that Boris created in 2012 to [the number 10 project by contributions on GitHub in 2018](https://octoverse.github.com/projects). I\'m part of that story. Basarat Ali Syed is part of that story. Masahi Wakame too. Blake Embrey. Steve Fenton. Igor Oleinikov. It\'s an amazing and unexpected tale. One that turns upon the actions of individuals. They changed your life and I\'d love you to learn how.\\n\\nSo, coming soon to a blog post near you, is the story of Definitely Typed. It\'s very exciting! Stay tuned..."},{"id":"/2019/08/17/symbiotic-definitely-typed","metadata":{"permalink":"/2019/08/17/symbiotic-definitely-typed","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2019-08-17-symbiotic-definitely-typed/index.md","source":"@site/blog/2019-08-17-symbiotic-definitely-typed/index.md","title":"Symbiotic Definitely Typed","description":"I did ponder calling this post \\"how to enable a good TypeScript developer experience for npm modules that aren\'t written in TypeScript\\"... Not exactly pithy though.","date":"2019-08-17T00:00:00.000Z","formattedDate":"August 17, 2019","tags":[{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"react-testing-library","permalink":"/tags/react-testing-library"},{"label":"Definitely Typed","permalink":"/tags/definitely-typed"}],"readingTime":5.635,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Symbiotic Definitely Typed","authors":"johnnyreilly","tags":["TypeScript","react-testing-library","Definitely Typed"],"hide_table_of_contents":false},"prevItem":{"title":"Coming Soon: Definitely Typed","permalink":"/2019/09/14/coming-soon-definitely-typed"},"nextItem":{"title":"ASP.NET Core authentication: hard-coding a claim in development","permalink":"/2019/08/02/asp-net-authentication-hard-coding-claims"}},"content":"I did ponder calling this post \\"how to enable a good TypeScript developer experience for npm modules that aren\'t written in TypeScript\\"... Not exactly pithy though.\\n\\nDefinitely Typed is the resource which allows developers to use TypeScript with existing JavaScript libraries that ship without their own type definitions.\\n\\nDT began as a way to enable interop between JS and TS. When DT started, everything on npm was JavaScript. Over time it has become more common for libraries (eg [Mobx](https://github.com/mobxjs/mobx) / [Angular](https://github.com/angular/angular)) to be written (or rewritten) in TypeScript. For publishing, they are compiled down to JS with perfect type definitions generated from the TypeScript alongside the compiled JavaScript. These libraries do not need to exist in Definitely Typed anymore.\\n\\nAnother pattern that has emerged over time is that of type definitions being removed from Definitely Typed to live and be maintained alongside the libraries they support. An example of this is [MomentJS](https://github.com/moment/moment).\\n\\nThis week, I think for the first time, there emerged another approach. [Kent C Dodds](https://kentcdodds.com/)\' `react-testing-library` had started out with the MomentJS approach of hosting type definitions alongside the JavaScript source code. [Alex Krolic raised a PR which proposed removing the type definitions from the RTL repo in favor of having the community maintain them at DefinitelyTyped.](https://github.com/testing-library/react-testing-library/pull/437)\\n\\nI\'ll directly quote Kent\'s explanation of the motivation for this:\\n\\n> We were getting a lot of drive-by contributions to the TypeScript typings and many pull requests would either sit without being reviewed by someone who knows TypeScript well enough, or be merged by a maintainer who just hoped the contributor knew what they were doing. This resulted in a poor experience for TypeScript users who could experience type definition churn and delays, and it became a burden on project maintainers as well (most of us don\'t know TypeScript very well). Moving the type definitions to DefinitelyTyped puts the maintenance in much more capable hands.\\n\\nI have to admit I was reticent about this idea in the first place. I like the idea that types ship with the package they support. It\'s a good developer experience; users install your package and it works with TypeScript straight out of the box. However Alex\'s PR addressed a real issue: what do you do when the authors of a package aren\'t interested / equipped / don\'t have the time to support TypeScript? Or don\'t want to deal with the noise of TypeScript related PRs which aren\'t relevant to them. What then?\\n\\nAlex was saying, let\'s not force it. Let the types and the library be maintained separately. This can and is done well already; React is a case in point. The React team does not work on the type definitions for React, that\'s done (excellently) by a crew of dedicated React lovers in Definitely Typed.\\n\\nIt\'s a fair point. The thing that was sad about this move was that the developer experience was going to have more friction. Users would have to `yarn add -D @testing-library/react` and then subsequently `yarn add -D @types/testing-library__react` to get the types.\\n\\nThis two step process isn\'t the end of the world, but it does make it marginally harder for TypeScript users to get up and running. It reduces the developer joy. As a side note, this is made more unlovely by `@testing-library/react` being a scoped package. [Types for a scoped package have a quirky convention for publishing.](https://stackoverflow.com/questions/47296731/how-can-i-install-typescript-declarations-for-scoped-namespaced-packages-via-ty) A fictional scoped package of `@foo/bar` would be published to npm as: `@types/foo__bar`. This is functional but non-obvious; it\'s tricky to discover. A two step process instead of a one step process is a non-useful friction that it would be great to eliminate.\\n\\nFortunately, Kent and [Daniel K](https://github.com/FredyC) had one of these moments:\\n\\n![](hang-on-lads-ive-got-a-great-idea.jpg)\\n\\nKent suggested that at the same time as dropping the type definitions that were shipped with the library, we try making `@types/testing-library__react` a dependency of `@testing-library/react`. This would mean that people installing `@testing-library/react` would get `@types/testing-library__react` installed _automatically_. So from the developers point of view, it\'s as though the type definitions shipped with the package directly.\\n\\nTo cut a long story short reader, that\'s what happened. If you\'re using `@testing-library/react` from 9.1.2 you\'re getting Definitely Typed under the covers. This was [nicely illustrated by Kent](https://github.com/testing-library/react-testing-library/pull/437#issuecomment-521763117) showing what the TypeScript consumption experience looked like before the Definitely Typed switch:\\n\\n![](RTL-9.1.1.png)\\n\\nAnd here\'s what it looked like after:\\n\\n![](RTL-9.1.2.png)\\n\\nIdentical! i.e it worked. I grant you this is one of the more boring before / after comparisons there is\u2026 But hopefully you can see it demonstrates that this is giving us exactly what we need.\\n\\nTo quote Kent once more:\\n\\n> By adding the type definitions to the dependencies of React Testing Library, the experience for users is completely unchanged. So it\'s a huge improvement for the maintenance of the type definitions without any breaking changes for the users of those definitions.\\n\\nThis is clearly an approach that\'s useful; it adds value. It would be tremendous to see other libraries that aren\'t written in TypeScript but would like to enable a good TypeScript experience for those people that do use TS also adopting this approach.\\n\\n## Update: Use a Loose Version Range in `package.json`\\n\\nWhen I [tweeted this article](https://twitter.com/johnny_reilly/status/1162843916661592064) it prompted this helpful response from [Andrew Branch](https://twitter.com/atcb) of the TypeScript team:\\n\\n> \\\\> use a loose version range This is my advice as well and should probably be mentioned in the article TBH.\\n>\\n> \u2014 Kent C. Dodds (@kentcdodds) [August 18, 2019](https://twitter.com/kentcdodds/status/1162876792287293440?ref_src=twsrc%5Etfw)\\n\\n<script async=\\"\\" src=\\"https://platform.twitter.com/widgets.js\\" charSet=\\"utf-8\\"><\/script>\\n\\nAndrew makes the useful point that if you are adding support for TypeScript via an `@types/...` dependency then it\'s wise to do so with a loose version range. [In the case of RTL we did it like this:](https://github.com/testing-library/react-testing-library/blob/c4ba755e42938018ec67dbc716037cfafca15e03/package.json#L46)\\n\\n```json\\n\\"@types/testing-library__react\\": \\"^9.1.0\\"\\n```\\n\\ni.e. Any type definition with a version of `9.1` or greater (whilst still lower than `10.0.0`) is considered valid. You could go even looser than that. If you really don\'t want to think about TypeScript beyond adding the dependency then a completely loose version range would do:\\n\\n```json\\n\\"@types/testing-library__react\\": \\"*\\"\\n```\\n\\nThis will always install the latest version of the `@types/testing-library__react` dependency and (importantly) allow users to override if there\'s a problematic `@types/testing-library__react` out there. This level of looseness is not really advised though. As in the scenario when a library (and associated type definitions) do a major release, users of the old major would get the wrong definitions by default when installing or upgrading (in range).\\n\\nProbably the most helpful approach is the approach followed by RTL; fixing the major version but allowing all minor and patch releases _inside_ a major version.\\n\\n## Update 2: Further Discussions!\\n\\nThe technique used in this blog post sparked an interesting conversation with members of the TypeScript team when it was applied to [`https://github.com/testing-library/jest-dom`](https://github.com/testing-library/jest-dom). [The conversation can be read here](https://github.com/testing-library/jest-dom/issues/123#issuecomment-523586977)."},{"id":"/2019/08/02/asp-net-authentication-hard-coding-claims","metadata":{"permalink":"/2019/08/02/asp-net-authentication-hard-coding-claims","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2019-08-02-asp-net-authentication-hard-coding-claims/index.md","source":"@site/blog/2019-08-02-asp-net-authentication-hard-coding-claims/index.md","title":"ASP.NET Core authentication: hard-coding a claim in development","description":"This post demonstrates how you can hard code user authentication claims in ASP.NET Core; a useful technique to facilate testing during development.","date":"2019-08-02T00:00:00.000Z","formattedDate":"August 2, 2019","tags":[{"label":"ASP.Net Core","permalink":"/tags/asp-net-core"},{"label":"Authentication","permalink":"/tags/authentication"}],"readingTime":2.77,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"ASP.NET Core authentication: hard-coding a claim in development","authors":"johnnyreilly","tags":["ASP.Net Core","Authentication"],"hide_table_of_contents":false},"prevItem":{"title":"Symbiotic Definitely Typed","permalink":"/2019/08/17/symbiotic-definitely-typed"},"nextItem":{"title":"Using TypeScript and ESLint with webpack (fork-ts-checker-webpack-plugin new feature!)","permalink":"/2019/07/13/typescript-and-eslint-meet-fork-ts-checker-webpack-plugin"}},"content":"This post demonstrates how you can hard code user authentication claims in ASP.NET Core; a useful technique to facilate testing during development.\\n\\nI was recently part of a hackathon team that put together an API in just 30 hours. We came second. (Not bitter, not bitter...)\\n\\nWe were moving pretty quickly during the hackathon and, when we came to the end of it, we had a working API which we were able to demo. The good news is that the API is going to graduate to be a product! We\'re going to ship this. Before we can do that though, there\'s a little tidy up to do.\\n\\nThe first thing I remembered / realised when I picked up the codebase again, was the shortcuts we\'d made on the developer experience. We\'d put the API together using ASP.Net Core. We\'re handling authentication using JWTs which is nicely supported. When we\'re deployed, an external facing proxy calls our application with the appropriate JWT and everything works as you\'d hope.\\n\\nThe question is, what\'s it like to develop against this on your laptop? Getting a JWT for when I\'m debugging locally is too much friction. I want to be able to work on the problem at hand, going away to get a JWT each time is a timesuck. So what to do? Well, during the hackathon, we just commented out `[Authorize]` attributes and hardcoded user ids in our controllers. This works, but it\'s a messy developer experience; it\'s easy to forget to uncomment things you\'ve commented and break things. There must be a better way.\\n\\nThe solution I landed on was this: in development mode (which we only use whilst debugging) we hardcode an authenticated user. The way our authentication works is that we have a claim on our principal called something like `\\"our-user-id\\"`, the value of which is our authenticated user id. So in the `ConfigureServices` method of our `Startup.cs` we have a conditional authentication registration like this:\\n\\n```cs\\n// Whilst developing, we don\'t want to authenticate; we hardcode to a particular users id\\nif (Env.IsDevelopment()) {\\n    services.AddAuthentication(nameof(DevelopmentModeAuthenticationHandler))\\n        .AddScheme<DevelopmentModeAuthenticationOptions, DevelopmentModeAuthenticationHandler>(\\n            nameof(DevelopmentModeAuthenticationHandler),\\n            options => {\\n                options.UserIdToSetInClaims = \\"this-is-a-user-id\\";\\n            }\\n        );\\n}\\nelse {\\n    // The application typically uses this\\n    services.AddAuthentication(JwtBearerDefaults.AuthenticationScheme)\\n        .AddJwtBearer(options => {\\n            // ...\\n        });\\n}\\n```\\n\\nAs you can see, we\'re using a special `DevelopmentModeAuthenticationHandler` authentication scheme in development mode, instead of JWT. As we register that, we declare the user id that we want to use. Whenever the app runs using the `DevelopmentModeAuthenticationHandler` auth, all requests will arrive using a principal with an `\\"our-user-id\\"` claim with a value of `\\"this-is-a-user-id\\"` (or whatever you\'ve set it to.)\\n\\nThe `DevelopmentModeAuthenticationHandler` looks like this:\\n\\n```cs\\nusing System.Collections.Generic;\\nusing System.Security.Claims;\\nusing System.Text.Encodings.Web;\\nusing System.Threading.Tasks;\\nusing Microsoft.AspNetCore.Authentication;\\nusing Microsoft.Extensions.Logging;\\nusing Microsoft.Extensions.Options;\\n\\nnamespace OurApp\\n{\\n    public class DevelopmentModeAuthenticationOptions : AuthenticationSchemeOptions\\n    {\\n        public string UserIdToSetInClaims { get; set; }\\n    }\\n\\n    public class DevelopmentModeAuthenticationHandler : AuthenticationHandler<DevelopmentModeAuthenticationOptions> {\\n        private readonly ILoggingService _loggingService;\\n\\n        public DevelopmentModeAuthenticationHandler(\\n            IOptionsMonitor<DevelopmentModeAuthenticationOptions> options,\\n            ILoggerFactory logger,\\n            UrlEncoder encoder,\\n            ISystemClock clock\\n        ) : base(options, logger, encoder, clock) {\\n        }\\n\\n        protected override Task<AuthenticateResult> HandleAuthenticateAsync() {\\n            var claims = new List<Claim> { new Claim(\\"our-user-id\\", Options.UserIdToSetInClaims) };\\n\\n            var identity = new ClaimsIdentity(claims, nameof(DevelopmentModeAuthenticationHandler));\\n            var ticket = new AuthenticationTicket(new ClaimsPrincipal(identity), Scheme.Name);\\n\\n            return Task.FromResult(AuthenticateResult.Success(ticket));\\n        }\\n    }\\n}\\n```\\n\\nNow, developing locally is frictionless! We don\'t comment out `[Authorize]` attributes, we don\'t hard code user ids in controllers."},{"id":"/2019/07/13/typescript-and-eslint-meet-fork-ts-checker-webpack-plugin","metadata":{"permalink":"/2019/07/13/typescript-and-eslint-meet-fork-ts-checker-webpack-plugin","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2019-07-13-typescript-and-eslint-meet-fork-ts-checker-webpack-plugin/index.md","source":"@site/blog/2019-07-13-typescript-and-eslint-meet-fork-ts-checker-webpack-plugin/index.md","title":"Using TypeScript and ESLint with webpack (fork-ts-checker-webpack-plugin new feature!)","description":"The fork-ts-checker-webpack-plugin has, since its inception, performed two classes of checking:","date":"2019-07-13T00:00:00.000Z","formattedDate":"July 13, 2019","tags":[{"label":"ESLint","permalink":"/tags/es-lint"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"fork-ts-checker-webpack-plugin","permalink":"/tags/fork-ts-checker-webpack-plugin"},{"label":"Webpack","permalink":"/tags/webpack"}],"readingTime":4.61,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Using TypeScript and ESLint with webpack (fork-ts-checker-webpack-plugin new feature!)","authors":"johnnyreilly","tags":["ESLint","TypeScript","fork-ts-checker-webpack-plugin","Webpack"],"hide_table_of_contents":false},"prevItem":{"title":"ASP.NET Core authentication: hard-coding a claim in development","permalink":"/2019/08/02/asp-net-authentication-hard-coding-claims"},"nextItem":{"title":"TypeScript / webpack - you down with PnP? Yarn, you know me!","permalink":"/2019/06/07/typescript-webpack-you-down-with-pnp"}},"content":"The `fork-ts-checker-webpack-plugin` has, since its inception, performed two classes of checking:\\n\\n1. Compilation errors which the TypeScript compiler surfaces up\\n2. Linting issues which TSLint reports\\n\\n[You may have caught the announcement that TSLint is being deprecated and ESLint is the future of linting in the TypeScript world.](https://eslint.org/blog/2019/01/future-typescript-eslint) This plainly has a bearing on linting in `fork-ts-checker-webpack-plugin`.\\n\\n[I\'ve been beavering away at adding support for ESLint to the fork-ts-checker-webpack-plugin.](https://github.com/TypeStrong/fork-ts-checker-webpack-plugin/pull/305) I\'m happy to say, the plugin now supports ESLint. Do you want to get your arms all around ESLint with `fork-ts-checker-webpack-plugin`? Read on!\\n\\n## How do you migrate from TSLint to ESLint?\\n\\nWell, first of all you need the latest and greatest `fork-ts-checker-webpack-plugin`. Support for ESLint shipped with [v1.4.0](https://github.com/TypeStrong/fork-ts-checker-webpack-plugin/releases/tag/v1.4.0).\\n\\nYou need to change the options you supply to the plugin in your `webpack.config.js` to look something like this:\\n\\n```js\\nnew ForkTsCheckerWebpackPlugin({ eslint: true });\\n```\\n\\nYou\'ll also need the various ESLint related packages to your `package.json`:\\n\\n```js\\nyarn add eslint @typescript-eslint/parser @typescript-eslint/eslint-plugin --dev\\n```\\n\\n- `eslint`\\n- `@typescript-eslint/parser`: The parser that will allow ESLint to lint TypeScript code\\n- `@typescript-eslint/eslint-plugin`: A plugin that contains ESLint rules that are TypeScript specific\\n\\nIf you want, you can pass options to ESLint using the `eslintOptions` option as well. These will be passed through to the underlying ESLint CLI Engine when it is instantiated. Docs on the supported options are [documented here](https://eslint.org/docs/developer-guide/nodejs-api#cliengine).\\n\\n## Go Configure\\n\\nNow you\'re ready to use ESLint, you just need to give it some configuration. Typically, an `.eslintrc.js` is what you want here.\\n\\n```js\\nconst path = require(\'path\');\\nmodule.exports = {\\n  parser: \'@typescript-eslint/parser\', // Specifies the ESLint parser\\n  plugins: [\'@typescript-eslint\'],\\n  env: {\\n    browser: true,\\n    jest: true,\\n  },\\n  extends: [\\n    \'plugin:@typescript-eslint/recommended\', // Uses the recommended rules from the @typescript-eslint/eslint-plugin\\n  ],\\n  parserOptions: {\\n    project: path.resolve(__dirname, \'./tsconfig.json\'),\\n    tsconfigRootDir: __dirname,\\n    ecmaVersion: 2018, // Allows for the parsing of modern ECMAScript features\\n    sourceType: \'module\', // Allows for the use of imports\\n  },\\n  rules: {\\n    // Place to specify ESLint rules. Can be used to overwrite rules specified from the extended configs\\n    // e.g. \\"@typescript-eslint/explicit-function-return-type\\": \\"off\\",\\n    \'@typescript-eslint/explicit-function-return-type\': \'off\',\\n    \'@typescript-eslint/no-unused-vars\': \'off\',\\n  },\\n};\\n```\\n\\nIf you\'re a React person (and I am!) then you\'ll also need: `yarn add eslint-plugin-react`. Then enrich your `eslintrc.js` a little:\\n\\n```js\\nconst path = require(\'path\');\\nmodule.exports = {\\n  parser: \'@typescript-eslint/parser\', // Specifies the ESLint parser\\n  plugins: [\\n    \'@typescript-eslint\',\\n    \'react\',\\n    // \'prettier\' commented as we don\'t want to run prettier through eslint because performance\\n  ],\\n  env: {\\n    browser: true,\\n    jest: true,\\n  },\\n  extends: [\\n    \'plugin:@typescript-eslint/recommended\', // Uses the recommended rules from the @typescript-eslint/eslint-plugin\\n    \'prettier/@typescript-eslint\', // Uses eslint-config-prettier to disable ESLint rules from @typescript-eslint/eslint-plugin that would conflict with prettier\\n    // \'plugin:react/recommended\', // Uses the recommended rules from @eslint-plugin-react\\n    \'prettier/react\', // disables react-specific linting rules that conflict with prettier\\n    // \'plugin:prettier/recommended\' // Enables eslint-plugin-prettier and displays prettier errors as ESLint errors. Make sure this is always the last configuration in the extends array.\\n  ],\\n  parserOptions: {\\n    project: path.resolve(__dirname, \'./tsconfig.json\'),\\n    tsconfigRootDir: __dirname,\\n    ecmaVersion: 2018, // Allows for the parsing of modern ECMAScript features\\n    sourceType: \'module\', // Allows for the use of imports\\n    ecmaFeatures: {\\n      jsx: true, // Allows for the parsing of JSX\\n    },\\n  },\\n  rules: {\\n    // Place to specify ESLint rules. Can be used to overwrite rules specified from the extended configs\\n    // e.g. \\"@typescript-eslint/explicit-function-return-type\\": \\"off\\",\\n    \'@typescript-eslint/explicit-function-return-type\': \'off\',\\n    \'@typescript-eslint/no-unused-vars\': \'off\',\\n\\n    // These rules don\'t add much value, are better covered by TypeScript and good definition files\\n    \'react/no-direct-mutation-state\': \'off\',\\n    \'react/no-deprecated\': \'off\',\\n    \'react/no-string-refs\': \'off\',\\n    \'react/require-render-return\': \'off\',\\n\\n    \'react/jsx-filename-extension\': [\\n      \'warn\',\\n      {\\n        extensions: [\'.jsx\', \'.tsx\'],\\n      },\\n    ], // also want to use with \\".tsx\\"\\n    \'react/prop-types\': \'off\', // Is this incompatible with TS props type?\\n  },\\n  settings: {\\n    react: {\\n      version: \'detect\', // Tells eslint-plugin-react to automatically detect the version of React to use\\n    },\\n  },\\n};\\n```\\n\\nYou can add Prettier into the mix too. You can see how it is used in the above code sample. But given the impact that has on performance I wouldn\'t recommend it; hence it\'s commented out. [There\'s a good piece by Rob Cooper\'s for more details on setting up Prettier and VS Code with TypeScript and ESLint.](https://dev.to/robertcoopercode/using-eslint-and-prettier-in-a-typescript-project-53jb)\\n\\n## Performance and Power Tools\\n\\nIt\'s worth noting that support for TypeScript in ESLint is still brand new. As such, the rule of \\"Make it Work, Make it Right, Make it Fast\\" applies.... ESLint with TypeScript still has some performance issues which should be ironed out in the fullness of time. You can [track them here](https://github.com/typescript-eslint/typescript-eslint/issues/389).\\n\\nThis is important to bear in mind as, when I converted a large codebase over to using ESLint, I discovered that initial performance of linting was terribly slow. Something that\'s worth doing right now is identifying which rules are costing you most timewise and tweaking based on whether you think they\'re earning their keep.\\n\\nThe [`TIMING` environment variable](https://eslint.org/docs/developer-guide/working-with-rules#per-rule-performance) can be used to provide a report on the relative cost performance wise of running each rule. A nice way to plug this into your workflow is to add the `cross-env` package to your project: `yarn add cross-env -D` and then add 2 scripts to your `package.json`:\\n\\n```\\n\\"lint\\": \\"eslint ./\\",\\n\\"lint-rule-timings\\": \\"cross-env TIMING=1 yarn lint\\"\\n```\\n\\n- `lint` \\\\- just runs the linter standalone\\n- `lint-rule-timings` \\\\- does the same but with the `TIMING` environment variable set to 1 so a report will be generated.\\n\\nI\'d advise, making use of `lint-rule-timings` to identify which rules are costing you performance and then turning `off` rules as you need to. Remember, different rules have different value.\\n\\n[Finally, if you\'d like to see how it\'s done, here\'s an example of porting from TSLint to ESLint.](https://github.com/TypeStrong/ts-loader/pull/960)"},{"id":"/2019/06/07/typescript-webpack-you-down-with-pnp","metadata":{"permalink":"/2019/06/07/typescript-webpack-you-down-with-pnp","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2019-06-07-typescript-webpack-you-down-with-pnp/index.md","source":"@site/blog/2019-06-07-typescript-webpack-you-down-with-pnp/index.md","title":"TypeScript / webpack - you down with PnP? Yarn, you know me!","description":"Yarn PnP is an innovation by the Yarn team designed to speed up module resolution by node. To quote the (excellent) docs:","date":"2019-06-07T00:00:00.000Z","formattedDate":"June 7, 2019","tags":[{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"yarn","permalink":"/tags/yarn"},{"label":"Webpack","permalink":"/tags/webpack"},{"label":"PnP","permalink":"/tags/pn-p"}],"readingTime":5.515,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"TypeScript / webpack - you down with PnP? Yarn, you know me!","authors":"johnnyreilly","tags":["TypeScript","yarn","Webpack","PnP"],"hide_table_of_contents":false},"prevItem":{"title":"Using TypeScript and ESLint with webpack (fork-ts-checker-webpack-plugin new feature!)","permalink":"/2019/07/13/typescript-and-eslint-meet-fork-ts-checker-webpack-plugin"},"nextItem":{"title":"TypeScript and high CPU usage - watch don\'t stare!","permalink":"/2019/05/23/typescript-and-high-cpu-usage-watch"}},"content":"Yarn PnP is an innovation by the Yarn team designed to speed up module resolution by node. To quote the [(excellent) docs](https://yarnpkg.com/en/docs/pnp):\\n\\n> Plug\u2019n\u2019Play is an alternative installation strategy unveiled in September 2018...\\n>\\n> The way regular installs work is simple: Yarn generates a `node_modules` directory that Node is then able to consume. In this context, Node doesn\u2019t know the first thing about what a package is: it only reasons in terms of files. \u201cDoes this file exist here? No? Let\u2019s look in the parent `node_modules` then. Does it exist here? Still no? Too bad\u2026 parent folder it is!\u201d - and it does this until it matches something that matches one of the possibilities. That\u2019s vastly inefficient.\\n>\\n> When you think about it, Yarn knows everything about your dependency tree - it evens installs it! So why is Node tasked with locating your packages on the disk? Why don\u2019t we simply query Yarn, and let it tell us where to look for a package X required by a package Y? That\u2019s what Plug\u2019n\u2019Play (abbreviated PnP) is. Instead of generating a node_modules directory and leaving the resolution to Node, we now generate a single .pnp.js file and let Yarn tell us where to find our packages.\\n\\nYarn has been worked upon, amongst others, by the excellent [Ma\xebl Nison](https://twitter.com/arcanis). You can hear him talking about it in person [in this talk at JSConfEU](https://youtu.be/XePfzVs852s).\\n\\nThanks particularly to Ma\xebl\'s work, it\'s possible to use Yarn PnP with TypeScript using webpack with `ts-loader` _and_`fork-ts-checker-webpack-plugin`. This post intends to show you just how simple it is to convert a project that uses either to work with Yarn PnP.\\n\\n## Vanilla `ts-loader`\\n\\nYour project is built using standalone `ts-loader`; i.e. a simple setup that handles both transpilation and type checking.\\n\\nFirst things first, add this property to your `package.json`: (this is only required if you are using Yarn 1; this tag will be optional starting from the v2, where projects will switch to PnP by default.)\\n\\n```\\n{\\n    \\"installConfig\\": {\\n        \\"pnp\\": true\\n    }\\n}\\n```\\n\\nAlso, because this is webpack, we\'re going to need to add an extra dependency in the form of `pnp-webpack-plugin`:\\n\\n```\\nyarn add -D pnp-webpack-plugin\\n```\\n\\nTo quote the excellent docs, make the following amends to your `webpack.config.js`:\\n\\n```\\nconst PnpWebpackPlugin = require(`pnp-webpack-plugin`);\\n\\nmodule.exports = {\\n    module: {\\n        rules: [{\\n            test: /\\\\.ts$/,\\n            loader: require.resolve(\'ts-loader\'),\\n            options: PnpWebpackPlugin.tsLoaderOptions(),\\n        }],\\n    },\\n    resolve: {\\n        plugins: [ PnpWebpackPlugin, ],\\n    },\\n    resolveLoader: {\\n        plugins: [ PnpWebpackPlugin.moduleLoader(module), ],\\n    },\\n};\\n```\\n\\nIf you have any options you want to pass to `ts-loader`, just pass them as parameter of `pnp-webpack-plugin`\'s `tsLoaderOptions` function and it will take care of forwarding them properly. Behind the scenes the `tsLoaderOptions` function is providing `ts-loader` with the options necessary to switch into Yarn PnP mode.\\n\\nCongratulations; you now have `ts-loader` functioning with Yarn PnP support!\\n\\n## `fork-ts-checker-webpack-plugin` with `ts-loader`\\n\\nYou may well be using `fork-ts-checker-webpack-plugin` to handle type checking whilst `ts-loader` gets on with the transpilation. This workflow is also supported using `pnp-webpack-plugin`. You\'ll have needed to follow the same steps as the `ts-loader` setup. It\'s just the `webpack.config.js` tweaks that will be different.\\n\\n```\\nconst PnpWebpackPlugin = require(`pnp-webpack-plugin`);\\n\\nmodule.exports = {\\n    plugins: {\\n        new ForkTsCheckerWebpackPlugin(PnpWebpackPlugin.forkTsCheckerOptions({\\n            useTypescriptIncrementalApi: false, // not possible to use this until: https://github.com/microsoft/TypeScript/issues/31056\\n        })),\\n    }\\n    module: {\\n        rules: [{\\n            test: /\\\\.ts$/,\\n            loader: require.resolve(\'ts-loader\'),\\n            options: PnpWebpackPlugin.tsLoaderOptions({ transpileOnly: true }),\\n        }],\\n    },\\n    resolve: {\\n        plugins: [ PnpWebpackPlugin, ],\\n    },\\n    resolveLoader: {\\n        plugins: [ PnpWebpackPlugin.moduleLoader(module), ],\\n    },\\n};\\n```\\n\\nAgain if you have any options you want to pass to `ts-loader`, just pass them as parameter of `pnp-webpack-plugin`\'s `tsLoaderOptions` function. As we\'re using `fork-ts-checker-webpack-plugin` we\'re going to want to stop `ts-loader` doing type checking with the `transpileOnly: true` option.\\n\\nWe\'re now initialising `fork-ts-checker-webpack-plugin` with `pnp-webpack-plugin`\'s `forkTsCheckerOptions` function. Behind the scenes the `forkTsCheckerOptions` function is providing the `fork-ts-checker-webpack-plugin` with the options necessary to switch into Yarn PnP mode.\\n\\nAnd that\'s it! You now have `ts-loader` and `fork-ts-checker-webpack-plugin` functioning with Yarn PnP support!\\n\\n## Living on the Bleeding Edge\\n\\nWhilst you can happily develop and build using Yarn PnP, it\'s worth bearing in mind that this is a new approach. As such, there\'s some rough edges right now.\\n\\nIf you\'re interested in Yarn PnP, it\'s worth taking the v2 of Yarn (Berry) for a spin. You can find it here: [https://github.com/yarnpkg/berry](https://github.com/yarnpkg/berry). It\'s where most of the Yarn PnP work happens, and it includes zip loading - two birds, one stone!\\n\\nBecause there isn\'t first class support for Yarn PnP in TypeScript itself yet, you cannot make use of the Watch API through `fork-ts-checker-webpack-plugin`. (You can read about that issue [here](https://github.com/microsoft/TypeScript/issues/31056))\\n\\nAs you\'ve likely noticed, the webpack configuration required makes for a noisy `webpack.config.js`. Further to that, VS Code (which is powered by TypeScript remember) has no support for Yarn PnP yet and so will present resolution errors to you. If you can ignore the sea of red squigglies all over your source files in the editor and just look at your webpack build you\'ll be fine.\\n\\nThere is a tool called `PnPify` that adds support for PnP to TypeScript (in particular tsc). You can find more information here: [https://yarnpkg.github.io/berry/advanced/pnpify](https://yarnpkg.github.io/berry/advanced/pnpify). For tsc it would be:\\n\\n```\\n$> yarn pnpify tsc [...]\\n```\\n\\nThe gist is that it simulates the existence of `node_modules` by leveraging the data from the PnP file. As such it\'s not a perfect fix (`pnp-webpack-plugin` is a better integration), but it\'s a very useful tool to have to unblock yourself when using a project that doesn\'t support it.\\n\\nPnPify actually allows us to use TypeScript in VSCode with PnP! Its documentation is here: [https://yarnpkg.github.io/berry/advanced/pnpify#vscode-support](https://yarnpkg.github.io/berry/advanced/pnpify#vscode-support)\\n\\nAll of these hindrances should hopefully be resolved in future. Ideally, one day a good developer experience can be the default experience. In the meantime, you can still dev - just be prepared for the rough edges. Here\'s some useful resources to track the future of support:\\n\\n- You can follow more on built in webpack support here: [https://github.com/webpack/enhanced-resolve/issues/162](https://github.com/webpack/enhanced-resolve/issues/162)\\n- And on built in TypeScript support here: [https://github.com/Microsoft/TypeScript/issues/18896](https://github.com/Microsoft/TypeScript/issues/18896)\\n- Finally, there it\'s worth watching the [nodejs/module](https://github.com/nodejs/modules) repository, which debates amongst other things how to properly integrate loaders with Node.\\n\\nThis last one would be nice because:\\n\\n- We\'d stop having to patch require\\n- We probably wouldn\'t have to use yarn node if Node itself was able to find the loader somehow (such as if it was listed in the package.json metadata)\\n\\nThanks to Ma\xebl for his tireless work on Yarn. To my mind Ma\xebl is certainly a candidate for the hardest worker in open source. I\'ve been shamelessly borrowing his excellent docs for this post - thanks for writing so excellently Ma\xebl!"},{"id":"/2019/05/23/typescript-and-high-cpu-usage-watch","metadata":{"permalink":"/2019/05/23/typescript-and-high-cpu-usage-watch","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2019-05-23-typescript-and-high-cpu-usage-watch/index.md","source":"@site/blog/2019-05-23-typescript-and-high-cpu-usage-watch/index.md","title":"TypeScript and high CPU usage - watch don\'t stare!","description":"I\'m one of the maintainers of the fork-ts-checker-webpack-plugin. Hi there!","date":"2019-05-23T00:00:00.000Z","formattedDate":"May 23, 2019","tags":[{"label":"cross-env","permalink":"/tags/cross-env"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"fork-ts-checker-webpack-plugin","permalink":"/tags/fork-ts-checker-webpack-plugin"},{"label":"watch API","permalink":"/tags/watch-api"},{"label":"Webpack","permalink":"/tags/webpack"}],"readingTime":2.73,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"TypeScript and high CPU usage - watch don\'t stare!","authors":"johnnyreilly","tags":["cross-env","TypeScript","fork-ts-checker-webpack-plugin","watch API","Webpack"],"hide_table_of_contents":false},"prevItem":{"title":"TypeScript / webpack - you down with PnP? Yarn, you know me!","permalink":"/2019/06/07/typescript-webpack-you-down-with-pnp"},"nextItem":{"title":"react-select with less typing lag","permalink":"/2019/04/27/react-select-with-less-typing-lag"}},"content":"I\'m one of the maintainers of the [fork-ts-checker-webpack-plugin](https://github.com/Realytics/fork-ts-checker-webpack-plugin). Hi there!\\n\\nRecently, various issues have been raised against create-react-app (which uses fork-ts-checker-webpack-plugin) as well as against the plugin itself. They\'ve been related to the level of CPU usage in watch mode on idle; i.e. it\'s high!\\n\\n- [https://github.com/Realytics/fork-ts-checker-webpack-plugin/issues/236](https://github.com/Realytics/fork-ts-checker-webpack-plugin/issues/236)\\n- [https://github.com/facebook/create-react-app/issues/6792](https://github.com/facebook/create-react-app/issues/6792)\\n\\n## Why High?\\n\\nNow, under the covers, the `fork-ts-checker-webpack-plugin` uses the TypeScript watch API.\\n\\nThe marvellous [John](https://github.com/NeKJ) (not me - another John) did some digging and discovered the root cause came down to the way that the TypeScript watch API watches files:\\n\\n> TS uses internally the `fs.watch` and `fs.watchFile` API functions of nodejs for their watch mode. The latter function [is even not recommended by nodejs documentation](https://nodejs.org/api/fs.html#fs_fs_watchfile_filename_options_listener) for performance reasons, and urges to use `fs.watch` instead.\\n>\\n> **NodeJS doc:**\\n>\\n> > Using fs.watch() is more efficient than fs.watchFile and fs.unwatchFile. fs.watch should be used instead of fs.watchFile and fs.unwatchFile when possible.\\n\\n## \\"there is another\\"\\n\\nJohn also found that there are other file watching behaviours offered by TypeScript. What\'s more, the file watching behaviour is _configurable with an environment variable_. That\'s right, if an environment variable called `TSC_WATCHFILE` is set, it controls the file watching approach used. Big news!\\n\\nJohn did some rough benchmarking of the performance of the different options that be set on his PC running linux 64 bit. Here\'s how it came out:\\n\\n| Value                                 | CPU usage on idle |\\n| ------------------------------------- | ----------------- |\\n| TS default _(TSC_WATCHFILE not set)_  | **7\\\\.4%**         |\\n| UseFsEventsWithFallbackDynamicPolling | 0\\\\.2%             |\\n| UseFsEventsOnParentDirectory          | 0\\\\.2%             |\\n| PriorityPollingInterval               | **6\\\\.2%**         |\\n| DynamicPriorityPolling                | 0\\\\.5%             |\\n| UseFsEvents                           | 0\\\\.2%             |\\n\\nAs you can see, the default performs poorly. On the other hand, an option like `UseFsEventsWithFallbackDynamicPolling` is comparative greasy lightning.\\n\\n## workaround!\\n\\nTo get this better experience into your world now, you could just set an environment variable on your machine. However, that doesn\'t scale; let\'s instead look at introducing the environment variable into your project explicitly.\\n\\nWe\'re going to do this in a cross platform way using [`cross-env`](https://github.com/kentcdodds/cross-env). This is a mighty useful utility by Kent C Dodds which allows you to set environment variables in a way that will work on Windows, Mac and Linux. Imagine it as the jQuery of the environment variables world :-)\\n\\nLet\'s add it as a `devDependency`:\\n\\n```\\nyarn add -D cross-env\\n```\\n\\nThen take a look at your `package.json`. You\'ve probably got a `start` script that looks something like this:\\n\\n```\\n\\"start\\": \\"webpack-dev-server --progress --color --mode development --config webpack.config.development.js\\",\\n```\\n\\nOr if you\'re a create-react-app user maybe this:\\n\\n```\\n\\"start\\": \\"react-scripts start\\",\\n```\\n\\nPrefix your `start` script with `cross-env TSC_WATCHFILE=UseFsEventsWithFallbackDynamicPolling`. This will, when run, initialise an environment variable called `TSC_WATCHFILE` with the value `UseFsEventsWithFallbackDynamicPolling`. Then it will start your development server as it did before. When TypeScript is fired up by webpack it will see this environment variable and use it to configure the file watching behaviour to one of the more performant options.\\n\\nSo, in the case of a `create-react-app` user, your finished `start` script would look like this:\\n\\n```\\n\\"start\\": \\"cross-env TSC_WATCHFILE=UseFsEventsWithFallbackDynamicPolling react-scripts start\\",\\n```\\n\\n## The Future\\n\\nThere\'s a possibility that the default watch behaviour may change in TypeScript in future. It\'s currently under discussion, you can read more [here](https://github.com/microsoft/TypeScript/issues/31048)."},{"id":"/2019/04/27/react-select-with-less-typing-lag","metadata":{"permalink":"/2019/04/27/react-select-with-less-typing-lag","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2019-04-27-react-select-with-less-typing-lag/index.md","source":"@site/blog/2019-04-27-react-select-with-less-typing-lag/index.md","title":"react-select with less typing lag","description":"This is going out to all those people using react-select with 1000+ items to render. To those people typing into the select and saying out loud \\"it\'s so laggy.... This can\'t be... It\'s 2019... I mean, right?\\" To the people who read this GitHub issue top to bottom 30 times and still came back unsure of what to do. This is for you.","date":"2019-04-27T00:00:00.000Z","formattedDate":"April 27, 2019","tags":[{"label":"large lists","permalink":"/tags/large-lists"},{"label":"react-select","permalink":"/tags/react-select"},{"label":"typing","permalink":"/tags/typing"}],"readingTime":2.035,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"react-select with less typing lag","authors":"johnnyreilly","tags":["large lists","react-select","typing"],"hide_table_of_contents":false},"prevItem":{"title":"TypeScript and high CPU usage - watch don\'t stare!","permalink":"/2019/05/23/typescript-and-high-cpu-usage-watch"},"nextItem":{"title":"Template Tricks for a Dainty DOM","permalink":"/2019/03/24/template-tricks-for-dainty-dom"}},"content":"This is going out to all those people using [`react-select`](https://react-select.com) with 1000+ items to render. To those people typing into the select and saying out loud \\"it\'s _so_ laggy.... This can\'t be... It\'s 2019... I mean, right?\\" To the people who read this [GitHub issue](https://github.com/JedWatson/react-select/issues/3128) top to bottom 30 times and still came back unsure of what to do. This is for you.\\n\\nI\'m lying. Mostly this goes out to me. I have a select box. I need it to render 2000+ items. I want it to be lovely. I want my users to be delighted as they use it. I want them to type in and (_this is the crucial part!_) for the control to feel responsive. Not laggy. Not like each keypress is going to Jupiter and back before it renders to the screen.\\n\\nAmongst the various gems on the GitHub issue are shared CodeSandboxes illustrating ways to integrate react-select with react-window. That\'s great and they do improve things. However, they don\'t do much to improve the laggy typing feel. There\'s [brief mention](https://github.com/JedWatson/react-select/issues/3128#issuecomment-431397942) of a props tweak you can make to react-select; this:\\n\\n```js\\nfilterOption={createFilter({ ignoreAccents: false })}\\n```\\n\\nWhat does this do? Well, this improves the typing lag experience _massively_. For why? Well, [if you look at the code](https://github.com/JedWatson/react-select/blob/292bad3298f2cafad6767f2134bd79a9c27e4073/src/filters.js#L21) you find that the default value is `ignoreAccents: true`. This default makes react-select invoke an expensive (and scary sounding) function called [`stripDiacritics`](https://github.com/JedWatson/react-select/blob/292bad3298f2cafad6767f2134bd79a9c27e4073/src/diacritics.js#L90). Not once but twice. Ouchy. And this kills performance.\\n\\nBut if you\'re okay with accents not being ignored (and _spoiler_: I am) then this is the option for you.\\n\\nHere\'s a CodeSandbox which also includes the `ignoreAccents: false` tweak. Enjoy!\\n\\n[![Edit johnnyreilly/react-window-with-react-select-less-laggy](https://codesandbox.io/static/img/play-codesandbox.svg)](https://codesandbox.io/s/zn70lqp31m?fontsize=14)\\n\\n```js\\nimport React, { Component } from \'react\';\\nimport ReactDOM from \'react-dom\';\\nimport Select, { createFilter } from \'react-select\';\\nimport { FixedSizeList as List } from \'react-window\';\\n\\nimport \'./styles.css\';\\n\\nconst options = [];\\nfor (let i = 0; i < 2500; i = i + 1) {\\n  options.push({ value: i, label: `Option ${i}` });\\n}\\n\\nconst height = 35;\\n\\nclass MenuList extends Component {\\n  render() {\\n    const { options, children, maxHeight, getValue } = this.props;\\n    const [value] = getValue();\\n    const initialOffset = options.indexOf(value) * height;\\n\\n    return (\\n      <List\\n        height={maxHeight}\\n        itemCount={children.length}\\n        itemSize={height}\\n        initialScrollOffset={initialOffset}\\n      >\\n        {({ index, style }) => <div style={style}>{children[index]}</div>}\\n      </List>\\n    );\\n  }\\n}\\n\\nconst App = () => (\\n  <Select\\n    filterOption={createFilter({ ignoreAccents: false })} // this makes all the difference!\\n    components={{ MenuList }}\\n    options={options}\\n  />\\n);\\n\\nReactDOM.render(<App />, document.getElementById(\'root\'));\\n```"},{"id":"/2019/03/24/template-tricks-for-dainty-dom","metadata":{"permalink":"/2019/03/24/template-tricks-for-dainty-dom","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2019-03-24-template-tricks-for-dainty-dom/index.md","source":"@site/blog/2019-03-24-template-tricks-for-dainty-dom/index.md","title":"Template Tricks for a Dainty DOM","description":"I\'m somewhat into code golf. Placing restrictions on what you\'re \\"allowed\\" to do in code and seeing what the happens as a result. I\'d like to share with you something that came out of some recent dabblings.","date":"2019-03-24T00:00:00.000Z","formattedDate":"March 24, 2019","tags":[{"label":"DOM","permalink":"/tags/dom"},{"label":"template","permalink":"/tags/template"},{"label":"Materialized","permalink":"/tags/materialized"}],"readingTime":5.265,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Template Tricks for a Dainty DOM","authors":"johnnyreilly","tags":["DOM","template","Materialized"],"hide_table_of_contents":false},"prevItem":{"title":"react-select with less typing lag","permalink":"/2019/04/27/react-select-with-less-typing-lag"},"nextItem":{"title":"Google Analytics API and ASP.Net Core","permalink":"/2019/03/22/google-analytics-api-and-aspnet-core"}},"content":"I\'m somewhat into code golf. Placing restrictions on what you\'re \\"allowed\\" to do in code and seeing what the happens as a result. I\'d like to share with you something that came out of some recent dabblings.\\n\\nTypically I spend a good amount of time playing with TypeScript. Either working on build tools or making web apps with it. (Usually with a portion of React on the side.) This is something different.\\n\\nI have a side project on the go which is essentially a mini analytics dashboard. For the purposes of this piece let\'s call it \\"StatsDash\\". When I was starting it I thought: let\'s try something different. Let\'s build StatsDash with HTML _only_. The actual HTML is hand cranked by me and generated in ASP.Net Core / C# using a combination of LINQ and string interpolation. (Who needs Razor? \ud83d\ude0e) I\'ll say it\'s pretty fun - but the back end is not what I want to focus on.\\n\\nI got something up and running pretty quickly in pure HTML. The first lesson I learned was this: HTML alone is hella ugly. So I relaxed my criteria; I allowed CSS to come play as long as I didn\'t have to write any / much myself. There followed some experimentation with different CSS frameworks. For a while I rolled with Bootstrap (old school!), then Bulma and finally I settled on [Materialized](https://materializecss.com/). Materialized is a heavily inspired by Google\'s Material Design and is hence quite beautiful. With my HTML and Materialize\'s CSS we were rolling. Beautiful stats - no JS.\\n\\n## \\"Oh All Right; Just a Splash\\"\\n\\nLovely as things were, StatsDash quickly got to the point where there was too much information on the screen. It was time to make some changes. If data is to convey a message, it must first be comprehensible.\\n\\nI needed a way to hide and show data as people interacted with StatsDash. I wanted to achieve this _without_ starting to render on the client side and also without going back to the server each time.\\n\\nIf you want interactions in your UI all roads lead to JS. It\'s certainly possible to do some tricks with CSS but that\'s a round of code golf I\'m ill equipped to play. So, I took a look at what Materialized had to offer. Usefully it has a [Modal](https://materializecss.com/modals.html) component. With that in play I\'d be able to separate the detailed information into different modals which the users could show and hide as required. Perfect!\\n\\nIt required a little JS. What\'s a line or two between friends? Dear reader, I compromised once more.\\n\\n## The DOM Bunker\\n\\nWith my handy modals, StatsDash was now a one stop shop for a great deal of information. Info which took the form of DOM nodes. Lots of them. And by \\"lots of them\\" I want you to think along the lines of \\"space is big, really big...\\".\\n\\nThis was impacting users. Clicking to open a modal resulted in a noticeable lag. It would take 2+ seconds for the browser to respond. Users found themselves clicking multiple times; wondering why nothing seemed to occur. In the end the modal would shuffle into view. However, this wasn\'t the best experience. The lack of responsiveness was getting in the way of users enjoying all StatsDash had to offer.\\n\\nRunning an audit of StatsDash in Chrome DevTools there was no doubt we had a DOM problem:\\n\\n![](DOM-massive.png)\\n\\nWhat to do? I still didn\'t want to go back to the server on each click in StatsDash. And I didn\'t want to start writing rendering code on the client as well either. I have in the past mixed client and server side rendering and I know well that it\'s a first class ticket to a confusing codebase.\\n\\n## Smuggling DOM in Templates\\n\\nThere\'s a mechanism that supports this use case directly: the `&lt;template&gt;` element. [To quote MDN](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/template):\\n\\n> The HTML Content Template (`&lt;template&gt;`) element is a mechanism for holding client-side content that is not to be rendered when a page is loaded but may subsequently be instantiated during runtime using JavaScript.\\n\\n> Think of a template as a content fragment that is being stored for subsequent use in the document.\\n\\nThis is _exactly_ what I\'m after. I can keep my rendering server side, but instead wrap content that isn\'t immediately visible to users inside a `&lt;template&gt;` element and render that only when users need it.\\n\\nSo in the case of my modals (where most of my DOM lives), I can tuck the contents of each modal into a `&lt;template&gt;` element. Then, when the user clicks to open a modal we move that template content into the DOM so they can see it. Likewise, as they close a modal we can clear out the modal\'s DOM content to ease the load on the dear old browser.\\n\\n## \\"That Sounds Complicated...\\"\\n\\nIt\'s not. Let me show you how easily this is accomplished. First of all, wrap all your modal contents into `&lt;template&gt;` elements. They should look a little something like this:\\n\\n```html\\n<div>\\n  <button data-target=\\"modalId\\" class=\\"btn modal-trigger\\">\\n    Open the Modal!\\n  </button>\\n\\n  <template>\\n    \x3c!--\\n        loads of DOM nodes\\n        --\x3e\\n  </template>\\n\\n  <div id=\\"modalId\\" class=\\"modal modal-fixed-footer\\"></div>\\n</div>\\n```\\n\\nNext, where you initialise your modals you need to make a little tweak:\\n\\n```js\\ndocument.addEventListener(\'DOMContentLoaded\', function () {\\n  M.Modal.init(document.querySelectorAll(\'.modal\'), {\\n    onOpenStart: (modalDiv) => {\\n      const template = modalDiv.parentNode.querySelector(\'template\');\\n\\n      modalDiv.appendChild(document.importNode(template.content, true));\\n    },\\n    onCloseEnd: (modalDiv) => {\\n      while (modalDiv.firstChild) {\\n        modalDiv.removeChild(modalDiv.firstChild);\\n      }\\n    },\\n  });\\n});\\n```\\n\\nThat\'s it! As you can see, before we open our modals, the `onOpenStart` callback will fire which creates the actual DOM elements based upon the `template`. And when the modals finish closing the `onCloseEnd` callback runs to remove those DOM elements once more.\\n\\nFor this minimal change, the client gets a dramatically different user experience. StatsDash went from super laggy to satisfyingly fast. Using `template`s, The number of initial DOM nodes dropped from more than _20,000_ to _200_. That\'s right \ud83d\udcaf times smaller!\\n\\n## Do It Yourself\\n\\nThe code examples above rely upon the Materialize modals. However the principles used here are broadly applicable. It\'s easy for you to take the approach outlined here and apply it in a different situation.\\n\\nIf you\'re interested in some of the other exciting things you can do with templates then I recommend [Eric Bidelman\'s post on the topic](https://www.html5rocks.com/en/tutorials/webcomponents/template/)."},{"id":"/2019/03/22/google-analytics-api-and-aspnet-core","metadata":{"permalink":"/2019/03/22/google-analytics-api-and-aspnet-core","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2019-03-22-google-analytics-api-and-aspnet-core/index.md","source":"@site/blog/2019-03-22-google-analytics-api-and-aspnet-core/index.md","title":"Google Analytics API and ASP.Net Core","description":"I recently had need to be able to access the API for Google Analytics from ASP.Net Core. Getting this up and running turned out to be surprisingly tough because of an absence of good examples. So here it is; an example of how you can access a simple page access stat using the API:","date":"2019-03-22T00:00:00.000Z","formattedDate":"March 22, 2019","tags":[{"label":"asp net core","permalink":"/tags/asp-net-core"},{"label":"google analytics","permalink":"/tags/google-analytics"}],"readingTime":1.885,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Google Analytics API and ASP.Net Core","authors":"johnnyreilly","tags":["asp net core","google analytics"],"hide_table_of_contents":false},"prevItem":{"title":"Template Tricks for a Dainty DOM","permalink":"/2019/03/24/template-tricks-for-dainty-dom"},"nextItem":{"title":"fork-ts-checker-webpack-plugin v1.0","permalink":"/2019/03/06/fork-ts-checker-webpack-plugin-v1"}},"content":"I recently had need to be able to access the API for Google Analytics from ASP.Net Core. Getting this up and running turned out to be surprisingly tough because of an absence of good examples. So here it is; an example of how you can access a simple page access stat using [the API](https://www.nuget.org/packages/Google.Apis.AnalyticsReporting.v4/):\\n\\n```cs\\nasync Task<SomeKindOfDataStructure[]> GetUsageFromGoogleAnalytics(DateTime startAtThisDate, DateTime endAtThisDate)\\n{\\n    // Create the DateRange object. Here we want data from last week.\\n    var dateRange = new DateRange\\n    {\\n        StartDate = startAtThisDate.ToString(\\"yyyy-MM-dd\\"),\\n        EndDate = endAtThisDate.ToString(\\"yyyy-MM-dd\\")\\n    };\\n    // Create the Metrics and dimensions object.\\n    // var metrics = new List<Metric> { new Metric { Expression = \\"ga:sessions\\", Alias = \\"Sessions\\" } };\\n    // var dimensions = new List<Dimension> { new Dimension { Name = \\"ga:pageTitle\\" } };\\n    var metrics = new List<Metric> { new Metric { Expression = \\"ga:uniquePageviews\\" } };\\n    var dimensions = new List<Dimension> {\\n        new Dimension { Name = \\"ga:date\\" },\\n        new Dimension { Name = \\"ga:dimension1\\" }\\n    };\\n\\n    // Get required View Id from configuration\\n    var viewId = $\\"ga:{\\"[VIEWID]\\"}\\";\\n\\n    // Create the Request object.\\n    var reportRequest = new ReportRequest\\n    {\\n        DateRanges = new List<DateRange> { dateRange },\\n        Metrics = metrics,\\n        Dimensions = dimensions,\\n        FiltersExpression = \\"ga:pagePath==/index.html\\",\\n        ViewId = viewId\\n    };\\n\\n    var getReportsRequest = new GetReportsRequest {\\n        ReportRequests = new List<ReportRequest> { reportRequest }\\n    };\\n\\n    //Invoke Google Analytics API call and get report\\n    var analyticsService = GetAnalyticsReportingServiceInstance();\\n    var response = await (analyticsService.Reports.BatchGet(getReportsRequest)).ExecuteAsync();\\n\\n    var logins = response.Reports[0].Data.Rows.Select(row => new SomeKindOfDataStructure {\\n        Date = new DateTime(\\n            year: Convert.ToInt32(row.Dimensions[0].Substring(0, 4)),\\n            month: Convert.ToInt32(row.Dimensions[0].Substring(4, 2)),\\n            day: Convert.ToInt32(row.Dimensions[0].Substring(6, 2))),\\n        NumberOfLogins = Convert.ToInt32(row.Metrics[0].Values[0])\\n    })\\n    .OrderByDescending(login => login.Date)\\n    .ToArray();\\n\\n    return logins;\\n}\\n\\n/// <summary>\\n/// Intializes and returns Analytics Reporting Service Instance\\n/// </summary>\\nAnalyticsReportingService GetAnalyticsReportingServiceInstance() {\\n    var googleAuthFlow = new GoogleAuthorizationCodeFlow(new GoogleAuthorizationCodeFlow.Initializer {\\n        ClientSecrets = new ClientSecrets {\\n            ClientId = \\"[CLIENTID]\\",\\n            ClientSecret = \\"[CLIENTSECRET]\\"\\n        }\\n    });\\n\\n    var responseToken = new TokenResponse {\\n        AccessToken = \\"[ANALYTICSTOKEN]\\",\\n        RefreshToken = \\"[REFRESHTOKEN]\\",\\n        Scope = AnalyticsReportingService.Scope.AnalyticsReadonly, //Read-only access to Google Analytics,\\n        TokenType = \\"Bearer\\",\\n    };\\n\\n    var credential = new UserCredential(googleAuthFlow, \\"\\", responseToken);\\n\\n    // Create the  Analytics service.\\n    return new AnalyticsReportingService(new BaseClientService.Initializer {\\n        HttpClientInitializer = credential,\\n        ApplicationName = \\"my-super-applicatio\\",\\n    });\\n}\\n```\\n\\nYou can see above that you need various credentials to be able to use the API. You can acquire these by logging into GA. Enjoy!"},{"id":"/2019/03/06/fork-ts-checker-webpack-plugin-v1","metadata":{"permalink":"/2019/03/06/fork-ts-checker-webpack-plugin-v1","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2019-03-06-fork-ts-checker-webpack-plugin-v1/index.md","source":"@site/blog/2019-03-06-fork-ts-checker-webpack-plugin-v1/index.md","title":"fork-ts-checker-webpack-plugin v1.0","description":"It\'s time for the first major version of fork-ts-checker-webpack-plugin. It\'s been a long time coming :-)","date":"2019-03-06T00:00:00.000Z","formattedDate":"March 6, 2019","tags":[{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"fork-ts-checker-webpack-plugin","permalink":"/tags/fork-ts-checker-webpack-plugin"},{"label":"ts-loader","permalink":"/tags/ts-loader"},{"label":"tslint","permalink":"/tags/tslint"},{"label":"1.0.0","permalink":"/tags/1-0-0"},{"label":"Webpack","permalink":"/tags/webpack"}],"readingTime":1.89,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"fork-ts-checker-webpack-plugin v1.0","authors":"johnnyreilly","tags":["TypeScript","fork-ts-checker-webpack-plugin","ts-loader","tslint","1.0.0","Webpack"],"hide_table_of_contents":false},"prevItem":{"title":"Google Analytics API and ASP.Net Core","permalink":"/2019/03/22/google-analytics-api-and-aspnet-core"},"nextItem":{"title":"ASP.NET Core: Proxying HTTP Requests with an AllowList","permalink":"/2019/02/22/aspnet-core-allowlist-proxying-http-requests"}},"content":"[It\'s time for the first major version of `fork-ts-checker-webpack-plugin`](https://github.com/Realytics/fork-ts-checker-webpack-plugin/releases/tag/v1.0.0). It\'s been a long time coming :-)\\n\\n## A Little History\\n\\nThe `fork-ts-checker-webpack-plugin` was originally the handiwork of [Piotr Ole\u015b](https://github.com/piotr-oles). He raised an issue with [`ts-loader`](https://github.com/TypeStrong/ts-loader/issues/537) suggesting it could be the McCartney to `ts-loader`\'s Lennon:\\n\\n> Hi everyone!\\n>\\n> I\'ve created webpack plugin: [fork-ts-checker-webpack-plugin](https://github.com/Realytics/fork-ts-checker-webpack-plugin) that plays nicely with `ts-loader`. The idea is to compile project with `transpileOnly: true` and check types on separate process (async). With this approach, webpack build is not blocked by type checker and we have semantic check with fast incremental build. More info on github repo :)\\n>\\n> So if you like it and you think it would be good to add some info in README/index.md about this plugin, I would be greatful.\\n>\\n> Thanks :)\\n\\nWe did like it. We did think it would be good. We took him up on his kind offer.\\n\\nSince that time many people have had their paws on the `fork-ts-checker-webpack-plugin` codebase. We love them all.\\n\\n## One Point Oh\\n\\nWe could have had our first major release a long time ago. The idea first occurred when webpack 5 alpha appeared. \\"Huh, look at that, a major version number.... Maybe we should do that?\\" \\"_Great_ idea chap - do it!\\" So here it is; fresh out the box: v1.0.0\\n\\nThere are actually no breaking changes that we\'re aware of; users of 0.x `fork-ts-checker-webpack-plugin` should be be able to upgrade without any drama.\\n\\n## Incremental Watch API on by Default\\n\\nUsers of TypeScript 3+ may notice a performance improvement as by default the plugin now uses the [incremental watch API](https://github.com/Microsoft/TypeScript/pull/20234) in TypeScript.\\n\\nShould this prove problematic you can opt out of using it by supplying `useTypescriptIncrementalApi: false`. We are aware of an [issue with Vue and the incremental API](https://github.com/Realytics/fork-ts-checker-webpack-plugin/issues/219). We hope it will be fixed soon - a generous member of the community is taking a look. In the meantime, we will _not_ default to using the incremental watch API when in Vue mode.\\n\\n## Compatibility\\n\\nAs it stands, the plugin supports webpack 2, 3, 4 and 5 alpha. It is compatible with TypeScript 2.1+ and TSLint 4+.\\n\\nRight that\'s it - enjoy it! And thanks everyone for contributing - we really dig your help. Much love."},{"id":"/2019/02/22/aspnet-core-allowlist-proxying-http-requests","metadata":{"permalink":"/2019/02/22/aspnet-core-allowlist-proxying-http-requests","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2019-02-22-aspnet-core-allowlist-proxying-http-requests/index.md","source":"@site/blog/2019-02-22-aspnet-core-allowlist-proxying-http-requests/index.md","title":"ASP.NET Core: Proxying HTTP Requests with an AllowList","description":"This post demonstrates a mechanism for proxying HTTP requests in ASP.NET Core. It doesn\'t proxy all requests; it only proxies requests that match entries on an \\"allowlist\\" - so we only proxy the traffic that we\'ve actively decided is acceptable as determined by taking the form of an expected URL and HTTP verb (GET / POST etc).","date":"2019-02-22T00:00:00.000Z","formattedDate":"February 22, 2019","tags":[{"label":"asp net core","permalink":"/tags/asp-net-core"},{"label":"proxy","permalink":"/tags/proxy"},{"label":"http requests","permalink":"/tags/http-requests"},{"label":"allowlist","permalink":"/tags/allowlist"}],"readingTime":6.485,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"ASP.NET Core: Proxying HTTP Requests with an AllowList","authors":"johnnyreilly","tags":["asp net core","proxy","http requests","allowlist"],"image":"./hang-on-lads-ive-got-a-great-idea.jpg","hide_table_of_contents":false},"prevItem":{"title":"fork-ts-checker-webpack-plugin v1.0","permalink":"/2019/03/06/fork-ts-checker-webpack-plugin-v1"},"nextItem":{"title":"TypeScript and webpack: Watch It","permalink":"/2019/01/13/typescript-and-webpack-watch-it"}},"content":"This post demonstrates a mechanism for proxying HTTP requests in ASP.NET Core. It doesn\'t proxy all requests; it only proxies requests that match entries on an \\"allowlist\\" - so we only proxy the traffic that we\'ve actively decided is acceptable as determined by taking the form of an expected URL and HTTP verb (GET / POST etc).\\n\\n## Why do we need to proxy?\\n\\nOnce upon a time there lived a young team who were building a product. They were ready to go live with their beta and so they set off on a journey to a mystical land they had heard tales of. This magical kingdom was called \\"Production\\". However, Production was a land with walls and but one gate. That gate was jealously guarded by a defender named \\"InfoSec\\". InfoSec was there to make sure that only the the right people, noble of thought and pure of deed were allowed into the promised land. InfoSec would ask questions like \\"are you serving over HTTPS\\" and \\"what are you doing about cross site scripting\\"?\\n\\nThe team felt they had good answers to InfoSec\'s questions. However, just as they were about to step through the gate, InfoSec held up their hand and said \\"your application wants to access a database... database access needs to take place on our own internal network. Not over the publicly accessible internet.\\"\\n\\nThe team, with one foot in the air, paused. They swallowed and said \\"can you give us five minutes?\\"\\n\\n![image taken from the end of the classic movie \\"The Italian Job\\" of the bus hanging half off a mountainside](hang-on-lads-ive-got-a-great-idea.jpg)\\n\\n## The Proxy Regroup\\n\\nAnd so it came to pass that the teams product (which took the form of ASP.Net Core web application) had to be changed. Where once there had been a single application, there would now be two; one that lived on the internet (the _web_ app) and one that lived on the companies private network (the _API_ app). The API app would do all the database access. In fact the product team opted to move all significant operations into the API as well. This left the web app with two purposes:\\n\\n1. the straightforward serving of HTML, CSS, JS and images\\n2. the proxying of API calls through to the API app\\n\\n## Proxy Part 1\\n\\nIn the early days of this proxying the team reached for [`AspNetCore.Proxy`](https://github.com/twitchax/AspNetCore.Proxy). It\'s a great open source project that allows you to proxy HTTP requests. It gives you complete control over the construction of proxy requests, so that you can have a request come into your API and end up proxying it to a URL with a completely different path on the proxy server.\\n\\n## Proxy Part 2\\n\\nThe approach offered by `AspNetCore.Proxy` is fantastically powerful in terms of control. However, we didn\'t actually need that level of configurability. In fact, it resulted in us writing a great deal of boilerplate code. You see in our case we\'d opted to proxy path for path, changing only the server name on each proxied request. So if a GET request came in going to https://web.app.com/api/version then we would want to proxy it to a GET request to https://api.app.com/api/version. You see? All we did was swap https://web.app.com for https://api.app.com. Nothing more. We did that as a rule. We knew we _always_ wanted to do just this.\\n\\nSo we ended up spinning up our own solution which allowed just the specification of paths we wanted to proxy with their corresponding HTTP verbs. Let\'s talk through it. Usage of our approach ended up as a middleware within our web app\'s `Startup.cs`:\\n\\n```cs\\npublic void Configure(IApplicationBuilder app) {\\n    // ...\\n\\n    app.UseProxyAllowList(\\n        // where ServerToProxyToBaseUrl is the server you want requests to be proxied to\\n        // eg \\"https://the-server-we-proxy-to\\"\\n        proxyAddressTweaker: (requestPath) => $\\"{ServerToProxyToBaseUrl}{requestPath}\\",\\n        allowListProxyRoutes: new [] {\\n            // An anonymous request\\n            AllowListProxy.AnonymousRoute(\\"api/version\\", HttpMethod.Get),\\n\\n            // An authenticated request; to send this we must know who the user is\\n            AllowListProxy.Route(\\"api/account/{accountId:int}/all-the-secret-info\\", HttpMethod.Get, HttpMethod.Post),\\n    });\\n\\n\\n    app.UseMvc();\\n\\n    // ...\\n}\\n```\\n\\nIf you look at the code above you can see that we are proxing requests to a single server: `ServerToProxyToBaseUrl`. We\'re also only proxying requests which match an entry on our allowlist (as represented by `allowListProxyRoutes`). So in this case we\'re proxying two different requests:\\n\\n1. `GET` requests to `api/version` are proxied through as _anonymous_`GET` requests.\\n2. `GET` and `POST` requests to `api/account/{accountId:int}/all-the-secret-info` are proxied through as `GET` and `POST` requests. These requests require that a user be authenticated first.\\n\\nThe `AllowListProxy` proxy class we\'ve been using looks like this:\\n\\n```cs\\nusing System;\\nusing System.Collections.Generic;\\nusing System.Net.Http;\\n\\nnamespace My.Web.Proxy {\\n    public class AllowListProxy {\\n        public string Path { get; set; }\\n        public IEnumerable<HttpMethod> Methods { get; set; }\\n        public bool IsAnonymous { get; set; }\\n\\n        private AllowListProxy(string path, bool isAnonymous, params HttpMethod[] methods) {\\n            if (methods == null || methods.Length == 0)\\n                throw new ArgumentException($\\"You need at least a single HttpMethod to be specified for {path}\\");\\n\\n            Path = path;\\n            IsAnonymous = isAnonymous;\\n            Methods = methods;\\n        }\\n\\n        public static AllowListProxy Route(string path, params HttpMethod[] methods) =>\\n            new AllowListProxy(path, isAnonymous: false, methods: methods);\\n\\n        public static AllowListProxy AnonymousRoute(string path, params HttpMethod[] methods) =>\\n            new AllowListProxy(path, isAnonymous: true, methods: methods);\\n    }\\n}\\n```\\n\\nThe middleware for proxying (our `UseProxyAllowList`) looks like this:\\n\\n```cs\\nusing System;\\nusing System.Collections.Generic;\\nusing System.ComponentModel;\\nusing System.Linq;\\nusing System.Net.Http;\\nusing System.Reflection;\\nusing System.Threading.Tasks;\\nusing Microsoft.AspNetCore.Authentication;\\nusing Microsoft.AspNetCore.Builder;\\nusing Microsoft.AspNetCore.Http;\\nusing Microsoft.AspNetCore.Routing;\\nusing Microsoft.Extensions.DependencyModel;\\nusing Microsoft.Extensions.DependencyInjection;\\nusing Serilog;\\n\\nnamespace My.Web.Proxy {\\n    public static class ProxyRouteExtensions {\\n        /// <summary>\\n        /// Middleware which proxies the supplied allowlist routes\\n        /// </summary>\\n        public static void UseProxyAllowList(\\n            this IApplicationBuilder app,\\n            Func<string, string> proxyAddressTweaker,\\n            Action<HttpContext, HttpRequestMessage> preSendProxyRequestAction,\\n            IEnumerable<AllowListProxy> allowListProxyRoutes\\n        ) {\\n            app.UseRouter(builder => {\\n                foreach (var allowListProxy in allowListProxyRoutes) {\\n                    foreach (var method in allowListProxy.Methods) {\\n                        builder.MapMiddlewareVerb(method.ToString(), allowListProxy.Path, proxyApp => {\\n                            proxyApp.UseProxy_Challenge(allowListProxy.IsAnonymous);\\n                            proxyApp.UseProxy_Run(proxyAddressTweaker, preSendProxyRequestAction);\\n                        });\\n                    }\\n                }\\n            });\\n        }\\n\\n        private static void UseProxy_Challenge(this IApplicationBuilder app, bool allowAnonymous) {\\n            app.Use((context, next) =>\\n            {\\n                var routePath = context.Request.Path.Value;\\n\\n                var weAreAuthenticatedOrWeDontNeedToBe =\\n                    context.User.Identity.IsAuthenticated || allowAnonymous;\\n                if (weAreAuthenticatedOrWeDontNeedToBe)\\n                    return next();\\n\\n                return context.ChallengeAsync();\\n            });\\n        }\\n\\n        private static void UseProxy_Run(\\n            this IApplicationBuilder app,\\n            Func<string, string> proxyAddressTweaker,\\n            Action<HttpContext, HttpRequestMessage> preSendProxyRequestAction\\n            )\\n        {\\n            app.Run(async context => {\\n                var proxyAddress = \\"\\";\\n                try {\\n                    proxyAddress = proxyAddressTweaker(context.Request.Path.Value);\\n\\n                    var proxyRequest = context.Request.CreateProxyHttpRequest(proxyAddress);\\n\\n                    if (preSendProxyRequestAction != null)\\n                        preSendProxyRequestAction(context, proxyRequest);\\n\\n                    var httpClients = context.RequestServices.GetService<IHttpClients>(); // IHttpClients is just a wrapper for HttpClient - insert your own here\\n\\n                    var proxyResponse = await httpClients.SendRequestAsync(proxyRequest,\\n                            HttpCompletionOption.ResponseHeadersRead, context.RequestAborted)\\n                        .ConfigureAwait(false);\\n\\n                    await context.CopyProxyHttpResponse(proxyResponse).ConfigureAwait(false);\\n                }\\n                catch (OperationCanceledException ex) {\\n                    if (ex.CancellationToken.IsCancellationRequested)\\n                        return;\\n\\n                    if (!context.Response.HasStarted)\\n                    {\\n                        context.Response.StatusCode = 408;\\n                        await context.Response\\n                            .WriteAsync(\\"Request timed out.\\");\\n                    }\\n                }\\n                catch (Exception e) {\\n                    if (!context.Response.HasStarted)\\n                    {\\n                        context.Response.StatusCode = 500;\\n                        await context.Response\\n                            .WriteAsync(\\n                                $\\"Request could not be proxied.\\\\n\\\\n{e.Message}\\\\n\\\\n{e.StackTrace}.\\");\\n                    }\\n                }\\n            });\\n        }\\n\\n        public static void AddOrReplaceHeader(this HttpRequestMessage request, string headerName, string headerValue) {\\n            // It\'s possible for there to be multiple headers with the same name; we only want a single header to remain.  Our one.\\n            while (request.Headers.TryGetValues(headerName, out var existingAuthorizationHeader)) {\\n                request.Headers.Remove(headerName);\\n            }\\n            request.Headers.TryAddWithoutValidation(headerName, headerValue);\\n        }\\n\\n        public static HttpRequestMessage CreateProxyHttpRequest(this HttpRequest request, string uriString) {\\n            var uri = new Uri(uriString + request.QueryString);\\n\\n            var requestMessage = new HttpRequestMessage();\\n            var requestMethod = request.Method;\\n            if (!HttpMethods.IsGet(requestMethod) &&\\n                !HttpMethods.IsHead(requestMethod) &&\\n                !HttpMethods.IsDelete(requestMethod) &&\\n                !HttpMethods.IsTrace(requestMethod)) {\\n                var streamContent = new StreamContent(request.Body);\\n                requestMessage.Content = streamContent;\\n            }\\n\\n            // Copy the request headers.\\n            if (requestMessage.Content != null)\\n                foreach (var header in request.Headers)\\n                    if (!requestMessage.Headers.TryAddWithoutValidation(header.Key, header.Value.ToArray()))\\n                        requestMessage.Content?.Headers.TryAddWithoutValidation(header.Key, header.Value.ToArray());\\n\\n            requestMessage.Headers.Host = uri.Authority;\\n            requestMessage.RequestUri = uri;\\n            requestMessage.Method = new HttpMethod(request.Method);\\n\\n            return requestMessage;\\n        }\\n\\n        public static async Task CopyProxyHttpResponse(this HttpContext context, HttpResponseMessage responseMessage) {\\n            var response = context.Response;\\n\\n            response.StatusCode = (int) responseMessage.StatusCode;\\n            foreach (var header in responseMessage.Headers) {\\n                response.Headers[header.Key] = header.Value.ToArray();\\n            }\\n\\n            if (responseMessage.Content != null) {\\n                foreach (var header in responseMessage.Content.Headers) {\\n                    response.Headers[header.Key] = header.Value.ToArray();\\n                }\\n            }\\n\\n            response.Headers.Remove(\\"transfer-encoding\\");\\n\\n            using(var responseStream = await responseMessage.Content.ReadAsStreamAsync().ConfigureAwait(false)) {\\n                await responseStream.CopyToAsync(response.Body, 81920, context.RequestAborted).ConfigureAwait(false);\\n            }\\n        }\\n    }\\n}\\n```\\n\\nThis works out to be a flexible and simple approach to allowlist proxying."},{"id":"/2019/01/13/typescript-and-webpack-watch-it","metadata":{"permalink":"/2019/01/13/typescript-and-webpack-watch-it","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2019-01-13-typescript-and-webpack-watch-it/index.md","source":"@site/blog/2019-01-13-typescript-and-webpack-watch-it/index.md","title":"TypeScript and webpack: Watch It","description":"All I ask for is a compiler and a tight feedback loop. Narrowing the gap between making a change to a program and seeing the effect of that is a productivity boon. The TypeScript team are wise cats and dig this. They\'ve taken strides to improve the developer experience of TypeScript users by introducing a \\"watch\\" API which can be leveraged by other tools. To quote the docs:","date":"2019-01-13T00:00:00.000Z","formattedDate":"January 13, 2019","tags":[{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"watch API","permalink":"/tags/watch-api"},{"label":"Webpack","permalink":"/tags/webpack"}],"readingTime":2.37,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"TypeScript and webpack: Watch It","authors":"johnnyreilly","tags":["TypeScript","watch API","Webpack"],"hide_table_of_contents":false},"prevItem":{"title":"ASP.NET Core: Proxying HTTP Requests with an AllowList","permalink":"/2019/02/22/aspnet-core-allowlist-proxying-http-requests"},"nextItem":{"title":"GitHub Actions and Yarn","permalink":"/2019/01/05/github-actions-and-yarn"}},"content":"All I ask for is a compiler and a tight feedback loop. Narrowing the gap between making a change to a program and seeing the effect of that is a productivity boon. The TypeScript team are wise cats and dig this. They\'ve taken strides to improve the developer experience of TypeScript users by [introducing a \\"watch\\" API which can be leveraged by other tools](https://github.com/Microsoft/TypeScript/wiki/Using-the-Compiler-API#writing-an-incremental-program-watcher). To quote the docs:\\n\\n> TypeScript 2.7 introduces two new APIs: one for creating \\"watcher\\" programs that provide set of APIs to trigger rebuilds, and a \\"builder\\" API that watchers can take advantage of... This can speed up large projects with many files.\\n\\nRecently the wonderful [0xorial](https://github.com/0xorial) [opened a PR to add support for the watch API](https://github.com/Realytics/fork-ts-checker-webpack-plugin/pull/198) to the [`fork-ts-checker-webpack-plugin`](https://github.com/Realytics/fork-ts-checker-webpack-plugin).\\n\\nI took this PR for a spin on a large project that I work on. With my machine, I was averaging 12 seconds between incremental builds. (I will charitably describe the machine in question as \\"challenged\\"; hobbled by one of the most aggressive virus checkers known to mankind. Fist bump InfoSec \ud83e\udd1c\ud83e\udd1b\ud83d\ude09) Switching to using the watch API dropped this to a mere 1.5 seconds!\\n\\n## You Can Watch Too\\n\\n0xorial\'s PR was merged toot suite and was been released as [`fork-ts-checker-webpack-plugin@1.0.0-alpha.2`](https://github.com/Realytics/fork-ts-checker-webpack-plugin/releases/tag/v1.0.0-alpha.2). If you\'d like to take this for a spin then you can. Just:\\n\\n1. Up your version of the plugin to `fork-ts-checker-webpack-plugin@next` in your `package.json`\\n2. Add `useTypescriptIncrementalApi: true` to the plugin when you initialise it in your `webpack.config.js`.\\n\\nThat\'s it.\\n\\n## Mary Poppins\\n\\nSorry, I was trying to paint a word picture of something you might watch that was also comforting. Didn\'t quite work...\\n\\nAnyway, you might be thinking \\"wait, just hold on a minute.... he said `@next` \\\\- I am _not_ that bleeding edge.\\" Well, it\'s not like that. Don\'t be scared.\\n\\n`fork-ts-checker-webpack-plugin` has merely been updated for webpack 5 (which is in alpha) and the `@next` reflects that. To be clear, the `@next` version of the plugin still supports (remarkably!) webpack 2, 3 and 4 as well as 5 alpha. Users of current and historic versions of webpack should feel safe using the `@next` version; for webpack 2, 3 and 4 expect stability. webpack 5 users should expect potential changes to align with webpack 5 as it progresses.\\n\\n## Roadmap\\n\\nThis is available now and we\'d love for you to try it out. As you can see, at the moment it\'s opt-in. You have to explicitly choose to use the new behaviour. Depending upon how testing goes, we may look to make this the default behaviour for the plugin in future (assuming users are running a high enough version of TypeScript). It would be great to hear from people if they have any views on that, or feedback in general.\\n\\nMuch \u2764\ufe0f y\'all. And many thanks to the very excellent [0xorial](https://github.com/0xorial) for the hard work."},{"id":"/2019/01/05/github-actions-and-yarn","metadata":{"permalink":"/2019/01/05/github-actions-and-yarn","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2019-01-05-github-actions-and-yarn/index.md","source":"@site/blog/2019-01-05-github-actions-and-yarn/index.md","title":"GitHub Actions and Yarn","description":"I\'d been meaning to automate the npm publishing of ts-loader for the longest time. I had attempted to use Travis to do this in the same way as fork-ts-checker-webpack-plugin. Alas using secure environment variables in Travis has unfortunate implications for ts-loader\'s test pack.","date":"2019-01-05T00:00:00.000Z","formattedDate":"January 5, 2019","tags":[{"label":"docker","permalink":"/tags/docker"},{"label":"yarn","permalink":"/tags/yarn"},{"label":"GitHub Actions","permalink":"/tags/git-hub-actions"}],"readingTime":3.995,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"GitHub Actions and Yarn","authors":"johnnyreilly","tags":["docker","yarn","GitHub Actions"],"hide_table_of_contents":false},"prevItem":{"title":"TypeScript and webpack: Watch It","permalink":"/2019/01/13/typescript-and-webpack-watch-it"},"nextItem":{"title":"You Might Not Need thread-loader","permalink":"/2018/12/22/you-might-not-need-thread-loader"}},"content":"I\'d been meaning to automate the npm publishing of [`ts-loader`](https://github.com/TypeStrong/ts-loader) for the longest time. I had attempted to use Travis to do this in the same way as [`fork-ts-checker-webpack-plugin`](https://github.com/Realytics/fork-ts-checker-webpack-plugin). Alas using secure environment variables in Travis has unfortunate implications for ts-loader\'s test pack.\\n\\nBe not afeard. I\'ve heard there\'s a new shiny thing from GitHub that I could use instead... It\'s a sign; I must use it!\\n\\nGitHub Actions are still in beta. Technically Actions are [code run in Docker containers](https://developer.github.com/actions/creating-github-actions/) in response to events. This didn\'t mean a great deal to me until I started thinking about what I wanted to do with `ts-loader`\'s publishing flow.\\n\\n## Automate What?\\n\\nEach time I publish a release of `ts-loader` I execute the following node commands by hand:\\n\\n1. `yarn install` \\\\- to install `ts-loader`\'s dependencies\\n2. `yarn build` \\\\- to build `ts-loader`\\n3. `yarn test` \\\\- to run `ts-loader`\'s test packs\\n4. `npm publish` \\\\- to publish the release of `ts-loader` to npm\\n\\nHaving read up on GitHub Actions it seemed like they were born to handle this sort of task.\\n\\n## GitHub Action for `npm`\\n\\nI quickly discovered that someone out there <s>loves me</s>\\n\\nhad [already written a GitHub Action for `npm`](https://github.com/actions/npm).\\n\\nThe example in the `README/index.md` could be easily tweaked to meet my needs with one caveat: I had to use `npm` in place of `yarn`. I didn\'t want to switch from `yarn`. What to do?\\n\\nWell, remember when I said actions are code run in Docker containers? Another way to phrase that is to say: GitHub Actions are Docker images. Let\'s look under the covers of the `npm` GitHub Action. As we peer inside the [`Dockerfile`](https://github.com/actions/npm/blob/e7aaefed7c9f2e83d493ff810f17fa5ccd7ed437/Dockerfile#L1) what do we find?\\n\\n```\\nFROM node:10-slim\\n```\\n\\nHmmmm.... Interesting. The base image of the `npm` GitHub Action is `node:10-slim`. Looking it up, it seems the `-slim` Docker images come with [`yarn` included](https://github.com/nodejs/docker-node/blob/master/Dockerfile-slim.template). Which means we should be able to use `yarn` inside the `npm` GitHub Action. Nice!\\n\\n## GitHub Action for `npm` for `yarn`\\n\\nUsing `yarn` from the GitHub Action for `npm` is delightfully simple. Here\'s what running `npm install` looks like:\\n\\n```\\n# install with npm\\naction \\"install\\" {\\n  uses = \\"actions/npm@1.0.0\\"\\n  args = \\"install\\"\\n}\\n```\\n\\nPivoting to use `yarn install` instead of `npm install` is as simple as:\\n\\n```\\n# install with yarn\\naction \\"install\\" {\\n  uses = \\"actions/npm@1.0.0\\"\\n  runs = \\"yarn\\"\\n  args = \\"install\\"\\n}\\n```\\n\\nYou can see we\'ve introduced the `runs = \\"yarn\\"` and after that the `args` are whatever you need them to be.\\n\\n## Going With The Workflow\\n\\nA GitHub Workflow that implements the steps I need would look like this:\\n\\n```\\nworkflow \\"build, test and publish on release\\" {\\n  on = \\"push\\"\\n  resolves = \\"publish\\"\\n}\\n\\n# install with yarn\\naction \\"install\\" {\\n  uses = \\"actions/npm@1.0.0\\"\\n  runs = \\"yarn\\"\\n  args = \\"install\\"\\n}\\n\\n# build with yarn\\naction \\"build\\" {\\n  needs = \\"install\\"\\n  uses = \\"actions/npm@1.0.0\\"\\n  runs = \\"yarn\\"\\n  args = \\"build\\"\\n}\\n\\n# test with yarn\\naction \\"test\\" {\\n  needs = \\"build\\"\\n  uses = \\"actions/npm@1.0.0\\"\\n  runs = \\"yarn\\"\\n  args = \\"test\\"\\n}\\n\\n# filter for a new tag\\naction \\"check for new tag\\" {\\n  needs = \\"Test\\"\\n  uses = \\"actions/bin/filter@master\\"\\n  args = \\"tag\\"\\n}\\n\\n# publish with npm\\naction \\"publish\\" {\\n  needs = \\"check for new tag\\"\\n  uses = \\"actions/npm@1.0.0\\"\\n  args = \\"publish\\"\\n  secrets = [\\"NPM_AUTH_TOKEN\\"]\\n}\\n```\\n\\nAs you can see, this is a direct automation of steps 1-4 I listed earlier. Since all these actions are executed in the same container, we can skip from `yarn` to `npm` with gay abandon.\\n\\nWhat\'s absolutely amazing is, when I got access to GitHub Actions [my hand crafted workflow](https://github.com/TypeStrong/ts-loader/blob/master/.github/main.workflow) looked like it should work first time! I know, right? Don\'t you love it when that happens? [Alas there\'s presently a problem with filters in GitHub Actions](https://github.com/actions/bin/issues/13). But that\'s by the by, if you\'re just looking to use a GitHub Action with yarn instead of npm then you are home free.\\n\\n## You Don\'t Actually Need the npm GitHub Action\\n\\nYou heard me right. Docker containers be Docker containers. You don\'t actually need to use this:\\n\\n```\\nuses = \\"actions/npm@1.0.0\\"\\n```\\n\\nYou can use _any_ Docker container which has node / npm installed! So if you\'d like to use say node 11 instead you could just do this:\\n\\n```\\nuses = \\"docker://node:11\\"\\n```\\n\\nWhich would use the node 11 image on [docker hub](https://hub.docker.com/_/node).\\n\\nWhich is pretty cool. You know what\'s even more incredible? Inside a workflow you can switch `uses` mid-workflow and keep the output. That\'s right; you can have a work flow with say three actions running `uses = \\"docker://node:11\\"` and then a fourth running `uses = \\"actions/npm@1.0.0\\"`. That\'s _so_ flexible and powerful!\\n\\nThanks to [Matt Colyer](https://github.com/mcolyer) and [Landon Schropp](https://github.com/LandonSchropp) for [schooling me on the intricicies of GitHub Actions](https://github.com/actions/npm/issues/9). Much \u2764"},{"id":"/2018/12/22/you-might-not-need-thread-loader","metadata":{"permalink":"/2018/12/22/you-might-not-need-thread-loader","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2018-12-22-you-might-not-need-thread-loader/index.md","source":"@site/blog/2018-12-22-you-might-not-need-thread-loader/index.md","title":"You Might Not Need thread-loader","description":"It all started with a GitHub issue. Ernst Ammann reported:","date":"2018-12-22T00:00:00.000Z","formattedDate":"December 22, 2018","tags":[{"label":"HappyPack","permalink":"/tags/happy-pack"},{"label":"thread-loader","permalink":"/tags/thread-loader"},{"label":"fork-ts-checker-webpack-plugin","permalink":"/tags/fork-ts-checker-webpack-plugin"},{"label":"ts-loader","permalink":"/tags/ts-loader"},{"label":"Webpack","permalink":"/tags/webpack"},{"label":"fast builds","permalink":"/tags/fast-builds"}],"readingTime":3.675,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"You Might Not Need thread-loader","authors":"johnnyreilly","tags":["HappyPack","thread-loader","fork-ts-checker-webpack-plugin","ts-loader","Webpack","fast builds"],"hide_table_of_contents":false},"prevItem":{"title":"GitHub Actions and Yarn","permalink":"/2019/01/05/github-actions-and-yarn"},"nextItem":{"title":"Cache Rules Everything Around Me","permalink":"/2018/12/10/cache-rules-everything-around-me"}},"content":"It all started with a GitHub issue. [Ernst Ammann reported](https://github.com/namics/webpack-config-plugins/issues/24):\\n\\n> Without the thread-loader, compilation takes three to four times less time on changes. We could remove it.\\n\\nIf you\'re not aware of the [`webpack-config-plugins`](https://github.com/namics/webpack-config-plugins) project then I commend it to you. Famously, webpack configuration can prove tricky. `webpack-config-plugins` borrows the idea of presets from Babel. It provides a number of pluggable webpack configurations which give a best practice setup for different webpack use cases. So if you\'re no expert with webpack and you want a good setup for building your TypeScript / Sass / JavaScript then `webpack-config-plugins` has got your back.\\n\\nOne of the people behind the project is the very excellent [Jan Nicklas](https://github.com/jantimon) who is well known for his work on the [`html-webpack-plugin`](https://github.com/jantimon/html-webpack-plugin).\\n\\nIt was Jan who responded to Ernst\'s issue and decided to look into it.\\n\\n## All I Want For Christmas is Faster Builds\\n\\nEveryone wants fast builds. I do. You do. We all do. `webpack-config-plugins` is about giving these to the user in a precooked package.\\n\\nThere\'s a webpack loader called [`thread-loader`](https://github.com/webpack-contrib/thread-loader) which spawns multiple processes and splits up work between them. It was originally inspired by the work in the happypack project which does a similar thing.\\n\\nI wrote [a blog post](https://medium.com/p/83cc568dea79) some time ago which gave details about ways to speed up your TypeScript builds by combining the [`ts-loader`](https://github.com/TypeStrong/ts-loader) project (which I manage) with the [`fork-ts-checker-webpack-plugin`](https://github.com/Realytics/fork-ts-checker-webpack-plugin) project (which I\'m heavily involved with).\\n\\nThat post was written back in the days of webpack 2 / 3. It advocated use of both `happypack` / `thread-loader` to drop your build times even further. As you\'ll see, now that we\'re well into the world of webpack 4 (with webpack 5 waiting in the wings) the advantage of `happypack` / `thread-loader` are no longer so profound.\\n\\n`webpack-config-plugins` follows the advice I set out in my post; it uses `thread-loader` in its pluggable configurations. Now, back to Ernst\'s issue.\\n\\n## `thread-loader`: Infinity War\\n\\nJan quickly identified the problem. He did that rarest of things; he read the documentation which said:\\n\\n```js\\n// timeout for killing the worker processes when idle\\n      // defaults to 500 (ms)\\n      // can be set to Infinity for watching builds to keep workers alive\\n      poolTimeout: 2000,\\n```\\n\\nThe `webpack-config-plugins` configurations (running in watch mode) were subject to the thread loaders being killed after 500ms. They got resurrected when they were next needed; but that\'s not as instant as you might hope. Jan then did a test:\\n\\n```sh\\n(default pool - 30 runs - 1000 components ) average: 2.668068965517241\\n(no thread-loader - 30 runs - 1000 components ) average: 1.2674137931034484\\n(Infinity pool - 30 runs - 1000 components ) average: 1.371827586206896\\n```\\n\\nThis demonstrates that using `thread-loader` in watch mode with `poolTimeout: Infinity` performs significantly better than when it defaults to 500ms. But perhaps more significantly, not using `thread-loader` performs even better still.\\n\\n## \\"Maybe You\'ve Thread Enough\\"\\n\\nWhen I tested using `thread-loader` in watch mode with `poolTimeout: Infinity` on my own builds I got the same benefit Jan had. I also got _even_ more benefit from dropping `thread-loader` entirely.\\n\\nA likely reason for this benefit is that typically when you\'re developing, you\'re working on one file at a time. Hence you only transpile one file at a time:\\n\\n![](ts-profile2.png)\\n\\nSo there\'s not a great deal of value that `thread-loader` can add here; mostly it\'s twiddling thumbs and adding an overhead. [To quote the docs:](https://github.com/webpack-contrib/thread-loader/blob/master/README/index.md#usage)\\n\\n> Each worker is a separate node.js process, which has an overhead of \\\\~600ms. There is also an overhead of inter-process communication.\\n>\\n> Use this loader only for expensive operations!\\n\\nNow, my build is not your build. I can\'t guarantee that you\'ll get the same results as Jan and I experienced; but I would encourage you to investigate if you\'re using `thread-loader` correctly and whether it\'s actually helping you. In these days of webpack 4+ perhaps it isn\'t.\\n\\nThere are still scenarios where `thread-loader` still provides an advantage. It can speed up production builds. It can speed up the initial startup of watch mode. [In fact Jan has subsequently actually improved the `thread-loader` to that specific end.](https://github.com/webpack-contrib/thread-loader/pull/52) Yay Jan!\\n\\nIf this is all too much for you, and you want to hand off the concern to someone else then perhaps all of this serves as a motivation to just sit back, put your feet up and start using [`webpack-config-plugins`](https://github.com/namics/webpack-config-plugins) instead of doing your own configuration."},{"id":"/2018/12/10/cache-rules-everything-around-me","metadata":{"permalink":"/2018/12/10/cache-rules-everything-around-me","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2018-12-10-cache-rules-everything-around-me/index.md","source":"@site/blog/2018-12-10-cache-rules-everything-around-me/index.md","title":"Cache Rules Everything Around Me","description":"One thing that ASP.Net Core really got right was caching. IMemoryCache is a caching implementation that does just what I want. I love it. I take it everywhere. I\'ve introduced it to my family.","date":"2018-12-10T00:00:00.000Z","formattedDate":"December 10, 2018","tags":[{"label":"asp net core","permalink":"/tags/asp-net-core"},{"label":"cache","permalink":"/tags/cache"},{"label":"wu-tang","permalink":"/tags/wu-tang"}],"readingTime":1.57,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Cache Rules Everything Around Me","authors":"johnnyreilly","tags":["asp net core","cache","wu-tang"],"hide_table_of_contents":false},"prevItem":{"title":"You Might Not Need thread-loader","permalink":"/2018/12/22/you-might-not-need-thread-loader"},"nextItem":{"title":"Snapshot Testing for C#","permalink":"/2018/11/17/snapshot-testing-for-c"}},"content":"One thing that ASP.Net Core really got right was caching. [`IMemoryCache`](https://docs.microsoft.com/en-us/aspnet/core/performance/caching/memory) is a caching implementation that does just what I want. I love it. I take it everywhere. I\'ve introduced it to my family.\\n\\n## TimeSpan, TimeSpan Expiration Y\'all\\n\\nTo make usage of the `IMemoryCache` _even_ more lovely I\'ve written an extension method. I follow pretty much one cache strategy: `SetAbsoluteExpiration` and I just vary the expiration by an amount of time. This extension method implements that in a simple way; I call it `GetOrCreateForTimeSpanAsync` - catchy right? It looks like this:\\n\\n```cs\\nusing System;\\nusing System.Threading.Tasks;\\nusing Microsoft.Extensions.Caching.Memory;\\n\\nnamespace My.Helpers {\\n\\n    public static class CacheHelpers {\\n\\n        public static async Task<TItem> GetOrCreateForTimeSpanAsync<TItem>(\\n            this IMemoryCache cache,\\n            string key,\\n            Func<Task<TItem>> itemGetterAsync,\\n            TimeSpan timeToCache\\n        ) {\\n            if (!cache.TryGetValue(key, out object result)) {\\n                result = await itemGetterAsync();\\n                if (result == null)\\n                    return default(TItem);\\n\\n                var cacheEntryOptions = new MemoryCacheEntryOptions()\\n                    .SetAbsoluteExpiration(timeToCache);\\n\\n                cache.Set(key, result, cacheEntryOptions);\\n            }\\n\\n            return (TItem) result;\\n        }\\n    }\\n}\\n```\\n\\nUsage looks like this:\\n\\n```cs\\nprivate Task<SuperInterestingThing> GetSuperInterestingThingFromCache(Guid superInterestingThingId) =>\\n    _cache.GetOrCreateForTimeSpanAsync(\\n        key: $\\"{nameof(MyClass)}:GetSuperInterestingThing:{superInterestingThingId}\\",\\n        itemGetterAsync: () => GetSuperInterestingThing(superInterestingThingId),\\n        timeToCache: TimeSpan.FromMinutes(5)\\n    );\\n```\\n\\nThis helper allows the consumer to provide three things:\\n\\n- The `key` key for the item to be cached with\\n- A `itemGetterAsync` which is the method that is used to retrieve a new value if an item cannot be found in the cache\\n- A `timeToCache` which is the period of time that an item should be cached\\n\\nIf an item can\'t be looked up by the `itemGetterAsync` then _nothing_ will be cached and a the `default` value of the expected type will be returned. This is important because lookups can fail, and there\'s nothing worse than a lookup failing and you caching `null` as a result.\\n\\nGo on, ask me how I know.\\n\\nThis is a simple, clear and helpful API which makes interacting with `IMemoryCache` even more lovely than it was. Peep it y\'all."},{"id":"/2018/11/17/snapshot-testing-for-c","metadata":{"permalink":"/2018/11/17/snapshot-testing-for-c","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2018-11-17-snapshot-testing-for-c/index.md","source":"@site/blog/2018-11-17-snapshot-testing-for-c/index.md","title":"Snapshot Testing for C#","description":"If you\'re a user of Jest, you\'ve no doubt heard of and perhaps made use of snapshot testing.","date":"2018-11-17T00:00:00.000Z","formattedDate":"November 17, 2018","tags":[{"label":"snapshot testing","permalink":"/tags/snapshot-testing"},{"label":"c#","permalink":"/tags/c"},{"label":"jest","permalink":"/tags/jest"}],"readingTime":5.685,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Snapshot Testing for C#","authors":"johnnyreilly","tags":["snapshot testing","c#","jest"],"hide_table_of_contents":false},"prevItem":{"title":"Cache Rules Everything Around Me","permalink":"/2018/12/10/cache-rules-everything-around-me"},"nextItem":{"title":"Making a Programmer","permalink":"/2018/10/27/making-a-programmer"}},"content":"If you\'re a user of Jest, you\'ve no doubt heard of and perhaps made use of [snapshot testing](https://jestjs.io/docs/en/snapshot-testing).\\n\\nSnapshot testing is an awesome tool that is generally discussed in the context of JavaScript React UI testing. But snapshot testing has a wider application than that. Essentially it is profoundly useful where you have functions which produce a complex structured output. It could be a React UI, it could be a list of FX prices. The type of data is immaterial; it\'s the amount of it that\'s key.\\n\\nTypically there\'s a direct correlation between the size and complexity of the output of a method and the length of the tests that will be written for it. Let\'s say you\'re outputting a class that contains 20 properties. Congratulations! You get to write 20 assertions in one form or another for each test case. Or a single assertion whereby you supply the expected output by hand specifying each of the 20 properties. Either way, that\'s not going to be fun. And just imagine the time it would take to update multiple test cases if you wanted to change the behaviour of the method in question. Ouchy.\\n\\nTime is money kid. What you need is snapshot testing. Say goodbye to handcrafted assertions and hello to JSON serialised output checked into source control. Let\'s unpack that a little bit. The usefulness of snapshot testing that I want in C# is predominantly about removing the need to write and maintain multiple assertions. Instead you write tests that compare the output of a call to your method with JSON serialised output you\'ve generated on a previous occasion.\\n\\nThis approach takes less time to write, less time to maintain and the solid readability of JSON makes it more likely you\'ll pick up on bugs. It\'s so much easier to scan JSON than it is a list of assertions.\\n\\n## Putting the Snapshot into C#\\n\\nNow if you\'re writing tests in JavaScript or TypeScript then Jest already has your back with CLI snapshot generation and `shouldMatchSnapshot`. However getting to nearly the same place in C# is delightfully easy. What are we going to need?\\n\\nFirst up, a serializer which can take your big bad data structures and render them as JSON. Also we\'ll use it to rehydrate our data structure into an object ready for comparison. We\'re going to use [Json.NET](https://www.newtonsoft.com/json).\\n\\nNext up we need a way to compare our outputs with our rehydrated snapshots - we need a C# `shouldMatchSnapshot`. There\'s many choices out there, but for my money [Fluent Assertions](https://fluentassertions.com) is king of the hill.\\n\\nFinally we\'re going to need Snapshot, a little helper utility I put together:\\n\\n```cs\\nusing System;\\nusing System.IO;\\nusing Newtonsoft.Json;\\nusing Newtonsoft.Json.Serialization;\\n\\nnamespace Test.Utilities {\\n    public static class Snapshot {\\n        private static readonly JsonSerializer StubSerializer = new JsonSerializer {\\n            ContractResolver = new CamelCasePropertyNamesContractResolver(),\\n            NullValueHandling = NullValueHandling.Ignore\\n        };\\n\\n        private static JsonTextWriter MakeJsonTextWriter(TextWriter sw) => new JsonTextWriter(sw) {\\n            Formatting = Formatting.Indented,\\n            IndentChar = \' \',\\n            Indentation = 2\\n        };\\n\\n        /// <summary>\\n        /// Make yourself some JSON! Usage looks like this:\\n        /// Stubs.Make($\\"{System.AppDomain.CurrentDomain.BaseDirectory}..\\\\\\\\..\\\\\\\\..\\\\\\\\data.json\\", myData);\\n        /// </summary>\\n        public static void Make<T>(string stubPath, T data) {\\n            try {\\n                if (string.IsNullOrEmpty(stubPath))\\n                    throw new ArgumentNullException(nameof(stubPath));\\n                if (data == null)\\n                    throw new ArgumentNullException(nameof(data));\\n\\n                using(var sw = new StreamWriter(stubPath))\\n                using(var writer = MakeJsonTextWriter(sw)) {\\n                    StubSerializer.Serialize(writer, data);\\n                }\\n            } catch (Exception exc) {\\n                throw new Exception($\\"Failed to make {stubPath}\\", exc);\\n            }\\n        }\\n\\n        public static string Serialize<T>(T data) {\\n            using (var sw = new StringWriter())\\n            using(var writer = MakeJsonTextWriter(sw)) {\\n                StubSerializer.Serialize(writer, data);\\n                return sw.ToString();\\n            }\\n        }\\n\\n        public static string Load(string filename) {\\n            var content = new StreamReader(\\n                File.OpenRead(filename)\\n            ).ReadToEnd();\\n\\n            return content;\\n        }\\n    }\\n}\\n```\\n\\nLet\'s look at the methods: `Make` and `Load`. Make is what we\'re going to use to create our snapshots. Load is what we\'re going to use to, uh, load our snapshots.\\n\\nWhat does usage look like? Great question. Let\'s go through the process of writing a C# snapshot test.\\n\\n## Taking Snapshot for a Spin\\n\\nFirst of all, we\'re going to need a method to test that outputs a data structure which is more than just a scalar value. Let\'s use this:\\n\\n```cs\\npublic class Leopard {\\n    public string Name { get; set; }\\n    public int Spots { get; set; }\\n}\\n\\npublic class LeopardService {\\n    public Leopard[] GetTheLeopards() {\\n        return new Leopard[] {\\n            new Leopard { Spots = 42, Name = \\"Nimoy\\" },\\n            new Leopard { Spots = 900, Name = \\"Dotty\\" }\\n        };\\n    }\\n}\\n```\\n\\nYes - our trusty `LeopardService`. As you can see, the `GetTheLeopards` method returns an array of `Leopard`s. For now, let\'s write a test using `Snapshot`: (ours is an XUnit test; but `Snapshot` is agnostic of this)\\n\\n```cs\\n[Fact]\\npublic void GetTheLeopards_should_return_expected_Leopards() {\\n    // Arrange\\n    var leopardService = new LeopardService();\\n\\n    // Act\\n    var leopards = leopardService.GetTheLeopards();\\n\\n    // UNCOMMENT THE LINE BELOW *ONLY* WHEN YOU WANT TO GENERATE THE SNAPSHOT\\n    Snapshot.Make($\\"{System.AppDomain.CurrentDomain.BaseDirectory}..\\\\\\\\..\\\\\\\\..\\\\\\\\Snapshots\\\\\\\\leopardsSnapshot.json\\", leopards);\\n\\n    // Assert\\n    var snapshotLeopards = JsonConvert.DeserializeObject<leopard[]>(Snapshot.Load(\\"Snapshots/leopardsSnapshot.json\\"));\\n    snapshotLeopards.Should().BeEquivalentTo(leopards);\\n}\\n</leopard[]>\\n```\\n\\nBefore we run this for the first time we need to setup our testing project to be ready for snapshots. First of all we add a `Snapshot` folder to the test project. The we also add the following to the `.csproj`:\\n\\n```xml\\n<ItemGroup>\\n    <Content Include=\\"Snapshots\\\\**\\">\\n      <CopyToOutputDirectory>Always</CopyToOutputDirectory>\\n    </Content>\\n  </ItemGroup>\\n```\\n\\nThis includes the snapshots in the compile output for when tests are being run.\\n\\nNow let\'s run the test. It will generate a `leopardsSnapshot.json` file:\\n\\n```json\\n[\\n  {\\n    \\"name\\": \\"Nimoy\\",\\n    \\"spots\\": 42\\n  },\\n  {\\n    \\"name\\": \\"Dotty\\",\\n    \\"spots\\": 900\\n  }\\n]\\n```\\n\\nWith our snapshot in place, we comment out the `Snapshot.Make...` line and we have a passing test. Let\'s commit our code, push and go about our business.\\n\\n## Time Passes...\\n\\nSomeone decides that the implementation of `GetTheLeopards` needs to change. Defying expectations it seems that Dotty the leopard should now have 90 spots. I know... Business requirements, right?\\n\\nIf we make that change we\'d ideally expect our trusty test to fail. Let\'s see what happens:\\n\\n```\\n----- Test Execution Summary -----\\n\\nLeopard.Tests.Services.LeopardServiceTests.GetTheLeopards_should_return_expected_Leopards:\\n    Outcome: Failed\\n    Error Message:\\n    Expected item[1].Spots to be 90, but found 900.\\n```\\n\\nBoom! We are protected!\\n\\nSince this is a change we\'re completely happy with we want to update our `leopardsSnapshot.json` file. We could make our test pass by manually updating the JSON. That\'d be fine. But why work when you don\'t have to? Let\'s uncomment our `Snapshot.Make...` line and run the test the once.\\n\\n```json\\n[\\n  {\\n    \\"name\\": \\"Nimoy\\",\\n    \\"spots\\": 42\\n  },\\n  {\\n    \\"name\\": \\"Dotty\\",\\n    \\"spots\\": 90\\n  }\\n]\\n```\\n\\nThat\'s right, we have an updated snapshot! Minimal effort.\\n\\n## Next Steps\\n\\nThis is a basic approach to getting the goodness of snapshot testing in C#. It could be refined further. To my mind the uncommenting / commenting of code is not the most elegant way to approach this and so there\'s some work that could be done around this area.\\n\\nHappy snapshotting!"},{"id":"/2018/10/27/making-a-programmer","metadata":{"permalink":"/2018/10/27/making-a-programmer","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2018-10-27-making-a-programmer/index.md","source":"@site/blog/2018-10-27-making-a-programmer/index.md","title":"Making a Programmer","description":"I recently had the good fortune to help run a coding bootcamp. The idea was simple: there are many people around us who are interested in programming but don\'t know where to start. Let\'s take some folk who do and share the knowledge.","date":"2018-10-27T00:00:00.000Z","formattedDate":"October 27, 2018","tags":[{"label":"coding bootcamp","permalink":"/tags/coding-bootcamp"},{"label":"retrospective","permalink":"/tags/retrospective"},{"label":"learning","permalink":"/tags/learning"},{"label":"feedback","permalink":"/tags/feedback"}],"readingTime":6.015,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Making a Programmer","authors":"johnnyreilly","tags":["coding bootcamp","retrospective","learning","feedback"],"hide_table_of_contents":false},"prevItem":{"title":"Snapshot Testing for C#","permalink":"/2018/11/17/snapshot-testing-for-c"},"nextItem":{"title":"Brand New Fonting Awesomeness","permalink":"/2018/10/07/font-awesome-brand-icons-react"}},"content":"I recently had the good fortune to help run a coding bootcamp. The idea was simple: there are many people around us who are interested in programming but don\'t know where to start. Let\'s take some folk who do and share the knowledge.\\n\\nThe bootcamp went tremendously! (Well, I say that... Frankly I had a blast. \ud83d\ude00 )\\n\\nCoding padawans walked in at the start with laptops and questions, and six weeks later they left with the groundwork of development experience. We ran a session for an hour during lunchtime once a week. Between that, people would have the opportunity to learn online, do exercises and reach out to the facilitators and their fellow apprentices for help.\\n\\nWe\'d never done this before. We were student teachers; learning how to teach as we ran the course. So what did we do? Are you curious? Read on, Macduff!\\n\\n## Code Review\\n\\nIt\'s worth saying now that we started our course with a plan: the plan was that we would be ready to change the plan. Or to put it another way, we were ready to pivot as we went.\\n\\nWe (by which I mean myself and the other course organisers) are interested in feedback. Sitting back and saying \\"Hey! We did this thing.... What do you think about it?\\" Because sometimes your plans are great. Do more of that hotness! But also, not all your ideas pan out... Maybe bail on those guys. Finally, never forget: other folk have brain tickling notions too.... We\'re with Picasso on this: good artists copy; great artists steal.\\n\\nWe\'re heavily invested in feedback in both what we build and how we build it. So we were totally going to apply this to doing something we\'d never done before. So seized were we of this that we made feedback part of the session. For the last five minutes each week we\'d run a short retrospective. We\'d stick up happy, sad and \\"meh\\" emojis to the wall, hand out post-its and everyone got to stick up their thoughts.\\n\\n![](not-so-sure-about-this-feedback.jpg)\\n\\nFrom that we learned what was working, what wasn\'t and when we were very lucky there were suggestions too. We listened to all the feedback and the next week\'s session would be informed by what we\'d just learned.\\n\\n## Merging to Master\\n\\nSo, what did we end up with? What did our coding bootcamp look like?\\n\\nWell, to start each session we kicked off with an icebreaker. We very much wanted the sessions to be interactive experiences; we wanted them to feel playful and human. So an icebreaker was a good way to get things off on the right foot.\\n\\nThe IBs were connected with the subject at hand. For example: Human FizzBuzz. We took the classic interview question and applied it to wannabe coders. We explained the rules, and went round in a circle, each person was the next iteration of the loop. As each dev-in-training opened their mouths they had to say a number or \\"Fizz\\" or \\"Buzz\\" or \\"FizzBuzz\\". (It turns out this is harder than you think; and makes for a surprisingly entertaining parlour game. I intend to do this at my next dinner party.)\\n\\nAfter that we covered the rules of the game. (Yup, learning is a game and it\'s a good \'un.) Certainly the most important rule was this: <u>there are <strong>_no_</strong> stupid questions</u>\\n\\n. If people think there are, then they might be hesitant to ask. And any question benched is a learning opportunity lost. We don\'t want that.\\n\\n\\"Ask any question!\\" we said each week. Kudos to the people who have the courage to pipe up. We salute you! You\'re likely putting voice to a common area of misunderstanding.\\n\\nThen we\'d move onto the main content. The initial plan was to make use of the excellent [EdX Python course](https://www.edx.org/learn/python) Between each session our learners would do a module and then we\'d come together and talk around that topic somewhat. Whilst this was a good initial plan it did make the learning experience somewhat passive and less interactive than we\'d hoped.\\n\\nOne week we tried something different. It turns out that the amazing [JMac](https://twitter.com/foldr) has quite the skill for writing programming exercises. Small coding challenges that people can tackle independently. JMac put together a [repl.it](https://repl.it/) of exercises and encouraged the class to get stuck in. They did. So much so that at the end of the session it was hard to get everyone\'s attention to let them know the session was over. They were in the zone. When we did finally disrupt their flow, the feedback was pretty unanimous: we\'d hit paydirt.\\n\\n![](we-dug-this-feedback.jpg)\\n\\nConsequently, that was the format going onwards. JMac would come up with a number of exercises for the class. Wisely they were constructed so that they gently levelled up in terms of complexity as you went on. You\'d get the dopamine hit of satisfaction as you did the earliest challenges that would give you the confidence to tackle the more complex later problems. If peeps got stuck they could ask someone to advise them, a facilitator or a peer. Or they could google it.... Like any other dev.\\n\\nHaving the chance to talk with others when you\'re stuck is fantastic. You can talk through a problem. The act of doing that is a useful exercise. When you talk through a problem out loud you can unlock your understanding and often get to the point where you can tackle this yourself. This is [rubber duck debugging](https://en.wikipedia.org/wiki/Rubber_duck_debugging). Any dev does this in their everyday; it makes complete sense to have it as part of a coding bootcamp.\\n\\nWe learned that it was useful, very useful, to have repitition in the exercises. Repitition. Repitition. Repitition. As the exercises started each week they would typically begin by recapping and repeating the content covered the previous week. The best way to learn is to practice. It\'s not for nothing the Karate Kid had to \\"wax on, wax off\\".\\n\\nFinally, we did this together. The course wasn\'t run by one person; we had a gang! We had three facilitators who helped to run the sessions; JMac, Jonesy and myself. We also had the amazing [Janice](https://twitter.com/janicewarden) who handled the general organisation and logistics. And made us laugh. A lot. This was obviously great from a camaraderie and sharing the load perspective. It turns out that having that number of facilitators in the session meant that everyone who needed help could get it. It\'s worth noting that having more than a single facilitator is useful in terms of the dynamic it creates. You can bounce things off one another; you can use each other for examples and illustrations. You can crack each other up. Done well it reduces the instructor / learner divide and that breaking down of barriers is something worth seeking.\\n\\n## RTM\\n\\nWe\'ve run a bootcamp once now. Where we are is informed by the experience we\'ve just had. A different group of learners may well have resulted in a slightly different format; though I have a feeling not overly dissimilar. We feel pretty sure that what we\'ve got is pretty solid. That said, just as the attendees are learning about development, we\'re still learning about learning!"},{"id":"/2018/10/07/font-awesome-brand-icons-react","metadata":{"permalink":"/2018/10/07/font-awesome-brand-icons-react","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2018-10-07-font-awesome-brand-icons-react/index.md","source":"@site/blog/2018-10-07-font-awesome-brand-icons-react/index.md","title":"Brand New Fonting Awesomeness","description":"Love me some Font Awesome. Absolutely wonderful. However, I came a cropper when following the instructions on using the all new Font Awesome 5 with React. The instructions for standard icons work fine. But if you want to use brand icons then this does not help you out much. There\'s 2 problems:","date":"2018-10-07T00:00:00.000Z","formattedDate":"October 7, 2018","tags":[{"label":"brand icons","permalink":"/tags/brand-icons"},{"label":"React","permalink":"/tags/react"},{"label":"font awesome","permalink":"/tags/font-awesome"}],"readingTime":1.43,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Brand New Fonting Awesomeness","authors":"johnnyreilly","tags":["brand icons","React","font awesome"],"hide_table_of_contents":false},"prevItem":{"title":"Making a Programmer","permalink":"/2018/10/27/making-a-programmer"},"nextItem":{"title":"ts-loader Project References: First Blood","permalink":"/2018/09/23/ts-loader-project-references-first-blood"}},"content":"Love me some [Font Awesome](https://fontawesome.com). Absolutely wonderful. However, I came a cropper when following the instructions [on using the all new Font Awesome 5 with React](https://fontawesome.com/how-to-use/on-the-web/using-with/react). The instructions for standard icons work _fine_. But if you want to use brand icons then this does not help you out much. There\'s 2 problems:\\n\\n1. Font Awesome\'s brand icons are not part of [`@fortawesome/free-solid-svg-icons`](https://www.npmjs.com/package/@fortawesome/free-solid-svg-icons) package\\n2. The method of icon usage illustrated (i.e. with the `FontAwesomeIcon` component) doesn\'t work. It doesn\'t render owt.\\n\\n## Brand Me Up Buttercup\\n\\nYou want brands? Well you need the [`@fortawesome/free-brands-svg-icons`](https://www.npmjs.com/package/@fortawesome/free-brands-svg-icons). Obvs, right?\\n\\n```sh\\nyarn add @fortawesome/fontawesome-svg-core\\nyarn add @fortawesome/free-brands-svg-icons\\nyarn add @fortawesome/react-fontawesome\\n```\\n\\nNow usage:\\n\\n```jsx\\nimport * as React from \'react\';\\nimport { FontAwesomeIcon } from \'@fortawesome/react-fontawesome\';\\nimport { faReact } from \'@fortawesome/free-brands-svg-icons\';\\n\\nexport const Framework = () => (\\n  <div>\\n    Favorite Framework: <FontAwesomeIcon icon={faReact} />\\n  </div>\\n);\\n```\\n\\nHere we\'ve ditched the \\"library / magic-string\\" approach from the documentation for one which explicitly imports and uses the required icons. I suspect this will be good for tree-shaking as well but, hand-on-heart, I haven\'t rigorously tested that. I\'m not sure why the approach I\'m using isn\'t documented actually. Mysterious! I\'ve seen no ill-effects from using it but perhaps YMMV. Proceed with caution...\\n\\n## Update: It is documented!\\n\\nYup - information on this approach is out there; but it\'s less obvious than you might hope. [Read all about it here.](https://github.com/FortAwesome/react-fontawesome#explicit-import) For what it\'s worth, the explicit import approach seems to be playing second fiddle to the library / magic-string one. I\'m not too sure why. For my money, explicit imports are clearer, less prone to errors and better setup for optimisation. Go figure...\\n\\nFeel free to set me straight in the comments!"},{"id":"/2018/09/23/ts-loader-project-references-first-blood","metadata":{"permalink":"/2018/09/23/ts-loader-project-references-first-blood","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2018-09-23-ts-loader-project-references-first-blood/index.md","source":"@site/blog/2018-09-23-ts-loader-project-references-first-blood/index.md","title":"ts-loader Project References: First Blood","description":"So project references eh? They shipped with TypeScript 3. We\'ve just shipped initial support for project references in ts-loader v5.2.0. All the hard work was done by the amazing Andrew Branch. In fact I\'d recommend taking a gander at the PR. Yay Andrew!","date":"2018-09-23T00:00:00.000Z","formattedDate":"September 23, 2018","tags":[{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"project references","permalink":"/tags/project-references"},{"label":"ts-loader","permalink":"/tags/ts-loader"},{"label":"Webpack","permalink":"/tags/webpack"}],"readingTime":3.385,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"ts-loader Project References: First Blood","authors":"johnnyreilly","tags":["TypeScript","project references","ts-loader","Webpack"],"hide_table_of_contents":false},"prevItem":{"title":"Brand New Fonting Awesomeness","permalink":"/2018/10/07/font-awesome-brand-icons-react"},"nextItem":{"title":"Semantic Versioning and Definitely Typed","permalink":"/2018/09/15/semantic-versioning-and-definitely-typed"}},"content":"So [project references](https://www.typescriptlang.org/docs/handbook/project-references.html) eh? They shipped with [TypeScript 3](https://blogs.msdn.microsoft.com/typescript/2018/07/30/announcing-typescript-3-0/#project-references). We\'ve just shipped initial support for project references in [`ts-loader v5.2.0`](https://github.com/TypeStrong/ts-loader/releases/tag/v5.2.0). All the hard work was done by the amazing [Andrew Branch](https://twitter.com/atcb). In fact I\'d recommend taking a gander at [the PR](https://github.com/TypeStrong/ts-loader/pull/817). Yay Andrew!\\n\\nThis post will take us through the nature of the support for project references in ts-loader now and what we hope the future will bring. It <strike>rips off shamelessly</strike>\\n\\nborrows from the [`README/index.md`](https://github.com/TypeStrong/ts-loader#projectreferences-boolean-defaultfalse) documentation that Andrew wrote as part of the PR. Because I am not above stealing.\\n\\n## TL;DR\\n\\nUsing project references currently requires building referenced projects outside of ts-loader. We don\u2019t want to keep it that way, but we\u2019re releasing what we\u2019ve got now. To try it out, you\u2019ll need to pass `projectReferences: true` to `loaderOptions`.\\n\\n## Like `tsc`, but _not_ like `tsc --build`\\n\\nts-loader has partial support for [project references](https://www.typescriptlang.org/docs/handbook/project-references.html) in that it will _load_ dependent composite projects that are already built, but will not currently _build/rebuild_ those upstream projects. The best way to explain exactly what this means is through an example. Say you have a project with a project reference pointing to the `lib/` directory:\\n\\n```sh\\ntsconfig.json\\napp.ts\\nlib/\\n  tsconfig.json\\n  niftyUtil.ts\\n```\\n\\nAnd we\u2019ll assume that the root `tsconfig.json` has `{ \\"references\\": { \\"path\\": \\"lib\\" } }`, which means that any import of a file that\u2019s part of the `lib` sub-project is treated as a reference to another project, not just a reference to a TypeScript file. Before discussing how ts-loader handles this, it\u2019s helpful to review at a really basic level what `tsc` itself does here. If you were to run `tsc` on this tiny example project, the build would fail with the error:\\n\\n```sh\\nerror TS6305: Output file \'lib/niftyUtil.d.ts\' has not been built from source file \'lib/niftyUtil.ts\'.\\n```\\n\\nUsing project references actually instructs `tsc`_not_ to build anything that\u2019s part of another project from source, but rather to look for any `.d.ts` and `.js` files that have already been generated from a previous build. Since we\u2019ve never built the project in `lib` before, those files don\u2019t exist, so building the root project fails. Still just thinking about how `tsc` works, there are two options to make the build succeed: either run `tsc -p lib/tsconfig.json`_first_, or simply run `tsc --build`, which will figure out that `lib` hasn\u2019t been built and build it first for you.\\n\\nOk, so how is that relevant to ts-loader? Because the best way to think about what ts-loader does with project references is that it acts like `tsc`, but _not_ like `tsc --build`. If you run ts-loader on a project that\u2019s using project references, and any upstream project hasn\u2019t been built, you\u2019ll get the exact same `error TS6305` that you would get with `tsc`. If you modify a source file in an upstream project and don\u2019t rebuild that project, `ts-loader` won\u2019t have any idea that you\u2019ve changed anything\u2014it will still be looking at the output from the last time you _built_ that file.\\n\\n## \u201cHey, don\u2019t you think that sounds kind of useless and terrible?\u201d\\n\\nWell, sort of. You can consider it a work-in-progress. It\u2019s true that on its own, as of today, ts-loader doesn\u2019t have everything you need to take advantage of project references in webpack. In practice, though, _consuming_ upstream projects and _building_ upstream projects are somewhat separate concerns. Building them will likely come in a future release. For background, see the [original issue](https://github.com/TypeStrong/ts-loader/issues/815).\\n\\n## `outDir` Windows problemo.\\n\\nAt the moment, composite projects built using the [`outDir` compiler option](https://www.typescriptlang.org/docs/handbook/compiler-options.html) cannot be consumed using ts-loader on Windows. If you try to, ts-loader throws a \\"`has not been built from source file`\\" error. [You can see Andrew and I puzzling over it in the PR.](https://github.com/TypeStrong/ts-loader/pull/817#issuecomment-422245998) We don\'t know why yet; it\'s possible there\'s a bug in `tsc`. It\'s more likely there\'s a bug in `ts-loader`. Hopefully it\'s going to get solved at some point. (Hey, maybe you\'re the one to solve it!) Either way, we didn\'t want to hold back from releasing. So if you\'re building on Windows then avoid building `composite` projects using `outDir`."},{"id":"/2018/09/15/semantic-versioning-and-definitely-typed","metadata":{"permalink":"/2018/09/15/semantic-versioning-and-definitely-typed","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2018-09-15-semantic-versioning-and-definitely-typed/index.md","source":"@site/blog/2018-09-15-semantic-versioning-and-definitely-typed/index.md","title":"Semantic Versioning and Definitely Typed","description":"This a tale of things that are and things that aren\'t. It\'s a tale of semantic versioning, the lack thereof and heartbreak. It\'s a story of terror and failing builds. But it has a bittersweet ending wherein our heroes learn a lesson and understand the need for compromise. We all come out better and wiser people. Hopefully there\'s something for everybody; let\'s start with an exciting opener and see where it goes...","date":"2018-09-15T00:00:00.000Z","formattedDate":"September 15, 2018","tags":[{"label":"DefinitelyTyped","permalink":"/tags/definitely-typed"},{"label":"semantic versioning","permalink":"/tags/semantic-versioning"},{"label":"SemVer","permalink":"/tags/sem-ver"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"react-router","permalink":"/tags/react-router"},{"label":"Ivan Drago","permalink":"/tags/ivan-drago"}],"readingTime":4.605,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Semantic Versioning and Definitely Typed","authors":"johnnyreilly","tags":["DefinitelyTyped","semantic versioning","SemVer","TypeScript","react-router","Ivan Drago"],"image":"./i-must-break-you.jpg","hide_table_of_contents":false},"prevItem":{"title":"ts-loader Project References: First Blood","permalink":"/2018/09/23/ts-loader-project-references-first-blood"},"nextItem":{"title":"Using TypeScript and webpack alias: goodbye relative paths","permalink":"/2018/08/21/typescript-webpack-alias-goodbye-relative-paths"}},"content":"This a tale of things that are and things that aren\'t. It\'s a tale of semantic versioning, the lack thereof and heartbreak. It\'s a story of terror and failing builds. But it has a bittersweet ending wherein our heroes learn a lesson and understand the need for compromise. We all come out better and wiser people. Hopefully there\'s something for everybody; let\'s start with an exciting opener and see where it goes...\\n\\n## Definitely Typed\\n\\nThis is often the experience people have of using type definitions from Definitely Typed:\\n\\n![Ivan Drago saying \\"I must break you\\"](i-must-break-you.jpg)\\n\\nSpecifically, people are used to the idea of semantic versioning and expect it from types published to npm by Definitely Typed. They wait in vain. [I\'ve written before about the Definitely Typed / @types semantic version compromise.](./2017-02-14-typescript-types-and-repeatable-builds/index.md) And I wanted to talk about it a little further as (watching the issues raised on DT) I don\'t think the message has quite got out there. To summarise:\\n\\n1. npm is built on top of [semantic versioning](http://semver.org/) and they [take it seriously](https://docs.npmjs.com/getting-started/semantic-versioning). When a package is published it should be categorised as a major release (breaking changes), a minor release (extra functionality which is backwards compatible) or a patch release (backwards compatible bug fixes).\\n\\n2. Definitely Typed publishes type definitions to npm under the `@types` namespace\\n\\n3. To make consumption of type definitions easier, the versioning of a type definition package will seek to emulate the versioning of the npm package it supports. For example, right now [`react-router`](https://www.npmjs.com/package/react-router)\'s latest version is `4.3.1`. The corresponding type definition [`@types/react-router`](https://www.npmjs.com/package/@types/react-router)\'s latest version is `4.0.31`. (It\'s fairly common for type definition versions to lag behind the package they type.)\\n\\nIf there\'s a breaking change to the `react-router` type definition then the new version published will have a version number that begins `\\"4.0.\\"`. If you are relying on semantic versioning this will break you.\\n\\n## I Couldn\'t Help But Notice Your Pain\\n\\nIf you\'re reading this and can\'t quite believe that @types would be so inconsiderate as to break the conventions of the ecosystem it lives in, I understand. But hopefully you can see there are reasons for this. In the end, being able to use npm as a delivery mechanism for versioned type definitions associated with another package has a cost; that cost is semantic versioning for the type definitions themselves. It wasn\'t a choice taken lightly; it\'s a pragmatic compromise.\\n\\n\\"But what about my failing builds? Fine, people are going to change type definitions, but why should I burn because of their choices?\\"\\n\\nExcellent question. Truly. Well here\'s my advice: don\'t expect semantic versioning where there is none. Use specific package versions. You can do that directly with your `package.json`. For example replace something like this: `\\"@types/react-router\\": \\"^4.0.0\\"` with a specific version number: `\\"@types/react-router\\": \\"4.0.31\\"`. With this approach it\'s a specific activity to upgrade your type definitions. A chore if you will; but a chore that guarantees builds will not fail unexpectedly due to changing type defs.\\n\\nMy own personal preference is [yarn](https://yarnpkg.com/lang/en/). Mother, I\'m in love with a `yarn.lock` file. It is the alternative npm client that came out of Facebook. It pins the exact versions of all packages used in your `yarn.lock` file and guarantees to install the same versions each time. Problem solved; and it even allows me to keep the semantic versioning in my `package.json` as is.\\n\\nThis has some value in that when I upgrade I probably want to upgrade to a newer version following the semantic versioning convention. I should just expect that I\'ll need to check valid compilation when I do so. yarn even has it\'s own built in utility that tells you when things are out of date: `yarn outdated`:\\n\\n![Screenshot of outdated dependencies in yarn](yarn-outdated.png)\\n\\nSo lovely.\\n\\n## You Were Already Broken - I Just Showed You How\\n\\nBefore I finish I wanted to draw out one reason why breaking changes can be a reason for happiness. Because sometimes your code is wrong. An update to a type definition may highlight that. This is analogous to when the TypeScript compiler ships a new version. When I upgrade to a newer version of TypeScript it lights up errors in my codebase that I hadn\'t spotted. Yay compiler!\\n\\nAn example of this is [a PR I submitted to DefinitelyTyped earlier this week](https://github.com/DefinitelyTyped/DefinitelyTyped/pull/28868). This PR changed how `react-router` models the parameters of a `Match`. Until now, an object was expected; the user could define any object they liked. However, `react-router` will only produce `string` values for a parameter. [If you look at the underlying code it\'s nothing more than an `exec` on a regular expression.](https://github.com/ReactTraining/react-router/blob/34ff1f8077d95edf01e9d5ca8ea4708b8d0290e2/packages/react-router/modules/matchPath.js#L36)\\n\\nMy PR enforces this at type level by changing this:\\n\\n```ts\\nexport interface match<P> {\\n  params: P;\\n  // ...\\n}\\n```\\n\\nTo this\\n\\n```ts\\nexport interface match<Params extends { [K in keyof Params]?: string } = {}> {\\n  params: Params;\\n  // ...\\n}\\n```\\n\\nSo any object definition supplied must have `string` values (and you don\'t actually need to supply an object definition; that\'s optional now).\\n\\nI expected this PR to break people [and it did](https://github.com/DefinitelyTyped/DefinitelyTyped/issues/28894). But this is a useful break. If they were relying upon their parameters to be types other than strings they would be experiencing some unexpected behaviour. In fact, it\'s exactly this that prompted my PR in the first place. A colleague had defined his parameters as `number`s and couldn\'t understand why they weren\'t behaving like `number`s. Because they weren\'t `number`s! And wonderfully, this will now be caught at compile time; not runtime. Yay!"},{"id":"/2018/08/21/typescript-webpack-alias-goodbye-relative-paths","metadata":{"permalink":"/2018/08/21/typescript-webpack-alias-goodbye-relative-paths","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2018-08-21-typescript-webpack-alias-goodbye-relative-paths/index.md","source":"@site/blog/2018-08-21-typescript-webpack-alias-goodbye-relative-paths/index.md","title":"Using TypeScript and webpack alias: goodbye relative paths","description":"This post shows how you can use TypeScript with webpack alias to move away from using relative paths in your import statements.","date":"2018-08-21T00:00:00.000Z","formattedDate":"August 21, 2018","tags":[{"label":"relative paths","permalink":"/tags/relative-paths"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"webpack","permalink":"/tags/webpack"},{"label":"alias","permalink":"/tags/alias"},{"label":"paths","permalink":"/tags/paths"},{"label":"resolve","permalink":"/tags/resolve"},{"label":"tsconfig-paths-webpack-plugin","permalink":"/tags/tsconfig-paths-webpack-plugin"}],"readingTime":2.86,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Using TypeScript and webpack alias: goodbye relative paths","authors":"johnnyreilly","tags":["relative paths","TypeScript","webpack","alias","paths","resolve","tsconfig-paths-webpack-plugin"],"hide_table_of_contents":false},"prevItem":{"title":"Semantic Versioning and Definitely Typed","permalink":"/2018/09/15/semantic-versioning-and-definitely-typed"},"nextItem":{"title":"Azure App Service: nested configuration for ASP.NET running in Web App for Containers using Application Settings","permalink":"/2018/07/28/azure-app-service-web-app-containers-asp-net-nested-configuration"}},"content":"This post shows how you can use TypeScript with webpack `alias` to move away from using relative paths in your `import` statements.\\n\\n## Long relative paths\\n\\nI write a lot of TypeScript. Because I like modularity, I split up my codebases into discreet modules and `import` from them as necessary.\\n\\nTake a look at this `import`:\\n\\n```ts\\nimport * as utils from \'../../../../../../../shared/utils\';\\n```\\n\\nNow take a look at this import:\\n\\n```ts\\nimport * as utils from \'shared/utils\';\\n```\\n\\nWhich do you prefer? If the answer was \\"the first\\" then read no further. You have all you need, go forth and be happy. If the answer was \\"the second\\" then stick around; I can help!\\n\\n## TypeScript\\n\\nThere\'s been a solution for this in TypeScript-land for some time. You can read the detail [in the \\"path mapping\\" docs here](https://www.typescriptlang.org/docs/handbook/module-resolution.html#path-mapping).\\n\\nLet\'s take a slightly simpler example; we have a folder structure that looks like this:\\n\\n```console\\nprojectRoot\\n\u251c\u2500\u2500 components\\n\u2502 \u2514\u2500\u2500 page.tsx (imports \'../shared/utils\')\\n\u251c\u2500\u2500 shared\\n\u2502 \u251c\u2500\u2500 folder1\\n\u2502 \u2514\u2500\u2500 folder2\\n\u2502 \u2514\u2500\u2500 utils.ts\\n\u2514\u2500\u2500 tsconfig.json\\n```\\n\\nWe would like `page.tsx` to import `\'shared/utils\'` instead of `\'../shared/utils\'`. We can, if we augment our `tsconfig.json` with the following properties:\\n\\n```json\\n{\\n  \\"compilerOptions\\": {\\n    \\"baseUrl\\": \\".\\",\\n    \\"paths\\": {\\n      \\"components/*\\": [\\"components/*\\"],\\n      \\"shared/*\\": [\\"shared/*\\"]\\n    }\\n  }\\n}\\n```\\n\\nThen we can use option 2. We can happily write:\\n\\n```ts\\nimport * as utils from \'shared/utils\';\\n```\\n\\nMy code compiles, yay.... Ship it!\\n\\nLet\'s not get over-excited. Actually, we\'re only part-way there; you can compile this code with the TypeScript compiler.... But is that enough?\\n\\nI bundle my TypeScript with [ts-loader](https://github.com/TypeStrong/ts-loader) and webpack. If I try and use my new exciting import statement above with my build system then disappointment is in my future. webpack will be all like \\"import whuuuuuuuut?\\"\\n\\nYou see, webpack doesn\'t know what we told the TypeScript compiler in the `tsconfig.json`. Why would it? It was our little secret.\\n\\n## webpack `resolve.alias` to the rescue!\\n\\nThis same functionality has existed in webpack for a long time; actually much longer than it has existed in TypeScript. It\'s the [`resolve.alias`](https://webpack.js.org/configuration/resolve/#resolve-alias) functionality.\\n\\nSo, looking at that I should be able to augment my `webpack.config.js` like so:\\n\\n```js\\nmodule.exports = {\\n  //...\\n  resolve: {\\n    alias: {\\n      components: path.resolve(process.cwd(), \'components/\'),\\n      shared: path.resolve(process.cwd(), \'shared/\'),\\n    },\\n  },\\n};\\n```\\n\\nAnd now both webpack and TypeScript are up to speed with how to resolve modules.\\n\\n## DRY with the [`tsconfig-paths-webpack-plugin`](https://github.com/dividab/tsconfig-paths-webpack-plugin)\\n\\nWhen I look at the `tsconfig.json` and the `webpack.config.js` something occurs to me: I don\'t like to repeat myself. As well as that, I don\'t like to repeat myself. It\'s so... Repetitive.\\n\\nThe declarations you make in the `tsconfig.json` are re-stated in the `webpack.config.js`. Who wants to maintain two sets of code where one would do? Not me.\\n\\nFortunately, you don\'t have to. There\'s the [`tsconfig-paths-webpack-plugin`](https://github.com/dividab/tsconfig-paths-webpack-plugin) for webpack which will do the job for you. You can replace your verbose `resolve.alias` with this:\\n\\n```ts\\nmodule.exports = {\\n  //...\\n  resolve: {\\n    plugins: [\\n      new TsconfigPathsPlugin({\\n        /*configFile: \\"./path/to/tsconfig.json\\" */\\n      }),\\n    ],\\n  },\\n};\\n```\\n\\nThis does the hard graft of reading your `tsconfig.json` and translating path mappings into webpack `alias`es. From this point forward, you need only edit the `tsconfig.json` and everything else will just work.\\n\\nThanks to [Jonas Kello](https://github.com/jonaskello), author of the plugin; it\'s tremendous! Thanks also to [Sean Larkin](https://twitter.com/TheLarkInn) and [Stanislav Panferov](https://github.com/s-panferov) (of [awesome-typescript-loader](https://github.com/s-panferov/awesome-typescript-loader)) who together worked on the original plugin that I understand the `tsconfig-paths-webpack-plugin` is based on. Great work!"},{"id":"/2018/07/28/azure-app-service-web-app-containers-asp-net-nested-configuration","metadata":{"permalink":"/2018/07/28/azure-app-service-web-app-containers-asp-net-nested-configuration","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2018-07-28-azure-app-service-web-app-containers-asp-net-nested-configuration/index.md","source":"@site/blog/2018-07-28-azure-app-service-web-app-containers-asp-net-nested-configuration/index.md","title":"Azure App Service: nested configuration for ASP.NET running in Web App for Containers using Application Settings","description":"How can we configure an ASP.NET application with nested properties Azure App Service Web App for Containers using Application Settings in Azure? Colons don\'t work.","date":"2018-07-28T00:00:00.000Z","formattedDate":"July 28, 2018","tags":[],"readingTime":1.89,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Azure App Service: nested configuration for ASP.NET running in Web App for Containers using Application Settings","authors":"johnnyreilly","tags":[],"image":"./appservice_classic.png","hide_table_of_contents":false},"prevItem":{"title":"Using TypeScript and webpack alias: goodbye relative paths","permalink":"/2018/08/21/typescript-webpack-alias-goodbye-relative-paths"},"nextItem":{"title":"Cypress and Auth0","permalink":"/2018/07/09/cypress-and-auth0"}},"content":"How can we configure an ASP.NET application with nested properties [Azure App Service Web App for Containers](https://azure.microsoft.com/en-gb/services/app-service/containers/) using Application Settings in Azure? Colons don\'t work.\\n\\n## Containers on App Service\\n\\nApp Services have long been a super simple way to spin up a web app in Azure. The barrier to entry is low, maintenance is easy. It just works. App Services recently got a turbo boost in the form of [Azure App Service on Linux](https://docs.microsoft.com/en-us/azure/app-service/containers/app-service-linux-intro). Being able to deploy to Linux is exciting enough; but another reason this is notable because [you can deploy Docker images that will be run as app services](https://docs.microsoft.com/en-us/azure/app-service/containers/tutorial-custom-docker-image).\\n\\nI cannot over-emphasise just how easy this makes getting a Docker image into Production. Yay Azure!\\n\\n## The Mystery of Configuration\\n\\nApplications need configuration. ASP.Net Core applications are typically configured by an `appsettings.json` file which might look like so:\\n\\n```json\\n{\\n  \\"Parent\\": {\\n    \\"ChildOne\\": \\"I\'m a little teapot\\",\\n    \\"ChildTwo\\": \\"Short and stout\\"\\n  }\\n}\\n```\\n\\nWith a classic App Service you could override a setting in the `appsettings.json` by updating \\"Application settings\\" within the Azure portal. You\'d do this in the style of creating an Application setting called `Parent:ChildOne` or `Parent:ChildTwo`. To be clear: using colons to target a specific piece of config.\\n\\n![screenshot of an App Service Application Settings in the Azure Portal, nested properties configured using colons](appservice_classic.png)\\n\\nYou can read about this approach [here](https://blogs.msdn.microsoft.com/waws/2018/06/12/asp-net-core-settings-for-azure-app-service/). Now there\'s something I want you to notice; consider the colons below:\\n\\n![screenshot of an App Service specific Application Setting nested property configured using colons - all good](appservice_colons_fine.png)\\n\\nIf you try and follow the same steps when you\'re using Web App for Containers / i.e. [a Docker image deployed to an Azure App Service on Linux ](https://docs.microsoft.com/en-us/azure/app-service/containers/app-service-linux-intro) you **cannot** use colons:\\n\\n![screenshot of a Web App for Containers specific Application Setting nested property configured using colons - errors](appservice_container_colons_bad.png)\\n\\nWhen you hover over the error you see this message: `This field can only contain letters, numbers (0-9), periods (\\".\\"), and underscores (\\"_\\")`. Using `.` does not work alas.\\n\\n## How do we configure without colons?\\n\\nIt\'s simple. Where you would use `:` on a classic App Service, you should use a `__` (double underscore) on an App Service with containers. So `Parent__ChildOne` instead of `Parent:ChildOne`. It\'s as simple as that."},{"id":"/2018/07/09/cypress-and-auth0","metadata":{"permalink":"/2018/07/09/cypress-and-auth0","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2018-07-09-cypress-and-auth0/index.md","source":"@site/blog/2018-07-09-cypress-and-auth0/index.md","title":"Cypress and Auth0","description":"Cypress is a fantastic way to write UI tests for your web apps. Just world class. Wait, no. Galaxy class. I\'m going to go one further: universe class. You get my drift.","date":"2018-07-09T00:00:00.000Z","formattedDate":"July 9, 2018","tags":[{"label":"auth0-js","permalink":"/tags/auth-0-js"},{"label":"Auth0","permalink":"/tags/auth-0"},{"label":"cypress","permalink":"/tags/cypress"},{"label":"login","permalink":"/tags/login"}],"readingTime":4.435,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Cypress and Auth0","authors":"johnnyreilly","tags":["auth0-js","Auth0","cypress","login"],"hide_table_of_contents":false},"prevItem":{"title":"Azure App Service: nested configuration for ASP.NET running in Web App for Containers using Application Settings","permalink":"/2018/07/28/azure-app-service-web-app-containers-asp-net-nested-configuration"},"nextItem":{"title":"VSTS and EF Core Migrations","permalink":"/2018/06/24/vsts-and-ef-core-migrations"}},"content":"[Cypress](https://www.cypress.io/) is a fantastic way to write UI tests for your web apps. Just world class. Wait, no. Galaxy class. I\'m going to go one further: universe class. You get my drift.\\n\\nHere\'s a pickle for you. You have functionality that lies only behind the walled garden of authentication. You want to write tests for these capabilities. Assuming that authentication takes place within your application that\'s no great shakes. Authentication is part of your app; it\'s no big deal using Cypress to automate logging in.\\n\\nAuth is a serious business and, as Cypress is best in class for UI testing, I\'ll say that Auth0 is romping home with the same title in the auth-as-a-service space. My app is using Auth0 for authentication. What\'s important to note about this is the flow. Typically when using auth-as-a-service, the user is redirected to the auth provider\'s site to authenticate and then be redirected back to the application post-login.\\n\\n[Brian Mann](https://github.com/brian-mann) (of Cypress fame) has been [fairly clear when talking about testing with this sort of authentication flow](https://github.com/cypress-io/cypress/issues/1342#issuecomment-366747803):\\n\\n> You\'re trying to test SSO - and we have recipes showing you exactly how to do this.\\n>\\n> Also best practice is never to visit or test 3rd party sites not under your control. You don\'t control `microsoftonline`, so there\'s no reason to use the UI to test this. You can programmatically test the integration between it and your app with `cy.request` \\\\- which is far faster, more reliable, and still gives you 100% confidence.\\n\\nI want to automate logging into Auth0 from my Cypress tests. But hopefully in a good way. Not a bad way. Wouldn\'t want to make Brian sad.\\n\\n## Commanding Auth0\\n\\nTo automate our login, we\'re going to use the [auth0-js client library](https://github.com/auth0/auth0.js). This is the same library the application uses; but we\'re going to do something subtly different with it.\\n\\nThe application uses [`authorize`](https://github.com/auth0/auth0.js#api) to log users in. This function redirects the user into the Auth0 lock screen, and then, post authentication, redirects the user back to the application with a token in the URL. The app parses the token (using the auth0 client library) and sets the token and the expiration of said token in the browser sessionStorage.\\n\\nWhat we\'re going to do is automate our login by using `login` instead. First of all, we need to add `auth0-js` as a dependency of our e2e tests:\\n\\n```js\\nyarn add auth0-js --dev\\n```\\n\\nNext, we\'re going to create ourselves a custom command called loginAsAdmin:\\n\\n```js\\nconst auth0 = require(\'auth0-js\');\\n\\nCypress.Commands.add(\'loginAsAdmin\', (overrides = {}) => {\\n  Cypress.log({\\n    name: \'loginAsAdminBySingleSignOn\',\\n  });\\n\\n  const webAuth = new auth0.WebAuth({\\n    domain: \'my-super-duper-domain.eu.auth0.com\', // Get this from https://manage.auth0.com/#/applications and your application\\n    clientID: \'myclientid\', // Get this from https://manage.auth0.com/#/applications and your application\\n    responseType: \'token id_token\',\\n  });\\n\\n  webAuth.client.login(\\n    {\\n      realm: \'Username-Password-Authentication\',\\n      username: \'mytestemail@something.co.uk\',\\n      password: \'SoVeryVeryVery$ecure\',\\n      audience: \'myaudience\', // Get this from https://manage.auth0.com/#/apis and your api, use the identifier property\\n      scope: \'openid email profile\',\\n    },\\n    function (err, authResult) {\\n      // Auth tokens in the result or an error\\n      if (authResult && authResult.accessToken && authResult.idToken) {\\n        const token = {\\n          accessToken: authResult.accessToken,\\n          idToken: authResult.idToken,\\n          // Set the time that the access token will expire at\\n          expiresAt: authResult.expiresIn * 1000 + new Date().getTime(),\\n        };\\n\\n        window.sessionStorage.setItem(\\n          \'my-super-duper-app:storage_token\',\\n          JSON.stringify(token)\\n        );\\n      } else {\\n        console.error(\'Problem logging into Auth0\', err);\\n        throw err;\\n      }\\n    }\\n  );\\n});\\n```\\n\\nThis command logs in using the `auth0-js` API and then sets the result into `sessionStorage` in the same way that our app does. This allows our app to read the value out of `sessionStorage` and use it. We\'re also going to put together one other command:\\n\\n```js\\nCypress.Commands.add(\'visitHome\', (overrides = {}) => {\\n  cy.visit(\'/\', {\\n    onBeforeLoad: (win) => {\\n      win.sessionStorage.clear();\\n    },\\n  });\\n});\\n```\\n\\nThis visits the root of our application and wipes the `sessionStorage`. This is necessary because Cypress doesn\'t clear down `sessionStorage` between tests. ([That\'s going to change though.](https://github.com/cypress-io/cypress/issues/413))\\n\\n## Using It\\n\\nLet\'s write a test that uses our new commands to see if it gets access to our admin functionality:\\n\\n```js\\ndescribe(\'access secret admin functionality\', () => {\\n  it(\'should be able to navigate to\', () => {\\n    cy.visitHome()\\n      .loginAsAdmin()\\n      .get(\'[href=\\"/secret-adminny-stuff\\"]\') // This link should only be visible to admins\\n      .click()\\n      .url()\\n      .should(\'contain\', \'secret-adminny-stuff/\'); // non-admins should be redirected away from this url\\n  });\\n});\\n```\\n\\nWell, the test looks good but it\'s failing. If I fire up the Chrome Dev Tools in Cypress (did I mention that Cypress is absolutely fabulous?) then I see this response tucked away in the network tab:\\n\\n```json\\n{error: \\"unauthorized_client\\",\u2026} error : \\"unauthorized_client\\" error_description : \\"Grant type \'http://auth0.com/oauth/grant-type/password-realm\' not allowed for the client.\\"\\n```\\n\\nHmmm... So sad. If you go to [https://manage.auth0.com/#/applications](https://manage.auth0.com/#/applications), select your application, `Show Advanced Settings` and `Grant Types` you\'ll see a `Password` option is unselected.\\n\\nSelect it, Save Changes and try again.\\n\\n![](auth0-enable-password-grant-type.png)\\n\\nYou now have a test which automates your Auth0 login using Cypress and goes on to test your application functionality with it!\\n\\n## One More Thing...\\n\\nIt\'s worth saying that it\'s worth setting up different tenants in Auth0 to support your testing scenarios. This is generally a good idea so you can separate your testing accounts from Production accounts. Further to that, you don\'t need to have your Production setup supporting the ` Password``Grant Type `.\\n\\nAlso, if you\'re curious about what the application under test is like then read [this](./2018-01-14-auth0-typescript-and-aspnet-core/index.md)."},{"id":"/2018/06/24/vsts-and-ef-core-migrations","metadata":{"permalink":"/2018/06/24/vsts-and-ef-core-migrations","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2018-06-24-vsts-and-ef-core-migrations/index.md","source":"@site/blog/2018-06-24-vsts-and-ef-core-migrations/index.md","title":"VSTS and EF Core Migrations","description":"Let me start by telling you a dirty secret. I have an ASP.Net Core project that I build with VSTS. It is deployed to Azure through a CI / CD setup in VSTS. That part I\'m happy with. Proud of even. Now to the sordid hiddenness: try as I might, I\'ve never found a nice way to deploy Entity Framework database migrations as part of the deployment flow. So I have [blushes with embarrassment] been using the Startup of my ASP.Net core app to run the migrations on my database. There. I said it. You all know. Absolutely filthy. Don\'t judge me.","date":"2018-06-24T00:00:00.000Z","formattedDate":"June 24, 2018","tags":[{"label":"vsts","permalink":"/tags/vsts"},{"label":"Entity Framework","permalink":"/tags/entity-framework"},{"label":"ef core","permalink":"/tags/ef-core"}],"readingTime":5.005,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"VSTS and EF Core Migrations","authors":"johnnyreilly","tags":["vsts","Entity Framework","ef core"],"hide_table_of_contents":false},"prevItem":{"title":"Cypress and Auth0","permalink":"/2018/07/09/cypress-and-auth0"},"nextItem":{"title":"VSTS... YAML up!","permalink":"/2018/06/16/vsts-yaml-up"}},"content":"Let me start by telling you a dirty secret. I have an ASP.Net Core project that I build with VSTS. It is deployed to Azure through a CI / CD setup in VSTS. That part I\'m happy with. Proud of even. Now to the sordid hiddenness: try as I might, I\'ve never found a nice way to deploy Entity Framework database migrations as part of the deployment flow. So I have [blushes with embarrassment] been using the `Startup` of my ASP.Net core app to run the migrations on my database. There. I said it. You all know. Absolutely filthy. Don\'t judge me.\\n\\nIf you care to google, you\'ll find various discussions around this, and various ways to tackle it. Most of which felt like too much hard work and so I never attempted.\\n\\nIt\'s also worth saying that being on VSTS made me less likely to give these approaches a go. Why? Well, the feedback loop for debugging a CI / CD setup is truly sucky. Make a change. Wait for it to trickle through the CI / CD flow (10 mins at least). Spot a problem, try and fix. Start waiting again. Repeat until you succeed. Or, if you\'re using the free tier of VSTS, repeat until you run out of build minutes. You have a limited number of build minutes per month with VSTS. Last time I fiddled with the build, I bled my way through a full month\'s minutes in 2 days. I have now adopted the approach of only playing with the setup in the last week of the month. That way if I end up running out of minutes, at least I\'ll roll over to the new allowance in a matter of days.\\n\\nDigression over. I could take the guilt of my EF migrations secret no longer, I decided to try and tackle it another way. I used the approach suggested by [Andre Broers](https://github.com/broersa)[here](https://github.com/aspnet/EntityFrameworkCore/issues/9841#issuecomment-395712061):\\n\\n> I worked around by adding a dotnetcore consoleapp project where I run the migration via the Context. In the Build I build this consoleapp in the release I execute it.\\n\\n## Console Yourself\\n\\nFirst things first, we need a console app added to our solution. Fire up PowerShell in the root of your project and:\\n\\n```console\\nmd MyAwesomeProject.MigrateDatabase\\ncd .\\\\MyAwesomeProject.MigrateDatabase\\\\\\ndotnet new console\\n```\\n\\nNext we need that project to know about Entity Framework and also our DbContext (which I store in a dedicated project):\\n\\n```console\\ndotnet add package Microsoft.EntityFrameworkCore.Design\\ndotnet add package Microsoft.EntityFrameworkCore.SqlServer\\ndotnet add reference ..\\\\MyAwesomeProject.Database\\\\MyAwesomeProject.Database.csproj\\n```\\n\\nAdd our new project to our solution: (I always forget to do this)\\n\\n```console\\ncd ../\\ndotnet sln add .\\\\MyAwesomeProject.MigrateDatabase\\\\MyAwesomeProject.MigrateDatabase.csproj\\n```\\n\\nYou should now be the proud possessor of a `.csproj` file that looks like this:\\n\\n```xml\\n<Project Sdk=\\"Microsoft.NET.Sdk\\">\\n\\n  <PropertyGroup>\\n    <OutputType>Exe</OutputType>\\n    <TargetFramework>netcoreapp2.1</TargetFramework>\\n  </PropertyGroup>\\n\\n  <ItemGroup>\\n    <PackageReference Include=\\"Microsoft.EntityFrameworkCore.Design\\" Version=\\"2.1.1\\" />\\n    <PackageReference Include=\\"Microsoft.EntityFrameworkCore.SqlServer\\" Version=\\"2.1.1\\" />\\n  </ItemGroup>\\n\\n  <ItemGroup>\\n    <ProjectReference Include=\\"..\\\\MyAwesomeProject.Database\\\\MyAwesomeProject.Database.csproj\\" />\\n  </ItemGroup>\\n\\n</Project>\\n```\\n\\nReplace the contents of the `Program.cs` file with this:\\n\\n```cs\\nusing System;\\nusing System.IO;\\nusing MyAwesomeProject.Database;\\nusing Microsoft.EntityFrameworkCore;\\n\\nnamespace MyAwesomeProject.MigrateDatabase {\\n    class Program {\\n        // Example usage:\\n        // dotnet MyAwesomeProject.MigrateDatabase.dll \\"Server=(localdb)\\\\\\\\mssqllocaldb;Database=MyAwesomeProject;Trusted_Connection=True;\\"\\n        static void Main(string[] args) {\\n            if (args.Length == 0)\\n                throw new Exception(\\"No connection string supplied!\\");\\n\\n            var myAwesomeProjectConnectionString = args[0];\\n\\n            // Totally optional debug information\\n            Console.WriteLine(\\"About to migrate this database:\\");\\n            var connectionBits = myAwesomeProjectConnectionString.Split(\\";\\");\\n            foreach (var connectionBit in connectionBits) {\\n                if (!connectionBit.StartsWith(\\"Password\\", StringComparison.CurrentCultureIgnoreCase))\\n                    Console.WriteLine(connectionBit);\\n            }\\n\\n            try {\\n                var optionsBuilder = new DbContextOptionsBuilder<MyAwesomeProjectContext>();\\n                optionsBuilder.UseSqlServer(myAwesomeProjectConnectionString);\\n\\n                using(var context = new MyAwesomeProjectContext(optionsBuilder.Options)) {\\n                    context.Database.Migrate();\\n                }\\n                Console.WriteLine(\\"This database is migrated like it\'s the Serengeti!\\");\\n            } catch (Exception exc) {\\n                var failedToMigrateException = new Exception(\\"Failed to apply migrations!\\", exc);\\n                Console.WriteLine($\\"Didn\'t succeed in applying migrations: {exc.Message}\\");\\n                throw failedToMigrateException;\\n            }\\n        }\\n    }\\n}\\n```\\n\\nThis code takes the database connection string passed as an argument, spins up a db context with that, and migrates like it\'s the Serengeti.\\n\\n## Build It!\\n\\nThe next thing we need is to ensure that this is included as part of the build process in VSTS. The following commands need to be run during the build to include the MigrateDatabase project in the build output in a `MigrateDatabase` folder:\\n\\n```cs\\ncd MyAwesomeProject.MigrateDatabase\\ndotnet build\\ndotnet publish --configuration Release --output $(build.artifactstagingdirectory)/MigrateDatabase\\n```\\n\\nThere\'s various ways to accomplish this which I wont reiterate now. [I recommend YAML](./2018-06-16-vsts-yaml-up/index.md).\\n\\n## Deploy It!\\n\\nNow to execute our console app as part of the deployment process we need to add a CommandLine task to our VSTS build definition. It should execute the following command:\\n\\n```cs\\ndotnet MyAwesomeProject.MigrateDatabase.dll \\"$(ConnectionStrings.MyAwesomeProjectDatabaseConnection)\\"\\n```\\n\\nIn the following folder:\\n\\n```cs\\n$(System.DefaultWorkingDirectory)/my-awesome-project-YAML/drop/MigrateDatabase\\n```\\n\\nDo note that the command uses the `ConnectionStrings.MyAwesomeProjectDatabaseConnection` variable which you need to create and set to the value of your connection string.\\n\\n![](Screenshot-2018-06-24-10.55.27.png)\\n\\n## Give It A Whirl\\n\\nLet\'s find out what happens when the rubber hits the road. I\'ll add a new entity to my database project:\\n\\n```cs\\nusing System;\\n\\nnamespace MyAwesomeProject.Database.Entities {\\n    public class NewHotness {\\n        public Guid NewHotnessId { get; set; }\\n    }\\n}\\n```\\n\\nAnd reference it in my DbContext:\\n\\n```cs\\nusing MyAwesomeProject.Database.Entities;\\nusing Microsoft.EntityFrameworkCore;\\n\\nnamespace MyAwesomeProject.Database {\\n    public class MyAwesomeProjectContext : DbContext {\\n        public MyAwesomeProjectContext(DbContextOptions<MyAwesomeProjectContext> options) : base(options) { }\\n\\n        // ...\\n\\n        public DbSet<NewHotness> NewHotnesses { get; set; }\\n\\n        // ...\\n    }\\n}\\n```\\n\\nLet\'s let EF know by adding a migration to my project:\\n\\n```cs\\ndotnet ef migrations add TestOurMigrationsApproach\\n```\\n\\nCommit my change, push it to VSTS, wait for the build to run and a deployment to take place.... Okay. It\'s done. Looks good.\\n\\n![](Screenshot-2018-06-24-09.02.22.png)\\n\\nLet\'s take a look in the database:\\n\\n```console\\nselect * from NewHotnesses\\ngo\\n```\\n\\n![](Screenshot-2018-06-24-08.59.00.png)\\n\\nIt\'s there! We are migrating our database upon deployment; and not in our ASP.Net Core app itself. I feel a burden lifted.\\n\\n## Wrapping Up\\n\\nThe EF Core team are aware of the lack of guidance around deploying migrations and have recently announced plans to fix that in the docs. You can track the progress of this issue [here](https://github.com/aspnet/EntityFramework.Docs/issues/691). There\'s good odds that once they come out with this I\'ll find there\'s a better way than the approach I\'ve outlined in this post. Until that glorious day!"},{"id":"/2018/06/16/vsts-yaml-up","metadata":{"permalink":"/2018/06/16/vsts-yaml-up","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2018-06-16-vsts-yaml-up/index.md","source":"@site/blog/2018-06-16-vsts-yaml-up/index.md","title":"VSTS... YAML up!","description":"For the longest time I\'ve been using the likes of Travis and AppVeyor to build open source projects that I work on. They rock. I\'ve also recently been dipping my toes back in the water of Visual Studio Team Services. VSTS offers a whole stack of stuff, but my own area of interest has been the Continuous Integration / Continuous Deployment offering.","date":"2018-06-16T00:00:00.000Z","formattedDate":"June 16, 2018","tags":[{"label":"yaml","permalink":"/tags/yaml"},{"label":"vsts","permalink":"/tags/vsts"},{"label":"ci","permalink":"/tags/ci"},{"label":"travis","permalink":"/tags/travis"},{"label":"AppVeyor","permalink":"/tags/app-veyor"}],"readingTime":4.415,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"VSTS... YAML up!","authors":"johnnyreilly","tags":["yaml","vsts","ci","travis","AppVeyor"],"hide_table_of_contents":false},"prevItem":{"title":"VSTS and EF Core Migrations","permalink":"/2018/06/24/vsts-and-ef-core-migrations"},"nextItem":{"title":"Compromising: A Guide for Developers","permalink":"/2018/05/13/compromising-guide-for-developers"}},"content":"For the longest time I\'ve been using the likes of [Travis](https://travis-ci.org/) and [AppVeyor](https://www.appveyor.com/) to build open source projects that I work on. They rock. I\'ve also recently been dipping my toes back in the water of [Visual Studio Team Services](https://www.visualstudio.com/team-services/). VSTS offers a whole stack of stuff, but my own area of interest has been the Continuous Integration / Continuous Deployment offering.\\n\\nHistorically I have been underwhelmed by the CI proposition of Team Foundation Server / VSTS. It was difficult to debug, difficult to configure, difficult to understand. If it worked... Great! If it didn\'t (and it often didn\'t), you were toast. But things done changed! I don\'t know when it happened, but VSTS is now super configurable. You add tasks / configure them, build and you\'re done! It\'s really nice.\\n\\nHowever, there\'s been something I\'ve been missing from Travis, AppVeyor et al. Keeping my build script with my code. Travis has `.travis.yml`, AppVeyor has `appveyor.yml`. VSTS, what\'s up?\\n\\n## The New Dawn\\n\\nUp until now, really not much. It just wasn\'t possible. Until it was:\\n\\n> If you prefer a build definition in YAML then we\u2019re currently hard at work on that. You can enable it as a preview feature: [https://t.co/hau9Sv8brf](https://t.co/hau9Sv8brf)\\n>\\n> \u2014 Martin Woodward (@martinwoodward) [March 4, 2018](https://twitter.com/martinwoodward/status/970250739510534144?ref_src=twsrc%5Etfw)\\n\\n<script async=\\"\\" src=\\"https://platform.twitter.com/widgets.js\\" charSet=\\"utf-8\\"><\/script>\\n\\nWhen I started testing it out I found things to like and some things I didn\'t understand. Crucially, my CI now builds based upon `.vsts-ci.yml`. YAML baby!\\n\\n## It Begins!\\n\\nYou can get to \\"Hello World\\" by looking at [the docs here](https://docs.microsoft.com/en-us/vsts/pipelines/build/yaml?view=vsts) and [the examples here](https://github.com/Microsoft/vsts-agent/blob/master/docs/preview/yamlgettingstarted/index.md). But what you really want is your existing build, configured in the UI, exported to YAML. That doesn\'t seem to quite exist, but there\'s something that gets you part way. Take a look:\\n\\n![screenshot of restore task in VSTS](vsts-screenshot-of-restore-task.png)\\n\\nIf you notice, in the top right of the screen, each task now allows you click on a new \\"View YAML\\" button. It\'s kinda [Ronseal](https://en.wikipedia.org/wiki/Ronseal):\\n\\n![screenshot of copy to clipboard in VSTS](vsts-screenshot-of-copy-to-clipboard.png)\\n\\nUsing this hotness you can build yourself a `.vsts-ci.yml` file task by task.\\n\\n## A Bump in the Road\\n\\nIf you look closely at the message above you\'ll see there\'s a message about an undefined variable.\\n\\n```yml\\n#Your build definition references an undefined variable named \u2018Parameters.RestoreBuildProjects\u2019. Create or edit the build definition for this YAML file, define the variable on the Variables tab. See https://go.microsoft.com/fwlink/?linkid=865972\\nsteps:\\n  - task: DotNetCoreCLI@2\\n    displayName: Restore\\n    inputs:\\n      command: restore\\n      projects: \'$(Parameters.RestoreBuildProjects)\'\\n```\\n\\nTry as I might, I couldn\'t locate `Parameters.RestoreBuildProjects`. So no working CI build for me. Then I remembered [Zoltan Erdos](https://github.com/zerdos). He\'s hard to forget. Or rather, I remembered an idea of his which I will summarise thusly: \\"Have a `package.json` in the root of your repo, use the `scripts` for individual tasks and you have a cross platform task runner\\".\\n\\nThis is a powerful idea and one I decided to put to work. My project is React and TypeScript on the front end, and ASP.Net Core on the back. I wanted a `package.json` in the root of the repo which I could install dependencies, build, test and publish my whole app. I could call into that from my `.vsts-ci.yml` file. Something like this:\\n\\n```json\\n{\\n  \\"name\\": \\"my-amazing-project\\",\\n  \\"version\\": \\"1.0.0\\",\\n  \\"author\\": \\"John Reilly <johnny_reilly@hotmail.com>\\",\\n  \\"license\\": \\"MIT\\",\\n  \\"private\\": true,\\n  \\"scripts\\": {\\n    \\"preinstall\\": \\"yarn run install:clientapp && yarn run install:web\\",\\n    \\"install:clientapp\\": \\"cd MyAmazingProject.ClientApp && yarn install\\",\\n    \\"install:web\\": \\"dotnet restore\\",\\n    \\"prebuild\\": \\"yarn install\\",\\n    \\"build\\": \\"yarn run build:clientapp && yarn run build:web\\",\\n    \\"build:clientapp\\": \\"cd MyAmazingProject.ClientApp && yarn run build\\",\\n    \\"build:web\\": \\"dotnet build --configuration Release\\",\\n    \\"postbuild\\": \\"yarn test\\",\\n    \\"test\\": \\"yarn run test:clientapp && yarn run test:web\\",\\n    \\"test:clientapp\\": \\"cd MyAmazingProject.ClientApp && yarn test\\",\\n    \\"test:web\\": \\"cd MyAmazingProject.Web.Tests && dotnet test\\",\\n    \\"publish:web\\": \\"cd MyAmazingProject.Web && dotnet publish MyAmazingProject.Web.csproj --configuration Release\\"\\n  }\\n}\\n</johnny_reilly@hotmail.com>\\n```\\n\\nIt doesn\'t matter if I have \\"an undefined variable named \u2018Parameters.RestoreBuildProjects\u2019\\". I now have no need to use all the individual tasks in a build. I can convert them into a couple of scripts in my `package.json`. So here\'s where I\'ve ended up for now. I\'ve a `.vsts-ci.yml` file which looks like this:\\n\\n```yml\\nqueue: Hosted VS2017\\n\\nsteps:\\n  - task: geeklearningio.gl-vsts-tasks-yarn.yarn-installer-task.YarnInstaller@2\\n    displayName: install yarn itself\\n    inputs:\\n      checkLatest: true\\n  - task: geeklearningio.gl-vsts-tasks-yarn.yarn-task.Yarn@2\\n    displayName: yarn build and test\\n    inputs:\\n      Arguments: build\\n  - task: geeklearningio.gl-vsts-tasks-yarn.yarn-task.Yarn@2\\n    displayName: yarn publish:web\\n    inputs:\\n      Arguments: \'run publish:web --output $(build.artifactstagingdirectory)/MyAmazingProject\'\\n  - task: PublishBuildArtifacts@1\\n    displayName: publish build artifact\\n    inputs:\\n      PathtoPublish: \'$(build.artifactstagingdirectory)\'\\n```\\n\\nThis file does the following:\\n\\n1. Installs yarn. (By the way VSTS, what\'s with not having yarn installed by default? I\'ll say this for the avoidance of doubt: in the npm cli space: yarn has won.)\\n2. Install our dependencies, build the front end and back end, run all the tests. Effectively `yarn build`.\\n3. Publish our web app to a directory. Effectively `yarn run publish:web`. This is only separate because we want to pass in the output directory and so it\'s just easier for it to be a separate step.\\n4. Publish the build artefact to TFS. (This will go on to be picked up by the continuous deployment mechanism and published out to Azure.)\\n\\nI much prefer this to what I had before. I feel there\'s much more that can be done here as well. I\'m looking forward to the continuous deployment piece becoming scriptable too.\\n\\nThanks to Zoltan and props to the TFVS team!"},{"id":"/2018/05/13/compromising-guide-for-developers","metadata":{"permalink":"/2018/05/13/compromising-guide-for-developers","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2018-05-13-compromising-guide-for-developers/index.md","source":"@site/blog/2018-05-13-compromising-guide-for-developers/index.md","title":"Compromising: A Guide for Developers","description":"It is a truth universally acknowledged, that a single developer, will not be short of an opinion. Opinions on tabs vs spaces. Upon OOP vs FP. Upon classes vs functions. Just opinions, opinions, opinions. Opinions that are felt with all the sincerity of a Witchfinder General. And, alas, not always the same level of empathy.","date":"2018-05-13T00:00:00.000Z","formattedDate":"May 13, 2018","tags":[{"label":"compromise","permalink":"/tags/compromise"},{"label":"empathy","permalink":"/tags/empathy"},{"label":"developers","permalink":"/tags/developers"},{"label":"code style","permalink":"/tags/code-style"},{"label":"teams","permalink":"/tags/teams"}],"readingTime":2.915,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Compromising: A Guide for Developers","authors":"johnnyreilly","tags":["compromise","empathy","developers","code style","teams"],"hide_table_of_contents":false},"prevItem":{"title":"VSTS... YAML up!","permalink":"/2018/06/16/vsts-yaml-up"},"nextItem":{"title":"Using Reflection to Identify Unwanted Dependencies","permalink":"/2018/04/28/using-reflection-to-identify-unwanted-dependencies"}},"content":"It is a truth universally acknowledged, that a single developer, will not be short of an opinion. Opinions on tabs vs spaces. Upon OOP vs FP. Upon `class`es vs `function`s. Just opinions, opinions, opinions. Opinions that are felt with all the sincerity of a Witchfinder General. And, alas, not always the same level of empathy.\\n\\nGiven the wealth of strongly felt desires, it\'s kind of amazing that developers ever manage to work together. It\'s rare to find a fellow dev that agrees entirely with your predilections. So how do people ever get past the \\"you don\'t use semi-colons; what\'s wrong with you\\"? Well, not easily to be honest. It involves compromise.\\n\\n## On Compromise\\n\\nWe\'ve all been in the position where we realise that there\'s something we don\'t like in a codebase. The ordering of members in a `class`, naming conventions, a lack of tests... Something.\\n\\nThen comes the moment of trepidation. You suggest a change. You suggest difference. It\'s time to find out if you\'re working with psychopaths. It\'s not untypical to find that you just have to go with the flow.\\n\\n- \\"You\'ve been using 3 spaces?\\"\\n- \\"Yes we use 3 spaces.\\"\\n- \\"Okay... So we\'ll be using 3 spaces...\\" [backs away carefully]\\n\\nI\'ve been in this position so many times I\'ve learned to adapt. It helps that I\'m a malleable sort anyway. But what if there were another way?\\n\\n## Weighting Opinion\\n\\nSometimes your opinion is... Well.... Just an opinion. Other opinions are legitimate. At least in theory. If you can acknowledge that, you already have a level of self knowledge not gifted to all in the dev community. If you\'re able to get that far I feel there\'s something you might want to consider.\\n\\nLet me frame this up: there\'s a choice to be made around an approach that could be used in a codebase. There are 2 camps in the team; 1 camp advocating for 1 approach. The other for a different approach. Either one is functionally legitimate. They work. It\'s just a matter of preference of choice. How do you choose now? Let\'s look at a technique for splitting the difference.\\n\\nVoting helps. But let\'s say 50% of the team wants 1 approach and 50% wants the other. What then? Or, to take a more interesting idea, what say 25% want 1 approach and 75% want the other? If it\'s just 1 person, 1 vote then the 75% wins and that\'s it.\\n\\nBut before we all move on, let\'s consider another factor. How much do people care? What if the 25% are really, really invested in the choice they\'re advocating for and the 75% just have a mild preference? From that point forwards the 25% are likely going to be less happy. Maybe they\'ll even burn inside. They\'re certainly going to be less productive.\\n\\nIt\'s because of situations like this that weighting votes becomes useful. Out of 5, how much do you care? If one person cares \\"5 out of 5\\" and the other three are \\"1 out of 5\\".... Well go with the 25% It matters to them and that it matters to them should matter to you.\\n\\nI\'ll contend that rolling like this makes for more content, happier and more productive teams. Making strength of feeling a factor in choices reduces friction and increases the peace.\\n\\n![](Bestival_2008_Increase_the_Peace_banner.jpg)\\n\\nI\'ve only recently discovered this technique and I can\'t claim credit for it. I learned it from the awesome [Jamie McCrindle](https://twitter.com/foldr). I commend to you! Be happier!"},{"id":"/2018/04/28/using-reflection-to-identify-unwanted-dependencies","metadata":{"permalink":"/2018/04/28/using-reflection-to-identify-unwanted-dependencies","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2018-04-28-using-reflection-to-identify-unwanted-dependencies/index.md","source":"@site/blog/2018-04-28-using-reflection-to-identify-unwanted-dependencies/index.md","title":"Using Reflection to Identify Unwanted Dependencies","description":"I having a web app which is fairly complex. It\'s made up of services, controllers and all sorts of things. So far, so unremarkable. However, I needed to ensure that the controllers did not attempt to access the database via any of their dependencies. Or their dependencies, dependencies. Or their dependencies. You get my point.","date":"2018-04-28T00:00:00.000Z","formattedDate":"April 28, 2018","tags":[{"label":"reflection","permalink":"/tags/reflection"},{"label":"test","permalink":"/tags/test"},{"label":"dependencies","permalink":"/tags/dependencies"}],"readingTime":2.54,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Using Reflection to Identify Unwanted Dependencies","authors":"johnnyreilly","tags":["reflection","test","dependencies"],"hide_table_of_contents":false},"prevItem":{"title":"Compromising: A Guide for Developers","permalink":"/2018/05/13/compromising-guide-for-developers"},"nextItem":{"title":"It\'s Not Dead 2: mobx-react-devtools and the undead","permalink":"/2018/03/26/its-not-dead-2-mobx-react-devtools-and-the-undead"}},"content":"I having a web app which is fairly complex. It\'s made up of services, controllers and all sorts of things. So far, so unremarkable. However, I needed to ensure that the controllers did not attempt to access the database via any of their dependencies. Or their dependencies, dependencies. Or their dependencies. You get my point.\\n\\nThe why is not important here. What\'s significant is the idea of walking a dependency tree and identifying, via a reflection based test, when such unwelcome dependencies occur, and where.\\n\\nWhen they do occur the test should fail, like this:\\n\\n```sh\\n[xUnit.net 00:00:01.6766691]     My.Web.Tests.HousekeepingTests.My_Api_Controllers_do_not_depend_upon_the_database [FAIL]\\n[xUnit.net 00:00:01.6782295]       Expected dependsUponTheDatabase.Any() to be False because My.Api.Controllers.ThingyController depends upon the database through My.Data.Services.OohItsAService, but found True.\\n```\\n\\nWhat follows is an example of how you can accomplish this. It is exceedingly far from the most beautiful code I\'ve ever written. But it works. One reservation I have about it is that it doesn\'t use the Dependency Injection mechanism used at runtime (AutoFac). If I had more time I would amend the code to use that instead; it would become an easier test to read if I did. Also it would better get round the limitations of the code below. Essentially the approach relies on the assumption of there being 1 interface and 1 implementation. That\'s often not true in complex systems. But this is good enough to roll with for now.\\n\\n```cs\\nusing System;\\nusing System.Collections.Generic;\\nusing System.Linq;\\nusing System.Reflection;\\nusing FluentAssertions;\\nusing My.Data;\\nusing My.Web.Controllers;\\nusing Xunit;\\n\\nnamespace My.Web.Tests {\\n    public class OiYouThereGetOutTests {\\n        [Fact]\\n        public void My_Controllers_do_not_depend_upon_the_database() {\\n            var myConcreteTypes = GetMyAssemblies()\\n                .SelectMany(assembly => assembly.GetTypes())\\n                .ToArray();\\n\\n            var controllerTypes = typeof(My.Web.Startup).Assembly.GetTypes()\\n                .Where(myWebType =>\\n                    myWebType != typeof(Microsoft.AspNetCore.Mvc.Controller) &&\\n                    typeof(Microsoft.AspNetCore.Mvc.Controller).IsAssignableFrom(myWebType));\\n\\n            foreach (var controllerType in controllerTypes) {\\n                var allTheTypes = GetDependentTypes(controllerType, myConcreteTypes);\\n                allTheTypes.Count.Should().BeGreaterThan(0);\\n                var dependsUponTheDatabase = allTheTypes.Where(keyValue => keyValue.Key == typeof(MyDbContext));\\n                dependsUponTheDatabase.Any().Should().Be(false, because: $\\"{controllerType} depends upon the database through {string.Join(\\", \\", dependsUponTheDatabase.Select(dod => dod.Value))}\\");\\n            }\\n        }\\n\\n        private static Dictionary<Type, Type> GetDependentTypes(Type type, Type[] typesToCheck, Dictionary<Type, Type> typesSoFar = null) {\\n            var types = typesSoFar ?? new Dictionary<Type, Type>();\\n            foreach (var constructor in type.GetConstructors().Where(ctor => ctor.IsPublic)) {\\n                foreach (var parameter in constructor.GetParameters()) {\\n                    if (parameter.ParameterType.IsInterface) {\\n                        if (parameter.ParameterType.IsGenericType) {\\n                            foreach (var genericType in parameter.ParameterType.GenericTypeArguments) {\\n                                AddIfMissing(types, genericType, type);\\n                            }\\n                        } else {\\n                            var typesImplementingInterface = TypesImplementingInterface(parameter.ParameterType, typesToCheck);\\n                            foreach (var typeImplementingInterface in typesImplementingInterface) {\\n                                AddIfMissing(types, typeImplementingInterface, type);\\n                                AddIfMissing(types, GetDependentTypes(typeImplementingInterface, typesToCheck, types).Keys.ToList(), type);\\n                            }\\n                        }\\n                    } else {\\n                        AddIfMissing(types, parameter.ParameterType, type);\\n                        AddIfMissing(types, GetDependentTypes(parameter.ParameterType, typesToCheck, types).Keys.ToList(), type);\\n                    }\\n                }\\n            }\\n            return types;\\n        }\\n\\n        private static void AddIfMissing(Dictionary<Type, Type> types, Type typeToAdd, Type parentType) {\\n            if (!types.Keys.Contains(typeToAdd))\\n                types.Add(typeToAdd, parentType);\\n        }\\n\\n        private static void AddIfMissing(Dictionary<Type, Type> types, IList<Type> typesToAdd, Type parentType) {\\n            foreach (var typeToAdd in typesToAdd) {\\n                AddIfMissing(types, typeToAdd, parentType);\\n            }\\n        }\\n\\n        private static Type[] TypesImplementingInterface(Type interfaceType, Type[] typesToCheck) =>\\n            typesToCheck.Where(type => !type.IsInterface && interfaceType.IsAssignableFrom(type)).ToArray();\\n\\n        private static bool IsRealClass(Type testType) =>\\n            testType.IsAbstract == false &&\\n            testType.IsGenericType == false &&\\n            testType.IsGenericTypeDefinition == false &&\\n            testType.IsInterface == false;\\n\\n        private static Assembly[] GetMyAssemblies() =>\\n            AppDomain\\n            .CurrentDomain\\n            .GetAssemblies()\\n            // Not strictly necessary but it reduces the amount of types returned\\n            .Where(assembly => assembly.GetName().Name.StartsWith(\\"My\\"))\\n            .ToArray();\\n    }\\n}\\n```"},{"id":"/2018/03/26/its-not-dead-2-mobx-react-devtools-and-the-undead","metadata":{"permalink":"/2018/03/26/its-not-dead-2-mobx-react-devtools-and-the-undead","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2018-03-26-its-not-dead-2-mobx-react-devtools-and-the-undead/index.md","source":"@site/blog/2018-03-26-its-not-dead-2-mobx-react-devtools-and-the-undead/index.md","title":"It\'s Not Dead 2: mobx-react-devtools and the undead","description":"I spent today digging through our webpack 4 config trying to work out why a production bundle contained code like this:","date":"2018-03-26T00:00:00.000Z","formattedDate":"March 26, 2018","tags":[{"label":"uglifyjs","permalink":"/tags/uglifyjs"},{"label":"mobx","permalink":"/tags/mobx"},{"label":"dead code elimination","permalink":"/tags/dead-code-elimination"},{"label":"Webpack","permalink":"/tags/webpack"}],"readingTime":2.035,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"It\'s Not Dead 2: mobx-react-devtools and the undead","authors":"johnnyreilly","tags":["uglifyjs","mobx","dead code elimination","Webpack"],"hide_table_of_contents":false},"prevItem":{"title":"Using Reflection to Identify Unwanted Dependencies","permalink":"/2018/04/28/using-reflection-to-identify-unwanted-dependencies"},"nextItem":{"title":"Uploading Images to Cloudinary with the Fetch API","permalink":"/2018/03/25/uploading-images-to-cloudinary-with-fetch"}},"content":"I spent today digging through our webpack 4 config trying to work out why a production bundle contained code like this:\\n\\n```js\\nif(\\"production\\"!==e.env.NODE_ENV){//...\\n```\\n\\nMy expectation was that with webpack 4 and `\'mode\': \'production\'` this meant that behind the scenes all `process.env.NODE_ENV` statements should be converted to `\'production\'`. Subsequently Uglify would automatically get its groove on with the resulting `if(\\"production\\"!==\\"production\\") ...` and et voil\xe0!... Strip the dead code.\\n\\nIt seemed that was not the case. I was seeing (regrettably) undead code. And who here actually likes the undead?\\n\\n## Who Betrayed Me?\\n\\nMy beef was with webpack. It done did me wrong. Or... So I thought. webpack did nothing wrong. It is pure and good and unjustly complained about. It was my other love: [mobx](https://github.com/mobxjs/mobx). Or to be more specific: [mobx-react-devtools](https://github.com/mobxjs/mobx-react-devtools).\\n\\nIt turns out that the way you use `mobx-react-devtools` reliably makes the difference. It\'s the cause of the stray `(\\"production\\"!==e.env.NODE_ENV)` statements in our bundle output. After a **long** time I happened upon [this issue](https://github.com/mobxjs/mobx-react-devtools/issues/66#issuecomment-365151531) which contained a gem by one [Giles Butler](https://github.com/gilesbutler). His suggested way to reference `mobx-react-devtools` is (as far as I can tell) the solution!\\n\\nOn a dummy project I had the `mobx-react-devtools` advised code in place:\\n\\n```js\\nimport * as React from \'react\';\\nimport { Layout } from \'./components/layout\';\\nimport DevTools from \'mobx-react-devtools\';\\n\\nexport const App: React.SFC<{}> = (_props) => (\\n  <div className=\\"ui container\\">\\n    <Layout />\\n    {process.env.NODE_ENV !== \'production\' ? (\\n      <DevTools position={{ bottom: 20, right: 20 }} />\\n    ) : null}\\n  </div>\\n);\\n```\\n\\nWith this I had a build size of 311kb. Closer examination of my bundle revealed that my `bundle.js` was riddled with `(\\"production\\"!==e.env.NODE_ENV)` statements. Sucks, right?\\n\\nThen I tried this instead:\\n\\n```js\\nimport * as React from \'react\';\\nimport { Layout } from \'./components/layout\';\\nconst { Fragment } = React;\\n\\nconst DevTools =\\n  process.env.NODE_ENV !== \'production\'\\n    ? require(\'mobx-react-devtools\').default\\n    : Fragment;\\n\\nexport const App: React.SFC<{}> = (_props) => (\\n  <div className=\\"ui container\\">\\n    <Layout />\\n    <DevTools position={{ bottom: 20, right: 20 }} />\\n  </div>\\n);\\n```\\n\\nWith this approach I got a build size of 191kb. This was thanks to the dead code being actually stripped. That\'s a saving of 120kb!\\n\\n## Perhaps We Change the Advice?\\n\\nThere\'s a suggestion that the README should be changed to reflect this advice - until that happens, I wanted to share this solution. Also, I\'ve a nagging feeling that I\'ve missed something pertinent here; if someone knows something that I should... Tell me please!"},{"id":"/2018/03/25/uploading-images-to-cloudinary-with-fetch","metadata":{"permalink":"/2018/03/25/uploading-images-to-cloudinary-with-fetch","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2018-03-25-uploading-images-to-cloudinary-with-fetch/index.md","source":"@site/blog/2018-03-25-uploading-images-to-cloudinary-with-fetch/index.md","title":"Uploading Images to Cloudinary with the Fetch API","description":"I was recently checking out a very good post which explained how to upload images using React Dropzone and SuperAgent to Cloudinary.","date":"2018-03-25T00:00:00.000Z","formattedDate":"March 25, 2018","tags":[{"label":"React Dropzone","permalink":"/tags/react-dropzone"},{"label":"Cloudinary","permalink":"/tags/cloudinary"},{"label":"Fetch API","permalink":"/tags/fetch-api"}],"readingTime":1.025,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Uploading Images to Cloudinary with the Fetch API","authors":"johnnyreilly","tags":["React Dropzone","Cloudinary","Fetch API"],"hide_table_of_contents":false},"prevItem":{"title":"It\'s Not Dead 2: mobx-react-devtools and the undead","permalink":"/2018/03/26/its-not-dead-2-mobx-react-devtools-and-the-undead"},"nextItem":{"title":"It\'s Not Dead: webpack and dead code elimination limitations","permalink":"/2018/03/07/its-not-dead-webpack-and-dead-code"}},"content":"I was recently checking out a [very good post](https://css-tricks.com/image-upload-manipulation-react/) which explained how to upload images using [React Dropzone](https://github.com/react-dropzone/react-dropzone) and [SuperAgent](https://github.com/visionmedia/superagent) to [Cloudinary](https://cloudinary.com/).\\n\\nIt\'s a brilliant post; you should totally read it. Even if you hate images, uploads and JavaScript. However, there was one thing in there that I didn\'t want; SuperAgent. It\'s lovely but I\'m a [Fetch](https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API) guy. That\'s just how I roll. The question is, how do I do the below using Fetch?\\n\\n```js\\nhandleImageUpload(file) {\\n    let upload = request.post(CLOUDINARY_UPLOAD_URL)\\n                     .field(\'upload_preset\', CLOUDINARY_UPLOAD_PRESET)\\n                     .field(\'file\', file);\\n\\n    upload.end((err, response) => {\\n      if (err) {\\n        console.error(err);\\n      }\\n\\n      if (response.body.secure_url !== \'\') {\\n        this.setState({\\n          uploadedFileCloudinaryUrl: response.body.secure_url\\n        });\\n      }\\n    });\\n  }\\n```\\n\\nWell it actually took me longer to work out than I\'d like to admit. But now I have, let me save you the bother. To do the above using Fetch you just need this:\\n\\n```js\\nhandleImageUpload(file) {\\n    const formData = new FormData();\\n    formData.append(\\"file\\", file);\\n    formData.append(\\"upload_preset\\", CLOUDINARY_UPLOAD_PRESET); // Replace the preset name with your own\\n\\n    fetch(CLOUDINARY_UPLOAD_URL, {\\n      method: \'POST\',\\n      body: formData\\n    })\\n      .then(response => response.json())\\n      .then(data => {\\n        if (data.secure_url !== \'\') {\\n          this.setState({\\n            uploadedFileCloudinaryUrl: data.secure_url\\n          });\\n        }\\n      })\\n      .catch(err => console.error(err))\\n  }\\n```\\n\\nTo get a pre-canned project to try this with take a look at [Damon\'s repo](https://github.com/damonbauer/react-cloudinary)."},{"id":"/2018/03/07/its-not-dead-webpack-and-dead-code","metadata":{"permalink":"/2018/03/07/its-not-dead-webpack-and-dead-code","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2018-03-07-its-not-dead-webpack-and-dead-code/index.md","source":"@site/blog/2018-03-07-its-not-dead-webpack-and-dead-code/index.md","title":"It\'s Not Dead: webpack and dead code elimination limitations","description":"Every now and then you can be surprised. Your assumptions turn out to be wrong.","date":"2018-03-07T00:00:00.000Z","formattedDate":"March 7, 2018","tags":[{"label":"webpack; dead code elimination; process.env.NODE_ENV; DefinePlugin","permalink":"/tags/webpack-dead-code-elimination-process-env-node-env-define-plugin"}],"readingTime":2.12,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"It\'s Not Dead: webpack and dead code elimination limitations","authors":"johnnyreilly","tags":["webpack; dead code elimination; process.env.NODE_ENV; DefinePlugin"],"hide_table_of_contents":false},"prevItem":{"title":"Uploading Images to Cloudinary with the Fetch API","permalink":"/2018/03/25/uploading-images-to-cloudinary-with-fetch"},"nextItem":{"title":"ts-loader 4 / fork-ts-checker-webpack-plugin 0.4","permalink":"/2018/02/25/ts-loader-400-fork-ts-checker-webpack"}},"content":"Every now and then you can be surprised. Your assumptions turn out to be wrong.\\n\\nWebpack has long supported the notion of dead code elimination. webpack facilitates this through use of the `DefinePlugin`. The compile time value of `process.env.NODE_ENV` is set either to `\'production\'` or something else. If it\'s set to `\'production\'` then some dead code hackery can happen. [Libraries like React make use of this to serve up different, and crucially smaller, production builds.](https://reactjs.org/docs/optimizing-performance.html#webpack)\\n\\nA (pre-webpack 4) production config file will typically contain this code:\\n\\n```js\\nnew webpack.DefinePlugin({\\n    \'process.env.NODE_ENV\': JSON.stringify(\'production\')\\n}),\\nnew UglifyJSPlugin(),\\n```\\n\\nThe result of the above config is that webpack will inject the value \'production\' everywhere in the codebase where a `process.env.NODE_ENV` can be found. (In fact, as of webpack 4 setting this magic value is out-of-the-box behaviour for Production mode; yay the #0CJS!)\\n\\nWhat this means is, if you\'ve written:\\n\\n```js\\nif (process.env.NODE_ENV !== \'production\') {\\n  // Do a development mode only thing\\n}\\n```\\n\\nwebpack can and will turn this into\\n\\n```js\\nif (\'production\' !== \'production\') {\\n  // Do a development mode only thing\\n}\\n```\\n\\nThe [UglifyJSPlugin](https://github.com/webpack-contrib/uglifyjs-webpack-plugin) is there to minify the JavaScript in your bundles. As an added benefit, this plugin is smart enough to know that `\'production\' !== \'production\'` is always `false`. And because it\'s smart, it chops the code. Dead code elimated.\\n\\nYou can read more about this [in the webpack docs](https://webpack.js.org/guides/production/#specify-the-environment).\\n\\n## Limitations\\n\\nGiven what I\'ve said, consider the following code:\\n\\n```js\\nexport class Config {\\n  // Other properties\\n\\n  get isDevelopment() {\\n    return process.env.NODE_ENV !== \'production\';\\n  }\\n}\\n```\\n\\nThis is a config class that exposes the expression `process.env.NODE_ENV !== \'production\'` with the friendly name `isDevelopment`. You\'d think that dead code elimination would be your friend here. It\'s not.\\n\\nMy personal expection was that dead code elimination would treat `Config.isDevelopment` and the expression `process.env.NODE_ENV !== \'production\'` identically. Because they\'re identical.\\n\\nHowever, this turns out not to be the case. Dead code elimination works just as you would hope when using the expression `process.env.NODE_ENV !== \'production\'` directly in code. However webpack **only** performs dead code elimination for the **direct** usage of the `process.env.NODE_ENV !== \'production\'` expression. I\'ll say that again: if you want dead code elimination then use the injected values; not an encapsulated version of them. It turns out you cannot rely on webpack flowing values through and performing dead code elimination on that basis.\\n\\nThe TL;DR: if you want to elimate dead code then \\\\*always\\\\* use `process.env.NODE_ENV !== \'production\'`; don\'t abstract it. It doesn\'t work.\\n\\nUglifyJS is smart. But not that smart."},{"id":"/2018/02/25/ts-loader-400-fork-ts-checker-webpack","metadata":{"permalink":"/2018/02/25/ts-loader-400-fork-ts-checker-webpack","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2018-02-25-ts-loader-400-fork-ts-checker-webpack/index.md","source":"@site/blog/2018-02-25-ts-loader-400-fork-ts-checker-webpack/index.md","title":"ts-loader 4 / fork-ts-checker-webpack-plugin 0.4","description":"webpack 4 has shipped!","date":"2018-02-25T00:00:00.000Z","formattedDate":"February 25, 2018","tags":[{"label":"webpack 4","permalink":"/tags/webpack-4"},{"label":"fork-ts-checker-webpack-plugin","permalink":"/tags/fork-ts-checker-webpack-plugin"},{"label":"ts-loader","permalink":"/tags/ts-loader"}],"readingTime":0.575,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"ts-loader 4 / fork-ts-checker-webpack-plugin 0.4","authors":"johnnyreilly","tags":["webpack 4","fork-ts-checker-webpack-plugin","ts-loader"],"hide_table_of_contents":false},"prevItem":{"title":"It\'s Not Dead: webpack and dead code elimination limitations","permalink":"/2018/03/07/its-not-dead-webpack-and-dead-code"},"nextItem":{"title":"Finding webpack 4 (use a Map)","permalink":"/2018/01/29/finding-webpack-4-use-map"}},"content":"webpack 4 has shipped!\\n\\n## `ts-loader`\\n\\nThe [`ts-loader`](https://github.com/TypeStrong/ts-loader) 4 is available too. For details see our release [here](https://github.com/TypeStrong/ts-loader/releases/tag/v4.0.0). To start using `ts-loader` 4:\\n\\n- When using `yarn`: `yarn add ts-loader@4.1.0 -D`\\n- When using `npm`: `npm install ts-loader@4.1.0 -D`\\n\\nRemember to use this in concert with the webpack 4. To see a working example take a look at [the \\"vanilla\\" example](https://github.com/johnnyreilly/ts-loader/tree/master/examples/vanilla).\\n\\n## `fork-ts-checker-webpack-plugin`\\n\\nThere\'s more! You may like to use the [`fork-ts-checker-webpack-plugin`](https://github.com/Realytics/fork-ts-checker-webpack-plugin), (aka the ts-loader turbo-booster). The webpack compatible version has been [released to npm as 0.4.1](https://github.com/Realytics/fork-ts-checker-webpack-plugin/releases/tag/v0.4.1):\\n\\n- When using `yarn`: `yarn add fork-ts-checker-webpack-plugin@0.4.1 -D`\\n- When using `npm`: `npm install fork-ts-checker-webpack-plugin@0.4.1 -D`\\n\\nTo see a working example take a look at [the \\"fork-ts-checker\\" example](https://github.com/johnnyreilly/ts-loader/tree/master/examples/fork-ts-checker)."},{"id":"/2018/01/29/finding-webpack-4-use-map","metadata":{"permalink":"/2018/01/29/finding-webpack-4-use-map","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2018-01-29-finding-webpack-4-use-map/index.md","source":"@site/blog/2018-01-29-finding-webpack-4-use-map/index.md","title":"Finding webpack 4 (use a Map)","description":"Update: 03/02/2018","date":"2018-01-29T00:00:00.000Z","formattedDate":"January 29, 2018","tags":[{"label":"webpack 4","permalink":"/tags/webpack-4"}],"readingTime":4.545,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Finding webpack 4 (use a Map)","authors":"johnnyreilly","tags":["webpack 4"],"hide_table_of_contents":false},"prevItem":{"title":"ts-loader 4 / fork-ts-checker-webpack-plugin 0.4","permalink":"/2018/02/25/ts-loader-400-fork-ts-checker-webpack"},"nextItem":{"title":"webpack 4 - ts-loader / fork-ts-checker-webpack-plugin betas","permalink":"/2018/01/28/webpack-4-ts-loader-fork-ts-checker"}},"content":"## Update: 03/02/2018\\n\\nTobias Koppers has written a migration guide for plugins / loaders as well - take a read [here](https://medium.com/webpack/webpack-4-migration-guide-for-plugins-loaders-20a79b927202). It\'s very useful.\\n\\n## webpack 4\\n\\nwebpack 4 is on the horizon. [The beta dropped last Friday](https://medium.com/webpack/webpack-4-beta-try-it-today-6b1d27d7d7e2). So what do you, as a plugin / loader author need to do? What needs to change to make your loader / plugin webpack 4 friendly?\\n\\nThis is a guide that should inform you about the changes you might need to make. It\'s based on my own experiences migrating [`ts-loader`](https://github.com/TypeStrong/ts-loader) and the [`fork-ts-checker-webpack-plugin`](https://github.com/Realytics/fork-ts-checker-webpack-plugin). If you\'d like to see this in action then take a look at the PRs related to these. The ts-loader PR can be found [here](https://github.com/TypeStrong/ts-loader/pull/710). The fork-ts-checker-webpack-plugin PR can be found [here](https://github.com/Realytics/fork-ts-checker-webpack-plugin/pull/93).\\n\\n## Plugins\\n\\nOne of the notable changes to webpack with v4 is the change to the plugin architecture. In terms of implications it\'s worth reading the comments made by [Tobias Koppers](https://twitter.com/wsokra)[here](https://github.com/webpack/webpack/issues/6244#issuecomment-357502113) and [here](https://github.com/webpack/webpack/issues/6064#issuecomment-349405474).\\n\\nPreviously, if your plugin was tapping into a compiler hook you\'d write code that looked something like this:\\n\\n```js\\nthis.compiler.plugin(\'watch-close\', () => {\\n  // do your thing here\\n});\\n```\\n\\nWith webpack 4 things done changed. You\'d now write something like this:\\n\\n```js\\nthis.compiler.hooks.watchClose.tap(\\n  \'name-to-identify-your-plugin-goes-here\',\\n  () => {\\n    // do your thing here\\n  }\\n);\\n```\\n\\nHopefully that\'s fairly clear; we\'re using the new `hooks` property and tapping into our event of choice by `camelCasing` what was previously `kebab-cased`. So in this case `plugin(\'watch-close\' =&gt; hooks.watchClose.tap`.\\n\\nIn the example above we were attaching to a sync hook. Now let\'s look at an async hook:\\n\\n```js\\nthis.compiler.plugin(\'watch-run\', (watching, callback) => {\\n  // do your thing here\\n  callback();\\n});\\n```\\n\\nThis would change to be:\\n\\n```js\\nthis.compiler.hooks.watchRun.tapAsync(\\n  \'name-to-identify-your-plugin-goes-here\',\\n  (compiler, callback) => {\\n    // do your thing here\\n    callback();\\n  }\\n);\\n```\\n\\nNote that rather than using `tap` here, we\'re using `tapAsync`. If you\'re more into promises there\'s a `tapPromise` you could use instead.\\n\\n## Custom Hooks\\n\\nPrior to webpack 4, you could use your own custom hooks within your plugin. Usage was as simple as this:\\n\\n```js\\nthis.compiler.applyPluginsAsync(\'fork-ts-checker-service-before-start\', () => {\\n  // do your thing here\\n});\\n```\\n\\nYou can still use custom hooks with webpack 4, but there\'s a little more ceremony involved. Essentially, you need to tell webpack up front what you\'re planning. Not hard, I promise you.\\n\\nFirst of all, you\'ll need to add the package [`tapable`](https://www.npmjs.com/package/tapable) as a dependency. Then, inside your plugin you\'ll need to import the type of hook that you want to use; in the case of the `fork-ts-checker-webpack-plugin` we used both a sync and an async hook:\\n\\n```js\\nconst AsyncSeriesHook = require(\'tapable\').AsyncSeriesHook;\\nconst SyncHook = require(\'tapable\').SyncHook;\\n```\\n\\nThen, inside your `apply` method you need to register your hooks:\\n\\n```js\\nif (\\n  this.compiler.hooks.forkTsCheckerServiceBeforeStart ||\\n  this.compiler.hooks.forkTsCheckerCancel ||\\n  // other hooks...\\n  this.compiler.hooks.forkTsCheckerEmit\\n) {\\n  throw new Error(\'fork-ts-checker-webpack-plugin hooks are already in use\');\\n}\\nthis.compiler.hooks.forkTsCheckerServiceBeforeStart = new AsyncSeriesHook([]);\\n\\nthis.compiler.hooks.forkTsCheckerCancel = new SyncHook([]);\\n// other sync hooks...\\nthis.compiler.hooks.forkTsCheckerDone = new SyncHook([]);\\n```\\n\\nIf you\'re interested in backwards compatibility then you should use the `_pluginCompat` to wire that in:\\n\\n```js\\nthis.compiler._pluginCompat.tap(\'fork-ts-checker-webpack-plugin\', (options) => {\\n  switch (options.name) {\\n    case \'fork-ts-checker-service-before-start\':\\n      options.async = true;\\n      break;\\n    case \'fork-ts-checker-cancel\':\\n    // other sync hooks...\\n    case \'fork-ts-checker-done\':\\n      return true;\\n  }\\n  return undefined;\\n});\\n```\\n\\nWith your registration in place, you just need to replace your calls to `compiler.applyPlugins(\'sync-hook-name\', ` and `compiler.applyPluginsAsync(\'async-hook-name\', ` with calls to `compiler.hooks.syncHookName.call(` and `compiler.hooks.asyncHookName.callAsync(`. So to migrate our `fork-ts-checker-service-before-start` hook we\'d write:\\n\\n```js\\nthis.compiler.hooks.forkTsCheckerServiceBeforeStart.callAsync(() => {\\n  // do your thing here\\n});\\n```\\n\\n## Loaders\\n\\nLoaders are impacted by the changes to the plugin architecture. Mostly this means applying the same plugin changes as discussed above. `ts-loader` hooks into 2 plugin events:\\n\\n```js\\nloader._compiler.plugin(\'after-compile\' /* callback goes here */);\\nloader._compiler.plugin(\'watch-run\' /* callback goes here */);\\n```\\n\\nWith webpack 4 these become:\\n\\n```js\\nloader._compiler.hooks.afterCompile.tapAsync(\\n  \'ts-loader\' /* callback goes here */\\n);\\nloader._compiler.hooks.watchRun.tapAsync(\'ts-loader\' /* callback goes here */);\\n```\\n\\nNote again, we\'re using the string `\\"ts-loader\\"` to identify our loader.\\n\\n## I need a `Map`\\n\\nWhen I initially ported to webpack 4, `ts-loader` simply wasn\'t working. In the end I tied this down to problems in our `watch-run` callback. There\'s 2 things of note here.\\n\\nFirstly, as per [the changelog](https://github.com/webpack/webpack/releases/tag/v4.0.0-beta.0), the `watch-run` hook now has the `Compiler` as the first parameter. Previously this was a subproperty on the supplied `watching` parameter. So swapping over to use the compiler directly was necessary. Incidentally, `ts-loader` previously made use of the `watching.startTime` property that was supplied in webpack\'s 1, 2 and 3. It seems to be coping without it; so hopefully that\'s fine.\\n\\nSecondly, with webpack 4 it\'s \\"ES2015 all the things!\\" That is to say, with webpack now requiring a minimum of node 6, the codebase is free to start using ES2015. So if you\'re a consumer of `compiler.fileTimestamps` (and `ts-loader` is) then it\'s time to make a change to cater for the different API that a `Map` offers instead of indexing into an object literal with a `string` key.\\n\\nWhat this means is, code that would once have looked like this:\\n\\n```js\\nObject.keys(watching.compiler.fileTimestamps)\\n  .filter(\\n    (filePath) =>\\n      watching.compiler.fileTimestamps[filePath] > lastTimes[filePath]\\n  )\\n  .forEach((filePath) => {\\n    lastTimes[filePath] = times[filePath];\\n    // ...\\n  });\\n```\\n\\nNow looks more like this:\\n\\n```js\\nfor (const [filePath, date] of compiler.fileTimestamps) {\\n  if (date > lastTimes.get(filePath)) {\\n    continue;\\n  }\\n\\n  lastTimes.set(filePath, date);\\n  // ...\\n}\\n```\\n\\n## Happy Porting!\\n\\nI hope your own port to webpack 4 goes well. Do let me know if there\'s anything I\'ve missed out / any inaccuracies etc and I\'ll update this guide."},{"id":"/2018/01/28/webpack-4-ts-loader-fork-ts-checker","metadata":{"permalink":"/2018/01/28/webpack-4-ts-loader-fork-ts-checker","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2018-01-28-webpack-4-ts-loader-fork-ts-checker/index.md","source":"@site/blog/2018-01-28-webpack-4-ts-loader-fork-ts-checker/index.md","title":"webpack 4 - ts-loader / fork-ts-checker-webpack-plugin betas","description":"The first webpack 4 beta dropped on Friday. Very exciting! Following hot on the heels of those announcements, I\'ve some news to share too. Can you guess what it is?","date":"2018-01-28T00:00:00.000Z","formattedDate":"January 28, 2018","tags":[{"label":"fork-ts-checker-webpack-plugin","permalink":"/tags/fork-ts-checker-webpack-plugin"},{"label":"ts-loader","permalink":"/tags/ts-loader"},{"label":"Webpack","permalink":"/tags/webpack"}],"readingTime":0.985,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"webpack 4 - ts-loader / fork-ts-checker-webpack-plugin betas","authors":"johnnyreilly","tags":["fork-ts-checker-webpack-plugin","ts-loader","Webpack"],"hide_table_of_contents":false},"prevItem":{"title":"Finding webpack 4 (use a Map)","permalink":"/2018/01/29/finding-webpack-4-use-map"},"nextItem":{"title":"Auth0, TypeScript and ASP.NET Core","permalink":"/2018/01/14/auth0-typescript-and-aspnet-core"}},"content":"[The first webpack 4 beta dropped on Friday](https://medium.com/webpack/webpack-4-beta-try-it-today-6b1d27d7d7e2). Very exciting! Following hot on the heels of those announcements, I\'ve some news to share too. Can you guess what it is?\\n\\n## `ts-loader`\\n\\nYes! The [`ts-loader`](https://github.com/TypeStrong/ts-loader) beta to work with webpack 4 is available. To get hold of the beta:\\n\\n- When using `yarn`: `yarn add ts-loader@4.0.0-beta.0 -D`\\n- When using `npm`: `npm install ts-loader@4.0.0-beta.0 -D`\\n\\nRemember to use this in concert with the webpack 4 beta. To see a working example take a look at [the \\"vanilla\\" example](https://github.com/johnnyreilly/ts-loader/tree/master/examples/vanilla).\\n\\n## `fork-ts-checker-webpack-plugin`\\n\\nThere\'s more! You may like to use the [`fork-ts-checker-webpack-plugin`](https://github.com/Realytics/fork-ts-checker-webpack-plugin), (which goes lovely with `ts-loader` and a biscuit). There is a beta available for that too:\\n\\n- When using `yarn`: `yarn add johnnyreilly/fork-ts-checker-webpack-plugin#4.0.0-beta.1 -D`\\n- When using `npm`: `npm install johnnyreilly/fork-ts-checker-webpack-plugin#4.0.0-beta.1 -D`\\n\\nTo see a working example take a look at [the \\"fork-ts-checker\\" example](https://github.com/johnnyreilly/ts-loader/tree/master/examples/fork-ts-checker).\\n\\n## PRs\\n\\nIf you would like to track the progress of these betas then I encourage you to take a look at the PRs they were built from. The ts-loader PR can be found [here](https://github.com/TypeStrong/ts-loader/pull/710). The fork-ts-checker-webpack-plugin PR can be found [here](https://github.com/Realytics/fork-ts-checker-webpack-plugin/pull/93).\\n\\nThese are betas so things may change further; though hopefully not significantly."},{"id":"/2018/01/14/auth0-typescript-and-aspnet-core","metadata":{"permalink":"/2018/01/14/auth0-typescript-and-aspnet-core","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2018-01-14-auth0-typescript-and-aspnet-core/index.md","source":"@site/blog/2018-01-14-auth0-typescript-and-aspnet-core/index.md","title":"Auth0, TypeScript and ASP.NET Core","description":"Most applications I write have some need for authentication and perhaps authorisation too. In fact, most apps most people write fall into that bracket. Here\'s the thing: Auth done well is a \\\\big\\\\ chunk of work. And the minute you start thinking about that you almost invariably lose focus on the thing you actually want to build and ship.","date":"2018-01-14T00:00:00.000Z","formattedDate":"January 14, 2018","tags":[{"label":"ASP.Net Core","permalink":"/tags/asp-net-core"},{"label":"Auth0","permalink":"/tags/auth-0"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"OAuth","permalink":"/tags/o-auth"},{"label":"React","permalink":"/tags/react"}],"readingTime":9.36,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Auth0, TypeScript and ASP.NET Core","authors":"johnnyreilly","tags":["ASP.Net Core","Auth0","TypeScript","OAuth","React"],"hide_table_of_contents":false},"prevItem":{"title":"webpack 4 - ts-loader / fork-ts-checker-webpack-plugin betas","permalink":"/2018/01/28/webpack-4-ts-loader-fork-ts-checker"},"nextItem":{"title":"ts-loader 2017 retrospective","permalink":"/2017/12/24/ts-loader-2017-retrospective"}},"content":"Most applications I write have some need for authentication and perhaps authorisation too. In fact, most apps most people write fall into that bracket. Here\'s the thing: Auth done well is a \\\\*big\\\\* chunk of work. And the minute you start thinking about that you almost invariably lose focus on the thing you actually want to build and ship.\\n\\nSo this Christmas I decided it was time to take a look into offloading that particular problem onto someone else. I knew there were third parties who provided Auth-As-A-Service - time to give them a whirl. On the recommendation of a friend, I made Auth0 my first port of call. Lest you be expecting a full breakdown of the various players in this space, let me stop you now; I liked Auth0 so much I strayed no further. Auth0 kicks AAAS. (I\'m so sorry)\\n\\n## What I wanted to build\\n\\nMy criteria for \\"auth success\\" was this:\\n\\n- I want to build a SPA, specifically a React SPA. Ideally, I shouldn\'t need a back end of my own at all\\n- I want to use TypeScript on my client.\\n\\nBut, for when I do implement a back end:\\n\\n- I want that to be able to use the client side\'s Auth tokens to allow access to Auth routes on my server.\\n- \u200eI want to able to identify the user, given the token, to provide targeted data\\n- Oh, and I want to use .NET Core 2 for my server.\\n\\nAnd in achieving all of the I want to add minimal code to my app. Not War and Peace. My code should remain focused on doing what it does.\\n\\n## Boil a Plate\\n\\nI ended up with unqualified ticks for all my criteria, but it took some work to find out. I will say that Auth0 do travel the extra mile in terms of getting you up and running. When you create a new Client in Auth0 you\'re given the option to download a quick start using the technology of your choice.\\n\\nThis was a massive plus for me. I took the quickstart provided and ran with it to get me to the point of meeting my own criteria. You can use this boilerplate for your own ends. Herewith, a walkthrough:\\n\\n## The Walkthrough\\n\\nFork and clone the repo at this location: [https://github.com/johnnyreilly/auth0-react-typescript-asp-net-core](https://github.com/johnnyreilly/auth0-react-typescript-asp-net-core).\\n\\nWhat have we got? 2 folders, ClientApp contains the React app, Web contains the ASP.NET Core app. Now we need to get setup with Auth0 and customise our config.\\n\\n## Setup Auth0\\n\\nHere\'s how to get the app set up with Auth0; you\'re going to need to sign up for a (free) Auth0 account. Then login into Auth0 and go to the management portal.\\n\\n### Client\\n\\n- Create a Client with the name of your choice and use the Single Page Web Applications template.\\n- From the new Client Settings page take the Domain and Client ID and update the similarly named properties in the `appsettings.Development.json` and `appsettings.Production.json` files with these settings.\\n- To the Allowed Callback URLs setting add the URLs: `http://localhost:3000/callback,http://localhost:5000/callback` \\\\- the first of these faciliates running in Debug mode, the second in Production mode. If you were to deploy this you\'d need to add other callback URLs in here too.\\n\\n### API\\n\\n- Create an API with the name of your choice (I recommend the same as the Client to avoid confusion), an identifier which can be anything you like; I like to use the URL of my app but it\'s your call.\\n- From the new API Settings page take the Identifier and update the Audience property in the `appsettings.Development.json` and `appsettings.Production.json` files with that value.\\n\\n## Running the App\\n\\n### Production build\\n\\nBuild the client app with `yarn build` in the `ClientApp` folder. (Don\'t forget to `yarn install` first.) Then, in the `Web` folder `dotnet restore`, `dotnet run` and open your browser to [`http://localhost:5000`](http://localhost:5000)\\n\\n### Debugging\\n\\nRun the client app using webpack-dev-server using `yarn start` in the `ClientApp` folder. Fire up VS Code in the root of the repo and hit F5 to debug the server. Then open your browser to [`http://localhost:3000`](http://localhost:3000)\\n\\n## The Tour\\n\\nWhen you fire up the app you\'re presented with \\"you are not logged in!\\" message and the option to login. Do it, it\'ll take you to the Auth0 \\"lock\\" screen where you can sign up / login. Once you do that you\'ll be asked to confirm access:\\n\\n![](Screenshot-2018-01-13-18.40.21.png)\\n\\nAll this is powered by Auth0\'s [auth0-js](https://www.npmjs.com/package/auth0-js) npm package. (Excellent type definition files are available from Definitely Typed; I\'m using the [@types/auth0-js](https://www.npmjs.com/package/@types/auth0-js) package DT publishes.) Usage of which is super simple; it exposes an `authorize` method that when called triggers the Auth0 lock screen. Once you\'ve \\"okayed\\" you\'ll be taken back to the app which will use the `parseHash` method to extract the access token that Auth0 has provided. Take a look at how our `authStore` makes use of auth0-js: (don\'t be scared; it uses mobx - but you could use anything)\\n\\n### authStore.ts\\n\\n```ts\\nimport { Auth0UserProfile, WebAuth } from \'auth0-js\';\\nimport { action, computed, observable, runInAction } from \'mobx\';\\nimport { IAuth0Config } from \'../../config\';\\nimport { StorageFacade } from \'../storageFacade\';\\n\\ninterface IStorageToken {\\n  accessToken: string;\\n  idToken: string;\\n  expiresAt: number;\\n}\\n\\nconst STORAGE_TOKEN = \'storage_token\';\\n\\nexport class AuthStore {\\n  @observable.ref auth0: WebAuth;\\n  @observable.ref userProfile: Auth0UserProfile;\\n  @observable.ref token: IStorageToken;\\n\\n  constructor(config: IAuth0Config, private storage: StorageFacade) {\\n    this.auth0 = new WebAuth({\\n      domain: config.domain,\\n      clientID: config.clientId,\\n      redirectUri: config.redirectUri,\\n      audience: config.audience,\\n      responseType: \'token id_token\',\\n      scope: \'openid email profile do:admin:thing\', // the do:admin:thing scope is custom and defined in the scopes section of our API in the Auth0 dashboard\\n    });\\n  }\\n\\n  initialise() {\\n    const token = this.parseToken(this.storage.getItem(STORAGE_TOKEN));\\n    if (token) {\\n      this.setSession(token);\\n    }\\n    this.storage.addEventListener(this.onStorageChanged);\\n  }\\n\\n  parseToken(tokenString: string) {\\n    const token = JSON.parse(tokenString || \'{}\');\\n    return token;\\n  }\\n\\n  onStorageChanged = (event: StorageEvent) => {\\n    if (event.key === STORAGE_TOKEN) {\\n      this.setSession(this.parseToken(event.newValue));\\n    }\\n  };\\n\\n  @computed get isAuthenticated() {\\n    // Check whether the current time is past the\\n    // access token\'s expiry time\\n    return this.token && new Date().getTime() < this.token.expiresAt;\\n  }\\n\\n  login = () => {\\n    this.auth0.authorize();\\n  };\\n\\n  handleAuthentication = () => {\\n    this.auth0.parseHash((err, authResult) => {\\n      if (authResult && authResult.accessToken && authResult.idToken) {\\n        const token = {\\n          accessToken: authResult.accessToken,\\n          idToken: authResult.idToken,\\n          // Set the time that the access token will expire at\\n          expiresAt: authResult.expiresIn * 1000 + new Date().getTime(),\\n        };\\n\\n        this.setSession(token);\\n      } else if (err) {\\n        // tslint:disable-next-line:no-console\\n        console.log(err);\\n        alert(`Error: ${err.error}. Check the console for further details.`);\\n      }\\n    });\\n  };\\n\\n  @action\\n  setSession(token: IStorageToken) {\\n    this.token = token;\\n    this.storage.setItem(STORAGE_TOKEN, JSON.stringify(token));\\n  }\\n\\n  getAccessToken = () => {\\n    const accessToken = this.token.accessToken;\\n    if (!accessToken) {\\n      throw new Error(\'No access token found\');\\n    }\\n    return accessToken;\\n  };\\n\\n  @action\\n  loadProfile = async () => {\\n    const accessToken = this.token.accessToken;\\n    if (!accessToken) {\\n      return;\\n    }\\n\\n    this.auth0.client.userInfo(accessToken, (err, profile) => {\\n      if (err) {\\n        throw err;\\n      }\\n\\n      if (profile) {\\n        runInAction(() => (this.userProfile = profile));\\n        return profile;\\n      }\\n\\n      return undefined;\\n    });\\n  };\\n\\n  @action\\n  logout = () => {\\n    // Clear access token and ID token from local storage\\n    this.storage.removeItem(STORAGE_TOKEN);\\n\\n    this.token = null;\\n    this.userProfile = null;\\n  };\\n}\\n```\\n\\nOnce you\'re logged in the app offers you more in the way of navigation options. A \\"Profile\\" screen shows you the details your React app has retrieved from Auth0 about you. This is backed by the `client.userInfo` method on `auth0-js`. There\'s also a \\"Ping\\" screen which is where your React app talks to your ASP.NET Core server. The screenshot below illustrates the result of hitting the \\"Get Private Data\\" button:\\n\\n![](Screenshot-2018-01-13-18.47.49.png)\\n\\nThe \\"Get Server to Retrieve Profile Data\\" button is interesting as it illustrates that the server can get access to your profile data as well. There\'s nothing insecure here; it gets the details using the access token retrieved from Auth0 by the ClientApp and passed to the server. It\'s the API we set up in Auth0 that is in play here. The app uses the Domain and the access token to talk to Auth0 like so:\\n\\n### UserController.cs\\n\\n```cs\\n// Retrieve the access_token claim which we saved in the OnTokenValidated event\\n    var accessToken = User.Claims.FirstOrDefault(c => c.Type == \\"access_token\\").Value;\\n\\n    // If we have an access_token, then retrieve the user\'s information\\n    if (!string.IsNullOrEmpty(accessToken))\\n    {\\n        var domain = _config[\\"Auth0:Domain\\"];\\n        var apiClient = new AuthenticationApiClient(domain);\\n        var userInfo = await apiClient.GetUserInfoAsync(accessToken);\\n\\n        return Ok(userInfo);\\n    }\\n```\\n\\nWe can also access the `sub` claim, which uniquely identifies the user:\\n\\n### UserController.cs\\n\\n```cs\\n// We\'re not doing anything with this, but hey! It\'s useful to know where the user id lives\\n    var userId = User.Claims.FirstOrDefault(c => c.Type == System.Security.Claims.ClaimTypes.NameIdentifier).Value; // our userId is the sub value\\n```\\n\\nThe reason our ASP.NET Core app works with Auth0 and that we have access to the access token here in the first place is because of our startup code:\\n\\n### Startup.cs\\n\\n```cs\\npublic void ConfigureServices(IServiceCollection services)\\n    {\\n        var domain = $\\"https://{Configuration[\\"Auth0:Domain\\"]}/\\";\\n        services.AddAuthentication(options =>\\n        {\\n            options.DefaultAuthenticateScheme = JwtBearerDefaults.AuthenticationScheme;\\n            options.DefaultChallengeScheme = JwtBearerDefaults.AuthenticationScheme;\\n        }).AddJwtBearer(options =>\\n        {\\n            options.Authority = domain;\\n            options.Audience = Configuration[\\"Auth0:Audience\\"];\\n            options.Events = new JwtBearerEvents\\n            {\\n                OnTokenValidated = context =>\\n                {\\n                    if (context.SecurityToken is JwtSecurityToken token)\\n                    {\\n                        if (context.Principal.Identity is ClaimsIdentity identity)\\n                        {\\n                            identity.AddClaim(new Claim(\\"access_token\\", token.RawData));\\n                        }\\n                    }\\n\\n                    return Task.FromResult(0);\\n                }\\n            };\\n        });\\n\\n        // ....\\n```\\n\\n## Authorization\\n\\nWe\'re pretty much done now; just one magic button to investigate: \\"Get Admin Data\\". If you presently try and access the admin data you\'ll get a `403 Forbidden`. It\'s forbidden because that endpoint relies on the `\\"do:admin:thing\\"` scope in our claims:\\n\\n### UserController.cs\\n\\n```cs\\n[Authorize(Scopes.DoAdminThing)]\\n    [HttpGet(\\"api/userDoAdminThing\\")]\\n    public IActionResult GetUserDoAdminThing()\\n    {\\n        return Ok(\\"Admin endpoint\\");\\n    }\\n```\\n\\n### Scopes.cs\\n\\n```cs\\npublic static class Scopes\\n    {\\n         // the do:admin:thing scope is custom and defined in the scopes section of our API in the Auth0 dashboard\\n        public const string DoAdminThing = \\"do:admin:thing\\";\\n    }\\n```\\n\\nThis wired up in our ASP.NET Core app like so:\\n\\n### Startup.cs\\n\\n```cs\\nservices.AddAuthorization(options =>\\n    {\\n        options.AddPolicy(Scopes.DoAdminThing, policy => policy.Requirements.Add(new HasScopeRequirement(Scopes.DoAdminThing, domain)));\\n    });\\n\\n    // register the scope authorization handler\\n    services.AddSingleton<iauthorizationhandler, hasscopehandler=\\"\\">();\\n</iauthorizationhandler,>\\n```\\n\\n### HasScopeHandler.cs\\n\\n```cs\\npublic class HasScopeHandler : AuthorizationHandler<hasscoperequirement>\\n    {\\n        protected override Task HandleRequirementAsync(AuthorizationHandlerContext context, HasScopeRequirement requirement)\\n        {\\n            // If user does not have the scope claim, get out of here\\n            if (!context.User.HasClaim(c => c.Type == \\"scope\\" && c.Issuer == requirement.Issuer))\\n                return Task.CompletedTask;\\n\\n            // Split the scopes string into an array\\n            var scopes = context.User.FindFirst(c => c.Type == \\"scope\\" && c.Issuer == requirement.Issuer).Value.Split(\' \');\\n\\n            // Succeed if the scope array contains the required scope\\n            if (scopes.Any(s => s == requirement.Scope))\\n                context.Succeed(requirement);\\n\\n            return Task.CompletedTask;\\n        }\\n    }\\n</hasscoperequirement>\\n```\\n\\nThe reason we\'re 403ing at present is because when our `HasScopeHandler` executes, `requirement.Scope` has the value of `\\"do:admin:thing\\"` and our `scopes` do not contain that value. To add it, go to your API in the Auth0 management console and add it:\\n\\n![](Screenshot-2018-01-14-08.26.54.png)\\n\\nNote that you can control how this scope is acquired using \\"Rules\\" in the Auth0 management portal.\\n\\nYou won\'t be able to access the admin endpoint yet because you\'re still rocking with the old access token; pre-newly-added scope. But when you next login to Auth0 you\'ll see a prompt like this:\\n\\n![](Screenshot-2018-01-14-08.32.59.png)\\n\\nWhich demonstrates that you\'re being granted an extra scope. With your new shiny access token you can now access the oh-so-secret Admin endpoint.\\n\\nI had some more questions about Auth0 as I\'m still new to it myself. To see my question (and the very helpful answer!) go here: [https://community.auth0.com/questions/13786/get-user-data-server-side-what-is-a-good-approach](https://community.auth0.com/questions/13786/get-user-data-server-side-what-is-a-good-approach)"},{"id":"/2017/12/24/ts-loader-2017-retrospective","metadata":{"permalink":"/2017/12/24/ts-loader-2017-retrospective","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2017-12-24-ts-loader-2017-retrospective/index.md","source":"@site/blog/2017-12-24-ts-loader-2017-retrospective/index.md","title":"ts-loader 2017 retrospective","description":"2017 is drawing to a close, and it\'s been a big, big year in webpack-land. It\'s been a big year for ts-loader too. At the start of the year v1.3.3 was the latest version available, officially supporting webpack 1. (Old school!) We end the year with ts-loader sitting pretty at v3.2.0 and supporting webpack 2 and 3.","date":"2017-12-24T00:00:00.000Z","formattedDate":"December 24, 2017","tags":[{"label":"Die Hard","permalink":"/tags/die-hard"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"ts-loader","permalink":"/tags/ts-loader"},{"label":"Webpack","permalink":"/tags/webpack"}],"readingTime":2.945,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"ts-loader 2017 retrospective","authors":"johnnyreilly","tags":["Die Hard","TypeScript","ts-loader","Webpack"],"hide_table_of_contents":false},"prevItem":{"title":"Auth0, TypeScript and ASP.NET Core","permalink":"/2018/01/14/auth0-typescript-and-aspnet-core"},"nextItem":{"title":"The TypeScript webpack PWA","permalink":"/2017/11/19/the-typescript-webpack-pwa"}},"content":"2017 is drawing to a close, and it\'s been a big, big year in webpack-land. It\'s been a big year for `ts-loader` too. At the start of the year v1.3.3 was the latest version available, officially supporting webpack 1. (Old school!) We end the year with `ts-loader` sitting pretty at v3.2.0 and supporting webpack 2 and 3.\\n\\nMany releases were shipped and that was down to a whole bunch of folk. People helped out with bug fixes, features, advice and docs improvements. **All of these help.**`ts-loader` wouldn\'t be where it is without you so thanks to everyone that helped out - you rock!\\n\\n![](https://avatars.githubusercontent.com/christiantinauer)\\n\\n![](https://avatars.githubusercontent.com/Pajn)\\n\\n![](https://avatars.githubusercontent.com/maier49)\\n\\n![](https://avatars.githubusercontent.com/false)\\n\\n![](https://avatars.githubusercontent.com/roddypratt)\\n\\n![](https://avatars.githubusercontent.com/ldrick)\\n\\n![](https://avatars.githubusercontent.com/mattlewis92)\\n\\n![](https://avatars.githubusercontent.com/Venryx)\\n\\n![](https://avatars.githubusercontent.com/WillMartin)\\n\\n![](https://avatars.githubusercontent.com/Loilo)\\n\\n![](https://avatars.githubusercontent.com/Brooooooklyn)\\n\\n![](https://avatars.githubusercontent.com/mengxy)\\n\\n![](https://avatars.githubusercontent.com/bsouthga)\\n\\n![](https://avatars.githubusercontent.com/zinserjan)\\n\\n![](https://avatars.githubusercontent.com/sokra)\\n\\n![](https://avatars.githubusercontent.com/vhqtvn)\\n\\n![](https://avatars.githubusercontent.com/HerringtonDarkholme)\\n\\n![](https://avatars.githubusercontent.com/johnnyreilly)\\n\\n![](https://avatars.githubusercontent.com/jbrantly)\\n\\n![](https://avatars.githubusercontent.com/octref)\\n\\n![](https://avatars.githubusercontent.com/rhyek)\\n\\n![](https://avatars.githubusercontent.com/develar)\\n\\n![](https://avatars.githubusercontent.com/donaldpipowitch)\\n\\n![](https://avatars.githubusercontent.com/schmuli)\\n\\n![](https://avatars.githubusercontent.com/longlho)\\n\\n![](https://avatars.githubusercontent.com/Igorbek)\\n\\n![](https://avatars.githubusercontent.com/aindlq)\\n\\n![](https://avatars.githubusercontent.com/wearymonkey)\\n\\n![](https://avatars.githubusercontent.com/bancek)\\n\\n![](https://avatars.githubusercontent.com/mredbishop)\\n\\nI\'m really grateful to all of you. Thanks so much! (Apologies for those I\'ve missed anyone out - I know there\'s more still.)\\n\\n## `fork-ts-checker-webpack-plugin` build speed improvements\\n\\nAlongside other\'s direct contributions to `ts-loader`, other projects improved the experience of using `ts-loader`. [Piotr Ole\u015b](https://github.com/piotr-oles) dropped his [`fork-ts-checker-webpack-plugin`](https://github.com/Realytics/fork-ts-checker-webpack-plugin) this year which nicely increased build speed when used with `ts-loader`.\\n\\nThat opened up the possibility of adding [HappyPack](https://github.com/amireh/happypack) support. I had the good fortune to work with webpack\'s [Tobias Koppers](https://github.com/sokra) and ExtraHop\'s [Alex Birmingham](https://github.com/abirmingham) on [improving TypeScript build speed further](https://www.extrahop.com/company/blog/2017/extrahop-webpack-accelerating-build-times/).\\n\\nSo what does the future hold?\\n\\n## ts-loader 4.0 (Live webpack or Die Hard)\\n\\nThe web marches on and webpack gallops alongside. Here\'s what\'s in the pipeline for ts-loader in 2018:\\n\\n### Start using the new watch API\\n\\n[A new watch API is being made available in the TypeScript API](https://github.com/Microsoft/TypeScript/pull/20234). We have [a PR](https://github.com/TypeStrong/ts-loader/pull/685) from the amazing [Sheetal Nandi](https://github.com/sheetalkamat) which adds support to ts-loader. Given that\'s quite a big PR we want to merge that before anything else lands. The watch API is still being finalised but once it lands in TypeScript we\'ll look to merge the PR and ship a new version of `ts-loader`.\\n\\n### Drop custom module resolution\\n\\nHistorically `ts-loader` has had it\'s own module resolution mechanism in place. We\'re going to look to move to use the TypeScript mechanism instead. The old module resolution be deprecated but will remain available behind a flag for a time. In future we\'ll look to drop the old mechanism entirely.\\n\\n### Drop support for TypeScript 2.3 and below\\n\\nThe codebase can be made simpler if we drop support for older versions of TypeScript so that\'s what we plan to do with our next breaking changes release.\\n\\n### webpack v4 is in alpha now\\n\\nIf any changes need to happen to ts-loader to support webpack 4 then they will be. Personally I\'m planning to help out with [`fork-ts-checker-webpack-plugin`](https://github.com/Realytics/fork-ts-checker-webpack-plugin) as there will likely be some changes required there.\\n\\n### `contextAsConfigBasePath` will be replaced with a `context`\\n\\nThe option that landed in the last month doesn\'t quite achieve the aims of the original PR\'s author [Christian Tinauer](https://github.com/christiantinauer). Consequently it\'s going to be replaced with a new option. This is queued up and ready to go [here](https://github.com/TypeStrong/ts-loader/pull/688).\\n\\n### `reportFiles` option to be added\\n\\n[Michel Rasschaert](https://github.com/freeman) is presently working on adding a `reportFiles` option to `ts-loader`. You can see the PR in progress [here](https://github.com/TypeStrong/ts-loader/pull/701).\\n\\n## Merry Christmas!\\n\\nYou can expect to see the first releases of ts-loader 4.0 in 2018. In the meantime, I\'d like to wish you Merry Christmas and a Happy New Year! And once more, thanks and thanks again to all you generous people who help build `ts-loader`. You\'re wonderful and so I\'m glad you do what you do... joyeux Noel!"},{"id":"/2017/11/19/the-typescript-webpack-pwa","metadata":{"permalink":"/2017/11/19/the-typescript-webpack-pwa","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2017-11-19-the-typescript-webpack-pwa/index.md","source":"@site/blog/2017-11-19-the-typescript-webpack-pwa/index.md","title":"The TypeScript webpack PWA","description":"So, there you sit, conflicted. You\'ve got a lovely build setup; it\'s a thing of beauty. Precious, polished like a diamond, sharpened like a circular saw. There at the core of your carefully crafted setup sits webpack. Heaving, mysterious... powerful.","date":"2017-11-19T00:00:00.000Z","formattedDate":"November 19, 2017","tags":[{"label":"workbox","permalink":"/tags/workbox"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"PWA","permalink":"/tags/pwa"},{"label":"Service Worker","permalink":"/tags/service-worker"},{"label":"Webpack","permalink":"/tags/webpack"}],"readingTime":3.33,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"The TypeScript webpack PWA","authors":"johnnyreilly","tags":["workbox","TypeScript","PWA","Service Worker","Webpack"],"hide_table_of_contents":false},"prevItem":{"title":"ts-loader 2017 retrospective","permalink":"/2017/12/24/ts-loader-2017-retrospective"},"nextItem":{"title":"TypeScript Definitions, webpack and Module Types","permalink":"/2017/10/20/typescript-definitions-webpack-and-module-types"}},"content":"So, there you sit, conflicted. You\'ve got a lovely build setup; it\'s a thing of beauty. Precious, polished like a diamond, sharpened like a circular saw. There at the core of your carefully crafted setup sits webpack. Heaving, mysterious... powerful.\\n\\nThere\'s more. Not only are you sold on webpack, you\'re all in TypeScript too. But now you\'ve heard tell of \\"Progressive Web Applications\\" and \\"Service Workers\\".... And you want to be dealt in. You want to build web apps that work offline. It can\'t work can it? Your build setup\'s going to be like the creature in the episode where they\'ve taken one too many jumps and it\'s gone into the foetal position.\\n\\nSo this is the plan kids. Let\'s take a simple TypeScript, webpack setup and make it a PWA. Like Victoria Wood said...\\n\\n## [Let\'s Do It Tonight](https://youtu.be/lNU5KVa_Tu8)\\n\\nHow to begin? Well first comes the plagiarism; [here\'s a simple TypeScript webpack setup](https://github.com/TypeStrong/ts-loader/tree/master/examples/core-js). Rob it. Stick a gun to its head and order it onto your hard drive. `yarn install` to pick up your dependencies and then `yarn start` to see what you\'ve got. Something like this:\\n\\n![](Screenshot-2017-11-19-18.29.15.png)\\n\\nBeautiful right? And if we `yarn build` we end up with a simple output:\\n\\n![](Screenshot-2017-11-19-18.34.12.png)\\n\\nTo test what we\'ve built out we want to use a simple web server to serve up the `dist` folder. I\'ve got the npm package [http-server](https://www.npmjs.com/package/http-server) installed globally for just such an eventuality. So let\'s `http-server ./dist` and I\'m once again looking at our simple app; it looks exactly the same as when I `yarn start`. Smashing. What would we see if we were offline? Well thanks to the magic of Chrome DevTools we can find out. Offline and refresh our browser...\\n\\n![](Screenshot-2017-11-19-20.05.19.png)\\n\\nNot very user friendly. Once we\'re done, we should be able to refresh and still see our app.\\n\\n## [Work(box) It](https://youtu.be/UODX_pYpVxk)\\n\\n[Workbox](https://developers.google.com/web/tools/workbox/) is a project that makes the setting up of Service Workers (aka the magic that powers PWAs) easier. It supports webpack use cases through the [workbox-webpack-plugin](https://www.npmjs.com/package/workbox-webpack-plugin); so let\'s give it a whirl. Incidentally, there\'s a [cracking example](https://developers.google.com/web/tools/workbox/get-started/webpack) on the Workbox site.\\n\\n`yarn add workbox-webpack-plugin --dev` adds the plugin to our project. To make use of it, punt your way over to the `webpack.production.config.js` and add an entry for the plugin. We also need to set the `hash` parameter of the html-webpack-plugin to be false; if it\'s true it\'ll cause problems for the ServiceWorker.\\n\\n```js\\nconst WorkboxPlugin = require(\'workbox-webpack-plugin\');\\n\\n//...\\n\\nmodule.exports = {\\n  //...\\n\\n  plugins: [\\n    //...\\n\\n    new HtmlWebpackPlugin({\\n      hash: false,\\n      inject: true,\\n      template: \'src/index.html\',\\n      minify: {\\n        removeComments: true,\\n        collapseWhitespace: true,\\n        removeRedundantAttributes: true,\\n        useShortDoctype: true,\\n        removeEmptyAttributes: true,\\n        removeStyleLinkTypeAttributes: true,\\n        keepClosingSlash: true,\\n        minifyJS: true,\\n        minifyCSS: true,\\n        minifyURLs: true,\\n      },\\n    }),\\n\\n    new WorkboxPlugin({\\n      // we want our service worker to cache the dist directory\\n      globDirectory: \'dist\',\\n      // these are the sorts of files we want to cache\\n      globPatterns: [\'**/*.{html,js,css,png,svg,jpg,gif,json}\'],\\n      // this is where we want our ServiceWorker to be created\\n      swDest: path.resolve(\'dist\', \'sw.js\'),\\n      // these options encourage the ServiceWorkers to get in there fast\\n      // and not allow any straggling \\"old\\" SWs to hang around\\n      clientsClaim: true,\\n      skipWaiting: true,\\n    }),\\n  ],\\n\\n  //...\\n};\\n```\\n\\nWith this in place, `yarn build` will generate a ServiceWorker. Now to alter our code to register it. Open up `index.tsx` and add this to the end of the file:\\n\\n```js\\nif (\'serviceWorker\' in navigator) {\\n  window.addEventListener(\'load\', () => {\\n    navigator.serviceWorker\\n      .register(\'/sw.js\')\\n      .then((registration) => {\\n        // tslint:disable:no-console\\n        console.log(\'SW registered: \', registration);\\n      })\\n      .catch((registrationError) => {\\n        console.log(\'SW registration failed: \', registrationError);\\n      });\\n  });\\n}\\n```\\n\\nPut it together and...\\n\\n## What Have We Got?\\n\\nLet\'s `yarn build` again.\\n\\n![](Screenshot-2017-11-19-21.55.18.png)\\n\\nOooohh look! A service worker is with us. Does it work? Let\'s find out... `http-server ./dist` Browse to [http://localhost:8080](http://localhost:8080) and let\'s have a look at the console.\\n\\n![](Screenshot-2017-11-19-21.34.54.png)\\n\\nLooks very exciting. So now the test; let\'s go offline and refresh:\\n\\n![](Screenshot-2017-11-19-22.01.37.png)\\n\\nYou are looking at the 200s of success. You\'re now running with webpack and TypeScript and you have built a Progressive Web Application. Feel good about life."},{"id":"/2017/10/20/typescript-definitions-webpack-and-module-types","metadata":{"permalink":"/2017/10/20/typescript-definitions-webpack-and-module-types","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2017-10-20-typescript-definitions-webpack-and-module-types/index.md","source":"@site/blog/2017-10-20-typescript-definitions-webpack-and-module-types/index.md","title":"TypeScript Definitions, webpack and Module Types","description":"A funny thing happened on the way to the registry the other day. Something changed in an npm package I was using and confusion arose. You can read my unfiltered confusion here but here\'s the slightly clearer explanation.","date":"2017-10-20T00:00:00.000Z","formattedDate":"October 20, 2017","tags":[{"label":"AMD","permalink":"/tags/amd"},{"label":"DefinitelyTyped","permalink":"/tags/definitely-typed"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"CommonJS","permalink":"/tags/common-js"}],"readingTime":3.59,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"TypeScript Definitions, webpack and Module Types","authors":"johnnyreilly","tags":["AMD","DefinitelyTyped","TypeScript","CommonJS"],"hide_table_of_contents":false},"prevItem":{"title":"The TypeScript webpack PWA","permalink":"/2017/11/19/the-typescript-webpack-pwa"},"nextItem":{"title":"Working with Extrahop on webpack and ts-loader","permalink":"/2017/10/19/working-with-extrahop-on-webpack-and-ts"}},"content":"A funny thing happened on the way to the registry the other day. Something changed in an npm package I was using and confusion arose. You can read my unfiltered confusion [here](https://github.com/Microsoft/TypeScript/issues/18791) but here\'s the slightly clearer explanation.\\n\\n## The TL;DR\\n\\nWhen modules are imported, your loader will decide which module format it wants to use. CommonJS / AMD etc. The loader decides. It\'s important that the export is of the same \\"shape\\" regardless of the module format. For 2 reasons:\\n\\n1. You want to be able to reliably use the module regardless of the choice that your loader has made for which export to use.\\n2. Because when it comes to writing type definition files for modules, there is support for a _single_ external definition. Not one for each module format.\\n\\n![](one-definition-to-rule-them-all.jpg)\\n\\n## The DR\\n\\nOnce upon a time we decided to use [big.js](https://github.com/MikeMcl/big.js/) in our project. It\'s popular and my old friend [Steve Ognibene](https://twitter.com/nycdotnet) apparently originally wrote the type definitions which can be found [here](https://github.com/DefinitelyTyped/DefinitelyTyped/tree/master/types/big.js). Then the definitions were updated by [Miika H\xe4nninen](https://github.com/googol). And then there was pain.\\n\\n## UMD / CommonJS \\\\*\\\\*and\\\\*\\\\* Global exports oh my!\\n\\nMy usage code was as simple as this:\\n\\n```js\\nimport * as BigJs from \'big.js\';\\nconst lookABigJs = new BigJs(1);\\n```\\n\\nIf you execute it in a browser it works. It makes me a `Big`. However the TypeScript compiler is \\\\*\\\\*not\\\\*\\\\* happy. No siree. Nope. It\'s bellowing at me:\\n\\n```ts\\n[ts] Cannot use \'new\' with an expression whose type lacks a call or construct signature.\\n```\\n\\nSo I think: \\"Huh! I guess Miika just missed something off when he updated the definition files. No bother. I\'ll fix it.\\" I take a look at how `big.js` exposes itself to the outside world. At the time, thusly:\\n\\n```js\\n//AMD.\\nif (typeof define === \'function\' && define.amd) {\\n  define(function () {\\n    return Big;\\n  });\\n\\n  // Node and other CommonJS-like environments that support module.exports.\\n} else if (typeof module !== \'undefined\' && module.exports) {\\n  module.exports = Big;\\n  module.exports.Big = Big;\\n  //Browser.\\n} else {\\n  global.Big = Big;\\n}\\n```\\n\\nNow, we were using webpack as our script bundler / loader. webpack is supersmart; it can take all kinds of module formats. So although it\'s more famous for supporting CommonJS, it can roll with AMD. That\'s exactly what\'s happening here. When webpack encounters the above code, it goes with the AMD export. So at runtime, `import * as BigJs from \'big.js\';` lands up resolving to the `return Big;` above.\\n\\nNow this turns out to be super-relevant. I took a look at the relevant portion of the definition file and found this:\\n\\n```js\\nexport const Big: BigConstructor;\\n```\\n\\nWhich tells me that `Big` is being exported as a subproperty of the module. That makes sense; that lines up with the `module.exports.Big = Big;` statement in the the big.js source code. There\'s a \\"gotcha\\" coming; can you guess what it is?\\n\\nThe problem is that our type definition is not exposing `Big` as a default export. So even though it\'s there; TypeScript won\'t let us use it. What\'s killing us further is that webpack is loading the AMD export which _doesn\'t_ have `Big` as a subproperty of the module. It only has it as a default.\\n\\n[Kitson Kelly](https://twitter.com/kitsonk) expressed the problem well when he said:\\n\\n> there is a different shape depending on which loader is being used and I am not sure that makes a huge amount of sense. The AMD shape is different than the CommonJS shape. While that is technically possible, that feels like that is an issue.\\n\\n## One Definition to Rule Them All\\n\\nHe\'s right; it is an issue. From a TypeScript perspective there is no way to write a definition file that allows for different module \\"shapes\\" depending upon the module type. If you really wanted to do that you\'re reduced to writing multiple definition files. That\'s blind alley anyway; what you want is a module to expose itself with the same \\"shape\\" regardless of the module type. What you want is this:\\n\\n`AMD === CommonJS === Global`\\n\\nAnd that\'s what we now have! Thanks to [Michael McLaughlin](https://github.com/mikemcl), author of big.js, [version 4.0 unified the export shape of the package](https://github.com/MikeMcl/big.js/pull/87#issuecomment-332663587). Miika H\xe4nninen submitted another [PR](https://github.com/DefinitelyTyped/DefinitelyTyped/pull/20096) which fixed up the type definitions. And once again the world is a beautiful place!"},{"id":"/2017/10/19/working-with-extrahop-on-webpack-and-ts","metadata":{"permalink":"/2017/10/19/working-with-extrahop-on-webpack-and-ts","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2017-10-19-working-with-extrahop-on-webpack-and-ts/index.md","source":"@site/blog/2017-10-19-working-with-extrahop-on-webpack-and-ts/index.md","title":"Working with Extrahop on webpack and ts-loader","description":"I\'m quite proud of this//www.extrahop.com/company/blog/2017/extrahop-webpack-accelerating-build-times/","date":"2017-10-19T00:00:00.000Z","formattedDate":"October 19, 2017","tags":[{"label":"extrahop","permalink":"/tags/extrahop"},{"label":"ts-loader","permalink":"/tags/ts-loader"},{"label":"Webpack","permalink":"/tags/webpack"}],"readingTime":0.625,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Working with Extrahop on webpack and ts-loader","authors":"johnnyreilly","tags":["extrahop","ts-loader","Webpack"],"hide_table_of_contents":false},"prevItem":{"title":"TypeScript Definitions, webpack and Module Types","permalink":"/2017/10/20/typescript-definitions-webpack-and-module-types"},"nextItem":{"title":"fork-ts-checker-webpack-plugin code clickability","permalink":"/2017/09/12/fork-ts-checker-webpack-plugin-code"}},"content":"I\'m quite proud of this: [https://www.extrahop.com/company/blog/2017/extrahop-webpack-accelerating-build-times/](https://www.extrahop.com/company/blog/2017/extrahop-webpack-accelerating-build-times/)\\n\\nIf you didn\'t know, I spend a good amount of my spare time hacking on open source software. You may not know what that is. I would describe OSS as software made with \u2764 by people, for other people to use.\\n\\nYou are currently reading this on a platform that was built using OSS. It\'s all around you, every day. It\'s on your phone, on your computer, on your TV. It\'s everywhere.\\n\\nIt\'s my hobby, it\'s part of my work. This specifically was one of those tremendously rare occasions when I got paid directly to work on my hobby, with people much brighter than me. It was brilliant. I loved it; it was a privilege.\\n\\nHere\'s to Open Source!"},{"id":"/2017/09/12/fork-ts-checker-webpack-plugin-code","metadata":{"permalink":"/2017/09/12/fork-ts-checker-webpack-plugin-code","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2017-09-12-fork-ts-checker-webpack-plugin-code/index.md","source":"@site/blog/2017-09-12-fork-ts-checker-webpack-plugin-code/index.md","title":"fork-ts-checker-webpack-plugin code clickability","description":"My name is John Reilly and I\'m a VS Code addict. There I said it. I\'m also a big fan of TypeScript and webpack. I\'ve recently switched to using the awesome fork-ts-checker-webpack-plugin to speed up my builds.","date":"2017-09-12T00:00:00.000Z","formattedDate":"September 12, 2017","tags":[{"label":"VS Code","permalink":"/tags/vs-code"},{"label":"console","permalink":"/tags/console"},{"label":"fork-ts-checker-webpack-plugin","permalink":"/tags/fork-ts-checker-webpack-plugin"},{"label":"ts-loader","permalink":"/tags/ts-loader"},{"label":"Webpack","permalink":"/tags/webpack"}],"readingTime":2.085,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"fork-ts-checker-webpack-plugin code clickability","authors":"johnnyreilly","tags":["VS Code","console","fork-ts-checker-webpack-plugin","ts-loader","Webpack"],"hide_table_of_contents":false},"prevItem":{"title":"Working with Extrahop on webpack and ts-loader","permalink":"/2017/10/19/working-with-extrahop-on-webpack-and-ts"},"nextItem":{"title":"TypeScript + Webpack: Super Pursuit Mode","permalink":"/2017/09/07/typescript-webpack-super-pursuit-mode"}},"content":"My name is John Reilly and I\'m a VS Code addict. There I said it. I\'m also a big fan of TypeScript and webpack. I\'ve recently switched to using the awesome [`fork-ts-checker-webpack-plugin`](https://www.npmjs.com/package/fork-ts-checker-webpack-plugin) to speed up my builds.\\n\\nOne thing I love is using VS Code both as my editor and my terminal. Using the fork-ts-checker-webpack-plugin I noticed a problem when TypeScript errors showed up in the terminal:\\n\\n![](Screenshot-2017-09-12-06.12.25.png)\\n\\nTake a look at the red file location in the console above. What\'s probably not obvious from the above screenshot is that it is **not clickable**. I\'m used to being able to click on link in the console and bounce straight to the error location. It\'s a really productive workflow; see a problem, click on it, be taken to the cause, fix it.\\n\\nI want to click on \\"`C:/source/ts-loader/examples/fork-ts-checker/src/fileWithError.ts(2,7)`\\" and have VS Code open up `fileWithError.ts`, ideally at line 2 and column 7. But here it\'s not working. Why?\\n\\nWell, I initially got this slightly wrong; I thought it was about the formatting of the file path. It is. I thought that having the line number and column number in parentheses after the path (eg `\\"(2,7)\\"`) was screwing over VS Code. It isn\'t. Something else is. Look closely at the screenshot; what do you see? Do you notice how the colour of the line number / column number is different to the path? In the words of [Delbert Wilkins](https://youtu.be/281jMxOvP5k): that\'s crucial.\\n\\nYup, the colour change between the path and the line number / column number is the problem. I\'ve submitted a [PR to fix this](https://github.com/Realytics/fork-ts-checker-webpack-plugin/pull/48) that I hope will get merged. In the meantime you can avoid this issue by dropping this code into your `webpack.config.js`:\\n\\n```js\\nvar chalk = require(\'chalk\');\\nvar os = require(\'os\');\\n\\nfunction clickableFormatter(message, useColors) {\\n  var colors = new chalk.constructor({ enabled: useColors });\\n  var messageColor = message.isWarningSeverity()\\n    ? colors.bold.yellow\\n    : colors.bold.red;\\n  var fileAndNumberColor = colors.bold.cyan;\\n  var codeColor = colors.grey;\\n  return [\\n    messageColor(message.getSeverity().toUpperCase() + \' in \') +\\n      fileAndNumberColor(\\n        message.getFile() +\\n          \'(\' +\\n          message.getLine() +\\n          \',\' +\\n          message.getCharacter() +\\n          \')\'\\n      ) +\\n      messageColor(\':\'),\\n\\n    codeColor(message.getFormattedCode() + \': \') + message.getContent(),\\n  ].join(os.EOL);\\n}\\n\\nmodule.exports = {\\n  // Other config...\\n  module: {\\n    rules: [\\n      {\\n        test: /\\\\.tsx?$/,\\n        loader: \'ts-loader\',\\n        options: { transpileOnly: true },\\n      },\\n    ],\\n  },\\n  resolve: {\\n    extensions: [\'.ts\', \'.tsx\', \'js\'],\\n  },\\n  plugins: [\\n    new ForkTsCheckerWebpackPlugin({ formatter: clickableFormatter }), // Here we get our clickability back\\n  ],\\n};\\n```\\n\\nWith that in place, what do you we have? This:\\n\\n![](Screenshot-2017-09-12-06.35.48.png)\\n\\nVS Code clickability; it\'s a beautiful thing."},{"id":"/2017/09/07/typescript-webpack-super-pursuit-mode","metadata":{"permalink":"/2017/09/07/typescript-webpack-super-pursuit-mode","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2017-09-07-typescript-webpack-super-pursuit-mode/index.md","source":"@site/blog/2017-09-07-typescript-webpack-super-pursuit-mode/index.md","title":"TypeScript + Webpack: Super Pursuit Mode","description":"This post also featured as a webpack Medium publication.","date":"2017-09-07T00:00:00.000Z","formattedDate":"September 7, 2017","tags":[{"label":"HappyPack","permalink":"/tags/happy-pack"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"cache-loader","permalink":"/tags/cache-loader"},{"label":"thread-loader","permalink":"/tags/thread-loader"},{"label":"fork-ts-checker-webpack-plugin","permalink":"/tags/fork-ts-checker-webpack-plugin"},{"label":"Webpack","permalink":"/tags/webpack"}],"readingTime":6.66,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"TypeScript + Webpack: Super Pursuit Mode","authors":"johnnyreilly","tags":["HappyPack","TypeScript","cache-loader","thread-loader","fork-ts-checker-webpack-plugin","Webpack"],"hide_table_of_contents":false},"prevItem":{"title":"fork-ts-checker-webpack-plugin code clickability","permalink":"/2017/09/12/fork-ts-checker-webpack-plugin-code"},"nextItem":{"title":"Oh the Glamour of Open Source","permalink":"/2017/08/30/oh-glamour-of-open-source"}},"content":"_[This post also featured as a webpack Medium publication](https://medium.com/webpack/typescript-webpack-super-pursuit-mode-83cc568dea79)._\\n\\nIf you\'re like me then you\'ll like TypeScript and you\'ll like module bundling with webpack. You may also like speedy builds. That\'s completely understandable. The fact of the matter is, you sacrifice a bit of build speed to have webpack in the mix. Wouldn\'t it be great if we could even up the difference?\\n\\nI\'m the primary maintainer of ts-loader, a TypeScript loader for webpack. Just recently a couple of PRs were submitted that said, in other words: ts-loader is like this:\\n\\n![](KITT.jpg)\\n\\nBut it could be like this:\\n\\n![](webkitt.jpg)\\n\\nApologies for the image quality above; there appear to be no high quality pictures out there of KITT in Super Pursuit Mode for me to defame with [Garan Jenkin](https://github.com/plemont)\'s atrocious puns.\\n\\n## fork-ts-checker-webpack-plugin\\n\\n[\\"Faster type checking with forked process\\"](https://github.com/TypeStrong/ts-loader/issues/537) read the enticing name of the issue. It turned out to be [Piotr Ole\u015b](https://github.com/piotr-oles) ([@OlesDev](https://twitter.com/OlesDev)) telling the world about his beautiful creation. He\'d put together a mighty fine plugin that can be used alongside ts-loader called the [fork-ts-checker-webpack-plugin](https://github.com/Realytics/fork-ts-checker-webpack-plugin). The name is a bit of a mouthful but the purpose is mouth-watering. To quote the README, it is a:\\n\\n> Webpack plugin that runs typescript type checker on a separate process.\\n\\nWhat does this mean and how does this fit with ts-loader? Well, ts-loader does 2 jobs:\\n\\n1. It transpiles your TypeScript into JavaScript and hands it off to webpack\\n2. It collects any TypeScript compilation errors and reports them to webpack\\n\\nWhat this plugin does is say, \\"forget about #2 - we\'ve got this.\\" It removes the responsibility for type checking from ts-loader, so the only work ts-loader does is transpilation. In the meantime, the all important type checking is still happening. To be honest, there would be little reason to recommend this approach otherwise. The difference is `fork-ts-checker-webpack-plugin` is doing the heavy lifting **in a separate process**. This provides a nice performance boost to your workflow. ts-loader is doing **less** and that\'s a <u>good thing</u>\\n\\n.\\n\\nThe approach used here is similar to that employed by awesome-typescript-loader. ATL is another TypeScript loader for webpack by the excellent [Stanislav Panferov](https://github.com/s-panferov). ATL also has a technique for performing typechecking in a forked process. fork-ts-checker-webpack-plugin was an effort by Piotr to implement something similar but with improved incremental build performance.\\n\\nHow do we use it? Add fork-ts-checker-webpack-plugin as a `devDependency` of your project and then amend the `webpack.config.js` to set ts-loader into `transpileOnly` mode and drop the plugin into the mix:\\n\\n```js\\nvar ForkTsCheckerWebpackPlugin = require(\'fork-ts-checker-webpack-plugin\');\\n\\nvar webpackConfig = {\\n  // other config...\\n  context: __dirname, // to automatically find tsconfig.json\\n  module: {\\n    rules: [\\n      {\\n        test: /\\\\.tsx?$/,\\n        loader: \'ts-loader\',\\n        options: {\\n          // disable type checker - we will use it in fork plugin\\n          transpileOnly: true,\\n        },\\n      },\\n    ],\\n  },\\n  plugins: [new ForkTsCheckerWebpackPlugin()],\\n};\\n```\\n\\nIf you\'d like to see an example of how to use the plugin then take a look at a [simple example](https://github.com/TypeStrong/ts-loader/tree/master/examples/fork-ts-checker) and a [more involved one](https://github.com/TypeStrong/ts-loader/tree/master/examples/react-babel-karma-gulp-fork-ts-checker).\\n\\n## HappyPack\\n\\nNot so long ago I didn\'t know what <strike>happyness</strike>\\n\\n[HappyPack](https://github.com/amireh/happypack) was. \\"Happiness in the form of faster webpack build times.\\" That\'s what it is.\\n\\n> HappyPack makes webpack builds faster by allowing you to transform multiple files in parallel.\\n\\nIt does this by spinning up multiple threads, each with their own loaders inside. We wanted to do this with ts-loader; to have multiple instances of ts-loader running. Work can then be divided up across these separate loaders. Isn\'t multi-threading great?\\n\\nts-loader did not initially play nicely with HappyPack; essentially this is because ts-loader touches parts of webpack\'s API that HappyPack replaces. The entirely wonderful [Artem Kozlov](https://github.com/aindlq) submitted a [PR which added HappyPack support to ts-loader](https://github.com/TypeStrong/ts-loader/pull/547). Support essentially amounts to switching ts-loader to run in `transpileOnly` mode and ensuring that there is no attempt to talk to parts of the webpack API that HappyPack removes.\\n\\nIt would be hard to recommend using HappyPack as is because, as with `transpileOnly` mode you lose all typechecking. Where it becomes worthwhile is where it is combined with the fork-ts-checker-webpack-plugin so you keep the typechecking.\\n\\nEnough with the chitter chatter; how can we achieve this? Add HappyPack as a `devDependency` of your project and then amend the `webpack.config.js` as follows:\\n\\n```js\\nvar HappyPack = require(\'happypack\');\\nvar ForkTsCheckerWebpackPlugin = require(\'fork-ts-checker-webpack-plugin\');\\n\\nmodule.exports = {\\n  // other config...\\n  context: __dirname, // to automatically find tsconfig.json\\n  module: {\\n    rules: [\\n      {\\n        test: /\\\\.tsx?$/,\\n        exclude: /node_modules/,\\n        loader: \'happypack/loader?id=ts\',\\n      },\\n    ],\\n  },\\n  plugins: [\\n    new HappyPack({\\n      id: \'ts\',\\n      threads: 2,\\n      loaders: [\\n        {\\n          path: \'ts-loader\',\\n          query: { happyPackMode: true },\\n        },\\n      ],\\n    }),\\n    new ForkTsCheckerWebpackPlugin({ checkSyntacticErrors: true }),\\n  ],\\n};\\n```\\n\\nNote that the ts-loader options are now configured via the HappyPack `query` and that we\'re setting ts-loader with the `happyPackMode` option set.\\n\\nThere\'s one other thing to note which is important; we\'re now passing the `checkSyntacticErrors` option to the fork plugin. This ensures that the plugin checks for both syntactic errors (eg `const array = [{} {}];`) and semantic errors (eg `const x: number = \'1\';`). By default the plugin only checks for semantic errors. This is because when ts-loader is used with `transpileOnly` set, ts-loader will still report syntactic errors. But when used in `happyPackMode` it does not.\\n\\nIf you\'d like to see an example of how to use HappyPack then once again we have a [simple example](https://github.com/TypeStrong/ts-loader/tree/master/examples/happypack) and a [more involved one](https://github.com/TypeStrong/ts-loader/tree/master/examples/react-babel-karma-gulp-happypack).\\n\\n## `thread-loader` \\\\+ `cache-loader`\\n\\nYou might have some reservations about using HappyPack. First of all the quirky configuration required makes your webpack config rather less comprehensible. Also, HappyPack is not officially blessed by webpack. It is a side project developed externally from webpack and there\'s no guarantees that new versions of webpack won\'t break it. Neither of these are reasons not to use HappyPack but they are things to bear in mind.\\n\\nWhat if there were a way to parallelise our builds which dealt with these issues? Well, there is! By using [thread-loader](https://github.com/webpack-contrib/thread-loader) and [cache-loader](https://github.com/webpack-contrib/cache-loader) in combination you can both feel happy that you\'re using an official webpack workflow and you can have a config that\'s less confusing.\\n\\nWhat would that config look like? This:\\n\\n```js\\nvar ForkTsCheckerWebpackPlugin = require(\'fork-ts-checker-webpack-plugin\');\\n\\nmodule.exports = {\\n  // other config...\\n  context: __dirname, // to automatically find tsconfig.json\\n  module: {\\n    rules: {\\n      test: /\\\\.tsx?$/,\\n      use: [\\n        { loader: \'cache-loader\' },\\n        {\\n          loader: \'thread-loader\',\\n          options: {\\n            // there should be 1 cpu for the fork-ts-checker-webpack-plugin\\n            workers: require(\'os\').cpus().length - 1,\\n          },\\n        },\\n        {\\n          loader: \'ts-loader\',\\n          options: {\\n            happyPackMode: true, // IMPORTANT! use happyPackMode mode to speed-up compilation and reduce errors reported to webpack\\n          },\\n        },\\n      ],\\n    },\\n  },\\n  plugins: [new ForkTsCheckerWebpackPlugin({ checkSyntacticErrors: true })],\\n};\\n```\\n\\nAs you can see the configuration is much cleaner than with HappyPack. Interestingly ts-loader still needs to run in \\"`happyPackMode`\\" and that\'s because thread-loader is essentially behaving in the same fashion as with HappyPack and so ts-loader needs to behave in the same way. Probably ts-loader should have a more generic flag name than \\"`happyPackMode`\\". (Famously, naming things is hard; so if you\'ve a good idea, tell me!)\\n\\nThese loaders are new and so tread carefully. My own experiences have been pretty positive but your mileage may vary. Do note that, as with HappyPack, the thread-loader is highly configurable.\\n\\nIf you\'d like to see an example of how to use thread-loader and cache-loader then once again we have a [simple example](https://github.com/TypeStrong/ts-loader/tree/master/examples/thread-loader) and a [more involved one](https://github.com/TypeStrong/ts-loader/tree/master/examples/react-babel-karma-gulp-thread-loader).\\n\\n## All This Could Be Yours...\\n\\n> Wow! It looks like we can cut our build time by 4 minutes! [\\\\#webpack](https://twitter.com/hashtag/webpack?src=hash)[@typescriptlang](https://twitter.com/typescriptlang) // cc [@johnny_reilly](https://twitter.com/johnny_reilly)[pic.twitter.com/gjvy9SLBAT](https://t.co/gjvy9SLBAT)\\n>\\n> \u2014 Donald Pipowitch (@PipoPeperoni) [June 23, 2017](https://twitter.com/PipoPeperoni/status/878148978356834304)\\n\\n<script async=\\"\\" src=\\"//platform.twitter.com/widgets.js\\" charSet=\\"utf-8\\"><\/script>\\n\\nIn this post we\'re improving build speeds with TypeScript and webpack in 3 ways:\\n\\n<dl><dt>fork-ts-checker-webpack-plugin</dt><dd>With this plugin in play ts-loader only performs transpilation. ts-loader is doing less so the build is faster.</dd><dt>HappyPack</dt><dd>With HappyPack in the mix, the build is parallelised. That parallelisation means the build is faster.</dd><dt>thread-loader / cache-loader</dt><dd>With thread-loader and cache-loader, again the build is parallelised and the build is faster.</dd></dl>\\n\\n<iframe src=\\"https://giphy.com/embed/Bo2WsocASVBm0\\" width=\\"240\\" height=\\"180\\" frameBorder=\\"0\\" allowFullScreen=\\"\\"></iframe>"},{"id":"/2017/08/30/oh-glamour-of-open-source","metadata":{"permalink":"/2017/08/30/oh-glamour-of-open-source","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2017-08-30-oh-glamour-of-open-source/index.md","source":"@site/blog/2017-08-30-oh-glamour-of-open-source/index.md","title":"Oh the Glamour of Open Source","description":"Here\'s how my life panned out in the early hours of Wednesday 30th September 2017:","date":"2017-08-30T00:00:00.000Z","formattedDate":"August 30, 2017","tags":[],"readingTime":1.205,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Oh the Glamour of Open Source","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false},"prevItem":{"title":"TypeScript + Webpack: Super Pursuit Mode","permalink":"/2017/09/07/typescript-webpack-super-pursuit-mode"},"nextItem":{"title":"Karma: From PhantomJS to Headless Chrome","permalink":"/2017/08/27/karma-from-phantomjs-to-headless-chrome"}},"content":"Here\'s how my life panned out in the early hours of Wednesday 30th September 2017:\\n\\n <dl><dt>2 am</dt><dd>awoken by Lisette having a nightmare</dd><dt>3 am</dt><dd>gave up hope of getting back to sleep upstairs and headed for the sofa</dd><dt>4 am</dt><dd>still not asleep and discovered a serious gap in an open source project I help out with</dd><dt>4:30 am</dt><dd> come up with idea for a fix</dd><dt>4:45 am</dt><dd> accidentally delete a repo that I and many others care about from GitHub</dd><dt>4:50 am</dt><dd> recover said repo from backups (sweet mercy how could I be so stupid?)</dd><dt>4:55 am</dt><dd> actually succeed in cloning the repo I want to hack on </dd><dt>5:30 am</dt><dd> implement fix and <a href=\\"https://github.com/Realytics/fork-ts-checker-webpack-plugin/pull/43\\">send PR</a></dd><dt>5:35 am</dt><dd> go for a walk round the river</dd><dt>6:30 am</dt><dd> realise I didn\'t submit a test for the changed functionality</dd><dt>6:35 am</dt><dd> write test only to discover I can\'t run the test pack on Windows</dd><dt>6:40 am</dt><dd> add test to PR anyway so I can see test results when Travis runs on each commit.</dd><dt>7 am</dt><dd>despair at the duration of my feedback loop, totally fail to get my tests to pass</dd><dt>7:10 am</dt><dd> stub my toe really badly on a train set Benjamin has been busily assembling beneath my feet</dd><dt>7:11 am</dt><dd> give in and literally beg the project owner in Paris to fix the tests for me. He takes pity on me and agrees. Possibly because I gave him emoji tulips \ud83c\udf37</dd><dt>7:12 am</dt><dd> feel like a slight failure and profoundly tired.</dd></dl>\\n\\nOh the glamour of open source."},{"id":"/2017/08/27/karma-from-phantomjs-to-headless-chrome","metadata":{"permalink":"/2017/08/27/karma-from-phantomjs-to-headless-chrome","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2017-08-27-karma-from-phantomjs-to-headless-chrome/index.md","source":"@site/blog/2017-08-27-karma-from-phantomjs-to-headless-chrome/index.md","title":"Karma: From PhantomJS to Headless Chrome","description":"Like pretty much everyone else I\'ve been using PhantomJS to run my JavaScript (or compiled-to-JS) unit tests. It\'s been great. So when I heard the news that PhantomJS was dead I was genuinely sad. However, the King is dead.... Long live the King! For there is a new hope; it\'s called Chrome Headless . It\'s not a separate version of Chrome; rather the ability to run Chrome without a UI is now baked into Google\'s favourite browser as of v59. (For those history buffs I might as well be clear: the main reason PhantomJS died is because Chrome Headless was in the works.)","date":"2017-08-27T00:00:00.000Z","formattedDate":"August 27, 2017","tags":[{"label":"Chrome","permalink":"/tags/chrome"},{"label":"Karma","permalink":"/tags/karma"},{"label":"PhantomJS","permalink":"/tags/phantom-js"},{"label":"Headless","permalink":"/tags/headless"}],"readingTime":1.905,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Karma: From PhantomJS to Headless Chrome","authors":"johnnyreilly","tags":["Chrome","Karma","PhantomJS","Headless"],"hide_table_of_contents":false},"prevItem":{"title":"Oh the Glamour of Open Source","permalink":"/2017/08/30/oh-glamour-of-open-source"},"nextItem":{"title":"A Haiku on the Problem with SemVer: Us","permalink":"/2017/07/29/a-haiku-on-problem-with-semver-us"}},"content":"Like pretty much everyone else I\'ve been using PhantomJS to run my JavaScript (or compiled-to-JS) unit tests. It\'s been great. So when I heard the news that [PhantomJS was dead](https://news.ycombinator.com/item?id=14105489) I was genuinely sad. However, the King is dead.... Long live the King! For there is a new hope; it\'s called [Chrome Headless ](https://developers.google.com/web/updates/2017/04/headless-chrome). It\'s not a separate version of Chrome; rather the ability to run Chrome without a UI is now baked into Google\'s favourite browser as of v59. (For those history buffs I might as well be clear: the main reason PhantomJS died is because Chrome Headless was in the works.)\\n\\n## Making the Switch\\n\\nAs long as you\'re running Chrome v59 or greater then you can switch. I\'ve just made ts-loader\'s execution test pack run with Chrome Headless instead of PhantomJS and I\'ve rarely been happier. Honest. Some context: the execution test pack runs Jasmine unit tests via the [Karma test runner](https://karma-runner.github.io/1.0/index.html). The move was surprisingly easy and you can see just how minimal it was in the PR [here](https://github.com/TypeStrong/ts-loader/pull/611/files). If you want to migrate a test that runs tests via Karma then this will take you through what you need to do.\\n\\n## `package.json`\\n\\nYou no longer need `phantomjs-prebuilt` as a dev dependency of your project. That\'s the PhantomJS browser disappearing in the rear view mirror. Next we need to replace `karma-phantomjs-launcher` with `karma-chrome-launcher`. These packages are responsible for firing up the browser that the tests are run in and we no longer want to invoke PhantomJS; we\'re Chrome all the way baby.\\n\\n## `karma.conf.js`\\n\\nYou need to tell Karma to use Chrome Headless instead of PhantomJS. You do that by replacing\\n\\n```js\\nbrowsers: [ \'PhantomJS\' ],\\n```\\n\\nwith\\n\\n```js\\nbrowsers: [ \'ChromeHeadless\' ],\\n```\\n\\nThat\'s it; job done!\\n\\n## Continuous Integration\\n\\nThere\'s always one more thing isn\'t there? Yup, ts-loader has CI builds that run on [Windows with AppVeyor](https://ci.appveyor.com/project/JohnReilly/ts-loader/branch/master) and [Linux with Travis](https://travis-ci.org/TypeStrong/ts-loader). The AppVeyor build went green on the first run; that\'s because Chrome is installed by default in the AppVeyor build environment. (yay!)\\n\\nTravis went red. (boooo!) Travis doesn\'t have Chrome installed by default. But it\'s no biggie; you just need to tweak your `.travis.yml` like so:\\n\\n```yml\\ndist: trusty\\naddons:\\n  chrome: stable\\n```\\n\\nThis includes Chrome in the Travis build environment. Green. Boom!"},{"id":"/2017/07/29/a-haiku-on-problem-with-semver-us","metadata":{"permalink":"/2017/07/29/a-haiku-on-problem-with-semver-us","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2017-07-29-a-haiku-on-problem-with-semver-us/index.md","source":"@site/blog/2017-07-29-a-haiku-on-problem-with-semver-us/index.md","title":"A Haiku on the Problem with SemVer: Us","description":"Version numbers wrong We release breaking changes We don\'t know we do","date":"2017-07-29T00:00:00.000Z","formattedDate":"July 29, 2017","tags":[{"label":"haiku","permalink":"/tags/haiku"},{"label":"semantic versioning","permalink":"/tags/semantic-versioning"}],"readingTime":0.06,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"A Haiku on the Problem with SemVer: Us","authors":"johnnyreilly","tags":["haiku","semantic versioning"],"hide_table_of_contents":false},"prevItem":{"title":"Karma: From PhantomJS to Headless Chrome","permalink":"/2017/08/27/karma-from-phantomjs-to-headless-chrome"},"nextItem":{"title":"Dynamic import: I\'ve been awaiting you...","permalink":"/2017/07/02/dynamic-import-ive-been-await-ing-you"}},"content":"Version numbers wrong We release breaking changes We don\'t know we do"},{"id":"/2017/07/02/dynamic-import-ive-been-await-ing-you","metadata":{"permalink":"/2017/07/02/dynamic-import-ive-been-await-ing-you","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2017-07-02-dynamic-import-ive-been-await-ing-you/index.md","source":"@site/blog/2017-07-02-dynamic-import-ive-been-await-ing-you/index.md","title":"Dynamic import: I\'ve been awaiting you...","description":"One of the most exciting features to ship with TypeScript 2.4 was support for the dynamic import expression. To quote the release blog post:","date":"2017-07-02T00:00:00.000Z","formattedDate":"July 2, 2017","tags":[{"label":"await","permalink":"/tags/await"},{"label":"async","permalink":"/tags/async"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"Karma","permalink":"/tags/karma"},{"label":"Babel","permalink":"/tags/babel"},{"label":"Webpack","permalink":"/tags/webpack"},{"label":"dynamic import","permalink":"/tags/dynamic-import"}],"readingTime":5.07,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Dynamic import: I\'ve been awaiting you...","authors":"johnnyreilly","tags":["await","async","TypeScript","Karma","Babel","Webpack","dynamic import"],"hide_table_of_contents":false},"prevItem":{"title":"A Haiku on the Problem with SemVer: Us","permalink":"/2017/07/29/a-haiku-on-problem-with-semver-us"},"nextItem":{"title":"Windows Defender Step Away From npm","permalink":"/2017/06/11/windows-defender-step-away-from-npm"}},"content":"One of the most exciting features to ship with TypeScript 2.4 was support for the dynamic import expression. To quote the [release blog post](https://blogs.msdn.microsoft.com/typescript/2017/06/27/announcing-typescript-2-4/#dynamic-import-expressions):\\n\\n> Dynamic `import` expressions are a new feature in ECMAScript that allows you to asynchronously request a module at any arbitrary point in your program. These modules come back as `Promise`s of the module itself, and can be `await`\\\\-ed in an async function, or can be given a callback with `.then`.\\n>\\n> ...\\n>\\n> Many bundlers have support for automatically splitting output bundles (a.k.a. \u201ccode splitting\u201d) based on these `import()` expressions, so consider using this new feature with the `esnext` module target. Note that this feature won\u2019t work with the `es2015` module target, since the feature is anticipated for ES2018 or later.\\n\\nAs the post makes clear, this adds support for a very bleeding edge ECMAScript feature. This is not fully standardised yet; it\'s currently at [stage 3](https://github.com/tc39/proposals) on the TC39 proposals list. That means it\'s at the [Candidate](https://tc39.github.io/process-document/) stage and is unlikely to change further. If you\'d like to read more about it then take a look at the official proposal [here](https://github.com/tc39/proposal-dynamic-import).\\n\\nWhilst this is super-new, we are still able to use this feature. We just have to jump through a few hoops first.\\n\\n## TypeScript Setup\\n\\nFirst of all, you need to install TypeScript 2.4. With that in place you need to make some adjustments to your `tsconfig.json` in order that the relevant compiler switches are flipped. What do you need? First of all you need to be targeting ECMAScript 2015 as a minimum. That\'s important specifically because ES2015 contained `Promise`s which is what dynamic `import`s produce. The second thing you need is to target the module type of `esnext`. You\'re likely targeting `es2015` now, `esnext` is that **plus** dynamic `import`s.\\n\\nHere\'s a `tsconfig.json` I made earlier which has the relevant settings set:\\n\\n```json\\n{\\n  \\"compilerOptions\\": {\\n    \\"allowSyntheticDefaultImports\\": true,\\n    \\"lib\\": [\\"dom\\", \\"es2015\\"],\\n    \\"target\\": \\"es2015\\",\\n    \\"module\\": \\"esnext\\",\\n    \\"moduleResolution\\": \\"node\\",\\n    \\"noImplicitAny\\": true,\\n    \\"noUnusedLocals\\": true,\\n    \\"noUnusedParameters\\": true,\\n    \\"removeComments\\": false,\\n    \\"preserveConstEnums\\": true,\\n    \\"sourceMap\\": true,\\n    \\"skipLibCheck\\": true\\n  }\\n}\\n```\\n\\n## Babel Setup\\n\\nAt the time of writing, browser support for dynamic `import` is non-existent. This will likely be the case for some time but it needn\'t hold us back. Babel can step in here and compile our super-new JS into JS that will run in our browsers today.\\n\\nYou\'ll need to decide for yourself how much you want Babel to do for you. In my case I\'m targeting old school browsers which don\'t yet support ES2015. You may not need to. However, the one thing that you\'ll certainly need is the [Syntax Dynamic Import](https://babeljs.io/docs/plugins/syntax-dynamic-import/) plugin. It\'s this that allows Babel to process dynamic `import` statements.\\n\\nThese are the options I\'m passing to Babel:\\n\\n```js\\nvar babelOptions = {\\n  plugins: [\'syntax-dynamic-import\'],\\n  presets: [\\n    [\\n      \'es2015\',\\n      {\\n        modules: false,\\n      },\\n    ],\\n  ],\\n};\\n```\\n\\nYou\'re also going to need something that actually execute the `import`s. In my case I\'m using webpack...\\n\\n## webpack\\n\\nwebpack 2 supports [`import()`](https://webpack.js.org/api/module-methods/#import-). So if you webpack set up with [ts-loader](https://github.com/TypeStrong/ts-loader) (or awesome-typescript-loader etc), chaining into [babel-loader](https://github.com/babel/babel-loader) you should find you have a setup that supports dynamic `import`. That means a `webpack.config.js` that looks something like this:\\n\\n```js\\nvar path = require(\'path\');\\nvar webpack = require(\'webpack\');\\n\\nvar babelOptions = {\\n  plugins: [\'syntax-dynamic-import\'],\\n  presets: [\\n    [\\n      \'es2015\',\\n      {\\n        modules: false,\\n      },\\n    ],\\n  ],\\n};\\n\\nmodule.exports = {\\n  entry: \'./app.ts\',\\n  output: {\\n    filename: \'bundle.js\',\\n  },\\n  module: {\\n    rules: [\\n      {\\n        test: /\\\\.ts(x?)$/,\\n        exclude: /node_modules/,\\n        use: [\\n          {\\n            loader: \'babel-loader\',\\n            options: babelOptions,\\n          },\\n          {\\n            loader: \'ts-loader\',\\n          },\\n        ],\\n      },\\n      {\\n        test: /\\\\.js$/,\\n        exclude: /node_modules/,\\n        use: [\\n          {\\n            loader: \'babel-loader\',\\n            options: babelOptions,\\n          },\\n        ],\\n      },\\n    ],\\n  },\\n  resolve: {\\n    extensions: [\'.ts\', \'.tsx\', \'.js\'],\\n  },\\n};\\n```\\n\\n## ts-loader example\\n\\nI\'m one of the maintainers of [ts-loader](https://github.com/TypeStrong/ts-loader) which is a TypeScript loader for webpack. When support for dynamic `import`s landed I wanted to add a test to cover usage of the new syntax with ts-loader.\\n\\nWe have 2 test packs for ts-loader, one of which is our \\"execution\\" test pack. It is so named because it works by spinning up webpack with ts-loader and then using [karma](https://github.com/karma-runner/karma) to execute a set of tests. Each \\"test\\" in our execution test pack is actually a mini-project with its own test suite (generally [jasmine](https://jasmine.github.io/) but that\'s entirely configurabe). Each complete with its own `webpack.config.js`, `karma.conf.js` and either a `typings.json` or `package.json` for bringing in dependencies. So it\'s a full test of whether code slung with ts-loader and webpack actually executes when the output is plugged into a browser.\\n\\nThis is the test pack for dynamic `import`s:\\n\\n```js\\nimport a from \\"../src/a\\";\\nimport b from \\"../src/b\\";\\n\\ndescribe(\\"app\\", () => {\\n  it(\\"a to be \'a\' and b to be \'b\' (classic)\\", () => {\\n    expect(a).toBe(\\"a\\");\\n    expect(b).toBe(\\"b\\");\\n  });\\n\\n  it(\\"import results in a module with a default export\\", done => {\\n    import(\\"../src/c\\").then(c => {\\n      // .default is the default export\\n      expect(c.default).toBe(\\"c\\");\\n\\n      done();\\n    }\\n  });\\n\\n  it(\\"import results in a module with an export\\", done => {\\n    import(\\"../src/d\\").then(d => {\\n      // .default is the default export\\n      expect(d.d).toBe(\\"d\\");\\n\\n      done();\\n    }\\n  });\\n\\n  it(\\"await import results in a module with a default export\\", async done => {\\n    const c = await import(\\"../src/c\\");\\n\\n    // .default is the default export\\n    expect(c.default).toBe(\\"c\\");\\n\\n    done();\\n  });\\n\\n  it(\\"await import results in a module with an export\\", async done => {\\n    const d = await import(\\"../src/d\\");\\n\\n    expect(d.d).toBe(\\"d\\");\\n\\n    done();\\n  });\\n});\\n```\\n\\nAs you can see, it\'s possible to use the dynamic `import` as a `Promise` directly. Alternatively, it\'s possible to consume the imported module using TypeScripts support for `async` / `await`. For my money the latter option makes for much clearer code.\\n\\nIf you\'re looking for a complete example of how to use the new syntax then you could do worse than taking the existing test pack and tweaking it to your own ends. The only change you\'d need to make is to strip out the `resolveLoader` statements in `webpack.config.js` and `karma.conf.js`. (They exist to lock the test in case to the freshly built ts-loader stored locally. You\'ll not need this.)\\n\\nYou can find the test in question [here](https://github.com/TypeStrong/ts-loader/tree/master/test/execution-tests/2.4.1_babel-importCodeSplitting). Happy code splitting!"},{"id":"/2017/06/11/windows-defender-step-away-from-npm","metadata":{"permalink":"/2017/06/11/windows-defender-step-away-from-npm","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2017-06-11-windows-defender-step-away-from-npm/index.md","source":"@site/blog/2017-06-11-windows-defender-step-away-from-npm/index.md","title":"Windows Defender Step Away From npm","description":"Update 18/06/2017","date":"2017-06-11T00:00:00.000Z","formattedDate":"June 11, 2017","tags":[{"label":"VS Code","permalink":"/tags/vs-code"},{"label":"code","permalink":"/tags/code"},{"label":"Windows","permalink":"/tags/windows"},{"label":"failed","permalink":"/tags/failed"},{"label":"npm install","permalink":"/tags/npm-install"},{"label":"windows defender","permalink":"/tags/windows-defender"}],"readingTime":1.68,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Windows Defender Step Away From npm","authors":"johnnyreilly","tags":["VS Code","code","Windows","failed","npm install","windows defender"],"hide_table_of_contents":false},"prevItem":{"title":"Dynamic import: I\'ve been awaiting you...","permalink":"/2017/07/02/dynamic-import-ive-been-await-ing-you"},"nextItem":{"title":"TypeScript: Spare the Rod, Spoil the Code","permalink":"/2017/05/20/typescript-spare-rod-spoil-code"}},"content":"## Update 18/06/2017\\n\\nWhilst things did improve by fiddling with Windows Defender it wasn\'t a 100% fix which makes me wary. Interestingly, VS Code was always open when I did experience the issue and I haven\'t experienced it when it\'s been closed. So it may be the cause. I\'ve opened [an issue for this against the VS Code repo](https://github.com/Microsoft/vscode/issues/28593) \\\\- it sounds like other people may be affected as I was. Perhaps this is VS Code and not Windows Defender. Watch that space...\\n\\n## Update 12/07/2017\\n\\nThe issue was VS Code. The bug has now been fixed and shipped last night with [VS Code 1.14.0](https://code.visualstudio.com/updates/v1_14). Yay!\\n\\n---\\n\\nI\'ve recently experienced many of my `npm install`s failing for no consistent reason. The error message would generally be something along the lines of:\\n\\n```sh\\nnpm ERR! Error: EPERM: operation not permitted, rename \'C:\\\\dev\\\\training\\\\drrug\\\\node_modules\\\\.staging\\\\@exponent\\\\ngrok-fc327f2a\' -> \'C:\\\\dev\\\\training\\\\drrug\\\\node_modules\\\\@exponent\\\\ngrok\'\\n```\\n\\nI spent a good deal of time changing the versions of node and npm I was running; all seemingly to no avail. Regular flakiness which I ascribed to node / npm. I was starting to give up when I read of [other people experiencing similar issues](https://github.com/react-community/create-react-native-app/issues/191#issuecomment-304073970). Encouragingly [Fernando Meira](https://github.com/fmeira) suggested a solution:\\n\\n> I got the same problem just doing an npm install. Run with antivirus disabled (if you use Windows Defender, turn off Real-Time protection and Cloud-based protection). That worked for me!\\n\\nI didn\'t really expect this to work - Windows Defender has been running in the background of my Windows 10 laptop since I\'ve had it. There\'s been no problems with npm installs up until a week or so ago. But given the experience I and others have had I thought I should put it out there: it looks like Windows Defender has it in for npm. Go figure.\\n\\nAlas Windows Defender doesn\'t stay dead for long; it\'s like a zombie that rises from the grave no matter how many times you kill it. So you might want to try configuring it to ignore node.exe:\\n\\n![](Screenshot-2017-06-11-15.05.47.png)\\n\\nOr switching to Linux..."},{"id":"/2017/05/20/typescript-spare-rod-spoil-code","metadata":{"permalink":"/2017/05/20/typescript-spare-rod-spoil-code","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2017-05-20-typescript-spare-rod-spoil-code/index.md","source":"@site/blog/2017-05-20-typescript-spare-rod-spoil-code/index.md","title":"TypeScript: Spare the Rod, Spoil the Code","description":"I\'ve recently started a new role. Perhaps unsurprisingly, part of the technology stack is TypeScript. A couple of days into the new codebase I found a bug. Well, I say I found a bug, TypeScript and VS Code found the bug - I just let everyone else know.","date":"2017-05-20T00:00:00.000Z","formattedDate":"May 20, 2017","tags":[{"label":"tsconfig.json","permalink":"/tags/tsconfig-json"},{"label":"TypeScript","permalink":"/tags/type-script"}],"readingTime":2.075,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"TypeScript: Spare the Rod, Spoil the Code","authors":"johnnyreilly","tags":["tsconfig.json","TypeScript"],"hide_table_of_contents":false},"prevItem":{"title":"Windows Defender Step Away From npm","permalink":"/2017/06/11/windows-defender-step-away-from-npm"},"nextItem":{"title":"Setting Build Version Using AppVeyor and ASP.Net Core","permalink":"/2017/04/25/setting-build-version-using-appveyor"}},"content":"I\'ve recently started a new role. Perhaps unsurprisingly, part of the technology stack is TypeScript. A couple of days into the new codebase I found a bug. Well, I say I found a bug, TypeScript and VS Code found the bug - I just let everyone else know.\\n\\nThe flexibility that TypeScript offers in terms of compiler settings is second to none. You can turn up the dial of strictness to your hearts content. Or down. I\'m an \\"up\\" man myself.\\n\\nThe project that I am working on has the dial set fairly low; it\'s pretty much using the default compiler values which are (sensibly) not too strict. I have to say this makes sense for helping people get on board with using TypeScript. Start from a point of low strictness and turn it up when you\'re ready. As you might have guessed, I cranked the dial up on day one on my own machine. I should say that as I did this, I didn\'t foist this on the project at large - I kept it just to my build... I\'m not \\\\***that**\\\\* guy!\\n\\nI made the below changes to the `tsconfig.json` file. Details of what each of these settings does can be found in the documentation [here](https://www.typescriptlang.org/docs/handbook/compiler-options.html).\\n\\n```json\\n\\"noImplicitAny\\": true,\\n    \\"noImplicitThis\\": true,\\n    \\"noUnusedLocals\\": true,\\n    \\"noImplicitReturns\\": true,\\n    \\"noUnusedParameters\\": true,\\n```\\n\\nI said I found a bug. The nature of the bug was an unused variable; a variable was created in a function but then not used. Here\'s a super simple example:\\n\\n```ts\\nfunction sayHi(name: string) {\\n  const greeting = `Hi ${name}`;\\n  return name;\\n}\\n```\\n\\nIt\'s an easy mistake to make. I\'ve made this mistake before myself. But with the `noUnusedLocals` compiler setting in place it\'s now an easy mistake to catch; VS Code lets you know loud and clear:\\n\\n![](Screenshot-2017-05-20-05.58.54.png)\\n\\nThe other compiler settings will similarly highlight simple mistakes it\'s possible to make and I\'d recommend using them. I should say I\'ve written this from the perspective of a VS Code user, but this really applies generally to TypeScript usage. So whether you\'re an [alm.tools](http://alm.tools/) guy, a WebStorm gal or something else entirely then this too can be yours!\\n\\nI\'d also say that the `strictNullChecks` compiler setting is worth looking into. However, switching an already established project to using that can involve fairly extensive code changes and will also require a certain amount of education of, and buy in from, your team. So whilst I\'d recommend it too, I\'d save that one until last."},{"id":"/2017/04/25/setting-build-version-using-appveyor","metadata":{"permalink":"/2017/04/25/setting-build-version-using-appveyor","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2017-04-25-setting-build-version-using-appveyor/index.md","source":"@site/blog/2017-04-25-setting-build-version-using-appveyor/index.md","title":"Setting Build Version Using AppVeyor and ASP.Net Core","description":"AppVeyor has support for setting the version of a binary during a build. However - this deals with the classic ASP.Net world of AssemblyInfo. I didn\'t find any reference to support for doing the same with dot net core. Remember, dot net core relies upon a &lt;Version&gt; or a &lt;VersionPrefix&gt; setting in the .csproj file. Personally, &lt;Version&gt; is my jam.","date":"2017-04-25T00:00:00.000Z","formattedDate":"April 25, 2017","tags":[{"label":"powershell","permalink":"/tags/powershell"},{"label":"Version","permalink":"/tags/version"},{"label":"dot net core","permalink":"/tags/dot-net-core"},{"label":"AppVeyor","permalink":"/tags/app-veyor"}],"readingTime":1.025,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Setting Build Version Using AppVeyor and ASP.Net Core","authors":"johnnyreilly","tags":["powershell","Version","dot net core","AppVeyor"],"hide_table_of_contents":false},"prevItem":{"title":"TypeScript: Spare the Rod, Spoil the Code","permalink":"/2017/05/20/typescript-spare-rod-spoil-code"},"nextItem":{"title":"I\'m looking for work!","permalink":"/2017/03/30/im-looking-for-work"}},"content":"AppVeyor has [support for setting the version of a binary during a build](https://www.appveyor.com/docs/build-configuration/#assemblyinfo-patching). However - this deals with the classic ASP.Net world of `AssemblyInfo`. I didn\'t find any reference to support for doing the same with dot net core. Remember, dot net core [relies upon a `&lt;Version&gt;` or a `&lt;VersionPrefix&gt;` setting in the `.csproj` file](https://docs.microsoft.com/en-us/dotnet/articles/core/tools/project-json-to-csproj#version). Personally, `&lt;Version&gt;` is my jam.\\n\\nHowever, coming up with your own bit of powershell that stamps the version during the build is a doddle; here we go:\\n\\n```ps\\nParam($projectFile, $buildNum)\\n\\n$content = [IO.File]::ReadAllText($projectFile)\\n\\n$regex = new-object System.Text.RegularExpressions.Regex (\'(<version>)([\\\\d]+.[\\\\d]+.[\\\\d]+)(.[\\\\d]+)(<\\\\/Version>)\',\\n         [System.Text.RegularExpressions.RegexOptions]::MultiLine)\\n\\n$version = $null\\n$match = $regex.Match($content)\\nif($match.Success) {\\n    # from \\"<version>1.0.0.0</version>\\" this will extract \\"1.0.0\\"\\n    $version = $match.groups[2].value\\n}\\n\\n# suffix build number onto $version. eg \\"1.0.0.15\\"\\n$version = \\"$version.$buildNum\\"\\n\\n# update \\"<version>1.0.0.0</version>\\" to \\"<version>$version</version>\\"\\n$content = $regex.Replace($content, \'${1}\' + $version + \'${4}\')\\n\\n# update csproj file\\n[IO.File]::WriteAllText($projectFile, $content)\\n\\n# update AppVeyor build\\nUpdate-AppveyorBuild -Version $version\\n</version>\\n```\\n\\nYou can invoke this script as part of the build process in AppVeyor by adding something like this to your `appveyor.yml`.\\n\\n```yml\\nbefore_build:\\n  - ps: .\\\\ModifyVersion.ps1 $env:APPVEYOR_BUILD_FOLDER\\\\src\\\\Proverb.Web\\\\Proverb.Web.csproj $env:APPVEYOR_BUILD_NUMBER\\n```\\n\\nIt will keep the first 3 parts of the version in your `.csproj` (eg \\"1.0.0\\") and suffix on the build number supplied by AppVeyor."},{"id":"/2017/03/30/im-looking-for-work","metadata":{"permalink":"/2017/03/30/im-looking-for-work","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2017-03-30-im-looking-for-work/index.md","source":"@site/blog/2017-03-30-im-looking-for-work/index.md","title":"I\'m looking for work!","description":"My name is John Reilly. I\'m a full stack developer based in London, UK. I\'m just coming to the end of a contract (due to finish in April 2017) and I\'m starting to look for my next role.","date":"2017-03-30T00:00:00.000Z","formattedDate":"March 30, 2017","tags":[],"readingTime":2.5,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"I\'m looking for work!","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false},"prevItem":{"title":"Setting Build Version Using AppVeyor and ASP.Net Core","permalink":"/2017/04/25/setting-build-version-using-appveyor"},"nextItem":{"title":"Debugging ASP.Net Core in VS or Code","permalink":"/2017/03/28/debugging-aspnet-core-in-vs-or-code"}},"content":"My name is John Reilly. I\'m a full stack developer based in London, UK. I\'m just coming to the end of a contract (due to finish in April 2017) and I\'m starting to look for my next role.\\n\\nI have more than 15 years experience developing software commercially. I\'ve worked in a number of industries including telecoms, advertising, technology (I worked at Microsoft for a time) and, of course, finance. The bulk of my experience is in the finance sector. I\'ve provided consultancy services, building and maintaining applications for both large and small companies; from enterprise to startup.\\n\\nMy most recent work has been full stack web work; using React on the front end and SignalR (ASP.Net) on the back end. I\'m pragmatic about the tools that I use to deliver software solutions and not tied to any particular technology. That said, I\'ve gravitated towards the handiwork of [Anders Hejlsberg](https://en.wikipedia.org/wiki/Anders_Hejlsberg); starting out with Delphi and being both an early C# and TypeScript adopter. I\'ve built everything from high volume trade feeds with no UI beyond a log file, WinForms apps for call centres, to fully fledged rich web applications with a heavy emphasis on UX.\\n\\nI enjoy the challenges of understanding problems and coming up with useful solutions to them. I\'m thrilled when something I\'ve built makes someone\'s life easier. I love to learn and to share my knowledge; both in person and also through writing this blog. (This is the first time I\'ve used a post to seek work.)\\n\\nIn my spare time I\'m involved with various open source projects including [ts-loader](https://github.com/typestrong/ts-loader) and [DefinitelyTyped](https://github.com/DefinitelyTyped/DefinitelyTyped) ([member of the core team](https://github.com/orgs/DefinitelyTyped/people)). Get in contact with me if you\'re interested in learning more about me. Mail me at [johnny_reilly@hotmail.com](mailto:johnny_reilly@hotmail.com) and I can provide you with a CV. You can also find me on [GitHub](https://github.com/johnnyreilly).\\n\\n## Update 25/04/2016: Position Filled\\n\\nI\'m happy to say that I\'ve lined up work for the next 6 months or so. Once again I\'ll be working in the financial services industry with one interesting twist. [In a blog post ages ago I bet that native apps would start to be replaced with SPAs.](https://blog.johnnyreilly.com/2014/02/wpf-and-mystic-meg-or-playing.html) This has started to happen. I\'ve started to see companies taking a \\"web-first-and-only\\" approach to building apps. In that vein, that\'s exactly what I\'m off to build.\\n\\nAs a result of publishing this blog post I\'ve had some interesting conversations with companies and got to think hard about the direction the industry is taking. I remain excited by JavaScript / TypeScript and React. I\'m hopeful of the possibilities offered by the container world of Docker etc. I\'m enjoying .NET Core and have very high hopes for it. I remain curious about Web Assembly.\\n\\nBefore I sign off, I know at some point I\'ll be looking for work once again. If there\'s a system you\'d like built, if there\'s some mentoring and training you\'d like done or if you\'d just like to have a conversation I\'m always available to talk. Drop me a line at [johnny_reilly@hotmail.com](mailto:johnny_reilly@hotmail.com)."},{"id":"/2017/03/28/debugging-aspnet-core-in-vs-or-code","metadata":{"permalink":"/2017/03/28/debugging-aspnet-core-in-vs-or-code","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2017-03-28-debugging-aspnet-core-in-vs-or-code/index.md","source":"@site/blog/2017-03-28-debugging-aspnet-core-in-vs-or-code/index.md","title":"Debugging ASP.Net Core in VS or Code","description":"I\'ve been using Visual Studio for a long time. Very good it is too. However, it is heavyweight; it does far more than I need. What I really want when I\'m working is a fast snappy editor, with intellisense and debugging. What I\'ve basically described is VS Code. It rocks and has long become my go-to editor for TypeScript.","date":"2017-03-28T00:00:00.000Z","formattedDate":"March 28, 2017","tags":[{"label":"VS Code","permalink":"/tags/vs-code"},{"label":"ASP.Net Core","permalink":"/tags/asp-net-core"},{"label":"Visual Studio","permalink":"/tags/visual-studio"}],"readingTime":3.7,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Debugging ASP.Net Core in VS or Code","authors":"johnnyreilly","tags":["VS Code","ASP.Net Core","Visual Studio"],"hide_table_of_contents":false},"prevItem":{"title":"I\'m looking for work!","permalink":"/2017/03/30/im-looking-for-work"},"nextItem":{"title":"Under the Duck: An Afternoon in Open Source","permalink":"/2017/02/23/under-duck-afternoon-in-open-source"}},"content":"I\'ve been using Visual Studio for a long time. Very good it is too. However, it is heavyweight; it does far more than I need. What I really want when I\'m working is a fast snappy editor, with intellisense and debugging. What I\'ve basically described is [VS Code](https://code.visualstudio.com/). It rocks and has long become my go-to editor for TypeScript.\\n\\nSince I\'m a big C# fan as well I was delighted that editing C# was also possible in Code. What I want now is to be able to debug ASP.Net Core in Visual Studio OR VS Code. Can it be done? Let\'s see....\\n\\nI fire up Visual Studio and `File -&gt; New Project` (yes it\'s a verb now). Select .NET Core and then ASP.Net Core Web Application. OK. We\'ll go for a Web Application. Let\'s not bother with authentication. OK. Wait a couple of seconds and Visual Studio serves up a new project. Hit F5 and we\'re debugging in Visual Studio.\\n\\nSo far, so straightforward. What will VS Code make of this?\\n\\nI cd my way to the root of my new ASP.Net Core Web Application and type the magical phrase \\"code .\\". Up it fires. I feel lucky, let\'s hit \\"F5\\". Huh, a dropdown shows up saying `\\"Select Environment\\"` and offering me the options of Chrome and Node. Neither do I want. It\'s about this time I remember this is a clean install of VS Code and doesn\'t yet have the C# extension installed. In fact, if I open a C# file it up it tells me and recommends that I install. Well that\'s nice. I take it up on the kind offer; install and reload.\\n\\nWhen it comes back up I see the following entries in the \\"output\\" tab:\\n\\n```ts\\nUpdating C# dependencies...\\nPlatform: win32, x86_64 (win7-x64)\\n\\nDownloading package \'OmniSharp (.NET 4.6 / x64)\' (20447 KB) .................... Done!\\nDownloading package \'.NET Core Debugger (Windows / x64)\' (39685 KB) .................... Done!\\n\\nInstalling package \'OmniSharp (.NET 4.6 / x64)\'\\nInstalling package \'.NET Core Debugger (Windows / x64)\'\\n\\nFinished\\n```\\n\\nNote that mention of \\"debugger\\" there? Sounds super-promising. There\'s also some prompts: `\\"There are unresolved dependencies from \'WebApplication1/WebApplication1.csproj\'. Please execute the restore command to continue\\"`\\n\\nSo it wants me to `dotnet restore`. It\'s even offering to do that for me! Have at you; I let it.\\n\\n```ts\\nWelcome to .NET Core!\\n---------------------\\nLearn more about .NET Core @ https://aka.ms/dotnet-docs. Use dotnet --help to see available commands or go to https://aka.ms/dotnet-cli-docs.\\n\\nTelemetry\\n--------------\\nThe .NET Core tools collect usage data in order to improve your experience. The data is anonymous and does not include command-line arguments. The data is collected by Microsoft and shared with the community.\\nYou can opt out of telemetry by setting a DOTNET_CLI_TELEMETRY_OPTOUT environment variable to 1 using your favorite shell.\\nYou can read more about .NET Core tools telemetry @ https://aka.ms/dotnet-cli-telemetry.\\n\\nConfiguring...\\n-------------------\\nA command is running to initially populate your local package cache, to improve restore speed and enable offline access. This command will take up to a minute to complete and will only happen once.\\nDecompressing Decompressing 100% 4026 ms\\nExpanding 100% 34814 ms\\n  Restoring packages for c:\\\\Source\\\\Debugging\\\\WebApplication1\\\\WebApplication1\\\\WebApplication1.csproj...\\n  Restoring packages for c:\\\\Source\\\\Debugging\\\\WebApplication1\\\\WebApplication1\\\\WebApplication1.csproj...\\n  Restore completed in 734.05 ms for c:\\\\Source\\\\Debugging\\\\WebApplication1\\\\WebApplication1\\\\WebApplication1.csproj.\\n  Generating MSBuild file c:\\\\Source\\\\Debugging\\\\WebApplication1\\\\WebApplication1\\\\obj\\\\WebApplication1.csproj.nuget.g.props.\\n  Writing lock file to disk. Path: c:\\\\Source\\\\Debugging\\\\WebApplication1\\\\WebApplication1\\\\obj\\\\project.assets.json\\n  Restore completed in 1.26 sec for c:\\\\Source\\\\Debugging\\\\WebApplication1\\\\WebApplication1\\\\WebApplication1.csproj.\\n\\n  NuGet Config files used:\\n      C:\\\\Users\\\\johnr\\\\AppData\\\\Roaming\\\\NuGet\\\\NuGet.Config\\n      C:\\\\Program Files (x86)\\\\NuGet\\\\Config\\\\Microsoft.VisualStudio.Offline.config\\n\\n  Feeds used:\\n      https://api.nuget.org/v3/index.json\\n      C:\\\\Program Files (x86)\\\\Microsoft SDKs\\\\NuGetPackages\\\\\\nDone: 0.\\n```\\n\\nThe other prompt says `\\"Required assets to build and debug are missing from \'WebApplication1\'. Add them?\\"`. This also sounds very promising and I give it the nod. This creates a `.vscode` directory and 2 enclosed files; `launch.json` and `tasks.json`.\\n\\nSo lets try that F5 thing again... http://localhost:5000/ is now serving the same app. That looks pretty good. So lets add a breakpoint to the `HomeController` and see if we can hit it:\\n\\n![](firstgo.png)\\n\\nWell I can certainly add a breakpoint but all those red squigglies are unnerving me. Let\'s clean the slate. If you want to simply do that in VS Code hold down `CTRL+SHIFT+P` and then type \\"reload\\". Pick \\"Reload window\\". A couple of seconds later we\'re back in and Code is looking much happier. Can we hit our breakpoint?\\n\\n![](secondgo.png)\\n\\nYes we can! So you\'re free to develop in either Code or VS; the choice is yours. I think that\'s pretty awesome - and well done to all the peeople behind Code who\'ve made this a pretty seamless experience!"},{"id":"/2017/02/23/under-duck-afternoon-in-open-source","metadata":{"permalink":"/2017/02/23/under-duck-afternoon-in-open-source","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2017-02-23-under-duck-afternoon-in-open-source/index.md","source":"@site/blog/2017-02-23-under-duck-afternoon-in-open-source/index.md","title":"Under the Duck: An Afternoon in Open Source","description":"Have you ever wondered what happens behind the scenes of open source projects? One that I\'m involved with is ts-loader; a TypeScript loader for webpack. Yesterday was an interesting day in the life of ts-loader and webpack; things unexpectedly broke. Oh and don\'t worry, they\'re fixed now.","date":"2017-02-23T00:00:00.000Z","formattedDate":"February 23, 2017","tags":[{"label":"breaking changes","permalink":"/tags/breaking-changes"},{"label":"ts-loader","permalink":"/tags/ts-loader"},{"label":"Webpack","permalink":"/tags/webpack"}],"readingTime":5.25,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Under the Duck: An Afternoon in Open Source","authors":"johnnyreilly","tags":["breaking changes","ts-loader","Webpack"],"hide_table_of_contents":false},"prevItem":{"title":"Debugging ASP.Net Core in VS or Code","permalink":"/2017/03/28/debugging-aspnet-core-in-vs-or-code"},"nextItem":{"title":"@types is rogue","permalink":"/2017/02/14/typescript-types-and-repeatable-builds"}},"content":"Have you ever wondered what happens behind the scenes of open source projects? One that I\'m involved with is [ts-loader](https://github.com/typestrong/ts-loader); a TypeScript loader for webpack. Yesterday was an interesting day in the life of ts-loader and webpack; things unexpectedly broke. Oh and don\'t worry, they\'re fixed now.\\n\\nHow things panned out reflects well on the webpack community. I thought it might be instructive to take a look at the legs furiously paddling underneath the duck of open source. What follows is a minute by minute account of my life on the afternoon of Wednesday 22nd February 2017:\\n\\n### 3:55pm\\n\\nI\'m sat at my desk in the City of London. I have to leave at 4pm to go to the dentist. I\'m working away on a project which is built and bundled using ts-loader and webpack. However, having just npm installed and tried to spin up webpack in watch mode, I discover that everything is broken. Watch mode is not working - there\'s an error being thrown in ts-loader. It\'s to do with a webpack property called `mtimes`. ts-loader depends upon it and it looks like it is no longer always passed through. Go figure. ### 4:01pm\\n\\nI\'ve got to go. I\'m 15 minutes from Bank station. So, I grab my bag and scarper out the door. On my phone I notice [an issue](https://github.com/TypeStrong/ts-loader/issues/479) has been raised - other people are being affected by the problem too. As I trot down the various alleys that lead to the station I wonder whether I can work around this issue. Using GitHub to fork, edit code and submit a PR on a mobile phone is possible. Just. But it\'s certainly not easy...\\n\\n[My PR is in](https://github.com/TypeStrong/ts-loader/pull/481), the various test packs are starting to execute somewhere out there in Travis and Appveyor-land. Then I notice [Ed Bishop](https://github.com/mredbishop) has submitted a [near identical PR](https://github.com/TypeStrong/ts-loader/pull/480). Yay Ed! I\'m always keen to encourage people to contribute and so I intend to merge that PR rather than my own.\\n\\n### 16:12\\n\\nRubbish. The Waterloo and City Line is out of action. I need to get across London to reach Waterloo or I\'ll miss my appointment. It\'s time to start running....\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/4IBGernmtKA\\" frameBorder=\\"0\\" allowFullScreen=\\"\\"></iframe>\\n\\n### 16:15\\n\\nIt\'s rather nagging at me that behaviour has changed without warning. This has been reliably in place the entire time I\'ve been involved with ts-loader / webpack. Why now? I don\'t see any obvious mentions on the webpack GitHub repo. So I head over to the webpack Slack channel and ask: (conversation slightly abridged)\\n\\n> #### johnny_reilly\\n>\\n> Hey all, has something happened to `mtimes`? Behaviour seems to have changed - now undefined occasionally during watch mode. A PR has been raised against ts-loader to work around this [https://github.com/TypeStrong/ts-loader/pull/480#issuecomment-281714600](https://github.com/TypeStrong/ts-loader/pull/480#issuecomment-281714600)\\n>\\n> However I\'m wondering if this should actually be merged given behaviour has changed unexpectedly\\n>\\n> #### sokra\\n>\\n> ah...\\n>\\n> i removed it. I thought it was unused.\\n>\\n> #### johnny_reilly\\n>\\n> It\'s definitely not!\\n>\\n> #### sokra\\n>\\n> it\'s not in the public API^^\\n>\\n> Any reason why you are not using `getTimes()`?\\n>\\n> ...\\n>\\n> #### johnny_reilly\\n>\\n> Okay, I\'m on a train and won\'t be near a computer for a while. ts-loader is presently broken because it depends on mtimes. Would it be possible for you to add this back at least for now. I\'m aware many people depend on ts-loader and are now broken. #### sokra\\n>\\n> sure, I readd it but deprecate it.\\n>\\n> ...\\n>\\n> #### sean.larkin\\n>\\n> @sokra is this the change you just made for that watchpack bug fix? Or unlrelated, just wanted to track if I didn\'t already have the change/issue #### sokra\\n>\\n> [https://github.com/webpack/watchpack/pull/48](https://github.com/webpack/watchpack/pull/48)\\n>\\n> #### johnny_reilly\\n>\\n> This is what the present code does:\\n>\\n> ```js\\n> const watcher =\\n>   watching.compiler.watchFileSystem.watcher ||\\n>   watching.compiler.watchFileSystem.wfs.watcher;\\n> ```\\n>\\n> And then `.mtimes`\\n>\\n> Should I be able to do `.getTimes()` instead?\\n>\\n> #### sokra\\n>\\n> actually you can\'t rely on `watchFileSystem` being `NodeJsWatchFileSystem`. But this is another topic\\n>\\n> ...\\n>\\n> but yes\\n>\\n> #### johnny_reilly\\n>\\n> Thanks @sokra - when I get to a keyboard I\'ll swap `mtimes` for `getTimes()` and report back.\\n\\n### 17:28\\n\\nDespite various trains being out of action / missing in action I\'ve made it to the dentists; phew! I go in for my checkup and plan to take a look at the issue later that evening. In the meantime I\'ve hoping that Tobias ([Sokra](https://twitter.com/wsokra)) will get chance to republish so that ts-loader users aren\'t too impacted.\\n\\n### 18:00\\n\\nDone at the dentist and I\'m heading home. Whilst I\'ve been opening wide and squinting at the ceiling, [TypeScript 2.2 has shipped](https://blogs.msdn.microsoft.com/typescript/2017/02/22/announcing-typescript-2-2/). Whilst this is super exciting, according to Greenkeeper, [the new version has broken the build](https://github.com/TypeStrong/ts-loader/pull/483). Arrrrghhhh...\\n\\nI start to look into this and realise we\'re not broken because of TypeScript 2.2; we were broken because of the `mtimes`. Tobias has now re-added `mtimes` and published. With that in place I requeue a build and.... drum roll.... we\'re green!\\n\\nThe good news just keeps on coming as [Luka Zakraj\u0161ek](https://twitter.com/bancek) has submitted a [PR which uses `getTimes()` in place of `mtimes`](https://github.com/TypeStrong/ts-loader/pull/482). And the tests pass. Awesome! MERGE. I just need to cut a release and we\'re done.\\n\\n### 18:15\\n\\nI\'m home. My youngest son has been suffering from chicken pox all week and as a result my wife has been in isolation, taking care of him. We chat whilst the boys watch Paw Patrol as the bath runs. I flick open the laptop and start doing the various housekeeping tasks around cutting a release. This is interrupted by various bathtime / bedtime activities and I abandon work for now.\\n\\n### 19:30\\n\\nThe boys are down and I get on with the release; updating the changelog, bumping the version number and running the tests. For various reasons this takes longer than it normally does.\\n\\n### 20:30\\n\\nFinally we\'re there; ts-loader 2.0.1 ships: [https://github.com/TypeStrong/ts-loader/releases/tag/v2.0.1](https://github.com/TypeStrong/ts-loader/releases/tag/v2.0.1).\\n\\nI\'m tremendously grateful to everyone that helped out - thank you all!\\n\\n> ts-loader 2.0.1 has shipped; thanks [@wsokra](https://twitter.com/wSokra)[@bancek](https://twitter.com/bancek) and @mredbishop [https://t.co/I00c7sJyFo](https://t.co/I00c7sJyFo)[\\\\#typescript](https://twitter.com/hashtag/typescript?src=hash)\\n>\\n> \u2014 John Reilly (@johnny_reilly) [February 22, 2017](https://twitter.com/johnny_reilly/status/834515296077627392)\\n\\n<script async=\\"\\" src=\\"//platform.twitter.com/widgets.js\\" charSet=\\"utf-8\\"><\/script>"},{"id":"/2017/02/14/typescript-types-and-repeatable-builds","metadata":{"permalink":"/2017/02/14/typescript-types-and-repeatable-builds","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2017-02-14-typescript-types-and-repeatable-builds/index.md","source":"@site/blog/2017-02-14-typescript-types-and-repeatable-builds/index.md","title":"@types is rogue","description":"Or perhaps I should call this \\"@types and repeatable builds\\"....","date":"2017-02-14T00:00:00.000Z","formattedDate":"February 14, 2017","tags":[],"readingTime":2.025,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"@types is rogue","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false},"prevItem":{"title":"Under the Duck: An Afternoon in Open Source","permalink":"/2017/02/23/under-duck-afternoon-in-open-source"},"nextItem":{"title":"Hands-free HTTPS","permalink":"/2017/02/01/hands-free-https"}},"content":"Or perhaps I should call this \\"@types and repeatable builds\\"....\\n\\nThe other day, on a React / TypeScript project I work on, the nightly CI build started failing. But nothing had changed in the project... What gives? After digging I discovered the reason; spome of the type definitions which my project depends upon had changed. Why did this break my build? Let\u2019s learn some more...\\n\\nWe acquire type definitions via npm. Type definitions from Definitely Typed are published to npm by an [automated process](https://github.com/Microsoft/types-publisher) and they are all published under the @types namespace on npm. So, the [react type definition](https://www.npmjs.com/package/react) is published as the [@types/react](https://www.npmjs.com/package/@types/react) package, the node type definition is published as the [@types/node](https://www.npmjs.com/package/@types/node) package. The hip bone\'s connected to the thigh bone. You get the picture.\\n\\nThe npm ecosystem is essentially built on top of [semantic versioning](http://semver.org/) and they [take it seriously](https://docs.npmjs.com/getting-started/semantic-versioning). Essentially, when a package is published it should be categorised as a major release (breaking changes), a minor release (extra functionality which is backwards compatible) or a patch release (backwards compatible bug fixes).\\n\\nNow we get to the meat of the matter: @types is rogue. You cannot trust the version numbers on @types packages to respect semantic versioning. They don\'t.\\n\\nThe main reason for this is that when it comes to versioning, the @types type definition essentially looks to mirror the version of the package they are seeking to type. _THIS MEANS THE TYPE DEFINITION CANNOT DO ITS OWN SEMANTIC VERSIONING._ A simple change in a type definition can lead to breakages in consuming code. That\'s what happened to me. Let\'s say an exported interface name changes; all code that relies upon the old name will now break. You see? Pain.\\n\\n## How do we respond to this?\\n\\nMy own take has been to pin the version numbers of @types packages; fixing to specific definitions. No `\\"~\\"` or `\\"^\\"` for my `@types devDependencies`.\\n\\nNo respect semantic versioning? No problem. You can go much further with repeatable builds and made use of [facebook\'s new npm client yarn](https://code.facebook.com/posts/1840075619545360) and [lockfiles](https://yarnpkg.com/blog/2016/11/24/lockfiles-for-all/) (very popular BTW) but I haven\'t felt the need yet. This should be ample for now.\\n\\nThe other question that may be nagging at your subconscious is this: what\u2019s an easy way to know when new packages are available for my project dependencies? Well, the `Get-Package -Updates` (nuget hat tip) for npm that I\u2019d recommend is this: [npm-check-updates](https://www.npmjs.com/package/npm-check-updates). It does the job wonderfully."},{"id":"/2017/02/01/hands-free-https","metadata":{"permalink":"/2017/02/01/hands-free-https","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2017-02-01-hands-free-https/index.md","source":"@site/blog/2017-02-01-hands-free-https/index.md","title":"Hands-free HTTPS","description":"I have had a \\\\*great\\\\* week. You? Take a look at this blog. Can you see what I can see? Here\'s a clue:","date":"2017-02-01T00:00:00.000Z","formattedDate":"February 1, 2017","tags":[{"label":"TLS","permalink":"/tags/tls"},{"label":"free","permalink":"/tags/free"},{"label":"HTTPS","permalink":"/tags/https"},{"label":"CloudFlare","permalink":"/tags/cloud-flare"},{"label":"Troy Hunt","permalink":"/tags/troy-hunt"}],"readingTime":1.62,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Hands-free HTTPS","authors":"johnnyreilly","tags":["TLS","free","HTTPS","CloudFlare","Troy Hunt"],"hide_table_of_contents":false},"prevItem":{"title":"@types is rogue","permalink":"/2017/02/14/typescript-types-and-repeatable-builds"},"nextItem":{"title":"webpack: resolveLoader / alias with query / options","permalink":"/2017/01/06/webpack-resolveloader-alias-with-query"}},"content":"I have had a \\\\***great**\\\\* week. You? Take a look at this blog. Can you see what I can see? Here\'s a clue:\\n\\n![](Screenshot-2017-01-29-14.45.57.png)\\n\\nYup, look at the top left hand corner.... see that beautiful padlock? Yeah - that\'s what\'s thrilled me. You see I have a dream; that one day on the red hills of the internet, the sons of former certificates and the sons of former certificate authorities will be able to sit down together at the table of HTTPS. Peace, love and TLS for all.\\n\\nThe world is turning and slowly but surely HTTPS is becoming the default of the web. [Search results get ranked higher if they\'re HTTPS.](https://security.googleblog.com/2014/08/https-as-ranking-signal_6.html)[HTTP/2 is, to all intents and purposes, a HTTPS-only game.](https://en.wikipedia.org/wiki/HTTP/2#Encryption)[Service Workers are HTTPS-only.](https://developer.mozilla.org/en/docs/Web/API/Service_Worker_API)\\n\\nI care about all of these. So it\'s _essential_ that I have HTTPS. But. But. But... Certificates, the administration that goes with them. It\'s boring. I mean, it just is. I want to be building interesting apps, I don\'t want to be devoting my time to acquiring certificates and fighting my way through the (never simple) administration of them. I\'m dimly aware that there\'s free certificates to be had thanks to the fine work of [LetsEncrypt](https://letsencrypt.org/). I believe that work is being done on reduce the onerous admin burden as well. And that\'s great. But I\'m still avoiding it...\\n\\nWhat if I told you you could have HTTPS on your blog, on your Azure websites, on your anywhere.... _FOR FREE. IN FIVE MINUTES?_. Well, you can thanks to [CloudFlare](https://www.cloudflare.com/). I did; you should too.\\n\\nThis is where I point you off to a number of resources to help you on your HTTPS way:\\n\\n1. [Read Troy Hunt\'s \\"How to get your SSL for free on a Shared Azure website with CloudFlare\\"](https://www.troyhunt.com/how-to-get-your-ssl-for-free-on-shared/)\\n2. [Watch Troy Hunt\'s Pluralsight course \\"Getting Started with CloudFlare\u2122 Security\\"](https://www.pluralsight.com/courses/cloudflare-security-getting-started)\\n3. [Go to Cloudflare\'s website and sign up](https://www.cloudflare.com/)\\n\\nIt just works. And that makes me very happy indeed."},{"id":"/2017/01/06/webpack-resolveloader-alias-with-query","metadata":{"permalink":"/2017/01/06/webpack-resolveloader-alias-with-query","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2017-01-06-webpack-resolveloader-alias-with-query/index.md","source":"@site/blog/2017-01-06-webpack-resolveloader-alias-with-query/index.md","title":"webpack: resolveLoader / alias with query / options","description":"Sometimes you write a post for the ages. Sometimes you write one you hope is out of date before you hit \\"publish\\". This is one of those.","date":"2017-01-06T00:00:00.000Z","formattedDate":"January 6, 2017","tags":[{"label":"enhanced-resolve","permalink":"/tags/enhanced-resolve"},{"label":"Webpack","permalink":"/tags/webpack"},{"label":"query","permalink":"/tags/query"},{"label":"options","permalink":"/tags/options"}],"readingTime":1.375,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"webpack: resolveLoader / alias with query / options","authors":"johnnyreilly","tags":["enhanced-resolve","Webpack","query","options"],"hide_table_of_contents":false},"prevItem":{"title":"Hands-free HTTPS","permalink":"/2017/02/01/hands-free-https"},"nextItem":{"title":"webpack: configuring a loader with query / options","permalink":"/2017/01/01/webpack-configuring-loader-with-query"}},"content":"Sometimes you write a post for the ages. Sometimes you write one you hope is out of date before you hit \\"publish\\". This is one of those.\\n\\nThere\'s a [bug](https://github.com/webpack/enhanced-resolve/issues/41) in webpack\'s enhanced-resolve. It means that you cannot configure an aliased loader using the `query` (or `options` in the webpack 2 nomenclature). Let me illustrate; consider the following code:\\n\\n```js\\nmodule.exports = {\\n  // ...\\n  module: {\\n    loaders: [\\n      {\\n        test: /\\\\.ts$/,\\n        loader: \'ts-loader\',\\n        query: {\\n            entryFileIsJs: true\\n        }\\n      }\\n    ]\\n  }\\n}\\n\\nmodule.exports.resolveLoader = { alias: { \'ts-loader\': require(\'path\').join(__dirname, \\"../../index.js\\")\\n```\\n\\nAt the time of writing, if you alias a loader as above, then the `query` / `options` will \\\\*_not_\\\\* be passed along. This is bad, particularly given the requirement in webpack 2 that configuration is no longer possible through extending the [`webpack.config.js`](https://webpack.js.org/guides/migrating/#loader-configuration-is-through-options). So what to do? Well, when this was a problem previously the marvellous [James Brantly](https://www.twitter.com/jbrantly) had a [workaround](https://github.com/webpack/webpack/issues/1289#issuecomment-125767499). I\'ve taken that and run with it:\\n\\n```js\\nvar config = {\\n  // ...\\n  module: {\\n    loaders: [\\n      {\\n        test: /\\\\.ts$/,\\n        loader: \'ts-loader\',\\n        query: {\\n          entryFileIsJs: true,\\n        },\\n      },\\n    ],\\n  },\\n};\\n\\nmodule.exports = config;\\n\\nvar loaderAliasPath = require(\'path\').join(__dirname, \'../../../index.js\');\\nvar rules = config.module.loaders || config.module.rules;\\nrules.forEach(function (rule) {\\n  var options = rule.query || rule.options;\\n  rule.loader = rule.loader.replace(\\n    \'ts-loader\',\\n    loaderAliasPath + (options ? \'?\' + JSON.stringify(options) : \'\')\\n  );\\n});\\n```\\n\\nThis approach stringifies the `query` / `options` and suffixes it to the aliased path. This works as long as the options you\'re passing are JSON-able (yes it\'s a word).\\n\\nAs I said earlier; hopefully by the time you read this the workaround will no longer be necessary again. But just in case...."},{"id":"/2017/01/01/webpack-configuring-loader-with-query","metadata":{"permalink":"/2017/01/01/webpack-configuring-loader-with-query","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2017-01-01-webpack-configuring-loader-with-query/index.md","source":"@site/blog/2017-01-01-webpack-configuring-loader-with-query/index.md","title":"webpack: configuring a loader with query / options","description":"webpack 2 is on it\'s way. As one of the maintainers of ts-loader I\'ve been checking out that ts-loader works with webpack 2. It does: phew!","date":"2017-01-01T00:00:00.000Z","formattedDate":"January 1, 2017","tags":[{"label":"webpack 2","permalink":"/tags/webpack-2"},{"label":"Webpack","permalink":"/tags/webpack"},{"label":"query","permalink":"/tags/query"},{"label":"options","permalink":"/tags/options"}],"readingTime":2.8,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"webpack: configuring a loader with query / options","authors":"johnnyreilly","tags":["webpack 2","Webpack","query","options"],"hide_table_of_contents":false},"prevItem":{"title":"webpack: resolveLoader / alias with query / options","permalink":"/2017/01/06/webpack-resolveloader-alias-with-query"},"nextItem":{"title":"Using ts-loader with webpack 2","permalink":"/2016/12/19/using-ts-loader-with-webpack-2"}},"content":"[webpack 2 is on it\'s way](https://medium.com/webpack/webpack-2-2-the-release-candidate-2e614d05d75f#.ntniu44u6). As one of the maintainers of [ts-loader](https://github.com/TypeStrong/ts-loader/) I\'ve been checking out that ts-loader works with webpack 2. It does: phew!\\n\\nts-loader has a continuous integration build that runs against webpack 1. When webpack 2 ships we\'re planning to move to running CI against webpack 2. However, webpack 2 has some breaking changes. The one that\'s particularly of relevance to our test packs is that a strict schema is now enforced for `webpack.config.js` with webpack 2. This has been the case since webpack 2 hit beta 23. Check the [PR that added it](https://github.com/webpack/webpack/pull/2974). You can see some of the [frankly tortured discussion that this generated as well](https://github.com/webpack/webpack/issues/3018).\\n\\nLet\'s all take a moment and realise that working on open source is sometimes a rather painful experience. Take a breath. Breathe out. Ready to carry on? Great.\\n\\nThere are 2 ways to configure loader options for ts-loader (and in fact this stands for most loaders). Loader options can be set either using a `query` when specifying the loader or through the `ts` (insert the name of alternative loaders here) property in the `webpack.config.js`.\\n\\nThe implicatations of the breaking change are: with webpack 2 you can **no longer** configure ts-loader (or any other loader) with a `ts` (insert the name of alternative loaders here) property in the `webpack.config.js`. It **must** be done through the `query` / `options`. The following code is no longer valid with webpack 2:\\n\\n```js\\nmodule.exports = {\\n  ...\\n  module: {\\n    loaders: [{\\n      test: /\\\\.tsx?$/,\\n      loader: \'ts-loader\'\\n    }]\\n  },\\n  // specify option using `ts` property - **only do this if you are using webpack 1**\\n  ts: {\\n    transpileOnly: false\\n  }\\n}\\n```\\n\\nThis change means that we have needed to adjust how our test pack works. We can no longer make use of `ts` for configuration. Since I wasn\'t terribly aware of `query` I thought it made sense to share my learnings.\\n\\n## What exactly is `query` / `options`?\\n\\nGood question. Well, strictly speaking it\'s 2 possible things; both ways to configure a webpack loader. Classically `query` was a string which could be appended to the name of the loader much like a [`query string`](https://en.wikipedia.org/wiki/Query_string) but actually with [greater powers](https://github.com/webpack/loader-utils#parsequery):\\n\\n```js\\nmodule.exports = {\\n  ...\\n  module: {\\n    loaders: [{\\n      test: /\\\\.tsx?$/,\\n      loader: \'ts-loader?\' + JSON.stringify({\\n        transpileOnly: false\\n      })\\n    }]\\n  }\\n}\\n```\\n\\nBut it can also be a separately specified object that\'s supplied alongside a loader (I understand this is relatively new behaviour):\\n\\n```js\\nmodule.exports = {\\n  ...\\n  module: {\\n    loaders: [{\\n      test: /\\\\.tsx?$/,\\n      loader: \'ts-loader\'\\n      query: {\\n        transpileOnly: false\\n      }\\n    }]\\n  }\\n}\\n```\\n\\n## webpack 2 is coming - look busy!\\n\\nSo if you\'re planning to move to webpack 2, be aware of this breaking change. You can start moving to using configuration via query right now with webpack 1. You don\'t need to be using webpack 2 to make the jump. So jump!\\n\\nFinally, and by way of a PS, `query` is renamed to `options` in webpack 2; a much better name to my mind. There\'s actually a bunch of other renames on the way as well - check out the [migration guide](https://webpack.js.org/guides/migrating/#module-loaders-is-now-module-rules) for more on this. The important thing to note is that **the old names work in webpack 2**. But you should plan to move to the new naming at some point as they\'ll likely disappear when webpack 3 ships."},{"id":"/2016/12/19/using-ts-loader-with-webpack-2","metadata":{"permalink":"/2016/12/19/using-ts-loader-with-webpack-2","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2016-12-19-using-ts-loader-with-webpack-2/index.md","source":"@site/blog/2016-12-19-using-ts-loader-with-webpack-2/index.md","title":"Using ts-loader with webpack 2","description":"Hands up, despite being one of the maintainers of ts-loader (a TypeScript loader for webpack) I have not been tracking webpack v2. My reasons? Well, I\'m keen on cutting edge but bleeding edge is often not a ton of fun as dealing with regularly breaking changes is frustrating. I\'m generally happy to wait for things to settle down a bit before leaping aboard. However, webpack 2 RC\'d last week and so it\'s time to take a look!","date":"2016-12-19T00:00:00.000Z","formattedDate":"December 19, 2016","tags":[{"label":"ts-loader","permalink":"/tags/ts-loader"},{"label":"webpack 2","permalink":"/tags/webpack-2"}],"readingTime":7.845,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Using ts-loader with webpack 2","authors":"johnnyreilly","tags":["ts-loader","webpack 2"],"hide_table_of_contents":false},"prevItem":{"title":"webpack: configuring a loader with query / options","permalink":"/2017/01/01/webpack-configuring-loader-with-query"},"nextItem":{"title":"webpack: syncing the enhanced-resolve","permalink":"/2016/12/11/webpack-syncing-enhanced-resolve"}},"content":"Hands up, despite being one of the maintainers of [ts-loader](https://github.com/TypeStrong/ts-loader) (a TypeScript loader for webpack) I have not been tracking webpack v2. My reasons? Well, I\'m keen on cutting edge but bleeding edge is often not a ton of fun as dealing with regularly breaking changes is frustrating. I\'m generally happy to wait for things to settle down a bit before leaping aboard. However, [webpack 2 RC\'d last week](https://github.com/webpack/webpack/releases/tag/v2.2.0-rc.0) and so it\'s time to take a look!\\n\\n## Porting our example\\n\\nLet\'s take [ts-loader\'s webpack 1 example](https://github.com/TypeStrong/ts-loader/tree/master/examples/webpack1-gulp-react-flux-babel-karma) and try and port it to webpack 2. Will it work? Probably; I\'m aware of other people using ts-loader with webpack 2. It\'ll be a voyage of discovery. Like Darwin on the Beagle, I shall document our voyage for a couple of reasons:\\n\\n- I\'m probably going to get some stuff wrong. That\'s fine; one of the best ways to learn is to make mistakes. So do let me know where I go wrong.\\n- I\'m doing this based on what I\'ve read in the new docs; they\'re very much a work in progress and the mistakes I make here may lead to those docs improving even more. That matters; **documentation matters**. I\'ll be leaning heavily on the [Migrating from v1 to v2](https://webpack.js.org/guides/migrating/) guide.\\n\\nSo here we go. Our example is one which uses TypeScript for static typing and uses Babel to transpile from ES-super-modern (yes - it\'s a thing) to ES-older-than-that. Our example also uses React; but that\'s somewhat incidental. It only uses webpack for typescript / javascript and karma. It uses gulp to perform various other tasks; so if you\'re reliant on webpack for less / sass compilation etc then I have no idea whether that works.\\n\\nFirst of all, let\'s install the latest RC of webpack:\\n\\n```ts\\nnpm install webpack@2.2.0-rc.1 --save-dev\\n```\\n\\n## `webpack.config.js`\\n\\nLet\'s look at our existing `webpack.config.js`:\\n\\n```js\\n\'use strict\';\\n\\nvar path = require(\'path\');\\n\\nmodule.exports = {\\n  cache: true,\\n  entry: {\\n    main: \'./src/main.tsx\',\\n    vendor: [\'babel-polyfill\', \'fbemitter\', \'flux\', \'react\', \'react-dom\'],\\n  },\\n  output: {\\n    path: path.resolve(__dirname, \'./dist/scripts\'),\\n    filename: \'[name].js\',\\n    chunkFilename: \'[chunkhash].js\',\\n  },\\n  module: {\\n    loaders: [\\n      {\\n        test: /\\\\.ts(x?)$/,\\n        exclude: /node_modules/,\\n        loader:\\n          \'babel-loader?presets[]=es2016&presets[]=es2015&presets[]=react!ts-loader\',\\n      },\\n      {\\n        test: /\\\\.js$/,\\n        exclude: /node_modules/,\\n        loader: \'babel\',\\n        query: {\\n          presets: [\'es2016\', \'es2015\', \'react\'],\\n        },\\n      },\\n    ],\\n  },\\n  plugins: [],\\n  resolve: {\\n    extensions: [\'\', \'.webpack.js\', \'.web.js\', \'.ts\', \'.tsx\', \'.js\'],\\n  },\\n};\\n```\\n\\nThere\'s a number of things we need to do here. First of all, we can get rid of the empty extension under resolve; I understand that\'s unnecessary now. Also, I\'m going to get rid of `\'.webpack.js\'` and `\'.web.js\'`; I never used them anyway. Also, just having `\'babel\'` as a loader won\'t fly anymore. We need that suffix as well.\\n\\nNow I could start renaming `loaders` to `rules` as the terminology is changing. But I\'d like to deal with that later since I know the old school names are still supported at present. More interestingly, I seem to remember hearing that one of the super exciting things about webpack is that it supports modules directly now. (I think that\'s supposed to be good for tree-shaking but I\'m not totally certain.)\\n\\nInitially I thought I was supposed to switch to a custom babel preset called [`babel-preset-es2015-webpack`](https://www.npmjs.com/package/babel-preset-es2015-webpack). However it has a big \\"DEPRECATED\\" mark at the top and it says I should just use `babel-preset-es2015` (which I already am) with the following option specified:\\n\\n```js\\n{\\n    \\"presets\\": [\\n        [\\n            \\"es2015\\",\\n            {\\n                \\"modules\\": false\\n            }\\n        ]\\n    ]\\n}\\n```\\n\\nLooking at our existing config you\'ll note that for `js` files we\'re using `query` (`options` in the new world I understand) to configure babel usage. We\'re using [query parameters](https://webpack.github.io/docs/using-loaders.html#query-parameters) for `ts` files. I have _zero_ idea how to configure preset options using query parameters. Fiddling with `query` / `options` didn\'t seem to work. So, I\'ve decided to abandon using query entirely and drop in a [`.babelrc`](http://babeljs.io/docs/usage/babelrc/) file using our presets combined with the [`modules`](https://babeljs.io/docs/plugins/#plugin-preset-options) setting:\\n\\n```js\\n{\\n   \\"presets\\": [\\n      \\"react\\",\\n      [\\n         \\"es2015\\",\\n         {\\n            \\"modules\\": false\\n         }\\n      ],\\n      \\"es2016\\"\\n   ]\\n}\\n```\\n\\nAs an aside; apparently these are applied in reverse order. So `es2016` is applied first, `es2015` second and `react` third. I\'m not totally certain this is correct; the `<a href=\\"http://babeljs.io/docs/usage/babelrc/\\">.babelrc</a> docs` are a little unclear.\\n\\nWith our query options extracted we\'re down to a simpler `webpack.config.js`:\\n\\n```js\\n\'use strict\';\\n\\nvar path = require(\'path\');\\n\\nmodule.exports = {\\n  cache: true,\\n  entry: {\\n    main: \'./src/main.tsx\',\\n    vendor: [\'babel-polyfill\', \'fbemitter\', \'flux\', \'react\', \'react-dom\'],\\n  },\\n  output: {\\n    path: path.resolve(__dirname, \'./dist/scripts\'),\\n    filename: \'[name].js\',\\n    chunkFilename: \'[chunkhash].js\',\\n  },\\n  module: {\\n    loaders: [\\n      {\\n        test: /\\\\.ts(x?)$/,\\n        exclude: /node_modules/,\\n        loader: \'babel-loader!ts-loader\',\\n      },\\n      {\\n        test: /\\\\.js$/,\\n        exclude: /node_modules/,\\n        loader: \'babel-loader\',\\n      },\\n    ],\\n  },\\n  plugins: [],\\n  resolve: {\\n    extensions: [\'.ts\', \'.tsx\', \'.js\'],\\n  },\\n};\\n```\\n\\n## `plugins`\\n\\nIn our example the `plugins` section of our `webpack.config.js` is extended in a separate process. Whilst we\'re developing we also set the `debug` flag to be `true`. [It seems we need to introduce a `LoaderOptionsPlugin` to do this for us.](https://webpack.js.org/guides/migrating/#debug)\\n\\nAs we introduce our `LoaderOptionsPlugin` we also need to make sure that we provide it with `options`. How do I know this? Well [someone raised an issue against ts-loader](https://github.com/TypeStrong/ts-loader/issues/283). I don\'t think this is actually an issue with ts-loader; I think it\'s just a webpack 2 thing. I could be wrong; answers on a postcard please.\\n\\nEither way, to get up and running we just need the `LoaderOptionsPlugin` in play. Consequently, most of what follows in our `webpack.js` file is unchanged:\\n\\n```js\\n// .....\\n\\nvar webpackConfig = require(\'../webpack.config.js\');\\nvar packageJson = require(\'../package.json\');\\n\\n// .....\\n\\nfunction buildProduction(done) {\\n  // .....\\n\\n  myProdConfig.plugins = myProdConfig.plugins.concat(\\n    // .....\\n\\n    // new webpack.optimize.DedupePlugin(), Not a thing anymore apparently\\n    new webpack.optimize.UglifyJsPlugin(),\\n\\n    // I understand this here matters...\\n    // but it doesn\'t seem to make any difference; perhaps I\'m missing something?\\n    new webpack.LoaderOptionsPlugin({\\n      minimize: true,\\n      debug: false,\\n    }),\\n\\n    failPlugin\\n  );\\n\\n  // .....\\n}\\n\\nfunction createDevCompiler() {\\n  var myDevConfig = webpackConfig;\\n  myDevConfig.devtool = \'inline-source-map\';\\n  // myDevConfig.debug = true; - not allowed in webpack 2\\n\\n  myDevConfig.plugins = myDevConfig.plugins.concat(\\n    new webpack.optimize.CommonsChunkPlugin({\\n      name: \'vendor\',\\n      filename: \'vendor.js\',\\n    }),\\n    new WebpackNotifierPlugin({\\n      title: \'Webpack build\',\\n      excludeWarnings: true,\\n    }),\\n\\n    // this is the Webpack 2 hotness!\\n    new webpack.LoaderOptionsPlugin({\\n      debug: true,\\n      options: myDevConfig,\\n    })\\n    // it ends here - there wasn\'t much really....\\n  );\\n\\n  // create a single instance of the compiler to allow caching\\n  return webpack(myDevConfig);\\n}\\n\\n// .....\\n```\\n\\n## `LoaderOptionsPlugin` we hardly new ya\\n\\nAfter a little more experimentation it seems that the `LoaderOptionsPlugin` is not necessary at all for our own use case. In fact it\'s probably not best practice to get used to using it as it\'s only intended to live a short while whilst people move from webpack 1 to webpack 2. In that vein let\'s tweak our `webpack.js` file once more:\\n\\n```js\\nfunction buildProduction(done) {\\n  // .....\\n\\n  myProdConfig.plugins = myProdConfig.plugins.concat(\\n    // .....\\n\\n    new webpack.optimize.UglifyJsPlugin({\\n      compress: {\\n        warnings: true,\\n      },\\n    }),\\n\\n    failPlugin\\n  );\\n\\n  // .....\\n}\\n\\nfunction createDevCompiler() {\\n  var myDevConfig = webpackConfig;\\n  myDevConfig.devtool = \'inline-source-map\';\\n\\n  myDevConfig.plugins = myDevConfig.plugins.concat(\\n    new webpack.optimize.CommonsChunkPlugin({\\n      name: \'vendor\',\\n      filename: \'vendor.js\',\\n    }),\\n    new WebpackNotifierPlugin({ title: \'Webpack build\', excludeWarnings: true })\\n  );\\n\\n  // create a single instance of the compiler to allow caching\\n  return webpack(myDevConfig);\\n}\\n\\n// .....\\n```\\n\\n## `karma.conf.js`\\n\\nFinally Karma. Our `karma.conf.js` with webpack 1 looked like this:\\n\\n```js\\n/* eslint-disable no-var, strict */\\n\'use strict\';\\n\\nvar webpackConfig = require(\'./webpack.config.js\');\\n\\nmodule.exports = function (config) {\\n  // Documentation: https://karma-runner.github.io/0.13/config/configuration-file.html\\n  config.set({\\n    browsers: [\'PhantomJS\'],\\n\\n    files: [\\n      // This ensures we have the es6 shims in place and then loads all the tests\\n      \'test/main.js\',\\n    ],\\n\\n    port: 9876,\\n\\n    frameworks: [\'jasmine\'],\\n\\n    logLevel: config.LOG_INFO, //config.LOG_DEBUG\\n\\n    preprocessors: {\\n      \'test/main.js\': [\'webpack\', \'sourcemap\'],\\n    },\\n\\n    webpack: {\\n      devtool: \'inline-source-map\',\\n      debug: true,\\n      module: webpackConfig.module,\\n      resolve: webpackConfig.resolve,\\n    },\\n\\n    webpackMiddleware: {\\n      quiet: true,\\n      stats: {\\n        colors: true,\\n      },\\n    },\\n\\n    // reporter options\\n    mochaReporter: {\\n      colors: {\\n        success: \'bgGreen\',\\n        info: \'cyan\',\\n        warning: \'bgBlue\',\\n        error: \'bgRed\',\\n      },\\n    },\\n  });\\n};\\n```\\n\\nWe just need to chop out the `debug` statement from the `webpack` section like so:\\n\\n```js\\nmodule.exports = function(config) {\\n\\n  // .....\\n\\n    webpack: {\\n      devtool: \'inline-source-map\',\\n      module: webpackConfig.module,\\n      resolve: webpackConfig.resolve\\n    },\\n\\n  // .....\\n\\n  });\\n};\\n```\\n\\n## Compare and contrast\\n\\nWe now have a repo that works with webpack 2 rc 1. Yay! If you\'d like to see it then take a look [here](https://github.com/TypeStrong/ts-loader/tree/master/examples/webpack2-gulp-react-flux-babel-karma).\\n\\nI thought I\'d compare performance / output size of compiling with webpack 1 to webpack 2. First of all in debug / development mode:\\n\\n```ts\\n// webpack 1\\n\\nVersion: webpack 1.14.0\\nTime: 5063ms\\n    Asset     Size  Chunks             Chunk Names\\n  main.js  37.2 kB       0  [emitted]  main\\nvendor.js  2.65 MB       1  [emitted]  vendor\\n\\n// webpack 2\\n\\nVersion: webpack 2.2.0-rc.1\\nTime: 5820ms\\n    Asset     Size  Chunks                    Chunk Names\\n  main.js  38.7 kB       0  [emitted]         main\\nvendor.js  2.63 MB       1  [emitted]  [big]  vendor\\n```\\n\\nSize and compilation time is not massively different from webpack 1 to webpack 2. It\'s all about the same. I\'m not sure if that\'s to be expected or not.... Though I\'ve a feeling in production mode I\'m supposed to feel the benefits of tree shaking so let\'s have a go:\\n\\n```ts\\n// webpack 1\\n\\nVersion: webpack 1.14.0\\nTime: 5788ms\\n                         Asset     Size  Chunks             Chunk Names\\n  main.269c66e1bc13b7426cee.js  10.5 kB       0  [emitted]  main\\nvendor.269c66e1bc13b7426cee.js   231 kB       1  [emitted]  vendor\\n\\n// webpack 2\\n\\nVersion: webpack 2.2.0-rc.1\\nTime: 5659ms\\n                         Asset     Size  Chunks             Chunk Names\\n  main.33e0d70eeec29206e9b6.js  9.22 kB       0  [emitted]  main\\nvendor.33e0d70eeec29206e9b6.js   233 kB       1  [emitted]  vendor\\n```\\n\\nTo my surprise this looks pretty much unchanged before and after as well. This may be a sign I have missed something crucial out. Or maybe that\'s to be expected. Do give me a heads up if I\'ve missed something..."},{"id":"/2016/12/11/webpack-syncing-enhanced-resolve","metadata":{"permalink":"/2016/12/11/webpack-syncing-enhanced-resolve","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2016-12-11-webpack-syncing-enhanced-resolve/index.md","source":"@site/blog/2016-12-11-webpack-syncing-enhanced-resolve/index.md","title":"webpack: syncing the enhanced-resolve","description":"Like Captain Ahab I resolve to sync the white whale that is webpack\'s enhanced-resolve... English you say? Let me start again:","date":"2016-12-11T00:00:00.000Z","formattedDate":"December 11, 2016","tags":[{"label":"enhanced-resolve","permalink":"/tags/enhanced-resolve"},{"label":"mild trolling","permalink":"/tags/mild-trolling"},{"label":"resolver","permalink":"/tags/resolver"},{"label":"Webpack","permalink":"/tags/webpack"},{"label":"sync","permalink":"/tags/sync"}],"readingTime":2.365,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"webpack: syncing the enhanced-resolve","authors":"johnnyreilly","tags":["enhanced-resolve","mild trolling","resolver","Webpack","sync"],"hide_table_of_contents":false},"prevItem":{"title":"Using ts-loader with webpack 2","permalink":"/2016/12/19/using-ts-loader-with-webpack-2"},"nextItem":{"title":"My Subconscious is a Better Developer Than I Am","permalink":"/2016/11/12/my-subconscious-is-better-developer"}},"content":"Like Captain Ahab I resolve to sync the white whale that is webpack\'s [`enhanced-resolve`](https://github.com/webpack/enhanced-resolve)... English you say? Let me start again:\\n\\nSo, you\'re working on a webpack loader. (In my case the typescript loader; [`ts-loader`](https://github.com/TypeStrong/ts-loader)) You have need of webpack\'s resolve capabilities. You dig around and you discover that that superpower is lodged in the very heart of the enhanced-resolve package. Fantastic. But wait, there\'s more: your needs are custom. You need a sync, not an async resolver. (Try saying that quickly.) You regard the description of `enhanced-resolve` with some concern:\\n\\n> \\"Offers an async require.resolve function. It\'s highly configurable.\\"\\n\\nWell that doesn\'t sound too promising. Let\'s have a look at the docs. Ah. Hmmm. You know how it goes with webpack. Why document anything clearly when people could just guess wildly until they near insanity and gibber? Right? It\'s well established that webpack\'s attitude to docs has been traditionally akin to Gordon Gecko\'s view on lunch.\\n\\n![](documentation-is-for-wimps.png)\\n\\nIn all fairness, things are beginning to change on that front. In fact the [new docs](https://webpack.js.org/) look very promising. But regrettably, the docs on the enhanced-resolve repo are old school. Which is to say: opaque. However, I\'m here to tell you that if a sync resolver is your baby then, contrary to appearances, `enhanced-resolve` has your back.\\n\\n## Sync, for lack of a better word, is good\\n\\nNestled inside enhanced-resolve is the [`ResolverFactory.js`](https://github.com/webpack/enhanced-resolve/blob/3f3f4cd1fcbafa1e98c3c6470fed1277817ed607/lib/ResolverFactory.js) which can be used to make a resolver. However, you can supply it with a million options and that\'s just like giving someone a gun with a predilection for feet.\\n\\nWhat you want is an example of how you could make a sync resolver. Well, surprise surprise it\'s right in front of your nose. Tucked away in [`node.js`](https://github.com/webpack/enhanced-resolve/blob/3f3f4cd1fcbafa1e98c3c6470fed1277817ed607/lib/node.js) (I do \\\\***not**\\\\* get the name) is exactly what you\'re after. It contains a number of factory functions which will construct a ready-made resolver for you; sync or async. Perfect! So here\'s how I\'m rolling:\\n\\n```js\\nconst node = require(\'enhanced-resolve/lib/node\');\\n\\nfunction makeSyncResolver(options) {\\n  return node.create.sync(options.resolve);\\n}\\n\\nconst resolveSync = makeSyncResolver(loader.options);\\n```\\n\\nThe loader options used above you\'ll be familiar with as the `resolve` section of your `webpack.config.js`. You can read more about them [here](https://github.com/webpack/enhanced-resolve/blob/master/README/index.md) and [here](https://webpack.js.org/configuration/resolve/).\\n\\nWhat you\'re left with at this point is a function; a `resolveSync` function if you will that takes 3 arguments:\\n\\n<dl><dt>context</dt><dd>I don\'t know what this is. So when using the function I just supply <code>undefined</code>; and that seems to be OK. Weird, right?</dd><dt>path</dt><dd>This is the path to your code (I think). So, a valid value to supply - handily lifted from the ts-loader test pack - would be: <code>C:\\\\source\\\\ts-loader\\\\.test\\\\babel-issue92</code></dd><dt>request</dt><dd>The actual module you\'re interested in; so using the same test the relevant value would be <code>./submodule/submodule</code></dd></dl>\\n\\nPut it all together and what have you got?\\n\\n```js\\nconst resolvedFileName = resolveSync(\\n  undefined,\\n  \'C:source\\\\ts-loader.test\\\\babel-issue92\',\\n  \'./submodule/submodule\'\\n);\\n\\n// resolvedFileName: C:\\\\source\\\\ts-loader\\\\.test\\\\babel-issue92\\\\submodule\\\\submodule.tsx\\n```\\n\\nBoom."},{"id":"/2016/11/12/my-subconscious-is-better-developer","metadata":{"permalink":"/2016/11/12/my-subconscious-is-better-developer","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2016-11-12-my-subconscious-is-better-developer/index.md","source":"@site/blog/2016-11-12-my-subconscious-is-better-developer/index.md","title":"My Subconscious is a Better Developer Than I Am","description":"Occasionally I flatter myself that I\'m alright at this development lark. Such egotistical talk is foolish. What makes me pause even more when I consider the proposition is this: my subconscious is a better developer than I am.","date":"2016-11-12T00:00:00.000Z","formattedDate":"November 12, 2016","tags":[],"readingTime":1.835,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"My Subconscious is a Better Developer Than I Am","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false},"prevItem":{"title":"webpack: syncing the enhanced-resolve","permalink":"/2016/12/11/webpack-syncing-enhanced-resolve"},"nextItem":{"title":"But you can\'t die... I love you!","permalink":"/2016/11/01/but-you-cant-die-i-love-you-ts-loader"}},"content":"Occasionally I flatter myself that I\'m alright at this development lark. Such egotistical talk is foolish. What makes me pause even more when I consider the proposition is this: my subconscious is a better developer than I am.\\n\\nWhat\'s this fellow talking about?\\n\\nThere\'s two of me. Not identical twins; masquerading as a single man (spoiler: I am not a Christopher Nolan movie). No. There\'s me, the chap who\'s tapping away at his keyboard and solving a problem. And there\'s the other chap too.\\n\\nI have days when I\'m working away at something and I\'ll hit a brick wall. I produce solutions that work but are not elegant. I\'m not proud of them. Or worse, I fail to come up with something that solves the problem I\'m facing. So I go home. I see my family, I have some food, I do something else. I context switch. I go to sleep.\\n\\nWhen I awake, sometimes (not always) I\'ll have waiting in my head a better solution. I can see the solution in my head. I can turn it over and compare it to what, if anything, I currently have and see the reasons the new approach is better. Great, right? Up to a point.\\n\\nWhat concerns me is this: I didn\'t work this out from first principles. The idea arrived sight unseen in my head. It totally works but whose work actually is it? I feel like I\'m taking credit for someone else\'s graft. This is probably why I\'m so keen on the MIT License. Don\'t want to be caught out.\\n\\nI think I\'d like it better if I was a better developer than my subconscious. I\'d come up with the gold and mock the half baked ideas he shows me in the morning. Alas it is not to be.\\n\\nI draw some comfort from the knowledge that I\'m not alone in my experience. I\'ve chatted to other devs in the same boat. There\'s probably two of you as well. Amarite? There\'s probably three of Jon Skeet; each more brilliant than the last...\\n\\n![a poster from the film Being John Malkovich](beingjohnm.png)\\n\\nPS I posted this to Hacker News and [the comments left by people are pretty fascinating](https://news.ycombinator.com/item?id=12942461)."},{"id":"/2016/11/01/but-you-cant-die-i-love-you-ts-loader","metadata":{"permalink":"/2016/11/01/but-you-cant-die-i-love-you-ts-loader","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2016-11-01-but-you-cant-die-i-love-you-ts-loader/index.md","source":"@site/blog/2016-11-01-but-you-cant-die-i-love-you-ts-loader/index.md","title":"But you can\'t die... I love you!","description":"That\'s how I was feeling on the morning of October 6th 2016. I\'d been feeling that way for some time. The target of my concern? ts-loader. ts-loader is a loader for webpack; the module bundler. ts-loader allows you use TypeScript with webpack. I\'d been a merry user of it for at least a year or so. But, at that point, all was not well in the land of ts-loader. Come with me and I\'ll tell you a story...","date":"2016-11-01T00:00:00.000Z","formattedDate":"November 1, 2016","tags":[{"label":"open source","permalink":"/tags/open-source"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"ts-loader","permalink":"/tags/ts-loader"}],"readingTime":4.78,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"But you can\'t die... I love you!","authors":"johnnyreilly","tags":["open source","TypeScript","ts-loader"],"hide_table_of_contents":false},"prevItem":{"title":"My Subconscious is a Better Developer Than I Am","permalink":"/2016/11/12/my-subconscious-is-better-developer"},"nextItem":{"title":"React Component Curry","permalink":"/2016/10/05/react-component-curry"}},"content":"That\'s how I was feeling on the morning of October 6th 2016. I\'d been feeling that way for some time. The target of my concern? [ts-loader](https://github.com/TypeStrong/ts-loader). ts-loader is a loader for [webpack; the module bundler](https://webpack.github.io/). ts-loader allows you use TypeScript with webpack. I\'d been a merry user of it for at least a year or so. But, at that point, all was not well in the land of ts-loader. Come with me and I\'ll tell you a story...\\n\\n## Going Red\\n\\nAt some point, I became a member of the [TypeStrong](https://github.com/TypeStrong) organisation on GitHub. I\'m honestly not entirely sure how. I think it may have been down to the very excellent [Basarat](https://github.com/basarat) (he of [ALM](http://alm.tools/) / [atom-typescript](https://github.com/TypeStrong/atom-typescript) / the list goes on fame) but I couldn\'t clearly say.\\n\\nEither way, [James Brantly](https://github.com/jbrantly)\'s ts-loader was also one of TypeStrong\'s projects. Since I used it, I occasionally contributed. Not much to be honest; mostly it was documentation tweaks. I mean I never really looked at the main code at all. It worked (thanks to other people). I just plugged it into my projects and ploughed on my merry way. I liked it. It was well established; with friendly maintainers. It had a continuous integration test pack that ran against multiple versions of TypeScript on both Windows and Linux. I trusted it. Then one day the continuous integration tests went red. And stayed red.\\n\\nThis is where we came in. On the morning of October 6th I was mulling what to do about this. I knew there was another alternative out there (awesome-typescript-loader) but I was a little wary of it. My understanding of ATL was that it targeted webpack 2.0 which has long been in beta. Where I ply my trade (mostly developing software for the financial sector in the City of London) beta is not a word that people trust. They don\'t do beta. What\'s more I was quite happy with ts-loader; I didn\'t want to switch if I didn\'t have to. I also rather suspected (rightly) that there wasn\'t much wrong; ts-loader just needed a little bit of love. So I thought: I bet I can help here.\\n\\n## The Statement of Intent\\n\\nSo that evening I raised [an issue against ts-loader](https://github.com/TypeStrong/ts-loader/issues/296). Not a \\"sort it out chap\\" issue. No. That wouldn\'t be terribly helpful. I raised a \\"here\'s how I can help\\" issue. I present an abridged version below:\\n\\n> Okay here\'s the deal; I\'ve been using ts-loader for a long time but my contributions up until now have mostly been documentation. Fixing of tests etc. As the commit history shows this is [@jbrantly](https://github.com/jbrantly)\'s baby and kudos to him.\\n>\\n> He\'s not been able to contribute much of late and since he\'s the main person who\'s worked on ts-loader not much has happened for a while; the code is a bit stale. As I\'m a member of TypeStrong I\'m going to have a go at improving the state of the project. I\'m going to do this as carefully as I can. This issue is intended as a meta issue to make it visible what I\'m plannning to do / doing.\\n>\\n> My immediate goal is to get a newer version of ts-loader built and shipped. Essentially all the bug fixes / tweaks since the last release should ship.\\n>\\n> ...\\n>\\n> I don\'t have npm publish rights for ts-loader. Fortunately both [@jbrantly](https://github.com/jbrantly) and [@blakeembrey](https://github.com/blakeembrey) do - and hopefully one of them will either be able to help out with a publish or let me have the requisite rights to do it.\\n>\\n> I can\'t promise this is all going to work; I\'ve got a limited amount of spare time I\'m afraid. Whatever happens it\'s going to take me a little while. But I\'m going to see where I can take this. Best foot forward! Please bear with me...\\n\\nI did wonder what would happen next. This happened next:\\n\\n> My [\\\\#opensourceguilt](https://twitter.com/hashtag/opensourceguilt?src=hash) has been lifted thanks to [@johnny_reilly](https://twitter.com/johnny_reilly) stepping up to take over ts-loader. Thanks man!\\n>\\n> \u2014 James Brantly (@jbrantly) [October 11, 2016](https://twitter.com/jbrantly/status/785931975064444928)\\n\\n<script async=\\"\\" src=\\"//platform.twitter.com/widgets.js\\" charSet=\\"utf-8\\"><\/script>\\n\\n## Caretaker, not [BDFL](https://en.wikipedia.org/wiki/Benevolent_dictator_for_life)\\n\\nSo that\'s how it came to pass that I became the present main caretaker of ts-loader. James very kindly gave me the rights to publish to npm and soon enough I did. I fixed up the existing integration test pack; made it less brittle. I wrote a new integration test pack (that performs a different sort of testing; execution rather than comparison). I merged pull requests, I closed issues. I introduced a regression (whoops!), a community member helped me fix it (thanks [Mike Mazmanyan](https://github.com/dopare)!). In the last month ts-loader has shipped 6 times.\\n\\nThe thing that matters most in the last paragraph are the phrases \\"I merged pull requests\\" and \\"a community member helped me fix it\\". I\'m wary of one man bands; you should be to. I want projects to be a thing communally built and maintained. If I go under a bus I want someone else to be able to carry on without me. So be part of this; I want you to help!\\n\\nI\'ve got plans to do a lot more. I\'m in the process of [refactoring ts-loader to make it more modular and hence easier for others to contribute](https://github.com/TypeStrong/ts-loader/pull/343). (Also it must be said, refactoring something is an excellent way to try and learn a codebase.) Version 1.0 of ts-loader should ship this week.\\n\\nI\'m working with [Herrington Darkholme](https://github.com/HerringtonDarkholme) (awesome name BTW!) to [add a hook-in point](https://github.com/TypeStrong/ts-loader/issues/270) that will allow ts-loader to support [vuejs](http://vuejs.org/). Stuff is happening and will continue to. But don\'t be shy; be part of this! ts-loader awaits your PRs and is happy to have as many caretakers as possible!\\n\\n![](caretaker.png)"},{"id":"/2016/10/05/react-component-curry","metadata":{"permalink":"/2016/10/05/react-component-curry","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2016-10-05-react-component-curry/index.md","source":"@site/blog/2016-10-05-react-component-curry/index.md","title":"React Component Curry","description":"Everyone loves curry don\'t they? I don\'t know about you but I\'m going for one on Friday.","date":"2016-10-05T00:00:00.000Z","formattedDate":"October 5, 2016","tags":[{"label":"currying","permalink":"/tags/currying"},{"label":"jsx","permalink":"/tags/jsx"},{"label":"React","permalink":"/tags/react"},{"label":"stateless functional components","permalink":"/tags/stateless-functional-components"}],"readingTime":1.39,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"React Component Curry","authors":"johnnyreilly","tags":["currying","jsx","React","stateless functional components"],"hide_table_of_contents":false},"prevItem":{"title":"But you can\'t die... I love you!","permalink":"/2016/11/01/but-you-cant-die-i-love-you-ts-loader"},"nextItem":{"title":"TypeScript 2.0, ES2016 and Babel","permalink":"/2016/09/22/typescript-20-es2016-and-babel"}},"content":"Everyone loves curry don\'t they? I don\'t know about you but I\'m going for one on Friday.\\n\\nWhen React 0.14 shipped, it came with a new way to write React components. Rather than as an ES2015 class or using `React.createClass` there was now another way: stateless functional components.\\n\\nThese are components which have no state (the name gives it away) and a simple syntax; they are a function which takes your component props as a single parameter and they return JSX. Think of them as the render method of a standard component just with props as a parameter.\\n\\nThe advantage of these components is that they can reduce the amount of code you have to write for a component which requires no state. This is even more true if you\'re using ES2015 syntax as you have arrow functions and destructuring to help.Embrace the terseness!\\n\\n## Mine\'s a Balti\\n\\nThere is another advantage of this syntax. If you have a number of components which share similar implementation you can easily make component factories by currying:\\n\\n```jsx\\nfunction iconMaker(fontAwesomeClassName: string) {\\n  return (props) => <i className={`fa ${fontAwesomeClassName}`} />;\\n}\\n\\nconst ThumbsUpIcon = iconMaker(\'fa-thumbs-up\');\\nconst TrophyIcon = iconMaker(\'fa-trophy\');\\n\\n// Somewhere in else inside a render function:\\n\\n<p>\\n  This is totally <ThumbsUpIcon />\\n  .... You should win a <TrophyIcon />\\n</p>;\\n```\\n\\nSo our `iconMaker` is a function which, when called with a [Font Awesome](http://fontawesome.io/) class name produces a function which, when invoked, will return a the HTML required to render that icon. This is a super simple example, a bhaji if you will, but you can imagine how useful this technique can be when you\'ve more of a banquet in mind."},{"id":"/2016/09/22/typescript-20-es2016-and-babel","metadata":{"permalink":"/2016/09/22/typescript-20-es2016-and-babel","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2016-09-22-typescript-20-es2016-and-babel/index.md","source":"@site/blog/2016-09-22-typescript-20-es2016-and-babel/index.md","title":"TypeScript 2.0, ES2016 and Babel","description":"TypeScript 2.0 has shipped! Naturally I\'m excited. For some time I\'ve been using TypeScript to emit ES2015 code which I pass onto Babel to transpile to ES \\"old school\\". You can see how here.","date":"2016-09-22T00:00:00.000Z","formattedDate":"September 22, 2016","tags":[{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"Babel","permalink":"/tags/babel"},{"label":"ES2016","permalink":"/tags/es-2016"}],"readingTime":2.315,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"TypeScript 2.0, ES2016 and Babel","authors":"johnnyreilly","tags":["TypeScript","Babel","ES2016"],"hide_table_of_contents":false},"prevItem":{"title":"React Component Curry","permalink":"/2016/10/05/react-component-curry"},"nextItem":{"title":"Integration Tests with SQL Server Database Snapshots","permalink":"/2016/09/12/integration-tests-with-sql-server"}},"content":"[TypeScript 2.0 has shipped!](https://blogs.msdn.microsoft.com/typescript/2016/09/22/announcing-typescript-2-0/) Naturally I\'m excited. For some time I\'ve been using TypeScript to emit ES2015 code which I pass onto Babel to transpile to ES \\"old school\\". You can see how [here](https://blog.johnnyreilly.com/2015/12/es6-typescript-babel-react-flux-karma.html).\\n\\nMerely upgrading my `package.json` to use `\\"typescript\\": \\"^2.0.3\\"` from `\\"typescript\\": \\"^1.8.10\\"` was painless. TypeScript now supports ES2016 (the previous major release 1.8 supported ES2015). I wanted to move on from writing ES2015 to writing ES2016 using my chosen build process. Fortunately, it\'s supported. Phew. However, due to some advances in ecmascript feature modularisation within the TypeScript compiler the upgrade path is slightly different. I figured that I\'d just be able to update the [`target`](https://www.typescriptlang.org/docs/handbook/compiler-options.html) in my `tsconfig.json` to `\\"es2016\\"` from `\\"es2015\\"`, add in the ES2016 preset for Babel and jobs a good \'un. Not so. There were a few more steps to follow. Here\'s the recipe:\\n\\n## `tsconfig.json` changes\\n\\nWell, there\'s no `\\"es2016\\"` target for TypeScript. You carry on with a target of `\\"es2015\\"`. What I need is a new entry: `\\"lib\\": [\\"dom\\", \\"es2015\\", \\"es2016\\"]`. This tells the compiler that we\'re expecting to be emitting to an environment which supports a browser (`\\"dom\\"`), and both ES2016 and ES2015. Our \\"environment\\" is Babel and it\'s going to pick up the baton from this point. My complete `tsconfig.json` looks like this:\\n\\n```json\\n{\\n  \\"compileOnSave\\": false,\\n  \\"compilerOptions\\": {\\n    \\"allowSyntheticDefaultImports\\": true,\\n    \\"lib\\": [\\"dom\\", \\"es2015\\", \\"es2016\\"],\\n    \\"jsx\\": \\"preserve\\",\\n    \\"module\\": \\"es2015\\",\\n    \\"moduleResolution\\": \\"node\\",\\n    \\"noEmitOnError\\": false,\\n    \\"noImplicitAny\\": true,\\n    \\"preserveConstEnums\\": true,\\n    \\"removeComments\\": false,\\n    \\"suppressImplicitAnyIndexErrors\\": true,\\n    \\"target\\": \\"es2015\\"\\n  }\\n}\\n```\\n\\n## Babel changes\\n\\nI needed the Babel preset for ES2016; with a quick [`npm install --save-dev babel-preset-es2016`](https://www.npmjs.com/package/babel-preset-es2016) that was sorted. Now just to kick Webpack into gear...\\n\\n## Webpack changes\\n\\nMy webpack config plugs together TypeScript and Babel with the help of [ts-loader](https://www.npmjs.com/package/ts-loader) and [babel-loader](https://www.npmjs.com/package/babel-loader). It allows the transpilation of my (few) JavaScript files so I can write ES2015. However, mainly it allows the transpilation of my (many) TypeScript files so I can write ES2015-flavoured TypeScript. I\'ll now tweak the `loaders` so they cater for ES2016 as well.\\n\\n```js\\nvar webpack = require(\'webpack\');\\n\\nmodule.exports = {\\n  // ....\\n\\n  module: {\\n    loaders: [\\n      {\\n        // Now transpiling ES2016 TS\\n        test: /\\\\.ts(x?)$/,\\n        exclude: /node_modules/,\\n        loader:\\n          \'babel-loader?presets[]=es2016&presets[]=es2015&presets[]=react!ts-loader\',\\n      },\\n      {\\n        // Now transpiling ES2016 JS\\n        test: /\\\\.js$/,\\n        exclude: /node_modules/,\\n        loader: \'babel\',\\n        query: {\\n          presets: [\'es2016\', \'es2015\', \'react\'],\\n        },\\n      },\\n    ],\\n  },\\n\\n  // ....\\n};\\n```\\n\\n## Wake Up and Smell the Jasmine\\n\\nAnd we\'re there; it works. How do I know? Well; here\'s the proof:\\n\\n```ts\\nit(\'Array.prototype.includes works\', () => {\\n  const result = [1, 2, 3].includes(2);\\n  expect(result).toBe(true);\\n});\\n\\nit(\'Exponentiation operator works\', () => {\\n  expect(1 ** 2 === Math.pow(1, 2)).toBe(true);\\n});\\n```\\n\\nMuch love to the TypeScript team for an awesome job; I can\'t wait to get stuck into some of the exciting new features of TypeScript 2.0. `strictNullChecks` FTW!"},{"id":"/2016/09/12/integration-tests-with-sql-server","metadata":{"permalink":"/2016/09/12/integration-tests-with-sql-server","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2016-09-12-integration-tests-with-sql-server/index.md","source":"@site/blog/2016-09-12-integration-tests-with-sql-server/index.md","title":"Integration Tests with SQL Server Database Snapshots","description":"Once More With Feeling","date":"2016-09-12T00:00:00.000Z","formattedDate":"September 12, 2016","tags":[{"label":"Database Snapshots","permalink":"/tags/database-snapshots"},{"label":"Integration Testing","permalink":"/tags/integration-testing"},{"label":"SQL Server","permalink":"/tags/sql-server"}],"readingTime":5.135,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Integration Tests with SQL Server Database Snapshots","authors":"johnnyreilly","tags":["Database Snapshots","Integration Testing","SQL Server"],"hide_table_of_contents":false},"prevItem":{"title":"TypeScript 2.0, ES2016 and Babel","permalink":"/2016/09/22/typescript-20-es2016-and-babel"},"nextItem":{"title":"The Ternary Operator <3 Destructuring","permalink":"/2016/08/19/the-ternary-operator-meets-destructuring"}},"content":"## Once More With Feeling\\n\\nThis is a topic that I have written about [before](https://blog.johnnyreilly.com/2014/01/integration-testing-with-entity.html).... But not well. I recently had cause to dust down my notes on how to use snapshotting in your integration tests. To my dismay, referring back to my original blog post was less helpful than I\'d hoped. Now I\'ve cracked the enigma code that my original scribings turned out to be, it\'s time to turn my relearnings back into something genuinely useful.\\n\\n## What\'s the Scenario?\\n\\nYou have a test database. You want to write integration tests. So what\'s the problem? Well, these tests will add records, delete records, update records within the tables of the database. They will mutate the data. And that\'s exactly what they ought to do; they\'re testing that our code uses the database in the way we would hope and expect.\\n\\nSo how do we handle this? Well, we could handle this by writing code at the end of each test that is responsible for reverting the database back to the state that it was in at the start of the test. So if we had a test that added a record and tested it, we\'d need the test to be responsible for removing that record before any subsequent tests run. Now that\'s a totally legitimate approach but it adds tax. Each test becomes more complicated and requires more code.\\n\\nSo what\'s another approach? Perhaps we could take a backup of our database before our first test runs. Then, at the end of each test, we could restore our backup to roll the database back to its initial state. Perfect, right? Less code to write, less scope for errors. So what\'s the downside? Backups are slowwwww. Restores likewise. We could be waiting minutes between each test that runs. That\'s not acceptable.\\n\\nThere is another way though: [database snapshots](https://msdn.microsoft.com/en-us/library/ms175158.aspx) \\\\- a feature that\'s been nestling inside SQL Server for a goodly number of years. For our use case, to all intents and purposes, database snapshots offers the same functionality as backups and restores. You can backup a database (take a snapshot of a database at a point in time), you can restore a database (roll back the database to the point of the snapshot). More importantly, you can do either operation in \\\\*_under a second_\\\\*. As it happens, Microsoft advocate using this approach themselves:\\n\\n> In a testing environment, it can be useful when repeatedly running a test protocol for the database to contain identical data at the start of each round of testing. Before running the first round, an application developer or tester can create a database snapshot on the test database. After each test run, the database can be quickly returned to its prior state by reverting the database snapshot.\\n\\nSold!\\n\\n## Talk is cheap, show me the code\\n\\nIn the end it comes down to 3 classes; `DatabaseSnapshot.cs` which does the actual snapshotting work and 2 classes that make use of it.\\n\\n### DatabaseSnapshot.cs\\n\\nThis is our `DatabaseSnapshot` class. Isn\'t it pretty?\\n\\n```cs\\nusing System.Data;\\nusing System.Data.SqlClient;\\n\\nnamespace Testing.Shared\\n{\\n    public class DatabaseSnapshot\\n    {\\n        private readonly string _dbName;\\n        private readonly string _dbSnapShotPath;\\n        private readonly string _dbSnapShotName;\\n        private readonly string _dbConnectionString;\\n\\n        public DatabaseSnapshot(string dbName, string dbSnapshotPath, string dbSnapshotName, string dbConnectionString)\\n        {\\n            _dbName = dbName;\\n            _dbSnapshotPath = dbSnapshotPath;\\n            _dbSnapshotName = dbSnapshotName;\\n            _dbConnectionString = dbConnectionString;\\n        }\\n\\n        public void CreateSnapshot()\\n        {\\n            if (!System.IO.Directory.Exists(_dbSnapshotPath))\\n                System.IO.Directory.CreateDirectory(_dbSnapshotPath);\\n\\n            var sql = $\\"CREATE DATABASE { _dbSnapshotName } ON (NAME=[{ _dbName }], FILENAME=\'{ _dbSnapshotPath }{ _dbSnapshotName }\') AS SNAPSHOT OF [{_dbName }]\\";\\n\\n            ExecuteSqlAgainstMaster(sql);\\n        }\\n\\n        public void DeleteSnapshot()\\n        {\\n            var sql = $\\"DROP DATABASE { _dbSnapshotName }\\";\\n\\n            ExecuteSqlAgainstMaster(sql);\\n        }\\n\\n        public void RestoreSnapshot()\\n        {\\n            var sql = \\"USE master;\\\\r\\\\n\\" +\\n\\n                $\\"ALTER DATABASE {_dbName} SET SINGLE_USER WITH ROLLBACK IMMEDIATE;\\\\r\\\\n\\" +\\n\\n                $\\"RESTORE DATABASE {_dbName}\\\\r\\\\n\\" +\\n                $\\"FROM DATABASE_SNAPSHOT = \'{ _dbSnapshotName }\';\\\\r\\\\n\\" +\\n\\n                $\\"ALTER DATABASE {_dbName} SET MULTI_USER;\\\\r\\\\n\\";\\n\\n            ExecuteSqlAgainstMaster(sql);\\n        }\\n\\n        private void ExecuteSqlAgainstMaster(string sql, params SqlParameter[] parameters)\\n        {\\n            using (var conn = new SqlConnection(_dbConnectionString))\\n            {\\n                conn.Open();\\n                var cmd = new SqlCommand(sql, conn) { CommandType = CommandType.Text };\\n                cmd.Parameters.AddRange(parameters);\\n                cmd.ExecuteNonQuery();\\n                conn.Close();\\n            }\\n        }\\n    }\\n}\\n```\\n\\nIt exposes 3 methods:\\n\\n<dl><dt>CreateSnapshot</dt><dd>This method creates the snapshot of the database. We will run this right at the start, before any of our tests run.</dd><dt>DeleteSnapshot</dt><dd>Deletes the snapshot we created. We will run this at the end, after all our tests have finished running.</dd><dt>RestoreSnapshot</dt><dd>Restores the database back to the snapshot we took earlier. We run this after each test has completed. This method relies on a connection to the database (perhaps unsurprisingly). It switches the database in use away from the database that is being restored prior to actually running the restore. It happens to shift to the master database (I believe that\'s entirely incidental; although I haven\'t tested).</dd></dl>\\n\\n### SetupAndTeardown.cs\\n\\nThis class is responsible for setting up the snapshot we\'re going to use in our tests right before any of the tests have run (in the `FixtureSetup` method). It\'s also responsible for deleting the snapshot once all the tests have finished running (in the `FixtureTearDown` method). It should be noted that in this example I\'m using NUnit and this class is written to depend on the hooks NUnit exposes for running code at the very beginning and end of the test cycle. All test frameworks have these hooks; if you\'re using something other than NUnit then it\'s just a case of swapping in the relevant attribute (everything tends to attribute driven in the test framework world).\\n\\n```cs\\nusing NUnit.Framework;\\n\\nnamespace Testing.Shared\\n{\\n   [SetUpFixture]\\n   public class SetupAndTeardown\\n   {\\n      public static DatabaseSnapshot DatabaseSnapshot;\\n\\n      [SetUp]\\n      public void FixtureSetup()\\n      {\\n         DatabaseSnapshot = new DatabaseSnapshot(\\"MyDbName\\", \\"C:\\\\\\\\\\", \\"MySnapshot\\", \\"Data Source=.;initial catalog=MyDbName;integrated security=True;\\");\\n\\n         try\\n         {\\n            // Try to delete the snapshot in case it was left over from aborted test runs\\n            DatabaseSnapshot.DeleteSnapShot();\\n         }\\n         catch { /* this should fail with snapshot does not exist */ }\\n\\n         DatabaseSnapshot.CreateSnapShot();\\n      }\\n\\n      [TearDown]\\n      public void FixtureTearDown()\\n      {\\n         DatabaseSnapshot.DeleteSnapShot();\\n      }\\n   }\\n}\\n```\\n\\n### TestBase.cs\\n\\nAll of our test classes are made to inherit from this class:\\n\\n```cs\\nusing NUnit.Framework;\\n\\nnamespace Testing.Shared\\n{\\n   public class TestBase\\n   {\\n      [TearDown]\\n      public void TearDown()\\n      {\\n         SetupAndTeardown.DatabaseSnapshot.RestoreSnapShot();\\n      }\\n   }\\n}\\n```\\n\\nWhich restores the database back to the snapshot position at the end of each test. And that... Is that!"},{"id":"/2016/08/19/the-ternary-operator-meets-destructuring","metadata":{"permalink":"/2016/08/19/the-ternary-operator-meets-destructuring","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2016-08-19-the-ternary-operator-meets-destructuring/index.md","source":"@site/blog/2016-08-19-the-ternary-operator-meets-destructuring/index.md","title":"The Ternary Operator <3 Destructuring","description":"I\'m addicted to the ternary operator. For reasons I can\'t explain, I cannot get enough of:","date":"2016-08-19T00:00:00.000Z","formattedDate":"August 19, 2016","tags":[{"label":"Destructuring","permalink":"/tags/destructuring"},{"label":"Ternary Operator","permalink":"/tags/ternary-operator"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"ES2015","permalink":"/tags/es-2015"}],"readingTime":2.205,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"The Ternary Operator <3 Destructuring","authors":"johnnyreilly","tags":["Destructuring","Ternary Operator","TypeScript","ES2015"],"hide_table_of_contents":false},"prevItem":{"title":"Integration Tests with SQL Server Database Snapshots","permalink":"/2016/09/12/integration-tests-with-sql-server"},"nextItem":{"title":"Understanding Webpack\'s DefinePlugin (and using with TypeScript)","permalink":"/2016/07/23/using-webpacks-defineplugin-with-typescript"}},"content":"I\'m addicted to the [ternary operator](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Conditional_Operator). For reasons I can\'t explain, I cannot get enough of:\\n\\n```js\\nconst thisOrThat = someCondition ? \'this\' : \'or that\';\\n```\\n\\nThe occasion regularly arises where I need to turn my lovely terse code into an if statement in order to set 2 variables instead of 1. I\'ve been heartbroken; I hate doing:\\n\\n```ts\\nlet legWear: string, coat: boolean;\\nif (weather === \'good\') {\\n  legWear = \'shorts\';\\n  coat = false;\\n} else {\\n  legWear = \'jeans\';\\n  coat = true;\\n}\\n```\\n\\nJust going from setting one variable to setting two has been really traumatic:\\n\\n- I\'ve had do stop using `const` and moved to `let`. This has made my code less \\"truthful\\" in the sense that I never intend to reassign these variables again; they are intended to be immutable.\\n- I\'ve gone from 1 line of code to _9 lines of code_. That\'s 9x the code for increasing the number of variables in play by 1. That\'s... heavy.\\n- This third point only applies if you\'re using TypeScript (and I am): I have to specify the types of my variables up front if I want type safety.\\n\\nES2015 gives us another option. We can move back to the ternary operator if we change the return type of each branch to be an object sharing the same signature. Then, using destructuring, we can pull out those object properties into `const`s:\\n\\n```ts\\nconst { legWear, coat } =\\n  weather === \'good\'\\n    ? { legWear: \'shorts\', coat: false }\\n    : { legWear: \'jeans\', coat: true };\\n```\\n\\nWith this approach we\'re keeping usage of `const` instead of `let` and we\'re only marginally increasing the amount of code we\'re writing. If you\'re using TypeScript you\'re back to being able to rely on the compiler correctly inferring your types; you don\'t need to specify. Awesome.\\n\\n## Crowdfund You A Tuple\\n\\nI thought I was done and then I saw this:\\n\\n> [@johnny_reilly](https://twitter.com/johnny_reilly) even neater with tuples: const [str, num] = test ? [\\"yes\\", 100] : [\\"no\\", 50];\\n>\\n> \u2014 Illustrated Pamphlet (@Rickenhacker) [August 20, 2016](https://twitter.com/Rickenhacker/status/766913766323781632)\\n\\n<script async=\\"\\" src=\\"//platform.twitter.com/widgets.js\\" charSet=\\"utf-8\\"><\/script>\\n\\n[Daniel](https://twitter.com/Rickenhacker) helpfully points out that there\'s an even terser syntax available to us:\\n\\n```ts\\nconst [legWear, coat] =\\n  weather === \'good\' ? [\'shorts\', false] : [\'jeans\', true];\\n```\\n\\nThe above is ES2015 array destructuring. We get exactly the same effect but it\'s a little terser as we don\'t have to repeat the prop names as we do when using object destructuring. From a TypeScript perspective the assignment side of the above is a [Tuple](https://github.com/Microsoft/TypeScript/pull/428) which allows our type inference to flow through in the manner we\'d hope.\\n\\nLovely. Thanks!"},{"id":"/2016/07/23/using-webpacks-defineplugin-with-typescript","metadata":{"permalink":"/2016/07/23/using-webpacks-defineplugin-with-typescript","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2016-07-23-using-webpacks-defineplugin-with-typescript/index.md","source":"@site/blog/2016-07-23-using-webpacks-defineplugin-with-typescript/index.md","title":"Understanding Webpack\'s DefinePlugin (and using with TypeScript)","description":"I\'ve been searching for a way to describe what the DefinePlugin actually does. The docs say\\\\*:","date":"2016-07-23T00:00:00.000Z","formattedDate":"July 23, 2016","tags":[{"label":"compile-time constants","permalink":"/tags/compile-time-constants"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"defineplugin","permalink":"/tags/defineplugin"},{"label":"Webpack","permalink":"/tags/webpack"}],"readingTime":2.76,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Understanding Webpack\'s DefinePlugin (and using with TypeScript)","authors":"johnnyreilly","tags":["compile-time constants","TypeScript","defineplugin","Webpack"],"hide_table_of_contents":false},"prevItem":{"title":"The Ternary Operator <3 Destructuring","permalink":"/2016/08/19/the-ternary-operator-meets-destructuring"},"nextItem":{"title":"Creating an ES2015 Map from an Array in TypeScript","permalink":"/2016/06/02/create-es2015-map-from-array-in-typescript"}},"content":"I\'ve been searching for a way to describe what the DefinePlugin actually does. The [docs](https://github.com/webpack/docs/wiki/list-of-plugins#defineplugin) say\\\\*:\\n\\n> Define free variables. Useful for having development builds with debug logging or adding global constants.\\n\\n<sub>\\\\* Actually that should read \\"used to say\\". I\'ve made some changes to the official docs.... (Surprisingly easy to do that by the way; it\'s just a wiki you can edit at will.)</sub>\\n\\nI think I would describe it thusly: the DefinePlugin allows you to create global constants which can be _configured at compile time_. I find this very useful for allowing different behaviour between development builds and release builds. This post will demonstrate usage of this approach, talk about what\'s actually happening and how to get this working nicely with TypeScript.\\n\\nIf you just want to see this in action then take a look at this [repo](https://github.com/johnnyreilly/poorclaresarundel/) and keep your eyes open for usage of [`__VERSION__`](https://github.com/johnnyreilly/poorclaresarundel/search?utf8=%E2%9C%93&q=__VERSION__) and [`__IN_DEBUG__`](https://github.com/johnnyreilly/poorclaresarundel/search?utf8=%E2%9C%93&q=__IN_DEBUG__).\\n\\n## What Globals?\\n\\nFor our example we want to define 2 global constants; a string called `__VERSION__` and a boolean called `__IN_DEBUG__`. The names are deliberately wacky to draw attention to the fact that these are not your everyday, common-or-garden variables. Them\'s \\"special\\". These constants will be initialised with different values depending on whether we are in a debug build or a production build. Usage of these constants in our code might look like this:\\n\\n```ts\\nif (__IN_DEBUG__) {\\n  console.log(`This app is version ${__VERSION__}`);\\n}\\n```\\n\\nSo, if `__IN_DEBUG__` is set to `true` this code would log out to the console the version of the app.\\n\\n## Configuring our Globals\\n\\nTo introduce these constants to webpack we\'re going to add this to our webpack configuration:\\n\\n```ts\\nvar webpack = require(\'webpack\');\\n\\n// ...\\n\\nplugins: [\\n  new webpack.DefinePlugin({\\n    __IN_DEBUG__: JSON.stringify(false),\\n    __VERSION__: JSON.stringify(\'1.0.0.\' + Date.now()),\\n  }),\\n  // ...\\n];\\n// ...\\n```\\n\\nWhat\'s going on here? Well, each key of the object literal above represents one of our global constants. When you look at the value, just imagine each outer `JSON.stringify( ... )` is not there. It\'s just noise. Imagine instead that you\'re seeing this:\\n\\n```ts\\n__IN_DEBUG__: false,\\n          __VERSION__: \'1.0.0.\' + Date.now()\\n```\\n\\nA little clearer, right? `__IN_DEBUG__` is given the boolean value `false` and `__VERSION__` is given the string value of `1.0.0.` plus the ticks off of `Date.now()`. What\'s happening here is well explained in Pete Hunt\'s excellent [webpack howto](https://github.com/petehunt/webpack-howto#6-feature-flags): \\"definePlugin takes raw strings and inserts them\\". `JSON.stringify` facilitates this; it produces a string representation of a value that can be inlined into code. When the inlining takes place the actual output would be something like this:\\n\\n```ts\\nif (false) {\\n  // Because at compile time, __IN_DEBUG__ === false\\n  console.log(`This app is version ${\'1.0.0.1469268116580\'}`); // And __VERSION__ === \\"1.0.0.1469268116580\\"\\n}\\n```\\n\\nAnd if you\'ve got some [UglifyJS](https://github.com/mishoo/UglifyJS) or similar in the mix then, in the example above, this would actually strip out the statement above entirely since it\'s clearly a [NOOP](https://en.wikipedia.org/wiki/NOP). Yay the dead code removal! If `__IN_DEBUG__` was `false` then (perhaps obviously) this statement would be left in place as it wouldn\'t be dead code.\\n\\n## TypeScript and Define\\n\\nThe final piece of the puzzle is making TypeScript happy. It doesn\'t know anything about our global constants. So we need to tell it:\\n\\n```ts\\ndeclare var __IN_DEBUG__: boolean;\\ndeclare var __VERSION__: string;\\n```\\n\\nAnd that\'s it. Compile time constants are a go!"},{"id":"/2016/06/02/create-es2015-map-from-array-in-typescript","metadata":{"permalink":"/2016/06/02/create-es2015-map-from-array-in-typescript","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2016-06-02-create-es2015-map-from-array-in-typescript/index.md","source":"@site/blog/2016-06-02-create-es2015-map-from-array-in-typescript/index.md","title":"Creating an ES2015 Map from an Array in TypeScript","description":"I\'m a great lover of ES2015\'s Map. However, just recently I tumbled over something I find a touch inconvenient about how you initialise a new Map from the contents of an Array in TypeScript.","date":"2016-06-02T00:00:00.000Z","formattedDate":"June 2, 2016","tags":[{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"ES6","permalink":"/tags/es-6"},{"label":"Array","permalink":"/tags/array"},{"label":"ES2015","permalink":"/tags/es-2015"},{"label":"Map","permalink":"/tags/map"}],"readingTime":2.1,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Creating an ES2015 Map from an Array in TypeScript","authors":"johnnyreilly","tags":["TypeScript","ES6","Array","ES2015","Map"],"hide_table_of_contents":false},"prevItem":{"title":"Understanding Webpack\'s DefinePlugin (and using with TypeScript)","permalink":"/2016/07/23/using-webpacks-defineplugin-with-typescript"},"nextItem":{"title":"The Mysterious Case of Webpack, Angular and jQuery","permalink":"/2016/05/24/the-mysterious-case-of-webpack-angular-and-jquery"}},"content":"I\'m a great lover of ES2015\'s [`Map`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Map). However, just recently I tumbled over something I find a touch inconvenient about how you initialise a new `Map` from the contents of an `Array` in TypeScript.\\n\\n## This Doesn\'t Work\\n\\nWe\'re going try to something like this: (pilfered from the MDN docs)\\n\\n```ts\\nvar kvArray = [\\n  [\'key1\', \'value1\'],\\n  [\'key2\', \'value2\'],\\n];\\n\\n// Use the regular Map constructor to transform a 2D key-value Array into a map\\nvar myMap = new Map(kvArray);\\n```\\n\\nSimple enough right? Well I\'d rather assumed that I should be able to do something like this in TypeScript:\\n\\n```ts\\nconst iAmAnArray [\\n  { value: \\"value1\\", text: \\"hello\\" }\\n  { value: \\"value2\\", text: \\"map\\" }\\n];\\n\\nconst iAmAMap = new Map<string, string>(\\n  iAmAnArray.map(x => [x.value, x.text])\\n);\\n```\\n\\nHowever, to my surprise this errored out with:\\n\\n```\\n[ts] Argument of type \'string[][]\' is not assignable to parameter of type \'Iterable<[string, string]>\'.\\n  Types of property \'[Symbol.iterator]\' are incompatible.\\n    Type \'() => IterableIterator<string[]>\' is not assignable to type \'() => Iterator<[string, string]>\'.\\n      Type \'IterableIterator<string[]>\' is not assignable to type \'Iterator<[string, string]>\'.\\n        Types of property \'next\' are incompatible.\\n          Type \'(value?: any) => IteratorResult<string[]>\' is not assignable to type \'(value?: any) => IteratorResult<[string, string]>\'.\\n            Type \'IteratorResult<string[]>\' is not assignable to type \'IteratorResult<[string, string]>\'.\\n              Type \'string[]\' is not assignable to type \'[string, string]\'.\\n                Property \'0\' is missing in type \'string[]\'.\\n```\\n\\nDisappointing right? It\'s expecting `Iterable&lt;[string, string]&gt;` and an `Array` with 2 elements that are strings is _not_ inferred to be that.\\n\\n## This Does\\n\\nIt emerges that there is a way to do this though; you just need to give the compiler a clue. You need to include a type assertion of ` as [string, string]` which tells the compiler that what you\'ve just declared is a `Tuple` of `string` and `string`. (Please note that `[string, string]` corresponds to the types of the `Key` and `Value` of your `Map` and should be set accordingly.)\\n\\nSo a working version of the code looks like this:\\n\\n```ts\\nconst iAmAnArray [\\n  { value: \\"value1\\", text: \\"hello\\" }\\n  { value: \\"value2\\", text: \\"map\\" }\\n];\\n\\nconst iAmAMap = new Map<string, string>(\\n  iAmAnArray.map(x => [x.value, x.text] as [string, string])\\n);\\n```\\n\\nOr, to be terser, this:\\n\\n```ts\\nconst iAmAnArray [\\n  { value: \\"value1\\", text: \\"hello\\" }\\n  { value: \\"value2\\", text: \\"map\\" }\\n];\\n\\nconst iAmAMap = new Map( // Look Ma!  No type annotations\\n  iAmAnArray.map(x => [x.value, x.text] as [string, string])\\n);\\n```\\n\\nI\'ve raised this as an issue with the TypeScript team; you can find details [here](https://github.com/Microsoft/TypeScript/issues/8936)."},{"id":"/2016/05/24/the-mysterious-case-of-webpack-angular-and-jquery","metadata":{"permalink":"/2016/05/24/the-mysterious-case-of-webpack-angular-and-jquery","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2016-05-24-the-mysterious-case-of-webpack-angular-and-jquery/index.md","source":"@site/blog/2016-05-24-the-mysterious-case-of-webpack-angular-and-jquery/index.md","title":"The Mysterious Case of Webpack, Angular and jQuery","description":"You may know that Angular ships with a cutdown version of jQuery called jQLite. It\'s still possible to use the full-fat jQuery; to quote the docs:","date":"2016-05-24T00:00:00.000Z","formattedDate":"May 24, 2016","tags":[{"label":"provideplugin","permalink":"/tags/provideplugin"},{"label":"jquery","permalink":"/tags/jquery"},{"label":"jqlite","permalink":"/tags/jqlite"},{"label":"Angular","permalink":"/tags/angular"},{"label":"Webpack","permalink":"/tags/webpack"}],"readingTime":1.89,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"The Mysterious Case of Webpack, Angular and jQuery","authors":"johnnyreilly","tags":["provideplugin","jquery","jqlite","Angular","Webpack"],"hide_table_of_contents":false},"prevItem":{"title":"Creating an ES2015 Map from an Array in TypeScript","permalink":"/2016/06/02/create-es2015-map-from-array-in-typescript"},"nextItem":{"title":"Inlining Angular Templates with WebPack and TypeScript","permalink":"/2016/05/13/inlining-angular-templates-with-webpack"}},"content":"You may know that [Angular ships with a cutdown version of jQuery called jQLite](https://docs.angularjs.org/api/ng/function/angular.element). It\'s still possible to use the full-fat jQuery; to quote the docs:\\n\\n> To use `jQuery`, simply ensure it is loaded before the `angular.js` file.\\n\\nNow the wording rather implies that you\'re not using any module loader / bundler. Rather that all files are being loaded via `script` tags and relies on the global variables that result from that. True enough, if you take a look at the [Angular source](https://github.com/angular/angular.js/blob/eaa1119d4252bed08dfa42f984ef9502d0f02775/src/Angular.js#L1791) you can see how this works:\\n\\n```ts\\n// bind to jQuery if present;\\nvar jqName = jq();\\njQuery = isUndefined(jqName)\\n  ? window.jQuery // use jQuery (if present)\\n  : !jqName\\n  ? undefined // use jqLite\\n  : window[jqName]; // use jQuery specified by `ngJq`\\n```\\n\\nAmongst other things it looks for a `jQuery` variable which has been placed onto the `window` object. If it is found then jQuery is used; if it is not then it\'s `jqLite` all the way.\\n\\n## But wait! I\'m using webpack\\n\\nMe too! And one of the reasons is that we get to move away from reliance upon the global scope and towards proper modularisation. So how do we get Angular to use jQuery given the code we\'ve seen above? Well, your first thought might be to `npm install` yourself some `jQuery` and then make sure you\'ve got something like this in your entry file:\\n\\n```ts\\nimport \'jquery\'; // This\'ll fix it... Right?\\nimport * as angular from \'angular\';\\n```\\n\\nWrong.\\n\\n## You need the `ProvidePlugin`\\n\\nIn your `webpack.config.js` you need to add the following entry to your plugins:\\n\\n```ts\\nnew webpack.ProvidePlugin({\\n          \\"window.jQuery\\": \\"jquery\\"\\n      }),\\n```\\n\\nThis uses the webpack [`ProvidePlugin`](https://github.com/webpack/docs/wiki/list-of-plugins#provideplugin) and, at the point of webpackification (\xa9 2016 John Reilly) all references in the code to `window.jQuery` will be replaced with a reference to the webpack module that contains jQuery. So when you look at the bundled file you\'ll see that the code that checks the `window` object for `jQuery` has become this:\\n\\n```ts\\njQuery = isUndefined(jqName)\\n  ? __webpack_provided_window_dot_jQuery // use jQuery (if present)\\n  : !jqName\\n  ? undefined // use jqLite\\n  : window[jqName]; // use jQuery specified by `ngJq`\\n```\\n\\nThat\'s right; webpack is providing Angular with jQuery whilst still _not_ placing a `jQuery` variable onto the `window`. Neat huh?"},{"id":"/2016/05/13/inlining-angular-templates-with-webpack","metadata":{"permalink":"/2016/05/13/inlining-angular-templates-with-webpack","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2016-05-13-inlining-angular-templates-with-webpack/index.md","source":"@site/blog/2016-05-13-inlining-angular-templates-with-webpack/index.md","title":"Inlining Angular Templates with WebPack and TypeScript","description":"This technique actually applies to pretty much any web stack where you have to supply templates; it just so happens that I\'m using Angular 1.x in this case. Also I have an extra technique which is useful to handle the ng-include scenario.","date":"2016-05-13T00:00:00.000Z","formattedDate":"May 13, 2016","tags":[{"label":"raw-loader","permalink":"/tags/raw-loader"},{"label":"Angular","permalink":"/tags/angular"},{"label":"templatecache","permalink":"/tags/templatecache"},{"label":"Webpack","permalink":"/tags/webpack"}],"readingTime":2.895,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Inlining Angular Templates with WebPack and TypeScript","authors":"johnnyreilly","tags":["raw-loader","Angular","templatecache","Webpack"],"hide_table_of_contents":false},"prevItem":{"title":"The Mysterious Case of Webpack, Angular and jQuery","permalink":"/2016/05/24/the-mysterious-case-of-webpack-angular-and-jquery"},"nextItem":{"title":"Instant Stubs with JSON.Net (just add hot water)","permalink":"/2016/04/25/instant-stubs-with-jsonnet"}},"content":"This technique actually applies to pretty much any web stack where you have to supply templates; it just so happens that I\'m using Angular 1.x in this case. Also I have an extra technique which is useful to handle the [ng-include](https://docs.angularjs.org/api/ng/directive/ngInclude) scenario.\\n\\n## Preamble\\n\\nFor some time I\'ve been using webpack to bundle my front end. I write ES6 TypeScript; import statements and all. This is all sewn together using the glorious [ts-loader](https://www.npmjs.com/package/ts-loader) to compile and emit ES6 code which is handed off to the wonderful [babel-loader](https://www.npmjs.com/package/babel-loader) which transpiles it to ESold code. All with full source map support. It\'s wonderful.\\n\\nHowever, up until now I\'ve been leaving Angular to perform the relevant http requests at runtime when it needs to pull in templates. That works absolutely fine but my preference is to preload those templates. In fact I\'ve [written before](http://blog.johnnyreilly.com/2015/02/using-gulp-in-asp-net-instead-of-web-optimization.html) about using the [gulp angular template cache](https://www.npmjs.com/package/gulp-angular-templatecache) to achieve just that aim.\\n\\nSo I was wondering; in this modular world what would be the equivalent approach? Sure I could still use the gulp angular template cache approach but I would like something a little more deliberate and a little less magic. Also, I\'ve discovered (to my cost) that when using the existing approach, it\'s possible to break the existing implementation without realising it; only finding out there\'s a problem in Production when unexpected http requests start happening. Finding these problems out at compile time rather than runtime is always to be strived for. So how?\\n\\n## [raw-loader](https://www.npmjs.com/package/raw-loader)!\\n\\nraw-loader allows you load file content using `require` statements. This works well with the use case of inlining html. So I drop it into my `webpack.config.js` like so:\\n\\n```js\\nvar path = require(\'path\');\\n\\nmodule.exports = {\\n  cache: true,\\n  entry: {\\n    main: \'./src/main.ts\',\\n\\n    vendor: [\\n      \'babel-polyfill\',\\n      \'angular\',\\n      \'angular-animate\',\\n      \'angular-sanitize\',\\n      \'angular-ui-bootstrap\',\\n      \'angular-ui-router\',\\n    ],\\n  },\\n  output: {\\n    path: path.resolve(__dirname, \'./dist/scripts\'),\\n    filename: \'[name].js\',\\n    chunkFilename: \'[chunkhash].js\',\\n  },\\n  module: {\\n    loaders: [\\n      {\\n        test: /\\\\.ts(x?)$/,\\n        exclude: /node_modules/,\\n        loader: \'babel-loader?presets[]=es2015!ts-loader\',\\n      },\\n      {\\n        test: /\\\\.js$/,\\n        exclude: /node_modules/,\\n        loader: \'babel\',\\n        query: {\\n          presets: [\'es2015\'],\\n        },\\n      },\\n      {\\n        // THIS IS THE MAGIC!\\n        test: /\\\\.html$/,\\n        exclude: /node_modules/,\\n        loader: \'raw\',\\n      },\\n    ], // THAT WAS THE MAGIC!\\n  },\\n  plugins: [\\n    // ....\\n  ],\\n  resolve: {\\n    extensions: [\'\', \'.ts\', \'.tsx\', \'.js\'],\\n  },\\n};\\n```\\n\\nWith this in place, if someone requires a file with the `html` suffix then raw-loader comes in. So now we can swap this:\\n\\n```js\\n$stateProvider.state(\'state1\', {\\n  url: \'/state1\',\\n  templateUrl: \'partials/state1.html\',\\n});\\n```\\n\\nFor this:\\n\\n```js\\n$stateProvider.state(\'state1\', {\\n  url: \'/state1\',\\n  template: require(\'./partials/state1.html\'),\\n});\\n```\\n\\nNow initially TypeScript is going to complain about your `require` statement. That\'s fair; outside of node-land it doesn\'t know what `require` is. No bother, you just need to drop in a one line simple definition file to sort this out; let me present `webpack-require.d.ts`:\\n\\n```ts\\ndeclare var require: (filename: string) => any;\\n```\\n\\nYou\'ve now inlined your template. And for bonus points, if you were to make a mistake in your path then webpack would shout at you at compile time; which is a _good, good_ thing.\\n\\n## ng-include\\n\\nThe one use case that this doesn\'t cover is where your templates import other templates through use of the [ng-include](https://docs.angularjs.org/api/ng/directive/ngInclude) directive. They will still trigger http requests as the templates are served. The simple way to prevent that is by priming the angular `<a href=\\"https://docs.angularjs.org/api/ng/service/$templateCache\\">$templateCache</a>` like so:\\n\\n```js\\napp.run([\\n  \'$templateCache\',\\n  ($templateCache: ng.ITemplateCacheService) => {\\n    $templateCache.put(\'justSome.html\', require(\'./justSome.html\'));\\n    // Other templates go here...\\n  },\\n]);\\n```\\n\\nNow when the app spins up it already has everything it needs pre-cached."},{"id":"/2016/04/25/instant-stubs-with-jsonnet","metadata":{"permalink":"/2016/04/25/instant-stubs-with-jsonnet","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2016-04-25-instant-stubs-with-jsonnet/index.md","source":"@site/blog/2016-04-25-instant-stubs-with-jsonnet/index.md","title":"Instant Stubs with JSON.Net (just add hot water)","description":"I\'d like you to close your eyes and imagine a scenario. You\'re handed a prototype system. You\'re told it works. It has no documentation. It has 0 unit tests. The hope is that you can take it on, refactor it, make it better and (crucially) not break it. Oh, and you don\'t really understand what the code does or why it does it either; information on that front is, alas, sorely lacking.","date":"2016-04-25T00:00:00.000Z","formattedDate":"April 25, 2016","tags":[{"label":"unit testing","permalink":"/tags/unit-testing"},{"label":"stub data","permalink":"/tags/stub-data"},{"label":"json.net","permalink":"/tags/json-net"},{"label":"mock data","permalink":"/tags/mock-data"}],"readingTime":4.045,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Instant Stubs with JSON.Net (just add hot water)","authors":"johnnyreilly","tags":["unit testing","stub data","json.net","mock data"],"hide_table_of_contents":false},"prevItem":{"title":"Inlining Angular Templates with WebPack and TypeScript","permalink":"/2016/05/13/inlining-angular-templates-with-webpack"},"nextItem":{"title":"Elvis and King Concat","permalink":"/2016/03/22/elvis-and-king-concat"}},"content":"I\'d like you to close your eyes and imagine a scenario. You\'re handed a prototype system. You\'re told it works. It has no documentation. It has 0 unit tests. The hope is that you can take it on, refactor it, make it better and (crucially) not break it. Oh, and you don\'t really understand what the code does or why it does it either; information on that front is, alas, sorely lacking.\\n\\nThis has happened to me; it\'s alas not that unusual. The common advice handed out in this situation is: \\"add unit tests before you change it\\". That\'s good advice. We need to take the implementation that embodies the correctness of the system and create unit tests that set that implementation in stone. However, what say the system that you\'re hoping to add tests to takes a number of large and complex inputs from some external source and produces a similarly large and complex output?\\n\\nYou could start with integration tests. They\'re good but slow and crucially they depend upon the external inputs being available and unchanged (which is perhaps unlikely). What you could do (what I have done) is debug a working working system. At each point that an input is obtained I have painstakingly transcribed the data which allows me to subsequently hand code stub data. There comes a point when this is plainly untenable; it\'s just too much data to transcribe. At this point the temptation is to think \\"it\'s okay; I can live without the tests. I\'ll just be super careful with my refactoring... It\'ll be fine It\'ll be fine It\'ll be fine It\'ll be fine\\".\\n\\nActually, it probably won\'t be fine. And even if it is (miracles do happen) you\'re going to be fairly stressed as you wonder if you\'ve been careful enough. What if there was another way? A way that wasn\'t quite so hard but that allowed you to add tests without requiring 3 months hand coding....\\n\\n## Instant Stubs\\n\\nWhat I\'ve come up with is a super simple utility class for creating stubs / fakes. (I\'m aware the naming of such things [can be a little contentious](http://martinfowler.com/articles/mocksArentStubs.html).)\\n\\n```cs\\nusing Newtonsoft.Json;\\nusing System;\\nusing System.IO;\\n\\nnamespace MakeFakeData.UnitTests\\n{\\n  public static class Stubs\\n  {\\n    private static JsonSerializer _serializer = new JsonSerializer { NullValueHandling = NullValueHandling.Ignore };\\n\\n    public static void Make<T>(string stubPath, T data)\\n    {\\n      try\\n      {\\n        if (string.IsNullOrEmpty(stubPath))\\n          throw new ArgumentNullException(nameof(stubPath));\\n        if (data == null)\\n          throw new ArgumentNullException(nameof(data));\\n\\n        using (var sw = new StreamWriter(stubPath))\\n        using (var writer = new JsonTextWriter(sw) {\\n            Formatting = Formatting.Indented,\\n            IndentChar = \' \',\\n            Indentation = 2})\\n        {\\n          _serializer.Serialize(writer, data);\\n        }\\n      }\\n      catch (Exception exc)\\n      {\\n        throw new Exception($\\"Failed to make {stubPath}\\", exc);\\n      }\\n    }\\n\\n    public static T Load<T>(string stubPath)\\n    {\\n      try\\n      {\\n        if (string.IsNullOrEmpty(stubPath))\\n          throw new ArgumentNullException(nameof(stubPath));\\n\\n        using (var file = File.OpenText(stubPath))\\n        using (var reader = new JsonTextReader(file))\\n        {\\n          return _serializer.Deserialize<T>(reader);\\n        }\\n      }\\n      catch (Exception exc)\\n      {\\n        throw new Exception($\\"Failed to load {stubPath}\\", exc);\\n      }\\n    }\\n  }\\n}\\n```\\n\\nAs you can see this class uses [JSON.Net](http://www.newtonsoft.com/json) and exposes 2 methods:\\n\\n<dl><dt>Make</dt><dd>Takes a given piece of data and uses JSON.Net to serialise it as JSON to a file. (nb I choose to format the JSON for readability and exclude null values; both totally optional)</dd><dt>Load</dt><dd>Takes the given path and loads the associated JSON file and deserialises it back into an object.</dd></dl>\\n\\nThe idea is this: we take our working implementation and, wherever it extracts data from an external source, we insert a temporary statement like this:\\n\\n```cs\\nvar data = _dataService.GetComplexData();\\n\\n    // Just inserted so we can generate the stub data...\\n    Stubs.Make($\\"{System.AppDomain.CurrentDomain.BaseDirectory}\\\\\\\\data.json\\", data);\\n```\\n\\nThe next time you run the implementation you\'ll find the app generates a `data.json` file containing the complex data serialized to JSON. Strip out your `Stubs.Make` statements from the implementation and we\'re ready for the next stage.\\n\\n## Using your JSON\\n\\nWhat you need to do now is to take the new and shiny `data.json` file and move it to your unit test project. It needs to be included within the unit test project. Also, for each JSON file you have, the `Build Action` in VS needs to be set to `Content` and the `Copy to Output Directory` to `Copy if newer`.\\n\\nThen within your unit tests you can write code like this:\\n\\n```ts\\nvar dummyData = Stubs.Load<ComplexDataType>(\'Stubs/data.json\');\\n```\\n\\nWhich pulls in your data from the JSON file and deserialises it into the original types. With this in hand you can plug together a unit test based on an existing implementation which depends on external data much faster than the hand-cranked method of old.\\n\\nFinally, before the wildebeest of TDD descend upon me howling and wailing, let me say again; I anticipate this being useful when you\'re trying to add tests to something that already exists but is untested. Clearly it would be better not to be in this situaion in the first place."},{"id":"/2016/03/22/elvis-and-king-concat","metadata":{"permalink":"/2016/03/22/elvis-and-king-concat","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2016-03-22-elvis-and-king-concat/index.md","source":"@site/blog/2016-03-22-elvis-and-king-concat/index.md","title":"Elvis and King Concat","description":"I hate LINQ\'s Enumerable.Concat when bringing together IEnumerables. Not the behaviour (I love that!) but rather how code ends up looking when you use it. Consider this:","date":"2016-03-22T00:00:00.000Z","formattedDate":"March 22, 2016","tags":[{"label":"Enumerable","permalink":"/tags/enumerable"},{"label":"Concat","permalink":"/tags/concat"}],"readingTime":2.525,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Elvis and King Concat","authors":"johnnyreilly","tags":["Enumerable","Concat"],"hide_table_of_contents":false},"prevItem":{"title":"Instant Stubs with JSON.Net (just add hot water)","permalink":"/2016/04/25/instant-stubs-with-jsonnet"},"nextItem":{"title":"Atom - Recovering from Corrupted Packages","permalink":"/2016/03/17/atom-recovering-from-corrupted-packages"}},"content":"I hate LINQ\'s `<a href=\\"https://msdn.microsoft.com/en-us/library/bb302894%28v=vs.110%29.aspx?f=255&amp;MSPPError=-2147217396\\">Enumerable.Concat</a>` when bringing together `IEnumerable`s. Not the behaviour (I love that!) but rather how code ends up looking when you use it. Consider this:\\n\\n```cs\\nvar concatenated = myCollection?.Select(x => new ConcatObj(x)) ?? new ConcatObj[0].Concat(\\n   myOtherCollection?.Select(x => new ConcatObj(x)) ?? new ConcatObj[0]\\n);\\n```\\n\\nIn this example I\'m bringing together 2 collections, either of which may be null (more on that later). I think we can all agree this doesn\'t represent a world of readability. I\'ve also had to create a custom class `ConcatObj` because you can\'t create an empty array for an anonymous type in C#.\\n\\n## Attempt #1: `ConcatMany`\\n\\nAfter toying around with a bunch of different ideas I created this extension method:\\n\\n```cs\\npublic static class FunctionalExtensions\\n{\\n    public static IEnumerable<T> ConcatMany<T>(\\n        this IEnumerable<T> original,\\n        params IEnumerable<T>[] enumerablesToConcat) => original.Concat(\\n            enumerablesToConcat.Where(e => e != null).SelectMany(c => c)\\n        );\\n}\\n```\\n\\nThanks to the joy of `params` this extension allows me to bring together multiple IEnumerables into a single one but has the advantage of considerably cleaner calling code:\\n\\n```cs\\nvar concatenated = Enumerable.Empty<ConcatObj>().ConcatMany(\\n    myCollection?.Select(x => new ConcatObj(x)),\\n    myOtherCollection?.Select(x => new ConcatObj(x))\\n    );\\n```\\n\\nFor my money this is more readable and intent is clearer. Particularly as the number of contributing IEnumerables goes up. The downside is that I can\u2019t use anonymous objects because you need to tell the compiler what the type is when using `<a href=\\"https://msdn.microsoft.com/en-us/library/bb341042%28v=vs.110%29.aspx?f=255&amp;MSPPError=-2147217396\\">Enumerable.Empty</a>`.\\n\\nWouldn\'t it be nice to have both:\\n\\n1. Readable code and\\n2. Anonymous objects?\\n\\n## Attempt #2: `EnumerableExtensions.Create`\\n\\nAfter batting round a few ideas (thanks Matt) I settled on this implementation:\\n\\n```cs\\npublic static class EnumerableExtensions\\n{\\n    public static IEnumerable<TSource> Create<TSource>(params IEnumerable<TSource>[] enumerables)\\n    {\\n        return Concat(enumerables.Where(e => e != null));\\n    }\\n\\n    private static IEnumerable<TSource> Concat<TSource>(IEnumerable<IEnumerable<TSource>> enumerables)\\n    {\\n        foreach (var enumerable in enumerables)\\n        {\\n            foreach (var item in enumerable)\\n            {\\n                yield return item;\\n            }\\n        }\\n    }\\n}\\n```\\n\\nWhich allows for calling code like this:\\n\\n```cs\\nvar concatenated = EnumerableExtensions.Create(\\n    myCollection?.Select(x => new { Anonymous = x.Types }),\\n    myOtherCollection?.Select(x => new { Anonymous = x.Types })\\n    );\\n```\\n\\nThat\'s right; anonymous types are back! Strictly speaking the `Concat` method above could be converted into a single `SelectMany` (and boy does ReSharper like telling me) but I\'m quite happy with it as is. And to be honest, I so rarely get to use `yield` in my own code; I thought it might be nice to give it a whirl \ud83d\ude0a\\n\\n## What Gives Elvis?\\n\\nIf you look closely at the implementation you\'ll notice that I purge all `null`s when I\'m bringing together the `Enumerable`s. For why? Some may legitimately argue this is a bad idea. However, there is method in my \\"bad practice\\".\\n\\nI\'ve chosen to treat `null` as \\"not important\\" for this use case. I\'m doing this because it emerges that ASP.NET MVC deserialises empty collections as nulls. (See [here](http://aspnetwebstack.codeplex.com/SourceControl/latest#src/System.Web.Mvc/ValueProviderResult.cs) and play spot the `return null;`) Which is a pain. But thanks to the null purging behaviour of `EnumerableExtensions.Create` I can trust in the [null-conditional (Elvis)](https://csharp.today/c-6-features-null-conditional-and-and-null-coalescing-operators/) operator to not do me wrong."},{"id":"/2016/03/17/atom-recovering-from-corrupted-packages","metadata":{"permalink":"/2016/03/17/atom-recovering-from-corrupted-packages","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2016-03-17-atom-recovering-from-corrupted-packages/index.md","source":"@site/blog/2016-03-17-atom-recovering-from-corrupted-packages/index.md","title":"Atom - Recovering from Corrupted Packages","description":"Every now and then when I try and update my packages in Atom I find this glaring back at me:","date":"2016-03-17T00:00:00.000Z","formattedDate":"March 17, 2016","tags":[{"label":"apm","permalink":"/tags/apm"},{"label":"Atom","permalink":"/tags/atom"},{"label":"packages","permalink":"/tags/packages"},{"label":"corrupt","permalink":"/tags/corrupt"}],"readingTime":0.705,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Atom - Recovering from Corrupted Packages","authors":"johnnyreilly","tags":["apm","Atom","packages","corrupt"],"hide_table_of_contents":false},"prevItem":{"title":"Elvis and King Concat","permalink":"/2016/03/22/elvis-and-king-concat"},"nextItem":{"title":"TFS 2012 meet PowerShell, Karma and BuildNumber","permalink":"/2016/03/04/tfs-2012-meet-powershell-karma-and-buildnumber"}},"content":"Every now and then when I try and update my packages in [Atom](https://atom.io/) I find this glaring back at me:\\n\\n![](Screenshot-2016-03-17-06.17.03.png)\\n\\nUg. The problem is that my atom packages have become corrupt. Quite how I couldn\'t say. But that\'s the problem. Atom, as I know from bitter experience, will not recover from this. It just sits there feeling sorry for itself. However, getting back to where you belong is simpler than you imagine:\\n\\n1. Shutdown Atom\\n2. In the file system go to `[Your name]/.atom` (and bear in mind this is Windows; Macs / Linux may be different)\\n\\n![](Screenshot-2016-03-17-06.17.53.png)\\n\\n3. You\'ll see an `.apm` folder that contains all your packages. Delete this.\\n\\nWhen you next fire up Atom these packages will automagically come back but this time they shouldn\'t be corrupt. Instead you should see the happiness of normality restored:\\n\\n![](Screenshot-2016-03-17-06.23.18.png)"},{"id":"/2016/03/04/tfs-2012-meet-powershell-karma-and-buildnumber","metadata":{"permalink":"/2016/03/04/tfs-2012-meet-powershell-karma-and-buildnumber","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2016-03-04-tfs-2012-meet-powershell-karma-and-buildnumber/index.md","source":"@site/blog/2016-03-04-tfs-2012-meet-powershell-karma-and-buildnumber/index.md","title":"TFS 2012 meet PowerShell, Karma and BuildNumber","description":"To my lasting regret, TFS 2012 has no direct support for PowerShell. Such a shame as PowerShell scripts can do a lot of heavy lifting in a build process. Well, here we\'re going to brute force TFS 2012 into running PowerShell scripts. And along the way we\'ll also get Karma test results publishing into TFS 2012 as an example usage. Nice huh? Let\'s go!","date":"2016-03-04T00:00:00.000Z","formattedDate":"March 4, 2016","tags":[{"label":"BuildDefinitionName","permalink":"/tags/build-definition-name"},{"label":"BuildNumber","permalink":"/tags/build-number"},{"label":"npm","permalink":"/tags/npm"},{"label":"Karma","permalink":"/tags/karma"},{"label":"powershell","permalink":"/tags/powershell"},{"label":"trx","permalink":"/tags/trx"},{"label":"TFS 2012","permalink":"/tags/tfs-2012"}],"readingTime":5.4,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"TFS 2012 meet PowerShell, Karma and BuildNumber","authors":"johnnyreilly","tags":["BuildDefinitionName","BuildNumber","npm","Karma","powershell","trx","TFS 2012"],"hide_table_of_contents":false},"prevItem":{"title":"Atom - Recovering from Corrupted Packages","permalink":"/2016/03/17/atom-recovering-from-corrupted-packages"},"nextItem":{"title":"Creating Angular UI Routes in the Controller","permalink":"/2016/02/29/creating-angular-ui-routes-in-controller"}},"content":"To my lasting regret, TFS 2012 has no direct support for PowerShell. Such a shame as PowerShell scripts can do a lot of heavy lifting in a build process. Well, here we\'re going to brute force TFS 2012 into running PowerShell scripts. And along the way we\'ll also get Karma test results publishing into TFS 2012 as an example usage. Nice huh? Let\'s go!\\n\\n## PowerShell via `csproj`\\n\\nIt\'s time to hack the `csproj` (or whatever project file you have) again. We\'re going to add an `AfterBuild` target to the end of the file. This target will be triggered after the build completes (as the name suggests):\\n\\n```xml\\n<Target Name=\\"AfterBuild\\">\\n    <Message Importance=\\"High\\" Text=\\"AfterBuild: PublishUrl = $(PublishUrl), BuildUri = $(BuildUri), Configuration = $(Configuration), ProjectDir = $(ProjectDir), TargetDir = $(TargetDir), TargetFileName = $(TargetFileName), BuildNumber = $(BuildNumber), BuildDefinitionName = $(BuildDefinitionName)\\" />\\n    <Exec Command=\\"powershell.exe -NonInteractive -ExecutionPolicy RemoteSigned \\"& \'$(ProjectDir)AfterBuild.ps1\' \'$(Configuration)\' \'$(ProjectDir)\' \'$(TargetDir)\' \'$(PublishUrl)\' \'$(BuildNumber)\' \'$(BuildDefinitionName)\'\\"\\" />\\n  </Target>\\n```\\n\\nThere\'s 2 things happening in this target:\\n\\n1. A message is printed out during compilation which contains details of the various compile time variables. This is nothing more than a `console.log` statement really; it\'s useful for debugging and so I keep it around. You\'ll notice one of them is called `$(BuildNumber)`; more on that later.\\n2. A command is executed; PowerShell! This invokes PowerShell with the `-NonInteractive` and `-ExecutionPolicy RemoteSigned` flags. It passes a script to be executed called `AfterBuild.ps1` that lives in the root of the project directory. To that script a number of parameters are supplied; compile time variables that we may use in the script.\\n\\n## Where\'s my `BuildNumber` and `BuildDefinitionName`?\\n\\nSo you\'ve checked in your changes and kicked off a build on the server. You\'re picking over the log messages and you\'re thinking: \\"Where\'s my `BuildNumber`?\\". Well, TFS 2012 does not have that set as a variable at compile time by default. This stumped me for a while but thankfully I wasn\'t the only person wondering... As ever, [Stack Overflow had the answer](http://stackoverflow.com/a/7330453/761388). So, deep breath, it\'s time to hack the TFS build template file.\\n\\nCheckout the `DefaultTemplate.11.1.xaml` file from TFS and open it in your text editor of choice. It\'s _find and replace_ time! (There are probably 2 instances that need replacement.) Perform a _find_ for the below\\n\\n```js\\n[String.Format(&quot;/p:SkipInvalidConfigurations=true {0}&quot;, MSBuildArguments)]\\n```\\n\\nAnd _replace_ it with this:\\n\\n```js\\n[\\n  String.Format(\\n    \'/p:SkipInvalidConfigurations=true /p:BuildNumber={1} /p:BuildDefinitionName={2} {0}\',\\n    MSBuildArguments,\\n    BuildDetail.BuildNumber,\\n    BuildDetail.BuildDefinition.Name\\n  ),\\n];\\n```\\n\\nPretty long line eh? Let\'s try breaking that up to make it more readable: (but remember in the XAML it needs to be a one liner)\\n\\n```js\\n[String.Format(\\"/p:SkipInvalidConfigurations=true\\n    /p:BuildNumber={1}\\n    /p:BuildDefinitionName={2} {0}\\", MSBuildArguments, BuildDetail.BuildNumber, BuildDetail.BuildDefinition.Name)]\\n```\\n\\nWe\'re just adding 2 extra parameters of `BuildNumber` and `BuildDefinitionName` to the invocation of MSBuild. Once we\'ve checked this back in, `BuildNumber` and `BuildDefinitionName` will be available on future builds. Yay! _Important! You must have a build name that does not feature spaces; probably there\'s a way to pass spaces here but I\'m not sure what it is._\\n\\n## `AfterBuild.ps1`\\n\\nYou can use your `AfterBuild.ps1` script to do any number of things. In my case I\'m going to use MSTest to publish some test results which have been generated by Karma into TFS:\\n\\n```ps\\nparam ([string]$configuration, [string]$projectDir, [string]$targetDir, [string]$publishUrl, [string]$buildNumber, [string] $buildDefinitionName)\\n\\n$ErrorActionPreference = \'Stop\'\\nClear\\n\\nfunction PublishTestResults([string]$resultsFile) {\\n Write-Host \'PublishTests\'\\n $mstest = \'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 14.0\\\\Common7\\\\IDE\\\\MSTest.exe\'\\n\\n Write-Host \\"Using $mstest at $pwd\\"\\n Write-Host \\"Publishing: $resultsFile\\"\\n\\n & $mstest /publishresultsfile:$resultsFile /publish:http://my-tfs-server:8080/tfs /teamproject:MyProject /publishbuild:$buildNumber /platform:\'Any CPU\' /flavor:Release\\n}\\n\\nfunction FailBuildIfThereAreTestFailures([string]$resultsFile) {\\n $results = [xml](GC $resultsFile)\\n $outcome = $results.TestRun.ResultSummary.outcome\\n $fgColor = if($outcome -eq \\"Failed\\") { \\"Red\\" } else { \\"Green\\" }\\n $total = $results.TestRun.ResultSummary.Counters.total\\n $passed = $results.TestRun.ResultSummary.Counters.passed\\n $failed = $results.TestRun.ResultSummary.Counters.failed\\n\\n $failedTests = $results.TestRun.Results.UnitTestResult | Where-Object { $_.outcome -eq \\"Failed\\" }\\n\\n Write-Host Test Results: $outcome -ForegroundColor $fgColor -BackgroundColor \\"Black\\"\\n Write-Host Total tests: $total\\n Write-Host Passed: $passed\\n Write-Host Failed: $failed\\n Write-Host\\n\\n $failedTests | % { Write-Host Failed test: $_.testName\\n  Write-Host $_.Output.ErrorInfo.Message\\n  Write-Host $_.Output.ErrorInfo.StackTrace }\\n\\n Write-Host\\n\\n if($outcome -eq \\"Failed\\") {\\n  Write-Host \\"Failing build as there are broken tests\\"\\n  $host.SetShouldExit(1)\\n }\\n}\\n\\nfunction Run() {\\n  Write-Host \\"Running AfterBuild.ps1 using Configuration: $configuration, projectDir: $projectDir, targetDir: $targetDir, publishUrl: $publishUrl, buildNumber: $buildNumber, buildDefinitionName: $buildDefinitionName\\"\\n\\n if($buildNumber) {\\n  $resultsFile = \\"$projectDir\\\\test-results.trx\\"\\n  PublishTestResults $resultsFile\\n  FailBuildIfThereAreTestFailures $resultsFile\\n }\\n}\\n\\n# Off we go...\\nRun\\n```\\n\\nAssuming we have a build number this script kicks off the `PublishTestResults` function above. So we won\'t attempt to publish test results when compiling in Visual Studio on our dev machine. The script looks for `MSTest.exe` in a certain location on disk (the default VS 2015 installation location in fact; it may be installed elsewhere on your build machine). MSTest is then invoked and passed a file called `test-results.trx` which is is expected to live in the root of the project. All being well, the test results will be registered with the running build and will be visible when you look at test results in TFS.\\n\\nFinally in `FailBuildIfThereAreTestFailures` we parse the `test-results.trx` file (and for this I\'m totally in the debt of [David Robert\'s helpful Gist](https://gist.github.com/davidroberts63/5655441)). We write out the results to the host so it\'ll show up in the MSBuild logs. Also, and this is crucial, if there are any failures we fail the build by exiting PowerShell with a code of 1. We are deliberately swallowing any error that Karma raises earlier when it detects failed tests. We do this because we want to publish the failed test results to TFS _before_ we kill the build.\\n\\n## Bonus Karma: `test-results.trx`\\n\\nIf you\'ve read a [previous post of mine](https://blog.johnnyreilly.com/2016/02/visual-studio-tsconfigjson-and-external.html) you\'ll be aware that it\'s possible to get MSBuild to kick off npm build tasks. Specifically I have MSBuild kicking off an `npm run build`. My `package.json` looks like this:\\n\\n```json\\n\\"scripts\\": {\\n    \\"test\\": \\"karma start --reporters mocha,trx --single-run --browsers PhantomJS\\",\\n    \\"clean\\": \\"gulp delete-dist-contents\\",\\n    \\"watch\\": \\"gulp watch\\",\\n    \\"build\\": \\"gulp build\\",\\n    \\"postbuild\\": \\"npm run test\\"\\n  },\\n```\\n\\nYou can see that the `postbuild` hook kicks off the `test` script in turn. And that `test` script kicks off a single run of karma. I\'m not going to go over setting up Karma at all here; there are other posts out there that cover that admirably. But I wanted to share news of the [karma trx reporter](https://www.npmjs.com/package/karma-trx-reporter). This reporter is the thing that produces our `test-results.trx` file; trx being the format that TFS likes to deal with.\\n\\nSo now we\'ve got a PowerShell hook into our build process (something very useful in itself) which we are using to publish Karma test results into TFS 2012. They said it couldn\'t be done. They were wrong. Huzzah!!!!!!!"},{"id":"/2016/02/29/creating-angular-ui-routes-in-controller","metadata":{"permalink":"/2016/02/29/creating-angular-ui-routes-in-controller","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2016-02-29-creating-angular-ui-routes-in-controller/index.md","source":"@site/blog/2016-02-29-creating-angular-ui-routes-in-controller/index.md","title":"Creating Angular UI Routes in the Controller","description":"So you\'re creating a link with the Angular UI Router. You\'re passing more than a few parameters and it\'s getting kinda big. Something like this:","date":"2016-02-29T00:00:00.000Z","formattedDate":"February 29, 2016","tags":[{"label":"ng-href","permalink":"/tags/ng-href"},{"label":"UI Router","permalink":"/tags/ui-router"},{"label":"Angular","permalink":"/tags/angular"},{"label":"ui-sref","permalink":"/tags/ui-sref"}],"readingTime":1.865,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Creating Angular UI Routes in the Controller","authors":"johnnyreilly","tags":["ng-href","UI Router","Angular","ui-sref"],"hide_table_of_contents":false},"prevItem":{"title":"TFS 2012 meet PowerShell, Karma and BuildNumber","permalink":"/2016/03/04/tfs-2012-meet-powershell-karma-and-buildnumber"},"nextItem":{"title":"Visual Studio, tsconfig.json and external TypeScript compilation","permalink":"/2016/02/19/visual-studio-tsconfigjson-and-external"}},"content":"So you\'re creating a link with the Angular UI Router. You\'re passing more than a few parameters and it\'s getting kinda big. Something like this:\\n\\n```xml\\n<a class=\\"contains-icon\\"\\n      ui-sref=\\"Entity.Edit({ entityId: (vm.selectedEntityId ? vm.selectedEntityId: null), initialData: vm.initialData })\\">\\n        <i class=\\"fa fa-pencil\\"></i>Edit\\n   </a>\\n```\\n\\nSee? It\'s too long to fit on the screen without wrapping. It\'s clearly mad and bad.\\n\\nGenerally I try to keep the logic in a view to a minimum. It makes the view harder to read, it makes behaviour of the app harder to reason about. Also, it\'s not testable and (if you\'re using some kind of static typing like TypeScript) it is entirely out of the realms that a compiler can catch. So what to do? Move the URL generation to the controller. That\'s what I decided to do after I had a typo in my view which I didn\'t catch until post-commit.\\n\\n## `ui-sref` in the Controller\\n\\nActually, that\'s not exactly what you want to do. If you look at the [Angular UI Router docs](http://angular-ui.github.io/ui-router/site/#/api/ui.router.state.directive:ui-sref) you will see that `ui-sref` is:\\n\\n> ...a directive that binds a link (`&lt;a&gt;` tag) to a state. If the state has an associated URL, the directive will automatically generate & update the href attribute via the [`$state.href()`](http://angular-ui.github.io/ui-router/site/#/api/ui.router.state.$state#methods_href) method.\\n\\nSo what we actually want to do is use the `$state.href()` method in our controller. To take our example above we\'ll create another method on our controller called `getEditUrl`\\n\\n```js\\nexport class EntityController {\\n  $state: angular.ui.IStateService;\\n\\n  static $inject = [\'$state\'];\\n  constructor($state: angular.ui.IStateService) {\\n    this.$state = $state;\\n  }\\n\\n  //... Other stuff\\n\\n  getEditUrl() {\\n    return this.$state.href(\'Entity.Edit\', {\\n      selectedEntityId: this.selectedEntityId ? this.selectedEntityId : null,\\n      initialData: this.initialData,\\n    });\\n  }\\n}\\n```\\n\\nYou can see I\'m using TypeScript here; but feel free to strip out the type annotations and go with raw ES6 classes; that\'ll still give you testability if not static typing.\\n\\nNow we\'ve added the `getEditUrl` method we just need to reference it in our view:\\n\\n```xml\\n<a class=\\"contains-icon\\" ng-href=\\"{{vm.getEditUrl()}}\\"><i class=\\"fa fa-pencil\\"></i>Edit</a>\\n```\\n\\nNote we\'ve ditched usage of the `ui-sref` directive and gone with Angular\'s native [`ng-href`](https://docs.angularjs.org/api/ng/directive/ngHref). Within that directive we execute our `getEditUrl` as an expression which gives us our route. As a bonus, our view is much less cluttered and comprehensible as a result. How lovely."},{"id":"/2016/02/19/visual-studio-tsconfigjson-and-external","metadata":{"permalink":"/2016/02/19/visual-studio-tsconfigjson-and-external","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2016-02-19-visual-studio-tsconfigjson-and-external/index.md","source":"@site/blog/2016-02-19-visual-studio-tsconfigjson-and-external/index.md","title":"Visual Studio, tsconfig.json and external TypeScript compilation","description":"TypeScript first gained support for tsconfig.json back with the 1\\\\.5 release. However, to my lasting regret and surprise Visual Studio will not be gaining meaningful support for it until TypeScript 1.8 ships. However, if you want it now, it\'s already available to use in beta.","date":"2016-02-19T00:00:00.000Z","formattedDate":"February 19, 2016","tags":[{"label":"TFS","permalink":"/tags/tfs"},{"label":"Visual Studio","permalink":"/tags/visual-studio"},{"label":"tsconfig.json","permalink":"/tags/tsconfig-json"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"Webpack","permalink":"/tags/webpack"}],"readingTime":5.855,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Visual Studio, tsconfig.json and external TypeScript compilation","authors":"johnnyreilly","tags":["TFS","Visual Studio","tsconfig.json","TypeScript","Webpack"],"hide_table_of_contents":false},"prevItem":{"title":"Creating Angular UI Routes in the Controller","permalink":"/2016/02/29/creating-angular-ui-routes-in-controller"},"nextItem":{"title":"TFS 2012, .NET 4.5 and C# 6","permalink":"/2016/02/01/tfs-2012-net-45-and-c-6"}},"content":"TypeScript first gained support for [`tsconfig.json`](https://github.com/Microsoft/TypeScript/wiki/tsconfig.json) back with the [1\\\\.5 release](https://blogs.msdn.microsoft.com/typescript/2015/07/20/announcing-typescript-1-5/). However, to my lasting regret and surprise Visual Studio will not be gaining meaningful support for it until [TypeScript 1.8](https://github.com/Microsoft/TypeScript/wiki/What%27s-new-in-TypeScript#improved-support-for-tsconfigjson-in-visual-studio-2015) ships. However, if you want it now, it\'s already available to use in [beta](https://blogs.msdn.microsoft.com/typescript/2016/01/28/announcing-typescript-1-8-beta/).\\n\\nI\'ve already leapt aboard. Whilst there\'s a number of bugs in the beta it\'s still totally usable. So use it.\\n\\n## External TypeScript Compilation and the VS build\\n\\nWhilst `tsconfig.json` is useful and super cool it has limitations. It allows you to deactivate compilation upon file saving using [`compileOnSave`](https://github.com/Microsoft/TypeScript/issues/2326#issuecomment-178294169). [What it doesn\'t allow is deactivation of the TypeScript compilation that happens as part of a Visual Studio build.](https://github.com/Microsoft/TypeScript/issues/7091) That may not matter for the vanilla workflow of just dropping TypeScript files in a Visual Studio web project and having VS invoke the TypeScript compilation. However it comes to matter when your workflow deviates from the norm, as mine does. Using external compilation of TypeScript within Visual Studio is a little tricky. My own use case is somewhat atypical but perhaps not uncommon.\\n\\nI\'m working on a project which has been built using TypeScript since TS 0.9. Not surprisingly, this started off using the default Visual Studio / TypeScript workflow. Active development on the project ceased around 9 months ago. Now it\'s starting up again. It\'s a reasonable sized web app and the existing functionality is, in the main, fine. But the users want to add some new screens.\\n\\nLike any developer, I want to build with the latest and greatest. In my case, this means I want to write modular ES6 using TypeScript. With this approach my code can be leaner and I have less script ordering drama in my life. (Yay import statements!) This can be done by bringing together webpack, TypeScript ([ts-loader](https://github.com/TypeStrong/ts-loader)) and [Babel](http://babeljs.io/) ([babel-loader](https://github.com/babel/babel-loader)). There\'s an example of this approach [here](https://blog.johnnyreilly.com/2015/12/es6-typescript-babel-react-flux-karma.html). Given the size of the existing codebase I\'d rather leave the legacy TypeScript as is and isolate my new approach to the screens I\'m going to build. Obviously I\'d like to have a common build process for all the codebase at some point but I\'ve got a deadline to meet and so a half-old / half-new approach is called for (at least for the time being).\\n\\n## Goodbye TypeScript Compilation in VS\\n\\nWriting modular ES6 TypeScript which is fully transpiled to old-school JS is _not possible_ using the Visual Studio tooling at present. For what it\'s worth I think that SystemJS compilation may make this more possible in the future but I don\'t really know enough about it to be sure. That\'s why I\'m bringing webpack / Babel into the mix right now. I don\'t want Visual Studio to do anything for the ES6 code; I don\'t want it to compile. I want to deactivate my TypeScript compilation for the ES6 code. I can\'t do this from the `tsconfig.json` so I\'m in a bit of a hole. What to do?\\n\\nWell, as of (I think) TypeScript 1.7 it\'s possible to deactivate TypeScript compilation in Visual Studio. To [quote](https://github.com/Microsoft/TypeScript/issues/2294#issuecomment-129367578):\\n\\n> there is an easier way to disable TypeScriptCompile:\\n>\\n> Just add `&lt;TypeScriptCompileBlocked&gt;true&lt;/TypeScriptCompileBlocked&gt;` to the `.csproj`, e.g. in the first `&lt;PropertyGroup&gt;`.\\n\\nAwesomeness!\\n\\nBut wait, this means that the legacy TypeScript isn\'t being compiled any longer. Bear in mind, I\'m totally happy with the existing / legacy TypeScript compilation. Nooooooooooooooo!!!!!!!!!!!!!!!\\n\\n## Hello TypeScript Compilation outside VS\\n\\nHave no fear, I gotcha. What we\'re going to do is ensure that Visual Studio triggers 2 external TypeScript builds as part of its own build process:\\n\\n- The modular ES6 TypeScript (new)\\n- The legacy TypeScript (old)\\n\\nHow do we do this? Through the magic of build targets. We need to add this to our `.csproj`: (I add it near the end; I\'m not sure if location matters though)\\n\\n```xml\\n<PropertyGroup>\\n    <CompileDependsOn>\\n      $(CompileDependsOn);\\n      WebClientBuild;\\n    </CompileDependsOn>\\n    <CleanDependsOn>\\n      $(CleanDependsOn);\\n      WebClientClean\\n    </CleanDependsOn>\\n    <CopyAllFilesToSingleFolderForPackageDependsOn>\\n      CollectGulpOutput;\\n      CollectLegacyTypeScriptOutput;\\n      $(CopyAllFilesToSingleFolderForPackageDependsOn);\\n    </CopyAllFilesToSingleFolderForPackageDependsOn>\\n    <CopyAllFilesToSingleFolderForMsdeployDependsOn>\\n      CollectGulpOutput;\\n      CollectLegacyTypeScriptOutput;\\n      $(CopyAllFilesToSingleFolderForPackageDependsOn);\\n    </CopyAllFilesToSingleFolderForMsdeployDependsOn>\\n  </PropertyGroup>\\n  <Target Name=\\"WebClientBuild\\">\\n    <Exec Command=\\"npm install\\" />\\n    <Exec Command=\\"npm run build-legacy-typescript\\" />\\n    <Exec Command=\\"npm run build -- --mode $(ConfigurationName)\\" />\\n  </Target>\\n  <Target Name=\\"WebClientClean\\">\\n    <Exec Command=\\"npm run clean\\" />\\n  </Target>\\n  <Target Name=\\"CollectGulpOutput\\">\\n    <ItemGroup>\\n      <_CustomFiles Include=\\"dist\\\\**\\\\*\\" />\\n      <FilesForPackagingFromProject Include=\\"%(_CustomFiles.Identity)\\">\\n        <DestinationRelativePath>dist\\\\%(RecursiveDir)%(Filename)%(Extension)</DestinationRelativePath>\\n      </FilesForPackagingFromProject>\\n    </ItemGroup>\\n    <Message Text=\\"CollectGulpOutput list: %(_CustomFiles.Identity)\\" />\\n  </Target>\\n  <Target Name=\\"CollectLegacyTypeScriptOutput\\">\\n    <ItemGroup>\\n      <_CustomFiles Include=\\"Scripts\\\\**\\\\*.js\\" />\\n      <FilesForPackagingFromProject Include=\\"%(_CustomFiles.Identity)\\">\\n        <DestinationRelativePath>Scripts\\\\%(RecursiveDir)%(Filename)%(Extension)</DestinationRelativePath>\\n      </FilesForPackagingFromProject>\\n    </ItemGroup>\\n    <Message Text=\\"CollectLegacyTypeScriptOutput list: %(_CustomFiles.Identity)\\" />\\n  </Target>\\n```\\n\\nThere\'s a few things going on here; let\'s take them one by one.\\n\\n## The `WebClientBuild` Target\\n\\nThis target triggers our external builds. One by one it runs the following commands:\\n\\n<dl><dt><code>npm install</code></dt><dd>Installs the npm packages.</dd><dt><code>npm run build-legacy-typescript</code></dt><dd>Runs the <code>\\"build-legacy-typescript\\"</code><code>script</code> in our <code>package.json</code></dd><dt><code>npm run build -- --mode $(ConfigurationName)</code></dt><dd>Runs the <code>\\"build\\"</code><code>script</code> in our <code>package.json</code> and passes through a <code>mode</code> parameter of either <code>\\"Debug\\"</code> or <code>\\"Release\\"</code> from MSBuild - indicating whether we\'re creating a debug or a release build.</dd></dl>\\n\\nAs you\'ve no doubt gathered, I\'m following the convention of using the `scripts` element of my `package.json` as repository for the various build tasks I might have for a web project. It looks like this:\\n\\n```json\\n{\\n  // ...\\n  \\"scripts\\": {\\n    \\"test\\": \\"karma start --reporters mocha,junit --single-run --browsers PhantomJS\\",\\n    \\"build-legacy-typescript\\": \\"tsc -v&&tsc --project Scripts\\",\\n    \\"clean\\": \\"gulp delete-dist-contents\\",\\n    \\"watch\\": \\"gulp watch\\",\\n    \\"build\\": \\"gulp build\\"\\n  }\\n  // ...\\n}\\n```\\n\\nAs you can see, `\\"build-legacy-typescript\\"` invokes `tsc` (which is registered as a `devDependency`) to print out the version of the compiler. Then it invokes `tsc` again using the [`project`](https://github.com/Microsoft/TypeScript/wiki/Compiler-Options) flag directly on the `Scripts` directory. This is where the legacy TypeScript and its associated `tsconfig.json` resides. Et voil\xe1, the old / existing TypeScript is compiled just as it was previously by VS itself.\\n\\nNext, the `\\"build\\"` invokes a `gulp` task called, descriptively, `\\"build\\"`. This task caters for our brand new codebase of modular ES6 TypeScript. When run, this task will invoke webpack, copy static files, build less etc. Quick digression: you can see there\'s a `\\"watch\\"` script that does the same thing on a file-watching basis; I use that during development.\\n\\n## The `WebClientClean` Target\\n\\nThe task that runs to clean up artefacts created by `WebClientBuild`.\\n\\n## The `CollectLegacyTypeScriptOutput` and `CollectGulpOutput` Targets\\n\\nSince we\'re compiling our TypeScript outside of VS we need to tell MSBuild / MSDeploy about the externally compiled assets in order that they are included in the publish pipeline. Here I\'m standing on the shoulders of [Steve Cadwallader\'s excellent post](http://www.codecadwallader.com/2015/03/15/integrating-gulp-into-your-tfs-builds-and-web-deploy/). Thanks Steve!\\n\\n`CollectLegacyTypeScriptOutput` and `CollectGulpOutput` respectively include all the built files contained in the `\\"Scripts\\"` and `\\"dist\\"` folders when a publish takes place. You don\'t need this for when you\'re building on your own machine but if you\'re looking to publish (either from your machine or from TFS) then you will need exactly this. Believe me that last sentence was typed with a memory of _great_ pain and frustration.\\n\\nSo in the end, as far as TypeScript is concerned, I\'m using Visual Studio solely as an editor. It\'s the hooks in the `.csproj` that ensure that compilation happens. It seems a little quirky that we still need to have the original TypeScript targets in the `.csproj` file as well; but it works. That\'s all that matters."},{"id":"/2016/02/01/tfs-2012-net-45-and-c-6","metadata":{"permalink":"/2016/02/01/tfs-2012-net-45-and-c-6","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2016-02-01-tfs-2012-net-45-and-c-6/index.md","source":"@site/blog/2016-02-01-tfs-2012-net-45-and-c-6/index.md","title":"TFS 2012, .NET 4.5 and C# 6","description":"So, you want to use C# 6 language features and you\u2019re working on an older project that\u2019s still rocking .NET 4.5. Well, with some caveats, you can.","date":"2016-02-01T00:00:00.000Z","formattedDate":"February 1, 2016","tags":[{"label":"C# 6","permalink":"/tags/c-6"},{"label":".Net 4.5","permalink":"/tags/net-4-5"},{"label":"TFS 2012","permalink":"/tags/tfs-2012"}],"readingTime":0.85,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"TFS 2012, .NET 4.5 and C# 6","authors":"johnnyreilly","tags":["C# 6",".Net 4.5","TFS 2012"],"hide_table_of_contents":false},"prevItem":{"title":"Visual Studio, tsconfig.json and external TypeScript compilation","permalink":"/2016/02/19/visual-studio-tsconfigjson-and-external"},"nextItem":{"title":"Coded UI and the Curse of the Docking Station","permalink":"/2016/01/14/coded-ui-and-curse-of-docking-station"}},"content":"So, you want to use C# 6 language features and you\u2019re working on an older project that\u2019s still rocking .NET 4.5. Well, with [some caveats](http://stackoverflow.com/a/28921749/761388), you can.\\n\\nThe new compiler will compile targeting older framework versions. Well that\u2019s all lovely; let\u2019s all go home.\\n\\nNow. What say you\u2019ve got an old, old build server? It\u2019s TFS 2012 Update 2, creaking away, still glad to alive and kind of wondering why it hasn\u2019t been upgraded or retired. This is where you want to compile .NET 4.5 from C# 6. Well it can be done. Here\u2019s how it\u2019s done:\\n\\n1. Install Visual Studio 2015 on the build server (I\u2019m told this can be achieved using [Microsoft Build Tools 2015](https://www.microsoft.com/en-us/download/details.aspx?id=48159) but I haven\u2019t tried it myelf so caveat emptor)\\n2. set the `MSBuild Arguments` in the build definition to `/p:VisualStudioVersion=14.0` (i.e. Visual Studio 2015 mode)\\n\\n![](EditBuildConfiguration.png)\\n\\n3. in each project that uses C# 6 syntax, install the NuGet package [Microsoft.Net.Compilers](https://www.nuget.org/packages/Microsoft.Net.Compilers) with a quick `install-package Microsoft.Net.Compilers`\\n\\nThat\u2019s it; huzzah! String interpolation here I come\u2026"},{"id":"/2016/01/14/coded-ui-and-curse-of-docking-station","metadata":{"permalink":"/2016/01/14/coded-ui-and-curse-of-docking-station","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2016-01-14-coded-ui-and-curse-of-docking-station/index.md","source":"@site/blog/2016-01-14-coded-ui-and-curse-of-docking-station/index.md","title":"Coded UI and the Curse of the Docking Station","description":"I\u2019ve a love / hate relationship with Coded UI. Well hate / hate might be more accurate. Hate perhaps married with a very grudging respect still underpinned by a wary bitterness. Yes, that\u2019s about the size of it. Why? Well, when Coded UI works, it\u2019s fab. But it\u2019s flaky as anything. Anybody who\u2019s used the technology is presently nodding sagely and holding back the tears. It\u2019s all a bit... tough.","date":"2016-01-14T00:00:00.000Z","formattedDate":"January 14, 2016","tags":[{"label":"Docking station","permalink":"/tags/docking-station"},{"label":"Surface Pro 3","permalink":"/tags/surface-pro-3"},{"label":"Coded UI","permalink":"/tags/coded-ui"},{"label":"Second monitor","permalink":"/tags/second-monitor"}],"readingTime":2.42,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Coded UI and the Curse of the Docking Station","authors":"johnnyreilly","tags":["Docking station","Surface Pro 3","Coded UI","Second monitor"],"hide_table_of_contents":false},"prevItem":{"title":"TFS 2012, .NET 4.5 and C# 6","permalink":"/2016/02/01/tfs-2012-net-45-and-c-6"},"nextItem":{"title":"UseStaticFiles for ASP.Net Framework","permalink":"/2016/01/01/usestaticfiles-for-aspnet-vold"}},"content":"I\u2019ve a love / hate relationship with Coded UI. Well hate / hate might be more accurate. Hate perhaps married with a very grudging respect still underpinned by a wary bitterness. Yes, that\u2019s about the size of it. Why? Well, when Coded UI works, it\u2019s fab. But it\u2019s flaky as anything. Anybody who\u2019s used the technology is presently nodding sagely and holding back the tears. It\u2019s all a bit... tough.\\n\\nI\u2019ve recently discovered another quirk to add to the list. Docking stations. I was back working on a project which had a Coded UI test suite. I\u2019d heard tell that there were problems with the tests and was just taking a look at them. The first hurdle I fell at was getting the tests to run locally. The tests had first been developed on a standard desktop build and, as much as this can ever be said of Coded UI tests, they worked. However, the future had happened. The company in question was no longer using the old school desktop towers. Nope, they\u2019d reached for the sky and equipped the whole office with Surface Pro 3\u2019s, hot desks, docking stations and big, big monitors. It looked terribly flash.\\n\\nCoded UI was not happy.\\n\\nThe `Mouse.Click` behaviour wasn\u2019t working. Most tests need the ability for users to click on buttons, dropdowns etc. That\u2019s part of a normal UI. And so it was with these tests. This is where they fell over. The reason they fell over at this point didn\u2019t become clear for a while. It wasn\u2019t until we tried tweaking our implementation of the tests that we realised what was happening. The tests normally found buttons / dropdowns etc on the screen and then attempted to perform a `Mouse.Click` upon them. We changed the implementation to be subtly different. Instead of just clicking on the element we amended the test to move the mouse to the button and then perform the click.\\n\\nAha!\\n\\nRather than steadily moving towards an element and clicking, the pointer was swerving like a drunk man crossing the road at 3am. It completely missed the element it was aiming for and clicked upon a seemingly random area of the screen. This is Coded UI doing \u201cpin the tail on the donkey\u201d.\\n\\nAfter more time than I\'d like to admit I happened upon the solution. I tended to dock my Surface and then tune my monitor resolution to the one most optimal for coding. (ie really high res.) This is what messes with Coded UI\'s head; the resolution change. If I wanted to be able to run tests successfully all I had to do was switch back to the resolution I initially booted with. Alternately I could restart my computer so it launched with the resolution I was presently using.\\n\\nOnce you do follow this guidance Coded UI has a moment of clarity, gets sober and starts `Mouse.Click`\\\\-ing like a pro."},{"id":"/2016/01/01/usestaticfiles-for-aspnet-vold","metadata":{"permalink":"/2016/01/01/usestaticfiles-for-aspnet-vold","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2016-01-01-usestaticfiles-for-aspnet-vold/index.md","source":"@site/blog/2016-01-01-usestaticfiles-for-aspnet-vold/index.md","title":"UseStaticFiles for ASP.Net Framework","description":"This is a guide on how not to expose all your static files to the world at large when working with the ASP.Net Framework. How to move from a blocklisting approach to a allowlisting approach.","date":"2016-01-01T00:00:00.000Z","formattedDate":"January 1, 2016","tags":[{"label":"HTML5 History API","permalink":"/tags/html-5-history-api"},{"label":"Single Page Applications","permalink":"/tags/single-page-applications"},{"label":"UseStaticFiles","permalink":"/tags/use-static-files"},{"label":"Routing","permalink":"/tags/routing"},{"label":"URL Rewrite","permalink":"/tags/url-rewrite"}],"readingTime":6.07,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"UseStaticFiles for ASP.Net Framework","authors":"johnnyreilly","tags":["HTML5 History API","Single Page Applications","UseStaticFiles","Routing","URL Rewrite"],"hide_table_of_contents":false},"prevItem":{"title":"Coded UI and the Curse of the Docking Station","permalink":"/2016/01/14/coded-ui-and-curse-of-docking-station"},"nextItem":{"title":"Live Reload Considered Harmful","permalink":"/2015/12/20/live-reload-considered-harmful"}},"content":"This is a guide on how _not_ to expose all your static files to the world at large when working with the ASP.Net Framework. How to move from a blocklisting approach to a allowlisting approach.\\n\\nNot clear? Stick around; I\'ll get better. Oh and that\'s not all, we\'ve also got.... drumroll:\\n\\n## Support for [HTML5 History API](https://html.spec.whatwg.org/multipage/browsers.html#the-history-interface)!\\n\\nWhat that means, in as close to English as I can get it, is real URLs for Single Page Applications. None of that hash-based routing malarkey. So, `https://i-am-your-domain.com/i-am-your-route` rather than `https://i-am-your-domain.com/<em>#/</em>i-am-your-route`. (For a more in depth look at the different sorts of routing SPA\'s can use then take a look at the [excellent docs](http://rackt.org/history/stable/GettingStarted.html) by the folk behind [React Router](https://github.com/rackt/react-router). These concepts are not React specific and can be applied to any SPA technology.)\\n\\n## `UseStaticFiles`\\n\\nYou may be aware that historically ASP.Net has been somewhat unusual in its approach to serving static files. Essentially, all the files in a project are theoretically servable. Okay, that\'s not entirely true; things like the `web.config` files etc are not going to be handed over to someone browsing your site. But other files that you might well want kept away from prying eyes may be. So your [TypeScript](http://www.typescriptlang.org/) files, your [Less](http://lesscss.org/) files are all up for grabs unless you take action to block access to them. This is, and has always been, bad.\\n\\nThe ASP.Net team know this and things are changing with ASP.Net 5. With the new stack you have to say \\"these are the static files we want people to access\\" in the form of an `<a href=\\"https://msdn.microsoft.com/en-us/library/dn782589(v=vs.113).aspx\\">app.UseStaticFiles()</a>` declaration. This is mighty similar to how you do things in other frameworks such as [Express](http://expressjs.com/en/starter/static-files.html). To quote the [docs](https://docs.asp.net/en/latest/fundamentals/static-files.html#serving-static-files):\\n\\n> By default, static files are stored in the webroot of your project. The location of the webroot is defined in the project\u2019s `project.json` file where the default is wwwroot.\\n>\\n> ```json\\n> \\"webroot\\": \\"wwwroot\\"\\n> ```\\n>\\n> Static files can be stored in any folder under the webroot and accessed with a relative path to that root. For example, when you create a default Web application project using Visual Studio, there are several folders created within the webroot folder - `css`, `images` and `js`. In order to directly access an image in the images subfolder, the URL would look like the following:\\n>\\n> `http://&lt;yourApp&gt;/images/&lt;imageFileName&gt;`\\n\\nSo how do we get this behaviour with ASP.Net vOld? Well, it\'s just a matter of `web.config` URL rewrite twiddling:\\n\\n```xml\\n<configuration>\\n  \x3c!-- other config --\x3e\\n\\n  <system.webServer>\\n    <rewrite>\\n      <rules>\\n        <rule name=\\"Map empty URLs to the index.html\\">\\n          <match url=\\"^$\\" />\\n          <action type=\\"Rewrite\\" url=\\"/index.html\\" />\\n        </rule>\\n        <rule name=\\"Map all requests with a \'.\' in to the \'build\' directory\\" stopProcessing=\\"true\\">\\n          <match url=\\"^(.*[.].*)$\\" />\\n          <action type=\\"Rewrite\\" url=\\"/build/{R:1}\\" />\\n        </rule>\\n      </rules>\\n    </rewrite>\\n  </system.webServer>\\n</configuration>\\n```\\n\\nMy `webroot` is named `build` rather than `wwwroot`. The 2 URL rewrite rules above do the following:\\n\\n<dl><dt>Map empty URLs to the index.html</dt><dd>Empty URLs (ie the URL for the root of your site) are mapped to <code>index.html</code>. The <code>index.html</code> in the <code>build</code> folder is the home page of this particular site and the next rule will make sure that we hit that. (Since we haven\'t set <code>stopProcessing</code> to <code>true</code> for this particular rule the next rule will be processed after this one.)</dd><dt>Map all requests with a \'.\' in to the \'build\' directory</dt><dd>All URLs with a \\".\\" in the title (including <code>index.html</code>) are redirected to the <code>build</code> folder. All static files have a \\".\\" in them because filenames have suffixes. This essentially means all requests for files are served from the <code>build</code> folder. In this case we have set <code>stopProcessing</code> to <code>true</code> which means that any URLs that matched this rule will be not be affected by any subsequent rules.</dd></dl>\\n\\nSo if anyone sneakily tries to sneakily browse to say, `http://&lt;yourApp&gt;/js/app.ts` then they\'ll be nicely redirected to the non-existent `build/js/app.ts`. 404 in your face!\\n\\n## \\"I am SPArtucus\\"\\n\\nWhen you have a Single Page Application you want the same web experience as a server side rendered web app. What I mean by this is: routing just works. You want people to be able to go to `https://i-am-your-domain.com/i-am-your-route` and get your site at the specified route. Happily, whether you\'re using React Router, Angular UI Router or something else, the client side is covered. They can be configured to detect the route that you enter at and serve up the SPA in the relevant state. But you have to meet them halfway; the server needs to do its bit.\\n\\nWhen a URL is requested that doesn\'t look like a request for a static file, let\'s make the (reasonable) assumption that this is a route URL and serve up the shell SPA page. So, for my own example of an Angular 1.x app I want the server to hand over `/build/index.html`.\\n\\nThis is the magic that makes real URLs and SPAs work. Provided the client hasn\'t requested a static file, every request to the server will be responded to with our very own \\"I am SPArtucus\\"; the shell SPA page. This is catered for by the addition of another new rule to our `web.config`:\\n\\n```xml\\n<configuration>\\n  \x3c!-- other config --\x3e\\n\\n  <system.webServer>\\n    <rewrite>\\n      <rules>\\n        <rule name=\\"Map empty URLs to the index.html\\">\\n          <match url=\\"^$\\" />\\n          <action type=\\"Rewrite\\" url=\\"/index.html\\" />\\n        </rule>\\n        <rule name=\\"Map all requests with a \'.\' in to the \'build\' directory\\" stopProcessing=\\"true\\">\\n          <match url=\\"^(.*[.].*)$\\" />\\n          <action type=\\"Rewrite\\" url=\\"/build/{R:1}\\" />\\n        </rule>\\n        <rule name=\\"Map all other URLs to the index.html - this to support our SPA routes\\">\\n          <match url=\\"^.*$\\" />\\n          <action type=\\"Rewrite\\" url=\\"/build/index.html\\" />\\n        </rule>\\n      </rules>\\n    </rewrite>\\n  </system.webServer>\\n</configuration>\\n```\\n\\n<dl><dt>Map all other URLs to the index.html - this to support our SPA routes</dt><dd>Our new rule says \\"whatever URL turns up, if it hasn\'t been catered for by an existing rule, well it must be a SPA route, so we\'ll serve up the shell SPA page of <code>build/index.html</code>\\".</dd></dl>\\n\\n## Data! Data! Data!.. I can\'t make bricks without clay.\\n\\nSherlock Holmes was onto something; most applications are nothing without data. What you\'ve got at present is an application that carefully restricts access to static files and, for all other requests, serves up our shell SPA page. So let\'s relax our final rule a little to make data access a thing:\\n\\n```xml\\n<configuration>\\n  \x3c!-- other config --\x3e\\n\\n  <system.webServer>\\n    <rewrite>\\n      <rules>\\n        <rule name=\\"Map empty URLs to the index.html\\">\\n          <match url=\\"^$\\" />\\n          <action type=\\"Rewrite\\" url=\\"/index.html\\" />\\n        </rule>\\n        <rule name=\\"Map all requests with a \'.\' in to the \'build\' directory\\" stopProcessing=\\"true\\">\\n          <match url=\\"^(.*[.].*)$\\" />\\n          <action type=\\"Rewrite\\" url=\\"/build/{R:1}\\" />\\n        </rule>\\n        <rule name=\\"Map non-api URLs to the index.html - this to support our SPA routes\\">\\n          <match url=\\"^(api/).*$\\" negate=\\"true\\" ignoreCase=\\"true\\" />\\n          <action type=\\"Rewrite\\" url=\\"/build/index.html\\" />\\n        </rule>\\n      </rules>\\n    </rewrite>\\n  </system.webServer>\\n</configuration>\\n```\\n\\n<dl><dt>Map non-api URLs to the index.html - this to support our SPA routes</dt><dd>This amended rule says \\"whatever URL turns up, <em>unless it begins <code>\\"api/\\"</code></em>, if it hasn\'t been catered for by an existing rule, well it must be a SPA route, so we\'ll serve up the shell SPA page of <code>build/index.html</code>\\".</dd></dl>\\n\\nThis allows us to perform data access by prefixing all the Web API routes with `\\"api/\\"`. I\'ve picked this because it is the default location for ASP.Net Web API routes. It is (like most things) entirely configurable. To see a working implementation of this complete approach then take a look at the PoorClaresAngular project [here](https://github.com/johnnyreilly/poorclaresarundel/tree/15e7d4ddc0f1c06fe326b44c3bdc71ceb554bf73)."},{"id":"/2015/12/20/live-reload-considered-harmful","metadata":{"permalink":"/2015/12/20/live-reload-considered-harmful","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2015-12-20-live-reload-considered-harmful/index.md","source":"@site/blog/2015-12-20-live-reload-considered-harmful/index.md","title":"Live Reload Considered Harmful","description":"I\'ve seen it go by many names; live reload, hot reload, browser sync... the list goes on. It\'s been the subject of a million demos. It\'s the focus of a thousand npm packages. Someone tweaks a file and... wait for it... doesn\'t have to refresh their browser to see the changes... The future is now!","date":"2015-12-20T00:00:00.000Z","formattedDate":"December 20, 2015","tags":[],"readingTime":2.455,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Live Reload Considered Harmful","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false},"prevItem":{"title":"UseStaticFiles for ASP.Net Framework","permalink":"/2016/01/01/usestaticfiles-for-aspnet-vold"},"nextItem":{"title":"ES6 + TypeScript + Babel + React + Flux + Karma: The Secret Recipe","permalink":"/2015/12/16/es6-typescript-babel-react-flux-karma"}},"content":"I\'ve seen it go by many names; [live reload](http://livereload.com/), hot reload, [browser sync](https://browsersync.io/)... the list goes on. It\'s been the subject of a million demos. It\'s the focus of a thousand npm packages. Someone tweaks a file and... wait for it... _doesn\'t have to refresh their browser to see the changes_... The future is now!\\n\\nForgive me the sarcasm, but I have come to the conclusion that whilst live reload is impressive... for my own purposes, it is not actually that useful. It certainly shouldn\'t be the default goto that it seems to have become.\\n\\nHear me out people, I may be the voice crying out in the wilderness but I\'m right dammit.\\n\\n![](tumblr_mxjpcobvcg...6_r2_250-4abb938.gif)\\n\\n## Why is Live Reload a Thing?\\n\\nWhat is live reload? Well having to hit F5 after you\'ve made a change... That seems like such hard work right? To quote [Phil Haack](http://haacked.com/archive/2011/12/13/better-git-with-powershell.aspx/):\\n\\n> ... we\u2019re software developers.... It\u2019s time to AWW TOE MATE!\\n\\nYup, automation. Anything that a developer can theoretically automate.... will be automated. Usually this is a good thing but automation can be addictive. And on this occasion it\'s time for an intervention.\\n\\nWhat else could be the attraction? Well, this is speculation but I would say that the implementation actually has something to do with it. Live reload is almost invariably powered by [WebSockets](https://en.wikipedia.org/wiki/WebSocket) and they are certainly cool. Developers I know what you are like. You\'re attracted by the new shiny thing. You can\'t resist the allure of WS. And there with live reload idling away in the background you\'re all bleeding edge. I can say all this because this is exactly what I am like.\\n\\n## Why is Live Reload a BAD Thing?\\n\\nWell the OCD part of me is instinctively repelled by the extra `script` tag of alien code that live reload foists upon your app. How very dare that `&lt;script src=\\"http://localhost:35729/livereload.js?snipver=1\\"&gt;&lt;/script&gt;` push its way into my pristine DOM. It\'s an outrage.\\n\\nPerhaps a more convincing rationale is how useful it is to have 2 different versions of your app up on screen at the same time. I like to try things out when I\'m working. I get a screen working one way and then I tweak and play with my implementation. I have the app of 10 minutes ago sat side by side with the newly adjusted one. Assess, compare and and declare a winner. That\'s so useful and live reload does away with it. That\'s a problem.\\n\\nFinally, I\'m an obsessive \'Ctrl-S\'-er. I\'ve been burned by unsaved changes too many times. I\'m saving every couple of keypresses. With live reload that usually means I have the noise of a dead application in the corner of my eye as LR obsessively forces the latest brokenness upon me. That sucks.\\n\\nI\'ve no doubt there are situations where live reload is useful. But for my money that\'s the exception rather than the rule. Let the madness end now. Just say \\"no\\", kids."},{"id":"/2015/12/16/es6-typescript-babel-react-flux-karma","metadata":{"permalink":"/2015/12/16/es6-typescript-babel-react-flux-karma","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2015-12-16-es6-typescript-babel-react-flux-karma/index.md","source":"@site/blog/2015-12-16-es6-typescript-babel-react-flux-karma/index.md","title":"ES6 + TypeScript + Babel + React + Flux + Karma: The Secret Recipe","description":"I wrote a while ago about how I was using some different tools in a current project:","date":"2015-12-16T00:00:00.000Z","formattedDate":"December 16, 2015","tags":[{"label":"ES6","permalink":"/tags/es-6"},{"label":"Karma","permalink":"/tags/karma"},{"label":"React","permalink":"/tags/react"},{"label":"ts-loader","permalink":"/tags/ts-loader"},{"label":"Webpack","permalink":"/tags/webpack"}],"readingTime":13.645,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"ES6 + TypeScript + Babel + React + Flux + Karma: The Secret Recipe","authors":"johnnyreilly","tags":["ES6","Karma","React","ts-loader","Webpack"],"hide_table_of_contents":false},"prevItem":{"title":"Live Reload Considered Harmful","permalink":"/2015/12/20/live-reload-considered-harmful"},"nextItem":{"title":"IQueryable... IEnumerable... Hmmm...","permalink":"/2015/11/30/iqueryable-ienumerable-hmmm"}},"content":"I wrote [a while ago](https://blog.johnnyreilly.com/2015/09/things-done-changed.html) about how I was using some different tools in a current project:\\n\\n- React with JSX\\n- Flux\\n- ES6 with Babel\\n- Karma for unit testing\\n\\nI have fully come to love and appreciate all of the above. I really like working with them. However. There was still an ache in my soul and a thorn in my side. Whilst I love the syntax of ES6 and even though I\'ve come to appreciate the clarity of JSX, I have been missing something. Perhaps you can guess? It\'s static typing.\\n\\nIt\'s actually been really good to have the chance to work without it because it\'s made me realise what a productivity boost having static typing actually is. The number of silly mistakes burning time that a compiler could have told me.... Sigh.\\n\\nBut the pain is over. The dark days are gone. It\'s possible to have strong typing, courtesy of TypeScript, plugged into this workflow. It\'s yours for the taking. Take it. Take it now!\\n\\n## What a Guy Wants\\n\\nI decided a couple of months ago what I wanted to have in my setup:\\n\\n1. I want to be able to write React / JSX in TypeScript. Naturally I couldn\'t achieve that by myself but handily the TypeScript team decided to add support for JSX with [TypeScript 1.6](https://blogs.msdn.com/b/typescript/archive/2015/09/16/announcing-typescript-1-6.aspx). Ooh yeah.\\n2. I wanted to be able to write ES6. When I realised [the approach for writing ES6 and having the transpilation handled by TypeScript wasn\'t clear](https://github.com/Microsoft/TypeScript/issues/3956) I had another idea. I thought [\\"what if I write ES6 and hand off the transpilation to Babel?\\"](https://github.com/Microsoft/TypeScript/issues/4765) i.e. Use TypeScript for type checking, not for transpilation. I realised that [James Brantly had my back](http://www.jbrantly.com/es6-modules-with-typescript-and-webpack/#configuringwebpack) here already. Enter [Webpack](https://webpack.github.io/) and [ts-loader](https://github.com/TypeStrong/ts-loader).\\n3. Debugging. Being able to debug my code is non-negotiable for me. If I can\'t debug it I\'m less productive. (I\'m also bitter and twisted inside.) I should say that I wanted to be able to debug my _original_ source code. Thanks to the magic of [sourcemaps](https://docs.google.com/document/d/1U1RGAehQwRypUTovF1KRlpiOFze0b-_2gc6fAH0KY0k/edit?usp=sharing), that mad thing is possible.\\n4. Karma for unit testing. I\'ve become accustomed to writing my tests in ES6 and running them on a continual basis with [Karma](https://karma-runner.github.io/0.13/index.html). This allows for a rather good debugging story as well. I didn\'t want to lose this when I moved to TypeScript. I didn\'t.\\n\\nSo I\'ve talked about what I want and I\'ve alluded to some of the solutions that there are. The question now is how to bring them all together. This post is, for the most part, going to be about correctly orchestrating a number of [gulp tasks](http://gulpjs.com/) to achieve the goals listed above. If you\'re after the [Blue Peter \\"here\'s one I made earlier\\"](https://en.wikipedia.org/wiki/Blue_Peter) moment then take a look at [the es6-babel-react-flux-karma repo](https://github.com/Microsoft/TypeScriptSamples/tree/master/es6-babel-react-flux-karma) in the [Microsoft/TypeScriptSamples repo on Github](https://github.com/Microsoft/TypeScriptSamples).\\n\\n## gulpfile.js\\n\\n```js\\n/* eslint-disable no-var, strict, prefer-arrow-callback */\\n\'use strict\';\\n\\nvar gulp = require(\'gulp\');\\nvar gutil = require(\'gulp-util\');\\nvar connect = require(\'gulp-connect\');\\nvar eslint = require(\'gulp-eslint\');\\nvar webpack = require(\'./gulp/webpack\');\\nvar staticFiles = require(\'./gulp/staticFiles\');\\nvar tests = require(\'./gulp/tests\');\\nvar clean = require(\'./gulp/clean\');\\nvar inject = require(\'./gulp/inject\');\\n\\nvar lintSrcs = [\'./gulp/**/*.js\'];\\n\\ngulp.task(\'delete-dist\', function (done) {\\n  clean.run(done);\\n});\\n\\ngulp.task(\'build-process.env.NODE_ENV\', function () {\\n  process.env.NODE_ENV = \'production\';\\n});\\n\\ngulp.task(\\n  \'build-js\',\\n  [\'delete-dist\', \'build-process.env.NODE_ENV\'],\\n  function (done) {\\n    webpack.build().then(function () {\\n      done();\\n    });\\n  }\\n);\\n\\ngulp.task(\\n  \'build-other\',\\n  [\'delete-dist\', \'build-process.env.NODE_ENV\'],\\n  function () {\\n    staticFiles.build();\\n  }\\n);\\n\\ngulp.task(\'build\', [\'build-js\', \'build-other\', \'lint\'], function () {\\n  inject.build();\\n});\\n\\ngulp.task(\'lint\', function () {\\n  return gulp.src(lintSrcs).pipe(eslint()).pipe(eslint.format());\\n});\\n\\ngulp.task(\'watch\', [\'delete-dist\'], function () {\\n  process.env.NODE_ENV = \'development\';\\n  Promise.all([\\n    webpack.watch(), //,\\n    //less.watch()\\n  ])\\n    .then(function () {\\n      gutil.log(\\n        \'Now that initial assets (js and css) are generated inject will start...\'\\n      );\\n      inject.watch(postInjectCb);\\n    })\\n    .catch(function (error) {\\n      gutil.log(\'Problem generating initial assets (js and css)\', error);\\n    });\\n\\n  gulp.watch(lintSrcs, [\'lint\']);\\n  staticFiles.watch();\\n  tests.watch();\\n});\\n\\ngulp.task(\'watch-and-serve\', [\'watch\'], function () {\\n  postInjectCb = stopAndStartServer;\\n});\\n\\nvar postInjectCb = null;\\nvar serverStarted = false;\\nfunction stopAndStartServer() {\\n  if (serverStarted) {\\n    gutil.log(\'Stopping server\');\\n    connect.serverClose();\\n    serverStarted = false;\\n  }\\n  startServer();\\n}\\n\\nfunction startServer() {\\n  gutil.log(\'Starting server\');\\n  connect.server({\\n    root: \'./dist\',\\n    port: 8080,\\n  });\\n  serverStarted = true;\\n}\\n```\\n\\nLet\'s start picking this apart; what do we actually have here? Well, we have 2 gulp tasks that I want you to notice:\\n\\n<dl><dt>build</dt><dd><p>This is likely the task you would use when deploying. It takes all of your source code, builds it, provides cache-busting filenames (eg <code>main.dd2fa20cd9eac9d1fb2f.js</code>), injects your shell SPA page with references to the files and deploys everything to the <code>./dist/</code> directory. So that\'s TypeScript, static assets like images and CSS all made ready for Production.</p><p>The build task also implements <a href=\\"https://facebook.github.io/react/blog/2015/09/10/react-v0.14-rc1.html\\">this advice</a>:</p><blockquote cite=\\"https://facebook.github.io/react/blog/2015/09/10/react-v0.14-rc1.html\\">When deploying your app, set the <code>NODE_ENV</code> environment variable to <code>production</code> to use the production build of React which does not include the development warnings and runs significantly faster. </blockquote></dd><dt>watch-and-serve</dt><dd><p>This task represents \\"development mode\\" or \\"debug mode\\". It\'s what you\'ll likely be running as you develop your app. It does the same as the build task but with some important distinctions.</p><ul><li>As well as building your source it also runs your tests using Karma</li><li>This task is not triggered on a once-only basis, rather your files are watched and each tweak of a file will result in a new build and a fresh run of your tests. Nice eh?</li><li>It spins up a simple web server and serves up the contents of <code>./dist</code> (i.e. your built code) in order that you can easily test out your app.</li><li>In addition, whilst it builds your source it does <em>not</em> minify your code and it emits sourcemaps. For why? For debugging! You can go to <code><a href=\\"http://localhost:8080/\\">http://localhost:8080/</a></code> in your browser of choice, fire up the dev tools and you\'re off to the races; debugging like gangbusters. It also doesn\'t bother to provide cache-busting filenames as Chrome dev tools are smart enough to not cache localhost.</li><li>Oh and Karma.... If you\'ve got problems with a failing test then head to <code><a href=\\"http://localhost:9876/\\">http://localhost:9876/</a></code> and you can debug the tests in your dev tools.</li><li>Finally, it runs ESLint in the console. Not all of my files are TypeScript; essentially the build process (aka \\"gulp-y\\") files are all vanilla JS. So they\'re easily breakable. ESLint is there to provide a little reassurance on that front.</li></ul></dd></dl>\\n\\nNow let\'s dig into each of these in a little more detail\\n\\n## WebPack\\n\\nLet\'s take a look at what\'s happening under the covers of `webpack.build()` and `webpack.watch()`.\\n\\n[WebPack](https://github.com/webpack/webpack) with [ts-loader](https://github.com/TypeStrong/ts-loader) and [babel-loader](https://github.com/babel/babel-loader) is what we\'re using to compile our ES6 TypeScript. ts-loader uses the TypeScript compiler to, um, compile TypeScript and emit ES6 code. This is then passed on to the babel-loader which transpiles it from ES6 down to ES-old-school. It all gets brought together in 2 files; `main.js` which contains the compiled result of the code written by us and `vendor.js` which contains the compiled result of 3rd party / vendor files. The reason for this separation is that vendor files are likely to change fairly rarely whilst our own code will constantly be changing. This separation allows for quicker compile times upon file changes as, for the most part, the vendor files will not need to included in this process.\\n\\nOur `gulpfile.js` above uses the following task:\\n\\n```js\\n\'use strict\';\\n\\nvar gulp = require(\'gulp\');\\nvar gutil = require(\'gulp-util\');\\nvar webpack = require(\'webpack\');\\nvar WebpackNotifierPlugin = require(\'webpack-notifier\');\\nvar webpackConfig = require(\'../webpack.config.js\');\\n\\nfunction buildProduction(done) {\\n  // modify some webpack config options\\n  var myProdConfig = Object.create(webpackConfig);\\n  myProdConfig.output.filename = \'[name].[hash].js\';\\n\\n  myProdConfig.plugins = myProdConfig.plugins.concat(\\n    // make the vendor.js file with cachebusting filename\\n    new webpack.optimize.CommonsChunkPlugin({\\n      name: \'vendor\',\\n      filename: \'vendor.[hash].js\',\\n    }),\\n    new webpack.optimize.DedupePlugin(),\\n    new webpack.optimize.UglifyJsPlugin()\\n  );\\n\\n  // run webpack\\n  webpack(myProdConfig, function (err, stats) {\\n    if (err) {\\n      throw new gutil.PluginError(\'webpack:build\', err);\\n    }\\n    gutil.log(\\n      \'[webpack:build]\',\\n      stats.toString({\\n        colors: true,\\n      })\\n    );\\n\\n    if (done) {\\n      done();\\n    }\\n  });\\n}\\n\\nfunction createDevCompiler() {\\n  // show me some sourcemap love people\\n  var myDevConfig = Object.create(webpackConfig);\\n  myDevConfig.devtool = \'inline-source-map\';\\n  myDevConfig.debug = true;\\n\\n  myDevConfig.plugins = myDevConfig.plugins.concat(\\n    // Make the vendor.js file\\n    new webpack.optimize.CommonsChunkPlugin({\\n      name: \'vendor\',\\n      filename: \'vendor.js\',\\n    }),\\n    new WebpackNotifierPlugin({ title: \'Webpack build\', excludeWarnings: true })\\n  );\\n\\n  // create a single instance of the compiler to allow caching\\n  return webpack(myDevConfig);\\n}\\n\\nfunction buildDevelopment(done, devCompiler) {\\n  // run webpack\\n  devCompiler.run(function (err, stats) {\\n    if (err) {\\n      throw new gutil.PluginError(\'webpack:build-dev\', err);\\n    }\\n    gutil.log(\\n      \'[webpack:build-dev]\',\\n      stats.toString({\\n        chunks: false, // dial down the output from webpack (it can be noisy)\\n        colors: true,\\n      })\\n    );\\n\\n    if (done) {\\n      done();\\n    }\\n  });\\n}\\n\\nfunction bundle(options) {\\n  var devCompiler;\\n\\n  function build(done) {\\n    if (options.shouldWatch) {\\n      buildDevelopment(done, devCompiler);\\n    } else {\\n      buildProduction(done);\\n    }\\n  }\\n\\n  if (options.shouldWatch) {\\n    devCompiler = createDevCompiler();\\n\\n    gulp.watch(\'src/**/*\', function () {\\n      build();\\n    });\\n  }\\n\\n  return new Promise(function (resolve, reject) {\\n    build(function (err) {\\n      if (err) {\\n        reject(err);\\n      } else {\\n        resolve(\'webpack built\');\\n      }\\n    });\\n  });\\n}\\n\\nmodule.exports = {\\n  build: function () {\\n    return bundle({ shouldWatch: false });\\n  },\\n  watch: function () {\\n    return bundle({ shouldWatch: true });\\n  },\\n};\\n```\\n\\nHopefully this is fairly self-explanatory; essentially `buildDevelopment` performs the development build (providing sourcemap support) and `buildProduction` builds for Production (providing minification support). Both are driven by this `webpack.config.js`:\\n\\n```js\\n/* eslint-disable no-var, strict, prefer-arrow-callback */\\n\'use strict\';\\n\\nvar path = require(\'path\');\\n\\nmodule.exports = {\\n  cache: true,\\n  entry: {\\n    // The entry point of our application; the script that imports all other scripts in our SPA\\n    main: \'./src/main.tsx\',\\n\\n    // The packages that are to be included in vendor.js\\n    vendor: [\'babel-polyfill\', \'events\', \'flux\', \'react\'],\\n  },\\n\\n  // Where the output of our compilation ends up\\n  output: {\\n    path: path.resolve(__dirname, \'./dist/scripts\'),\\n    filename: \'[name].js\',\\n    chunkFilename: \'[chunkhash].js\',\\n  },\\n\\n  module: {\\n    loaders: [\\n      {\\n        // The loader that handles ts and tsx files.  These are compiled\\n        // with the ts-loader and the output is then passed through to the\\n        // babel-loader.  The babel-loader uses the es2015 and react presets\\n        // in order that jsx and es6 are processed.\\n        test: /\\\\.ts(x?)$/,\\n        exclude: /node_modules/,\\n        loader: \'babel-loader?presets[]=es2015&presets[]=react!ts-loader\',\\n      },\\n      {\\n        // The loader that handles any js files presented alone.\\n        // It passes these to the babel-loader which (again) uses the es2015\\n        // and react presets.\\n        test: /\\\\.js$/,\\n        exclude: /node_modules/,\\n        loader: \'babel\',\\n        query: {\\n          presets: [\'es2015\', \'react\'],\\n        },\\n      },\\n    ],\\n  },\\n  plugins: [],\\n  resolve: {\\n    // Files with the following extensions are fair game for webpack to process\\n    extensions: [\'\', \'.webpack.js\', \'.web.js\', \'.ts\', \'.tsx\', \'.js\'],\\n  },\\n};\\n```\\n\\n## Inject\\n\\nYour compiled output needs to be referenced from some kind of HTML page. So we\'ve got this:\\n\\n```html\\n<!DOCTYPE html>\\n<html lang=\\"en\\">\\n  <head>\\n    <meta charset=\\"utf-8\\" />\\n    <meta http-equiv=\\"X-UA-Compatible\\" content=\\"IE=edge\\" />\\n    <meta name=\\"viewport\\" content=\\"width=device-width, initial-scale=1\\" />\\n\\n    <title>ES6 + Babel + React + Flux + Karma: The Secret Recipe</title>\\n\\n    \x3c!-- inject:css --\x3e\\n    \x3c!-- endinject --\x3e\\n    <link\\n      rel=\\"stylesheet\\"\\n      href=\\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css\\"\\n    />\\n  </head>\\n  <body>\\n    <div id=\\"content\\"></div>\\n    \x3c!-- inject:js --\x3e\\n    \x3c!-- endinject --\x3e\\n  </body>\\n</html>\\n```\\n\\nWhich is no more than a boilerplate HTML page with a couple of key features:\\n\\n- a single `&lt;div /&gt;` element in the `&lt;body /&gt;` which is where our React app is going to be rendered.\\n- `&lt;!-- inject:css --&gt;` and `&lt;!-- inject:js --&gt;` placeholders where css and js is going to be injected by `gulp-inject`.\\n- a single `&lt;link /&gt;` to the Bootstrap CDN. This sample app doesn\'t actually serve up any css generated as part of the project. It could but it doesn\'t. When it comes to injection time no css will actually be injected. This has been left in place as, more typically, a project would have some styling served up.\\n\\nThis is fed into our inject task in `inject.build()` and `inject.watch()`. They take css and javascript and, using our shell template, create a new page which has the css and javascript dropped into their respective placeholders:\\n\\n```js\\n\'use strict\';\\n\\nvar gulp = require(\'gulp\');\\nvar inject = require(\'gulp-inject\');\\nvar glob = require(\'glob\');\\n\\nfunction injectIndex(options) {\\n  var postInjectCb = options.postInjectCb;\\n  var postInjectCbTriggerId = null;\\n  function run() {\\n    var target = gulp.src(\'./src/index.html\');\\n    var sources = gulp.src(\\n      [\\n        //\'./dist/styles/main*.css\',\\n        \'./dist/scripts/vendor*.js\',\\n        \'./dist/scripts/main*.js\',\\n      ],\\n      { read: false }\\n    );\\n\\n    return target\\n      .on(\'end\', function () {\\n        // invoke postInjectCb after 1s\\n        if (postInjectCbTriggerId || !postInjectCb) {\\n          return;\\n        }\\n\\n        postInjectCbTriggerId = setTimeout(function () {\\n          postInjectCb();\\n          postInjectCbTriggerId = null;\\n        }, 1000);\\n      })\\n      .pipe(\\n        inject(sources, {\\n          ignorePath: \'/dist/\',\\n          addRootSlash: false,\\n          removeTags: true,\\n        })\\n      )\\n      .pipe(gulp.dest(\'./dist\'));\\n  }\\n\\n  var jsCssGlob = \'dist/**/*.{js,css}\';\\n\\n  function checkForInitialFilesThenRun() {\\n    glob(jsCssGlob, function (er, files) {\\n      var filesWeNeed = [\\n        \'dist/scripts/main\',\\n        \'dist/scripts/vendor\' /*, \'dist/styles/main\'*/,\\n      ];\\n\\n      function fileIsPresent(fileWeNeed) {\\n        return files.some(function (file) {\\n          return file.indexOf(fileWeNeed) !== -1;\\n        });\\n      }\\n\\n      if (filesWeNeed.every(fileIsPresent)) {\\n        run(\'initial build\');\\n      } else {\\n        checkForInitialFilesThenRun();\\n      }\\n    });\\n  }\\n\\n  checkForInitialFilesThenRun();\\n\\n  if (options.shouldWatch) {\\n    gulp.watch(jsCssGlob, function (evt) {\\n      if (evt.path && evt.type === \'changed\') {\\n        run(evt.path);\\n      }\\n    });\\n  }\\n}\\n\\nmodule.exports = {\\n  build: function () {\\n    return injectIndex({ shouldWatch: false });\\n  },\\n  watch: function (postInjectCb) {\\n    return injectIndex({ shouldWatch: true, postInjectCb: postInjectCb });\\n  },\\n};\\n```\\n\\nThis also triggers the server to serve up the new content.\\n\\n## Static Files\\n\\nYour app will likely rely on a number of static assets; images, fonts and whatnot. This script picks up the static assets you\'ve defined and places them in the `dist` folder ready for use:\\n\\n```js\\n\'use strict\';\\n\\nvar gulp = require(\'gulp\');\\nvar cache = require(\'gulp-cached\');\\n\\nvar targets = [\\n  // In my own example I don\'t use any of the targets below, they\\n  // are included to give you more of a feel of how you might use this\\n  { description: \'FONTS\', src: \'./fonts/*\', dest: \'./dist/fonts\' },\\n  { description: \'STYLES\', src: \'./styles/*\', dest: \'./dist/styles\' },\\n  { description: \'FAVICON\', src: \'./favicon.ico\', dest: \'./dist\' },\\n  { description: \'IMAGES\', src: \'./images/*\', dest: \'./dist/images\' },\\n];\\n\\nfunction copy(options) {\\n  // Copy files from their source to their destination\\n  function run(target) {\\n    gulp\\n      .src(target.src)\\n      .pipe(cache(target.description))\\n      .pipe(gulp.dest(target.dest));\\n  }\\n\\n  function watch(target) {\\n    gulp.watch(target.src, function () {\\n      run(target);\\n    });\\n  }\\n\\n  targets.forEach(run);\\n\\n  if (options.shouldWatch) {\\n    targets.forEach(watch);\\n  }\\n}\\n\\nmodule.exports = {\\n  build: function () {\\n    return copy({ shouldWatch: false });\\n  },\\n  watch: function () {\\n    return copy({ shouldWatch: true });\\n  },\\n};\\n```\\n\\n## Karma\\n\\nFinally, we\'re ready to get our tests set up to run continually with Karma. `tests.watch()` triggers the following task:\\n\\n```js\\n\'use strict\';\\n\\nvar Server = require(\'karma\').Server;\\nvar path = require(\'path\');\\nvar gutil = require(\'gulp-util\');\\n\\nmodule.exports = {\\n  watch: function () {\\n    // Documentation: https://karma-runner.github.io/0.13/dev/public-api.html\\n    var karmaConfig = {\\n      configFile: path.join(__dirname, \'../karma.conf.js\'),\\n      singleRun: false,\\n\\n      plugins: [\\n        \'karma-webpack\',\\n        \'karma-jasmine\',\\n        \'karma-mocha-reporter\',\\n        \'karma-sourcemap-loader\',\\n        \'karma-phantomjs-launcher\',\\n        \'karma-phantomjs-shim\',\\n      ], // karma-phantomjs-shim only in place until PhantomJS hits 2.0 and has function.bind\\n      reporters: [\'mocha\'],\\n    };\\n\\n    new Server(karmaConfig, karmaCompleted).start();\\n\\n    function karmaCompleted(exitCode) {\\n      gutil.log(\'Karma has exited with:\', exitCode);\\n      process.exit(exitCode);\\n    }\\n  },\\n};\\n```\\n\\nWhen running in watch mode it\'s possible to debug the tests by going to: `<a href=\\"http://localhost:9876/\\">http://localhost:9876/</a>`. It\'s also possible to run the tests standalone with a simple `npm run test`. Running them like this also outputs the results to an [XML file in JUnit format](http://stackoverflow.com/q/442556/761388); this can be useful for integrating into CI solutions that don\'t natively pick up test results.\\n\\nWhichever approach we use for running tests, we use the following `karma.conf.js` file to configure Karma:\\n\\n```js\\n/* eslint-disable no-var, strict */\\n\'use strict\';\\n\\nvar webpackConfig = require(\'./webpack.config.js\');\\n\\nmodule.exports = function (config) {\\n  // Documentation: https://karma-runner.github.io/0.13/config/configuration-file.html\\n  config.set({\\n    browsers: [\'PhantomJS\'],\\n\\n    files: [\\n      \'test/import-babel-polyfill.js\', // This ensures we have the es6 shims in place from babel\\n      \'test/**/*.tests.ts\',\\n      \'test/**/*.tests.tsx\',\\n    ],\\n\\n    port: 9876,\\n\\n    frameworks: [\'jasmine\', \'phantomjs-shim\'],\\n\\n    logLevel: config.LOG_INFO, //config.LOG_DEBUG\\n\\n    preprocessors: {\\n      \'test/import-babel-polyfill.js\': [\'webpack\', \'sourcemap\'],\\n      \'src/**/*.{ts,tsx}\': [\'webpack\', \'sourcemap\'],\\n      \'test/**/*.tests.{ts,tsx}\': [\'webpack\', \'sourcemap\'],\\n    },\\n\\n    webpack: {\\n      devtool: \'eval-source-map\', //\'inline-source-map\', - inline-source-map doesn\'t work at present\\n      debug: true,\\n      module: webpackConfig.module,\\n      resolve: webpackConfig.resolve,\\n    },\\n\\n    webpackMiddleware: {\\n      quiet: true,\\n      stats: {\\n        colors: true,\\n      },\\n    },\\n\\n    // reporter options\\n    mochaReporter: {\\n      colors: {\\n        success: \'bgGreen\',\\n        info: \'cyan\',\\n        warning: \'bgBlue\',\\n        error: \'bgRed\',\\n      },\\n    },\\n\\n    junitReporter: {\\n      outputDir: \'test-results\', // results will be saved as $outputDir/$browserName.xml\\n      outputFile: undefined, // if included, results will be saved as $outputDir/$browserName/$outputFile\\n      suite: \'\',\\n    },\\n  });\\n};\\n```\\n\\nAs you can see, we\'re still using our webpack configuration from earlier to configure much of how the transpilation takes place.\\n\\nAnd that\'s it; we have a workflow for developing in TypeScript using React with tests running in an automated fashion. I appreciated this has been a rather long blog post but I hope I\'ve clarified somewhat how this all plugs together and works. Do leave a comment if you think I\'ve missed something.\\n\\n## Babel 5 -> Babel 6\\n\\nThis post has actually been sat waiting to be published for some time. I\'d got this solution up and running with Babel 5. Then they shipped Babel 6 and (as is the way with \\"breaking changes\\") [broke sourcemap support](https://phabricator.babeljs.io/T2864) and thus torpedoed this workflow. Happily that\'s now [been resolved](https://github.com/babel/babel/pull/3108). But if you should experience any wonkiness - it\'s worth checking that you\'re using the latest and greatest of Babel 6."},{"id":"/2015/11/30/iqueryable-ienumerable-hmmm","metadata":{"permalink":"/2015/11/30/iqueryable-ienumerable-hmmm","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2015-11-30-iqueryable-ienumerable-hmmm/index.md","source":"@site/blog/2015-11-30-iqueryable-ienumerable-hmmm/index.md","title":"IQueryable... IEnumerable... Hmmm...","description":"So there I was, tip-tapping away at my keyboard when I became aware of the slowly loudening noise of a debate. It wasn\'t about poverty, war, civil rights or anything like that. No; this was far more contentious. It was about the behaviour of IQueryable&lt;T&gt; when mixed with IEnumerable&lt;T&gt;. I know, right, how could I not get involved?","date":"2015-11-30T00:00:00.000Z","formattedDate":"November 30, 2015","tags":[{"label":"LINQ","permalink":"/tags/linq"},{"label":"IEnumerable vs IQueryable","permalink":"/tags/i-enumerable-vs-i-queryable"}],"readingTime":4.36,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"IQueryable... IEnumerable... Hmmm...","authors":"johnnyreilly","tags":["LINQ","IEnumerable vs IQueryable"],"hide_table_of_contents":false},"prevItem":{"title":"ES6 + TypeScript + Babel + React + Flux + Karma: The Secret Recipe","permalink":"/2015/12/16/es6-typescript-babel-react-flux-karma"},"nextItem":{"title":"The Names Have Been Changed...","permalink":"/2015/10/23/the-names-have-been-changed"}},"content":"So there I was, tip-tapping away at my keyboard when I became aware of the slowly loudening noise of a debate. It wasn\'t about poverty, war, civil rights or anything like that. No; this was far more contentious. It was about the behaviour of `<a href=\\"https://msdn.microsoft.com/en-gb/library/bb351562(v=vs.100).aspx\\">IQueryable&lt;T&gt;</a>` when mixed with `<a href=\\"https://msdn.microsoft.com/en-gb/library/9eekhta0(v=vs.100).aspx\\">IEnumerable&lt;T&gt;</a>`. I know, right, how could I not get involved?\\n\\nThe code that was being debated was a database query that was being facilitated by Entity Framework. Now let me ask you a question: what is the problem with the methods below?\\n\\n```cs\\nprivate IEnumerable<Sage> GetSagesWithSayings()\\n{\\n    IQueryable<Sage> sageWithSayings =\\n        from s in DbContext.Sages.Include(x => x.Sayings)\\n        select s;\\n\\n    return sageWithSayings;\\n}\\n\\npublic IEnumerable<Sage> GetSagesWithSayingsBornWithinTheLast100Years()\\n{\\n    var aHundredYearsAgo = DateTime.Now.AddYears(-100);\\n    var sageWithSayings = GetSagesWithSayings().Where(x => x.DateOfBirth > aHundredYearsAgo);\\n\\n    return sageWithSayings;\\n}\\n```\\n\\nI\'ve rather emphasised the problem by expressly declaring types in the `GetSagesWithSayings` method. More typically the `IQueryable&lt;Sage&gt;` would be hiding itself beneath a `var` making the problem less obvious. But you get the point; it\'s something to do with an `IQueryable&lt;Sage&gt;` being passed back as an `IEnumerable&lt;Sage&gt;`.\\n\\nThe debate was raging around what this piece of code (or one much like it) actually did. One side positing \\"it\'ll get every record from the database and then throw away what it doesn\'t need in C#-land...\\" The opposing view being \\"are you sure about that? Doesn\'t it just get the records from the last hundred years from the database?\\"\\n\\nSo it comes down the SQL that ends up being generated. On the one hand it\'s going to get everything from the Sages table...\\n\\n```sql\\nselect ...\\nfrom Sages ...\\n```\\n\\nOr does it include a filter clause as well?\\n\\n```sql\\nselect ...\\nfrom Sages ...\\nwhere DateOfBirth > \'1915-11-30\'\\n```\\n\\nYou probably know the answer... It gets everything. Every record is brought back from the database and those that are older than 100 years are then casually thrown away. So kinda wasteful. That\'s the problem. But why? And what does that tell us?\\n\\n## LINQ to Objects vs LINQ to ... ?\\n\\n> The term \\"LINQ to Objects\\" refers to the use of LINQ queries with any `IEnumerable` or `IEnumerable&lt;T&gt;` collection directly, without the use of an intermediate LINQ provider or API such as LINQ to SQL or LINQ to XML.\\n\\n> The `<a href=\\"https://msdn.microsoft.com/en-gb/library/bb351562(v=vs.100).aspx\\">IQueryable&lt;T&gt;</a>` interface is intended for implementation by query providers.\\n>\\n> This interface inherits the `<a href=\\"https://msdn.microsoft.com/en-gb/library/9eekhta0(v=vs.100).aspx\\">IEnumerable&lt;T&gt;</a>` interface so that if it represents a query, the results of that query can be enumerated. Enumeration forces the expression tree associated with an `<a href=\\"https://msdn.microsoft.com/en-gb/library/bb351562(v=vs.100).aspx\\">IQueryable&lt;T&gt;</a>` object to be executed. Queries that do not return enumerable results are executed when the `<a href=\\"https://msdn.microsoft.com/en-gb/library/bb549414(v=vs.100).aspx\\">Execute&lt;TResult&gt;(Expression)</a>` method is called.\\n>\\n> The definition of \\"executing an expression tree\\" is specific to a query provider. For example, it may involve translating the expression tree to a query language appropriate for an underlying data source.\\n\\nI know - check me out with my \\"quotes\\".\\n\\nNow, `IEnumerable` and `IQueryable` are similar; for instance they are both considered \\"lazy\\" as they offer deferred execution. But there is an important difference between `IEnumerable` and `IQueryable`; namely that `IQueryable` hands off information about a query to another provider in order that they may decide how to do the necessary work. `IEnumerable` does not; its work is done in memory by operating on the data it has.\\n\\nSo let\'s apply this to our issue. We have an `IQueryable&lt;Sage&gt;` and we return it as an `IEnumerable&lt;Sage&gt;`. By doing this we haven\'t changed the underlying type; it\'s still an `IQueryable&lt;Sage&gt;`. But by upcasting to `IEnumerable&lt;Sage&gt;` we have told the compiler that we don\'t have an `IQueryable&lt;Sage&gt;`. We\'ve lied. I trust you\'re feeling guilty.\\n\\nNo doubt whoever raised you told you not to tell lies. This was probably the very situation they had in mind. The implications of our dirty little fib come back to haunt us when we start to chain on subsequent filters. So when we perform our filter of `.Where(x =&gt; x.DateOfBirth &gt; aHundredYearsAgo)` the compiler isn\'t going to get LINQ to Entities\'s extension methods in on this. No, it\'s going to get the LINQ to object extension methods instead.\\n\\nThis is the cause of our problem. When it comes to execution we\'re not getting the database to do the heavy lifting because we\'ve moved away from using `IQueryable`.\\n\\n## Fixing the Problem\\n\\nThere are 2 courses of action open to you. The obvious course of action (and 99% of the time what you\'d look to do) is change the signature of the `` method to return an IQueryable like so:\\n\\n```cs\\nprivate IQueryable<Sage> GetSagesWithSayings()\\n    var sageWithSayings = // I prefer \'var\', don\'t you?\\n        from s in DbContext.Sages.Include(x => x.Sayings)\\n        select s;\\n\\n    return sageWithSayings;\\n}\\n```\\n\\nThe other alternative is what I like to think of as \\"the escape hatch\\": `<a href=\\"https://msdn.microsoft.com/en-gb/library/bb353734(v=vs.100).aspx\\">AsQueryable</a>`. This takes an `IEnumerable`, checks if it\'s actually an `IQueryable` slumming it and casts back to that if it is. You might use this in a situation where you didn\'t have control over the data access code. Using it looks like this: (and would work whether `GetSagesWithSayings` was returning `IEnumerable`_or_`IQueryable`)\\n\\n```cs\\npublic IEnumerable<Sage> GetSagesWithSayingsBornWithinTheLast100Years()\\n{\\n    var aHundredYearsAgo = DateTime.Now.AddYears(-100);\\n    var sageWithSayings =GetSagesWithSayings().AsQueryable().Where(x => x.DateOfBirth > aHundredYearsAgo);\\n\\n    return sageWithSayings;\\n}\\n```"},{"id":"/2015/10/23/the-names-have-been-changed","metadata":{"permalink":"/2015/10/23/the-names-have-been-changed","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2015-10-23-the-names-have-been-changed/index.md","source":"@site/blog/2015-10-23-the-names-have-been-changed/index.md","title":"The Names Have Been Changed...","description":"...to protect my wallet.","date":"2015-10-23T00:00:00.000Z","formattedDate":"October 23, 2015","tags":[],"readingTime":0.745,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"The Names Have Been Changed...","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false},"prevItem":{"title":"IQueryable... IEnumerable... Hmmm...","permalink":"/2015/11/30/iqueryable-ienumerable-hmmm"},"nextItem":{"title":"jQuery Validation Globalize hits 1.0","permalink":"/2015/10/05/jquery-validation-globalize-hits-10"}},"content":"...to protect my wallet.\\n\\nSubsequent to this blog getting [a proper domain name a year ago](http://blog.johnnyreilly.com/2014/12/whats-in-a-name.html) it\'s now got a new one. That\'s right, `blog.icanmakethiswork.io` is dead! Long live `blog.johnnyreilly.com`!\\n\\nThere\'s nothing particularly exciting about this, it\'s more that `.io` domain names are _wayyyyy_ expensive. And also I noticed that johnnyreilly.com was available. By an accident of history I\'ve ended up either being johnny_reilly or johnnyreilly online. (\\"johnreilly@hotmail.com\\" was already taken back in 2000 and \\"johnny\\\\_reilly@hotmail.com\\" was available. I\'ve subsequently become [@johnny_reilly](https://twitter.com/johnny_reilly) on Twitter, [johnnyreilly](https://github.com/johnnyreilly) on GitHub so I guess you could say it\'s stuck.)\\n\\nSo I thought I\'d kill 2 birds with one stone and make the switch. I\'ve set up a redirect on [blog.icanmakethiswork.io](http://blog.icanmakethiswork.io) and so, anyone who goes to the old site should be 301\'d over here. At least until my old domain name expires. Last time it\'ll change I promise. Well.... until next time anyway..."},{"id":"/2015/10/05/jquery-validation-globalize-hits-10","metadata":{"permalink":"/2015/10/05/jquery-validation-globalize-hits-10","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2015-10-05-jquery-validation-globalize-hits-10/index.md","source":"@site/blog/2015-10-05-jquery-validation-globalize-hits-10/index.md","title":"jQuery Validation Globalize hits 1.0","description":"This is just a quick post - the tl;dr is this: jQuery Validation Globalize has been ported to Globalize 1.x. Yay! In one of those twists of fate I\'m not actually using this plugin in my day job anymore but I thought it might be useful to other people. So here you go. You can read more about this plugin in an older post and you can see a demo of it in action here.","date":"2015-10-05T00:00:00.000Z","formattedDate":"October 5, 2015","tags":[{"label":"Globalize","permalink":"/tags/globalize"},{"label":"jQuery Validation","permalink":"/tags/j-query-validation"}],"readingTime":2.865,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"jQuery Validation Globalize hits 1.0","authors":"johnnyreilly","tags":["Globalize","jQuery Validation"],"hide_table_of_contents":false},"prevItem":{"title":"The Names Have Been Changed...","permalink":"/2015/10/23/the-names-have-been-changed"},"nextItem":{"title":"Definitely Typed Shouldn\'t Exist","permalink":"/2015/09/23/authoring-npm-modules-with-typescript"}},"content":"This is just a quick post - the tl;dr is this: jQuery Validation Globalize has been ported to Globalize 1.x. Yay! In one of those twists of fate I\'m not actually using this plugin in my day job anymore but I thought it might be useful to other people. So here you go. You can read more about this plugin in an [older post](https://blog.johnnyreilly.com/2012/09/globalize-and-jquery-validate.html) and you can see a demo of it in action [here](http://johnnyreilly.github.io/jQuery.Validation.Unobtrusive.Native/AdvancedDemo/Globalize.html).\\n\\nThe code did not change drastically - essentially it was just a question of swapping `parseFloat` for `parseNumber` and `parseDate` for a slightly different `parseDate`. So, we went from this:\\n\\n```js\\n(function ($, Globalize) {\\n  // Clone original methods we want to call into\\n  var originalMethods = {\\n    min: $.validator.methods.min,\\n    max: $.validator.methods.max,\\n    range: $.validator.methods.range,\\n  };\\n\\n  // Tell the validator that we want numbers parsed using Globalize\\n\\n  $.validator.methods.number = function (value, element) {\\n    var val = Globalize.parseFloat(value);\\n    return this.optional(element) || $.isNumeric(val);\\n  };\\n\\n  // Tell the validator that we want dates parsed using Globalize\\n\\n  $.validator.methods.date = function (value, element) {\\n    var val = Globalize.parseDate(value);\\n    return this.optional(element) || val instanceof Date;\\n  };\\n\\n  // Tell the validator that we want numbers parsed using Globalize,\\n  // then call into original implementation with parsed value\\n\\n  $.validator.methods.min = function (value, element, param) {\\n    var val = Globalize.parseFloat(value);\\n    return originalMethods.min.call(this, val, element, param);\\n  };\\n\\n  $.validator.methods.max = function (value, element, param) {\\n    var val = Globalize.parseFloat(value);\\n    return originalMethods.max.call(this, val, element, param);\\n  };\\n\\n  $.validator.methods.range = function (value, element, param) {\\n    var val = Globalize.parseFloat(value);\\n    return originalMethods.range.call(this, val, element, param);\\n  };\\n})(jQuery, Globalize);\\n```\\n\\nTo this:\\n\\n```js\\n(function ($, Globalize) {\\n  // Clone original methods we want to call into\\n  var originalMethods = {\\n    min: $.validator.methods.min,\\n    max: $.validator.methods.max,\\n    range: $.validator.methods.range,\\n  };\\n\\n  // Globalize options - initially just the date format used for parsing\\n  // Users can customise this to suit them\\n  $.validator.methods.dateGlobalizeOptions = {\\n    dateParseFormat: { skeleton: \'yMd\' },\\n  };\\n\\n  // Tell the validator that we want numbers parsed using Globalize\\n  $.validator.methods.number = function (value, element) {\\n    var val = Globalize.parseNumber(value);\\n    return this.optional(element) || $.isNumeric(val);\\n  };\\n\\n  // Tell the validator that we want dates parsed using Globalize\\n  $.validator.methods.date = function (value, element) {\\n    var val = Globalize.parseDate(\\n      value,\\n      $.validator.methods.dateGlobalizeOptions.dateParseFormat\\n    );\\n    return this.optional(element) || val instanceof Date;\\n  };\\n\\n  // Tell the validator that we want numbers parsed using Globalize,\\n  // then call into original implementation with parsed value\\n\\n  $.validator.methods.min = function (value, element, param) {\\n    var val = Globalize.parseNumber(value);\\n    return originalMethods.min.call(this, val, element, param);\\n  };\\n\\n  $.validator.methods.max = function (value, element, param) {\\n    var val = Globalize.parseNumber(value);\\n    return originalMethods.max.call(this, val, element, param);\\n  };\\n\\n  $.validator.methods.range = function (value, element, param) {\\n    var val = Globalize.parseNumber(value);\\n    return originalMethods.range.call(this, val, element, param);\\n  };\\n})(jQuery, Globalize);\\n```\\n\\nAll of which is pretty self-explanatory. The only thing I\'d like to draw out is that Globalize 0.1.x didn\'t force you to specify a date parsing format and, as I recall, would attempt various methods of parsing. For that reason jQuery Validation Globalize 1.0 exposes a `$.validator.methods.dateGlobalizeOptions` which allows you to specify the data parsing format you want to use. This means, should you be using a different format than the out of the box one then you can tweak it like so:\\n\\n```js\\n$.validator.methods.dateGlobalizeOptions.dateParseFormat = // your data parsing format goes here...\\n```\\n\\nTheoretically, this functionality could be tweaked to allow the user to specify multiple possible date parsing formats to attempt. I\'m not certain if that\'s a good idea though, so it remains unimplemented for now."},{"id":"/2015/09/23/authoring-npm-modules-with-typescript","metadata":{"permalink":"/2015/09/23/authoring-npm-modules-with-typescript","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2015-09-23-authoring-npm-modules-with-typescript/index.md","source":"@site/blog/2015-09-23-authoring-npm-modules-with-typescript/index.md","title":"Definitely Typed Shouldn\'t Exist","description":"OK - the title\'s total clickbait but stay with me; there\'s a point here.","date":"2015-09-23T00:00:00.000Z","formattedDate":"September 23, 2015","tags":[{"label":"npm","permalink":"/tags/npm"},{"label":"DefinitelyTyped","permalink":"/tags/definitely-typed"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"atom-typescript","permalink":"/tags/atom-typescript"}],"readingTime":10.855,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Definitely Typed Shouldn\'t Exist","authors":"johnnyreilly","tags":["npm","DefinitelyTyped","TypeScript","atom-typescript"],"hide_table_of_contents":false},"prevItem":{"title":"jQuery Validation Globalize hits 1.0","permalink":"/2015/10/05/jquery-validation-globalize-hits-10"},"nextItem":{"title":"Things Done Changed","permalink":"/2015/09/10/things-done-changed"}},"content":"OK - the title\'s total clickbait but stay with me; there\'s a point here.\\n\\nI\'m a member of the Definitely Typed team - and hopefully I won\'t be kicked out for writing this. My point is this: `.d.ts` files should live with the package they provide typing information for, in npm / GitHub etc. Not separately. TypeScript 1.6 has just been released. Yay! In the [release blog post](https://blogs.msdn.com/b/typescript/archive/2015/09/16/announcing-typescript-1-6.aspx) it says this:\\n\\n> We\u2019ve changed module resolution when doing CommonJS output to work more closely to how Node does module resolution. If a module name is non-relative, we now follow these steps to find the associated typings:\\n>\\n> 1. Check in `node_modules` for `&lt;module name&gt;.d.ts`\\n> 2. Search `node_modules\\\\&lt;module name&gt;\\\\package.json` for a `typings` field\\n> 3. Look for `node_modules\\\\&lt;module name&gt;\\\\index.d.ts`\\n> 4. Then we go one level higher and repeat the process\\n>\\n> **Please note:** when we search through node_modules, we assume these are the packaged node modules which have type information and a corresponding `.js` file. As such, we resolve only `.d.ts` files (not `.ts` file) for non-relative names.\\n>\\n> Previously, we treated all module names as relative paths, and therefore we would never properly look in node_modules... We will continue to improve module resolution, including improvements to AMD, in upcoming releases.\\n\\nThe TL;DR is this: consuming npm packages which come with definition files should JUST WORK\u2122... npm is now a first class citizen in TypeScriptLand. So everyone who has a package on npm should now feel duty bound to include a `.d.ts` when they publish and Definitely Typed can shut up shop. Simple right?\\n\\n## Wrong!\\n\\nYeah, it\'s never going to happen. Surprising as it is, there are many people who are quite happy without TypeScript in their lives (I know - mad right?). These poor unfortunates are unlikely to ever take the extra steps necessary to write definition files. For this reason, there will probably _always_ be a need for a provider of typings such as Definitely Typed. As well as that, the vast majority of people using TypeScript probably don\'t use npm to manage dependencies. There are, however, an increasing number of users who are using npm. Some (like me) may even be using tools like [Browserify](http://browserify.org/) (with the [TSIFY plugin](https://github.com/smrq/tsify)) or [WebPack](https://webpack.github.io/) (with the [TS loader](https://github.com/jbrantly/ts-loader)) to bring it all together. My feeling is that, over time, using npm will become more common; particularly given the improvements being made to module resolution in the language.\\n\\nAn advantage of shipping typings with an npm package is this: those typings should accurately describe their accompanying package. In Definitely Typed we only aim to support the latest and greatest typings. So if you find yourself looking for the typings of an older version of a package you\'re going to have to pick your way back through the history of a `.d.ts` file and hope you happen upon the version you\'re looking for. Not a fantastic experience.\\n\\nSo I guess what I\'m saying is this: if you\'re an npm package author then it would be fantastic to start shipping a package with typings in the box. If you\'re using npm to consume packages then using Definitely Typed ought to be the second step you might take after installing a package; the step you only need to take if the package doesn\'t come with typings. Using DT should be a fallback, not a default.\\n\\n## Authoring npm modules with TypeScript\\n\\nYup - that\'s what this post is actually about. See how I lured you in with my mild trolling and pulled the old switcheroo? That\'s edutainment my friend. So, how do we write npm packages in TypeScript and publish them with their typings? Apparently Gandhi [didn\'t actually say](http://www.nytimes.com/2011/08/30/opinion/falser-words-were-never-spoken.html?_r=0) \\"Be the change you wish to see in the world.\\" Which is a shame. But anyway, I\'m going to try and embrace the sentiment here.\\n\\nNot so long ago I wrote a small npm module called [globalize-so-what-cha-want](https://www.npmjs.com/package/globalize-so-what-cha-want). It is used to determine what parts of Globalize 1.x you need depending on the modules you\'re planning to use. It also, contains a little demo UI / online tool written in React which powers [this](http://johnnyreilly.github.io/globalize-so-what-cha-want/).\\n\\nFor this post, the purpose of the package is rather irrelevant. And even though I\'ve just told you about it, I want you to pretend that the online tool doesn\'t exist. Pretend I never mentioned it.\\n\\nWhat is relevant, and what I want you to think about, is this: I wrote globalize-so-what-cha-want in plain old, honest to goodness JavaScript. Old school.\\n\\n[But, my love of static typing could be held in abeyance for only so long.](https://www.youtube.com/watch?v=V4YPFHyGWaY&feature=youtu.be&t=49s) Once the initial package was written, unit tested and published I got the itch. THIS SHOULD BE WRITTEN IN TYPESCRIPT!!! Well, it didn\'t have to be but I wanted it to be. Despite having used TypeScript since the early days I\'d only been using it for front end work; not for writing npm packages. My mission was clear: port globalize-so-what-cha-want to TypeScript and re-publish to npm.\\n\\n## Port, port, port!!!\\n\\nAt this point globalize-so-what-cha-want consisted of a single `index.js` file in the root of the package. My end goal was to end up with that file still sat there, but now generated from TypeScript. Alongside it I wanted to see a `index.d.ts` which was generated from the same TypeScript.\\n\\n`index.js`[before](https://github.com/johnnyreilly/globalize-so-what-cha-want/tree/6cce84289134a555fe8462247b43eddb051303e3) looked like this:\\n\\n```js\\n/* jshint varstmt: false, esnext: false */\\nvar DEPENDENCY_TYPES = {\\n  SHARED_JSON: \'Shared JSON (used by all locales)\',\\n  LOCALE_JSON: \'Locale specific JSON (supplied for each locale)\',\\n};\\n\\nvar moduleDependencies = {\\n  core: {\\n    dependsUpon: [],\\n    cldrGlobalizeFiles: [\\n      \'cldr.js\',\\n      \'cldr/event.js\',\\n      \'cldr/supplemental.js\',\\n      \'globalize.js\',\\n    ],\\n    json: [\\n      {\\n        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,\\n        dependency: \'cldr/supplemental/likelySubtags.json\',\\n      },\\n    ],\\n  },\\n\\n  currency: {\\n    dependsUpon: [\'number\', \'plural\'],\\n    cldrGlobalizeFiles: [\'globalize/currency.js\'],\\n    json: [\\n      {\\n        dependencyType: DEPENDENCY_TYPES.LOCALE_JSON,\\n        dependency: \'cldr/main/{locale}/currencies.json\',\\n      },\\n      {\\n        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,\\n        dependency: \'cldr/supplemental/currencyData.json\',\\n      },\\n    ],\\n  },\\n\\n  date: {\\n    dependsUpon: [\'number\'],\\n    cldrGlobalizeFiles: [\'globalize/date.js\'],\\n    json: [\\n      {\\n        dependencyType: DEPENDENCY_TYPES.LOCALE_JSON,\\n        dependency: \'cldr/main/{locale}/ca-gregorian.json\',\\n      },\\n      {\\n        dependencyType: DEPENDENCY_TYPES.LOCALE_JSON,\\n        dependency: \'cldr/main/{locale}/timeZoneNames.json\',\\n      },\\n      {\\n        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,\\n        dependency: \'cldr/supplemental/timeData.json\',\\n      },\\n      {\\n        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,\\n        dependency: \'cldr/supplemental/weekData.json\',\\n      },\\n    ],\\n  },\\n\\n  message: {\\n    dependsUpon: [\'core\'],\\n    cldrGlobalizeFiles: [\'globalize/message.js\'],\\n    json: [],\\n  },\\n\\n  number: {\\n    dependsUpon: [\'core\'],\\n    cldrGlobalizeFiles: [\'globalize/number.js\'],\\n    json: [\\n      {\\n        dependencyType: DEPENDENCY_TYPES.LOCALE_JSON,\\n        dependency: \'cldr/main/{locale}/numbers.json\',\\n      },\\n      {\\n        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,\\n        dependency: \'cldr/supplemental/numberingSystems.json\',\\n      },\\n    ],\\n  },\\n\\n  plural: {\\n    dependsUpon: [\'core\'],\\n    cldrGlobalizeFiles: [\'globalize/plural.js\'],\\n    json: [\\n      {\\n        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,\\n        dependency: \'cldr/supplemental/plurals.json\',\\n      },\\n      {\\n        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,\\n        dependency: \'cldr/supplemental/ordinals.json\',\\n      },\\n    ],\\n  },\\n\\n  relativeTime: {\\n    dependsUpon: [\'number\', \'plural\'],\\n    cldrGlobalizeFiles: [\'globalize/relative-time.js\'],\\n    json: [\\n      {\\n        dependencyType: DEPENDENCY_TYPES.LOCALE_JSON,\\n        dependency: \'cldr/main/{locale}/dateFields.json\',\\n      },\\n    ],\\n  },\\n};\\n\\nfunction determineRequiredCldrData(globalizeOptions) {\\n  return determineRequired(\\n    globalizeOptions,\\n    _populateDependencyCurrier(\'json\', function (json) {\\n      return json.dependency;\\n    })\\n  );\\n}\\n\\nfunction determineRequiredCldrGlobalizeFiles(globalizeOptions) {\\n  return determineRequired(\\n    globalizeOptions,\\n    _populateDependencyCurrier(\\n      \'cldrGlobalizeFiles\',\\n      function (cldrGlobalizeFile) {\\n        return cldrGlobalizeFile;\\n      }\\n    )\\n  );\\n}\\n\\nfunction determineRequired(globalizeOptions, populateDependencies) {\\n  var modules = Object.keys(globalizeOptions);\\n  modules.forEach(function (module) {\\n    if (!moduleDependencies[module]) {\\n      throw new TypeError(\\"There is no \'\\" + module + \\"\' module\\");\\n    }\\n  });\\n\\n  var requireds = [];\\n  modules.forEach(function (module) {\\n    if (globalizeOptions[module]) {\\n      populateDependencies(module, requireds);\\n    }\\n  });\\n\\n  return requireds;\\n}\\n\\nfunction _populateDependencyCurrier(requiredArray, requiredArrayGetter) {\\n  var popDepFn = function (module, requireds) {\\n    var dependencies = moduleDependencies[module];\\n\\n    dependencies.dependsUpon.forEach(function (requiredModule) {\\n      popDepFn(requiredModule, requireds);\\n    });\\n\\n    dependencies[requiredArray].forEach(function (required) {\\n      var newRequired = requiredArrayGetter(required);\\n      if (requireds.indexOf(newRequired) === -1) {\\n        requireds.push(newRequired);\\n      }\\n    });\\n\\n    return requireds;\\n  };\\n\\n  return popDepFn;\\n}\\n\\nmodule.exports = {\\n  determineRequiredCldrData: determineRequiredCldrData,\\n  determineRequiredCldrGlobalizeFiles: determineRequiredCldrGlobalizeFiles,\\n};\\n```\\n\\nYou can even kind of tell that it was written in JavaScript thanks to the jshint rules at the top.\\n\\nI fired up Atom and created a new folder `src/lib` and inside there I created `index.ts` (yes, `index.js` renamed) and `tsconfig.json`. By the way, you\'ll notice I\'m not leaving Atom - I\'m making use of the magnificent [atom-typescript](https://atom.io/packages/atom-typescript) which you should totally be using too. It rocks.\\n\\n![](Screenshot-2015-09-23-05.51.14.png)\\n\\nNow I\'m not going to bore you with what I had to do to port the JS to TS (not much). If you\'re interested, the source is [here](https://github.com/johnnyreilly/globalize-so-what-cha-want/blob/master/src/lib/index.ts). What\'s more interesting is the `tsconfig.json` \\\\- as it\'s this that is going to lead the generation of the JS and TS that we need:\\n\\n```json\\n{\\n  \\"compileOnSave\\": true,\\n  \\"compilerOptions\\": {\\n    \\"module\\": \\"commonjs\\",\\n    \\"declaration\\": true,\\n    \\"target\\": \\"es5\\",\\n    \\"noImplicitAny\\": true,\\n    \\"suppressImplicitAnyIndexErrors\\": true,\\n    \\"removeComments\\": false,\\n    \\"preserveConstEnums\\": true,\\n    \\"sourceMap\\": false,\\n    \\"outDir\\": \\"../../\\"\\n  },\\n  \\"files\\": [\\"index.ts\\"]\\n}\\n```\\n\\nThe things to notice are:\\n\\n<dl><dt>module</dt><dd>Publishing a commonjs module means it will play well with npm</dd><dt>declaration</dt><dd>This is what makes TypeScript generate <code>index.d.ts</code></dd><dt>outDir</dt><dd>We want to regenerate the <code>index.js</code> in the root (2 directories above this)</dd></dl>\\n\\nSo now, what do we get when we build in Atom? Well, we\'re generating an [`index.js`](https://github.com/johnnyreilly/globalize-so-what-cha-want/blob/master/index.js) file which looks like this:\\n\\n```js\\nvar DEPENDENCY_TYPES = {\\n  SHARED_JSON: \'Shared JSON (used by all locales)\',\\n  LOCALE_JSON: \'Locale specific JSON (supplied for each locale)\',\\n};\\nvar moduleDependencies = {\\n  core: {\\n    dependsUpon: [],\\n    cldrGlobalizeFiles: [\\n      \'cldr.js\',\\n      \'cldr/event.js\',\\n      \'cldr/supplemental.js\',\\n      \'globalize.js\',\\n    ],\\n    json: [\\n      {\\n        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,\\n        dependency: \'cldr/supplemental/likelySubtags.json\',\\n      },\\n    ],\\n  },\\n  currency: {\\n    dependsUpon: [\'number\', \'plural\'],\\n    cldrGlobalizeFiles: [\'globalize/currency.js\'],\\n    json: [\\n      {\\n        dependencyType: DEPENDENCY_TYPES.LOCALE_JSON,\\n        dependency: \'cldr/main/{locale}/currencies.json\',\\n      },\\n      {\\n        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,\\n        dependency: \'cldr/supplemental/currencyData.json\',\\n      },\\n    ],\\n  },\\n  date: {\\n    dependsUpon: [\'number\'],\\n    cldrGlobalizeFiles: [\'globalize/date.js\'],\\n    json: [\\n      {\\n        dependencyType: DEPENDENCY_TYPES.LOCALE_JSON,\\n        dependency: \'cldr/main/{locale}/ca-gregorian.json\',\\n      },\\n      {\\n        dependencyType: DEPENDENCY_TYPES.LOCALE_JSON,\\n        dependency: \'cldr/main/{locale}/timeZoneNames.json\',\\n      },\\n      {\\n        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,\\n        dependency: \'cldr/supplemental/timeData.json\',\\n      },\\n      {\\n        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,\\n        dependency: \'cldr/supplemental/weekData.json\',\\n      },\\n    ],\\n  },\\n  message: {\\n    dependsUpon: [\'core\'],\\n    cldrGlobalizeFiles: [\'globalize/message.js\'],\\n    json: [],\\n  },\\n  number: {\\n    dependsUpon: [\'core\'],\\n    cldrGlobalizeFiles: [\'globalize/number.js\'],\\n    json: [\\n      {\\n        dependencyType: DEPENDENCY_TYPES.LOCALE_JSON,\\n        dependency: \'cldr/main/{locale}/numbers.json\',\\n      },\\n      {\\n        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,\\n        dependency: \'cldr/supplemental/numberingSystems.json\',\\n      },\\n    ],\\n  },\\n  plural: {\\n    dependsUpon: [\'core\'],\\n    cldrGlobalizeFiles: [\'globalize/plural.js\'],\\n    json: [\\n      {\\n        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,\\n        dependency: \'cldr/supplemental/plurals.json\',\\n      },\\n      {\\n        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,\\n        dependency: \'cldr/supplemental/ordinals.json\',\\n      },\\n    ],\\n  },\\n  relativeTime: {\\n    dependsUpon: [\'number\', \'plural\'],\\n    cldrGlobalizeFiles: [\'globalize/relative-time.js\'],\\n    json: [\\n      {\\n        dependencyType: DEPENDENCY_TYPES.LOCALE_JSON,\\n        dependency: \'cldr/main/{locale}/dateFields.json\',\\n      },\\n    ],\\n  },\\n};\\nfunction determineRequired(globalizeOptions, populateDependencies) {\\n  var modules = Object.keys(globalizeOptions);\\n  modules.forEach(function (module) {\\n    if (!moduleDependencies[module]) {\\n      throw new TypeError(\\"There is no \'\\" + module + \\"\' module\\");\\n    }\\n  });\\n  var requireds = [];\\n  modules.forEach(function (module) {\\n    if (globalizeOptions[module]) {\\n      populateDependencies(module, requireds);\\n    }\\n  });\\n  return requireds;\\n}\\nfunction _populateDependencyCurrier(requiredArray, requiredArrayGetter) {\\n  var popDepFn = function (module, requireds) {\\n    var dependencies = moduleDependencies[module];\\n    dependencies.dependsUpon.forEach(function (requiredModule) {\\n      popDepFn(requiredModule, requireds);\\n    });\\n    dependencies[requiredArray].forEach(function (required) {\\n      var newRequired = requiredArrayGetter(required);\\n      if (requireds.indexOf(newRequired) === -1) {\\n        requireds.push(newRequired);\\n      }\\n    });\\n    return requireds;\\n  };\\n  return popDepFn;\\n}\\n/**\\n * The string array returned will contain a list of the required cldr json data you need. I don\'t believe ordering matters for the json but it is listed in the same dependency order as the cldr / globalize files are.\\n *\\n * @param options The globalize modules being used.\\n */\\nfunction determineRequiredCldrData(globalizeOptions) {\\n  return determineRequired(\\n    globalizeOptions,\\n    _populateDependencyCurrier(\'json\', function (json) {\\n      return json.dependency;\\n    })\\n  );\\n}\\nexports.determineRequiredCldrData = determineRequiredCldrData;\\n/**\\n * The string array returned will contain a list of the required cldr / globalize files you need, listed in the order they are required.\\n *\\n * @param options The globalize modules being used.\\n */\\nfunction determineRequiredCldrGlobalizeFiles(globalizeOptions) {\\n  return determineRequired(\\n    globalizeOptions,\\n    _populateDependencyCurrier(\\n      \'cldrGlobalizeFiles\',\\n      function (cldrGlobalizeFile) {\\n        return cldrGlobalizeFile;\\n      }\\n    )\\n  );\\n}\\nexports.determineRequiredCldrGlobalizeFiles =\\n  determineRequiredCldrGlobalizeFiles;\\n```\\n\\nAside from one method moving internally and me adding some JSDoc, the only really notable change is the end of the file. TypeScript, when generating commonjs, doesn\'t use the `module.exports = {}` approach. Rather, it drops exported functions onto the `exports` object as functions are exported. Functionally this is _identical_.\\n\\nNow for our big finish: happily sat alongside is `index.js` is the [`index.d.ts`](https://github.com/johnnyreilly/globalize-so-what-cha-want/blob/master/index.d.ts) file:\\n\\n```ts\\nexport interface Options {\\n  currency?: boolean;\\n  date?: boolean;\\n  message?: boolean;\\n  number?: boolean;\\n  plural?: boolean;\\n  relativeTime?: boolean;\\n}\\n/**\\n * The string array returned will contain a list of the required cldr json data you need. I don\'t believe ordering matters for the json but it is listed in the same dependency order as the cldr / globalize files are.\\n *\\n * @param options The globalize modules being used.\\n */\\nexport declare function determineRequiredCldrData(\\n  globalizeOptions: Options\\n): string[];\\n/**\\n * The string array returned will contain a list of the required cldr / globalize files you need, listed in the order they are required.\\n *\\n * @param options The globalize modules being used.\\n */\\nexport declare function determineRequiredCldrGlobalizeFiles(\\n  globalizeOptions: Options\\n): string[];\\n```\\n\\nWe\'re there, huzzah! This has been now published to npm - anyone consuming this package can use TypeScript straight out of the box. I really hope that publishing npm packages in this fashion becomes much more commonplace. Time will tell.\\n\\n## PS I\'m not the only one\\n\\nI was just about to hit \\"publish\\" when I happened upon [Basarat](https://twitter.com/basarat)\'s [ts-npm-module](https://github.com/basarat/ts-npm-module) which is a project on GitHub which demo\'s how to publish and consume TypeScript using npm. I\'d say great minds think alike but I\'m pretty sure Basarat\'s mind is far greater than mine! (Cough, atom-typescript, cough.) Either way, it\'s good to see validation for the approach I\'m suggesting.\\n\\n## PPS Update 23/09/2015 09:51\\n\\nOne of the useful things about writing a blog is that you get to learn. Since I published I\'ve become aware of a few things somewhat relevant to this post. First of all, there is still work ongoing in TypeScript land around this topic. Essentially there are problems resolving dependency conflicts when different dependencies have different versions - you can take part in the ongoing discussion [here](https://github.com/Microsoft/TypeScript/issues/4665). There\'s also some useful resources to look at:\\n\\n- [https://github.com/Microsoft/TypeScript/wiki/Typings-for-npm-packages](https://github.com/Microsoft/TypeScript/wiki/Typings-for-npm-packages)\\n- [https://basarat.gitbooks.io/typescript/content/docs/node/nodejs.html](https://basarat.gitbooks.io/typescript/content/docs/node/nodejs.html)"},{"id":"/2015/09/10/things-done-changed","metadata":{"permalink":"/2015/09/10/things-done-changed","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2015-09-10-things-done-changed/index.md","source":"@site/blog/2015-09-10-things-done-changed/index.md","title":"Things Done Changed","description":"Some people fear change. Most people actually. I\'m not immune to that myself, but not in the key area of technology. Any developer that fears change when it comes to the tools and languages that he / she is using is in the wrong business. Because what you\'re using to cut code today will not last. The language will evolve, the tools and frameworks that you love will die out and be replaced by new ones that are different and strange. In time, the language you feel you write as a native will fall out of favour, replaced by a new upstart.","date":"2015-09-10T00:00:00.000Z","formattedDate":"September 10, 2015","tags":[{"label":"Browserify","permalink":"/tags/browserify"},{"label":"ES6","permalink":"/tags/es-6"},{"label":"Atom","permalink":"/tags/atom"},{"label":"Babel","permalink":"/tags/babel"},{"label":"React","permalink":"/tags/react"},{"label":"WebSockets","permalink":"/tags/web-sockets"}],"readingTime":10.125,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Things Done Changed","authors":"johnnyreilly","tags":["Browserify","ES6","Atom","Babel","React","WebSockets"],"hide_table_of_contents":false},"prevItem":{"title":"Definitely Typed Shouldn\'t Exist","permalink":"/2015/09/23/authoring-npm-modules-with-typescript"},"nextItem":{"title":"(Top One, Nice One) Get Sorted","permalink":"/2015/08/13/top-one-nice-one-get-sorted"}},"content":"Some people fear change. Most people actually. I\'m not immune to that myself, but not in the key area of technology. Any developer that fears change when it comes to the tools and languages that he / she is using is in the _wrong_ business. Because what you\'re using to cut code today will not last. The language will evolve, the tools and frameworks that you love will die out and be replaced by new ones that are different and strange. In time, the language you feel you write as a native will fall out of favour, replaced by a new upstart.\\n\\nMy first gig was writing telecoms software using Delphi. I haven\'t touched Delphi (or telecoms for that matter) for over 10 years now. Believe me, I grok that things change.\\n\\nThat is the developer\'s lot. If you\'re able to accept that then you\'ll be just fine. For my part I\'ve always rather relished the new and so I embrace it. However, I\'ve met a surprising number of devs that are outraged when they realise that the language and tools they have used since their first job are not going to last. They do not go gentle into that good dawn. They rage, rage against the death of WebForms. My apologies to Dylan Thomas.\\n\\nI recently started a new contract. This always brings a certain amount of change. This is part of the fun of contracting. However, the change was more significant in this case. As a result, the tools that I\'ve been using for the last couple of months have been rather different to those that I\'m used to. I\'ve been outside my comfort zone. I\'ve loved it. And now I want to reflect upon it. Because, in the words of Socrates, \\"the unexamined life is not worth living\\".\\n\\n## The Shock of the New (Toys)\\n\\nI\'d been brought in to work on a full stack ASP.Net project. However, I\'ve initially been working on a separate project which is _entirely_ different. A web client app which has nothing to do with ASP.Net at all. It\'s a greenfield app which is built using the following:\\n\\n1. [React](https://facebook.github.io/react/) / [Flux](https://facebook.github.io/flux/docs/overview.html)\\n2. [WebSockets](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API) / [Protocol Buffers](https://developers.google.com/protocol-buffers/)\\n3. [Browserify](http://browserify.org/)\\n4. [ES6 with Babel](http://babeljs.io/)\\n5. [Karma](https://karma-runner.github.io)\\n6. [Gulp](http://gulpjs.com/)\\n7. [Atom](https://atom.io/)\\n\\nWhere to begin? Perhaps at the end - Atom.\\n\\n## How Does it Feel to be on Your Own?\\n\\nWhen all around you, as far as the eye can see, are monitors displaying Visual Studio in all its grey glory whilst I was hammering away on Atom. It felt pretty good actually.\\n\\nThe app I was working on was a React / Flux app. You know what that means? JSX! At the time the project began Visual Studio did not have good editor support for JSX (something that the shipping of VS 2015 may have remedied but I haven\'t checked). So, rather than endure a life dominated with red squigglies I jumped ship and moved over to using GitHub\'s Atom to edit code.\\n\\nI rather like it. Whilst VS is a full blown IDE, Atom is a text editor. A very pretty one. And crucially, one that can be extended by plugins of which there is a rich ecosystem. You want JSX support? [There\'s a plugin for that](https://atom.io/packages/jshint). You want something that formats JSON nicely for you? [There\'s a plugin for that too](https://atom.io/packages/pretty-json).\\n\\nMy only criticism of Atom really is that it doesn\'t handle large files well and it crashes a lot. I\'m quite surprised by both of these characteristics given that in contrast to VS it is so small. You\'d think the basics would be better covered. Go figure. It still rocks though. It looks so sexy - how could I not like it?\\n\\n## Someone to watch over me\\n\\nI\'ve been using Gulp for a while now. It\'s a great JavaScript task runner and incredibly powerful. Previously though, I\'ve used it as part of a manual build step (even plumbing it into my csproj). With the current project I\'ve moved over to using the watch functionality of gulp. So I\'m scrapping triggering gulp tasks manually. Instead we have gulp configured to gaze lovingly at the source files and, when they change, re-run the build steps.\\n\\nThis is nice for a couple of reasons:\\n\\n- When I want to test out the app the build is already done - I don\'t have to wait for it to happen.\\n- When I do bad things I find out faster. So I\'ve got JSHint being triggered by my watch. If I write code that makes JSHint sad (and I haven\'t noticed the warnings from the [atom plugin](https://atom.io/packages/jshint)) then they\'ll appear in the console. Likewise, my unit tests are continuously running in response to file changes (in an [ncrunch](http://www.ncrunch.net/)\\\\-y sorta style) and so I know straight away if I\'m breaking tests. Rather invaluable in the dynamic world of JavaScript.\\n\\n## Karma, Karma, Karma, Chameleon\\n\\nIn the past, when using Visual Studio, it made sense to use the mighty [Chutzpah](http://mmanela.github.io/chutzpah/) which allows you to run JS unit tests from within VS itself. I needed a new way to run my Jasmine unit tests. The obvious choice was Karma (built by the Angular team I think?). It\'s really flexible.\\n\\nYou\'re using Browserify? [No bother](https://www.npmjs.com/package/karma-browserify). You\'re writing ES6 and transpiling with Babel? Not a problem. You want code coverage? [That we can do](https://www.npmjs.com/package/karma-coverage). You want an integration for TeamCity? [That too is sorted](https://www.npmjs.com/package/karma-teamcity-reporter)....\\n\\nKarma is fantastic. Fun fact: originally it was called Testacular. I kind of get why they changed the name but the world is all the duller for it. A side effect of the name change is that due to invalid search results I know a lot more about Buddhism than I used to.\\n\\n## I cry Babel, Babel, look at me now\\n\\nCan you not wait for the future? Me neither. Even though it\'s 2015 and Back to the Future II takes place in [only a month\'s time](http://www.october212015.com/). So I\'m not waiting for a million and one browsers to implement ES6 and IE 8 to finally die dammit. Nope, I have a plan and it\'s [Babel](http://babeljs.io/). Transpile the tears away, write ES6 and have Babel spit out EStoday.\\n\\nI\'ve found this pretty addictive. Once you\'ve started using ES6 features it\'s hard to go back. Take [destructuring](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Destructuring_assignment) \\\\- I can\'t get enough of it.\\n\\nWhilst I love Babel, it has caused me some sadness as well. My beloved TypeScript is currently not in the mix, Babel is instead sat squarely where I want TS to be. I\'m without static types and rather bereft. You can certainly live without them but having done so for a while I\'m pretty clear now that static typing is a massive productivity win. You don\'t have to hold the data structures that you\'re working on in your head so much, code completion gives you what you need there, you can focus more on the problem that you\'re trying to solve. You also burn less time on silly mistakes. If you accidentally change the return type of a function you\'re likely to know straight away. Refactoring is so much harder without static types. I could go on.\\n\\nAll this goes to say: I want my static typing back. It wasn\'t really an option to use TypeScript in the beginning because we were using JSX and TS didn\'t support it. However! TypeScript is due to add support for JSX in [TS 1.6 (currently in beta)](https://blogs.msdn.com/b/typescript/archive/2015/09/02/announcing-typescript-1-6-beta-react-jsx-better-error-checking-and-more.aspx). I\'ve plans to see if I can get TypeScript to emit ES6 and then keep using Babel to do the transpilation. Whether this will work, I don\'t know yet. But it seems likely. So I\'m hoping for a bright future.\\n\\n## Browserify (there are no song lyrics that can be crowbarred in)\\n\\nEmitting scripts in the required order has been a perpetual pain for everyone in the web world for the longest time. I\'ve taken about 5 different approaches to solving this problem over the years. None felt particularly good. So Browserify.\\n\\nBrowserify solves the problem of script ordering for you by allowing you to define an entry point to your application and getting you to write `require` (npm style) or `import` ([ES6 modules](http://exploringjs.com/es6/ch_modules.html)) to bring in the modules that you need. This allows Browserify (which we\'re using with Babel thanks to the [babelify transform](https://github.com/babel/babelify)) to create a ginormous js file that contains the scripts served as needed. Thanks to the magic of source maps it also allows us to debug our original code (yup, the original ES6!) Browserify has the added bonus of allowing us free reign to pull in npm packages to use in our app without a great deal of ceremony.\\n\\nBrowserify is pretty fab - my only real reservation is that if you step outside the normal use cases you can quickly find yourself in deep water. Take for instance web workers. We were briefly looking into using them as an optimisation (breaking IO onto a separate process from the UI). A prime reason for backing away from this is that [Web Workers don\'t play particularly well with Browserify](https://github.com/substack/webworkify/issues/14). And when you\'ve got Babel (or [Babelify](https://github.com/babel/babelify)) in the mix the problems just multiply. That apart, I really dig Browserify. I think I\'d like to give WebPack a go as well as I understand it fulfills a similar purpose.\\n\\n## WebSockets / Protocol Buffers\\n\\nThe application I\'m working on is plugging into an existing system which uses WebSockets for communication. Since WebSockets are native to the web we\'ve been able to plumb these straight into our app. We\'re also using Protocol Buffers as another optimisation; a way to save a few extra bytes from going down the wire. I don\'t have much to say about either, just some observations really:\\n\\n1. WebSockets is a slightly different way of working - permanently open connections as opposed to the request / response paradigm of classic HTTP\\n2. WebSockets are wicked fast (due in part to those permanent connections). So performance is _amazing_. Fast like native, type amazing. In our case performance is pretty important and so this has been really great.\\n\\n## React / Flux\\n\\nFinally, React and Flux. I was completely new to these when I came onto the project and I quickly came to love them. There was a prejudice for me to overcome and that was JSX. When I first saw it I felt a little sick. \\"Have we learned _NOTHING_???\\" I wailed. \\"Don\'t we know that embedding strings in our controllers is a _BAD_ thing?\\" I was wrong. I had an epiphany. I discovered that JSX is not, as I first imagined, embedded HTML strings. Actually it\'s syntactic sugar for object creation. A simple example:\\n\\n```jsx\\nvar App;\\n\\n// Some JSX:\\nvar app = <App version=\\"1.0.0\\" />;\\n\\n// What that JSX transpiles into:\\nvar app = React.createElement(App, { version: \'1.0.0\' });\\n```\\n\\nNow that I\'m well used to JSX and React I\'ve really got to like it. I keep my views / components as dumb as possible and do all the complex logic in the stores. The stores are just standard JavaScript and so, pretty testable (simple Jasmine gives you all you need - I haven\'t felt the need for [Jest](https://facebook.github.io/jest/)). The components / views are also completely testable. I\'d advise anyone coming to React afresh to make use of the [`ReactShallowRenderer`](https://facebook.github.io/react/docs/test-utils.html#shallow-rendering) for these purposes. This means you can test without a DOM - much better all round.\\n\\nI don\'t have a great deal to say about Flux; I think my views on it aren\'t fully formed yet. I do really like predictability that unidirectional data flow gives you. However, I\'m mindful that the app that I\'ve been writing is very much about displaying data and doesn\'t require much user input. I know that I\'m living without 2-way data binding and I do wonder if I would come to miss it. Time will tell.\\n\\nI really want to get back to static typing. That either means TypeScript (which I know and love) or Facebook\'s Flow. ([A Windows version of Flow is in the works](https://github.com/facebook/flow/issues/6).) I\'ll be very happy if I get either into the mix... Watch this space."},{"id":"/2015/08/13/top-one-nice-one-get-sorted","metadata":{"permalink":"/2015/08/13/top-one-nice-one-get-sorted","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2015-08-13-top-one-nice-one-get-sorted/index.md","source":"@site/blog/2015-08-13-top-one-nice-one-get-sorted/index.md","title":"(Top One, Nice One) Get Sorted","description":"I was recently reading a post by Jaime Gonz\xe1lez Garc\xeda which featured the following mind-bending proposition:","date":"2015-08-13T00:00:00.000Z","formattedDate":"August 13, 2015","tags":[{"label":"sort","permalink":"/tags/sort"},{"label":"javascript","permalink":"/tags/javascript"},{"label":"OrderBy","permalink":"/tags/order-by"},{"label":"LINQ","permalink":"/tags/linq"}],"readingTime":7.88,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"(Top One, Nice One) Get Sorted","authors":"johnnyreilly","tags":["sort","javascript","OrderBy","LINQ"],"hide_table_of_contents":false},"prevItem":{"title":"Things Done Changed","permalink":"/2015/09/10/things-done-changed"},"nextItem":{"title":"Upgrading to Globalize 1.x for Dummies","permalink":"/2015/07/30/upgrading-to-globalize-1x-for-dummies"}},"content":"I was recently reading [a post by Jaime Gonz\xe1lez Garc\xeda](http://www.barbarianmeetscoding.com/blog/2015/07/09/mastering-the-arcane-art-of-javascript-mancy-for-c-sharp-developers-chapter-7-using-linq-in-javascript/) which featured the following mind-bending proposition:\\n\\n> What if I told you that JavaScript has [LINQ](https://msdn.microsoft.com/en-us/library/bb397926.aspx)??\\n\\nIt got me thinking about one of favourite features of LINQ: [ordering using OrderBy, ThenBy...](http://www.dotnetperls.com/orderby-extension) The ability to simply expose a collection of objects in a given order with a relatively terse and descriptive syntax. It is fantastically convenient, expressive and something I\'ve been missing in JavaScript. But if Jaime is right... Well, let\'s see what we can do.\\n\\n## Sort\\n\\nJavaScript arrays have a [sort](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/sort) method. To quote MDN:\\n\\n> `arr.sort([compareFunction])`### `compareFunction`\\n>\\n> Optional. Specifies a function that defines the sort order. If omitted, the array is sorted according to each character\'s Unicode code point value, according to the string conversion of each element.\\n\\nWe want to use the `sort` function to introduce some LINQ-ish ordering goodness. Sort of. See what I did there?\\n\\nBefore we get going it\'s worth saying that LINQ\'s `OrderBy` and JavaScript\'s `sort` are not the same thing. `sort` actually changes the order of the array. However, `OrderBy` returns an `IOrderedEnumerable` which when iterated returns the items of the collection in a particular order. An important difference. If preserving the original order of my array was important to me (spoiler: mostly it isn\'t) then I could make a call to [`slice`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/slice) prior to calling `sort`.\\n\\n`sort` also returns the array to the caller which is nice for chaining and means we can use it in a similar fashion to the way we use `OrderBy`. With that in mind, we\'re going to create comparer functions which will take a lambda / arrow function (ES6 alert!) and return a function which will compare based on the supplied lambda.\\n\\n## String Comparer\\n\\nLet\'s start with ordering by string properties:\\n\\n```js\\nfunction stringComparer(propLambda) {\\n  return (obj1, obj2) => {\\n    const obj1Val = propLambda(obj1) || \'\';\\n    const obj2Val = propLambda(obj2) || \'\';\\n    return obj1Val.localeCompare(obj2Val);\\n  };\\n}\\n```\\n\\nWe need some example data to sort: (I can only apologise for my lack of inspiration here)\\n\\n```js\\nconst foodInTheHouse = [\\n  { what: \'cake\', daysSincePurchase: 2 },\\n  { what: \'apple\', daysSincePurchase: 8 },\\n  { what: \'orange\', daysSincePurchase: 6 },\\n  { what: \'apple\', daysSincePurchase: 2 },\\n];\\n```\\n\\nIf we were doing a sort by strings in LINQ we wouldn\'t need to implement our own comparer. And the code we\'d write would look something like this:\\n\\n```js\\nvar foodInTheHouseSorted = foodInTheHouse.OrderBy((x) => x.what);\\n```\\n\\nWith that in mind, here\'s how it would look to use our shiny and new `stringComparer`:\\n\\n```js\\nconst foodInTheHouseSorted = foodInTheHouse.sort(stringComparer((x) => x.what));\\n\\n// foodInTheHouseSorted: [\\n//   { what: \'apple\',  daysSincePurchase: 8 },\\n//   { what: \'apple\',  daysSincePurchase: 2 },\\n//   { what: \'cake\',   daysSincePurchase: 2 },\\n//   { what: \'orange\', daysSincePurchase: 6 }\\n// ]\\n\\n// PS Don\'t forget, for our JavaScript: foodInTheHouse === foodInTheHouseSorted\\n//    But for the LINQ:                 foodInTheHouse !=  foodInTheHouseSorted\\n//\\n//    However, if I\'d done this:\\n\\nconst foodInTheHouseSlicedAndSorted = foodInTheHouse\\n  .slice()\\n  .sort(stringComparer((x) => x.what));\\n\\n//    then:                             foodInTheHouse !== foodInTheHouseSlicedAndSorted\\n//\\n//    I shan\'t mention this again.\\n```\\n\\n## Number Comparer\\n\\nWell that\'s strings sorted (quite literally). Now, what about numbers?\\n\\n```js\\nfunction numberComparer(propLambda) {\\n  return (obj1, obj2) => {\\n    const obj1Val = propLambda(obj1);\\n    const obj2Val = propLambda(obj2);\\n    if (obj1Val > obj2Val) {\\n      return 1;\\n    } else if (obj1Val < obj2Val) {\\n      return -1;\\n    }\\n    return 0;\\n  };\\n}\\n```\\n\\nIf we use the `numberComparer` on our original array it looks like this:\\n\\n```js\\nconst foodInTheHouseSorted = foodInTheHouse.sort(\\n  numberComparer((x) => x.daysSincePurchase)\\n);\\n\\n// foodInTheHouseSorted: [\\n//   { what: \'cake\',   daysSincePurchase: 2 },\\n//   { what: \'apple\',  daysSincePurchase: 2 },\\n//   { what: \'orange\', daysSincePurchase: 6 },\\n//   { what: \'apple\',  daysSincePurchase: 8 }\\n// ]\\n```\\n\\n## Descending Into the Pit of Success\\n\\nWell this is all kinds of fabulous. But something\'s probably nagging at you... What about `OrderByDescending`? What about when I want to sort in the reverse order? May I present the `reverse` function:\\n\\n```js\\nfunction reverse(comparer) {\\n  return (obj1, obj2) => comparer(obj1, obj2) * -1;\\n}\\n```\\n\\nAs the name suggests, this function takes a given comparer that\'s handed to it and returns a function that inverts the results of executing that comparer. Clear as mud? A comparer can return 3 types of return values:\\n\\n- 0 - implies equality for `obj1` and `obj2`\\n- positive - implies `obj1` is greater than `obj2` by the ordering criterion\\n- negative - implies `obj1` is less than `obj2` by the ordering criterion\\n\\nOur `reverse` function takes the comparer it is given and returns a new comparer that will return a positive value where the old one would have returned a negative and vica versa. (Equality is unaffected.) An alternative implementation would have been this:\\n\\n```js\\nfunction reverse(comparer) {\\n  return (obj1, obj2) => comparer(obj2, obj1);\\n}\\n```\\n\\nWhich is more optimal and even simpler as it just swaps the values supplied to the comparer. Whatever tickles your fancy. Either way, when used it looks like this:\\n\\n```js\\nconst foodInTheHouseSorted = foodInTheHouse.sort(\\n  reverse(stringComparer((x) => x.what))\\n);\\n\\n// foodInTheHouseSorted: [\\n//   { what: \'orange\', daysSincePurchase: 6 },\\n//   { what: \'cake\',   daysSincePurchase: 2 },\\n//   { what: \'apple\',  daysSincePurchase: 8 },\\n//   { what: \'apple\',  daysSincePurchase: 2 }\\n// ]\\n```\\n\\nIf you\'d rather not have a function wrapping a function inline then you could create `stringComparerDescending`, a `numberComparerDescending` etc implementations. Arguably it might make for a nicer API. I\'m not unhappy with the present approach myself and so I\'ll leave it as is. But it\'s an option.\\n\\n## `ThenBy`\\n\\nSo far we can sort arrays by strings, we can sort arrays by numbers and we can do either in descending order. It\'s time to take it to the next level people. That\'s right `ThenBy`; I want to be able to sort by one criteria and then by a subcriteria. So perhaps I want to eat the food in the house in alphabetical order, but if I have multiple apples I want to eat the ones I bought most recently first (because the other ones look old, brown and yukky). This may also be a sign I haven\'t thought my life through, but it\'s a choice that people make. People that I know. People I may have married.\\n\\nIt\'s time to compose our comparers together. May I present... drum roll.... the `composeComparers` function:\\n\\n```js\\nfunction composeComparers(...comparers) {\\n  return (obj1, obj2) => {\\n    const comparer = comparers.find((c) => c(obj1, obj2) !== 0);\\n    return comparer ? comparer(obj1, obj2) : 0;\\n  };\\n}\\n```\\n\\nThis fine function takes any number of comparers that have been supplied to it. It then returns a comparer function which, when called, iterates through each of the original comparers and executes them until it finds one that returns a value that is not 0 (ie represents that the 2 items are not equal). It then sends that non-zero value back or if all was equal then sends back 0.\\n\\n```js\\nconst foodInTheHouseSorted = foodInTheHouse.sort(\\n  composeComparers(\\n    stringComparer((x) => x.what),\\n    numberComparer((x) => x.daysSincePurchase)\\n  )\\n);\\n\\n// foodInTheHouseSorted: [\\n//   { what: \'apple\',  daysSincePurchase: 2 },\\n//   { what: \'apple\',  daysSincePurchase: 8 },\\n//   { what: \'cake\',   daysSincePurchase: 2 },\\n//   { what: \'orange\', daysSincePurchase: 6 }\\n// ]\\n```\\n\\n## `composeComparers`: The Sequel\\n\\nI\'m not gonna lie - I was feeling quite pleased with this approach. I shared it with my friend (and repeated colleague) [Peter Foldi](http://blog.peterfoldi.com/). The next day I found this in my inbox:\\n\\n```js\\nfunction composeComparers(...comparers) {\\n  return (obj1, obj2) =>\\n    comparers.reduce((prev, curr) => prev || curr(obj1, obj2), 0);\\n}\\n```\\n\\nDammit he\'s improved it. It\'s down to 1 line of code, it doesn\'t execute a non-zero returning comparer twice and it doesn\'t rely on [`find`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/find) which only arrives with ES6. So if you wanted to backport to ES5 then this is a better choice.\\n\\nThe only criticism I can make of it is that it iterates through each of the comparers even when it doesn\'t need to execute them. But that\'s just carping really.\\n\\n## `composeComparers`: The Ultimate\\n\\nSo naturally I thought I was done. Showing Peter\'s improvements to the estimable Matthew Horsley I learned that this was not so. Because he reached for the keyboard and entered this:\\n\\n```js\\nfunction composeComparers(...comparers) {\\n  // README: <a href=\\"https://wiki.haskell.org/Function_composition\\">https://wiki.haskell.org/Function_composition</a>\\n  return comparers.reduce((prev, curr) => (a, b) => prev(a, b) || curr(a, b));\\n}\\n```\\n\\nThat\'s right, he\'s created a function which takes a number of comparers and reduced them up front into a single comparer function. This means that when the sort takes place there is no longer a need to iterate through the comparers, just execute them.\\n\\nI know.\\n\\n![](3qknmj.jpg)\\n\\nI\'ll get my coat...\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/22f7c05b02c2129b89ef.js\\"><\/script>\\n\\n## Update 08/10/2018: Now TypeScript\\n\\nYou want to do this with TypeScript? Use this:\\n\\n```ts\\ntype Comparer<TObject> = (obj1: TObject, obj2: TObject) => number;\\n\\nexport function stringComparer<TObject>(\\n  propLambda: (obj: TObject) => string\\n): Comparer<TObject> {\\n  return (obj1: TObject, obj2: TObject) => {\\n    const obj1Val = propLambda(obj1) || \'\';\\n    const obj2Val = propLambda(obj2) || \'\';\\n    return obj1Val.localeCompare(obj2Val);\\n  };\\n}\\n\\nexport function numberComparer<TObject>(\\n  propLambda: (obj: TObject) => number\\n): Comparer<TObject> {\\n  return (obj1: TObject, obj2: TObject) => {\\n    const obj1Val = propLambda(obj1);\\n    const obj2Val = propLambda(obj2);\\n    if (obj1Val > obj2Val) {\\n      return 1;\\n    } else if (obj1Val < obj2Val) {\\n      return -1;\\n    }\\n    return 0;\\n  };\\n}\\n\\nexport function reverse<TObject>(comparer: Comparer<TObject>) {\\n  return (obj1: TObject, obj2: TObject) => comparer(obj2, obj1);\\n}\\n\\nexport function composeComparers<TObject>(...comparers: Comparer<TObject>[]) {\\n  return comparers.reduce((prev, curr) => (a, b) => prev(a, b) || curr(a, b));\\n}\\n```"},{"id":"/2015/07/30/upgrading-to-globalize-1x-for-dummies","metadata":{"permalink":"/2015/07/30/upgrading-to-globalize-1x-for-dummies","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2015-07-30-upgrading-to-globalize-1x-for-dummies/index.md","source":"@site/blog/2015-07-30-upgrading-to-globalize-1x-for-dummies/index.md","title":"Upgrading to Globalize 1.x for Dummies","description":"Globalize has hit 1.0. Anyone who reads my blog will likely be aware that I\'m a long time user of Globalize 0.1.x. I\'ve been a little daunted by the leap that the move from 0.1.x to 1.x represents. It appears to be the very definition of \\"breaking changes\\". :-) But hey, this is Semantic Versioning being used correctly so how could I complain? Either way, I\'ve decided to write up the migration here as I\'m not expecting this to be easy.","date":"2015-07-30T00:00:00.000Z","formattedDate":"July 30, 2015","tags":[{"label":"Globalize","permalink":"/tags/globalize"},{"label":"migration","permalink":"/tags/migration"}],"readingTime":9.41,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Upgrading to Globalize 1.x for Dummies","authors":"johnnyreilly","tags":["Globalize","migration"],"hide_table_of_contents":false},"prevItem":{"title":"(Top One, Nice One) Get Sorted","permalink":"/2015/08/13/top-one-nice-one-get-sorted"},"nextItem":{"title":"npm please stop hurting Visual Studio","permalink":"/2015/06/29/npm-please-stop-hurting-visual-studio"}},"content":"Globalize has hit 1.0. Anyone who reads my blog will likely be aware that I\'m a long time user of [Globalize 0.1.x](http://blog.icanmakethiswork.io/2012/05/globalizejs-number-and-date.html). I\'ve been a little daunted by the leap that the move from 0.1.x to 1.x represents. It appears to be the very definition of \\"breaking changes\\". :-) But hey, this is Semantic Versioning being used correctly so how could I complain? Either way, I\'ve decided to write up the migration here as I\'m not expecting this to be easy.\\n\\nTo kick things off I\'ve set up a very [simple repo](https://github.com/johnnyreilly/globalize-migration/tree/v0.1.x) that consists of a single page that depends upon Globalize 0.1.x to render a number and a date in German. It looks like this:\\n\\n```html\\n<html>\\n  <head>\\n    <title>Globalize demo</title>\\n    <link\\n      href=\\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css\\"\\n      rel=\\"stylesheet\\"\\n    />\\n  </head>\\n  <body>\\n    <div class=\\"container-fluid\\">\\n      <h4>Globalize demo for the <em id=\\"locale\\"></em> culture / locale</h4>\\n      <p>\\n        This is a the number <strong id=\\"number\\"></strong> formatted by\\n        Globalize: <strong id=\\"numberFormatted\\"></strong>\\n      </p>\\n      <p>\\n        This is a the number <strong id=\\"date\\"></strong> formatted by Globalize:\\n        <strong id=\\"dateFormatted\\"></strong>\\n      </p>\\n    </div>\\n\\n    <script src=\\"bower_components/globalize/lib/globalize.js\\"><\/script>\\n    <script src=\\"bower_components/globalize/lib/cultures/globalize.culture.de-DE.js\\"><\/script>\\n    <script>\\n      var locale = \'de-DE\';\\n      var number = 12345.67;\\n      var date = new Date(2012, 5, 15);\\n\\n      Globalize.culture(locale);\\n      document.querySelector(\'#locale\').innerText = locale;\\n      document.querySelector(\'#number\').innerText = number;\\n      document.querySelector(\'#date\').innerText = date;\\n      document.querySelector(\'#numberFormatted\').innerText = Globalize.format(\\n        number,\\n        \'n2\'\\n      );\\n      document.querySelector(\'#dateFormatted\').innerText = Globalize.format(\\n        date,\\n        \'d\'\\n      );\\n    <\/script>\\n  </body>\\n</html>\\n```\\n\\nWhen it\'s run it looks like this:\\n\\n![](Screenshot-2015-07-29-06.03.04.png)\\n\\nLet\'s see how we go about migrating this super simple example.\\n\\n## Update our Bower dependencies\\n\\nFirst things first, we want to move Globalize from 0.1.x to 1.x using Bower. To do that we update our `bower.json`:\\n\\n```js\\n\\"dependencies\\": {\\n    \\"globalize\\": \\"^1.0.0\\"\\n  }\\n```\\n\\nNow we enter: `bower update`. And we\'re off!\\n\\n```sh\\nbower globalize#^1.0.0          cached git://github.com/jquery/globalize.git#1.0.0\\nbower globalize#^1.0.0        validate 1.0.0 against git://github.com/jquery/globalize.git#^1.0.0\\nbower cldr-data#>=25            cached git://github.com/rxaviers/cldr-data-bower.git#27.0.3\\nbower cldr-data#>=25          validate 27.0.3 against git://github.com/rxaviers/cldr-data-bower.git#>=25\\nbower cldrjs#0.4.1              cached git://github.com/rxaviers/cldrjs.git#0.4.1\\nbower cldrjs#0.4.1            validate 0.4.1 against git://github.com/rxaviers/cldrjs.git#0.4.1\\nbower globalize#^1.0.0         install globalize#1.0.0\\nbower cldr-data#>=25           install cldr-data#27.0.3\\nbower cldrjs#0.4.1             install cldrjs#0.4.1\\n\\nglobalize#1.0.0 bower_components\\\\globalize\\n\u251c\u2500\u2500 cldr-data#27.0.3\\n\u2514\u2500\u2500 cldrjs#0.4.1\\n\\ncldr-data#27.0.3 bower_components\\\\cldr-data\\n\\ncldrjs#0.4.1 bower_components\\\\cldrjs\\n\u2514\u2500\u2500 cldr-data#27.0.3\\n```\\n\\nThis all looks happy enough. Except it\'s actually not.\\n\\n## We need fuel\\n\\nOr as I like to call it cldr-data. We just pulled down Globalize 1.x but we didn\'t pull down the data that Globalize 1.x relies upon. This is one of the differences between Globalize 0.1.x and 1.x. Globalize 1.x does not include the \\"culture\\" data. By which I mean all the `globalize.culture.de-DE.js` type files. Instead Globalize 1.x relies upon [CLDR - Unicode Common Locale Data Repository](http://cldr.unicode.org/). It does this in the form of [cldr-json](https://github.com/unicode-cldr/cldr-json).\\n\\nNow before you start to worry, you shouldn\'t actually need to go and get this by yourself, the lovely [Rafael Xavier de Souza](https://github.com/rxaviers) has saved you a job by putting together [Bower](https://github.com/rxaviers/cldr-data-bower) and [npm](https://github.com/rxaviers/cldr-data-npm) modules to do the hard work for you.\\n\\nI\'m using Bower for my client side package management and so I\'ll use that. Looking at the Bower dependencies downloaded when I upgraded my package I can see there is a `cldr-data` package. Yay! However it appears to be missing the associated json files. Boo!\\n\\nTo the documentation Batman. It says you need a `.bowerrc` file in your repo which contains this:\\n\\n```js\\n{\\n  \\"scripts\\": {\\n    \\"preinstall\\": \\"npm install cldr-data-downloader@0.2.x\\",\\n    \\"postinstall\\": \\"node ./node_modules/cldr-data-downloader/bin/download.js -i bower_components/cldr-data/index.json -o bower_components/cldr-data/\\"\\n  }\\n}\\n```\\n\\nUnfortunately, because I\'ve already upgraded to v1 adding this file alone doesn\'t do anything for me. To get round that I delete my `bower_components` folder and enter `bower install`. Boom!\\n\\n```\\nbower globalize#^1.0.0          cached git://github.com/jquery/globalize.git#1.0.0\\nbower globalize#^1.0.0        validate 1.0.0 against git://github.com/jquery/globalize.git#^1.0.0\\nbower cldrjs#0.4.1                        cached git://github.com/rxaviers/cldrjs.git#0.4.1\\nbower cldrjs#0.4.1                      validate 0.4.1 against git://github.com/rxaviers/cldrjs.git#0.4.1\\nbower cldr-data#>=25                      cached git://github.com/rxaviers/cldr-data-bower.git#27.0.3\\nbower cldr-data#>=25                    validate 27.0.3 against git://github.com/rxaviers/cldr-data-bower.git#>=25\\nbower                                 preinstall npm install cldr-data-downloader@0.2.x\\nbower                                 preinstall cldr-data-downloader@0.2.3 node_modules\\\\cldr-data-downloader\\nbower                                 preinstall \u251c\u2500\u2500 progress@1.1.8\\nbower                                 preinstall \u251c\u2500\u2500 q@1.0.1\\nbower                                 preinstall \u251c\u2500\u2500 request-progress@0.3.1 (throttleit@0.0.2)\\nbower                                 preinstall \u251c\u2500\u2500 nopt@3.0.3 (abbrev@1.0.7)\\nbower                                 preinstall \u251c\u2500\u2500 mkdirp@0.5.0 (minimist@0.0.8)\\nbower                                 preinstall \u251c\u2500\u2500 adm-zip@0.4.4\\nbower                                 preinstall \u251c\u2500\u2500 npmconf@2.0.9 (uid-number@0.0.5, ini@1.3.4, inherits@2.0.1, once@1.3.2, osenv@0.1.3, config-chain@1.1.9, semver@4.3.6)\\nbower                                 preinstall \u2514\u2500\u2500 request@2.53.0 (caseless@0.9.0, forever-agent@0.5.2, aws-sign2@0.5.0, stringstream@0.0.4, tunnel-agent@0.4.1, oauth-sign@0.6.0, isstream@0.1.2, json-stringify-safe@5.0.1, qs@2.3.3, node-uuid@1.4.3, combined-stream@0.0.7, mime-types@2.0.14, form-data@0.2.0, tough-cookie@2.0.0, bl@0.9.4, http-signature@0.10.1, hawk@2.3.1)\\nbower globalize#^1.0.0                   install globalize#1.0.0\\nbower cldrjs#0.4.1                       install cldrjs#0.4.1\\nbower cldr-data#>=25                     install cldr-data#27.0.3\\nbower                                postinstall node ./node_modules/cldr-data-downloader/bin/download.js -i bower_components/cldr-data/index.json -o bower_components/cldr-data/\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-core/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-dates-modern/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-buddhist-modern/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-chinese-modern/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-coptic-modern/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-dangi-modern/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-ethiopic-modern/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-hebrew-modern/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-indian-modern/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-islamic-modern/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-japanese-modern/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-persian-modern/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-roc-modern/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-localenames-modern/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-misc-modern/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-numbers-modern/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-segments-modern/archive/27.0.3.zip`\\nbower                                postinstall GET `https://github.com/unicode-cldr/cldr-units-modern/archive/27.0.3.zip`\\nbower                                postinstall Received 28728K total.\\nbower                                postinstall Received 28753K total.\\nbower                                postinstall Unpacking it into `./bower_components\\\\cldr-data`\\n\\nglobalize#1.0.0 bower_components\\\\globalize\\n\u251c\u2500\u2500 cldr-data#27.0.3\\n\u2514\u2500\u2500 cldrjs#0.4.1\\n\\ncldrjs#0.4.1 bower_components\\\\cldrjs\\n\u2514\u2500\u2500 cldr-data#27.0.3\\n\\ncldr-data#27.0.3 bower_components\\\\cldr-data\\n```\\n\\nThat\'s right - I\'m golden. And if I didn\'t want to do that I could have gone straight to the command line and entered this: (look familiar?)\\n\\n```\\nnpm install cldr-data-downloader@0.2.x\\nnode ./node_modules/cldr-data-downloader/bin/download.js -i bower_components/cldr-data/index.json -o bower_components/cldr-data/\\n```\\n\\n## Some bitching and moaning.\\n\\nIf, like me, you were a regular user of Globalize 0.1.x then you know that you needed very little to get going. As you can see from our example you just serve up `Globalize.js` and the culture files you are interested in (eg `globalize.culture.de-DE.js`). That\'s it - you have all you need; job\'s a good\'un. This is all very convenient and entirely lovely.\\n\\nGlobalize 1.x has a different approach and one that (I have to be honest) I\'m not entirely on board with. The thing that you need to know about the new Globalize is that _nothing comes for free_. It\'s been completely modularised and [you have to include extra libraries depending on the functionality you require.](https://github.com/jquery/globalize#pick-the-modules-you-need) On top of that you then have to work out the [portions of the cldr data that you require for those modules](https://github.com/jquery/globalize#2-cldr-content) and supply them. This means that getting up and running with Globalize 1.x is much harder. Frankly I think it\'s a little painful.\\n\\nI realise this is a little [\\"Who moved my cheese\\"](https://en.wikipedia.org/wiki/Who_Moved_My_Cheese%3F). I\'ll get over it. I do actually see the logic of this. It is certainly good that the culture date is not frozen in aspic but will evolve as the world does. But it\'s undeniable that in our brave new world Globalize is no longer a doddle to pick up. Or at least right now.\\n\\n## Take the modules and run\\n\\nSo. What do we actually need? Well I\'ve consulted the [documentation](https://github.com/jquery/globalize#pick-the-modules-you-need) and I think I\'m clear. Our simple demo cares about dates and numbers. So I\'m going to guess that means I need:\\n\\n- [`globalize.js`](https://github.com/jquery/globalize#core-module)\\n- [`globalize/date.js`](https://github.com/jquery/globalize#date-module)\\n- [`globalize/number.js`](https://github.com/jquery/globalize#number-module)\\n\\nOn top of that I\'m also going to need the various cldr dependencies too. That\'s not all. Given that I\'ve decided which modules I will use I now need to acquire the associated cldr data. According to the docs [here](https://github.com/jquery/globalize#2-cldr-content) we\'re going to need:\\n\\n- `cldr/supplemental/likelySubtags.json`\\n- `cldr/main/<i>locale</i>/ca-gregorian.json`\\n- `cldr/main/<i>locale</i>/timeZoneNames.json`\\n- `cldr/supplemental/timeData.json`\\n- `cldr/supplemental/weekData.json`\\n- `cldr/main/locale/numbers.json`\\n- `cldr/supplemental/numberingSystems.json`\\n\\nFiguring that all out felt like really hard work. But I think that now we\'re ready to do the actual migration.\\n\\n### Update 30/08/2015: Globalize \xb7 So What\'cha Want\\n\\nTo make working out what you need when using Globalize I\'ve built [Globalize \xb7 So What\'cha Want](http://johnnyreilly.github.io/globalize-so-what-cha-want/). You\'re so very welcome.\\n\\n## The Actual Migration\\n\\nTo do this I\'m going to lean heavily upon [an example put together by Rafael](https://github.com/jquery/globalize/blob/master/examples/plain-javascript/index.html). The migrated code looks like this:\\n\\n```html\\n<html>\\n  <head>\\n    <title>Globalize demo</title>\\n    <link\\n      href=\\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css\\"\\n      rel=\\"stylesheet\\"\\n    />\\n  </head>\\n  <body>\\n    <div class=\\"container-fluid\\">\\n      <h4>Globalize demo for the <em id=\\"locale\\"></em> culture / locale</h4>\\n      <p>\\n        This is a the number <strong id=\\"number\\"></strong> formatted by\\n        Globalize: <strong id=\\"numberFormatted\\"></strong>\\n      </p>\\n      <p>\\n        This is a the number <strong id=\\"date\\"></strong> formatted by Globalize:\\n        <strong id=\\"dateFormatted\\"></strong>\\n      </p>\\n    </div>\\n\\n    \x3c!-- First, we load Globalize\'s dependencies (`cldrjs` and its supplemental module). --\x3e\\n    <script src=\\"bower_components/cldrjs/dist/cldr.js\\"><\/script>\\n    <script src=\\"bower_components/cldrjs/dist/cldr/event.js\\"><\/script>\\n    <script src=\\"bower_components/cldrjs/dist/cldr/supplemental.js\\"><\/script>\\n\\n    \x3c!-- Next, we load Globalize and its modules. --\x3e\\n    <script src=\\"bower_components/globalize/dist/globalize.js\\"><\/script>\\n    <script src=\\"bower_components/globalize/dist/globalize/number.js\\"><\/script>\\n\\n    \x3c!-- Load after globalize/number.js --\x3e\\n    <script src=\\"bower_components/globalize/dist/globalize/date.js\\"><\/script>\\n\\n    <script>\\n      var locale = \'de\';\\n\\n      Promise.all([\\n        // Core\\n        fetch(\'bower_components/cldr-data/supplemental/likelySubtags.json\'),\\n\\n        // Date\\n        fetch(\\n          \'bower_components/cldr-data/main/\' + locale + \'/ca-gregorian.json\'\\n        ),\\n        fetch(\\n          \'bower_components/cldr-data/main/\' + locale + \'/timeZoneNames.json\'\\n        ),\\n        fetch(\'bower_components/cldr-data/supplemental/timeData.json\'),\\n        fetch(\'bower_components/cldr-data/supplemental/weekData.json\'),\\n\\n        // Number\\n        fetch(\'bower_components/cldr-data/main/\' + locale + \'/numbers.json\'),\\n        fetch(\'bower_components/cldr-data/supplemental/numberingSystems.json\'),\\n      ])\\n        .then(function (responses) {\\n          return Promise.all(\\n            responses.map(function (response) {\\n              return response.json();\\n            })\\n          );\\n        })\\n        .then(Globalize.load)\\n        .then(function () {\\n          var number = 12345.67;\\n          var date = new Date(2012, 5, 15);\\n\\n          var globalize = Globalize(locale);\\n          document.querySelector(\'#locale\').innerText = locale;\\n          document.querySelector(\'#number\').innerText = number;\\n          document.querySelector(\'#date\').innerText = date;\\n          document.querySelector(\'#numberFormatted\').innerText =\\n            globalize.formatNumber(number, {\\n              minimumFractionDigits: 2,\\n              useGrouping: true,\\n            });\\n          document.querySelector(\'#dateFormatted\').innerText =\\n            globalize.formatDate(date, {\\n              date: \'medium\',\\n            });\\n        });\\n    <\/script>\\n  </body>\\n</html>\\n```\\n\\nBy the way, I\'m using [fetch](http://jakearchibald.com/2015/thats-so-fetch/) and [promises](http://www.html5rocks.com/en/tutorials/es6/promises/) to load the cldr-data. This isn\'t mandatory - I use it because Chrome lets me. (I\'m so bleeding edge.) Some standard jQuery ajax calls would do just as well. There\'s an example of that approach [here](https://github.com/jquery/globalize/blob/master/doc/cldr/index.md#how-do-i-load-cldr-data-into-globalize).\\n\\n## Observations\\n\\nWe\'ve gone from not a lot of code to... well, more than a little. A medium amount. Almost all of that extra code relates to getting Globalize 1.x to spin up so it\'s ready to work. We\'ve also gone from 2 HTTP requests to 13 which is unlucky for you. 6 of them took place via ajax after the page had loaded. It\'s worth noting that we\'re not even loading all of Globalize either. On top of that there\'s the old order-of-loading shenanigans to deal with. All of these can be mitigated by introducing a custom build step of your own to concatenate and minify the associated cldr / Globalize files.\\n\\nLoading the data via ajax isn\'t mandatory by the way. If you wanted to you could create your own style of `globalize.culture.de.js` files which would allow you load the page without recourse to post-page load HTTP requests. Something like this Gulp task I\'ve knocked up would do the trick:\\n\\n```js\\ngulp.task(\'make-globalize-culture-de-js\', function () {\\n  var locale = \'de\';\\n  var jsonWeNeed = [\\n    require(\'./bower_components/cldr-data/supplemental/likelySubtags.json\'),\\n    require(\'./bower_components/cldr-data/main/\' +\\n      locale +\\n      \'/ca-gregorian.json\'),\\n    require(\'./bower_components/cldr-data/main/\' +\\n      locale +\\n      \'/timeZoneNames.json\'),\\n    require(\'./bower_components/cldr-data/supplemental/timeData.json\'),\\n    require(\'./bower_components/cldr-data/supplemental/weekData.json\'),\\n    require(\'./bower_components/cldr-data/main/\' + locale + \'/numbers.json\'),\\n    require(\'./bower_components/cldr-data/supplemental/numberingSystems.json\'),\\n  ];\\n\\n  var jsonStringWithLoad =\\n    \'Globalize.load(\' +\\n    jsonWeNeed\\n      .map(function (json) {\\n        return JSON.stringify(json);\\n      })\\n      .join(\', \') +\\n    \');\';\\n\\n  var fs = require(\'fs\');\\n  fs.writeFile(\\n    \'./globalize.culture.\' + locale + \'.js\',\\n    jsonStringWithLoad,\\n    function (err) {\\n      if (err) {\\n        console.log(err);\\n      } else {\\n        console.log(\'The file was created!\');\\n      }\\n    }\\n  );\\n});\\n```\\n\\nThe above is standard node/io type code by the way; just take the contents of the function and run in node and you should be fine. If you do use this approach then you\'re very much back to the simplicity of Globalize 0.1.x\'s approach.\\n\\nAnd here is the page in all its post migration glory:\\n\\n![](Screenshot-2015-07-30-20.21.19.png)\\n\\nIt looks exactly the same except \'de-DE\' has become simply \'de\' (since that\'s how the cldr rolls).\\n\\nThe migrated code is [there for the taking](https://github.com/johnnyreilly/globalize-migration). Make sure you remember to `bower install` \\\\- and you\'ll need to host the demo on a simple server since it makes ajax calls.\\n\\nBefore I finish off I wanted to say \\"well done!\\" to all the people who have worked on Globalize. It\'s an important project and I do apologise for my being a little critical of it here. I should say that I think it\'s just the getting started that\'s hard. Once you get over that hurdle you\'ll be fine. Hopefully this post will help people do just that. Pip, pip!"},{"id":"/2015/06/29/npm-please-stop-hurting-visual-studio","metadata":{"permalink":"/2015/06/29/npm-please-stop-hurting-visual-studio","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2015-06-29-npm-please-stop-hurting-visual-studio/index.md","source":"@site/blog/2015-06-29-npm-please-stop-hurting-visual-studio/index.md","title":"npm please stop hurting Visual Studio","description":"I don\'t know about you but I personally feel that the following sentence may well be the saddest in the English language:","date":"2015-06-29T00:00:00.000Z","formattedDate":"June 29, 2015","tags":[{"label":"rimraf","permalink":"/tags/rimraf"},{"label":"npm","permalink":"/tags/npm"},{"label":"long paths","permalink":"/tags/long-paths"},{"label":"Windows","permalink":"/tags/windows"}],"readingTime":4.485,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"npm please stop hurting Visual Studio","authors":"johnnyreilly","tags":["rimraf","npm","long paths","Windows"],"hide_table_of_contents":false},"prevItem":{"title":"Upgrading to Globalize 1.x for Dummies","permalink":"/2015/07/30/upgrading-to-globalize-1x-for-dummies"},"nextItem":{"title":"Back to the Future with Code First Migrations","permalink":"/2015/06/19/Back-to-the-Future-with-Code-First-Migrations"}},"content":"I don\'t know about you but I personally feel that the following sentence may well be the saddest in the English language:\\n\\n`2&gt;ASPNETCOMPILER : error ASPRUNTIME: The specified path, file name, or both are too long. The fully qualified file name must be less than 260 characters, and the directory name must be less than 248 characters.`\\n\\nThe message above would suggest there is some kind of ASP.Net issue going on. There isn\'t - the problem actually lies with Windows. It\'s [not the first time it\'s come up](http://blog.icanmakethiswork.io/2014/12/gulp-npm-long-paths-and-visual-studio-fight.html) but for those of you not aware there is something you need to know about Windows: _It handles long paths badly._\\n\\nThere\'s a number of caveats which people may attach the above sentence. But essentially what I have said is true. And it becomes brutally apparent to you the moment you start using a few node / npm powered tools in your workflow. You will likely see that horrible message and you won\'t be able to get much further forward. Sigh. I thought this was the future...\\n\\nThis post is about how to deal with the long path issue when using npm with Visual Studio. This should very much be a short term workaround as [npm 3.0](https://github.com/npm/npm/releases/tag/v3.0.0) is planned to make long paths with npm a thing of the past. But until that golden dawn....\\n\\n## The Latest Infraction\\n\\nI\'m a big fan of Gulp and Bower. They rock. [Steve Cadwallader](https://twitter.com/codecadwallader) wrote an excellent blog post about [integrating Gulp into your Visual Studio build](http://www.codecadwallader.com/2015/03/15/integrating-gulp-into-your-tfs-builds-and-web-deploy/). Essentially the Gist of his post is this: forget using [Task Runner Explorer](https://visualstudiogallery.msdn.microsoft.com/8e1b4368-4afb-467a-bc13-9650572db708) to trigger your Gulp / Grunt jobs. No, actually plug it into the build process by tweaking your `.csproj` file. The first time I used this approach it was a dream come true. It just worked and I was a very happy man.\\n\\nSince this approach was so marvellous I took a look at the demo / docs part of [jQuery Validation Unobtrusive Native](https://github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native) with a view to applying it there. I originally wrote this back in 2013 and at the time used NuGet for both server and client side package management. I decided to migrate it to use Bower for the client side packages (which I planned to combine with a Gulp script which was going to pull out the required JS / CSS etc as needed). However it wasn\'t the plain sailing I\'d imagined. The actual switchover from NuGet to Bower was simple. Just a case of removing NuGet packages and adding their associated Bower counterpart. The problem came when the migration was done and I hit \\"compile\\". That\'s when I got to see `2&gt;ASPNETCOMPILER : error ASPRUNTIME: The specified path, file name, or both are too long...` etc\\n\\nFor reasons that I don\'t fully understand, Visual Studio is really upset by the presence in the project structure of one almighty long path. Oddly enough, not a path that\'s actually part of the Visual Studio project in question at all. Rather one that has come along as a result of our Gulp / Bower / npm shenanigans. Quick as a flash, I whipped out Daniel Schroeder\'s [Path Length Checker](https://pathlengthchecker.codeplex.com/) to see where the problem lay:\\n\\n![](bower-with-the-long-paths.png)\\n\\nAnd lo, the fault lay with Bower. Poor show, Bower, poor show.\\n\\n## rimraf to the Rescue\\n\\n[rimraf](https://github.com/isaacs/rimraf) is \\"the [UNIX command](<https://en.wikipedia.org/wiki/Rm_(Unix)>)`rm -rf` for node\\". (By the way, what is it with node and the pathological hatred of capital letters?)\\n\\nWhat this means is: rimraf can delete. Properly. So let\'s get it: `npm install -g rimraf`. Then at any time at the command line we can dispose of a long path in 2 shakes of lamb\'s tail.\\n\\nIn my current situation the contents of the `node_modules` folder is causing me heartache. But with rimraf in play I can get rid of it with the magic words: `rimraf ./node_modules`. Alakazam! So let\'s poke this command into the extra commands that I\'ve already shoplifted from Steve\'s blog post. I\'ll end up with the following section of XML at the end of my `.csproj`:\\n\\n```xml\\n<PropertyGroup>\\n    <CompileDependsOn>\\n      $(CompileDependsOn);\\n      GulpBuild;\\n    </CompileDependsOn>\\n    <CleanDependsOn>\\n      $(CleanDependsOn);\\n      GulpClean\\n    </CleanDependsOn>\\n    <CopyAllFilesToSingleFolderForPackageDependsOn>\\n      CollectGulpOutput;\\n      $(CopyAllFilesToSingleFolderForPackageDependsOn);\\n    </CopyAllFilesToSingleFolderForPackageDependsOn>\\n    <CopyAllFilesToSingleFolderForMsdeployDependsOn>\\n      CollectGulpOutput;\\n      $(CopyAllFilesToSingleFolderForPackageDependsOn);\\n    </CopyAllFilesToSingleFolderForMsdeployDependsOn>\\n  </PropertyGroup>\\n  <Target Name=\\"GulpBuild\\">\\n    <Exec Command=\\"npm install\\" />\\n    <Exec Command=\\"bower install\\" />\\n    <Exec Command=\\"gulp\\" />\\n    <Exec Command=\\"rimraf ./node_modules\\" />\\n  </Target>\\n  <Target Name=\\"GulpClean\\">\\n    <Exec Command=\\"npm install\\" />\\n    <Exec Command=\\"gulp clean\\" />\\n    <Exec Command=\\"rimraf ./node_modules\\" />\\n  </Target>\\n  <Target Name=\\"CollectGulpOutput\\">\\n    <ItemGroup>\\n      <_CustomFiles Include=\\"build\\\\**\\\\*\\" />\\n      <FilesForPackagingFromProject Include=\\"%(_CustomFiles.Identity)\\">\\n        <DestinationRelativePath>build\\\\%(RecursiveDir)%(Filename)%(Extension)</DestinationRelativePath>\\n      </FilesForPackagingFromProject>\\n    </ItemGroup>\\n    <Message Text=\\"CollectGulpOutput list: %(_CustomFiles.Identity)\\" />\\n  </Target>\\n```\\n\\nSo let\'s focus on the important bits in the `GulpBuild` target:\\n\\n- `&lt;Exec Command=\\"npm install\\" /&gt;` \\\\- install the node packages our project uses as specified in `package.json`. This will include Gulp and Bower. The latter package is going to contain super-long, Windows wrecking paths.\\n- `&lt;Exec Command=\\"bower install\\" /&gt;` \\\\- install the bower packages specified in `bower.json` using Bower (which was installed by npm just now).\\n- `&lt;Exec Command=\\"gulp\\" /&gt;` \\\\- do a little dance, make a little love, copy a few files, get down tonight.\\n- `&lt;Exec Command=\\"rimraf ./node_modules\\" /&gt;` \\\\- remove the `node_modules` folder populated by the `npm install` command.\\n\\nWith that addition of `rimraf ./node_modules` to the build phase the problem goes away. During each build a big, big Windows path is being constructed but then it\'s wiped again before it has chance to upset anyone. I\'ve also added the same to the `GulpClean` target.\\n\\nYou are very welcome."},{"id":"/2015/06/19/Back-to-the-Future-with-Code-First-Migrations","metadata":{"permalink":"/2015/06/19/Back-to-the-Future-with-Code-First-Migrations","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2015-06-19-Back-to-the-Future-with-Code-First-Migrations/index.md","source":"@site/blog/2015-06-19-Back-to-the-Future-with-Code-First-Migrations/index.md","title":"Back to the Future with Code First Migrations","description":"Code First Migrations. They look a little like this in Visual Studio:","date":"2015-06-19T00:00:00.000Z","formattedDate":"June 19, 2015","tags":[{"label":"Emmett Brown","permalink":"/tags/emmett-brown"},{"label":"Entity Framework","permalink":"/tags/entity-framework"},{"label":"Code First Migrations","permalink":"/tags/code-first-migrations"}],"readingTime":2.26,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Back to the Future with Code First Migrations","authors":"johnnyreilly","tags":["Emmett Brown","Entity Framework","Code First Migrations"],"hide_table_of_contents":false},"prevItem":{"title":"npm please stop hurting Visual Studio","permalink":"/2015/06/29/npm-please-stop-hurting-visual-studio"},"nextItem":{"title":"Angular UI Bootstrap Datepicker Weirdness","permalink":"/2015/05/23/angular-ui-bootstrap-datepicker-weirdness"}},"content":"Code First Migrations. They look a little like this in Visual Studio:\\n\\n![](Migrations.png)\\n\\nThe thing I want you to notice about the image above is not the pithily named migrations. It isn\'t the natty opacity on everything but the migration files (which I can assure you took me to the very limits of my [GIMP](http://www.gimp.org/) expertise). No, whilst exciting in themselves what I want you to think about is _the order in which migrations are applied_. Essentially how the `__MigrationHistory` table in SQL Server ends up being populated in this manner:\\n\\n![](MigrationHistory.png)\\n\\nBecause, myself, I didn\'t really think about this until it came time for me to try and change the ordering of some migrations manually. Do you know how migrations end up the order they do? I bet you don\'t. But either way, let\'s watch and see what happens to the pre-enlightenment me as I attempt to take a migration which appears _before_ a migration I have created locally and move it to _after_ that same migration.\\n\\n## Great Scott! It\'s clearly filename driven\\n\\nThat\'s right - it\'s blindingly obvious to me. All I need do is take the migration I want to move forwards in time and rename it in Visual Studio. So take our old migration (\\"2014 is so pass\xe9 darling\\"):\\n\\n![](Screenshot-2015-06-19-13.07.50.png)\\n\\nAnd rename it to make it new and shiny (\\"2015! Gorgeous - I love it sweetie!\\"):\\n\\n![](Screenshot-2015-06-19-13.08.46.png)\\n\\nPerfection right? Wrong! What you\'ve done makes not the slightest jot of difference.\\n\\n## Whoa, this is heavy! Gimme the project file\\n\\nHow could I be so dim? I mean it makes perfect sense - before the days of [TypeScript\'s tsconfig.json ](http://blog.icanmakethiswork.io/2015/02/hey-tsconfigjson-where-have-you-been.html) the default ordering of `*.ts` files being passed to the TypeScript compiler was determined by the ordering of the `*.ts` files in the `.csproj` file. It must be the same for Code First Migrations.\\n\\nSo, simply spin up [Notepad++](https://notepad-plus-plus.org/) and let\'s play hack the XML until each file is referenced in the required order.\\n\\nWell, I\'m glad we sorted that out. A quick test to reassure myself of my astuteness. Drum roll.... Fail!! Things are just as they were. Shame on you John Reilly, shame on you.\\n\\n## Designer.cs... Your kids are gonna love it\\n\\n![](Screenshot-2015-06-19-13.35.40.png)\\n\\nI want you to look very carefully at this and tell me what you see. We\'re looking at the mysterious `201508121401253_AddSagacityToSage.Designer.cs` file that sits underneath the main `201508121401253_AddSagacityToSage.cs` file. What could it be.... Give in?\\n\\nThe `IMigrationMetadata.Id` property is returning `<u>201408121401253</u>_AddSagacityToSage`. That is the _old_ date! Remember? The pass\xe9 one. If you change that property to line up with the file name change you\'re done. It works.\\n\\n![](where-were-going.jpg)\\n\\nLet\'s say it together: \\"Automatic Migrations? Where we\'re going, we don\'t need Automatic Migrations.\\""},{"id":"/2015/05/23/angular-ui-bootstrap-datepicker-weirdness","metadata":{"permalink":"/2015/05/23/angular-ui-bootstrap-datepicker-weirdness","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2015-05-23-angular-ui-bootstrap-datepicker-weirdness/index.md","source":"@site/blog/2015-05-23-angular-ui-bootstrap-datepicker-weirdness/index.md","title":"Angular UI Bootstrap Datepicker Weirdness","description":"The Angular UI Bootstrap Datepicker is fan-dabby-dozy. But it has a ... pecularity. You can use the picker like this:","date":"2015-05-23T00:00:00.000Z","formattedDate":"May 23, 2015","tags":[{"label":"Angular UI Bootstrap Datepicker","permalink":"/tags/angular-ui-bootstrap-datepicker"}],"readingTime":2.48,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Angular UI Bootstrap Datepicker Weirdness","authors":"johnnyreilly","tags":["Angular UI Bootstrap Datepicker"],"hide_table_of_contents":false},"prevItem":{"title":"Back to the Future with Code First Migrations","permalink":"/2015/06/19/Back-to-the-Future-with-Code-First-Migrations"},"nextItem":{"title":"NgValidationFor Baby Steps","permalink":"/2015/05/11/ngvalidationfor-baby-steps"}},"content":"The [Angular UI Bootstrap Datepicker](https://angular-ui.github.io/bootstrap/#/datepicker) is fan-dabby-dozy. But it has a ... pecularity. You can use the picker like this:\\n\\n```html\\n<div ng-app=\\"peskyDatepicker\\">\\n  <div ng-controller=\\"DatepickerDemoCtrl as vm\\">\\n    <input\\n      type=\\"text\\"\\n      class=\\"form-control\\"\\n      datepicker-popup=\\"mediumDate\\"\\n      is-open=\\"vm.valuationDatePickerIsOpen\\"\\n      ng-click=\\"vm.valuationDatePickerOpen()\\"\\n      ng-model=\\"vm.valuationDate\\"\\n    />\\n  </div>\\n</div>\\n```\\n\\n```js\\nangular\\n  .module(\'peskyDatepicker\', [\'ui.bootstrap\'])\\n  .controller(\'DatepickerDemoCtrl\', [\\n    function () {\\n      var vm = this;\\n\\n      vm.valuationDate = new Date();\\n      vm.valuationDatePickerIsOpen = false;\\n\\n      vm.valuationDatePickerOpen = function () {\\n        this.valuationDatePickerIsOpen = true;\\n      };\\n    },\\n  ]);\\n```\\n\\nThe above code produces a textbox which, when clicked upon, renders the datepicker popup (which vanishes upon date selection). This works because the `ng-click` directive calls the `valuationDatePickerOpen` function on the controller which sets the `valuationDatePickerIsOpen` property to be `true` and that property happens to be bound to the `is-open` attribute. Your knee bone connected to your thigh bone, Your thigh bone connected to your hip bone... This makes sense. This works. Great.\\n\\nBut I want something a little prettier - I want to use the lovely calendar glyph to trigger the datepicker popup like in the docs. That should be really easy right? I just tweak the HTML to add a calendar button and the associated `ng-click=\\"vm.valuationDatePickerOpen()\\"`:\\n\\n```html\\n<div ng-app=\\"peskyDatepicker\\">\\n  <div ng-controller=\\"DatepickerDemoCtrl as vm\\">\\n    <p class=\\"input-group\\">\\n      <input\\n        type=\\"text\\"\\n        class=\\"form-control\\"\\n        datepicker-popup=\\"mediumDate\\"\\n        is-open=\\"vm.valuationDatePickerIsOpen\\"\\n        ng-click=\\"vm.valuationDatePickerOpen()\\"\\n        ng-model=\\"vm.valuationDate\\"\\n      />\\n      <span class=\\"input-group-btn\\">\\n        <button\\n          type=\\"button\\"\\n          class=\\"btn btn-default\\"\\n          ng-click=\\"vm.valuationDatePickerOpen()\\"\\n        >\\n          <i class=\\"glyphicon glyphicon-calendar\\"></i>\\n        </button>\\n      </span>\\n    </p>\\n  </div>\\n</div>\\n```\\n\\nMiraculously, this _doesn\'t_ work. Which is strange - I mean it ought to... The same `ng-click` directive is sat on our new calendar button as is in place on the datepicker itself. So what\'s happening? Well let\'s do some investigation. If you take a look at the docs you\'ll see that their example with the calendar glyph is subtly different to our own. Namely, when the opener function is invoked, the official docs pass along `$event`. To what end? Well, the docs opener function does something that our own does not. This:\\n\\n```js\\n$scope.open = function ($event) {\\n  $event.preventDefault();\\n  $event.stopPropagation();\\n\\n  $scope.opened = true;\\n};\\n```\\n\\nIgnore all the `$scope` malarkey - I want you to pay attention to what is happening with `$event`. `preventDefault` and `stopPropogation` are being called. This is probably relevant.\\n\\nI decided to do a little experimentation. I created a Plunk which demonstrates the datepicker and uses `$watch` to track what happens to `valuationDatePickerIsOpen`. The Plunk featured 2 calendar glyphs - the left one doesn\'t pass along `$event` to `valuationDatePickerOpen` when it is clicked and the right one does. When `$event` is passed we call `preventDefault` and `stopPropogation`.\\n\\n<iframe src=\\"https://embed.plnkr.co/dJyF531w0QRGiAScRf15/preview\\" width=\\"100%\\" height=\\"450\\"></iframe>\\n\\nAfter a little experimentation of my own I discovered that calling `$event.stopPropogation()` is the magic bullet. Without that in place `valuationDatePickerIsOpen` gets set to `true` and then immediately back to `false` again. I do not know why. There may be an entirely sane reason for this - if so then please do post a comment and let me know. It wouldn\'t hurt for the Angular UI Bootstrap Datepicker docs to mention this. [Perhaps it\'s time to submit a PR....](https://github.com/angular-ui/bootstrap/issues/3705)"},{"id":"/2015/05/11/ngvalidationfor-baby-steps","metadata":{"permalink":"/2015/05/11/ngvalidationfor-baby-steps","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2015-05-11-ngvalidationfor-baby-steps/index.md","source":"@site/blog/2015-05-11-ngvalidationfor-baby-steps/index.md","title":"NgValidationFor Baby Steps","description":"I thought as I start the NgValidationFor project I\'d journal my progress. I\'m writing this with someone particular in mind welcome!","date":"2015-05-11T00:00:00.000Z","formattedDate":"May 11, 2015","tags":[{"label":"asp.net mvc","permalink":"/tags/asp-net-mvc"},{"label":"AngularJS","permalink":"/tags/angular-js"},{"label":"NgValidationFor","permalink":"/tags/ng-validation-for"}],"readingTime":3.38,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"NgValidationFor Baby Steps","authors":"johnnyreilly","tags":["asp.net mvc","AngularJS","NgValidationFor"],"hide_table_of_contents":false},"prevItem":{"title":"Angular UI Bootstrap Datepicker Weirdness","permalink":"/2015/05/23/angular-ui-bootstrap-datepicker-weirdness"},"nextItem":{"title":"A tale of Angular, html5mode, ASP.Net MVC and ASP.Net Web API","permalink":"/2015/05/05/a-tale-of-angular-html5mode-aspnet-mvc"}},"content":"I thought as I start the [NgValidationFor project](http://blog.icanmakethiswork.io/2015/04/tonight-ill-start-open-source-project.html) I\'d journal my progress. I\'m writing this with someone particular in mind: me. Specifically, me in 2 years who will no doubt wonder why I made some of the choices I did. Everyone else, move along now - nothing to see. Unless the inner workings of someone else\'s mind are interesting to you... In which case: welcome!\\n\\n## Getting up and running\\n\\nI\'ve got a project on [GitHub](https://github.com/johnnyreilly/NgValidationFor) and I\'m starting to think about implementations. One thing that bit me on [jVUN](http://johnnyreilly.github.io/jQuery.Validation.Unobtrusive.Native/) was being tied to a specific version of ASP.Net MVC. For each major release of ASP.Net MVC I needed separate builds / NuGet packages and the like. A pain. Particularly when it came to bug fixes for prior versions - the breaking changes with each version of MVC meant far more work was required when it came to shipping fixes for MVC 4 / MVC 3.\\n\\nSo with that in mind I\'m going to try and limit my dependencies. I\'m not saying I will never depend upon ASP.Net MVC - I may if I think it becomes useful to give the users a nicer API or if there\'s another compelling reason. But to start with I\'m just going to focus on the translation of data annotations to Angular validation directive attributes.\\n\\nTo that end I\'m going to begin with just a class library and an associated test project. I\'m going to try and minimise the dependencies that NgValidationFor has. At least initially I may even see if I can sensibly avoid depending on `System.Web` (mindful of the upcoming ASP.Net 5 changes). Let\'s see.\\n\\nA little time passes.......\\n\\n## So what have we got?\\n\\nMy first efforts have resulted in the implementation of the `<a href=\\"https://msdn.microsoft.com/en-us/library/system.componentmodel.dataannotations.requiredattribute(v=vs.110).aspx\\">RequiredAttribute</a>`. This is the [code right now](https://github.com/johnnyreilly/NgValidationFor/tree/6cf862a7638d3ed933cd0e075a1807b1414847da). It\'s made up of:\\n\\n1. NgValidationFor.Core - the core part of the project which converts data annotations into AngularJS 1.x validation directive attributes.\\n2. NgValidationFor.Core.UnitTests - the unit tests for the core\\n3. NgValidationFor.Documentation - this is an ASP.Net MVC project which will become a documentation site for NgValidationFor. It also doubles as a way for me to try out NgValidationFor.\\n4. NgValidationFor.Documentation.UnitTests - unit tests for the documentation (there\'s none yet as I\'m still spiking - but when I\'m a little clearer, they will be)\\n\\nHow can it be used? Well fairly easily. Take this simple model:\\n\\n```cs\\nusing System.ComponentModel.DataAnnotations;\\n\\nnamespace NgValidationFor.Documentation.Models\\n{\\n    public class RequiredDemoModel\\n    {\\n        [Required]\\n        public string RequiredField { get; set; }\\n    }\\n}\\n```\\n\\nWhen used in an MVC View for which `RequiredDemoModel` is the Model, NgValiditionFor can be used thusly:\\n\\n```html\\n@using NgValidationFor.Core @using NgValidationFor.Documentation.Models @model\\nRequiredDemoModel <input type=\\"text\\" name=\\"userName\\" ng-model=\\"user.name\\"\\n@Html.Raw(Model.GetAttributes(x => Model.RequiredField)) >\\n```\\n\\nWhich results in this HTML:\\n\\n```html\\n<input type=\\"text\\" name=\\"userName\\" ng-model=\\"user.name\\" required=\\"required\\" />\\n```\\n\\nTada!!!! It works.\\n\\n## So what now?\\n\\nYes it works, but I\'m not going to pretend it\'s pretty. I don\'t like having to wrap the usage of NgValidationFor with `Html.Raw(...)`. I\'m having to do that because `GetAttributes` returns a `string`. This string is then HTML encoded by MVC. To avoid my quotation marks turning into `&amp;quot;` I need to actually be exposing an `<a href=\\"https://msdn.microsoft.com/en-us/library/system.web.ihtmlstring(v=vs.110).aspx\\">IHtmlString</a>`. So I\'m going to need to depend upon `System.Web`. That\'s not so bad - at least I\'m not tied to a specific MVC version.\\n\\nI\'m not too keen on the implementation I\'ve come up with for NgValidationFor either. It\'s a single static method at the minute which does everything. It breaks the [Single Responsibility Priniciple](https://en.wikipedia.org/wiki/Single_responsibility_principle) and the [Open/Closed Principle](https://en.wikipedia.org/wiki/Open/closed_principle). I need to take a look at that - I want people to be able to extend this and I need to think about a good and simple way to achieve that.\\n\\nFinally, usage. `Model.GetAttributes(x =&gt; Model.RequiredField)` feels wrong to me. I think I\'m happy with having this used as an extension method but it needs to be clearer what\'s happening. Perhaps `Model.NgValidationFor(x =&gt; Model.RequiredField)` would be better. I need to try a few things out and come up with a nicer way to use NgValidationFor."},{"id":"/2015/05/05/a-tale-of-angular-html5mode-aspnet-mvc","metadata":{"permalink":"/2015/05/05/a-tale-of-angular-html5mode-aspnet-mvc","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2015-05-05-a-tale-of-angular-html5mode-aspnet-mvc/index.md","source":"@site/blog/2015-05-05-a-tale-of-angular-html5mode-aspnet-mvc/index.md","title":"A tale of Angular, html5mode, ASP.Net MVC and ASP.Net Web API","description":"So. You want to kick hash based routing to the kerb. You want real URLs. You\'ve read the HTML5 mode section of the Angular $location docs and you\'re good to go. It\'s just a matter of dropping $locationProvider.html5Mode(true) into your app initialisation right?","date":"2015-05-05T00:00:00.000Z","formattedDate":"May 5, 2015","tags":[{"label":"asp.net mvc","permalink":"/tags/asp-net-mvc"},{"label":"asp.net","permalink":"/tags/asp-net"},{"label":"html5mode","permalink":"/tags/html-5-mode"},{"label":"AngularJS","permalink":"/tags/angular-js"},{"label":"ASP.Net Web API","permalink":"/tags/asp-net-web-api"}],"readingTime":2.945,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"A tale of Angular, html5mode, ASP.Net MVC and ASP.Net Web API","authors":"johnnyreilly","tags":["asp.net mvc","asp.net","html5mode","AngularJS","ASP.Net Web API"],"hide_table_of_contents":false},"prevItem":{"title":"NgValidationFor Baby Steps","permalink":"/2015/05/11/ngvalidationfor-baby-steps"},"nextItem":{"title":"Tonight I\'ll Start an Open Source Project...","permalink":"/2015/04/24/tonight-ill-start-open-source-project"}},"content":"So. You want to kick hash based routing to the kerb. You want _real_ URLs. You\'ve read the HTML5 mode section of the [Angular $location docs](https://docs.angularjs.org/guide/$location) and you\'re good to go. It\'s just a matter of dropping `$locationProvider.html5Mode(true)` into your app initialisation right?\\n\\nWrong.\\n\\nYou want your URLs to be shareable. If, when you copy the URL out of your browser and send it someone else, they do not get taken to the same position in the application as you do then I\'ve got news for you: THAT\'S NOT REALLY A URL. And just using `$locationProvider.html5Mode(true)` has done nothing useful for you. You want to ensure that, if the URL entered in the browser does not relate to a specific server-side end-point, the self-same HTML root page is _always_ served up. Then Angular can load the correct resources for the URL you have entered and get you to the required state.\\n\\nThere are tips to be found in Angular UI\'s [How to: Configure your server to work with html5Mode](https://github.com/angular-ui/ui-router/wiki/Frequently-Asked-Questions#how-to-configure-your-server-to-work-with-html5mode) doc. However they required a little extra fiddling to get my ASP.Net back end working quite as I wanted. To save you pain, here are my cultural learnings.\\n\\n## ASP.Net MVC\\n\\nI had an ASP.Net MVC app which I wanted to use `html5mode` with. To do this is simply a matter of tweaking your `RouteConfig.cs` like so:\\n\\n```cs\\npublic class RouteConfig\\n    {\\n        public static void RegisterRoutes(RouteCollection routes)\\n        {\\n            routes.IgnoreRoute(\\"{resource}.axd/{*pathInfo}\\");\\n\\n            // Here go the routes that you still want to be able to hit\\n            routes.MapRoute(\\n                name: \\"IAmARouteThatYouStillWantToHit\\",\\n                url: \\"ThatsWhyIAmRegisteredFirst\\",\\n                defaults: new { controller = \\"Hittable\\", action = \\"Index\\" }\\n            );\\n\\n            // Everything else will hit Home/Index which serves up the root angular app page\\n            routes.MapRoute(\\n                name: \\"Default\\",\\n                url: \\"{*anything}\\", // THIS IS THE MAGIC!!!!\\n                defaults: new { controller = \\"Home\\", action = \\"Index\\" }\\n            );\\n        }\\n```\\n\\nWith this in place my existing routes work just as I would hope. Any route that doesn\'t fit that registered can be assumed to be `html5mode` related and will serve up the root angular app page as I\'d hope.\\n\\n## ASP.Net Web API\\n\\nLater I realised that the app in question was mostly static content. Certainly the root angular app page was and so it seemed wasteful to require an ASP.Net MVC controller to serve up that static content. So I stripped out MVC from the app entirely, choosing to serve raw HTML instead. For the dynamic parts I switched to using Web API. This was \\"hittable\\" as long as I had my `WebApiConfig.cs` and my `system.webServer` section in my `web.config` lined up correctly, viz:\\n\\n```cs\\npublic static class WebApiConfig\\n    {\\n        public static void Register(HttpConfiguration config)\\n        {\\n            // Web API routes\\n            config.MapHttpAttributeRoutes();\\n\\n            config.Routes.MapHttpRoute(\\n                name: \\"DefaultApi\\",\\n                routeTemplate: \\"api/{controller}/{id}\\",\\n                defaults: new { id = RouteParameter.Optional }\\n            );\\n\\n            // other stuff\\n        }\\n    }\\n```\\n\\n```xml\\n<configuration>\\n\\n    <system.webServer>\\n\\n        <defaultDocument>\\n            <files>\\n                <clear />\\n                <add value=\\"build/index.html\\" /> \x3c!-- This is the root document for the Angular app --\x3e\\n            </files>\\n        </defaultDocument>\\n\\n        <rewrite>\\n            <rules>\\n                <rule name=\\"Main Rule\\" stopProcessing=\\"true\\">\\n                    <match url=\\".*\\" />\\n                    <conditions logicalGrouping=\\"MatchAll\\">\\n                        \x3c!-- Allows \\"api/\\" prefixed URLs to still hit Web API controllers\\n                             as defined in WebApiConfig --\x3e\\n                        <add input=\\"{REQUEST_URI}\\" pattern=\\"api/\\" ignoreCase=\\"true\\" negate=\\"true\\" />\\n\\n                        \x3c!-- Static files and directories can be served so partials etc can be loaded --\x3e\\n                        <add input=\\"{REQUEST_FILENAME}\\" matchType=\\"IsFile\\" negate=\\"true\\" />\\n                        <add input=\\"{REQUEST_FILENAME}\\" matchType=\\"IsDirectory\\" negate=\\"true\\" />\\n                    </conditions>\\n                    <action type=\\"Rewrite\\" url=\\"/\\" />\\n                </rule>\\n            </rules>\\n        </rewrite>\\n\\n    </system.webServer>\\n\\n</configuration>\\n```\\n\\nWith this in place I can happily hit \\"api\\" prefixed URLs and still land on my Web API controllers whilst other URLs will serve up the root angular app page. Lovely."},{"id":"/2015/04/24/tonight-ill-start-open-source-project","metadata":{"permalink":"/2015/04/24/tonight-ill-start-open-source-project","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2015-04-24-tonight-ill-start-open-source-project/index.md","source":"@site/blog/2015-04-24-tonight-ill-start-open-source-project/index.md","title":"Tonight I\'ll Start an Open Source Project...","description":"Further posts on this topic","date":"2015-04-24T00:00:00.000Z","formattedDate":"April 24, 2015","tags":[{"label":"asp.net mvc","permalink":"/tags/asp-net-mvc"},{"label":"validation","permalink":"/tags/validation"},{"label":"AngularJS","permalink":"/tags/angular-js"}],"readingTime":5.07,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Tonight I\'ll Start an Open Source Project...","authors":"johnnyreilly","tags":["asp.net mvc","validation","AngularJS"],"hide_table_of_contents":false},"prevItem":{"title":"A tale of Angular, html5mode, ASP.Net MVC and ASP.Net Web API","permalink":"/2015/05/05/a-tale-of-angular-html5mode-aspnet-mvc"},"nextItem":{"title":"How to activate your emoji keyboard on Android 5.0 (Lollipop)","permalink":"/2015/04/17/how-to-activate-your-emoji-keyboard-on"}},"content":"### Further posts on this topic\\n\\n- [NgValidationFor Baby Steps](https://blog.johnnyreilly.com/2015/05/ngvalidationfor-baby-steps.html)\\n\\nI\'m excited. Are you? I\'m babysitting for a friend, I\'ve my laptop, time to kill and (crucially) an idea...\\n\\n## The Idea\\n\\nYou\'re likely aware of the various form element directives that AngularJS offers. For instance the [input directive](https://docs.angularjs.org/api/ng/directive/input):\\n\\n> HTML input element control. When used together with ngModel, it provides data-binding, input state control, and _validation_.\\n\\nYou\'ll notice that I emphasised the word \\"validation\\" there. That\'s important - that\'s my idea. I\'m using AngularJS to build SPA\'s and for the server side I\'m using ASP.Net MVC / Web API. Crucially, my templates are actually ASP.Net MVC Partial Views. That\'s key.\\n\\nWhen I send data back from my SPA back to the server it gets unmarshalled / deserialized into a C# class (view model) of some kind. When data goes the other way it\'s flowing back from a JSON\'d view model and being used by my Angular code.\\n\\nNow historically if I was building a fairly vanilla MVC app then I\'d be making use of all the `TextboxFor` extension methods etc to generate my input elements. For example, with a view model like this:\\n\\n```cs\\nusing System.ComponentModel.DataAnnotations;\\n\\nnamespace App.ViewModels\\n{\\n public class RequiredModel\\n {\\n  [Required]\\n  public string RequiredField{ get; set; }\\n }\\n}\\n```\\n\\nI\'d have a view like this:\\n\\n```html\\n@model App.ViewModels.RequiredModel @using (Html.BeginForm()) {\\n<div class=\\"row\\">\\n  @Html.LabelFor(x => x.TextBox, \\"Something must be entered:\\")\\n  @Html.TextBoxFor(x => x.TextBox, true)\\n</div>\\n}\\n```\\n\\nAnd that would generate HTML like this:\\n\\n```html\\n<form action=\\"/Demo/Required\\" method=\\"post\\">\\n  <div class=\\"row\\">\\n    <label for=\\"TextBox\\">Something must be entered:</label>\\n    <input\\n      data-msg-required=\\"The TextBox field is required.\\"\\n      data-rule-required=\\"true\\"\\n      id=\\"TextBox\\"\\n      name=\\"TextBox\\"\\n      type=\\"text\\"\\n      value=\\"\\"\\n    />\\n  </div>\\n</form>\\n```\\n\\nIf you look at the HTML you\'ll see that the `Required` data annotations have been propogated into the HTML in the HTML in the form of `data-rule-*` and `data-msg-*` attributes. The code above is built using my [jQuery.Validation.Unobtrusive.Native project](http://johnnyreilly.github.io/jQuery.Validation.Unobtrusive.Native/) which in turn was inspired by / based upon the [Unobtrusive Client Validation in ASP.NET MVC](http://bradwilson.typepad.com/blog/2010/10/mvc3-unobtrusive-validation.html). That\'s right - I\'ve done this before - or at least something quite like it.\\n\\nThere\'s clearly a strong crossover between AngularJS\'s input directive parameters and unobtrusive client validation. I\'m planning to take the principles (and maybe some of the code) that I used on that project and see if I can\'t make something useful with it here. [Server side validation is jolly important](https://blog.johnnyreilly.com/2014/08/angularjs-meet-aspnet-server-validation.html) but I can probably save a few compute cycles on the server by making use of client side validation as well. If I\'m right then I should able to come up with a mechanism that saves me from manually duplicating my server validation on the client.\\n\\n## The Aim\\n\\nI want to be able to use HTML Helpers to propogate validation metadata from the server view models into angular form validation directive attributes. Quite a mouthful I know. What does that actually mean? Well I\'ve got 2 ideas. Possibly I want to be able to code something like this:\\n\\n```html\\n@model App.ViewModels.RequiredModel @using (Html.BeginForm()) {\\n<div class=\\"row\\">\\n  @Html.LabelFor(x => x.TextBox, \\"Something must be entered:\\")\\n  @Html.NgTextBoxFor(x => x.TextBox)\\n</div>\\n}\\n```\\n\\nAnd have HTML like this generated:\\n\\n```html\\n<form action=\\"/Demo/Required\\" method=\\"post\\">\\n  <div class=\\"row\\">\\n    <label for=\\"TextBox\\">Something must be entered:</label>\\n    <input\\n      ng-required=\\"true\\"\\n      id=\\"TextBox\\"\\n      name=\\"TextBox\\"\\n      type=\\"text\\"\\n      value=\\"\\"\\n    />\\n  </div>\\n</form>\\n```\\n\\nThe reservation I have about this approach is that it rather takes you away from the HTML. Yes it works (and to your seasoned MVC-er it will feel quite natural in some ways) but it feels rather heavy handed. But I\'d like what I\'m building to be easy for users to plug into existing code without a ton of rework. So, the other idea I\'m toying with is having HTML helpers that just return a string of attributes. So if I had an angular form that looked like this:\\n\\n```html\\n<div ng-controller=\\"ExampleController\\">\\n  <form>\\n    <div class=\\"row\\">\\n      <label\\n        >Something must be entered:\\n        <input name=\\"RequiredField\\" type=\\"text\\" value=\\"\\" />\\n      </label>\\n    </div>\\n  </form>\\n</div>\\n```\\n\\nI could tweak it to push in the validation directive attributes like this:\\n\\n```html\\n@model App.ViewModels.RequiredModel\\n<div ng-controller=\\"ExampleController\\">\\n  <form>\\n    <div class=\\"row\\">\\n      <label\\n        >Something must be entered: <input name=\\"RequiredField\\" type=\\"text\\"\\n        value=\\"\\" @Html.NgValidationFor(x => x.RequiredField) />\\n      </label>\\n    </div>\\n  </form>\\n</div>\\n```\\n\\nAnd end up with HTML like this:\\n\\n```html\\n<div ng-controller=\\"ExampleController\\">\\n  <form>\\n    <div class=\\"row\\">\\n      <label\\n        >Something must be entered:\\n        <input name=\\"RequiredField\\" type=\\"text\\" value=\\"\\" ng-required=\\"true\\" />\\n      </label>\\n    </div>\\n  </form>\\n</div>\\n```\\n\\nThis is a simplified example of course - it\'s likely that any number of validation directive attributes might be returned from `NgValidationFor`. And crucially if these attributes were changed on the server view model then the validation changes would automatically end up in the client HTML with this approach.\\n\\n## The Approach\\n\\nAt least to start off with I\'m going to aim at creating the second of my approaches. I may come back and implement the first at some point but I think the second is a better place to start.\\n\\nI\'m kind of surprised no-one else has built this already actually - but I\'m not aware of anything. I\'ve had a little duckduckgo around and found no takers. The closest I\'ve come is the excellent [BreezeJS](http://www.breezejs.com/sites/all/apidocs/classes/Validator.html). BreezeJS does way more than I want it to - I\'m planning to restrict the scope of this project to simply turning data annotations on my ASP.Net MVC server models into `ng-*` directive attributes in HTML. That\'s it.\\n\\nSo, general housekeeping.... I\'m going to host this project on [GitHub](http://www.github.com), I\'m going to have Continuous Integration with [AppVeyor](http://www.appveyor.com/) and I\'m planning to publish this via [NuGet](http://www.nuget.org/) (when and if I\'ve created something useful).\\n\\nI just need a name and I\'ll begin. What shall I call it? Some options:\\n\\n- Angular ASP.Net MVC Extensions\\n- angular-aspnet-mvc-extensions\\n- Angular MVC Element Extensions\\n- Angular Validation Html Helpers\\n- NgValidationFor (the name of the HTML helper I made up)\\n\\nHmmmm.... None of them is particularly lighting my fire. The first four are all a bit [RonSeal](https://en.wikipedia.org/wiki/Ronseal) \\\\- which is fine.... Ug. The last one... It\'s a bit more pithy. Okay - I\'ll go with \\"NgValidationFor\\" at least for now. If something better occurs I can always change my mind.\\n\\n[And we\'re off!](https://github.com/johnnyreilly/NgValidationFor)"},{"id":"/2015/04/17/how-to-activate-your-emoji-keyboard-on","metadata":{"permalink":"/2015/04/17/how-to-activate-your-emoji-keyboard-on","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2015-04-17-how-to-activate-your-emoji-keyboard-on/index.md","source":"@site/blog/2015-04-17-how-to-activate-your-emoji-keyboard-on/index.md","title":"How to activate your emoji keyboard on Android 5.0 (Lollipop)","description":"A departure from from my normal content - I need to tell you about emoji! You\'ll probably already know about them - just imagine a emoticon but about 300,000 times better. They really add spice to to textual content. Oh and they\'re Japanese - which is also way cool.","date":"2015-04-17T00:00:00.000Z","formattedDate":"April 17, 2015","tags":[{"label":"emoji","permalink":"/tags/emoji"},{"label":"android","permalink":"/tags/android"}],"readingTime":0.85,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"How to activate your emoji keyboard on Android 5.0 (Lollipop)","authors":"johnnyreilly","tags":["emoji","android"],"hide_table_of_contents":false},"prevItem":{"title":"Tonight I\'ll Start an Open Source Project...","permalink":"/2015/04/24/tonight-ill-start-open-source-project"},"nextItem":{"title":"PartialView.ToString()","permalink":"/2015/03/20/partialview-tostring"}},"content":"A departure from from my normal content - I need to tell you about [emoji](http://en.wikipedia.org/wiki/Emoji)! You\'ll probably already know about them - just imagine a emoticon but about 300,000 times better. They really add spice to to textual content. Oh and they\'re Japanese - which is also way cool.\\n\\nSince I\'ve discovered emoji I\'ve felt a pressing need to have them on my (Android) phone. This is harder than you might imagine. But totally do-able.... Here\'s how you get the emoji love on your Android Lollipop phone:\\n\\n- goto settings (the cog)\\n- select \\"Language and Input\\"\\n- select your \\"Current keyboard\\" and then select the \\"Choose keyboards\\" option\\n- look for a keyboard that says \\"iWnn IME Japanese\\". Select it\\n- drop back to the \\"Language and Input\\" menu where you will see \\"iWnn IME Japanese\\" is now there.\\n- select it and deactivate \\"Japanese\\" and activate \\"Emoji\\" like this: [![](https://4.bp.blogspot.com/-toFgqIFcTs4/VTC6JXxwmtI/AAAAAAAAA0s/OT7O7MdGvSc/s320/Screenshot_2015-04-16-07-21-06-741564.png)](https://4.bp.blogspot.com/-toFgqIFcTs4/VTC6JXxwmtI/AAAAAAAAA0s/OT7O7MdGvSc/s1600/Screenshot_2015-04-16-07-21-06-741564.png)\\n\\n- now you should find your keyboard contains a little globe icon. When you select it.... Emoji!!!! [![](https://2.bp.blogspot.com/-xtdHdGuc6YU/VTC6I-_43tI/AAAAAAAAA0g/JnlckIUnS48/s320/Screenshot_2015-04-16-07-23-56%257E2-739197.jpg)](https://2.bp.blogspot.com/-xtdHdGuc6YU/VTC6I-_43tI/AAAAAAAAA0g/JnlckIUnS48/s1600/Screenshot_2015-04-16-07-23-56%257E2-739197.jpg)"},{"id":"/2015/03/20/partialview-tostring","metadata":{"permalink":"/2015/03/20/partialview-tostring","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2015-03-20-partialview-tostring/index.md","source":"@site/blog/2015-03-20-partialview-tostring/index.md","title":"PartialView.ToString()","description":"In the name of DRY I found myself puzzling how one could take a PartialViewResult and render it as a string. Simple, right?","date":"2015-03-20T00:00:00.000Z","formattedDate":"March 20, 2015","tags":[{"label":"asp.net mvc","permalink":"/tags/asp-net-mvc"},{"label":"sward","permalink":"/tags/sward"},{"label":"PartialView","permalink":"/tags/partial-view"}],"readingTime":3.705,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"PartialView.ToString()","authors":"johnnyreilly","tags":["asp.net mvc","sward","PartialView"],"hide_table_of_contents":false},"prevItem":{"title":"How to activate your emoji keyboard on Android 5.0 (Lollipop)","permalink":"/2015/04/17/how-to-activate-your-emoji-keyboard-on"},"nextItem":{"title":"Hey tsconfig.json, where have you been all my life?","permalink":"/2015/02/27/hey-tsconfigjson-where-have-you-been"}},"content":"In the name of [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself) I found myself puzzling how one could take a `PartialViewResult` and render it as a `string`. Simple, right?\\n\\nIn fact, in my head this was already a solved problem. I mean I\'ve written about this [before](http://blog.icanmakethiswork.io/2012/07/rendering-partial-view-to-string.html) already! Except I haven\'t. Not really - what I did back then was link to what someone else had written and say \\"yay! well done chap - like he said!\\". It turns out that was a bad move. That blog appears to be gone and so I\'m back to where I was. Ug. Lesson learned.\\n\\n## What are we trying to do?\\n\\nSo, for the second time of asking, here is how to take a `PartialViewResult` and turn it into a `string`. It\'s an invaluable technique to deal with certain scenarios.\\n\\nIn my own case I have a toolbar in my application that is first pushed into the UI in my `_Layout.cshtml` by means of a trusty `@Html.Action(\\"Toolbar\\")`. I wanted to be able to re-use the `PartialViewResult` returned by `Toolbar` on my controller inside a `JSON` payload. And despite the title of this post, `PartialView.ToString()`_doesn\'t_ quite cut the mustard. Obvious really, if it did then why would I be writing this and you be reading this?\\n\\nThe solution is actually fairly simple. And, purely for swank, I\'m going to offer it you 3 ways. Whatever\'s your poison.\\n\\n## Inheritance (it\'s so yesterday darling)\\n\\nYes there was a time when everything was inheritance based. You were rewarded handsomely for making sure that was the case. However, times have changed and (with good reason) people tend to [favour composition over inheritance](https://en.wikipedia.org/wiki/Composition_over_inheritance). So, perhaps just for the memories, let first offer you the inheritance based approach:\\n\\n```cs\\nprotected string ConvertPartialViewToString(PartialViewResult partialView)\\n{\\n  using (var sw = new StringWriter())\\n  {\\n    partialView.View = ViewEngines.Engines\\n      .FindPartialView(ControllerContext, partialView.ViewName).View;\\n\\n    var vc = new ViewContext(\\n      ControllerContext, partialView.View, partialView.ViewData, partialView.TempData, sw);\\n    partialView.View.Render(vc, sw);\\n\\n    var partialViewString = sw.GetStringBuilder().ToString();\\n\\n    return partialViewString;\\n  }\\n}\\n```\\n\\nThe idea being that the above method is placed onto a base controller which your controllers subclass. Thus using this method inside one of the controllers is as simple as:\\n\\n```cs\\nvar toolbarHtml = ConvertPartialViewToString(partialViewResult);\\n```\\n\\n## Extension method (sexier syntax)\\n\\nSo the next choice is implementing this as an extension method. Here\'s my static class which adds `ConvertToString` onto `PartialViewResult`:\\n\\n```cs\\nusing System.IO;\\nusing System.Web.Mvc;\\n\\nnamespace My.Utilities.Extensions\\n{\\n  public static class PartialViewResultExtensions\\n  {\\n    public static string ConvertToString(this PartialViewResult partialView,\\n                                              ControllerContext controllerContext)\\n    {\\n      using (var sw = new StringWriter())\\n      {\\n        partialView.View = ViewEngines.Engines\\n          .FindPartialView(controllerContext, partialView.ViewName).View;\\n\\n        var vc = new ViewContext(\\n          controllerContext, partialView.View, partialView.ViewData, partialView.TempData, sw);\\n        partialView.View.Render(vc, sw);\\n\\n        var partialViewString = sw.GetStringBuilder().ToString();\\n\\n        return partialViewString;\\n      }\\n    }\\n  }\\n}\\n```\\n\\nI don\'t know about you but I do love an extension method - it often makes for much more readable code. In this case we can use:\\n\\n```cs\\nvar toolbarHtml = partialViewResult.ConvertToString(ControllerContext);\\n```\\n\\nWhich I think we can all agree is really rather lovely. Perhaps it would be more lovely if I didn\'t have to pass `ControllerContext` \\\\- but hey! Still quite nice.\\n\\n## Favouring Composition over Inheritance (testable)\\n\\nAlthough ASP.Net MVC was designed to be testable there are times when you think \\"really? Can it be that hard?\\". In fact for a well thought through discussion on the topic I advise you read [this](http://volaresystems.com/blog/post/2010/08/19/Dont-mock-HttpContext). (I\'m aware of the irony implicit in linking to another blog post in a blog post that I only wrote because I first linked to another blog which vanished.... Infinite recursion anybody?)\\n\\nThe conclusion of the linked blog post is twofold\\n\\n1. Don\'t mock HTTPContext\\n2. Use the [facade pattern](https://en.wikipedia.org/wiki/Facade_pattern) instead\\n\\nHaving testable code is not a optional bauble in my view - it\'s a necessity. So with my final approach that\'s exactly what I\'ll do.\\n\\n```cs\\nusing System.Web.Mvc;\\n\\nnamespace My.Interfaces\\n{\\n  public interface IMvcInternals\\n  {\\n    string ConvertPartialViewToString(PartialViewResult partialView, ControllerContext controllerContext);\\n  }\\n}\\n\\n// ....\\n\\nusing System.IO;\\nusing System.Web.Mvc;\\nusing My.Interfaces;\\n\\nnamespace My.Utilities\\n{\\n  public class MvcInternals : IMvcInternals\\n  {\\n    public string ConvertPartialViewToString(PartialViewResult partialView,\\n                                             ControllerContext controllerContext)\\n    {\\n      using (var sw = new StringWriter())\\n      {\\n        partialView.View = ViewEngines.Engines\\n          .FindPartialView(controllerContext, partialView.ViewName).View;\\n\\n        var vc = new ViewContext(\\n          controllerContext, partialView.View, partialView.ViewData, partialView.TempData, sw);\\n        partialView.View.Render(vc, sw);\\n\\n        var partialViewString = sw.GetStringBuilder().ToString();\\n\\n        return partialViewString;\\n      }\\n    }\\n  }\\n}\\n```\\n\\nSo here I have a simple interface with a `ConvertPartialViewToString` method on it. This interface can be passed into a controller and then used like this:\\n\\n```cs\\nvar toolbarHtml = _mvcInternals.ConvertPartialViewToString(partialViewResult, ControllerContext);\\n```\\n\\nAh... that\'s the sweet mellifluous sound of easily testable code."},{"id":"/2015/02/27/hey-tsconfigjson-where-have-you-been","metadata":{"permalink":"/2015/02/27/hey-tsconfigjson-where-have-you-been","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2015-02-27-hey-tsconfigjson-where-have-you-been/index.md","source":"@site/blog/2015-02-27-hey-tsconfigjson-where-have-you-been/index.md","title":"Hey tsconfig.json, where have you been all my life?","description":"Sometimes, you just miss things. Something seismic happens and you had no idea. So it was with tsconfig.json.","date":"2015-02-27T00:00:00.000Z","formattedDate":"February 27, 2015","tags":[{"label":"tsconfig.json","permalink":"/tags/tsconfig-json"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"implicit references","permalink":"/tags/implicit-references"}],"readingTime":4.37,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Hey tsconfig.json, where have you been all my life?","authors":"johnnyreilly","tags":["tsconfig.json","TypeScript","implicit references"],"hide_table_of_contents":false},"prevItem":{"title":"PartialView.ToString()","permalink":"/2015/03/20/partialview-tostring"},"nextItem":{"title":"Using Gulp to inject scripts and styles tags directly into your HTML","permalink":"/2015/02/17/using-gulp-in-asp-net-instead-of-web-optimization"}},"content":"Sometimes, you just miss things. Something seismic happens and you had no idea. So it was with `tsconfig.json`.\\n\\nThis blog post started life with the name \\"TypeScript: Some IDEs are more equal than others\\". I\'d intended to use it summarise a discussion on the [TypeScript GitHub repo](https://github.com/Microsoft/TypeScript/issues/1066) about implicit referencing including a fist shaken at the sky at the injustice of it all. But whilst I was writing it I dicovered things had changed without my knowledge. That\'s a rather wonderful thing.\\n\\n## Implicit Referencing\\n\\nImplicit referencing, if you\'re not aware, is the thing that separates Visual Studio from all other IDEs / text editors. Implicit referencing means that in Visual Studio you don\'t need to make use of comments at the head of each TypeScript file in order to tell the compiler where it can find the related TypeScript files.\\n\\nThe `reference` comments aren\'t necessary when using Visual Studio because the VS project file is used to drive the files passed to the TypeScript compiler (tsc).\\n\\nThe upshot of this is that, at time of writing, you can generally look at a TypeScript codebase and tell whether it was written using Visual Studio by opening it up a file at random and eyeballing for something like this at the top:\\n\\n```ts\\n/// <reference path=\\"other-file.ts\\" />\\n```\\n\\n_\\"A-ha! They\'re using \\"reference\\" comments Watson. From this I deduce that the individuals in question are using the internal module approach and using Visual Studio as their IDE. Elementary, my dear fellow, quite elementary.\\"_\\n\\nThis has important implications. Important I tell you, yes important! Well, important if you want to reduce the barriers between Visual Studio and everyone else. And I do. Whilst I love Visual Studio - it\'s been my daily workhorse for many years - I also love stepping away from it and using something more stripped down. I also like working with other people without mandating that they need to use Visual Studio as well. In the words of Rodney King, \\"can\'t we all get along?\\".\\n\\n## Cross-IDE TypeScript projects\\n\\nI feel I should be clear - you can already set up TypeScript projects to work regardless of IDE. But there\'s friction. It\'s not clear cut. You can see a full on discussion around this [here](https://github.com/Microsoft/TypeScript/issues/1066) but in the end it comes down to making a choice between these 3 options:\\n\\n1. Set <TypeScriptEnabled>false</TypeScriptEnabled> in a project file. [This flag effectively deactivates implicit referencing.](https://github.com/Microsoft/TypeScript/issues/1066#issuecomment-63727612) This approach requires that all developers (regardless of IDE) use `/// &lt;reference`s to build context. Compiler options in VS can be controlled using the project file as is.\\n2. Using Visual Studio without any csproj tweaks. This approach requires that all files will need `/// &lt;reference`s at their heads in order to build compilation context _outside_ of Visual Studio. It\'s possible that `/// &lt;reference`s and the csproj could get out of line - care is required to avoid this. Compiler options in VS can be controlled using the project file as is.\\n3. Using just files in Visual Studio with `/// &lt;reference`s to build compilation context. This scenario also requires that all developers (regardless of IDE) use `/// &lt;reference`s to build context. In Visual Studio there will be no control over compiler options.\\n\\nAs you can see - this is sub-optimal. But don\'t worry - there\'s a new sheriff in town....\\n\\n## `tsconfig.json`\\n\\nI\'d decided to give [Atom TypeScript plugin](https://github.com/TypeStrong/atom-typescript) a go as I heard much enthusiastic noise about it. I fired it up and pointed it at a a TypeScript AngularJS project built in Visual Studio. I was mentally preparing myself for the job of adding all the /// references in when I suddenly noticed a file blinking at me:\\n\\n![](Screenshot-2015-02-27-16.05.29.png)\\n\\n`tsconfig.json`? What\'s that? Time to read [the docs](https://github.com/TypeStrong/atom-typescript#project-support):\\n\\n> Supported via tsconfig.json ([read more](https://github.com/TypeStrong/atom-typescript/blob/master/docs/tsconfig/index.md)) which is going to be the defacto Project file format for the next versions of TypeScript.\\n\\n\\"read more\\"? Oh yes indeedy - I think I will \\"read more\\"!\\n\\n> A unified project format for TypeScript ([see merged PR on Microsoft/TypeScript](https://github.com/Microsoft/TypeScript/pull/1692)). The TypeScript compiler (1.4 and above) only cares about compilerOptions and files. We add additional features to this [with the typescript team\'s approval to extend the file as long as we don\'t conflict:](https://github.com/Microsoft/TypeScript/issues/1955)\\n>\\n> - [compilerOptions](https://github.com/TypeStrong/atom-typescript/blob/e2fa67c4715189b71430f766ed9a92d9fb3255f9/lib/main/tsconfig/tsconfig.ts#L8-L35) similar to what you would pass on the commandline to tsc.\\n> - [filesGlob](https://github.com/TypeStrong/atom-typescript/blob/master/docs/tsconfig/index.md#filesglob): To make it easier for you to just add / remove files in your project we add filesGlob which accepts an array of glob / minimatch / RegExp patterns (similar to grunt)to specify source files.\\n> - [format](https://github.com/TypeStrong/atom-typescript/blob/master/docs/tsconfig/index.md#format): Code formatting options\\n> - [version](https://github.com/TypeStrong/atom-typescript/blob/master/docs/tsconfig/index.md#version): The TypeScript version\\n\\nThat\'s right folks, we don\'t need `/// &lt;reference`s comments anymore. In a blinding flash of light it all changes. We\'re going from the dark end of the street, to the bright side of the road. `tsconfig.json` is here to ease away the pain and make it all better. Let\'s enjoy it while we can.\\n\\nThis change should ship with TypeScript 1.5 (hopefully) for those using Visual Studio. For those using Atom TypeScript (and as of today that\'s includes me) the carnival celebrations can begin now!\\n\\nThanks to [@basarat](https://github.com/basarat) who have quoted at length and [Daniel Earwicker](https://smellegantcode.wordpress.com/) who is the reason that I came to discover `tsconfig.json`."},{"id":"/2015/02/17/using-gulp-in-asp-net-instead-of-web-optimization","metadata":{"permalink":"/2015/02/17/using-gulp-in-asp-net-instead-of-web-optimization","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2015-02-17-using-gulp-in-asp-net-instead-of-web-optimization/index.md","source":"@site/blog/2015-02-17-using-gulp-in-asp-net-instead-of-web-optimization/index.md","title":"Using Gulp to inject scripts and styles tags directly into your HTML","description":"This is very probably the dullest title for a blog post I\'ve ever come up with. Read on though folks - it\'s definitely going to pick up...","date":"2015-02-17T00:00:00.000Z","formattedDate":"February 17, 2015","tags":[{"label":"asp.net","permalink":"/tags/asp-net"},{"label":"gulp-inject","permalink":"/tags/gulp-inject"},{"label":"Web Optimization","permalink":"/tags/web-optimization"},{"label":"gulpjs","permalink":"/tags/gulpjs"},{"label":"wiredep","permalink":"/tags/wiredep"}],"readingTime":10.465,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Using Gulp to inject scripts and styles tags directly into your HTML","authors":"johnnyreilly","tags":["asp.net","gulp-inject","Web Optimization","gulpjs","wiredep"],"hide_table_of_contents":false},"prevItem":{"title":"Hey tsconfig.json, where have you been all my life?","permalink":"/2015/02/27/hey-tsconfigjson-where-have-you-been"},"nextItem":{"title":"The Convent with Continuous Delivery","permalink":"/2015/02/11/the-convent-with-continuous-delivery"}},"content":"This is very probably the dullest title for a blog post I\'ve ever come up with. Read on though folks - it\'s definitely going to pick up...\\n\\nI [wrote last year](https://blog.johnnyreilly.com/2014/11/using-gulp-in-visual-studio-instead-of-web-optimization.html) about my first usage of Gulp in an ASP.Net project. I used Gulp to replace the Web Optimization functionality that is due to disappear when ASP.Net v5 ships. What I came up with was an approach that provided pretty much the same functionality; raw source in debug mode, bundling + minification in release mode.\\n\\nIt worked by having a launch page which was straight HTML. Embedded within this page was JavaScript that would, at runtime, load the required JavaScript / CSS and inject it dynamically into the document. This approach worked but it had a number of downsides:\\n\\n1. Each time you fired up the app the following sequence of events would happen: - jQuery would load (purely there to simplify the making of various startup AJAX calls)\\n\\n- the page would make an AJAX call to the server to load various startup data, including whether the app is running in debug or release mode\\n- Depending on the result of the startup data either the debug or release package manifest would be loaded.\\n- For each entry in the package manifest `script` and `link` tags would be created and added to the document. These would generate further requests to the server to load the resources.\\n\\nQuite a lot going on here isn\'t there? Accordingly, initial startup time was slower than you might hope. 2. The \\"F\\" word: [FOUC](https://en.wikipedia.org/wiki/Flash_of_unstyled_content). Flash Of Unstyled Content - whilst all the hard work of the page load was going on (before the CSS had been loaded) the page would look rather ... bare. Not a terrible thing but none too slick either. 3. The gulpfile built both the debug and the release package each time it was run. This meant the gulp task generally did double the work that it needed to do.\\n\\nI wanted to see if I could tackle these issues. I\'ve recently been watching [John Papa](https://twitter.com/John_Papa)\'s excellent [Pluralsight course on Gulp](http://www.pluralsight.com/courses/javascript-build-automation-gulpjs) and picked up a number of useful tips. With that in hand let\'s see what we can come up with...\\n\\n## Death to dynamic loading\\n\\nThe main issue with the approach I\'ve been using is the dynamic loading. It makes the app slower and more complicated. So the obvious solution is to have my gulpfile inject scripts and css into the template. To that end it\'s [wiredep](https://www.npmjs.com/package/wiredep) & [gulp-inject](https://www.npmjs.com/package/gulp-inject) to the rescue!\\n\\ngulp-inject (as the name suggests) is used to inject `script` and `link` tags into source code. I\'m using [Bower](http://bower.io/) as my client side package manager and so I\'m going to use wiredep to determine the vendor scripts I need. It will determine what packages my app is using from looking at my `bower.json`, and give me a list of file paths in _dependency order_ (which I can then pass on to gulp-inject in combination with my own app script files). This means I don\'t have to think about ordering bower dependencies myself and I no longer need to separately maintain a list of these files within my gulpfile.\\n\\nSo, let\'s get the launch page (`index.html`) ready for gulp-inject:\\n\\n```html\\n<!DOCTYPE html>\\n<html>\\n  <head>\\n    <meta http-equiv=\\"X-UA-Compatible\\" content=\\"IE=edge, chrome=1\\" />\\n    <style>\\n      .ng-hide {\\n        display: none !important;\\n      }\\n    </style>\\n    <title ng-bind=\\"title\\">Proverb</title>\\n    <meta charset=\\"utf-8\\" />\\n    <meta\\n      name=\\"viewport\\"\\n      content=\\"width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no\\"\\n    />\\n\\n    \x3c!-- inject:css --\x3e\\n    \x3c!-- endinject --\x3e\\n\\n    <link rel=\\"icon\\" type=\\"image/png\\" href=\\"content/images/icon.png\\" />\\n  </head>\\n  <body>\\n    <div>\\n      <div ng-include=\\"\'app/layout/shell.html\'\\"></div>\\n      <div id=\\"splash-page\\" ng-show=\\"false\\" class=\\"dissolve-animation\\">\\n        <div class=\\"page-splash\\">\\n          <div class=\\"page-splash-message\\">Proverb</div>\\n\\n          <div class=\\"progress\\">\\n            <div\\n              class=\\"progress-bar progress-bar-striped active\\"\\n              role=\\"progressbar\\"\\n              style=\\"width: 20%;\\"\\n            >\\n              <span class=\\"sr-only\\">loading...</span>\\n            </div>\\n          </div>\\n        </div>\\n      </div>\\n    </div>\\n\\n    <script src=\\"https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js\\"><\/script>\\n    <script>\\n      window.jQuery ||\\n        document.write(\'<script src=\\"/build/jquery.min.js\\">\\\\x3C/script>\');\\n    <\/script>\\n\\n    \x3c!-- inject:js --\x3e\\n    \x3c!-- endinject --\x3e\\n\\n    <script>\\n      (function () {\\n        // Load startup data from the server\\n        $.getJSON(\'api/Startup\').done(function (startUpData) {\\n          angularApp.start({\\n            thirdPartyLibs: {\\n              moment: window.moment,\\n              toastr: window.toastr,\\n              underscore: window._,\\n            },\\n            appConfig: startUpData,\\n          });\\n        });\\n      })();\\n    <\/script>\\n  </body>\\n</html>\\n```\\n\\nThe important thing to notice here are the `&lt;!-- inject:css --&gt;` and `&lt;!-- inject:js --&gt;` injection placeholders. It\'s here that our script and style tags will be injected into the template. You\'ll notice that jQuery is _not_ being injected - and that\'s because I\'ve opted to use a CDN for jQuery and then only fallback to serving jQuery myself if the CDN fails.\\n\\nThe other thing to notice here is that our launch page has become oh so much simpler in comparison with the dynamic loading approach. Which is fab.\\n\\nNow before we start looking at our gulpfile I want to split out the configuration into a standalone file called gulpfile.config.js:\\n\\n```js\\nvar tsjsmapjsSuffix = \'.{ts,js.map,js}\';\\n\\nvar bower = \'bower_components/\';\\nvar app = \'app/\';\\n\\nvar config = {\\n  base: \'.\',\\n  buildDir: \'./build/\',\\n  debug: \'debug\',\\n  release: \'release\',\\n  css: \'css\',\\n\\n  bootFile: app + \'index.html\',\\n  bootjQuery: bower + \'jquery/dist/jquery.min.js\',\\n\\n  // The fonts we want Gulp to process\\n  fonts: [bower + \'fontawesome/fonts/*.*\'],\\n\\n  images: \'images/**/*.{gif,jpg,png}\',\\n\\n  // The scripts we want Gulp to process\\n  scripts: [\\n    // Bootstrapping\\n    app + \'app\' + tsjsmapjsSuffix,\\n    app + \'config.route\' + tsjsmapjsSuffix,\\n\\n    // common Modules\\n    app + \'common/common\' + tsjsmapjsSuffix,\\n    app + \'common/logger\' + tsjsmapjsSuffix,\\n    app + \'common/spinner\' + tsjsmapjsSuffix,\\n\\n    // common.bootstrap Modules\\n    app + \'common/bootstrap/bootstrap.dialog\' + tsjsmapjsSuffix,\\n\\n    // directives\\n    app + \'directives/**/*\' + tsjsmapjsSuffix,\\n\\n    // services\\n    app + \'services/**/*\' + tsjsmapjsSuffix,\\n\\n    // controllers\\n    app + \'about/**/*\' + tsjsmapjsSuffix,\\n    app + \'admin/**/*\' + tsjsmapjsSuffix,\\n    app + \'dashboard/**/*\' + tsjsmapjsSuffix,\\n    app + \'layout/**/*\' + tsjsmapjsSuffix,\\n    app + \'sages/**/*\' + tsjsmapjsSuffix,\\n    app + \'sayings/**/*\' + tsjsmapjsSuffix,\\n  ],\\n\\n  // The styles we want Gulp to process\\n  styles: [\'content/styles.css\'],\\n\\n  wiredepOptions: {\\n    exclude: [/jquery/],\\n    ignorePath: \'..\',\\n  },\\n};\\n\\nconfig.debugFolder = config.buildDir + config.debug + \'/\';\\nconfig.releaseFolder = config.buildDir + config.release + \'/\';\\n\\nconfig.templateFiles = [\\n  app + \'**/*.html\',\\n  \'!\' + config.bootFile, // Exclude the launch page\\n];\\n\\nmodule.exports = config;\\n```\\n\\nNow to the meat of the matter - let me present the gulpfile:\\n\\n```js\\n/// <vs AfterBuild=\'default\' />\\nvar gulp = require(\'gulp\');\\n\\n// Include Our Plugins\\nvar concat = require(\'gulp-concat\');\\nvar ignore = require(\'gulp-ignore\');\\nvar minifyCss = require(\'gulp-minify-css\');\\nvar uglify = require(\'gulp-uglify\');\\nvar rev = require(\'gulp-rev\');\\nvar del = require(\'del\');\\nvar path = require(\'path\');\\nvar templateCache = require(\'gulp-angular-templatecache\');\\nvar eventStream = require(\'event-stream\');\\nvar order = require(\'gulp-order\');\\nvar gulpUtil = require(\'gulp-util\');\\nvar wiredep = require(\'wiredep\');\\nvar inject = require(\'gulp-inject\');\\n\\n// Get our config\\nvar config = require(\'./gulpfile.config.js\');\\n\\n/**\\n * Get the scripts or styles the app requires by combining bower dependencies and app dependencies\\n *\\n * @param {string} jsOrCss Should be \\"js\\" or \\"css\\"\\n */\\nfunction getScriptsOrStyles(jsOrCss) {\\n  var bowerScriptsAbsolute = wiredep(config.wiredepOptions)[jsOrCss];\\n\\n  var bowerScriptsRelative = bowerScriptsAbsolute.map(\\n    function makePathRelativeToCwd(file) {\\n      return path.relative(\'\', file);\\n    }\\n  );\\n\\n  var appScripts = bowerScriptsRelative.concat(\\n    jsOrCss === \'js\' ? config.scripts : config.styles\\n  );\\n\\n  return appScripts;\\n}\\n\\n/**\\n * Get the scripts the app requires\\n */\\nfunction getScripts() {\\n  return getScriptsOrStyles(\'js\');\\n}\\n\\n/**\\n * Get the styles the app requires\\n */\\nfunction getStyles() {\\n  return getScriptsOrStyles(\'css\');\\n}\\n\\n/**\\n * Get the scripts and the templates combined streams\\n *\\n * @param {boolean} isDebug\\n */\\nfunction getScriptsAndTemplates(isDebug) {\\n  var options = isDebug ? { base: config.base } : undefined;\\n  var appScripts = gulp.src(getScripts(), options);\\n\\n  //Get the view templates for $templateCache\\n  var templates = gulp\\n    .src(config.templateFiles)\\n    .pipe(templateCache({ module: \'app\', root: \'app/\' }));\\n\\n  var combined = eventStream.merge(appScripts, templates);\\n\\n  return combined;\\n}\\n\\ngulp.task(\'clean\', function (cb) {\\n  gulpUtil.log(\'Delete the build folder\');\\n\\n  return del([config.buildDir], cb);\\n});\\n\\ngulp.task(\'boot-dependencies\', [\'clean\'], function () {\\n  gulpUtil.log(\'Get dependencies needed for boot (jQuery and images)\');\\n\\n  var jQuery = gulp.src(config.bootjQuery);\\n  var images = gulp.src(config.images, { base: config.base });\\n\\n  var combined = eventStream\\n    .merge(jQuery, images)\\n    .pipe(gulp.dest(config.buildDir));\\n\\n  return combined;\\n});\\n\\ngulp.task(\'inject-debug\', [\'styles-debug\', \'scripts-debug\'], function () {\\n  gulpUtil.log(\'Inject debug links and script tags into \' + config.bootFile);\\n\\n  var scriptsAndStyles = [].concat(getScripts(), getStyles());\\n\\n  return gulp\\n    .src(config.bootFile)\\n    .pipe(\\n      inject(\\n        gulp\\n          .src(\\n            [\\n              config.debugFolder + \'**/*.{js,css}\',\\n              \'!build\\\\\\\\debug\\\\\\\\bower_components\\\\\\\\spin.js\', // Exclude weird spin js path\\n            ],\\n            { read: false }\\n          )\\n          .pipe(order(scriptsAndStyles))\\n      )\\n    )\\n    .pipe(gulp.dest(config.buildDir));\\n});\\n\\ngulp.task(\'inject-release\', [\'styles-release\', \'scripts-release\'], function () {\\n  gulpUtil.log(\'Inject release links and script tags into \' + config.bootFile);\\n\\n  return gulp\\n    .src(config.bootFile)\\n    .pipe(\\n      inject(gulp.src(config.releaseFolder + \'**/*.{js,css}\', { read: false }))\\n    )\\n    .pipe(gulp.dest(config.buildDir));\\n});\\n\\ngulp.task(\'scripts-debug\', [\'clean\'], function () {\\n  gulpUtil.log(\'Copy across all JavaScript files to build/debug\');\\n\\n  return getScriptsAndTemplates(true).pipe(gulp.dest(config.debugFolder));\\n});\\n\\ngulp.task(\'scripts-release\', [\'clean\'], function () {\\n  gulpUtil.log(\'Concatenate & Minify JS for release into a single file\');\\n\\n  return getScriptsAndTemplates(false)\\n    .pipe(ignore.exclude(\'**/*.{ts,js.map}\')) // Exclude ts and js.map files - not needed in release mode\\n    .pipe(concat(\'app.js\')) // Make a single file\\n    .pipe(uglify()) // Make the file titchy tiny small\\n    .pipe(rev()) // Suffix a version number to it\\n    .pipe(gulp.dest(config.releaseFolder)); // Write single versioned file to build/release folder\\n});\\n\\ngulp.task(\'styles-debug\', [\'clean\'], function () {\\n  gulpUtil.log(\'Copy across all CSS files to build/debug\');\\n\\n  return gulp\\n    .src(getStyles(), { base: config.base })\\n    .pipe(gulp.dest(config.debugFolder));\\n});\\n\\ngulp.task(\'styles-release\', [\'clean\'], function () {\\n  gulpUtil.log(\'Copy across all files in config.styles to build/debug\');\\n\\n  return gulp\\n    .src(getStyles())\\n    .pipe(concat(\'app.css\')) // Make a single file\\n    .pipe(minifyCss()) // Make the file titchy tiny small\\n    .pipe(rev()) // Suffix a version number to it\\n    .pipe(gulp.dest(config.releaseFolder + \'/\' + config.css)); // Write single versioned file to build/release folder\\n});\\n\\ngulp.task(\'fonts-debug\', [\'clean\'], function () {\\n  gulpUtil.log(\'Copy across all fonts in config.fonts to debug location\');\\n\\n  return gulp\\n    .src(config.fonts, { base: config.base })\\n    .pipe(gulp.dest(config.debugFolder));\\n});\\n\\ngulp.task(\'fonts-release\', [\'clean\'], function () {\\n  gulpUtil.log(\'Copy across all fonts in config.fonts to release location\');\\n\\n  return gulp\\n    .src(config.fonts)\\n    .pipe(gulp.dest(config.releaseFolder + \'/fonts\'));\\n});\\n\\ngulp.task(\'build-debug\', [\'boot-dependencies\', \'inject-debug\', \'fonts-debug\']);\\n\\ngulp.task(\'build-release\', [\\n  \'boot-dependencies\',\\n  \'inject-release\',\\n  \'fonts-release\',\\n]);\\n\\n// Use the web.config to determine whether the default task should create a debug or a release build\\n// If the web.config contains this: \'<compilation debug=\\"true\\"\' then we do a default build, otherwise\\n// we do a release build.  It\'s a little hacky but generally works\\nvar fs = require(\'fs\');\\nvar data = fs.readFileSync(__dirname + \'/web.config\', \'UTF-8\');\\nvar inDebug = !!data.match(/<compilation debug=\\"true\\"/);\\n\\ngulp.task(\'default\', [inDebug ? \'build-debug\' : \'build-release\']);\\n```\\n\\nThat\'s a big old lump of code. So let\'s go through this a task by task...\\n\\n### clean\\n\\nDeletes the `build` folder so we have a clean slate to build into.\\n\\n### boot-dependencies\\n\\nCopy across all files that are needed to allow the page to \\"boot\\" / startup. At present this is only jQuery and images.\\n\\n### inject-debug and inject-release\\n\\nThis is the magic. This picks up the launch page (`index.html`), takes the JavaScript and CSS and injects the corresponding `script` and `link` tags into the page and writing it to the `build` folder. Either the original source code or the bundled / minified equivalent will be used depending on whether it\'s debug or release.\\n\\n### scripts-debug and scripts-release\\n\\nHere we collect up the following:\\n\\n- the Bower specified JavaScript files\\n- the TypeScript + associated JavaScript files\\n- and we use our template files to construct a `templates.js` file to prime the Angular template cache\\n\\nIf it\'s the scripts-debug task we copy all these files into the `build/debug` folder. If it\'s the scripts-release task we also bundle, minify and strip the TypeScript out too and copy into the `build/release` folder.\\n\\n### styles-debug and styles-release\\n\\nHere we collect up the following:\\n\\n- the Bower specified CSS files\\n- our own app CSS\\n\\nIf it\'s the styles-debug task we copy all these files into the `build/debug` folder. If it\'s the styles-release task we also bundle and minify and copy into the `build/release` folder.\\n\\n### fonts-debug and fonts-release\\n\\nWhether it\'s the debug or the release build we copy across the font-awesome assets and place them in a location which works for the associated CSS (as the CSS will depend upon font-awesome).\\n\\n### build-debug, build-release and default\\n\\nbuild-debug and build-release (as their name suggests) either perform a build for release or a build for debug. If you remember, the web optimization library in ASP.Net serves up the raw code (\\"debug\\" code) if the `compilation debug` flag in the `web.config` is set to `true`. If it is set to `false` then we get the bundled and minified code (\\"release\\" code) instead. Our default task tries its best to emulate this behaviour by doing a very blunt regex against the `web.config`. Simply, if it can match `&lt;compilation debug=\\"true\\"` then it runs the debug build. Otherwise, the release build. It could be more elegant but there\'s a dearth of XML readers on npm that support synchronous parsing (which you kinda need for this scenario).\\n\\nWhat I intend to do soon is switch from using the web.config to drive the gulp build to using the approach outlined [here](http://www.codecadwallader.com/2015/03/15/integrating-gulp-into-your-tfs-builds-and-web-deploy/). Namely plugging the build directly into Visual Studio\'s build process and using the type of build there.\\n\\nHopefully what I\'ve written here makes it fairly clear how to use Gulp to directly inject scripts and styles directly into your HTML. If you want to look directly at the source then check out the Proverb.Web folder in [this repo](https://github.com/johnnyreilly/proverb-offline)."},{"id":"/2015/02/11/the-convent-with-continuous-delivery","metadata":{"permalink":"/2015/02/11/the-convent-with-continuous-delivery","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2015-02-11-the-convent-with-continuous-delivery/index.md","source":"@site/blog/2015-02-11-the-convent-with-continuous-delivery/index.md","title":"The Convent with Continuous Delivery","description":"I\'ve done it. I\'ve open sourced the website that I maintain for my aunt what is a nun. Because I think we can all agree that nuns need open source and continuous integration about as much as anyone else.","date":"2015-02-11T00:00:00.000Z","formattedDate":"February 11, 2015","tags":[{"label":"Poor Clares","permalink":"/tags/poor-clares"},{"label":"Continuous Delivery","permalink":"/tags/continuous-delivery"},{"label":"Arundel","permalink":"/tags/arundel"},{"label":"AppVeyor","permalink":"/tags/app-veyor"}],"readingTime":3.53,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"The Convent with Continuous Delivery","authors":"johnnyreilly","tags":["Poor Clares","Continuous Delivery","Arundel","AppVeyor"],"hide_table_of_contents":false},"prevItem":{"title":"Using Gulp to inject scripts and styles tags directly into your HTML","permalink":"/2015/02/17/using-gulp-in-asp-net-instead-of-web-optimization"},"nextItem":{"title":"TypeScript: In Praise of Union Types","permalink":"/2015/01/20/typescript-using-functions-with-union-types"}},"content":"I\'ve done it. I\'ve open sourced the [website that I maintain for my aunt what is a nun](http://www.poorclaresarundel.org/). Because I think we can all agree that nuns need open source and continuous integration about as much as anyone else.\\n\\nFor a long time now I\'ve been maintaining a website for one of my (many) aunts that is a Poor Clare. ([That\'s a subtype of \\"nun\\" you OO enthusiasts.](https://en.wikipedia.org/wiki/Subtyping)) It\'s not a terribly exciting site - it\'s mostly static content. It\'s built with a combination of AngularJS / TypeScript / Bootstrap and ASP.Net MVC. It\'s hosted on [Azure Websites](http://azure.microsoft.com/en-us/documentation/services/websites/). In fact I have written about it (slightly more cagily) before [here](https://blog.johnnyreilly.com/2014/06/migrating-from-angularjs-to-angularts.html).\\n\\nI\'ll say up front: presentation-wise the site is not a work of art. However the nuns seem pretty happy with it. (Or perhaps secretly they\'re forgiving me the shonkiness and sparing my feelings - who can say?) If I put my mind to it the site could look much more lovely. But there\'s only so much time I can spare - and that\'s actually one of the reasons I\'ve set up Continuous Delivery.\\n\\n## Why on earth did you bother?\\n\\nWell, you\'d be surprised how often tweaks can be requested. Sometimes it appears to be forgotten for months at a time, and then all of a sudden my inbox is daily filled with a list of minor alterations. You know, slight text changes and the like.\\n\\nSo what I was generally doing was getting home of an evening, waiting until the children were in bed, chomping down some food and then firing up Visual Studio to make the changes and hit \\"Publish\\". Yes that\'s right; I was essentially using Visual Studio to edit text files and push a website out to Azure. The very definition of using a sledgehammer to crack a nut I think we can all agree.\\n\\nIt occurred to me that if I had Continuous Delivery set up then I could make these tweaks and not have to worry about the site being published. Which would be nice. I wouldn\'t need Visual Studio anymore - any text editor would do. Also nice. Finally, if the source control was accessible online then I could probably get away with doing most tweaks on my mobile phone whilst I was travelling home. Timesaver!\\n\\n## How did you go about it?\\n\\nSince [Visual Studio Online (then \\"Team Foundation Service\\")](http://www.visualstudioonline.com) was released I have been using it to host the source code. So the obvious solution was to use the tools offered there to do the deployment. However, this wasn\'t the smooth experience you might have hoped for. I had quite a frustrating afternoon trying things out before deciding it was becoming more trouble than it was worth. VSO appeared to make it supremely hard to customise builds.\\n\\nJust recently though I have been having the most wonderful experience with [AppVeyor](http://www.appveyor.com/). AppVeyor market themselves as _\\"#1 Continuous Delivery service for Windows\\"_ \\\\- I think they\'re right. Their build process is entirely flexible and customisable. It is, in short, a joy to use. (The support is fantastic too - very helpful indeed. Go [Feodor](https://github.com/FeodorFitsner)!)\\n\\nIf you look just below the header you\'ll read a very important sentence: _\\"Free for open-source projects\\"_. You hear that? By the time I\'d finished reading that sentence I\'d decided that the Poor Clares website was about to become an open source project.\\n\\nAnd now it is.\\n\\n## Where is it?\\n\\nThe source on [GitHub](https://github.com/johnnyreilly/poorclaresarundel). The builds and deployment are taken care of by [AppVeyor](https://ci.appveyor.com/project/JohnReilly/poorclaresarundel).\\n\\n## Will you take pull requests?\\n\\nIf they\'re serious, then yes, certainly! My long term plan is to try and get the nuns set up as collaborators in GitHub. That way they can make their own minor tweaks without me getting involved.\\n\\nOn another front, I do wonder if open-sourcing Poor Clares, Arundel might have other hidden benefits. There\'s a number of things I\'m not too keen on in the code. Up until now I think my attitude was possibly \\"it works so that\'s good enough\\". It was only me aware of the shortcomings. Now it\'s public I\'ll probably have more of an incentive to tidy up the rough edges. That\'s the theory anyway - Embarrassment Driven Development anyone? :-)"},{"id":"/2015/01/20/typescript-using-functions-with-union-types","metadata":{"permalink":"/2015/01/20/typescript-using-functions-with-union-types","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2015-01-20-typescript-using-functions-with-union-types/index.md","source":"@site/blog/2015-01-20-typescript-using-functions-with-union-types/index.md","title":"TypeScript: In Praise of Union Types","description":"(& How to Express Functions in UTs)","date":"2015-01-20T00:00:00.000Z","formattedDate":"January 20, 2015","tags":[{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"Option bags","permalink":"/tags/option-bags"},{"label":"Union Types","permalink":"/tags/union-types"},{"label":"Function syntax","permalink":"/tags/function-syntax"}],"readingTime":6.29,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"TypeScript: In Praise of Union Types","authors":"johnnyreilly","tags":["TypeScript","Option bags","Union Types","Function syntax"],"hide_table_of_contents":false},"prevItem":{"title":"The Convent with Continuous Delivery","permalink":"/2015/02/11/the-convent-with-continuous-delivery"},"nextItem":{"title":"Deploying from ASP.Net MVC to GitHub Pages using AppVeyor part 2","permalink":"/2015/01/07/deploying-aspnet-mvc-to-github-pages-with-appveyor-part-2"}},"content":"## (& How to Express Functions in UTs)\\n\\nHave you heard the good news my friend? I refer, of course, to the shipping of TypeScript 1.4 and my _favourite_ language feature since generics.... Union Types.\\n\\nIn the [1\\\\.4 announcement](https://blogs.msdn.com/b/typescript/archive/2015/01/16/announcing-typescript-1-4.aspx) Jonathan Turner described Union Types thusly:\\n\\n> JavaScript functions may take a number of possible argument types. Up to now, we\u2019ve supported this using function overloads. Starting with TypeScript 1.4, we\u2019ve generalized this capability and now allow you to specify that that a value is one of a number of different types using a union type:\\n>\\n> ```ts\\n> function f(x: number | number[]) {\\n>   if (typeof x === \'number\') {\\n>     return x + 10;\\n>   } else {\\n>     // return sum of numbers\\n>   }\\n> }\\n> ```\\n>\\n> Once you have a value of a union type, you can use a typeof and instanceof checks to use the value in a type-safe way. You\'ll notice we use this in the above example and can treat x as a number type inside of the if-block.\\n>\\n> Union types are a new kind of type and work any place you specify a type.\\n\\nLovely right? But what\'s missing? Well, to my mind, the most helpful aspect of Union Types. Definition file creation.\\n\\n## A little history\\n\\n### That\'s right - the days before Union Types are now \\"history\\" :-)\\n\\nWhen creating definition files (`*.d.ts`) in the past there was a problem with TypeScript. A limitation. JavaScript often relies on \\"option bags\\" to pass configuration into a method. An \\"option bag\\" is essentially a JavaScript object literal which contains properties which are used to perform configuration. A good example of this is the `route` parameter passed into Angular\'s ngRoute `<a href=\\"https://docs.angularjs.org/api/ngRoute/provider/$routeProvider#when\\">when</a>` method.\\n\\nI\'d like to draw your attention to 2 of the properties that can be passed in (quoted from the documentation):\\n\\n> - controller \u2013 `{(string|function()=}` \u2013 Controller fn that should be associated with newly created scope or the name of a registered controller if passed as a string.\\n> - template \u2013 `{string=|function()=}` \u2013 html template as a string or a function that returns an html template as a string which should be used by ngView or ngInclude directives. This property takes precedence over templateUrl.\\n>\\n>   If template is a function, it will be called with the following parameters:\\n>\\n>   `{Array.&lt;Object&gt;}` \\\\- route parameters extracted from the current $location.path() by applying the current route\\n\\nBoth of these properties can be of more than 1 type.\\n\\n- `controller` can be a `string`_or_ a `function`.\\n- `template` can be a `string`_or_ a `function` that returns a `string` and has `$routeParams` as a parameter.\\n\\nThere\'s the rub. Whilst it was possible to overload functions in TypeScript pre 1.4, it was <u>not</u>\\n\\npossible to overload interface members. This meant the only way to model these sorts of properties was by seeking out a best common type which would fit all scenarios. This invariably meant using the `any` type. Whilst that worked it didn\'t lend any consuming code a great deal of type safety. Let\'s look at a truncated version of [`angular-route.d.ts`](https://github.com/borisyankov/DefinitelyTyped/blob/c71628e0765eb8e240d8eabd2225f64ea2e2fdb8/angularjs/angular-route.d.ts) for these properties prior to union types:\\n\\n```ts\\ndeclare module ng.route {\\n  // ...\\n\\n  interface IRoute {\\n    /**\\n     * {(string|function()=}\\n     * Controller fn that should be associated with newly created scope or\\n     * the name of a registered controller if passed as a string.\\n     */\\n    controller?: any;\\n\\n    /**\\n     * {string=|function()=}\\n     * Html template as a string or a function that returns an html template\\n     * as a string which should be used by ngView or ngInclude directives. This\\n     * property takes precedence over templateUrl.\\n     *\\n     * If template is a function, it will be called with the following parameters:\\n     *\\n     * {Array.<Object>} - route parameters extracted from the current\\n     * $location.path() by applying the current route\\n     */\\n    template?: any;\\n\\n    // ...\\n  }\\n\\n  // ...\\n}\\n```\\n\\nIt\'s `any` city... Kind of sticks in the craw doesn\'t it?\\n\\n## A new dawn\\n\\nTypeScript 1.4 has shipped and Union Types are with us. We can do better than `any`. So what does [`angular-route.d.ts`](https://github.com/borisyankov/DefinitelyTyped/blob/30ce45e0e706322f34608ab6fa5de141bba59c90/angularjs/angular-route.d.ts) look like now we have Union Types?\\n\\n```ts\\ndeclare module ng.route {\\n  // ...\\n\\n  interface IRoute {\\n    /**\\n     * {(string|function()=}\\n     * Controller fn that should be associated with newly created scope or\\n     * the name of a registered controller if passed as a string.\\n     */\\n    controller?: string | Function;\\n\\n    /**\\n     * {string=|function()=}\\n     * Html template as a string or a function that returns an html template\\n     * as a string which should be used by ngView or ngInclude directives. This\\n     * property takes precedence over templateUrl.\\n     *\\n     * If template is a function, it will be called with the following parameters:\\n     *\\n     * {Array.<Object>} - route parameters extracted from the current\\n     * $location.path() by applying the current route\\n     */\\n    template?:\\n      | string\\n      | { ($routeParams?: ng.route.IRouteParamsService): string };\\n\\n    // ...\\n  }\\n\\n  // ...\\n}\\n```\\n\\nWith these changes in place we are now accurately modelling the `route` option bags in TypeScript. Hoorah!!!\\n\\nLet\'s dig in a little. If you look at the `controller` definition it\'s pretty straightforward. `string|Function` \\\\- clearly the `controller` can be a `string`_or_ a `Function`. Simple.\\n\\nNow let\'s look at the `template` definition by itself:\\n\\n```ts\\ntemplate?: string | { ($routeParams?: ng.route.IRouteParamsService) : string; }\\n```\\n\\nAs with the `controller` the `template` can be a string - that is pretty clear. But what\'s that hovering on the other side of the \\"\\\\|\\"? What could `{ ($routeParams?: ng.route.IRouteParamsService) : string; }` be exactly?\\n\\nWell, in a word, it\'s a `Function`. The `controller` would allow any kind of function at all. However the `template` definition is deliberately more restrictive. This defines a function which must return a `string` and which receives an optional parameter of `$routeParams` of type `ng.route.IRouteParamsService`.\\n\\n## State of the Union\\n\\nHopefully you can now see just how useful Union Types are and how you can express specific sorts of function definitions as part of a Union Type.\\n\\nThe thing that prompted me first to write this post was seeing that there don\'t appear to be any examples out there of how to express functions inside Union Types. I only landed on the syntax myself after a little experimentation in Visual Studio after I\'d installed TS 1.4. I\'ve started work on bringing Union Types to the typings inside [DefinitelyTyped](https://github.com/borisyankov/DefinitelyTyped) and so you\'ll start to see them appearing more and more. But since it\'s rather \\"hidden knowledge\\" at present I wanted to do my bit to make it a little better known.\\n\\nAs [Daniel](https://twitter.com/Rickenhacker) helpfully points out in the comments there is an alternate syntax - lambda style. So instead of this:\\n\\n```ts\\ntemplate?: string | { ($routeParams?: ng.route.IRouteParamsService) : string; }\\n```\\n\\nYou could write this:\\n\\n```ts\\ntemplate?: string | (($routeParams?: ng.route.IRouteParamsService) => string);\\n```\\n\\nJust remember to place parentheses around the lambda to clearly delineate it.\\n\\n## Bonfire of the Overloads\\n\\nBefore I sign off I should mention the ability Union Types give you to define a much terser definition file. Basically the \\"\\\\|\\" operator makes for a bonfire of the overloads. Where you previously may have had 6 overloads for the same method (each with identical JSDoc) you now only need 1. Which is beautiful (and DRY).\\n\\nIt\'s surprising just what a difference it makes. This is [`jQuery.d.ts`](https://github.com/borisyankov/DefinitelyTyped/blob/9bd7fe69d98337db56144c3da131d413f5b7e895/jquery/jquery.d.ts) last week (pre TypeScript 1.4). This is [`jQuery.d.ts`](https://github.com/borisyankov/DefinitelyTyped/blob/9f64372a065541fe2b8f6c5c5cd9b55a1d631f19/jquery/jquery.d.ts) now - with Union Types aplenty. Last week it was \\\\~4000 lines of code. This week it\'s \\\\~3200 lines of code. With the same functionality. Union Types are _FANTASTIC_!"},{"id":"/2015/01/07/deploying-aspnet-mvc-to-github-pages-with-appveyor-part-2","metadata":{"permalink":"/2015/01/07/deploying-aspnet-mvc-to-github-pages-with-appveyor-part-2","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2015-01-07-deploying-aspnet-mvc-to-github-pages-with-appveyor-part-2/index.md","source":"@site/blog/2015-01-07-deploying-aspnet-mvc-to-github-pages-with-appveyor-part-2/index.md","title":"Deploying from ASP.Net MVC to GitHub Pages using AppVeyor part 2","description":"\\"Automation, automation, automation.\\" Those were and are Tony Blair\'s priorities for keeping open source projects well maintained.","date":"2015-01-07T00:00:00.000Z","formattedDate":"January 7, 2015","tags":[{"label":"GitHub Personal Access Token","permalink":"/tags/git-hub-personal-access-token"},{"label":"Continuous Integration","permalink":"/tags/continuous-integration"},{"label":"powershell","permalink":"/tags/powershell"},{"label":"github pages","permalink":"/tags/github-pages"},{"label":"AppVeyor","permalink":"/tags/app-veyor"}],"readingTime":4.775,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Deploying from ASP.Net MVC to GitHub Pages using AppVeyor part 2","authors":"johnnyreilly","tags":["GitHub Personal Access Token","Continuous Integration","powershell","github pages","AppVeyor"],"hide_table_of_contents":false},"prevItem":{"title":"TypeScript: In Praise of Union Types","permalink":"/2015/01/20/typescript-using-functions-with-union-types"},"nextItem":{"title":"Deploying from ASP.Net MVC to GitHub Pages using AppVeyor part 1","permalink":"/2014/12/29/deploying-aspnet-mvc-to-github-pages-with-appveyor-part-1"}},"content":"\\"Automation, automation, automation.\\" Those were and are Tony Blair\'s priorities for keeping open source projects well maintained.\\n\\nOK, that\'s not quite true... But what is certainly true is that maintaining an open source project takes time. And there\'s only so much free time that anyone has. For that reason, wherever you can it makes sense to _AUTOMATE_!\\n\\n[Last time](https://blog.johnnyreilly.com/2014/12/deploying-aspnet-mvc-to-github-pages-with-appveyor-part-1.html) we looked at how you can take an essentially static ASP.Net MVC site (in this case my jVUNDemo documentation site) and generate an entirely static version using Wget. This static site has been pushed to [GitHub Pages](https://pages.github.com/) and is serving as the documentation for [jQuery Validation Unobtrusive Native](http://johnnyreilly.github.io/jQuery.Validation.Unobtrusive.Native/) (and for bonus points is costing me no money at all).\\n\\nSo what next? Well, automation clearly! If I make a change to jQuery Validation Unobtrusive Native then AppVeyor already bounds in and performs a [continuous integration build](https://ci.appveyor.com/project/JohnReilly/jquery-validation-unobtrusive-native) for me. It picks up the [latest source](https://github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native) from GitHub, pulls in my dependencies, performs a build and runs my tests. Lovely.\\n\\nSo the obvious thing to do is to take this process and plug in the generation of my static site and the publication thereof to GitHub pages. The minute a change is made to my project the documentation should be updated without me having to break sweat. That\'s the goal.\\n\\n## GitHub Personal Access Token\\n\\nIn order to complete our chosen mission we\'re going to need a GitHub Personal Access Token. We\'re going to use it when we clone, update and push our GitHub Pages branch. To get one we biff over to Settings / Applications in GitHub and click the \\"Generate New Token\\" button.\\n\\n![](GitHubApplicationSettings.png)\\n\\nThe token I\'m using for my project has the following scopes selected:\\n\\n![](GitHub-Personal-Access-Token.png)\\n\\n## `appveyor.yml`\\n\\nWith our token in hand we turn our attention to AppVeyor build configuration. This is possible using a file called [`appveyor.yml`](http://www.appveyor.com/docs/build-configuration) stored in the root of your repo. You can also use the AppVeyor web UI to do this. However, for the purposes of ease of demonstration I\'m using the file approach. The [jQuery Validation Unobtrusive Native `appveyor.yml`](https://github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native/blob/master/appveyor.yml) looks like this:\\n\\n```yml\\n---\\n#---------------------------------#\\n#      general configuration      #\\n#---------------------------------#\\n\\n# version format\\nversion: 1.0.{build}\\n\\n#---------------------------------#\\n#    environment configuration    #\\n#---------------------------------#\\n\\n# environment variables\\nenvironment:\\n  GithubEmail: johnny_reilly@hotmail.com\\n  GithubUsername: johnnyreilly\\n  GithubPersonalAccessToken:\\n    secure: T4M/N+e/baksVoeWoYKPWIpfahOsiSFw/+Zc81VuThZmWEqmrRtgEHUyin0vCWhl\\n\\nbranches:\\n  only:\\n    - master\\n\\ninstall:\\n  - ps: choco install wget\\n\\nbuild:\\n  verbosity: minimal\\n\\nafter_test:\\n  - ps: ./makeStatic.ps1 $env:APPVEYOR_BUILD_FOLDER\\n  - ps: ./pushStatic.ps1 $env:APPVEYOR_BUILD_FOLDER $env:GithubEmail $env:GithubUsername $env:GithubPersonalAccessToken\\n```\\n\\nThere\'s a number of things you should notice from the yml file:\\n\\n- We create 3 environment variables: GithubEmail, GithubUsername and GithubPersonalAccessToken (more on this in a moment).\\n- We only build the master branch.\\n- We use [Chocolatey](https://chocolatey.org/packages/Wget) to install Wget which is used by the `makeStatic.ps1` Powershell script.\\n- After the tests have completed we run 2 Powershell scripts. First [`makeStatic.ps1`](https://github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native/blob/master/makeStatic.ps1) which builds the static version of our site. This is the exact same script we discussed in the previous post - we\'re just passing it the build folder this time (one of AppVeyor\'s environment variables). Second, we run [`pushStatic.ps1`](https://github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native/blob/master/pushStatic.ps1) which publishes the static site to GitHub Pages.\\n\\nWe pass 4 arguments to `pushStatic.ps1`: the build folder, my email address, my username and my personal access token. For the sake of security the GithubPersonalAccessToken has been encrypted as indicated by the `secure` keyword. This is a capability available in AppVeyor [here](https://ci.appveyor.com/tools/encrypt).\\n\\n![](AppVeyor-encrypt.png)\\n\\nThis allows me to mask my personal access token rather than have it available as free text for anyone to grab.\\n\\n## `pushStatic.ps1`\\n\\nFinally we can turn our attention to how our Powershell script `pushStatic.ps1` goes about pushing our changes up to GitHub Pages:\\n\\n```ps\\nparam([string]$buildFolder, [string]$email, [string]$username, [string]$personalAccessToken)\\n\\nWrite-Host \\"- Set config settings....\\"\\ngit config --global user.email $email\\ngit config --global user.name $username\\ngit config --global push.default matching\\n\\nWrite-Host \\"- Clone gh-pages branch....\\"\\ncd \\"$($buildFolder)\\\\..\\\\\\"\\nmkdir gh-pages\\ngit clone --quiet --branch=gh-pages https://$($username):$($personalAccessToken)@github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native.git .\\\\gh-pages\\\\\\ncd gh-pages\\ngit status\\n\\nWrite-Host \\"- Clean gh-pages folder....\\"\\nGet-ChildItem -Attributes !r | Remove-Item -Recurse -Force\\n\\nWrite-Host \\"- Copy contents of static-site folder into gh-pages folder....\\"\\ncopy-item -path ..\\\\static-site\\\\* -Destination $pwd.Path -Recurse\\n\\ngit status\\n$thereAreChanges = git status | select-string -pattern \\"Changes not staged for commit:\\",\\"Untracked files:\\" -simplematch\\nif ($thereAreChanges -ne $null) {\\n    Write-host \\"- Committing changes to documentation...\\"\\n    git add --all\\n    git status\\n    git commit -m \\"skip ci - static site regeneration\\"\\n    git status\\n    Write-Host \\"- Push it....\\"\\n    git push --quiet\\n    Write-Host \\"- Pushed it good!\\"\\n}\\nelse {\\n    write-host \\"- No changes to documentation to commit\\"\\n}\\n```\\n\\nSo what\'s happening here? Let\'s break it down:\\n\\n- Git is configured with the passed in username and email address.\\n- A folder is created that sits alongside the build folder called \\"gh-pages\\".\\n- We clone the \\"gh-pages\\" branch of jQuery Validation Unobtrusive Native into our \\"gh-pages\\" directory. You\'ll notice that we are using our GitHub personal access token in the clone URL itself.\\n- We delete the contents of the \\"gh-pages\\" directory leaving it empty.\\n- We copy across the contents of the \\"static-site\\" folder (created by `makeStatic.ps1`) into the \\"gh-pages\\".\\n- We use `git status` to check if there are any changes. (This method is completely effective but a little crude to my mind - there\'s probably better approaches to this.... shout me in the comments if you have a suggestion.)\\n- If we have no changes then we do nothing.\\n- If we have changes then we stage them, commit them and push them to GitHub Pages. Then we sign off with an allusion to [80\'s East Coast hip-hop](<https://en.wikipedia.org/wiki/Push_It_(Salt-n-Pepa_song)>)... \'Cos that\'s how we roll.\\n\\nWith this in place, any changes to the docs will be automatically published out to our \\"gh-pages\\" branch. Our documentation will always be up to date thanks to the goodness of AppVeyor\'s Continuous Integration service."},{"id":"/2014/12/29/deploying-aspnet-mvc-to-github-pages-with-appveyor-part-1","metadata":{"permalink":"/2014/12/29/deploying-aspnet-mvc-to-github-pages-with-appveyor-part-1","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2014-12-29-deploying-aspnet-mvc-to-github-pages-with-appveyor-part-1/index.md","source":"@site/blog/2014-12-29-deploying-aspnet-mvc-to-github-pages-with-appveyor-part-1/index.md","title":"Deploying from ASP.Net MVC to GitHub Pages using AppVeyor part 1","description":"There\'s a small open source project I\'m responsible for called jQuery Validation Unobtrusive Native. (A catchy name is a must for any good open source project. Alas I\'m not quite meeting my own exacting standards on this particular point... I should have gone with my gut and called it \\"Livingstone\\" instead. Too late now...)","date":"2014-12-29T00:00:00.000Z","formattedDate":"December 29, 2014","tags":[{"label":"wget","permalink":"/tags/wget"},{"label":"powershell","permalink":"/tags/powershell"},{"label":"github pages","permalink":"/tags/github-pages"},{"label":"AppVeyor","permalink":"/tags/app-veyor"}],"readingTime":5.8,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Deploying from ASP.Net MVC to GitHub Pages using AppVeyor part 1","authors":"johnnyreilly","tags":["wget","powershell","github pages","AppVeyor"],"hide_table_of_contents":false},"prevItem":{"title":"Deploying from ASP.Net MVC to GitHub Pages using AppVeyor part 2","permalink":"/2015/01/07/deploying-aspnet-mvc-to-github-pages-with-appveyor-part-2"},"nextItem":{"title":"Gulp, npm, long paths and Visual Studio.... Fight!","permalink":"/2014/12/12/gulp-npm-long-paths-and-visual-studio-fight"}},"content":"There\'s a small open source project I\'m responsible for called [jQuery Validation Unobtrusive Native](https://github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native). (A catchy name is a must for any good open source project. Alas I\'m not quite meeting my own exacting standards on this particular point... I should have gone with my gut and called it \\"Livingstone\\" instead. Too late now...)\\n\\nThe project itself is fairly simple in purpose. It\'s essentially a bridge between ASP.Net MVC\'s inbuilt support for driving validation from data attributes and jQuery Validation\'s native support for the same. It is, in the end, a collection of ASP.Net MVC HTML helper extensions. It is not massively complicated.\\n\\nI believe it was Tony Blair that said \\"documentation, documentation, documentation\\" were his priorities for open source projects. Or maybe it was someone else... Anyway, the point stands. Documentation is supremely important if you want your project to be in any way useful to anyone other than yourself. A project, no matter how fantastic, which lacks decent documentation is a missed opportunity.\\n\\nAnyway I\'m happy to say that jQuery Validation Unobtrusive Native _has_ documentation! And pretty good documentation at that. The documentation takes the form of the [jVUNDemo](https://github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native/tree/master/jVUNDemo) project which is part of the jQuery Validation Unobtrusive Native repo. jVUNDemo is an ASP.Net MVC web application which is built on top of the jQuery Validation Unobtrusive Native helpers. It demonstrates the helpers in action and documents how you might go about using them. It looks a bit like this:\\n\\n![](Screenshot-2014-12-29-06.22.46.png)\\n\\nWhen I first put jVUNDemo together I hosted it on Azure so the world could see it in all it\'s finery. And that worked just fine. However, there\'s something you ought to know about me:\\n\\n## I\'m quite cheap\\n\\nNo really, I am. My attention was grabbed by the essentially \\"free\\" nature of [GitHub Pages](https://pages.github.com/). I was immediately seized by the desire to somehow deploy jVUNDemo to GitHub Pages and roll around joyfully in all that lovely free hosting.\\n\\n\\"But\\", I hear you cry, \\"you can\'t deploy an ASP.Net MVC application to Git Hub Pages... It only hosts static sites!\\" Quite right. Sort of. This is where I get to pull my ace of spades: jVUNDemo is not really an \\"app\\" so much as a static site. Yup, once the HTML that makes up each page is generated there isn\'t any app like business to do. It\'s just a collection of text and 1 screen demo\'s really.\\n\\nThat being the case, there\'s no reason why I shouldn\'t be able to make use of GitHub Pages. So that\'s what I decided to do. Whilst I was at it I also wanted to automate the deployment process. When I tweak jVUNDemo I don\'t want to have to manually push out a new version of jVUNDemo to wherever it\'s being hosted. No, I\'m a developer so I\'ll automate it.\\n\\nI\'ve broken this up into 2 posts. This first one will cover how you generate a static site from an ASP.Net MVC site. The second will be about how to use [AppVeyor](http://www.appveyor.com/) to ensure that on each build your documentation is getting republished.\\n\\n## You Wget me?\\n\\nSo, static site generation. There\'s a well known Unix utility called [Wget](https://en.wikipedia.org/wiki/Wget) which covers exactly that ground and so we\'re going to use it. It downloads and saves HTML, it recursively walks the links inside the site and grabs those pages too and it converts our routes so they are locally browsable (\\"Demo/Required\\" becomes \\"Demo/Required.html\\").\\n\\nYou can use [Chocolatey](https://chocolatey.org/packages/Wget) to get a copy of Wget. We\'re also going to need IIS Express to host jVUNDemo whilst Wget converts it. Once jVUNDemo has been built successfully you should be be able to kick off the process like so:\\n\\n```ps\\ncd C:\\\\projects\\\\jquery-validation-unobtrusive-native\\n.\\\\makeStatic.ps1 $pwd.path\\n```\\n\\nThis invokes the `makeStatic` Powershell script in the root of the jQuery Validation Unobtrusive Native repo:\\n\\n```ps\\nparam([string]$buildFolder)\\n\\n$jVUNDemo = \\"$($buildFolder)\\\\jVUNDemo\\"\\n$staticSiteParentPath = (get-item $buildFolder).Parent.FullName\\n$staticSite = \\"static-site\\"\\n$staticSitePath = \\"$($staticSiteParentPath)\\\\$($staticSite)\\"\\n$wgetLogPath = \\"$($staticSiteParentPath)\\\\wget.log\\"\\n$port = 57612\\n$servedAt = \\"http://localhost:$($port)/\\"\\nwrite-host \\"jVUNDemo location: $jVUNDemo\\"\\nwrite-host \\"static site parent location: $staticSiteParentPath\\"\\nwrite-host \\"static site location: $staticSitePath\\"\\nwrite-host \\"wget log path: $wgetLogPath\\"\\n\\nwrite-host \\"Spin up jVUNDemo site at $($servedAt)\\"\\n$process = Start-Process \'C:\\\\Program Files (x86)\\\\IIS Express\\\\iisexpress.exe\' -NoNewWindow -ArgumentList \\"/path:$($jVUNDemo) /port:$($port)\\"\\n\\nwrite-host \\"Wait a moment for IIS to startup\\"\\nStart-sleep -s 15\\n\\nif (Test-Path $staticSitePath) {\\n    write-host \\"Removing $($staticSitePath)...\\"\\n    Remove-Item -path $staticSitePath -Recurse -Force\\n}\\n\\nwrite-host \\"Create static version of demo site here: $($staticSitePath)\\"\\nPush-Location $staticSiteParentPath\\n# 2>&1 used to combine stderr and stdout\\nwget.exe --recursive --convert-links -E --directory-prefix=$staticSite --no-host-directories $servedAt > $wgetLogPath 2>&1\\nwrite-host \\"lastExitCode: $($lastExitCode)\\"\\ncat $wgetLogPath\\nPop-Location\\n\\nwrite-host \\"Shut down jVUNDemo site\\"\\nstop-process -Name iisexpress\\n\\nif (Test-Path $staticSitePath) {\\n    write-host \\"Contents of $($staticSitePath)\\"\\n    ls $staticSitePath\\n}\\n```\\n\\nThe above Powershell does the following:\\n\\n1. Starts up IIS Express serving jVUNDemo on http://localhost:57612/\\n2. Waits 15 seconds for IIS Express to get itself together (probably a shorter wait time would be sufficient)\\n3. Points Wget at jVUNDemo and bellows \\"go!!!!\\"\\n4. Wget downloads and saves the static version of jVUNDemo to a directory called \\"static-site\\"\\n5. Stops IIS Express\\n6. Prints out the contents of the \\"static-site\\" directory\\n\\nWhen run, it pumps something like this out:\\n\\n```\\njVUNDemo location: C:\\\\projects\\\\jquery-validation-unobtrusive-native\\\\jVUNDemo\\nstatic site parent location: C:\\\\projects\\nstatic site location: C:\\\\projects\\\\static-site\\nwget log path: C:\\\\projects\\\\wget.log\\nSpin up jVUNDemo site at http://localhost:57612/\\nWait a moment for IIS to startup\\nCreate static version of demo site here: C:\\\\projects\\\\static-site\\nwget.exe : --2014-12-29 07:49:56--  http://localhost:57612/\\nResolving localhost...\\n127.0.0.1\\nConnecting to localhost|127.0.0.1|:57612... connected.\\nHTTP request sent, awaiting response...\\n200 OK\\n\\n..... lots of HTTP requests.....\\n\\nDownloaded: 30 files, 1.0M in 0.09s (10.8 MB/s)\\nConverting static-site/Demo/CreditCard.html... 34-0\\nConverting static-site/Demo/Number.html... 34-0\\nConverting static-site/Demo/Range.html... 34-0\\nConverting static-site/Demo/Email.html... 34-0\\nConverting static-site/AdvancedDemo/CustomValidation.html... 35-0\\nConverting static-site/Demo/Date.html... 36-0\\nConverting static-site/Home/License.html... 27-0\\nConverting static-site/index.html... 29-0\\nConverting static-site/AdvancedDemo/Dynamic.html... 35-0\\nConverting static-site/Demo/MaxLengthMinLength.html... 34-0\\nConverting static-site/Demo/Required.html... 34-0\\nConverting static-site/AdvancedDemo.html... 32-0\\nConverting static-site/Demo/Remote.html... 35-0\\nConverting static-site/Demo/EqualTo.html... 34-0\\nConverting static-site/AdvancedDemo/Globalize.html... 38-0\\nConverting static-site/Demo/Url.html... 34-0\\nConverting static-site/Demo.html... 37-0\\nConverting static-site/Home/GettingStarted.html... 29-0\\nConverting static-site/Home/Download.html... 27-0\\nConverting static-site/AdvancedDemo/Tooltip.html... 34-0\\nConverted 20 files in 0.03 seconds.\\n\\nShut down jVUNDemo site\\nContents of C:\\\\projects\\\\static-site\\n\\n\\n    Directory: C:\\\\projects\\\\static-site\\n\\n\\nMode                LastWriteTime     Length Name\\n----                -------------     ------ ----\\nd----        12/29/2014   7:50 AM            AdvancedDemo\\nd----        12/29/2014   7:50 AM            Content\\nd----        12/29/2014   7:50 AM            Demo\\nd----        12/29/2014   7:50 AM            Home\\nd----        12/29/2014   7:50 AM            Scripts\\n-a---        12/29/2014   7:50 AM       5967 AdvancedDemo.html\\n-a---        12/29/2014   7:50 AM       6802 Demo.html\\n-a---        12/29/2014   7:47 AM      12862 favicon.ico\\n-a---        12/29/2014   7:50 AM       8069 index.html\\n```\\n\\nAnd that\'s it for part 1 my friends! You now have a static version of the ASP.Net MVC site to dazzle the world with. I should say for the purposes of full disclosure that there are 2 pages in the site which are not entirely \\"static\\" friendly. For these 2 pages I\'ve put messages in that are displayed when the page is served up in a static format explaining the limitations. Their full glory can still be experienced by cloning the project and running locally.\\n\\n[Next time](https://blog.johnnyreilly.com/2015/01/deploying-aspnet-mvc-to-github-pages-with-appveyor-part-2.html) we\'ll take the mechanism detailed above and plug it into AppVeyor for some Continuous Integration happiness."},{"id":"/2014/12/12/gulp-npm-long-paths-and-visual-studio-fight","metadata":{"permalink":"/2014/12/12/gulp-npm-long-paths-and-visual-studio-fight","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2014-12-12-gulp-npm-long-paths-and-visual-studio-fight/index.md","source":"@site/blog/2014-12-12-gulp-npm-long-paths-and-visual-studio-fight/index.md","title":"Gulp, npm, long paths and Visual Studio.... Fight!","description":"How I managed to gulp-angular-templatecache working inside Visual Studio","date":"2014-12-12T00:00:00.000Z","formattedDate":"December 12, 2014","tags":[{"label":"npm","permalink":"/tags/npm"},{"label":"Visual Studio","permalink":"/tags/visual-studio"},{"label":"long paths","permalink":"/tags/long-paths"},{"label":"gulp-angular-templatecache","permalink":"/tags/gulp-angular-templatecache"}],"readingTime":2.71,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Gulp, npm, long paths and Visual Studio.... Fight!","authors":"johnnyreilly","tags":["npm","Visual Studio","long paths","gulp-angular-templatecache"],"hide_table_of_contents":false},"prevItem":{"title":"Deploying from ASP.Net MVC to GitHub Pages using AppVeyor part 1","permalink":"/2014/12/29/deploying-aspnet-mvc-to-github-pages-with-appveyor-part-1"},"nextItem":{"title":"What\'s in a (Domain) Name?","permalink":"/2014/12/05/whats-in-a-name"}},"content":"## <sub>How I managed to gulp-angular-templatecache working inside Visual Studio</sub>\\n\\nEvery now and then something bites you unexpectedly. After a certain amount of pain, the answer comes to you and you know you want to save others from falling into the same deathtrap.\\n\\nThere I was minding my own business and having a play with a Gulp plugin called [gulp-angular-templatecache](https://www.npmjs.com/package/gulp-angular-templatecache). If you\'re not aware of it, it \\"Concatenates and registers AngularJS templates in the $templateCache\\". I was planning to use it so that all the views in an [Angular app of mine](https://github.com/johnnyreilly/proverb-offline) were loaded up-front rather than on demand. (It\'s a first step in making an \\"offline-first\\" version of that particular app.)\\n\\nI digress already. No sooner had I tapped in:\\n\\n```ps\\nnpm install gulp-angular-templatecache --saveDev\\n```\\n\\nThen I noticed my Visual Studio project was no longer compiling. It was dying a death on build with this error:\\n\\n```ps\\nASPNETCOMPILER : error ASPRUNTIME: The specified path, file name, or both are too long. The fully qualified file name must be less than 260 characters, and the directory name must be less than 248 characters.\\n```\\n\\nI was dimly aware that there were issues with the nested [node_modules](https://github.com/joyent/node/issues/6960) leading to Windows-killing paths. This sounded just like that.... And it was! `gulp-angular-templatecache` had a dependency on `gulp-footer` which had a dependency on `lodash.assign` which had a dependency on `lodash._basecreatecallback` which had.... You see where I\'m going? It seems that the lovely lodash has created the path from hell.\\n\\nFor reasons that aren\'t particularly clear this kills Visual Studio\'s build process. This is slightly surprising given that our rogue path is sat in the `node_modules` directory which isn\'t part of the project in Visual Studio. That being the case you\'d imagine that you could do what you liked there. But no, it seems VS is a delicate flower and we must be careful not to offend. Strange.\\n\\n## It\'s Workaround Time!\\n\\nAfter a _great deal_ of digging I found the answer nestled in the middle of an [answer on Stack Overflow](http://stackoverflow.com/a/24144479/761388). To quote:\\n\\n> If you will add \\"lodash.bind\\" module to your project\'s package.json as dependency it will be installed in one level with gulp and not as gulp\'s dependency\\n\\nThat\'s right, I just needed to tap enter this at the root of my project:\\n\\n```ps\\nnpm install lodash.bind --saveDev\\n```\\n\\nAnd all was sweetness and light once more - no more complaints from VS.\\n\\n## The Future\\n\\nIt looks like lodash are [on course to address this issue](https://github.com/lodash/lodash-cli/issues/23). So one day this this workaround won\'t be necessary anymore which is good.\\n\\nHowever, the general long path issue concerning node / npm hasn\'t vanished for Windows users. Given VS 2015 is planning to make Gulp and Grunt 1st class citizens of Visual Studio I\'m going to guess that sort of issue is likely to arise again and again for other packages. I\'m hoping that means that someone will actually fix the underlying path issues that upset Windows with npm.\\n\\nIt sounds like npm are planning to take [some steps](https://github.com/joyent/node/issues/6960#issuecomment-46704998) which is great. But I can\'t be alone in having a slightly nagging feeling that Windows isn\'t quite a first class citizen for node / io / npm yet. I really hope it will become one."},{"id":"/2014/12/05/whats-in-a-name","metadata":{"permalink":"/2014/12/05/whats-in-a-name","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2014-12-05-whats-in-a-name/index.md","source":"@site/blog/2014-12-05-whats-in-a-name/index.md","title":"What\'s in a (Domain) Name?","description":"The observant amongst you may have noticed that this blog has a brand new and shiny domain name! That\'s right, after happily trading under \\"icanmakethiswork.blogspot.com\\" for the longest time it\'s now \\"blog.icanmakethiswork.io\\". Trumpets and fanfare!","date":"2014-12-05T00:00:00.000Z","formattedDate":"December 5, 2014","tags":[{"label":"cybersquatting","permalink":"/tags/cybersquatting"}],"readingTime":2.605,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"What\'s in a (Domain) Name?","authors":"johnnyreilly","tags":["cybersquatting"],"hide_table_of_contents":false},"prevItem":{"title":"Gulp, npm, long paths and Visual Studio.... Fight!","permalink":"/2014/12/12/gulp-npm-long-paths-and-visual-studio-fight"},"nextItem":{"title":"Pretending to be someone you\'re not and the dark pit of despair","permalink":"/2014/11/26/Coded-UI-IE-11-and-the-runas-problem"}},"content":"The observant amongst you may have noticed that this blog has a brand new and shiny domain name! That\'s right, after happily trading under \\"icanmakethiswork.blogspot.com\\" for the longest time it\'s now \\"blog.icanmakethiswork.io\\". Trumpets and fanfare!\\n\\nWhy the change? Well let\'s break that question down a little. First of all, why change at all? Secondly, why change to blog.icanmakethiswork.io?\\n\\n## Why do things have to change at all?\\n\\nI mean, weren\'t we happy? Wasn\'t it all good? Well quite. For the record, I have no complaints of Blogger who have hosted my blog since it began. They\'ve provided good tools and a good service and I\'m happy with them.\\n\\nThat said, I\'ve been toying with the idea for a while now of trying out a few other blogging solutions - possibly even hosting it myself. Whilst my plans are far from definite at the moment I\'m aware that I don\'t own icanmakethiswork.blogspot.com - I can\'t take it with me. So if I want to make a move to change my blogging solution a first step is establishing my own domain name for my blog. I\'ve done that now. If and when I up sticks, people will hopefully come with me as the URL for my blog should not change.\\n\\nAlso, in the back of my mind I\'m aware that Google owns Blogger. Given their recent spate of closing services it\'s certainly possible that the Google reaper could one day call for Blogger. So it makes sense to be ready to move my blog elsewhere should that day come.\\n\\n## Why blog.icanmakethiswork.io?\\n\\nWhy indeed? And why the \\".io\\" suffix? Doesn\'t that just make you a desperate follower of fashion?\\n\\nGood questions all, and \\"no, I hope not\\". My original plans were to use the domain name \\"icanmakethiswork.com\\". icanmakethiswork was the name of the blog and it made sense to keep it in the URL. So off I went to register the domain name when to my surprise I discovered this:\\n\\n![](Screenshot-2014-12-05-05.39.00.png)\\n\\nMy domain is being [cybersquatted](https://en.wikipedia.org/wiki/Cybersquatting)! I mean.... What??!!!!\\n\\nI started to wonder \\"is there another icanmakethiswork out there\\"? Am I not the [one and only](http://youtu.be/z8f2mW1GFSI)? So I checked with DuckDuckGo (\\"The search engine that doesn\'t track you.\\") and look what I found:\\n\\n![](Screenshot-2014-12-05-05.41.59.png)\\n\\nA whole screen of me. Just me.\\n\\nAs of June 3rd 2014 someone has been sitting on my blog name. I was actually rather outraged by this. I became even more so as I discovered that there was a mechanism (not free) by which I could try and buy it off the squatter. I could instead be like my life idol Madonna and go to court to get it back. But frankly in this sense I\'m more like Rachel Green in Friends; not litigous.\\n\\nSo that\'s why I went for icanmakethiswork.io instead. Path of least resistance and all that. I\'d still like icanmakethiswork.com to be mine but I\'m not going to court and I\'m not paying the squatter. Maybe one day I\'ll get it. Who knows?\\n\\nEither way, from now on this is blog.icanmakethiswork.io - please stick around!\\n\\n## Is anything else going to change?\\n\\nNot for now, no."},{"id":"/2014/11/26/Coded-UI-IE-11-and-the-runas-problem","metadata":{"permalink":"/2014/11/26/Coded-UI-IE-11-and-the-runas-problem","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2014-11-26-Coded-UI-IE-11-and-the-runas-problem/index.md","source":"@site/blog/2014-11-26-Coded-UI-IE-11-and-the-runas-problem/index.md","title":"Pretending to be someone you\'re not and the dark pit of despair","description":"(Coded UI, IE 11 and the \\"runas\\" problem)","date":"2014-11-26T00:00:00.000Z","formattedDate":"November 26, 2014","tags":[{"label":"IE 11","permalink":"/tags/ie-11"},{"label":"runas","permalink":"/tags/runas"},{"label":"Coded UI","permalink":"/tags/coded-ui"},{"label":"nomerge","permalink":"/tags/nomerge"},{"label":"Internet Exporer","permalink":"/tags/internet-exporer"}],"readingTime":10.425,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Pretending to be someone you\'re not and the dark pit of despair","authors":"johnnyreilly","tags":["IE 11","runas","Coded UI","nomerge","Internet Exporer"],"hide_table_of_contents":false},"prevItem":{"title":"What\'s in a (Domain) Name?","permalink":"/2014/12/05/whats-in-a-name"},"nextItem":{"title":"Using Gulp in Visual Studio instead of Web Optimization","permalink":"/2014/11/04/using-gulp-in-visual-studio-instead-of-web-optimization"}},"content":"## <sub>(Coded UI, IE 11 and the \\"runas\\" problem)</sub>\\n\\n\\"I\'m not angry, I\'m just disappointed.\\"\\n\\nThat\'s kind of how I feel about Coded UI tests. It may well be that you\'ve never heard of them - in my experience very few people seem to be aware of them. What are they? Well, I\'ve never used [Selenium](http://www.seleniumhq.org/) but as best I understand Coded UI is Microsoft\'s own version of that. Namely it\'s a way to automate testing, in my case browser-based testing. You can write a suite of tests that will spin up your application and test it out, going from screen to screen, URL to URL and asserting all is as you would expect.\\n\\nThe project that I\'m currently working on has a pretty comprehensive set of tests covering the use of the application. Each night as the clock strikes midnight a lonely computer in the West End of London whirrs into life and runs the full suite. It takes about 8 hours and at the end a report slips into your inbox letting you know of any failures.\\n\\n## Sounds brilliant right? How could someone not love this?\\n\\nWell a number of reasons. First of all, _it takes 8 hours_!!!! That\'s a long time; I\'d rather learn what I broke today rather than tomorrow.\\n\\nAlso, and this is probably more significant, Coded UI tests are pretty flaky. Let me qualify that. For a test to be particularly useful it has to be quick, repeatable and reliable. As I\'ve said, Coded UI tests are not quick.\\n\\nBy their very nature integration tests (of which Coded UI tests are a type) can never be entirely reliably repeatable. They test your app in it\'s entirety. So, for example, if a 3rd party service goes down for 5 minutes then you will get failed tests. You\'ll burn time investigating these false positives.\\n\\nFurther to that, Coded UI tests are repeatable, except when they\'re not. I\'ve seen colleagues reduced to near tears by incredible sensitivity of Coded UI tests. Out of the box Coded UI tests appear to ship with the [\\"Works on my machine\\"](http://blog.codinghorror.com/the-works-on-my-machine-certification-program/) guarantee. It requires far more effort that you\'d expect to come up with tests that can be reliably expected to pass. They will fail for surprising reasons. For instance, did you know that using the 2.x branch of jQuery won\'t work with Coded UI? [Neither did I.](https://connect.microsoft.com/VisualStudio/Feedback/Details/794841) I\'ve lost track of the time that has been wasted running the same test in multiple different environments trying to identify what exactly is upsetting Coded UI about the environment this time.\\n\\nIt is sad but true that with Coded UI tests you can spend an _enormous_ amount of time maintaining the test pack on a day to day basis. As infrastructure and project dependencies are upgraded you will sadly discover Coded UI has once again gone into the foetal position and has to tempted back to normal functioning by whispering sweet nothings in it\'s ear. (_\\"It\'s not true that they\'ve ended support for Windows XP\\" / \\"IE 6 will live forever\\"_ and so on)\\n\\nCoded UI also appears to be badly supported by Microsoft. Documentation is pretty sparse and, as we\'ll come back to in a minute, Coded UI is sometimes broken or damaged by other products shipped by Microsoft. This makes it hard to have faith in Coded UI. Indeed, if you\'re thinking of automating your QA testing my advice would be \\"look into Selenium\\". Not because I\'ve used it (I haven\'t) but those I\'ve met who have used Selenium and Coded UI say Selenium wins hands down.\\n\\n## And yet, and yet...\\n\\nAll of the above said, if you have a Coded UI test suite it can still pay dividends. Significant dividends. As I mentioned, my current project has a significant coverage of Coded UI tests. We\'ve crawled over a lot of broken glass to put these together. But now they\'re there it is undeniably useful.\\n\\nEvery now and then we\'ll do a significant refactor of part of the application. For instance, we\'ve entirely changed our persistence strategy in the app but been able to check the code in with a high degree of confidence gleaned from running our test suite using the refactored codebase.\\n\\nLet me be clear: Coded UI tests can be useful.\\n\\n## The \\"runas\\" Problem\\n\\nLong preamble over, this post is about how to work around the latest issue Coded UI has thrown in our direction. I call it the \\"runas\\" problem. Our application is a Knockout / ASP.Net MVC web app built to be used in an intranet environment. By that I mean that identity is handled by Active Directory / [Windows Authentication](http://en.wikipedia.org/wiki/Integrated_Windows_Authentication). When someone logs into our app we know who they are without them having to directly supply us with a username and password. No, by logging into their computer they have announced just who they are and Internet Explorer (for it is he) will pass along the credentials. (The app can be used with pretty much any browser but we\'re only mandated to support IE 9+.)\\n\\nIn order that we can test the app we have a number of test accounts set up in Active Directory. These test accounts have been assigned various roles (viewer / editor / administrator etc). Our tests are designed to run using these accounts in order that all scenarios can be adequately tested.\\n\\nTo achieve this lofty goal the following code (or something very like it) is executed as the first step in any Coded UI test:\\n\\n```cs\\nstring browserLocation = \\"C:\\\\\\\\Program Files\\\\\\\\Internet Explorer\\\\\\\\iexplore.exe\\";\\nstring url = \\"http://localhost:12345/\\";\\nstring username = \\"test.editor\\";\\nstring domain = \\"theDomain\\";\\nvar password = new SecureString();\\nforeach (char c in \\"test.editor.password\\")\\n{\\n    password.AppendChar(c);\\n}\\n\\nApplicationUnderTest.Launch(browserLocation, null, url, username, password, domain);\\n```\\n\\nWhat this does is fire up Internet Explorer as the supplied user of theDomain\\\\test.editor, and navigate to the home page. With that as our starting place we could dependably then run a test as this test user. This was a solution not without quirks (on occasion Coded UI tests would \\"stutter\\" - repeating each keypress 3 times with calamitous effects). But generally, this worked.\\n\\nUntil that is either Visual Studio 2013 Update 3 or Internet Explorer 11 was installed. One of these (and it appears to be hotly contested) broke the ability to run the above code successfully. After these were installed running the above code resulted in the following error message:\\n\\n> \\"The application cannot be started. This could be due to one of the following reasons:\\n>\\n> 1.  Another instance of the application is already running and only one instance can be running at a time.\\n> 2.  The application started another process and has now stopped. You may need to launch the process directly.\\n> 3.  You do not have sufficient privileges for this application.\\" File: C:\\\\Program Files\\\\Internet Explorer\\\\iexplore.exe.\\"\\n\\nLamentably, this was pretty much unresolvable and [logging it with Microsoft yielded nothing helpful](https://connect.microsoft.com/VisualStudio/feedbackdetail/view/949049/coded-ui-cannot-run-as-a-different-user-with-visual-studio-2013-update-3). This is what I mean about Coded UI being badly supported by Microsoft. Despite my best efforts to report this issue both to Connect and [elsewhere](http://social.msdn.microsoft.com/Forums/vstudio/en-US/f48665e4-569a-4b67-9bdb-5522b2adffb2/cannot-run-coded-ui-tests-as-different-user-on-windows-81?forum=vsmantest#28c9decb-b579-4848-a7a9-f41c57584d59) and in the end nothing useful happened.\\n\\nSo what to do? I still have Coded UI tests, I still need to be able to run them. And crucially I need to be able to run them impersonating a different user. What to do indeed....\\n\\n## The <strike>hack</strike>\\n\\nworkaround\\n\\nAfter IE 11 / Visual Studio Update 3 / whatev\'s was installed I was left with a setup that allowed me to run Coded UI tests, <u>but only</u>\\n\\nas the current user. On that basis I started looking into a little MVC jiggery pokery. All my controllers inherit from a single base controller. Inside there I placed the following extra override:\\n\\n```cs\\npublic abstract class BaseController : System.Web.Mvc.Controller\\n{\\n  //...\\n\\n  protected override void OnAuthorization(AuthorizationContext filterContext)\\n  {\\n#if DEBUG\\n    if (filterContext.HttpContext.IsDebuggingEnabled)// Is compilation debug=\\"true\\" set in the web.config?\\n    {\\n      var userToImpersonate = Session[\\"UserToImpersonate\\"] as string;\\n      if (!string.IsNullOrEmpty(userToImpersonate))\\n      {\\n        // userToImpersonate example: \\"test.editor@theDomain.com\\"\\n        filterContext.HttpContext.User = new RolePrincipal(new WindowsIdentity(userToImpersonate));\\n      }\\n    }\\n#endif\\n      base.OnAuthorization(filterContext);\\n  }\\n\\n  //...\\n}\\n```\\n\\nEach request will trigger this method as one of the first steps in the MVC pipeline. What it does is checks the `Session` for a user to impersonate. (Yes I\'m as wary of Session as the next chap - but in this case it\'s the right tool for the job.) If a user has been specified then it replaces the current user with the `Session` user. From this point forwards the app is effectively running as that user. That\'s great!\\n\\nIn order that Coded UI can make use of this mechanism we need to introduce a \\"hook\\". This is going to look a bit hacky - bear with me. Inside `Global.asax.cs` we\'re going to add a `Session_Start` method:\\n\\n```cs\\nprotected void Session_Start(object sender, EventArgs eventArgs)\\n{\\n#if DEBUG\\n    // If a user to impersonate has been supplied then add this user to the session\\n    // Impersonation will happen in the OnAuthorization method of our base MVC controller\\n    // Note, this is only allowed in debug mode - not in release mode\\n    // This exists purely to support coded ui tests\\n    if (Context.IsDebuggingEnabled)  // Is compilation debug=\\"true\\" set in the web.config?\\n    {\\n        var userToImpersonate = Request.QueryString[\\"UserToImpersonate\\"] as string;\\n        if (!string.IsNullOrEmpty(userToImpersonate))\\n        {\\n            Session.Add(\\"UserToImpersonate\\", userToImpersonate);\\n        }\\n    }\\n#endif\\n}\\n```\\n\\nFor the first Request in a Session this checks the `QueryString` for a parameter called `UserToImpersonate`. If it\'s found then it\'s placed into `Session`. With this hook exposed we can now amend the first step that all our Coded UI tests follow:\\n\\n```cs\\n// Various lines commented out as doesn\'t work with IE 11 - left as an example of how it could be done in the past\\n//string browserLocation = \\"C:\\\\\\\\Program Files\\\\\\\\Internet Explorer\\\\\\\\iexplore.exe\\";\\nstring url = \\"http://localhost:12345/\\";\\nstring username = \\"test.editor\\";\\nstring domain = \\"theDomain.com\\";\\n//var password = new SecureString();\\n//foreach (char c in \\"test.editor.password\\")\\n//{\\n//    password.AppendChar(c);\\n//}\\n\\n//ApplicationUnderTest.Launch(browserLocation, null, url, username, password, domain);\\n\\n// Suffixing url with UrlToImpersonate which will be picked up in Session_Start and used to impersonate\\n// in OnAuthorization in BaseController.  Also no longer using ApplicationUnderTest.Launch; switched to\\n// BrowserWindow.Launch\\n// No longer used parameters: browserLocation, password\\nvar userToImpersonate = username + \\"@\\" + domain; // eg \\"test.editor@theDomain.com\\"\\nvar urlWithUser = url + \\"?UserToImpersonate=\\" + HttpUtility.UrlEncode(userToImpersonate);\\nvar browser = BrowserWindow.Launch(urlWithUser, \\"-nomerge\\"); // \\"-nomerge\\" flag forces a new session\\n```\\n\\nAs you can see we actually need less when we\'re using this approach. We no longer need to directly specify the password or the browser location. And the user to impersonate is now passed in as the part of the initial URL used to launch the test.\\n\\nPay careful attention to the \\"-nomerge\\" flag that is passed in. This ensures that when another browser instance is opened a new session will be started. This is essential for \\"multi-user\\" tests that run tests for _different_ users as part of the same test. It ensures that \\"test.editor\\" and \\"test.different.editor\\" can co-exist happily.\\n\\n## What do I think of the workaround?\\n\\nThis approach works reliably and dependably. More so than the original approach which on occasion wouldn\'t work or would \\"stutter\\" keypresses. That\'s the good news.\\n\\nThe not so good news is that this approach is, in my view, a bit of hack. I want you to know that this isn\'t my ideal.\\n\\nI _really_ don\'t like having to change the actual system code to facilitate the impersonation requirement. Naturally we only ship the release and not the debug builds to Production so the \\"back door\\" that this approach provides will not exist in our Production builds. It will only be accessible in our development environments and on our Coded UI test server. But it feels oh so wrong that there is an effective potential back door in the system now. Well, only if the stars were to align in a really terrible (and admittedly rather unlikely) way. But still, you take my point. Caveat emptor and all that. This is something of a cutdown example to illustrate the point. If anyone else intends to use this then I\'d suggest doing more to safeguard your approach. Implementing impersonation allowlists so \\"any\\" user cannot be impersonated would be a sensible precaution to start with.\\n\\nPerhaps this is just one more reason that I\'m not that enamoured of Coded UI. Once again it is useful but I\'ve had to compromise more than I\'d like to keep it\'s use. If anyone out there has a better solution I would _love_ to hear from you."},{"id":"/2014/11/04/using-gulp-in-visual-studio-instead-of-web-optimization","metadata":{"permalink":"/2014/11/04/using-gulp-in-visual-studio-instead-of-web-optimization","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2014-11-04-using-gulp-in-visual-studio-instead-of-web-optimization/index.md","source":"@site/blog/2014-11-04-using-gulp-in-visual-studio-instead-of-web-optimization/index.md","title":"Using Gulp in Visual Studio instead of Web Optimization","description":"Update 17/02/2015: I\'ve taken the approach discussed in this post a little further - you can see here","date":"2014-11-04T00:00:00.000Z","formattedDate":"November 4, 2014","tags":[{"label":"Task Runner Explorer","permalink":"/tags/task-runner-explorer"},{"label":"Visual Studio","permalink":"/tags/visual-studio"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"javascript","permalink":"/tags/javascript"},{"label":"Gulp","permalink":"/tags/gulp"}],"readingTime":15.43,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Using Gulp in Visual Studio instead of Web Optimization","authors":"johnnyreilly","tags":["Task Runner Explorer","Visual Studio","TypeScript","javascript","Gulp"],"hide_table_of_contents":false},"prevItem":{"title":"Pretending to be someone you\'re not and the dark pit of despair","permalink":"/2014/11/26/Coded-UI-IE-11-and-the-runas-problem"},"nextItem":{"title":"Caching and Cache-Busting in AngularJS with HTTP interceptors","permalink":"/2014/10/06/caching-and-cache-busting-in-angularjs-with-http-interceptors"}},"content":"### Update 17/02/2015: I\'ve taken the approach discussed in this post a little further - you can see [here](https://blog.johnnyreilly.com/2015/02/using-gulp-in-asp-net-instead-of-web-optimization.html)\\n\\nI\'ve used a number of tools to package up JavaScript and CSS in my web apps. [Andrew Davey\'s tremendous Cassette](http://getcassette.net/) has been really useful. Also good (although less powerful/magical) has been Microsoft\'s very own [Microsoft.AspNet.Web.Optimization](https://www.nuget.org/packages/Microsoft.AspNet.Web.Optimization/) that ships with MVC.\\n\\nI was watching the [ASP.NET Community Standup from October 7th, 2014](http://youtu.be/NgbA2BxNweE?list=PL0M0zPgJ3HSftTAAHttA3JQU4vOjXFquF) and learned that the ASP.Net team is not planning to migrate [Microsoft.AspNet.Web.Optimization](https://www.nuget.org/packages/Microsoft.AspNet.Web.Optimization/) to the next version of ASP.Net. Instead they\'re looking to make use of JavaScript task runners like [Grunt](http://gruntjs.com/) and maybe [Gulp](http://gulpjs.com/). Perhaps you\'re even dimly aware that they\'ve been taking steps to make these runners more of a first class citizen in Visual Studio, hence the recent release of the new and groovy [Task Runner Explorer](http://visualstudiogallery.msdn.microsoft.com/8e1b4368-4afb-467a-bc13-9650572db708).\\n\\nGulp has been on my radar for a while now as has Grunt. By \\"on my radar\\" what I really mean is \\"Hmmmm, I really need to learn this..... perhaps I could wait until the [Betamax vs VHS battles](http://en.wikipedia.org/wiki/Videotape_format_war) are done? Oh never mind, here we go...\\".\\n\\nMy understanding is that Grunt and Gulp essentially do the same thing (run tasks in JavaScript) but have different approaches. Grunt is more about configuration, Gulp is more about code. At present Gulp also has a performance advantage as it does less IO than Grunt - though I understand that\'s due to change in the future. But generally my preference is code over configuration. On that basis I decided that I was going to give Gulp first crack.\\n\\n## Bub bye Web Optimization\\n\\nI already had a project that used [Web Optimization](https://github.com/johnnyreilly/Proverb) to bundle JavaScript and CSS files. When debugging on my own machine Web Optimization served up the full JavaScript and CSS files. Thanks to the magic of source maps I was able to debug the TypeScript that created the JavaScript files too. Which was nice. When I deployed to production, Web Optimization minified and concatenated the JavaScript and CSS files. This meant I had a single HTTP request for JavaScript and a single HTTP request for CSS. This was also... nooice!\\n\\nI took a copy of my existing project and created a [new repo for it on GitHub](https://github.com/johnnyreilly/Proverb-gulp). It was very simple in terms of bundling. It had a `BundleConfig` that created 2 bundles; 1 for JavaScript and 1 for CSS:\\n\\n```cs\\nusing System.Web;\\nusing System.Web.Optimization;\\n\\nnamespace Proverb.Web\\n{\\n    public class BundleConfig\\n    {\\n        // For more information on bundling, visit http://go.microsoft.com/fwlink/?LinkId=301862\\n        public static void RegisterBundles(BundleCollection bundles)\\n        {\\n            var angularApp = new ScriptBundle(\\"~/angularApp\\").Include(\\n\\n                // Vendor Scripts\\n                \\"~/scripts/jquery-{version}.js\\",\\n                \\"~/scripts/angular.js\\",\\n                \\"~/scripts/angular-animate.js\\",\\n                \\"~/scripts/angular-route.js\\",\\n                \\"~/scripts/angular-sanitize.js\\",\\n                \\"~/scripts/angular-ui/ui-bootstrap-tpls.js\\",\\n\\n                \\"~/scripts/toastr.js\\",\\n                \\"~/scripts/moment.js\\",\\n                \\"~/scripts/spin.js\\",\\n                \\"~/scripts/underscore.js\\",\\n\\n                // Bootstrapping\\n                \\"~/app/app.js\\",\\n                \\"~/app/config.route.js\\",\\n\\n                // common Modules\\n                \\"~/app/common/common.js\\",\\n                \\"~/app/common/logger.js\\",\\n                \\"~/app/common/spinner.js\\",\\n\\n                // common.bootstrap Modules\\n                \\"~/app/common/bootstrap/bootstrap.dialog.js\\"\\n                );\\n\\n            // directives\\n            angularApp.IncludeDirectory(\\"~/app/directives\\", \\"*.js\\", true);\\n\\n            // services\\n            angularApp.IncludeDirectory(\\"~/app/services\\", \\"*.js\\", true);\\n\\n            // controllers\\n            angularApp.IncludeDirectory(\\"~/app/admin\\", \\"*.js\\", true);\\n            angularApp.IncludeDirectory(\\"~/app/about\\", \\"*.js\\", true);\\n            angularApp.IncludeDirectory(\\"~/app/dashboard\\", \\"*.js\\", true);\\n            angularApp.IncludeDirectory(\\"~/app/layout\\", \\"*.js\\", true);\\n            angularApp.IncludeDirectory(\\"~/app/sayings\\", \\"*.js\\", true);\\n            angularApp.IncludeDirectory(\\"~/app/sages\\", \\"*.js\\", true);\\n\\n            bundles.Add(angularApp);\\n\\n            bundles.Add(new StyleBundle(\\"~/Content/css\\").Include(\\n                \\"~/content/ie10mobile.css\\",\\n                \\"~/content/bootstrap.css\\",\\n                \\"~/content/font-awesome.css\\",\\n                \\"~/content/toastr.css\\",\\n                \\"~/content/styles.css\\"\\n            ));\\n        }\\n    }\\n}\\n```\\n\\nI set myself a task. I wanted to be able to work in \\\\***exactly**\\\\* the way I was working now. But using Gulp instead of Web Optimization. I wanted to lose the BundleConfig above and remove Web Optimization from my application, secure in the knowledge that I had lost nothing. Could it be done? Read on!\\n\\n## Installing Gulp (and Associates)\\n\\nI fired up Visual Studio and looked for an excuse to use the Task Runner Explorer. The first thing I needed was Gulp. My machine already had Node and NPM installed so I went to the command line to install Gulp globally:\\n\\n```ps\\nnpm install gulp -g\\n```\\n\\nNow to start to plug Gulp into my web project. It was time to make the introductions: Visual Studio meet NPM. At the root of the web project I created a `package.json` file by executing the following command and accepting all the defaults:\\n\\n```ps\\nnpm init\\n```\\n\\nI wanted to add Gulp as a development dependency of my project: (\\"Development\\" because I only need to run tasks at development time. My app has no dependency on Gulp at runtime - at that point it\'s just about serving up static files.)\\n\\n```ps\\nnpm install gulp --save-dev\\n```\\n\\nThis installs gulp local to the project as a development dependency. As a result we now have a \\"node_modules\\" folder sat in our root which contains our node packages. Currently, as our `package.json` reveals, this is only gulp:\\n\\n```json\\n\\"devDependencies\\": {\\n    \\"gulp\\": \\"^3.8.8\\"\\n  }\\n```\\n\\nIt\'s time to go to town. Let\'s install all the packages we\'re going to need to bundle and minify JavaScript and CSS:\\n\\n```sh\\nnpm install gulp-concat gulp-uglify gulp-rev del path gulp-ignore gulp-asset-manifest gulp-minify-css --save-dev\\n```\\n\\nThis installs the packages as dev dependencies (as you\'ve probably guessed) and leaves us with a list of dev dependencies like this:\\n\\n```json\\n\\"devDependencies\\": {\\n    \\"del\\": \\"^0.1.3\\",\\n    \\"gulp\\": \\"^3.8.8\\",\\n    \\"gulp-asset-manifest\\": \\"0.0.5\\",\\n    \\"gulp-concat\\": \\"^2.4.1\\",\\n    \\"gulp-ignore\\": \\"^1.2.1\\",\\n    \\"gulp-minify-css\\": \\"^0.3.10\\",\\n    \\"gulp-rev\\": \\"^1.1.0\\",\\n    \\"gulp-uglify\\": \\"^1.0.1\\",\\n    \\"path\\": \\"^0.4.9\\"\\n  }\\n```\\n\\n## Making `gulpfile.js`\\n\\nSo now I was ready. I had everything I needed to replace my `BundleConfig.cs`. I created a new file called `gulpfile.js` in the root of my web project that looked like this:\\n\\n```js\\n/// <vs AfterBuild=\'default\' />\\nvar gulp = require(\'gulp\');\\n\\n// Include Our Plugins\\nvar concat = require(\'gulp-concat\');\\nvar ignore = require(\'gulp-ignore\');\\nvar manifest = require(\'gulp-asset-manifest\');\\nvar minifyCss = require(\'gulp-minify-css\');\\nvar uglify = require(\'gulp-uglify\');\\nvar rev = require(\'gulp-rev\');\\nvar del = require(\'del\');\\nvar path = require(\'path\');\\n\\nvar tsjsmapjsSuffix = \'.{ts,js.map,js}\';\\nvar excludetsjsmap = \'**/*.{ts,js.map}\';\\n\\nvar bundleNames = { scripts: \'scripts\', styles: \'styles\' };\\n\\nvar filesAndFolders = {\\n  base: \'.\',\\n  buildBaseFolder: \'./build/\',\\n  debug: \'debug\',\\n  release: \'release\',\\n  css: \'css\',\\n\\n  // The fonts we want Gulp to process\\n  fonts: [\'./fonts/*.*\'],\\n\\n  // The scripts we want Gulp to process - adapted from BundleConfig\\n  scripts: [\\n    // Vendor Scripts\\n    \'./scripts/angular.js\',\\n    \'./scripts/angular-animate.js\',\\n    \'./scripts/angular-route.js\',\\n    \'./scripts/angular-sanitize.js\',\\n    \'./scripts/angular-ui/ui-bootstrap-tpls.js\',\\n\\n    \'./scripts/toastr.js\',\\n    \'./scripts/moment.js\',\\n    \'./scripts/spin.js\',\\n    \'./scripts/underscore.js\',\\n\\n    // Bootstrapping\\n    \'./app/app\' + tsjsmapjsSuffix,\\n    \'./app/config.route\' + tsjsmapjsSuffix,\\n\\n    // common Modules\\n    \'./app/common/common\' + tsjsmapjsSuffix,\\n    \'./app/common/logger\' + tsjsmapjsSuffix,\\n    \'./app/common/spinner\' + tsjsmapjsSuffix,\\n\\n    // common.bootstrap Modules\\n    \'./app/common/bootstrap/bootstrap.dialog\' + tsjsmapjsSuffix,\\n\\n    // directives\\n    \'./app/directives/**/*\' + tsjsmapjsSuffix,\\n\\n    // services\\n    \'./app/services/**/*\' + tsjsmapjsSuffix,\\n\\n    // controllers\\n    \'./app/about/**/*\' + tsjsmapjsSuffix,\\n    \'./app/admin/**/*\' + tsjsmapjsSuffix,\\n    \'./app/dashboard/**/*\' + tsjsmapjsSuffix,\\n    \'./app/layout/**/*\' + tsjsmapjsSuffix,\\n    \'./app/sages/**/*\' + tsjsmapjsSuffix,\\n    \'./app/sayings/**/*\' + tsjsmapjsSuffix,\\n  ],\\n\\n  // The styles we want Gulp to process - adapted from BundleConfig\\n  styles: [\\n    \'./content/ie10mobile.css\',\\n    \'./content/bootstrap.css\',\\n    \'./content/font-awesome.css\',\\n    \'./content/toastr.css\',\\n    \'./content/styles.css\',\\n  ],\\n};\\n\\nfilesAndFolders.debugFolder =\\n  filesAndFolders.buildBaseFolder + \'/\' + filesAndFolders.debug + \'/\';\\nfilesAndFolders.releaseFolder =\\n  filesAndFolders.buildBaseFolder + \'/\' + filesAndFolders.release + \'/\';\\n\\n/**\\n * Create a manifest depending upon the supplied arguments\\n *\\n * @param {string} manifestName\\n * @param {string} bundleName\\n * @param {boolean} includeRelativePath\\n * @param {string} pathPrepend\\n */\\nfunction getManifest(\\n  manifestName,\\n  bundleName,\\n  includeRelativePath,\\n  pathPrepend\\n) {\\n  // Determine filename (\\"./build/manifest-debug.json\\" or \\"./build/manifest-release.json\\"\\n  var manifestFile =\\n    filesAndFolders.buildBaseFolder + \'manifest-\' + manifestName + \'.json\';\\n\\n  return manifest({\\n    bundleName: bundleName,\\n    includeRelativePath: includeRelativePath,\\n    manifestFile: manifestFile,\\n    log: true,\\n    pathPrepend: pathPrepend,\\n    pathSeparator: \'/\',\\n  });\\n}\\n\\n// Delete the build folder\\ngulp.task(\'clean\', function (cb) {\\n  del([filesAndFolders.buildBaseFolder], cb);\\n});\\n\\n// Copy across all files in filesAndFolders.scripts to build/debug\\ngulp.task(\'scripts-debug\', [\'clean\'], function () {\\n  return gulp\\n    .src(filesAndFolders.scripts, { base: filesAndFolders.base })\\n    .pipe(gulp.dest(filesAndFolders.debugFolder));\\n});\\n\\n// Create a manifest.json for the debug build - this should have lots of script files in\\ngulp.task(\'manifest-scripts-debug\', [\'scripts-debug\'], function () {\\n  return gulp\\n    .src(filesAndFolders.scripts, { base: filesAndFolders.base })\\n    .pipe(ignore.exclude(\'**/*.{ts,js.map}\')) // Exclude ts and js.map files from the manifest (as they won\'t become script tags)\\n    .pipe(getManifest(filesAndFolders.debug, bundleNames.scripts, true));\\n});\\n\\n// Copy across all files in filesAndFolders.styles to build/debug\\ngulp.task(\'styles-debug\', [\'clean\'], function () {\\n  return gulp\\n    .src(filesAndFolders.styles, { base: filesAndFolders.base })\\n    .pipe(gulp.dest(filesAndFolders.debugFolder));\\n});\\n\\n// Create a manifest.json for the debug build - this should have lots of style files in\\ngulp.task(\\n  \'manifest-styles-debug\',\\n  [\'styles-debug\', \'manifest-scripts-debug\'],\\n  function () {\\n    return (\\n      gulp\\n        .src(filesAndFolders.styles, { base: filesAndFolders.base })\\n        //.pipe(ignore.exclude(\\"**/*.{ts,js.map}\\")) // Exclude ts and js.map files from the manifest (as they won\'t become script tags)\\n        .pipe(getManifest(filesAndFolders.debug, bundleNames.styles, true))\\n    );\\n  }\\n);\\n\\n// Concatenate & Minify JS for release into a single file\\ngulp.task(\'scripts-release\', [\'clean\'], function () {\\n  return (\\n    gulp\\n      .src(filesAndFolders.scripts)\\n      .pipe(ignore.exclude(\'**/*.{ts,js.map}\')) // Exclude ts and js.map files - not needed in release mode\\n\\n      .pipe(concat(\'app.js\')) // Make a single file - if you want to see the contents then include the line below\\n      //.pipe(gulp.dest(releaseFolder))\\n\\n      .pipe(uglify()) // Make the file titchy tiny small\\n      .pipe(rev()) // Suffix a version number to it\\n      .pipe(gulp.dest(filesAndFolders.releaseFolder))\\n  ); // Write single versioned file to build/release folder\\n});\\n\\n// Create a manifest.json for the release build - this should just have a single file for scripts\\ngulp.task(\'manifest-scripts-release\', [\'scripts-release\'], function () {\\n  return gulp\\n    .src(filesAndFolders.buildBaseFolder + filesAndFolders.release + \'/*.js\')\\n    .pipe(getManifest(filesAndFolders.release, bundleNames.scripts, false));\\n});\\n\\n// Copy across all files in filesAndFolders.styles to build/debug\\ngulp.task(\'styles-release\', [\'clean\'], function () {\\n  return (\\n    gulp\\n      .src(filesAndFolders.styles)\\n      .pipe(concat(\'app.css\')) // Make a single file - if you want to see the contents then include the line below\\n      //.pipe(gulp.dest(releaseFolder))\\n\\n      .pipe(minifyCss()) // Make the file titchy tiny small\\n      .pipe(rev()) // Suffix a version number to it\\n      .pipe(\\n        gulp.dest(filesAndFolders.releaseFolder + \'/\' + filesAndFolders.css)\\n      )\\n  ); // Write single versioned file to build/release folder\\n});\\n\\n// Create a manifest.json for the debug build - this should have a single style files in\\ngulp.task(\\n  \'manifest-styles-release\',\\n  [\'styles-release\', \'manifest-scripts-release\'],\\n  function () {\\n    return gulp\\n      .src(filesAndFolders.releaseFolder + \'**/*.css\')\\n      .pipe(\\n        getManifest(\\n          filesAndFolders.release,\\n          bundleNames.styles,\\n          false,\\n          filesAndFolders.css + \'/\'\\n        )\\n      );\\n  }\\n);\\n\\n// Copy across all fonts in filesAndFolders.fonts to both release and debug locations\\ngulp.task(\'fonts\', [\'clean\'], function () {\\n  return gulp\\n    .src(filesAndFolders.fonts, { base: filesAndFolders.base })\\n    .pipe(gulp.dest(filesAndFolders.debugFolder))\\n    .pipe(gulp.dest(filesAndFolders.releaseFolder));\\n});\\n\\n// Default Task\\ngulp.task(\'default\', [\\n  \'scripts-debug\',\\n  \'manifest-scripts-debug\',\\n  \'styles-debug\',\\n  \'manifest-styles-debug\',\\n  \'scripts-release\',\\n  \'manifest-scripts-release\',\\n  \'styles-release\',\\n  \'manifest-styles-release\',\\n  \'fonts\',\\n]);\\n```\\n\\n## What `gulpfile.js` does\\n\\nThis file does a number of things each time it is run. First of all it deletes any `build` folder in the root of the web project so we\'re ready to build anew. Then it packages up files both for debug and for release mode. For debug it does the following:\\n\\n1. It copies the `ts`, `js.map` and `js` files declared in `filesAndFolders.scripts` to the `build/debug` folder preserving their original folder structure. (So, for example, `app/app.ts`, `app/app.js.map` and `app/app.js` will all end up at `build/debug/app/app.ts`, `build/debug/app/app.js.map` and `build/debug/app/app.js` respectively.) This is done to allow the continued debugging of the original TypeScript files when running in debug mode.\\n2. It copies the `css` files declared in `filesAndFolders.styles` to the `build/debug` folder preserving their original folder structure. (So `content/bootstrap.css` will end up at `build/debug/content/bootstrap.css`.)\\n3. It creates a `build/manifest-debug.json` file which contains details of all the script and style files that have been packaged up:\\n\\n```json\\n{\\n  \\"scripts\\": [\\n    \\"scripts/angular.js\\",\\n    \\"scripts/angular-animate.js\\",\\n    \\"scripts/angular-route.js\\",\\n    \\"scripts/angular-sanitize.js\\",\\n    \\"scripts/angular-ui/ui-bootstrap-tpls.js\\",\\n    \\"scripts/toastr.js\\",\\n    \\"scripts/moment.js\\",\\n    \\"scripts/spin.js\\",\\n    \\"scripts/underscore.js\\",\\n    \\"app/app.js\\",\\n    \\"app/config.route.js\\",\\n    \\"app/common/common.js\\",\\n    \\"app/common/logger.js\\",\\n    \\"app/common/spinner.js\\",\\n    \\"app/common/bootstrap/bootstrap.dialog.js\\",\\n    \\"app/directives/imgPerson.js\\",\\n    \\"app/directives/serverError.js\\",\\n    \\"app/directives/sidebar.js\\",\\n    \\"app/directives/spinner.js\\",\\n    \\"app/directives/waiter.js\\",\\n    \\"app/directives/widgetClose.js\\",\\n    \\"app/directives/widgetHeader.js\\",\\n    \\"app/directives/widgetMinimize.js\\",\\n    \\"app/services/datacontext.js\\",\\n    \\"app/services/repositories.js\\",\\n    \\"app/services/repository.sage.js\\",\\n    \\"app/services/repository.saying.js\\",\\n    \\"app/about/about.js\\",\\n    \\"app/admin/admin.js\\",\\n    \\"app/dashboard/dashboard.js\\",\\n    \\"app/layout/shell.js\\",\\n    \\"app/layout/sidebar.js\\",\\n    \\"app/layout/topnav.js\\",\\n    \\"app/sages/sageDetail.js\\",\\n    \\"app/sages/sageEdit.js\\",\\n    \\"app/sages/sages.js\\",\\n    \\"app/sayings/sayingEdit.js\\",\\n    \\"app/sayings/sayings.js\\"\\n  ],\\n  \\"styles\\": [\\n    \\"content/ie10mobile.css\\",\\n    \\"content/bootstrap.css\\",\\n    \\"content/font-awesome.css\\",\\n    \\"content/toastr.css\\",\\n    \\"content/styles.css\\"\\n  ]\\n}\\n```\\n\\nFor release our gulpfile works with the same resources but has a different aim. Namely to minimise the the number of HTTP requests, obfuscate the code and version the files produced to prevent caching issues. To achieve those lofty aims it does the following:\\n\\n1. It concatenates together all the `js` files declared in `filesAndFolders.scripts`, minifies them and writes them to a single `build/release/app-{xxxxx}.js` file (where `-{xxxxx}` represents a version created by gulp-rev).\\n2. It concatenates together all the `css` files declared in `filesAndFolders.styles`, minifies them and writes them to a single `build/release/css/app-{xxxxx}.css` file. The file is placed in a css subfolder because of relative paths specified in the CSS file.\\n3. It creates a `build/manifest-release.json` file which contains details of all the script and style files that have been packaged up:\\n\\n```json\\n{\\n  \\"scripts\\": [\\"app-95d1e06d.js\\"],\\n  \\"styles\\": [\\"css/app-1a6256ea.css\\"]\\n}\\n```\\n\\nAs you can see, the number of files included are reduced down to 2; 1 for JavaScript and 1 for CSS.\\n\\nFinally, for both the debug and release packages the contents of the `fonts` folder is copied across wholesale, preserving the original folder structure. This is because the CSS files contain relative references that point to the font files. If I had image files which were referenced by my CSS I\'d similarly need to include these in the build process.\\n\\n## Task Runner Explorer gets in on the action\\n\\nThe eagle eyed amongst you will also have noticed a peculiar first line to our `gulpfile.js`:\\n\\n```js\\n/// <vs AfterBuild=\'default\' />\\n```\\n\\nThis mysterious comment is actually how the Task Runner Explorer hooks our `gulpfile.js` into the Visual Studio build process. Our \\"magic comment\\" ensures that on the `AfterBuild` event, Task Runner Explorer runs the `default` task in our `gulpfile.js`. The reason we\'re using the `AfterBuild` event rather than the `BeforeBuild` event is because our project contains TypeScript and we need the transpiled JavaScript to be created before we can usefully run our package tasks. If we were using JavaScript alone then that wouldn\'t be an issue and either build event would do.\\n\\n![](Screenshot-2014-10-21-17.02.11.png)\\n\\n## How do I use this in my HTML?\\n\\nWell this is magnificent - we have a gulpfile that builds our debug and release packages. The question now is, how do we use it?\\n\\nWeb Optimization made our lives really easy. Up in my head I had a `@Styles.Render(\\"~/Content/css\\")` which pushed out my CSS and down at the foot of the body tag I had a `@Scripts.Render(\\"~/angularApp\\")` which pushed out my script tags. `Styles` and `Scripts` are server-side utilities. It would be very easy to write equivalent utility classes that, depending on whether we were in debug or not, read the appropriate `build/manifest-xxxxxx.json` file and served up either debug or release `style` / `script` tags.\\n\\nThat would be pretty simple - and for what it\'s worth \\\\*\\\\*simple is <u>good</u>\\n\\n\\\\*\\\\*. But today I felt like a challenge. What say server side rendering had been outlawed? A draconian ruling had been passed and all you had to play with was HTML / JavaScript and a server API that served up JSON? What would you do then? (All fantasy I know... But go with me on this - it\'s a journey.) Or more sensibly, what if you just want to remove some of the work your app is doing server-side to bundle and minify. Just serve up static assets instead. Spend less money in Azure why not?\\n\\nBefore I make all the changes let\'s review where we were. I had a single MVC view which, in terms of bundles, CSS and JavaScript pretty much looked like this:\\n\\n```html\\n<!DOCTYPE html>\\n<html>\\n  <head>\\n    \x3c!-- ... --\x3e\\n    @Styles.Render(\\"~/Content/css\\")\\n  </head>\\n  <body>\\n    \x3c!-- ... --\x3e\\n\\n    @Scripts.Render(\\"~/angularApp\\")\\n    <script>\\n      (function () {\\n        $.getJSON(\'@Url.Content(\\"~/Home/StartApp\\")\').done(function (\\n          startUpData\\n        ) {\\n          var appConfig = $.extend({}, startUpData, {\\n            appRoot: \'@Url.Content(\\"~/\\")\',\\n            remoteServiceRoot: \'@Url.Content(\\"~/api/\\")\',\\n          });\\n\\n          angularApp.start({\\n            thirdPartyLibs: {\\n              moment: window.moment,\\n              toastr: window.toastr,\\n              underscore: window._,\\n            },\\n            appConfig: appConfig,\\n          });\\n        });\\n      })();\\n    <\/script>\\n  </body>\\n</html>\\n```\\n\\nThis is already more a complicated example than most peoples use cases. Essentially what\'s happening here is both bundles are written out as part of the HTML and then, once the scripts have loaded the Angular app is bootstrapped with some configuration loaded from the server by a good old jQuery AJAX call.\\n\\nAfter reading [an article about script loading by the magnificently funny Jake Archibald](http://www.html5rocks.com/en/tutorials/speed/script-loading/) I felt ready. I cast my MVC view to the four winds and created myself a straight HTML file:\\n\\n```html\\n<!DOCTYPE html>\\n<html>\\n  <head>\\n    \x3c!-- ... --\x3e\\n  </head>\\n  <body>\\n    \x3c!-- ... --\x3e\\n\\n    <script src=\\"Scripts/jquery-2.1.1.min.js\\"><\/script>\\n    <script>\\n      (function () {\\n        var appConfig = {};\\n        var scriptsToLoad;\\n\\n        /**\\n         * Handler which fires as each script loads\\n         */\\n        function onScriptLoad(event) {\\n          scriptsToLoad -= 1;\\n\\n          // Now all the scripts are present start the app\\n          if (scriptsToLoad === 0) {\\n            angularApp.start({\\n              thirdPartyLibs: {\\n                moment: window.moment,\\n                toastr: window.toastr,\\n                underscore: window._,\\n              },\\n              appConfig: appConfig,\\n            });\\n          }\\n        }\\n\\n        // Load startup data from the server\\n        $.getJSON(\'api/Startup\').done(function (startUpData) {\\n          appConfig = startUpData;\\n\\n          // Determine the assets folder depending upon whether in debug mode or not\\n          var buildFolder = appConfig.appRoot + \'build/\';\\n          var debugOrRelease = appConfig.inDebug ? \'debug\' : \'release\';\\n          var manifestFile =\\n            buildFolder + \'manifest-\' + debugOrRelease + \'.json\';\\n          var outputFolder = buildFolder + debugOrRelease + \'/\';\\n\\n          // Load JavaScript and CSS listed in manifest file\\n          $.getJSON(manifestFile).done(function (manifest) {\\n            manifest.styles.forEach(function (href) {\\n              var link = document.createElement(\'link\');\\n\\n              link.rel = \'stylesheet\';\\n              link.media = \'all\';\\n              link.href = outputFolder + href;\\n\\n              document.head.appendChild(link);\\n            });\\n\\n            scriptsToLoad = manifest.scripts.length;\\n            manifest.scripts.forEach(function (src) {\\n              var script = document.createElement(\'script\');\\n\\n              script.onload = onScriptLoad;\\n              script.src = outputFolder + src;\\n              script.async = false;\\n\\n              document.head.appendChild(script);\\n            });\\n          });\\n        });\\n      })();\\n    <\/script>\\n  </body>\\n</html>\\n```\\n\\nIf you very carefully compare the HTML above the MVC view that it replaces you can see the commonalities. They are doing pretty much the same thing - the only real difference is the bootstrapping API. Previously it was an MVC endpoint at `/Home/StartApp`. Now it\'s a Web API endpoint at `api/Startup`. Here\'s how it works:\\n\\n1. A jQuery AJAX call kicks off a call to load the bootstrapping / app config data. Importantly this data includes whether the app is running in debug or not.\\n2. Depending on the `isDebug` flag the app either loads the `build/manifest-debug.json` or `build/manifest-release.json` manifest.\\n3. For each CSS file in the styles bundle a `link` element is created and added to the page.\\n4. For each JavaScript file in the scripts bundle a `script` element is created and added to the page.\\n\\nIt\'s worth pointing out that this also has a performance edge over Web Optimization as the assets are loaded asynchronously! (Yes I know it says `script.async = false` but that\'s not what you think it is... Go read Jake\'s article!)\\n\\nTo finish off I had to make a few tweaks to my `web.config`:\\n\\n```xml\\n\x3c!-- Allow ASP.Net to serve up JSON files --\x3e\\n    <system.webServer>\\n        <staticContent>\\n            <mimeMap fileExtension=\\".json\\" mimeType=\\"application/json\\"/>\\n        </staticContent>\\n    </system.webServer>\\n\\n    \x3c!-- The build folder (and it\'s child folder \\"debug\\") will not be cached.\\n         When people are debugging they don\'t want to cache --\x3e\\n    <location path=\\"build\\">\\n        <system.webServer>\\n            <staticContent>\\n                <clientCache cacheControlMode=\\"DisableCache\\"/>\\n            </staticContent>\\n        </system.webServer>\\n    </location>\\n\\n    \x3c!-- The release folder will be cached for a loooooong time\\n         When you\'re in Production caching is your friend --\x3e\\n    <location path=\\"build/release\\">\\n        <system.webServer>\\n            <staticContent>\\n                <clientCache cacheControlMode=\\"UseMaxAge\\"/>\\n            </staticContent>\\n        </system.webServer>\\n    </location>\\n```\\n\\n## I want to publish, how do I include my assets?\\n\\nIt\'s time for some `csproj` trickery. I must say I think I\'ll be glad to see the back of project files when ASP.Net vNext ships. This is what you need:\\n\\n```xml\\n<Target Name=\\"AfterBuild\\">\\n    <ItemGroup>\\n      \x3c!-- what ever is in the build folder should be included in the project --\x3e\\n      <Content Include=\\"build\\\\**\\\\*.*\\" />\\n    </ItemGroup>\\n  </Target>\\n```\\n\\nWhat\'s happening here is that \\\\*_after_\\\\* a build Visual Studio considers the complete contents of the build folder to part of the project. It\'s after the build because the folder will be deleted and reconstructed as part of the build."},{"id":"/2014/10/06/caching-and-cache-busting-in-angularjs-with-http-interceptors","metadata":{"permalink":"/2014/10/06/caching-and-cache-busting-in-angularjs-with-http-interceptors","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2014-10-06-caching-and-cache-busting-in-angularjs-with-http-interceptors/index.md","source":"@site/blog/2014-10-06-caching-and-cache-busting-in-angularjs-with-http-interceptors/index.md","title":"Caching and Cache-Busting in AngularJS with HTTP interceptors","description":"Loading On-Demand and Caching","date":"2014-10-06T00:00:00.000Z","formattedDate":"October 6, 2014","tags":[{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"$http","permalink":"/tags/http"},{"label":"AngularJS","permalink":"/tags/angular-js"},{"label":"interceptors","permalink":"/tags/interceptors"},{"label":"caching","permalink":"/tags/caching"}],"readingTime":4.07,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Caching and Cache-Busting in AngularJS with HTTP interceptors","authors":"johnnyreilly","tags":["TypeScript","$http","AngularJS","interceptors","caching"],"hide_table_of_contents":false},"prevItem":{"title":"Using Gulp in Visual Studio instead of Web Optimization","permalink":"/2014/11/04/using-gulp-in-visual-studio-instead-of-web-optimization"},"nextItem":{"title":"He tasks me; he heaps me.... I will wreak that MOQ upon him.","permalink":"/2014/10/03/he-tasks-me-he-heaps-me-i-will-wreak"}},"content":"## Loading On-Demand and Caching\\n\\n[I\'ve written before about my own needs for caching and cache-busting when using RequireJS.](http://icanmakethiswork.blogspot.com/2014/03/caching-and-cache-busting-with-requirejs.html) Long story short, when I\'m loading _static_ resources (scripts / views etc) on demand from the server I want to do a little URL fiddling along the way. I want to do that to cater for these 2 scenarios:\\n\\n1. _In Development_ \\\\- I want my URLs for static resources to have a unique querystring with each request to ensure that resources are loaded afresh each time. (eg so a GET request URL might look like this: \\"/app/layout/sidebar.html?v=IAmRandomYesRandomRandomIsWhatIAm58965782\\")\\n2. _In Production_ \\\\- I want my URLs for static resources to have a querystring with that is driven by the application version number. This means that static resources can potentially be cached with a given querystring - subsequent requests should result in a 304 status code (indicating \u201cNot Modified\u201d) and local cache should be used. But when a new version of the app is rolled out and the app version is incremented then the querystring will change and resources will be loaded anew. (eg a GET request URL might look like this: \\"/app/layout/sidebar.html?v=1.0.5389.16180\\")\\n\\n## Loading Views in AngularJS Using this Approach\\n\\nI have exactly the same use cases when I\'m using AngularJS for views. Out of the box with AngularJS 1.x views are loaded lazily (unlike controllers, services etc). For that reason I want to use the same approach I\'ve outlined above to load my views. Also, I want to prepend my URLs with the root of my application - this allows me to cater for my app being deployed in a virtual folder.\\n\\nIt turns out that\'s pretty easy thanks to [HTTP interceptors](https://docs.angularjs.org/api/ng/service/$http#interceptors). They allow you to step into the pipeline and access and modify requests and responses made by your application. When AngularJS loads a view it\'s the HTTP service doing the heavy lifting. So to deal with my own use case, I just need to add in an HTTP interceptor that amends the get request. This is handled in the example that follows in the `configureHttpProvider` function: (The example that follows is TypeScript - though if you just chopped out the interface and the type declarations you\'d find this is pretty much idiomatic JavaScript)\\n\\n```js\\ninterface config {\\n  appRoot: string; // eg \\"/\\"\\n  inDebug: boolean; // eg true or false\\n  urlCacheBusterSuffix: string; // if in debug this might look like this: \\"v=1412608547047\\",\\n  // if not in debug this might look like this: \\"v=1.0.5389.16180\\"\\n}\\n\\nfunction configureHttpProvider() {\\n  // This is the name of our HTTP interceptor\\n  var serviceId = \'urlInterceptor\';\\n\\n  // We\'re going to create a service factory which will be our HTTP interceptor\\n  // It will be injected with a config object which is represented by the config interface above\\n  app.factory(serviceId, [\\n    \'$templateCache\',\\n    \'config\',\\n    function ($templateCache: ng.ITemplateCacheService, config: config) {\\n      // We\'re returning an object literal with a single function; the \\"request\\" function\\n      var service = {\\n        request: request,\\n      };\\n\\n      return service;\\n\\n      // Request will be called with a request config object which includes the URL which we will amend\\n      function request(requestConfig: ng.IRequestConfig) {\\n        // For the loading of HTML templates we want the appRoot to be prefixed to the path\\n        // and we want a suffix to either allow caching or prevent caching\\n        // (depending on whether in debug mode or not)\\n        if (\\n          requestConfig.method === \'GET\' &&\\n          endsWith(requestConfig.url, \'.html\')\\n        ) {\\n          // If this has already been placed into a primed template cache then we should leave the URL as is\\n          // so that the version in templateCache is served.  If we tweak the URL then it will not be found\\n          var cachedAlready = $templateCache.get(requestConfig.url);\\n          if (!cachedAlready) {\\n            // THIS IS THE MAGIC!!!!!!!!!!!!!!!\\n\\n            requestConfig.url =\\n              config.appRoot + requestConfig.url + config.urlCacheBusterSuffix;\\n\\n            // WE NOW HAVE A URL WHICH IS CACHE-FRIENDLY FOR OUR PURPOSES - REJOICE!!!!!!!!!!!\\n          }\\n        }\\n\\n        return requestConfig;\\n      }\\n\\n      // <a href=\\"http://stackoverflow.com/a/2548133/761388\\">a simple JavaScript string \\"endswith\\" implementation</a>\\n      function endsWith(str: string, suffix: string) {\\n        return str.indexOf(suffix, str.length - suffix.length) !== -1;\\n      }\\n    },\\n  ]);\\n\\n  // This adds our service factory interceptor into the pipeline\\n  app.config([\\n    \'$httpProvider\',\\n    function ($httpProvider: ng.IHttpProvider) {\\n      $httpProvider.interceptors.push(serviceId);\\n    },\\n  ]);\\n}\\n```\\n\\nThis interceptor steps in and amends each ajax request when all the following conditions hold true:\\n\\n1. It\'s a GET request.\\n2. It\'s requesting a file that ends \\".html\\" - a template basically.\\n3. The template cache does not already contain the template. I left this out at first and got bitten when I found that the contents of the template cache were being ignored for pre-primed templates. Ugly.\\n\\n## Interesting technique.... How do I apply it?\\n\\nIsn\'t it always much more helpful when you can see an example of code in the context of which it is actually used? Course it is! If you want that then take a look at [`app.ts`](https://github.com/johnnyreilly/Proverb/blob/master/Proverb.Web/app/app.ts) on GitHub. And if you\'d like the naked JavaScript well [that\'s there too](https://github.com/johnnyreilly/Proverb/blob/master/Proverb.Web/app/app.js)."},{"id":"/2014/10/03/he-tasks-me-he-heaps-me-i-will-wreak","metadata":{"permalink":"/2014/10/03/he-tasks-me-he-heaps-me-i-will-wreak","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2014-10-03-he-tasks-me-he-heaps-me-i-will-wreak/index.md","source":"@site/blog/2014-10-03-he-tasks-me-he-heaps-me-i-will-wreak/index.md","title":"He tasks me; he heaps me.... I will wreak that MOQ upon him.","description":"Enough with the horrific misquotes - this is about Moq and async (that\'s my slight justification for robbing Herman Melville).","date":"2014-10-03T00:00:00.000Z","formattedDate":"October 3, 2014","tags":[{"label":"unit testing","permalink":"/tags/unit-testing"},{"label":"async","permalink":"/tags/async"},{"label":"MOQ","permalink":"/tags/moq"},{"label":"metaphysics","permalink":"/tags/metaphysics"}],"readingTime":3.04,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"He tasks me; he heaps me.... I will wreak that MOQ upon him.","authors":"johnnyreilly","tags":["unit testing","async","MOQ","metaphysics"],"hide_table_of_contents":false},"prevItem":{"title":"Caching and Cache-Busting in AngularJS with HTTP interceptors","permalink":"/2014/10/06/caching-and-cache-busting-in-angularjs-with-http-interceptors"},"nextItem":{"title":"Journalling the Migration of Jasmine Tests to TypeScript","permalink":"/2014/09/13/migrating-jasmine-tests-to-typescript"}},"content":"Enough with the horrific misquotes - this is about Moq and async (that\'s my slight justification for robbing Herman Melville).\\n\\nIt\'s pretty straightforward to use Moq to do async testing thanks to it\'s marvellous `ReturnsAsync` method. That means it\'s really easy to test a class that consumes an async API. Below is an example of a class that does just that: (it so happens that this class is a Web API controller but that\'s pretty irrelevant to be honest)\\n\\n```cs\\nnamespace Proverb.Web.Controllers\\n{\\n    // ISageService included inline for ease of explanation\\n    public interface ISageService\\n    {\\n        Task<int> DeleteAsync(int id);\\n    }\\n\\n    public class SageController : ApiController\\n    {\\n        ISageService _sageService;\\n\\n        public SageController(ISageService userService)\\n        {\\n            _sageService = userService;\\n        }\\n\\n        public async Task<IHttpActionResult> Delete(int id)\\n        {\\n            int deleteCount = await _sageService.DeleteAsync(id);\\n\\n            if (deleteCount == 0)\\n                return NotFound();\\n            else\\n                return Ok();\\n        }\\n   }\\n}\\n```\\n\\nTo mock the `_sageService.DeleteAsync` method it\'s as easy as this:\\n\\n```cs\\nnamespace Proverb.Web.Tests.ASPNet.Controllers\\n{\\n    [TestClass]\\n    public class SageControllerTests\\n    {\\n        private Mock<ISageService> _sageServiceMock;\\n        private SageController _controller;\\n\\n        [TestInitialize]\\n        public void Initialise()\\n        {\\n            _sageServiceMock = new Mock<ISageService>();\\n\\n            _controller = new SageController(_sageServiceMock.Object);\\n        }\\n\\n        [TestMethod]\\n        public async Task Delete_returns_a_NotFound()\\n        {\\n            _sageServiceMock\\n                .Setup(x => x.DeleteAsync(_sage.Id))\\n                .ReturnsAsync(0); // This makes me *so* happy...\\n\\n            IHttpActionResult result = await _controller.Delete(_sage.Id);\\n\\n            var notFound = result as NotFoundResult;\\n            Assert.IsNotNull(notFound);\\n            _sageServiceMock.Verify(x => x.DeleteAsync(_sage.Id));\\n        }\\n\\n        [TestMethod]\\n        public async Task Delete_returns_an_Ok()\\n        {\\n            _sageServiceMock\\n                .Setup(x => x.DeleteAsync(_sage.Id))\\n                .ReturnsAsync(1); // I\'m still excited now!\\n\\n            IHttpActionResult result = await _controller.Delete(_sage.Id);\\n\\n            var ok = result as OkResult;\\n            Assert.IsNotNull(ok);\\n            _sageServiceMock.Verify(x => x.DeleteAsync(_sage.Id));\\n        }\\n    }\\n}\\n```\\n\\n## But wait.... What if there\'s like... Nothing?\\n\\nNope, I\'m not getting into metaphysics. Something more simple. What if the `async` API you\'re consuming returns just a `Task`? Not a `Task` of `int` but a simple old humble `Task`.\\n\\nSo to take our example we\'re going from this:\\n\\n```cs\\npublic interface ISageService\\n    {\\n        Task<int> DeleteAsync(int id);\\n    }\\n```\\n\\nTo this:\\n\\n```ts\\npublic interface ISageService\\n    {\\n        Task DeleteAsync(int id);\\n    }\\n```\\n\\nYour initial thought might be \\"well that\'s okay, I\'ll just lop off the `ReturnsAsync` statements and I\'m home free\\". That\'s what I thought anyway.... And I was \\\\***WRONG**\\\\*! A moments thought and you realise that there\'s still a return type - it\'s just `Task` now. What you want to do is something like this:\\n\\n```cs\\n_sageServiceMock\\n                .Setup(x => x.DeleteAsync(_sage.Id))\\n                .ReturnsAsync(void); // This\'ll definitely work... Probably\\n```\\n\\nNo it won\'t - `void` is not a real type and much as you might like it to, this is not going to work.\\n\\nSo right now you\'re thinking, well Moq probably has my back - it\'ll have something like `ReturnsTask`, right? Wrong! It\'s intentional it turns out - there\'s a discussion on [GitHub about the issue](https://github.com/Moq/moq4/issues/117). And in that discussion there\'s just what we need. We can use `Task.Delay` or `Task.FromResult` alongside Moq\'s good old `Returns` method and we\'re home free!\\n\\n## Here\'s one I made earlier...\\n\\n```cs\\nnamespace Proverb.Web.Controllers\\n{\\n    // ISageService again included inline for ease of explanation\\n    public interface ISageService\\n    {\\n        Task DeleteAsync(int id);\\n    }\\n\\n    public class SageController : ApiController\\n    {\\n        ISageService _sageService;\\n\\n        public SageController(ISageService userService)\\n        {\\n            _sageService = userService;\\n        }\\n\\n        public async Task<IHttpActionResult> Delete(int id)\\n        {\\n            await _sageService.DeleteAsync(id);\\n\\n            return Ok();\\n        }\\n   }\\n}\\n```\\n\\n```cs\\nnamespace Proverb.Web.Tests.ASPNet.Controllers\\n{\\n    [TestClass]\\n    public class SageControllerTests\\n    {\\n        private Mock<ISageService> _sageServiceMock;\\n        private SageController _controller;\\n\\n        readonly Task TaskOfNowt = Task.Delay(0);\\n        // Or you could use this equally valid but slightly more verbose approach:\\n        //readonly Task TaskOfNowt = Task.FromResult<object>(null);\\n\\n        [TestInitialize]\\n        public void Initialise()\\n        {\\n            _sageServiceMock = new Mock<ISageService>();\\n\\n            _controller = new SageController(_sageServiceMock.Object);\\n        }\\n\\n        [TestMethod]\\n        public async Task Delete_returns_an_Ok()\\n        {\\n            _sageServiceMock\\n                .Setup(x => x.DeleteAsync(_sage.Id))\\n                .Returns(TaskOfNowt); // Feels good doesn\'t it?\\n\\n            IHttpActionResult result = await _controller.Delete(_sage.Id);\\n\\n            var ok = result as OkResult;\\n            Assert.IsNotNull(ok);\\n            _sageServiceMock.Verify(x => x.DeleteAsync(_sage.Id));\\n        }\\n    }\\n}\\n```"},{"id":"/2014/09/13/migrating-jasmine-tests-to-typescript","metadata":{"permalink":"/2014/09/13/migrating-jasmine-tests-to-typescript","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2014-09-13-migrating-jasmine-tests-to-typescript/index.md","source":"@site/blog/2014-09-13-migrating-jasmine-tests-to-typescript/index.md","title":"Journalling the Migration of Jasmine Tests to TypeScript","description":"I previously attempted to migrate my Jasmine tests from JavaScript to TypeScript. The last time I tried it didn\'t go so well and I bailed. Thank the Lord for source control. But feeling I shouldn\'t be deterred I decided to have another crack at it.","date":"2014-09-13T00:00:00.000Z","formattedDate":"September 13, 2014","tags":[{"label":"Jasmine","permalink":"/tags/jasmine"},{"label":"TypeScript Language Service","permalink":"/tags/type-script-language-service"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"javascript","permalink":"/tags/javascript"},{"label":"implicit references","permalink":"/tags/implicit-references"}],"readingTime":9.495,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Journalling the Migration of Jasmine Tests to TypeScript","authors":"johnnyreilly","tags":["Jasmine","TypeScript Language Service","TypeScript","javascript","implicit references"],"hide_table_of_contents":false},"prevItem":{"title":"He tasks me; he heaps me.... I will wreak that MOQ upon him.","permalink":"/2014/10/03/he-tasks-me-he-heaps-me-i-will-wreak"},"nextItem":{"title":"Unit Testing an Angular Controller with Jasmine","permalink":"/2014/09/10/unit-testing-angular-controller-with"}},"content":"I previously attempted to migrate my Jasmine tests from JavaScript to TypeScript. The last time I tried it didn\'t go so well and I bailed. Thank the Lord for source control. But feeling I shouldn\'t be deterred I decided to have another crack at it.\\n\\nI did manage it this time... Sort of. Unfortunately there was a problem which I discovered right at the end. An issue with the TypeScript / Visual Studio tooling. So, just to be clear, this is not a blog post of \\"do this and it will work perfectly\\". On this occasion there will be some rough edges. This post exists, as much as anything else, as a record of the problems I experienced - I hope it will prove useful. Here we go:\\n\\n## What to Migrate?\\n\\nI\'m going to use one of the test files in my my side project [Proverb](https://github.com/johnnyreilly/Proverb). It\'s the tests for an AngularJS controller called `sageDetail` \\\\- I\'ve written about it [before](http://icanmakethiswork.blogspot.co.uk/2014/09/unit-testing-angular-controller-with.html). Here it is in all it\'s JavaScript-y glory:\\n\\n```ts\\ndescribe(\'Proverb.Web -> app-> controllers ->\', function () {\\n  beforeEach(function () {\\n    module(\'app\');\\n  });\\n\\n  describe(\'sageDetail ->\', function () {\\n    var $rootScope,\\n      getById_deferred, // deferred used for promises\\n      $location,\\n      $routeParams_stub,\\n      common,\\n      datacontext, // controller dependencies\\n      sageDetailController; // the controller\\n\\n    beforeEach(inject(function (\\n      _$controller_,\\n      _$rootScope_,\\n      _$q_,\\n      _$location_,\\n      _common_,\\n      _datacontext_\\n    ) {\\n      $rootScope = _$rootScope_;\\n      $q = _$q_;\\n\\n      $location = _$location_;\\n      common = _common_;\\n      datacontext = _datacontext_;\\n\\n      $routeParams_stub = { id: \'10\' };\\n      getById_deferred = $q.defer();\\n\\n      spyOn(datacontext.sage, \'getById\').and.returnValue(\\n        getById_deferred.promise\\n      );\\n      spyOn(common, \'activateController\').and.callThrough();\\n      spyOn(common.logger, \'getLogFn\').and.returnValue(\\n        jasmine.createSpy(\'log\')\\n      );\\n      spyOn($location, \'path\').and.returnValue(jasmine.createSpy(\'path\'));\\n\\n      sageDetailController = _$controller_(\'sageDetail\', {\\n        $location: $location,\\n        $routeParams: $routeParams_stub,\\n        common: common,\\n        datacontext: datacontext,\\n      });\\n    }));\\n\\n    describe(\'on creation ->\', function () {\\n      it(\\"controller should have a title of \'Sage Details\'\\", function () {\\n        expect(sageDetailController.title).toBe(\'Sage Details\');\\n      });\\n\\n      it(\'controller should have no sage\', function () {\\n        expect(sageDetailController.sage).toBeUndefined();\\n      });\\n\\n      it(\'datacontext.sage.getById should be called\', function () {\\n        expect(datacontext.sage.getById).toHaveBeenCalledWith(10, true);\\n      });\\n    });\\n\\n    describe(\'activateController ->\', function () {\\n      var sage_stub;\\n      beforeEach(function () {\\n        sage_stub = { name: \'John\' };\\n      });\\n\\n      it(\'should set sages to be the resolved promise values\', function () {\\n        getById_deferred.resolve(sage_stub);\\n        $rootScope.$digest(); // So Angular processes the resolved promise\\n\\n        expect(sageDetailController.sage).toBe(sage_stub);\\n      });\\n\\n      it(\\"should log \'Activated Sage Details View\' and set title with name\\", function () {\\n        getById_deferred.resolve(sage_stub);\\n        $rootScope.$digest(); // So Angular processes the resolved promise\\n\\n        expect(sageDetailController.log).toHaveBeenCalledWith(\\n          \'Activated Sage Details View\'\\n        );\\n        expect(sageDetailController.title).toBe(\\n          \'Sage Details: \' + sage_stub.name\\n        );\\n      });\\n    });\\n\\n    describe(\'gotoEdit ->\', function () {\\n      var sage_stub;\\n      beforeEach(function () {\\n        sage_stub = { id: 20 };\\n      });\\n\\n      it(\'should set $location.path to edit URL\', function () {\\n        getById_deferred.resolve(sage_stub);\\n        $rootScope.$digest(); // So Angular processes the resolved promise\\n\\n        sageDetailController.gotoEdit();\\n\\n        expect($location.path).toHaveBeenCalledWith(\\n          \'/sages/edit/\' + sage_stub.id\\n        );\\n      });\\n    });\\n  });\\n});\\n```\\n\\n## Off we go\\n\\nRighteo. Let\'s flip the switch. `sageDetail.js` you shall go to the ball! One wave of my magic wand and `sageDetail.js` becomes `sageDetail.ts`... Alakazam!! Of course we\'ve got to do the fiddling with the `csproj` file to include the dependent JavaScript files. (I\'ll be very pleased when ASP.Net vNext ships and I don\'t have to do this anymore....) So find this:\\n\\n```xml\\n<TypeScriptCompile Include=\\"app\\\\sages\\\\sageDetail.ts\\" />\\n```\\n\\nAnd add this:\\n\\n```xml\\n<Content Include=\\"app\\\\sages\\\\sageDetail.js\\">\\n  <DependentUpon>sageDetail.ts</DependentUpon>\\n</Content>\\n<Content Include=\\"app\\\\sages\\\\sageDetail.js.map\\">\\n  <DependentUpon>sageDetail.ts</DependentUpon>\\n</Content>\\n```\\n\\nWhat next? I\'ve a million red squigglies in my code. It\'s \\"could not find symbol\\" city. Why? Typings! We need typings! So let\'s begin - I\'m needing the Jasmine typings for starters. So let\'s hit NuGet and it looks like we need [this](http://www.nuget.org/packages/jasmine.TypeScript.DefinitelyTyped/):\\n\\n`Install-Package jasmine.TypeScript.DefinitelyTyped`That did no good at all. Still red squigglies. I\'m going to hazard a guess that this is something to do with the fact my JavaScript Unit Test project doesn\'t contain the various TypeScript artefacts that Visual Studio kindly puts into the web csproj for you. This is because I\'m keeping my JavaScript tests in a separate project from the code being tested. Also, the Visual Studio TypeScript tooling seems to work on the assumption that TypeScript will only be used within a web project; not a test project. Well I won\'t let that hold me back... Time to port the TypeScript artefacts in the web csproj over by hand. I\'ll take this:\\n\\n```xml\\n<Import Project=\\"$(MSBuildExtensionsPath32)\\\\Microsoft\\\\VisualStudio\\\\v$(VisualStudioVersion)\\\\TypeScript\\\\Microsoft.TypeScript.Default.props\\" Condition=\\"Exists(\'$(MSBuildExtensionsPath32)\\\\Microsoft\\\\VisualStudio\\\\v$(VisualStudioVersion)\\\\TypeScript\\\\Microsoft.TypeScript.Default.props\')\\" />\\n```\\n\\nAnd I\'ll also take this\\n\\n```xml\\n<PropertyGroup Condition=\\"\'$(Configuration)\' == \'Debug\'\\">\\n  <TypeScriptNoImplicitAny>True</TypeScriptNoImplicitAny>\\n</PropertyGroup>\\n<Import Project=\\"$(MSBuildExtensionsPath32)\\\\Microsoft\\\\VisualStudio\\\\v$(VisualStudioVersion)\\\\TypeScript\\\\Microsoft.TypeScript.targets\\" Condition=\\"Exists(\'$(MSBuildExtensionsPath32)\\\\Microsoft\\\\VisualStudio\\\\v$(VisualStudioVersion)\\\\TypeScript\\\\Microsoft.TypeScript.targets\')\\" />\\n```\\n\\nBingo bango - a difference. I no longer have red squigglies under the Jasmine statements (`describe`, `it` etc). But alas, I do everywhere else. One in particular draws my eye...\\n\\n## Could not find symbol \'$q\'\\n\\nOnce again TypeScript picks up the hidden bugs in my JavaScript:\\n\\n```ts\\n$q = _$q_;\\n```\\n\\nThat\'s right it\'s an implicit global. Quickly fixed:\\n\\n```ts\\nvar $q = _$q_;\\n```\\n\\n## Typings? Where we\'re going, we need typings...\\n\\nWe need more types. We\'re going to need the types created by our application; our controllers / services / directives etc. As well that we need the types used in the creation of the app. So the Angular typings etc. Since we\'re going to need to use `reference` statements to pull in the types created by our application I might as well use them to pull in the required definition files as well (eg `angular.d.ts`):\\n\\n```xml\\n/// <reference path=\\"../../../proverb.web/scripts/typings/angularjs/angular.d.ts\\" />\\n/// <reference path=\\"../../../proverb.web/scripts/typings/angularjs/angular-mocks.d.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/sages/sagedetail.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/common/common.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/services/datacontext.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/services/repository.sage.ts\\" />\\n```\\n\\nNow we need to work our way through the \\"variable \'x\' implicitly has an \'any\' type\\" messages. One thing we need to do is to amend our original sageDetails.ts file so that the `sageDetailRouteParams` interface and `SageDetail` class are exported from the controllers module. We can\'t use the types otherwise. Now we can add typings to our file - once finished it looks like this:\\n\\n```ts\\n/// <reference path=\\"../../../proverb.web/scripts/typings/angularjs/angular.d.ts\\" />\\n/// <reference path=\\"../../../proverb.web/scripts/typings/angularjs/angular-mocks.d.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/sages/sagedetail.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/common/common.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/services/datacontext.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/services/repository.sage.ts\\" />\\ndescribe(\'Proverb.Web -> app-> controllers ->\', function () {\\n  beforeEach(function () {\\n    module(\'app\');\\n  });\\n\\n  describe(\'sageDetail ->\', function () {\\n    var $rootScope: ng.IRootScopeService,\\n      // deferred used for promises\\n      getById_deferred: ng.IDeferred<sage>,\\n      // controller dependencies\\n      $location: ng.ILocationService,\\n      $routeParams_stub: controllers.sageDetailRouteParams,\\n      common: common,\\n      datacontext: datacontext,\\n      sageDetailController: controllers.SageDetail; // the controller\\n\\n    beforeEach(inject(function (\\n      _$controller_: any,\\n      _$rootScope_: ng.IRootScopeService,\\n      _$q_: ng.IQService,\\n      _$location_: ng.ILocationService,\\n      _common_: common,\\n      _datacontext_: datacontext\\n    ) {\\n      $rootScope = _$rootScope_;\\n      var $q = _$q_;\\n\\n      $location = _$location_;\\n      common = _common_;\\n      datacontext = _datacontext_;\\n\\n      $routeParams_stub = { id: \'10\' };\\n      getById_deferred = $q.defer();\\n\\n      spyOn(datacontext.sage, \'getById\').and.returnValue(\\n        getById_deferred.promise\\n      );\\n      spyOn(common, \'activateController\').and.callThrough();\\n      spyOn(common.logger, \'getLogFn\').and.returnValue(\\n        jasmine.createSpy(\'log\')\\n      );\\n      spyOn($location, \'path\').and.returnValue(jasmine.createSpy(\'path\'));\\n\\n      sageDetailController = _$controller_(\'sageDetail\', {\\n        $location: $location,\\n        $routeParams: $routeParams_stub,\\n        common: common,\\n        datacontext: datacontext,\\n      });\\n    }));\\n\\n    describe(\'on creation ->\', function () {\\n      it(\\"controller should have a title of \'Sage Details\'\\", function () {\\n        expect(sageDetailController.title).toBe(\'Sage Details\');\\n      });\\n\\n      it(\'controller should have no sage\', function () {\\n        expect(sageDetailController.sage).toBeUndefined();\\n      });\\n\\n      it(\'datacontext.sage.getById should be called\', function () {\\n        expect(datacontext.sage.getById).toHaveBeenCalledWith(10, true);\\n      });\\n    });\\n\\n    describe(\'activateController ->\', function () {\\n      var sage_stub: sage;\\n      beforeEach(function () {\\n        sage_stub = {\\n          name: \'John\',\\n          id: 10,\\n          username: \'John\',\\n          email: \'john@\',\\n          dateOfBirth: new Date(),\\n        };\\n      });\\n\\n      it(\'should set sages to be the resolved promise values\', function () {\\n        getById_deferred.resolve(sage_stub);\\n        $rootScope.$digest(); // So Angular processes the resolved promise\\n\\n        expect(sageDetailController.sage).toBe(sage_stub);\\n      });\\n\\n      it(\\"should log \'Activated Sage Details View\' and set title with name\\", function () {\\n        getById_deferred.resolve(sage_stub);\\n        $rootScope.$digest(); // So Angular processes the resolved promise\\n\\n        expect(sageDetailController.log).toHaveBeenCalledWith(\\n          \'Activated Sage Details View\'\\n        );\\n        expect(sageDetailController.title).toBe(\\n          \'Sage Details: \' + sage_stub.name\\n        );\\n      });\\n    });\\n\\n    describe(\'gotoEdit ->\', function () {\\n      var sage_stub: sage;\\n      beforeEach(function () {\\n        sage_stub = {\\n          name: \'John\',\\n          id: 20,\\n          username: \'John\',\\n          email: \'john@\',\\n          dateOfBirth: new Date(),\\n        };\\n      });\\n\\n      it(\'should set $location.path to edit URL\', function () {\\n        getById_deferred.resolve(sage_stub);\\n        $rootScope.$digest(); // So Angular processes the resolved promise\\n\\n        sageDetailController.gotoEdit();\\n\\n        expect($location.path).toHaveBeenCalledWith(\\n          \'/sages/edit/\' + sage_stub.id\\n        );\\n      });\\n    });\\n  });\\n});\\n```\\n\\n## So That\'s All Good...\\n\\nExcept it\'s not. When I run the tests using Chutzpah my `sageDetail` controller tests aren\'t found. My spider sense is tingling. This is something to do with the `reference` statements. They\'re throwing Chutzpah off. No bother, I can fix that with a quick tweak of the project file:\\n\\n```xml\\n<PropertyGroup Condition=\\"\'$(Configuration)\' == \'Debug\'\\">\\n    <TypeScriptNoImplicitAny>True</TypeScriptNoImplicitAny>\\n    <TypeScriptRemoveComments>True</TypeScriptRemoveComments>\\n  </PropertyGroup>\\n```\\n\\nThe TypeScript compiler will now strip comments; which includes the `reference` statements. Now my tests are detected \\\\***and**\\\\* they run. Yay!\\n\\n## Who Killed the TypeScript Language Service?\\n\\nYup it\'s dead. Whilst the compilation itself has no issues, take a look at the errors being presented for just one of the files back in the original web project:\\n\\n![](Screenshot-2014-09-12-23.15.22.png)\\n\\nIt looks like having one TypeScript project in a solution which uses `reference` comments somehow breaks the implicit referencing behaviour built into Visual Studio for other TypeScript projects in the solution. I can say this with some confidence as if I pull out the `reference` comments from the top of the test file that we\'ve converted then it\'s business as usual - the TypeScript Language Service lives once more. I\'m sure you can see the problem here though: the TypeScript test file doesn\'t compile. All rather unsatisfactory.\\n\\nI suspect that if I added `reference` comments throughout the web project the TypeScript Language Service would be just fine. But I rather like the implicit referencing functionality so I\'m not inclined to do that. After reaching something of a brick wall and thinking I had encountered a bug in the TypeScript Language service I [raised an issue on GitHub](https://github.com/Microsoft/TypeScript/issues/673).\\n\\n## Solutions....\\n\\nThanks to the help of [Mohamed Hegazy](https://github.com/mhegazy) it emerged that the problem was down to missing `reference` comments in my `sageDetail` controller tests. One thing I had not considered was the 2 different ways each of my TypeScript projects were working:\\n\\n- Proverb.Web uses the Visual Studio implicit referencing functionality. This means that I do not need to use `reference` comments in the TypeScript files in Proverb.Web.\\n- Proverb.Web.JavaScript does \\\\***not**\\\\* uses the implicit referencing functionality. It needs `reference` comments to resolve references.\\n\\nThe important thing to take away from this (and the thing I had overlooked) was that Proverb.Web.JavaScript uses `reference` comments to pull in Proverb.Web TypeScript files. Those files have dependencies which are \\\\***not**\\\\* stated using `reference` comments. So the compiler trips up when it tries to walk the dependency tree - there are no `reference` comments to be followed! So for example, `common.ts` has a dependency upon `logger.ts`. Fixing the TypeScript Language Service involves ensuring that the full dependency list is included in the `sageDetail` controller tests file, like so:\\n\\n```ts\\n/// <reference path=\\"../../../proverb.web/scripts/typings/angularjs/angular.d.ts\\" />\\n/// <reference path=\\"../../../proverb.web/scripts/typings/angularjs/angular-mocks.d.ts\\" />\\n/// <reference path=\\"../../../proverb.web/scripts/typings/angularjs/angular-route.d.ts\\" />\\n/// <reference path=\\"../../../proverb.web/scripts/typings/toastr/toastr.d.ts\\" />\\n/// <reference path=\\"../../../proverb.web/scripts/typings/underscore/underscore.d.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/sages/sagedetail.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/common/logger.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/common/common.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/services/datacontext.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/services/repositories.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/services/repository.sage.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/services/repository.saying.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/app.ts\\" />\\n/// <reference path=\\"../../../proverb.web/app/config.route.ts\\" />\\n```\\n\\nWith this in place you have a working solution, albeit one that is a little flaky. [An alternative solution was suggested by Noel Abrahams](https://github.com/Microsoft/TypeScript/issues/673#issuecomment-56024348) which I quote here:\\n\\n> Why not do the following?\\n>\\n> - Compile Proverb.Web with --declarations and the option for combining output into a single file. This should create a Proverb.Web.d.ts in your output directory.\\n> - In Proverb.Web.Tests.JavaScript add a reference to this file.\\n> - Right-click Proverb.Web.Tests.JavaScript select \\"Build Dependencies\\" > \\"Project Dependencies\\" and add a reference to Proverb.Web.\\n>\\n> I don\'t think directly referencing TypeScript source files is a good idea, because it causes the file to be rebuilt every time the dependant project is compiled.\\n\\nMohamed rather liked this solution. It looks like some more work is due to be done on the TypeScript tooling to make this less headache-y in future."},{"id":"/2014/09/10/unit-testing-angular-controller-with","metadata":{"permalink":"/2014/09/10/unit-testing-angular-controller-with","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2014-09-10-unit-testing-angular-controller-with/index.md","source":"@site/blog/2014-09-10-unit-testing-angular-controller-with/index.md","title":"Unit Testing an Angular Controller with Jasmine","description":"Anyone who reads my blog will know that I have been long in the habit of writing unit tests for my C# code. I\'m cool like that. However, it took me a while to get up and running writing unit tests for my JavaScript code. I finally got there using a combination of Jasmine 2.0 and Chutzpah. (Jasmine being my test framework and Chutzpah being my test runner.)","date":"2014-09-10T00:00:00.000Z","formattedDate":"September 10, 2014","tags":[{"label":"Jasmine","permalink":"/tags/jasmine"},{"label":"Controllers","permalink":"/tags/controllers"},{"label":"promises","permalink":"/tags/promises"},{"label":"Unit tests","permalink":"/tags/unit-tests"},{"label":"AngularJS","permalink":"/tags/angular-js"}],"readingTime":7.705,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Unit Testing an Angular Controller with Jasmine","authors":"johnnyreilly","tags":["Jasmine","Controllers","promises","Unit tests","AngularJS"],"hide_table_of_contents":false},"prevItem":{"title":"Journalling the Migration of Jasmine Tests to TypeScript","permalink":"/2014/09/13/migrating-jasmine-tests-to-typescript"},"nextItem":{"title":"Running JavaScript Unit Tests in AppVeyor","permalink":"/2014/09/06/running-javascript-unit-tests-in-appveyor"}},"content":"Anyone who reads my blog will know that I have been long in the habit of writing unit tests for my C# code. I\'m cool like that. However, it took me a while to get up and running writing unit tests for my JavaScript code. I finally [got there](http://icanmakethiswork.blogspot.com/2014/03/the-surprisingly-happy-tale-of-visual.html) using a combination of Jasmine 2.0 and Chutzpah. (Jasmine being my test framework and Chutzpah being my test runner.)\\n\\nI\'m getting properly into the habit of testing my JavaScript. I won\'t pretend it\'s been particularly fun but I firmly believe it will end up being useful... That\'s what I tell myself during the long dark tea-times of the soul anyway.\\n\\nI have a side project called [Proverb](https://github.com/johnnyreilly/Proverb). It doesn\'t do anything in particular - for the most part it\'s a simple application that displays the collected wise sayings of a team that I used to be part of. There\'s not much to it - a bit of CRUD, a dashboard. Not much more. Because of the project\'s simplicity it\'s ideal to use Proverb\'s underlying idea when trying out new technologies / frameworks. [The best way to learn is to do](http://en.wikipedia.org/wiki/Paul_Halmos). So if I want to learn \\"X\\", then building Proverb using \\"X\\" is a good way to go.\\n\\nI digress already. I had a version of Proverb built using a combination of [AngularJS and TypeScript](https://github.com/johnnyreilly/Proverb/tree/master/AngularTypeScript). I had written the Angular side of Proverb without any tests. Now I was able to write JavaScript tests for my Angular code that\'s just what I set out to do. It should prove something of a of [Code Kata](<http://en.wikipedia.org/wiki/Kata_(programming)>) too.\\n\\nWhilst I\'m at it I thought it might prove helpful if I wrote up how I approached writing unit tests for a single Angular controller. So here goes.\\n\\n## What I\'m Testing\\n\\nI have an Angular controller called `sagesDetail`. It powers this screen:\\n\\n![](sageDetailScreen.png)\\n\\n`sagesDetail` is a very simple controller. It does these things:\\n\\n1. Load the \\"sage\\" (think of it as just a \\"user\\") and make it available on the controller so it can be bound to the view.\\n2. Set the view title.\\n3. Log view activation.\\n4. Expose a `gotoEdit` method which, when called, redirects the user to the edit screen.\\n\\nThe controller is written in TypeScript and looks like this:\\n\\n### sagesDetail.ts\\n\\n```ts\\nmodule controllers {\\n  \'use strict\';\\n\\n  var controllerId = \'sageDetail\';\\n\\n  interface sageDetailRouteParams extends ng.route.IRouteParamsService {\\n    id: string;\\n  }\\n\\n  class SageDetail {\\n    log: loggerFunction;\\n    sage: sage;\\n    title: string;\\n\\n    static $inject = [\'$location\', \'$routeParams\', \'common\', \'datacontext\'];\\n    constructor(\\n      private $location: ng.ILocationService,\\n      private $routeParams: sageDetailRouteParams,\\n      private common: common,\\n      private datacontext: datacontext\\n    ) {\\n      this.sage = undefined;\\n      this.title = \'Sage Details\';\\n\\n      this.log = common.logger.getLogFn(controllerId);\\n\\n      this.activate();\\n    }\\n\\n    // Prototype methods\\n\\n    activate() {\\n      var id = parseInt(this.$routeParams.id, 10);\\n      var dataPromises: ng.IPromise<any>[] = [\\n        this.datacontext.sage\\n          .getById(id, true)\\n          .then((data) => (this.sage = data)),\\n      ];\\n\\n      this.common\\n        .activateController(dataPromises, controllerId, this.title)\\n        .then(() => {\\n          this.log(\'Activated Sage Details View\');\\n          this.title = \'Sage Details: \' + this.sage.name;\\n        });\\n    }\\n\\n    gotoEdit() {\\n      this.$location.path(\'/sages/edit/\' + this.sage.id);\\n    }\\n  }\\n\\n  angular.module(\'app\').controller(controllerId, SageDetail);\\n}\\n```\\n\\nWhen compiled to JavaScript it looks like this:\\n\\n### sageDetail.js\\n\\n```js\\nvar controllers;\\n(function (controllers) {\\n  \'use strict\';\\n\\n  var controllerId = \'sageDetail\';\\n\\n  var SageDetail = (function () {\\n    function SageDetail($location, $routeParams, common, datacontext) {\\n      this.$location = $location;\\n      this.$routeParams = $routeParams;\\n      this.common = common;\\n      this.datacontext = datacontext;\\n      this.sage = undefined;\\n      this.title = \'Sage Details\';\\n\\n      this.log = common.logger.getLogFn(controllerId);\\n\\n      this.activate();\\n    }\\n    // Prototype methods\\n    SageDetail.prototype.activate = function () {\\n      var _this = this;\\n      var id = parseInt(this.$routeParams.id, 10);\\n      var dataPromises = [\\n        this.datacontext.sage.getById(id, true).then(function (data) {\\n          return (_this.sage = data);\\n        }),\\n      ];\\n\\n      this.common\\n        .activateController(dataPromises, controllerId, this.title)\\n        .then(function () {\\n          _this.log(\'Activated Sage Details View\');\\n          _this.title = \'Sage Details: \' + _this.sage.name;\\n        });\\n    };\\n\\n    SageDetail.prototype.gotoEdit = function () {\\n      this.$location.path(\'/sages/edit/\' + this.sage.id);\\n    };\\n    SageDetail.$inject = [\'$location\', \'$routeParams\', \'common\', \'datacontext\'];\\n    return SageDetail;\\n  })();\\n\\n  angular.module(\'app\').controller(controllerId, SageDetail);\\n})(controllers || (controllers = {}));\\n//# sourceMappingURL=sageDetail.js.map\\n```\\n\\n## Now for the Tests\\n\\nI haven\'t yet made the move of switching over my Jasmine tests from JavaScript to TypeScript. (It\'s on my list but there\'s only so many things you can do at once...) For that reason the tests you\'ll see here are straight JavaScript. Below you will see the tests for the `sageDetail` controller.\\n\\nI have put very comments in the test code to make clear the intent to you, dear reader. Annotated the life out of them. Naturally I wouldn\'t expect a test to be so heavily annotated in a typical test suite - and you can be sure mine normally aren\'t!\\n\\n### Jasmine tests for sageDetail.js\\n\\n```js\\ndescribe(\'Proverb.Web -> app-> controllers ->\', function () {\\n  // Before each test runs we\'re going to need ourselves an Angular App to test - go fetch!\\n  beforeEach(function () {\\n    module(\'app\'); // module is an alias for <a href=\\"https://docs.angularjs.org/api/ngMock/function/angular.mock.module\\">angular.mock.module</a>\\n  });\\n\\n  // Tests for the sageDetail controller\\n  describe(\'sageDetail ->\', function () {\\n    // Declare describe-scoped variables\\n    var $rootScope,\\n      getById_deferred, // deferred used for promises\\n      $location,\\n      $routeParams_stub,\\n      common,\\n      datacontext, // controller dependencies\\n      sageDetailController; // the controller\\n\\n    // Before each test runs set up the controller using inject - an alias for <a href=\\"https://docs.angularjs.org/api/ngMock/function/angular.mock.inject\\">angular.mock.inject</a>\\n    beforeEach(inject(function (\\n      _$controller_,\\n      _$rootScope_,\\n      _$q_,\\n      _$location_,\\n      _common_,\\n      _datacontext_\\n    ) {\\n      // Note how each parameter is prefixed and suffixed with \\"_\\" - this an Angular nicety\\n      // which allows you to have variables in your tests with the original reference name.\\n      // So here we assign the injected parameters to the describe-scoped variables:\\n      $rootScope = _$rootScope_;\\n      $q = _$q_;\\n      $location = _$location_;\\n      common = _common_;\\n      datacontext = _datacontext_;\\n\\n      // Our controller has a dependency on an \\"id\\" property passed on the $routeParams\\n      // We\'re going to stub this out with a JavaScript object literal\\n      $routeParams_stub = { id: \'10\' };\\n\\n      // Our controller depends on a promise returned from this function: datacontext.sage.getById\\n      // Well strictly speaking it also uses a promise for activateController but since the activateController\\n      // promise just wraps the getById promise it will be resolved when the getById promise is.\\n      // Here we create a deferred representing the getById promise which we can resolve as we need to\\n      getById_deferred = $q.defer();\\n\\n      // set up a spy on datacontext.sage.getById and set it to return the promise of getById_deferred\\n      // this allows us to #1 detect that getById has been called\\n      // and #2 resolve / reject our promise as our test requires using getById_deferred\\n      spyOn(datacontext.sage, \'getById\').and.returnValue(\\n        getById_deferred.promise\\n      );\\n\\n      // set up a spy on common.activateController and set it to call through\\n      // this allows us to detect that activateController has been called whilst\\n      // maintaining existing controller functionality\\n      spyOn(common, \'activateController\').and.callThrough();\\n\\n      // set up spys on common.logger.getLogFn and $location.path so we can detect they have been called\\n      spyOn(common.logger, \'getLogFn\').and.returnValue(\\n        jasmine.createSpy(\'log\')\\n      );\\n      spyOn($location, \'path\').and.returnValue(jasmine.createSpy(\'path\'));\\n\\n      // create a sageDetail controller and inject the dependencies we have set up\\n      sageDetailController = _$controller_(\'sageDetail\', {\\n        $location: $location,\\n        $routeParams: $routeParams_stub,\\n        common: common,\\n        datacontext: datacontext,\\n      });\\n    }));\\n\\n    // Tests for the controller state at the point of the sageDetail controller\'s creation\\n    // ie before the getById / activateController promises have been resolved\\n    // So this tests the constructor (function) and the activate function up to the point\\n    // of the promise calls\\n    describe(\'on creation ->\', function () {\\n      it(\\"controller should have a title of \'Sage Details\'\\", function () {\\n        // tests this code has executed:\\n        // this.title = \\"Sage Details\\";\\n        expect(sageDetailController.title).toBe(\'Sage Details\');\\n      });\\n\\n      it(\'controller should have no sage\', function () {\\n        // tests this code has executed:\\n        // this.sage = undefined;\\n        expect(sageDetailController.sage).toBeUndefined();\\n      });\\n\\n      it(\'datacontext.sage.getById should be called\', function () {\\n        // tests this code has executed:\\n        // this.datacontext.sage.getById(id, true)\\n        expect(datacontext.sage.getById).toHaveBeenCalledWith(10, true);\\n      });\\n    });\\n\\n    // Tests for the controller state at the point of the resolution of the getById promise\\n    // ie after the getById / activateController promises have been resolved\\n    // So this tests the constructor (function) and the activate function after the point\\n    // of the promise calls\\n    describe(\'activateController ->\', function () {\\n      var sage_stub;\\n      beforeEach(function () {\\n        // Create a sage stub which will be used when resolving the getById promise\\n        sage_stub = { name: \'John\' };\\n      });\\n\\n      it(\'should set sages to be the resolved promise values\', function () {\\n        // Resolve the getById promise with the sage stub\\n        getById_deferred.resolve(sage_stub);\\n        $rootScope.$digest(); // So Angular processes the resolved promise\\n\\n        // tests this code has executed:\\n        // this.sage = data\\n        expect(sageDetailController.sage).toBe(sage_stub);\\n      });\\n\\n      it(\\"should log \'Activated Sage Details View\' and set title with name\\", function () {\\n        // Resolve the getById promise with the sage stub\\n        getById_deferred.resolve(sage_stub);\\n        $rootScope.$digest(); // So Angular processes the resolved promise\\n\\n        // tests this code has executed:\\n        // this.log(\\"Activated Sage Details View\\");\\n        // this.title = \\"Sage Details: \\" + this.sage.name;\\n        expect(sageDetailController.log).toHaveBeenCalledWith(\\n          \'Activated Sage Details View\'\\n        );\\n        expect(sageDetailController.title).toBe(\\n          \'Sage Details: \' + sage_stub.name\\n        );\\n      });\\n    });\\n\\n    // Tests for the gotoEdit function on the controller\\n    // Note that this will only be called *after* a controller has been created\\n    // and it depends upon a sage having first been loaded\\n    describe(\'gotoEdit ->\', function () {\\n      var sage_stub;\\n      beforeEach(function () {\\n        // Create a sage stub which will be used when resolving the getById promise\\n        sage_stub = { id: 20 };\\n      });\\n\\n      it(\'should set $location.path to edit URL\', function () {\\n        // Resolve the getById promise with the sage stub\\n        getById_deferred.resolve(sage_stub);\\n        $rootScope.$digest(); // So Angular processes the resolved promise\\n\\n        sageDetailController.gotoEdit();\\n\\n        // tests this code has executed:\\n        // this.$location.path(\\"/sages/edit/\\" + this.sage.id);\\n        expect($location.path).toHaveBeenCalledWith(\\n          \'/sages/edit/\' + sage_stub.id\\n        );\\n      });\\n    });\\n  });\\n});\\n```"},{"id":"/2014/09/06/running-javascript-unit-tests-in-appveyor","metadata":{"permalink":"/2014/09/06/running-javascript-unit-tests-in-appveyor","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2014-09-06-running-javascript-unit-tests-in-appveyor/index.md","source":"@site/blog/2014-09-06-running-javascript-unit-tests-in-appveyor/index.md","title":"Running JavaScript Unit Tests in AppVeyor","description":"With a little help from Chutzpah...","date":"2014-09-06T00:00:00.000Z","formattedDate":"September 6, 2014","tags":[{"label":"Jasmine","permalink":"/tags/jasmine"},{"label":"javascript","permalink":"/tags/javascript"},{"label":"Unit tests","permalink":"/tags/unit-tests"},{"label":"Continuous Integration","permalink":"/tags/continuous-integration"},{"label":"AppVeyor","permalink":"/tags/app-veyor"},{"label":"Chutzpah","permalink":"/tags/chutzpah"}],"readingTime":2.955,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Running JavaScript Unit Tests in AppVeyor","authors":"johnnyreilly","tags":["Jasmine","javascript","Unit tests","Continuous Integration","AppVeyor","Chutzpah"],"hide_table_of_contents":false},"prevItem":{"title":"Unit Testing an Angular Controller with Jasmine","permalink":"/2014/09/10/unit-testing-angular-controller-with"},"nextItem":{"title":"My Unrequited Love for Isolate Scope","permalink":"/2014/08/12/my-unrequited-love-for-isolate-scope"}},"content":"## With a little help from Chutzpah...\\n\\n[AppVeyor](http://www.appveyor.com) (if you\'re not aware of it) is a Continuous Integration provider. If you like, it\'s plug-and-play CI for .NET developers. It\'s lovely. And what\'s more it\'s [\\"free for open-source projects with public repositories hosted on GitHub and BitBucket\\"](http://www.appveyor.com/pricing). Boom! I recently hooked up 2 of my GitHub projects with AppVeyor. It took me all of... 10 minutes. If that? It really is \\\\***that**\\\\* good.\\n\\nBut.... There had to be a \\"but\\" otherwise I wouldn\'t have been writing the post you\'re reading. For a little side project of mine called [Proverb](https://github.com/johnnyreilly/Proverb) there were C# unit tests and there were JavaScript unit tests. And the JavaScript unit tests weren\'t being run... No fair!!!\\n\\n[Chutzpah](https://chutzpah.codeplex.com/) is a JavaScript test runner which at this point runs QUnit, Jasmine and Mocha JavaScript tests. I use the [Visual Studio extension](http://visualstudiogallery.msdn.microsoft.com/f8741f04-bae4-4900-81c7-7c9bfb9ed1fe) to run Jasmine tests on my machine during development. I\'ve also been able to use [Chutzpah for CI purposes with Visual Studio Online / Team Foundation Server](http://icanmakethiswork.blogspot.com/2014/03/the-surprisingly-happy-tale-of-visual.html). So what say we try and do the triple and make it work with AppVeyor too?\\n\\n## NuGet me?\\n\\nIn order that I could run Chutzpah I needed Chutzpah to be installed on the build machine. So I had 2 choices:\\n\\n1. Add Chutzpah direct to the repo\\n2. Add the [Chutzpah Nuget package](http://www.nuget.org/packages/chutzpah) to the solution\\n\\nUnsurprisingly I chose #2 - much cleaner.\\n\\n## Now to use Chutzpah\\n\\nTime to dust down the PowerShell. I created myself a \\"before tests script\\" and added it to my build. It looked a little something like this:\\n\\n```ps\\n# Locate Chutzpah\\n\\n$ChutzpahDir = get-childitem chutzpah.console.exe -recurse | select-object -first 1 | select -expand Directory\\n\\n# Run tests using Chutzpah and export results as JUnit format to chutzpah-results.xml\\n\\n$ChutzpahCmd = \\"$($ChutzpahDir)\\\\chutzpah.console.exe $($env:APPVEYOR_BUILD_FOLDER)\\\\AngularTypeScript\\\\Proverb.Web.Tests.JavaScript /junit .\\\\chutzpah-results.xml\\"\\nWrite-Host $ChutzpahCmd\\nInvoke-Expression $ChutzpahCmd\\n\\n# Upload results to AppVeyor one by one\\n\\n$testsuites = [xml](get-content .\\\\chutzpah-results.xml)\\n\\n$anyFailures = $FALSE\\nforeach ($testsuite in $testsuites.testsuites.testsuite) {\\n    write-host \\" $($testsuite.name)\\"\\n    foreach ($testcase in $testsuite.testcase){\\n        $failed = $testcase.failure\\n        $time = $testsuite.time\\n        if ($testcase.time) { $time = $testcase.time }\\n        if ($failed) {\\n            write-host \\"Failed   $($testcase.name) $($testcase.failure.message)\\"\\n            Add-AppveyorTest $testcase.name -Outcome Failed -FileName $testsuite.name -ErrorMessage $testcase.failure.message -Duration $time\\n            $anyFailures = $TRUE\\n        }\\n        else {\\n            write-host \\"Passed   $($testcase.name)\\"\\n            Add-AppveyorTest $testcase.name -Outcome Passed -FileName $testsuite.name -Duration $time\\n        }\\n\\n    }\\n}\\n\\nif ($anyFailures -eq $TRUE){\\n    write-host \\"Failing build as there are broken tests\\"\\n    $host.SetShouldExit(1)\\n}\\n```\\n\\nWhat this does is:\\n\\n1. Run Chutzpah from the installed NuGet package location, passing in the location of my Jasmine unit tests. In the case of my project there is a `chutzpah.json` file in the project which dictates how Chutzpah should run the tests. Also, [the JUnit flag is also passed](https://chutzpah.codeplex.com/wikipage?title=Command%20Line%20Options&referringTitle=Documentation) in order that Chutzpah creates a `chutzpah-results.xml` file of test results in the JUnit format.\\n2. We iterate through test results and tell AppVeyor about the the test passes and failures using the [Build Worker API](http://www.appveyor.com/docs/build-worker-api).\\n3. If there have been any failed tests then we fail the build. If you look [here](https://ci.appveyor.com/project/JohnReilly/proverb/build/1.0.17) you can see a deliberately failed build which demo\'s that this works as it should.\\n\\nThat\'s a wrap - We now have CI which includes our JavaScript tests! That\'s right we get to see beautiful screens like these:\\n\\n![](Screenshot-2014-09-06-21.43.15.png)\\n\\n![](Screenshot-2014-09-06-21.49.38.png)\\n\\n## Thanks to...\\n\\nThanks to Dan Jones, whose comments on [this discussion](http://help.appveyor.com/discussions/questions/390-running-jasmine-on-appveyor#comment_34433599) provided a number of useful pointers which moved me in the right direction. And thanks to Feador Fitzner who has generously [said AppVeyor will support JUnit in the future](http://help.appveyor.com/discussions/questions/495-integrating-chutzpah-into-appveyor#comment_34447202) which may simplify use of Chutzpah with AppVeyor even further."},{"id":"/2014/08/12/my-unrequited-love-for-isolate-scope","metadata":{"permalink":"/2014/08/12/my-unrequited-love-for-isolate-scope","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2014-08-12-my-unrequited-love-for-isolate-scope/index.md","source":"@site/blog/2014-08-12-my-unrequited-love-for-isolate-scope/index.md","title":"My Unrequited Love for Isolate Scope","description":"I wrote a little while ago about creating a directive to present server errors on the screen in an Angular application. In my own (not so humble opinion), it was really quite nice. I was particularly proud of my usage of isolate scope. However, pride comes before a fall.","date":"2014-08-12T00:00:00.000Z","formattedDate":"August 12, 2014","tags":[{"label":"Directives","permalink":"/tags/directives"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"javascript","permalink":"/tags/javascript"},{"label":"UI Bootstrap","permalink":"/tags/ui-bootstrap"},{"label":"Isolated Scope","permalink":"/tags/isolated-scope"},{"label":"AngularJS","permalink":"/tags/angular-js"}],"readingTime":4.5,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"My Unrequited Love for Isolate Scope","authors":"johnnyreilly","tags":["Directives","TypeScript","javascript","UI Bootstrap","Isolated Scope","AngularJS"],"hide_table_of_contents":false},"prevItem":{"title":"Running JavaScript Unit Tests in AppVeyor","permalink":"/2014/09/06/running-javascript-unit-tests-in-appveyor"},"nextItem":{"title":"Getting more RESTful with Web API and IHttpActionResult","permalink":"/2014/08/08/getting-more-RESTful-with-Web-API"}},"content":"[I wrote a little while ago about creating a directive to present server errors on the screen in an Angular application](http://icanmakethiswork.blogspot.com/2014/08/angularjs-meet-aspnet-server-validation.html). In my own (not so humble opinion), it was really quite nice. I was particularly proud of my usage of isolate scope. However, pride comes before a fall.\\n\\nIt turns out that using isolate scope in a directive is not always wise. Or rather \u2013 not always possible. And this is why:\\n\\n`Error: [$compile:multidir] Multiple directives [datepickerPopup, serverError] asking for new/isolated scope on: &lt;input name=\\"sage.dateOfBirth\\" class=\\"col-xs-12 col-sm-9\\" type=\\"text\\" value=\\"\\" ng-click=\\"vm.dateOfBirthDatePickerOpen()\\" server-error=\\"vm.errors\\" ng-model=\\"vm.sage.dateOfBirth\\" is-open=\\"vm.dateOfBirthDatePickerIsOpen\\" datepicker-popup=\\"dd MMM yyyy\\"&gt; `Ug. What happened here? Well, I had a date field that I was using my serverError directive on. Nothing too controversial there. The problem came when I tried to plug in [UI Bootstrap\u2019s datepicker](http://angular-ui.github.io/bootstrap/) as well. That\u2019s right the directives are fighting. Sad face.\\n\\nTo be more precise, it turns out that only one directive on an element is allowed to create an isolated scope. So if I want to use UI Bootstrap\u2019s datepicker (and I do) \u2013 well my serverError directive is toast.\\n\\n## A New Hope\\n\\nSo ladies and gentlemen, let me present serverError 2.0 \u2013 this time without isolated scope:\\n\\n### serverError.ts\\n\\n```ts\\n(function () {\\n  \'use strict\';\\n\\n  var app = angular.module(\'app\');\\n\\n  // Plant a validation message to the right of the element when it is declared invalid by the server\\n  app.directive(\'serverError\', [\\n    function () {\\n      // Usage:\\n      // <input class=\\"col-xs-12 col-sm-9\\"\\n      //        name=\\"sage.name\\" ng-model=\\"vm.sage.name\\" server-error=\\"vm.errors\\" />\\n\\n      var directive = {\\n        link: link,\\n        restrict: \'A\',\\n        require: \'ngModel\', // supply the ngModel controller as the 4th parameter in the link function\\n      };\\n      return directive;\\n\\n      function link(\\n        scope: ng.IScope,\\n        element: ng.IAugmentedJQuery,\\n        attrs: ng.IAttributes,\\n        ngModelController: ng.INgModelController\\n      ) {\\n        // Extract values from attributes (deliberately not using isolated scope)\\n        var errorKey: string = attrs[\'name\']; // eg \\"sage.name\\"\\n        var errorDictionaryExpression: string = attrs[\'serverError\']; // eg \\"vm.errors\\"\\n\\n        // Bootstrap alert template for error\\n        var template =\\n          \'<div class=\\"alert alert-danger col-xs-9 col-xs-offset-2\\" role=\\"alert\\"><i class=\\"glyphicon glyphicon-warning-sign larger\\"></i> %error%</div>\';\\n\\n        // Create an element to hold the validation message\\n        var decorator = angular.element(\'<div></div>\');\\n        element.after(decorator);\\n\\n        // Watch ngModelController.$error.server & show/hide validation accordingly\\n        scope.$watch(\\n          safeWatch(() => ngModelController.$error.server),\\n          showHideValidation\\n        );\\n\\n        function showHideValidation(serverError: boolean) {\\n          // Display an error if serverError is true otherwise clear the element\\n          var errorHtml = \'\';\\n          if (serverError) {\\n            var errorDictionary: { [field: string]: string } = scope.$eval(\\n              errorDictionaryExpression\\n            );\\n            errorHtml = template.replace(\\n              /%error%/,\\n              errorDictionary[errorKey] || \'Unknown error occurred...\'\\n            );\\n          }\\n          decorator.html(errorHtml);\\n        }\\n\\n        // wipe the server error message upon keyup or change events so can revalidate with server\\n        element.on(\'keyup change\', (event) => {\\n          scope.$apply(() => {\\n            ngModelController.$setValidity(\'server\', true);\\n          });\\n        });\\n      }\\n    },\\n  ]);\\n\\n  // Thanks @Basarat! http://stackoverflow.com/a/24863256/761388\\n  function safeWatch<T extends Function>(expression: T) {\\n    return () => {\\n      try {\\n        return expression();\\n      } catch (e) {\\n        return null;\\n      }\\n    };\\n  }\\n})();\\n```\\n\\n### serverError.js\\n\\n```js\\n(function () {\\n  \'use strict\';\\n\\n  var app = angular.module(\'app\');\\n\\n  // Plant a validation message to the right of the element when it is declared invalid by the server\\n  app.directive(\'serverError\', [\\n    function () {\\n      // Usage:\\n      // <input class=\\"col-xs-12 col-sm-9\\"\\n      //        name=\\"sage.name\\" ng-model=\\"vm.sage.name\\" server-error=\\"vm.errors\\" />\\n      var directive = {\\n        link: link,\\n        restrict: \'A\',\\n        require: \'ngModel\',\\n      };\\n      return directive;\\n\\n      function link(scope, element, attrs, ngModelController) {\\n        // Extract values from attributes (deliberately not using isolated scope)\\n        var errorKey = attrs[\'name\'];\\n        var errorDictionaryExpression = attrs[\'serverError\'];\\n\\n        // Bootstrap alert template for error\\n        var template =\\n          \'<div class=\\"alert alert-danger col-xs-9 col-xs-offset-2\\" role=\\"alert\\"><i class=\\"glyphicon glyphicon-warning-sign larger\\"></i> %error%</div>\';\\n\\n        // Create an element to hold the validation message\\n        var decorator = angular.element(\'<div></div>\');\\n        element.after(decorator);\\n\\n        // Watch ngModelController.$error.server & show/hide validation accordingly\\n        scope.$watch(\\n          safeWatch(function () {\\n            return ngModelController.$error.server;\\n          }),\\n          showHideValidation\\n        );\\n\\n        function showHideValidation(serverError) {\\n          // Display an error if serverError is true otherwise clear the element\\n          var errorHtml = \'\';\\n          if (serverError) {\\n            var errorDictionary = scope.$eval(errorDictionaryExpression);\\n            errorHtml = template.replace(\\n              /%error%/,\\n              errorDictionary[errorKey] || \'Unknown error occurred...\'\\n            );\\n          }\\n          decorator.html(errorHtml);\\n        }\\n\\n        // wipe the server error message upon keyup or change events so can revalidate with server\\n        element.on(\'keyup change\', function (event) {\\n          scope.$apply(function () {\\n            ngModelController.$setValidity(\'server\', true);\\n          });\\n        });\\n      }\\n    },\\n  ]);\\n\\n  // Thanks @Basarat! http://stackoverflow.com/a/24863256/761388\\n  function safeWatch(expression) {\\n    return function () {\\n      try {\\n        return expression();\\n      } catch (e) {\\n        return null;\\n      }\\n    };\\n  }\\n})();\\n```\\n\\nThis version of the serverError directive is from a users perspective identical to the previous version. But it doesn\u2019t use isolated scope \u2013 this means it can be used in concert with other directives which do.\\n\\nIt works by pulling the `name` and `serverError` values off the attrs parameter. `name` is just a string - the value of which never changes so it can be used as is. `serverError` is an expression that represents the error dictionary that is used to store the server error messages. This is accessed through use of `scope.$eval` as an when it needs to.\\n\\n## My Plea\\n\\nWhat I\u2019ve outlined here works. I\u2019ll admit that usage of `$eval` makes me feel a little bit dirty (I\u2019ve got [\u201ceval is evil\u201d](http://www.jslint.com/lint.html#evil) running through my head). Whilst it works, I\u2019m not sure what I\u2019ve done is necessarily best practice. After all [the Angular docs themselves say](https://docs.angularjs.org/guide/directive):\\n\\n> **\\\\*Best Practice:** Use the scope option to create isolate scopes when making components that you want to reuse throughout your app. \\\\*\\n\\nBut as we\u2019ve seen this isn\u2019t always an option. I\u2019ve written this post to document my own particular struggle and ask the question \u201cis there a better way?\u201d If you know then please tell me!"},{"id":"/2014/08/08/getting-more-RESTful-with-Web-API","metadata":{"permalink":"/2014/08/08/getting-more-RESTful-with-Web-API","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2014-08-08-getting-more-RESTful-with-Web-API/index.md","source":"@site/blog/2014-08-08-getting-more-RESTful-with-Web-API/index.md","title":"Getting more RESTful with Web API and IHttpActionResult","description":"Up until, well yesterday really, I tended to have my Web API action methods all returning 200\'s no matter what. Successful request? 200 for you sir! Some validation error in the model? 200 for you too ma\'am - but I\'ll wrap up the validation errors and send them back too. Database error? 200 and and an error message.","date":"2014-08-08T00:00:00.000Z","formattedDate":"August 8, 2014","tags":[{"label":"Web API 2","permalink":"/tags/web-api-2"},{"label":"IHttpActionResult","permalink":"/tags/i-http-action-result"}],"readingTime":2.68,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Getting more RESTful with Web API and IHttpActionResult","authors":"johnnyreilly","tags":["Web API 2","IHttpActionResult"],"hide_table_of_contents":false},"prevItem":{"title":"My Unrequited Love for Isolate Scope","permalink":"/2014/08/12/my-unrequited-love-for-isolate-scope"},"nextItem":{"title":"AngularJS meet ASP.Net Server Validation","permalink":"/2014/08/01/angularjs-meet-aspnet-server-validation"}},"content":"Up until, well yesterday really, I tended to have my Web API action methods all returning [200](http://en.wikipedia.org/wiki/HTTP_200#2xx_Success)\'s no matter what. Successful request? 200 for you sir! Some validation error in the model? 200 for you too ma\'am - but I\'ll wrap up the validation errors and send them back too. Database error? 200 and and an error message.\\n\\nIt kind of looked like this (this example taken from a [previous post](http://icanmakethiswork.blogspot.co.uk/2014/08/angularjs-meet-aspnet-server-validation.html)):\\n\\n```cs\\npublic class SageController : ApiController\\n{\\n  // ...\\n\\n  public IHttpActionResult Post(User sage)\\n  {\\n    if (!ModelState.IsValid) {\\n\\n      return Ok(new {\\n        Success = false,\\n        Errors = ModelState.ToErrorDictionary()\\n      });\\n    }\\n\\n    sage = _userService.Save(sage);\\n\\n    return Ok(new {\\n      Success = true,\\n      Entity = sage\\n    });\\n  }\\n\\n  // ...\\n}\\n```\\n\\nWell I\'m no RESTafarian but this felt a little... wrong. Like I wasn\'t fully embracing the web. I didn\'t want to have to include my own `Success` flag to indicate whether the request was good or not. I decided that I\'d rather have it at least a little more webby. To that end, I decided I\'d like to have 2xx success status codes for genuine success only and 4xx client error status codes for failures.\\n\\nLose the wrapper - embrace the web. This post is about doing just that.\\n\\n## Web API 2 - Bad Job on on the BadRequest Helper\\n\\nWeb API 2 ships with a whole host of API helper methods. Things like `Ok` (which you can see me using above) and `BadRequest`. `BadRequest` was what I had in mind to use in place of `Ok` where I had some kind of error I wanted to report to the client like so:\\n\\n```cs\\npublic class SageController : ApiController\\n{\\n  // ...\\n\\n  public IHttpActionResult Post(User sage)\\n  {\\n    if (!ModelState.IsValid) {\\n\\n      return BadRequest(new  {\\n        Errors = ModelState.ToErrorDictionary()\\n      });\\n    }\\n\\n    sage = _userService.Save(sage);\\n\\n    return Ok(new {\\n      Entity = sage\\n    });\\n  }\\n\\n  // ...\\n}\\n```\\n\\nLooks good right? No more need for my `Success` flag. Terser. Less code is better code. Unfortunately the built in `BadRequest` helper method doesn\'t have the flexibility of the `Ok` helper method - it doesn\'t allow you to send anything back you want. Fortunately this is easily remedied with a short extension method for `ApiController`:\\n\\n```cs\\nusing System.Net;\\nusing System.Web.Http;\\nusing System.Web.Http.Results;\\n\\nnamespace System.Web.Http\\n{\\n    public static class ControllerExtensions\\n    {\\n        public static IHttpActionResult BadRequest<T>(this ApiController controller, T obj)\\n        {\\n            return new NegotiatedContentResult<T>(HttpStatusCode.BadRequest, obj, controller);\\n        }\\n    }\\n}\\n```\\n\\nWith this in place I can then tweak my implementation to hook into the extension method:\\n\\n```cs\\npublic class SageController : ApiController\\n{\\n  // ...\\n\\n  public IHttpActionResult Post(User sage)\\n  {\\n    if (!ModelState.IsValid) {\\n      // See how we have \\"this.\\" before BadRequest so the Extension method is invoked\\n      return this.BadRequest(new  {\\n        Errors = ModelState.ToErrorDictionary()\\n      });\\n    }\\n\\n    sage = _userService.Save(sage);\\n\\n    return Ok(new {\\n      Entity = sage\\n    });\\n  }\\n\\n  // ...\\n}\\n```\\n\\nAnd now we have have an endpoint that serves up 2xx status codes or 4xx status codes just as I\'d hoped. Obviously this change in the way my action methods are returning will have implications for the consuming client (in my case an app built using AngularJS and $q). Essentially I can now use my `then` to handle the successes and my `catch` to handle the errors."},{"id":"/2014/08/01/angularjs-meet-aspnet-server-validation","metadata":{"permalink":"/2014/08/01/angularjs-meet-aspnet-server-validation","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2014-08-01-angularjs-meet-aspnet-server-validation/index.md","source":"@site/blog/2014-08-01-angularjs-meet-aspnet-server-validation/index.md","title":"AngularJS meet ASP.Net Server Validation","description":"So. You\'re using AngularJS to build your front end with ASP.Net running on the server side. You\'re a trustworthy dev - you know that validation on the client will only get you so far. You need to validate on the server.","date":"2014-08-01T00:00:00.000Z","formattedDate":"August 1, 2014","tags":[{"label":"asp.net","permalink":"/tags/asp-net"},{"label":"directive","permalink":"/tags/directive"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"server validation","permalink":"/tags/server-validation"},{"label":"AngularJS","permalink":"/tags/angular-js"}],"readingTime":10.4,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"AngularJS meet ASP.Net Server Validation","authors":"johnnyreilly","tags":["asp.net","directive","TypeScript","server validation","AngularJS"],"hide_table_of_contents":false},"prevItem":{"title":"Getting more RESTful with Web API and IHttpActionResult","permalink":"/2014/08/08/getting-more-RESTful-with-Web-API"},"nextItem":{"title":"HotTowel-Angular meet TypeScript","permalink":"/2014/07/03/hottowel-angular-meet-typescript"}},"content":"So. You\'re using AngularJS to build your front end with ASP.Net running on the server side. You\'re a trustworthy dev - you know that validation on the client will only get you so far. You need to validate on the server.\\n\\nMy particular scenario is where you have a form which you are saving. Angular serves you well when it comes to hooking in your own client side validation. But it doesn\'t really ship with anything that supports **nicely** presenting server side validation on the client. Invariably when you look around you find people duplicating their server side validation on the client and presenting all their server side validation in a `&lt;div&gt;` at the top of the screen.\\n\\nThis works but it\'s not as helpful to the user as it might be. It groups together all the validation from the server into one place. What I want is field level validation from the server that\'s presented on a field level basis on the screen.\\n\\nLet us travel together to this promised land...\\n\\n## What do we need client side?\\n\\nWell, let\'s start with a directive which I\'ll call `serverError`. This plants a validation message just _after_ the element being validated which is displayed when that element is declared invalid by the server. (That is to say when the `ngModel` has a `$error.server` set.) When the element is changed then the `$error.server` is unset in order that validation can be hidden and the form can be revalidated against the server.\\n\\nI\'m using TypeScript with Angular so for my JavaScript examples I\'ll give you both the TypeScript which I originally wrote and the generated JavaScript as well.\\n\\n### TypeScript\\n\\n```ts\\ninterface serverErrorScope extends ng.IScope {\\n    name: string;\\n    serverError: { [field: string]: string };\\n}\\n\\napp.directive(\\"serverError\\", [function () {\\n\\n  // Usage:\\n  // <input class=\\"col-xs-12 col-sm-9\\"\\n  //        name=\\"sage.name\\" ng-model=\\"vm.sage.name\\" server-error=\\"vm.errors\\" />\\n  var directive = {\\n    link: link,\\n    restrict: \\"A\\",\\n    require: \\"ngModel\\", // supply the ngModel controller as the 4th parameter in the link function\\n    scope: { // Pass in name and serverError to the scope\\n      name: \\"@\\",\\n      serverError: \\"=\\"\\n    }\\n  };\\n  return directive;\\n\\n  function link(scope: serverErrorScope, element: ng.IAugmentedJQuery, attrs: ng.IAttributes, ngModelController: ng.INgModelController) {\\n\\n    // Bootstrap alert template for error\\n    var template = \'<div class=\\"alert alert-danger\\" role=\\"alert\\">\' +\\n                               \'<i class=\\"glyphicon glyphicon-warning-sign\\"></i> \' +\\n                               \'%error%</div>\';\\n\\n    // Create an element to hold the validation message\\n    var decorator = angular.element(\'<div></div>\');\\n    element.after(decorator);\\n\\n    // Watch ngModelController.$error.server & show/hide validation accordingly\\n    scope.$watch(safeWatch(() => ngModelController.$error.server), showHideValidation);\\n\\n    function showHideValidation(serverError: boolean) {\\n\\n      // Display an error if serverError is true otherwise clear the element\\n      var errorHtml = \\"\\";\\n      if (serverError) {\\n        // Aliasing serverError and name to make it more obvious what their purpose is\\n        var errorDictionary = scope.serverError;\\n        var errorKey = scope.name;\\n        errorHtml = template.replace(/%error%/, errorDictionary[errorKey] || \\"Unknown error occurred...\\");\\n      }\\n      decorator.html(errorHtml);\\n    }\\n\\n    // wipe the server error message upon keyup or change events so can revalidate with server\\n    element.on(\\"keyup change\\", (event) => {\\n      scope.$apply(() => { ngModelController.$setValidity(\\"server\\", true); });\\n    });\\n  }\\n}]);\\n\\n// Thanks @Basarat! http://stackoverflow.com/a/24863256/761388\\nfunction safeWatch<t extends=\\"\\" function=\\"\\">(expression: T) {\\n  return () => {\\n    try {\\n      return expression();\\n    }\\n    catch (e) {\\n      return null;\\n    }\\n  };\\n}\\n</t>\\n```\\n\\n### JavaScript\\n\\n```js\\napp.directive(\'serverError\', [\\n  function () {\\n    // Usage:\\n    // <input class=\\"col-xs-12 col-sm-9\\"\\n    //        name=\\"sage.name\\" ng-model=\\"vm.sage.name\\" server-error=\\"vm.errors\\" />\\n    var directive = {\\n      link: link,\\n      restrict: \'A\',\\n      require: \'ngModel\',\\n      scope: {\\n        name: \'@\',\\n        serverError: \'=\',\\n      },\\n    };\\n    return directive;\\n\\n    function link(scope, element, attrs, ngModelController) {\\n      // Bootstrap alert template for error\\n      var template =\\n        \'<div class=\\"alert alert-danger\\" role=\\"alert\\">\' +\\n        \'<i class=\\"glyphicon glyphicon-warning-sign\\"></i> \' +\\n        \'%error%</div>\';\\n\\n      // Create an element to hold the validation message\\n      var decorator = angular.element(\'<div></div>\');\\n      element.after(decorator);\\n\\n      // Watch ngModelController.$error.server & show/hide validation accordingly\\n      scope.$watch(\\n        safeWatch(function () {\\n          return ngModelController.$error.server;\\n        }),\\n        showHideValidation\\n      );\\n\\n      function showHideValidation(serverError) {\\n        // Display an error if serverError is true otherwise clear the element\\n        var errorHtml = \'\';\\n        if (serverError) {\\n          // Aliasing serverError and name to make it more obvious what their purpose is\\n          var errorDictionary = scope.serverError;\\n          var errorKey = scope.name;\\n          errorHtml = template.replace(\\n            /%error%/,\\n            errorDictionary[errorKey] || \'Unknown error occurred...\'\\n          );\\n        }\\n        decorator.html(errorHtml);\\n      }\\n\\n      // wipe the server error message upon keyup or change events so can revalidate with server\\n      element.on(\'keyup change\', function (event) {\\n        scope.$apply(function () {\\n          ngModelController.$setValidity(\'server\', true);\\n        });\\n      });\\n    }\\n  },\\n]);\\n\\n// Thanks @Basarat! http://stackoverflow.com/a/24863256/761388\\nfunction safeWatch(expression) {\\n  return function () {\\n    try {\\n      return expression();\\n    } catch (e) {\\n      return null;\\n    }\\n  };\\n}\\n```\\n\\nIf you look closely at this directive you\'ll see it is restricted to be used as an attribute and it depends on 2 things:\\n\\n1. The value that the `server-error` attribute is set to should be an object which will contain key / values where the keys represent fields that are being validated.\\n2. The element being validated must have a name property (which will be used to look up the validation message in the `server-error` error \\"dictionary\\".\\n\\nTotally not clear, right? Let\'s have an example. Here is my \\"sageEdit\\" screen which you saw the screenshot of earlier:\\n\\n```html\\n<section class=\\"mainbar\\" ng-controller=\\"sageEdit as vm\\">\\n  <section class=\\"matter\\">\\n    <div class=\\"container-fluid\\">\\n      <form name=\\"form\\" novalidate role=\\"form\\">\\n        <div>\\n          <button\\n            class=\\"btn btn-info\\"\\n            ng-click=\\"vm.save()\\"\\n            ng-disabled=\\"!vm.canSave\\"\\n          >\\n            <i class=\\"glyphicon glyphicon-save\\"></i>Save\\n          </button>\\n          <span ng-show=\\"vm.hasChanges\\" class=\\"dissolve-animation ng-hide\\">\\n            <i class=\\"glyphicon glyphicon-asterisk orange\\"></i>\\n          </span>\\n        </div>\\n        <div class=\\"widget wblue\\">\\n          <div data-cc-widget-header title=\\"{{vm.title}}\\"></div>\\n          <div class=\\"widget-content form-horizontal\\">\\n            <div class=\\"form-group\\">\\n              <label class=\\"col-xs-12 col-sm-2\\">Name</label>\\n              <input\\n                class=\\"col-xs-12 col-sm-9\\"\\n                name=\\"sage.name\\"\\n                ng-model=\\"vm.sage.name\\"\\n                server-error=\\"vm.errors\\"\\n              />\\n            </div>\\n            <div class=\\"form-group\\">\\n              <label class=\\"col-xs-12 col-sm-2\\">Username</label>\\n              <input\\n                class=\\"col-xs-12 col-sm-9\\"\\n                name=\\"sage.userName\\"\\n                ng-model=\\"vm.sage.userName\\"\\n                server-error=\\"vm.errors\\"\\n              />\\n            </div>\\n            <div class=\\"form-group\\">\\n              <label class=\\"col-xs-12 col-sm-2\\">Email</label>\\n              <input\\n                class=\\"col-xs-12 col-sm-9\\"\\n                type=\\"email\\"\\n                name=\\"sage.email\\"\\n                ng-model=\\"vm.sage.email\\"\\n                server-error=\\"vm.errors\\"\\n              />\\n            </div>\\n          </div>\\n        </div>\\n      </form>\\n    </div>\\n  </section>\\n</section>\\n```\\n\\nIf you look closely at where `server-error` is used we have a name attribute set (eg \\"sage.email\\") and we\'re passing in something called `<em>vm.</em>errors` as the `server-error` attribute value. That\'s because we\'re using the \\"controller as\\" syntax and our controller is called `vm`.\\n\\nOn that controller we\'re going to have a dictionary style object called `errors`. If you wanted to you could put that object on the scope instead and omit the \\"vm.\\" prefix. You could call it `wrongThingsWhatISpottedWithYourModel` or `barry` \\\\- whatever floats your boat really. You get my point; it\'s flexible.\\n\\nLet\'s take a look at our sageEdit Angular controller:\\n\\n### TypeScript\\n\\n```ts\\nmodule controllers {\\n  \'use strict\';\\n\\n  interface sageEditRouteParams extends ng.route.IRouteParamsService {\\n    id: number;\\n  }\\n\\n  interface sageEditScope extends ng.IScope {\\n    form: ng.IFormController;\\n  }\\n\\n  class SageEdit {\\n    errors: { [field: string]: string };\\n    log: loggerFunction;\\n    logError: loggerFunction;\\n    logSuccess: loggerFunction;\\n    sage: sage;\\n    title: string;\\n\\n    private _isSaving: boolean;\\n\\n    static $inject = [\\n      \'$location\',\\n      \'$routeParams\',\\n      \'$scope\',\\n      \'common\',\\n      \'datacontext\',\\n    ];\\n    constructor(\\n      private $location: ng.ILocationService,\\n      private $routeParams: sageEditRouteParams,\\n      private $scope: sageEditScope,\\n      private common: common,\\n      private datacontext: datacontext\\n    ) {\\n      this.errors = {};\\n      this.log = common.logger.getLogFn(controllerId);\\n      this.logError = common.logger.getLogFn(controllerId, \'error\');\\n      this.logSuccess = common.logger.getLogFn(controllerId, \'success\');\\n      this.sage = undefined;\\n      this.title = \'Sage Edit\';\\n\\n      this._isSaving = false;\\n\\n      this.activate();\\n    }\\n\\n    // Prototype methods\\n\\n    activate() {\\n      var id = this.$routeParams.id;\\n      var dataPromises: ng.IPromise<any>[] = [this.getSage(id)];\\n\\n      this.common\\n        .activateController(dataPromises, controllerId, this.title)\\n        .then(() => {\\n          this.log(\'Activated Sage Edit View\');\\n          this.title = \'Sage Edit: \' + this.sage.name;\\n        });\\n    }\\n\\n    getSage(id: number) {\\n      return this.datacontext.sage.getById(id).then((sage) => {\\n        this.sage = sage;\\n      });\\n    }\\n\\n    save() {\\n      this.errors = {}; //Wipe server errors\\n      this._isSaving = true;\\n      this.datacontext.sage.save(this.sage).then((response) => {\\n        if (response.success) {\\n          this.sage = response.entity;\\n          this.logSuccess(\\n            \'Saved \' + this.sage.name + \' [\' + this.sage.id + \']\'\\n          );\\n          this.$location.path(\'/sages/detail/\' + this.sage.id);\\n        } else {\\n          this.logError(\'Failed to save\', response.errors);\\n\\n          angular.forEach(response.errors, (errors, field) => {\\n            (<ng.INgModelController>this.$scope.form[field]).$setValidity(\\n              \'server\',\\n              false\\n            );\\n            this.errors[field] = errors.join(\',\');\\n          });\\n        }\\n\\n        this._isSaving = false;\\n      });\\n    }\\n\\n    // Properties\\n\\n    get hasChanges(): boolean {\\n      return this.$scope.form.$dirty;\\n    }\\n\\n    get canSave(): boolean {\\n      return this.hasChanges && !this._isSaving && this.$scope.form.$valid;\\n    }\\n  }\\n\\n  var controllerId = \'sageEdit\';\\n  angular.module(\'app\').controller(controllerId, SageEdit);\\n}\\n```\\n\\n### JavaScript\\n\\n```js\\nvar controllers;\\n(function (controllers) {\\n  \'use strict\';\\n\\n  var SageEdit = (function () {\\n    function SageEdit($location, $routeParams, $scope, common, datacontext) {\\n      this.$location = $location;\\n      this.$routeParams = $routeParams;\\n      this.$scope = $scope;\\n      this.common = common;\\n      this.datacontext = datacontext;\\n      this.errors = {};\\n      this.log = common.logger.getLogFn(controllerId);\\n      this.logError = common.logger.getLogFn(controllerId, \'error\');\\n      this.logSuccess = common.logger.getLogFn(controllerId, \'success\');\\n      this.sage = undefined;\\n      this.title = \'Sage Edit\';\\n\\n      this._isSaving = false;\\n\\n      this.activate();\\n    }\\n    // Prototype methods\\n    SageEdit.prototype.activate = function () {\\n      var _this = this;\\n      var id = this.$routeParams.id;\\n      var dataPromises = [this.getSage(id)];\\n\\n      this.common\\n        .activateController(dataPromises, controllerId, this.title)\\n        .then(function () {\\n          _this.log(\'Activated Sage Edit View\');\\n          _this.title = \'Sage Edit: \' + _this.sage.name;\\n        });\\n    };\\n\\n    SageEdit.prototype.getSage = function (id) {\\n      var _this = this;\\n      return this.datacontext.sage.getById(id).then(function (sage) {\\n        _this.sage = sage;\\n      });\\n    };\\n\\n    SageEdit.prototype.save = function () {\\n      var _this = this;\\n      this.errors = {}; //Wipe server errors\\n      this._isSaving = true;\\n      this.datacontext.sage.save(this.sage).then(function (response) {\\n        if (response.success) {\\n          _this.sage = response.entity;\\n          _this.logSuccess(\\n            \'Saved \' + _this.sage.name + \' [\' + _this.sage.id + \']\'\\n          );\\n\\n          _this.$location.path(\'/sages/detail/\' + _this.sage.id);\\n        } else {\\n          _this.logError(\'Failed to save\', response.errors);\\n\\n          angular.forEach(response.errors, function (errors, field) {\\n            _this.$scope.form[field].$setValidity(\'server\', false);\\n            _this.errors[field] = errors.join(\',\');\\n          });\\n        }\\n\\n        _this._isSaving = false;\\n      });\\n    };\\n\\n    Object.defineProperty(SageEdit.prototype, \'hasChanges\', {\\n      // Properties\\n      get: function () {\\n        return this.$scope.form.$dirty;\\n      },\\n      enumerable: true,\\n      configurable: true,\\n    });\\n\\n    Object.defineProperty(SageEdit.prototype, \'canSave\', {\\n      get: function () {\\n        return this.hasChanges && !this._isSaving && this.$scope.form.$valid;\\n      },\\n      enumerable: true,\\n      configurable: true,\\n    });\\n    SageEdit.$inject = [\\n      \'$location\',\\n      \'$routeParams\',\\n      \'$scope\',\\n      \'common\',\\n      \'datacontext\',\\n    ];\\n    return SageEdit;\\n  })();\\n\\n  var controllerId = \'sageEdit\';\\n  angular.module(\'app\').controller(controllerId, SageEdit);\\n})(controllers || (controllers = {}));\\n```\\n\\nOkay - this is a shedload of code and most of it isn\'t relevant to you. I share it as I like to see things in context. Let\'s focus in on the important bits that you should take away. Firstly, our controller has a property called `errors`.\\n\\nSecondly, when we attempt to save our server sends back a JSON payload which, given a validation failure, looks something like this:\\n\\n```json\\n{\\n  \\"success\\": false,\\n  \\"errors\\": {\\n    \\"sage.name\\": [\\"The Name field is required.\\"],\\n    \\"sage.userName\\": [\\n      \\"The UserName field is required.\\",\\n      \\"The field UserName must be a string with a minimum length of 3 and a maximum length of 30.\\"\\n    ],\\n    \\"sage.email\\": [\\"The Email field is not a valid e-mail address.\\"]\\n  }\\n}\\n```\\n\\nSo let\'s pare back our `save` function to the bare necessities (those simple bare necessities, forget about your worries and your strife...):\\n\\n### TypeScript\\n\\n```ts\\nsave() {\\n\\n      this.errors = {}; //Wipe server errors\\n\\n      this.datacontext.sage.save(this.sage).then(response => {\\n\\n        if (response.success) {\\n          this.sage = response.entity;\\n        }\\n        else {\\n          angular.forEach(response.errors, (errors, field) => {\\n            (<ng.INgModelController>this.$scope.form[field]).$setValidity(\\"server\\", false);\\n            this.errors[field] = errors.join(\\",\\");\\n          });\\n        }\\n      });\\n    }\\n```\\n\\n### JavaScript\\n\\n```js\\nSageEdit.prototype.save = function () {\\n  var _this = this;\\n  this.errors = {}; //Wipe server errors\\n  this.datacontext.sage.save(this.sage).then(function (response) {\\n    if (response.success) {\\n      _this.sage = response.entity;\\n    } else {\\n      angular.forEach(response.errors, function (errors, field) {\\n        _this.$scope.form[field].$setValidity(\'server\', false);\\n        _this.errors[field] = errors.join(\',\');\\n      });\\n    }\\n  });\\n};\\n```\\n\\nAt the point of save we wipe any server error messages that might be stored on the client. Then, if we receive back a payload with errors we store those errors and set the validity of the relevant form element to false. This will trigger the display of the message by our directive.\\n\\nThat\'s us done for the client side. You\'re no doubt now asking yourself this question:\\n\\n## How can I get ASP.Net to send me this information?\\n\\nSo glad you asked. We\'ve a simple model that looks like this which has a number of data annotations:\\n\\n```cs\\npublic class Sage\\n{\\n  public int Id { get; set; }\\n\\n  [Required]\\n  public string Name { get; set; }\\n\\n  [Required]\\n  [StringLength(30, MinimumLength = 3)]\\n  public string UserName { get; set; }\\n\\n  [EmailAddress]\\n  public string Email { get; set; }\\n}\\n```\\n\\nWhen we save we post back to a Web API controller that looks like this:\\n\\n```cs\\npublic class SageController : ApiController\\n{\\n  // ...\\n\\n  public IHttpActionResult Post(User sage)\\n  {\\n    if (!ModelState.IsValid) {\\n\\n      return Ok(new\\n      {\\n        Success = false,\\n        Errors = ModelState.ToErrorDictionary()\\n      });\\n    }\\n\\n    sage = _userService.Save(sage);\\n\\n    return Ok(new\\n    {\\n      Success = true,\\n      Entity = sage\\n    });\\n  }\\n\\n  // ...\\n}\\n```\\n\\nAs you can see, when `ModelState` is not valid we send back a dictionary of the `ModelState` error messages keyed by property name. We generate this with an extension method I wrote called `ToErrorDictionary`:\\n\\n```cs\\npublic static class ModelStateExtensions\\n{\\n  public static Dictionary<string, IEnumerable<string>> ToErrorDictionary(\\n    this System.Web.Http.ModelBinding.ModelStateDictionary modelState, bool camelCaseKeyName = true)\\n  {\\n    var errors = modelState\\n      .Where(x => x.Value.Errors.Any())\\n      .ToDictionary(\\n        kvp => CamelCasePropNames(kvp.Key),\\n        kvp => kvp.Value.Errors.Select(e => e.ErrorMessage)\\n      );\\n\\n    return errors;\\n  }\\n\\n  private static string CamelCasePropNames(string propName)\\n  {\\n    var array = propName.Split(\'.\');\\n    var camelCaseList = new string[array.Length];\\n    for (var i = 0; i < array.Length; i++)\\n    {\\n      var prop = array[i];\\n      camelCaseList[i] = prop.Substring(0, 1).ToLower() + prop.Substring(1, prop.Length - 1);\\n    }\\n    return string.Join(\\".\\", camelCaseList);\\n  }\\n}\\n```\\n\\nThat\'s it - your solution front to back. It would be quite easy to hook other types of validation in server-side (database level checks etc). I hope you find this useful."},{"id":"/2014/07/03/hottowel-angular-meet-typescript","metadata":{"permalink":"/2014/07/03/hottowel-angular-meet-typescript","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2014-07-03-hottowel-angular-meet-typescript/index.md","source":"@site/blog/2014-07-03-hottowel-angular-meet-typescript/index.md","title":"HotTowel-Angular meet TypeScript","description":"I\'ve recently ported John Papa\'s popular Hot Towel Angular SPA Template to TypeScript. Why? Because it was there.","date":"2014-07-03T00:00:00.000Z","formattedDate":"July 3, 2014","tags":[{"label":"HotTowel","permalink":"/tags/hot-towel"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"AngularJS","permalink":"/tags/angular-js"},{"label":"JohnPapa","permalink":"/tags/john-papa"}],"readingTime":2.715,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"HotTowel-Angular meet TypeScript","authors":"johnnyreilly","tags":["HotTowel","TypeScript","AngularJS","JohnPapa"],"hide_table_of_contents":false},"prevItem":{"title":"AngularJS meet ASP.Net Server Validation","permalink":"/2014/08/01/angularjs-meet-aspnet-server-validation"},"nextItem":{"title":"A folk story wherein we shall find dates, DataAnnotations & data impedance mismatch","permalink":"/2014/06/20/dates-DataAnnotations-and-data-impedance-mismatch"}},"content":"I\'ve recently ported John Papa\'s popular [Hot Towel Angular SPA Template](https://github.com/johnpapa/HotTowel-Angular) to TypeScript. Why? [Because it was there.](http://en.wikipedia.org/wiki/George_Mallory)\\n\\nIf you\'d like to read more about HotTowel-Angular then have a read of [John Papa\'s post](http://www.johnpapa.net/hot-towel-angular/). You can find my port on GitHub [here](https://github.com/johnnyreilly/HotTowel-Angular-TypeScript).\\n\\n## What is this port you speak of?\\n\\nIt is **intentionally** a \\"bare bones\\" port of the HotTowel-Angular JavaScript code across to TypeScript. It\'s essentially the same code as John\'s - just with added type annotations (and yes it is `noImplicitAny` compliant).\\n\\nYou could, if you wanted to, go much further. You could start using a whole host of TypeScripts functionality: modules / classes / arrow functions... the whole shebang. But my port is deliberately not that; I didn\'t want to scare your horses... I wanted you to see how easy it is to move from JS to TS. And I\'m standing on the shoulders of that great giant [John Papa](https://twitter.com/john_papa) for that purpose.\\n\\nIf you wanted an example of how you might go further in an Angular port to TypeScript then you could take a look at my [previous post](http://icanmakethiswork.blogspot.co.uk/2014/06/migrating-from-angularjs-to-angularts.html) on the topic.\\n\\n## What\'s in the repo?\\n\\nThe repo contains the contents of HotTowel-Angular\'s app folder, with each JavaScript file converted over to TypeScript. The compiled JavaScript files are also included so that you can compare just how similar the compiled JavaScript is to John\'s original code.\\n\\nIn fact there are only 2 differences in the end:\\n\\n### 1\\\\. sidebar.js\'s `getNavRoutes`\\n\\n...had the filtering changed from this:\\n\\n```ts\\nreturn r.config.settings && r.config.settings.nav;\\n```\\n\\nto this:\\n\\n```ts\\nreturn r.config.settings && r.config.settings.nav ? true : false;\\n```\\n\\nThis was necessary as TypeScript insists that the array `filter` predicate returns a `boolean`. John\'s original method returns a number (`nav`\'s value to be clear) which actually seems to work fine. My assumption is that JavaScript\'s filter method is happy with a truth-y / false-y test which John\'s implementation would satisfy.\\n\\n### 2\\\\. common.js\'s `$broadcast`\\n\\n...had to be given a rest parameter to satisfy the TS compiler. John\'s original method exposed no parameters as it just forwards on whatever arguments are passed to it. This means that `$broadcast` has a bit of unused code in the head of the generated method:\\n\\n```js\\nvar args = [];\\nfor (var _i = 0; _i < arguments.length - 0; _i++) {\\n  args[_i] = arguments[_i + 0];\\n}\\n```\\n\\n## If you want to use this\\n\\nThen simply follow the instructions for installing [HotTowel-Angular](https://github.com/johnpapa/HotTowel-Angular) and then drop this repo\'s app folder over the one just created when HotTowel-Angular was installed. If you\'re using Visual Studio then make sure that you include the new TS files into your project and give them the `BuildAction` of `TypeScriptCompile`.\\n\\nYou\'ll need the following NuGet packages for the relevant DefinitelyTyped Typings:\\n\\n```ps\\nInstall-Package angularjs.TypeScript.DefinitelyTyped\\n    Install-Package angular-ui-bootstrap.TypeScript.DefinitelyTyped\\n    Install-Package jquery.TypeScript.DefinitelyTyped\\n    Install-Package spin.TypeScript.DefinitelyTyped\\n    Install-Package toastr.TypeScript.DefinitelyTyped\\n```\\n\\nAnd you\'re good to go. If you\'re not using Visual Studio then you may need to add in some `&lt;reference path=\\"angular.d.ts\\" /&gt;` etc. statements to the TypeScript files as well.\\n\\nIf you\'re interested in the specific versions of the typings that I used then you can find them in the `packages.config` of the repo.\\n\\n## Thanks\\n\\nTo John Papa for creating HotTowel-Angular. Much love.\\n\\nAnd my mum too... Just because."},{"id":"/2014/06/20/dates-DataAnnotations-and-data-impedance-mismatch","metadata":{"permalink":"/2014/06/20/dates-DataAnnotations-and-data-impedance-mismatch","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2014-06-20-dates-DataAnnotations-and-data-impedance-mismatch/index.md","source":"@site/blog/2014-06-20-dates-DataAnnotations-and-data-impedance-mismatch/index.md","title":"A folk story wherein we shall find dates, DataAnnotations & data impedance mismatch","description":"If you ever take a step back from what you\'re doing it can sometimes seem pretty abstract. Here\'s an example. I was looking at an issue in an app that I was supporting. The problem concerned a field which was to store a date value. Let\'s call it, for the sake of argument, valuation_date. (Clearly in reality the field name was entirely different... Probably.) This field was supposed to represent a specific date, like June 15th 2012 or 19th August 2014. To be clear, a date and \\\\*not\\\\* in any way, a time.","date":"2014-06-20T00:00:00.000Z","formattedDate":"June 20, 2014","tags":[{"label":"Date","permalink":"/tags/date"},{"label":"DateTime","permalink":"/tags/date-time"},{"label":"Moment.JS","permalink":"/tags/moment-js"},{"label":"DataAnnotations","permalink":"/tags/data-annotations"},{"label":"ValidationAttribute","permalink":"/tags/validation-attribute"}],"readingTime":4.035,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"A folk story wherein we shall find dates, DataAnnotations & data impedance mismatch","authors":"johnnyreilly","tags":["Date","DateTime","Moment.JS","DataAnnotations","ValidationAttribute"],"hide_table_of_contents":false},"prevItem":{"title":"HotTowel-Angular meet TypeScript","permalink":"/2014/07/03/hottowel-angular-meet-typescript"},"nextItem":{"title":"Migrating from AngularJS to AngularTS - a walkthrough","permalink":"/2014/06/01/migrating-from-angularjs-to-angularts"}},"content":"If you ever take a step back from what you\'re doing it can sometimes seem pretty abstract. Here\'s an example. I was looking at an issue in an app that I was supporting. The problem concerned a field which was to store a date value. Let\'s call it, for the sake of argument, `valuation_date`. (Clearly in reality the field name was entirely different... Probably.) This field was supposed to represent a specific date, like June 15th 2012 or 19th August 2014. To be clear, a date and \\\\***not**\\\\* in any way, a time.\\n\\n`valuation_date` was stored in a SQL database as a [`datetime`](http://msdn.microsoft.com/en-gb/library/ms187819.aspx). That\'s right a date with a time portion. I\'ve encountered this sort of scenario many times on systems I\'ve inherited. Although there is a [`date`](http://msdn.microsoft.com/en-gb/library/bb630352.aspx) type in SQL it\'s pretty rarely used. I think it only shipped in SQL Server with 2008 which may go some way to explaining this. Anyway, I digress...\\n\\n`valuation_date` was read into a field in a C# application called `ValuationDate` which was of type [`DateTime`](http://msdn.microsoft.com/en-us/library/system.datetime.aspx). As the name suggests this is also a date with a time portion. After a travelling through various layers of application this ended up being serialized as JSON and sent across the wire where it became a JavaScript variable by the name of `valuationDate` which had the type [`Date`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Date). Despite the deceptive name this is also, you guessed it, a date with a time portion. (Fine naming work there JavaScript!)\\n\\nYou can probably guess where I\'m going with this... Despite our (cough) rock solid naming convention, the situation had arisen where actual datetimes had snuck in. That\'s right, in the wilds of production, records with `valuation_date`s with time components had been spotted. My mission was to hunt them, kill them and stop them reproducing...\\n\\n## A Primitive Problem\\n\\nDates is a sticky topic in many languages. As I mentioned, SQL Server has a [`date`](http://msdn.microsoft.com/en-gb/library/bb630352.aspx) data type. C# has [`DateTime`](http://msdn.microsoft.com/en-gb/library/system.datetime.aspx). If you want to operate on Dates alone then you\'re best off talking looking at Jon Skeet\'s [NodaTime](http://nodatime.org/) \\\\- though most people start with `DateTime` and stick with it. (After all, it\'s native.) As to JavaScript, well primitive-wise there\'s no alternative to `Date` \\\\- but [`Moment.js`](http://momentjs.com/) may help.\\n\\nMy point is that it is a long standing issue in the development world. We represent data in types that aren\'t entirely meant for the purpose that they are used. It\'s not just restricted to dates, numbers have a comparable story around the issue of [decimals and doubles](http://csharpindepth.com/Articles/General/Decimal.aspx). As a result of data type issues, developers experience problems. Like the one I was facing.\\n\\n## An Attribute Solution\\n\\nThe source of the problem turned out to be the string JavaScript [`Date constructor`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Date) in an earlier version of Internet Explorer. The fix was switching away from using the JavaScript Date constructor in favour of using Moment.js\'s more dependable ability to parse strings into dates. Happy days we\'re working once more! Some quick work to put together a SQL script to fix up the data and we have ourselves our patch!\\n\\nBut we didn\'t want to get bitten again. We wanted ourselves a little [belts and braces](http://dictionary.cambridge.org/dictionary/british/belt-and-braces). What do do? Hang on a minute, lads \u2013 I\'ve got a great idea... It\'s `<a href=\\"http://msdn.microsoft.com/en-us/library/system.componentmodel.dataannotations.validationattribute(v=vs.110).aspx\\">ValidationAttribute</a>` time!\\n\\nWe whipped ourselves up an attribute that looked like this:\\n\\n```cs\\nusing System;\\nusing System.ComponentModel.DataAnnotations;\\nusing System.Globalization;\\n\\nnamespace My.Attributes\\n{\\n    [AttributeUsage(AttributeTargets.Property | AttributeTargets.Field, Inherited = false, AllowMultiple = false)]\\n    public class DateOnlyAttribute: ValidationAttribute\\n    {\\n        protected override ValidationResult IsValid(object value, ValidationContext validationContext)\\n        {\\n            if (value != null)\\n            {\\n                if (value is DateTime)\\n                {\\n                    // Date but not Time check\\n                    var date = (DateTime) value;\\n                    if (date.TimeOfDay != TimeSpan.Zero)\\n                    {\\n                        return new ValidationResult(date.ToString(\\"O\\", CultureInfo.InvariantCulture) + \\" is not a date - it is a date with a time\\", new[] { validationContext.MemberName });\\n                    }\\n                }\\n                else\\n                {\\n                    return new ValidationResult(\\"DateOnlyAttribute can only be used on DateTime? and DateTime\\", new[] { validationContext.MemberName });\\n                }\\n            }\\n\\n            return ValidationResult.Success;\\n        }\\n    }\\n}\\n```\\n\\nThis attribute does 2 things:\\n\\n1. Most importantly it fails validation for any `DateTime` or `DateTime?` that includes a time portion. It only allows through DateTimes where the clock strikes midnight. It\'s optimised for Cinderella.\\n2. It fails validation if the attribute is applied to any property which is not a `DateTime` or `DateTime?`.\\n\\nYou can decorate `DateTime` or `DateTime?` properties on your model with this attribute like so:\\n\\n```cs\\nnamespace My.Models\\n{\\n    public class ImAModelYouKnowWhatIMean\\n    {\\n        public int Id { get; set; }\\n\\n        [DateOnlyAttribute]\\n        public DateTime ValuationDate { get; set; }\\n\\n        // Other properties...\\n    }\\n}\\n```\\n\\nAnd if you\'re using MVC (or anything that makes use of the validation data annotations) then you\'ll now find that you are nicely protected from DateTimes masquerading as dates. Should they show up you\'ll find that `ModelState.IsValid` is false and you can kick them to the curb with alacrity!"},{"id":"/2014/06/01/migrating-from-angularjs-to-angularts","metadata":{"permalink":"/2014/06/01/migrating-from-angularjs-to-angularts","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2014-06-01-migrating-from-angularjs-to-angularts/index.md","source":"@site/blog/2014-06-01-migrating-from-angularjs-to-angularts/index.md","title":"Migrating from AngularJS to AngularTS - a walkthrough","description":"It started with nuns. Don\'t all good stories start that way? One of my (many) aunts is a Poor Clare nun. At some point in the distant past I was cajoled into putting together a simple website for her convent. This post is a walkthrough of how to migrate from AngularJS using JavaScript to AngularJS using TypeScript. It just so happens that the AngularJS app in question is the one that belongs to my mother\'s sister\'s convent.","date":"2014-06-01T00:00:00.000Z","formattedDate":"June 1, 2014","tags":[{"label":"Jasmine","permalink":"/tags/jasmine"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"Unit tests","permalink":"/tags/unit-tests"},{"label":"AngularJS","permalink":"/tags/angular-js"}],"readingTime":12.545,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Migrating from AngularJS to AngularTS - a walkthrough","authors":"johnnyreilly","tags":["Jasmine","TypeScript","Unit tests","AngularJS"],"hide_table_of_contents":false},"prevItem":{"title":"A folk story wherein we shall find dates, DataAnnotations & data impedance mismatch","permalink":"/2014/06/20/dates-DataAnnotations-and-data-impedance-mismatch"},"nextItem":{"title":"Team Foundation Server, Continuous Integration and separate projects for JavaScript unit tests","permalink":"/2014/05/15/team-foundation-server-continuous-integration-and-javascript-unit-tests-in-unit-test-project"}},"content":"It started with nuns. Don\'t all good stories start that way? One of my (many) aunts is a Poor Clare nun. At some point in the distant past I was cajoled into putting together a simple website for her convent. This post is a walkthrough of how to migrate from AngularJS using JavaScript to AngularJS using TypeScript. It just so happens that the AngularJS app in question is the one that belongs to my mother\'s sister\'s convent.\\n\\n## TL;DR - grab what you need\\n\\nFor reference the complete \\"before\\" and \\"after\\" projects can be found on GitHub [here](https://github.com/johnnyreilly/AngularJS2AngularTS). This is available so people can see clearly what changes have been made in the migration.\\n\\nThe content of the site is available for <u>reference only</u>\\n\\n. (Not that I can really imagine people creating their own \\"Poor Clares\\" site and hawking it to convents around the globe but I thought I\'d make the point.)\\n\\n## Background\\n\\nI\'ve been quietly maintaining this website / app for quite a while now. It\'s a very simple site; 95% of it is static content about the convent. The one piece of actual functionality is a page which allows the user of the website to send a prayer request to the nuns at the convent.\\n\\nBehind the scenes this sends 2 emails:\\n\\n- The first back to the person who submitted the prayer request assuring them that they will be prayed for.\\n- The second to the convent telling them the details of what the person would like prayer for.\\n\\n<aside><em>It\'s not accidental that I am not sharing the location of my aunt\'s website in this post. Given the inherent mischievousness of most developers (I should know, I am one) I harbour a fear that readers of this post might go away and submit many an insincere prayer request (or worse) to the convent. If that\'s you I don\'t intend to help you. You\'re clever, you\'ll find the site if you are so minded. But please know that the nuns who read any of your prayer requests are wonderful people (nuns get a bad rep) and that they love you. They *<strong>will</strong>* pray for you. They\'re good like that. I appeal to your better nature on this.</em></aside>\\n\\nRight now you are probably thinking this is an unusual post. Perhaps it is, but bear with me.\\n\\nOver time the website has had many incarnations. It\'s been table-based layout, it\'s used Kendo UI, it\'s used Bootstrap. It\'s been static HTML, it\'s been ASP.Net WebForms, it\'s been ASP.Net MVC and it\'s currently built using **AngularJS** with **MVC** on the back-end to handle bundling / minification and dispatching of emails.\\n\\nI decided to migrate this AngularJS app to use TypeScript. As I did that I thought I\'d document the process for anyone else who might be considering doing something similar. As it happens this is a particularly good candidate for migration as there\'s a full unit test suite for the app (written with Jasmine). Once I\'ve finished the migration these unit tests should pass, just as they do currently.\\n\\nYou are probably thinking to yourself \\"but TypeScript is just about adding compile-time annotations right? How could the unit tests not pass after migration?\\" Fair point, well made. Well that is generally true but I have something slightly different planned when we get to the controllers - you\'ll see what I mean...\\n\\nIt\'s also a good candidate for documenting a walkthrough as it\'s a particularly small and simple Angular app. It consists of just **3 controllers**, **2 services** and **1 app**.\\n\\nBefore I kick off I thought I\'d list a couple of guidelines / caveats on this post:\\n\\n- I don\'t intend to say much about the architecture of this application - I want to focus on the migration from JavaScript to TypeScript.\\n- The choices that I make for the migration path do not necessarily reflect the \\"one true way\\". Rather, they are pragmatic choices that I am making - there may be alternatives approaches here and there that could be used instead.\\n- I love Visual Studio - it\'s my IDE of choice and the one I am using as I perform the migration. Some of the points that I will make are Visual Studio specific - I will try and highlight that when appropriate.\\n\\n## Typings\\n\\nThe first thing we\'re going to need to get going are the Angular typing files which can be found on Definitely Typed [here](https://github.com/borisyankov/DefinitelyTyped/tree/master/angularjs). Since these typings are made available over [NuGet](https://www.nuget.org/packages/angularjs.TypeScript.DefinitelyTyped/) I\'m going to pull them in with a wave of my magic `Install-Package angularjs.TypeScript.DefinitelyTyped`.\\n\\nAs well as pulling in the typing files Visual Studio 2013 has also made some tweaks to my `PoorClaresAngular.csproj` file which it tells me about.\\n\\nAnd these are the TypeScript specific additions that Visual Studio has made to `PoorClaresAngular.csproj`:\\n\\n```xml\\n<Import\\n   Project=\\"$(MSBuildExtensionsPath32)\\\\Microsoft\\\\VisualStudio\\\\v$(VisualStudioVersion)\\\\TypeScript\\\\Microsoft.TypeScript.Default.props\\"\\n   Condition=\\"Exists(\'$(MSBuildExtensionsPath32)\\\\Microsoft\\\\VisualStudio\\\\v$(VisualStudioVersion)\\\\TypeScript\\\\Microsoft.TypeScript.Default.props\')\\" />\\n\\n  <TypeScriptToolsVersion>1.0</TypeScriptToolsVersion>\\n\\n  <Import\\n   Project=\\"$(MSBuildExtensionsPath32)\\\\Microsoft\\\\VisualStudio\\\\v$(VisualStudioVersion)\\\\TypeScript\\\\Microsoft.TypeScript.targets\\"\\n   Condition=\\"Exists(\'$(MSBuildExtensionsPath32)\\\\Microsoft\\\\VisualStudio\\\\v$(VisualStudioVersion)\\\\TypeScript\\\\Microsoft.TypeScript.targets\')\\" />\\n```\\n\\nI\'m going to add one extra of my own:\\n\\n```xml\\n<TypeScriptNoImplicitAny>True</TypeScriptNoImplicitAny>\\n```\\n\\nThis prevents you having variables of type `any` in your TypeScript codebase without you implicitly specifying the type. You can live without this but I\'ve found it\'s useful to catch where you\'re missing out on the benefit of static typing. Further to that, this option can be particularly useful when performing a migration. It will become obvious why this is the case as we go on.\\n\\nI decline the kind opportunity to further search NuGet as I\'m already on my way typing-wise. So let\'s review what has happened. Below you can see the typing files that have been pulled in and that the project and packages files were amended.\\n\\n## Changing JS files to TS files\\n\\nThis really should be as simple as changing all the JavaScript files underneath the `js` directory to have the suffix `ts`.\\n\\nAnd if you\'re not using Visual Studio it is. But if you are using Visual Studio there\'s a certain amount of fiddling required to include the generated `.js` and `.js.map` files associated with each `.ts` file. The easiest (hah!) thing to do is to crack open the project and wherever you find a `&lt;TypeScriptCompile Include=\\"js\\\\somePath.ts\\" /&gt;` to add in 2 `Content` statements, one for each generated file which states the dependency on the TypeScript file. For example:\\n\\n```xml\\n<TypeScriptCompile Include=\\"js\\\\services\\\\siteSectionService.ts\\" />\\n    <Content Include=\\"js\\\\services\\\\siteSectionService.js\\">\\n      <DependentUpon>siteSectionService.ts</DependentUpon>\\n    </Content>\\n    <Content Include=\\"js\\\\services\\\\siteSectionService.js.map\\">\\n      <DependentUpon>siteSectionService.ts</DependentUpon>\\n    </Content>\\n```\\n\\nIt\'s a bit of a pain to have to do this at the moment. Hopefully the Visual Studio tooling will catch up so this sort of tweaking becomes unnecessary.\\n\\n## Recap\\n\\nSo, where are we? Well, we\'ve got our project ready for TypeScript, we\'ve pulled in the Angular typings from Definitely Typed and we\'ve turned all our JavaScript files in the `js` directory into TypeScript files.\\n\\nNow we can actually start working through our TypeScript files and ensuring we\'re all typed correctly. Please note that because I\'m working in Visual Studio I get the benefit of implicit referencing; I don\'t have to explicitly state the typing files each TypeScript file relies on at the head of the file (eg `/// &lt;reference path=\\"angularjs/angular.d.ts\\" /&gt;`). If you aren\'t working in Visual Studio then you\'d need to add these yourself.\\n\\n## TypeScriptify `app.ts`\\n\\nOpening up `app.ts` we\'re presented with a few red squigglies.\\n\\nThese red squigglies are the direct result of my earlier opting in to `NoImplicitAny`. So in my view it\'s already paid for itself as it\'s telling me where I could start using typings. So to get things working nicely I\'ll give `$routeProvider` the type of `ng.route.IRouteProvider` and I\'ll explicitly specify the type of `any` for the 2 `params` parameters:\\n\\n```ts\\n// ...\\n    function ($routeProvider: ng.route.IRouteProvider) {\\n\\n        function getTheConventTemplateUrl(params: any) {\\n            var view = params.view || \\"home\\";\\n            return \\"partials/theConvent/\\" + view + \\".html\\";\\n        }\\n\\n        function getMainTemplateUrl(params: any) {\\n            var view = params.view || \\"home\\";\\n            return \\"partials/main/\\" + view + \\".html\\";\\n        }\\n\\n        // ...\\n    }\\n    // ...\\n```\\n\\n## TypeScriptify `siteSectionService.ts`\\n\\nOpening up `siteSectionService.ts` we\'re only presented with a single squiggly, and for the same reason as last time.\\n\\nThis error is easily remedied by giving `path` the type of `string`.\\n\\nWhat\'s more interesting / challenging is thinking about how we want to enforce the definition of `siteSectionService`. Remember, this is a service and as such it will be re-used elsewhere in the application (in both `navController` and `mainController`). What we need is an interface that describes what our (revealing module pattern) service exposes:\\n\\n```ts\\n\'use strict\';\\n\\ninterface ISiteSectionService {\\n  getSiteSection: () => string;\\n  determineSiteSection: (path: string) => void;\\n}\\n\\nangular.module(\'poorClaresApp.services\').factory(\\n  \'siteSectionService\',\\n\\n  [\\n    // No dependencies at present\\n    function (): ISiteSectionService {\\n      var siteSection = \'home\';\\n\\n      function getSiteSection() {\\n        return siteSection;\\n      }\\n\\n      function determineSiteSection(path: string) {\\n        var newSiteSection = \'home\';\\n\\n        if (path.indexOf(\'/theConvent/\') !== -1) {\\n          newSiteSection = \'theConvent\';\\n        } else if (path !== \'/\') {\\n          newSiteSection = \'main\';\\n        }\\n\\n        siteSection = newSiteSection;\\n      }\\n\\n      return {\\n        getSiteSection: getSiteSection,\\n        determineSiteSection: determineSiteSection,\\n      };\\n    },\\n  ]\\n);\\n```\\n\\nAs you can see the `ISiteSectionService ` interface is marked as the return type of the function. This ensures that what we return from the function satisfies that definition. Also, it allows us to re-use that interface elsewhere (as we will do later).\\n\\n## TypeScriptify `prayerRequestService.ts`\\n\\nOpening up `prayerRequestService.ts` we\'re again in `NoImplicitAny` country.\\n\\nThis is fixed up by defining `$http` as `ng.IHttpService` and `email` and `prayFor` as `string`.\\n\\nAs with `siteSectionService` we need to create an interface to define what `prayerRequestService` exposes. This leaves us with this:\\n\\n```ts\\n\'use strict\';\\n\\ninterface IPrayerRequestService {\\n  sendPrayerRequest: (\\n    email: string,\\n    prayFor: string\\n  ) => ng.IPromise<{\\n    success: boolean;\\n    text: string;\\n  }>;\\n}\\n\\nangular.module(\'poorClaresApp.services\').factory(\\n  \'prayerRequestService\',\\n\\n  [\\n    \'$http\',\\n    function ($http: ng.IHttpService): IPrayerRequestService {\\n      var url = \'/PrayerRequest\';\\n\\n      function sendPrayerRequest(email: string, prayFor: string) {\\n        var params = { email: email, prayFor: prayFor };\\n\\n        return $http.post(url, params).then(function (response) {\\n          return {\\n            success: response.data.success,\\n            text: response.data.text,\\n          };\\n        });\\n      }\\n\\n      return {\\n        sendPrayerRequest: sendPrayerRequest,\\n      };\\n    },\\n  ]\\n);\\n```\\n\\n## TypeScriptify `prayerRequestController.ts`\\n\\nOpening up `prayerRequestController.ts` leads me to the conclusion that I have **no interesting way left** of telling you that we once more need to supply types for our parameters. Let\'s take it as read that the same will happen on all remaining files as well eh? Hopefully by now it\'s fairly clear that this option is useful, even if only for a migration. I say this because using it forces you to think about what typings should be applied to your code.\\n\\nWe\'ll define `$scope` as `ng.IScope`, `prayerRequestService` as `IPrayerRequestService` (which we created just now) and `prayerRequest` as `{ email: string; prayFor: string }`. Which leaves me with this:\\n\\n```ts\\n\'use strict\';\\n\\nangular.module(\'poorClaresApp.controllers\').controller(\\n  \'PrayerRequestController\',\\n\\n  [\\n    \'$scope\',\\n    \'prayerRequestService\',\\n    function ($scope: ng.IScope, prayerRequestService: IPrayerRequestService) {\\n      var vm = this;\\n\\n      vm.send = function (prayerRequest: { email: string; prayFor: string }) {\\n        vm.message = {\\n          success: true,\\n          text: \'Sending...\',\\n        };\\n\\n        prayerRequestService\\n          .sendPrayerRequest(prayerRequest.email, prayerRequest.prayFor)\\n          .then(function (response) {\\n            vm.message = {\\n              success: response.success,\\n              text: response.text,\\n            };\\n          })\\n          .then(null, function (error) {\\n            // IE 8 friendly alias for catch\\n            vm.message = {\\n              success: false,\\n              text: \'Sorry your email was not sent\',\\n            };\\n          });\\n      };\\n    },\\n  ]\\n);\\n```\\n\\nI could move on but let\'s go for bonus points (and now you\'ll see why the unit test suite is so handy). To quote the Angular documentation:\\n\\n> In Angular, a Controller is a JavaScript constructor function that is used to augment the Angular Scope.\\n\\nSo let\'s see if we can swap over our vanilla contructor function for a TypeScript class. This will (in my view) better express the intention of the code. To do this I am essentially following the example laid down by my Definitely Typed colleague [Basarat](https://twitter.com/basarat). I highly recommend his [screencast on the topic](https://www.youtube.com/watch?v=WdtVn_8K17E). Also kudos to [Andrew Davey](https://twitter.com/andrewdavey) whose [post on the topic](http://aboutcode.net/2013/10/20/typescript-angularjs-controller-classes.html) also fed into this.\\n\\n```ts\\n\'use strict\';\\n\\nmodule poorClaresApp.controllers {\\n  class PrayerRequestController {\\n    static $inject = [\'$scope\', \'prayerRequestService\'];\\n    constructor(\\n      private $scope: ng.IScope,\\n      private prayerRequestService: IPrayerRequestService\\n    ) {}\\n\\n    message: { success: boolean; text: string };\\n\\n    send(prayerRequest: { email: string; prayFor: string }) {\\n      this.message = {\\n        success: true,\\n        text: \'Sending...\',\\n      };\\n\\n      this.prayerRequestService\\n        .sendPrayerRequest(prayerRequest.email, prayerRequest.prayFor)\\n        .then((response) => {\\n          this.message = {\\n            success: response.success,\\n            text: response.text,\\n          };\\n        })\\n        .then(null, (error) => {\\n          // IE 8 friendly alias for catch\\n          this.message = {\\n            success: false,\\n            text: \'Sorry your email was not sent\',\\n          };\\n        });\\n    }\\n  }\\n\\n  angular\\n    .module(\'poorClaresApp.controllers\')\\n    .controller(\'PrayerRequestController\', PrayerRequestController);\\n}\\n```\\n\\nMy only reservation with this approach is that we have to declare the TypeScript class outside the `angular.module...` statement. To avoid cluttering up global scope I\'ve placed our class in a module called `poorClaresApp.controllers` which maps nicely to our Angular module name. It would be nice if I could place the class definition in an [IIFE](http://en.wikipedia.org/wiki/Immediately-invoked_function_expression) to completely keep this completely isolated but TypeScript doesn\'t allow for that syntax (for reasons I\'m unclear about - the output would be legal JavaScript).\\n\\nFor a small class this seems to add a little noise but as classes grow in complexity I think this approach will quickly start to pay dividends. There are a few things worth noting about the above approach:\\n\\n- The required injectable parameters have moved into the class definition in the form of the `static $inject` statement. I personally like that this no longer sits outside the code it relates to.\\n- Because we\'re using TypeScript arrow functions (which preserve the outer \\"this\\" context) we are now free to dispose of the `var vm = this;` mechanism we\'re were previously using for the same purpose. Much more intuitive code to my mind.\\n- We are not actually using `$scope` at all in this controller - maybe it should be removed entirely in the long run.\\n\\n## TypeScriptify `navController.ts`\\n\\n`navController` can be simply converted like so:\\n\\n```ts\\n\'use strict\';\\n\\ninterface INavControllerScope extends ng.IScope {\\n  isCollapsed: boolean;\\n  siteSection: string;\\n}\\n\\nangular.module(\'poorClaresApp.controllers\').controller(\\n  \'NavController\',\\n\\n  [\\n    \'$scope\',\\n    \'siteSectionService\',\\n    function (\\n      $scope: INavControllerScope,\\n      siteSectionService: ISiteSectionService\\n    ) {\\n      $scope.isCollapsed = true;\\n      $scope.siteSection = siteSectionService.getSiteSection();\\n\\n      $scope.$watch(\\n        siteSectionService.getSiteSection,\\n        function (newValue, oldValue) {\\n          $scope.siteSection = newValue;\\n        }\\n      );\\n    },\\n  ]\\n);\\n```\\n\\nI\'d draw your attention to the creation of a the `INavControllerScope` interface that extends the default Angular $scope of `ng.IScope` with 2 extra properties.\\n\\nLet\'s also switch this over to the class based approach (there is less of a reason to on this occasion just looking at the size of the codebase but I\'m all about consistency of approach):\\n\\n```ts\\n\'use strict\';\\n\\nmodule poorClaresApp.controllers {\\n  interface INavControllerScope extends ng.IScope {\\n    isCollapsed: boolean;\\n    siteSection: string;\\n  }\\n\\n  class NavController {\\n    static $inject = [\'$scope\', \'siteSectionService\'];\\n    constructor(\\n      private $scope: INavControllerScope,\\n      private siteSectionService: ISiteSectionService\\n    ) {\\n      $scope.isCollapsed = true;\\n      $scope.siteSection = siteSectionService.getSiteSection();\\n\\n      $scope.$watch(\\n        siteSectionService.getSiteSection,\\n        function (newValue, oldValue) {\\n          $scope.siteSection = newValue;\\n        }\\n      );\\n    }\\n  }\\n\\n  angular\\n    .module(\'poorClaresApp.controllers\')\\n    .controller(\'NavController\', NavController);\\n}\\n```\\n\\n## TypeScriptify `mainController.ts`\\n\\nFinally, `mainController` can be converted as follows:\\n\\n```ts\\n\'use strict\';\\n\\nangular.module(\'poorClaresApp.controllers\').controller(\\n  \'MainController\',\\n\\n  [\\n    \'$location\',\\n    \'siteSectionService\',\\n    function (\\n      $location: ng.ILocationService,\\n      siteSectionService: ISiteSectionService\\n    ) {\\n      siteSectionService.determineSiteSection($location.path());\\n    },\\n  ]\\n);\\n```\\n\\nAgain it\'s just a case of assigning the undeclared types. For completeness lets also switch this over to the class based approach:\\n\\n```ts\\n\'use strict\';\\n\\nmodule poorClaresApp.controllers {\\n  class MainController {\\n    static $inject = [\'$location\', \'siteSectionService\'];\\n    constructor(\\n      private $location: ng.ILocationService,\\n      private siteSectionService: ISiteSectionService\\n    ) {\\n      siteSectionService.determineSiteSection($location.path());\\n    }\\n  }\\n\\n  angular\\n    .module(\'poorClaresApp.controllers\')\\n    .controller(\'MainController\', MainController);\\n}\\n```"},{"id":"/2014/05/15/team-foundation-server-continuous-integration-and-javascript-unit-tests-in-unit-test-project","metadata":{"permalink":"/2014/05/15/team-foundation-server-continuous-integration-and-javascript-unit-tests-in-unit-test-project","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2014-05-15-team-foundation-server-continuous-integration-and-javascript-unit-tests-in-unit-test-project/index.md","source":"@site/blog/2014-05-15-team-foundation-server-continuous-integration-and-javascript-unit-tests-in-unit-test-project/index.md","title":"Team Foundation Server, Continuous Integration and separate projects for JavaScript unit tests","description":"Do you like to separate out your unit tests from the project you are testing? I imagine so. My own practice when creating a new project in Visual Studio is to create a separate unit test project alongside whose responsibility is to house unit tests for that new project.","date":"2014-05-15T00:00:00.000Z","formattedDate":"May 15, 2014","tags":[{"label":"Jasmine","permalink":"/tags/jasmine"},{"label":"Visual Studio","permalink":"/tags/visual-studio"},{"label":"Continuous Integration","permalink":"/tags/continuous-integration"},{"label":"Team Foundation Server","permalink":"/tags/team-foundation-server"},{"label":"Chutzpah","permalink":"/tags/chutzpah"}],"readingTime":2.645,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Team Foundation Server, Continuous Integration and separate projects for JavaScript unit tests","authors":"johnnyreilly","tags":["Jasmine","Visual Studio","Continuous Integration","Team Foundation Server","Chutzpah"],"hide_table_of_contents":false},"prevItem":{"title":"Migrating from AngularJS to AngularTS - a walkthrough","permalink":"/2014/06/01/migrating-from-angularjs-to-angularts"},"nextItem":{"title":"TypeScript, JSDoc and Intellisense","permalink":"/2014/05/05/typescript-jsdoc-and-intellisense"}},"content":"Do you like to separate out your unit tests from the project you are testing? I imagine so. My own practice when creating a new project in Visual Studio is to create a separate unit test project alongside whose responsibility is to house unit tests for that new project.\\n\\nWhen I check in code for that project I expect the continuous integration build to kick off and, as part of that, the unit tests to be run. When it comes to running .NET tests then Team Foundation Server (and it\'s cloud counterpart Visual Studio Online) has your back. When it comes to running JavaScript tests then... not so much.\\n\\nThis post will set out:\\n\\n1. How to get JavaScript tests to run on TFS / VSO in a continuous integration scenario.\\n2. How to achieve this \\\\***without**\\\\* having to include your tests as part of web project.\\n\\nTo do this I will lean heavily (that\'s fancy language for \\"rip off entirely\\") on an [excellent blog post by Mathew Aniyan](https://blogs.msdn.com/b/visualstudioalm/archive/2012/07/09/javascript-unit-tests-on-team-foundation-service-with-chutzpah.aspx) which covers point #1. My contribution is point #2.\\n\\n## Points #1 and #2 in short order\\n\\nFirst of all, install Chutzpah on TFS / VSO. You can do this by following [Steps 1 - 6 from Mathew Aniyan\'s post](https://blogs.msdn.com/b/visualstudioalm/archive/2012/07/09/javascript-unit-tests-on-team-foundation-service-with-chutzpah.aspx). Instead of following steps 7 and 8 create a new unit test project in your solution.\\n\\n<aside>This unit test project will effectively be a C# project that hosts no real C# code at all. Instead we\'re going to use it to house JavaScript tests. If there is another way to have a separate project which TFS / VSO can pick up on and run tests in then please let me know. As far as I\'m aware though, this is the only game in town.</aside>\\n\\n**Edit 29/05/2014:** Matthew Manela (creator of Chutzpah) has confirmed that this is the correct approach - thanks chap!\\n\\n> [@johnny_reilly](https://twitter.com/johnny_reilly) Nope that is pretty much what you need to do.\\n>\\n> \u2014 Matthew Manela (@mmanela) [May 15, 2014](https://twitter.com/mmanela/statuses/466962743400996864)\\n\\n<script async=\\"\\" src=\\"//platform.twitter.com/widgets.js\\" charSet=\\"utf-8\\"><\/script>\\n\\nTo our unit test project add your JavaScript unit tests. These should be marked in Visual Studio with a Build Action of \\"Content\\" and a Copy to Output Directory of \\"Do not copy\\". You should be able to run these tests locally using the Visual Studio Chutzpah extension - or indeed in some other JavaScript test runner. Then, and this is the important part, edit the csproj file of your unit test project and add this `Import Project` statement:\\n\\n```xml\\n<Import Project=\\"$(VSToolsPath)\\\\WebApplications\\\\Microsoft.WebApplication.targets\\" Condition=\\"\'$(VSToolsPath)\' != \'\'\\" />\\n```\\n\\nOrdering is important in this case. It matters that this new statement sits after the other `Import Project` statements. So you should end up with a csproj file that looks in part like this: (comments added by me for clarity)\\n\\n```xml\\n\x3c!-- Pre-existing Import Project statements start --\x3e\\n  <Import Project=\\"$(VSToolsPath)\\\\TeamTest\\\\Microsoft.TestTools.targets\\" Condition=\\"Exists(\'$(VSToolsPath)\\\\TeamTest\\\\Microsoft.TestTools.targets\')\\" />\\n  <Import Project=\\"$(MSBuildToolsPath)\\\\Microsoft.CSharp.targets\\" />\\n  \x3c!-- Pre-existing Import Project statements end --\x3e\\n\\n  \x3c!-- New addition start --\x3e\\n  <Import Project=\\"$(VSToolsPath)\\\\WebApplications\\\\Microsoft.WebApplication.targets\\" Condition=\\"\'$(VSToolsPath)\' != \'\'\\" />\\n  \x3c!-- New addition end --\x3e\\n```\\n\\nCheck in your amended csproj and the unit tests to TFS / VSO. You should see the JavaScript unit tests being run as part of the build."},{"id":"/2014/05/05/typescript-jsdoc-and-intellisense","metadata":{"permalink":"/2014/05/05/typescript-jsdoc-and-intellisense","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2014-05-05-typescript-jsdoc-and-intellisense/index.md","source":"@site/blog/2014-05-05-typescript-jsdoc-and-intellisense/index.md","title":"TypeScript, JSDoc and Intellisense","description":"Days of Yore","date":"2014-05-05T00:00:00.000Z","formattedDate":"May 5, 2014","tags":[{"label":"jquery","permalink":"/tags/jquery"},{"label":"JSDoc","permalink":"/tags/js-doc"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"Intellisense","permalink":"/tags/intellisense"},{"label":"jQuery.d.ts","permalink":"/tags/j-query-d-ts"}],"readingTime":14.35,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"TypeScript, JSDoc and Intellisense","authors":"johnnyreilly","tags":["jquery","JSDoc","TypeScript","Intellisense","jQuery.d.ts"],"hide_table_of_contents":false},"prevItem":{"title":"Team Foundation Server, Continuous Integration and separate projects for JavaScript unit tests","permalink":"/2014/05/15/team-foundation-server-continuous-integration-and-javascript-unit-tests-in-unit-test-project"},"nextItem":{"title":"TypeScript this is what I want! (the unfortunate neglect of Instance Methods / callback functions)","permalink":"/2014/04/01/typescript-instance-methods"}},"content":"## Days of Yore\\n\\nIt was my first job. The web was alive and well at this point but still very much in it\'s infancy. Newspapers had only recently moved on from calling it \\"the information superhighway\\". No-one was doing _real_ programming for the web - the desktop was where it was at.\\n\\nAs for me, I was writing call centre software. It was all very exciting. Here was the idea: the phone on your desk would start ringing and through the magic of [TAPI](http://en.wikipedia.org/wiki/Telephony_Application_Programming_Interface) our app would be presented with the telephone number of the dialer. It would then look up that telephone number in the appropriate CRM application and pop the callers details on the screen. You\'d pick up the phone and bellow \\"why hello Mr Jones!\\" and either impress the caller with your incredible fore-knowledge of who had rung you or perhaps terrify them with our [Brave New Orwellian World](http://en.wikipedia.org/wiki/Nineteen_Eighty-Four).\\n\\nMy job was to work out how to call into the APIs of the various CRM applications / databases being used and extract the relevant information. So it goes without saying that I have spent a lot of time with badly documented APIs. Or in fact _undocumented_ APIs. I know pain my friend...\\n\\nHours and days were spent debugging and walking APIs just to find out what they could do and what information they exposed. This, I need hardly say, was dull and tedious work. Having spent longer than I care to remember with no more information on an API than method names has left its mark on me. I am consequently keener than your average dev on documentation and intellisense. When you\'ve stared at the coalface of the [Lotus Notes](http://en.wikipedia.org/wiki/IBM_Notes) API for 2 weeks with only Dephi 3 as your constant companion you\'d feel the same way too. (This was [before the days of Google](http://en.wikipedia.org/wiki/AltaVista) and actually being able to find stuff on the internet.)\\n\\nIf you can convey information about the API that you\'re building then I\'d say you\'re duty-bound to do so. Or at least that it\'s good manners.\\n\\n## Definitely Intellisensed\\n\\nWhen I started getting involved with the [Definitely Typed project](https://github.com/DefinitelyTyped) my focus was on giving good Intellisense. Where there was documentation for an API I wanted to get that popping in front of users when they hit the \\".\\" key:\\n\\n![screenshot of intellisense in visual studio](1200.JSDoc_in_VS.png-486x314.png)\\n\\nAs the above screenshot demonstrates [TypeScript supports Intellisense](https://devblogs.microsoft.com/typescript/announcing-typescript-0-8-2/) through a slightly tweaked implementation of [JSDoc](http://en.wikipedia.org/wiki/JSDoc):\\n\\n> With 0.8.2, the TypeScript compiler and tools now support JSDoc comments.\\n>\\n> In the TypeScript implementation, because types are already part of the system, we allow the JSDoc type annotation to be elided, as in the example above.\\n>\\n> You can now document a variety of language constructs (including classes, modules, interfaces, and functions) with comments that become part of the information displayed to the user. We\u2019ve also started extending lib.d.ts, the default JS and DOM API library, with JSDoc comments.\\n\\nPartly as an exercise in getting better acquainted with TypeScript and partly responding to my instinctive need to have nicely documented APIs I decided to start adding JSDoc comments to the world\'s most popular typings file [`jquery.d.ts`](https://github.com/borisyankov/DefinitelyTyped/blob/master/jquery/jquery.d.ts).\\n\\n## Why `jquery.d.ts`?\\n\\nWell a number of reasons:\\n\\n1. I used `jquery.d.ts` already myself and I\'m a firm believer in [eating your own dogfood](https://en.wikipedia.org/wiki/Eating_your_own_dog_food)\\n2. jQuery is well documented. I needed a source of information to power my JSDoc and <a href=\\"//api.jquery.com\\">api.jquery.com</a> had my back.\\n3. `jquery.d.ts` was widely used. Given how ubiquitous jQuery has become this typing file was unsurprisingly the most popular in the world. That was key for me as I wanted feedback - if I was making a mess of the typings I wanted someone to pitch in and tell me.\\n\\nJust to digress once more, points #2 and #3 turned out to be of particular note.\\n\\nConcerning point #2, I did find the occasional [error](https://github.com/borisyankov/DefinitelyTyped/pull/1471#issuecomment-31204115) or [inconsistency](https://github.com/borisyankov/DefinitelyTyped/pull/1835#issuecomment-37533088) in the jQuery API documentation. These were definitely the exception rather than the rule though. And thanks to the very helpful [Dave Methvin](https://github.com/dmethvin) these actually lead to [minor improvements to the jQuery API documentation](https://github.com/jquery/api.jquery.com/pull/460).\\n\\n![Tweet by @basarat at 8:47 PM on Dec 26, 2013 reading \\"#TypeScript definitions pointing out errors in JavaScript docs of a project #Jquery : https://github.com/borisyankov/DefinitelyTyped/pull/1471#issuecomment-31204115 caught by @johnny_reilly\\" original tweet here: https://twitter.com/basarat/status/416309213430689792](jquery-type-definition-tweet.png)\\n\\nConcerning point #3 I did indeed get feedback. As well as enriching `jquery.d.ts` with JSDoc goodness I also found myself fixing slight errors in the typings. Here and there I would find examples where `jquery.d.ts` was out of line the with API documentation. Where this was the case I would amend the typings to bring them into line - trying to make `jquery.d.ts` entirely API-compliant. This was <a href=\\"https://github.com/borisyankov/DefinitelyTyped/issues/1499\\">not always popular</a>. But despite the heat it generated I think it ended up leading to a better typing file. I\'m again grateful for Dave Methvin\'s thoughtful contributions.\\n\\n## Turning API documentation into JSDoc\\n\\nI wanted to take an example of API documentation and demonstrate how that can be applied to a typing file with particular focus on how JSDoc comments can be created to drive Intellisense. So let\'s take everyone\'s favourite jQuery method: `val`. The documentation of `val` can be found here: [api.jquery.com/val](http://api.jquery.com/val)\\n\\nBy the way, check out the \\\\*_entirely_\\\\* intuitive URL. Now you\'ve clocked just how straightforward that is you\'ve probably a fair idea how you could find pretty much any jQuery documentation you might need without recourse to Google. Brilliant!\\n\\nLet\'s take a look at what `val` looked like [before JSDoc](https://github.com/borisyankov/DefinitelyTyped/blob/c98eebb13724b5156f12318b68fc2d875ca6e4a3/jquery/jquery.d.ts#L364-L368) in the first version of the typing available on GitHub. (By the way, remember the original `jquery.d.ts`[ came out of the TypeScript team](https://typescript.codeplex.com/sourcecontrol/latest#samples/jquery/jquery.d.ts)):\\n\\n```ts\\nval(): any;\\nval(value: string[]): JQuery;\\nval(value: string): JQuery;\\nval(value: number): JQuery;\\nval(func: (index: any, value: any) => any): JQuery;\\n```\\n\\nAnd now let\'s look at `jquery.d.ts`[after JSDoc](https://github.com/borisyankov/DefinitelyTyped/blob/c259dba094121a389b41c573d5000dda7bdf2092/jquery/jquery.d.ts#L1494-L1545):\\n\\n```ts\\n/**\\n * Get the current value of the first element in the set of matched elements.\\n */\\nval(): any;\\n/**\\n * Set the value of each element in the set of matched elements.\\n *\\n * @param value A string of text or an array of strings corresponding to the value of each matched element to set as selected/checked.\\n */\\nval(value: string): JQuery;\\n/**\\n * Set the value of each element in the set of matched elements.\\n *\\n * @param value A string of text or an array of strings corresponding to the value of each matched element to set as selected/checked.\\n */\\nval(value: string[]): JQuery;\\n/**\\n * Set the value of each element in the set of matched elements.\\n *\\n * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.\\n */\\nval(func: (index: number, value: string) => string): JQuery;\\n/**\\n * Set the value of each element in the set of matched elements.\\n *\\n * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.\\n */\\nval(func: (index: number, value: string[]) => string): JQuery;\\n/**\\n * Set the value of each element in the set of matched elements.\\n *\\n * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.\\n */\\nval(func: (index: number, value: number) => string): JQuery;\\n/**\\n * Set the value of each element in the set of matched elements.\\n *\\n * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.\\n */\\nval(func: (index: number, value: string) => string[]): JQuery;\\n/**\\n * Set the value of each element in the set of matched elements.\\n *\\n * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.\\n */\\nval(func: (index: number, value: string[]) => string[]): JQuery;\\n/**\\n * Set the value of each element in the set of matched elements.\\n *\\n * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.\\n */\\nval(func: (index: number, value: number) => string[]): JQuery;\\n```\\n\\nMany changes yes? Let\'s break it down a little.\\n\\n## 1. You have 20 seconds to comply (with the API)\\n\\nThe first thing to note is the `number` setter method:\\n\\n```ts\\nval(value: number): JQuery;\\n```\\n\\nLet\'s have a look at the jQuery documentation for the simple setter:\\n\\n> ## [`.val( value )`](http://api.jquery.com/val/#val-value)\\n>\\n> <div><strong>value</strong></div>\\n>\\n> <div>Type: <a href=\\"http://api.jquery.com/Types/#String\\">String</a> or <a href=\\"http://api.jquery.com/Types/#Array\\">Array</a></div>\\n>\\n> <div>A string of text or an array of strings corresponding to the value of each matched element to set as selected/checked.</div>\\n\\nSee the problem? There is \\\\*_no_\\\\* `number` setter. The typings are wrong. So let\'s remedy this:\\n\\n```ts\\n<strike>val(value: number): JQuery;</strike>\\n```\\n\\n## 2. `String` and `Array of String` setters\\n\\nThe documentation states that we have setters which accept `String` and `Array of String`. These are already modeled in the existing typings by the `string` and `string[]` overloads:\\n\\n```ts\\nval(value: string[]): JQuery;\\n    val(value: string): JQuery;\\n```\\n\\nSo let\'s enrich these typings with some JSDoc:\\n\\n```ts\\n/**\\n * Set the value of each element in the set of matched elements.\\n *\\n * @param value A string of text or an array of strings corresponding to the value of each matched element to set as selected/checked.\\n */\\nval(value: string): JQuery;\\n/**\\n * Set the value of each element in the set of matched elements.\\n *\\n * @param value A string of text or an array of strings corresponding to the value of each matched element to set as selected/checked.\\n */\\nval(value: string[]): JQuery;\\n```\\n\\nIf you look you can see we\'ve added a related JSDoc style comment block prior to each overload. The first part of the comment (_\\"Set the value of...\\"_) is the overarching Intellisense that is displayed. Each of the `@param` statements represents each of the parameters and it\'s associated comment. By comparing the [API documentation](http://api.jquery.com/val/#val-value) to the JSDoc it\'s pretty clear how the API has been transformed into useful JSDoc.\\n\\nIt\'s worth noting that I could have taken the choice to customise the `@param value` comments based on the overload I was JSDoc-ing. Arguably it would have been more useful to have something like this instead:\\n\\n```ts\\n/**\\n     * Set the value of each element in the set of matched elements.\\n     *\\n     * @param value A string of text <strike>or an array of strings</strike> corresponding to the value of each matched element to set as selected/checked.\\n     */\\n    val(value: string): JQuery;\\n    /**\\n     * Set the value of each element in the set of matched elements.\\n     *\\n     * @param value <strike>A string of text or</strike> an array of strings corresponding to the value of each matched element to set as selected/checked.\\n     */\\n    val(value: string[]): JQuery;\\n```\\n\\nAfter some pondering I decided not to take this approach, just to maintain that close relationship between `jquery.d.ts` and [api.jquery.com](http://api.jquery.com/). It\'s open to debate how useful that relationship actually is so I thought I\'d just highlight this as a choice I made.\\n\\n## 3. Getter\\n\\nThe jQuery documentation for the getter looks like this:\\n\\n> [`.val()`](http://api.jquery.com/val/#val)\\n>\\n> Returns: <a href=\\"http://api.jquery.com/Types/#String\\">String</a> or <a href=\\"http://api.jquery.com/Types/#Number\\">Number</a> or <a href=\\"http://api.jquery.com/Types/#Array\\">Array</a>\\n>\\n> **Description: **Get the current value of the first element in the set of matched elements.\\n\\nSo the `val()` overload can return a `string`, a `number` or a `string[]`. Unfortunately there is no real way to model that in TypeScript at present due to the absence of [\\"union types\\"](https://typescript.codeplex.com/workitem/1364). Union types are being [discussed at present](https://typescript.codeplex.com/discussions/543598#PostDetailsCell_1239340) but in TypeScript v1.0 world the only viable approach is returning the `any` type. This implies `val()` returns any possible JavaScript value from `boolean` to `Function` and straight on \'til morning. So clearly this isn\'t accurate but importantly it also allows for the possibility of `val()` returning `string`, `number` or `string[]`.\\n\\nThe final getter typing with JSDoc applied ends up looking like this:\\n\\n```ts\\n/**\\n     * Get the current value of the first element in the set of matched elements.\\n     */\\n    val(): any;\\n```\\n\\nAs you can see the _\\"Get the current value...\\"_ from the API docs has been used as the overarching Intellisense that is displayed for the getter.\\n\\n## 4. The `Function` setter\\n\\nFinally we\'re going to take a look at the `Function` setter which is documented as follows:\\n\\n> [`.val( function(index, value) )`](http://api.jquery.com/val/#val-functionindex--value)\\n>\\n> `function(index, value)`\\n>\\n> <div>Type: <a href=\\"http://api.jquery.com/Types/#Function\\">Function</a>()</div>\\n>\\n> <div>A function returning the value to set. <code>this</code> is the current element. Receives the index position of the element in the set and the old value as arguments.</div>\\n\\nIf you cast your eyes back to the original typings for the `Function` setter you\'ll see they look like this:\\n\\n```ts\\nval(func: (index: any, value: any) => any): JQuery;\\n```\\n\\nThis is a good start but it\'s less accurate than it could be in a number of ways:\\n\\n1. `index` is a `number` \\\\- we needn\'t keep it as an `any`\\n2. `value` is the old value - we know from our getter that this can be a `string`, `number` or `string[]`. So we can lose the `any` in favour of overloads which specify different types for `value` in each.\\n3. The return value of the function is the value that should be set. We know from our other setters that the possible types allowed here are `string` and `string[]`. (And yes I\'m as puzzled as you are that the getter can return a `number` but the setter can\'t set one.) That being the case it makes sense for us to have overloads with functions that return both `string` and `string[]`\\n\\nSo, we\'ve got a little tidy up to do for #1 and extra overloads to add for #2 and #3. We\'re going to replace the single `Function` setter with 3 overloads to cater for #2. Then for #3 we\'re going to take each of the 3 overloads we\'ve just created and make 2 overloads place of each to handle the different return types. This will lead us with the grand total of 6 overloads to model our `Function` setter!\\n\\n```ts\\n/**\\n     * Set the value of each element in the set of matched elements.\\n     *\\n     * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.\\n     */\\n    val(func: (index: number, value: string) => string): JQuery;\\n    /**\\n     * Set the value of each element in the set of matched elements.\\n     *\\n     * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.\\n     */\\n    val(func: (index: number, value: string[]) => string): JQuery;\\n    /**\\n     * Set the value of each element in the set of matched elements.\\n     *\\n     * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.\\n     */\\n    val(func: (index: number, value: number) => string): JQuery;\\n    /**\\n     * Set the value of each element in the set of matched elements.\\n     *\\n     * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.\\n     */\\n    val(func: (index: number, value: string) => string[]): JQuery;\\n    /**\\n     * Set the value of each element in the set of matched elements.\\n     *\\n     * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.\\n     */\\n    val(func: (index: number, value: string[]) => string[]): JQuery;\\n    /**\\n     * Set the value of each element in the set of matched elements.\\n     *\\n     * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.\\n     */\\n    val(func: (index: number, value: number) => string[]): JQuery;\\n```\\n\\nA cursory glance shows that each of the overloads above shares the same JSDoc. Each has the _\\"Set the value...\\"_ from the API docs as the overarching Intellisense that is displayed for the `Function` setter. And each has the same `@param func` comment as well.\\n\\n## It could be you...\\n\\nThis post is much longer than I ever intended it to be. But I wanted to show how easy it is to create typings with JSDoc to drive Intellisense. For no obvious reason people generally don\'t make a great deal of use of JSDoc when creating typings. Perhaps the creators have no good source of documentation (a common problem). Or perhaps people are not even aware it\'s a possibility - they don\'t know about the TypeScript support of JSDoc. In case it\'s the latter I think this post was worth writing."},{"id":"/2014/04/01/typescript-instance-methods","metadata":{"permalink":"/2014/04/01/typescript-instance-methods","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2014-04-01-typescript-instance-methods/index.md","source":"@site/blog/2014-04-01-typescript-instance-methods/index.md","title":"TypeScript this is what I want! (the unfortunate neglect of Instance Methods / callback functions)","description":"I was recently reading Jeff Walker\'s blog post \\"Why TypeScript Isn\'t the Answer\\". This is part of series in which Jeff goes through various compile-to-JavaScript technologies including TypeScript, CoffeeScript and Dart and explains his view of why he feels they don\'t quite hit the mark.","date":"2014-04-01T00:00:00.000Z","formattedDate":"April 1, 2014","tags":[{"label":"callback functions","permalink":"/tags/callback-functions"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"Lexical scoping","permalink":"/tags/lexical-scoping"},{"label":"closure","permalink":"/tags/closure"},{"label":"Instance methods","permalink":"/tags/instance-methods"}],"readingTime":4.965,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"TypeScript this is what I want! (the unfortunate neglect of Instance Methods / callback functions)","authors":"johnnyreilly","tags":["callback functions","TypeScript","Lexical scoping","closure","Instance methods"],"hide_table_of_contents":false},"prevItem":{"title":"TypeScript, JSDoc and Intellisense","permalink":"/2014/05/05/typescript-jsdoc-and-intellisense"},"nextItem":{"title":"The Surprisingly Happy Tale of Visual Studio Online, Continous Integration and Chutzpah","permalink":"/2014/03/17/the-surprisingly-happy-tale-of-visual"}},"content":"I was recently reading [Jeff Walker\'s blog post \\"Why TypeScript Isn\'t the Answer\\"](http://www.walkercoderanger.com/blog/2014/02/typescript-isnt-the-answer/). This is part of series in which Jeff goes through various compile-to-JavaScript technologies including TypeScript, CoffeeScript and Dart and explains his view of why he feels they don\'t quite hit the mark.\\n\\nAs a user (and big fan) of TypeScript I read the post with interest and picked up on one particular issue that Jeff mentions:\\n\\n> Classes make the unchanged behaviour of the `this` keyword more confusing. For example, in a class like `Greeter` from the [TypeScript playground](http://www.typescriptlang.org/Playground), the use of `this` is confusing:\\n>\\n> ```ts\\n> class Greeter {\\n>   greeting: string;\\n>   constructor(message: string) {\\n>     this.greeting = message;\\n>   }\\n>   greet() {\\n>     return \'Hello, \' + this.greeting;\\n>   }\\n> }\\n> ```\\n>\\n> One can\u2019t help but feel the `this` keyword in the methods of `Greeter` should always reference a `Greeter` instance. However, the semantics of this are unchanged from JavaScript:\\n>\\n> ```js\\n> var greeter = new Greeter(\'world\');\\n> var unbound = greeter.greet;\\n> alert(unbound());\\n> ```\\n>\\n> The above code displays \u201cHello, undefined\u201d instead of the naively expected \u201cHello, world\u201d.\\n\\nNow Jeff is quite correct in everything he says above. However, he\'s also missing a trick. Or rather, he\'s missing out on a very useful feature of TypeScript.\\n\\n## Instance Methods to the Rescue!\\n\\nStill in the early days of TypeScript, the issue Jeff raises had already been identified. (And for what it\'s worth, this issue wasn\'t there by mistake - remember TypeScript is quite deliberately a \\"superset of JavaScript\\".) Happily with the [release of TypeScript 0.9.1](https://blogs.msdn.com/b/typescript/archive/2013/08/06/announcing-0-9-1.aspx) a nice remedy was included in the language in the form of \\"Instance Methods\\".\\n\\nInstance Methods are lexically scoped; bound to a specific instance of a JavaScript object. i.e. These methods are \\\\***not**\\\\* vulnerable to the \u201cHello, undefined\u201d issue Jeff raises. To quote the blog post:\\n\\n> We\'ve relaxed the restrictions on field initializers to now allow `\'this\'`. This means that classes can now contain both methods on the prototype, and **callback functions on the instance**. The latter are particularly useful when you want to use a member on the class as a callback function, as in the code above. This lets you mix-n-match between \u2018closure\u2019 style and \u2018prototype\u2019 style class member patterns easily.\\n\\n## `Greeter` with Instance Methods\\n\\nSo, if we take the `Greeter` example, how do we apply Instance Methods to it? Well, like this:\\n\\n```ts\\nclass Greeter {\\n  greeting: string;\\n  constructor(message: string) {\\n    this.greeting = message;\\n  }\\n  greet = () => {\\n    return \'Hello, \' + this.greeting;\\n  };\\n}\\n```\\n\\nCan you tell the difference? It\'s subtle. That\'s right; the mere swapping out of `()` with `= () =&gt;` on the `greet` method takes us from a `prototype` method to an Instance Method.\\n\\nObservant readers will have noticed that we are using TypeScript / [ES6\'s Arrow Function syntax](https://developer.mozilla.org/en/docs/Web/JavaScript/Reference/arrow_functions). In fact with that in mind I could actually have gone super-terse if I was so inclined:\\n\\n```ts\\nclass Greeter {\\n  greeting: string;\\n  constructor(message: string) {\\n    this.greeting = message;\\n  }\\n  greet = () => \'Hello, \' + this.greeting;\\n}\\n```\\n\\nBut either way, both of the above class declarations compile down to the following JavaScript:\\n\\n```js\\nvar Greeter = (function () {\\n  function Greeter(message) {\\n    var _this = this;\\n    this.greet = function () {\\n      return \'Hello, \' + _this.greeting;\\n    };\\n    this.greeting = message;\\n  }\\n  return Greeter;\\n})();\\n```\\n\\nWhich differs from the pre-Instance Methods generated JavaScript:\\n\\n```js\\nvar Greeter = (function () {\\n  function Greeter(message) {\\n    this.greeting = message;\\n  }\\n  Greeter.prototype.greet = function () {\\n    return \'Hello, \' + this.greeting;\\n  };\\n  return Greeter;\\n})();\\n```\\n\\nAs you can see the Instance Methods approach does \\\\***not**\\\\* make use of the `prototype` on `Greeter` to add the method. (As the pre-Instance Methods `greet()` declaration did.) Instead it creates a function directly on the created object and internally uses the `_this` variable inside the Instance Methods. (`_this` being a previously captured instance of `this`.)\\n\\nSo with Instance Methods we can repeat Jeff\'s experiment from earlier:\\n\\n```js\\nvar greeter = new Greeter(\'world\');\\nvar bound = greeter.greet;\\nalert(bound());\\n```\\n\\nBut this time round the code displays \u201cHello, world\u201d and no longer \u201cHello, undefined\u201d.\\n\\n## Update 02/04/2014 - mixing and matching `prototype` and Instance Methods\\n\\n[Bart Verkoeijen](https://twitter.com/bgever) made an excellent comment concerning the extra memory that Instance Methods require as opposed to `prototype` methods. Not everyone reads the comments and so I thought I\'d add a little suffix to my post.\\n\\nWhat I\u2019ve come to realise is that it comes down to problem that you\u2019re trying to solve. Instance methods are bulletproof in terms of relying on a specific instance of `this` regardless of how a method is invoked. But for many of my use cases that\u2019s overkill. Let\u2019s take the original (`prototype` methods) `Greeter` example:\\n\\n```js\\nvar Greeter = (function () {\\n  function Greeter(message) {\\n    this.greeting = message;\\n  }\\n  Greeter.prototype.greet = function () {\\n    return \'Hello, \' + this.greeting;\\n  };\\n  return Greeter;\\n})();\\n\\nvar greeter = new Greeter(\'world\');\\nvar greeter2 = new Greeter(\'universe\');\\n\\nconsole.log(greeter.greet()); // Logs \\"Hello, world\\"\\nconsole.log(greeter2.greet()); // Logs \\"Hello, universe\\"\\n```\\n\\nAs you can see above, provided I invoke my `greet` method in the context of my created object then I can rely on `this` being what I would hope.\\n\\nThat being the case my general practice has not been to use exclusively Instance methods \\\\***or**\\\\* `prototype` methods. What I tend to do is start out only with `prototype` methods on my classes and switch them over to be an Instance method if there is an actual need to ensure context. So my TypeScript classes tend to be a combination of `prototype` methods and Instance methods.\\n\\nMore often than not the `prototype` methods are just fine. It tends to be where an object is interacting with some kind of presentation framework (Knockout / Angular etc) or being invoked as part of a callback (eg AJAX scenarios) where I need Instance methods."},{"id":"/2014/03/17/the-surprisingly-happy-tale-of-visual","metadata":{"permalink":"/2014/03/17/the-surprisingly-happy-tale-of-visual","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2014-03-17-the-surprisingly-happy-tale-of-visual/index.md","source":"@site/blog/2014-03-17-the-surprisingly-happy-tale-of-visual/index.md","title":"The Surprisingly Happy Tale of Visual Studio Online, Continous Integration and Chutzpah","description":"Going off piste","date":"2014-03-17T00:00:00.000Z","formattedDate":"March 17, 2014","tags":[{"label":"Jasmine","permalink":"/tags/jasmine"},{"label":"TFS","permalink":"/tags/tfs"},{"label":"unit testing","permalink":"/tags/unit-testing"},{"label":"javascript","permalink":"/tags/javascript"},{"label":"Continuous Integration","permalink":"/tags/continuous-integration"}],"readingTime":5.865,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"The Surprisingly Happy Tale of Visual Studio Online, Continous Integration and Chutzpah","authors":"johnnyreilly","tags":["Jasmine","TFS","unit testing","javascript","Continuous Integration"],"hide_table_of_contents":false},"prevItem":{"title":"TypeScript this is what I want! (the unfortunate neglect of Instance Methods / callback functions)","permalink":"/2014/04/01/typescript-instance-methods"},"nextItem":{"title":"Knockout + Globalize = valueNumber Binding Handler","permalink":"/2014/03/11/knockout-globalize-valuenumber-binding"}},"content":"## Going off piste\\n\\nThe post that follows is a slightly rambly affair which is pretty much my journal of the first steps of getting up and running with JavaScript unit testing. I will not claim that much of this blog is down to me. In fact in large part is me working my way through [Mathew Aniyan\'s excellent blog post on integrating Chutzpah with TFS](https://blogs.msdn.com/b/visualstudioalm/archive/2012/07/09/javascript-unit-tests-on-team-foundation-service-with-chutzpah.aspx). But a few deviations from this post have made me think it worth keeping hold of this record for my benefit (if no-one else\'s).\\n\\nThat\'s the disclaimers out of the way now...\\n\\n## ...Try, try, try again...\\n\\nGetting started with JavaScript unit testing has not been the breeze I\u2019d expected. Frankly I\u2019ve found the docs out there not particularly helpful. But if at first you don\'t succeed then try, try, try again.\\n\\nSo after a number of failed attempts I\u2019m going to give it another go. [Rushaine McBean](http://www.hanselminutes.com/412/getting-started-with-javascript-unit-testing-with-jasmine-and-rushaine-mcbean) says Jasmine is easiest so I\'m going to follow her lead and give it a go.\\n\\nLet\u2019s new up a new (empty) ASP.NET project. Yes, I know ASP.NET has nothing to do with JavaScript unit testing but my end goal is to be able to run JS unit tests in Visual Studio and as part of Continuous Integration. Further to that, I\'m anticipating a future where I have a solution that contains JavaScript unit tests and C# unit tests as well. It is indeed a bold and visionary Brave New World. We\'ll see how far we get.\\n\\nFirst up, download Jasmine from [GitHub](http://jasmine.github.io/) \\\\- I\'ll use [v2.0](https://github.com/pivotal/jasmine/blob/master/dist/jasmine-standalone-2.0.0.zip). Unzip it and fire up SpecRunner.html and whaddya know... It works!\\n\\nAs well it might. I\u2019d be worried if it didn\u2019t. So I\u2019ll move the contents of the release package into my empty project. Now let\u2019s see if we can get those tests running inside Visual Studio. I\u2019d heard of [Chutzpah](https://chutzpah.codeplex.com/) which describes itself thusly:\\n\\n> _\u201cChutzpah is an open source JavaScript test runner which enables you to run unit tests using QUnit, Jasmine, Mocha, CoffeeScript and TypeScript.\u201d _\\n\\nWhat I\u2019m after is the Chutzpah test adapter for Visual Studio 2012/2013 which can be found [here](http://visualstudiogallery.msdn.microsoft.com/f8741f04-bae4-4900-81c7-7c9bfb9ed1fe). I download the VSIX and install. Pretty painless. Once I restart Visual Studio I can see my unit tests in the test explorer. Nice! Run them and...\\n\\nAll fail. This makes me sad. All the errors say \u201cCan\u2019t find variable: Player in file\u201d. Hmmm. Why? Dammit I\u2019m actually going to have to read the [documentation](https://chutzpah.codeplex.com/wikipage?title=Chutzpah%20File%20References&referringTitle=Documentation)... It turns out the issue can be happily resolved by adding these 3 references to the top of PlayerSpec.js:\\n\\n```js\\n/// <reference path=\\"../src/Player.js\\" />\\n/// <reference path=\\"../src/Song.js\\" />\\n/// <reference path=\\"SpecHelper.js\\" />\\n```\\n\\nNow the tests pass.\\n\\nThe question is: can we get this working with Visual Studio Online?\\n\\nFortunately another has gone before me. Mathew Aniyan has written a [superb blog post called \\"Javascript Unit Tests on Team Foundation Service with Chutzpah\\"](https://blogs.msdn.com/b/visualstudioalm/archive/2012/07/09/javascript-unit-tests-on-team-foundation-service-with-chutzpah.aspx). Using this post as a guide (it was written 18 months ago which is frankly aeons in the world of the web) I\'m hoping that I\'ll be able to, without too many tweaks, get Javascript unit tests running on Team Foundation Service / Visual Studio Online ( / insert this weeks rebranding here).\\n\\nFirst of all in Visual Studio Online I\u2019ll create a new project called \\"GettingStartedWithJavaScriptUnitTesting\\" (using all the default options). Apparently _\u201cYour project is created and your team is going to absolutely love this.\u201d_ Hmmmm... I think I\u2019ll be judge of that.\\n\\nLet\'s navigate to the project. I\'ll fire up Visual Studio by clicking on the \u201cOpen in Visual Studio\u201d link. Once fired up and all the workspace mapping is sorted I\u2019ll move my project into the GettingStartedWithJavaScriptUnitTesting folder that now exists on my machine and add this to source control.\\n\\nBack to Mathew\'s blog. It suggests renaming Chutzpah.VS2012.vsix to Chutzpah.VS2012.zip and checking certain files into TFS. I think Chutzpah has changed a certain amount since this was written. To be on the safe side I\u2019ll create a new folder in the root of my project called Chutzpah.VS2012 and put the contents of Chutzpah.VS2012.zip in there and add it to TFS (being careful to ensure that no dll\u2019s are excluded).\\n\\nThen I\'ll follow steps 3 and 4 from the blog post:\\n\\n> \\\\*3\\\\. In Visual Studio, Open Team Explorer & connect to Team Foundation Service. Bring up the Manage Build Controllers dialog. [Build \u2013> Manage Build Controllers] Select Hosted Build Controller Click on Properties button to bring up the Build Controller Properties dialog.\\n>\\n> 4\\\\. Change Version Control Path to custom Assemblies to refer to the folder where you checked in the binaries in step 2.\\n>\\n> -\\n\\nIn step 5 the blog tells me to edit my build definition. Well I don\u2019t have one in this new project so let\u2019s click on \u201cNew Build Definition\u201d, create one and then follow step 5:\\n\\n> \\\\*5\\\\. In Team Explorer, go to the Builds section and Edit your Build Definition which will run the javascript tests. Click on the Process tab Select the row named Automated Tests. Click on \u2026 button next to the value.\\n>\\n> -\\n\\nRather than following step 6 (which essentially nukes the running of any .NET tests you might have) I chose to add another row by clicking \\"Add\\". In the dialog presented I changed the Test assembly specification to \\\\*\\\\*\\\\\\\\\\\\*.js and checked the \\"Fail build on test failure\\" checkbox.\\n\\nStep 7 says:\\n\\n> \\\\*7\\\\. Create your Web application in Visual Studio and add your Qunit or Jasmine unit tests to them. <u>Make sure that the js files (that contain the tests) are getting copied to the build output directory.</u>\\n>\\n> -\\n\\nThe picture below step 7 suggests that you should be setting your test / spec files to have a `Copy to Output Directory` setting of `Copy always`. **This did not work for me!!!** Instead, setting a `Build Action` of `Content` and a `Copy to Output Directory` setting of `Do not copy` did work.\\n\\nFinally I checked everything into source control and queued a build. I honestly did not expect this to work. It couldn\u2019t be this easy could it? And...\\n\\nWow! It did! Here\u2019s me cynically expecting some kind of \u201cpermission denied\u201d error and it actually works! Brilliant! Look up in the cloud it says the same thing!\\n\\nFantastic!\\n\\nI realise that I haven\u2019t yet written a single JavaScript unit test of my own and so I\u2019ve still a way to go. What I have done is quietened those voices in my head that said \u201cthere\u2019s not too much point having a unit test suite that isn\u2019t plugged into continuous integration\u201d. Although it\'s not documented here I\'m happy to be able to report that I have been able to follow the self-same instructions for Team Foundation Service / Visual Studio Online and get CI working with TFS 2012 on our build server as well.\\n\\nHaving got up and running off the back of other peoples hard work I best try and write some of my own tests now...."},{"id":"/2014/03/11/knockout-globalize-valuenumber-binding","metadata":{"permalink":"/2014/03/11/knockout-globalize-valuenumber-binding","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2014-03-11-knockout-globalize-valuenumber-binding/index.md","source":"@site/blog/2014-03-11-knockout-globalize-valuenumber-binding/index.md","title":"Knockout + Globalize = valueNumber Binding Handler","description":"I\u2019ve long used Globalize for my JavaScript number formatting / parsing needs. In a current project I\u2019m using Knockout for the UI. When it came to data-binding numeric values none of the default binding handlers seemed appropriate. What I wanted was a binding handler that:","date":"2014-03-11T00:00:00.000Z","formattedDate":"March 11, 2014","tags":[{"label":"Globalize","permalink":"/tags/globalize"},{"label":"Knockout","permalink":"/tags/knockout"},{"label":"bindingHandler","permalink":"/tags/binding-handler"}],"readingTime":3.88,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Knockout + Globalize = valueNumber Binding Handler","authors":"johnnyreilly","tags":["Globalize","Knockout","bindingHandler"],"hide_table_of_contents":false},"prevItem":{"title":"The Surprisingly Happy Tale of Visual Studio Online, Continous Integration and Chutzpah","permalink":"/2014/03/17/the-surprisingly-happy-tale-of-visual"},"nextItem":{"title":"Caching and cache-busting with RequireJS","permalink":"/2014/03/05/caching-and-cache-busting-with-requirejs"}},"content":"I\u2019ve long used [Globalize](https://github.com/jquery/globalize/) for my JavaScript number formatting / parsing needs. In a current project I\u2019m using Knockout for the UI. When it came to data-binding numeric values none of the default binding handlers seemed appropriate. What I wanted was a binding handler that:\\n\\n1. Was specifically purposed for dealing with numeric values\\n2. Handled the parsing / formatting for the current locale (and I naturally intended to use Globalize for this purpose)\\n\\nLike so much development we start by standing on the shoulders of giants. In this case it\u2019s the fantastic [Ryan Niemeyer](https://twitter.com/RPNiemeyer) who put up a [post on StackOverflow](http://stackoverflow.com/a/12647270/761388) that got me on the right track.\\n\\nEssentially his approach provides an \u201cinterceptor\u201d mechanism that allows you to validate numeric data entry on input and format numeric data going out as well. Very nice. Into this I plugged Globalize to handle the parsing and formatting. I ended up with the \u201cvalueNumber\u201d binding handler:\\n\\n```js\\nko.bindingHandlers.valueNumber = {\\n  init: function (\\n    element,\\n    valueAccessor,\\n    allBindingsAccessor,\\n    viewModel,\\n    bindingContext\\n  ) {\\n    /**\\n     * Adapted from the KO hasfocus handleElementFocusChange function\\n     */\\n    function elementIsFocused() {\\n      var isFocused = false,\\n        ownerDoc = element.ownerDocument;\\n      if (\'activeElement\' in ownerDoc) {\\n        var active;\\n        try {\\n          active = ownerDoc.activeElement;\\n        } catch (e) {\\n          // IE9 throws if you access activeElement during page load\\n          active = ownerDoc.body;\\n        }\\n        isFocused = active === element;\\n      }\\n\\n      return isFocused;\\n    }\\n\\n    /**\\n     * Adapted from the KO hasfocus handleElementFocusChange function\\n     *\\n     * @param {boolean} isFocused whether the current element has focus\\n     */\\n    function handleElementFocusChange(isFocused) {\\n      elementHasFocus(isFocused);\\n    }\\n\\n    var observable = valueAccessor(),\\n      properties = allBindingsAccessor(),\\n      elementHasFocus = ko.observable(elementIsFocused()),\\n      handleElementFocusIn = handleElementFocusChange.bind(null, true),\\n      handleElementFocusOut = handleElementFocusChange.bind(null, false);\\n\\n    var interceptor = ko.computed({\\n      read: function () {\\n        var currentValue = ko.utils.unwrapObservable(observable);\\n        if (elementHasFocus()) {\\n          return !isNaN(currentValue) &&\\n            currentValue !== null &&\\n            currentValue !== undefined\\n            ? currentValue\\n                .toString()\\n                .replace(\'.\', Globalize.findClosestCulture().numberFormat[\'.\']) // Displays correct decimal separator for the current culture (so de-DE would format 1.234 as \\"1,234\\")\\n            : null;\\n        } else {\\n          var format = properties.numberFormat || \'n2\',\\n            formattedNumber = Globalize.format(currentValue, format);\\n\\n          return formattedNumber;\\n        }\\n      },\\n      write: function (newValue) {\\n        var currentValue = ko.utils.unwrapObservable(observable),\\n          numberValue = Globalize.parseFloat(newValue);\\n\\n        if (!isNaN(numberValue)) {\\n          if (numberValue !== currentValue) {\\n            // The value has changed so update the observable\\n            observable(numberValue);\\n          }\\n        } else if (newValue.length === 0) {\\n          if (properties.isNullable) {\\n            // If newValue is a blank string and the isNullable property has been set then nullify the observable\\n            observable(null);\\n          } else {\\n            // If newValue is a blank string and the isNullable property has not been set then set the observable to 0\\n            observable(0);\\n          }\\n        }\\n      },\\n    });\\n\\n    ko.utils.registerEventHandler(element, \'focus\', handleElementFocusIn);\\n    ko.utils.registerEventHandler(element, \'focusin\', handleElementFocusIn); // For IE\\n    ko.utils.registerEventHandler(element, \'blur\', handleElementFocusOut);\\n    ko.utils.registerEventHandler(element, \'focusout\', handleElementFocusOut); // For IE\\n\\n    if (element.tagName.toLowerCase() === \'input\') {\\n      ko.applyBindingsToNode(element, { value: interceptor });\\n    } else {\\n      ko.applyBindingsToNode(element, { text: interceptor });\\n    }\\n  },\\n};\\n```\\n\\nUsing this binding handler you just need to drop in a `valueNumber` into your `data-bind` statement where you might previously have used a `value` binding. The binding also has a couple of nice hooks in place which you might find useful:\\n\\n<dl><dt>numberFormat (defaults to \\"n2\\")</dt><dd>allows you to specify a format to display your number with. Eg, \\"c2\\" would display your number as a currency to 2 decimal places, \\"p1\\" would display your number as a percentage to 1 decimal place etc</dd><dt>isNullable (defaults to false)</dt><dd>specifies whether your number should be treated as nullable. If it\'s not then clearing the elements value will set the underlying observable to 0.</dd></dl>\\n\\nFinally when the element gains focus / becomes active the full underlying value is displayed. (Kind of like Excel - like many an app, the one I\'m working on started life as Excel and the users want to keep some of the nice aspects of Excel\'s UI.) To take a scenario, let\'s imagine we have an input element which is applying the \\"n1\\" format. The underlying value backing this is 1.234. The valueNumber binding displays this as \\"1.2\\" when the input does not have focus and when the element gains focus the full \\"1.234\\" is displayed. Credit where it\u2019s due, this is thanks to [Robert Westerlund](http://stackoverflow.com/users/1105996/robert-westerlund) who was kind enough to respond to a [question of mine on StackOverflow](http://stackoverflow.com/a/22313546/761388).\\n\\nFinally, here\u2019s a demo using the \\"de-DE\\" locale:\\n\\n<iframe width=\\"100%\\" height=\\"400\\" src=\\"https://jsfiddle.net/johnny_reilly/jRt3k/embedded/result,js,html\\" allowFullScreen=\\"allowFullScreen\\" frameBorder=\\"0\\"></iframe>\\n\\n## PS Globalize is a-changing\\n\\nThe version of Globalize used in the binding handler is Globalize v0.1.1. This has been available in various forms for quite some time but as I write this the Globalize plugin is in the process of being ported to the [CLDR](http://cldr.unicode.org/). As part of that work it looks like the Globalize API will change. When that gets finalized I\u2019ll try and come back and update this."},{"id":"/2014/03/05/caching-and-cache-busting-with-requirejs","metadata":{"permalink":"/2014/03/05/caching-and-cache-busting-with-requirejs","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2014-03-05-caching-and-cache-busting-with-requirejs/index.md","source":"@site/blog/2014-03-05-caching-and-cache-busting-with-requirejs/index.md","title":"Caching and cache-busting with RequireJS","description":"Having put together a demo of using TypeScript with RequireJS my attention turned quickly to caching. Or rather, IE forced me to think about caching.","date":"2014-03-05T00:00:00.000Z","formattedDate":"March 5, 2014","tags":[{"label":"asp.net","permalink":"/tags/asp-net"},{"label":"RequireJS","permalink":"/tags/require-js"},{"label":"cache","permalink":"/tags/cache"},{"label":"caching","permalink":"/tags/caching"}],"readingTime":8.92,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Caching and cache-busting with RequireJS","authors":"johnnyreilly","tags":["asp.net","RequireJS","cache","caching"],"hide_table_of_contents":false},"prevItem":{"title":"Knockout + Globalize = valueNumber Binding Handler","permalink":"/2014/03/11/knockout-globalize-valuenumber-binding"},"nextItem":{"title":"TypeScript and RequireJS (Keep It Simple)","permalink":"/2014/02/27/typescript-and-requirejs-keep-it-simple"}},"content":"Having put together a demo of using TypeScript with RequireJS my attention turned quickly to caching. Or rather, IE forced me to think about caching.\\n\\nEveryone has their own workflow, their own tools. The things they like to use as they put things together. And for me I\u2019m a Visual Studio man \u2013 it\u2019s not everyone\u2019s bag but I really like it. I find the JavaScript tooling is now really solid combined with IE and it (generally) makes me more productive. I want to use it. But, as you know, nothing is perfect...\\n\\nSo there I was, delighted with the TypeScript / RequireJS demo. It was working just lovely. I started investigating the debugging story. What would happen if I change a script file on the fly? When I refresh IE does it pick up the tweaks?\\n\\nLet\u2019s find out. I\'ll open up alerter.ts and change this:\\n\\n```ts\\nvar name = \'John\';\\n```\\n\\nto this:\\n\\n```js\\nvar name = \'Bobby\';\\n```\\n\\nAnd \\\\***boom**\\\\*! Nothing. I\u2019ve refreshed IE and I\u2019m expecting to see \u201cWelcome to Code Camp, Bobby\u201d. But I\u2019m still reading \u201cWelcome to Code Camp, John\u201d... I bet Chrome wouldn\u2019t do this to me... And I\u2019m right! It doesn\u2019t. I don\u2019t want to get too much into the details of this but it looks like it comes down to Chrome sending an \\"If-Modified-Since\\" request header where IE does not. I\u2019m pretty sure that IE could be configured to behave likewise but I\u2019d rather not have to remember that. (And furthermore I don\u2019t want to have to remind every person that works on the app to do that as well.)\\n\\nThis raises a number of issues but essentially it gets me to think about the sort of caching I want. Like most of you I have 2 significant use cases:\\n\\n1. Development\\n2. Production\\n\\nFor Development I want any changes to JavaScript files to be picked up \u2013 I do \\\\***not**\\\\* want caching. For Production I want caching in order that users have better performance / faster loading. If I ship a new version of the app to Production I also want users to pick up the new versions of a file and cache those.\\n\\n## Research\\n\\nI did a little digging. The most useful information I found was [a StackOverflow post on RequireJS and caching](http://stackoverflow.com/q/8315088/761388). Actually I\u2019d recommend anyone reading this to head over and read that from top to bottom. Read the question and all of the answers as well \u2013 pretty much everything will add to your understanding of RequireJS.\\n\\nAs with any set of answers there are different and conflicting views. [Phil McCull\u2019s (accepted) answer](http://stackoverflow.com/a/8479953/761388) was for my money the most useful. It pointed [back to the RequireJS documentation](http://requirejs.org/docs/api.html#config-urlArgs).\\n\\n> \\\\*\\"urlArgs: Extra query string arguments appended to URLs that RequireJS uses to fetch resources. Most useful to cache bust when the browser or server is not configured correctly. Example cache bust setting for urlArgs:\\n>\\n> ```js\\n> urlArgs: \'bust=\' + new Date().getTime();\\n> ```\\n>\\n> During development it can be useful to use this, however be sure to remove it before deploying your code.\\"\\n>\\n> -\\n\\nPhil\u2019s answer suggests using urlArgs \\\\***both**\\\\* for Production and for Development in 2 different ways. Using what amounts to a random number in the Development environment (as in the official docs) for cache-busting. For the Production environment he suggests using a specific version number which allows for client-side caching between different build versions.\\n\\nFull disclosure, this is not the approach favoured by James Burke (author of RequireJS). He doesn\u2019t go into why in the RequireJS docs but has [elsewhere expounded on this](https://groups.google.com/forum/#!msg/requirejs/3E9dP_BSQoY/36ut2Gtko7cJ):\\n\\n> _For deployed assets, I prefer to put the version or hash for the whole build as a build directory, then just modify the baseUrl config used for the project to use that versioned directory as the baseUrl. Then no other files change, and it helps avoid some proxy issues where they may not cache an URL with a query string on it. _\\n\\nI\u2019m not so worried about the proxy caching issue. My users tend to be people who use the application repeatedly and so the caching I most care about is their local machine caching. From what I understand urlArgs will work fine in this scenario. Yes the downside of this approach is that some proxy servers may not cache these assets. That\u2019s a shame but it\u2019s not a dealbreaker for me. As I said, I still have client side caching.\\n\\nIf you want to go a little deeper I recommend reading [Steve Souders post](http://www.stevesouders.com/blog/2008/08/23/revving-filenames-dont-use-querystring/) on the topic (in case you\u2019re not aware Steve is Google\u2019s Mr Performance). Interestingly, looking at the comments on the post it sounds like the lack of support for proxy caching with querystrings may that may be starting to change.\\n\\nBut either way, I\u2019m happy with this approach. As I always say, if it\u2019s good enough for Stack Overflow then it\u2019s good enough for me.\\n\\n## Implementation\\n\\nI\u2019m going to start off using the demo from [my last blog post](http://icanmakethiswork.blogspot.com/2014/02/typescript-and-requirejs-keep-it-simple.html) as a basis. Let\u2019s take that and evolve it. As a result my solution is going to work with TypeScript and RequireJS (since the previous demo was about that) but the implementation I\u2019m going to come up with would work as well with vanilla JS as it would with TypeScript compiled JS.\\n\\nLet\u2019s take a look at our index.html. First we\u2019ll drop our usage of `main.ts` / `main.js` (our bootstrapper file that defines config and kicks off the \\"app\\"). We\u2019ll pull out the use of `data-main` and instead, just after the reference to require we\u2019ll add the contents of `main.js` much in [the style of the RequireJS docs](http://requirejs.org/docs/api.html#config). We\u2019ll also include a urlArgs that as a cache-buster that uses the approach outlined [in the RequireJS docs](http://requirejs.org/docs/api.html#config-urlArgs):\\n\\n```html\\n<script src=\\"/scripts/require.js\\"><\/script>\\n<script>\\n  require.config({\\n    baseUrl: \'/scripts\',\\n    paths: {\\n      jquery: \'jquery-2.1.0\',\\n    },\\n    urlArgs: \'v=\' + new Date().getTime(),\\n  });\\n\\n  require([\'alerter\'], function (alerter) {\\n    alerter.showMessage();\\n  });\\n<\/script>\\n```\\n\\nSpinning up the site all runs as you would expect. The question is: does this work as a cache-buster? Let\u2019s tweak `alerter.ts` / `alerter.js`.\\n\\nOh yeah! We\u2019re cache-busting like gangbusters!\\n\\nSo now let\u2019s comment out our existing urlArgs (which represents the Development solution from Phil\u2019s answer) and replace it with a fixed value like this:\\n\\n```js\\n//urlArgs: \\"v=\\" +  (new Date()).getTime()\\nurlArgs: \'v=1\';\\n```\\n\\nThis represents the Production solution from Phil\u2019s answer. Now let\u2019s run, refresh a couple of times and ensure that our fixed querystring value results in a 304 status code (indicating \u201cNot Modified\u201d and cached item used).\\n\\nIt does! Now let\u2019s increment the value:\\n\\n```js\\nurlArgs: \'v=2\';\\n```\\n\\nWhen we refresh the browser this should result in 200 status codes (indicating the cached version has not been used and the client has picked up a new version from the server).\\n\\nSuccess! That\u2019s our premise tested \u2013 both Development and Production scenarios. Now we want to turn this into a slightly more sophisticated reusable solution like this:\\n\\n```html\\n<script src=\\"/scripts/require.js\\"><\/script>\\n<script>\\n  var inDevelopment = true,\\n    version = \'1\';\\n\\n  require.config({\\n    baseUrl: \'/scripts\',\\n    paths: {\\n      jquery: \'jquery-2.1.0\',\\n    },\\n    urlArgs: \'v=\' + (inDevelopment ? new Date().getTime() : version),\\n  });\\n\\n  require([\'alerter\'], function (alerter) {\\n    alerter.showMessage();\\n  });\\n<\/script>\\n```\\n\\nIn the tweaked script above 2 variables are defined. The first is `inDevelopment` which models whether you are in the Development scenario (true) or the Production scenario (false). The second is `version` which represents the application version number. With this in place I can simply flip between the Development and Production scenario by changing the value of `inDevelopment`. And when a new version ships I can change the version number to force a cache refresh on the users.\\n\\nWhat drives the values of `inDevelopment` / `version` is down to you. You could load the `inDevelopment` / `version` values from some application endpoint. You could hardcode them in your screen. The choices are yours. I\u2019m going to finish off with a simple approach that I\'ve found useful.\\n\\n## Let\u2019s get the server involved!\\n\\nI want the server to drive my urlArgs value. Why? Well this project happens to be an ASP.NET project which handily has the concept of Development / Production scenarios nicely modelled by the [web.config\u2019s compilation debug flag](<http://msdn.microsoft.com/en-us/library/s10awwz0(v=vs.85).aspx>).\\n\\n```xml\\n<configuration>\\n  <system.web>\\n    <compilation debug=\\"true\\" targetFramework=\\"4.5\\" />\\n    <httpRuntime targetFramework=\\"4.5\\" />\\n  </system.web>\\n</configuration>\\n```\\n\\nIf debug is `true` then that reflects the Development scenario. If debug is `false` then that reflects the Production scenario.\\n\\nSo bearing that in mind I want to use the value of debug to drive my `urlArgs`. If I have my debug flag set to `true` I want to cache-bust all the way. Likewise, if debug is set to `false` then I want to serve up the version number so that caching is used until the version number changes. Time to break out the C#:\\n\\n```cs\\nnamespace RequireJSandCaching\\n{\\n    public static class RequireJSHelpers\\n    {\\n        private static readonly bool _inDebug;\\n        private static readonly string _version;\\n\\n        static RequireJSHelpers()\\n        {\\n            _inDebug = System.Web.HttpContext.Current.IsDebuggingEnabled;\\n            _version = (_inDebug)\\n                ? \\"InDebug\\"\\n                : System.Reflection.Assembly.GetExecutingAssembly().GetName().Version.ToString();\\n        }\\n\\n        public static string Version\\n        {\\n            get\\n            {\\n                return (_inDebug)\\n                    ? System.DateTime.Now.Ticks.ToString()\\n                    : _version;\\n            }\\n        }\\n    }\\n}\\n```\\n\\nThis is a static helper class called `RequireJSHelpers`. It has a static constructor which initialises 2 fields. `_inDebug` is taken from `System.Web.HttpContext.Current.IsDebuggingEnabled` which exposes the compilation debug value. `_version` is initialised, when debug is `false`, to the version number of the dll (driven by this `AssemblyInfo.cs [assembly: AssemblyVersion(\\"1.0.*\\")]` attribute)\\n\\nThere\u2019s 1 property on this helper class called version. Depending on whether the app is in debug mode or not this attribute either exposes the application version or effectively the C# equivalent to JavaScript\u2019s `(new Date()).getTime()`. (Well strictly speaking they have a different starting point in history but that\u2019s by-the-by... Both are of equal value as cache-busters.)\\n\\nYou probably see where this is all going.\\n\\nLet\u2019s clone our `index.html` page and call it `serverUrlArgs.cshtml` (note the suffix). Let\u2019s replace the script section with this:\\n\\n```html\\n<script>\\n  require.config({\\n    baseUrl: \'/scripts\',\\n    paths: {\\n      jquery: \'jquery-2.1.0\',\\n    },\\n    urlArgs: \'v=@RequireJSandCaching.RequireJSHelpers.Version\',\\n  });\\n\\n  require([\'alerter\'], function (alerter) {\\n    alerter.showMessage();\\n  });\\n<\/script>\\n```\\n\\nWhich drives `urlArgs` from the `RequireJSHelpers.Version` property. If we fire it up now (with debug set to true in our web.config) then we see requests like this:\\n\\nAnd if we set debug to false in our web.config then (after the initial requests have been cached) we see requests like this:\\n\\nThis leaves us with a simple mechanism to drive our RequireJS caching. If debug is set to `true` in our `web.config` then Require will perform cache-busting. If debug is set to `false` then RequireJS will perform only version-changing cache-busting and will, whilst the version remains constant, support client-side caching.\\n\\nFinished. In case it helps I\u2019ve put the code for this [up on GitHub](https://github.com/johnnyreilly/RequireJSandCaching)."},{"id":"/2014/02/27/typescript-and-requirejs-keep-it-simple","metadata":{"permalink":"/2014/02/27/typescript-and-requirejs-keep-it-simple","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2014-02-27-typescript-and-requirejs-keep-it-simple/index.md","source":"@site/blog/2014-02-27-typescript-and-requirejs-keep-it-simple/index.md","title":"TypeScript and RequireJS (Keep It Simple)","description":"I\'m not the first to take a look at mixing TypeScript and RequireJS but I wanted to get it clear in my head. Also, I\'ve always felt the best way to learn is to do. So here we go. I\'m going to create a TypeScript and RequireJS demo based on John Papa\'s \\"Keep It Simple RequireJS Demo\\".","date":"2014-02-27T00:00:00.000Z","formattedDate":"February 27, 2014","tags":[{"label":"RequireJS","permalink":"/tags/require-js"},{"label":"AMD","permalink":"/tags/amd"},{"label":"TypeScript","permalink":"/tags/type-script"}],"readingTime":4.055,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"TypeScript and RequireJS (Keep It Simple)","authors":"johnnyreilly","tags":["RequireJS","AMD","TypeScript"],"hide_table_of_contents":false},"prevItem":{"title":"Caching and cache-busting with RequireJS","permalink":"/2014/03/05/caching-and-cache-busting-with-requirejs"},"nextItem":{"title":"WPF and Mystic Meg or Playing Futurologist","permalink":"/2014/02/12/wpf-and-mystic-meg-or-playing"}},"content":"I\'m not the first to take a look at mixing TypeScript and RequireJS but I wanted to get it clear in my head. Also, I\'ve always felt the best way to learn is to do. So here we go. I\'m going to create a TypeScript and RequireJS demo based on [John Papa\'s \\"Keep It Simple RequireJS Demo\\"](https://github.com/johnpapa/kis-requirejs-demo/).\\n\\nSo let\'s fire up Visual Studio 2013 and create a new ASP.NET Web Application called \u201cRequireJSandTypeScript\u201d (the empty project template is fine).\\n\\nAdd a new HTML file to the root called \u201cindex.html\u201d and base it on \u201cindex3.html\u201d from [John Papa\u2019s demo](https://github.com/johnpapa/kis-requirejs-demo/blob/master/ModularDemo/index3.html):\\n\\n```html\\n<!DOCTYPE html>\\n<html>\\n  <head>\\n    <title>TypeScript with RequireJS</title>\\n  </head>\\n  <body>\\n    <div>\\n      <h1>TypeScript with RequireJS loading jQuery in Visual Studio land</h1>\\n    </div>\\n\\n    \x3c!-- use jquery to load this message--\x3e\\n    <p id=\\"message\\"></p>\\n\\n    \x3c!-- Shortcut to load require and then load main--\x3e\\n    <script\\n      src=\\"/scripts/require.js\\"\\n      data-main=\\"/scripts/main\\"\\n      type=\\"text/javascript\\"\\n    ><\/script>\\n  </body>\\n</html>\\n```\\n\\nJohn\u2019s demo depends on jQuery and RequireJS (not too surprisingly) so let\u2019s fire up Nuget and get them:\\n\\n```ps\\nInstall-Package RequireJS\\nInstall-Package jQuery\\n```\\n\\nWhilst we\u2019re at it, let\u2019s get the Definitely Typed typings as well:\\n\\n```ps\\nInstall-Package jQuery.TypeScript.DefinitelyTyped\\n```\\n\\nTo my surprise this popped up a dialog.\\n\\nBy \\"Your project has been configured to support TypeScript.\\" it means that the csproj file has had the following entries added:\\n\\n```xml\\n<Project ToolsVersion=\\"12.0\\" DefaultTargets=\\"Build\\" xmlns=\\"http://schemas.microsoft.com/developer/msbuild/2003\\">\\n  <Import Project=\\"$(MSBuildExtensionsPath32)\\\\Microsoft\\\\VisualStudio\\\\v$(VisualStudioVersion)\\\\TypeScript\\\\Microsoft.TypeScript.Default.props\\" Condition=\\"Exists(\'$(MSBuildExtensionsPath32)\\\\Microsoft\\\\VisualStudio\\\\v$(VisualStudioVersion)\\\\TypeScript\\\\Microsoft.TypeScript.Default.props\')\\" />\\n  ...\\n  <PropertyGroup>\\n    ...\\n    <TypeScriptToolsVersion>0.9</TypeScriptToolsVersion>\\n  </PropertyGroup>\\n  ...\\n  <Import Project=\\"$(MSBuildExtensionsPath32)\\\\Microsoft\\\\VisualStudio\\\\v$(VisualStudioVersion)\\\\TypeScript\\\\Microsoft.TypeScript.targets\\" Condition=\\"Exists(\'$(MSBuildExtensionsPath32)\\\\Microsoft\\\\VisualStudio\\\\v$(VisualStudioVersion)\\\\TypeScript\\\\Microsoft.TypeScript.targets\')\\" />\\n  ...\\n</Project>\\n```\\n\\nI\u2019m not sure when this tweak to the Visual Studio tooling was added was added. Perhaps it\'s part of the [TypeScript 1.0 RC release](https://blogs.msdn.com/b/typescript/archive/2014/02/25/announcing-typescript-1-0rc.aspx); either way it\u2019s pretty nice. Let\'s press on.\\n\\nWhilst we\u2019re at it let\u2019s make sure that we\u2019re compiling to AMD (to be RequireJS friendly) by adding in the following csproj tweaks just before the Microsoft.TypeScript.targets Project import statement:\\n\\n```xml\\n<PropertyGroup Condition=\\"\'$(Configuration)\' == \'Debug\'\\">\\n    <TypeScriptModuleKind>amd</TypeScriptModuleKind>\\n  </PropertyGroup>\\n  <PropertyGroup Condition=\\"\'$(Configuration)\' == \'Release\'\\">\\n    <TypeScriptModuleKind>amd</TypeScriptModuleKind>\\n  </PropertyGroup>\\n```\\n\\nWhere was I? Oh yes, typings. So let\u2019s get the RequireJS typings too:\\n\\n```ps\\nInstall-Package requirejs.TypeScript.DefinitelyTyped\\n```\\n\\nRight \u2013 looking at index.html we can see from the data-main tag that the first file loaded by RequireJS, our bootstrapper if you will, is main.js. So let\u2019s add ourselves a main.ts based on [John\'s example](https://github.com/johnpapa/kis-requirejs-demo/blob/master/ModularDemo/Scripts3/main.js) (which will in turn generate a main.js):\\n\\n```ts\\n(function () {\\n  requirejs.config({\\n    baseUrl: \'scripts\',\\n    paths: {\\n      jquery: \'jquery-2.1.0\',\\n    },\\n  });\\n\\n  require([\'alerter\'], (alerter) => {\\n    alerter.showMessage();\\n  });\\n})();\\n```\\n\\nmain.ts depends upon [alerter](https://github.com/johnpapa/kis-requirejs-demo/blob/master/ModularDemo/Scripts3/alerter.js) so let\u2019s add ourselves an alerter.ts as well:\\n\\n```ts\\ndefine(\'alerter\', [\'jquery\', \'dataservice\'], function ($, dataservice) {\\n  var name = \'John\',\\n    showMessage = function () {\\n      var msg = dataservice.getMessage();\\n\\n      $(\'#message\').text(msg + \', \' + name);\\n    };\\n\\n  return {\\n    showMessage: showMessage,\\n  };\\n});\\n```\\n\\nAnd a [dataservice.ts](https://github.com/johnpapa/kis-requirejs-demo/blob/master/ModularDemo/Scripts3/dataservice.js):\\n\\n```ts\\ndefine(\'dataservice\', [], function () {\\n  var msg = \'Welcome to Code Camp\',\\n    getMessage = function () {\\n      return msg;\\n    };\\n\\n  return {\\n    getMessage: getMessage,\\n  };\\n});\\n```\\n\\nThat all compiles fine. But we\u2019re missing a trick. We\u2019re supposed to be using TypeScripts AMD support so let\u2019s change the code to do just that. First dataservice.ts:\\n\\n```ts\\nvar msg = \'Welcome to Code Camp\';\\n\\nexport function getMessage() {\\n  return msg;\\n}\\n```\\n\\nThen alerter.ts:\\n\\n```ts\\nimport $ = require(\'jquery\');\\nimport dataservice = require(\'dataservice\');\\n\\nvar name = \'John\';\\n\\nexport function showMessage() {\\n  var msg = dataservice.getMessage();\\n\\n  $(\'#message\').text(msg + \', \' + name);\\n}\\n```\\n\\nI know both of the above look slightly different but if you look close you\'ll see it\'s really only boilerplate changes. The actual application code is unaffected. Finally, main.ts remains as it is and that\'s us done; we have ourselves a working demo... Yay!\\n\\nThanks to John Papa for creating such a simple demo I could use as the basis for my own demo.\\n\\n## Closing Thoughts\\n\\nUnfortunately there is no typing on the alerter reference within main.ts. To my knowledge there is no way to implicitly import the typings here \u2013 the only thing you can do is specify them manually. (By the way, if I\'m wrong about this then please do set me straight!) That said, this is not so bad really since this main.ts file is essentially just a bootstrapper that kicks things off. All the other files contain the real application code and they have have typings a-go-go. So we\'re happy.\\n\\n## Finally for bonus points....\\n\\nI\u2019ve included the js and js.map files in the project file as they don\'t seem to be added into the project by Visual Studio when the TS file is created or when it is compiled for the first time. I\'ve also ensured that these files are dependent upon the typescript files they were generated from.\\n\\n```xml\\n<TypeScriptCompile Include=\\"Scripts\\\\alerter.ts\\" />\\n    <Content Include=\\"Scripts\\\\alerter.js\\">\\n        <DependentUpon>alerter.ts</DependentUpon>\\n    </Content>\\n    <Content Include=\\"Scripts\\\\alerter.js.map\\">\\n        <DependentUpon>alerter.ts</DependentUpon>\\n    </Content>\\n    <TypeScriptCompile Include=\\"Scripts\\\\dataservice.ts\\" />\\n    <Content Include=\\"Scripts\\\\dataservice.js\\">\\n        <DependentUpon>dataservice.ts</DependentUpon>\\n    </Content>\\n    <Content Include=\\"Scripts\\\\dataservice.js.map\\">\\n        <DependentUpon>dataservice.ts</DependentUpon>\\n    </Content>\\n    <TypeScriptCompile Include=\\"Scripts\\\\main.ts\\" />\\n    <Content Include=\\"Scripts\\\\main.js\\">\\n        <DependentUpon>main.ts</DependentUpon>\\n    </Content>\\n    <Content Include=\\"Scripts\\\\main.js.map\\">\\n        <DependentUpon>main.ts</DependentUpon>\\n    </Content>\\n```\\n\\n## Want the code for your very own?\\n\\nWell you can grab it from [GitHub](https://github.com/johnnyreilly/RequireJSandTypeScript)."},{"id":"/2014/02/12/wpf-and-mystic-meg-or-playing","metadata":{"permalink":"/2014/02/12/wpf-and-mystic-meg-or-playing","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2014-02-12-wpf-and-mystic-meg-or-playing/index.md","source":"@site/blog/2014-02-12-wpf-and-mystic-meg-or-playing/index.md","title":"WPF and Mystic Meg or Playing Futurologist","description":"Time for an unusual post. Most of what gets put down here is technical \\"how-to\'s\\". It\'s usually prompted by something I\'ve been working on and serves, as much as anything else, as an aide-memoire. Not this time.","date":"2014-02-12T00:00:00.000Z","formattedDate":"February 12, 2014","tags":[{"label":"SPA","permalink":"/tags/spa"},{"label":"WPF","permalink":"/tags/wpf"}],"readingTime":2.915,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"WPF and Mystic Meg or Playing Futurologist","authors":"johnnyreilly","tags":["SPA","WPF"],"hide_table_of_contents":false},"prevItem":{"title":"TypeScript and RequireJS (Keep It Simple)","permalink":"/2014/02/27/typescript-and-requirejs-keep-it-simple"},"nextItem":{"title":"Integration Testing with Entity Framework and Snapshot Backups","permalink":"/2014/01/24/integration-testing-with-entity"}},"content":"Time for an unusual post. Most of what gets put down here is technical \\"how-to\'s\\". It\'s usually prompted by something I\'ve been working on and serves, as much as anything else, as an aide-memoire. Not this time.\\n\\nI\u2019ve been watching the changes in the world of development of the last couple of years and I\u2019ve come to a controversial conclusion... So I wanted to write about it. Hopefully I\'ll be able to return to this in 5 years and say \\"wow - I\'m so insightful - almost visionary really\\". Or not. Either way, let\'s put it out there - it\'s sink or swim time. Ready for it? Here\u2019s my bet: WPF will die.\\n\\nSounds dramatic right? OK - I\'ve overstated my case just to get you to read on (I should work for the tabloids). Let me flesh this out a little. First of all, I think WPF is a fine technology - great apps are built with it. My personal favourite being the fantastic [GitHub for Windows](https://github.com/blog/1151-designing-github-for-windows). And actually I don\'t think WPF will die at all. What I think will happen is that it will become a more niche way to build applications.\\n\\nMore broadly, I think that native client apps (be they Windows, Mac, iOS, Android etc) will eventually come to be replaced by [rich web apps / SPAs](http://en.wikipedia.org/wiki/Single-page_application) of the Angular / Ember / Durandal ilk. I realise that at the moment that seems like a ludicrous statement \u2013 native apps are heavily used throughout enterprises worldwide and certainly will continue to be used and actively developed for at least the next 5 years.\\n\\nBut as the web comes to [perform like native](http://arstechnica.com/information-technology/2013/05/native-level-performance-on-the-web-a-brief-examination-of-asm-js/), as [JavaScript become a compile target](https://github.com/jashkenas/coffee-script/wiki/List-of-languages-that-compile-to-JS), as [HTML 5 provides rich UI](http://davidwalsh.name/canvas-demos) and as [interactive communication becomes possible](https://developer.mozilla.org/en/docs/WebSockets) I reckon this is a fairly probable scenario. Particularly when you consider the [API work Firefox is doing around Firefox OS](https://developer.mozilla.org/en-US/Apps/Reference). I could be wrong, but my expectation is that the day will come when people will have apps that they can access from anywhere, on any platform and those apps can be deployed without infrastructure having to push out new versions to each client machine.\\n\\nThe web undeniably has issues but I think it will likely win out. And the cost case for a single client app is pretty compelling to anyone funding a system.\\n\\nAs a dev I\u2019m always working with an ever-evolving grab bag of technology whether it be front end, middle tier, database or services. In fact that will likely always be the case (change being the only constant in the world of software). But on the basis of my expectations I\u2019m planning to always keep at least a toe in the world of web apps as a form of \u201ccareer future-proofing\u201d.\\n\\nGoing less broad again when I look at the Microsoft stack, I think XAML will live on for some time. Obviously Silverlight is no longer being actively developed but MS are using it in Windows 8 (Phone and WinJS) as well as WPF. But I do kind of wonder if it will become like a bit like VB.Net, still around, still in use, but slowly dropping off in terms of popularity. Particularly as you can write WinJS apps in HTML / CSS / JavaScript.\\n\\nAs I say, I could very much be wrong about all of this. I don\u2019t know what your view of the future of the development landscape is? You may have a different insight? I\u2019d be intrigued to know!"},{"id":"/2014/01/24/integration-testing-with-entity","metadata":{"permalink":"/2014/01/24/integration-testing-with-entity","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2014-01-24-integration-testing-with-entity/index.md","source":"@site/blog/2014-01-24-integration-testing-with-entity/index.md","title":"Integration Testing with Entity Framework and Snapshot Backups","description":"I\'ve written before about how unit testing Entity Framework is a contentious and sometimes pointless activity. The TL;DR is that LINQ-to-Objects != Linq-to-Entities and so if you want some useful tests around your data tier then integration tests that actually hit a database are what you want.","date":"2014-01-24T00:00:00.000Z","formattedDate":"January 24, 2014","tags":[{"label":"Database Snapshot Backups","permalink":"/tags/database-snapshot-backups"},{"label":"Integration Testing","permalink":"/tags/integration-testing"},{"label":"SQL Server","permalink":"/tags/sql-server"}],"readingTime":14.385,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Integration Testing with Entity Framework and Snapshot Backups","authors":"johnnyreilly","tags":["Database Snapshot Backups","Integration Testing","SQL Server"],"hide_table_of_contents":false},"prevItem":{"title":"WPF and Mystic Meg or Playing Futurologist","permalink":"/2014/02/12/wpf-and-mystic-meg-or-playing"},"nextItem":{"title":"Upgrading to TypeScript 0.9.5 - A Personal Memoir","permalink":"/2014/01/09/upgrading-to-typescript-095-personal"}},"content":"I\'ve written before about how unit testing [Entity Framework is a contentious and sometimes pointless activity](http://icanmakethiswork.blogspot.co.uk/2012/10/unit-testing-and-entity-framework-filth.html). The TL;DR is that LINQ-to-Objects != Linq-to-Entities and so if you want some useful tests around your data tier then integration tests that actually hit a database are what you want.\\n\\nHowever hitting an actual database is has serious implications. For a start you need a database server and you need a database. But the real issue lies around cleanup. When you write a test that amends data in the database you need the test to clean up after itself. If it doesn\'t then the next test that runs may trip over the amended data and that\'s your test pack instantly useless.\\n\\nWhat you want is a way to wipe the slate clean - to return the database back to the state that it was in before your test ran. Kind of like a database restore - except that would be slow. And this is where [SQL Server\'s snapshot backups](<http://technet.microsoft.com/en-us/library/ms189548(v=sql.105).aspx>) have got your back. To quote MSDN:\\n\\n> \\\\*Snapshot backups have the following primary benefits:\\n>\\n> - A backup can be created quickly, typically measured in seconds, with little or no effect on the server.\\n> - A restore operation can be accomplished from a disk backup just as quickly.\\n> - Backup to tape can be accomplished by another host without an effect on the production system.\\n> - **A copy of a production database can be created instantly for reporting or testing.**\\n>\\n> *\\n\\nJust the ticket.\\n\\n## Our Mission\\n\\nIn this post I want to go through the process of taking an existing database, pointing Entity Framework at it, setting up some repositories and then creating an integration test pack that uses snapshot backups to cleanup after each test runs. The code detailed in this post is available in this [GitHub repo](https://github.com/johnnyreilly/SnapshotBackupsIntegrationTesting) if you want to have a go yourself.\\n\\n## We need a database\\n\\nYou can find a whole assortment of databases [here](https://msftdbprodsamples.codeplex.com/releases). I\'m going to use [AdventureWorksLT](https://msftdbprodsamples.codeplex.com/wikipage?title=AWLTDocs) as it\'s small and simple. So I\'ll download [this](https://msftdbprodsamples.codeplex.com/downloads/get/478217) and unzip it. I\'ll drop `AdventureWorksLT2008R2_Data/index.mdf` and `AdventureWorksLT2008R2_log.LDF` in my data folder and attach AdventureWorksLT2008R2 to my database server. And now I have a database.\\n\\n## Assemble me your finest DbContext\\n\\nOr in English: we want to point Entity Framework at our new shiny database. So let\'s fire up Visual Studio (I\'m using 2013) and create a new solution called \\"AdventureWorks\\".\\n\\nTo our solution let\'s add a new class library project called \\"AdventureWorks.EntityFramework\\". And to that we\'ll add an ADO.NET Entity Data Model which we\'ll call \\"AdventureWorks.edmx\\". When the wizard fires up we\'ll use the \\"Generate from database\\" option, click Next and select \\"New Connection\\". In the dialog we\'ll select our newly attached AdventureWorksLT2008R2 database. We\'ll leave the \\"save entity connection settings in App.Config\\" option selected and click Next. I\'m going to use Entity Framework 6.0 - though I think that any version would do. I\'m going to pull in all tables / store procs and views. And now Entity Framework is pointing at my database.\\n\\n## Let There be Repositories!\\n\\nIn the name of testability let\'s create a new project to house repositories called \\"AdventureWorks.Repositories\\". I\'m going to use [K. Scott Allen](http://odetocode.com/about/scott-allen)\'s fine [article on MSDN](http://msdn.microsoft.com/en-us/library/ff714955.aspx) to create a very basic set of repositories wrapped in a unit of work.\\n\\nIn my new project I\'ll add a reference to the `AdventureWorks.EntityFramework` project and create a new `IRepository` interface that looks like this:\\n\\n```cs\\nusing System;\\nusing System.Linq;\\nusing System.Linq.Expressions;\\n\\nnamespace AdventureWorks.Repositories\\n{\\n    public interface IRepository<T> where T : class\\n    {\\n        IQueryable<T> FindAll();\\n        IQueryable<T> FindWhere(Expression<Func<T, bool>> predicate);\\n        T Add(T newEntity);\\n        T Remove(T entity);\\n    }\\n}\\n```\\n\\nAnd a new `IUnitOfWork` interface that looks like this:\\n\\n```cs\\nusing AdventureWorks.EntityFramework;\\n\\nnamespace AdventureWorks.Repositories\\n{\\n    public interface IUnitOfWork\\n    {\\n        public IRepository<ErrorLog> ErrorLogs { get; }\\n        public IRepository<Address> Addresses { get; }\\n        public IRepository<Customer> Customers { get; }\\n        public IRepository<CustomerAddress> CustomerAddresses { get; }\\n        public IRepository<Product> Products { get; }\\n        public IRepository<ProductCategory> ProductCategories { get; }\\n        public IRepository<ProductDescription> ProductDescriptions { get; }\\n        public IRepository<ProductModel> ProductModels { get; }\\n        public IRepository<ProductModelProductDescription> ProductModelProductDescriptions { get; }\\n        public IRepository<SalesOrderDetail> SalesOrderDetails { get; }\\n        public IRepository<SalesOrderHeader> SalesOrderHeaders { get; }\\n        public IRepository<BuildVersion> BuildVersions { get; }\\n\\n        void Commit();\\n    }\\n}\\n```\\n\\nNow for the implementation of `IRepository`. For this we\'ll need a reference to Entity Framework in our project. Then we\'ll create a class called `SqlRepository`:\\n\\n```cs\\nusing System;\\nusing System.Data.Entity;\\nusing System.Linq;\\nusing System.Linq.Expressions;\\n\\nnamespace AdventureWorks.Repositories\\n{\\n    public class SqlRepository<T> : IRepository<T> where T : class\\n    {\\n        public SqlRepository(DbContext context)\\n        {\\n            _dbSet = context.Set<T>();\\n        }\\n\\n        public IQueryable<T> FindAll()\\n        {\\n            return _dbSet;\\n        }\\n\\n        public IQueryable<T> FindWhere(Expression<Func<T, bool>> predicate)\\n        {\\n            return _dbSet.Where(predicate);\\n        }\\n\\n        public T Add(T newEntity)\\n        {\\n            return _dbSet.Add(newEntity);\\n        }\\n\\n        public T Remove(T entity)\\n        {\\n            return _dbSet.Remove(entity);\\n        }\\n\\n        protected DbSet<T> _dbSet;\\n    }\\n}\\n```\\n\\nAnd we also need the implementation of `IUnitOfWork`. So we\'ll create a class called `SqlUnitOfWork`:\\n\\n```cs\\nusing System;\\nusing System.Linq;\\nusing System.Data.Entity;\\nusing AdventureWorks.EntityFramework;\\n\\nnamespace AdventureWorks.Repositories\\n{\\n    public class SqlUnitOfWork : IUnitOfWork\\n    {\\n        public SqlUnitOfWork()\\n        {\\n            _context = new AdventureWorksLT2008R2Entities();\\n        }\\n\\n        public IRepository<ErrorLog> ErrorLogs\\n        {\\n            get\\n            {\\n                if (_errorLogs == null) _errorLogs = new SqlRepository<ErrorLog>(_context);\\n                return _errorLogs;\\n            }\\n        }\\n\\n        public IRepository<Address> Addresses\\n        {\\n            get\\n            {\\n                if (_addresses == null) _addresses = new SqlRepository<Address>(_context);\\n                return _addresses;\\n            }\\n        }\\n\\n        public IRepository<Customer> Customers\\n        {\\n            get\\n            {\\n                if (_customers == null) _customers = new SqlRepository<Customer>(_context);\\n                return _customers;\\n            }\\n        }\\n\\n        public IRepository<CustomerAddress> CustomerAddresses\\n        {\\n            get\\n            {\\n                if (_customerAddresses == null) _customerAddresses = new SqlRepository<CustomerAddress>(_context);\\n                return _customerAddresses;\\n            }\\n        }\\n\\n        public IRepository<Product> Products\\n        {\\n            get\\n            {\\n                if (_products == null) _products = new SqlRepository<Product>(_context);\\n                return _products;\\n            }\\n        }\\n\\n        public IRepository<ProductCategory> ProductCategories\\n        {\\n            get\\n            {\\n                if (_productCategories == null) _productCategories = new SqlRepository<ProductCategory>(_context);\\n                return _productCategories;\\n            }\\n        }\\n\\n        public IRepository<ProductDescription> ProductDescriptions\\n        {\\n            get\\n            {\\n                if (_productDescriptions == null) _productDescriptions = new SqlRepository<ProductDescription>(_context);\\n                return _productDescriptions;\\n            }\\n        }\\n\\n        public IRepository<ProductModel> ProductModels\\n        {\\n            get\\n            {\\n                if (_productModels == null) _productModels = new SqlRepository<ProductModel>(_context);\\n                return _productModels;\\n            }\\n        }\\n\\n        public IRepository<ProductModelProductDescription> ProductModelProductDescriptions\\n        {\\n            get\\n            {\\n                if (_productModelProductDescriptions == null) _productModelProductDescriptions = new SqlRepository<ProductModelProductDescription>(_context);\\n                return _productModelProductDescriptions;\\n            }\\n        }\\n\\n        public IRepository<SalesOrderDetail> SalesOrderDetails\\n        {\\n            get\\n            {\\n                if (_salesOrderDetails == null) _salesOrderDetails = new SqlRepository<SalesOrderDetail>(_context);\\n                return _salesOrderDetails;\\n            }\\n        }\\n\\n        public IRepository<SalesOrderHeader> SalesOrderHeaders\\n        {\\n            get\\n            {\\n                if (_salesOrderHeaders == null) _salesOrderHeaders = new SqlRepository<SalesOrderHeader>(_context);\\n                return _salesOrderHeaders;\\n            }\\n        }\\n\\n        public IRepository<BuildVersion> BuildVersions\\n        {\\n            get\\n            {\\n                if (_buildVersions == null) _buildVersions = new SqlRepository<BuildVersion>(_context);\\n                return _buildVersions;\\n            }\\n        }\\n\\n        public void Commit()\\n        {\\n            _context.SaveChanges();\\n        }\\n\\n        SqlRepository<ErrorLog> _errorLogs = null;\\n        SqlRepository<Address> _addresses = null;\\n        SqlRepository<Customer> _customers = null;\\n        SqlRepository<CustomerAddress> _customerAddresses = null;\\n        SqlRepository<Product> _products = null;\\n        SqlRepository<ProductCategory> _productCategories = null;\\n        SqlRepository<ProductDescription> _productDescriptions = null;\\n        SqlRepository<ProductModel> _productModels = null;\\n        SqlRepository<ProductModelProductDescription> _productModelProductDescriptions = null;\\n        SqlRepository<SalesOrderDetail> _salesOrderDetails = null;\\n        SqlRepository<SalesOrderHeader> _salesOrderHeaders = null;\\n        SqlRepository<BuildVersion> _buildVersions = null;\\n\\n        readonly DbContext _context;\\n    }\\n}\\n```\\n\\n## And Now Let\'s Start Integration Testing!\\n\\nLet\'s create a new Unit Test project called \\"AdventureWorks.Repositories.IntegrationTests\\". (And just to be clear: this is \\\\*not\\\\* a unit test project - it is an **_integration_** test project.) We\'ll add a reference back to our `AdventureWorks.Repositories` project for the repositories and one back to `AdventureWorks.EntityFramework` for our domain models. And finally you\'ll need a reference to Entity Framework in your IntegrationTest project as well as well.\\n\\nWe\'ll copy across the `app.config` from `AdventureWorks.EntityFramework` to `AdventureWorks.Repositories.IntegrationTests` as it contains the database connection details. It\'ll look something like this:\\n\\n```xml\\n<?xml version=\\"1.0\\" encoding=\\"utf-8\\"?>\\n<configuration>\\n    <configSections>\\n        <section name=\\"entityFramework\\" type=\\"System.Data.Entity.Internal.ConfigFile.EntityFrameworkSection, EntityFramework, Version=6.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089\\" requirePermission=\\"false\\" />\\n        \x3c!-- For more information on Entity Framework configuration, visit http://go.microsoft.com/fwlink/?LinkID=237468 --\x3e\\n    </configSections>\\n    <connectionStrings>\\n        <add name=\\"AdventureWorksLT2008R2Entities\\"\\n             connectionString=\\"metadata=res://*/AdventureWorks.csdl|res://*/AdventureWorks.ssdl|res://*/AdventureWorks.msl;provider=System.Data.SqlClient;provider connection string=&quot;data source=.;initial catalog=AdventureWorksLT2008R2;integrated security=True;MultipleActiveResultSets=True;App=EntityFramework&quot;\\"\\n             providerName=\\"System.Data.EntityClient\\" />\\n    </connectionStrings>\\n    <entityFramework>\\n        <defaultConnectionFactory type=\\"System.Data.Entity.Infrastructure.SqlConnectionFactory, EntityFramework\\" />\\n        <providers>\\n            <provider invariantName=\\"System.Data.SqlClient\\" type=\\"System.Data.Entity.SqlServer.SqlProviderServices, EntityFramework.SqlServer\\" />\\n        </providers>\\n    </entityFramework>\\n</configuration>\\n```\\n\\nNow we\'re ready for a test. We\'ll add ourselves a class called `BuildVersionTests`:\\n\\n```cs\\nusing System;\\nusing System.Linq;\\nusing System.Linq.Expressions;\\nusing Microsoft.VisualStudio.TestTools.UnitTesting;\\n\\nnamespace AdventureWorks.Repositories.IntegrationTests\\n{\\n    [TestClass]\\n    public class BuildVersionTests\\n    {\\n        [TestMethod]\\n        public void BuildVersions_should_return_the_correct_version_information()\\n        {\\n            // Arrange\\n            var uow = new SqlUnitOfWork();\\n\\n            // Act\\n            var buildVersions = uow.BuildVersions.FindAll().ToList();\\n\\n            // Assert\\n            Assert.AreEqual(1, buildVersions.Count);\\n            Assert.AreEqual(\\"10.00.80404.00\\", buildVersions[0].Database_Version);\\n            Assert.AreEqual(new DateTime(2008, 4, 4), buildVersions[0].ModifiedDate);\\n            Assert.AreEqual(1, buildVersions[0].SystemInformationID);\\n            Assert.AreEqual(new DateTime(2008, 4, 4), buildVersions[0].VersionDate);\\n        }\\n    }\\n}\\n```\\n\\nThis is as simple as it gets - our test creates a new unit of work and queries the `BuildVersions` table to see what we can see. All it\'s really doing is demonstrating that we can now hit our database through our repositories. As a side note, we could have the exact same test operating directly on the `DbContext` like this:\\n\\n```cs\\n[TestMethod]\\n        public void DbContext_BuildVersions_should_return_the_correct_version_information()\\n        {\\n            // Arrange\\n            var dbContext = new AdventureWorks.EntityFramework.AdventureWorksLT2008R2Entities();\\n\\n            // Act\\n            var buildVersions = dbContext.BuildVersions.ToList();\\n\\n            // Assert\\n            Assert.AreEqual(1, buildVersions.Count);\\n            Assert.AreEqual(\\"10.00.80404.00\\", buildVersions[0].Database_Version);\\n            Assert.AreEqual(new DateTime(2008, 4, 4), buildVersions[0].ModifiedDate);\\n            Assert.AreEqual(1, buildVersions[0].SystemInformationID);\\n            Assert.AreEqual(new DateTime(2008, 4, 4), buildVersions[0].VersionDate);\\n        }\\n```\\n\\nFor the most part we won\'t be doing this but I wanted to be clear that full power of Entity Framework is available to you as you\'re putting together your integration tests.\\n\\n## Database Snapshotting Time\\n\\nUp until this point we\'ve essentially been laying our infrastructure and doing our plumbing. We now have a database, domain models and data access courtesy of Entity Framework, a testable repository layer and finally an integration test pack. What we want now is to get our database snapshot / backup and restore mechanism set up and integrated into the test pack.\\n\\nLet\'s add references to the `System.Data` and `System.Configuration` assemblies to our integration testing project and then add a new class called `DatabaseSnapshot`:\\n\\n```cs\\nusing System.Configuration;\\nusing System.Data;\\nusing System.Data.SqlClient;\\n\\nnamespace AdventureWorks.Repositories.IntegrationTests\\n{\\n    public static class DatabaseSnapshot\\n    {\\n        private const string SpCreateSnapShotName = \\"SnapshotBackup_Create\\";\\n        private const string SpCreateSnapShot =\\n@\\"CREATE PROCEDURE [dbo].[\\" + SpCreateSnapShotName + @\\"]\\n    @databaseName        varchar(512),\\n    @databaseLogicalName varchar(512),\\n    @snapshotBackupPath  varchar(512),\\n    @snapshotBackupName  varchar(512)\\nAS\\nBEGIN\\n    SET NOCOUNT ON;\\n\\n    DECLARE @sql varchar(500)\\n    SELECT @sql = \'CREATE DATABASE \' + @snapshotBackupName +\\n                  \' ON (NAME=[\' + @databaseLogicalName +\\n                  \'], FILENAME=\'\'\' + @snapshotBackupPath + @snapshotBackupName +\\n                  \'\'\') AS SNAPSHOT OF [\' + @databaseName + \']\'\\n    EXEC(@sql)\\nEND\\";\\n\\n        private const string SpRestoreSnapShotName = \\"SnapshotBackup_Restore\\";\\n        private const string SpRestoreSnapShot =\\n@\\"CREATE PROCEDURE [dbo].[\\" + SpRestoreSnapShotName + @\\"]\\n    @databaseName varchar(512),\\n    @snapshotBackupName varchar(512)\\nAS\\nBEGIN\\n    SET NOCOUNT ON;\\n\\n    DECLARE @sql varchar(500)\\n    SET @sql  = \'ALTER DATABASE [\' + @databaseName + \'] SET SINGLE_USER WITH ROLLBACK IMMEDIATE\'\\n    EXEC (@sql)\\n\\n    RESTORE DATABASE @databaseName\\n    FROM DATABASE_SNAPSHOT = @snapshotBackupName\\n\\n    SET @sql = \'ALTER DATABASE [\' + @databaseName + \'] SET MULTI_USER\'\\n    EXEC (@sql)\\nEND\\";\\n\\n        private const string SpDeleteSnapShotName = \\"SnapshotBackup_Delete\\";\\n        private const string SpDeleteSnapShot =\\n@\\"CREATE PROCEDURE [dbo].[\\" + SpDeleteSnapShotName + @\\"]\\n    @snapshotBackupName varchar(512)\\nAS\\nBEGIN\\n    SET NOCOUNT ON;\\n\\n    DECLARE @sql varchar(500)\\n\\n    SELECT @sql = \'DROP DATABASE \' + @snapshotBackupName\\n    EXEC(@sql)\\nEND\\";\\n\\n        private static string _masterDbConnectionString;\\n        private static string _dbName;\\n        private static ConnectionStringSettings _dbConnectionStringSettings;\\n\\n        private static ConnectionStringSettings DbConnectionStringSettings\\n        {\\n            get\\n            {\\n                if (_dbConnectionStringSettings == null)\\n                    _dbConnectionStringSettings = ConfigurationManager.ConnectionStrings[\\"SnapshotBackup\\"];\\n\\n                return _dbConnectionStringSettings;\\n            }\\n        }\\n\\n        /// <summary>\\n        /// Stored procedures should be executed against master database\\n        /// </summary>\\n        private static string MasterDbConnectionString\\n        {\\n            get\\n            {\\n                if (string.IsNullOrEmpty(_masterDbConnectionString))\\n                {\\n                    var sqlConnection = new SqlConnection(DbConnectionStringSettings.ConnectionString);\\n                    _masterDbConnectionString = DbConnectionStringSettings.ConnectionString.Replace(sqlConnection.Database, \\"master\\");\\n                }\\n                return _masterDbConnectionString;\\n            }\\n        }\\n\\n        private static string DbName\\n        {\\n            get\\n            {\\n                if (string.IsNullOrEmpty(_dbName))\\n                    _dbName = new SqlConnection(DbConnectionStringSettings.ConnectionString).Database.TrimStart(\'[\').TrimEnd(\']\');\\n\\n                return _dbName;\\n            }\\n        }\\n\\n        public static void SetupStoredProcedures()\\n        {\\n            using (var conn = new SqlConnection(MasterDbConnectionString))\\n            {\\n                conn.Open();\\n\\n                // Drop the existing stored procedures\\n                SqlCommand cmd;\\n                const string dropProcSql = \\"IF EXISTS (SELECT * FROM sys.objects WHERE object_id = OBJECT_ID(N\'[dbo].[{0}]\') AND type in (N\'P\', N\'PC\')) DROP PROCEDURE [dbo].[{0}]\\";\\n                foreach (var spName in new[] { SpCreateSnapShotName, SpDeleteSnapShotName, SpRestoreSnapShotName })\\n                {\\n                    cmd = new SqlCommand(string.Format(dropProcSql, spName), conn);\\n                    cmd.ExecuteNonQuery();\\n                }\\n\\n                // Create the stored procedures anew\\n                foreach (var createProcSql in new[] { SpCreateSnapShot, SpDeleteSnapShot, SpRestoreSnapShot })\\n                {\\n                    cmd = new SqlCommand(createProcSql, conn);\\n                    cmd.ExecuteNonQuery();\\n                }\\n\\n                conn.Close();\\n            }\\n        }\\n\\n        public static void CreateSnapShot()\\n        {\\n            var databaseName = new SqlParameter { ParameterName = \\"@databaseName\\", SqlValue = SqlDbType.VarChar, Value = DbName };\\n            var databaseLogicalName = new SqlParameter { ParameterName = \\"@databaseLogicalName\\", SqlValue = SqlDbType.VarChar, Value = ConfigurationManager.AppSettings[\\"DatabaseLogicalName\\"] };\\n            var snapshotBackupPath = new SqlParameter { ParameterName = \\"@snapshotBackupPath\\", SqlValue = SqlDbType.VarChar, Value = ConfigurationManager.AppSettings[\\"SnapshotBackupPath\\"] };\\n            var snapshotBackupName = new SqlParameter { ParameterName = \\"@snapshotBackupName\\", SqlValue = SqlDbType.VarChar, Value = ConfigurationManager.AppSettings[\\"SnapshotBackupName\\"] };\\n\\n            ExecuteStoredProcAgainstMaster(SpCreateSnapShotName, new[] { databaseName, databaseLogicalName, snapshotBackupPath, snapshotBackupName });\\n        }\\n\\n        public static void DeleteSnapShot()\\n        {\\n            var snapshotBackupName = new SqlParameter { ParameterName = \\"@snapshotBackupName\\", SqlValue = SqlDbType.VarChar, Value = ConfigurationManager.AppSettings[\\"SnapshotBackupName\\"] };\\n\\n            ExecuteStoredProcAgainstMaster(SpDeleteSnapShotName, new[] { snapshotBackupName });\\n        }\\n\\n        public static void RestoreSnapShot()\\n        {\\n            var databaseName = new SqlParameter { ParameterName = \\"@databaseName\\", SqlValue = SqlDbType.VarChar, Value = DbName };\\n            var snapshotBackupName = new SqlParameter { ParameterName = \\"@snapshotBackupName\\", SqlValue = SqlDbType.VarChar, Value = ConfigurationManager.AppSettings[\\"SnapshotBackupName\\"] };\\n\\n            ExecuteStoredProcAgainstMaster(SpRestoreSnapShotName, new[] { databaseName, snapshotBackupName });\\n        }\\n\\n        private static void ExecuteStoredProcAgainstMaster(string storedProc, SqlParameter[] parameters)\\n        {\\n            using (var conn = new SqlConnection(MasterDbConnectionString))\\n            {\\n                conn.Open();\\n                var cmd = new SqlCommand(storedProc, conn) { CommandType = CommandType.StoredProcedure };\\n                cmd.Parameters.AddRange(parameters);\\n                cmd.ExecuteNonQuery();\\n                conn.Close();\\n            }\\n        }\\n    }\\n}\\n```\\n\\nThe `DatabaseSnapshot` class exposes 4 methods:\\n\\n<dl><dt>SetupStoredProcedures</dt><dd>This method creates 3 stored procedures on the master database: <code>SnapshotBackup_Create</code>, <code>SnapshotBackup_Restore</code> and <code>SnapshotBackup_Delete</code>. These procs do pretty much what their names suggest and the other 3 methods call these stored procedures when creating, restoring and deleting snapshot backups respectively. You can see the (fairly minimal) SQL for these stored procs at the top of the<code>DatabaseSnapshot</code> class.</dd><dt>CreateSnapShot</dt><dd>This method creates a snapshot backup of the database at this point in time.</dd><dt>RestoreSnapShot</dt><dd>This method restores the database back to state it was in when the snapshot backup was created.</dd><dt>DeleteSnapShot</dt><dd>This method attempts to delete the existing snapshot backup.</dd></dl>\\n\\nIn order that we can use the `DatabaseSnapshot` class we need to add the following entries to our `app.config`:\\n\\n```xml\\n<configuration>\\n\\n    <connectionStrings>\\n\\n        <add name=\\"SnapshotBackup\\"\\n             connectionString=\\"data source=.;initial catalog=AdventureWorksLT2008R2;Trusted_Connection=true;Connection Timeout=200\\" />\\n\\n    </connectionStrings>\\n\\n    <appSettings>\\n        <add key=\\"DatabaseLogicalName\\" value=\\"AdventureWorksLT2008_Data\\" />\\n        <add key=\\"SnapshotBackupPath\\" value=\\"C:\\\\DbSnapshots\\\\\\" />\\n        <add key=\\"SnapshotBackupName\\" value=\\"AdventureWorksLT2008R2_Snapshot\\" />\\n    </appSettings>\\n</configuration>\\n```\\n\\nThese settings allow have the following purposes:\\n\\n<dl><dt>SnapshotBackup</dt><dd>A connection string that allows <code>DatabaseSnapshot</code> to connect to the database.</dd><dt>DatabaseLogicalName</dt><dd>The logical name of the database you want to backup. (This can be found on the Files tab of the Database Properties in SSMS)</dd><dt>SnapshotBackupPath</dt><dd>The location where the snapshot backup is to be stored. You need to make sure that this exists on your machine.</dd><dt>SnapshotBackupName</dt><dd>The name of the snapshot backup that will be created.</dd></dl>\\n\\nNow to make use of `DatabaseSnapshot`. Let\'s add a new class called `SetUpTearDown`:\\n\\n```cs\\nusing Microsoft.VisualStudio.TestTools.UnitTesting;\\n\\nnamespace AdventureWorks.Repositories.IntegrationTests\\n{\\n    [TestClass]\\n    public static class SetUpTearDown\\n    {\\n        [AssemblyInitialize]\\n        public static void TestRunInitialize(TestContext context)\\n        {\\n            try\\n            {\\n                // Try to delete the snapshot in case it was left over from aborted test runs\\n                DatabaseSnapshot.DeleteSnapShot();\\n            }\\n            catch { /* this should fail with snapshot does not exist */ }\\n\\n            DatabaseSnapshot.SetupStoredProcedures();\\n            DatabaseSnapshot.CreateSnapShot();\\n        }\\n\\n\\n        [AssemblyCleanup]\\n        public static void TestRunCleanup()\\n        {\\n            DatabaseSnapshot.DeleteSnapShot();\\n        }\\n    }\\n}\\n```\\n\\nAt the start of the test run this will create a snapshot in case one doesn\'t exist already. And at the end of the test run it will be a good citizen and delete the snapshot. We\'ll also add an extra method to our `BuildVersionTests` class:\\n\\n```cs\\nnamespace AdventureWorks.Repositories.IntegrationTests\\n{\\n    [TestClass]\\n    public class BuildVersionTests\\n    {\\n        // ...\\n\\n        [TestCleanup]\\n        public void TestCleanup()\\n        {\\n            DatabaseSnapshot.RestoreSnapShot();\\n        }\\n    }\\n}\\n```\\n\\nThis will ensure that after each test runs the database will be restored back to the snapshot created in `SetUpTearDown`. Now if you re-run your tests, in between each test the restore back to the snapshot is taking place.\\n\\n## Prove it!\\n\\nOf course the tests we have in place at present don\'t actually change the data at all. So I could be lying. I\'m not. Let\'s prove it by adding one more class called `CustomerTests`:\\n\\n```cs\\nusing System;\\nusing System.Linq;\\nusing System.Linq.Expressions;\\nusing Microsoft.VisualStudio.TestTools.UnitTesting;\\nusing AdventureWorks.EntityFramework;\\n\\nnamespace AdventureWorks.Repositories.IntegrationTests\\n{\\n    [TestClass]\\n    public class CustomerTests\\n    {\\n        [TestMethod]\\n        public void Should_change_a_customers_first_and_last_name()\\n        {\\n            // Arrange\\n            var uow = new SqlUnitOfWork();\\n\\n            // Act\\n            var customer = uow.Customers.FindWhere(x => x.FirstName == \\"Jay\\" && x.LastName == \\"Adams\\").First();\\n            var customerId = customer.CustomerID;\\n            customer.FirstName = \\"John\\";\\n            customer.LastName = \\"Reilly\\";\\n            uow.Commit();\\n\\n            // Assert\\n            Assert.IsNotNull(uow.Customers.FindWhere(x => x.FirstName == \\"John\\" && x.LastName == \\"Reilly\\" && x.CustomerID == customerId).SingleOrDefault());\\n        }\\n\\n        [TestCleanup]\\n        public void TestCleanup()\\n        {\\n            DatabaseSnapshot.RestoreSnapShot();\\n        }\\n    }\\n}\\n```\\n\\nThe above test checks that you can look up an existing customer, Mr Jay Adams, and change his name to my name - to John Reilly. If I execute the test above and there was no restore in place then subsequently when I came to exercise this test it should start to fail as it no longer has a Mr Jay Adams to lookup. But with this restore mechanism in place I can execute this test repeatedly without worrying.\\n\\n## Rounding off\\n\\nAnd that\'s us finished - we now have a database snapshot restore mechanism in place. With this we can develop integration tests that thoroughly change the data in our database secure in the knowledge that once the test is complete our database will be restored back to it\'s initial state.\\n\\nObviously there are other alternative approaches for integration testing available to that which I\'ve laid out in this post. But I can imagine that this approach is very useful for applying to legacy applications that you might inherit and need to continue supporting. Also, this approach should fit in well with a continuous integration setup. It would be pretty straightforward to have database that existed purely for testing purposes against which all the integration tests could be set to run at the point of each check in.\\n\\nThanks to Marc Talary, Sandeep Deo and Tishul Vadher who all contributed to `DatabaseSnapshot`. Credit is also due to Google due to the hundreds of articles the team ended up reading on snapshot backups."},{"id":"/2014/01/09/upgrading-to-typescript-095-personal","metadata":{"permalink":"/2014/01/09/upgrading-to-typescript-095-personal","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2014-01-09-upgrading-to-typescript-095-personal/index.md","source":"@site/blog/2014-01-09-upgrading-to-typescript-095-personal/index.md","title":"Upgrading to TypeScript 0.9.5 - A Personal Memoir","description":"I recently made the step to upgrade from TypeScript 0.9.1.1 to 0.9.5. To my surprise this process was rather painful and certainly not an unalloyed pleasure. Since I\'m now on the other side, so to speak, I thought I\'d share my experience and cast back a rope bridge to those about to journey over the abyss.","date":"2014-01-09T00:00:00.000Z","formattedDate":"January 9, 2014","tags":[{"label":"Q","permalink":"/tags/q"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"0.9.1.1","permalink":"/tags/0-9-1-1"},{"label":"0.9.5","permalink":"/tags/0-9-5"},{"label":"upgrading","permalink":"/tags/upgrading"}],"readingTime":7.72,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Upgrading to TypeScript 0.9.5 - A Personal Memoir","authors":"johnnyreilly","tags":["Q","TypeScript","0.9.1.1","0.9.5","upgrading"],"hide_table_of_contents":false},"prevItem":{"title":"Integration Testing with Entity Framework and Snapshot Backups","permalink":"/2014/01/24/integration-testing-with-entity"},"nextItem":{"title":"NuGet and WebMatrix: How to install a specific version of a package","permalink":"/2013/12/13/nuget-and-webmatrix-how-to-install"}},"content":"I recently made the step to upgrade from TypeScript 0.9.1.1 to 0.9.5. To my surprise this process was rather painful and certainly not an unalloyed pleasure. Since I\'m now on the other side, so to speak, I thought I\'d share my experience and cast back a rope bridge to those about to journey over the abyss.\\n\\n## TL;DR\\n\\nTypeScript 0.9.5 is worth making the jump to. However, if you are using Visual Studio (as I would guess many are) then you should be aware of a number of problems with the TypeScript Visual Studio tooling for TS 0.9.5. These problems can be worked around if you follow the instructions in this post.\\n\\n## Upgrading the Plugin\\n\\nAt home I upgraded the moment TS 0.9.5 was released. This allowed me to help with migrating the [Definitely Typed typings](https://github.com/borisyankov/DefinitelyTyped) over from 0.9.1.1. And allowed me to give TS 0.9.5 a little test drive. However, I deliberately held off performing the upgrade at work until I knew that all the Definitely Typed typings had been upgraded. This was completed [by the end of 2013](https://github.com/borisyankov/DefinitelyTyped/pull/1385). So in the new year it seemed a good time to make the move.\\n\\nIf, like me, you are using TypeScript inside Visual Studio then you\'d imagine it\'s as simple as closing down VS, uninstalling TypeScript 0.9.1.1 from Programs and Features and then installing the [new plugin](http://www.typescriptlang.org/#Download). And it is if you are running IE 10 or IE 11 on your Windows machine. If you are running a lower IE version then there is a problem.\\n\\nRegrettably, the TypeScript 0.9.5 plugin installer has a dependency on IE 10. Fortunately TypeScript itself has no dependency on IE 10 at all (and why would it?). This dependency appears to have been a mistake. I [raised it as an issue](https://typescript.codeplex.com/workitem/1975) and the TS team have said that this will be resolved in the next major release.\\n\\nHappily there is a workaround if you\'re running IE 9 or lower which has been noted in the [comments underneath the TS 0.9.5 release blog post](https://blogs.msdn.com/b/typescript/archive/2013/12/05/announcing-typescript-0-9-5.aspx). All you do is set the `HKEY_LOCAL_MACHINE\\\\SOFTWARE\\\\Wow6432Node\\\\Microsoft\\\\Internet Explorer\\\\svcVersion` registry key value to `10.0.9200.16384` for the duration of the install.\\n\\nFirst hurdle jumped, the upgrade continues simple enough. Then the fun starts...\\n\\n## Declaration Merging is dead... Sort of\\n\\nHaving upgraded my plugin I opened up the project I\'m working on in Visual Studio. I used NuGet to upgrade all the Definitely Typed packages to the latest (TS 0.9.5) versions. Then I tried, and failed, to compile. It was the the most obscure error I\'ve seen in a while:\\n\\n```ts\\nVSTSC : tsc.js(37574, 25) Microsoft JScript runtime error : Unable to get value of the property \'wrapsSomeTypeParameter\': object is null or undefined\\n```\\n\\nAs you can see there was no indication where in my code the problem was being caused. Fortunately someone had already suffered this particular problem and logged an issue [here](https://typescript.codeplex.com/workitem/1995). Digging through the comments I found a common theme; everyone experiencing the problem was using the [Q typings](https://github.com/borisyankov/DefinitelyTyped/blob/master/q/Q.d.ts). So what\'s up with that?\\n\\nStrangely, if you directly referenced the Q typings everything was okay - which is how the Definitely Typed tests came to pass in the first place. But if you wanted to make use of these typings with implicit referencing (in Visual Studio since TS 0.9.1, all TypeScript files in a project are considered to be referencing each other) - well it doesn\'t work.\\n\\nI decided to take a look at the Q typings at this point to see what was so upsetting about them. The one thing that was obvious was that these typings make use of [Declaration Merging](https://blogs.msdn.com/b/typescript/archive/2013/06/18/announcing-typescript-0-9.aspx). And this made them slightly different to most of the other typing libraries that I was using. So I decided to refactor the Q typings to use the more interface driven approach the other typing libraries used in the hope that might resolve the issue.\\n\\nRoughly speaking I went from:\\n\\n```ts\\ndeclare function Q<T>(promise: Q.IPromise<T>): Q.Promise<T>;\\ndeclare function Q<T>(promise: JQueryPromise<T>): Q.Promise<T>;\\ndeclare function Q<T>(value: T): Q.Promise<T>;\\n\\ndeclare module Q {\\n  //\u2026 functions etc in here\\n}\\n\\ndeclare module \'q\' {\\n  export = Q;\\n}\\n```\\n\\nTo:\\n\\n```ts\\ninterface QIPromise<T> {\\n    //\u2026 functions etc in here\\n}\\n\\ninterface QDeferred<T> {\\n    //\u2026 functions etc in here\\n}\\n\\ninterface QPromise<T> {\\n    //\u2026 functions etc in here\\n}\\n\\ninterface QPromiseState<T> {\\n    //\u2026 functions etc in here\\n}\\n\\ninterface QStatic {\\n\\n    <t>(promise: QIPromise<T>): QPromise<T>;\\n    <t>(promise: JQueryPromise<T>): QPromise<T>;\\n    <t>(value: T): QPromise<T>;\\n\\n    //\u2026 other functions etc continue here\\n}\\n\\ndeclare module \\"q\\" {\\n    export = Q;\\n}\\ndeclare var Q: QStatic;\\n</t></t></t>\\n```\\n\\nAnd that fixed the obscure \'wrapsSomeTypeParameter\' error. The full source code of these amended typings can be found as a GitHub Repo [here](https://github.com/johnnyreilly/Q-TS-0.9.5-WorkAround) in case you want to use it yourself. (I did originally consider adding this to Definitely Typed but opted not to in the end - [see discussion on GitHub](https://github.com/borisyankov/DefinitelyTyped/pull/1497).)\\n\\n\x3c!-- <h4>TypeScript Language Service</h4> <p>At this point I could compile - which was fantastic.  However, the strangest thing: all the typings from other files were undetected.  Despite having the jQuery, Q, Knockout etc typings within my project the TypeScript Language Service was not detecting them.  The TypeScript Language Service (if you\'re not aware of it) is the supplier of Intellisense and all that good stuff which Visual Studio uses to give you a rich IDE.  This lead to the odd experience of being able to compile my TypeScript successfully (the compiler could detect my typings) but having a code editor that was a sea of red squiggly lines.</p> <p>There\'s a happy ending here - for although TypeScript 0.9.5 had delivered the problem it had also delivered a solution.  <a href=\\"https://blogs.msdn.com/b/typescript/archive/2013/12/05/announcing-typescript-0-9-5.aspx\\" target=\\"_blank\\">With TypeScript 0.9.5 you can now make use of a <code>_references.ts</code> file</a>:</p> <blockquote cite=\\"https://blogs.msdn.com/b/typescript/archive/2013/12/05/announcing-typescript-0-9-5.aspx\\"><em><p>\\"With the previous improvements to the Visual Studio experience, we\'ve moved to projects implicitly referencing the .ts files contained in the project.  This cut down on having to explicitly reference your files in the project, bringing the experience much closer to C#.  Unfortunately, it also did not work well when using the option to concatenate your output .js file.</p> <p>We\'re continuing to improve this experience.  Starting with 0.9.5, you can now add an <code>_references.ts</code> file to your project.  This file will be the first passed to the compiler, allowing you more control over the order the generated .js file when used in combination with the Combine JavaScript output into file option (the equivalent of using the --out commandline option).\\"</p></em></blockquote> <p>By adding an <code>_references.ts</code> file to our project we able to get the TypeScript Language Service functioning once more.  There were a couple of \\"gotchas\\" that you should be aware of:</p> <ul><li>You may already have a <code>_references.<strong>js</strong></code> file in your project.  It drives the JavaScript Intellisense Visual Studio provides.  So if you have parts of your application that are just straight JavaScript (we do) and you still want your Intellisense to persist then be certain to place your <code>_references.ts</code> file where it doesn\'t compile and delete your a <code>_references.<strong>js</strong></code> file.</li><li>Make sure your <code>_references.ts</code> contains <em>all</em> TypeScript files in your project.  Without this you don\'t have a functioning TypeScript Language Service.</li><li>Occasionally the problem will re-occur; the TypeScript Language Service will stop working again.  This can generally be righted by opening your <code>_references.ts</code> inside Visual Studio.  A little flaky I know.</li></ul> <p>In the end <a href=\\"https://typescript.codeplex.com/workitem/2071\\" target=\\"_blank\\">I logged the issue on CodePlex</a> and I\'m hopeful it will be resolved in subsequent versions of TypeScript.</p>--\x3e\\n\\n## The Promised Land\\n\\nYou\'re there. You\'ve upgraded to the new plugin and the new typings. All is compiling as it should and the language service is working as well. Was it worth it? I think yes, for the following reasons:\\n\\n1. TS 0.9.5 compiles faster, and hogs less memory.\\n2. When we compiled with TS 0.9.5 we found there were a couple of bugs in our codebase which the tightened up compiler was now detecting. Essentially where we\'d assumed types were flowing through to functions there were a couple of occasions with TS 0.9.1.1 where they weren\'t. Where we\'d assumed we had a type of `T` available in a function whereas it was actually a type of `any`. I was really surprised that this was the case since we were already making use of `noImplicitAny` compiler flag in our project. So where a type had changed and a retired property was being referenced TS 0.9.5 picked up an error that TS 0.9.1.1 had not. Good catch!\\n3. And finally (and I know these are really minor), the compiled JS is a little different now. Firstly, the compiled JS features all of TypeScript comments in the positions that you might hope for. Previously it seemed that about 75% came along for the ride and ended up in some strange locations sometimes. Secondly, enums are treated differently during compilation now - where it makes sense the actual backing value of an enum is used rather than going through the JavaScript construct. So it\'s a bit like a `const` I guess - presumably this allows JavaScript engines to optimise a little more.\\n\\nI hope I haven\'t put you off with this post. I think TypeScript 0.9.5 is well worth making the leap for - and hopefully by reading this you\'ll have saved yourself from a few of the rough edges."},{"id":"/2013/12/13/nuget-and-webmatrix-how-to-install","metadata":{"permalink":"/2013/12/13/nuget-and-webmatrix-how-to-install","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2013-12-13-nuget-and-webmatrix-how-to-install/index.md","source":"@site/blog/2013-12-13-nuget-and-webmatrix-how-to-install/index.md","title":"NuGet and WebMatrix: How to install a specific version of a package","description":"I\'ve recently been experimenting with WebMatrix. If you haven\'t heard of it, WebMatrix is Microsoft\'s \\"free, lightweight, cloud-connected web development tool\\". All marketing aside, it\'s pretty cool. You can whip up a site in next to no time, it has source control, publishing abilities, intellisense. Much good stuff. And one thing it has, that I genuinely hadn\'t expected is NuGet. Brilliant!","date":"2013-12-13T00:00:00.000Z","formattedDate":"December 13, 2013","tags":[{"label":"jquery","permalink":"/tags/jquery"},{"label":"package","permalink":"/tags/package"},{"label":"WebMatrix","permalink":"/tags/web-matrix"},{"label":"NuGet","permalink":"/tags/nu-get"}],"readingTime":2.375,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"NuGet and WebMatrix: How to install a specific version of a package","authors":"johnnyreilly","tags":["jquery","package","WebMatrix","NuGet"],"hide_table_of_contents":false},"prevItem":{"title":"Upgrading to TypeScript 0.9.5 - A Personal Memoir","permalink":"/2014/01/09/upgrading-to-typescript-095-personal"},"nextItem":{"title":"Simple fading in and out using CSS transitions and classes","permalink":"/2013/12/04/simple-fading-in-and-out-using-css-transitions"}},"content":"I\'ve recently been experimenting with WebMatrix. If you haven\'t heard of it, WebMatrix is Microsoft\'s _[\\"free, lightweight, cloud-connected web development tool\\"](http://www.microsoft.com/web/webmatrix/)_. All marketing aside, it\'s pretty cool. You can whip up a site in next to no time, it has source control, publishing abilities, intellisense. Much good stuff. And one thing it has, that I genuinely hadn\'t expected is [NuGet](https://www.nuget.org/). Brilliant!\\n\\nBut like any free product there are disadvantages. As a long time Visual Studio user I\'ve become very used to the power of the NuGet command line. I\'ve been spoiled. You don\'t have this in WebMatrix. You have a nice UI.\\n\\nLooks great right? However, if you want to install a specific version of a NuGet package... well let\'s see what happens...\\n\\nAs you\'re probably aware jQuery currently exists in 2 branches; the 1.10.x branch which supports IE 6-8 and the 2.0.x branch which doesn\'t. However there is only 1 jQuery inside NuGet. Let\'s click on install and see if we can select a specific version.\\n\\nHmmm.... As you can see it\'s 2.0.3 or bust. We can\'t select a specific version; we\'re forced to go with the latest and greatest which is a problem if you need to support IE 6-8. So the obvious strategy if you\'re in this particular camp is to forego NuGet entirely. Go old school. And we could. But let\'s say we want to keep using NuGet, mindful that a little while down the road we\'ll be ready to do that upgrade. Can it be done? Let\'s find out.\\n\\n## NuGet, by hook or by crook\\n\\nI\'ve created a new site in WebMatrix using the Empty Site template.\\n\\nLovely.\\n\\nNow to get me some jQuery 1.10.2 goodness. To the console Batman! We\'ve already got the NuGet command line installed (if you haven\'t you could get it from [here](http://nuget.org/nuget.exe)) and so we follow these steps:\\n\\n- At the `C:\\\\` prompt we enter `nuget install jQuery -Version 1.10.2` and down comes jQuery 1.10.2.\\n- We move `C:\\\\jQuery.1.10.2` to `C:\\\\Users\\\\me\\\\Documents\\\\My Web Sites\\\\Empty Site\\\\App_Data\\\\packages\\\\jQuery.1.10.2`.\\n- Then we delete the `C:\\\\Users\\\\me\\\\Documents\\\\My Web Sites\\\\Empty Site\\\\App_Data\\\\packages\\\\jQuery.1.10.2\\\\Tools` subfolder.\\n- We move `C:\\\\Users\\\\me\\\\Documents\\\\My Web Sites\\\\Empty Site\\\\App_Data\\\\packages\\\\jQuery.1.10.2\\\\Content\\\\Scripts` to `C:\\\\Users\\\\me\\\\Documents\\\\My Web Sites\\\\Empty Site\\\\Scripts`.\\n- And finally we delete the `C:\\\\Users\\\\me\\\\Documents\\\\My Web Sites\\\\Empty Site\\\\App_Data\\\\packages\\\\jQuery.1.10.2\\\\Content` folder.\\n\\nIf we go to NuGet and select updates you\'ll see that jQuery is now considered \\"installed\\" and an update is available. So, in short, our plan worked - yay!\\n\\n## Now for bonus points\\n\\nJust to prove that you can upgrade using the WebMatrix tooling following our manual install let\'s do it. Click \\"Update\\", then \\"Yes\\" and finally \\"I Accept\\" to the EULA. You\'ll now see we\'re now on jQuery 2.0.3.\\n\\n## Rounding off\\n\\nIn my example I\'m only looking at a simple JavaScript library. But the same principal should be able to be applied to any NuGet package as far as I\'m aware. Hope that helps!"},{"id":"/2013/12/04/simple-fading-in-and-out-using-css-transitions","metadata":{"permalink":"/2013/12/04/simple-fading-in-and-out-using-css-transitions","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2013-12-04-simple-fading-in-and-out-using-css-transitions/index.md","source":"@site/blog/2013-12-04-simple-fading-in-and-out-using-css-transitions/index.md","title":"Simple fading in and out using CSS transitions and classes","description":"Caveat emptor folks... Let me start off by putting my hands up and saying I am no expert on CSS. And furthermore let me say that this blog post is essentially the distillation of a heady session of googling on the topic of CSS transitions. The credit for the technique detailed here belongs to many others, I\'m just documenting it for my own benefit (and for anyone who stumbles upon this).","date":"2013-12-04T00:00:00.000Z","formattedDate":"December 4, 2013","tags":[{"label":"CSS 3","permalink":"/tags/css-3"},{"label":"transitionend","permalink":"/tags/transitionend"},{"label":"fadeIn","permalink":"/tags/fade-in"},{"label":"fadeOut","permalink":"/tags/fade-out"},{"label":"transitions","permalink":"/tags/transitions"}],"readingTime":3.635,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Simple fading in and out using CSS transitions and classes","authors":"johnnyreilly","tags":["CSS 3","transitionend","fadeIn","fadeOut","transitions"],"hide_table_of_contents":false},"prevItem":{"title":"NuGet and WebMatrix: How to install a specific version of a package","permalink":"/2013/12/13/nuget-and-webmatrix-how-to-install"},"nextItem":{"title":"Rolling your own confirm mechanism using Promises and jQuery UI","permalink":"/2013/11/26/rolling-your-own-confirm-mechanism"}},"content":"Caveat emptor folks... Let me start off by putting my hands up and saying I am no expert on CSS. And furthermore let me say that this blog post is essentially the distillation of a heady session of googling on the topic of CSS transitions. The credit for the technique detailed here belongs to many others, I\'m just documenting it for my own benefit (and for anyone who stumbles upon this).\\n\\n## What do we want to do?\\n\\nMost web developers have likely reached at some point for jQuery\'s [`fadeIn`](http://api.jquery.com/fadeIn/) and [`fadeOut`](http://api.jquery.com/fadeOut/) awesomeness. What could be cooler than fading in or out your UI, right?\\n\\nBehind the scenes of `fadeIn` and `fadeOut` JavaScript is doing an awful lot of work to create that animation. And in our modern world we simply don\'t need to do that work anymore; it\'s gone native and is covered by [CSS transitions](https://developer.mozilla.org/en-US/docs/Web/Guide/CSS/Using_CSS_transitions).\\n\\nAdded to the \\"[because it\'s there](http://en.wikipedia.org/wiki/George_Mallory)\\" reason for using CSS transitions to do fading there is a more important reason; let me quote [HTML5 rocks](http://www.html5rocks.com/en/tutorials/speed/html5/#toc-css3-transitions):\\n\\n> \\"_CSS Transitions make style animation trivial for everyone, but they also are a smart performance feature. Because a CSS transition is managed by the browser, the fidelity of its animation can be greatly improved, and in many cases hardware accelerated. Currently WebKit (Chrome, Safari, iOS) have hardware accelerated CSS transforms, but it\'s coming quickly to other browsers and platforms._\\"\\n\\nAdded to this, if you have mobile users then the usage of native functionality (as opposed to doing it manually in JavaScript) actually saves battery life.\\n\\n## I\'m sold - let\'s do it!\\n\\nThis is the CSS we\'ll need:\\n\\n```css\\n.fader {\\n  -moz-transition: opacity 0.7s linear;\\n  -o-transition: opacity 0.7s linear;\\n  -webkit-transition: opacity 0.7s linear;\\n  transition: opacity 0.7s linear;\\n}\\n\\n.fader.fadedOut {\\n  opacity: 0;\\n}\\n```\\n\\nNote we have 2 CSS classes:\\n\\n- `fader` \\\\- if this class is applied to an element then when the opacity of that element is changed it will be an animated change. The duration of the transition and the timing function used are customisable - in this case it takes 0.7 seconds and is linear.\\n- `fadedOut` \\\\- when used in conjunction with `fader` this class creates a fading in or fading out effect as it is removed or applied respectively. (This relies upon the default value of opacity being 1.)\\n\\nLet\'s see it in action:\\n\\n<iframe width=\\"100%\\" height=\\"200\\" src=\\"https://jsfiddle.net/johnny_reilly/86amq/embedded/result,js,html,css\\" allowFullScreen=\\"allowFullScreen\\" frameBorder=\\"0\\"></iframe>\\n\\nIt goes without saying that one day in the not too distant future (I hope) we\'ll be able to leave behind the horrible world of vendor prefixes. Then we\'ll be down to just the single `transition` statement. One day...\\n\\n## Now, a warning...\\n\\nUnfortunately the technique detailed above differs from [`fadeIn`](http://api.jquery.com/fadeIn/) and [`fadeOut`](http://api.jquery.com/fadeOut/) in one important way. When the `fadeOut` animation completes it sets removes the element from the flow of the DOM using `display: none`. However, display is not a property that can be animated and so you can\'t include this in your CSS transition. If removing the element from the flow of the DOM is something you need then you\'ll need to bear this in mind. If anyone has any suggestions for an nice way to approach this I\'d love to hear from you.\\n\\n## A halfway there solution to the `display: none`\\n\\nAndrew Davey tweeted me the suggestion below:\\n\\n> [@johnny_reilly](https://twitter.com/johnny_reilly) Yep, transitions are sweet. You could use the transitionend event to remove the element from the DOM [http://t.co/Q1oWy3g8Lp](http://t.co/Q1oWy3g8Lp)\\n>\\n> \u2014 Andrew Davey (@andrewdavey) [December 5, 2013](https://twitter.com/andrewdavey/statuses/408545283606212608)\\n\\n<script async=\\"\\" src=\\"//platform.twitter.com/widgets.js\\" charSet=\\"utf-8\\"><\/script>\\n\\nSo I thought I\'d give it a go. However, whilst we\'ve a `transitionend` event to play with we don\'t have a corresponding `transitionstart` or `transitionbegin`. So I tried this:\\n\\n```js\\n$(\'#showHideButton\').click(function () {\\n  var $alertDiv = $(\'#alertDiv\');\\n  if ($alertDiv.hasClass(\'fadedOut\')) {\\n    $alertDiv.removeClass(\'fadedOut\').css(\'display\', \'\');\\n  } else {\\n    $(\'#alertDiv\').addClass(\'fadedOut\');\\n  }\\n});\\n\\n$(document).on(\\n  \'webkitTransitionEnd transitionend oTransitionEnd\',\\n  \'.fader\',\\n  function (evnt) {\\n    var $faded = $(evnt.target);\\n    if ($faded.hasClass(\'fadedOut\')) {\\n      $faded.css(\'display\', \'none\');\\n    }\\n  }\\n);\\n```\\n\\nEssentially, on the `transitionend` event `display: none` is applied to the element in question. Groovy. In the absence of a `transitionstart` or `transitionbegin`, when removing the `fadeOut` class I\'m first manually clearing out the `display: none`. Whilst this works in terms of adding it back into the flow of the DOM it takes away all the `fadeIn` gorgeousness. So it\'s not quite the fully featured solution you might hope for. But it\'s a start."},{"id":"/2013/11/26/rolling-your-own-confirm-mechanism","metadata":{"permalink":"/2013/11/26/rolling-your-own-confirm-mechanism","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2013-11-26-rolling-your-own-confirm-mechanism/index.md","source":"@site/blog/2013-11-26-rolling-your-own-confirm-mechanism/index.md","title":"Rolling your own confirm mechanism using Promises and jQuery UI","description":"We\'re here to talk about the confirm dialog. Or, more specifically, how we can make our own confirm dialog.","date":"2013-11-26T00:00:00.000Z","formattedDate":"November 26, 2013","tags":[{"label":"Q","permalink":"/tags/q"},{"label":"jQuery UI","permalink":"/tags/j-query-ui"},{"label":"promises","permalink":"/tags/promises"},{"label":"confirm","permalink":"/tags/confirm"}],"readingTime":4.325,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Rolling your own confirm mechanism using Promises and jQuery UI","authors":"johnnyreilly","tags":["Q","jQuery UI","promises","confirm"],"hide_table_of_contents":false},"prevItem":{"title":"Simple fading in and out using CSS transitions and classes","permalink":"/2013/12/04/simple-fading-in-and-out-using-css-transitions"},"nextItem":{"title":"TypeScript: Don\'t forget Build Action for Implicit Referencing...","permalink":"/2013/11/04/typescript-dont-forget-build-action-for-implicit-referencing"}},"content":"We\'re here to talk about the [confirm](https://developer.mozilla.org/en-US/docs/Web/API/Window.confirm) dialog. Or, more specifically, how we can make our own confirm dialog.\\n\\nJavaScript in the browser has had the `window.confirm` method for the longest time. This method takes a string as an argument and displays it in the form of a dialog, giving the user the option to click on either an \\"OK\\" or a \\"Cancel\\" button. If the user clicks \\"OK\\" the method returns `true`, if the user clicks \\"Cancel\\" the method returns `false`.\\n\\n`window.confirm` is wonderful in one way - it has a simple API which is easy to grok. But regardless of the browser, `window.confirm` is always as ugly as sin. Look at the first picture in this blog post; hideous. Or, put more dispassionately, it\'s not terribly configurable; want to change the button text? You can\'t. Want to change the styling of the dialog? You can\'t. You get the picture.\\n\\n## Making confirm 2.0\\n\\n[jQuery UI\'s dialog](http://jqueryui.com/dialog/#modal-confirmation) has been around for a long time. I\'ve been using it for a long time. But, if you look at the API, you\'ll see it works in a very different way to `window.confirm` \\\\- basically it\'s all about the callbacks. My intention was to create a mechanism which allowed me to prompt the user with jQuery UI\'s tried and tested dialog, but to expose it in a way that embraced the simplicity of the `window.confirm` API.\\n\\nHow to do this? Promises! To quote [Martin Fowler](http://martinfowler.com/bliki/JavascriptPromise.html) (makes you look smart when you do that):\\n\\n> _\\"In Javascript, promises are objects which represent the pending result of an asynchronous operation. You can use these to schedule further activity after the asynchronous operation has completed by supplying a callback.\\"_\\n\\nWhen we show our dialog we are in asynchronous land; waiting for the user to click \\"OK\\" or \\"Cancel\\". When they do, we need to act on their response. So if our custom confirm dialog returns a promise of a boolean (`true` when the users click \\"OK\\", `false` otherwise) then that should be exactly what we need. I\'m going to use [Q](https://github.com/kriskowal/q) for promises. (Nothing particularly special about Q - it\'s one of many [Promises / A+](https://github.com/promises-aplus/promises-spec/blob/master/implementations/index.md) compliant implementations available.)\\n\\nHere\'s my custom confirm dialog:\\n\\n```js\\n/**\\n * Show a \\"confirm\\" dialog to the user (using jQuery UI\'s dialog)\\n *\\n * @param {string} message The message to display to the user\\n * @param {string} okButtonText OPTIONAL - The OK button text, defaults to \\"Yes\\"\\n * @param {string} cancelButtonText OPTIONAL - The Cancel button text, defaults to \\"No\\"\\n * @param {string} title OPTIONAL - The title of the dialog box, defaults to \\"Confirm...\\"\\n * @returns {Q.Promise<boolean>} A promise of a boolean value\\n */\\nfunction confirmDialog(message, okButtonText, cancelButtonText, title) {\\n  okButtonText = okButtonText || \'Yes\';\\n  cancelButtonText = cancelButtonText || \'No\';\\n  title = title || \'Confirm...\';\\n\\n  var deferred = Q.defer();\\n  $(\'<div title=\\"\' + title + \'\\">\' + message + \'</div>\').dialog({\\n    modal: true,\\n    buttons: [\\n      {\\n        // The OK button\\n        text: okButtonText,\\n        click: function () {\\n          // Resolve the promise as true indicating the user clicked \\"OK\\"\\n          deferred.resolve(true);\\n          $(this).dialog(\'close\');\\n        },\\n      },\\n      {\\n        // The Cancel button\\n        text: cancelButtonText,\\n        click: function () {\\n          $(this).dialog(\'close\');\\n        },\\n      },\\n    ],\\n    close: function (event, ui) {\\n      // Destroy the jQuery UI dialog and remove it from the DOM\\n      $(this).dialog(\'destroy\').remove();\\n\\n      // If the promise has not yet been resolved (eg the user clicked the close icon)\\n      // then resolve the promise as false indicating the user did *not* click \\"OK\\"\\n      if (deferred.promise.isPending()) {\\n        deferred.resolve(false);\\n      }\\n    },\\n  });\\n\\n  return deferred.promise;\\n}\\n```\\n\\nWhat\'s happening here? Well first of all, if `okButtonText`, `cancelButtonText` or `title` have false-y values then they are initialised to defaults. Next, we create a deferred object with Q. Then we create our modal dialog using jQuery UI. There\'s a few things worth noting about this:\\n\\n- We\'re not dependent on the dialog markup being in our HTML from the off. We create a brand new element which gets added to the DOM when the dialog is created. (I draw attention to this as the jQuery UI dialog documentation doesn\'t mention that you can use this approach - and frankly I prefer it.)\\n- The \\"OK\\" and \\"Cancel\\" buttons are initialised with the string values stored in `okButtonText` and `cancelButtonText`. So by default, \\"Yes\\" and \\"No\\".\\n- If the user clicks the \\"OK\\" button then the promise is resolved with a value of `true`.\\n- If the dialog closes and the promise has not been resolved then the promise is resolved with a value of `false`. This covers people clicking on the \\"Cancel\\" button as well as closing the dialog through other means.\\n\\nFinally we return the promise from our deferred object.\\n\\n## Going from `window.confirm` to `confirmDialog`\\n\\nIt\'s very simple to move from using `window.confirm` to `confirmDialog`. Take this example:\\n\\n```js\\nif (window.confirm(\'Are you sure?\')) {\\n  // Do something\\n}\\n```\\n\\nBecomes:\\n\\n```js\\nconfirmDialog(\'Are you sure?\').then(function (confirmed) {\\n  if (confirmed) {\\n    // Do something\\n  }\\n});\\n```\\n\\nThere\'s no more to it than that.\\n\\n## And finally a demo...\\n\\nWith the JSFiddle below you can create your own custom dialogs and see the result of clicking on either the \\"OK\\" or \\"Cancel\\" buttons.\\n\\n<iframe width=\\"100%\\" height=\\"500\\" src=\\"https://jsfiddle.net/johnny_reilly/ARWL5/embedded/result,js,html,css\\" allowFullScreen=\\"allowFullScreen\\" frameBorder=\\"0\\"></iframe>"},{"id":"/2013/11/04/typescript-dont-forget-build-action-for-implicit-referencing","metadata":{"permalink":"/2013/11/04/typescript-dont-forget-build-action-for-implicit-referencing","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2013-11-04-typescript-dont-forget-build-action-for-implicit-referencing/index.md","source":"@site/blog/2013-11-04-typescript-dont-forget-build-action-for-implicit-referencing/index.md","title":"TypeScript: Don\'t forget Build Action for Implicit Referencing...","description":"As part of the known breaking changes between 0.9 and 0.9.1 there was this subtle but significant switch:","date":"2013-11-04T00:00:00.000Z","formattedDate":"November 4, 2013","tags":[{"label":"TypeScriptCompile","permalink":"/tags/type-script-compile"},{"label":"BuildAction","permalink":"/tags/build-action"},{"label":"DefinitelyTyped","permalink":"/tags/definitely-typed"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"NuGet","permalink":"/tags/nu-get"}],"readingTime":1.955,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"TypeScript: Don\'t forget Build Action for Implicit Referencing...","authors":"johnnyreilly","tags":["TypeScriptCompile","BuildAction","DefinitelyTyped","TypeScript","NuGet"],"hide_table_of_contents":false},"prevItem":{"title":"Rolling your own confirm mechanism using Promises and jQuery UI","permalink":"/2013/11/26/rolling-your-own-confirm-mechanism"},"nextItem":{"title":"Getting TypeScript Compile-on-Save and Continuous Integration to play nice","permalink":"/2013/10/30/getting-typescript-compile-on-save-and-continous-integration-to-play-nice"}},"content":"As part of the [known breaking changes between 0.9 and 0.9.1](https://typescript.codeplex.com/wikipage?title=Known%20breaking%20changes%20between%200.8%20and%200.9&referringTitle=Documentation) there was this subtle but significant switch:\\n\\n> In Visual Studio, all TypeScript files in a project are considered to be referencing each other\\n>\\n> _Description:_ Previously, all TypeScript files in a project had to reference each other explicitly. With 0.9.1, they now implicitly reference all other TypeScript files in the project. For existing projects that fit multiple projects into a single projects, these will now have to be separate projects.\\n>\\n> _Reason:_ This greatly simplifies using TypeScript in the project context.\\n\\nHaving been [initially resistant](https://typescript.codeplex.com/workitem/1471) to this change I recently decided to give it a try. That is to say I started pulling out the `/// &lt;reference`\'s from my TypeScript files. However, to my surprise, pulling out these references stopped my TypeScript from compiling and killed my Intellisense. After wrestling with this for a couple of hours I finally [filed an issue on the TypeScript CodePlex site](https://typescript.codeplex.com/workitem/1855). (Because clearly the problem was with TypeScript and not how I was using it, right?)\\n\\n## Wrong!\\n\\nWhen I looked through my typing files (\\\\*.d.ts) I found that, pretty much without exception, all had a Build Action of \\"Content\\" and not \\"TypeScriptCompile\\". I went through the project and switched the files over to being \\"TypeScriptCompile\\". This resolved the issue and I was then able to pull out the remaining `/// &lt;reference` comments from the codebase (though I did have to restart Visual Studio to get the Intellisense working).\\n\\nMost, if not all, of the typing files had been pulled in from NuGet and are part of the [DefinitelyTyped](https://github.com/borisyankov/DefinitelyTyped) project on GitHub. Unfortunately, at present, when TypeScript NuGet packages are added they are added without the \\"TypeScriptCompile\\" Build Action. I was going to post an issue there and ask if it\'s possible for NuGet packages to pull in typings files as \\"TypeScriptCompile\\" from the off - fortunately a chap called Natan Vivo [already has](https://github.com/borisyankov/DefinitelyTyped/issues/1138).\\n\\nSo until this issue is resolved it\'s probably a good idea to check that your TypeScript files are set to the correct Build Action in your project. And every time you upgrade your TypeScript NuGet packages double check that you still have the correct Build Action afterwards (and to get Intellisense working in VS 2012 at least you\'ll need to close and re-open the solution as well)."},{"id":"/2013/10/30/getting-typescript-compile-on-save-and-continous-integration-to-play-nice","metadata":{"permalink":"/2013/10/30/getting-typescript-compile-on-save-and-continous-integration-to-play-nice","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2013-10-30-getting-typescript-compile-on-save-and-continous-integration-to-play-nice/index.md","source":"@site/blog/2013-10-30-getting-typescript-compile-on-save-and-continous-integration-to-play-nice/index.md","title":"Getting TypeScript Compile-on-Save and Continuous Integration to play nice","description":"Well sort of... Perhaps this post should more accurately called \\"How to get CI to ignore your TypeScript whilst Visual Studio still compiles it...\\"","date":"2013-10-30T00:00:00.000Z","formattedDate":"October 30, 2013","tags":[{"label":"TFS","permalink":"/tags/tfs"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"Build Server","permalink":"/tags/build-server"},{"label":"Continuous Integration","permalink":"/tags/continuous-integration"}],"readingTime":3.925,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Getting TypeScript Compile-on-Save and Continuous Integration to play nice","authors":"johnnyreilly","tags":["TFS","TypeScript","Build Server","Continuous Integration"],"hide_table_of_contents":false},"prevItem":{"title":"TypeScript: Don\'t forget Build Action for Implicit Referencing...","permalink":"/2013/11/04/typescript-dont-forget-build-action-for-implicit-referencing"},"nextItem":{"title":"Migrating from jquery.validate.unobtrusive.js to jQuery.Validation.Unobtrusive.Native","permalink":"/2013/10/04/migrating-from-jquery.validate.unobtrusive.js-to-jQuery.Validation.Unobtrusive.Native"}},"content":"Well sort of... Perhaps this post should more accurately called \\"How to get CI to ignore your TypeScript whilst Visual Studio still compiles it...\\"\\n\\n## Once there was Web Essentials\\n\\nWhen I first started using TypeScript, I was using it in combination with Web Essentials. Those were happy days. I saved my TS file and Web Essentials would kick off TypeScript compilation. Ah bliss. But the good times couldn\'t last forever and sure enough when version 3.0 of Web Essentials shipped it [pulled support for TypeScript](http://madskristensen.net/post/Web-Essentials-2013-Where-is-the-TypeScript-support).\\n\\nThis made me, [and others](https://typescript.codeplex.com/workitem/1616), very sad. Essentially we were given the choice between sticking with an old version of Web Essentials (2.9 - the last release before 3.0) and keeping our Compile-on-Save \\\\***or**\\\\* keeping with the latest version of Web Essentials and losing it. And since I understood that newer versions of TypeScript had differences in the compiler flags which slightly broke compatibility with WE 2.9 the latter choice seemed the most sensible...\\n\\n## But there is still Compile on Save hope!\\n\\nThe information was that we need not lose our Compile on Save. We just need to follow the instructions [here](https://typescript.codeplex.com/wikipage?title=Compile-on-Save). Or to quote them:\\n\\n> Then additionally add (or replace if you had an older PreBuild action for TypeScript) the following at the end of your project file to include TypeScript compilation in your project.\\n>\\n> ...\\n>\\n> For C#-style projects (.csproj):\\n>\\n> ```xml\\n> <PropertyGroup Condition=\\"\'$(Configuration)\' == \'Debug\'\\">\\n>     <TypeScriptTarget>ES5</TypeScriptTarget>\\n>     <TypeScriptIncludeComments>true</TypeScriptIncludeComments>\\n>     <TypeScriptSourceMap>true</TypeScriptSourceMap>\\n>   </PropertyGroup>\\n>   <PropertyGroup Condition=\\"\'$(Configuration)\' == \'Release\'\\">\\n>     <TypeScriptTarget>ES5</TypeScriptTarget>\\n>     <TypeScriptIncludeComments>false</TypeScriptIncludeComments>\\n>     <TypeScriptSourceMap>false</TypeScriptSourceMap>\\n>   </PropertyGroup>\\n>   <Import Project=\\"$(MSBuildExtensionsPath32)\\\\Microsoft\\\\VisualStudio\\\\v$(VisualStudioVersion)\\\\TypeScript\\\\Microsoft.TypeScript.targets\\" />\\n> ```\\n\\nI followed these instructions (well I had to tweak the `Import Project` location) and I was in business again. But I when I came to check my code into TFS I came unstuck. The automated build kicked off and then, in short order, kicked me:\\n\\n> ```\\n> C:\\\\Builds\\\\1\\\\MyApp\\\\MyApp Continuous Integration\\\\src\\\\MyApp\\\\MyApp.csproj (1520): The imported project \\"C:\\\\Program Files (x86)\\\\MSBuild\\\\Microsoft\\\\VisualStudio\\\\v11.0\\\\TypeScript\\\\Microsoft.TypeScript.targets\\" was not found. Confirm that the path in the <import> declaration is correct, and that the file exists on disk.\\n> C:\\\\Builds\\\\1\\\\MyApp\\\\MyApp Continuous Integration\\\\src\\\\MyApp\\\\MyApp.csproj (1520): The imported project \\"C:\\\\Program Files (x86)\\\\MSBuild\\\\Microsoft\\\\VisualStudio\\\\v11.0\\\\TypeScript\\\\Microsoft.TypeScript.targets\\" was not found. Confirm that the path in the <import> declaration is correct, and that the file exists on disk.\\n> </import></import>\\n> ```\\n\\nThat\'s right, TypeScript wasn\'t installed on the build server. And since TypeScript was now part of the build process my builds were now failing. Ouch.\\n\\n## So what now?\\n\\nI did a little digging and found [this issue report on the TypeScript CodePlex site](https://typescript.codeplex.com/workitem/1518). To quote the issue, it seemed there were 2 possible solutions to get continuous integration and typescript playing nice:\\n\\n1. Install TypeScript on the build server\\n2. Copy the required files for Microsoft.TypeScript.targets to a different source-controlled folder and change the path references in the csproj file to this folder.\\n\\n\\\\#1 wasn\'t an option for us - we couldn\'t install on the build server. And covering both #1 and #2, I wasn\'t particularly inclined to kick off builds on the build server since I was wary of [reported problems with memory leaks](https://typescript.codeplex.com/workitem/1432) etc with the TS compiler. I may feel differently later when TS is no longer in Alpha and has stabilised but it didn\'t seem like the right time.\\n\\n## A solution\\n\\nSo, to sum up, what I wanted was to be able to compile TypeScript in Visual Studio on my machine, and indeed in VS on the machine of anyone else working on the project. But I \\\\***didn\'t**\\\\* want TypeScript compilation to be part of the build process on the server.\\n\\nThe solution in the end was pretty simple - I replaced the `.csproj` changes with the code below:\\n\\n```xml\\n<PropertyGroup Condition=\\"\'$(Configuration)\' == \'Debug\'\\">\\n    <TypeScriptTarget>ES5</TypeScriptTarget>\\n    <TypeScriptRemoveComments>false</TypeScriptRemoveComments>\\n    <TypeScriptSourceMap>false</TypeScriptSourceMap>\\n    <TypeScriptModuleKind>AMD</TypeScriptModuleKind>\\n    <TypeScriptNoImplicitAny>true</TypeScriptNoImplicitAny>\\n  </PropertyGroup>\\n  <PropertyGroup Condition=\\"\'$(Configuration)\' == \'Release\'\\">\\n    <TypeScriptTarget>ES5</TypeScriptTarget>\\n    <TypeScriptRemoveComments>false</TypeScriptRemoveComments>\\n    <TypeScriptSourceMap>false</TypeScriptSourceMap>\\n    <TypeScriptModuleKind>AMD</TypeScriptModuleKind>\\n    <TypeScriptNoImplicitAny>true</TypeScriptNoImplicitAny>\\n  </PropertyGroup>\\n  <Import Project=\\"$(VSToolsPath)\\\\TypeScript\\\\Microsoft.TypeScript.targets\\" Condition=\\"Exists(\'$(VSToolsPath)\\\\TypeScript\\\\Microsoft.TypeScript.targets\')\\" />\\n```\\n\\nWhat this does is enable TypeScript compilation \\\\***only**\\\\* if TypeScript is installed. So when I\'m busy developing with Visual Studio on my machine with the plugin installed I can compile TypeScript. But when I check in the TypeScript compilation is \\\\***not**\\\\* performed on the build server. This is because TypeScript is not installed on the build server and we are only compiling if it is installed. (Just to completely labour the point.)\\n\\n## Final thoughts\\n\\nI do consider this an interim solution. As I mentioned earlier, when TypeScript has stabilised I think I\'d like TS compilation to be part of the build process. Like with any other code I think compiling on check-in to catch bugs early is an excellent idea. But I think I\'ll wait until there\'s some clearer guidance on the topic from the TypeScript team before I take this step."},{"id":"/2013/10/04/migrating-from-jquery.validate.unobtrusive.js-to-jQuery.Validation.Unobtrusive.Native","metadata":{"permalink":"/2013/10/04/migrating-from-jquery.validate.unobtrusive.js-to-jQuery.Validation.Unobtrusive.Native","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2013-10-04-migrating-from-jquery.validate.unobtrusive.js-to-jQuery.Validation.Unobtrusive.Native/index.md","source":"@site/blog/2013-10-04-migrating-from-jquery.validate.unobtrusive.js-to-jQuery.Validation.Unobtrusive.Native/index.md","title":"Migrating from jquery.validate.unobtrusive.js to jQuery.Validation.Unobtrusive.Native","description":"So, you\'re looking at jQuery.Validation.Unobtrusive.Native. You\'re thinking to yourself \\"Yeah, I\'d really like to use the native unobtrusive support in jQuery Validation. But I\'ve already got this app which is using jquery.validate.unobtrusive.js \\\\- actually how easy is switching over?\\" Well I\'m here to tell you that it\'s pretty straightforward - here\'s a walkthrough of how it might be done.","date":"2013-10-04T00:00:00.000Z","formattedDate":"October 4, 2013","tags":[{"label":"migrating","permalink":"/tags/migrating"},{"label":"jquery.validate.unobtrusive.js","permalink":"/tags/jquery-validate-unobtrusive-js"},{"label":"getting started","permalink":"/tags/getting-started"},{"label":"jQuery.Validation.Unobtrusive.Native","permalink":"/tags/j-query-validation-unobtrusive-native"},{"label":"jQuery Validation","permalink":"/tags/j-query-validation"}],"readingTime":3.71,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Migrating from jquery.validate.unobtrusive.js to jQuery.Validation.Unobtrusive.Native","authors":"johnnyreilly","tags":["migrating","jquery.validate.unobtrusive.js","getting started","jQuery.Validation.Unobtrusive.Native","jQuery Validation"],"hide_table_of_contents":false},"prevItem":{"title":"Getting TypeScript Compile-on-Save and Continuous Integration to play nice","permalink":"/2013/10/30/getting-typescript-compile-on-save-and-continous-integration-to-play-nice"},"nextItem":{"title":"Using Bootstrap Tooltips to display jQuery Validation error messages","permalink":"/2013/08/17/using-bootstrap-tooltips-to-display"}},"content":"So, you\'re looking at [jQuery.Validation.Unobtrusive.Native](https://github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native). You\'re thinking to yourself \\"Yeah, I\'d really like to use the native unobtrusive support in jQuery Validation. But I\'ve already got this app which is using [jquery.validate.unobtrusive.js](https://www.nuget.org/packages/jQuery.Validation.Unobtrusive/) \\\\- actually how easy is switching over?\\" Well I\'m here to tell you that it\'s pretty straightforward - here\'s a walkthrough of how it might be done.\\n\\n## I need something to migrate\\n\\nSo let\'s File > New Project ourselves a new MVC 4 application using the Internet Application template. I\'ve picked this template as I know it ships with account registration / login screens in place which make use of jquery.validate.unobtrusive.js. To demo this just run the project, click the \\"Log in\\" link and then click the \\"Log in\\" button.\\n\\nWhat you\'ve just witnessed is jquery.validate.unobtrusive.js doing its thing. Both the `UserName` and `Password` properties on the `LoginModel` are decorated with the `Required` data annotation which, in the above scenario, causes the validation to be triggered on the client thanks to MVC rendering data attributes in the HTML which jquery.validate.unobtrusive.js picks up on. The question is, how can we take the log in screen above and migrate it across to to using jQuery.Validation.Unobtrusive.Native?\\n\\n## Hit me up NuGet!\\n\\nTime to dive into NuGet and install jQuery.Validation.Unobtrusive.Native. We\'ll install the MVC 4 version using this command:\\n\\n```shell\\nInstall-Package jQuery.Validation.Unobtrusive.Native.MVC4\\n```\\n\\nWhat has this done to my project? Well 2 things\\n\\n1. It\'s upgraded jQuery Validation ([jquery.validate.js](http://jqueryvalidation.org/)) from v1.10.0 (the version that is currently part of the MVC 4 template) to v1.11.1 (the latest and greatest jQuery Validation as of the time of writing)\\n2. It\'s added a reference to the jQuery.Validation.Unobtrusive.Native.MVC4 assembly, like so:\\n\\nIn case you were wondering, doing this hasn\'t broken the existing jquery.validate.unobtrusive.js - if you head back to the Log in screen you\'ll still see the same behaviour as before.\\n\\n## Migrating...\\n\\nWe need to switch our TextBox and Password helpers over to using jQuery.Validation.Unobtrusive.Native, which we achieve by simply passing a second argument of `true` to `useNativeUnobtrusiveAttributes`. So we go from this:\\n\\n```cs\\n// ...\\n@Html.TextBoxFor(m => m.UserName)\\n// ...\\n@Html.PasswordFor(m => m.Password)\\n// ...\\n```\\n\\nTo this:\\n\\n```cs\\n// ...\\n@Html.TextBoxFor(m => m.UserName, true)\\n// ...\\n@Html.PasswordFor(m => m.Password, true)\\n// ...\\n```\\n\\nWith these minor tweaks in place the natively supported jQuery Validation data attributes will be rendered into the textbox / password elements instead of the jquery.validate.unobtrusive.js ones.\\n\\nNext lets do the JavaScript. If you take a look at the bottom of the `Login.cshtml` view you\'ll see this:\\n\\n```cs\\n@section Scripts {\\n    @Scripts.Render(\\"~/bundles/jqueryval\\")\\n}\\n```\\n\\nWhich renders the following scripts:\\n\\n```html\\n<script src=\\"/Scripts/jquery.unobtrusive-ajax.js\\"><\/script>\\n<script src=\\"/Scripts/jquery.validate.js\\"><\/script>\\n<script src=\\"/Scripts/jquery.validate.unobtrusive.js\\"><\/script>\\n```\\n\\nIn our brave new world we\'re only going to need jquery.validate.js - so let\'s create ourselves a new bundle in `BundleConfig.cs` which only contains that single file:\\n\\n```cs\\nbundles.Add(new ScriptBundle(\\"~/bundles/jqueryvalnative\\")\\n    .Include(\\"~/Scripts/jquery.validate.js\\"));\\n```\\n\\nTo finish off our migrated screen we need to do 2 things. First we need to switch over the `Login.cshtml` view to only render the jquery.validate.js script (in the form of our new bundle). Secondly, the other thing that jquery.validate.unobtrusive.js did was to trigger validation for the current form. So we need to do that ourselves now. So our finished Scripts section looks like this:\\n\\n```html\\n@section Scripts { @Scripts.Render(\\"~/bundles/jqueryvalnative\\")\\n<script>\\n  $(\'form\').validate();\\n<\/script>\\n}\\n```\\n\\nWhich renders the following script:\\n\\n```html\\n<script src=\\"/Scripts/jquery.validate.js\\"><\/script>\\n<script>\\n  $(\'form\').validate();\\n<\/script>\\n```\\n\\nAnd, pretty much, that\'s it. If you run the app now and go to the Log in screen and try to log in without credentials.\\n\\nWhich is functionally exactly the same as previously. The eagle eyed will notice some styling differences but that\'s all it comes down to really; style. And if you were so inclined you could easily style this up as you liked using CSS and the options you can pass to jQuery Validation (in fact a quick rummage through jquery.validate.unobtrusive.js should give you everything you need).\\n\\n## Rounding off\\n\\nBefore I sign off I\'d like to illustrate how little we\'ve had to change the code to start using jQuery.Validation.Unobtrusive.Native.\\n\\nAs you see, it takes very little effort to migrate from one approach to the other. And it\'s \\\\***your**\\\\* choice. If you want to have one screen that uses jQuery.Validation.Unobtrusive.Native and one screen that uses jquery.validation.unobtrusive.js then you can! Including jQuery.Validation.Unobtrusive.Native in your project gives you the **option** to use it. It doesn\'t force you to, you can do so as you need to and when you want to. It\'s down to you."},{"id":"/2013/08/17/using-bootstrap-tooltips-to-display","metadata":{"permalink":"/2013/08/17/using-bootstrap-tooltips-to-display","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2013-08-17-using-bootstrap-tooltips-to-display/index.md","source":"@site/blog/2013-08-17-using-bootstrap-tooltips-to-display/index.md","title":"Using Bootstrap Tooltips to display jQuery Validation error messages","description":"I love jQuery Validation. I was recently putting together a screen which had a lot of different bits of validation going on. And the default jQuery Validation approach of displaying the validation messages next to the element being validated wasn\'t working for me. That is to say, because of the amount of elements on the form, the appearance of validation messages was really making a mess of the presentation. So what to do?","date":"2013-08-17T00:00:00.000Z","formattedDate":"August 17, 2013","tags":[{"label":"Tooltip","permalink":"/tags/tooltip"},{"label":"Bootstrap","permalink":"/tags/bootstrap"},{"label":"jQuery Validation","permalink":"/tags/j-query-validation"}],"readingTime":1.225,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Using Bootstrap Tooltips to display jQuery Validation error messages","authors":"johnnyreilly","tags":["Tooltip","Bootstrap","jQuery Validation"],"hide_table_of_contents":false},"prevItem":{"title":"Migrating from jquery.validate.unobtrusive.js to jQuery.Validation.Unobtrusive.Native","permalink":"/2013/10/04/migrating-from-jquery.validate.unobtrusive.js-to-jQuery.Validation.Unobtrusive.Native"},"nextItem":{"title":"Announcing jQuery Validation Unobtrusive Native...","permalink":"/2013/08/08/announcing-jquery-validation"}},"content":"I love jQuery Validation. I was recently putting together a screen which had a lot of different bits of validation going on. And the default jQuery Validation approach of displaying the validation messages next to the element being validated wasn\'t working for me. That is to say, because of the amount of elements on the form, the appearance of validation messages was really making a mess of the presentation. So what to do?\\n\\n## Tooltips to the rescue!\\n\\nI was chatting to [Marc Talary](https://plus.google.com/u/0/116859810359377785616/posts) about this and he had the bright idea of using tooltips to display the error messages. Tooltips would allow the existing presentation of the form to remain as is whilst still displaying the messages to the users. Brilliant idea!\\n\\nAfter a certain amount of fiddling I came up with a fairly solid mechanism for getting jQuery Validation to display error messages as tooltips which I\'ll share here. It\'s worth saying that for the application that Marc and I were working on we already had [jQuery UI](http://jqueryui.com/) in place and so we decided to use the [jQuery UI tooltip](http://jqueryui.com/tooltip/). This example will use the [Bootstrap tooltip](http://getbootstrap.com/javascript/#tooltips) instead. As much as anything else this demonstrates that you could swap out the tooltip mechanism here with any of your choosing.\\n\\n<iframe src=\\"https://htmlpreview.github.io/?https://gist.github.com/johnnyreilly/5867188/raw/2543a12fbd5c0aaad1da6793b7a7437492be3baf/DemoTooltip.html\\" width=\\"100%\\" height=\\"350\\"></iframe>\\n\\nBeautiful isn\'t it? Now look at the source:\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/5867188.js?file=DemoTooltip.html\\"><\/script>\\n\\nAll the magic is in the JavaScript, specifically the `showErrors` function that\'s passed as an option to jQuery Validation. Enjoy!"},{"id":"/2013/08/08/announcing-jquery-validation","metadata":{"permalink":"/2013/08/08/announcing-jquery-validation","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2013-08-08-announcing-jquery-validation/index.md","source":"@site/blog/2013-08-08-announcing-jquery-validation/index.md","title":"Announcing jQuery Validation Unobtrusive Native...","description":"I\'ve been busy working on an open source project called jQuery Validation Unobtrusive Native. To see it in action take a look here.","date":"2013-08-08T00:00:00.000Z","formattedDate":"August 8, 2013","tags":[],"readingTime":2.29,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Announcing jQuery Validation Unobtrusive Native...","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false},"prevItem":{"title":"Using Bootstrap Tooltips to display jQuery Validation error messages","permalink":"/2013/08/17/using-bootstrap-tooltips-to-display"},"nextItem":{"title":"How I\'m Using Cassette part 3:Cassette and TypeScript Integration","permalink":"/2013/07/06/how-im-using-cassette-part-3-typescript"}},"content":"I\'ve been busy working on an open source project called **[jQuery Validation Unobtrusive Native](http://github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native)**. [To see it in action take a look here](http://johnnyreilly.github.io/jQuery.Validation.Unobtrusive.Native/).\\n\\n## A Little Background\\n\\nI noticed a little while ago that jQuery Validation was now providing native support for validation driven by HTML 5 data attributes. As you may be aware, Microsoft shipped [jquery.validate.unobtrusive.js](http://bradwilson.typepad.com/blog/2010/10/mvc3-unobtrusive-validation.html) back with MVC 3. ([I have written about it before.](http://icanmakethiswork.blogspot.com/2012/08/jquery-unobtrusive-validation.html)) It provided a way to apply data model validations to the client side using a combination of jQuery Validation and HTML 5 data attributes.\\n\\nThe principal of this was and is fantastic. But since that time the jQuery Validation project has implemented its own support for driving validation unobtrusively (shipping with [jQuery Validation 1.11.0](http://jquery.bassistance.de/validate/changelog.txt)). I\'ve been looking at a way to directly use the native support instead of jquery.validate.unobtrusive.js.\\n\\n## So... What is jQuery Validation Unobtrusive Native?\\n\\njQuery Validation Unobtrusive Native is a collection of ASP.Net MVC HTML helper extensions. These make use of jQuery Validation\'s native support for validation driven by HTML 5 data attributes. The advantages of the native support over jquery.validate.unobtrusive.js are:\\n\\n- Dynamically created form elements are parsed automatically. jquery.validate.unobtrusive.js does not support this whilst jQuery Validation does. [Take a look at a demo using Knockout.](http://johnnyreilly.github.io/jQuery.Validation.Unobtrusive.Native/AdvancedDemo/Knockout.html)\\n- jquery.validate.unobtrusive.js restricts how you use jQuery Validation. If you want to use showErrors or something similar then you may find that you need to go native (or at least you may find that significantly easier than working with the jquery.validate.unobtrusive.js defaults)...\\n- Send less code to your browser, make your browser to do less work and even get a (marginal) performance benefit .\\n\\nThis project intends to be a bridge between MVC\'s inbuilt support for driving validation from data attributes and jQuery Validation\'s native support for the same. This is achieved by hooking into the MVC data attribute creation mechanism and using it to generate the data attributes natively supported by jQuery Validation.\\n\\n## Future Plans\\n\\nSo far the basic set of the HtmlHelpers and their associated unobtrusive mappings have been implemented. If any have been missed then let me know. As time goes by I intend to:\\n\\n- fill in any missing gaps there may be\\n- maintain MVC 3, 4 (and when the time comes 5+) versions of this on Nuget\\n- not all data annotations generate client data attributes - if it makes sense I may look to implement some of these where it seems sensible. (eg the [MinLengthAttribute](http://msdn.microsoft.com/en-us/library/system.componentmodel.dataannotations.minlengthattribute.aspx) annotation could be mapped to [minlength](http://jqueryvalidation.org/minlength-method/) validation...)\\n- get the unit test coverage to a good level and finally (and perhaps most importantly)\\n- create some really useful [demos and documentation](http://johnnyreilly.github.io/jQuery.Validation.Unobtrusive.Native/Demo.html).\\n\\nHelp is appreciated so feel free to pitch in! You can find the project on GitHub [here](http://github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native)..."},{"id":"/2013/07/06/how-im-using-cassette-part-3-typescript","metadata":{"permalink":"/2013/07/06/how-im-using-cassette-part-3-typescript","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2013-07-06-how-im-using-cassette-part-3-typescript/index.md","source":"@site/blog/2013-07-06-how-im-using-cassette-part-3-typescript/index.md","title":"How I\'m Using Cassette part 3:Cassette and TypeScript Integration","description":"The modern web is JavaScript. There\'s no two ways about it. HTML 5 has new CSS, new HTML but the most important aspect of it from an application development point of view is JavaScript. It\'s the engine. Without it HTML 5 wouldn\'t be the exciting application platform that it is. Half the posts on Hacker News would vanish.","date":"2013-07-06T00:00:00.000Z","formattedDate":"July 6, 2013","tags":[{"label":"Andrew Davey","permalink":"/tags/andrew-davey"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"javascript","permalink":"/tags/javascript"},{"label":"cassette","permalink":"/tags/cassette"},{"label":"Web Essentials","permalink":"/tags/web-essentials"}],"readingTime":5.39,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"How I\'m Using Cassette part 3:Cassette and TypeScript Integration","authors":"johnnyreilly","tags":["Andrew Davey","TypeScript","javascript","cassette","Web Essentials"],"hide_table_of_contents":false},"prevItem":{"title":"Announcing jQuery Validation Unobtrusive Native...","permalink":"/2013/08/08/announcing-jquery-validation"},"nextItem":{"title":"jQuery Validate - Native Unobtrusive Validation Support!","permalink":"/2013/06/26/jquery-validate-native-unobtrusive-validation"}},"content":"The modern web is JavaScript. There\'s no two ways about it. HTML 5 has new CSS, new HTML but the most important aspect of it from an application development point of view is JavaScript. It\'s the engine. Without it HTML 5 wouldn\'t be the exciting application platform that it is. Half the posts on [Hacker News](https://news.ycombinator.com/) would vanish.\\n\\nIt\'s easy to break a JavaScript application. One false keypress and you can mysteriously turn a fully functioning app into toast. And not know why. There\'s tools you can use to help yourself - [JSHint / JSLint](http://icanmakethiswork.blogspot.co.uk/2012/04/jshint-customising-your-hurt-feelings.html) but whilst these make error detection a little easier it remains very easy to shoot yourself in the foot with JavaScript. Because of this I\'ve come to really rather love [TypeScript](http://www.typescriptlang.org/). If you didn\'t already know, TypeScript can be summed up as JavaScript with optional static typing. It\'s a **_superset_** of JavaScript - JavaScript with go-faster stripes. When run through the compiler TypeScript is [transpiled](https://en.wikipedia.org/wiki/Source-to-source_compiler) into JavaScript. And importantly, if you have bugs in your code, the compiler should catch them at this point and let you know.\\n\\nNow very few of us are working on greenfield applications. Most of us have existing applications to maintain and support. Happily, TypeScript fits very well with this purely because TypeScript is a superset of JavaScript. That is to say: all JavaScript is valid TypeScript in the same way that all CSS is valid [LESS](http://lesscss.org/). This means that you can take an existing `.js` file, rename it to have a `.ts` suffix, run the TypeScript compiler over it and out will pop your JavaScript file just as it was before. You\'re then free to enrich your TypeScript file with the relevant type annotations at your own pace. Increasing the robustness of your codebase is a choice left to you.\\n\\nThe project I am working on has recently started to incorporate TypeScript. It\'s an ASP.Net MVC 4 application which makes use of [Knockout](http://knockoutjs.com/). The reason we started to incorporate TypeScript is because certain parts of the app, particularly the Knockout parts, were becoming more complex. This complexity wasn\'t really an issue when we were writing the relevant JavaScript. However, when it came to refactoring and handing files from one team member to another we realised it was very easy to introduce bugs into the codebase, particularly around the JavaScript. Hence TypeScript.\\n\\n## Cassette and TypeScript\\n\\nEnough of the pre-amble. The project was making use of Cassette for serving up its CSS and JavaScript. Because Cassette rocks. One of the reasons we use it is that we\'re making extensive use of [Cassette\'s ability to serve scripts in dependency order](http://icanmakethiswork.blogspot.co.uk/2013/06/how-im-using-cassette-part-2.html). So if we were to move to using TypeScript it was important that TypeScript and Cassette would play well together.\\n\\nI\'m happy to report that Cassettes and TypeScript do work well together, but there are a few things that you need to get up and running. Or, to be a little clearer, if you want to make use of Cassette\'s in-file Asset Referencing then you\'ll need to follow these steps. If you don\'t need Asset Referencing then you\'ll be fine using Cassette with TypeScript generated JavaScript as is \\\\***provided**\\\\* you ensure the TypeScript compiler is not preserving comments in the generated JavaScript.\\n\\n## The Fly in the Ointment: Asset References\\n\\nTypeScript is designed to allow you to break up your application into modules. However, the referencing mechanism which allows you to reference one TypeScript file / module from another is exactly the same as the existing Visual Studio XML reference comments mechanism that was originally introduced to drive JavaScript Intellisense in Visual Studio. To quote the [TypeScript spec](http://www.typescriptlang.org/Content/TypeScript%20Language%20Specification.pdf):\\n\\n- _A comment of the form /// <reference path=\\"\u2026\\"/> adds a dependency on the source file specified in the path argument. The path is resolved relative to the directory of the containing source file._\\n- _An external import declaration that specifies a relative external module name (section 11.2.1) resolves the name relative to the directory of the containing source file. If a source file with the resulting path and file extension \u2018.ts\u2019 exists, that file is added as a dependency. Otherwise, if a source file with the resulting path and file extension \u2018.d.ts\u2019 exists, that file is added as a dependency._\\n\\nThe problem is that [Cassette \\\\***also**\\\\* supports Visual Studio XML reference comments to drive Asset References](http://getcassette.net/documentation/v1/AssetReferences). The upshot of this is, that Cassette will parse the `/// &lt;reference path=\\"*.ts\\"/&gt;`s and will attempt to serve up the TypeScript files in the browser... Calamity!\\n\\n## Pulling the Fly from the Ointment\\n\\nAgain I\'m going to take the demo from last time ([the References branch of my CassetteDemo project](https://github.com/johnnyreilly/CassetteDemo/tree/References)) and build on top of it. First of all, we need to update the Cassette package. This is because to get Cassette working with TypeScript you need to be running at least Cassette 2.1. So let\'s let NuGet do it\'s thing:\\n\\n`Update-Package Cassette.Aspnet`\\n\\nAnd whilst we\'re at it let\'s grab the jQuery TypeScript typings - we\'ll need them later:\\n\\n`Install-Package jquery.TypeScript.DefinitelyTyped`\\n\\nNow we need to add a couple of classes to the project. First of all this:\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/5934706.js?file=ParseJavaScriptNotTypeScriptReferences.cs\\"><\/script>\\n\\nWhich subclasses `ParseJavaScriptReferences` and ensures TypeScript files are excluded when JavaScript references are being parsed. And to make sure that Cassette makes use of `ParseJavaScriptNotTypeScriptReferences` in place of `ParseJavaScriptReferences` we need this:\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/5934706.js?file=InsertIntoPipelineParseJavaScriptNotTypeScriptReferences.cs\\"><\/script>\\n\\nNow we\'re in a position to use TypeScript with Cassette. To demonstrate this let\'s take the `Index.js` and rename it to `Index.ts`. And now it\'s TypeScript. However before it can compile it needs to know what jQuery is - so we drag in the jQuery typings from [Definitely Typed](http://github.com/borisyankov/DefinitelyTyped). And now it can compile from this:\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/5934706.js?file=Index.ts\\"><\/script>\\n\\nTo this: (Please note that I get the TypeScript compiler to preserve my comments in order that I can continue to use Cassettes Asset Referencing)\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/5934706.js?file=Index.js\\"><\/script>\\n\\nAs you can see the output JavaScript has both the TypeScript and the Cassette references in place. However thanks to `ParseJavaScriptNotTypeScriptReferences` those TypeScript references will be ignored by Cassette.\\n\\nSo that\'s it - we\'re home free. Before I finish off I\'d like to say thanks to Cassette\'s [Andrew Davey](http://twitter.com/andrewdavey) who [set me on the right path](https://groups.google.com/forum/?fromgroups=#!topic/cassette/SM3Rxh48D7Q) when trying to work out how to do this. A thousand thank yous Andrew!\\n\\nAnd finally, again as last time you can see what I\'ve done in this post by just looking at the repository on [GitHub](https://github.com/johnnyreilly/CassetteDemo/tree/TypeScript). The changes I made are on the TypeScript branch of that particular repository."},{"id":"/2013/06/26/jquery-validate-native-unobtrusive-validation","metadata":{"permalink":"/2013/06/26/jquery-validate-native-unobtrusive-validation","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2013-06-26-jquery-validate-native-unobtrusive-validation/index.md","source":"@site/blog/2013-06-26-jquery-validate-native-unobtrusive-validation/index.md","title":"jQuery Validate - Native Unobtrusive Validation Support!","description":"Did you know that jQuery Validate natively supports the use of HTML 5 data attributes to drive validation unobtrusively? Neither did I - I haven\'t seen any documentation for it. However, I was reading the jQuery Validate test suite and that\'s what I spotted being used in some of the tests.","date":"2013-06-26T00:00:00.000Z","formattedDate":"June 26, 2013","tags":[{"label":"Native","permalink":"/tags/native"},{"label":"jQuery Validate","permalink":"/tags/j-query-validate"},{"label":"Unobtrusive","permalink":"/tags/unobtrusive"}],"readingTime":2.28,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"jQuery Validate - Native Unobtrusive Validation Support!","authors":"johnnyreilly","tags":["Native","jQuery Validate","Unobtrusive"],"hide_table_of_contents":false},"prevItem":{"title":"How I\'m Using Cassette part 3:Cassette and TypeScript Integration","permalink":"/2013/07/06/how-im-using-cassette-part-3-typescript"},"nextItem":{"title":"How I\'m Using Cassette part 2:Get Cassette to Serve Scripts in Dependency Order","permalink":"/2013/06/06/how-im-using-cassette-part-2"}},"content":"Did you know that jQuery Validate natively supports the use of [HTML 5 data attributes](http://ejohn.org/blog/html-5-data-attributes/) to drive validation unobtrusively? Neither did I - I haven\'t seen any documentation for it. However, I was reading the [jQuery Validate test suite](https://github.com/jzaefferer/jquery-validation/blob/master/test/index.html) and that\'s what I spotted being used in some of the tests.\\n\\nI was quite keen to give it a try as I\'ve found the Microsoft produced [unobtrusive extensions](http://nuget.org/packages/jQuery.Validation.Unobtrusive/) both fantastic and frustrating in nearly equal measure. Fantastic because they work and they\'re [integrated nicely with MVC](http://icanmakethiswork.blogspot.co.uk/2012/08/jquery-unobtrusive-validation.html). Frustrating, because they don\'t allow you do all the things that jQuery Validate in the raw does.\\n\\nSo when I realised that there was native alternative available I was delighted. Enough with the fine words - what we want is a demo:\\n\\n<iframe src=\\"https://htmlpreview.github.io/?http://gist.github.com/johnnyreilly/5867188/raw/272b1b42f4773fe6df843550b3e3d457013522a8/Demo.html\\" width=\\"100%\\" height=\\"575\\"></iframe>\\n\\nNot particularly exciting? Not noticably different to any other jQuery Validate demo you\'ve ever seen? Fair enough. Now look at the source:\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/5867188.js?file=Demo.html\\"><\/script>\\n\\nDo you see what I see? Data attributes (both `data-rule-*` and `data-msg-*`s) being used to drive the validation unobtrusively! And if you look at the JavaScript files referenced you will see \\\\***no sign**\\\\* of `jquery.validate.unobtrusive.js` \\\\- this is all raw jQuery Validate. Nothing else.\\n\\n## Why is this useful?\\n\\nFirst of all, I\'m of the opinion that it makes intuitive sense to have the validation information relevant to various DOM elements stored directly with those DOM elements. There will be occasions where you may not want to use this approach but, in the main, I think it\'s very sensible. It saves you bouncing back and forth between your HTML and your JavaScript and it means when you read the HTML you know there and then what validation applies to your form.\\n\\nI think this particularly applies when it comes to adding elements to the DOM dynamically. If I use data attributes to drive my validation and I dynamically add elements then jQuery Validate will parse the validation rules for me. I won\'t have to subsequently apply validation to those new elements once they\'ve been added to the DOM. 1 step instead of 2. It makes for simpler code and that\'s always a win.\\n\\n## Wrapping up\\n\\nFor myself I\'m in the early stages of experimenting with this but I thought it might be good to get something out there to show how this works. If anyone knows of any official documentation for this please do let me know - I\'d love to have a read of it. Maybe it\'s been out there all along and it\'s just my Googling powers are inadequate.\\n\\n## Update 09/08/2012\\n\\nIf you\'re using ASP.Net MVC 3+ and this post has been of interest to you then you might want to take a look at [this](http://icanmakethiswork.blogspot.co.uk/2013/08/announcing-jquery-validation.html)."},{"id":"/2013/06/06/how-im-using-cassette-part-2","metadata":{"permalink":"/2013/06/06/how-im-using-cassette-part-2","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2013-06-06-how-im-using-cassette-part-2/index.md","source":"@site/blog/2013-06-06-how-im-using-cassette-part-2/index.md","title":"How I\'m Using Cassette part 2:Get Cassette to Serve Scripts in Dependency Order","description":"Last time I wrote about Cassette I was talking about how to generally get up and running. How to use Cassette within an ASP.Net MVC project. What I want to write about now is (in my eyes) the most useful feature of Cassette by a country mile. This is Cassettes ability to ensure scripts are served in dependency order.","date":"2013-06-06T00:00:00.000Z","formattedDate":"June 6, 2013","tags":[{"label":"script references","permalink":"/tags/script-references"},{"label":"RequireJS","permalink":"/tags/require-js"},{"label":"cassette","permalink":"/tags/cassette"}],"readingTime":7.16,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"How I\'m Using Cassette part 2:Get Cassette to Serve Scripts in Dependency Order","authors":"johnnyreilly","tags":["script references","RequireJS","cassette"],"hide_table_of_contents":false},"prevItem":{"title":"jQuery Validate - Native Unobtrusive Validation Support!","permalink":"/2013/06/26/jquery-validate-native-unobtrusive-validation"},"nextItem":{"title":"How I\'m Using Cassette part 1:Getting Up and Running","permalink":"/2013/05/04/how-im-using-cassette"}},"content":"[Last time](http://icanmakethiswork.blogspot.co.uk/2013/05/how-im-using-cassette.html) I wrote about Cassette I was talking about how to generally get up and running. How to use Cassette within an ASP.Net MVC project. What I want to write about now is (in my eyes) the most useful feature of Cassette by a country mile. This is Cassettes ability to ensure scripts are served in dependency order.\\n\\n## Why does this matter?\\n\\nYou might well ask. If we go back 10 years or so then really this wasn\'t a problem. No-one was doing a great deal with JavaScript. And if they did anything it tended to be code snippets in amongst the HTML; nothing adventurous. But unless you\'ve had your head in the sand for the last 3 years then you will have clearly noticed that JavaScript is in rude health and being used for all kinds of things you\'d never have imagined. In fact some would have it that it\'s the [assembly language of the web](http://www.hanselman.com/blog/JavaScriptisAssemblyLanguagefortheWebPart2MadnessorjustInsanity.aspx).\\n\\nFor my part, I\'ve been doing more and more with JavaScript. And as I do more and more with it I seek to modularise my code; ([like any good developer would](http://en.wikipedia.org/wiki/Separation_of_concerns)) breaking it up into discrete areas of functionality. I aim to only serve up the JavaScript that I need on a given page. And that would be all well and good but for one of the languages shortcomings. Modules. JavaScript doesn\'t yet have a good module loading story to tell. (Apparently one\'s coming in [EcmaScript 6](http://wiki.ecmascript.org/doku.php?id=harmony:modules)). (I don\'t want to get diverted into this topic as it\'s a big area. But if you\'re interested then you can read up a little on different approaches being used [here](http://requirejs.org/docs/whyamd.html#today). The ongoing contest between RequireJS and CommonJS frankly makes me want to keep my distance for now.)\\n\\n## It Depends\\n\\nBack to my point, JavaScripts native handling of script dependencies is non-existent. It\'s real \\"here be dragons\\" territory. If you serve up, for example, Slave.js that depends on things set up in Master.js before you\'ve actually served up Master.js, well it\'s not a delightful debugging experience. The errors tend be obscure and it\'s not always obvious what the correct ordering should be.\\n\\nNaturally this creates something of a headache around my own JavaScript modules. A certain amount of jiggery-pokery is required to ensure that scripts are served in the correct order so that they run as expected. And as your application becomes more complicated / modular, the number of problems around this area increase exponentially. It\'s **really** tedious. I don\'t want to be thinking about managing that as I\'m developing - I want to be focused on solving the problem at hand.\\n\\nIn short, what I want to do is reference a script file somewhere in my server-side pipeline. I could be in a view, a layout, a controller, a partial view, a HTML helper... - I just want to know that that script is going to turn up at the client in the right place in the HTML so it works. Always. And I don\'t want to have to think about it any further than that.\\n\\n## Enter Cassette, riding a white horse\\n\\nAnd this is where Cassette takes the pain away. To quote the documentation:\\n\\n> \\"_Some assets must be included in a page before others. For example, your code may use jQuery, so the jQuery script must be included first. Cassette will sort all assets based on references they declare._\\"\\n\\nJust the ticket!\\n\\n## Declaring References Server-Side\\n\\nWhat does this look like in reality? Let\'s build on what I did last time to demonstrate how I make use of Asset References to ensure my scripts turn up in the order I require.\\n\\nIn my `_Layout.cshtml` file I\'m going to remove the following reference from the head of the file:\\n\\n`Bundles.Reference(\\"~/bundles/core\\");`\\n\\nI\'m pulling this out of my layout page because it\'s presence means that **every** page MVC serves up is also serving up jQuery and jQuery UI (which is what `~/bundles/core` is). If a page doesn\'t actually make use of jQuery and / or jQuery UI then there\'s no point in doing this.\\n\\n\\"_But wait!_\\", I hear you cry, \\"_Haven\'t you just caused a bug with your reckless action? I distinctly recall that the `Login.cshtml` page has the following code in place:_\\"\\n\\n`Bundles.Reference(\\"~/bundles/validate\\");`\\n\\n\\"_And now with your foolhardy, nay, reckless attitude to the `~/bundles/core` bundle you\'ve broken your Login screen. How can jQuery Validation be expected to work if there\'s no jQuery there to extend?_\\"\\n\\nWell, I understand your concerns but really you needn\'t worry - Cassette\'s got my back. Look closely at the code below:\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/5693071.js?file=ReferencesValidateDependesOnCoreCassetteConfiguration.cs\\"><\/script>\\n\\nSee it? The `~/bundles/validate` bundle declares a reference to the `~/bundles/core` bundle. The upshot of this is, that if you tell Cassette to reference `~/bundles/validate` it will ensure that before it renders that bundle it first renders any bundles that bundle depends on (in this case the `~/bundles/core` bundle).\\n\\nThis is a very simple demonstration of the feature but I can\'t underplay just how useful I find this.\\n\\n## Declaring References in your JavaScript itself\\n\\nAnd the good news doesn\'t stop there. Let\'s say you **don\'t** want to maintain your references in a separate file. You\'d rather declare references inside your JavaScript files themselves. Well - you can. Cassette caters for this through the usage of [Asset References](http://getcassette.net/documentation/v1/AssetReferences).\\n\\nLet\'s demo this. First of all add the following file at this location in the project: `~/Scripts/Views/Home/Index.js`\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/5693071.js?file=Index.js\\"><\/script>\\n\\nThe eagle-eyed amongst you will have noticed\\n\\n1. I\'m mirroring the MVC folder structure inside the Scripts directory. (There\'s nothing special about that by the way - it\'s just a file structure I\'ve come to find useful. It\'s very easy to find the script associated with a View if the scripts share the same organisational approach as the Views.).\\n2. The purpose of the script is very simple, it fades out the main body of the screen, re-writes the HTML in that tag and then fades back in. It\'s purpose is just to do something that is obvious to the user - so they can see the evidence of JavaScript executing.\\n3. Lastly and most importantly, do you notice that `// @reference ~/bundles/core` is the first line of the file? This is our script reference. It\'s this that Cassette will be reading to pick up references.\\n\\nTo make sure Cassette is picking up our brand new file let\'s take a look at `CassetteConfiguration.cs` and uncomment the line of code below:\\n\\n`bundles.AddPerIndividualFile<scriptbundle>(\\"~/Scripts/Views\\");</scriptbundle>`\\n\\nWith this in place Cassette will render out a bundle for each script in the Views subdirectory. Let\'s see if it works. Add the following reference to our new JavaScript file in `~/Views/Home/Index.cshtml`:\\n\\n`Bundles.Reference(\\"~/Scripts/Views/Home/Index.js\\");`\\n\\nIf you browse to the home page of the application this is what you should now see:\\n\\n![](https://2.bp.blogspot.com/-tGZTEhhkGz8/Ua7xlgl3n5I/AAAAAAAAAcs/miNZsysrJeY/s320/Index.js.png)\\n\\nWhat this means is, `Index.js` was served up by Cassette. And more importantly before `Index.js` was served the referenced `~/bundles/core` was served too.\\n\\n## Avoiding the Gotcha\\n\\nThere is a gotcha which I\'ve discovered whilst using Cassette\'s Asset References. Strictly speaking it\'s a Visual Studio gotcha rather than a Cassette gotcha. It concerns Cassette\'s support for Visual Studio XML style reference comments. In the example above I could have written this:\\n\\n`/// &lt;reference path=\\"~/bundles/core\\" /&gt;`\\n\\nInstead of this:\\n\\n`// @reference ~/bundles/core`\\n\\nIt would fulfil exactly the same purpose and would work identically. But there\'s a problem. Using Visual Studio XML style reference comments to refer to Cassette bundles appears to trash the Visual Studios JavaScript Intellisense. You\'ll lose the Intellisense that\'s driven by `~/Scripts/_references.js` in VS 2012. So if you value your Intellisense (and I do) my advice is to stick to using the standard Cassette references style instead.\\n\\n## Go Forth and Reference\\n\\nThere is also support in Cassette for CSS referencing (as well as other types of referencing relating to LESS and even CoffeeScript). I haven\'t made use of CSS referencing myself as, in stark contrast to my JS, my CSS is generally one bundle of styles which I\'m happy to be rendered on each page. But it\'s nice to know the option is there if I wanted it.\\n\\nFinally, as last time you can see what I\'ve done in this post by just looking at the repository on [GitHub](https://github.com/johnnyreilly/CassetteDemo/tree/References). The changes I made are on the References branch of that particular repository.\\n\\n\x3c!-- I don\'t want to serve up a monster JavaScript payload with each screen refresh.  Quite besides anything else, if I did that each screen refresh would be slower as more JavaScript was served up and parsed - the UX would suffer.  I don\'t want that.  I want *<strong>performance</strong>*! --\x3e"},{"id":"/2013/05/04/how-im-using-cassette","metadata":{"permalink":"/2013/05/04/how-im-using-cassette","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2013-05-04-how-im-using-cassette/index.md","source":"@site/blog/2013-05-04-how-im-using-cassette/index.md","title":"How I\'m Using Cassette part 1:Getting Up and Running","description":"Backing into the light","date":"2013-05-04T00:00:00.000Z","formattedDate":"May 4, 2013","tags":[{"label":"asp.net mvc","permalink":"/tags/asp-net-mvc"},{"label":"Andrew Davey","permalink":"/tags/andrew-davey"},{"label":"cassette","permalink":"/tags/cassette"}],"readingTime":5.395,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"How I\'m Using Cassette part 1:Getting Up and Running","authors":"johnnyreilly","tags":["asp.net mvc","Andrew Davey","cassette"],"hide_table_of_contents":false},"prevItem":{"title":"How I\'m Using Cassette part 2:Get Cassette to Serve Scripts in Dependency Order","permalink":"/2013/06/06/how-im-using-cassette-part-2"},"nextItem":{"title":"A navigation animation (for your users delectation)","permalink":"/2013/04/26/a-navigation-animation-for-your-users"}},"content":"## Backing into the light\\n\\nFor a while now, I\'ve been seeking a bulletproof way to handle the following scenarios... all at the same time in the context of an ASP.Net MVC application:\\n\\n1. How to serve full-fat JavaScript in debug mode and minified in release mode\\n2. When debugging, ensure that the full-fat JS being served is definitely the latest version; and \\\\***not**\\\\* from the cache. (The time I\'ve wasted due to [304\'s](http://en.wikipedia.org/wiki/List_of_HTTP_status_codes#304)...)\\n3. How to add Javascript assets that need to be served up from any point in an ASP.Net MVC application (including views, layouts, partial views... even controllers if so desired) whilst preventing duplicate scripts from being served.\\n4. How to ensure that Javascript files are served up last to any web page to ensure a speedy feel to users (don\'t want JS blocking rendering).\\n5. And last but certainly not least the need to load Javascript files in dependency order. If `myView.js` depends on jQuery then clearly `jQuery-latest.js` needs to be served before `myView.js`.\\n\\nNow the best, most comprehensive and solid looking solution to this problem has for some time seemed to me to be [Andrew Davey\'s](http://aboutcode.net/)[Cassette](http://getcassette.net/). This addresses all my issues in one way or another, as well as bringing in a raft of other features (support for Coffeescript etc).\\n\\nHowever, up until now I\'ve slightly shied away from using Cassette as I was under the impression it had a large number of dependencies. That doesn\'t appear to be the case at all. I also had some vague notion that I could quite simply build my own solution to these problems making use of Microsoft\'s [Web Optimization](http://nuget.org/packages/Microsoft.AspNet.Web.Optimization/1.0.0) which nicely handles my #1 problem above. However, looking again at the documentation Cassette was promising to handle scenarios #1 - #5 without breaking sweat. How could I ignore that? I figured I should do the sensible thing and take another look at it. And, lo and behold, when I started evaluating it again it seemed to be just what I needed.\\n\\nWith the minumum of fuss I was able to get an ASP.Net MVC 4 solution up and running, integrated with Cassette, which dealt with all my scenarios very nicely indeed. I thought it might be good to write this up over a short series of posts and share what my finished code looks like. If you follow the steps I go through below it\'ll get you started using Cassette. Or you could skip to the end of this post and look at the repo on GitHub. Here we go...\\n\\n## Adding Cassette to a raw MVC 4 project\\n\\nFire up Visual Studio and create a new MVC 4 project (I used the internet template to have some content in place).\\n\\nGo to the Package Manager Console and key in \\"`Install-Package Cassette.Aspnet`\\". Cassette will install itself.\\n\\nNow you\'ve got Cassette in place you may as well pull out usage of Web Optimization as you\'re not going to need it any more.Be ruthless, delete App_Start/BundleConfig.cs and delete the line of code that references it in Global.asax.cs. If you take the time to run the app now you\'ll see you\'ve miraculously lost your CSS and your JavaScript. The code referencing it is still in place but there\'s nothing for it to serve up. Don\'t worry about that - we\'re going to come back and Cassette-ify things later on...\\n\\nYou\'ll also notice you now have a CassetteConfiguration.cs file in your project. Open it. Replace the contents with this (I\'ve just commented out the default code and implemented my own CSS and Script bundles based on what is available in the default template of an MVC 4 app):\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/5393608.js?file=CassetteConfiguration.cs\\"><\/script>\\n\\nIn the script above I\'ve created 4 bundles, 1 stylesheet bundle and 3 JavaScript bundles - each of these is roughly equivalent to Web Optimization bundles that are part of the MVC 4 template:\\n\\n<dl><dt>~/bundles/css</dt><dd>Our site CSS - this includes both our own CSS and the jQuery UI CSS as well. This is the rough equivalent of the Web Optimization bundles <em>~/Content/css</em> and <em>~/Content/themes/base/css</em> brought together.</dd><dt>~/bundles/head</dt><dd>What scripts we want served in the head tag - Modernizr basically. Do note the setting of the <em>PageLocation</em> property - the purpose of this will become apparent later. This is the direct equivalent of the Web Optimization bundle: <em>~/bundles/modernizr</em>.</dd><dt>~/bundles/core</dt><dd>The scripts we want served on every page. For this example project I\'ve picked jQuery and jQuery UI. This is the rough equivalent of the Web Optimization bundles <em>~/bundles/jquery</em> and <em>~/bundles/jqueryui</em> brought together.</dd><dt>~/bundles/validate</dt><dd>The validation scripts (that are dependent on the core scripts). This is the rough equivalent of the Web Optimization bundle: <em>~/bundles/jqueryval</em>.</dd></dl>\\n\\nAt this point we\'ve set up Cassette in our project - although we\'re not making use of it yet. If you want to double check that everything is working properly then you can fire up your project and browse to \\"Cassette.axd\\" in the root.\\n\\n## How Web Optimization and Cassette Differ\\n\\nIf you\'re more familiar with the workings of Web Optimization than Cassette then it\'s probably worth taking a moment to appreciate an important distinction between the slightly different ways each works.\\n\\n**Web Optimization**\\n\\n1. Create bundles as desired.\\n2. Serve up bundles and / or straight JavaScript files as you like within your MVC views / partial views / layouts.\\n\\n**Cassette**\\n\\n1. Create bundles for \\\\***all**\\\\* JavaScript files you wish to serve up. You may wish to create some bundles which consist of a number of a number of JavaScript files pushed together. But for each individual file you wish to serve you also need to create an individual bundle. (Failure to do so may mean you fall prey to the \\"_Cannot find an asset bundle containing the path \\"\\\\~/Scripts/somePath.js\\"._\\")\\n2. Reference bundles and / or individual JavaScript files in their individual bundles as you like within your MVC views / partial views / layouts / controllers / HTML helpers... the list goes on!\\n3. Render the referenced scripts to the page (typically just before the closing `body` tag)\\n\\n## Making use of our Bundles\\n\\nNow we\'ve created our bundles let\'s get the project serving up CSS and JavaScript using Cassette. First the layout file. Take the `_Layout.cshtml` file from this:\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/5393608.js?file=_LayoutBefore.cshtml\\"><\/script>\\n\\nTo this:\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/5393608.js?file=_LayoutAfter.cshtml\\"><\/script>\\n\\nAnd now let\'s take one of the views, `Login.cshtml` and take it from this:\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/5393608.js?file=LoginBefore.cshtml\\"><\/script>\\n\\nTo this:\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/5393608.js?file=LoginAfter.cshtml\\"><\/script>\\n\\nSo now you should be up and running with Cassette. If you want the code behind this then take I\'ve put it on GitHub [here](https://github.com/johnnyreilly/CassetteDemo)."},{"id":"/2013/04/26/a-navigation-animation-for-your-users","metadata":{"permalink":"/2013/04/26/a-navigation-animation-for-your-users","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2013-04-26-a-navigation-animation-for-your-users/index.md","source":"@site/blog/2013-04-26-a-navigation-animation-for-your-users/index.md","title":"A navigation animation (for your users delectation)","description":"The Vexation","date":"2013-04-26T00:00:00.000Z","formattedDate":"April 26, 2013","tags":[{"label":"navigation animation","permalink":"/tags/navigation-animation"},{"label":"css load","permalink":"/tags/css-load"},{"label":"UX","permalink":"/tags/ux"},{"label":"CSS animation","permalink":"/tags/css-animation"},{"label":"Modernizr","permalink":"/tags/modernizr"}],"readingTime":5.385,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"A navigation animation (for your users delectation)","authors":"johnnyreilly","tags":["navigation animation","css load","UX","CSS animation","Modernizr"],"hide_table_of_contents":false},"prevItem":{"title":"How I\'m Using Cassette part 1:Getting Up and Running","permalink":"/2013/05/04/how-im-using-cassette"},"nextItem":{"title":"IE 10 Install Torches JavaScript Debugging in Visual Studio 2012 Through Auto Update (Probably)","permalink":"/2013/04/17/ie-10-install-torches-javascript"}},"content":"## The Vexation\\n\\nThe current application I\'m working on lives within an `iframe`. A side effect of that is that my users no longer get the visual feedback that they\'re used to as they navigate around the site. By \\"visual feedback\\" what I mean are the little visual tics that are displayed in the browser when you\'re in the process of navigating from one screen to the next.\\n\\nWhen an application is nested in an `iframe` it seems that these visual tics aren\'t propogated up to the top frame of the browser as the user navigates around. Clicking on links results in a short lag whilst nothing appears to be happening and then, BANG!, a new page is rendered. This is not a great user experience. There\'s nothing to indicate that the link has been clicked on and the browser is doing something. Well, not in Internet Explorer at least - Chrome (my browser of choice) appears to do just that. But that\'s really by the by, the people using my app will be using the corporate browser, IE; so I need to think about them.\\n\\nNow I\'m fully aware that this is more in the region of nice-to-have rather than absolute necessity. That said, my experience is that when users think an application isn\'t responding fast enough their action point is usually \\"click it again, and maybe once more for luck\\". To prevent this from happening, I wanted to give the users back some kind of steer when they were in the process of navigation, `iframe` or no `iframe`.\\n\\n## The Agreeable Resolution\\n\\nTo that end, I\'ve come up with something that I feel does the job, and does it well. I\'ve taken a CSS animation courtesy of the good folk at [CSS Load](http://cssload.net/) and embedded it in the layout of my application. This animation is hidden from view until the user navigates to another page. At that point, the CSS animation appears in the header of the screen and remains in place until the new screen is rendered.\\n\\n\x3c!-- <p>And because they haven\'t yet invented the animated screenshot here\'s what the CSS animation looks like in full flight:</p> <style>#navigationAnimation {   margin: 7px;   clear: both; }   #circleG {   width: 46.666666666666664px;   height: 20px; }   .circleG {   background-color: #ffffff;   float: left;   height: 10px;   margin-left: 5px;   width: 10px;   -moz-border-radius: 7px;   -webkit-border-radius: 7px;   border-radius: 7px;   -moz-animation-name: bounce_circleG;   -moz-animation-duration: 0.6000000000000001s;   -moz-animation-iteration-count: infinite;   -moz-animation-direction: linear;   -webkit-animation-name: bounce_circleG;   -webkit-animation-duration: 0.6000000000000001s;   -webkit-animation-iteration-count: infinite;   -webkit-animation-direction: linear;   -ms-animation-name: bounce_circleG;   -ms-animation-duration: 0.6000000000000001s;   -ms-animation-iteration-count: infinite;   -ms-animation-direction: linear;   animation-name: bounce_circleG;   animation-duration: 0.6000000000000001s;   animation-iteration-count: infinite;   animation-direction: linear; }   #circleG_1 {   -moz-animation-delay: 0.12000000000000002s;   -webkit-animation-delay: 0.12000000000000002s;   -ms-animation-delay: 0.12000000000000002s;   animation-delay: 0.12000000000000002s; }   #circleG_2 {   -moz-animation-delay: 0.28s;   -webkit-animation-delay: 0.28s;   -ms-animation-delay: 0.28s;   animation-delay: 0.28s; }   #circleG_3 {   -moz-animation-delay: 0.36s;   -webkit-animation-delay: 0.36s;   -ms-animation-delay: 0.36s;   animation-delay: 0.36s; }   @-moz-keyframes bounce_circleG {   50% {     background-color: #000000;   } }   @-webkit-keyframes bounce_circleG {   50% {     background-color: #000000;   } }   @-ms-keyframes bounce_circleG {   50% {     background-color: #000000;   } }   @keyframes bounce_circleG {   50% {     background-color: #000000;   } }   </style> <div id=\\"navigationAnimation\\">    <div id=\\"circleG\\">        <div id=\\"circleG_1\\" class=\\"circleG\\"></div>        <div id=\\"circleG_2\\" class=\\"circleG\\"></div>        <div id=\\"circleG_3\\" class=\\"circleG\\"></div>    </div></div> <p>Beautiful don\'t you think?</p>--\x3e\\n\\n## How\'s that work then guv?\\n\\nYou\'re no doubt dazzled by the glory of it all. How was it accomplished? Well, it was actually a great deal easier than you might think. First of all we have the html:\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/5466370.js?file=navigationAnimation.html\\"><\/script>\\n\\nApart from the outer `div` tag (#navigationAnimation) all of this is the HTML taken from [CSS Load](http://cssload.net/). If you wanted to use a different navigation animation you could easily replace the inner HTML with something else instead. Next up is the CSS, again courtesy of CSS Load (and it\'s this that turns this simple HTML into sumptuous animated goodness):\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/5466370.js?file=navigationAnimation.css\\"><\/script>\\n\\nAnd finally we have the JavaScript which is responsible for showing animation when the user starts navigating:\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/5466370.js?file=navigationAnimation.js\\"><\/script>\\n\\nIt\'s helped along with a little jQuery here but this could easily be accomplished with vanilla JS if you fancied. The approach works by hooking into the [beforeunload](https://developer.mozilla.org/en-US/docs/DOM/Mozilla_event_reference/beforeunload) event that fires when \\"_the window, the document and its resources are about to be unloaded_\\". There\'s a little bit more to the functionality in the JavaScript abover which I go into in the PPS below. Essentially that covers backwards compatibility with earlier versions of IE.\\n\\nI\'ve coded this up in a manner that lends itself to re-use. I can imagine that you might also want to make use of the navigation animation if, for example, you had an expensive AJAX operation on a page and you didn\'t want the users to despair. So the navigation animation could become a kind of a generic \\"I am doing something\\" animation instead - I leave it to your disgression.\\n\\n## Oh, and a final PS\\n\\nI had initially planned to use an old school animated GIF instead of a CSS animation. The thing that stopped me taking this course of action is that, to quote an [answer on Stack Overflow](http://stackoverflow.com/a/780617/761388) \\"_IE assumes that the clicking of a link heralds a new navigation where the current page contents will be replaced. As part of the process for perparing for that it halts the code that animates the GIFs._\\". So I needed animation that stayed animated. And lo, there were CSS animations...\\n\\n## Better make that a PPS - catering for IE 9 and earlier\\n\\nI spoke a touch too soon when I expounded on how CSS animations were going to get me out of a hole. Unfortunately, and to my lasting regret, they aren\'t supported in IE 9. And yes, at least for now that is what the users have. To get round this I\'ve delved a little bit further and discovered a frankly hacky way to make animated gifs stay animated after beforeunload has fired. It works by rendering an animated gif to the screen when beforeunload is fired. Why this works I couldn\'t say - but if you\'re interested to research more then take a look at [this answer on Stack Overflow](http://stackoverflow.com/a/1904931/761388). In my case I\'ve found an animated gif on [AjaxLoad](http://www.ajaxload.info/) which looks pretty similar to the CSS animation:\\n\\n![null](https://4.bp.blogspot.com/-_9OgkLfflAg/UYEXn7dgByI/AAAAAAAAAb8/3Q33pAs6WeM/s320/navigationAnimation.gif)This is now saved away as `navigationAnimation.gif` in the application. The JavaScript uses Modernizr to detect if CSS animations are in play. If they\'re not then the animated gif is rendered to the screen in place of the CSS animation HTML. Ugly, but it seems to work well; I think this will work on IE 6 - 9. The CSS animations will work on IE 10+."},{"id":"/2013/04/17/ie-10-install-torches-javascript","metadata":{"permalink":"/2013/04/17/ie-10-install-torches-javascript","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2013-04-17-ie-10-install-torches-javascript/index.md","source":"@site/blog/2013-04-17-ie-10-install-torches-javascript/index.md","title":"IE 10 Install Torches JavaScript Debugging in Visual Studio 2012 Through Auto Update (Probably)","description":"OK the title of this post is a little verbose. I\'ve just wasted a morning of my life trying to discover what happened to my ability to debug JavaScript in Visual Studio 2012. If you don\'t want to experience the same pain then read on...","date":"2013-04-17T00:00:00.000Z","formattedDate":"April 17, 2013","tags":[{"label":"Visual Studio 2012","permalink":"/tags/visual-studio-2012"},{"label":"JavaScript debugging","permalink":"/tags/java-script-debugging"},{"label":"IE 10","permalink":"/tags/ie-10"}],"readingTime":1.17,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"IE 10 Install Torches JavaScript Debugging in Visual Studio 2012 Through Auto Update (Probably)","authors":"johnnyreilly","tags":["Visual Studio 2012","JavaScript debugging","IE 10"],"hide_table_of_contents":false},"prevItem":{"title":"A navigation animation (for your users delectation)","permalink":"/2013/04/26/a-navigation-animation-for-your-users"},"nextItem":{"title":"Making IE 10\'s clear field (X) button and jQuery UI autocomplete play nice","permalink":"/2013/04/09/making-ie-10s-clear-field-x-button-and"}},"content":"OK the title of this post is a little verbose. I\'ve just wasted a morning of my life trying to discover what happened to my ability to debug JavaScript in Visual Studio 2012. If you don\'t want to experience the same pain then read on...\\n\\n## The Symptoms\\n\\n1. I\'m not hitting my JavaScript breakpoints when I hit F5 in Visual Studio.\\n2. [Script Documents](http://msdn.microsoft.com/en-us/library/bb385621.aspx) is missing from the Solution Explorer when I\'m debugging in Visual Studio.\\n\\n## The Cure\\n\\nIn the end, after a great deal of frustration, I happened upon [this answer](http://stackoverflow.com/a/15908391/761388) on Stack Overflow. It set me in the right direction.\\n\\nI was seeing exactly the same as this list but with **TWO** instances of Internet Explorer in the list instead of one. Odd, I know.\\n\\nI fixed this up by selecting Google Chrome as my target instead of IE, running it and then setting it back to IE. And interestingly, when I went to set it back to IE there was only one instance of Internet Explorer in the list again.\\n\\n## The Probable Cause\\n\\nMy machine was auto updated from IE 9 to IE 10 just the other day. I \\\\***think**\\\\* my JavaScript debugging issue appeared at the same time. This would explain to me why I had two instances of \\"Internet Explorer\\" in my list. Not certain but I\'d say the evidence is fairly compelling.\\n\\nPainful Microsoft. Painful"},{"id":"/2013/04/09/making-ie-10s-clear-field-x-button-and","metadata":{"permalink":"/2013/04/09/making-ie-10s-clear-field-x-button-and","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2013-04-09-making-ie-10s-clear-field-x-button-and/index.md","source":"@site/blog/2013-04-09-making-ie-10s-clear-field-x-button-and/index.md","title":"Making IE 10\'s clear field (X) button and jQuery UI autocomplete play nice","description":"This morning when I logged on I was surprised to discover IE 10 had been installed onto my machine. I hadn\'t taken any action to trigger this myself and so I\u2019m assuming that this was part of the general Windows Update mechanism. I know Microsoft had planned to push IE 10 out through this mechanism.","date":"2013-04-09T00:00:00.000Z","formattedDate":"April 9, 2013","tags":[{"label":"autocomplete","permalink":"/tags/autocomplete"},{"label":"jQuery UI","permalink":"/tags/j-query-ui"},{"label":"clear field button","permalink":"/tags/clear-field-button"},{"label":"IE 10","permalink":"/tags/ie-10"}],"readingTime":1.515,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Making IE 10\'s clear field (X) button and jQuery UI autocomplete play nice","authors":"johnnyreilly","tags":["autocomplete","jQuery UI","clear field button","IE 10"],"hide_table_of_contents":false},"prevItem":{"title":"IE 10 Install Torches JavaScript Debugging in Visual Studio 2012 Through Auto Update (Probably)","permalink":"/2013/04/17/ie-10-install-torches-javascript"},"nextItem":{"title":"Death to compatibility mode","permalink":"/2013/04/01/death-to-compatibility-mode"}},"content":"This morning when I logged on I was surprised to discover IE 10 had been installed onto my machine. I hadn\'t taken any action to trigger this myself and so I\u2019m assuming that this was part of the general Windows Update mechanism. I know [Microsoft had planned to push IE 10 out through this mechanism](http://technet.microsoft.com/en-us/ie/jj898508.aspx).\\n\\nI was a little surprised that my work desktop had been upgraded without any notice. And I was initially rather concerned given that most of my users have IE 9 and now I didn\'t have a test harness on my development machine any more. (I\'ve generally found that having the majority users browser on your own machine is a good idea.) However, I wasn\'t too concerned as I didn\u2019t think it would makes much of a difference to my development experience. I say that because IE10, as far as I understand, is basically IE 9 + more advanced CSS 3 and extra HTML 5 features. The rendering of my existing content developed for the IE 9 target should look pixel for pixel identical in IE 10. That\u2019s the theory anyway.\\n\\nHowever, I have found one exception to this rule already. IE 10 provides clear field buttons in text boxes.\\n\\nUnhappily I found these were clashing with our jQuery UI auto complete loading gif.\\n\\nI know; ugly isn\'t it? Happily I was able to resolve this with a CSS <strike>hack</strike>\\n\\nfix which looks like this:\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/5345373.js?file=ie10jQueryUI.css\\"><\/script>\\n\\nAnd now the jQuery UI autocomplete looks like we expect during the loading phase.\\n\\nBut happily when the autocomplete is not in the loading phase we still have access to the IE 10 clear field button. This works because the CSS selector above only applies to the _ui-autocomplete-loading_ class (which is only applied to the textbox when the loading is taking place)."},{"id":"/2013/04/01/death-to-compatibility-mode","metadata":{"permalink":"/2013/04/01/death-to-compatibility-mode","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2013-04-01-death-to-compatibility-mode/index.md","source":"@site/blog/2013-04-01-death-to-compatibility-mode/index.md","title":"Death to compatibility mode","description":"For just over 10 years my bread and butter has been the development and maintenance of line of business apps. More particularly, web apps built on the Microsoft stack of love (\xa9 Scott Hanselman). These sort of apps are typically accessed via the company intranet and since \\"bring your own device\\" is still a relatively new innovation these apps are invariably built for everyones favourite browser: Internet Explorer. As we all know, enterprises are generally not that speedy when it comes to upgrades. So we\'re basically talking IE 9 at best, but more often than not, IE 8.","date":"2013-04-01T00:00:00.000Z","formattedDate":"April 1, 2013","tags":[{"label":"css","permalink":"/tags/css"},{"label":"intranet","permalink":"/tags/intranet"},{"label":"meta","permalink":"/tags/meta"},{"label":"internet explorer","permalink":"/tags/internet-explorer"},{"label":"compatibility mode","permalink":"/tags/compatibility-mode"},{"label":"header","permalink":"/tags/header"}],"readingTime":6.11,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Death to compatibility mode","authors":"johnnyreilly","tags":["css","intranet","meta","internet explorer","compatibility mode","header"],"hide_table_of_contents":false},"prevItem":{"title":"Making IE 10\'s clear field (X) button and jQuery UI autocomplete play nice","permalink":"/2013/04/09/making-ie-10s-clear-field-x-button-and"},"nextItem":{"title":"DecimalModelBinder for nullable Decimals","permalink":"/2013/03/11/decimalmodelbinder-for-nullable-decimals"}},"content":"For just over 10 years my bread and butter has been the development and maintenance of line of business apps. More particularly, web apps built on the Microsoft stack of love ([\xa9 Scott Hanselman](https://channel9.msdn.com/Events/MIX/MIX11/FRM02)). These sort of apps are typically accessed via the company intranet and since \\"bring your own device\\" is still a relatively new innovation these apps are invariably built for everyones favourite browser: Internet Explorer. As we all know, enterprises are generally not that speedy when it comes to upgrades. So we\'re basically talking IE 9 at best, but more often than not, IE 8.\\n\\nNow, unlike many people, I don\'t regard IE as a work of evil. I spent a fair number of years working for an organization which had IE 6 as the only installed browser on company desktops. (In fact, this was still the case as late as 2012!) Now, because JavaScript is so marvellously flexible I was still able to do a great deal with the help of a number of [shivs / shims](http://paulirish.com/2011/the-history-of-the-html5-shiv/).\\n\\nBut rendering and CSS - well that\'s another matter. Because here we\'re at the mercy of \\"compatibility mode\\". Perhaps a quick history lesson is in order. What is this \\"compatibility mode\\" of which you speak?\\n\\n## A Brief History\\n\\nWell it all started when Microsoft released IE 8. To quote them:\\n\\n> _A fundamental problem discussed during each and every Internet Explorer release is balancing new features and functionality with site compatibility for the existing Web. On the one hand, new features and functionality push the Web forward. On the other hand, the Web is a large expanse; requiring every legacy page to support the \\"latest and greatest\\" browser version immediately at product launch just isn\'t feasible. Internet Explorer 8 addresses this challenge by introducing compatibility modes which gives a way to introduce new features and stricter compliance to standards while enabling it to be backward compliant._ \\\\- excerpted from [understanding compatibility modes in Internet Explorer 8](https://blogs.msdn.com/b/askie/archive/2009/03/23/understanding-compatibility-modes-in-internet-explorer-8.aspx).\\n\\n## There\'s the rub\\n\\nSounds fair enough? Of course it does. Microsoft have generally bent over backwards to facilitate backwards compatibility. Quite right too - good business sense and all that. However, one of the choices made around backwards compatibility I\'ve come to regard as somewhat irksome. Later down in the article you\'ll find this doozy: (emphasis mine)\\n\\n> _\\"**for Intranet pages, 7 (IE 7 Standards) rendering mode is used by default** and can be changed.\\"_\\n\\nFor whatever reason, this decision was not particularly well promoted. As a result, a fair number of devs I\'ve encountered have little or no knowledge of compatibility mode. Certainly it came as a surprise to me. Here was I, developing away on my desktop. I\'d fire up the app hosted on my machine and test on my local install of IE 8. All would look new and shiny (well non-anchor tags would have `:hover` support). Happy and content, I\'d push to our test system and browse to it. Wait, what\'s happened? Where\'s the new style rendering? What\'s up with my CSS? This is a bug right?\\n\\nObviously I know now it\'s not a bug it\'s a \\"feature\\". And I have learned how to get round the intranet default of compatibility mode through cunning deployment of meta tags and custom http headers. Recently compatibility mode has come to bite me for the second time (in this case I was building for IE 9 and was left wondering where all my rounded corners had vanished to when I deployed...).\\n\\nFor my own sanity I thought it might be good to document the various ways that exist to solve this particular problem. Just to clarify terms, \\"solve\\" in this context means \\"force IE to render in the most standards compliant / like other browsers fashion it can muster\\". You can use compatibility mode to do more than just that and if you\'re interested in more about this then I recommend [this Stack Overflow answer](http://stackoverflow.com/a/6771584/761388).\\n\\n## Solution 1: Custom HTTP Header through web.config\\n\\nIf you\'re running IIS7 or greater then, for my money, this is the simplest and most pain free solution. All you need do is include the following snippet in your web config file:\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/5283462.js?file=web.config\\"><\/script>\\n\\nThis will make IIS serve up the above custom response HTTP header with each page.\\n\\n## Solution 2: Custom HTTP Header the hard way\\n\\nMaybe you\'re running II6 and so you making a change to the web.config won\'t make a difference. That\'s fine, you can still get the same behaviour by going to the HTTP headers tab in IIS (see below) and adding the `X-UA-Compatible: IE=edge` header by hand.\\n\\n![](https://4.bp.blogspot.com/-78CYavaCiUk/UVlGNv87U_I/AAAAAAAAAZQ/qtchMc14JsY/s320/CustomHeadersIIS6.gif)\\n\\nOr, if you don\'t have access to IIS (don\'t laugh - it happens) you can fall back to doing this in code like this:\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/5283462.js?file=servingUpTheHardWay.cs\\"><\/script>\\n\\nObviously there\'s a whole raft of ways you could get this in, using `Application_BeginRequest` in `Global.asax.cs` would probably as good an approach as any.\\n\\n## Solution 3: Meta Tags are go!\\n\\nThe final approach uses meta tags. And, in my experience it is the most quirky approach - it doesn\'t always seem to work. First up, what do we do? Well, in each page served we include the following meta tag like this:\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/5283462.js?file=any.html\\"><\/script>\\n\\nHaving crawled over the WWW equivalent of broken glass I now know why this \\\\***sometimes**\\\\* doesn\'t work. (And credit where it\'s due the answer came from [here](http://stackoverflow.com/a/3960197/761388).) It\'s all down to the positioning of the meta tag:\\n\\n> _The X-UA-compatible header is not case sensitive; however, it must appear in the Web page\'s header (the HEAD section) before all other elements, except for the title element and other meta elements._ \\\\- excerpted from [specifying legacy document modes](<http://msdn.microsoft.com/en-gb/library/jj676915(v=vs.85).aspx>)\\n\\nThat\'s right, get your meta tag in the wrong place and things won\'t work. And you won\'t know why. Lovely. But get it right and it\'s all gravy. This remains the most unsatisfactory approach in my book though.\\n\\n## And for bonus points: `IFRAME`s!\\n\\nBefore I finish off I thought it worth sharing a little known feature of `IFRAME`s. If page is running in compatibility mode and it contains an `IFRAME` then the page loaded in that `IFRAME` will **also run in compatibility mode**. No ifs, no buts.\\n\\nIn the case that I encountered this behaviour, the application was being hosted in an `IFRAME` inside Sharepoint. Because of the way our Sharepoint was configured it ended up that the only real game in town for us was the meta tags approach - which happily worked once we\'d correctly placed our meta tag.\\n\\nAgain, it\'s lamentable that this behaviour isn\'t better documented - hopefully the act of writing this here will mean that it becomes a little better known. There\'s probably a good reason for this behaviour, though I\'m frankly, I don\'t know what it is. If anyone does, I\'d be interested.\\n\\n## That\'s it\\n\\nArmed with the above I hope you have less compatibility mode pain than I have. The following blog entry is worth a read by the way:\\n\\n[https://blogs.msdn.com/b/ie/archive/2009/02/16/just-the-facts-recap-of-compatibility-view.aspx](https://blogs.msdn.com/b/ie/archive/2009/02/16/just-the-facts-recap-of-compatibility-view.aspx)\\n\\nFinally, I have an open question about compatibility mode. I _think_ (but I don\'t know) that even in compatibility mode IE runs using the same JavaScript engine. However I suspect it has a different DOM to play with. If anyone knows a little more about this and wants to let me know that\'d be fantastic."},{"id":"/2013/03/11/decimalmodelbinder-for-nullable-decimals","metadata":{"permalink":"/2013/03/11/decimalmodelbinder-for-nullable-decimals","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2013-03-11-decimalmodelbinder-for-nullable-decimals/index.md","source":"@site/blog/2013-03-11-decimalmodelbinder-for-nullable-decimals/index.md","title":"DecimalModelBinder for nullable Decimals","description":"My memory appears to be a sieve. Twice in the last year I\'ve forgotten that MVCs ModelBinding doesn\'t handle regionalised numbers terribly well. Each time I\'ve thought \\"hmmmm.... best Google that\\" and lo and behold come upon this post on the issue by the fantastic Phil Haack:","date":"2013-03-11T00:00:00.000Z","formattedDate":"March 11, 2013","tags":[{"label":"Phil Haack","permalink":"/tags/phil-haack"},{"label":"Globalization","permalink":"/tags/globalization"},{"label":"ModelBinder","permalink":"/tags/model-binder"},{"label":"nullable","permalink":"/tags/nullable"},{"label":"decimal","permalink":"/tags/decimal"}],"readingTime":1.08,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"DecimalModelBinder for nullable Decimals","authors":"johnnyreilly","tags":["Phil Haack","Globalization","ModelBinder","nullable","decimal"],"hide_table_of_contents":false},"prevItem":{"title":"Death to compatibility mode","permalink":"/2013/04/01/death-to-compatibility-mode"},"nextItem":{"title":"Unit testing ModelState","permalink":"/2013/03/03/unit-testing-modelstate"}},"content":"My memory appears to be a sieve. Twice in the last year I\'ve forgotten that MVCs ModelBinding doesn\'t handle regionalised numbers terribly well. Each time I\'ve thought \\"hmmmm.... best Google that\\" and lo and behold come upon this post on the issue by the fantastic Phil Haack:\\n\\n[http://haacked.com/archive/2011/03/19/fixing-binding-to-decimals.aspx ](http://haacked.com/archive/2011/03/19/fixing-binding-to-decimals.aspx)\\n\\nThis post has got me 90% of the way there, the last 10% being me tweaking it so the model binder can handle nullable decimals as well.\\n\\nIn the expectation I that I may forget this again I thought I\'d note down my tweaks now and hopefully save myself sometime when I\'m next looking at this next...\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/5135647.js?file=DecimalModelBinder.cs\\"><\/script>\\n\\n## And now a question...\\n\\nWhy hasn\'t MVC got an out-of-the-box model binder that does this anyway? In Phil Haack\'s original post it looks like they were considering putting this into MVC itself at some point:\\n\\n\\"_... In that case, the DefaultModelBinder chokes on the value. This is unfortunate because jQuery Validate allows that value just fine. I\u2019ll talk to the rest of my team about whether we should fix this in the next version of ASP.NET MVC, but for now it\u2019s good to know there\u2019s a workaround..._\\"\\n\\nIf anyone knows the reason this never made it into core I\'d love to know. Maybe there\'s a good reason?"},{"id":"/2013/03/03/unit-testing-modelstate","metadata":{"permalink":"/2013/03/03/unit-testing-modelstate","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2013-03-03-unit-testing-modelstate/index.md","source":"@site/blog/2013-03-03-unit-testing-modelstate/index.md","title":"Unit testing ModelState","description":"- Me: \\"It can\'t be done\\"","date":"2013-03-03T00:00:00.000Z","formattedDate":"March 3, 2013","tags":[{"label":"asp.net mvc","permalink":"/tags/asp-net-mvc"},{"label":"Marc Talary","permalink":"/tags/marc-talary"},{"label":"DataAnnotations","permalink":"/tags/data-annotations"},{"label":"Controller","permalink":"/tags/controller"},{"label":"ModelState","permalink":"/tags/model-state"}],"readingTime":4.065,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Unit testing ModelState","authors":"johnnyreilly","tags":["asp.net mvc","Marc Talary","DataAnnotations","Controller","ModelState"],"hide_table_of_contents":false},"prevItem":{"title":"DecimalModelBinder for nullable Decimals","permalink":"/2013/03/11/decimalmodelbinder-for-nullable-decimals"},"nextItem":{"title":"Unit testing MVC controllers / Mocking UrlHelper","permalink":"/2013/02/18/unit-testing-mvc-controllers-mocking"}},"content":"- Me: \\"It can\'t be done\\"\\n- Him: \\"Yes it can\\"\\n- Me: \\"No it can\'t\\"\\n- Him: \\"Yes it can, I\'ve just done it\\"\\n- Me: \\"Ooooh! Show me ...\\"\\n\\nThe above conversation (or one much like it) took place between my colleague Marc Talary and myself a couple of weeks ago. It was one of those faintly embarrassing situations where you state your case with absolute certainty only to subsequently discover that you were \\\\***completely**\\\\* wrong. Ah arrogance, thy name is Reilly...\\n\\nThe disputed situation in this case was ModelState validation in ASP.Net MVC. How can you unit test a models validation driven by `DataAnnotations`? If at all. Well it can be done, and here\'s how.\\n\\n## Simple scenario\\n\\nLet\'s start with a simple model:\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/5069901.js?file=CarModel.cs\\"><\/script>\\n\\nAnd let\'s have a controller which makes use of that model:\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/5069901.js?file=CarController.cs\\"><\/script>\\n\\nWhen I was first looking at unit testing this I was slightly baffled by the behaviour I witnessed. I took an invalid model (where the properties set on the model were violating the model\'s validation `DataAnnotations`):\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/5069901.js?file=NaomiCampbell.cs\\"><\/script>\\n\\nI passed the invalid model to the `Edit` controller action inside a unit test. My expectation was that the `ModelState.IsValid` code path would \\\\***not**\\\\* be followed as this was \\\\***not**\\\\* a valid model. So `ModelState.IsValid` should evaluate to `false`, right? Wrong!\\n\\nContrary to my expectation the validity of `ModelState` is not evaluated on the fly inside the controller. Rather it is determined during the model binding that takes place \\\\***before**\\\\* the actual controller action method is called. And that completely explains why during my unit test with an invalid model we find we\'re following the `ModelState.IsValid` code path.\\n\\n## Back to the dispute\\n\\nAs this blog post started off I was slightly missing Marc\'s point. I thought he was saying we should be testing the `ModelState.IsValid == false` code path. And given that `ModelState` is determined before we reach the controller my view was that the only way to achieve this was through making use of `ModelState.AddModelError` in our unit test (you can read a good explanation of that [here](http://stackoverflow.com/a/3816143/761388)). And indeed we were already testing for this; we were surfacing errors via a `JsonResult` and so had a test in place to ensure that `ModelState` errors were transformed in the manner we would expect.\\n\\nHowever, Marc\'s point was actually that we should have unit tests that enforced our design. That is to say, if we\'d decided a certain property on a model was mandatory we should have a test that checked that this was indeed the case. If someone came along later and removed the `Required` data annotation then we wanted that test to fail.\\n\\nIt\'s worth saying, we didn\'t want a unit test to ensure that ASP.Net MVC worked as expected. Rather, where we had used DataAnnotations against our models to drive validation, we wanted to ensure the validation didn\'t disappear further down the track. Just to be clear: we wanted to test our code, not Microsoft\'s.\\n\\n## Now I get to learn something\\n\\nWhen I grasped Marc\'s point I thought that the the only way to write these tests would be to make use of reflection. And whilst we could certainly do that I wasn\'t entirely happy with that as a solution. To my mind it was kind of testing \\"at one remove\\", if you see what I mean. What I really wanted was to see that MVC was surfacing validations in the manner I might have hoped. And you can!\\n\\n.... Drum roll... Ladies and gents may I present Marc\'s `ModelStateTestController`:\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/5069901.js?file=ModelStateTestController.cs\\"><\/script>\\n\\nThis class is, as you can see, incredibly simple. It is a controller, it inherits from `System.Web.Mvc.Controller` and establishes a mock context in the constructor using MOQ. This controller exposes a single method: `TestTryValidateModel`. This method internally determines the controller\'s `ModelState` given the supplied object by calling off to Mvc\'s (protected) `TryValidateModel` method (`TryValidateModel` evaluates `ModelState`).\\n\\nThis simple class allows us to test the validations on a model in a simple fashion that stays close to the way our models will actually be used in the wild. It\'s pragmatic and it\'s useful.\\n\\n## An example\\n\\nLet me wrap up with an example unit test. The test below makes use of the `ModelStateTestController` to check the application of the DataAnnotations on our model:\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/5069901.js?file=ModelStateUnitTests.cs\\"><\/script>\\n\\n## Wrapping up\\n\\nIn a way I think it\'s a shame that `TryValidateModel` is a protected method. If it weren\'t it would be simplicity to write a unit test which tested the ModelState directly in context of the action method. It would be possible to get round this by establishing a base controller class which all our controllers would inherit from which implemented the `TestTryValidateModel` method from above. On the other hand maybe it\'s good to have clarity of the difference between testing model validations and testing controller actions. Something to ponder..."},{"id":"/2013/02/18/unit-testing-mvc-controllers-mocking","metadata":{"permalink":"/2013/02/18/unit-testing-mvc-controllers-mocking","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2013-02-18-unit-testing-mvc-controllers-mocking/index.md","source":"@site/blog/2013-02-18-unit-testing-mvc-controllers-mocking/index.md","title":"Unit testing MVC controllers / Mocking UrlHelper","description":"I have put a name to my pain...","date":"2013-02-18T00:00:00.000Z","formattedDate":"February 18, 2013","tags":[{"label":"MVC","permalink":"/tags/mvc"},{"label":"unit testing","permalink":"/tags/unit-testing"},{"label":"AreaRegistration.RegisterAllAreas()","permalink":"/tags/area-registration-register-all-areas"},{"label":"MOQ","permalink":"/tags/moq"},{"label":"UrlHelper","permalink":"/tags/url-helper"}],"readingTime":2.445,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Unit testing MVC controllers / Mocking UrlHelper","authors":"johnnyreilly","tags":["MVC","unit testing","AreaRegistration.RegisterAllAreas()","MOQ","UrlHelper"],"hide_table_of_contents":false},"prevItem":{"title":"Unit testing ModelState","permalink":"/2013/03/03/unit-testing-modelstate"},"nextItem":{"title":"Using Expressions with Constructors","permalink":"/2013/02/13/using-expressions-with-constructors"}},"content":"## I have put a name to my pain...\\n\\nAnd it is unit testing ASP.Net MVC controllers.\\n\\nWell perhaps that\'s unfair. I have no problem unit testing MVC controllers.... **until** it comes to making use of the \\"innards\\" of MVC. Let me be more specific. This week I had a controller action that I needed to test. It looked a little like this:\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/4959924.js?file=DemoController.cs\\"><\/script>\\n\\nLooks fine right? It\'s an action that takes a simple object as an argument. That\'s ok. It returns a JsonResult. No worries. The JsonResult consists of an anonymous class. De nada. The anonymous class has one property that is driven by the controllers `UrlHelper`. Yeah that shouldn\'t be an issue... **Hold your horses sunshine - you\'re going nowhere!**\\n\\n## Getting disillusioned\\n\\nYup, the minute you start pumping in asserts around that `UrlHelper` driven property you\'re going to be mighty disappointed. What, you didn\'t expect the result to be `null`? Damn shame.\\n\\nDespite [articles](http://msdn.microsoft.com/en-us/magazine/dd942838.aspx) on MSDN about how the intention is for MVC to be deliberately testable the sad fact of the matter is that there is a yawning hole around the testing support for controllers in ASP.Net MVC. Whenever you try to test something that makes use of controller \\"gubbins\\" you have **serious** problems. And unfortunately I didn\'t find anyone out there who could offer the whole solution.\\n\\nAfter what I can best describe as a day of pain I found a way to scratch my particular itch. I found a way to write unit tests for controllers that made use of UrlHelper. As a bonus I managed to include the unit testing of Routes and Areas (well kind of) too.\\n\\n## MvcMockControllers updated\\n\\nThis solution is heavily based on the work of Scott Hanselman who [wrote and blogged about `MvcMockHelpers`](http://www.hanselman.com/blog/ASPNETMVCSessionAtMix08TDDAndMvcMockHelpers.aspx) back in 2008. Essentially I\'ve taken this and tweaked it so I could achieve my ends. My version of `MvcMockHelpers` looks a little like this:\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/4959924.js?file=MvcMockHelpers.cs\\"><\/script>\\n\\n## What I want to test\\n\\nI want to be able to unit test the controller `Edit` method I mentioned earlier. This method calls the `Action` method on the controllers `Url` member (which is, in turn, a `UrlHelper`) to generate a URL for passing pack to the client. The URL generated should fit with the routing mechanism I have set up. In this case the route we expect a URL for was mapped by the following area registration:\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/4959924.js?file=DemoAreaRegistration.cs\\"><\/script>\\n\\n## Enough of the waffle - show me a unit test\\n\\nNow to the meat; here\'s a unit test which demonstrates how this is used:\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/4959924.js?file=UnitTestingAnAreaUsingUrlHelper.cs\\"><\/script>\\n\\nLet\'s go through this unit test and breakdown what\'s happening:\\n\\n1. Arrange\\n2. Act\\n3. Assert\\n\\nThe most interesting thing you\'ll note is the controller\'s UrlHelper is now generating a URL as we might have hoped. The URL is generated making use of our routing, yay! Finally we\'re also managing to unit test a route registered by our area."},{"id":"/2013/02/13/using-expressions-with-constructors","metadata":{"permalink":"/2013/02/13/using-expressions-with-constructors","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2013-02-13-using-expressions-with-constructors/index.md","source":"@site/blog/2013-02-13-using-expressions-with-constructors/index.md","title":"Using Expressions with Constructors","description":"Every now and then you think \\"x should be easy\\" - and it isn\'t. I had one of those situations this morning. Something I thought would take 5 minutes had me still pondering 30 minutes later. I finally cracked it (with the help of a colleague - thanks Marc!) and I wanted to note down what I did since I\'m sure to forget this.","date":"2013-02-13T00:00:00.000Z","formattedDate":"February 13, 2013","tags":[{"label":"Expression","permalink":"/tags/expression"},{"label":"Constructors","permalink":"/tags/constructors"},{"label":"Generic","permalink":"/tags/generic"}],"readingTime":1.835,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Using Expressions with Constructors","authors":"johnnyreilly","tags":["Expression","Constructors","Generic"],"hide_table_of_contents":false},"prevItem":{"title":"Unit testing MVC controllers / Mocking UrlHelper","permalink":"/2013/02/18/unit-testing-mvc-controllers-mocking"},"nextItem":{"title":"Twitter.Bootstrap.MVC4 meet Bootstrap Datepicker *and* get your Internationalization on...","permalink":"/2013/01/14/twitterbootstrapmvc4-meet-bootstrap_14"}},"content":"Every now and then you think \\"x should be easy\\" - and it isn\'t. I had one of those situations this morning. Something I thought would take 5 minutes had me still pondering 30 minutes later. I finally cracked it (with the help of a colleague - thanks Marc!) and I wanted to note down what I did since I\'m sure to forget this.\\n\\n## So what\'s the problem?\\n\\nIn our project we had a very simple validation class. It looked a bit like this:\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/4944545.js?file=FieldValidationBefore.cs\\"><\/script>\\n\\nI wanted to take this class and extend it to have a constructor which allowed me to specify a Type and subsequently an Expression of that Type that allowed me to specify a property. 10 points if you read the last sentence and understood it without reading it a second time.\\n\\nCode is a better illustration; take a look below. I wanted to go from #1 to #2:\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/4944545.js?file=HowItIsUsed.cs\\"><\/script>\\n\\n\\"Why?\\" I hear you ask. Well we had a swathe of statements in the code which test each property for a problem and would create a `FieldValidation` with the very same property name if one was found. There\'s no real problem with that but I\'m a man that likes to refactor. Property names change and I didn\'t want to have to remember to manually go through each `FieldValidation` keeping these in line. If I was using the actual property name to drive the creation of my `FieldValidations` then that problem disappears. And I like that.\\n\\n## So what\'s the solution?\\n\\nWell it\'s this:\\n\\n<script src=\\"https://gist.github.com/johnnyreilly/4944545.js?file=FieldValidationAfter.cs\\"><\/script>\\n\\nAs you can see we have taken the original FieldValidation class and added in a generic constructor which instead of taking `string fieldName` as a first argument it takes `Expression&lt;Func&lt;T, object&gt;&gt; expression`. LINQ\'s Expression magic is used to determine the supplied property name which is smashing. If you were wondering, the first `MemberExpression` code is used for _reference_ types. The `UnaryExpression` wrapping a `MemberExpression` code is used for _value_ types. A good explanation of this can be found [here](http://stackoverflow.com/a/12975480/761388).\\n\\nMy colleague directed me to [this crucial StackOverflow answer](http://stackoverflow.com/a/2916344) which provided some much needed direction when I was thrashing. And that\'s it; we\'re done, home free."},{"id":"/2013/01/14/twitterbootstrapmvc4-meet-bootstrap_14","metadata":{"permalink":"/2013/01/14/twitterbootstrapmvc4-meet-bootstrap_14","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2013-01-14-twitterbootstrapmvc4-meet-bootstrap_14/index.md","source":"@site/blog/2013-01-14-twitterbootstrapmvc4-meet-bootstrap_14/index.md","title":"Twitter.Bootstrap.MVC4 meet Bootstrap Datepicker *and* get your Internationalization on...","description":"Last time I wrote about marrying up Twitter.Bootstrap.MVC4 and Bootstrap Datepicker. It came together quite nicely but when I took a more in depth look at what I\'d done I discovered a problem. The brief work on regionalisation / internationalisation / localisation / globalisation / whatever it\'s called this week... wasn\'t really working. We had problems with the validation.","date":"2013-01-14T00:00:00.000Z","formattedDate":"January 14, 2013","tags":[{"label":"Internationalization","permalink":"/tags/internationalization"},{"label":"Twitter.Bootstrap.MVC4","permalink":"/tags/twitter-bootstrap-mvc-4"},{"label":"Globalize JS","permalink":"/tags/globalize-js"},{"label":"Twitter Bootstrap","permalink":"/tags/twitter-bootstrap"}],"readingTime":2.91,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Twitter.Bootstrap.MVC4 meet Bootstrap Datepicker *and* get your Internationalization on...","authors":"johnnyreilly","tags":["Internationalization","Twitter.Bootstrap.MVC4","Globalize JS","Twitter Bootstrap"],"hide_table_of_contents":false},"prevItem":{"title":"Using Expressions with Constructors","permalink":"/2013/02/13/using-expressions-with-constructors"},"nextItem":{"title":"Twitter.Bootstrap.MVC4 meet Bootstrap Datepicker","permalink":"/2013/01/09/twitterbootstrapmvc4-meet-bootstrap"}},"content":"[Last time](http://icanmakethiswork.blogspot.co.uk/2013/01/twitterbootstrapmvc4-meet-bootstrap.html) I wrote about marrying up Twitter.Bootstrap.MVC4 and Bootstrap Datepicker. It came together quite nicely but when I took a more in depth look at what I\'d done I discovered a problem. The brief work on regionalisation / internationalisation / localisation / globalisation / whatever it\'s called this week... wasn\'t really working. We had problems with the validation.\\n\\nI also discovered that Stefan Petre\'s Bootstrap Datepicker appears to have been abandoned. Andrew Rowls has taken it on and created a GitHub repository for it [here](https://github.com/eternicode/bootstrap-datepicker). Besides bug fixes he\'s also introduced the ability for the Bootstrap Datepicker to customised for different cultures.\\n\\nSince these 2 subjects are linked I tackled them together and thought it might be worth writing up here. You can find the conclusion of my work in a GitHub repository I created [here](https://github.com/johnnyreilly/BootstrapMvcSample).\\n\\n## Going global down in Acapulco\\n\\nFirst step in internationalising any ASP.Net web app is adding the following to the `web.config`:\\n\\n<script src=\\"https://gist.github.com/4528994.js?file=web.config\\"><\/script>\\n\\nThen I pulled [Globalize](https://github.com/jquery/globalize) and the [Andrew Rowls fork of Bootstrap Datepicker](https://github.com/eternicode/bootstrap-datepicker) into the project (replacing Stefan\'s original assets). As well as this I pulled in the `jQuery.validate.globalize.js` extension [I wrote about here](http://icanmakethiswork.blogspot.co.uk/2012/09/globalize-and-jquery-validate.html). (This replaces some of the default jQuery Validate functionality for culture-specific functionality based on Globalize.) This extension depends on a meta tag that is generated using the following file (which also handles the serving up of the relevant JavaScript culture bundles, more of which shortly):\\n\\n<script src=\\"https://gist.github.com/4528994.js?file=GlobalizationHelpers.cs\\"><\/script>\\n\\n## Culture-specific script bundles\\n\\nWith all of my dependancies in place I was now ready to press on. Since both Globalize and the new Bootstrap Datepicker come with their own culture-specific JavaScript files it seemed a good idea to make use of ASP.Nets new bundling functionality. This I did here:\\n\\n<script src=\\"https://gist.github.com/4528994.js?file=BootstrapBundleConfig.cs\\"><\/script>\\n\\nThe code above creates a script bundle for each culture when the application starts up. This script bundle contains the culture-specific Globalize and Bootstrap Datepicker JavaScript files. If further culture-specific components were added to the application it would make sense to include these here as well.\\n\\n`_BootstrapLayout.basic.cshtml` has been amended to make use of the new bundles and also to include a meta tag that will used to drive regionalisation:\\n\\n<script src=\\"https://gist.github.com/4528994.js?file=_BootstrapLayout.basic.cshtml\\"><\/script>\\n\\nTo illustrate how this works, a German user running a machine with the de-DE culture would be served up the following 2 files:\\n\\n- `globalize.culture.de-DE.js`\\n- `bootstrap-datepicker.de.js`\\n\\n## Where have we got to?\\n\\nWith all this done we have now fixed the validation issues we were experiencing previously. This was done by including the Globalize library, the accept-language meta tag and the jQuery Validate Globalize extensions.\\n\\nBesides this we\'ve laid the groundwork for introducing internationalised datepickers by introducing Andrew Rowls fork of the Bootstrap Datepicker. That\'s what we\'ll do next...\\n\\n## International Bootstrap Datepicker\\n\\nThe final steps of switching over to using a culture-specific date picker are achieved by making a change to the Scripts section in the `Create.cshtml` file. The existing (and very simple) section should be replaced with this:\\n\\n<script src=\\"https://gist.github.com/4528994.js?file=Create.cshtml\\"><\/script>\\n\\nThe script above takes the region from the accept-language meta tag and attempts to look up an associated \\"language\\" for the Bootstrap Datepicker. If it finds one it uses it, if not then the default language of \\"en\\" / English will be used.\\n\\n## Summary\\n\\nIn this post we:\\n\\n1. fixed the validation issues we\'d introduced by marrying up Twitter.Bootstrap.MVC4 and the Bootstrap Datepicker\\n2. switched over to using the Andrew Rowls fork of Bootstrap Datepicker and made use of the internationalisation functionality it exposes."},{"id":"/2013/01/09/twitterbootstrapmvc4-meet-bootstrap","metadata":{"permalink":"/2013/01/09/twitterbootstrapmvc4-meet-bootstrap","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2013-01-09-twitterbootstrapmvc4-meet-bootstrap/index.md","source":"@site/blog/2013-01-09-twitterbootstrapmvc4-meet-bootstrap/index.md","title":"Twitter.Bootstrap.MVC4 meet Bootstrap Datepicker","description":"Update 14/01/2013","date":"2013-01-09T00:00:00.000Z","formattedDate":"January 9, 2013","tags":[{"label":"asp.net mvc","permalink":"/tags/asp-net-mvc"},{"label":"Twitter.Bootstrap.MVC4","permalink":"/tags/twitter-bootstrap-mvc-4"},{"label":"Responsive","permalink":"/tags/responsive"},{"label":"Twitter Bootstrap","permalink":"/tags/twitter-bootstrap"},{"label":"Bootstrap Datepicker","permalink":"/tags/bootstrap-datepicker"}],"readingTime":3.05,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Twitter.Bootstrap.MVC4 meet Bootstrap Datepicker","authors":"johnnyreilly","tags":["asp.net mvc","Twitter.Bootstrap.MVC4","Responsive","Twitter Bootstrap","Bootstrap Datepicker"],"hide_table_of_contents":false},"prevItem":{"title":"Twitter.Bootstrap.MVC4 meet Bootstrap Datepicker *and* get your Internationalization on...","permalink":"/2013/01/14/twitterbootstrapmvc4-meet-bootstrap_14"},"nextItem":{"title":"HTML to PDF using a WCF Service","permalink":"/2013/01/03/html-to-pdf-using-wcf-service"}},"content":"## Update 14/01/2013\\n\\nSince I wrote this I\'ve taken things on a little further - to read about that go [here](http://icanmakethiswork.blogspot.co.uk/2013/01/twitterbootstrapmvc4-meet-bootstrap_14.html).\\n\\n## Getting Responsive\\n\\nIt\'s the new year, it\'s time for new things. Long on my list of \\"things to do\\" was getting up to speed with [Responsive web design](http://en.wikipedia.org/wiki/Responsive_web_design). No doubt like everyone else I\'ve been hearing more and more about this over the last year (by the way there was a [good article on Mashable](http://mashable.com/2012/12/11/responsive-web-design/) about this last month). RWD (in case you don\'t already know) is pretty much about having web interfaces that format their presentation based on the device they\'re running to provide a good user experience. (I kind of think of it as a [write once, run anywhere](http://en.wikipedia.org/wiki/Write_once,_run_anywhere) approach - though hopefully without the negative connotations...)\\n\\nRather than diving straight in myself I\'d heard at a user group that it might be worth taking [Twitter Bootstrap](http://twitter.github.com/bootstrap/) as a baseline. I\'m a <strike>lazy</strike>\\n\\nbusy fellow so this sounded ideal.\\n\\n## I like ASP.Net MVC...\\n\\n... and this flavoured my investigations. I quickly stumbled on an [article written by Eric Hexter](http://lostechies.com/erichexter/2012/11/20/twitter-bootstrap-mvc4-the-template-nuget-package-for-asp-net-mvc4-projects/). Eric had brought together Twitter Bootstrap and ASP.Net MVC 4 in a [NuGet package](http://nuget.org/packages/twitter.bootstrap.mvc4). Excellent work chap!\\n\\nTo get up and running with Eric\'s work was a straightforward proposition. I...\\n\\n1. Created new MVC 4 application in Visual Studio called \u201cBootstrapMvcSample\u201d using the \u201cEmpty\u201d Project Template.\\n2. Executed the following commands at the NuGet Package Manager Console: - `Install-Package twitter.bootstrap.mvc4`\\n   - `Install-Package twitter.bootstrap.mvc4.sample`\\n\\nThis is just 1 page, with `@media` queries doing the heavy lifting.\\n\\n## Bootstrap Datepicker\\n\\nThe eagle-eyed amongst you will have noticed that the edit screen above features a date field. I\'ve long been a fan of datepickers to allow users to enter a date in an application in an intuitive fashion. Until native browser datepickers become the norm we\'ll be relying on some kind of component. Up until now my datepicker of choice has been the [jQuery UI one](http://jqueryui.com/datepicker/). Based on a quick Google it seemed that jQuery UI and Twitter Bootstrap were not necessarily natural bedfellows. (Though [Addy Osmani\'s jQuery UI Bootstrap](http://addyosmani.github.com/jquery-ui-bootstrap/) shows some promise...)\\n\\nSince I feared ending up down a blind alley I found myself casting around for a Twitter Bootstrap datepicker. I quickly happened upon [Stefan Petre\'s Bootstrap Datepicker](http://www.eyecon.ro/bootstrap-datepicker/) which looked just the ticket.\\n\\n## Shake hands and play nice...\\n\\nIncorporating the Bootstrap Datepicker into Twitter.Bootstrap.MVC4 was actually a pretty straightforward affair. I added the following datepicker assets to the ASP.Net MVC project as follows:\\n\\n- `bootstrap-datepicker.js` was added to `~\\\\Scripts`.\\n- `datepicker.css` was added to `~\\\\Content`. I renamed this file to `bootstrap-datepicker.css` to stay in line with the other css files.\\n\\nOnce this was done I amended the `BootstrapBundleConfig.cs` bundles to include these assets. Once this was done the bundle file looked like this:\\n\\n<script src=\\"https://gist.github.com/4529746.js?file=BootstrapBundleConfig.cs\\"><\/script>\\n\\nI then created this folder:`~\\\\Views\\\\Shared\\\\EditorTemplates`. To this folder I added the following `Date.cshtml` Partial to hold the datepicker EditorTemplate: (Having this in place meant that properties with the `[DataType(DataType.Date)]` attribute would automatically use this EditorTemplate when rendering an editor - I understand `[UIHint]` attributes can be used to the same end.)\\n\\n<script src=\\"https://gist.github.com/4529746.js?file=Date.cshtml\\"><\/script>\\n\\nAnd finally I amended the `Create.cshtml` View (which perhaps more accurately might be called the Edit View?) to include a bit of JavaScript at the bottom to initialise any datepickers on the screen.\\n\\n<script src=\\"https://gist.github.com/4529746.js?file=Create.cshtml\\"><\/script>\\n\\nEt voil\xe0 - it works!\\n\\nMy thanks to [Eric Hexter](https://twitter.com/ehexter) and Stefan Petre for doing all the hard work!\\n\\n## Still to do\\n\\nI haven\'t really tested how this all fits together (if at all) with browsers running a non-English culture. There may still be a little tinkering require to get that working..."},{"id":"/2013/01/03/html-to-pdf-using-wcf-service","metadata":{"permalink":"/2013/01/03/html-to-pdf-using-wcf-service","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2013-01-03-html-to-pdf-using-wcf-service/index.md","source":"@site/blog/2013-01-03-html-to-pdf-using-wcf-service/index.md","title":"HTML to PDF using a WCF Service","description":"TL; DR - \\"Talk is cheap. Show me the code.\\"","date":"2013-01-03T00:00:00.000Z","formattedDate":"January 3, 2013","tags":[{"label":"wkhtmltopdf","permalink":"/tags/wkhtmltopdf"},{"label":"html","permalink":"/tags/html"},{"label":"WCF","permalink":"/tags/wcf"},{"label":"pdf","permalink":"/tags/pdf"}],"readingTime":3.235,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"HTML to PDF using a WCF Service","authors":"johnnyreilly","tags":["wkhtmltopdf","html","WCF","pdf"],"hide_table_of_contents":false},"prevItem":{"title":"Twitter.Bootstrap.MVC4 meet Bootstrap Datepicker","permalink":"/2013/01/09/twitterbootstrapmvc4-meet-bootstrap"},"nextItem":{"title":"Getting up to speed with Bloomberg\'s Open API...","permalink":"/2012/11/13/a-nicer-net-api-for-bloombergs-open-api"}},"content":"## TL; DR - \\"Talk is cheap. Show me the code.\\"\\n\\nSome time ago I wrote a [post which demonstrated how you could make PDFs from HTML](http://icanmakethiswork.blogspot.com/2012/04/making-pdfs-from-html-in-c-using.html) using C# and [wkhtmltopdf](http://code.google.com/p/wkhtmltopdf/). To my lasting surprise this has been the most popular post I\'ve written. I recently put together an ASP.NET WCF service which exposed this functionality which I thought might be worth sharing. The code can be found on GitHub [here](https://github.com/johnnyreilly/PdfMakerWcfService).\\n\\n## A little more detail\\n\\nI should say up front that I\'m still a little ambivalent about how sensible an idea this is. Behind the scenes this WCF service is remotely firing up wkhtmltopdf using `System.Diagnostics.Process`. I feel a little wary about recommending this as a solution for a variety of not particularly defined reasons. However, I have to say I\'ve found this pretty stable and reliable. Bottom line it seems to work and work consistently. But I though I should include a caveat emptor; there is probably a better approach than this available. Anyway...\\n\\nThere isn\'t actually a great deal to say about this WCF service. It should (hopefully) just do what it says on the tin. Putting it together didn\'t involve a great deal of work; essentially it takes the code from the initial blog post and just wraps it in a WCF service called `PdfMaker`. The service exposes 2 methods:\\n\\n1. `GetPdf` \\\\- given a supplied URL this method creates a PDF and then returns it as a Stream to the client\\n2. `GetPdfUrl` \\\\- given a supplied URL this method creates a PDF and then returns the location of it to the client\\n\\nBoth of these methods also set a Location header in the response indicating the location of the created PDF.\\n\\n## That which binds us\\n\\nThe service uses `webHttpBinding`. This is commonly employed when people want to expose a RESTful WCF service. The reason I\'ve used this binding is I wanted a simple \\"in\\" when calling the service. I wanted to be able to call the service via AJAX as well as directly by browsing to the service and supplying a URL-encoded URL like this:\\n\\n`http://localhost:59002/PdfMaker.svc/GetPdf?url=http%3A%2F%2Fnews.ycombinator.com/`You may wonder why I\'m using [http://news.ycombinator.com](http://news.ycombinator.com) for the example above. I chose this as Hacker News is a very simple site; very few resources and a small page size. This means the service has less work to do when creating the PDF; it\'s a quick demo.\\n\\nI should say that this service is arguably \\\\*\\\\*not\\\\*\\\\* completely RESTful as each GET operation behind the scenes attempts to create a new PDF (arguably a side-effect). These should probably be POST operations as they create a new resource each time they\'re hit. However, if they were I wouldn\'t be able to just enter a URL into a browser for testing and that\'s really useful. So tough, I shake my fist at the devotees of pure REST on this occasion. (If I should be attacked in the street shortly after this blog is posted then the police should be advised this is good line of inquiry...)\\n\\n## Good behaviour\\n\\nIt\'s worth noting that `automaticFormatSelectionEnabled` set to true on the behaviour so that content negotiation is enabled. Obviously for the `GetPdf` action this is rather meaningless as it\'s a stream that\'s passed back. However, for the `GetPdfUrl` action the returned string can either be JSON or XML. The Fiddler screenshots below demonstrate this in action.\\n\\n## Test Harness\\n\\nAs a final touch I added in a test harness in the form of `Demo.aspx`. If you browse to it you\'ll see a screen a little like this:\\n\\n![](https://2.bp.blogspot.com/-zoyt7ufl9FQ/UOVmD0VPh0I/AAAAAAAAAYE/DnmZmbx-Mxc/s400/PdfMakerDemo.png)\\n\\nIt\'s fairly self-explanatory as you can see. And here\'s an example of the output generated when pointing at Hacker News:\\n\\n<iframe src=\\"https://docs.google.com/file/d/0B87K8-qxOZGFMGNCUWRneUFsVFU/preview\\" width=\\"500\\" height=\\"500\\"></iframe>\\n\\nAnd that\'s it. If there was a need this service could be easily extended to leverage the [various options](http://madalgo.au.dk/~jakobt/wkhtmltoxdoc/wkhtmltopdf-0.9.9-doc.html) that wkhtmltopdf makes available. Hope people find it useful."},{"id":"/2012/11/13/a-nicer-net-api-for-bloombergs-open-api","metadata":{"permalink":"/2012/11/13/a-nicer-net-api-for-bloombergs-open-api","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2012-11-13-a-nicer-net-api-for-bloombergs-open-api/index.md","source":"@site/blog/2012-11-13-a-nicer-net-api-for-bloombergs-open-api/index.md","title":"Getting up to speed with Bloomberg\'s Open API...","description":"A good portion of any devs life is usually spent playing with APIs. If you need to integrate some other system into the system you\'re working on (and it\'s rare to come upon a situation where this doesn\'t happen at some point) then it\'s API time.","date":"2012-11-13T00:00:00.000Z","formattedDate":"November 13, 2012","tags":[{"label":".NET","permalink":"/tags/net"},{"label":"c#","permalink":"/tags/c"},{"label":"Bloomberg","permalink":"/tags/bloomberg"},{"label":"Open API","permalink":"/tags/open-api"}],"readingTime":5.625,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Getting up to speed with Bloomberg\'s Open API...","authors":"johnnyreilly","tags":[".NET","c#","Bloomberg","Open API"],"hide_table_of_contents":false},"prevItem":{"title":"HTML to PDF using a WCF Service","permalink":"/2013/01/03/html-to-pdf-using-wcf-service"},"nextItem":{"title":"XSD/XML Schema Generator + Xsd.exe:Taking the pain out of manual XML","permalink":"/2012/11/02/xsdxml-schema-generator-xsdexe-taking"}},"content":"A good portion of any devs life is usually spent playing with APIs. If you need to integrate some other system into the system you\'re working on (and it\'s rare to come upon a situation where this doesn\'t happen at some point) then it\'s API time.\\n\\nSome APIs are well documented and nice to use. Some aren\'t. I recently spent a goodly period of time investigating [Bloomberg\'s Open API](http://www.openbloomberg.com/open-api/) and it was a slightly painful experience. So much so that I thought it best to write up my own experiences and maybe I can save others time and a bit of pain.\\n\\nAlso, as I investigated the Bloomberg Open API I found myself coming up with my own little mini-C#-API. (It\'s generally a sure sign you\'ve found an API you don\'t love if you end up writing your own wrapper.) This mini API did the heavy lifting for me and just handed back nicely structured data to deal with. I have included this wrapper here as well.\\n\\n## Research\\n\\nThe initial plan was to, through code, extract Libor and Euribor rates from Bloomberg. I had access to a Bloomberg terminal and I had access to the internet - what could stop me? After digging around for a little while I found some useful resources that could be accessed from the Bloomberg terminal:\\n\\n1. Typing \u201c`WAPI&lt;GO&gt;`\u201d into Bloomberg lead me to the Bloomberg API documentation.\\n2. Typing \u201c`DOCS 2055451&lt;GO&gt;`\u201d into Bloomberg (I know - it\'s a bit cryptic) provided me with samples of how to use the Bloomberg API in VBA\\n\\n![](https://4.bp.blogspot.com/-mZxP0-jXRIo/UKJ8y8Gs5AI/AAAAAAAAAW0/qNyIN9hGBiQ/s400/bloombergwapidocumentation.gif)\\n\\nTo go with this I found some useful documentation of the Bloomberg Open API [here](http://www.openbloomberg.com/files/2012/10/blpapi-developers-guide.pdf) and I found the .NET Bloomberg Open API itself [here](http://www.openbloomberg.com/open-api/).\\n\\n## Hello World?\\n\\nThe first goal when getting up to speed with an API is getting it to do something. Anything. Just stick a fork into it and see if it croaks. Sticking a fork into Open API was achieved by taking the 30-odd example apps included in the Bloomberg Open API and running each in turn on the Bloomberg box until I had my \\"he\'s alive!!\\" moment. (I did find it surprising that not all of the examples worked - I don\'t know if there\'s a good reason for this...)\\n\\nHowever, when I tried to write my own C# console application to interrogate the Open API it wasn\'t as plain sailing as I\'d hoped. I\'d write something that looked correct, compiled successfully and deploy it onto the Bloomberg terminal only to have it die a sad death whenever I tried to fire it off.\\n\\nI generally find the fastest way to get up and running with an API is to debug it. To make calls to the API and then examine, field by field and method by method, what is actually there. This wasn\'t really an option with my console app though. I was using a shared Bloomberg terminal with very limited access. No Visual Studio on the box and no remote debugging enabled.\\n\\nIt was then that I had something of a eureka moment. I realised that the code in the VBA samples I\'d downloaded from Bloomberg looked quite similar to the C# code samples that shipped with Open API. Hmmmm.... Shortly after this I found myself sat at the Bloomberg machine debugging the Bloomberg API using the VBA IDE in Excel. (For the record, these debugging tools are aren\'t too bad at all - they\'re nowhere near as slick as their VS counterparts but they do the job.) This was my [Rosetta Stone](http://en.wikipedia.org/wiki/Rosetta_Stone) \\\\- I could take what I\'d learned from the VBA samples and translate that into equivalent C# / .NET code (bearing in mind what I\'d learned from debugging in Excel and in fact sometimes bringing along the VBA comments themselves if they provided some useful insight).\\n\\n## He\'s the Bloomberg, I\'m the Wrapper\\n\\nSo I\'m off and romping... I have something that works. Hallelujah! Now that that hurdle had been crossed I found myself examining the actual Bloomberg API code itself. It functioned just fine but it did a couple of things that I wasn\'t too keen on:\\n\\n1. The Bloomberg API came with custom data types. I didn\'t want to use these unless it was absolutely necessary - I just wanted to stick to the standard .NET types. This way if I needed to hand data onto another application I wouldn\'t be making each of these applications dependant on the Bloomberg Open API.\\n2. To get the data out of the Bloomberg API there was an awful lot of boilerplate. Code which handled the possibilities of very large responses that might be split into several packages. Code which walked the element tree returned from Bloomberg parsing out the data. It wasn\'t a beacon of simplicity.\\n\\nI wanted an API that I could simply invoke with security codes and required fields. And in return I wanted to be passed nicely structured data. As I\'ve already mentioned a desire to not introduce unnecessary dependencies I thought it might well suit to make use of nested Dictionaries. I came up with a simple C# Console project / application which had a reference to the Bloomberg Open API. It contained the following class; essentially my wrapper for Open API operations: (please note this is deliberately a very \\"bare-bones\\" implementation)\\n\\n<script src=\\"https://gist.github.com/4065815.js?file=BloombergApi.cs\\"><\/script>\\n\\nThe project also contained this class which demonstrates how I made use of my wrapper:\\n\\n<script src=\\"https://gist.github.com/4065815.js?file=NicerBloombergApiDemo.cs\\"><\/script>\\n\\nThis covered my bases. It was simple, it was easy to consume and it didn\'t require any custom types. My mini-API is only really catering for my own needs (unsurprisingly). However, there\'s lots more to the Bloomberg Open API and I may end up taking this further in the future if I encounter use cases that my current API doesn\'t cover.\\n\\n## Update (07/12/2012)\\n\\nFinally, a PS. I found in the [Open API FAQs](http://www.openbloomberg.com/faq/) that _\\"Testing any of that functionality currently requires a valid Bloomberg Desktop API (DAPI), Server API (SAPI) or Managed B-Pipe subscription. Bloomberg is planning on releasing a stand-alone simulator which will not require a subscription.\\"_ There isn\'t any word yet on this stand-alone simulator. I emailed Bloomberg at [open-tech@bloomberg.net](mailto:open-tech@bloomberg.net) to ask about this. They kindly replied that _\\"Unfortunately it is not yet available. We understand that this makes testing API applications somewhat impractical, so we\'re continuing to work on this tool.\\"_ Fingers crossed for something we can test soon!\\n\\n## Note to self (because I keep forgetting)\\n\\nIf you\'re looking to investigate what data is available about a security in Bloomberg it\'s worth typing \u201c`FLDS&lt;GO&gt;`\u201d into Bloomberg. This is the Bloomberg Fields Finder. Likewise, if you\'re trying to find a security you could try typing \u201c`SECF&lt;GO&gt;`\u201d into Bloomberg as this is the Security Finder."},{"id":"/2012/11/02/xsdxml-schema-generator-xsdexe-taking","metadata":{"permalink":"/2012/11/02/xsdxml-schema-generator-xsdexe-taking","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2012-11-02-xsdxml-schema-generator-xsdexe-taking/index.md","source":"@site/blog/2012-11-02-xsdxml-schema-generator-xsdexe-taking/index.md","title":"XSD/XML Schema Generator + Xsd.exe:Taking the pain out of manual XML","description":"Is it 2003 again?!?","date":"2012-11-02T00:00:00.000Z","formattedDate":"November 2, 2012","tags":[{"label":"Xsd.exe","permalink":"/tags/xsd-exe"},{"label":"XSD/XML Schema Generator","permalink":"/tags/xsd-xml-schema-generator"},{"label":"LINQ to XML","permalink":"/tags/linq-to-xml"}],"readingTime":5.89,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"XSD/XML Schema Generator + Xsd.exe:Taking the pain out of manual XML","authors":"johnnyreilly","tags":["Xsd.exe","XSD/XML Schema Generator","LINQ to XML"],"hide_table_of_contents":false},"prevItem":{"title":"Getting up to speed with Bloomberg\'s Open API...","permalink":"/2012/11/13/a-nicer-net-api-for-bloombergs-open-api"},"nextItem":{"title":"MVC 3 meet Dictionary","permalink":"/2012/10/22/mvc-3-meet-dictionary"}},"content":"## Is it 2003 again?!?\\n\\nI\'ve just discovered Xsd.exe. It\'s not new. Or shiny. And in fact it\'s been around since .NET 1.1. Truth be told, I\'ve been aware of it for years but up until now I\'ve not had need of it. But now now I\'ve investigated it a bit I\'ve found that it, combined with the XSD/XML Schema Generator can make for a nice tool to add to the utility belt.\\n\\nGranted XML has long since stopped being sexy. But if you need it, as I did recently, then this is for you.\\n\\n## To the XML Batman!\\n\\nNow XML is nothing new to me (or I imagine anyone who\'s been developing within the last 10 years). But most of the time when I use XML I\'m barely aware that it\'s going on - by and large it\'s XML doing the heavy lifting underneath my web services. But the glory of this situation is, I never have to think about it. It just works. All I have to deal with are nice strongly typed objects which makes writing robust code a doddle.\\n\\nI recently came upon a situation where I was working with XML in the raw; that is to say strings. I was going to be supplied with strings of XML which would represent various objects. It would be my job to take the supplied XML, extract out the data I needed and proceed accordingly.\\n\\n## We Don\'t Need No Validation...\\n\\nI lied!\\n\\nIn order to write something reliable I needed to be able to validate that the supplied XML was as I expected. So, [XSD](<http://en.wikipedia.org/wiki/XML_Schema_(W3C)>) time. If you\'re familiar with XML then you\'re probably equally familar with XSD which, to quote Wikipedia _\\"can be used to express a set of rules to which an XML document must conform in order to be considered \'valid\'\\"_.\\n\\nNow I\'ve written my fair share of XSDs over the years and I\'ve generally found it a slightly tedious exercise. So I was delighted to discover an online tool to simplify the task. It\'s called the [XSD/XML Schema Generator](http://www.freeformatter.com/xsd-generator.html). What this marvellous tool does is allow you to enter an example of your XML which it then uses to reverse engineer an XSD.\\n\\nHere\'s an example. I plugged in this:\\n\\n<script src=\\"https://gist.github.com/4000326.js?file=contact.xml\\"><\/script>\\n\\nAnd pulled out this:\\n\\n<script src=\\"https://gist.github.com/4000326.js?file=contact.xsd\\"><\/script>\\n\\nFantastic! It doesn\'t matter if the tool gets something slightly wrong; you can tweak the generated XSD to your hearts content. This is great because it does the hard work for you, allowing you to step back, mop your brow and then heartily approve the results. This tool is a labour saving device. Put simply, it\'s a dishwasher.\\n\\n## Tools of the Trade\\n\\nHow to get to the actual data? I was initially planning to break out the [`XDocument`](<http://msdn.microsoft.com/en-us/library/system.xml.linq.xdocument(v=vs.100).aspx>), plug in my XSD and use the `Validate` method. Which would do the job just dandy.\\n\\nHowever I resisted. As much as I like LINQ to XML I turned to use [Xsd.exe](<http://msdn.microsoft.com/en-us/library/x6c1kb0s(v=vs.100).aspx>) instead. As I\'ve mentioned, this tool is as old as the hills. But there\'s gold in them thar hills, listen: _\\"The XML Schema Definition (Xsd.exe) tool generates XML schema or common language runtime classes from XDR, XML, and XSD files, or from classes in a runtime assembly.\\"_\\n\\nExcited? Thought not. But what this means is we can hurl our XSD at this tool and it will toss back a nicely formatted C# class for me to use. Good stuff! So how\'s it done? Well MSDN is roughly as informative as it ever is (which is to say, not terribly) but fortunately there\'s not a great deal to it. You fire up the Visual Studio Command Prompt (and I advise doing this in Administrator mode to escape permissions pain). Then you enter a command to generate your class. Here\'s an example using the Contact.xsd file we generated earlier:\\n\\n`xsd.exe \\"C:\\\\\\\\Contact.xsd\\" /classes /out:\\"C:\\\\\\\\\\" /namespace:\\"MyNameSpace\\"`\\n\\nAnd you\'re left with the lovely Contact.cs class:\\n\\n<script src=\\"https://gist.github.com/4000326.js?file=Contact.cs\\"><\/script>\\n\\n## Justify Your Actions\\n\\nBut why is this good stuff? Indeed why is this more interesting than the newer, and hence obviously cooler, LINQ to XML? Well for my money it\'s the following reasons that are important:\\n\\n1. Intellisense - I have always loved this. Call me lazy but I think intellisense frees up the mind to think about what problem you\'re actually trying to solve. Xsd.exe\'s generated classes give me that; I don\'t need to hold the whole data structure in my head as I code.\\n2. Terse code - I\'m passionate about less code. I think that a noble aim in software development is to write as little code as possible in order to achieve your aims. I say this as generally I have found that writing a minimal amount of code expresses the intention of the code in a far clearer fashion. In service of that aim Xsd.exe\'s generated classes allow me to write less code than would be required with LINQ to XML.\\n3. To quote Scott Hanselman \\"[successful compilation is just the first unit test](http://www.hanselman.com/blog/NuGetPackageOfTheWeek6DynamicMalleableEnjoyableExpandoObjectsWithClay.aspx)\\". That it is but it\'s a doozy. If I\'m making changes to the code and I\'ve been using LINQ to XML I\'m not going to see the benefits of strong typing that I would with Xsd.exe\'s generated classes. I like learning if I\'ve broken the build sooner rather than later; strong typing gives me that safety net.\\n\\n## Serialization / Deserialization Helper\\n\\nAs you read this you\'re no doubt thinking \\"but wait he\'s shown us how to create XSDs from XML and classes from XSDs but how do we take XML and turn it into objects? And how do we turn those objects back into XML?\\"\\n\\nSee how I read your mind just there? It\'s a gift. Well, I\'ve written a little static helper class for the very purpose:\\n\\n<script src=\\"https://gist.github.com/4000326.js?file=XmlConverter.cs\\"><\/script>\\n\\nAnd here\'s an example of how to use it:\\n\\n<script src=\\"https://gist.github.com/4000326.js?file=XmlConverterUsage.cs\\"><\/script>\\n\\nI was tempted to name my methods in tribute to Crockford\'s JSON (namely `ToXML` becoming `stringify` and `ToObject` becoming `parse`). Maybe later.\\n\\nAnd that\'s us done. Whilst it\'s no doubt unfashionable I think that this is a very useful approach indeed and I commend it to the interweb!\\n\\n## Update - using Xsd.exe to generate XSD from XML\\n\\nI was chatting to a friend about this blog post and he mentioned that you can actually use Xsd.exe to generate XSD files from XML as well. He\'s quite right - this feature does exist. To go back to our example from earlier we can execute the following command:\\n\\n`xsd.exe \\"C:\\\\\\\\Contact.xml\\" /out:\\"C:\\\\\\\\\\" `And this will generate the following file:\\n\\n<script src=\\"https://gist.github.com/4000326.js?file=Generated by XSD contact.xsd\\"><\/script>\\n\\nHowever, the XSD generated above is very much a \\"Microsoft XSD\\"; it\'s an XSD which features MS properties and so on. It\'s fine but I think that generally I prefer my XSDs to be as vanilla as possible. To that end I\'m likely to stick to using the XSD/XML Schema Generator as it doesn\'t appear to be possible to get Xsd.exe to generate \\"vanilla XSD\\".\\n\\nThanks to Ajay for bringing it to my attention though."},{"id":"/2012/10/22/mvc-3-meet-dictionary","metadata":{"permalink":"/2012/10/22/mvc-3-meet-dictionary","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2012-10-22-mvc-3-meet-dictionary/index.md","source":"@site/blog/2012-10-22-mvc-3-meet-dictionary/index.md","title":"MVC 3 meet Dictionary","description":"Documenting a JsonValueProviderFactory Gotcha","date":"2012-10-22T00:00:00.000Z","formattedDate":"October 22, 2012","tags":[{"label":"MVC 3","permalink":"/tags/mvc-3"},{"label":"Dictionary","permalink":"/tags/dictionary"}],"readingTime":2.66,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"MVC 3 meet Dictionary","authors":"johnnyreilly","tags":["MVC 3","Dictionary"],"hide_table_of_contents":false},"prevItem":{"title":"XSD/XML Schema Generator + Xsd.exe:Taking the pain out of manual XML","permalink":"/2012/11/02/xsdxml-schema-generator-xsdexe-taking"},"nextItem":{"title":"Using Web Optimization with MVC 3","permalink":"/2012/10/05/using-web-optimization-with-mvc-3"}},"content":"## Documenting a JsonValueProviderFactory Gotcha\\n\\nAbout a year ago I was involved in the migration of an ASP.NET WebForms application over to MVC 3. We\'d been doing a lot of AJAX-y / Single Page Application-y things in the project and had come to the conclusion that MVC might be a slightly better fit since we intended to continue down this path.\\n\\nDuring the migration we encountered a bug in MVC 3 concerning Dictionary deserialization. This bug has subsequently tripped me up a few more times as I failed to remember the nature of the problem correctly. So I\'ve written the issue up here as an aide to my own lamentable memory.\\n\\nBefore I begin I should say that the problem \\\\*<u>has been resolved in MVC 4</u>\\n\\n\\\\*. However given that I imagine many MVC 3 projects will not upgrade instantly there\'s probably some value in documenting the issue (and how to work around it). By the way, you can see my initial plea for assistance in [this StackOverflow question](http://stackoverflow.com/q/6881440/761388).\\n\\n## The Problem\\n\\nThe problem is that deserialization of Dictionary objects does not behave in the expected and desired fashion. When you fire off a dictionary it arrives at your endpoint as the enormously unhelpful `null`. To see this for yourself you can try using this JavaScript:\\n\\n<script src=\\"https://gist.github.com/3931778.js?file=PostDictionaryTest.js\\"><\/script>\\n\\nWith this C#:\\n\\n<script src=\\"https://gist.github.com/3931778.js?file=HomeController.cs\\"><\/script>\\n\\nYou get a null `null` dictionary.\\n\\nAfter a long time googling around on the topic I eventually discovered, much to my surprise, that I was actually tripping over a bug in MVC 3. It was filed by [Darin Dimitrov](http://stackoverflow.com/users/29407/darin-dimitrov) of Stack Overflow fame and I found details about it filed as an official bug [here](http://connect.microsoft.com/VisualStudio/feedback/details/636647/make-jsonvalueproviderfactory-work-with-dictionary-types-in-asp-net-mvc). To quote Darin:\\n\\n\\"_The System.Web.Mvc.JsonValueProviderFactory introduced in ASP.NET MVC 3 enables action methods to send and receive JSON-formatted text and to model-bind the JSON text to parameters of action methods. Unfortunately it doesn\'t work with dictionaries_\\"\\n\\n## The Workaround\\n\\nMy colleague found a workaround for the issue [here](http://stackoverflow.com/a/5397743/761388). There are 2 parts to this:\\n\\n1. Dictionaries in JavaScript are simple JavaScript Object Literals. In order to workaround this issue it is necessary to `JSON.stringify` our Dictionary / JOL before sending it to the endpoint. This is done so a string can be picked up at the endpoint.\\n2. The signature of your action is switched over from a Dictionary reference to a string reference. Deserialization is then manually performed back from the string to a Dictionary within the Action itself.\\n\\nI\'ve adapted my example from earlier to demonstrate this; first the JavaScript:\\n\\n<script src=\\"https://gist.github.com/3931778.js?file=PostDictionaryTestWorkaround.js\\"><\/script>\\n\\nThen the C#:\\n\\n<script src=\\"https://gist.github.com/3931778.js?file=HomeControllerWorkaround.cs\\"><\/script>\\n\\nAnd now we\'re able to get a dictionary.\\n\\n## Summary and a PS\\n\\nSo that\'s it; a little unglamourous but this works. I\'m slightly surprised that that wasn\'t picked up before MVC 3 was released but at least it\'s been fixed for MVC 4. I look forward to this blog post being irrelevant and out of date \u263a.\\n\\nFor what it\'s worth in my example above we\'re using the trusty old `System.Web.Script.Serialization.JavaScriptSerializer` to perform deserialization. My preference is actually to use [JSON.Nets](http://james.newtonking.com/projects/json-net.aspx) implementation but for the sake of simplicity I went with .NETs internal one here. To be honest, either is fine to my knowledge."},{"id":"/2012/10/05/using-web-optimization-with-mvc-3","metadata":{"permalink":"/2012/10/05/using-web-optimization-with-mvc-3","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2012-10-05-using-web-optimization-with-mvc-3/index.md","source":"@site/blog/2012-10-05-using-web-optimization-with-mvc-3/index.md","title":"Using Web Optimization with MVC 3","description":"A while ago I wrote about optimally serving up JavaScript in web applications. I mentioned that Microsoft had come up with a NuGet package called Microsoft ASP.NET Web Optimization which could help with that by minifying and bundling CSS and JavaScript. At the time I was wondering if I would be able to to use this package with pre-existing MVC 3 projects (given that the package had been released together with MVC 4). Happily it turns out you can. But it\'s not quite as straightforward as I might have liked so I\'ve documented how to get going with this here...","date":"2012-10-05T00:00:00.000Z","formattedDate":"October 5, 2012","tags":[{"label":"asp.net","permalink":"/tags/asp-net"},{"label":"Bundling","permalink":"/tags/bundling"},{"label":"MVC 3","permalink":"/tags/mvc-3"},{"label":"Web Optimization","permalink":"/tags/web-optimization"},{"label":"Minification","permalink":"/tags/minification"}],"readingTime":3.61,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Using Web Optimization with MVC 3","authors":"johnnyreilly","tags":["asp.net","Bundling","MVC 3","Web Optimization","Minification"],"hide_table_of_contents":false},"prevItem":{"title":"MVC 3 meet Dictionary","permalink":"/2012/10/22/mvc-3-meet-dictionary"},"nextItem":{"title":"Unit Testing and Entity Framework: The Filth and the Fury","permalink":"/2012/10/03/unit-testing-and-entity-framework-filth"}},"content":"A while ago I [wrote](http://icanmakethiswork.blogspot.com/2012/06/how-im-structuring-my-javascript-in-web.html#WebOptimization) about optimally serving up JavaScript in web applications. I mentioned that Microsoft had come up with a NuGet package called [Microsoft ASP.NET Web Optimization](http://nuget.org/packages/Microsoft.AspNet.Web.Optimization) which could help with that by minifying and bundling CSS and JavaScript. At the time I was wondering if I would be able to to use this package with pre-existing MVC 3 projects (given that the package had been released together with MVC 4). Happily it turns out you can. But it\'s not quite as straightforward as I might have liked so I\'ve documented how to get going with this here...\\n\\n## Getting the Basics in Place\\n\\nTo keep it simple I\'m going to go through taking a \\"vanilla\\" MVC 3 app and enhancing it to work with Web Optimization. To start, follow these basic steps:\\n\\n1. Open Visual Studio (bet you didn\'t see that coming!)\\n2. Create a new MVC 3 application (I called mine \\"WebOptimizationWithMvc3\\" to demonstrate my imaginative flair). It doesn\'t really matter which sort of MVC 3 project you create - I chose an Intranet application but really that\'s by the by.\\n3. Update pre-existing NuGet packages\\n4. At the NuGet console type: \\"`Install-Package Microsoft.AspNet.Web.Optimization`\\"\\n\\nWhilst the NuGet package adds the necessary references to your MVC 3 project it doesn\'t add the corresponding namespaces to the web.configs. To fix this manually add the following child XML element to the `&lt;namespaces&gt;` element in your root and Views web.config files:\\n\\n`&lt;add namespace=\\"System.Web.Optimization\\" /&gt;`\\n\\nThis gives you access to `Scripts` and `Styles` in your views without needing the fully qualified namespace. For reasons best known to Microsoft I had to close down and restart Visual Studio before intellisense started working. You may need to do likewise.\\n\\nNext up we want to get some JavaScript / CSS bundles in place. To do this, create a folder in the root of your project called \\"App_Start\\". There\'s nothing magical about this to my knowledge; this is just a convention that\'s been adopted to store all the bits of startup in one place and avoid clutterage. (I think this grew out of Nuget; see [David Ebbo talking about this here](http://blog.davidebbo.com/2011/02/appstart-folder-convention-for-nuget.html).) Inside your new folder you should add a new class called `BundleConfig.cs` which looks like this:\\n\\n<script src=\\"https://gist.github.com/3839486.js?file=BundleConfig.cs\\"><\/script>\\n\\nThe above is what you get when you create a new MVC 4 project (as it includes Web Optimization out of the box). All it does is create some JavaScript and CSS bundles relating to jQuery, jQuery UI, jQuery Validate, Modernizr and the standard site CSS. Nothing radical here but this example should give you an idea of how bundling can be configured and used. To make use of `BundleConfig.cs` you should modify your `Global.asax.cs` so it looks like this:\\n\\n<script src=\\"https://gist.github.com/3839486.js?file=Global.asax.cs\\"><\/script>\\n\\nOnce you\'ve done this you\'re ready to start using Web Optimization in your MVC 3 application.\\n\\n## Switching over \\\\_Layout.cshtml to use Web Optimization\\n\\nWith a \\"vanilla\\" MVC 3 app the only use of CSS and JavaScript files is found in `_Layout.cshtml`. To switch over to using Web Optimization you should replace the existing `_Layout.cshtml` with this: (you\'ll see that the few differences that there are between the 2 are solely around the replacement of link / script tags with references to `Scripts` and `Styles` instead)\\n\\n<script src=\\"https://gist.github.com/3839486.js?file=_Layout.cshtml\\"><\/script>\\n\\nDo note that in the above `Scripts.Render` call we\'re rendering out 3 bundles; jQuery, jQuery UI and jQuery Validate. We\'re not using any of these in `_Layout.cshtml` but rendering these (and their associated link tags) gives us a chance to demonstrate that everything is working as expected.\\n\\nIn your root web.config file make sure that the following tag is in place: `&lt;compilation debug=\\"<b>true</b>\\" targetFramework=\\"4.0\\"&gt;`. Then run, the generated HTML should look something like this:\\n\\n<script src=\\"https://gist.github.com/3839486.js?file=debug  true\\"><\/script>\\n\\nThis demonstrates that when the application has debug set to true you see the full scripts / links being rendered out as you would hope (to make your debugging less painful).\\n\\nNow go back to your root `web.config` file and chance the debug tag to false: `&lt;compilation debug=\\"<b>false</b>\\" targetFramework=\\"4.0\\"&gt;`. This time when you run, the generated HTML should look something like this:\\n\\n<script src=\\"https://gist.github.com/3839486.js?file=debug  false\\"><\/script>\\n\\nThis time you can see that in non-debug mode (ie how it would run in Production) minified bundles of scripts and css files are being served up instead of the raw files. And that\'s it; done."},{"id":"/2012/10/03/unit-testing-and-entity-framework-filth","metadata":{"permalink":"/2012/10/03/unit-testing-and-entity-framework-filth","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2012-10-03-unit-testing-and-entity-framework-filth/index.md","source":"@site/blog/2012-10-03-unit-testing-and-entity-framework-filth/index.md","title":"Unit Testing and Entity Framework: The Filth and the Fury","description":"Just recently I\'ve noticed that there appears to be something of a controversy around Unit Testing and Entity Framework. I first came across it as I was Googling around for useful posts on using MOQ in conjunction with EF. I\'ve started to notice the topic more and more and as I have mixed feelings on the subject (that is to say I don\'t have a settled opinion) I thought I\'d write about this and see if I came to any kind of conclusion...","date":"2012-10-03T00:00:00.000Z","formattedDate":"October 3, 2012","tags":[{"label":"unit testing","permalink":"/tags/unit-testing"},{"label":"Entity Framework","permalink":"/tags/entity-framework"},{"label":"anti-pattern","permalink":"/tags/anti-pattern"},{"label":"MOQ","permalink":"/tags/moq"}],"readingTime":7.315,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Unit Testing and Entity Framework: The Filth and the Fury","authors":"johnnyreilly","tags":["unit testing","Entity Framework","anti-pattern","MOQ"],"hide_table_of_contents":false},"prevItem":{"title":"Using Web Optimization with MVC 3","permalink":"/2012/10/05/using-web-optimization-with-mvc-3"},"nextItem":{"title":"Giving OData to CRM 4.0","permalink":"/2012/09/24/giving-odata-to-crm-40"}},"content":"Just recently I\'ve noticed that there appears to be something of a controversy around Unit Testing and Entity Framework. I first came across it as I was Googling around for useful posts on using MOQ in conjunction with EF. I\'ve started to notice the topic more and more and as I have mixed feelings on the subject (that is to say I don\'t have a settled opinion) I thought I\'d write about this and see if I came to any kind of conclusion...\\n\\n## The Setup\\n\\nIt started as I was working on a new project. We were using ASP.NET MVC 3 and Entity Framework with DbContext as our persistence layer. Rather than crowbarring the tests in afterwards the intention was to write tests to support the ongoing development. Not quite test driven development but certainly [test supported development](http://blog.troyd.net/Test+Supported+Development+TSD+Is+NOT+Test+Driven+Development+TDD.aspx). (Let\'s not get into the internecine conflict as to whether this is black belt testable code or not - it isn\'t but he who pays the piper etc.) Oh and we were planning to use MOQ as our mocking library.\\n\\nIt was the first time I\'d used DbContext rather than ObjectContext and so I thought I\'d do a little research on how people were using DbContext with regards to testability. I had expected to find that there was some kind of consensus and an advised way forwards. I didn\'t get that at all. Instead I found a number of conflicting opinions.\\n\\n## Using the Repository / Unit of Work Patterns\\n\\nOne thread of advice that came out was that people advised using the Repository / Unit of Work patterns as wrappers when it came to making testable code. This is kind of interesting in itself as to the best of my understanding ObjectSet / ObjectContext and DbSet / DbContext are both in themselves implementations of the Repository / Unit of Work patterns. So the advice was to build a Repository / Unit of Work pattern to wrap an existing Repository / Unit of Work pattern.\\n\\nNot as mad as it sounds. The reason for the extra abstraction is that ObjectContext / DbContext in the raw are not MOQ-able.\\n\\n## Or maybe I\'m wrong, maybe you can MOQ DbContext?\\n\\nNo you can\'t. Well that\'s not true. You can and it\'s documented [here](http://romiller.com/2012/02/14/testing-with-a-fake-dbcontext/) but there\'s a \\"but\\". You need to be using Entity Frameworks Code First approach; actually coding up your DbContext yourself. Before I\'d got on board the project had already begun and we were already some way down the road of using the Database First approach. So this didn\'t seem to be a go-er really.\\n\\nThe best article I found on testability and Entity Framework was [this one](http://msdn.microsoft.com/en-us/library/ff714955.aspx) by [K. Scott Allen](http://odetocode.com/) which essentially detailed how you could implement the Repository / Unit of Work patterns on top of ObjectSet / ObjectContext. In the end I adapted this to do the same thing sat on top of DbSet / DbContext instead.\\n\\nWith this in place I had me my testable code. I was quite happy with this as it seemed quite intelligible. My new approach looked similar to the existing DbSet / DbContext code and so there wasn\'t a great deal of re-writing to do. Sorted, right?\\n\\n## Here come the nagging doubts...\\n\\nI did wonder, given that I found a number of articles about applying the Repository / Unit of Work patterns on top of ObjectSet / ObjectContext that there didn\'t seem to be many examples to do the same for DbSet / DbContext. (I did find a few examples of this but none that felt satisfactory to me for a variety of reasons.) This puzzled me.\\n\\nI also started to notice that a 1 man war was being waged against the approach I was using by [Ladislav Mrnka](http://www.ladislavmrnka.com/about/). Here are a couple of examples of his crusade:\\n\\n- [An answer on StackOverflow](http://stackoverflow.com/a/6904479/761388) (there\'s quite a few similar answers around on StackOverflow saying similar)\\n- [A comment on Rowan Millers post about fake DbContexts](http://romiller.com/2012/02/14/testing-with-a-fake-dbcontext/#div-comment-1620)\\n\\nLadislav is quite strongly of the opinion that wrapping DbSet / DbContext (and I presume ObjectSet / ObjectContext too) in a further Repository / Unit of Work is an antipattern. To quote him: _\\"The reason why I don\u2019t like it is leaky abstraction in Linq-to-entities queries ... In your test you have Linq-to-Objects which is superset of Linq-to-entities and only subset of queries written in L2O is translatable to L2E\\"_. It\'s worth looking at [Jon Skeets explanation of \\"leaky abstractions\\"](http://www.youtube.com/watch?v=gNeSZYke-_Q) which he did for TekPub.\\n\\nAs much as I didn\'t want to admit it - I have come to the conclusion Ladislav probably has a point for a number of reasons:\\n\\n### 1\\\\. Just because it compiles and passes unit tests don\'t imagine that means it works...\\n\\nUnfortunately, a LINQ query that looks right, compiles and has passing unit tests written for it doesn\'t necessarily work. You can take a query that fails when executed against Entity Framework and come up with test data that will pass that unit test. As Ladislav rightly points out: `LINQ-to-Objects != LINQ-to-Entities`.\\n\\nSo in this case unit tests of this sort don\'t provide you with any security. What you need are \\\\*\\\\*<u>integration</u>\\n\\n\\\\*\\\\* tests. Tests that run against an instance of the database and demonstrate that LINQ will actually translate queries / operations into valid SQL.\\n\\n### 2\\\\. Complex queries\\n\\nYou can write some pretty complex LINQ queries if you want. This is made particularly easy if you\'re using [comprehension syntax](https://blogs.msdn.com/b/ericlippert/archive/2009/12/07/query-transformations-are-syntactic.aspx). Whilst these queries may be simple to write it can be uphill work to generate test data to satisfy this. So much so that at times it can feel you\'ve made a rod for your own back using this approach.\\n\\n### 3\\\\. Lazy Loading\\n\\nBy default Entity Framework employs lazy loading. This a useful approach which reduces the amount of data that is transported. Sometimes this approach forces you to specify up front if you require a particular entity through use of `Include` statements. This again doesn\'t lend itself to testing particularly well.\\n\\n## Where does this leave us?\\n\\nHaving considered all of the above for a while and tried out various different approaches I think I\'m coming to the conclusion that Ladislav is probably right. Implementing the Repository / Unit of Work patterns on top of ObjectSet / ObjectContext or DbSet / DbContext doesn\'t seem a worthwhile effort in the end.\\n\\nSo what\'s a better idea? I think that in the name of simplicity you might as well have a simple class which wraps all of your Entity Framework code. This class could implement an interface and hence be straightforwardly MOQ-able (or alternatively all methods could be virtual and you could forego the interface). Along with this you should have integration tests in place which test the execution of the actual Entity Framework code against a test database.\\n\\nNow I should say this approach is not necessarily my final opinion. It seems sensible and practical. I think it is likely to simplify the tests that are written around a project. It will certainly be more reliable than just having unit tests in place.\\n\\nIn terms of the project I\'m working on at the moment we\'re kind of doing this in a halfway house sense. That is to say, we\'re still using our Repository / Unit of Work wrappers for DbSet / DbContext but where things move away from simple operations we\'re adding extra methods to our Unit of Work class or Repository classes which wrap this functionality and then testing it using our integration tests.\\n\\nI\'m open to the possibility that my opinion may be modified further. And I\'d be very interested to know what other people think on the subject.\\n\\n## Update\\n\\nIt turns out that I\'m not alone in thinking about this issue and indeed others have expressed this rather better than me - take a look at Jimmy Bogard\'s post for an example: [http://lostechies.com/jimmybogard/2012/09/20/limiting-your-abstractions/](http://lostechies.com/jimmybogard/2012/09/20/limiting-your-abstractions/).\\n\\n## Update 2\\n\\nI\'ve also recently watched the following Pluralsight course by Julie Lerman: [http://pluralsight.com/training/Courses/TableOfContents/efarchitecture#efarchitecture-m3-archrepo](http://pluralsight.com/training/Courses/TableOfContents/efarchitecture#efarchitecture-m3-archrepo). In this course Julie talks about different implementations of the Repository and Unit of Work patterns in conjunction with Entity Framework. Julie is in favour of using this approach but in this module she elaborates on different \\"flavours\\" of these patterns that you might want to use for different reasons (bounded contexts / reference contexts etc). She makes a compelling case and helpfully she is open enough to say that this a point of contention in the community. At the end of watching this I think I felt happy that our \\"halfway house\\" approach seems to fit and seems to work. More than anything else Julie made clear that there isn\'t one definitively \\"true\\" approach. Rather many different but similar approaches for achieving the same goal. Good stuff Julie!"},{"id":"/2012/09/24/giving-odata-to-crm-40","metadata":{"permalink":"/2012/09/24/giving-odata-to-crm-40","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2012-09-24-giving-odata-to-crm-40/index.md","source":"@site/blog/2012-09-24-giving-odata-to-crm-40/index.md","title":"Giving OData to CRM 4.0","description":"Just recently I was tasked with seeing if we could provide a way to access our Dynamics CRM instance via OData. My initial investigations made it seem like there was nothing for me to do; CRM 2011 provides OData support out of the box. Small problem. We were running CRM 4.0.","date":"2012-09-24T00:00:00.000Z","formattedDate":"September 24, 2012","tags":[{"label":"OData","permalink":"/tags/o-data"},{"label":"WCF Data Services","permalink":"/tags/wcf-data-services"},{"label":"CRM 4.0","permalink":"/tags/crm-4-0"},{"label":"LINQ","permalink":"/tags/linq"}],"readingTime":3.715,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Giving OData to CRM 4.0","authors":"johnnyreilly","tags":["OData","WCF Data Services","CRM 4.0","LINQ"],"hide_table_of_contents":false},"prevItem":{"title":"Unit Testing and Entity Framework: The Filth and the Fury","permalink":"/2012/10/03/unit-testing-and-entity-framework-filth"},"nextItem":{"title":"Globalize and jQuery Validation","permalink":"/2012/09/06/globalize-and-jquery-validate"}},"content":"Just recently I was tasked with seeing if we could provide a way to access our Dynamics CRM instance via OData. My initial investigations made it seem like there was nothing for me to do; [CRM 2011 provides OData support out of the box](http://msdn.microsoft.com/en-us/library/gg309461.aspx). Small problem. We were running CRM 4.0.\\n\\nIt could well have ended there apart from the fact that Microsoft makes it astonishingly easy to to create your own OData service using WCF Data Services. Because it\'s so straightforward I was able to get an OData solution for CRM 4.0 up and running with very little heavy lifting at all. Want to know how it\'s done?\\n\\n## LINQ to CRM\\n\\nTo start with you\'re going to need the [CRM SDK 4.0](http://www.microsoft.com/en-us/download/details.aspx?id=38). This contains a \\"vanilla\\" LINQ to CRM client which is used in each of the example applications that can be found in `microsoft.xrm\\\\samples`. We want this client (or something very like it) to use as the basis for our OData service.\\n\\nIn order to get a LINQ to CRM provider that caters for your own customised CRM instance you need to use the `crmsvcutil` utility from the CRM SDK (found in the `microsoft.xrm\\\\tools\\\\` directory). Detailed instructions on how to use this can be found in this Word document: `microsoft.xrm\\\\advanced_developer_extensions_-_developers_guide.docx`. Extra information around the topic can be found using these links:\\n\\n- [MSDN docs on xRM](http://msdn.microsoft.com/en-us/library/ff681559)\\n- [MSDN examples of LINQ queries](http://msdn.microsoft.com/en-us/library/ff681573)\\n- [CRM blog site](http://www.dynamicscrmtrickbag.com/)\\n- [Another site listing examples of LINQ to CRM](http://community.adxstudio.com/products/adxstudio-portals/developers-guide/archive/linq-to-crm-22/)\\n\\nYou should end up with custom generated data context classes which look not dissimilar to similar classes that you may already have in place for Entity Framework etc. With your `Xrm.DataContext` in hand (a subclass of `Microsoft.Xrm.Client.Data.Services.CrmDataContext`) you\'ll be ready to move forwards.\\n\\n## Make me an OData Service\\n\\nAs I said, Microsoft makes it fantastically easy to get an OData service up and running. [In this example](http://msdn.microsoft.com/en-US/library/dd728275) an entity context model is created from the Northwind database and then exposed as an OData service. To create my CRM OData service I followed a similar process. But rather than creating an entity context model using a database I plugged in the `Xrm.DataContext` instance of CRM that we created a moment ago. These are the steps I followed to make my service:\\n\\n1. Create a new ASP.NET Web Application called \\"CrmOData\\" (in case it\'s relevant I was using Visual Studio 2010 to do this).\\n2. Remove all ASPXs / JavaScript / CSS files etc leaving you with an essentially empty project.\\n3. Add references to the following DLLs that come with the SDK: - microsoft.crm.sdk.dll\\n\\n   - microsoft.crm.sdktypeproxy.dll\\n   - microsoft.crm.sdktypeproxy.xmlserializers.dll\\n   - microsoft.xrm.client.dll\\n   - microsoft.xrm.portal.dll\\n   - microsoft.xrm.portal.files.dll\\n\\n4. Add the `&lt;microsoft.xrm.client&gt;` config section to your web.config (not forgetting the associated Xrm connection string)\\n5. Add this new file below to the root of the project:\\n\\n<script src=\\"https://gist.github.com/3765280.js?file=Crm.svc.cs\\"><\/script>\\n\\nAnd that\'s it - done. When you run this web application you will find an OData service exposed at http://localhost:12345/Crm.svc. You could have it even simpler if you wanted - you could pull out the logging that\'s in place and leave only the `InitializeService` there. That\'s all you need. (The `GetEntityById` method is a helper method of my own for identifying the GUIDs of CRM.)\\n\\nYou may have noticed that I have made use of caching for my OData service following the steps I found [here](https://blogs.msdn.com/b/peter_qian/archive/2010/11/17/using-asp-net-output-caching-with-wcf-data-services.aspx). Again you may or may not want to use this.\\n\\n## Now, a warning...\\n\\nOkay - not so much a warning as a limitation. Whilst most aspects of the OData service work as you would hope there is no support for the $select operator. I had a frustrating time trying to discover why and then came upon this explanation:\\n\\n_\\"$select statements are not supported. This problem is being discussed here [http://social.msdn.microsoft.com/Forums/en/adodotnetdataservices/thread/366086ee-dcef-496a-ad15-f461788ae678](http://social.msdn.microsoft.com/Forums/en/adodotnetdataservices/thread/366086ee-dcef-496a-ad15-f461788ae678) and is caused by the fact that CrmDataContext implements the IExpandProvider interface which in turn causes the DataService to lose support for $select projections\\"_\\n\\nYou can also see [here](http://social.microsoft.com/Forums/en/crmdevelopment/thread/31daedb4-3d75-483a-8d7f-269af3375d74) for the original post discussing this.\\n\\n## Finishing off\\n\\nIn the example I set out here I used the version of WCF Data Services that shipped with Visual Studio 2010. WCF Data Services now ships separately from the .NET Framework and you can [pick up the latest and greatest from Nuget](http://nuget.org/packages?q=wcf+data+services). I understand that you could easily switch over to using the latest versions but since I didn\'t see any feature that I needed on this occasion I haven\'t.\\n\\nI hope you find this useful."},{"id":"/2012/09/06/globalize-and-jquery-validate","metadata":{"permalink":"/2012/09/06/globalize-and-jquery-validate","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2012-09-06-globalize-and-jquery-validate/index.md","source":"@site/blog/2012-09-06-globalize-and-jquery-validate/index.md","title":"Globalize and jQuery Validation","description":"Update 05/10/2015","date":"2012-09-06T00:00:00.000Z","formattedDate":"September 6, 2012","tags":[{"label":"asp.net mvc","permalink":"/tags/asp-net-mvc"},{"label":"Internationalisation","permalink":"/tags/internationalisation"},{"label":"Globalization","permalink":"/tags/globalization"},{"label":"Globalize JS","permalink":"/tags/globalize-js"},{"label":"Localisation","permalink":"/tags/localisation"},{"label":"jQuery Validation","permalink":"/tags/j-query-validation"},{"label":"jQuery.validate.js","permalink":"/tags/j-query-validate-js"}],"readingTime":3.785,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Globalize and jQuery Validation","authors":"johnnyreilly","tags":["asp.net mvc","Internationalisation","Globalization","Globalize JS","Localisation","jQuery Validation","jQuery.validate.js"],"hide_table_of_contents":false},"prevItem":{"title":"Giving OData to CRM 4.0","permalink":"/2012/09/24/giving-odata-to-crm-40"},"nextItem":{"title":"How to attribute encode a PartialView in MVC (Razor)","permalink":"/2012/08/24/how-to-attribute-encode-partialview-in"}},"content":"## Update 05/10/2015\\n\\nIf you\'re after a version of this that works with Globalize 1.x then take a look [here](https://blog.johnnyreilly.com/2015/10/jquery-validation-globalize-hits-10.html).\\n\\n## Update 27/08/2013\\n\\nTo make it easier for people to use the approach detailed in this post I have created a repository for `jquery.validate.globalize.js` on GitHub [here](https://github.com/johnnyreilly/jquery-validation-globalize).\\n\\nThis is also available as a nuget package [here](https://www.nuget.org/packages/jQuery.Validation.Globalize/).\\n\\nTo see a good demo take a look [here](http://jqueryvalidationunobtrusivenative.azurewebsites.net/AdvancedDemo/Globalize).\\n\\n## Background\\n\\n[I\'ve written before about a great little library called Globalize](http://icanmakethiswork.blogspot.co.uk/2012/05/globalizejs-number-and-date.html) which makes locale specific number / date formatting simple within JavaScript. And I\'ve just stumbled upon an [old post written by Scott Hanselman about the business of Globalisation / Internationalisation / Localisation within ASP.NET](http://www.hanselman.com/blog/GlobalizationInternationalizationAndLocalizationInASPNETMVC3JavaScriptAndJQueryPart1.aspx). It\'s a great post and I recommend reading it (I\'m using many of the approaches he discusses).\\n\\n## jQuery Global is dead... Long live Globalize!\\n\\nHowever, there\'s one tweak I would make to Scotts suggestions and that\'s to use Globalize in place of the jQuery Global plugin. The jQuery Global plugin has now effectively been reborn as Globalize (with no dependancy on jQuery). As far as I can tell jQuery Global is now disappearing from the web - certainly the link in Scotts post is dead now at least. I\'ve ~~ripped off~~ been inspired by the \\"Globalized jQuery Unobtrusive Validation\\" section of Scotts article and made `jquery.validate.globalize.js`.\\n\\nAnd for what it\'s worth `jquery.validate.globalize.js` applies equally to standard jQuery Validation as well as to jQuery Unobtrusive Validation. I say that as the above JavaScript is effectively a monkey patch to the number / date / range / min / max methods of jQuery.validate.js which forces these methods to use Globalize\'s parsing support instead.\\n\\nHere\'s the JavaScript:\\n\\n<script src=\\"https://gist.github.com/3651751.js?file=jquery.validate.globalize.js\\"><\/script>\\n\\nThe above script does 2 things. Firstly it monkey patches jquery.validate.js to make use of Globalize.js number and date parsing in place of the defaults. Secondly it initialises Globalize to relevant current culture driven by the `html lang` property. So if the html tag looked like this:\\n\\n```html\\n<html lang=\\"de-DE\\">\\n  ...\\n</html>\\n```\\n\\nThen Globalize would be initialised with the \\"de-DE\\" culture assuming that culture was available and had been served up to the client. (By the way, the Globalize initialisation logic has only been placed in the code above to demonstrate that Globalize needs to be initialised to the culture. It\'s more likely that this initialisation step would sit elsewhere in a \\"proper\\" app.)\\n\\n## Wait, where\'s `html lang` getting set?\\n\\nIn Scott\'s article he created a `MetaAcceptLanguage` helper to generate a META tag like this: `&lt;meta name=\\"accept-language\\" content=\\"en-GB\\" /&gt;` which he used to drive Globalizes specified culture.\\n\\nRather than generating a meta tag I\'ve chosen to use the `lang` attribute of the `html` tag to specify the culture. I\'ve chosen to do this as it\'s more in line with the [W3C spec](http://www.w3.org/TR/i18n-html-tech-lang/#ri20030510.102829377). But it should be noted this is just a different way of achieving exactly the same end.\\n\\nSo how\'s it getting set? Well, it\'s no great shakes; in my `_Layout.cshtml` file my html tag looks like this:\\n\\n```html\\n<html lang=\\"@System.Globalization.CultureInfo.CurrentUICulture.Name\\"></html>\\n```\\n\\nAnd in my `web.config` I have following setting set:\\n\\n```xml\\n<configuration>\\n  <system.web>\\n    <globalization culture=\\"auto\\" uiCulture=\\"auto\\" />\\n    \x3c!--- Other stuff.... --\x3e\\n  </system.web>\\n</configuration>\\n```\\n\\nWith both of these set this means I get `&lt;html lang=\\"de-DE\\"&gt;` or `&lt;html lang=\\"en-GB\\"&gt;` etc. depending on a users culture.\\n\\n## Serving up the right Globalize culture files\\n\\nIn order that I send the correct Globalize culture to the client I\'ve come up with this static class which provides the user with the relevant culture URL (falling back to the en-GB culture if it can\'t find one based your culture):\\n\\n<script src=\\"https://gist.github.com/3651751.js?file=GlobalizeUrls.cs\\"><\/script>\\n\\n## Putting it all together\\n\\nTo make use of all of this together you\'ll need to have the `html lang` attribute set as described earlier and some scripts output in your layout page like this:\\n\\n```html\\n<script src=\\"@Url.Content(\\"~/Scripts/jquery.js\\")\\" type=\\"text/javascript\\"><\/script>\\n<script src=\\"@Url.Content(GlobalizeUrls.Globalize)\\" type=\\"text/javascript\\"><\/script>\\n<script src=\\"@Url.Content(GlobalizeUrls.GlobalizeCulture)\\" type=\\"text/javascript\\"><\/script>\\n<script src=\\"@Url.Content(\\"~/Scripts/jquery.validate.js\\")\\" type=\\"text/javascript\\"><\/script>\\n<script src=\\"@Url.Content(\\"~/scripts/jquery.validate.globalize.js\\")\\" type=\\"text/javascript\\"><\/script>\\n\\n@* Only serve the following script if you need it: *@\\n<script src=\\"@Url.Content(\\"~/scripts/jquery.validate.unobtrusive.js\\")\\" type=\\"text/javascript\\"><\/script>\\n```\\n\\nWhich will render something like this:\\n\\n```html\\n<script src=\\"/Scripts/jquery.js\\" type=\\"text/javascript\\"><\/script>\\n<script src=\\"/Scripts/globalize.js\\" type=\\"text/javascript\\"><\/script>\\n<script\\n  src=\\"/scripts/globalize/globalize.culture.en-GB.js\\"\\n  type=\\"text/javascript\\"\\n><\/script>\\n<script src=\\"/Scripts/jquery.validate.js\\" type=\\"text/javascript\\"><\/script>\\n<script\\n  src=\\"/Scripts/jquery.validate.globalize.js\\"\\n  type=\\"text/javascript\\"\\n><\/script>\\n<script\\n  src=\\"/Scripts/jquery.validate.unobtrusive.js\\"\\n  type=\\"text/javascript\\"\\n><\/script>\\n```\\n\\nThis will load up jQuery, Globalize, your Globalize culture, jQuery Validate, jQuery Validates unobtrusive extensions (which you don\'t need if you\'re not using them) and the jQuery Validate Globalize script which will set up culture aware validation.\\n\\nFinally and just to re-iterate, it\'s highly worthwhile to give [Scott Hanselman\'s original article a look](http://www.hanselman.com/blog/GlobalizationInternationalizationAndLocalizationInASPNETMVC3JavaScriptAndJQueryPart1.aspx). Most all the ideas in here were taken wholesale from him!"},{"id":"/2012/08/24/how-to-attribute-encode-partialview-in","metadata":{"permalink":"/2012/08/24/how-to-attribute-encode-partialview-in","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2012-08-24-how-to-attribute-encode-partialview-in/index.md","source":"@site/blog/2012-08-24-how-to-attribute-encode-partialview-in/index.md","title":"How to attribute encode a PartialView in MVC (Razor)","description":"This post is plagiarism. But I\'m plagiarising myself so I don\'t feel too bad.","date":"2012-08-24T00:00:00.000Z","formattedDate":"August 24, 2012","tags":[{"label":"asp.net mvc","permalink":"/tags/asp-net-mvc"},{"label":"encode","permalink":"/tags/encode"},{"label":"PartialView","permalink":"/tags/partial-view"},{"label":"razor","permalink":"/tags/razor"},{"label":"attribute","permalink":"/tags/attribute"}],"readingTime":2.42,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"How to attribute encode a PartialView in MVC (Razor)","authors":"johnnyreilly","tags":["asp.net mvc","encode","PartialView","razor","attribute"],"hide_table_of_contents":false},"prevItem":{"title":"Globalize and jQuery Validation","permalink":"/2012/09/06/globalize-and-jquery-validate"},"nextItem":{"title":"ClosedXML - the real SDK for Excel","permalink":"/2012/08/16/closedxml-real-sdk-for-excel"}},"content":"This post is plagiarism. But I\'m plagiarising myself so I don\'t feel too bad.\\n\\nI posted a [question](http://stackoverflow.com/q/12093005/761388) on StackOverflow recently asking if there was a simple way to attribute encode a PartialView in Razor / ASP.NET MVC. I ended up answering my own question and since I thought it was a useful solution it might be worth sharing.\\n\\n## The Question\\n\\nIn the project I was working on I was using PartialViews to store the HTML that would be rendered in a tooltip in my ASP.NET MVC application. (In case you\'re curious I was using the [jQuery Tools library for my tooltip](http://jquerytools.org/demos/tooltip/index.html) effect.)\\n\\nI had thought that Razor, clever beast that it is, would automatically attribute encode anything sat between quotes in my HTML. Unfortunately this doesn\'t appear to be the case. In the short term I was able to workaround this by using single quotation marks to encapsulate my PartialViews HTML. See below for an example:\\n\\n```xml\\n<div class=\\"tooltip\\"\\n     title=\'@Html.Partial(\\"_MyTooltipInAPartial\\")\'>\\n    Some content\\n</div>\\n```\\n\\nNow this worked just fine but I was aware that if any PartialView needed to use single quotation marks I would have a problem. Let\'s say for a moment that `_MyTooltipInAPartial.cshtml` contained this:\\n\\n```xml\\n<span style=\\"color:green\\">fjkdsjf\'lksdjdlks</span>\\n```\\n\\nWell when I used my handy little single quote workaround, the following would result:\\n\\n```xml\\n<div class=\\"tooltip\\"\\n     title=\'<span style=\\"color:green\\">fjkdsjf\'lksdjdlks</span>\'>\\n    Some content\\n</div>\\n```\\n\\nWhich although it doesn\'t show up so well in the code sample above is definite _\\"does not compute, does not compute, does not compute \\\\*LOUD EXPLOSION\\\\*\\"_ territory.\\n\\n## The Answer\\n\\nThis took me back to my original intent which was to encapsulate the HTML in double quotes like this:\\n\\n```xml\\n<div class=\\"tooltip\\"\\n     title=\\"@Html.Partial(\\"_MyTooltipInAPartial\\")\\">\\n    Some content\\n</div>\\n```\\n\\nThough with the example discussed above we clearly had a problem whether we used single or double quotes. What to do?\\n\\nWell the answer wasn\'t too complicated. After a little pondering I ended up scratching my own itch by writing an HTML helper method called `PartialAttributeEncoded` which made use of `HttpUtility.HtmlAttributeEncode` to HTML attribute encode a PartialView.\\n\\nHere\'s the code:\\n\\n<script src=\\"https://gist.github.com/3449462.js?file=PartialExtensions.cs\\"><\/script>\\n\\nUsing the above helper is simplicity itself:\\n\\n```xml\\n<div class=\\"tooltip\\"\\n     title=\\"@Html.PartialAttributeEncoded(\\"_MyTooltipInAPartial\\")\\">\\n    Some content\\n</div>\\n```\\n\\nAnd, given the example I\'ve been going through, it would provide you with this output:\\n\\n```xml\\n<div class=\\"tooltip\\"\\n     title=\\"&lt;span style=&quot;color:green&quot;>fjkdsjf&#39;lksdjdlks</span>\\">\\n    Some content\\n</div>\\n```\\n\\nNow the HTML in the title attribute above might be an unreadable mess - but it\'s the unreadable mess you need. That\'s what the HTML we\'ve been discussing looks like when it\'s been encoded.\\n\\n## Final thoughts\\n\\nI was surprised that Razor didn\'t handle this out of the box. I wonder if this is something that will come along with a later version? It\'s worth saying that I experienced this issue when working on an MVC 3 application. It\'s possible that this issue may actually have been solved with MVC 4 already; I haven\'t had chance to check yet though."},{"id":"/2012/08/16/closedxml-real-sdk-for-excel","metadata":{"permalink":"/2012/08/16/closedxml-real-sdk-for-excel","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2012-08-16-closedxml-real-sdk-for-excel/index.md","source":"@site/blog/2012-08-16-closedxml-real-sdk-for-excel/index.md","title":"ClosedXML - the real SDK for Excel","description":"Simplicity appeals to me. It always has. Something that is simple is straightforward to comprehend and is consequently easy to use. It\'s clarity.","date":"2012-08-16T00:00:00.000Z","formattedDate":"August 16, 2012","tags":[{"label":"MDeLeon","permalink":"/tags/m-de-leon"},{"label":"Open XML","permalink":"/tags/open-xml"},{"label":"Excel","permalink":"/tags/excel"},{"label":"ClosedXML","permalink":"/tags/closed-xml"}],"readingTime":3.74,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"ClosedXML - the real SDK for Excel","authors":"johnnyreilly","tags":["MDeLeon","Open XML","Excel","ClosedXML"],"hide_table_of_contents":false},"prevItem":{"title":"How to attribute encode a PartialView in MVC (Razor)","permalink":"/2012/08/24/how-to-attribute-encode-partialview-in"},"nextItem":{"title":"jQuery Unobtrusive Validation (+ associated gotchas)","permalink":"/2012/08/06/jquery-unobtrusive-validation"}},"content":"Simplicity appeals to me. It always has. Something that is simple is straightforward to comprehend and is consequently easy to use. It\'s clarity.\\n\\n## Open XML\\n\\nSo imagine my joy when I first encountered [Open XML](http://msdn.microsoft.com/en-us/office/bb265236.aspx). In Microsofts own words:\\n\\nECMA Office Open XML (\\"Open XML\\") is an international, open standard for word-processing documents, presentations, and spreadsheets that can be freely implemented by multiple applications on multiple platforms.\\n\\nWhat does that actually mean? Well, from my perspective in the work I was doing I needed to be able to programmatically interact with Excel documents from C#. I needed to be able to create spreadsheets, to use existing template spreadsheets which I could populate dynamically in code. I needed to do Excel. And according to Microsoft, the Open XML SDK was how I did this.\\n\\nWhat can I say about it? Open XML works. The API functions. You can use this to achieve your aims; and I did (initially). However, there\'s a but and it\'s this: it became quickly apparent just how hard Open XML makes you work to achieve relatively simple goals. Things that ought to be, in my head, a doddle require reams and reams of obscure code. Sadly, I feel that Open XML is probably the most frustrating API that I have yet encountered (and I\'ve coded against the old school Lotus Notes API).\\n\\n## Closed XML - Open XML\'s DbContext\\n\\nAs I\'ve intimated I found Open XML to be enormously frustrating. I\'d regularly find myself thinking I\'d achieved my goal. I may have written War and Peace code-wise but it compiled, it looked right - the end was in sight. More fool me. I\'d run, sit back watch my Excel doc get created / updated / whatever. Then I\'d open it and be presented with some obscure error about a corrupt file. Not great.\\n\\nAs I was Googling around looking for answers to my problem that I discovered an open source project on CodePlex called [Closed XML](http://closedxml.codeplex.com/). I wasn\'t alone in frustrations with Open XML - there were many of us sharing the same opinion. And some fantastic person had stepped into the breach to save us! In ClosedXMLs own words:\\n\\nClosedXML makes it easier for developers to create Excel 2007/2010 files. It provides a nice object oriented way to manipulate the files (similar to VBA) without dealing with the hassles of XML Documents. It can be used by any .NET language like C# and Visual Basic (VB).\\n\\nHallelujah!!!\\n\\nThe way it works (as far as I understand) is that ClosedXML sits on top of Open XML and exposes a really straightforward API for you to interact with. I haven\'t looked into the guts of it but my guess is that it internally uses Open XML to achieve this (as to use ClosedXML you must reference DocumentFormat.OpenXml.dll).\\n\\nI\'ve found myself thinking of ClosedXML\'s relationship to Open XML in the same way as I think about Entity Frameworks DbContexts relationship to ObjectContext. They do the same thing but the former in both cases offers a better API. They makes achieving the same goals \\\\***much**\\\\* easier. (Although in fairness to the EF team I should say that ObjectContext was not particularly problematic to use; just DbContext made life even easier.)\\n\\n## Support - This is how it should be done!\\n\\nShortly after I started using ClosedXML I was asked if we could use it to perform a certain task. I tested. We couldn\'t.\\n\\nWhen I discovered this [I raised a ticket](http://closedxml.codeplex.com/workitem/8174) against the project asking if the functionality was likely to be added at any point. I honestly didn\'t expect to hear back any time soon and was mentally working out ways to get round the issue for now.\\n\\nTo my surprise within _5 hours_[MDeLeon](http://www.codeplex.com/site/users/view/MDeLeon) the developer behind ClosedXML had released a patch to the source code! By any stretch of the imagination that is fast! As it happened there were a few bugs that needed ironing out and over the course of the next 3 working days MDeLeon performed a number of fixes and left me quickly in the position of having a version of ClosedXML which allowed me to achieve my goal.\\n\\nSo this blog post exists in part to point anyone who is battling Open XML to ClosedXML. It\'s brilliant, well documented and I\'d advise anyone to use it. You won\'t be disappointed. And in part I wanted to say thanks and well done to MDeLeon who quite made my week! Thank you!\\n\\n[http://closedxml.codeplex.com/](http://closedxml.codeplex.com/)"},{"id":"/2012/08/06/jquery-unobtrusive-validation","metadata":{"permalink":"/2012/08/06/jquery-unobtrusive-validation","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2012-08-06-jquery-unobtrusive-validation/index.md","source":"@site/blog/2012-08-06-jquery-unobtrusive-validation/index.md","title":"jQuery Unobtrusive Validation (+ associated gotchas)","description":"I was recently working on a project which had client side validation manually set up which essentially duplicated the same logic on the server. Like many things this had started out small and grown and grown until it became arduos and tedious to maintain.","date":"2012-08-06T00:00:00.000Z","formattedDate":"August 6, 2012","tags":[{"label":"jquery unobtrusive validation","permalink":"/tags/jquery-unobtrusive-validation"}],"readingTime":4.475,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"jQuery Unobtrusive Validation (+ associated gotchas)","authors":"johnnyreilly","tags":["jquery unobtrusive validation"],"hide_table_of_contents":false},"prevItem":{"title":"ClosedXML - the real SDK for Excel","permalink":"/2012/08/16/closedxml-real-sdk-for-excel"},"nextItem":{"title":"Rendering Partial View to a String","permalink":"/2012/07/16/rendering-partial-view-to-string"}},"content":"I was recently working on a project which had client side validation manually set up which essentially duplicated the same logic on the server. Like many things this had started out small and grown and grown until it became arduos and tedious to maintain.\\n\\nTime to break out the unobtrusive jQuery validation.\\n\\nIf you\u2019re not aware of this, as part of MVC 3 Microsoft leveraged the pre-existing [jQuery Validate library](http://bassistance.de/jquery-plugins/jquery-plugin-validation/) and introduced an \u201cunobtrusive\u201d extension to this which allows the library to be driven by HTML 5 data attributes. I have mentioned this lovely extension before but I haven\'t been using it for the last 6 months or so. And coming back to it I realised that I had forgotten a few of the details / quirks.\\n\\nFirst up, \\"where do these HTML 5 data attributes come from?\\" I hear you cry. Why from the [Validation attributes that live in System.ComponentModel.DataAnnotations](http://msdn.microsoft.com/en-us/library/system.componentmodel.dataannotations.validationattribute.aspx).\\n\\nLet me illustrate. This decoration:\\n\\n```cs\\n[Required(),\\n   Range(0.01, Double.MaxValue, ErrorMessage = \\"A positive value is required for Price\\"),\\n   Display(Name = \\"My Price\\")]\\n  public double Price { get; set; }\\n```\\n\\nspecifies that the Price field on the model is required, that it requires a positive numeric value and that it\u2019s official name is \u201cMy Price\u201d. As a result of this decoration, when you use syntax like this in your view:\\n\\n```xml\\n@Html.LabelFor(x => x.Price)\\n  @Html.TextBoxFor(x => x.Price, new { id = \\"itsMyPrice\\", type = \\"number\\" })\\n```\\n\\nYou end up with this HTML:\\n\\n```xml\\n<label for=\\"Price\\">My Price</label>\\n  <input data-val=\\"true\\" data-val-number=\\"The field My Price must be a number.\\" data-val-range=\\"A positive value is required for My Price\\" data-val-range-max=\\"1.79769313486232E+308\\" data-val-range-min=\\"0.01\\" data-val-required=\\"The My Price field is required.\\" id=\\"itsMyPrice\\" name=\\"Price\\" type=\\"number\\" value=\\"\\">\\n```\\n\\nAs you can see MVC has done the hard work of translating these data annotations into HTML 5 data attributes so you don\u2019t have to. With this in place you can apply your validation in 1 place (the model) and 1 place only. This reduces the code you need to write exponentially. It also reduces duplication and therefore reduces the likelihood of mistakes.\\n\\nTo validate a form it\u2019s as simple as this:\\n\\n```js\\n$(\'form\').validate();\\n```\\n\\nOr if you wanted to validate a single element:\\n\\n```js\\n$(\'form\').validate().element(\'elementSelector\');\\n```\\n\\nOr if you wanted to prevent default form submission until validation was passed:\\n\\n```js\\n$(\'form\').submit(function (event) {\\n  var isValid = $(this).validate().valid();\\n\\n  return isValid; //True will allow submission, false will not\\n});\\n```\\n\\nSee what I mean? Simple!\\n\\nIf you want to read up on this further I recommend these links:\\n\\n- [The home of jQuery Validate](http://bassistance.de/jquery-plugins/jquery-plugin-validation/) \\\\- by the way it seems to be important to work with the latest version (1.9 at time of writing). I found some strange AJAX issues when using 1.7...\\n- [Brad Wilson\'s walkthrough of unobtrusive client validation](http://bradwilson.typepad.com/blog/2010/10/mvc3-unobtrusive-validation.html)\\n- [An example of how to implement your own custom validation both server side \\\\*and\\\\* client side](http://www.devtrends.co.uk/blog/the-complete-guide-to-validation-in-asp.net-mvc-3-part-2)\\n- [How to apply unobtrusive jQuery validation to dynamic content](http://xhalent.wordpress.com/2011/01/24/applying-unobtrusive-validation-to-dynamic-content/) \\\\- handy if you\'re creating HTML on the client which you want to be validated.\\n- And finally, a workaround for [a bug in MVC 3](http://aspnet.codeplex.com/workitem/7629) which means that data attributes aren\u2019t emitted when using DropDownListFor for nested objects: [http://forums.asp.net/t/1649193.aspx/1/10](http://forums.asp.net/t/1649193.aspx/1/10). In fact because I\'ve only seen this on a forum I\'ve copied and the pasted the code there to below because I feared it being lost: **Update: It turns out the self-same issue exists for TextAreaFor as well. Details of this and a workaround can be found [here](http://aspnet.codeplex.com/workitem/8576)... **\\n\\n```cs\\n/// <summary>\\n    /// MVC HtmlHelper extension methods - html element extensions\\n    /// These are drop down list extensions that work round a bug in MVC 3: http://aspnet.codeplex.com/workitem/7629\\n    /// These workarounds were taken from here: http://forums.asp.net/t/1649193.aspx/1/10\\n    /// </summary>\\n    public static class DropDownListExtensions\\n    {\\n        [SuppressMessage(\\"Microsoft.Design\\", \\"CA1006:DoNotNestGenericTypesInMemberSignatures\\", Justification = \\"This is an appropriate nesting of generic types\\")]\\n        public static MvcHtmlString SelectListFor<TModel, TProperty>(this HtmlHelper<TModel> htmlHelper, Expression<Func<TModel, TProperty>> expression, IEnumerable<SelectListItem> selectList)\\n        {\\n            return SelectListFor(htmlHelper, expression, selectList, null /* optionLabel */, null /* htmlAttributes */);\\n        }\\n\\n\\n        [SuppressMessage(\\"Microsoft.Design\\", \\"CA1006:DoNotNestGenericTypesInMemberSignatures\\", Justification = \\"This is an appropriate nesting of generic types\\")]\\n        public static MvcHtmlString SelectListFor<TModel, TProperty>(this HtmlHelper<TModel> htmlHelper, Expression<Func<TModel, TProperty>> expression, IEnumerable<SelectListItem> selectList, object htmlAttributes)\\n        {\\n            return SelectListFor(htmlHelper, expression, selectList, null /* optionLabel */, new RouteValueDictionary(htmlAttributes));\\n        }\\n\\n\\n        [SuppressMessage(\\"Microsoft.Design\\", \\"CA1006:DoNotNestGenericTypesInMemberSignatures\\", Justification = \\"This is an appropriate nesting of generic types\\")]\\n        public static MvcHtmlString SelectListFor<TModel, TProperty>(this HtmlHelper<TModel> htmlHelper, Expression<Func<TModel, TProperty>> expression, IEnumerable<SelectListItem> selectList, IDictionary<string, object> htmlAttributes)\\n        {\\n            return SelectListFor(htmlHelper, expression, selectList, null /* optionLabel */, htmlAttributes);\\n        }\\n\\n\\n        [SuppressMessage(\\"Microsoft.Design\\", \\"CA1006:DoNotNestGenericTypesInMemberSignatures\\", Justification = \\"This is an appropriate nesting of generic types\\")]\\n        public static MvcHtmlString SelectListFor<TModel, TProperty>(this HtmlHelper<TModel> htmlHelper, Expression<Func<TModel, TProperty>> expression, IEnumerable<SelectListItem> selectList, string optionLabel)\\n        {\\n            return SelectListFor(htmlHelper, expression, selectList, optionLabel, null /* htmlAttributes */);\\n        }\\n\\n\\n        [SuppressMessage(\\"Microsoft.Design\\", \\"CA1006:DoNotNestGenericTypesInMemberSignatures\\", Justification = \\"This is an appropriate nesting of generic types\\")]\\n        public static MvcHtmlString SelectListFor<TModel, TProperty>(this HtmlHelper<TModel> htmlHelper, Expression<Func<TModel, TProperty>> expression, IEnumerable<SelectListItem> selectList, string optionLabel, object htmlAttributes)\\n        {\\n            return SelectListFor(htmlHelper, expression, selectList, optionLabel, new RouteValueDictionary(htmlAttributes));\\n        }\\n\\n\\n        [SuppressMessage(\\"Microsoft.Design\\", \\"CA1011:ConsiderPassingBaseTypesAsParameters\\", Justification = \\"Users cannot use anonymous methods with the LambdaExpression type\\")]\\n        [SuppressMessage(\\"Microsoft.Design\\", \\"CA1006:DoNotNestGenericTypesInMemberSignatures\\", Justification = \\"This is an appropriate nesting of generic types\\")]\\n        public static MvcHtmlString SelectListFor<TModel, TProperty>(this HtmlHelper<TModel> htmlHelper, Expression<Func<TModel, TProperty>> expression, IEnumerable<SelectListItem> selectList, string optionLabel, IDictionary<string, object> htmlAttributes)\\n        {\\n            if (expression == null)\\n            {\\n                throw new ArgumentNullException(\\"expression\\");\\n            }\\n\\n\\n            ModelMetadata metadata = ModelMetadata.FromLambdaExpression(expression, htmlHelper.ViewData);\\n\\n\\n            IDictionary<string, object> validationAttributes = htmlHelper\\n                .GetUnobtrusiveValidationAttributes(ExpressionHelper.GetExpressionText(expression), metadata);\\n\\n\\n            if (htmlAttributes == null)\\n                htmlAttributes = validationAttributes;\\n            else\\n                htmlAttributes = htmlAttributes.Concat(validationAttributes).ToDictionary(k => k.Key, v => v.Value);\\n\\n\\n            return SelectExtensions.DropDownListFor(htmlHelper, expression, selectList, optionLabel, htmlAttributes);\\n        }\\n    }\\n```"},{"id":"/2012/07/16/rendering-partial-view-to-string","metadata":{"permalink":"/2012/07/16/rendering-partial-view-to-string","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2012-07-16-rendering-partial-view-to-string/index.md","source":"@site/blog/2012-07-16-rendering-partial-view-to-string/index.md","title":"Rendering Partial View to a String","description":"Well done that man!","date":"2012-07-16T00:00:00.000Z","formattedDate":"July 16, 2012","tags":[{"label":"asp.net mvc","permalink":"/tags/asp-net-mvc"},{"label":"Kevin Craft","permalink":"/tags/kevin-craft"},{"label":"PartialView","permalink":"/tags/partial-view"},{"label":"JsonResult","permalink":"/tags/json-result"}],"readingTime":4.055,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Rendering Partial View to a String","authors":"johnnyreilly","tags":["asp.net mvc","Kevin Craft","PartialView","JsonResult"],"hide_table_of_contents":false},"prevItem":{"title":"jQuery Unobtrusive Validation (+ associated gotchas)","permalink":"/2012/08/06/jquery-unobtrusive-validation"},"nextItem":{"title":"Optimally Serving Up JavaScript","permalink":"/2012/07/01/how-im-structuring-my-javascript-in-web"}},"content":"## Well done that man!\\n\\nEvery now and then I\'m thinking to myself \\"_wouldn\'t it be nice if you could do x..._\\" And then I discover that someone else has thought the self same thoughts and better yet they have the answer! I had this situation recently and discovered the wonderful Kevin Craft had been there, done that and made the T-shirt. Here\'s his blog: [http://craftycodeblog.com/2010/05/15/asp-net-mvc-render-partial-view-to-string/](http://craftycodeblog.com/2010/05/15/asp-net-mvc-render-partial-view-to-string/) I wanted to talk about how this simple post provided me with an elegant solution to something I\'ve found niggling and unsatisfactory for a while now... ## How it helped\\n\\nJust last week I was thinking about `Partial Views`. Some background. I\'m working on an ASP.NET MVC 3 project which provides users with a nice web interface to manage the workflow surrounding certain types of financial asset. The user is presented with a web page which shows a kind of grid to the user. As the user hovers over a row they are presented with a context menu which allows them to perform certain workflow actions. If they perform an action then that row will need to be updated to reflect this. Back in the day this would have been achieved by doing a full postback to the server. At the server the action would be taken, the persistent storage updated and then the whole page would be served up to the user again with the relevant row of HTML updated but everything else staying as is. Now there\'s nothing wrong with this approach as such. I mean it works just fine. But in my case since I knew that it was only that single row of HTML that was going to be updated and so I was loath to re-render the whole page. It seemed a waste to get so much data back from the server when only a marginal amount was due to change. And also I didn\'t want the user to experience the screen refresh flash. Looks ugly. Now in the past when I\'ve had a solution to this problem which from a UI perspective is good but from a development perspective slightly unsatisfactory. I would have my page call a controller method (via `jQuery.ajax`) to perform the action. This controller would return a `JsonResult` indicating success or failure and any data necessary to update the screen. Then in the `success` function I would manually update the HTML on the screen using the data provided. Now this solution works but there\'s a problem. [Can you tell what it is yet?](http://en.wikipedia.org/wiki/Rolf_Harris) It\'s not very DRY. I\'m repeating myself. When the page is initially rendered I have a `View` which renders (in this example) all the relevant HTML for the screen \\\\*including\\\\* the HTML for my rows of data. And likewise I have my JavaScript method for updating the screen too. So with this solution I have duplicated my GUI logic. If I update 1, I need to update the other. It\'s not a massive hardship but it is, as I say, unsatisfactory. I was recently thinking that it would be nice if I could refactor my row HTML into a `Partial View` which I could then use in 2 places: 1. In my standard `View` as I iterated through each element for display and 2. Nested inside a `JsonResult`...\\n\\nThe wonderful thing about approach 2 is that it allows me to massively simplify my `success` to this:\\n\\n```js\\n$(\'myRowSelector\').empty().html(data.RowHTML); //Where RowHTML is the property that\\n//contains my stringified PartialView\\n```\\n\\nand if I later make changes to the `Partial View` these changes will not require me to make any changes to my JavaScript at all. Brilliant! And entirely satisfactory. On the grounds that someone else might have had the same idea I did a little googling around. Sure enough I discovered [Kevin Craft\'s post](http://craftycodeblog.com/2010/05/15/asp-net-mvc-render-partial-view-to-string/) which was just the ticket. It does exactly what I\'d hoped. Besides being a nice and DRY solution this approach has a number of other advantages as well: - Given it\'s a `Partial View` the Visual Studio IDE provides a nice experience when coding it up with regards to intellisense / highlighting etc. Not something available when you\'re hand coding up a string which contains the HTML you\'d like passed back...\\n\\n- A wonderful debug experience. You can debug the rendering of a `Partial View` being rendered to a string in the same way as if the ASP.NET MVC framework was serving it up. I could have lived without this but it\'s fantastic to have it available.\\n- It\'s possible to nest \\\\***multiple**\\\\* `Partial Views` within your `JsonResult`. THIS IS WONDERFUL!!! This means that if several parts of your screen need to be updated (perhaps the row and a status panel as well) then as long as both are refactored into a `Partial View` you can generate them on the fly and pass them back.\\n\\nExcellent stuff!\\n\\n```\\n\\n```"},{"id":"/2012/07/01/how-im-structuring-my-javascript-in-web","metadata":{"permalink":"/2012/07/01/how-im-structuring-my-javascript-in-web","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2012-07-01-how-im-structuring-my-javascript-in-web/index.md","source":"@site/blog/2012-07-01-how-im-structuring-my-javascript-in-web/index.md","title":"Optimally Serving Up JavaScript","description":"I have occasionally done some server-side JavaScript with Rhino and Node.js but this is the exception rather than the rule. Like most folk at the moment, almost all the JavaScript I write is in a web context.","date":"2012-07-01T00:00:00.000Z","formattedDate":"July 1, 2012","tags":[{"label":"asp.net mvc","permalink":"/tags/asp-net-mvc"},{"label":"html helper","permalink":"/tags/html-helper"},{"label":"javascript","permalink":"/tags/javascript"},{"label":"cassette","permalink":"/tags/cassette"}],"readingTime":6.03,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Optimally Serving Up JavaScript","authors":"johnnyreilly","tags":["asp.net mvc","html helper","javascript","cassette"],"hide_table_of_contents":false},"prevItem":{"title":"Rendering Partial View to a String","permalink":"/2012/07/16/rendering-partial-view-to-string"},"nextItem":{"title":"Reasons to be Cheerful (why now is a good time to be a dev)","permalink":"/2012/06/04/reasons-to-be-cheerful-why-now-is-good"}},"content":"I have occasionally done some server-side JavaScript with Rhino and Node.js but this is the exception rather than the rule. Like most folk at the moment, almost all the JavaScript I write is in a web context.\\n\\nOver time I\'ve come to adopt a roughly standard approach to how I structure my JavaScript; both the JavaScript itself and how it is placed / rendered in the an HTML document. I wanted to write about the approach I\'m using. Partly just to document the approach but also because I often find writing about something crystalises my feelings on the subject in one way or another. I think that most of what I\'m doing is sensible and rational but maybe as I write about this I\'ll come to some firmer conclusions about my direction of travel.\\n\\n## What are you up to?\\n\\nBefore I get started it\'s probably worth mentioning the sort of web development I\'m generally called to do (as this has obviously influenced my decisions).\\n\\nMost of my work tends to be on web applications used internally within a company. That is to say, web applications accessible on a Company intranet. Consequently, the user base for my applications tends to be smaller than the Amazons and Googles of this world. It almost invariably sits on the ASP.NET stack in some way. Either classic WebForms or MVC.\\n\\n## \\"Render first. JS second.\\"\\n\\nI took 2 things away from [Steve Souder\'s article](http://www.stevesouders.com/blog/2010/09/30/render-first-js-second/):\\n\\n1. Async script loading is better than synchronous script loading\\n2. Get your screen rendered and \\\\***then**\\\\* execute your JavaScript\\n\\nI\'m not doing any async script loading as yet; although I am thinking of giving it a try at some point. In terms of choosing a loader I\'ll probably give RequireJS first crack of the whip (purely as it looks like most people are tending it\'s direction and that can\'t be without reason).\\n\\nHowever - it seems that the concept of async script loading is kind of conflict with one of the other tenets of web wisdom: script bundling. Script bundling, if you\'re not already aware, is the idea that you should combine all your scripts into a single file and then just serve that. This prevents multiple HTTP requests as each script loads in. Async script loading is obviously okay with multiple HTTP requests, presumably because of the asynchronous non-blocking pattern of loading. So. 2 different ideas. And there\'s further movement on this front right now as [Microsoft are baking in script bundling to .NET 4.5](http://www.hanselman.com/blog/VisualStudio2012RCIsReleasedTheBigWebRollup.aspx).\\n\\nRather than divide myself between these 2 horses I have at the moment tried to follow the \\"JS second\\" part of this advice in my own (perhaps slightly old fashioned) way...\\n\\n## I want to serve you...\\n\\nI have been making sure that scripts are the last thing served to the screen by using a customised version of [Michael J. Ryan\'s HtmlHelper](http://frugalcoder.us/post/2009/06/29/Handling-Scripts-in-ASPNet-MVC.aspx). This lovely helper allows you to add script references as required from a number of different sources (layout page, view, partial view etc - even the controller if you so desired). It\'s simple to control the ordering of scripts by allowing you to set a priority for each script which determines the render order.\\n\\nThen as a final step before rendering the `&lt;/body&gt;` tag the scripts can be rendered in one block. By this point the web page is rendered visually and a marginal amount of blocking is, in my view, acceptable.\\n\\nIf anyone is curious - the class below is my own version of Michael\'s helper. My contribution is the go faster stripes relating to the caching suffix and the ability to specify dependancies using script references rather than using numeric priority mechanism):\\n\\n<script src=\\"https://gist.github.com/3019159.js?file=ScriptExtensions.cs\\"><\/script>\\n\\n## Minification - I want to serve you less...\\n\\nAnother tweak I made to the script helper meant that when compiling either the debug or production (minified) versions of common JS files will be included if available. This means in a production environment the users get minified JS files so faster loading. And in a development environment we get the full JS files which make debugging more straightforward.\\n\\nWhat I haven\'t started doing is minifying my own JS files as yet. I know I\'m being somewhat inconsistent here by sometimes serving minified files and sometimes not. I\'m not proud. Part of my rationale for this that since most of my users use my apps on a daily basis they will for the most part be using cached JS files. Obviously there\'ll be slightly slower load times the first time they go to a page but nothing that significant I hope.\\n\\nI have thought of starting to do my own minification as a build step but have held off for now. Again this is something being baked into .NET 4.5; another reason why I have held off doing this a different way for now.\\n\\nUpdate\\n\\nIt now looks like this Microsofts optimisations have become [this Nuget package](http://nuget.org/packages/Microsoft.AspNet.Web.Optimization). It\'s early days (well it was released on 15th August 2012 and I\'m writing this on the 16th) but I think this looks not to be tied to MVC 4 or .NET 4.5 in which case I could use it in my current MVC 3 projects. I hope so...\\n\\nBy the way there\'s a [nice rundown of how to use this by K. Scott Allen of Pluralsight](http://www.pluralsight.com/training/Courses/TableOfContents/mvc4#mvc4-m3-optimization). It\'s fantastic. Recommended.\\n\\nUpdate 2\\n\\nHaving done a little asking around I now understand that this \\\\***can**\\\\* be used with MVC 3 / .NET 4.0. Excellent!\\n\\nOne rather nice alternative script serving mechanism I\'ve seen (but not yet used) is Andrew Davey\'s [Cassette](http://getcassette.net) which I mean to take for a test drive soon. This looks fantastic (and is available as a [Nuget package](http://nuget.org/packages/Cassette) \\\\- 10 points!).\\n\\n## CDNs (they want to serve you)\\n\\nI\'ve never professionally made use of CDNs at all. There are [clearly good reasons why you should](http://encosia.com/3-reasons-why-you-should-let-google-host-jquery-for-you/) but most of those good reasons relate most to public facing web apps.\\n\\nAs I\'ve said, the applications I tend to work on sit behind firewalls and it\'s not always guaranteed what my users can see from the grand old world of web beyond. (Indeed what they see can change on hour by hour basis sometimes...) Combined with that, because my apps are only accessible by a select few I don\'t face the pressure to reduce load on the server that public web apps can face.\\n\\nSo while CDN\'s are clearly a good thing. I don\'t use them at present. And that\'s unlikely to change in the short term.\\n\\n## TL:DR\\n\\n1. I don\'t use CDNs - they\'re clearly useful but they don\'t suit my particular needs\\n2. I serve each JavaScript file individually just before the body tag. I don\'t bundle.\\n3. I don\'t minify my own scripts (though clearly it wouldn\'t be hard) but I do serve the minified versions of 3rd party libraries (eg jQuery) in a Production environment.\\n4. I don\'t use async script loaders at present. I may in future; we shall see.\\n\\nI expect some of the above may change (well, possibly not point #1) but this general approach is working well for me at present.\\n\\nI haven\'t touched at all on how I\'m structuring my JavaScript code itself. Perhaps next time."},{"id":"/2012/06/04/reasons-to-be-cheerful-why-now-is-good","metadata":{"permalink":"/2012/06/04/reasons-to-be-cheerful-why-now-is-good","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2012-06-04-reasons-to-be-cheerful-why-now-is-good/index.md","source":"@site/blog/2012-06-04-reasons-to-be-cheerful-why-now-is-good/index.md","title":"Reasons to be Cheerful (why now is a good time to be a dev)","description":"I\'ve been a working as a developer in some way, shape or form for just over 10 years now. And it occurred to me the other day that I can\'t think of a better time to be a software developer than right now","date":"2012-06-04T00:00:00.000Z","formattedDate":"June 4, 2012","tags":[],"readingTime":7.525,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Reasons to be Cheerful (why now is a good time to be a dev)","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false},"prevItem":{"title":"Optimally Serving Up JavaScript","permalink":"/2012/07/01/how-im-structuring-my-javascript-in-web"},"nextItem":{"title":"Dad Didn\'t Buy Any Games","permalink":"/2012/05/30/dad-didnt-buy-any-games"}},"content":"I\'ve been a working as a developer in some way, shape or form for just over 10 years now. And it occurred to me the other day that I can\'t think of a better time to be a software developer than <u>right now</u>\\n\\n. This year was better than last year. Last year was better than the year before. This is a happily recurring theme. So why? Well I guess there are a whole host of reasons; this is my effort to identify just some of them... ## Google and the World Wide Web (other search providers are available)\\n\\nWhen I first started out as a humble Delphi developer back in 1999 learning was not the straightforward proposition it is today. If you want to know how to do something these days a good place to start is firing up your browser and putting your question into Google. If I was to ask the question _\\"how do I use AJAX\\"_ of a search engine 10 years ago and now I would see very different things.\\n\\n![](AJAX-bleach.jpg)\\n\\nOn the left the past, on the right the present. Do try not to let the presence of W3Schools in the search results detract... And also best ignore that the term AJAX wasn\'t coined until 2006... What I\'m getting at is that finding out information these days is can be done really quickly. Excellent search engines are now the norm. Back when I started out this was not the case and you were essentially reliant on what had been written down in books and the kindliness of more experienced developers. Google (and others like them) have done us a great service. They\'ve made it easier to learn. ## Blogs / Screencasts / Training websites\\n\\nSomething else that has made it easier to learn is the rise and rise of blogs, screencasts and training websites. Over the last 5 years the internet has been filling up with people writing blogs talking about tools, techniques and approaches they are using. When you\'re searching for advice on how to do something you can pretty much guarantee these days that some good soul will have written about it already. The most generous devs out there have gone a step further producing screencasts demonstrating them coding and sharing it with the world \\\\***for free**\\\\*. See an example from the ever awesome Rebecca Murphey below:\\n\\n<iframe src=\\"https://player.vimeo.com/video/20457625\\" width=\\"500\\" height=\\"281\\" frameBorder=\\"0\\" mozallowfullscreen=\\"\\" allowFullScreen=\\"\\"></iframe>\\n\\nSimilarly, there are now a number of commercially available screencasts which make it really easy to ramp up and learn. There\'s [TekPub](http://tekpub.com/), there\'s [Pluralsight](http://www.pluralsight-training.net) (who have massively improved my commute with their mobile app by the way). All of these help tug away the curtain away from the software development Wizard of Oz. All this is a very wonderful thing indeed! ## Podcasts\\n\\nIf you\'re a Boogie Down Productions fan then you may be aware of the concept of [Edutainment](<http://en.wikipedia.org/wiki/Edutainment_(album)>). That is to say, the bridge that can exist between entertainment and education. This is what I\'ve found podcasts to be. I listen to a lot. [Hanselminutes](http://www.hanselminutes.com/). [Herding Code](http://herdingcode.com/). [JavaScript Jabber](http://javascriptjabber.com/). [The JavaScript Show](http://javascriptshow.com/). [Yet Another Podcast](http://jesseliberty.com/podcast/). There\'s more. There\'s something wonderful about about listening to other developers who are passionate about what they are doing. Interested in their work. Enthusiastic about their projects. It\'s infectious. It makes you want to grab a keyboard and start trying things out. I can\'t imagine I\'m the only dev that feels this way. And of course I couldn\'t fail to mention my favourite podcast: [This Developer\'s Life](http://www.thisdeveloperslife.com/). Put together by Scott Hanselman and Rob Conery (I love these guys by the way), and inspired by [This American Life](http://www.thisamericanlife.org/), this show tells some of the stories experienced by developers. It gives an insight into what it\'s like to be a developer. This podcast is more entertaining than educational but it\'s absolutely \\\\***fantastic**\\\\*. ## JavaScript (and HTML and CSS too)\\n\\nAll of the above have eased the learning path of developers and made it easier to keep in touch with the latest and greatest happenings in the dev world. Along with this there has, in my opinion, also been something of a unifying of purpose in the developer community of late. I attribute this to JavaScript, HTML and CSS. Back when I started out it seemed much more the case that developers were split into different tribes. There was the Delphi tribe, the Visual Basic tribe, the C++ tribe, the Java tribe (very much the \\"hip young gunslingers\\" tribe back then - I guess these days it\'d be the Node.JS guys) as well as many others. And each tribe more or less seemed to keep themselves to themselves. This wasn\'t malicious that I could tell; that just seemed to be the way it was. But shortly after I started out the idea of the web application took off in a major way. I was involved in this coming from the position of being an early adopter of ASP.NET (which I used, and loved, since it was first in beta). Many other web application technologies were available; JSP, PHP, Perl and the like. But what they all had in common was this: they all pumped out HTML and CSS to the user. Suddenly all these developers from subtly different backgrounds were all targeting the same technology for their GUI. This unifying effect has been \\\\***massively**\\\\* reinforced by JavaScript. Whereas HTML is a markup language, JavaScript is a programming language. And more by accident than grand design JavaScript has kind of become the [VM of the web](http://www.hanselman.com/blog/JavaScriptIsAssemblyLanguageForTheWebPart2MadnessOrJustInsanity.aspx). Given the rise and rise of the rich web client (driven onwards and upwards by the popularity of AJAX, Backbone.JS etc) this has meant that devs of all creeds and colours have been forced to pitch a tent on the same patch of dirt. Pretty much all of the tribes now have an embassy in JavaScript land. So there are all these devs out there who are used to working with different server-side technologies from each other. But when it comes to the client, we are all sharing the common language of JavaScript. To a certain extent we\'re all creating data services that just pump out JSON to the client. Through forums like [StackOverflow](http://stackoverflow.com/) devs of all the tribes are helping each other with web client \\"stuff\\". They\'re all interacting in ways that they probably wouldn\'t otherwise if the web client environment was as diverse as the server-side environment... ## The Browser Wars Begin Again\\n\\nDidn\'t things seem a little dull around 2003/2004? IE 6 had come out 3 years previously and had vanquished all comers. Microsoft was really the only game in town browser-wise. Things had stopped changing; it seemed like browsers were \\"done\\". You know, good enough and there was no need to take things any further. Then came Firefox. This lone browser appeared as an alternative to might of IE. I must admit the thing that first attracted me to Firefox was the fact it had tabs. I mean technically I knew Firefox was more secure than IE but honestly it was the tabs that attracted me in the first place. (This may offer some insight as to why so many people still smoke...) And somehow Firefox managed to jolt Microsoft out of it\'s inertia on the web. Microsoft started caring about IE again. (Not enough until quite recently in my book but you\'ve got to start somewhere.) I\'m a firm believer that change for it\'s own sake can often be a good thing. Change makes you think about why you do what you do and wonder if there might be better approaches that could be used instead. And these changes kind of feed into... ## ...HTML 5!\\n\\nThat\'s right HTML 5 which is all about change. It\'s taking HTML as we know and love it and bolting on new stuff. New elements (canvas), new styling (CSS 3), new JavaScript APIs, faster JavaScript engines, support for JavaScript 5. The list goes on... And all this new stuff is exciting, whizzy, fun to play with. That which wasn\'t possible yesterday is possible now. Playing with new toys is half the fun of being a dev. There\'s a lot of new toys about right now. ## The Feeling of Possibilites\\n\\nThis is what it comes down to I think. It\'s so easy to learn these days and there\'s so much to learn about. Right now lots of things are happening above and beyond what I\'ve mentioned above. Open source has come of age and gone mainstream. Github is with us. Google are making contentious forays into new languages with Dart and Native Client. Microsoft aren\'t remotely evil empire-like these days; they\'ve made .NET like a Swiss army knife. You can even run Node.js on IIS these days! Signal-R, Websockets, Coffeescript, JS.Next, Backbone.JS, Entity Framework, LINQ, the mobile web, ASP.NET MVC, Razor, Knockout.JS, the cloud, Windows Azure... So much is happening right now. People are making things. It\'s a very interesting time to be a dev. There are many reasons to be cheerful."},{"id":"/2012/05/30/dad-didnt-buy-any-games","metadata":{"permalink":"/2012/05/30/dad-didnt-buy-any-games","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2012-05-30-dad-didnt-buy-any-games/index.md","source":"@site/blog/2012-05-30-dad-didnt-buy-any-games/index.md","title":"Dad Didn\'t Buy Any Games","description":"Inspired by Hanselmans post on how he got started in programming I thought I\'d shared my own tale about how it all began... I grew up the 80\'s just outside London. For those of you of a different vintage let me paint a picture. These were the days when \\"Personal Computers\\", as they were then styled, were taking the world by storm. Every house would be equipped with either a ZX Spectrum, a Commodore 64 or an Amstrad CPC. These were 8 bit computers which were generally plugged into the family television and spent a good portion of their time loading games like Target","date":"2012-05-30T00:00:00.000Z","formattedDate":"May 30, 2012","tags":[],"readingTime":2.055,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Dad Didn\'t Buy Any Games","authors":"johnnyreilly","tags":[],"hide_table_of_contents":false},"prevItem":{"title":"Reasons to be Cheerful (why now is a good time to be a dev)","permalink":"/2012/06/04/reasons-to-be-cheerful-why-now-is-good"},"nextItem":{"title":"Globalize.js - number and date localisation made easy","permalink":"/2012/05/07/globalizejs-number-and-date"}},"content":"Inspired by [Hanselmans post on how he got started in programming](http://www.hanselman.com/blog/SheLetMeTakeTheComputerHomeHowDidYouGetStartedInComputersAndProgramming.aspx) I thought I\'d shared my own tale about how it all began... I grew up the 80\'s just outside London. For those of you of a different vintage let me paint a picture. These were the days when \\"Personal Computers\\", as they were then styled, were taking the world by storm. Every house would be equipped with either a ZX Spectrum, a Commodore 64 or an Amstrad CPC. These were 8 bit computers which were generally plugged into the family television and spent a good portion of their time loading games like [Target: Renegade](http://en.wikipedia.org/wiki/Target:_Renegade) from an audio cassette. But not in our house; we didn\'t have a computer. I remember mournfully pedalling home from friends houses on a number of occasions, glum as I compared my lot with theirs. Whereas my friends would be spending their evenings gleefully battering their keyboards as they thrashed the life out of various end-of-level bosses I was reduced to \\\\***wasting**\\\\* my time reading. That\'s right Enid Blyton - you were second best in my head. Then one happy day (and it may have been a Christmas present although I\'m not certain) our family became the proud possessors of an [Amstrad CPC 6128](http://en.wikipedia.org/wiki/Amstrad_CPC):\\n\\n![](CPC6128.jpg)\\n\\nGlory be! I was going to play so many games! I would have such larks! My evenings would be filled with pixelated keyboard related destruction! Hallelujah!! But I was wrong. I had reckoned without my father. For reasons that I\'ve never really got to the bottom of Dad had invested in the computer but not in the games. Whilst I was firmly of the opinion that these 2 went together like Lennon and McCartney he was having none of it. \\"You can write your own son\\" he intoned and handed over a manual which contained listings for games:\\n\\n![](6a0120a85dcdae970b0120a86ddeee970b.png)\\n\\nAnd that\'s where it first began really. I would spend my evenings typing the Locomotive Basic listings for computer games into the family computer. Each time I started I would be filled with great hopes for what might result. Each time I tended to be rewarded with something that looked a bit like this:\\n\\n![](images.jpg)\\n\\nI\'m not sure that it\'s possible to learn to program by osmosis but if it is I\'m definitely a viable test case. I didn\'t become an expert Locomotive Basic programmer (was there ever such a thing?) but I did undoubtedly begin my understanding of software.... Thanks Dad!"},{"id":"/2012/05/07/globalizejs-number-and-date","metadata":{"permalink":"/2012/05/07/globalizejs-number-and-date","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2012-05-07-globalizejs-number-and-date/index.md","source":"@site/blog/2012-05-07-globalizejs-number-and-date/index.md","title":"Globalize.js - number and date localisation made easy","description":"I wanted to write about a JavaScript library which seems to have had very little attention so far. And that surprises me as it\'s","date":"2012-05-07T00:00:00.000Z","formattedDate":"May 7, 2012","tags":[{"label":"jqueryui","permalink":"/tags/jqueryui"},{"label":"Globalize.JS","permalink":"/tags/globalize-js"},{"label":"javascript","permalink":"/tags/javascript"},{"label":"ASP.NET AJAX","permalink":"/tags/asp-net-ajax"},{"label":"Globalization","permalink":"/tags/globalization"},{"label":"Richard D. Worth","permalink":"/tags/richard-d-worth"}],"readingTime":7.515,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Globalize.js - number and date localisation made easy","authors":"johnnyreilly","tags":["jqueryui","Globalize.JS","javascript","ASP.NET AJAX","Globalization","Richard D. Worth"],"hide_table_of_contents":false},"prevItem":{"title":"Dad Didn\'t Buy Any Games","permalink":"/2012/05/30/dad-didnt-buy-any-games"},"nextItem":{"title":"Beg, Steal or Borrow a Decent JavaScript DateTime Converter","permalink":"/2012/04/28/beg-steal-or-borrow-decent-javascript"}},"content":"I wanted to write about a JavaScript library which seems to have had very little attention so far. And that surprises me as it\'s\\n\\n1. Brilliant!\\n2. Solves a common problem that faces many app developers who work in the wonderful world of web; myself included\\n\\nThe library is called Globalize.js and can be found on [GitHub here](https://github.com/jquery/globalize). Globalize.js is a simple JavaScript library that allows you to format and parse numbers and dates in culture specific fashion.\\n\\n## Why does this matter?\\n\\nBecause different countries and cultures do dates and numbers in different ways. Christmas Day this year in England will be `25/12/2012` (dd/MM/yyyy). But for American eyes this should be `12/25/2012` (M/d/yyyy). And for German `25.12.2012` (dd.MM.yyyy). Likewise, if I was to express numerically the value of \\"one thousand exactly - to 2 decimal places\\", as a UK citizen I would do it like so: `1,000.00`. But if I was French I\'d express it like this: `1.000,00`. You see my point?\\n\\n## Why does this matter to me?\\n\\nFor a number of years I\'ve been working on applications that are used globally, from London to Frankfurt to Shanghai to New York to Singapore and many other locations besides. The requirement has always been to serve up localised dates and numbers so users experience of the system is more natural. Since our applications are all ASP.NET we\'ve never really had a problem server-side. Microsoft have blessed us with all the goodness of [System.Globalization](http://msdn.microsoft.com/en-us/library/system.globalization.aspx) which covers hundreds of different cultures and localisations. It makes it frankly easy:\\n\\n```cs\\nusing System.Globalization;\\n\\n//Produces: \\"06.05.2012\\"\\nnew DateTime(2012,5,6).ToString(\\"d\\", new CultureInfo(\\"de-DE\\"));\\n\\n//Produces: \\"45,56\\"\\n45.56M.ToString(\\"n\\", new CultureInfo(\\"fr-FR\\"));\\n```\\n\\nThe problem has always been client-side. If you need to localise dates and numbers on the client what do you do?\\n\\n## JavaScript Date / Number Localisation - the Status Quo\\n\\nWell to be frank - it\'s a bit rubbish really. What\'s on offer natively at present basically amounts to this:\\n\\n- [Date.toLocaleDateString](https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/Date/toLocaleDateString)\\n- [Number.ToLocaleString](https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/Number/ToLocaleString)\\n\\nThis is better than nothing - but not by much. There\'s no real control or flexibility here. If you don\'t like the native localisation format or you want something slightly different then tough. This is all you\'ve got to play with.\\n\\nFor the longest time this didn\'t matter too much. Up until relatively recently the world of web was far more about the thin client and the fat server. It would be quite standard to have all HTML generated on the server. And, as we\'ve seen .NET (and many other back end enviroments as well) give you all the flexiblility you might desire given this approach.\\n\\n[But the times they are a-changing](http://www.youtube.com/watch?v=k2sYIIjS-cQ). And given the ongoing explosion of HTML 5 the rich client is very definitely with us. So we need tools.\\n\\n## Microsoft doing \\\\*good things\\\\*\\n\\nHands up who remembers when Microsoft first shipped it\'s [ASP.NET AJAX](http://msdn.microsoft.com/en-us/magazine/cc163300.aspx) library back in 2007?\\n\\nWell a small part of this was the extensions ASP.NET AJAX added to JavaScripts native Date and Number objects.... These extensions allowed the localisation of Dates and Numbers to the current UI culture and the subsequent string parsing of these back into Dates / Numbers. These extensions pretty much gave JavaScript the functionality that the server already had in `System.Globalization`. (not quite like-for-like but near enough the mark)\\n\\nI\'m not aware of a great fuss ever being made about this - a fact I find surprising since one would imagine this is a common need. There\'s good documentation about this on MSDN - here\'s some useful links:\\n\\n- [Ajax Script Globalization and Localization](http://msdn.microsoft.com/en-us/library/bb386572.aspx)\\n- [Walkthrough: Globalizing a Date by Using Client Script](http://msdn.microsoft.com/en-us/library/bb386581.aspx)\\n- [JavaScript Base Type Extensions](http://msdn.microsoft.com/en-us/library/bb397506.aspx)\\n- [Date.parseLocale](http://msdn.microsoft.com/en-us/library/bb397521.aspx)\\n- [Date.localeFormat](http://msdn.microsoft.com/en-us/library/bb383816.aspx)\\n- [Number.localeFormat](http://msdn.microsoft.com/en-us/library/bb310813.aspx)\\n- [Number.parseLocale](http://msdn.microsoft.com/en-us/library/bb310985.aspx)\\n\\nWhen our team became aware of this we started to make use of it in our web applications. I imagine we weren\'t alone...\\n\\n## Microsoft doing \\\\*even better things\\\\* (Scott Gu to the rescue!)\\n\\nI started to think about this again when MVC reared it\'s lovely head.\\n\\nLike many, I found I preferred the separation of concerns / testability etc that MVC allowed. As such, our team was planning to, over time, migrate our ASP.NET WebForms applications over to MVC. However, before we could even begin to do this we had a problem. Our JavaScript localisation was dependant on the ScriptManager. The [ScriptManager](http://msdn.microsoft.com/en-us/library/system.web.ui.scriptmanager.aspx) is very much a WebForms construct.\\n\\nWhat to do? To the users it wouldn\'t be acceptable to remove the localisation functionality from the web apps. The architecture of an application is, to a certain extent, meaningless from the users perspective - they\'re only interested in what directly impacts them. That makes sense, even if it was a problem for us.\\n\\nFortunately the Great Gu had it in hand. Lo and behold the [this post](http://forum.jquery.com/topic/proposal-for-a-globalization-plugin-jquery-glob-js) appeared on the jQuery forum and the following post appeared on Guthrie\'s blog:\\n\\n[http://weblogs.asp.net/scottgu/archive/2010/06/10/jquery-globalization-plugin-from-microsoft.aspx](http://weblogs.asp.net/scottgu/archive/2010/06/10/jquery-globalization-plugin-from-microsoft.aspx)\\n\\nYes that\'s right. Microsoft were giving back to the jQuery community by contributing a jQuery globalisation plug-in. They\'d basically taken the work done with ASP.NET AJAX Date / Number extensions, jQuery-plug-in-ified it and put it out there. Fantastic!\\n\\nUsing this we could localise / globalise dates and numbers whether we were working in WebForms or in MVC. Or anything else for that matter. If we were suddenly seized with a desire to re-write our apps in PHP we\'d \\\\***still**\\\\* be able to use Globalize.js on the client to handle our regionalisation of dates and numbers.\\n\\n## History takes a funny course...\\n\\nNow for my part I would have expected that this announcement to be followed in short order by dancing in the streets and widespread adoption. Surprisingly, not so. All went quiet on the globalisation front for some time and then out of the blue the following comment appeared on the jQuery forum by [Richard D. Worth](http://rdworth.org/blog/) (he of jQuery UI fame):\\n\\n[http://blog.jquery.com/2011/04/16/official-plugins-a-change-in-the-roadmap/#comment-527484](http://blog.jquery.com/2011/04/16/official-plugins-a-change-in-the-roadmap/#comment-527484)\\n\\nThe long and short of which was:\\n\\n- The jQuery UI team were now taking care of (the re-named) Globalize.js library as the grid control they were developing had a need for some of Globalize.js\'s goodness. Consequently a home for Globalize.js appeared on the jQuery UI website: [http://wiki.jqueryui.com/Globalize](http://wiki.jqueryui.com/Globalize)\\n- The source of Globalize.js moved to this location on GitHub: [https://github.com/jquery/globalize/](https://github.com/jquery/globalize/)\\n- Perhaps most significantly, the jQuery globalisation plug-in as developed by Microsoft had now been made a standalone JavaScript library. This was clearly brilliant news for Node.js developers as they would now be able to take advantage of this and perform localisation / globalisation server-side - they wouldn\'t need to have jQuery along for the ride. Also, this would be presumably be good news for users of other client side JavaScript libraries like Dojo / YUI etc.\\n\\nGlobalize.js clearly has a rosy future in front of it. Using the new Globalize.js library was still simplicity itself. Here\'s some examples of localising dates / numbers using the German culture:\\n\\n```js\\n<script\\n  src=\\"/Scripts/Globalize/globalize.js\\"\\n  type=\\"text/javascript\\"><\/script>\\n<script\\n  src=\\"/Scripts/Globalize/cultures/globalize.culture.de-DE.js\\"\\n  type=\\"text/javascript\\"><\/script>\\n\\nGlobalize.culture(\\"de-DE\\");\\n\\n//\\"2012-05-06\\" - ISO 8601 format\\nGlobalize.format(new Date(2012,4,6), \\"yyyy-MM-dd\\");\\n\\n//\\"06.05.2012\\" - standard German short date format of dd.MM.yyyy\\nGlobalize.format(new Date(2012,4,6), Globalize.culture().calendar.patterns.d);\\n\\n//\\"4.576,3\\" - a number rendered to 1 decimal place\\nGlobalize.format(4576.34, \\"n1\\");\\n```\\n\\n## Stick a fork in it - it\'s done\\n\\nThe entry for Globalize.js on the jQuery UI site reads as follows:\\n\\n> _\\"version: 0.1.0a1 (not a jQuery UI version number, as this is a standalone utility) status: in development (part of Grid project)\\"_\\n\\nI held back from making use of the library for some time, deterred by the \\"in development\\" status. However, I had a bit of dialog with one of the jQuery UI team (I forget exactly who) who advised that the API was unlikely to change further and that the codebase was actually pretty stable. Our team did some testing of Globalize.js and found this very much to be case. Everything worked just as we expected and hoped. We\'re now using Globalize.js in a production environment with no problems reported; it\'s been doing a grand job.\\n\\nIn my opinion, Number / Date localisation on the client is ready for primetime right now - it works! Unfortunately, because Globalize.js has been officially linked in with the jQuery UI grid project it seems unlikely that this will officially ship until the grid does. Looking at the jQuery UI [roadmap](http://wiki.jqueryui.com/Roadmap) the grid is currently slated to release with jQuery UI 2.1. There isn\'t yet a release date for jQuery UI 1.9 and so it could be a long time before the grid actually sees the light of day.\\n\\nI\'m hoping that the jQuery UI team will be persuaded to \\"officially\\" release Globalize.js long before the grid actually ships. Obviously people can use Globalize.js as is right now (as we are) but it seems a shame that many others will be missing out on using this excellent functionality, deterred by the \\"in development\\" status. Either way, [the campaign to release Globalise.js officially starts here!](http://www.youtube.com/watch?v=qEMytPF8YuY)\\n\\n## The Future?\\n\\nThere are plans to bake globalisation right into JavaScript natively with EcmaScript 5.1. There\'s a good post on the topic [here](http://generatedcontent.org/post/59403168016/esintlapi). And here\'s a couple of historical links worth reading too:\\n\\n[http://norbertlindenberg.com/2012/02/ecmascript-internationalization-api/](http://norbertlindenberg.com/2012/02/ecmascript-internationalization-api/)[http://wiki.ecmascript.org/doku.php?id=globalization:specification_drafts](http://wiki.ecmascript.org/doku.php?id=globalization:specification_drafts)"},{"id":"/2012/04/28/beg-steal-or-borrow-decent-javascript","metadata":{"permalink":"/2012/04/28/beg-steal-or-borrow-decent-javascript","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2012-04-28-beg-steal-or-borrow-decent-javascript/index.md","source":"@site/blog/2012-04-28-beg-steal-or-borrow-decent-javascript/index.md","title":"Beg, Steal or Borrow a Decent JavaScript DateTime Converter","description":"I\'ve so named this blog post because it shamelessly borrows from the fine work of others 1. http 2. http://n8v.enteuxis.org/2010/12/parsing-iso-8601-dates-in-javascript/","date":"2012-04-28T00:00:00.000Z","formattedDate":"April 28, 2012","tags":[{"label":"Date","permalink":"/tags/date"},{"label":"Sebastian Markb\xe5ge","permalink":"/tags/sebastian-markbage"},{"label":"DateTime","permalink":"/tags/date-time"},{"label":"System.Web.Script.Serialization.JavaScriptSerializer","permalink":"/tags/system-web-script-serialization-java-script-serializer"},{"label":"EMCAScript standard","permalink":"/tags/emca-script-standard"},{"label":"json","permalink":"/tags/json"},{"label":"javascript","permalink":"/tags/javascript"},{"label":"Serialization","permalink":"/tags/serialization"},{"label":"Nathan Vonnahme","permalink":"/tags/nathan-vonnahme"}],"readingTime":6.49,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Beg, Steal or Borrow a Decent JavaScript DateTime Converter","authors":"johnnyreilly","tags":["Date","Sebastian Markb\xe5ge","DateTime","System.Web.Script.Serialization.JavaScriptSerializer","EMCAScript standard","json","javascript","Serialization","Nathan Vonnahme"],"hide_table_of_contents":false},"prevItem":{"title":"Globalize.js - number and date localisation made easy","permalink":"/2012/05/07/globalizejs-number-and-date"},"nextItem":{"title":"JSHint - Customising your hurt feelings","permalink":"/2012/04/23/jshint-customising-your-hurt-feelings"}},"content":"I\'ve so named this blog post because it shamelessly borrows from the fine work of others: Sebastian Markb\xe5ge and Nathan Vonnahme. Sebastian wrote a blog post documenting a good solution to the ASP.NET JavaScriptSerializer DateTime problem at the tail end of last year. However, his solution didn\'t get me 100% of the way there when I tried to use it because of a need to support IE 8 which lead me to use Nathan Vonnahme\'s ISO 8601 JavaScript Date parser. I thought it was worth documenting this, hence this post, but just so I\'m clear; the hard work here was done by Sebastian Markb\xe5ge and Nathan Vonnahme and not me. Consider me just a curator in this case. The original blog posts that I am drawing upon can be found here: 1. [http://blog.calyptus.eu/seb/2011/12/custom-datetime-json-serialization/](http://blog.calyptus.eu/seb/2011/12/custom-datetime-json-serialization/) and here: 2. [http://n8v.enteuxis.org/2010/12/parsing-iso-8601-dates-in-javascript/](http://n8v.enteuxis.org/2010/12/parsing-iso-8601-dates-in-javascript/)\\n\\n## DateTime, JSON, JavaScript Dates....\\n\\nLike many, I\'ve long been frustrated with the quirky DateTime serialisation employed by the `System.Web.Script.Serialization.JavaScriptSerializer` class. When serialising DateTimes so they can be JSON.parsed on the client, this serialiser uses the following approach: (from MSDN) [_Date object, represented in JSON as \\"\\\\/Date(number of ticks)\\\\/\\". The number of ticks is a positive or negative long value that indicates the number of ticks (milliseconds) that have elapsed since midnight 01 January, 1970 UTC.\\"_](http://msdn.microsoft.com/en-us/library/system.web.script.serialization.javascriptserializer.aspx) Now this is not particularly helpful in my opinion because it\'s not human readable (at least not this human; perhaps [Jon Skeet](http://stackoverflow.com/users/22656/jon-skeet)...) When consuming your data from web services / PageMethods using [jQuery.ajax](http://api.jquery.com/jQuery.ajax/) you are landed with the extra task of having to convert what were DateTimes on the server from Microsofts string Date format (eg `\\"\\\\/Date(1293840000000)\\\\/\\"`) into actual JavaScript Dates. It\'s also unhelpful because it\'s divergent from the approach to DateTime / Date serialisation used by a native JSON serialisers:\\n\\n![](FireBug-Dates.png)\\n\\nJust as an aside it\'s worth emphasising that one of the limitations of JSON is that the JSON.parsing of a JSON.stringified date will \\\\***not**\\\\* return you to a JavaScript Date but rather an ISO 8601 date string which will need to be subsequently converted into a Date. Not JSON\'s fault - essentially down to the absence of a Date literal within JavaScript. ## Making JavaScriptSerializer behave more JSON\'y\\n\\nAnyway, I didn\'t think there was anything I could really do about this in an ASP.NET classic / WebForms world because, to my knowledge, it is not possible to swap out the serialiser that is used. JavaScriptSerializer is the only game in town. (Though I am optimistic about the future; given the announcement that I first picked up on Rick Strahl\'s blog that [Json.NET was going to be adopted as the default JSON serializer for ASP.NET Web API](http://www.west-wind.com/weblog/posts/2012/Mar/09/Using-an-alternate-JSON-Serializer-in-ASPNET-Web-API); what with Json.NET having out-of-the-box [ISO 8601 support](http://james.newtonking.com/archive/2009/02/20/good-date-times-with-json-net.aspx). I digress...) Because it can make debugging a much more straightforward process I place a lot of value on being able to read the network traffic that web apps generate. It\'s much easier to drop into Fiddler / FireBug / Chrome dev tools etc and watch what\'s happening there and then instead of having to manually process the data separately first so that you can understand it. I think this is nicely aligned with the [KISS principle](http://en.wikipedia.org/wiki/KISS_principle). For that reason I\'ve been generally converting DateTimes to ISO 8601 strings on the server before returning them to the client. A bit of extra overhead but generally worth it for the gains in clarity in my opinion. So I was surprised and delighted when I happened upon [Sebastian Markb\xe5ge\'s blog post](http://blog.calyptus.eu/seb/2011/12/custom-datetime-json-serialization/) which provided a DateTime JavaScriptConverter that could be plugged into the JavaScriptSerializer. You can see the code below (or on Sebastian\'s original post with a good explanation of how it works): <script src=\\"https://gist.github.com/2489976.js?file=DateTimeJavaScriptConverter.cs\\"><\/script>\\n\\nUsing this converter meant that a DateTime that previously would have been serialised as `\\"\\\\/Date(1293840000000)\\\\/\\"` would now be serialised as `\\"2011-01-01T00:00:00.0000000Z\\"` instead. This is entirely agreeable because 1. it\'s entirely clear what a `\\"2011-01-01T00:00:00.0000000Z\\"` style date represents and 2. this is more in line with native browser JSON implementations and `&lt;statingTheObvious&gt;`consistency is a good thing.`&lt;/statingTheObvious&gt;`\\n\\n## Getting your web services to use the ISO 8601 DateTime Converter\\n\\nSebastian alluded in his post to a `web.config` setting that could be used to get web services / pagemethods etc. implementing his custom DateTime serialiser. This is it: <script src=\\"https://gist.github.com/2489976.js?file=web.config\\"><\/script>\\n\\nWith this in place your web services / page methods will happily be able to serialise / deserialise ISO style date strings to your hearts content. ## What no ISO 8601 date string Date constructor?\\n\\nAs I mentioned earlier, Sebastian\'s solution didn\'t get me 100% of the way there. There was still a fly in the ointment in the form of IE 8. Unfortunately IE 8 doesn\'t have JavaScript [Date constructor that takes ISO 8601 date strings](https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/Date/parse). This lead me to using Nathan Vonnahme\'s ISO 8601 JavaScript Date parser, the code of which is below (or see his original post [here](http://n8v.enteuxis.org/2010/12/parsing-iso-8601-dates-in-javascript/)): <script src=\\"https://gist.github.com/2489976.js?file=parseISO8601Date.js\\"><\/script>\\n\\nWith this in place I could parse ISO 8601 Dates just like anyone else. Great stuff. `parseISO8601Date(\\"2011-01-01T00:00:00.0000000Z\\")` would give me a JavaScript Date of `Sat Jan 1 00:00:00 UTC 2011`. Obviously in the fullness of time the parseISO8601Date solution should no longer be necessary because [EcmaScript 5 specifies an ISO 8601 date string constructor](http://es5.github.com/#x15.9.3.2). However, in the interim Nathan\'s solution is a lifesaver. Thanks again both to Sebastian Markb\xe5ge and Nathan Vonnahme who have both generously allowed me use their work as the basis for this post. ## PS And it would have worked if it wasn\'t for that pesky IE 9...\\n\\nSubsequent to writing this post I thought I\'d check that IE 9 had implemented a JavaScript Date constructor that would process an ISO 8601 date string like this: `new Date(\\"2011-01-01T00:00:00.0000000Z\\")`. It hasn\'t. Take a look:\\n\\n![](IE9-screenshot.png)\\n\\nThis is slightly galling as the above code works dandy in Firefox and Chrome. As you can see from the screenshot you can get the JavaScript IE 9 Date constructor to play nice by trimming off the final 4 \\"0\\"\'s from the string. Frustrating. Obviously we can still use Nathan\'s solution but it\'s a shame that we can\'t use the native support. Based on what I\'ve read [here](http://msdn.microsoft.com/en-us/library/az4se3k1.aspx#Roundtrip) I think it would be possible to amend Sebastians serializer to fall in line with IE 9\'s pendantry by changing this:\\n\\n```cs\\nreturn new CustomString(((DateTime)obj).ToUniversalTime()\\n  .ToString(<b>\\"O\\"</b>)\\n);\\n```\\n\\nTo this:\\n\\n```cs\\nreturn new CustomString(((DateTime)obj).ToUniversalTime()\\n  .ToString(<b>\\"yyyy\'-\'MM\'-\'dd\'T\'HH\':\'mm\':\'ss\'.\'fffzzz\\"</b>)\\n);\\n```\\n\\nI\'ve held off from doing this myself as I rather like Sebastian\'s idea of being able to use Microsoft\'s Round-trip (\\"O\\", \\"o\\") Format Specifier. And it seems perverse that we should have to move away from using Microsoft\'s Round-trip Format Specifier purely because of (Microsoft\'s) IE! But it\'s a possibility to consider and so I put it out there. I would hope that MS will improve their JavaScript Date constructor with IE 10. A missed opportunity if they don\'t I think. ## PPS Just when you thought is over... IE 9 was right!\\n\\nSebastian got in contact after I first published this post and generously pointed out that, contrary to my expectation, IE 9 technically had the correct implementation. According to the [EMCAScript standard](http://es5.github.com/#x15.9.1.15) the Date constructor should not allow more than millisecond precision. In this case, Chrome and Firefox are being less strict - not more correct. On reflection this does rather make sense as the result of a `JSON.stringify(new Date())` never results in an ISO date string to the 10 millionths of a second detail. Sebastian has himself stopped using Microsoft\'s Round-trip (\\"O\\", \\"o\\") Format Specifier in favour of this format string: ```cs\\nreturn new CustomString(((DateTime)obj).ToUniversalTime()\\n\\n.ToString(<b>\\"yyyy-MM-ddTHH:mm:ss.fffZ\\"</b>)\\n\\n);\\n\\n```\\n\\n This results in date strings that comply perfectly with the ECMAScript spec. I suspect I\'ll switch to using this also now. Though I\'ll probably leave the first part of the post intact as I think the background remains interesting. Thanks again Sebastian!\\n```"},{"id":"/2012/04/23/jshint-customising-your-hurt-feelings","metadata":{"permalink":"/2012/04/23/jshint-customising-your-hurt-feelings","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2012-04-23-jshint-customising-your-hurt-feelings/index.md","source":"@site/blog/2012-04-23-jshint-customising-your-hurt-feelings/index.md","title":"JSHint - Customising your hurt feelings","description":"As I\'ve started making greater use of JavaScript to give a richer GUI experience the amount of JS in my ASP.NET apps has unsurprisingly ballooned. If I\'m honest, I hadn\'t given much consideration to the code quality of my JavaScript in the past. However, if I was going to make increasing use of it (and given the way the web is going at the moment I\'d say that\'s a given) I didn\'t think this was tenable position to maintain. A friend of mine works for Coverity which is a company that provides tools for analysing code quality. I understand, from conversations with him, that their tools provide static analysis for compiled languages such as C++ / C# / Java etc. I was looking for something similar for JavaScript. Like many, I have read and loved Douglas Crockford\'s \\"JavaScript","date":"2012-04-23T00:00:00.000Z","formattedDate":"April 23, 2012","tags":[{"label":"JSLint for Visual Studio","permalink":"/tags/js-lint-for-visual-studio"},{"label":"Coverity","permalink":"/tags/coverity"},{"label":"Anton Kovalyov","permalink":"/tags/anton-kovalyov"},{"label":"javascript","permalink":"/tags/javascript"},{"label":"paul irish","permalink":"/tags/paul-irish"},{"label":"static code analysis","permalink":"/tags/static-code-analysis"},{"label":"JSLint","permalink":"/tags/js-lint"},{"label":"JSHint","permalink":"/tags/js-hint"},{"label":"douglas crockford","permalink":"/tags/douglas-crockford"}],"readingTime":4.5,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"JSHint - Customising your hurt feelings","authors":"johnnyreilly","tags":["JSLint for Visual Studio","Coverity","Anton Kovalyov","javascript","paul irish","static code analysis","JSLint","JSHint","douglas crockford"],"hide_table_of_contents":false},"prevItem":{"title":"Beg, Steal or Borrow a Decent JavaScript DateTime Converter","permalink":"/2012/04/28/beg-steal-or-borrow-decent-javascript"},"nextItem":{"title":"A Simple Technique for Initialising Properties with Internal Setters for Unit Testing","permalink":"/2012/04/16/simple-technique-for-initialising"}},"content":"As I\'ve started making greater use of JavaScript to give a richer GUI experience the amount of JS in my ASP.NET apps has unsurprisingly ballooned. If I\'m honest, I hadn\'t given much consideration to the code quality of my JavaScript in the past. However, if I was going to make increasing use of it (and given the way the web is going at the moment I\'d say that\'s a given) I didn\'t think this was tenable position to maintain. A friend of mine works for [Coverity](http://www.coverity.com/) which is a company that provides tools for analysing code quality. I understand, from conversations with him, that their tools provide static analysis for compiled languages such as C++ / C# / Java etc. I was looking for something similar for JavaScript. Like many, I have read and loved [Douglas Crockford\'s \\"JavaScript: The Good Parts\\"](http://www.amazon.com/JavaScript-Good-Parts-Douglas-Crockford/dp/0596517742); it is by some margin the most useful and interesting software related book I have read.So I was aware that Crockford had come up with his own JavaScript code quality tool called [JSLint](http://www.jslint.com/). JSLint is quite striking when you first encounter it:\\n\\n![](JSLint.png)\\n\\nIt\'s the \\"Warning! JSLint will hurt your feelings.\\" that grabs you. And it\'s not wrong. I\'ve copied and pasted code that I\'ve written into JSLint and then gasped at the reams of errors JSLint would produce. I subsequently tried JSLint-ing various well known JS libraries (jQuery etc) and saw that JSLint considered they were thoroughly problematic as well. This made me feel slightly better. It was when I started examining some of the \\"errors\\" JSLint reported that I took exception. Yes, I took exception to exceptions! (I\'m \\\\***very**\\\\* pleased with that!) Here\'s a few of the errors generated by JSLint when inspecting [jquery-1.7.2.js](http://ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.js): - `Problem at line 16 character 10: Expected exactly one space between \'function\' and \'(\'.`\\n\\n- `Problem at line 25 character 1: Expected \'var\' at column 13, not column 1.`\\n- `Problem at line 31 character 5: Unexpected dangling \'_\' in \'_jQuery\'.`\\n\\nJSLint is, much like it\'s creator, quite opinionated. Which is no bad thing. Many of Crockfords opinions are clearly worth their salt. It\'s just I didn\'t want all of them enforced upon me. As you can see above most of these \\"problems\\" are essentially complaints about a different style rather than bugs or potential issues. Now there are options in JSLint that you can turn on and off which looked quite promising. But before I got to investigating them I heard about [JSHint](http://www.jshint.com), brainchild of Anton Kovalyov and Paul Irish. In their own words: _JSHint is a fork of JSLint, the tool written and maintained by Douglas Crockford. The project originally started as an effort to make a more configurable version of JSLint\u2014one that doesn\'t enforce one particular coding style on its users\u2014but then transformed into a separate static analysis tool with its own goals and ideals._ This sounded right up my alley! So I thought I\'d repeat my jQuery test. Here\'s a sample of what JSHint threw back at me, with its default settings in place: - `Line 230: return num == null ? Expected \'===\' and instead saw \'==\'. `\\n\\n- `Line 352: if ( (options = arguments[ i ]) != null ) { Expected \'!==\' and instead saw \'!=\'. `\\n- `Line 354: for ( name in options ) { The body of a for in should be wrapped in an if statement to filter unwanted properties from the prototype. `\\n\\nThese were much more the sort of \\"issues\\" I was interested in. Plus it seemed there was plenty of scope to tweak my options. Excellent. This was good. The icing on my cake would have been a plug-in for Visual Studio which would allow me to evaluate my JS files from within my IDE. Happily the world seems to be full of developers doing good turns for one another. I discovered an extension for VS called [JSLint for Visual Studio 2010](http://jslint4vs2010.codeplex.com/):\\n\\n![](Extensions.png)\\n\\nThis was an extension that provided either JSLint \\\\***or**\\\\* JSHint evaluation as you preferred from within Visual Studio. Fantastic! With this extension in play you could add JavaScript static code analysis to your compilation process and so learn of all the issues in your code at the same time, whether they lay in C# or JS or [insert language here]. You could control how JS problems were reported; as warnings, errors etc. You could straightforwardly exclude files from evaluation (essential if you\'re reliant on a number of 3rd party JS libraries which you are not responsible for maintaining). You could cater for predefined variables; allow for jQuery or DOJO. You could simply evaluate a single file in your solution by right clicking it and hitting the \\"JS Lint\\" option in the context menu. And it was simplicity itself to activate and deactivate the JSHint / JSLint extension as required. For a more exhaustive round up of the options available I advise taking a look here: [http://jslint4vs2010.codeplex.com](http://jslint4vs2010.codeplex.com/). I would heartily recommend using JSHint if you\'re looking to improve your JS code quality. I\'m grateful to Crockford for making JSHint possible by first writing JSLint. For my part though I think JSHint is the more pragmatic and useful tool and likely to be the one I stick with. For interest (and frankly sheer entertainment value at the crotchetiness of Crockford) it\'s definitely worth having a read up on how JSHint came to pass: - [http://anton.kovalyov.net/2011/02/20/why-i-forked-jslint-to-jshint/](http://anton.kovalyov.net/2011/02/20/why-i-forked-jslint-to-jshint/)\\n\\n- [http://badassjs.com/post/3364925033/jshint-an-community-driven-fork-of-jslint](http://badassjs.com/post/3364925033/jshint-an-community-driven-fork-of-jslint)"},{"id":"/2012/04/16/simple-technique-for-initialising","metadata":{"permalink":"/2012/04/16/simple-technique-for-initialising","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2012-04-16-simple-technique-for-initialising/index.md","source":"@site/blog/2012-04-16-simple-technique-for-initialising/index.md","title":"A Simple Technique for Initialising Properties with Internal Setters for Unit Testing","description":"I was recently working with my colleagues on refactoring a legacy application. We didn\'t have an immense amount of time available for this but the plan was to try and improve what was there as much as possible. In its initial state the application had no unit tests in place at all and so the plan was to refactor the code base in such a way as to make testing it a realistic proposition. To that end the domain layer was being heavily adjusted and the GUI was being migrated from WebForms to MVC 3. The intention was to build up a pretty solid collection of unit tests. However, as we were working on this we realised we had a problem with properties on our models with internal setters...","date":"2012-04-16T00:00:00.000Z","formattedDate":"April 16, 2012","tags":[{"label":"unit testing","permalink":"/tags/unit-testing"},{"label":"InternalsVisibleTo","permalink":"/tags/internals-visible-to"},{"label":"MOQ","permalink":"/tags/moq"},{"label":"mocking","permalink":"/tags/mocking"}],"readingTime":5.695,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"A Simple Technique for Initialising Properties with Internal Setters for Unit Testing","authors":"johnnyreilly","tags":["unit testing","InternalsVisibleTo","MOQ","mocking"],"hide_table_of_contents":false},"prevItem":{"title":"JSHint - Customising your hurt feelings","permalink":"/2012/04/23/jshint-customising-your-hurt-feelings"},"nextItem":{"title":"Making PDFs from HTML in C# using WKHTMLtoPDF","permalink":"/2012/04/05/making-pdfs-from-html-in-c-using"}},"content":"I was recently working with my colleagues on refactoring a legacy application. We didn\'t have an immense amount of time available for this but the plan was to try and improve what was there as much as possible. In its initial state the application had no unit tests in place at all and so the plan was to refactor the code base in such a way as to make testing it a realistic proposition. To that end the [domain layer](http://en.wikipedia.org/wiki/Domain_layer) was being heavily adjusted and the GUI was being migrated from WebForms to MVC 3. The intention was to build up a pretty solid collection of unit tests. However, as we were working on this we realised we had a problem with properties on our models with [`internal`](<http://msdn.microsoft.com/en-us/library/7c5ka91b(v=vs.80).aspx>) setters...\\n\\n## Background\\n\\nThe entities of the project in question used an approach which would store pertinent bits of [normalised](http://en.wikipedia.org/wiki/Database_normalization) data for read-only purposes in related entities. I\'ve re-read that sentence and realise it\'s as clear as mud. Here is an example to clarify:\\n\\n```cs\\npublic class Person\\n{\\n  public int Id { get; set; }\\n  public string FirstName { get; set; }\\n  public string LastName { get; set; }\\n  public string Address { get; set; }\\n  public DateTime DateOfBirth { get; set; }\\n  /* Other fascinating properties... */\\n}\\n\\npublic class Order\\n{\\n  public int Id { get; set; }\\n  public string ProductOrdered { get; set; }\\n  public string OrderedById { get; set; }\\n  public string OrderedByFirstName { get; internal set; }\\n  public string OrderedByLastName { get; internal set; }\\n}\\n```\\n\\nIn the example above you have 2 types of entity: `Person` and `Order`. The `Order` entity makes use of the the `Id`, `FirstName` and `LastName` properties of the `Person` entity in the properties `OrderedById`, `OrderedByFirstName` and `OrderedByLastName`. For persistence (ie saving to the database) purposes the only necessary `Person` property is `OrderedById` identity. `OrderedByFirstName` and `OrderedByLastName` are just \\"nice to haves\\" - essentially present to make implementing the GUI more straightforward.\\n\\nTo express this behaviour / intention in the object model the setters for `OrderedByFirstName` and `OrderedByLastName` are marked as `internal`. The implication of this is that properties like this can only be initialised within the current assembly - or any explicitly associated \\"friend\\" assemblies. In practice this meant that internally set properties were only populated when an object was read in from the database. It wasn\'t possible to set these properties in other assemblies which meant less code was written (<u>a good thing</u>\\n\\n) - after all, why set a property when you don\'t need to?\\n\\nBackground explanation over. It may still be a little unclear but I hope you get the gist.\\n\\n## What\'s our problem?\\n\\nI was writing unit tests for the controllers in our main web application and was having problems with my arrangements. I was mocking the database calls in my controllers much in the manner that you might expect:\\n\\n```ts\\n// Arrange\\n  var orderDb = new Mock<IOrderDb>();\\n  orderDb\\n    .Setup(x => x.GetOrder(It.IsAny<int>()))\\n    .Returns(new Order{\\n      Id = 123,\\n      ProductOrdered = \\"Packet of coffee\\",\\n      OrderedById = 987456,\\n      OrderedByFirstName = \\"John\\",\\n      OrderedByLastName = \\"Reilly\\"\\n    });\\n}\\n```\\n\\nAll looks fine doesn\'t it? It\'s not. Because `OrderedByFirstName` and `OrderedByLastName` have internal setters we are <u>unable</u>\\n\\nto initialise them from within the context of our test project. So what to do?\\n\\nWe toyed with 3 approaches and since each has merits I thought it worth going through each of them:\\n\\n1. To the MOQumentation Batman!: [http://code.google.com/p/moq/wiki/QuickStart](http://code.google.com/p/moq/wiki/QuickStart)! Looking at the MOQ documentation it states the following:\\n\\n   _Mocking internal types of another project: add the following assembly attributes (typically to the AssemblyInfo.cs) to the project containing the internal types:_\\n\\n   ```cs\\n   // This assembly is the default dynamic assembly generated Castle DynamicProxy,\\n   // used by Moq. Paste in a single line.\\n   [assembly:InternalsVisibleTo(\\"DynamicProxyGenAssembly2,PublicKey=0024000004800000940000000602000000240000525341310004000001000100c547cac37abd99c8db225ef2f6c8a3602f3b3606cc9891605d02baa56104f4cfc0734aa39b93bf7852f7d9266654753cc297e7d2edfe0bac1cdcf9f717241550e0a7b191195b7667bb4f64bcb8e2121380fd1d9d46ad2d92d2d15605093924cceaf74c4861eff62abf69b9291ed0a340e113be11e6a7d3113e92484cf7045cc7\\")]\\n   [assembly: InternalsVisibleTo(\\"The.NameSpace.Of.Your.Unit.Test\\")] //I\'d hope it was shorter than that...\\n   ```\\n\\n   This looked to be exactly what we needed and in most situations it would make sense to go with this. Unfortunately for us there was a gotcha. Certain core shared parts of our application platform were [GAC](http://en.wikipedia.org/wiki/Global_Assembly_Cache)\'d. A requirement for GAC-ing an assembly is that it is [signed](http://msdn.microsoft.com/en-us/library/xc31ft41.aspx).\\n\\n   The upshot of this was that if we wanted to use the `InternalsVisibleTo` approach then we would need to sign our web application test project. We weren\'t particularly averse to that and initially did so without much thought. It was then we remembered that every assembly referenced by a signed assembly must also be signed as well. We didn\'t really want to sign our main web application purely for testing purposes. We could and if there weren\'t viable alternatives we well might have. But it just seemed like the wrong reason to be taking that decision. Like using a sledgehammer to crack a nut.\\n\\n2. The next approach we took was using mock objects. Instead of using our objects straight we would mock them as below:\\n\\n   ```cs\\n   //Create mock and set internal properties\\n         var orderMock = new Mock<Order>();\\n         orderMock.SetupGet(x => x.OrderedByFirstName).Returns(\\"John\\");\\n         orderMock.SetupGet(x => x.OrderedByLastName).Returns(\\"Reilly\\");\\n\\n         //Set up standard properties\\n         orderMock.SetupAllProperties();\\n         var orderStub = orderMock.Object;\\n         orderStub.Id = 123;\\n         orderStub.ProductOrdered = \\"Packet of coffee\\";\\n         orderStub.OrderedById = 987456;\\n   ```\\n\\n   Now this approach worked fine but had a couple of snags:\\n\\n   - As you can see it\'s pretty verbose and much less clear to read than it was previously.\\n   - It required that we add the `virtual` keyword to all our internally set properties like so:\\n\\n     ```cs\\n     public class Order\\n     {\\n       // ....\\n       public virtual string OrderedByFirstName { get; internal set; }\\n       public virtual string OrderedByLastName { get; internal set; }\\n       // ...\\n     }\\n     ```\\n\\n   - Our standard constructor already initialised the value of our internally set properties. So adding `virtual` to the internally set properties generated [ReSharper](http://www.jetbrains.com/resharper/) warnings aplenty about virtual properties being initialised in the constructor. Fair enough.\\n\\n   Because of the snags it still felt like we were in nutcracking territory...\\n\\n3. ... and this took us to the approach that we ended up adopting: a special mocking constructor for each class we wanted to test, for example:\\n\\n   ```cs\\n   /// <summary>\\n   /// Mocking constructor used to initialise internal properties\\n   /// </summary>\\n   public Order(string orderedByFirstName = null, string orderedByLastName = null)\\n   : this()\\n   {\\n   OrderedByFirstName = orderedByFirstName;\\n   OrderedByLastName = orderedByLastName;\\n   }\\n\\n   ```\\n\\n   Thanks to the ever lovely [Named and Optional Arguments](http://msdn.microsoft.com/en-us/library/dd264739.aspx) feature of C# combined with [Object Initializers](http://msdn.microsoft.com/en-us/library/bb397680.aspx) it meant it was possible to write quite expressive, succinct code using this approach; for example:\\n\\n   ```cs\\n   var order = new Order(\\n           orderedByFirstName: \\"John\\",\\n           orderedByLastName: \\"Reilly\\"\\n         )\\n         {\\n           Id = 123,\\n           ProductOrdered = \\"Packet of coffee\\",\\n           OrderedById = 987456\\n         };\\n   ```\\n\\n   Here we\'re calling the mocking constructor to set the internally set properties and subsequently initialising the other properties using the object initialiser mechanism.\\n\\n   Implementing these custom constructors wasn\'t a massive piece of work and so we ended up settling on this technique for initialising internal properties."},{"id":"/2012/04/05/making-pdfs-from-html-in-c-using","metadata":{"permalink":"/2012/04/05/making-pdfs-from-html-in-c-using","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2012-04-05-making-pdfs-from-html-in-c-using/index.md","source":"@site/blog/2012-04-05-making-pdfs-from-html-in-c-using/index.md","title":"Making PDFs from HTML in C# using WKHTMLtoPDF","description":"Update 03/01/2013","date":"2012-04-05T00:00:00.000Z","formattedDate":"April 5, 2012","tags":[{"label":"wkhtmltopdf","permalink":"/tags/wkhtmltopdf"},{"label":"webkit","permalink":"/tags/webkit"},{"label":"c#","permalink":"/tags/c"},{"label":"html","permalink":"/tags/html"},{"label":"pdf","permalink":"/tags/pdf"}],"readingTime":6.815,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Making PDFs from HTML in C# using WKHTMLtoPDF","authors":"johnnyreilly","tags":["wkhtmltopdf","webkit","c#","html","pdf"],"hide_table_of_contents":false},"prevItem":{"title":"A Simple Technique for Initialising Properties with Internal Setters for Unit Testing","permalink":"/2012/04/16/simple-technique-for-initialising"},"nextItem":{"title":"WCF - moving from Config to Code, a simple WCF service harness (plus implementing your own Authorization)","permalink":"/2012/03/22/wcf-moving-from-config-to-code-simple"}},"content":"## Update 03/01/2013\\n\\nI\'ve written a subsequent post which builds on the work of this original post. The new post exposes this functionality via a WCF service and can be found [here](http://icanmakethiswork.blogspot.co.uk/2013/01/html-to-pdf-using-wcf-service.html).\\n\\n## Making PDFs from HTML\\n\\nI wanted to talk about an approach I\'ve discovered for making PDFs directly from HTML. I realise that in these wild and crazy days of [PDF.js](http://mozilla.github.com/pdf.js/) and the like that techniques like this must seem very old hat. That said, this technique works and more importantly it solves a problem I was faced with but without forcing the users to move the \\"newest hottest version of X\\". Much as many of would love to solve problems this way, alas many corporations move slower than that and in the meantime we still have to deliver - we still have to meet requirements. Rather than just say \\"I did this\\" I thought I\'d record how I got to this point in the first place. I don\'t know about you but I find the reasoning behind why different technical decisions get made quite an interesting topic...\\n\\nFor some time I\'ve been developing / supporting an application which is used in an intranet environment where the company mandated browser is still IE 6. It was a requirement that there be \\"print\\" functionality in this application. As is well known (even by Microsoft themselves) the print functionality in IE 6 was never fantastic. But the requirement for usable printouts remained.\\n\\nThe developers working on the system before me decided to leverage Crystal Reports (remember that?). Essentially there was a reporting component to the application at the time which created custom reports using Crystal and rendered them to the user in the form of PDFs (which have been eminently printable for as long as I care to remember). One of the developers working on the system realised that it would be perfectly possible to create some \\"reports\\" within Crystal which were really \\"print to PDF\\" screens for the app.\\n\\nIt worked well and this solution stayed in place for a very long time. However, some years down the line the Crystal Reports was discarded as the reporting mechanism for the app. But we were unable to decommission Crystal entirely because we still needed it for printing.\\n\\nI\'d never really liked the Crystal solution for a number of reasons:\\n\\n1. We needed custom stored procs to drive the Crystal print screens which were near duplicates of the main app procs. This duplication of effort never felt right.\\n2. We had to switch IDEs whenever we were maintaining our print screens. And the Crystal IDE is not a joy to use.\\n3. Perhaps most importantly, for certain users we needed to hide bits of information from the print. The version of Crystal we were using did not make the dynamic customisation of our print screens a straightforward proposition. (In its defence we weren\'t really using it for what it was designed for.) As a result the developers before me had ended up creating various versions of each print screen revealing different levels of information. As you can imagine, this meant that the effort involved in making changes to the print screens had increased exponentially\\n\\nIt occurred to me that it would be good if we could find some way of generating our own PDF reports without using Crystal that would be a step forward. It was shortly after this that I happened upon [WKHTMLtoPDF](http://code.google.com/p/wkhtmltopdf/). This is an open source project which describes itself as a _\\"Simple shell utility to convert html to pdf using the webkit rendering engine, and qt.\\"_ I tested it out on various websites and it worked. It wasn\'t by any stretch of the imagination a perfect HTML to PDF tool but the quality it produced greatly outstripped the presentation currently in place via Crystal.\\n\\nThis was just the ticket. Using WKHTMLtoPDF I could have simple web pages in the application which could be piped into WKHTMLtoPDF to make a PDF as needed. It could be dynamic - because ASP.NET is dynamic. We wouldn\'t need to write and maintain custom stored procs anymore. And happily we would no longer need to use Crystal.\\n\\nBefore we could rid ourselves of Crystal though, I needed a way that I could generate these PDFs on the fly within the website. For this I ended up writing a simple wrapper class for WKHTMLtoPDF which could be used to invoke it on the fly. In fact a good portion of this was derived from various contributions on [a post on StackOverflow](http://stackoverflow.com/q/1331926). It ended up looking like this:\\n\\n<script src=\\"https://gist.github.com/2341776.js?file=PdfGenerator.cs\\"><\/script>\\n\\nWith this wrapper I could pass in URLs and extract out PDFs. Here\'s a couple of examples of me doing just that:\\n\\n```cs\\n//Create PDF from a single URL\\n    var pdfUrl = PdfGenerator.HtmlToPdf(pdfOutputLocation: \\"~/PDFs/\\",\\n        outputFilenamePrefix: \\"GeneratedPDF\\",\\n        urls: new string[] { \\"http://news.bbc.co.uk\\" });\\n\\n    //Create PDF from multiple URLs\\n    var pdfUrl = PdfGenerator.HtmlToPdf(pdfOutputLocation: \\"~/PDFs/\\",\\n        outputFilenamePrefix: \\"GeneratedPDF\\",\\n        urls: new string[] { \\"http://www.google.co.uk\\", \\"http://news.bbc.co.uk\\" });\\n```\\n\\nAs you can see from the second example above it\'s possible to pipe a number of URLs into the wrapper all to be rendered to a single PDF. Most of the time this was surplus to our requirements but it\'s good to know it\'s possible. Take a look at the BBC website PDF generated by the first example:\\n\\n<iframe src=\\"https://docs.google.com/file/d/0B87K8-qxOZGFYktEWGtXRXJSSS1ZWFR4emFfMmVxZw/preview\\" width=\\"500\\" height=\\"500\\"></iframe>\\n\\nPretty good, no? As you can see it\'s not perfect from looking at the titles (bit squashed) but I deliberately picked a more complicated page to show what WKHTMLtoPDF was capable of. The print screens I had in mind to build would be significantly simpler than this.\\n\\nOnce this was in place I was able to scrap the Crystal solution. It was replaced with a couple of \\"print to PDF\\" ASPXs in the main web app which would be customised when rendering to hide the relevant bits of data from the user. These ASPXs would be piped into the HtmlToPdf method as needed and then the user would be redirected to that PDF. If for some reason the PDF failed to render the users would see the straight \\"print to PDF\\" ASPX - just not as a PDF if you see what I mean. I should say that it was pretty rare for a PDF to not render but this was my failsafe.\\n\\nThis new solution had a number of upsides from our perspective:\\n\\n- Development maintenance time (and consequently cost for our customers) for print screens was significantly reduced. This was due to the print screens being part of the main web app. This meant they shared styling etc with all the other web screens and the dynamic nature of ASP.NET made customising a screen on the fly simplicity itself.\\n- We were now able to regionalise our print screens for the users in the same way as we did with our main web app. This just wasn\'t realistic with the Crystal solution because of the amount of work involved.\\n- I guess this is kind of a [DRY](http://en.wikipedia.org/wiki/Don%27t_repeat_yourself) solution :-)\\n\\nYou can easily make use of the above approach yourself. All you need do is download and install [WKHTMLtoPDF](http://code.google.com/p/wkhtmltopdf/) on your machine. I advise using version 0.9.9 as the later release candidates appear slightly buggy at present.\\n\\nCouple of gotchas:\\n\\n1. Make sure that you pass the correct installation path to the HtmlToPdf method if you installed it anywhere other than the default location. You\'ll see that the class assumes the default if it wasn\'t passed\\n2. Ensure that Read and Execute rights are granted to the wkhtmltopdf folder for the relevant process\\n3. Ensure that Write rights are granted for the location you want to create your PDFs for the relevant process\\n\\nIn our situation we are are invoking this directly in our web application on demand. I have no idea how this would scale - perhaps not well. This is not really an issue for us as our user base is fairly small and this functionality isn\'t called excessively. I think if this was used much more than it is I\'d be tempted to hive off this functionality into a separate app. But this works just dandy for now."},{"id":"/2012/03/22/wcf-moving-from-config-to-code-simple","metadata":{"permalink":"/2012/03/22/wcf-moving-from-config-to-code-simple","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2012-03-22-wcf-moving-from-config-to-code-simple/index.md","source":"@site/blog/2012-03-22-wcf-moving-from-config-to-code-simple/index.md","title":"WCF - moving from Config to Code, a simple WCF service harness (plus implementing your own Authorization)","description":"Last time I wrote about WCF I was getting up and running with WCF Transport Windows authentication using NetTcpBinding in an Intranet environment. I ended up with a WCF service hosted in a Windows Service which did pretty much what the previous post name implies.","date":"2012-03-22T00:00:00.000Z","formattedDate":"March 22, 2012","tags":[{"label":"ServiceAuthorizationManager","permalink":"/tags/service-authorization-manager"},{"label":"Windows Account","permalink":"/tags/windows-account"},{"label":"Windows Service","permalink":"/tags/windows-service"},{"label":"configuration","permalink":"/tags/configuration"},{"label":"WCF","permalink":"/tags/wcf"},{"label":"authorisation","permalink":"/tags/authorisation"},{"label":"NetTcpBinding","permalink":"/tags/net-tcp-binding"}],"readingTime":10.65,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"WCF - moving from Config to Code, a simple WCF service harness (plus implementing your own Authorization)","authors":"johnnyreilly","tags":["ServiceAuthorizationManager","Windows Account","Windows Service","configuration","WCF","authorisation","NetTcpBinding"],"hide_table_of_contents":false},"prevItem":{"title":"Making PDFs from HTML in C# using WKHTMLtoPDF","permalink":"/2012/04/05/making-pdfs-from-html-in-c-using"},"nextItem":{"title":"Using the PubSub / Observer pattern to emulate constructor chaining without cluttering up global scope","permalink":"/2012/03/17/using-pubsub-observer-pattern-to"}},"content":"Last time I wrote about WCF I was getting up and running with [WCF Transport Windows authentication using NetTcpBinding in an Intranet environment](http://icanmakethiswork.blogspot.com/2012/02/wcf-transport-windows-authentication.html). I ended up with a WCF service hosted in a Windows Service which did pretty much what the previous post name implies.\\n\\nSince writing that I\'ve taken things on a bit further and I thought it worth recording my approach whilst it\'s still fresh in my mind. There\'s 3 things I want to go over:\\n\\n1. I\'ve moved away from the standard config driven WCF approach to a more \\"code-first\\" style\\n2. I\'ve established a basic Windows Service hosted WCF service / client harness which is useful if you\'re trying to get up and running with a WCF service quickly\\n3. I\'ve locked down the WCF authorization to a single Windows account through the use of my own [ServiceAuthorizationManager](http://msdn.microsoft.com/en-us/library/ms731774.aspx)\\n\\n## Moving from Config to Code\\n\\nSo, originally I was doing what all the cool kids are doing and driving the configuration of my WCF service and all its clients through config files. And why not? I\'m in good company.\\n\\nHere\'s why not: it gets \\\\***very**\\\\* verbose \\\\***very**\\\\* quickly....\\n\\nOkay - that\'s not the end of the world. My problem was that I had \\\\~10 Windows Services and 3 Web applications that needed to call into my WCF Service. I didn\'t want to have to separately tweak 15 or so configs each time I wanted to make one standard change to WCF configuration settings. I wanted everything in one place.\\n\\nNow there\'s newer (and probably hipper) ways of achieving this. [Here\'s one possibility I happened upon on StackOverflow that looks perfectly fine.](http://stackoverflow.com/a/2814286)\\n\\nWell I didn\'t use a hip new approach - no I went Old School with my old friend the [appSettings file attribute](http://msdn.microsoft.com/en-us/library/ms228154.aspx). Remember that? It\'s just a simple way to have all your common appSettings configuration settings in a single file which can be linked to from as many other apps as you like. It\'s wonderful and I\'ve been using it for a long time now. Unfortunately it\'s pretty basic in that it\'s only the appSettings section that can be shared out; no `&lt;system.serviceModel&gt;` or similar.\\n\\nBut that wasn\'t really a problem from my perspective. I realised that there were actually very few things that needed to be configurable for my WCF service. Really I wanted a basic WCF harness that could be initialised in code which implicitly set all the basic configuration with settings that worked (ie it was set up with defaults like maximum message size which were sufficiently sized). On top of that I would allow myself to configure just those things that I needed to through the use of my own custom WCF config settings in the shared appSettings.config file.\\n\\nOnce done I massively reduced the size of my configs from frankly gazillions of entries to just these appSettings.config entries which were shared across each of my WCF service clients and by my Windows Service harness:\\n\\n```xml\\n<appSettings>\\n  <add key=\\"WcfBaseAddressForClient\\" value=\\"net.tcp://localhost:9700/\\"/>\\n  <add key=\\"WcfWindowsSecurityApplied\\" value=\\"true\\" />\\n  <add key=\\"WcfCredentialsUserName\\" value=\\"myUserName\\" />\\n  <add key=\\"WcfCredentialsPassword\\" value=\\"myPassword\\" />\\n  <add key=\\"WcfCredentialsDomain\\" value=\\"myDomain\\" />\\n  </appSettings>\\n```\\n\\nAnd these config settings used only by my Windows Service harness:\\n\\n```xml\\n<appSettings file=\\"../Shared/AppSettings.config\\">\\n    <add key=\\"WcfBaseAddressForService\\" value=\\"net.tcp://localhost:9700/\\"/>\\n  </appSettings>\\n```\\n\\n## Show me your harness\\n\\nI ended up with a quite a nice basic \\"vanilla\\" framework that allowed me to quickly set up Windows Service hosted WCF services. The framework also provided me with a simple way to consume these WCF services with a minimum of code an configuration. No muss. No fuss. :-) So pleased with it was I that I thought I\'d go through it here much in the manner of a chef baking a cake...\\n\\nTo start with I created myself a Windows Service in Visual Studio which I grandly called \\"WcfWindowsService\\". The main service class looked like this:\\n\\n```cs\\npublic class WcfWindowsService: ServiceBase\\n  {\\n    public static string WindowsServiceName = \\"WCF Windows Service\\";\\n    public static string WindowsServiceDescription = \\"Windows service that hosts a WCF service.\\";\\n\\n    private static readonly log4net.ILog _logger = log4net.LogManager.GetLogger(System.Reflection.MethodBase.GetCurrentMethod().DeclaringType);\\n\\n    public List<ServiceHost> _serviceHosts = null;\\n\\n    public WcfWindowsService()\\n    {\\n      ServiceName = WindowsServiceName;\\n    }\\n\\n    public static void Main()\\n    {\\n      ServiceBase.Run(new WcfWindowsService());\\n    }\\n\\n    /// <summary>\\n    /// The Windows Service is starting\\n    /// </summary>\\n    /// <param name=\\"args\\"></param>\\n    protected override void OnStart(string[] args)\\n    {\\n      try\\n      {\\n        CloseAndClearServiceHosts();\\n\\n        //Make log4net startup\\n        XmlConfigurator.Configure();\\n        _logger.Warn(\\"WCF Windows Service starting...\\");\\n        _logger.Info(\\"Global.WcfWindowsSecurityApplied = \\" + Global.WcfWindowsSecurityApplied.ToString().ToLower());\\n\\n        if (Global.WcfWindowsSecurityApplied)\\n        {\\n          _logger.Info(\\"Global.WcfOnlyAuthorizedForWcfCredentials = \\" + Global.WcfOnlyAuthorizedForWcfCredentials.ToString().ToLower());\\n\\n          if (Global.WcfOnlyAuthorizedForWcfCredentials)\\n          {\\n            _logger.Info(\\"Global.WcfCredentialsDomain = \\" + Global.WcfCredentialsDomain);\\n            _logger.Info(\\"Global.WcfCredentialsUserName = \\" + Global.WcfCredentialsUserName);\\n          }\\n        }\\n\\n        //Create binding\\n        var wcfBinding = WcfHelper.CreateBinding(Global.WcfWindowsSecurityApplied);\\n\\n        // Create a servicehost and endpoints for each service and open each\\n        _serviceHosts = new List<ServiceHost>();\\n        _serviceHosts.Add(WcfServiceFactory<IHello>.CreateAndOpenServiceHost(typeof(HelloService), wcfBinding));\\n        _serviceHosts.Add(WcfServiceFactory<IGoodbye>.CreateAndOpenServiceHost(typeof(GoodbyeService), wcfBinding));\\n\\n        _logger.Warn(\\"WCF Windows Service started.\\");\\n      }\\n      catch (Exception exc)\\n      {\\n        _logger.Error(\\"Problem starting up\\", exc);\\n\\n        throw exc;\\n      }\\n    }\\n\\n    /// <summary>\\n    /// The Windows Service is stopping\\n    /// </summary>\\n    protected override void OnStop()\\n    {\\n      CloseAndClearServiceHosts();\\n\\n      _logger.Warn(\\"WCF Windows Service stopped\\");\\n    }\\n\\n    /// <summary>\\n    /// Close and clear service hosts in list and clear it down\\n    /// </summary>\\n    private void CloseAndClearServiceHosts()\\n    {\\n      if (_serviceHosts != null)\\n      {\\n        foreach (var serviceHost in _serviceHosts)\\n        {\\n          CloseAndClearServiceHost(serviceHost);\\n        }\\n\\n        _serviceHosts.Clear();\\n      }\\n    }\\n\\n    /// <summary>\\n    /// Close and clear the passed service host\\n    /// </summary>\\n    /// <param name=\\"serviceHost\\"></param>\\n    private void CloseAndClearServiceHost(ServiceHost serviceHost)\\n    {\\n      if (serviceHost != null)\\n      {\\n        _logger.Info(string.Join(\\", \\", serviceHost.BaseAddresses) + \\" is closing...\\");\\n\\n        serviceHost.Close();\\n\\n        _logger.Info(string.Join(\\", \\", serviceHost.BaseAddresses) + \\" is closed\\");\\n      }\\n    }\\n  }\\n```\\n\\nAs you\'ve no doubt noticed this makes use of [Log4Net](http://logging.apache.org/log4net/) for logging purposes (I\'ll assume you\'re aware of it). My Windows Service implements such fantastic WCF services as HelloService and GoodbyeService. Each revolutionary in their own little way. To give you a taste of the joie de vivre that these services exemplify take a look at this:\\n\\n```cs\\n// Implement the IHello service contract in a service class.\\n  public class HelloService : WcfServiceAuthorizationManager, IHello\\n  {\\n    // Implement the IHello methods.\\n    public string GreetMe(string thePersonToGreet)\\n    {\\n      return \\"well hello there \\" + thePersonToGreet;\\n    }\\n  }\\n```\\n\\nExciting! WcfWindowsService also references another class called \\"Global\\" which is a helper class - to be honest not much more than a wrapper for my config settings. It looks like this:\\n\\n```cs\\nstatic public class Global\\n  {\\n    #region Properties\\n\\n    // eg \\"net.tcp://localhost:9700/\\"\\n    public static string WcfBaseAddressForService { get { return ConfigurationManager.AppSettings[\\"WcfBaseAddressForService\\"]; } }\\n\\n    // eg true\\n    public static bool WcfWindowsSecurityApplied { get { return bool.Parse(ConfigurationManager.AppSettings[\\"WcfWindowsSecurityApplied\\"]); } }\\n\\n    // eg true\\n    public static bool WcfOnlyAuthorizedForWcfCredentials { get { return bool.Parse(ConfigurationManager.AppSettings[\\"WcfOnlyAuthorizedForWcfCredentials\\"]); } }\\n\\n    // eg \\"myDomain\\"\\n    public static string WcfCredentialsDomain { get { return ConfigurationManager.AppSettings[\\"WcfCredentialsDomain\\"]; } }\\n\\n    // eg \\"myUserName\\"\\n    public static string WcfCredentialsUserName { get { return ConfigurationManager.AppSettings[\\"WcfCredentialsUserName\\"]; } }\\n\\n    // eg \\"myPassword\\" - this should *never* be stored unencrypted and is only ever used by clients that are not already running with the approved Windows credentials\\n    public static string WcfCredentialsPassword { get { return ConfigurationManager.AppSettings[\\"WcfCredentialsPassword\\"]; } }\\n\\n    #endregion\\n  }\\n```\\n\\nWcfWindowsService creates and hosts a HelloService and a GoodbyeService when it starts up. It does this using my handy WcfServiceFactory:\\n\\n```cs\\npublic class WcfServiceFactory<TInterface>\\n  {\\n    private static readonly log4net.ILog _logger = log4net.LogManager.GetLogger(System.Reflection.MethodBase.GetCurrentMethod().DeclaringType);\\n\\n    public static ServiceHost CreateAndOpenServiceHost(Type serviceType, NetTcpBinding wcfBinding)\\n    {\\n      var serviceHost = new ServiceHost(serviceType, new Uri(Global.WcfBaseAddressForService + ServiceHelper<TInterface>.GetServiceName()));\\n      serviceHost.AddServiceEndpoint(typeof(TInterface), wcfBinding, \\"\\");\\n      serviceHost.Authorization.ServiceAuthorizationManager = new WcfServiceAuthorizationManager(); // This allows us to control authorisation within WcfServiceAuthorizationManager\\n      serviceHost.Open();\\n\\n      _logger.Info(string.Join(\\", \\", serviceHost.BaseAddresses) + \\" is now listening.\\");\\n\\n      return serviceHost;\\n    }\\n  }\\n```\\n\\nTo do this it also uses my equally handy WcfHelper class:\\n\\n```cs\\nstatic public class WcfHelper\\n  {\\n    /// <summary>\\n    /// Create a NetTcpBinding\\n    /// </summary>\\n    /// <param name=\\"useWindowsSecurity\\"></param>\\n    /// <returns></returns>\\n    public static NetTcpBinding CreateBinding(bool useWindowsSecurity)\\n    {\\n      var wcfBinding = new NetTcpBinding();\\n      if (useWindowsSecurity)\\n      {\\n        wcfBinding.Security.Mode = SecurityMode.Transport;\\n        wcfBinding.Security.Transport.ClientCredentialType = TcpClientCredentialType.Windows;\\n      }\\n      else\\n        wcfBinding.Security.Mode = SecurityMode.None;\\n\\n      wcfBinding.MaxBufferSize = int.MaxValue;\\n      wcfBinding.MaxReceivedMessageSize = int.MaxValue;\\n      wcfBinding.ReaderQuotas.MaxArrayLength = int.MaxValue;\\n      wcfBinding.ReaderQuotas.MaxDepth = int.MaxValue;\\n      wcfBinding.ReaderQuotas.MaxStringContentLength = int.MaxValue;\\n      wcfBinding.ReaderQuotas.MaxBytesPerRead = int.MaxValue;\\n\\n      return wcfBinding;\\n    }\\n  }\\n\\n  /// <summary>\\n  /// Create a WCF Client for use anywhere (be it Windows Service or ASP.Net web application)\\n  /// nb Credential fields are optional and only likely to be needed by web applications\\n  /// </summary>\\n  /// <typeparam name=\\"TInterface\\"></typeparam>\\n  public class WcfClientFactory<TInterface>\\n  {\\n    public static TInterface CreateChannel(bool useWindowsSecurity, string wcfBaseAddress, string wcfCredentialsUserName = null, string wcfCredentialsPassword = null, string wcfCredentialsDomain = null)\\n    {\\n      //Create NetTcpBinding using universally\\n      var wcfBinding = WcfHelper.CreateBinding(useWindowsSecurity);\\n\\n      //Get Service name from examining the ServiceNameAttribute decorating the interface\\n      var serviceName = ServiceHelper<TInterface>.GetServiceName();\\n\\n      //Create the factory for creating your channel\\n      var factory = new ChannelFactory<TInterface>(\\n        wcfBinding,\\n        new EndpointAddress(wcfBaseAddress + serviceName)\\n        );\\n\\n      //if credentials have been supplied then use them\\n      if (!string.IsNullOrEmpty(wcfCredentialsUserName))\\n      {\\n        factory.Credentials.Windows.ClientCredential = new System.Net.NetworkCredential(wcfCredentialsUserName, wcfCredentialsPassword, wcfCredentialsDomain);\\n      }\\n\\n      //Create the channel\\n      var channel = factory.CreateChannel();\\n\\n      return channel;\\n    }\\n  }\\n```\\n\\nNow the above WcfHelper class and it\'s comrade-in-arms the WcfClientFactory don\'t live in the WcfWindowsService project with the other classes. No. They live in a separate project called the WcfWindowsServiceContracts project with their old mucker the ServiceHelper:\\n\\n```cs\\npublic class ServiceHelper<T>\\n  {\\n    public static string GetServiceName()\\n    {\\n      var customAttributes = typeof(T).GetCustomAttributes(false);\\n      if (customAttributes.Length > 0)\\n      {\\n        foreach (var customAttribute in customAttributes)\\n        {\\n          if (customAttribute is ServiceNameAttribute)\\n          {\\n            return ((ServiceNameAttribute)customAttribute).ServiceName;\\n          }\\n        }\\n      }\\n\\n      throw new ArgumentException(\\"Interface is missing ServiceNameAttribute\\");\\n    }\\n  }\\n\\n  [AttributeUsage(AttributeTargets.Interface, AllowMultiple = false)]\\n  public class ServiceNameAttribute : System.Attribute\\n  {\\n    public ServiceNameAttribute(string serviceName)\\n    {\\n      this.ServiceName = serviceName;\\n    }\\n\\n    public string ServiceName { get; set; }\\n  }\\n```\\n\\nNow can you guess what the WcfWindowsServiceContracts project might contain? Yes; contracts for your services (oh the excitement)! What might one of these contracts look like I hear you ask... Well, like this:\\n\\n```cs\\n[ServiceContract()]\\n  [ServiceName(\\"HelloService\\")]\\n  public interface IHello\\n  {\\n    [OperationContract]\\n    string GreetMe(string thePersonToGreet);\\n  }\\n```\\n\\nThe WcfWindowsServiceContracts project is included in \\\\***any**\\\\* WCF client solution that wants to call your WCF services. It is also included in the WCF service solution. It facilitates the calling of services. What you\'re no doubt wondering is how this might be achieved. Well here\'s how, it uses our old friend the `WcfClientFactory`:\\n\\n```cs\\nvar helloClient = WcfClientFactory<IHello>\\n    .CreateChannel(\\n      useWindowsSecurity:     Global.WcfWindowsSecurityApplied,  // eg true\\n      wcfBaseAddress:         Global.WcfBaseAddressForClient,    // eg \\"net.tcp://localhost:9700/\\"\\n      wcfCredentialsUserName: Global.WcfCredentialsUserName,     // eg \\"myUserName\\" - Optional parameter - only passed by web applications that need to impersonate the valid user\\n      wcfCredentialsPassword: Global.WcfCredentialsPassword,     // eg \\"myPassword\\" - Optional parameter - only passed by web applications that need to impersonate the valid user\\n      wcfCredentialsDomain:   Global.WcfCredentialsDomain        // eg \\"myDomain\\" - Optional parameter - only passed by web applications that need to impersonate the valid user\\n    );\\n  var greeting = helloClient.GreetMe(\\"John\\"); //\\"well hello there John\\"\\n```\\n\\nSee? Simple as simple. The eagle eyed amongst you will have noticed that client example above is using \\"`Global`\\" which is essentially a copy of the `Global` class mentioned above that is part of the WcfWindowsService project.\\n\\n## Locking down Authorization to a single Windows account\\n\\nI can tell you think i\'ve forgotten something. \\"Tell me about this locking down to the single Windows account / what is this mysterious `WcfServiceAuthorizationManager` class that all your WCF services inherit from? Don\'t you fob me off now.... etc\\"\\n\\nWell ensuring that only a single Windows account is authorised (yes dammit the original English spelling) to access our WCF services is achieved by implementing our own `ServiceAuthorizationManager` class. This implementation is used for authorisation by your `ServiceHost` and the logic sits in the overridden `CheckAccessCore` method. All of our WCF service classes will inherit from our `ServiceAuthorizationManager` class and so trigger the `CheckAccessCore` authorisation each time they are called.\\n\\nAs you can see from the code below, depending on our configuration, we lock down access to all our WCF services to a specific Windows account. This is far from the only approach that you might want to take to authorisation; it\'s simply the one that we\'ve been using. However the power of being able to implement your own authorisation in the `CheckAccessCore` method allows you the flexibility to do pretty much anything you want:\\n\\n```cs\\npublic class WcfServiceAuthorizationManager : ServiceAuthorizationManager\\n  {\\n    protected static readonly log4net.ILog _logger = log4net.LogManager.GetLogger(System.Reflection.MethodBase.GetCurrentMethod().DeclaringType);\\n\\n    protected override bool CheckAccessCore(OperationContext operationContext)\\n    {\\n      if (Global.WcfWindowsSecurityApplied)\\n      {\\n        if ((operationContext.ServiceSecurityContext.IsAnonymous) ||\\n          (operationContext.ServiceSecurityContext.PrimaryIdentity == null))\\n        {\\n          _logger.Error(\\"WcfWindowsSecurityApplied = true but no credentials have been supplied\\");\\n          return false;\\n        }\\n\\n        if (Global.WcfOnlyAuthorizedForWcfCredentials)\\n        {\\n          if (operationContext.ServiceSecurityContext.PrimaryIdentity.Name.ToLower() == Global.WcfCredentialsDomain.ToLower() + \\"\\\\\\\\\\" + Global.WcfCredentialsUserName.ToLower())\\n          {\\n            _logger.Debug(\\"WcfOnlyAuthorizedForWcfCredentials = true and the valid user (\\" + operationContext.ServiceSecurityContext.PrimaryIdentity.Name + \\") has been supplied and access allowed\\");\\n            return true;\\n          }\\n          else\\n          {\\n            _logger.Error(\\"WcfOnlyAuthorizedForWcfCredentials = true and an invalid user (\\" + operationContext.ServiceSecurityContext.PrimaryIdentity.Name + \\") has been supplied and access denied\\");\\n            return false;\\n          }\\n        }\\n        else\\n        {\\n          _logger.Debug(\\"WcfOnlyAuthorizedForWcfCredentials = false, credentials were supplied (\\" + operationContext.ServiceSecurityContext.PrimaryIdentity.Name + \\") so access allowed\\");\\n          return true;\\n        }\\n      }\\n      else\\n      {\\n        _logger.Info(\\"WcfWindowsSecurityApplied = false so we are allowing unfettered access\\");\\n        return true;\\n      }\\n    }\\n  }\\n```\\n\\nPhewwww... I know this has ended up as a bit of a brain dump but hopefully people will find it useful. At some point I\'ll try to put up the above solution on GitHub so people can grab it easily for themselves."},{"id":"/2012/03/17/using-pubsub-observer-pattern-to","metadata":{"permalink":"/2012/03/17/using-pubsub-observer-pattern-to","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2012-03-17-using-pubsub-observer-pattern-to/index.md","source":"@site/blog/2012-03-17-using-pubsub-observer-pattern-to/index.md","title":"Using the PubSub / Observer pattern to emulate constructor chaining without cluttering up global scope","description":"Yes the title of this post is \\\\*painfully\\\\* verbose. Sorry about that. Couple of questions for you: - Have you ever liked the way you can have base classes in C# which can then be inherited and subclassed in a different file / class","date":"2012-03-17T00:00:00.000Z","formattedDate":"March 17, 2012","tags":[{"label":"inheritance","permalink":"/tags/inheritance"},{"label":"javascript","permalink":"/tags/javascript"},{"label":"c#","permalink":"/tags/c"},{"label":"observer pattern","permalink":"/tags/observer-pattern"},{"label":"pubsub","permalink":"/tags/pubsub"}],"readingTime":5.475,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Using the PubSub / Observer pattern to emulate constructor chaining without cluttering up global scope","authors":"johnnyreilly","tags":["inheritance","javascript","c#","observer pattern","pubsub"],"hide_table_of_contents":false},"prevItem":{"title":"WCF - moving from Config to Code, a simple WCF service harness (plus implementing your own Authorization)","permalink":"/2012/03/22/wcf-moving-from-config-to-code-simple"},"nextItem":{"title":"Striving for (JavaScript) Convention","permalink":"/2012/03/12/striving-for-javascript-convention"}},"content":"Yes the title of this post is \\\\***painfully**\\\\* verbose. Sorry about that. Couple of questions for you: - Have you ever liked the way you can have base classes in C# which can then be inherited and subclassed <u>in a different file / class</u>\\n\\n?\\n\\n- Have you ever thought; gosh it\'d be nice to do something like that in JavaScript...\\n- Have you then looked at JavaScripts prototypical inheritance and thought \\"right.... I\'m sure it\'s possible but this going to end up like [War and Peace](http://en.wikipedia.org/wiki/War_and_Peace)\\"\\n- Have you then subsequently thought \\"and hold on a minute... even if I did implement this using the prototype and split things between different files / modules wouldn\'t I have to pollute the global scope to achieve that? And wouldn\'t that mean that my code was exposed to the vagaries of any other scripts on the page? Hmmm...\\"\\n- [Men! Are you skinny? Do bullies kick sand in your face?](http://www.thrillingdetective.com/eyes/oxford.html) (Just wanted to see if you were still paying attention...)\\n\\n## The Problem\\n\\nWell, the above thoughts occurred to me just recently. I had a situation where I was working on an MVC project and needed to build up quite large objects within JavaScript representing various models. The models in question were already implemented on the server side using classes and made extensive use of inheritance because many of the properties were shared between the various models. That is to say we would have models which were implemented through the use of a class inheriting a base class which in turn inherits a further base class. With me? Good. Perhaps I can make it a little clearer with an example. Here are my 3 classes. First BaseReilly.cs:\\n\\n```cs\\npublic class BaseReilly\\n{\\n    public string LastName { get; set; }\\n\\n        public BaseReilly()\\n        {\\n            LastName = \\"Reilly\\";\\n        }\\n    }\\n```\\n\\nNext BoyReilly.cs (which inherits from BaseReilly):\\n\\n```cs\\npublic class BoyReilly : BaseReilly\\n{\\n    public string Sex { get; set; }\\n\\n    public BoyReilly()\\n        : base()\\n    {\\n        Sex = \\"It is a manchild\\";\\n    }\\n}\\n```\\n\\nAnd finally JohnReilly.cs (which inherits from BoyReilly which in turn inherits from BaseReilly):\\n\\n```cs\\npublic class JohnReilly : BoyReilly\\n{\\n    public string FirstName { get; set; }\\n\\n    public JohnReilly()\\n        : base()\\n    {\\n        FirstName = \\"John\\";\\n    }\\n}\\n```\\n\\nUsing the above I can create myself my very own \\"JohnReilly\\" like so:\\n\\n```cs\\nvar johnReilly = new JohnReilly();\\n```\\n\\nAnd it will look like this:\\n\\n![](CSharp-version-of-JohnReilly.png)\\n\\nI was looking to implement something similar on the client and within JavaScript. I was keen to ensure [code reuse](http://en.wikipedia.org/wiki/Code_reuse). And my inclination to keep things simple made me wary of making use of the [prototype](http://bonsaiden.github.com/JavaScript-Garden/#object.prototype). It is undoubtedly powerful but I don\'t think even the mighty [Crockford](http://javascript.crockford.com/prototypal.html) would consider it \\"simple\\". Also I had the reservation of exposing my object to the global scope. So what to do? I had an idea.... ## The Big Idea\\n\\nFor a while I\'ve been making use explicit use of the [Observer pattern](http://en.wikipedia.org/wiki/Observer_pattern) in my JavaScript, better known by most as the publish/subscribe (or \\"PubSub\\") pattern. There\'s a million JavaScript libraries that facilitate this and after some experimentation I finally settled on [higgins](https://github.com/phiggins42/bloody-jquery-plugins/blob/master/pubsub.js) implementation as it\'s simple and I saw a [JSPerf](http://jsperf.com/pubsubjs-vs-jquery-custom-events/11) which demonstrated it as either the fastest or second fastest in class. Up until now my main use for it had been to facilitate loosely coupled GUI interactions. If I wanted one component on the screen to influence anothers behaviour I simply needed to get the first component to publish out the relevant events and the second to subscribe to these self-same events. One of the handy things about publishing out events this way is that with them you can also include data. This data can be useful when driving the response in the subscribers. However, it occurred to me that it would be equally possible to pass an object when publishing an event. \\\\*\\\\*<u>And the subscribers could enrich that object with data as they saw fit.</u>\\n\\n\\\\*\\\\* Now this struck me as a pretty useful approach. It\'s not rock solid secure as it\'s always possible that someone could subscribe to your events and get access to your object as you published out. However, that\'s pretty unlikely to happen accidentally; certainly far less likely than someone else\'s global object clashing with your global object. ## What might this look like in practice?\\n\\nSo this is what it ended up looking like when I turned my 3 classes into JavaScript files / modules. First BaseReilly.js:\\n\\n```js\\n$(function () {\\n  $.subscribe(\'PubSub.Inheritance.Emulation\', function (obj) {\\n    obj.LastName = \'Reilly\';\\n  });\\n});\\n```\\n\\nNext BoyReilly.js:\\n\\n```js\\n$(function () {\\n  $.subscribe(\'PubSub.Inheritance.Emulation\', function (obj) {\\n    obj.Sex = \'It is a manchild\';\\n  });\\n});\\n```\\n\\nAnd finally JohnReilly.js:\\n\\n```js\\n$(function () {\\n  $.subscribe(\'PubSub.Inheritance.Emulation\', function (obj) {\\n    obj.FirstName = \'John\';\\n  });\\n});\\n```\\n\\nIf the above scripts have been included in a page I can create myself my very own \\"JohnReilly\\" in JavaScript like so:\\n\\n```js\\nvar oJohnReilly = {}; //Empty object\\n\\n$.publish(\'PubSub.Inheritance.Emulation\', [oJohnReilly]); //Empty object \\"published\\" so it can be enriched by subscribers\\n\\nconsole.log(JSON.stringify(oJohnReilly)); //Show me this thing you call \\"JohnReilly\\"\\n```\\n\\nAnd it will look like this:\\n\\n![](JavaScript-version-of-JohnReilly.png)\\n\\nAnd it works. Obviously the example I\'ve given above it somewhat naive - in reality my object properties are driven by GUI components rather than hard-coded. But I hope this illustrates the point. This technique allows you to simply share functionality between different JavaScript files and so keep your codebase tight. I certainly wouldn\'t recommend it for all circumstances but when you\'re doing something as simple as building up an object to be used to pass data around (as I am) then it works very well indeed. ## A Final Thought on Script Ordering\\n\\nA final thing that maybe worth mentioning is script ordering. The order in which functions are called is driven by the order in which subscriptions are made. In my example I was registering the scripts in this order:\\n\\n```html\\n<script src=\\"/Scripts/PubSubInheritanceDemo/BaseReilly.js\\"><\/script>\\n<script src=\\"/Scripts/PubSubInheritanceDemo/BoyReilly.js\\"><\/script>\\n<script src=\\"/Scripts/PubSubInheritanceDemo/JohnReilly.js\\"<>/script>\\n```\\n\\nSo when my event was published out the functions in the above JS files would be called in this order: 1. BaseReilly.js 2. BoyReilly.js 3. JohnReilly.js\\n\\nIf you were so inclined you could use this to emulate inheritance in behaviour. Eg you could set a property in `BaseReilly.js` which was subsequently overridden in `JohnReilly.js` or `BoyReilly.js` if you so desired. I\'m not doing that myself but it occurred as a possibility. ## PS\\n\\nIf you\'re interested in learning more about JavaScript stabs at inheritance you could do far worse than look at Bob Inces in depth StackOverflow [answer](http://stackoverflow.com/a/1598077/761388).\\n\\n```\\n\\n```"},{"id":"/2012/03/12/striving-for-javascript-convention","metadata":{"permalink":"/2012/03/12/striving-for-javascript-convention","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2012-03-12-striving-for-javascript-convention/index.md","source":"@site/blog/2012-03-12-striving-for-javascript-convention/index.md","title":"Striving for (JavaScript) Convention","description":"Update","date":"2012-03-12T00:00:00.000Z","formattedDate":"March 12, 2012","tags":[{"label":"naming convention","permalink":"/tags/naming-convention"},{"label":"javascript","permalink":"/tags/javascript"},{"label":"Hungarian Notation","permalink":"/tags/hungarian-notation"},{"label":"DOJO","permalink":"/tags/dojo"}],"readingTime":9.63,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Striving for (JavaScript) Convention","authors":"johnnyreilly","tags":["naming convention","javascript","Hungarian Notation","DOJO"],"hide_table_of_contents":false},"prevItem":{"title":"Using the PubSub / Observer pattern to emulate constructor chaining without cluttering up global scope","permalink":"/2012/03/17/using-pubsub-observer-pattern-to"},"nextItem":{"title":"jQuery Unobtrusive Remote Validation","permalink":"/2012/03/03/jquery-unobtrusive-remote-validation"}},"content":"## Update\\n\\nThe speed of change makes fools of us all. Since I originally wrote this post all of 3 weeks ago Visual Studio 11 beta has been released and the issues I was seeking to solve have pretty much been resolved by the new innovations found therein. It\'s nicely detailed in [@carlbergenhem](http://www.twitter.com/carlbergenhem)\'s blog post: [My Top 5 Visual Studio 11 Designer Improvements for ASP.NET 4.5 Development](https://blogs.telerik.com/blogs/posts/12-03-26/my-top-5-visual-studio-11-designer-improvements-for-asp-net-4-5-development.aspx). I\'ve left the post in place below but much of what I said (particularly with regard to Hungarian Notation) I\'ve now moved away from. That was originally my intention anyway so that\'s no bad thing. The one HN artefact that I\'ve held onto is using \\"$\\" as a prefix for jQuery objects. I think that still makes sense. I would have written my first line of JavaScript in probably 2000. It probably looked something like this: `alert(\'hello world\')`. I know. Classy. As I\'ve mentioned before it was around 2010 before I took JavaScript in any way seriously. Certainly it was then when I started to actively learn the language. Because up until this point I\'d been studiously avoiding writing any JavaScript at all I\'d never really given thought to forms and conventions. When I wrote any JavaScript I just used the same style and approaches as I used in my main development language (of C#). By and large I have been following the .net naming conventions which are ably explained by Pete Brown [here](http://10rem.net/articles/net-naming-conventions-and-programming-standards---best-practices). Over time I have started to move away from this approach. Without a deliberate intention to do so I have found myself adopting a different style for my JavaScript code as compared with anything else I write. I wouldn\'t go so far as to say I\'m completely happy with the style I\'m currently using. But I find it more helpful than not and thought it might be worth talking about. It was really 2 things that started me down the road of \\"rolling my own\\" convention: dynamic typing and the lack of safety nets. Let\'s take each in turn.... ### 1\\\\. Dynamic typing\\n\\nHaving grown up (in a development sense) using compiled and strongly-typed languages I was used to the IDE making it pretty clear what was what through friendly tooltips and the like:\\n\\n![](IDE.png)\\n\\nJavaScript is loosely / dynamically typed ([occasionally called \\"untyped\\" but let\'s not go there](http://stackoverflow.com/questions/9154388/does-untyped-also-mean-dynamically-typed-in-the-academic-cs-world)). This means that the IDE can\'t easily determine what\'s what. So no tooltips for you sunshine. ### 2\\\\. The lack of safety nets / running with scissors\\n\\nNow I\'ve come to love it but what I realised pretty quickly when getting into JavaScript was this: you are running with scissors. If you\'re not careful and you don\'t take precautions it can bloody quickly. If I\'m writing C# I have a lot of safety nets. Not the least of which is \\"does it compile\\"? If I declare an integer and then subsequently try to assign a string value to it <u>it won\'t let me</u>\\n\\n. But JavaScript is forgiving. Some would say too forgiving. Let\'s do something mad:\\n\\n```js\\nvar iAmANumber = 77;\\n\\nconsole.log(iAmANumber); //Logs a number\\n\\niAmANumber = \\"It\'s a string\\";\\n\\nconsole.log(iAmANumber); //Logs a string\\n\\niAmANumber = {\\n  description: \'I am an object\',\\n};\\n\\nconsole.log(iAmANumber); //Logs an object\\n\\niAmANumber = function (myVariable) {\\n  console.log(myVariable);\\n};\\n\\nconsole.log(iAmANumber); //Logs a function\\niAmANumber(\'I am not a number, I am a free man!\'); //Calls a function which performs a log\\n```\\n\\nNow if I were to attempt something similar in C# fuggedaboudit but JavaScript; no I\'m romping home free:\\n\\n![](Mad-Stuff.png)\\n\\nNow I\'m not saying that you should ever do the above, and thinking about it I can\'t think of a situation where you\'d want to (suggestions on a postcard). But the point is it\'s possible. And because it\'s possible to do this deliberately, it\'s doubly possible to do this accidentally. My point is this: it\'s easy to make bugs in JavaScript. ## What ~~Katy~~ Johnny Did Next\\n\\nI\'d started making more and more extensive use of JavaScript. I was beginning to move in the direction of using the [single-page application](http://en.wikipedia.org/wiki/Single-page_application) approach (_although more in the sense of giving application style complexity to individual pages rather than ensuring that entire applications ended up in a single page_). This meant that whereas in the past I\'d had the occasional 2 lines of JavaScript I now had a multitude of functions which were all interacting in response to user input. All these functions would contain a number of different variables. As well as this I was making use of jQuery for both Ajax purposes and to smooth out the DOM inconsistencies between various browsers. This only added to the mix as variables in one of my functions could be any one of the following: - a number\\n\\n- a string\\n- a boolean\\n- a date\\n- an object\\n- an array\\n- a function\\n- a jQuery object - not strictly a distinct JavaScript type obviously but treated pretty much as one in the sense that it has a particular functions / properties etc associated with it\\n\\nAs I started doing this sort of work I made no changes to my coding style. Wherever possible I did \\\\***exactly**\\\\* what I would have been doing in C# in JavaScript. And it worked fine. Until.... Okay there is no \\"until\\" as such, it did work fine. But what I found was that I would do a piece of work, check it into source control, get users to test it, release the work into Production and promptly move onto the next thing. However, a little way down the line there would be a request to add a new feature or perhaps a bug was reported and I\'d find myself back looking at the code. And, as is often the case, despite the comments I would realise that it wasn\'t particularly clear why something worked in the way it did. (Happily it\'s not just me that has this experience, paranoia has lead me to ask many a fellow developer and they have confessed to similar) When it came to bug hunting in particular I found myself cursing the lack of friendly tooltips and the like. Each time I wanted to look at a variable I\'d find myself tracking back through the function, looking for the initial use of the variable to determine the type. Then I\'d be tracking forward through the function for each subsequent use to ensure that it conformed. Distressingly, I would find examples of where it looked like I\'d forgotten the type of the variable towards the end of a function (for which I can only, regrettably, blame myself). Most commonly I would have a situation like this:\\n\\n```js\\nvar tableCell = $(\'#ItIsMostDefinitelyATableCell\'); //I jest ;-)\\n\\n/* ...THERE WOULD BE SOME CODE DOING SOMETHING HERE... */\\n\\ntableCell.className = \'makeMeProminent\'; //Oh dear - not good.\\n```\\n\\nYou see what happened above? I forgot I had a jQuery object and instead treated it like it was a standard DOM element. Oh dear. ## Spinning my own safety net; Hungarian style\\n\\nAfter I\'d experienced a few of the situations described above I decided that steps needed to be taken to minimise the risk of this. In this case, I decided that \\"steps\\" meant [Hungarian notation](http://en.wikipedia.org/wiki/Hungarian_notation). I know. I bet you\'re wincing right now. For those of you that don\'t remember HN was pretty much the standard way of coding at one point (although at the point that I started coding professionally it had already started to decline). It was adopted in simpler times long before the modern IDE\'s that tell you what each variable is became the norm. Back when you couldn\'t be sure of the types you were dealing with. In short, kind of like my situation with JavaScript right now. There\'s not much to it. By and large HN simply means having a lowercase prefix of 1-3 characters on all your variables indicating type. It doesn\'t solve all your problems. It doesn\'t guarantee to stop bugs. But because each instance of the variables use implicitly indicates it\'s type it makes bugs more glaringly obvious. This means when writing code I\'m less likely to misuse a variable (eg `iNum = \\"JIKJ\\"`) because part of my brain would be bellowing: \\"that just looks wrong... pay better attention lad!\\". Likewise, if I\'m scanning through some JavaScript and searching for a bug then this can make it more obvious. Here\'s some examples of different types of variables declared using the style I have adopted:\\n\\n```js\\nvar iInteger = 4;\\nvar dDecimal = 10.5;\\nvar sString = \'I am a string\';\\nvar bBoolean = true;\\nvar dteDate = new Date();\\nvar oObject = {\\n  description: \'I am an object\',\\n};\\nvar aArray = [34, 77];\\nvar fnFunction = function () {\\n  //Do something\\n};\\nvar $jQueryObject = $(\'#ItIsMostDefinitelyATableCell\');\\n```\\n\\nSome of you have read this and thought \\"hold on a minute... JavaScript doesn\'t have integers / decimals etc\\". You\'re quite right. My style is not specifically stating the type of a variable. More it is seeking to provide a guide on how a variable should be used. JavaScript does not have integers. But oftentimes I\'ll be using a number variable which i will only ever want to treat as an integer. And so I\'ll name it accordingly. ## Spinning a better safety net; DOJO style\\n\\nI would be the first to say that alternative approaches are available. And here\'s one I recently happened upon that I rather like the look of: look 2/3rds down at the parameters section of [the DOJO styleguide](http://dojotoolkit.org/community/styleGuide) Essentially they advise specifying parameter types through the use of prefixed comments. See the examples below:\\n\\n```js\\nfunction(/*String*/ foo, /*int*/ bar)...\\n```\\n\\nor\\n\\n```js\\nfunction(/_String?_/ foo, /_int_/ bar, /_String[]?_/ baz)...\\n```\\n\\nI really rather like this approach and I\'m thinking about starting to adopt it. It\'s not possible in Hungarian Notation to be so clear about the purpose of a variable. At least not without starting to adopt all kinds of kooky conventions that take in all the possible permutations of variable types. And if you did that you\'d really be defeating yourself anyway as it would simply reduce the clarity of your code and make bugs more likely. ## Spinning a better safety net; unit tests\\n\\nDespite being quite used to writing unit tests for all my server-side code I have not yet fully embraced unit testing on the client. Partly I\'ve been holding back because of the variety of JavaScript testing frameworks available. I wasn\'t sure which to start with. But given that it is so easy to introduce bugs into JavaScript I have come to the conclusion that it\'s better to have some tests in place rather than none. Time to embrace the new. ## Conclusion\\n\\nI\'ve found using Hungarian Notation useful whilst working in JavaScript. Not everyone will feel the same and I think that\'s fair enough; within reason I think it\'s generally a good idea to go with what you find useful. However, I am giving genuine consideration to moving to the DOJO style and moving back to my more standard camel-cased variable names instead of Hungarian Notation. Particularly since I strive to keep my functions short with the view that ideally each should 1 thing well. Keep it simple etc... And so in a perfect world the situation of forgetting a variables purpose shouldn\'t really arise... I think once I\'ve got up and running with JavaScript unit tests I may make that move. Hungarian Notation may have proved to be just a stop-gap measure until better techniques were employed...\\n\\n```\\n\\n```"},{"id":"/2012/03/03/jquery-unobtrusive-remote-validation","metadata":{"permalink":"/2012/03/03/jquery-unobtrusive-remote-validation","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2012-03-03-jquery-unobtrusive-remote-validation/index.md","source":"@site/blog/2012-03-03-jquery-unobtrusive-remote-validation/index.md","title":"jQuery Unobtrusive Remote Validation","description":"Just recently I have been particularly needing to make use of remote / server-side validation in my ASP.NET MVC application and found that the unobtrusive way of using this seemed to be rather inadequately documented (of course it\'s possible that it\'s well documented and I just didn\'t find the resources). Anyway I\'ve rambled on much longer than I intended to in this post so here\'s the TL;DR:","date":"2012-03-03T00:00:00.000Z","formattedDate":"March 3, 2012","tags":[{"label":"jquery","permalink":"/tags/jquery"},{"label":"jquery remote validation","permalink":"/tags/jquery-remote-validation"},{"label":"jquery unobtrusive validation","permalink":"/tags/jquery-unobtrusive-validation"}],"readingTime":8.995,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"jQuery Unobtrusive Remote Validation","authors":"johnnyreilly","tags":["jquery","jquery remote validation","jquery unobtrusive validation"],"hide_table_of_contents":false},"prevItem":{"title":"Striving for (JavaScript) Convention","permalink":"/2012/03/12/striving-for-javascript-convention"},"nextItem":{"title":"The Joy of JSON","permalink":"/2012/02/23/joy-of-json"}},"content":"Just recently I have been particularly needing to make use of remote / server-side validation in my ASP.NET MVC application and found that the unobtrusive way of using this seemed to be rather inadequately documented (of course it\'s possible that it\'s well documented and I just didn\'t find the resources). Anyway I\'ve rambled on much longer than I intended to in this post so here\'s the TL;DR:\\n\\n- You \\\\***can**\\\\* use remote validation driven by unobtrusive data attributes\\n- Using remote validation you can supply \\\\***multiple**\\\\* parameters to be evaluated\\n- It is possible to block validation and force it to be re-evaluted - although using a slightly hacky method which I document here. For what it\'s worth I acknowledge up front that this is \\\\***not**\\\\* an ideal solution but it does seem to work. I really hope there is a better solution out there and if anyone knows about it then please get in contact and let me know.\\n\\nOff we go... So, jQuery unobtrusive validation; clearly the new cool right?\\n\\nI\'d never been particularly happy with the validation that I had traditionally been using with ASP.NET classic. It worked... but it always seemed a little... clunky? I realise that\'s not the most well expressed concern. For basic scenarios it seemed fine, but I have recollections of going through some pain as soon as I stepped outside of the basic form validation. Certainly when it came to validating custom controls that we had developed it never seemed entirely straightforward to get validation to play nice.\\n\\nBased on this I was keen to try something new and the opportunity presented itself when we started integrating MVC into our classic WebForms app. (By the way if you didn\'t know that MVC and ASP.NET could live together in perfect harmony, well, they can! And a good explanation on how to achieve it is offered by Colin Farr [here](http://www.britishdeveloper.co.uk/2011/05/convert-web-forms-mvc3-how-to.html).)\\n\\nJ\xf6rn Zaefferer came out with the [jQuery validation plug-in](http://bassistance.de/jquery-plugins/jquery-plugin-validation/) way back in 2006. And mighty fine it is too. Microsoft (gor\' bless \'em) really brought something new to the jQuery validation party when they came out with their unobtrusive javascript validation library along with MVC 3. What this library does, in short, is allows for jQuery validation to be driven by `data-val-*` attributes alone as long as the [jquery.validate.js](http://ajax.aspnetcdn.com/ajax/jquery.validate/1.9/jquery.validate.js) and [jquery.validate.unobtrusive.js](http://ajax.aspnetcdn.com/ajax/mvc/3.0/jquery.validate.unobtrusive.js) libraries are included in the screen (I have assumed you are already including jQuery). I know; powerful stuff!\\n\\nA good explanation of unobtrusive validation is given by Brad Wilson [here](http://bradwilson.typepad.com/blog/2010/10/mvc3-unobtrusive-validation.html).\\n\\nAnyway, to my point: what about remote validation? That is to say, what about validation which needs to go back to the server to perform the necessary tests? Well I struggled to find decent examples of how to use this. Those that I did find seemed to universally be php examples; not so useful for an ASP.NET user. Also, when I did root out an ASP.NET example there seemed to be a fundamental flaw. Namely, if remote validation hadn\'t been triggered and completed successfully then the submit could fire anyway. This seems to be down to the asynchronous nature of the test; ie because it is \\\\***not**\\\\* synchronous there is no \\"block\\" to the submit. And out of the box with unobtrusive validation there seems no way to make this synchronous. I could of course wire this up manually and simply side-step the restrictions of unobtrusive validation but that wasn\'t what I wanted.\\n\\n\\\\*\\\\*\\\\*Your mission John, should you decide to accept it, is this: <u>block the submit until remote validation has completed successfully</u>\\n\\n. As always, should you or any of your I.M. Force be caught or killed, the Secretary will disavow any knowledge of your actions.\\\\*\\\\*\\\\*\\n\\nSo that\'s what I wanted to do. Make it act like it\'s synchronous even though it\'s asynchronous. Bit horrible but I had a deadline to meet and so this is my pragmatic solution. There may be better alternatives but this worked for me.\\n\\nFirst of all the HTML:\\n\\n```html\\n<form\\n  action=\\"/Dummy/ValidationDemo.mvc/SaveUser\\"\\n  id=\\"ValidationForm\\"\\n  method=\\"post\\"\\n>\\n  First name:\\n  <input\\n    data-val=\\"true\\"\\n    data-val-required=\\"First Name required\\"\\n    id=\\"FirstName\\"\\n    name=\\"FirstName\\"\\n    type=\\"text\\"\\n    value=\\"\\"\\n  />\\n\\n  Last name:\\n  <input\\n    data-val=\\"true\\"\\n    data-val-required=\\"Last Name required\\"\\n    id=\\"LastName\\"\\n    name=\\"LastName\\"\\n    type=\\"text\\"\\n    value=\\"\\"\\n  />\\n\\n  User name:\\n  <input\\n    id=\\"UserName\\"\\n    name=\\"UserName\\"\\n    type=\\"text\\"\\n    value=\\"\\"\\n    data-val=\\"true\\"\\n    data-val-required=\\"You must enter a user name before we can validate it remotely\\"\\n    data-val-remote=\\"&amp;#39;UserNameInput&amp;#39; is invalid.\\"\\n    data-val-remote-additionalfields=\\"*.FirstName,*.LastName\\"\\n    data-val-remote-url=\\"/Dummy/ValidationDemo/IsUserNameValid\\"\\n  />\\n\\n  <input\\n    id=\\"SaveMyDataButton\\"\\n    name=\\"SaveMyDataButton\\"\\n    type=\\"button\\"\\n    value=\\"Click to Save\\"\\n  />\\n</form>\\n```\\n\\nI should mention that on my actual page (a cshtml partial view) the HTML for the inputs is generated by the use of the [InputExtensions.TextBoxFor](http://msdn.microsoft.com/en-us/library/system.web.mvc.html.inputextensions.textboxfor.aspx) method which is lovely. It takes your model and using the validation attributes that decorate your models properties it generates the relevant jQuery unobtrusive validation data attributes so you don\'t have to do it manually.\\n\\nBut for the purposes of seeing what\'s \\"under the bonnet\\" I thought it would be more useful to post the raw HTML so it\'s entirely clear what is being used. Also there doesn\'t appear to be a good way (that I\'ve yet seen) for automatically generating Remote validation data attributes in the way that I\'ve found works. So I\'m manually specifying the `data-val-remote-*` attributes using the htmlAttributes parameter of the TextBoxFor ([using \\"\\\\_\\" to replace \\"-\\"](http://stackoverflow.com/questions/4844001/html5-data-with-asp-net-mvc-textboxfor-html-attributes) obviously).\\n\\nNext the JavaScript that performs the validation:\\n\\n```js\\n$(document).ready(function () {\\n  var intervalId = null,\\n    //\\n    // DECLARE FUNCTION EXPRESSIONS\\n    //\\n\\n    //======================================================\\n    // function that triggers update when remote validation\\n    // completes successfully\\n    //======================================================\\n    pendingValidationComplete = function () {\\n      var i, errorList, errorListForUsers;\\n      var $ValidationForm = $(\'#ValidationForm\');\\n      if ($ValidationForm.data(\'validator\').pendingRequest === 0) {\\n        clearInterval(intervalId);\\n\\n        //Force validation to present to user\\n        //(this will *not* retrigger remote validation)\\n        if ($ValidationForm.valid()) {\\n          alert(\'Validation has succeeded - you can now submit\');\\n        } else {\\n          //Validation failed!\\n          errorList = $ValidationForm.data(\'validator\').errorList;\\n          errorListForUsers = [];\\n          for (i = 0; i < errorList.length; i++) {\\n            errorListForUsers.push(errorList[i].message);\\n          }\\n\\n          alert(errorListForUsers.join(\'\\\\r\\\\n\'));\\n        }\\n      }\\n    },\\n    //======================================================\\n    // Trigger validation\\n    //======================================================\\n    triggerValidation = function (evt) {\\n      //Removed cached values where remote is concerned\\n      // so remote validation is retriggered\\n      $(\'#UserName\').removeData(\'previousValue\');\\n\\n      //Trigger validation\\n      $(\'#ValidationForm\').valid();\\n\\n      //Setup interval which will evaluate validation\\n      //(this approach because of remote validation)\\n      intervalId = setInterval(pendingValidationComplete, 50);\\n    };\\n\\n  //\\n  //ASSIGN EVENT HANDLERS\\n  //\\n  $(\'#SaveMyDataButton\').click(triggerValidation);\\n});\\n```\\n\\nAnd finally the Controller:\\n\\n```cs\\npublic JsonResult IsUserNameValid(string UserName,\\n                                  string FirstName,\\n                                  string LastName)\\n{\\n  var userNameIsUnique = IsUserNameUnique(UserName);\\n  if (userNameIsUnique)\\n    return Json(true, JsonRequestBehavior.AllowGet);\\n  else\\n    return Json(string.Format(\\n                  \\"{0} is already taken I\'m afraid {1} {2}\\",\\n                  UserName, FirstName, LastName),\\n                JsonRequestBehavior.AllowGet);\\n}\\n\\nprivate bool IsUserNameUnique(string potentialUserName)\\n{\\n  return false;\\n}\\n```\\n\\nSo what happens here exactly? Well it\'s like this:\\n\\n1. The user enters their first name, last name and desired user name and hits the \\"Click to Save\\" button.\\n2. This forces validation by first removing any cached validation values stored in `previousValue` data attribute and then triggering the `valid` method. Disclaimer: I KNOW THIS IS A LITTLE HACKY. I would have expected there would be some way in the API to manually re-force validation. Unless I\'ve missed something there doesn\'t appear to be. ([And the good citizens of Stack Overflow would seem to concur.](http://stackoverflow.com/a/3797712/761388)) I would guess that the underlying assumption is that if nothing has changed on the client then that\'s all that matters. Clearly that\'s invalid for our remote example given that a username could be \\"claimed\\" at any time; eg in between people first entering their username (when validation should have fired automatically) and actually submitting the form. Anyway - this approach seems to get us round the problem.\\n3. When validation takes place the IsUserNameValid action / method on our controller will be called. It\'s important to note that I have set up a method that takes 3 inputs; UserName, which is supplied by default as the UserName input is the one which is decorated with remote validation attributes as well as the 2 extra inputs of FirstName and LastName. In the example I\'ve given I don\'t actually need these extra attributes. I\'m doing this because I know that I have situations in remote validation where I \\\\***need**\\\\* to supply multiple inputs and so essentially I did it here as a proof of concept. The addition of these 2 extra inputs was achieved through the use of the `data-val-remote-additionalfields` attribute. When searching for documentation about this I found absolutely <u>none</u>\\n\\n. I assume there is some out there - if anyone knows then I\'d very pleased to learn about it. I only learned about it in the end by finding an example of someone using this out in the great wide world and understanding how to use it based on their example. To understand how the `data-val-remote-additionalfields` attribute works you can look at jquery.validate.unobtrusive.js. If you\'re just looking to get up and running then I found that the following works: `data-val-remote-additionalfields=\\"*.FirstName,*.LastName\\"` You will notice that: - Each parameter is supplied in the format _\\\\*.[InputName]_ and inputs are delimited by \\",\\"\'s - Name is a <u>required</u>\\n\\nattribute for an input if you wish it to be evaluated with unobtrusive validation. (Completely obvious statement I realise; I\'m writing that sentence more for my benefit than yours) - Finally, our validation always fails. That\'s deliberate - I just wanted to be clear on the approach used to get remote unobtrusive validation with extra parameters up and running. 4. Using `setInterval` we intend to trigger the `pendingValidationComplete` function to check if remote validation has completed every 50ms - again I try to avoid setInterval wherever possible but this seems to be the most sensible solution in this case. 5. When the remote request finally completes (ie when `pendingRequest` has a value of 0) then we can safely proceed on the basis of our validation results. In the example above I\'m simply alerting to the screen based on my results; this is \\\\***not**\\\\* advised for any finished work; I\'m just using this mechanism here to demonstrate the principle.\\n\\nValidation in action:\\n\\n![](validation-screenshot2.png)\\n\\nWell I\'ve gone on for far too long but I am happy to have an approach that does what I need. It does feel like a slightly hacky solution and I expect that there is a better approach for this that I\'m not aware of. As much as anything else I\'ve written this post in the hope that someone who knows this better approach will set me straight. In summary, this works. But if you\'re aware of a better solution then please do get in contact - I\'d love to know!\\n\\n**PS:**Just in case you\'re in the process of initially getting up and running with unobtrusive validation I\'ve listed below a couple of general helpful bits of config etc:\\n\\nThe following setting is essential for Application_Start in Global.asax.cs:\\n\\n```cs\\nDataAnnotationsModelValidatorProvider.AddImplicitRequiredAttributeForValueTypes = false;\\n```\\n\\nThe following settings should be used in your Web.Config:\\n\\n```xml\\n<appSettings>\\n  <add key=\\"ClientValidationEnabled\\" value=\\"true\\" />\\n  <add key=\\"UnobtrusiveJavaScriptEnabled\\" value=\\"true \\"/>\\n</appSettings>\\n```\\n\\nMy example used the following scripts:\\n\\n```html\\n<script src=\\"Scripts/jquery-1.7.1.js\\"><\/script>\\n<script src=\\"Scripts/jquery.validate.js\\"><\/script>\\n<script src=\\"Scripts/jquery.validate.unobtrusive.js\\"><\/script>\\n<script src=\\"Scripts/ValidationDemo.js\\"><\/script>\\n```"},{"id":"/2012/02/23/joy-of-json","metadata":{"permalink":"/2012/02/23/joy-of-json","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2012-02-23-joy-of-json/index.md","source":"@site/blog/2012-02-23-joy-of-json/index.md","title":"The Joy of JSON","description":"So back to JSON. For those of you that don\'t know JSON stands for JavaScript Object Notation and is lightweight text based data interchange format. Rather than quote other people verbatim you can find thorough explanations of JSON here: - Introducing JSON","date":"2012-02-23T00:00:00.000Z","formattedDate":"February 23, 2012","tags":[{"label":"Dave Ward","permalink":"/tags/dave-ward"},{"label":"json","permalink":"/tags/json"},{"label":"Encosia","permalink":"/tags/encosia"},{"label":"Christian Heilmann javascript object literal","permalink":"/tags/christian-heilmann-javascript-object-literal"},{"label":"douglas crockford","permalink":"/tags/douglas-crockford"}],"readingTime":3.545,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"The Joy of JSON","authors":"johnnyreilly","tags":["Dave Ward","json","Encosia","Christian Heilmann javascript object literal","douglas crockford"],"hide_table_of_contents":false},"prevItem":{"title":"jQuery Unobtrusive Remote Validation","permalink":"/2012/03/03/jquery-unobtrusive-remote-validation"},"nextItem":{"title":"WCF Transport Windows authentication using NetTcpBinding in an Intranet environment","permalink":"/2012/02/15/wcf-transport-windows-authentication"}},"content":"So back to JSON. For those of you that don\'t know JSON stands for JavaScript Object Notation and is lightweight text based data interchange format. Rather than quote other people verbatim you can find thorough explanations of JSON here: - [Introducing JSON](http://www.json.org/)\\n\\n- [JSON in Javascript](http://www.json.org/js.html)\\n\\nAs mentioned in my previous [post on Ajax](http://icanmakethiswork.blogspot.com/2012/02/potted-history-of-using-ajax-on.html) I came upon JSON quite by accident and was actually using it for some time without having any idea. But let\'s pull back a bit. Let\'s start with the JavaScript Object Literal. Some years ago I came upon this article by Christan Heilmann about the JavaScript Object Literal which had been published all the way back in 2006: [Show love to the JavaScript Object Literal](http://christianheilmann.com/2006/02/16/show-love-to-the-object-literal/) Now when I read this it was a revelation to me. I hadn\'t really used JavaScript objects a great deal at this point (yes I am one of those people that started using JavaScript without actually learning the thing) and when I had used them is was through the `var obj = new Object()` pattern (as that\'s the only approach I knew). So it was wonderful to discover that instead of the needlessly verbose:\\n\\n```js\\nvar myCar = new Object();\\nmyCar.wheels = 4;\\nmyCar.colour = \'blue\';\\n```\\n\\nI could simply use the much more concise object literal syntax to declare an object instead:\\n\\n```js\\nvar myCar = { wheels: 4, colour: \'blue\' };\\n```\\n\\nLovely. Henceforth I adopted this approach in my code as I\'m generally a believer that brevity is best. It was sometime later that I happened upon JSON (when I started looking into [jqGrid](http://icanmakethiswork.blogspot.com/2012/01/jqgrid-its-just-far-better-grid.html)). Basically I was looking to pass complex data structures backward and forward to the server and, as far as I knew, there was no way to achieve this simply in JavaScript. I was expecting that I would have to manually serialise and deserialise (yes dammit I will use the English spellings!) objects when ever I wanted to do this sort of thing. However, I was reading the the fantastic Dave Ward\'s [Encosia](http://encosia.com/) blog which on this occasion was talking about the [troubles of UpdatePanels](http://encosia.com/why-aspnet-ajax-updatepanels-are-dangerous/) (a subject close to my heart by the way) and more interestingly the use of PageMethods in ASP.NET. This is what he said that made me prick up my ears: _\\"Page methods allow ASP.NET AJAX pages to directly execute a page\u2019s static methods, using JSON (JavaScript Object Notation). JSON is basically a minimalistic version of SOAP, which is perfectly suited for light weight communication between client and server.\\"_ JSON is a lightweight SOAP eh? I\'ve used SOAP. I wonder if I could use this.... To my complete surprise, and may I say delight, I discovered that a wonderful fellow called Douglas Crockford, he of [JavaScript, The Good Parts](http://www.amazon.co.uk/JavaScript-Good-Parts-Douglas-Crockford/dp/0596517742) fame had quietly come up with JSON some time ago. JSON, from my perspective, turned out to be a simple way to turn an object into a string and then from a string back into an object. So simple that it consists of 2 methods on a JSON object: - JSON.stringify(myObject) - take an object and make me a JSON string. (and by the way isn\'t \\"stringify\\" just the loveliest method name ever?)\\n\\n- JSON.parse(myJSONString) - take a JSON string and make me an object\\n\\nLet me illustrate the above method names using the myCar example from earlier:\\n\\n```js\\nvar myCar = { wheels: 4, colour: \'blue\' };\\n// myCar is an object\\n\\nvar myCarJSON = JSON.stringify(myCar);\\n//myCarJSON will look like this: \'{\\"wheels\\":4,\\"colour\\":\\"blue\\"}\'\\n\\nvar anotherCarMadeFromMyJSON = JSON.parse(myCarJSON);\\n//anotherCarMadeFromMyJSON will be a brand new \\"car\\" object\\n```\\n\\nI\'ve also demonstrated this using the Chrome Console:\\n\\n![](Using-JSON.png)\\n\\nCrockford initially invented/discovered JSON himself and wrote a little helper library which provided a JSON object to be used by all and sundry. This can be found here: [JSON on GitHub](https://github.com/douglascrockford/JSON-js) Because JSON was so clearly wonderful, glorious and useful it ended up becoming a part of the EcmaScript 5 spec (in fact it\'s worth reading the brilliant [John Resig\'s blog post](http://ejohn.org/blog/ecmascript-5-strict-mode-json-and-more/) on this). This has lead to JSON being offered [natively in browsers](http://en.wikipedia.org/wiki/JSON#Native_encoding_and_decoding_in_browsers) for quite some time. However, for those of us (and I am one alas) still supporting IE 6 and the like we still have Crockfords JSON2.js to fall back on.\\n\\n```\\n\\n```"},{"id":"/2012/02/15/wcf-transport-windows-authentication","metadata":{"permalink":"/2012/02/15/wcf-transport-windows-authentication","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2012-02-15-wcf-transport-windows-authentication/index.md","source":"@site/blog/2012-02-15-wcf-transport-windows-authentication/index.md","title":"WCF Transport Windows authentication using NetTcpBinding in an Intranet environment","description":"Update","date":"2012-02-15T00:00:00.000Z","formattedDate":"February 15, 2012","tags":[{"label":"clientCredentialType","permalink":"/tags/client-credential-type"},{"label":"Security","permalink":"/tags/security"},{"label":"Windows","permalink":"/tags/windows"},{"label":"WCF","permalink":"/tags/wcf"},{"label":"Authentication","permalink":"/tags/authentication"},{"label":"the server has rejected the client credentials","permalink":"/tags/the-server-has-rejected-the-client-credentials"},{"label":"NetTcpBinding","permalink":"/tags/net-tcp-binding"}],"readingTime":4.545,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"WCF Transport Windows authentication using NetTcpBinding in an Intranet environment","authors":"johnnyreilly","tags":["clientCredentialType","Security","Windows","WCF","Authentication","the server has rejected the client credentials","NetTcpBinding"],"hide_table_of_contents":false},"prevItem":{"title":"The Joy of JSON","permalink":"/2012/02/23/joy-of-json"},"nextItem":{"title":"A Potted History of using Ajax (on the Microsoft Stack of Love)","permalink":"/2012/02/05/potted-history-of-using-ajax-on"}},"content":"## Update\\n\\nSince I wrote this initial post I\'ve taken thinks on a bit further. Take a look at this post to see what I mean: [http://icanmakethiswork.blogspot.com/2012/03/wcf-moving-from-config-to-code-simple.html](http://icanmakethiswork.blogspot.com/2012/03/wcf-moving-from-config-to-code-simple.html) I know I said I\'d write about JSON this time. I will get to that but not this time. This time WCF authentication quirks. I\'ve been working on a project that uses .NET Remoting to have a single central point to which web applications and Windows services can call into. This is used in an intranet environment and all the websites and Windows services were hosted on the same single server along with our .NET Remoting Windows service. (They could quite easily have been on different servers but there was no need in this case.) It was decided to \\"embrace the new\\" by migrating this .NET Remoting project over to WCF. The plan wasn\'t to do anything revolutionary, just to move from one approach to the other as easily as possible. I found the following useful article on MSDN: [http://msdn.microsoft.com/en-us/library/aa730857%28v=vs.80%29.aspx](http://msdn.microsoft.com/en-us/library/aa730857%28v=vs.80%29.aspx) This particular article was helpful and following the steps enclosed I was quickly up and running with a basic WCF service hosted in a Windows service. It was at this point I started thinking about security. The existing .NET Remoting approach had no security in place. This wasn\'t ideal but also probably wasn\'t the worry you might think. It was hosted in an intranet environment and hence not so exposed to the rigours of the Wild Wild Web. However, since I was looking at WCF I thought it would be a good opportunity to get some basic security in place. This generally pleases auditors. I opted to use [Windows Transport authentication](http://msdn.microsoft.com/en-us/library/ms733089.aspx) as this seemed pretty appropriate for an intranet environment. The idea being that we\'d authenticate with Windows for an account in our domain. After headbutting Windows for some time I managed to get a successful client call going from the website running on my development machine to the (separate) development server that was hosting our WCF Window service using Transport Windows authentication. However, when deploying the website to the development server I discovered we would experience the following error when the website attempted to call the WCF service (on the same server).\\n\\n```\\nEvent Type: Failure Audit\\nEvent Source: Security\\nEvent Category: Logon/Logoff\\nEvent ID: 537\\nDate: 15/02/2012\\nTime: 16:32:04\\nUser: NT AUTHORITY\\\\SYSTEM\\nComputer: MINE999\\nDescription:\\nLogon Failure:\\nReason: An error occurred during logon\\nLogon Type: 3\\nLogon Process: ^\\nAuthentication Package: NTLM\\nStatus code: 0xC000006D\\n```\\n\\nNot terribly helpful. At the end of the day it seemed we were suffering from a security \\"feature\\" introduced by Microsoft to prevent services calling services on the same box with a fully qualified name. An explanation of this can be found here: [http://developers.de/blogs/damir_dobric/archive/2009/08/28/authentication-problems-by-using-of-ntlm.aspx](http://developers.de/blogs/damir_dobric/archive/2009/08/28/authentication-problems-by-using-of-ntlm.aspx) Using method 1 in the enclosed link I initially worked round this by amending the registry and rebooting the server: [http://support.microsoft.com/kb/887993](http://support.microsoft.com/kb/887993) This was not a fantastic solution. Fortunately I subsequently found a better one but since the resources on the web are \\\\***ATROCIOUS**\\\\* on this point I thought I should take the time to note down the full explanation since otherwise it\'ll be lost in the mists of time. Here we go: The equivalent security to the previous .NET Remoting solution in WCF was to use this config setting on client and service:\\n\\n```xml\\n<security mode=\\"None\\" />\\n```\\n\\nAs I\'ve said, this is an intranet environment and so having this \\"none\\" security setting in place is made less worrying by the fact that the network itself is secured. But obviously this is not ideal and unlikely to be audit compliant. To use Windows security you need this netTcpBinding config setting on client and service:\\n\\n```xml\\n<security mode=\\"Transport\\">\\n<transport clientCredentialType=\\"Windows\\" />\\n</security>\\n```\\n\\nTo call the service with this setting in place you will need to be an authenticated Windows user. (Or at the very least impersonating one - but you knew that.) **NOW FOR THE MOST IMPORTANT BIT.....** The endpoint addresses \\\\***must**\\\\* be \\"localhost\\" for _both_ client and service when both are deployed to the same server. If this is not the case then you will suffer from the aforementioned security \\"feature\\" which will provide you with unhelpful \\"the server has rejected the client credentials\\" messages and \\\\***nothing**\\\\* else. **OK FINISHED - MOVE ALONG NOW... NOTHING MORE TO SEE HERE** With WCF Windows Transport authentication in place you can interrogate the calling user id within the service methods by simply evaluating ServiceSecurityContext.Current.PrimaryIdentity.Name (which will be something like \\"myDomain\\\\myUserName\\"). So we you wanted to, we could have a simple step which evaluated if the calling user is on the \\"approved\\" / \\"authorised\\" list. I\'m sure this could be made more sophisticated by using groups etc I guess - though I haven\'t investigated it further as yet. In fact, I suspect Microsoft may have something even more sophisticated still available for use which I\'m unaware of - if anyone knows a simple explanation of this then please do let me know! In closing, I do think Microsoft could work on providing more helpful error messages than \\"the server has rejected the client credentials\\". Going by what I read as I researched this error many people seem to have struggled much as I did before eventually bailing out and ended up chancing it by turning security off in their applications. Clearly it is not desirable to have people so confused by errors that they give up and settle for a less secure solution."},{"id":"/2012/02/05/potted-history-of-using-ajax-on","metadata":{"permalink":"/2012/02/05/potted-history-of-using-ajax-on","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2012-02-05-potted-history-of-using-ajax-on/index.md","source":"@site/blog/2012-02-05-potted-history-of-using-ajax-on/index.md","title":"A Potted History of using Ajax (on the Microsoft Stack of Love)","description":"This post originally started out as an explanation of JSON. However as I wrote this quickly got abandoned in favour of writing about how I came to use JSON in the first place - which was through the use of Ajax. Having written a goodly amount I\'ve now decided to move the actual JSON stuff into another post since I think Ajax is probably worth thinking about by itself rather than as an aside. So let me start at the beginning and explain how I came to use Ajax in the first place (this may take some time so please bear with me). In late 2004 I first started working on a project which I was to remain involved with (on and off) for a very long time indeed. The project was part financial reporting system and part sales incentivisation tool; it was used internally in the investment bank in which I was working. The project had been in existence for a number of years and had a web front end which at that point would been built in a combination of HTML, JavaScript, classic ASP and with a Visual Basic 6.0 back end. One of the reasons I had been brought on to the project was to help \\".Net-ify\\" the thing and migrate it to ASP.NET and C#. I digress. The interesting thing about this app was that there were actually some quite advanced things being done with it (despite the classic ASP / VB). The users could enter trades into the system which represented actual trades that had been entered into a trading system elsewhere in the organisation. These trades would be assigned a reporting value which would be based on their various attributes. (Stay with me people this will get more interesting I \\\\*promise\\\\*.) The calculation of the reporting value was quite an in depth process and needed to be performed server-side. However, the users had decreed that it wasn\'t acceptable to do a full postback to the server to perform this calculation; they wanted it done \\"on-the-fly\\". Now if you asked me at the time I\'d have said \\"can\'t be done\\". Fortunately the other people working on the project then weren\'t nearly so defeatist. Instead they went away and found Microsoft\'s webservice.htc library. For those of you that don\'t know this was a JavaScript library that Microsoft came up with to enable the access of Web Services on the client. Given that it was designed to work with IE 5 I suspect it was created between 1999-2001 (but I\'m not certain about that). Now it came as a revelation to me but this was a JavaScript library that talked to our web services through the medium of XML. In short it was my first encounter with anything remotely Ajax\\\\-y. It was exciting! However, the possibilities of what we could do didn\'t actually become apparent to me for some years. It\'s worth saying that the way we were using webservice.htc was exceedingly simplistic and rather than investigating further I took the limited ways we were using it as indications of the limitations of Ajax and / or webservice.htc. So for a long time I thought the following: - The only way to pass multiple arguments to a web service was to package up arguments into a single string with delimiters which you could split and unpackage as your first step on the server.","date":"2012-02-05T00:00:00.000Z","formattedDate":"February 5, 2012","tags":[{"label":"ajax","permalink":"/tags/ajax"},{"label":"jquery","permalink":"/tags/jquery"},{"label":"webservice.htc","permalink":"/tags/webservice-htc"},{"label":"json","permalink":"/tags/json"},{"label":"microsoft","permalink":"/tags/microsoft"}],"readingTime":7.235,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"A Potted History of using Ajax (on the Microsoft Stack of Love)","authors":"johnnyreilly","tags":["ajax","jquery","webservice.htc","json","microsoft"],"hide_table_of_contents":false},"prevItem":{"title":"WCF Transport Windows authentication using NetTcpBinding in an Intranet environment","permalink":"/2012/02/15/wcf-transport-windows-authentication"},"nextItem":{"title":"JavaScript - getting to know the beast...","permalink":"/2012/01/30/javascript-getting-to-know-beast"}},"content":"This post originally started out as an explanation of JSON. However as I wrote this quickly got abandoned in favour of writing about how I came to use JSON in the first place - which was through the use of Ajax. Having written a goodly amount I\'ve now decided to move the actual JSON stuff into another post since I think Ajax is probably worth thinking about by itself rather than as an aside. So let me start at the beginning and explain how I came to use Ajax in the first place (this may take some time so please bear with me). In late 2004 I first started working on a project which I was to remain involved with (on and off) for a very long time indeed. The project was part financial reporting system and part sales incentivisation tool; it was used internally in the investment bank in which I was working. The project had been in existence for a number of years and had a web front end which at that point would been built in a combination of HTML, JavaScript, classic ASP and with a Visual Basic 6.0 back end. One of the reasons I had been brought on to the project was to help \\".Net-ify\\" the thing and migrate it to ASP.NET and C#. I digress. The interesting thing about this app was that there were actually some quite advanced things being done with it (despite the classic ASP / VB). The users could enter trades into the system which represented actual trades that had been entered into a trading system elsewhere in the organisation. These trades would be assigned a reporting value which would be based on their various attributes. (Stay with me people this will get more interesting I \\\\***promise**\\\\*.) The calculation of the reporting value was quite an in depth process and needed to be performed server-side. However, the users had decreed that it wasn\'t acceptable to do a full postback to the server to perform this calculation; they wanted it done \\"on-the-fly\\". Now if you asked me at the time I\'d have said \\"can\'t be done\\". Fortunately the other people working on the project then weren\'t nearly so defeatist. Instead they went away and found Microsoft\'s [webservice.htc](http://msdn.microsoft.com/en-us/library/ie/ms531033%28v=vs.85%29.aspx) library. For those of you that don\'t know this was a JavaScript library that Microsoft came up with to enable the access of Web Services on the client. Given that it was designed to work with IE 5 I suspect it was created between 1999-2001 (but I\'m not certain about that). Now it came as a revelation to me but this was a JavaScript library that talked to our web services through the medium of XML. In short it was my first encounter with anything remotely [Ajax](<http://en.wikipedia.org/wiki/Ajax_(programming)>)\\\\-y. It was exciting! However, the possibilities of what we could do didn\'t actually become apparent to me for some years. It\'s worth saying that the way we were using webservice.htc was exceedingly simplistic and rather than investigating further I took the limited ways we were using it as indications of the limitations of Ajax and / or webservice.htc. So for a long time I thought the following: - The only way to pass multiple arguments to a web service was to package up arguments into a single string with delimiters which you could [split](<http://en.wikipedia.org/wiki/Comparison_of_programming_languages_(string_functions)#split>) and unpackage as your first step on the server.\\n\\n- The only valid return type was a single string. And so if you wanted to return a number of numeric values (as we did) the only way to do this was to package up return values into a very long string with delimiters in and (you guessed it!) [split](<http://en.wikipedia.org/wiki/Comparison_of_programming_languages_(string_functions)#split>) and unpackage as your first step on the client.\\n- The only thing that you could (or would want to) send back and forth between client and server was XML\\n\\nSo to recap, I\'m now aware that it\'s possible for JavaScript to interact with the server through the use of web services. It\'s possible, but ugly, not that quick and requires an awful lot of manual serialization / deserialization operations. It\'s clearly powerful but not much fun at all. And that\'s where I left it for a number of years. Let\'s fade to black... It\'s now 2007 and Microsoft have released ASP.NET Ajax, the details of which are well explained in this [article](http://msdn.microsoft.com/en-us/magazine/cc163499.aspx) (which I have only recently discovered). Now I\'m always interested in \\"the new\\" and so I was naturally interested in this. Just to be completely upfront about this I should confess that when I first discovered ASP.NET Ajax I didn\'t clock the power of it at all. Initially I just switched over from using webservice.htc to ASP.NET Ajax. This alone gave us a \\\\***massive**\\\\* performance improvement (I know it was massive since we actually received a \\"well done\\" email from our users which is testament to the difference it was making to their experience of the system). But we were still performing our manual serialisation / deserialisation of values on the client and the server. ie. Using Ajax was now much faster but still not too much fun. Let\'s jump forward in time again to around 2010 to the point in time when I was discovering jQuery and that JavaScript wasn\'t actually evil. It\'s not unusual for me to play around with \\"what if\\" scenarios in my code, just to see what might might be possible. Sometimes I discover things. So it was with JSON. We had a web service in the system that allowed us to look up a counterparty (ie a bank account) with an identifier. Once we looked it up we packaged up the counterparty details (eg name, location etc) into a big long string with delimiters and sent it back to client. One day I decided to change the return type on the web service from a string to the actual counterparty class. So we went from something like this:\\n\\n```cs\\n[WebService(Namespace = \\"http://tempuri.org/\\")]\\n[WebServiceBinding(ConformsTo = WsiProfiles.BasicProfile1_1)]\\n[ScriptService]\\npublic class CounterpartyWebService : System.Web.Services.WebService\\n{\\n  [WebMethod]\\n  public string GetCounterparty(string parameters)\\n  {\\n    string[] aParameters = parameters.Split(\\"|\\");\\n    int counterpartyId = int.Parse(aParameters[0]);\\n    bool includeLocation = (aParameters[1] == \\"1\\");\\n    Counterparty counterparty = \\\\_counterpartyDb\\n    .GetCounterparty(counterpartyId);\\n\\n        string returnValue = counterparty.Id +\\n                          \\"|\\" + counterparty.Name +\\n                          (includeLocation\\n                            ? \\"|\\" + counterparty.Location\\n                            : \\"\\");\\n\\n        return returnValue;\\n  }\\n}\\n```\\n\\nTo something like this:\\n\\n```cs\\n[WebMethod]\\npublic Counterparty GetCounterparty(string parameters)\\n{\\n  string[] aParameters = parameters.Split(\\"|\\");\\n  int counterpartyId = int.Parse(aParameters[0]);\\n  bool includeLocation = (aParameters[1] == \\"1\\");\\n  Counterparty counterparty = _counterpartyDb\\n    .GetCounterparty(counterpartyId);\\n\\n  return counterparty;\\n}\\n```\\n\\nI genuinely expected that this was just going to break. It didn\'t. Suddenly on the client I\'m sat there with a full blown object that looks just like the object I had on the server.\\n\\n**WHAT STRANGE MAGIC COULD THIS BE??????????** Certain that I\'d discovered witchcraft I decided to try something else. What would happen if I changed the signature on the method so it received individual parameters and passed my individual parameters to the web service instead of packaging them up into a string? I tried this:\\n\\n```cs\\n[WebMethod]\\npublic Counterparty GetCounterparty(int counterpartyId, bool includeLocation)\\n{\\n  Counterparty counterparty = \\\\_counterpartyDb\\n  .GetCounterparty(counterpartyId);\\n\\n  return counterparty;\\n}\\n```\\n\\nAnd it worked! **[IT WORKED!!!!!!!!!!!!!!!!!!!!!](http://www.youtube.com/watch?v=N_dWpCy8rdc&feature=related)** (And yes I know I wasn\'t actually using the includeLocation parameter - but the point was it was being passed to the server and I could have used it if I\'d wanted to.) I couldn\'t believe it. For **years** I\'d been using Ajax and without **any** idea of the power available to me. The ignorance! The stupidity of the man! To my complete surprise it turned out that: - Ajax could be quick! ASP.NET Ajax was lightening fast when compared to webservice.htc\\n\\n- You could send multiple arguments to a web service without all that packaging nonsense\\n- You could return complex objects without the need for packaging it all up yourself.\\n\\nEssentially the source of all this goodness was the magic of JSON. I wouldn\'t really come to comprehend this until I moved away from using the ASP.NET Ajax client libraries in favour of using the [jQuery.ajax](http://api.jquery.com/jQuery.ajax/) functionality. (Yes, having mostly rattled on about using webservice.htc and ASP.NET Ajax I should clarify that I have now forsaken both for jQuery as I find it more powerful and more configurable - but it\'s the journey that counts I guess!) It\'s abysmal that I didn\'t discover the power of Ajax sooner but the difference this discovery made to me was immense. Approaches that I would have dismissed or shied away from previously because of the amount of \\"plumbing\\" involved now became easy. This massively contributed to my [programmer joy](http://www.hanselman.com/blog/HanselminutesPodcast260NETAPIDesignThatOptimizesForProgrammerJoyWithJonathanCarter.aspx)! Next time I promise I\'ll aim to actually get onto JSON."},{"id":"/2012/01/30/javascript-getting-to-know-beast","metadata":{"permalink":"/2012/01/30/javascript-getting-to-know-beast","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2012-01-30-javascript-getting-to-know-beast/index.md","source":"@site/blog/2012-01-30-javascript-getting-to-know-beast/index.md","title":"JavaScript - getting to know the beast...","description":"So it\'s 2010 and I\'ve started using jQuery. jQuery is a JavaScript library. This means that I\'m writing JavaScript... Gulp! I should say that at this point in time I \\\\*hated\\\\* JavaScript (I have mentioned this previously). But what I know now is that I barely understood the language at all. All the JavaScript I knew was the result of copying and pasting after I\'d hit \\"view source\\". I don\'t feel too bad about this - not because my ignorance was laudable but because I certainly wasn\'t alone in this. It seems that up until recently hardly anyone knew anything about JavaScript. It puzzles me now that I thought this was okay. I suppose like many people I didn\'t think JavaScript was capable of much and hence felt time spent researching it would be wasted. Just to illustrate where I was then, here is 2009 John\'s idea of some pretty \\"advanced\\" JavaScript:","date":"2012-01-30T00:00:00.000Z","formattedDate":"January 30, 2012","tags":[{"label":"javascript","permalink":"/tags/javascript"},{"label":"c#","permalink":"/tags/c"},{"label":"elijah manor","permalink":"/tags/elijah-manor"},{"label":"douglas crockford","permalink":"/tags/douglas-crockford"}],"readingTime":5.805,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"JavaScript - getting to know the beast...","authors":"johnnyreilly","tags":["javascript","c#","elijah manor","douglas crockford"],"hide_table_of_contents":false},"prevItem":{"title":"A Potted History of using Ajax (on the Microsoft Stack of Love)","permalink":"/2012/02/05/potted-history-of-using-ajax-on"},"nextItem":{"title":"What on earth is jQuery?  And why should I care?","permalink":"/2012/01/24/what-on-earth-is-jquery-and-why-should"}},"content":"So it\'s 2010 and I\'ve started using jQuery. jQuery is a JavaScript library. This means that I\'m writing JavaScript... Gulp! I should say that at this point in time I \\\\***hated**\\\\* JavaScript (I have mentioned this previously). But what I know now is that I barely understood the language at all. All the JavaScript I knew was the result of copying and pasting after I\'d hit \\"view source\\". I don\'t feel too bad about this - not because my ignorance was laudable but because I certainly wasn\'t alone in this. It seems that up until recently hardly anyone knew anything about JavaScript. It puzzles me now that I thought this was okay. I suppose like many people I didn\'t think JavaScript was capable of much and hence felt time spent researching it would be wasted. Just to illustrate where I was then, here is 2009 John\'s idea of some pretty \\"advanced\\" JavaScript:\\n\\n```html\\nfunction GiveMeASum(iNum1, iNum2) { var dteDate = new Date(); var iTotal = iNum1\\n+ iNum2; return \\"This is your total: \\" + iTotal + \\", at this time: \\" +\\ndteDate.toString(); }\\n\\n<input type=\\"text\\" id=\\"Number1\\" value=\\"4\\" />\\n<input type=\\"text\\" id=\\"Number2\\" value=\\"6\\" />\\n<input\\n  type=\\"button\\"\\n  value=\\"Click Me To Add\\"\\n  onclick=\\"alert(GiveMeASum(parseInt(document.getElementById(Number1).value, 10), parseInt(document.getElementById(Number2).value, 10)))\\"\\n/>\\n```\\n\\nI know - I\'m not to proud of it... Certainly if it was a horse you\'d shoot it. Basically, at that point I knew the following: - JavaScript had functions (but I knew only one way to use them - see above)\\n\\n- It had some concept of numbers (but I had no idea of the type of numbers I was dealing with; integer / float / decimal / who knows?)\\n- It had some concept of strings\\n- It had a date object\\n\\nThis was about the limit of my knowledge. If I was right, and that\'s all there was to JavaScript then my evaluation of it as utter rubbish would have been accurate. I was wrong. SOOOOOOOOOOOO WRONG! I first realised how wrong I was when I opened up the jQuery source to have a read. Put simply I had \\\\***no**\\\\* idea what I was looking at. For a while I wondered if I was actually looking at JavaScript; the code was so different to what I was expecting that for a goodly period I considered jQuery to be some kind of strange black magic; written in a language I did not understand. I was half right. jQuery wasn\'t black magic. But it was written in a language I didn\'t understand; namely JavaScript. :-( Here beginneth the lessons... I started casting around looking for information about JavaScript. Before very long I discovered one [Elijah Manor](http://www.elijahmanor.com/) who had helpfully done a number of talks and blog posts directed at C# developers (which I was) about JavaScript. My man! - [How good C# habits can encourage bad JavaScript habits part 1](http://enterprisejquery.com/2010/10/how-good-c-habits-can-encourage-bad-javascript-habits-part-1/)\\n\\n- [How good C# habits can encourage bad JavaScript habits part 2](http://enterprisejquery.com/2010/10/how-good-c-habits-can-encourage-bad-javascript-habits-part-2/)\\n- [How good C# habits can encourage bad JavaScript habits part 3](http://enterprisejquery.com/2010/10/how-good-c-habits-can-encourage-bad-javascript-habits-part-3/)\\n- [Video of Elijah Manor talking through the above material](https://blogs.msdn.com/b/ukmsdn/archive/2011/06/10/javascript-for-the-c-developer.aspx)\\n\\nFor me this was all massively helpful. In my development life so far I had only ever dealt with strongly typed, compiled \\"classical\\" languages. I had little to no experience of functional, dynamic and loosely typed languages (essentially what JavaScript is). Elijahs work opened up my eyes to some of the massive differences that exist. He also pointed me in the direction of the (never boring) Doug Crockford, author of the best programming book I have ever purchased: [JavaScript: The Good Parts](http://www.amazon.co.uk/JavaScript-Good-Parts-Douglas-Crockford/dp/0596517742). Who could not like a book about JavaScript which starts each chapter with a quote from Shakespeare and still comes in at only a 100 pages? It\'s also worth watching the man in person as he\'s a thoroughly engaging presence. There\'s loads of videos of him out there but this one is pretty good: [Douglas Crockford: The JavaScript Programming Language](http://www.youtube.com/watch?v=v2ifWcnQs6M). I don\'t want to waste your time by attempting to rehash what these guys have done already. I think it\'s always best to go to the source so I\'d advise you to check them out for yourselves. That said it\'s probably worth summarising some of the main points I took away from them (you can find better explanations of all of these through looking at their posts): 1. JavaScript has objects but has no classes. Instead it has (what I still consider to be) the weirdest type of inheritance going: prototypical inheritance. 2. JavaScript has the simplest and loveliest way of creating a new object out there; the \\"JavaScript Object Literal\\". Using this we can simply `var myCar = { wheels: 4, colour: \\"blue\\" }` and ladies and gents we have ourselves a car! (object) 3. In JavaScript functions are [first class objects](http://en.wikipedia.org/wiki/First-class_function). This means functions can be assigned to variables (as easily as you\'d assign a string to a variable) and crucially you can pass them as parameters to a function and pass them back as a return type. Herein lies power! 4. JavaScript has 6 possible values (false, null, undefined, empty strings, 0 and NaN) which it evaluates as false. These are known as the \\"false-y\\" values. It\'s a bit weird but on the plus side this can lead to some nicely terse code. 5. To perform comparisons in JavaScript you should avoid == and != and instead use === and !==. Before I discovered this I had been using == and != and then regularly puzzling over some truly odd behaviour. Small though it may sound, this may be the most important discovery of the lot as it was this that lead to me actually \\\\***trusting**\\\\* the language. Prior to this I vaguely thought I was picking up on some kind of bug in the JavaScript language which I plain didn\'t understand. (After all, in any sane universe should this really evaluate to true?: `0 == \\"\\"`) 6. Finally JavaScript has function scope rather than block scope. Interestingly it \\"hoists\\" variable declaration to the top of each function which can lead to some very surprising behaviour if you don\'t realise what is happening.\\n\\nI now realise that JavaScript is a fantastic language because of it\'s flexibility. It is also a deeply flawed language; in part due to it\'s unreasonably forgiving nature (you haven\'t finished your line with a semi-colon; that\'s okay - I can see you meant to so I\'ll stick one in / you haven\'t declared your variable; not a problem I won\'t tell you but I\'ll create a new variable stick it in global scope and off we go etc). It is without question the easiest language with which to create a proper dogs breakfast. To get the best out of JavaScript we need to understand the quirks of the language and we need good patterns. If you\'re interested in getting to grips with it I really advise you to check out the Elijah and Dougs work - it really helped me."},{"id":"/2012/01/24/what-on-earth-is-jquery-and-why-should","metadata":{"permalink":"/2012/01/24/what-on-earth-is-jquery-and-why-should","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2012-01-24-what-on-earth-is-jquery-and-why-should/index.md","source":"@site/blog/2012-01-24-what-on-earth-is-jquery-and-why-should/index.md","title":"What on earth is jQuery?  And why should I care?","description":"What on earth is jQuery? What\'s a jQuery plugin?","date":"2012-01-24T00:00:00.000Z","formattedDate":"January 24, 2012","tags":[{"label":"jqgrid","permalink":"/tags/jqgrid"},{"label":"ajax","permalink":"/tags/ajax"},{"label":"jquery","permalink":"/tags/jquery"},{"label":"scott gu","permalink":"/tags/scott-gu"},{"label":"microsoft","permalink":"/tags/microsoft"}],"readingTime":4.525,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"What on earth is jQuery?  And why should I care?","authors":"johnnyreilly","tags":["jqgrid","ajax","jquery","scott gu","microsoft"],"hide_table_of_contents":false},"prevItem":{"title":"JavaScript - getting to know the beast...","permalink":"/2012/01/30/javascript-getting-to-know-beast"},"nextItem":{"title":"jqGrid - it\'s just a far better grid","permalink":"/2012/01/14/jqgrid-its-just-far-better-grid"}},"content":"What on earth is jQuery? What\'s a jQuery plugin?\\n\\nThese were the questions I was asking myself shortly after discovering that jqGrid was a \\"jQuery plugin\\". I\'d been vaguely aware of the phrase \\"jQuery\\" being increasingly mentioned on various techical websites since about 2009. But for some reason I\'d felt no urge to find out what it was. I seem to remember that I read the name \\"jQuery\\" and jumped to the perfectly logical (in my head) conclusion that this must be a Java SQL engine of some sort. (After all \\"j\\" as a prefix to anything so far had generally been Java and \\"Query\\" just rang of databases to me.) Clearly I was wrong - life\'s full of surprises.\\n\\nI soon discovered that, contrary to expectations, jQuery had nothing to do with Java \\\\***and**\\\\* nothing to do with databases either. It was in fact a JavaScript library written by the amazing [John Resig](http://ejohn.org/about/). At the time I had no love for JavaScript. I now realise I knew nearly nothing about it but my feeling was that JavaScript was awful - evil even. However, given JavaScripts ubiquity in the world of web it seemed to be a necessary evil.\\n\\nI took a look at the [jQuery website](http://jquery.com/) and after reading round a bit I noticed that it could be used for [Ajax](http://en.wikipedia.org/wiki/Ajax_%28programming%29) operations. This lead to me reaching the (incorrect) conclusion that jQuery was basically an alternative to the [Microsoft Ajax library](http://en.wikipedia.org/wiki/ASP.NET_AJAX#Microsoft_Ajax_Library) which we were already using to call various Web Services. But I remained frankly suspicious of jQuery. What was the point of this library? Why did it exist?\\n\\nI read the the [blog](http://weblogs.asp.net/scottgu/archive/2008/09/28/jquery-and-microsoft.aspx) by Scott Gu announcing Microsoft was going to start shipping jQuery with Visual Studio. The Great Gu trusted it. Therefore, I figured, it must be okay... Right?\\n\\nThe thing was, I was quite happy with the Microsoft Ajax library. I was familiar with it. It worked. Why switch? I saw the various operations Scott Gu was doing to divs on the screen using jQuery. I didn\'t want to do anything like that at all. As I said; I had no love for JavaScript - I viewed it as C#\'s simple-minded idiot cousin. My unofficial motto when doing web stuff was \\"wherever possible, do it on the server\\".\\n\\nI think I would have ignored jQuery entirely but for the fact of jqGrid. If I wanted to use jqGrid I had to use jQuery as well. In the end I decided I\'d allow it house room just for the sake of jqGrid and I\'d just ignore it apart from that. And that\'s how it was for a while.\\n\\nThen I had an epiphany. Okay - that\'s overplaying it. What actually happened was I realised that something we were doing elsewhere could be done faster and easier with jQuery. It\'s something so ridiculously feeble that I feel vaguely embarrassed sharing it. Anyway.\\n\\nSo, you know the css hover behaviour is only implemented for anchor tags in IE6? No? Well read this [Stack Overflow](http://stackoverflow.com/questions/36605/ie-6-css-hover-non-anchor-tag) entry - it\'ll clarify. Well, the app that I was working on was an internal web application only used by people with the corporate installation of IE 6 on their desktops. And it was \\"terribly important\\" that buttons had hover behaviour. For reasons that now escape me we were doing this by manually adding inline onmouseover / onmouseout event handlers to each input button on the screen in turn in every page in the [Page_Load](http://msdn.microsoft.com/en-us/library/ms178472.aspx) event server side. I think we were aware it wasn\'t fantastic to have to wire up each button in turn. But it worked and as with so many development situations we had other pressures, other requirements to fulfil and other fish to fry - so we left it at that.\\n\\nAnd then it occurred to me... What about using the [jQuery class selector](http://api.jquery.com/class-selector/) in conjunction with the [jQuery hover event](http://api.jquery.com/hover/)? I could have one method that I called on a page which would wire up all of my hover behaviours in one fell swoop. I wouldn\'t need to do input-by-input wireups anymore! Hallelujah! This is what I did:\\n\\nThe buttons I would like to style:\\n\\n```html\\n<input type=\\"button\\" value=\\"I am a button\\" class=\\"itIsAButton\\" />\\n<input type=\\"button\\" value=\\"So am I\\" class=\\"itIsAButton\\" />\\n<input type=\\"button\\" value=\\"Me too\\" class=\\"itIsAButton\\" />\\n```\\n\\nMy CSS (filter, by the way, is just linear gradients in IE 6-9):\\n\\n```css\\n.itIsAButton {\\n  filter: progid:DXImageTransform.Microsoft.Gradient (GradientType=0,StartColorStr=\'#ededed\',EndColorStr=\'#cdcdcd\');\\n}\\n\\n.itIsAButton:hover, .itIsAButton_hover /* \\"_hover\\" is for IE6 */ {\\n  filter: progid:DXImageTransform.Microsoft.Gradient (GradientType=0,StartColorStr=\'#f6f6f6\',EndColorStr=\'#efefef\');\\n}\\n```\\n\\nMy jQuery:\\n\\n```js\\n$(document).ready(function () {\\n  //Add hover behaviour on picker buttons for IE6\\n  if ($.browser.msie && parseInt($.browser.version, 10) < 7) {\\n    var fnButtonHover = function (handlerInOut) {\\n      var $btn = $(this);\\n      var sOriginalClass = $btn.prop(\'class\');\\n\\n      if (handlerInOut.type === \'mouseenter\') {\\n        //If not already hovering class then apply it\\n        if (sOriginalClass.indexOf(\'_hover\') === -1) {\\n          $btn.prop(\'class\', sOriginalClass + \'_hover\');\\n        }\\n      } else if (handlerInOut.type === \'mouseleave\') {\\n        //If not already non-hovering class then apply it\\n        if (sOriginalClass.indexOf(\'_hover\') !== -1) {\\n          $btn.prop(\'class\', sOriginalClass.split(\'_\')[0]);\\n        }\\n      }\\n    };\\n\\n    $(\'.itIsAButton\').hover(fnButtonHover);\\n  }\\n});\\n```\\n\\nAnd it worked. I didn\'t really understand this much about this jQuery \\"thing\\" at that point but I could now see that it clearly had at least one use. I\'ve come to appreciate that jQuery is one of the best pieces of software I\'ve ever encountered. Over time I may go further into some of the good stuff of jQuery. It is, quite simply, brilliant."},{"id":"/2012/01/14/jqgrid-its-just-far-better-grid","metadata":{"permalink":"/2012/01/14/jqgrid-its-just-far-better-grid","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2012-01-14-jqgrid-its-just-far-better-grid/index.md","source":"@site/blog/2012-01-14-jqgrid-its-just-far-better-grid/index.md","title":"jqGrid - it\'s just a far better grid","description":"The year was 2010 (not really that long ago I know) and the project that I was working on was sorely in need of a new grid component. It was an ASP.NET WebForms project and for some time we\'d been using what was essentially a glorified datagrid which had a few extra features implemented to allow us to change column order / columns displayed / copy contents to clipboard etc. Our grid worked perfectly fine - it gave us the functionality we needed. However, it looked pretty terrible, and had some \\"quirky\\" approaches in place for supporting IE and Firefox side by side. Also, at the time we were attempting to make our app seem new and exciting again for the users. The surprising truth is that users seem to be more impressed with a visual revamp than with new or amended functionality. So I was looking for something which would make them sit up and say \\"oooh - isn\'t it pretty!\\". Unfortunately the nature of the organisation I was working for was not one that lended itself to paying for components. They were occasionally willing to do that but the hoops that would have to be jumped through first, the forms that would need to be signed in triplicate by people that had nearly nothing to do with the project made that an unattractive prospect. So I began my search initially looking at the various open source offerings that were around. As a minimum I was looking for something that would do what our home-grown component did already (change column order / columns displayed / copy contents to clipboard etc) but hopefully in a \\"nicer\\" way. Also, I had long been unhappy with the fact that to get our current grid to render results we did a \\\\*full postback\\\\* to the server and re-rendered the whole page. Pointless! Why should you need to do all this each time when you only wanted to refresh the data? Instead I was thinking about using an Ajax approach; a grid that could just get the data that it needed and render it to the client. This seemed to me a vastly \\"cleaner\\" solution - why update a whole screen when you only want to update a small part of it? Why not save yourself the trouble of having to ensure that all other screen controls are persisted just as you\'d like them after the postback? I also thought it was probably something that would scale better as it would massively reduce the amount of data moving backwards and forwards between client and server. No need for a full page life cycle on the server each time the grid refreshes. Just simple data travelling down the pipes of web. With the above criteria in mind I set out on my Google quest for a grid. Quite soon I found that there was a component out there which seemed to do all that I wanted and far more besides. It was called jqGrid:","date":"2012-01-14T00:00:00.000Z","formattedDate":"January 14, 2012","tags":[{"label":"jqgrid","permalink":"/tags/jqgrid"},{"label":"ajax","permalink":"/tags/ajax"},{"label":"jquery","permalink":"/tags/jquery"},{"label":"datagrid","permalink":"/tags/datagrid"},{"label":"Dave Ward","permalink":"/tags/dave-ward"},{"label":"json","permalink":"/tags/json"},{"label":"no postback","permalink":"/tags/no-postback"},{"label":"Encosia","permalink":"/tags/encosia"},{"label":"tony tomov","permalink":"/tags/tony-tomov"}],"readingTime":5.405,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"jqGrid - it\'s just a far better grid","authors":"johnnyreilly","tags":["jqgrid","ajax","jquery","datagrid","Dave Ward","json","no postback","Encosia","tony tomov"],"hide_table_of_contents":false},"prevItem":{"title":"What on earth is jQuery?  And why should I care?","permalink":"/2012/01/24/what-on-earth-is-jquery-and-why-should"},"nextItem":{"title":"Standing on the Shoulders of Giants...","permalink":"/2012/01/07/standing-on-shoulders-of-giants"}},"content":"The year was 2010 (not really that long ago I know) and the project that I was working on was sorely in need of a new grid component. It was an [ASP.NET WebForms](http://www.asp.net/web-forms) project and for some time we\'d been using what was essentially a glorified [datagrid](http://msdn.microsoft.com/en-us/library/system.web.ui.webcontrols.datagrid.aspx) which had a few extra features implemented to allow us to change column order / columns displayed / copy contents to clipboard etc. Our grid worked perfectly fine - it gave us the functionality we needed. However, it looked pretty terrible, and had some \\"quirky\\" approaches in place for supporting IE and Firefox side by side. Also, at the time we were attempting to make our app seem new and exciting again for the users. The surprising truth is that users seem to be more impressed with a visual revamp than with new or amended functionality. So I was looking for something which would make them sit up and say \\"oooh - isn\'t it pretty!\\". Unfortunately the nature of the organisation I was working for was not one that lended itself to paying for components. They were occasionally willing to do that but the hoops that would have to be jumped through first, the forms that would need to be signed in triplicate by people that had nearly nothing to do with the project made that an unattractive prospect. So I began my search initially looking at the various open source offerings that were around. As a minimum I was looking for something that would do what our home-grown component did already (change column order / columns displayed / copy contents to clipboard etc) but hopefully in a \\"nicer\\" way. Also, I had long been unhappy with the fact that to get our current grid to render results we did a \\\\***full postback**\\\\* to the server and re-rendered the whole page. Pointless! Why should you need to do all this each time when you only wanted to refresh the data? Instead I was thinking about using an [Ajax](http://en.wikipedia.org/wiki/Ajax_%28programming%29) approach; a grid that could just get the data that it needed and render it to the client. This seemed to me a vastly \\"cleaner\\" solution - why update a whole screen when you only want to update a small part of it? Why not save yourself the trouble of having to ensure that all other screen controls are persisted just as you\'d like them after the postback? I also thought it was probably something that would scale better as it would massively reduce the amount of data moving backwards and forwards between client and server. No need for a full page life cycle on the server each time the grid refreshes. Just simple data travelling down the pipes of web. With the above criteria in mind I set out on my Google quest for a grid. Quite soon I found that there was a component out there which seemed to do all that I wanted and far more besides. It was called [jqGrid](http://www.trirand.com/blog/):\\n\\n![](jqgrid-in-all-its-glory.png)\\n\\nOooh look at the goodness! It had both column re-ordering and column choosing built in!: This was a \\\\***very promising sign**\\\\*! Now it\'s time for me to demonstrate my ignorance. According to the website this grid component was a \\"jQuery plugin\\". At the time I read this I had no idea what jQuery was at all - let alone what a plugin for it was. Anyway, I don\'t want to get diverted so let\'s just say that reading this lead to me getting an urgent education about some of the client side aspects of the modern web that I had been previously unaware of. I digress. This component did exactly what I wanted in terms of just sending data down the pipe. jqGrid worked with a whole number of possible data sources; XML, Array but the most exciting for me was obviously [JSON](http://www.json.org/). Take a look a the grid rendered below and the JSON that powered it (all from a simple [GET](http://www.trirand.com/blog/jqgrid/server.php?q=2&_search=false&nd=1326531357333&rows=10&page=1&sidx=id&sord=desc) request):\\n\\n![](Check-out-the-JSON.png)\\n\\nAs you can see from the above screenshot, the grid has populated itself using the results of a web request. The only information that has gone to the server are the relevant criteria to drive the search results. The only information that has come back from the server is the data needed to drive the grid. Simple. Beautiful. I loved it and I wanted to use it. So I did! I had to take a few steps that most people thinking about using a grid component probably wont need to. First of all I had to write an ASP.Net WebForms wrapper for jqGrid which could be implemented in a similar way to our current custom datagrid. This was because, until the users were convinced that the new grid was better than the old both had to co-exist in the project and the user would have the option to switch between the two. This WebForms wrapper plugged into our old school XML column definition files and translated them into JSON for the grid. It also took [datasets](http://msdn.microsoft.com/en-us/library/system.data.dataset.aspx) (which drove our old grid) and translated them into jqGrid-friendly JSON. I wanted to power the jqGrid using WebMethods on ASPX\'s. After a little digging I found [Dave Ward of Encosia\'s post](http://encosia.com/using-jquery-to-directly-call-aspnet-ajax-page-methods/) which made it very simple (and in line with this I switched over from [GET](http://en.wikipedia.org/wiki/GET_%28HTTP%29#Request_methods) requests to [POSTs](http://en.wikipedia.org/wiki/POST_%28HTTP%29)). Finally I wrote some custom javascript which added a button to jqGrid which, if clicked, would copy the contents of the jqGrid to the clipboard (this was the only bit of functionality that didn\'t appear to be implemented out of the box with jqGrid). I think I\'m going to leave it there for now but I just wanted to say that I think jqGrid is a fantastic component and it\'s certainly made my life better! It\'s: - well supported, there is lots on [StackOverflow](http://stackoverflow.com/questions/tagged/jqgrid) and the like about it\\n\\n- there are regular [releases / upgrades](http://www.trirand.com/blog/)\\n- there are good online [demonstrations](http://trirand.com/blog/jqgrid/jqgrid.html) and [documentation](http://www.trirand.com/jqgridwiki/doku.php)\\n\\nI think Tony Tomov (the man behind jqGrid) has come up with something truly brilliant. It\'s worth saying that the equally brilliant jQueryUI team are in the process of writing an official [jQuery UI grid component](http://wiki.jqueryui.com/w/page/34246941/Grid) which uses jqGrid as one of its inspirations. However, this is still a long way from even a \\"zero feature\\" release. In the meantime jqGrid is continuing to go from strength to strength and as such I heartily recommend it. Finally, you can take a look at jqGrid\'s source on [GitHub](https://github.com/tonytomov/jqGrid)."},{"id":"/2012/01/07/standing-on-shoulders-of-giants","metadata":{"permalink":"/2012/01/07/standing-on-shoulders-of-giants","editUrl":"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/blog/2012-01-07-standing-on-shoulders-of-giants/index.md","source":"@site/blog/2012-01-07-standing-on-shoulders-of-giants/index.md","title":"Standing on the Shoulders of Giants...","description":"It started with Scott Hanselman. I had no particular plans to start a blog at all. However, I was reading Scott Hanselman\'s turn of the year post and I was struck with an idea.","date":"2012-01-07T00:00:00.000Z","formattedDate":"January 7, 2012","tags":[{"label":"delphi","permalink":"/tags/delphi"},{"label":"asp.net","permalink":"/tags/asp-net"},{"label":"Hanselman","permalink":"/tags/hanselman"},{"label":"developer","permalink":"/tags/developer"},{"label":"amstrad","permalink":"/tags/amstrad"},{"label":"microsoft","permalink":"/tags/microsoft"}],"readingTime":3.225,"truncated":false,"authors":[{"name":"John Reilly","url":"https://twitter.com/johnny_reilly","imageURL":"https://blog.johnnyreilly.com/img/profile.jpg","key":"johnnyreilly"}],"frontMatter":{"title":"Standing on the Shoulders of Giants...","authors":"johnnyreilly","tags":["delphi","asp.net","Hanselman","developer","amstrad","microsoft"],"hide_table_of_contents":false},"prevItem":{"title":"jqGrid - it\'s just a far better grid","permalink":"/2012/01/14/jqgrid-its-just-far-better-grid"}},"content":"It started with Scott Hanselman. I had no particular plans to start a blog at all. However, I was reading Scott Hanselman\'s turn of the year [post](http://www.hanselman.com/blog/YourBlogIsTheEngineOfCommunity.aspx) and I was struck with an idea.\\n\\nFirst, let me give a little background about myself. I\'m a software developer. I\'ve been in the industry for coming up to 15 years. I started out professionally writing call centre software. I moved on to code in a variety of different industries from straight IT to marketing and, for the last 7 years, finance.\\n\\nThough I initially started out writing in Delphi I fast found myself moving toward the Microsoft \\"stack of love\\". I should say that this move was not because I instinctively liked Microsofts stuff (in fact in the beginning I actively disliked it - moving from Delphi 3.0 to Visual Studio 5 left me finding Microsoft\'s offering very much wanting). Rather it was pragmatic. I needed a job and at the time VB was a far more transferable skill than Delphi. What with the all encompassing [dot-com bubble](http://en.wikipedia.org/wiki/Dot-com_bubble) of the late 90\'s I soon found myself working in the webtastic world of classic ASP (weep) and VB server components (remember them?).\\n\\nThough things can improve - and in my opinion they really did when Microsoft coughed up the first furball of ASP.NET Beta in (I think) 2001. I grabbed on with both hands. Since that point I\'ve been earning my bread pretty much, though not exclusively, in the ASP.NET universe.\\n\\nThe one thing that might not be clear from the above curriculum vitae is this: **I AM A COMPLETE AMATEUR.** I mean this in both senses of the word:\\n\\n1. I have no formal training to speak of - I didn\'t do a computer sciences degree. In fact my first real coding experience was writing a program in [Locomotive Basic](http://en.wikipedia.org/wiki/Locomotive_BASIC) for my father on our humble Amstrad CPC.\\n2. That said, I love it. I find writing code an intellectually, emotionally, creatively satisfying act. And whilst I undoubtedly have less of the theoretical knowledge which most professional developers seem to have, I probably counter-balance that with a hunger to keep learning and keep trying new things. And since software never sits still that\'s probably just as well. Keep watching the horizon - there will be something coming over it! And it\'s worth saying, I have an instinct for developing which serves me pretty well. I\'m good at coming up with elegant and pragmatic solutions. Put simply: I\'m good at making code work.\\n\\nSo back to the point. In my daily work life, like any other developer, I am repeatedly called on to turn someones requirement into a reality. Very rarely do I achieve this on my own. Like most of us I\'m a dwarf standing on the shoulders of giants. There\'s a lot of people out there who come up with useful tools / components / plug-ins that make it possible for me to deliver much more than I would given my own abilities.\\n\\nSo that\'s what I want to do: I want to talk about the tools, components and techniques that I have found useful in the everyday working life of a developer. It\'s likely to be quite a \\"webby\\" blog as I probably find that the most interesting area of development at the moment.\\n\\nI don\'t know how often I will write but my plan is that when I do, each time I\'ll talk about something I\'ve found useful - why I found it useful, what problems it solved, what issues it still presented me with and so on. This is probably not going to be a \\"techie techie\\" blog. Rather a blog that deals with the situations that can confront a developer and how I\'ve responded to them. I hope you find it interesting. And if you don\'t; please keep it to yourself :-)"}]}')}}]);