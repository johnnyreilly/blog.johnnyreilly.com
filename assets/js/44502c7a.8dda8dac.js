"use strict";(self.webpackChunkjohnnyreilly_com=self.webpackChunkjohnnyreilly_com||[]).push([[16796],{3905:(e,t,n)=>{n.d(t,{Zo:()=>l,kt:()=>y});var o=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,o,a=function(e,t){if(null==e)return{};var n,o,a={},i=Object.keys(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var c=o.createContext({}),p=function(e){var t=o.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},l=function(e){var t=p(e.components);return o.createElement(c.Provider,{value:t},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},d=o.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,c=e.parentName,l=s(e,["components","mdxType","originalType","parentName"]),u=p(n),d=a,y=u["".concat(c,".").concat(d)]||u[d]||m[d]||i;return n?o.createElement(y,r(r({ref:t},l),{},{components:n})):o.createElement(y,r({ref:t},l))}));function y(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,r=new Array(i);r[0]=d;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s[u]="string"==typeof e?e:a,r[1]=s;for(var p=2;p<i;p++)r[p]=n[p];return o.createElement.apply(null,r)}return o.createElement.apply(null,n)}d.displayName="MDXCreateElement"},79561:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>c,default:()=>d,frontMatter:()=>s,metadata:()=>p,toc:()=>u});n(67294);var o=n(3905);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){return t=null!=t?t:{},Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):function(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))})),e}function r(e,t){if(null==e)return{};var n,o,a=function(e,t){if(null==e)return{};var n,o,a={},i=Object.keys(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}const s={slug:"azure-open-ai-capacity-quota-bicep",title:"Azure Open AI: handling capacity and quota limits with Bicep",authors:"johnnyreilly",image:"./title-image.png",tags:["azure","bicep"],description:"This post details how to control the capacity of an Azure Open AI deployment with Bicep so that you don't exceed your quota.",hide_table_of_contents:!1},c=void 0,p={permalink:"/azure-open-ai-capacity-quota-bicep",editUrl:"https://github.com/johnnyreilly/blog.johnnyreilly.com/edit/main/blog-website/blog/2023-08-17-azure-open-ai-capacity-quota-bicep/index.md",source:"@site/blog/2023-08-17-azure-open-ai-capacity-quota-bicep/index.md",title:"Azure Open AI: handling capacity and quota limits with Bicep",description:"This post details how to control the capacity of an Azure Open AI deployment with Bicep so that you don't exceed your quota.",date:"2023-08-17T00:00:00.000Z",formattedDate:"August 17, 2023",tags:[{label:"azure",permalink:"/tags/azure"},{label:"bicep",permalink:"/tags/bicep"}],readingTime:3.445,hasTruncateMarker:!0,authors:[{name:"John Reilly",title:"OSS Engineer - TypeScript, Azure, React, Node.js, .NET",url:"https://johnnyreilly.com/about",imageURL:"https://johnnyreilly.com/img/profile.jpg",key:"johnnyreilly"}],frontMatter:{slug:"azure-open-ai-capacity-quota-bicep",title:"Azure Open AI: handling capacity and quota limits with Bicep",authors:"johnnyreilly",image:"./title-image.png",tags:["azure","bicep"],description:"This post details how to control the capacity of an Azure Open AI deployment with Bicep so that you don't exceed your quota.",hide_table_of_contents:!1},nextItem:{title:"Azure Pipelines meet Vitest",permalink:"/azure-pipelines-meet-vitest"}},l={image:n(21843).Z,authorsImageUrls:[void 0]},u=[{value:"Viewing capacity and quota limits in the Azure Open AI Studio",id:"viewing-capacity-and-quota-limits-in-the-azure-open-ai-studio",level:2},{value:"Controlling capacity and quota limits with Bicep",id:"controlling-capacity-and-quota-limits-with-bicep",level:2}],m={toc:u};function d(e){var{components:t}=e,s=r(e,["components"]);return(0,o.kt)("wrapper",i(function(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{},o=Object.keys(n);"function"==typeof Object.getOwnPropertySymbols&&(o=o.concat(Object.getOwnPropertySymbols(n).filter((function(e){return Object.getOwnPropertyDescriptor(n,e).enumerable})))),o.forEach((function(t){a(e,t,n[t])}))}return e}({},m,s),{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"We're currently in the gold rush period of AI. The world cannot get enough. A consequence of this, is that rationing is in force. It's like the end of the second world war, but with GPUs. This is a good thing, because it means that we can't just spin up as many resources as we like. It's a bad thing, for the exact same reason."),(0,o.kt)("p",null,"If you're making use of Azure's Open AI resources for your AI needs, you'll be aware that there are ",(0,o.kt)("a",{parentName:"p",href:"https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/quota?tabs=bicep"},'limits known as "quotas"')," in place. If you're looking to control how many resources you're using, you'll want to be able to control the capacity of your deployments. This is possible with Bicep."),(0,o.kt)("p",null,"This post grew out of a ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/Azure/bicep-types-az/issues/1660#issuecomment-1643484703"},"GitHub issue")," around the topic where people were bumping on the message ",(0,o.kt)("inlineCode",{parentName:"p"},"the capacity should be null for standard deployment")," as they attempted to deploy. At the time that issue was raised, there was very little documentation on how to handle this. Since then, things have improved, but I thought it would be useful to have a post on the topic."),(0,o.kt)("p",null,(0,o.kt)("img",{loading:"eager",fetchpriority:"high",alt:"title image reading &quot;Azure Open AI: handling capacity and quota limits with Bicep&quot; with the Azure Open AI / Bicep logos",src:n(21843).Z,width:"800",height:"450"})),(0,o.kt)("h2",{id:"viewing-capacity-and-quota-limits-in-the-azure-open-ai-studio"},"Viewing capacity and quota limits in the Azure Open AI Studio"),(0,o.kt)("p",null,"If you take a look at the ",(0,o.kt)("a",{parentName:"p",href:"https://oai.azure.com/"},"Azure Open AI Studio"),' you\'ll notice a "Quotas" section:'),(0,o.kt)("p",null,(0,o.kt)("img",{loading:"lazy",alt:"screenshot of azure open ai studio with quotas highlighted",src:n(31668).Z,width:"3076",height:"1290"})),(0,o.kt)("p",null,"You'll see above that we've got two deployments of GPT-35-Turbo in our subscription. Both of these contribute towards an overall limit of 360K TPM. If we try and deploy resources and have an overall capacity total that exceeds that, our deployment will fail."),(0,o.kt)("p",null,"That being the case, we need to be able to control the capacity of our deployments. This is possible with Bicep."),(0,o.kt)("h2",{id:"controlling-capacity-and-quota-limits-with-bicep"},"Controlling capacity and quota limits with Bicep"),(0,o.kt)("p",null,"Consider the following ",(0,o.kt)("inlineCode",{parentName:"p"},"account-deployments.bicep"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bicep"},"@description('Name of the Cognitive Services resource')\nparam cognitiveServicesName string\n\n@description('Name of the deployment resource.')\nparam deploymentName string\n\n@description('Deployment model format.')\nparam format string\n\n@description('Deployment model name.')\nparam name string\n\n@description('Deployment model version.')\nparam version string = '1'\n\n@description('The name of RAI policy.')\nparam raiPolicyName string = 'Default'\n\n@allowed([\n  'NoAutoUpgrade'\n  'OnceCurrentVersionExpired'\n  'OnceNewDefaultVersionAvailable'\n])\n@description('Deployment model version upgrade option. see https://learn.microsoft.com/en-us/azure/templates/microsoft.cognitiveservices/2023-05-01/accounts/deployments?pivots=deployment-language-bicep#deploymentproperties')\nparam versionUpgradeOption string = 'OnceNewDefaultVersionAvailable'\n\n@description('''Deployments SKU see: https://learn.microsoft.com/en-us/azure/templates/microsoft.cognitiveservices/2023-05-01/accounts/deployments?pivots=deployment-language-bicep#sku\neg:\n\nsku: {\n  name: 'Standard'\n  capacity: 10\n}\n\n''')\nparam sku object\n\n// https://learn.microsoft.com/en-us/azure/templates/microsoft.cognitiveservices/2023-05-01/accounts?pivots=deployment-language-bicep\nresource cog 'Microsoft.CognitiveServices/accounts@2023-05-01' existing = {\n  name: cognitiveServicesName\n}\n\n// https://learn.microsoft.com/en-us/azure/templates/microsoft.cognitiveservices/2023-05-01/accounts/deployments?pivots=deployment-language-bicep\nresource deployment 'Microsoft.CognitiveServices/accounts/deployments@2023-05-01' = {\n  name: deploymentName\n  parent: cog\n  sku: sku\n  properties: {\n    model: {\n      format: format\n      name: name\n      version: version\n    }\n    raiPolicyName: raiPolicyName\n    versionUpgradeOption: versionUpgradeOption\n  }\n}\n\noutput deploymentName string = deployment.name\noutput deploymentResourceId string = deployment.id\n")),(0,o.kt)("p",null,"We can use this to deploy.... deployments (naming here is definitely confusing) to Azure like so:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bicep"},"var cognitiveServicesDeployments = [\n  {\n    name: 'OpenAi-gpt-35-turbo'\n    shortName: 'gpt35t'\n    model: {\n      format: 'OpenAI'\n      name: 'gpt-35-turbo'\n      version: '0301'\n    }\n    sku: {\n      name: 'Standard'\n      capacity: repositoryBranch == 'refs/heads/main' ? 100 : 10 // capacity in thousands of TPM\n    }\n  }\n]\n\n// Model Deployment - one at a time as parallel deployments are not supported\n@batchSize(1)\nmodule openAiAccountsDeployments35Turbo 'account-deployments.bicep' = [for deployment in cognitiveServicesDeployments: {\n  name: '${deployment.shortName}-cog-accounts-deployments'\n  params: {\n    cognitiveServicesName: openAi.outputs.cognitiveServicesName\n    deploymentName: deployment.name\n    format: deployment.model.format\n    name: deployment.model.name\n    version: deployment.model.version\n    sku: deployment.sku\n  }\n}]\n")),(0,o.kt)("p",null,"We're currently only deploying a single account deployment in our array, but we do it this way as it's not unusual to deploy multiple deployments together. Notice the ",(0,o.kt)("inlineCode",{parentName:"p"},"sku")," portion above:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bicep"},"    sku: {\n      name: 'Standard'\n      capacity: repositoryBranch == 'refs/heads/main' ? 100 : 10\n    }\n")),(0,o.kt)("p",null,"Here we provision a larger ",(0,o.kt)("inlineCode",{parentName:"p"},"capacity")," for our feature branch deployments than our ",(0,o.kt)("inlineCode",{parentName:"p"},"main")," branch deployments. This demonstrates our own usage, whereby we deploy a smaller capacity for our feature branches so that we can test things out, but then deploy a larger capacity for our main branch deployments."),(0,o.kt)("p",null,"Significantly, we're controlling the capacity of our deployments. The way in which you choose to decide on the capacity of your deployments is up to you, but the above demonstrates how you can do it with Bicep and stick within your quota limits."))}d.isMDXComponent=!0},31668:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/screenshot-azure-ai-studio-75ce4e148dad67f942e31aaa9874030d.webp"},21843:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/title-image-015ac7f920c42c69f461711f0fd46156.png"}}]);