<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>I CAN MAKE THIS WORK Blog</title>
        <link>https://blog.johnnyreilly.com/</link>
        <description>I CAN MAKE THIS WORK Blog</description>
        <lastBuildDate>Sun, 05 Dec 2021 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <item>
            <title><![CDATA[Azure Static Web App Deploy Previews with Azure DevOps]]></title>
            <link>https://blog.johnnyreilly.com/2021/12/05/azure-static-web-app-deploy-previews-with-azure-devops</link>
            <guid>Azure Static Web App Deploy Previews with Azure DevOps</guid>
            <pubDate>Sun, 05 Dec 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[I love Netlify deploy previews. This post implements a pull request deployment preview mechanism for Azure Static Web Apps in the context of Azure DevOps which is very much inspired by the Netflix offering.]]></description>
            <content:encoded><![CDATA[<p>I love <a href="https://www.netlify.com/products/deploy-previews/">Netlify deploy previews</a>. This post implements a pull request deployment preview mechanism for Azure Static Web Apps in the context of Azure DevOps which is very much inspired by the Netflix offering.</p><p><img src="../static/blog/2021-12-05-azure-static-web-app-deploy-previews-with-azure-devops/title-image.png" alt="title image reading &quot;Azure Static Web App Deploy Previews with Azure DevOps&quot; with a Azure, Bicep and Azure DevOps logos"/></p><p>Having a build of your latest pull request which is deployed and clickable from the PR itself is a wonderful developer experience. It reduces friction for testing out changes by allowing you to see the impact from within the PR itself. No checking to see if an environment is free with the rest of the team, then manually running a pipeline and waiting whilst a deployment happens. No. It&#x27;s all there without you having to lift a finger. I use Netlify deploy previews on my blog and have become accustomed to the delight that is this:</p><p><img src="../static/blog/2021-12-05-azure-static-web-app-deploy-previews-with-azure-devops/screenshot-of-netlify-deploy-preview-in-pull-request.png" alt="screenshot of a Netlify deploy preview on my latest blog post"/></p><p>I love this and I wanted to implement the &quot;browse the preview&quot; mechanism in Azure DevOps as well, using Azure Static Web Apps. This blog post contains two things:</p><ol><li>A pull request deployment environment mechanism using Azure and Azure Pipelines with Bicep.</li><li>A mechanism for updating a pull request in Azure DevOps with a link to the deployment environment (the &quot;browse the preview&quot;)</li></ol><p>It&#x27;s worth bearing in mind that there&#x27;s a very similar feature to what we&#x27;re going to build for <strong>1.</strong> in SWAs now called &quot;staging environments&quot; that is presently only available on GitHub and not Azure DevOps:</p><p><a href="https://docs.microsoft.com/en-us/answers/questions/574288/creating-environments-for-azure-static-web-app.html"><img src="../static/blog/2021-12-05-azure-static-web-app-deploy-previews-with-azure-devops/screenshot-of-staging-environments-not-available-yet.png" alt="screenshot of Anthony Chu at Microsoft saying &quot;Unfortunately environments is not yet available for Azure DevOps.&quot;"/></a></p><p>It&#x27;s possible that in future the deployment environment aspect of this blog post may be rendered redundant by staging environments landing in Azure DevOps. However, the second part, which updates a PR in ADO with a link is probably generally useful. And it may be the case that the approach of provisioning an environment on demand and extracting a URL could be reworked to work with App Service and similar too.</p><p>I wrote about using <a href="./2021-08-15-bicep-azure-static-web-apps-azure-devops.md">SWAs with Azure DevOps earlier this year</a>. This blog post will take the form of a <a href="https://dev.azure.com/johnnyreilly/azure-static-web-apps/_git/azure-static-web-apps/pullrequest/3">pull request on the code written in that post</a>.</p><h2>Getting <code>defaultHostName</code> from Static Web Apps</h2><p>The first thing we&#x27;re going to do is take the Bicep from that post and tweak it to the following:</p><pre><code class="language-bicep">param appName string
param repositoryUrl string
param repositoryBranch string

param skuName string = &#x27;Free&#x27;
param skuTier string = &#x27;Free&#x27;

resource staticWebApp &#x27;Microsoft.Web/staticSites@2021-02-01&#x27; = {
  name: repositoryBranch == &#x27;main&#x27; ? appName : &#x27;${appName}-${repositoryBranch}&#x27;
  location: resourceGroup().location
  sku: {
    name: skuName
    tier: skuTier
  }
  properties: {
    // The provider, repositoryUrl and branch fields are required for successive deployments to succeed
    // for more details see: https://github.com/Azure/static-web-apps/issues/516
    provider: &#x27;DevOps&#x27;
    repositoryUrl: repositoryUrl
    branch: repositoryBranch
    buildProperties: {
      skipGithubActionWorkflowGeneration: true
    }
  }
}

output staticWebAppDefaultHostName string = staticWebApp.properties.defaultHostname // eg gentle-bush-0db02ce03.azurestaticapps.net
output staticWebAppId string = staticWebApp.id
output staticWebAppName string = staticWebApp.name
</code></pre><p>There&#x27;s some changes in here. First of all we&#x27;re using a newer version of the <code>staticSites</code> resource in Azure. You&#x27;ll also see that we name the resource conditionally now. If we&#x27;re on the <code>main</code> branch we name it as we did before with <code>appName</code>. But if we aren&#x27;t then we suffix the <code>name</code> with the <code>repositoryBranch</code>. It&#x27;s worth knowing that <a href="https://docs.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-best-practices/resource-abbreviations#compute-and-web">there are restrictions and conventions for Azure resource naming</a>. If you have a branch name that is just alphanumerics and hyphens you&#x27;ll be fine.</p><p>You&#x27;ll see the output of the Bicep file has changed. Previously we were outputting the <code>apiKey</code> that we used for deployment. This isn&#x27;t the securest of approaches as, by having this as a deployment output, this data can be accessed by people who share access with your Azure portal. So we&#x27;re going to use a different (and more secure) approach to acquire this in our pipeline later.</p><p>More significantly, we are now outputting the <code>staticWebAppDefaultHostName</code> of our newly provisioned SWA. This is the location where people will be able to view the deployment preview. Since we want to pump that into our pull request description, so people can click on the link, we are going to need this. We&#x27;re also pumping out the <code>staticWebAppId</code> and <code>staticWebAppName</code>. We&#x27;ll use the <code>staticWebAppName</code> to acquire the <code>apiKey</code> in our pipeline.</p><h2>Azure Pipelines tweaks</h2><p>Now to the pipeline. After the deployment, our updated pipeline is going to acquire the <code>apiKey</code> for deployment like so:</p><pre><code class="language-yml">- task: AzureCLI@2
  displayName: &#x27;Acquire API key for deployment&#x27;
  inputs:
    azureSubscription: $(serviceConnection)
    scriptType: bash
    scriptLocation: inlineScript
    inlineScript: |
      APIKEY=$(az staticwebapp secrets list --name $(staticWebAppName) | jq -r &#x27;.properties.apiKey&#x27;)
      echo &quot;##vso[task.setvariable variable=apiKey;issecret=true]$APIKEY&quot;
</code></pre><p>The above uses the <a href="https://docs.microsoft.com/en-us/azure/devops/pipelines/tasks/deploy/azure-cli?view=azure-devops">Azure CLI task</a> to acquire the <code>apiKey</code>. It uses <a href="https://stedolan.github.io/jq/">jq</a> to pull out the required property from the JSON and writes it as a secret variable in the pipeline to be used in the deployment.</p><p>At the end of the pipeline, if we&#x27;re not on the <code>main</code> branch, the the pipeline is going to run a custom script that will update the PR with the preview URL:</p><pre><code class="language-yml">- task: Npm@1
  displayName: &#x27;Pull request preview install&#x27;
  condition: and(succeeded(), ne(variables.isMain, &#x27;true&#x27;))
  inputs:
    command: &#x27;install&#x27;
    workingDir: pull-request-preview

- task: Npm@1
  displayName: &#x27;Pull request preview&#x27;
  condition: and(succeeded(), ne(variables.isMain, &#x27;true&#x27;))
  inputs:
    command: &#x27;custom&#x27;
    customCommand: &#x27;run pull-request-preview -- --sat &quot;$(System.AccessToken)&quot; --project &quot;$(System.TeamProject)&quot; --repository &quot;$(Build.Repository.Name)&quot; --systemCollectionUri &quot;$(System.CollectionUri)&quot; --pullRequestId $(System.PullRequest.PullRequestId) --previewUrl &quot;https://$(staticWebAppDefaultHostName)&quot;&#x27;
    workingDir: pull-request-preview
</code></pre><p>We haven&#x27;t written that script yet; we will in a moment.</p><p>The complete <code>azure-piplines.yml</code> is below, and you&#x27;ll notice we&#x27;ve moved all variables save for the <code>subscriptionId</code> into the <code>azure-pipelines.yml</code> and we&#x27;re using a <code>westeurope</code> location / resource group as at present <code>staticSites</code> is not available everywhere:</p><pre><code class="language-yml">pool:
  vmImage: ubuntu-latest

variables:
  # subscriptionId is a variable defined on the pipeline itself
  - name: appName
    value: &#x27;our-static-web-app&#x27;
  - name: location
    value: &#x27;westeurope&#x27; # at time of writing static sites are available in limited locations such as westeurope
  - name: serviceConnection
    value: &#x27;azureRMWestEurope&#x27;
  - name: azureResourceGroup # this resource group lives in westeurope
    value: &#x27;johnnyreilly&#x27;

steps:
  - checkout: self
    submodules: true

  - bash: az bicep build --file infra/static-web-app/main.bicep
    displayName: &#x27;Compile Bicep to ARM&#x27;

  - task: AzureResourceManagerTemplateDeployment@3
    name: DeployStaticWebAppInfra
    displayName: Deploy Static Web App infra
    inputs:
      deploymentScope: Resource Group
      azureResourceManagerConnection: $(serviceConnection)
      subscriptionId: $(subscriptionId)
      action: Create Or Update Resource Group
      resourceGroupName: $(azureResourceGroup)
      location: $(location)
      templateLocation: Linked artifact
      csmFile: &#x27;infra/static-web-app/main.json&#x27; # created by bash script
      overrideParameters: &gt;-
        -repositoryUrl $(Build.Repository.Uri)
        -repositoryBranch $(Build.SourceBranchName)
        -appName $(appName)
      deploymentMode: Incremental
      deploymentOutputs: deploymentOutputs

  - task: PowerShell@2
    name: &#x27;SetDeploymentOutputVariables&#x27;
    displayName: &#x27;Set Deployment Output Variables&#x27;
    inputs:
      targetType: inline
      script: |
        $armOutputObj = &#x27;$(deploymentOutputs)&#x27; | ConvertFrom-Json
        $armOutputObj.PSObject.Properties | ForEach-Object {
          $keyname = $_.Name
          $value = $_.Value.value

          # Creates a standard pipeline variable
          Write-Output &quot;##vso[task.setvariable variable=$keyName;]$value&quot;

          # Display keys and values in pipeline
          Write-Output &quot;output variable: $keyName $value&quot;
        }
      pwsh: true

  - task: AzureCLI@2
    displayName: &#x27;Acquire API key for deployment&#x27;
    inputs:
      azureSubscription: $(serviceConnection)
      scriptType: bash
      scriptLocation: inlineScript
      inlineScript: |
        APIKEY=$(az staticwebapp secrets list --name $(staticWebAppName) | jq -r &#x27;.properties.apiKey&#x27;)
        echo &quot;##vso[task.setvariable variable=apiKey;issecret=true]$APIKEY&quot;

  - task: AzureStaticWebApp@0
    name: DeployStaticWebApp
    displayName: Deploy Static Web App
    inputs:
      app_location: &#x27;static-web-app&#x27;
      # api_location: &#x27;api&#x27;
      output_location: &#x27;build&#x27;
      azure_static_web_apps_api_token: $(apiKey)

  - task: Npm@1
    displayName: &#x27;Pull request preview install&#x27;
    condition: and(succeeded(), ne(variables.isMain, &#x27;true&#x27;))
    inputs:
      command: &#x27;install&#x27;
      workingDir: pull-request-preview

  - task: Npm@1
    displayName: &#x27;Pull request preview&#x27;
    condition: and(succeeded(), ne(variables.isMain, &#x27;true&#x27;))
    inputs:
      command: &#x27;custom&#x27;
      customCommand: &#x27;run pull-request-preview -- --sat &quot;$(System.AccessToken)&quot; --project &quot;$(System.TeamProject)&quot; --repository &quot;$(Build.Repository.Name)&quot; --systemCollectionUri &quot;$(System.CollectionUri)&quot; --pullRequestId $(System.PullRequest.PullRequestId) --previewUrl &quot;https://$(staticWebAppDefaultHostName)&quot;&#x27;
      workingDir: pull-request-preview
</code></pre><h2>Updating the PR with a preview URL</h2><p>We want to be able to update our pull request with our deploy URL. To make that happen, we&#x27;re going to whiz up a little node app using TypeScript, ts-node and <a href="https://github.com/microsoft/azure-devops-node-api">the azure-devops-node-api package</a>.</p><p>Let&#x27;s create our app:</p><pre><code class="language-bash">mkdir pull-request-preview
cd pull-request-preview
npm init --yes
npm install @types/node @types/yargs ts-node typescript azure-devops-node-api yargs --save
</code></pre><p>We&#x27;ll update our newly created <code>package.json</code> file with a <code>pull-request-preview</code> script which will be the entry point.</p><pre><code class="language-json">  &quot;scripts&quot;: {
    &quot;pull-request-preview&quot;: &quot;ts-node ./index.ts&quot;
  },
</code></pre><p>We&#x27;ll add a <code>tsconfig.json</code> file that looks like this:</p><pre><code class="language-json">{
  &quot;compilerOptions&quot;: {
    &quot;target&quot;: &quot;ES2015&quot;,
    &quot;module&quot;: &quot;CommonJS&quot;,
    &quot;strict&quot;: true,
    &quot;esModuleInterop&quot;: true,
    &quot;moduleResolution&quot;: &quot;node&quot;
  }
}
</code></pre><p>Finally we&#x27;ll add our script in a new <code>index.ts</code> file:</p><pre><code class="language-ts">#!/usr/bin/env node
import yargs from &#x27;yargs/yargs&#x27;;
import * as nodeApi from &#x27;azure-devops-node-api&#x27;;
import { IGitApi } from &#x27;azure-devops-node-api/GitApi&#x27;;
import { PullRequestStatus } from &#x27;azure-devops-node-api/interfaces/GitInterfaces&#x27;;

const parser = yargs(process.argv.slice(2)).options({
  pat: { type: &#x27;string&#x27;, default: &#x27;&#x27; },
  sat: { type: &#x27;string&#x27;, default: &#x27;&#x27; },
  systemCollectionUri: { type: &#x27;string&#x27;, demandOption: true },
  project: { type: &#x27;string&#x27;, demandOption: true },
  repository: { type: &#x27;string&#x27;, demandOption: true },
  pullRequestId: { type: &#x27;number&#x27; },
  previewUrl: { type: &#x27;string&#x27;, demandOption: true },
});

(async () =&gt; {
  await run(await parser.argv);
})();

async function run({
  pat,
  sat,
  project,
  repository,
  systemCollectionUri,
  pullRequestId,
  previewUrl,
}: {
  pat: string;
  sat: string;
  systemCollectionUri: string;
  project: string;
  repository: string;
  pullRequestId: number | undefined;
  previewUrl: string;
}) {
  const config: Config = { project, repository };
  const gitApi = await getGitApi({ pat, sat, systemCollectionUri });

  if (!pullRequestId)
    console.log(
      &#x27;No pull request id supplied, so will look up latest active PR&#x27;
    );

  const pullRequestIdToUpdate =
    pullRequestId || (await getActivePullRequestId({ gitApi, config }));
  if (!pullRequestIdToUpdate) {
    console.log(&#x27;No pull request found&#x27;);
    return;
  }

  console.log(
    `Updating ${systemCollectionUri}/${project}/_git/${repository}/pullrequest/${pullRequestIdToUpdate} with a preview URL of ${previewUrl}`
  );

  const pullRequest = await getPullRequest({
    gitApi,
    config,
    pullRequestId: pullRequestIdToUpdate,
  });

  await updatePullRequestDescription({
    gitApi,
    config,
    pullRequestId: pullRequestIdToUpdate,
    description: makePreviewDescriptionMarkdown(
      pullRequest.description!,
      previewUrl
    ),
  });

  console.log(
    `Updated pull request description a preview URL of ${previewUrl}`
  );
}

interface Config {
  project: string;
  repository: string;
}

async function getGitApi({
  sat,
  pat,
  systemCollectionUri,
}: {
  pat: string;
  sat: string;
  systemCollectionUri: string;
}) {
  const authHandler = pat
    ? nodeApi.getPersonalAccessTokenHandler(
        pat,
        /** allowCrossOriginAuthentication */ true
      )
    : nodeApi.getHandlerFromToken(
        sat,
        /** allowCrossOriginAuthentication */ true
      );

  const webApi = new nodeApi.WebApi(systemCollectionUri, authHandler);
  const gitApi = await webApi.getGitApi();

  return gitApi;
}

async function getActivePullRequestId({
  gitApi,
  config,
}: {
  gitApi: IGitApi;
  config: Config;
}) {
  const topActivePullRequest = await gitApi.getPullRequests(
    config.repository, // repository.id!,
    { status: PullRequestStatus.Active },
    config.project,
    undefined,
    /** skip */ 0,
    /** top */ 1
  );

  return topActivePullRequest.length &gt; 0
    ? topActivePullRequest[0].pullRequestId
    : undefined;
}

async function getPullRequest({
  gitApi,
  config,
  pullRequestId,
}: {
  gitApi: IGitApi;
  config: Config;
  pullRequestId: number;
}) {
  const pullRequest = await gitApi.getPullRequest(
    config.repository, // repository.id!,
    pullRequestId,
    config.project,
    undefined,
    /** skip */ 0,
    /** top */ 1,
    /** includeCommits */ false,
    /** includeWorkItemRefs */ false
  );
  return pullRequest;
}

async function updatePullRequestDescription({
  gitApi,
  config,
  pullRequestId,
  description,
}: {
  gitApi: IGitApi;
  config: Config;
  pullRequestId: number;
  description: string;
}) {
  // To do an update with the API you must provide a new object with only the properties you are updating
  const updatePullRequest = {
    description,
  };
  await gitApi.updatePullRequest(
    updatePullRequest,
    config.repository,
    pullRequestId,
    config.project
  );
}

function makePreviewDescriptionMarkdown(desc: string, previewUrl: string) {
  const previewRegex = /(&gt; -*\n&gt; # Preview:\n.*\n&gt;.*\n&gt; -*\n)/;

  const makePreview = (previewUrl: string) =&gt; `&gt; ---
&gt; # Preview:
&gt; ${previewUrl}
&gt; 
&gt; ---
`;

  const alreadyHasPreview = desc.match(previewRegex);
  return alreadyHasPreview
    ? desc.replace(previewRegex, makePreview(previewUrl))
    : makePreview(previewUrl) + desc;
}
</code></pre><p>The above code does two things:</p><ol><li>Looks up the pull request, using the details supplied from the pipeline. It&#x27;s worth noting that the <code>System.PullRequest.PullRequestId</code> variable is <a href="https://docs.microsoft.com/en-us/azure/devops/pipelines/build/variables?view=azure-devops&amp;tabs=yaml">initialized only if the build ran because of a Git PR affected by a branch policy</a>. If you don&#x27;t have that set up, the script falls back to using the latest active pull request. This is generally useful when you&#x27;re getting set up in the first place; you won&#x27;t want to rely on this behaviour.</li><li>Updates the pull request description with a prefix piece of markdown that provides the link to the preview URL. This is our &quot;browse the preview&quot;:
<img src="../static/blog/2021-12-05-azure-static-web-app-deploy-previews-with-azure-devops/screenshot-of-deploy-preview-small.png" alt="screenshot of rendered markdown with the preview link"/></li></ol><p>This script could be refactored into a dedicated Azure Pipelines custom task.</p><h2>Permissions</h2><p>The first time you run this you may encounter a permissions error of the form:</p><pre><code>Error: TF401027: You need the Git &#x27;PullRequestContribute&#x27; permission to perform this action.
</code></pre><p>To remedy this you need to give your build service the relevant permissions to update a pull request. You can do that by going to the security settings of your repo and setting &quot;Contribute to pull requests&quot; to &quot;Allow&quot; for your build service:</p><p><img src="../static/blog/2021-12-05-azure-static-web-app-deploy-previews-with-azure-devops/screenshot-of-git-repository-security-settings.png" alt="Screenshot of &quot;Contribute to pull requests&quot; permission in Azure DevOps Git security being set to &quot;Allow&quot; "/></p><h2>Enjoy! (and keep Azure tidy)</h2><p>When the pipeline is now run you can see that a deployment preview link is now updated onto the PR description:</p><p><img src="../static/blog/2021-12-05-azure-static-web-app-deploy-previews-with-azure-devops/screenshot-of-deploy-preview.png" alt="Screenshot of deployment preview on PR"/></p><p>This will happen whenever a PR is raised which is tremendous.</p><p>A thing to remember, is that there&#x27;s nothing in this post that tears down the temporary deployment after the pull request has been merged. It will hang around. We happen to be using free resources in this post, but if we weren&#x27;t there would be cost implications. Either way, you&#x27;ll want to clean up unused environments as a matter of course. And I&#x27;d advise automating that.</p><p>So be tidy and cost aware with this approach.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[TypeScript vs JSDoc JavaScript]]></title>
            <link>https://blog.johnnyreilly.com/2021/11/22/typescript-vs-jsdoc-javascript</link>
            <guid>TypeScript vs JSDoc JavaScript</guid>
            <pubDate>Mon, 22 Nov 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[There's a debate to be had about whether using JavaScript or TypeScript leads to better outcomes when building a project. The introduction of using JSDoc annotations to type a JavaScript codebase introduces a new dynamic to this discussion. This post will investigate what that looks like, and come to an (opinionated) conclusion.]]></description>
            <content:encoded><![CDATA[<p>There&#x27;s a debate to be had about whether using JavaScript or TypeScript leads to better outcomes when building a project. The introduction of using JSDoc annotations to type a JavaScript codebase introduces a new dynamic to this discussion. This post will investigate what that looks like, and come to an (opinionated) conclusion.</p><p><img src="../static/blog/2021-11-22-typescript-vs-jsdoc-javascript/title-image.png" alt="title image reading &quot;JSDoc JavaScript vs TypeScript&quot; with a JavaScript logo and TypeScript logo"/></p><h2>Updated 6th December 2021</h2><p>This blog evolved to become a talk:</p><iframe width="560" height="315" src="https://www.youtube.com/embed/5MZoAcheyE4?start=240" title="YouTube video player" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"></iframe><p>If you&#x27;d talked to me in 2018, I would have solidly recommended using TypeScript, and steering away from JavaScript. The rationale is simple: I&#x27;m exceedingly convinced of the value that static typing provides in terms of productivity / avoiding bugs in production. I appreciate this can be a contentious issue, but that is my settled opinion on the subject. Other opinions are available.</p><p>TypeScript has long had a good static typing story. JavaScript is dynamically typed and so historically has not. Thanks to TypeScript support for JSDoc, JavaScript can now be statically type checked.</p><h2>What is JSDoc JavaScript?</h2><p>JSDoc itself actually dates way back to 1999. According to the <a href="https://en.wikipedia.org/wiki/JSDoc">Wikipedia entry</a>:</p><blockquote><p>JSDoc is a markup language used to annotate JavaScript source code files. Using comments containing JSDoc, programmers can add documentation describing the application programming interface of the code they&#x27;re creating.</p></blockquote><p>The TypeScript team have taken JSDoc support and run with it. You can now use a <a href="https://www.typescriptlang.org/docs/handbook/jsdoc-supported-types.html">variant of JSDoc annotations</a> to provide type information in JavaScript files.</p><p>What does this look like? Well, to take a simple example, a TypeScript statement like so:</p><pre><code class="language-ts">let myString: string;
</code></pre><p>Could become the equivalent JavaScript statement with a JSDoc annotation:</p><pre><code class="language-ts">/** @type {string} */
let myString;
</code></pre><p>This is type enhanced JavaScript which the TypeScript compiler can understand and type check.</p><h2>Why use JSDoc JavaScript?</h2><p>Why would you use JSDoc JavaScript instead of TypeScript? Well there&#x27;s a number of possible use cases.</p><p>Perhaps you&#x27;re writing simple node scripts and you&#x27;d like a little type safety to avoid mistakes. Or perhaps you want to dip your project&#x27;s toe in the waters of static type checking but without fully committing. JSDoc allows for that. Or perhaps your team simply prefers not having a compile step.</p><p>That, in fact, was the rationale of the webpack team. A little bit of history: webpack has always been a JavaScript codebase. As the codebase grew and grew, there was often discussion about using static typing. However, having a compilation step wasn&#x27;t desired.</p><p>TypeScript had been quietly adding support for type checking JavaScript with the assistance of JSDoc for some time. Initial support arrived with the <code>--checkJs</code> compiler option in <a href="https://www.typescriptlang.org/docs/handbook/release-notes/typescript-2-3.html#errors-in-js-files-with---checkjs">TypeScript 2.3</a>.</p><p>A community member by the name of <a href="https://twitter.com/mohsen____">Mohsen Azimi</a> experimentally started out using this approach to type check the webpack codebase. <a href="https://github.com/webpack/webpack/pull/6862">His PR</a> ended up being a test case that helped improve the type checking of JavaScript by TypeScript. TypeScript v2.9 shipped with a whole host of JSDoc improvements as a consequence of the webpack work. Being such a widely used project this also helped popularise the approach of using JSDoc to type check JavaScript codebases. It demonstrated that this approach could work on a significantly sized codebase.</p><p>These days, JSDoc type checking with TypeScript is extremely powerful. Whilst not quite on par with TypeScript (not all TypeScript syntax is supported in JSDoc) the gap in functionality is pretty small.</p><p>It&#x27;s a completely legitimate choice to build a JavaScript codebase with all the benefits of static typing.</p><h2>Why use TypeScript?</h2><p>So if you were starting a project today, and you&#x27;d decided you wanted to make use of static typing, how do you choose? TypeScript or JavaScript with JSDoc?</p><p>Well, unless you&#x27;ve a compelling need to avoid a compilation step, I&#x27;m going to suggest that TypeScript may be the better choice for a number of reasons.</p><p>Firstly, the tooling support for using TypeScript directly is better than that for JSDoc JavaScript. At the time of writing, things like refactoring tools etc in your editor work more effectively with TypeScript than with JSDoc JavaScript. (Although these are improving as time goes by.)</p><p>Secondly, working with JSDoc is distinctly &quot;noisier&quot;. It requires far more keystrokes to achieve the same level of type safety. Consider the following TypeScript:</p><pre><code class="language-ts">function stringsStringStrings(
  p1: string,
  p2?: string,
  p3?: string,
  p4 = &#x27;test&#x27;
): string {
  // ...
}
</code></pre><p>As compared to the equivalent JSDoc JavaScript:</p><pre><code class="language-ts">/**
 * @param {string}  p1
 * @param {string=} p2
 * @param {string} [p3]
 * @param {string} [p4=&quot;test&quot;]
 * @return {string}
 */
function stringsStringStrings(p1, p2, p3, p4) {
  // ...
}
</code></pre><p>It may be my own familiarity with TypeScript speaking, but I find that the TypeScript is easier to read and comprehend as compared to the JSDoc JavaScript alternative. The fact that all JSDoc annotations live in comments, rather than directly in syntax, makes it harder to follow. (It certainly doesn&#x27;t help that many VS Code themes present comments in a very faint colour.)</p><p>My final reason for favouring TypeScript comes down to falling into the <a href="https://blog.codinghorror.com/falling-into-the-pit-of-success/">&quot;pit of success&quot;</a>. You&#x27;re cutting <em>against</em> the grain when it comes to static typing and JavaScript. You can have it, but you have to work that bit harder to ensure that you have statically typed code. On the other hand, you&#x27;re cutting <em>with</em> the grain when it comes to static typing and TypeScript. You have to work hard to opt out of static typing. The TypeScript defaults tend towards static typing, whilst the JavaScript defaults tend away.</p><p>As someone who very much favours static typing, you can imagine how this is compelling to me!</p><h2>It&#x27;s your choice!</h2><p>So in a way, I don&#x27;t feel super strongly whether people use JavaScript or TypeScript. But having static typing will likely be a benefit to new projects. Bottom line, I&#x27;m keen that people fall into the &quot;pit of success&quot;, so my recommendation for a new project would be TypeScript.</p><p>I really like JSDoc myself, and will often use it on small projects. It&#x27;s a fantastic addition to TypeScript&#x27;s capabilities. For bigger projects, I&#x27;ll likely go with TypeScript from the get go. But really, this is a choice - and either is great.</p><p><a href="https://blog.logrocket.com/typescript-vs-jsdoc-javascript/">This post was originally published on LogRocket.</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Azure standard availability tests with Bicep]]></title>
            <link>https://blog.johnnyreilly.com/2021/11/18/azure-standard-tests-with-bicep</link>
            <guid>Azure standard availability tests with Bicep</guid>
            <pubDate>Thu, 18 Nov 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Azure standard tests are a tremendous way to monitor the uptime of your services in Azure. Sometimes also called availability tests, web tests and ping tests, this post goes through how to deploy one using Bicep. It also looks at some of the gotchas that you may encounter as you're setting it up.]]></description>
            <content:encoded><![CDATA[<p>Azure standard tests are a tremendous way to monitor the uptime of your services in Azure. Sometimes also called availability tests, web tests and ping tests, this post goes through how to deploy one using Bicep. It also looks at some of the gotchas that you may encounter as you&#x27;re setting it up.</p><p><img src="../static/blog/2021-11-18-azure-standard-tests-with-bicep/title-image.png" alt="title image reading &quot;Azure standard availability tests with Bicep&quot; with a Bicep logo and Azure logos"/></p><h2>What are standard tests?</h2><p>To quote the <a href="https://docs.microsoft.com/en-us/azure/azure-monitor/app/availability-standard-tests">docs</a>:</p><blockquote><p>Standard tests are a single request test that is similar to the URL ping test but more advanced. In addition to validating whether an endpoint is responding and measuring the performance, Standard tests also includes SSL certificate validity, proactive lifetime check, HTTP request verb (for example GET,HEAD,POST, etc.), custom headers, and custom data associated with your HTTP request.</p></blockquote><p>So we can use these to:</p><ul><li>send requests to a URL</li><li>from a variety of geographic locations</li><li>and determine if it is responding with a 200 status code</li></ul><p>The URL may be one of our own service URLs, but it could be checking any kind of URL. It&#x27;s web specific, not Azure specific.</p><h2>Standard test Bicep</h2><p>Now we&#x27;re going to write a Bicep module that provisions a standard test named <code>standard-test.bicep</code>:</p><pre><code class="language-bicep">@description(&#x27;Tags that our resources need&#x27;)
param tags object

@description(&#x27;The resource id of the app insights which the webtest will reference&#x27;)
param appInsightsResourceId string

@description(&#x27;The name of the webtest to create&#x27;)
param standardTestName string

@description(&#x27;URL to test&#x27;)
param urlToTest string

@description(&#x27;Interval in seconds between test runs for this WebTest. Default value is 300.&#x27;)
param frequency int = 300

@description(&#x27;Seconds until this WebTest will timeout and fail. Default value is 30.&#x27;)
param timeout int = 30

// useful reference:
// https://docs.microsoft.com/en-us/azure/azure-monitor/app/monitor-web-app-availability#azure
@allowed([
  &#x27;emea-au-syd-edge&#x27; // Australia East
  &#x27;latam-br-gru-edge&#x27; // Brazil South
  &#x27;us-fl-mia-edge&#x27; // Central US
  &#x27;apac-hk-hkn-azr&#x27; // East Asia
  &#x27;us-va-ash-azr&#x27; // East US
  &#x27;emea-ch-zrh-edge&#x27; // France South (Formerly France Central)
  &#x27;emea-fr-pra-edge&#x27; // France Central
  &#x27;apac-jp-kaw-edge&#x27; // Japan East
  &#x27;emea-gb-db3-azr&#x27; // North Europe
  &#x27;us-il-ch1-azr&#x27; // North Central US
  &#x27;us-tx-sn1-azr&#x27; // South Central US
  &#x27;apac-sg-sin-azr&#x27; // Southeast Asia
  &#x27;emea-se-sto-edge&#x27; // UK West
  &#x27;emea-nl-ams-azr&#x27; // West Europe
  &#x27;us-ca-sjc-azr&#x27; // West US
  &#x27;emea-ru-msa-edge&#x27; // UK South
])
@description(&#x27;The populations (locations) for the test&#x27;)
param testPopulations array = [
  &#x27;emea-se-sto-edge&#x27; // UK West
  &#x27;emea-ru-msa-edge&#x27; // UK South
  &#x27;emea-gb-db3-azr&#x27; // North Europe
  &#x27;us-va-ash-azr&#x27; // East US
  &#x27;apac-sg-sin-azr&#x27; // Southeast Asia
]

var tagsWithHiddenLink = union({
  &#x27;hidden-link:${appInsightsResourceId}&#x27;: &#x27;Resource&#x27;
}, tags)

resource standardWebTest &#x27;Microsoft.Insights/webtests@2018-05-01-preview&#x27; = {
  name: standardTestName
  location: resourceGroup().location
  tags: tagsWithHiddenLink
  kind: &#x27;ping&#x27;
  properties: {
    SyntheticMonitorId: urlToTest
    Name: urlToTest
    Description: null
    Enabled: true
    Frequency: frequency
    Timeout: timeout
    Kind: &#x27;standard&#x27;
    RetryEnabled: true
    Locations: [for testPopulation in testPopulations: {
      Id: testPopulation
    }]
    Configuration: null
    Request: {
      RequestUrl: urlToTest
      Headers: null
      HttpVerb: &#x27;GET&#x27;
      RequestBody: null
      ParseDependentRequests: false
      FollowRedirects: null
    }
    ValidationRules: {
      ExpectedHttpStatusCode: 200
      IgnoreHttpsStatusCode: false
      ContentValidation: null
      SSLCheck: true
      SSLCertRemainingLifetimeCheck: 7
    }
  }
}

output standardWebTestName string = standardWebTest.name
output standardWebTestId string = standardWebTest.id
</code></pre><h3>Locations / populations</h3><p>You&#x27;ll note that a parameter to the Bicep module is <code>testPopulations</code>. These are the geographical places where requests will be sent from. You&#x27;ll note we have a default value of five populations, but these could be any of the (presently) sixteen valid values. If you were wondering where those are sourced from, <a href="https://docs.microsoft.com/en-us/azure/azure-monitor/app/availability-standard-tests#location-population-tags">here is the link to the Azure docs</a>.</p><h3>The <code>hidden-link</code> tag</h3><p>Another significant call out should go to the <code>hidden-link</code> tag. The <code>hidden-link</code> tag is a mandatory tag that connects the test (known in Azure as a &quot;webtest&quot;) to an app insights instance.</p><p>If you do not provide a <code>hidden-link</code> tag, or if you try to specify a resource group other than the app insights resource group, Azure will fail to deploy your test and you may find yourself presented with an error like this in the deployments section of the Azure Portal.</p><blockquote><p>Resource should exist in the same resource group as the linked component</p></blockquote><p><img src="../static/blog/2021-11-18-azure-standard-tests-with-bicep/screenshot-azure-portal-deployments-resource-should-exist-in-the-same-resource-group.png" alt="screenshot of the Azure Portal Deployments section saying &quot;Resource should exist in the same resource group as the linked component&quot;"/></p><p>In our module we set both the <code>hidden-link</code> tag as well as the tags that have been supplied via the <code>tags</code> parameter.</p><h3>App insights and standard tests share a resource group</h3><p>Another thing that can cause issues is the deployment of your app insights resource. It&#x27;s not unusual to spin up Azure resources on demand, for a given branch of your source code. Those resources will be named in relation to the branch and will depend upon one another. I&#x27;ve never managed to successfully create an app insights resource, and reference it from a standard test within the same Bicep file. It appears to be necessary to separate the two actions, such that Azure recognises the existence of the app insights resource when the standard test is deployed.</p><p>If you are working with long-lived app insights it won&#x27;t be an issue for you, but if you aren&#x27;t it&#x27;s worth being aware of.</p><h2>Using <code>standard-test.bicep</code></h2><p>Our Bicep module can be invoked from another Bicep module named <code>ping-them.bicep</code> like so:</p><pre><code class="language-bicep">@description(&#x27;Tags that our resources need&#x27;)
param tags object

@description(&#x27;The name of the app insights&#x27;)
param appInsightsName string

@description(&#x27;An object where the keys are the name of the web test and the values are the URL eg {&quot;my-standard-test&quot;: &quot;https://status.azure.com/en-gb/status&quot;} &#x27;)
param standardTests object

var appInsightsResourceId = resourceId(&#x27;Microsoft.Insights/components&#x27;, appInsightsName)

module standardTestsToCreate &#x27;standard-test.bicep&#x27; = [for standardTest in items(standardTests): {
  name: standardTest.key
  params: {
    tags: tags
    appInsightsResourceId: appInsightsResourceId
    standardTestName: standardTest.key
    urlToTest: standardTest.value
  }
}]
</code></pre><p>As you can see, this module itself takes a number of parameters, and will typically be invoked from some kind of continuous integration mechanism such as Azure Pipelines or GitHub Actions.</p><p>This module is written in the expectation that multiple URLs will need to be pinged, and so it has a parameter named <code>standardTests</code> which is effectively a dictionary of key-value pairs, where the key is the name of the standard test, and the value is the URL to test.</p><p>The module makes use of the <a href="https://docs.microsoft.com/en-us/azure/azure-resource-manager/bicep/bicep-functions-array#items"><code>items</code></a> array helper in Bicep to convert the object into an array that can be iterated over.</p><h2>Azure Pipelines test</h2><p>We&#x27;re going to use Azure Pipelines to test this out. Here&#x27;s an <code>azure-pipelines.yml</code> file:</p><pre><code class="language-yml">trigger:
  - main

pool:
  vmImage: ubuntu-latest

steps:
  - checkout: self
    submodules: true

  - bash: az bicep build --file ping-them.bicep
    displayName: &#x27;Compile Bicep to ARM&#x27;

  - task: AzureResourceManagerTemplateDeployment@3
    name: DeploySharedWebTests
    displayName: Deploy Shared Web Tests
    inputs:
      deploymentScope: Resource Group
      azureResourceManagerConnection: ${{ variables.serviceConnection }}
      subscriptionId: $(subscriptionId)
      action: Create Or Update Resource Group
      resourceGroupName: $(resourceGroup)
      location: $(location)
      templateLocation: Linked artifact
      csmFile: &#x27;ping-them.json&#x27; # created by bash script
      overrideParameters: &gt;-
        -tags {&quot;owner&quot;: &quot;@johnny_reilly&quot;, &quot;branch&quot;: &quot;$(Build.SourceBranchName)&quot;}
        -appInsightsName $(appInsightsName)
        -standardTests {&quot;my-standard-test&quot;: &quot;https://status.azure.com/en-gb/status&quot;}
      deploymentMode: Incremental
</code></pre><p>When run, it invokes our <code>ping-them.bicep</code> module, passing two URLs to test.</p><p>When executed, you end up with a delightful &quot;availability test&quot; (which is your standard test) in Azure:</p><p><img src="../static/blog/2021-11-18-azure-standard-tests-with-bicep/screenshot-azure-portal-availability.png" alt="screenshot of an Availability test in the Azure Portal"/></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[NSwag generated C# client: Open API property name clashes and decimal types rather than double]]></title>
            <link>https://blog.johnnyreilly.com/2021/10/31/nswag-generated-c-sharp-client-property-name-clash</link>
            <guid>NSwag generated C# client: Open API property name clashes and decimal types rather than double</guid>
            <pubDate>Sun, 31 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[NSwag is a great tool for generating client libraries in C# and TypeScript from Open API / Swagger definitions. You can face issues where Open API property names collide due to the nature of the C# language, and when you want to use decimal for your floating point numeric type over double. This post demonstrates how to get over both issues.]]></description>
            <content:encoded><![CDATA[<p>NSwag is a great tool for generating client libraries in C# and TypeScript from Open API / Swagger definitions. You can face issues where Open API property names collide due to the nature of the C# language, and when you want to use <code>decimal</code> for your floating point numeric type over <code>double</code>. This post demonstrates how to get over both issues.</p><p><img src="../static/blog/2021-10-31-nswag-generated-c-sharp-client-property-name-clash/title-image.png" alt="title image reading &quot;NSwag generated C# client: Open API property name clashes and decimal types rather than double&quot; with a C# logo and Open API logos"/></p><h2>Make a C# Client Generator</h2><p>Let&#x27;s get a console app set up that will allow us to generate a C# client using an Open API file:</p><pre><code class="language-sh">dotnet new console -o NSwag
cd NSwag
dotnet add package NSwag.CodeGeneration.CSharp
</code></pre><p>We&#x27;ll also add a <code>petstore-simple.json</code> file to our project which we&#x27;ll borrow from <a href="https://github.com/OAI/OpenAPI-Specification/blob/main/examples/v2.0/json/petstore-simple.json">https://github.com/OAI/OpenAPI-Specification/blob/main/examples/v2.0/json/petstore-simple.json</a> (home of the Open API specification):</p><pre><code class="language-json">{
  &quot;swagger&quot;: &quot;2.0&quot;,
  &quot;info&quot;: {
    &quot;version&quot;: &quot;1.0.0&quot;,
    &quot;title&quot;: &quot;Swagger Petstore&quot;,
    &quot;description&quot;: &quot;A sample API that uses a petstore as an example to demonstrate features in the swagger-2.0 specification&quot;,
    &quot;termsOfService&quot;: &quot;http://swagger.io/terms/&quot;,
    &quot;contact&quot;: {
      &quot;name&quot;: &quot;Swagger API Team&quot;
    },
    &quot;license&quot;: {
      &quot;name&quot;: &quot;MIT&quot;
    }
  },
  &quot;host&quot;: &quot;petstore.swagger.io&quot;,
  &quot;basePath&quot;: &quot;/api&quot;,
  &quot;schemes&quot;: [&quot;http&quot;],
  &quot;consumes&quot;: [&quot;application/json&quot;],
  &quot;produces&quot;: [&quot;application/json&quot;],
  &quot;paths&quot;: {
    &quot;/pets&quot;: {
      &quot;get&quot;: {
        &quot;description&quot;: &quot;Returns all pets from the system that the user has access to&quot;,
        &quot;operationId&quot;: &quot;findPets&quot;,
        &quot;produces&quot;: [
          &quot;application/json&quot;,
          &quot;application/xml&quot;,
          &quot;text/xml&quot;,
          &quot;text/html&quot;
        ],
        &quot;parameters&quot;: [
          {
            &quot;name&quot;: &quot;tags&quot;,
            &quot;in&quot;: &quot;query&quot;,
            &quot;description&quot;: &quot;tags to filter by&quot;,
            &quot;required&quot;: false,
            &quot;type&quot;: &quot;array&quot;,
            &quot;items&quot;: {
              &quot;type&quot;: &quot;string&quot;
            },
            &quot;collectionFormat&quot;: &quot;csv&quot;
          },
          {
            &quot;name&quot;: &quot;limit&quot;,
            &quot;in&quot;: &quot;query&quot;,
            &quot;description&quot;: &quot;maximum number of results to return&quot;,
            &quot;required&quot;: false,
            &quot;type&quot;: &quot;integer&quot;,
            &quot;format&quot;: &quot;int32&quot;
          }
        ],
        &quot;responses&quot;: {
          &quot;200&quot;: {
            &quot;description&quot;: &quot;pet response&quot;,
            &quot;schema&quot;: {
              &quot;type&quot;: &quot;array&quot;,
              &quot;items&quot;: {
                &quot;$ref&quot;: &quot;#/definitions/Pet&quot;
              }
            }
          },
          &quot;default&quot;: {
            &quot;description&quot;: &quot;unexpected error&quot;,
            &quot;schema&quot;: {
              &quot;$ref&quot;: &quot;#/definitions/ErrorModel&quot;
            }
          }
        }
      },
      &quot;post&quot;: {
        &quot;description&quot;: &quot;Creates a new pet in the store.  Duplicates are allowed&quot;,
        &quot;operationId&quot;: &quot;addPet&quot;,
        &quot;produces&quot;: [&quot;application/json&quot;],
        &quot;parameters&quot;: [
          {
            &quot;name&quot;: &quot;pet&quot;,
            &quot;in&quot;: &quot;body&quot;,
            &quot;description&quot;: &quot;Pet to add to the store&quot;,
            &quot;required&quot;: true,
            &quot;schema&quot;: {
              &quot;$ref&quot;: &quot;#/definitions/NewPet&quot;
            }
          }
        ],
        &quot;responses&quot;: {
          &quot;200&quot;: {
            &quot;description&quot;: &quot;pet response&quot;,
            &quot;schema&quot;: {
              &quot;$ref&quot;: &quot;#/definitions/Pet&quot;
            }
          },
          &quot;default&quot;: {
            &quot;description&quot;: &quot;unexpected error&quot;,
            &quot;schema&quot;: {
              &quot;$ref&quot;: &quot;#/definitions/ErrorModel&quot;
            }
          }
        }
      }
    },
    &quot;/pets/{id}&quot;: {
      &quot;get&quot;: {
        &quot;description&quot;: &quot;Returns a user based on a single ID, if the user does not have access to the pet&quot;,
        &quot;operationId&quot;: &quot;findPetById&quot;,
        &quot;produces&quot;: [
          &quot;application/json&quot;,
          &quot;application/xml&quot;,
          &quot;text/xml&quot;,
          &quot;text/html&quot;
        ],
        &quot;parameters&quot;: [
          {
            &quot;name&quot;: &quot;id&quot;,
            &quot;in&quot;: &quot;path&quot;,
            &quot;description&quot;: &quot;ID of pet to fetch&quot;,
            &quot;required&quot;: true,
            &quot;type&quot;: &quot;integer&quot;,
            &quot;format&quot;: &quot;int64&quot;
          }
        ],
        &quot;responses&quot;: {
          &quot;200&quot;: {
            &quot;description&quot;: &quot;pet response&quot;,
            &quot;schema&quot;: {
              &quot;$ref&quot;: &quot;#/definitions/Pet&quot;
            }
          },
          &quot;default&quot;: {
            &quot;description&quot;: &quot;unexpected error&quot;,
            &quot;schema&quot;: {
              &quot;$ref&quot;: &quot;#/definitions/ErrorModel&quot;
            }
          }
        }
      },
      &quot;delete&quot;: {
        &quot;description&quot;: &quot;deletes a single pet based on the ID supplied&quot;,
        &quot;operationId&quot;: &quot;deletePet&quot;,
        &quot;parameters&quot;: [
          {
            &quot;name&quot;: &quot;id&quot;,
            &quot;in&quot;: &quot;path&quot;,
            &quot;description&quot;: &quot;ID of pet to delete&quot;,
            &quot;required&quot;: true,
            &quot;type&quot;: &quot;integer&quot;,
            &quot;format&quot;: &quot;int64&quot;
          }
        ],
        &quot;responses&quot;: {
          &quot;204&quot;: {
            &quot;description&quot;: &quot;pet deleted&quot;
          },
          &quot;default&quot;: {
            &quot;description&quot;: &quot;unexpected error&quot;,
            &quot;schema&quot;: {
              &quot;$ref&quot;: &quot;#/definitions/ErrorModel&quot;
            }
          }
        }
      }
    }
  },
  &quot;definitions&quot;: {
    &quot;Pet&quot;: {
      &quot;type&quot;: &quot;object&quot;,
      &quot;allOf&quot;: [
        {
          &quot;$ref&quot;: &quot;#/definitions/NewPet&quot;
        },
        {
          &quot;required&quot;: [&quot;id&quot;],
          &quot;properties&quot;: {
            &quot;id&quot;: {
              &quot;type&quot;: &quot;integer&quot;,
              &quot;format&quot;: &quot;int64&quot;
            }
          }
        }
      ]
    },
    &quot;NewPet&quot;: {
      &quot;type&quot;: &quot;object&quot;,
      &quot;required&quot;: [&quot;name&quot;],
      &quot;properties&quot;: {
        &quot;name&quot;: {
          &quot;type&quot;: &quot;string&quot;
        },
        &quot;tag&quot;: {
          &quot;type&quot;: &quot;string&quot;
        }
      }
    },
    &quot;ErrorModel&quot;: {
      &quot;type&quot;: &quot;object&quot;,
      &quot;required&quot;: [&quot;code&quot;, &quot;message&quot;],
      &quot;properties&quot;: {
        &quot;code&quot;: {
          &quot;type&quot;: &quot;integer&quot;,
          &quot;format&quot;: &quot;int32&quot;
        },
        &quot;message&quot;: {
          &quot;type&quot;: &quot;string&quot;
        }
      }
    }
  }
}
</code></pre><p>We&#x27;ll tweak our <code>NSwag.csproj</code> file to ensure that the <code>json</code> file is included in our build output:</p><pre><code class="language-xml">&lt;Project Sdk=&quot;Microsoft.NET.Sdk&quot;&gt;
  &lt;!-- ... ---&gt;
  &lt;ItemGroup&gt;
    &lt;Content Include=&quot;**\*.json&quot;&gt;
      &lt;CopyToOutputDirectory&gt;PreserveNewest&lt;/CopyToOutputDirectory&gt;
    &lt;/Content&gt;
  &lt;/ItemGroup&gt;
&lt;/Project&gt;
</code></pre><p>This will give us a console app with a reference to NSwag. Now we&#x27;ll flesh out the <code>Program.cs</code> file thusly:</p><pre><code class="language-cs">using System;
using System.IO;
using System.Reflection;
using System.Threading.Tasks;
using NJsonSchema;
using NJsonSchema.Visitors;
using NSwag.CodeGeneration.CSharp;

namespace NSwag {
    class Program {
        static async Task Main(string[] args) {
            Console.WriteLine(&quot;Generating client...&quot;);
            await ClientGenerator.GenerateCSharpClient();
            Console.WriteLine(&quot;Generated client.&quot;);
        }
    }

    public static class ClientGenerator {

        public async static Task GenerateCSharpClient() =&gt;
            GenerateClient(
                // https://github.com/OAI/OpenAPI-Specification/blob/main/examples/v2.0/json/petstore-simple.json
                document: await GetDocumentFromFile(&quot;petstore-simple.json&quot;),
                generatedLocation: &quot;GeneratedClient.cs&quot;,
                generateCode: (OpenApiDocument document) =&gt; {
                    var settings = new CSharpClientGeneratorSettings();

                    var generator = new CSharpClientGenerator(document, settings);
                    var code = generator.GenerateFile();
                    return code;
                }
            );

        private static void GenerateClient(OpenApiDocument document, string generatedLocation, Func&lt;OpenApiDocument, string&gt; generateCode) {
            var root = Path.GetDirectoryName(Assembly.GetEntryAssembly().Location);
            var location = Path.GetFullPath(Path.Join(root, @&quot;../../../&quot;, generatedLocation));

            Console.WriteLine($&quot;Generating {location}...&quot;);

            var code = generateCode(document);

            System.IO.File.WriteAllText(location, code);
        }

        private static async Task&lt;OpenApiDocument&gt; GetDocumentFromFile(string swaggerJsonFilePath) {
            var root = Path.GetDirectoryName(Assembly.GetEntryAssembly().Location);
            var swaggerJson = await File.ReadAllTextAsync(Path.GetFullPath(Path.Join(root, swaggerJsonFilePath)));
            var document = await OpenApiDocument.FromJsonAsync(swaggerJson);

            return document;
        }
    }
}
</code></pre><p>If we perform a <code>dotnet run</code> we now pump out a <code>GeneratedClient.cs</code> file which is a C# client library for the pet store. Fabulous.</p><p>So far so dandy. We&#x27;re taking an Open API <code>json</code> file and generating a C# client library from it.</p><h2>When properties collide</h2><p>It&#x27;s time to break things. We&#x27;re presently generating a <code>Pet</code> class that looks like this:</p><pre><code class="language-cs">[System.CodeDom.Compiler.GeneratedCode(&quot;NJsonSchema&quot;, &quot;10.5.2.0 (Newtonsoft.Json v13.0.0.0)&quot;)]
public partial class Pet : NewPet
{
    [Newtonsoft.Json.JsonProperty(&quot;id&quot;, Required = Newtonsoft.Json.Required.Always)]
    public long Id { get; set; }
}
</code></pre><p>We&#x27;re going to take our <code>Pet</code> definition in the <code>petstore-simple.json</code> file, and add a new <code>@id</code> property alongside the <code>id</code> property:</p><pre><code class="language-json">&quot;Pet&quot;: {
    &quot;type&quot;: &quot;object&quot;,
    &quot;allOf&quot;: [
        {
            &quot;$ref&quot;: &quot;#/definitions/NewPet&quot;
        },
        {
            &quot;required&quot;: [
                &quot;id&quot;
            ],
            &quot;properties&quot;: {
                &quot;id&quot;: {
                    &quot;type&quot;: &quot;integer&quot;,
                    &quot;format&quot;: &quot;int64&quot;
                },
                &quot;@id&quot;: {
                    &quot;type&quot;: &quot;integer&quot;,
                    &quot;format&quot;: &quot;int64&quot;
                }
            }
        }
    ]
},
</code></pre><p>For why? Whilst this may seem esoteric, this is a scenario that can present. It&#x27;s not unknown to encounter properties which are identical, save for an <code>@</code> prefix. This is often the case for meta-properties.</p><p>What do we get if we run our generator over that?</p><pre><code class="language-cs">[System.CodeDom.Compiler.GeneratedCode(&quot;NJsonSchema&quot;, &quot;10.5.2.0 (Newtonsoft.Json v13.0.0.0)&quot;)]
public partial class Pet : NewPet
{
    [Newtonsoft.Json.JsonProperty(&quot;id&quot;, Required = Newtonsoft.Json.Required.Always)]
    public long Id { get; set; }

    [Newtonsoft.Json.JsonProperty(&quot;@id&quot;, Required = Newtonsoft.Json.Required.Always)]
    public long Id { get; set; }
}
</code></pre><p>We get code that doesn&#x27;t compile. You can&#x27;t have two properties in a C# class with the same name. You also cannot have <code>@</code> as a character in a C# property or variable name. To quote the <a href="https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/tokens/verbatim">docs</a>:</p><blockquote><p>The @ special character serves as a verbatim identifier.</p></blockquote><p>It so happens that, by default, NSwag purges <code>@</code> characters from property names. If there isn&#x27;t another property which is named the same save for an <code>@</code> prefix, this is a fine strategy. If there is, as for us now, you&#x27;re toast.</p><p>There&#x27;s a workaround. We&#x27;ll create a new <code>HandleAtCSharpPropertyNameGenerator</code> class:</p><pre><code class="language-cs">/// &lt;summary&gt;
/// Replace characters which will not comply with C# syntax with something that will
/// &lt;/summary&gt;
public class HandleAtCSharpPropertyNameGenerator : NJsonSchema.CodeGeneration.IPropertyNameGenerator {
    /// &lt;summary&gt;Generates the property name.&lt;/summary&gt;
    /// &lt;param name=&quot;property&quot;&gt;The property.&lt;/param&gt;
    /// &lt;returns&gt;The new name.&lt;/returns&gt;
    public virtual string Generate(JsonSchemaProperty property) =&gt;
        ConversionUtilities.ConvertToUpperCamelCase(property.Name
            .Replace(&quot;\&quot;&quot;, string.Empty)
            .Replace(&quot;@&quot;, &quot;__&quot;) // make &quot;@&quot; =&gt; &quot;__&quot;, so &quot;@type&quot; =&gt; &quot;__type&quot;
            .Replace(&quot;?&quot;, string.Empty)
            .Replace(&quot;$&quot;, string.Empty)
            .Replace(&quot;[&quot;, string.Empty)
            .Replace(&quot;]&quot;, string.Empty)
            .Replace(&quot;(&quot;, &quot;_&quot;)
            .Replace(&quot;)&quot;, string.Empty)
            .Replace(&quot;.&quot;, &quot;-&quot;)
            .Replace(&quot;=&quot;, &quot;-&quot;)
            .Replace(&quot;+&quot;, &quot;plus&quot;), true)
            .Replace(&quot;*&quot;, &quot;Star&quot;)
            .Replace(&quot;:&quot;, &quot;_&quot;)
            .Replace(&quot;-&quot;, &quot;_&quot;)
            .Replace(&quot;#&quot;, &quot;_&quot;);
}
</code></pre><p>This is a replacement for the <code>CSharpPropertyNameGenerator</code> that NSwag ships with. Rather than purging the <code>@</code> character, it replaces usage with a double underscore: <code>__</code>.</p><p>We&#x27;ll make use of our new <code>PropertyNameGenerator</code>:</p><pre><code class="language-cs">public async static Task GenerateCSharpClient() =&gt;
    GenerateClient(
        // https://github.com/OAI/OpenAPI-Specification/blob/main/examples/v2.0/json/petstore-simple.json
        document: await GetDocumentFromFile(&quot;petstore-simple.json&quot;),
        generatedLocation: &quot;GeneratedClient.cs&quot;,
        generateCode: (OpenApiDocument document) =&gt; {
            var settings = new CSharpClientGeneratorSettings {
                CSharpGeneratorSettings = {
                    PropertyNameGenerator = new HandleAtCSharpPropertyNameGenerator() // @ shouldn&#x27;t cause us problems
                }
            };

            var generator = new CSharpClientGenerator(document, settings);
            var code = generator.GenerateFile();
            return code;
        }
    );
</code></pre><p>With this in place, when we <code>dotnet run</code> we create a class that looks like this:</p><pre><code class="language-cs">[System.CodeDom.Compiler.GeneratedCode(&quot;NJsonSchema&quot;, &quot;10.5.2.0 (Newtonsoft.Json v13.0.0.0)&quot;)]
public partial class Pet : NewPet
{
    [Newtonsoft.Json.JsonProperty(&quot;id&quot;, Required = Newtonsoft.Json.Required.Always)]
    public long Id { get; set; }

    [Newtonsoft.Json.JsonProperty(&quot;@id&quot;, Required = Newtonsoft.Json.Required.Always)]
    public long __id { get; set; }
}
</code></pre><p>So the newly generated property name is <code>__id</code> rather than the clashing <code>Id</code>. Rather wonderfully, this works. It resolves the issue we faced. We&#x27;ve chosen to use <code>__</code> as our prefix - we could choose something else if that worked better for us.</p><p>Knowing that this hook exists is super useful.</p><h2>Use <code>decimal</code> not <code>double</code> for floating point numbers</h2><p>Another common problem with generated C# clients is the number type used to represent floating point numbers. The default for C# is <code>double</code>.</p><p>This is a reasonable choice when you consider the <a href="https://swagger.io/docs/specification/data-models/data-types/#numbers">official format</a> for highly precise floating point numbers is <code>double</code>:</p><blockquote><p>OpenAPI has two numeric types, <code>number</code> and <code>integer</code>, where <code>number</code> includes both integer and floating-point numbers. An optional <code>format</code> keyword serves as a hint for the tools to use a specific numeric type:</p><p><code>float</code> - Floating-point numbers.
<code>double</code> - Floating-point numbers with double precision.</p></blockquote><p>Let&#x27;s tweak our pet definition to reflect this:</p><pre><code class="language-json">&quot;Pet&quot;: {
    &quot;type&quot;: &quot;object&quot;,
    &quot;allOf&quot;: [
        {
            &quot;$ref&quot;: &quot;#/definitions/NewPet&quot;
        },
        {
            &quot;required&quot;: [
                &quot;id&quot;
            ],
            &quot;properties&quot;: {
                &quot;id&quot;: {
                    &quot;type&quot;: &quot;number&quot;,
                    &quot;format&quot;: &quot;double&quot;
                },
                &quot;@id&quot;: {
                    &quot;type&quot;: &quot;number&quot;,
                    &quot;format&quot;: &quot;double&quot;
                }
            }
        }
    ]
},
</code></pre><p>With this in place, when we <code>dotnet run</code> we create a class that looks like this:</p><pre><code class="language-cs">[System.CodeDom.Compiler.GeneratedCode(&quot;NJsonSchema&quot;, &quot;10.5.2.0 (Newtonsoft.Json v13.0.0.0)&quot;)]
public partial class Pet : NewPet
{
    [Newtonsoft.Json.JsonProperty(&quot;id&quot;, Required = Newtonsoft.Json.Required.Always)]
    public double Id { get; set; }

    [Newtonsoft.Json.JsonProperty(&quot;@id&quot;, Required = Newtonsoft.Json.Required.Always)]
    public double __id { get; set; }
}
</code></pre><p>C# developers may well rather work with a <a href="https://docs.microsoft.com/en-us/dotnet/api/system.decimal?view=net-5.0"><code>decimal</code></a> type which can handle &quot;financial calculations that require large numbers of significant integral and fractional digits and no round-off errors&quot;.</p><p>There is a way to switch from using <code>double</code> to <code>decimal</code> in your generated clients. I&#x27;ve been using the approach for some years, and I suspect I first adapted it from <a href="https://github.com/RicoSuter/NSwag/issues/1814#issuecomment-448752684">a comment on GitHub</a>.</p><p>It uses the <a href="https://en.m.wikipedia.org/wiki/Visitor_pattern">visitor pattern</a> and looks like this:</p><pre><code class="language-cs">/// &lt;summary&gt;
/// By default the C# decimal number type used is double; this makes it decimal
/// &lt;/summary&gt;
public class DoubleToDecimalVisitor : JsonSchemaVisitorBase {
    protected override JsonSchema VisitSchema(JsonSchema schema, string path, string typeNameHint) {
        if (schema.Type == JsonObjectType.Number)
            schema.Format = JsonFormatStrings.Decimal;

        return schema;
    }
}
</code></pre><p>The code above, when invoked upon our <code>OpenApiDocument</code>, changes the format of all number types to be <code>decimal</code>. Which results in code along these lines:</p><pre><code class="language-cs">[System.CodeDom.Compiler.GeneratedCode(&quot;NJsonSchema&quot;, &quot;10.5.2.0 (Newtonsoft.Json v13.0.0.0)&quot;)]
public partial class Pet : NewPet
{
    [Newtonsoft.Json.JsonProperty(&quot;id&quot;, Required = Newtonsoft.Json.Required.Always)]
    public decimal Id { get; set; }

    [Newtonsoft.Json.JsonProperty(&quot;@id&quot;, Required = Newtonsoft.Json.Required.Always)]
    public decimal __id { get; set; }
}
</code></pre><p>If we take all the code, and put it together, we end up with this:</p><pre><code class="language-cs">using System;
using System.IO;
using System.Reflection;
using System.Threading.Tasks;
using NJsonSchema;
using NJsonSchema.Visitors;
using NSwag.CodeGeneration.CSharp;

namespace NSwag {
    class Program {
        static async Task Main(string[] args) {
            Console.WriteLine(&quot;Generating client...&quot;);
            await ClientGenerator.GenerateCSharpClient();
            Console.WriteLine(&quot;Generated client.&quot;);
        }
    }

    public static class ClientGenerator {

        public async static Task GenerateCSharpClient() =&gt;
            GenerateClient(
                // https://github.com/OAI/OpenAPI-Specification/blob/main/examples/v2.0/json/petstore-simple.json
                document: await GetDocumentFromFile(&quot;petstore-simple.json&quot;),
                generatedLocation: &quot;GeneratedClient.cs&quot;,
                generateCode: (OpenApiDocument document) =&gt; {
                    new DoubleToDecimalVisitor().Visit(document); // we want decimals not doubles

                    var settings = new CSharpClientGeneratorSettings {
                        CSharpGeneratorSettings = {
                            PropertyNameGenerator = new HandleAtCSharpPropertyNameGenerator() // @ shouldn&#x27;t cause us problems
                        }
                    };

                    var generator = new CSharpClientGenerator(document, settings);
                    var code = generator.GenerateFile();
                    return code;
                }
            );

        private static void GenerateClient(OpenApiDocument document, string generatedLocation, Func&lt;OpenApiDocument, string&gt; generateCode) {
            var root = Path.GetDirectoryName(Assembly.GetEntryAssembly().Location);
            var location = Path.GetFullPath(Path.Join(root, @&quot;../../../&quot;, generatedLocation));

            Console.WriteLine($&quot;Generating {location}...&quot;);

            var code = generateCode(document);

            System.IO.File.WriteAllText(location, code);
        }

        private static async Task&lt;OpenApiDocument&gt; GetDocumentFromFile(string swaggerJsonFilePath) {
            var root = Path.GetDirectoryName(Assembly.GetEntryAssembly().Location);
            var swaggerJson = await File.ReadAllTextAsync(Path.GetFullPath(Path.Join(root, swaggerJsonFilePath)));
            var document = await OpenApiDocument.FromJsonAsync(swaggerJson);

            return document;
        }
    }

    /// &lt;summary&gt;
    /// By default the C# decimal number type used is double; this makes it decimal
    /// &lt;/summary&gt;
    public class DoubleToDecimalVisitor : JsonSchemaVisitorBase {
        protected override JsonSchema VisitSchema(JsonSchema schema, string path, string typeNameHint) {
            if (schema.Type == JsonObjectType.Number)
                schema.Format = JsonFormatStrings.Decimal;

            return schema;
        }
    }

    /// &lt;summary&gt;
    /// Replace characters which will not comply with C# syntax with something that will
    /// &lt;/summary&gt;
    public class HandleAtCSharpPropertyNameGenerator : NJsonSchema.CodeGeneration.IPropertyNameGenerator {
        /// &lt;summary&gt;Generates the property name.&lt;/summary&gt;
        /// &lt;param name=&quot;property&quot;&gt;The property.&lt;/param&gt;
        /// &lt;returns&gt;The new name.&lt;/returns&gt;
        public virtual string Generate(JsonSchemaProperty property) =&gt;
            ConversionUtilities.ConvertToUpperCamelCase(property.Name
                .Replace(&quot;\&quot;&quot;, string.Empty)
                .Replace(&quot;@&quot;, &quot;__&quot;) // make &quot;@&quot; =&gt; &quot;__&quot;, so &quot;@type&quot; =&gt; &quot;__type&quot;
                .Replace(&quot;?&quot;, string.Empty)
                .Replace(&quot;$&quot;, string.Empty)
                .Replace(&quot;[&quot;, string.Empty)
                .Replace(&quot;]&quot;, string.Empty)
                .Replace(&quot;(&quot;, &quot;_&quot;)
                .Replace(&quot;)&quot;, string.Empty)
                .Replace(&quot;.&quot;, &quot;-&quot;)
                .Replace(&quot;=&quot;, &quot;-&quot;)
                .Replace(&quot;+&quot;, &quot;plus&quot;), true)
                .Replace(&quot;*&quot;, &quot;Star&quot;)
                .Replace(&quot;:&quot;, &quot;_&quot;)
                .Replace(&quot;-&quot;, &quot;_&quot;)
                .Replace(&quot;#&quot;, &quot;_&quot;);
    }
}
</code></pre><h2>Conclusion</h2><p>This post takes the tremendous NSwag, and demonstrates a mechanism for using it to create C# clients from an Open API / Swagger documents which:</p><ul><li>can handle property names with an <code>@</code> prefix which might collide with the same property without the prefix</li><li>use <code>decimal</code> as the preferred number type for floating point numbers</li></ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Docusaurus, meta tags and Google Discover]]></title>
            <link>https://blog.johnnyreilly.com/2021/10/18/docusaurus-meta-tags-and-google-discover</link>
            <guid>Docusaurus, meta tags and Google Discover</guid>
            <pubDate>Mon, 18 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Google Discover is a way that people can find your content. To make your content more attractive, Google encourage using high quality images which are enabled by setting the max-image-preview:large meta tag. This post shows you how to achieve that with Docusaurus.]]></description>
            <content:encoded><![CDATA[<p>Google Discover is a way that people can find your content. To make your content more attractive, Google encourage using high quality images which are enabled by setting the <code>max-image-preview:large</code> meta tag. This post shows you how to achieve that with Docusaurus.</p><p><img src="../static/blog/2021-10-18-docusaurus-meta-tags-and-google-discover/title-image.png" alt="title image reading &quot;Docusaurus, meta tags and Google Discover&quot; with a Docusaurus logo and the Google Discover phone photo taken from https://developers.google.com/search/docs/advanced/mobile/google-discover"/></p><h2>Google Discover</h2><p>I&#x27;m an Android user. Google Discover will present articles to me in various places on my phone. <a href="https://developers.google.com/search/docs/advanced/mobile/google-discover">According to the docs</a>:</p><blockquote><p>With Discover, you can get updates for your interests, like your favorite sports team or news site, without searching for them. You can choose the types of updates you want to see in Discover in the Google app or when you’re browsing the web on your phone.</p></blockquote><p>It turns out that my own content is showing up in Discover. I (ahem) discovered this by looking at the Google search console and noticing a &quot;Discover&quot; tab:</p><p><img src="../static/blog/2021-10-18-docusaurus-meta-tags-and-google-discover/screenshot-of-discover-in-search-console.png" alt="screenshot of the Google search console featuring a &quot;discover&quot; image"/></p><p>As I read up about Discover I noticed this:</p><blockquote><p>To increase the likelihood of your content appearing in Discover, we recommend the following:
...</p><ul><li>Include compelling, high-quality images in your content, especially large images that are more likely to generate visits from Discover. Large images need to be at least 1200 px wide and enabled by the <code>max-image-preview:large</code> setting...</li></ul></blockquote><p>I was already trying to include images with my blog posts as described... But <code>max-image-preview:large</code> was news to me. <a href="https://developers.google.com/search/docs/advanced/robots/robots_meta_tag#max-image-preview">Reading up further</a> revealed that the &quot;setting&quot; was simply a meta tag to be added to the HTML that looked like this:</p><pre><code class="language-html">&lt;meta name=&quot;robots&quot; content=&quot;max-image-preview:standard&quot; /&gt;
</code></pre><p>Incidentally, applying this setting will affect all forms of search results. So not just Discover, but Google web search, Google Images and Assistant as well. The result of having this meta tag will be that bigger images are displayed in search results, which should make the content more attractive.</p><h2>Docusaurus let&#x27;s get meta</h2><p>Now we understand what we want (an extra meta tag on all our pages), how do we apply this to Docusaurus?</p><p>Well, it&#x27;s remarkably simple. There&#x27;s an optional <a href="https://docusaurus.io/docs/api/themes/configuration#metadatas"><code>metadatas</code></a> property in <code>docusaurus.config.js</code>. This property allows you to configure additional html metadatas (and override existing ones). The property is an array of <code>Metadata</code>, each entry of which will be directly passed to the <code>&lt;meta /&gt;</code> tag.</p><p>So in our case we&#x27;d want to pass an object with <code>name: &#x27;robots&#x27;</code> and <code>content: &#x27;max-image-preview:large&#x27;</code> to render our desired meta tag. Which looks like this:</p><pre><code class="language-js">/** @type {import(&#x27;@docusaurus/types&#x27;).DocusaurusConfig} */
module.exports = {
  //...
  themeConfig: {
    // &lt;meta name=&quot;robots&quot; content=&quot;max-image-preview:large&quot;&gt;
    metadatas: [{ name: &#x27;robots&#x27;, content: &#x27;max-image-preview:large&#x27; }],
    //...
  },
  //...
};
</code></pre><p>With that in place, we find our expected <code>meta</code> tag is now part of our rendered HTML:</p><p><img src="../static/blog/2021-10-18-docusaurus-meta-tags-and-google-discover/screenshot-of-meta-tag.png" alt="screenshot of the &lt;meta name=&quot;robots&quot; content=&quot;max-image-preview:large&quot;&gt; tag taken from Chrome Devtools"/></p><h2>Meta meta</h2><p>We should now have a more Google Discover-friendly website which is tremendous!</p><p>Before signing off, here&#x27;s a fun fact: the PR that published this blog post is the <em>same</em> PR that added <code>max-image-preview:standard</code> to my blog. <a href="https://github.com/johnnyreilly/blog.johnnyreilly.com/pull/114">Peep it here</a> - meta in so many ways 😉</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Structured data, SEO and React]]></title>
            <link>https://blog.johnnyreilly.com/2021/10/15/structured-data-seo-and-react</link>
            <guid>Structured data, SEO and React</guid>
            <pubDate>Fri, 15 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[People being able to discover your website when they search is important. This post is about how you can add structured data to a site. Adding structured data will help search engines like Google understand your content, and get it in front of more eyeballs. We'll illustrate this by making a simple React app which incorporates structured data.]]></description>
            <content:encoded><![CDATA[<p>People being able to discover your website when they search is important. This post is about how you can add structured data to a site. Adding structured data will help search engines like Google understand your content, and get it in front of more eyeballs. We&#x27;ll illustrate this by making a simple React app which incorporates structured data.</p><p><img src="../static/blog/2021-10-15-structured-data-seo-and-react/structured-data-seo-and-react.png" alt="title image reading &quot;Structured data, SEO and React&quot; with a screenshot of the rich results tool in the background"/></p><h2>Updated 28th October 2021</h2><p>This blog evolved to become a talk:</p><iframe width="560" height="315" src="https://www.youtube.com/embed/zi1CHB-eVck?start=282" title="YouTube video player" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"></iframe><h2>What is structured data?</h2><p>Google, DuckDuckGo and others are proficient at understanding the content of websites. However, scraping HTML is not a highly reliable way to categorise content. HTML is about presentation and it can have all manner of different structures. To make the life of search engines easier, there&#x27;s a standardized format known as &quot;structured data&quot; which can be embedded within a page. That standardized format allows you to explicitly declare the type of content the page contains.</p><p>So let&#x27;s say you&#x27;ve written an article, you can reliably state in a language that Google understands &quot;this page is an article, it has this title, this description and image and was published on this date&quot;. There are hundreds of types of structured data available, and you can read about all of them in depth at <a href="https://schema.org/">https://schema.org/</a> which is maintained by representatives of the search engine community.</p><p>It&#x27;s worth knowing that whilst there are many types of structured data available to choose from, there are definitely more popular options and those that are more niche. So <a href="https://schema.org/Article">Article</a> is likely to be used a great deal more than, perhaps, <a href="https://schema.org/MolecularEntity">MolecularEntity</a>.</p><p>As well as there being different types of structured data, there also a variety of formats which can be used to provide it; these include <a href="http://json-ld.org/">JSON-LD</a>, <a href="https://www.w3.org/TR/microdata/">Microdata</a> and <a href="https://rdfa.info/">RDFa</a>. Google explicitly prefer JSON-LD and so that&#x27;s what we&#x27;ll focus on. JSON-LD is effectively a rending of a piece of JSON inside a <code>script</code> tag with the custom type of <code>application/ld+json</code>. For example:</p><pre><code class="language-html">&lt;script type=&quot;application/ld+json&quot;&gt;
  {
    &quot;@context&quot;: &quot;https://schema.org/&quot;,
    &quot;@type&quot;: &quot;Recipe&quot;,
    &quot;name&quot;: &quot;Chocolate Brownie&quot;,
    &quot;author&quot;: {
      &quot;@type&quot;: &quot;Person&quot;,
      &quot;name&quot;: &quot;John Reilly&quot;
    },
    &quot;datePublished&quot;: &quot;2014-09-01&quot;,
    &quot;description&quot;: &quot;The most amazing chocolate brownie recipe&quot;,
    &quot;prepTime&quot;: &quot;PT60M&quot;
  }
&lt;/script&gt;
</code></pre><h2>Structured data in action</h2><p>Whilst structured data is helpful for search engines in general, it can also make a difference to the way your content is rendered <em>inside</em> search results. For instance, let&#x27;s search for &quot;best brownie recipe&quot; in Google and see what shows up:</p><p><img src="../static/blog/2021-10-15-structured-data-seo-and-react/screenshot-of-rich-text-results.png" alt="screenshot of google search results for &quot;best brownie recipe&quot; including a rich text results set at the top of the list showing recipes from various sources"/></p><p>When you look at the screenshot above, you&#x27;ll notice that at the top of the list (before the main search results) there&#x27;s a carousel which shows various brownie recipe links, with dedicated pictures, titles and descriptions. Where did this come from? The answer, unsurprisingly, is structured data.</p><p>If we click on the first link, we&#x27;re taken to the recipe in question. Looking at the HTML of that page we find a number of JSON-LD sections:</p><p><img src="../static/blog/2021-10-15-structured-data-seo-and-react/structured-data-in-action.png" alt="screenshot of JSON-LD sections in the BBC Good Food website"/></p><p>If we grab the content of one JSON-LD section and paste it into the devtools console, it becomes much easier to read:</p><p><img src="../static/blog/2021-10-15-structured-data-seo-and-react/single-structured-data-as-JSON.png" alt="screenshot of JSON-LD section transformed into a JavaScript Object Literal"/></p><p>If we look at the <code>@type</code> property we can see it&#x27;s a <code>&quot;Recipe&quot;</code>. This means it&#x27;s an example of the <a href="https://schema.org/Recipe">https://schema.org/Recipe</a> schema. If we look further at the <code>headline</code> property, it reads <code>&quot;Best ever chocolate brownies recipe&quot;</code>. That matches up with headline that was displayed in the search results.</p><p>Now we have a sense of what the various search engines are using as they categorise the page, and we understand exactly what is powering the carousel in the Google search results.</p><p>Incidentally, there&#x27;s a special name for this &quot;carousel&quot;; it is a &quot;rich result&quot;. A rich result is a search result singled out for special treatment when it is displayed. Google provide a <a href="https://search.google.com/test/rich-results">Rich Results Test tool</a> which allows you to validate if a site provides structured data which is eligible to be featured in rich results. We&#x27;ll make use of this later.</p><h2>Adding structured data to a website</h2><p>Now we&#x27;ll make ourselves a React app and add structured data to it. In the console we&#x27;ll execute the following command:</p><pre><code>npx create-react-app my-app
</code></pre><p>We now have a simple React app which consists of a single page. Let&#x27;s replace the content of the existing <code>App.js</code> file with this:</p><pre><code class="language-jsx">//@ts-check
import &#x27;./App.css&#x27;;

function App() {
  // https://schema.org/Article
  const articleStructuredData = {
    &#x27;@context&#x27;: &#x27;https://schema.org&#x27;,
    &#x27;@type&#x27;: &#x27;Article&#x27;,
    headline: &#x27;Structured data for you&#x27;,
    description: &#x27;This is an article that demonstrates structured data.&#x27;,
    image: &#x27;https://upload.wikimedia.org/wikipedia/commons/4/40/JSON-LD.svg&#x27;,
    datePublished: new Date(&#x27;2021-09-04T09:25:01.340Z&#x27;).toISOString(),
    author: {
      &#x27;@type&#x27;: &#x27;Person&#x27;,
      name: &#x27;John Reilly&#x27;,
      url: &#x27;https://twitter.com/johnny_reilly&#x27;,
    },
  };

  return (
    &lt;div className=&quot;App&quot;&gt;
      &lt;script type=&quot;application/ld+json&quot;&gt;
        {JSON.stringify(articleStructuredData)}
      &lt;/script&gt;

      &lt;h1&gt;{articleStructuredData.headline}&lt;/h1&gt;
      &lt;h3&gt;
        by{&#x27; &#x27;}
        &lt;a href={articleStructuredData.author.url}&gt;
          {articleStructuredData.author.name}
        &lt;/a&gt;{&#x27; &#x27;}
        on {articleStructuredData.datePublished}
      &lt;/h3&gt;

      &lt;img
        style={{ width: &#x27;5em&#x27; }}
        alt=&quot;https://json-ld.org/ - Website content released under a Creative Commons CC0 Public Domain Dedication except where an alternate is specified., CC0, via Wikimedia Commons&quot;
        src={articleStructuredData.image}
      /&gt;

      &lt;p&gt;{articleStructuredData.description}&lt;/p&gt;

      &lt;p&gt;Take a look at the source of this page and find the JSON-LD!&lt;/p&gt;
    &lt;/div&gt;
  );
}

export default App;
</code></pre><p>If we look at the code above, we can see we&#x27;re creating a JavaScript object literal named <code>articleStructuredData</code> which contains the data of an <a href="https://schema.org/Article">https://schema.org/Article</a>. <code>articleStructuredData</code> is then used to do two things:</p><ol><li>to contribute to the content of the page</li><li>to render a JSON-LD script tag: <code>&lt;script type=&quot;application/ld+json&quot;&gt;</code> which is populated by calling <code>JSON.stringify(articleStructuredData)</code></li></ol><p>When we run our site locally with <code>npm start</code> we see a simple article site that looks like this:</p><p><img src="../static/blog/2021-10-15-structured-data-seo-and-react/screenshot-of-article.png" alt="screenshot of article page"/></p><p>Now let&#x27;s see if it supports structured data in the way we hope.</p><h2>Using the Rich Results Test</h2><p>If we go to <a href="https://search.google.com/test/rich-results">https://search.google.com/test/rich-results</a> we find the Rich Results Test tool. There&#x27;s two ways you can test; providing a URL or providing code. In our case we don&#x27;t have a public facing URL and so we&#x27;re going to use the HTML that React is rendering.</p><p>In devtools we&#x27;ll use the &quot;copy outerHTML&quot; feature to grab the HTML, then we&#x27;ll paste it into Rich Results:</p><p><img src="../static/blog/2021-10-15-structured-data-seo-and-react/screenshot-of-rich-results-tool.png" alt="screenshot of rich results tool in code view"/></p><p>We hit the &quot;TEST CODE&quot; button and we see results that look like this:</p><p><img src="../static/blog/2021-10-15-structured-data-seo-and-react/screenshot-of-rich-results-tool-test.png" alt="screenshot of the results of testing our site using the rich results tool"/></p><p>So we&#x27;ve been successful in building a website that renders structured data. More than that, we&#x27;re doing it in a way that we know Google will recognise and can use to render rich results in search. That&#x27;s a really useful way to drive traffic to our website.</p><p>This post has illustrated what it looks like to create an <code>Article</code>. Google has some <a href="https://developers.google.com/search/docs/advanced/structured-data/search-gallery">great resources</a> on other types that it supports and prioritises for rich results which should help you build the structured data you need for your particular content.</p><p><a href="https://blog.logrocket.com/react-structured-data-and-seo/">This post was originally published on LogRocket.</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Permissioning Azure Pipelines with Bicep and Azure RBAC Role Assignments]]></title>
            <link>https://blog.johnnyreilly.com/2021/09/12/permissioning-azure-pipelines-bicep-role-assignments</link>
            <guid>Permissioning Azure Pipelines with Bicep and Azure RBAC Role Assignments</guid>
            <pubDate>Sun, 12 Sep 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[How can we deploy resources to Azure, and then run an integration test through them in the context of an Azure Pipeline? This post will show how to do this by permissioning our Azure Pipeline to access these resources using Azure RBAC role assignments. It will also demonstrate a dotnet test that runs in the context of the pipeline and makes use of those role assignments.]]></description>
            <content:encoded><![CDATA[<p>How can we deploy resources to Azure, and then run an integration test through them in the context of an Azure Pipeline? This post will show how to do this by permissioning our Azure Pipeline to access these resources using Azure RBAC role assignments. It will also demonstrate a dotnet test that runs in the context of the pipeline and makes use of those role assignments.</p><p><img src="../static/blog/2021-09-12-permissioning-azure-pipelines-bicep-role-assignments/permissioning-azure-pipelines-with-bicep-and-role-assignments.png" alt="title image reading &quot;Permissioning Azure Pipelines with Bicep and Role Assignments&quot; and some Azure logos"/></p><p>We&#x27;re following this approach as an alternative to <a href="./2021-07-07-output-connection-strings-and-keys-from-azure-bicep.md">exporting connection strings</a>, as these can be viewed in the Azure Portal; which may be an security issue if you have many people who are able to access the portal and view deployment outputs.</p><p>We&#x27;re going to demonstrate this approach using Event Hubs. It&#x27;s worth calling out that this is a generally useful approach which can be applied to any Azure resources that support Azure RBAC Role Assignments. So wherever in this post you read &quot;Event Hubs&quot;, imagine substituting other Azure resources you&#x27;re working with.</p><p>The post will do the following:</p><ul><li>Add Event Hubs to our Azure subscription</li><li>Permission our service connection / service principal</li><li>Deploy to Azure with Bicep</li><li>Write an integration test</li><li>Write a pipeline to bring it all together</li></ul><h2>Add Event Hubs to your subscription</h2><p>First of all, we may need to add Event Hubs to our Azure subscription.</p><p>Without this in place, we may encounter errors of the type:</p><blockquote><p>##<!-- -->[error]<!-- -->MissingSubscriptionRegistration: The subscription is not registered to use namespace &#x27;Microsoft.EventHub&#x27;. See <a href="https://aka.ms/rps-not-found">https://aka.ms/rps-not-found</a> for how to register subscriptions.</p></blockquote><p>We do this by going to &quot;Resource Providers&quot; in the <a href="https://portal.azure.com">Azure Portal</a> and registering the resources you need. Lots are registered by default, but not all.</p><p><img src="../static/blog/2021-09-12-permissioning-azure-pipelines-bicep-role-assignments/screenshot-azure-portal-subscription-resource-providers.png" alt="Screenshot of the Azure Portal, subscriptions -&gt; resource providers section, showing that Event Hubs have been registered"/></p><h2>Permission our service connection / service principal</h2><p>In order that we can run pipelines related to Azure, we mostly need to have an Azure Resource Manager service connection set up in Azure DevOps. Once that exists, we also need to give it a role assignment to allow it to create role assignments of its own when pipelines are running.</p><p>Without this in place, we may encounter errors of the type:</p><blockquote><p>##<!-- -->[error]<!-- -->The template deployment failed with error: &#x27;Authorization failed for template resource &#x27;{GUID-THE-FIRST}&#x27; of type &#x27;Microsoft.Authorization/roleAssignments&#x27;. The client &#x27;{GUID-THE-SECOND}&#x27; with object id &#x27;{GUID-THE-SECOND}&#x27; does not have permission to perform action &#x27;Microsoft.Authorization/roleAssignments/write&#x27; at scope &#x27;/subscriptions/<!-- -->*<!-- -->*<!-- -->*<!-- -->/resourceGroups/johnnyreilly/providers/Microsoft.EventHub/namespaces/evhns-demo/providers/Microsoft.Authorization/roleAssignments/{GUID-THE-FIRST}&#x27;.&#x27;.</p></blockquote><p>Essentially, we want to be able to run pipelines that say &quot;hey Azure, we want to give permissions to our service connection&quot;. We are doing this <em>with</em> the self same service connection, so (chicken and egg) we first need to give it permission to give those commands in future. This is a little confusing; but let&#x27;s role with it. (Pun most definitely intended. 😉)</p><p>To grant that permission / add that role assignment, we go to the service connection in Azure Devops:</p><p><img src="../static/blog/2021-09-12-permissioning-azure-pipelines-bicep-role-assignments/screenshot-azure-devops-service-connection.png" alt="Screenshot of the service connection in Azure DevOps"/></p><p>We can see there&#x27;s two links here; first we&#x27;ll click on &quot;Manage Service Principal&quot;, which will take us to the service principal in the Azure Portal:</p><p><img src="../static/blog/2021-09-12-permissioning-azure-pipelines-bicep-role-assignments/screenshot-azure-portal-service-principal.png" alt="Screenshot of the service principal in the Azure Portal"/></p><p>Take note of the display name of the service principal; we&#x27;ll need that as we click on the &quot;Manage service connection roles&quot; link, which will take us to the resource groups IAM page in the Azure Portal:</p><p><img src="../static/blog/2021-09-12-permissioning-azure-pipelines-bicep-role-assignments/screenshot-azure-portal-service-principal-access-control.png" alt="Screenshot of the resource groups IAM page in the Azure Portal"/></p><p>Here we can click on &quot;Add role assignment&quot;, select &quot;Owner&quot;:</p><p><img src="../static/blog/2021-09-12-permissioning-azure-pipelines-bicep-role-assignments/screenshot-azure-portal-add-role-assignment.png" alt="Screenshot of the add role assignment IAM page in the Azure Portal"/></p><p>Then when selecting members we should be able to look up the service principal to assign it:</p><p><img src="../static/blog/2021-09-12-permissioning-azure-pipelines-bicep-role-assignments/screenshot-azure-portal-add-role-assignment-member.png" alt="Screenshot of the add role assignment select member IAM page in the Azure Portal"/></p><p>We now have a service connection which we should be able to use for granting permissions / role assignments, which is what we need.</p><h2>Event Hub and Role Assignment with Bicep</h2><p>Next we want a Bicep file that will, when run, provision an Event Hub and a role assignment which will allow our Azure Pipeline (via its service connection) to interact with it.</p><pre><code class="language-bicep">@description(&#x27;Name of the eventhub namespace&#x27;)
param eventHubNamespaceName string

@description(&#x27;Name of the eventhub name&#x27;)
param eventHubName string

@description(&#x27;The service principal&#x27;)
param principalId string

// Create an Event Hub namespace
resource eventHubNamespace &#x27;Microsoft.EventHub/namespaces@2021-01-01-preview&#x27; = {
  name: eventHubNamespaceName
  location: resourceGroup().location
  sku: {
    name: &#x27;Standard&#x27;
    tier: &#x27;Standard&#x27;
    capacity: 1
  }
  properties: {
    zoneRedundant: true
  }
}

// Create an Event Hub inside the namespace
resource eventHub &#x27;Microsoft.EventHub/namespaces/eventhubs@2021-01-01-preview&#x27; = {
  parent: eventHubNamespace
  name: eventHubName
  properties: {
    messageRetentionInDays: 7
    partitionCount: 1
  }
}

// give Azure Pipelines Service Principal permissions against the Event Hub

var roleDefinitionAzureEventHubsDataOwner = subscriptionResourceId(&#x27;Microsoft.Authorization/roleDefinitions&#x27;, &#x27;f526a384-b230-433a-b45c-95f59c4a2dec&#x27;)

resource integrationTestEventHubReceiverNamespaceRoleAssignment &#x27;Microsoft.Authorization/roleAssignments@2018-01-01-preview&#x27; = {
  name: guid(principalId, eventHub.id, roleDefinitionAzureEventHubsDataOwner)
  scope: eventHubNamespace
  properties: {
    roleDefinitionId: roleDefinitionAzureEventHubsDataOwner
    principalId: principalId
  }
}
</code></pre><p>Do note that our bicep template takes the service principal id as a parameter. We&#x27;re going to supply this later from our Azure Pipeline.</p><h2>Our test</h2><p>We&#x27;re now going to write a dotnet integration test which will make use of the infrastructure deployed by our Bicep template. Let&#x27;s create a new test project:</p><pre><code>mkdir src
cd src
dotnet new xunit -o IntegrationTests
cd IntegrationTests
dotnet add package Azure.Identity
dotnet add package Azure.Messaging.EventHubs
dotnet add package FluentAssertions
dotnet add package Microsoft.Extensions.Configuration.EnvironmentVariables
</code></pre><p>We&#x27;ll create a test file called <code>EventHubTest.cs</code> with these contents:</p><pre><code class="language-cs">using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading;
using System.Threading.Tasks;
using Azure.Identity;
using Azure.Messaging.EventHubs;
using Azure.Messaging.EventHubs.Consumer;
using Azure.Messaging.EventHubs.Producer;
using FluentAssertions;
using Microsoft.Extensions.Configuration;
using Newtonsoft.Json;
using Xunit;
using Xunit.Abstractions;

namespace IntegrationTests
{
    public record EchoMessage(string Id, string Message, DateTime Timestamp);

    public class EventHubTest
    {
        private readonly ITestOutputHelper _output;

        public EventHubTest(ITestOutputHelper output)
        {
            _output = output;
        }

        [Fact]
        public async Task Can_post_message_to_event_hub_and_read_it_back()
        {
            // ARRANGE
            var configuration = new ConfigurationBuilder()
                .AddEnvironmentVariables()
                .Build();

            // populated by variables specified in the Azure Pipeline
            var eventhubNamespaceName = configuration[&quot;EVENTHUBNAMESPACENAME&quot;];
            eventhubNamespaceName.Should().NotBeNull();
            var eventhubName = configuration[&quot;EVENTHUBNAME&quot;];
            eventhubName.Should().NotBeNull();
            var tenantId = configuration[&quot;TENANTID&quot;];
            tenantId.Should().NotBeNull();

            // populated as a consequence of the addSpnToEnvironment in the azure-pipelines.yml
            var servicePrincipalId = configuration[&quot;SERVICEPRINCIPALID&quot;];
            servicePrincipalId.Should().NotBeNull();
            var servicePrincipalKey = configuration[&quot;SERVICEPRINCIPALKEY&quot;];
            servicePrincipalKey.Should().NotBeNull();

            var fullyQualifiedNamespace = $&quot;{eventhubNamespaceName}.servicebus.windows.net&quot;;

            var clientCredential = new ClientSecretCredential(tenantId, servicePrincipalId, servicePrincipalKey);
            var eventHubClient = new EventHubProducerClient(
                fullyQualifiedNamespace: fullyQualifiedNamespace,
                eventHubName: eventhubName,
                credential: clientCredential
            );
            var ourGuid = Guid.NewGuid().ToString();
            var now = DateTime.UtcNow;
            var sentEchoMessage = new EchoMessage(Id: ourGuid, Message: $&quot;Test message&quot;, Timestamp: now);
            var sentEventData = new EventData(
                Encoding.UTF8.GetBytes(JsonConvert.SerializeObject(sentEchoMessage))
            );

            // ACT
            await eventHubClient.SendAsync(new List&lt;EventData&gt; { sentEventData }, CancellationToken.None);

            var eventHubConsumerClient = new EventHubConsumerClient(
                consumerGroup: EventHubConsumerClient.DefaultConsumerGroupName,
                fullyQualifiedNamespace: fullyQualifiedNamespace,
                eventHubName: eventhubName,
                credential: clientCredential
            );

            List&lt;PartitionEvent&gt; partitionEvents = new();
            await foreach (var partitionEvent in eventHubConsumerClient.ReadEventsAsync(new ReadEventOptions
            {
                MaximumWaitTime = TimeSpan.FromSeconds(10)
            }))
            {
                if (partitionEvent.Data == null) break;
                _output.WriteLine(Encoding.UTF8.GetString(partitionEvent.Data.EventBody.ToArray()));
                partitionEvents.Add(partitionEvent);
            }

            // ASSERT
            partitionEvents.Count.Should().BeGreaterOrEqualTo(1);
            var firstOne = partitionEvents.FirstOrDefault(evnt =&gt;
              ExtractTypeFromEventBody&lt;EchoMessage&gt;(evnt, _output)?.Id == ourGuid
            );
            var receivedEchoMessage = ExtractTypeFromEventBody&lt;EchoMessage&gt;(firstOne, _output);
            receivedEchoMessage.Should().BeEquivalentTo(sentEchoMessage, because: &quot;the event body should be the same one posted to the message queue&quot;);
        }

        private static T ExtractTypeFromEventBody&lt;T&gt;(PartitionEvent evnt, ITestOutputHelper _output)
        {
            try
            {
                return JsonConvert.DeserializeObject&lt;T&gt;(Encoding.UTF8.GetString(evnt.Data.EventBody.ToArray()));
            }
            catch (JsonException)
            {
                _output.WriteLine(&quot;[&quot; + Encoding.UTF8.GetString(evnt.Data.EventBody.ToArray()) + &quot;] is probably not JSON&quot;);
                return default(T);
            }
        }
    }
}
</code></pre><p>Let&#x27;s talk through what happens in the test above:</p><ol><li>We read in Event Hub connection configuration for the test from environment variables. (These will be supplied by an Azure Pipeline that we will create shortly.)</li><li>We post a message to the Event Hub.</li><li>We read a message back from the Event Hub.</li><li>We confirm that the message we read back matches the one we posted.</li></ol><p>Now that we have our test, we want to be able to execute it. For that we need an Azure Pipeline!</p><h2>Azure Pipeline</h2><p>We&#x27;re going to add an <code>azure-pipelines.yml</code> file which Azure DevOps can use to power a pipeline:</p><pre><code class="language-yml">variables:
  - name: eventHubNamespaceName
    value: evhns-demo
  - name: eventHubName
    value: evh-demo

pool:
  vmImage: ubuntu-latest

steps:
  - task: AzureCLI@2
    displayName: Get Service Principal Id
    inputs:
      azureSubscription: $(serviceConnection)
      scriptType: bash
      scriptLocation: inlineScript
      addSpnToEnvironment: true
      inlineScript: |
        PRINCIPAL_ID=$(az ad sp show --id $servicePrincipalId --query objectId -o tsv)
        echo &quot;##vso[task.setvariable variable=PIPELINE_PRINCIPAL_ID;]$PRINCIPAL_ID&quot;

  - bash: az bicep build --file infra/main.bicep
    displayName: &#x27;Compile Bicep to ARM&#x27;

  - task: AzureResourceManagerTemplateDeployment@3
    name: DeployEventHubInfra
    displayName: Deploy Event Hub infra
    inputs:
      deploymentScope: Resource Group
      azureResourceManagerConnection: $(serviceConnection)
      subscriptionId: $(subscriptionId)
      action: Create Or Update Resource Group
      resourceGroupName: $(azureResourceGroup)
      location: $(location)
      templateLocation: Linked artifact
      csmFile: &#x27;infra/main.json&#x27; # created by bash script
      overrideParameters: &gt;-
        -eventHubNamespaceName $(eventHubNamespaceName)
        -eventHubName $(eventHubName)
        -principalId $(PIPELINE_PRINCIPAL_ID)
      deploymentMode: Incremental

  - task: UseDotNet@2
    displayName: &#x27;Install .NET SDK 5.0.x&#x27;
    inputs:
      packageType: &#x27;sdk&#x27;
      version: 5.0.x

  - task: AzureCLI@2
    displayName: dotnet integration test
    inputs:
      azureSubscription: $(serviceConnection)
      scriptType: pscore
      scriptLocation: inlineScript
      addSpnToEnvironment: true # allows access to service principal details in script
      inlineScript: |
        cd $(Build.SourcesDirectory)/src/IntegrationTests
        dotnet test
</code></pre><p>When the pipeline is run, it does the following:</p><ol><li>Gets the service principal id from the service connection.</li><li>Compiles our Bicep into an ARM template</li><li>Deploys the compiled ARM template to Azure</li><li>Installs the dotnet SDK</li><li>Uses the <a href="https://docs.microsoft.com/en-us/azure/devops/pipelines/tasks/deploy/azure-cli?view=azure-devops">Azure CLI task</a> which allows us to access service principal details in the pipeline to run our dotnet test.</li></ol><p>We&#x27;ll create a pipeline in Azure DevOps pointing to this file, and we&#x27;ll also create the variables that it depends upon:</p><ul><li><code>azureResourceGroup</code> - the name of your resource group in Azure where the app will be deployed</li><li><code>location</code> - where your app is deployed, eg <code>northeurope</code></li><li><code>serviceConnection</code> - the name of your AzureRM service connection in Azure DevOps</li><li><code>subscriptionId</code> - your Azure subscription id from the <a href="https://portal.azure.com">Azure Portal</a></li><li><code>tenantId</code> - the Azure tenant id from the <a href="https://portal.azure.com">Azure Portal</a></li></ul><h2>Running the pipeline</h2><p>Now we&#x27;re ready to run our pipeline:</p><p><img src="../static/blog/2021-09-12-permissioning-azure-pipelines-bicep-role-assignments/screenshot-azure-pipelines-tests-passing.png" alt="screenshot of pipeline running successfully"/></p><p>Here we can see that the pipeline runs and the test passes. That means we&#x27;ve successfully provisioned the Event Hub and permissioned our pipeline to be able to access it using Azure RBAC role assignments. We then wrote a test which used the pipeline credentials to interact with the Event Hub. To see the repo that demostrates this, <a href="https://dev.azure.com/johnnyreilly/blog-demos/_git/permissioning-azure-pipelines-bicep-role-assignments">look here</a>.</p><p>Just to reiterate: we&#x27;ve demonstrated this approach using Event Hubs. This is a generally useful approach which can be applied to any Azure resources that support Azure RBAC Role Assignments.</p><p>Thanks to <a href="https://twitter.com/foldr">Jamie McCrindle</a> for helping out with permissioning the service connection / service principal. <a href="https://foldr.uk/rotating-azure-credentials-in-github-with-terraform">His post on rotating <code>AZURE_CREDENTIALS</code> in GitHub with Terraform</a> provides useful background for those who would like to do similar permissioning using Terraform.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google APIs: authentication with TypeScript]]></title>
            <link>https://blog.johnnyreilly.com/2021/09/10/google-apis-authentication-with-typescript</link>
            <guid>Google APIs: authentication with TypeScript</guid>
            <pubDate>Fri, 10 Sep 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Google has a wealth of APIs which we can interact with. At the time of writing, there's more than two hundred available; including YouTube, Google Calendar and GMail (alongside many others). To integrate with these APIs, it's necessary to authenticate and then use that credential with the API. This post will take you through how to do just that using TypeScript. It will also demonstrate how to use one of those APIs: the Google Calendar API.]]></description>
            <content:encoded><![CDATA[<p>Google has a wealth of APIs which we can interact with. At the time of writing, there&#x27;s more than two hundred available; including YouTube, Google Calendar and GMail (alongside many others). To integrate with these APIs, it&#x27;s necessary to authenticate and then use that credential with the API. This post will take you through how to do just that using TypeScript. It will also demonstrate how to use one of those APIs: the Google Calendar API.</p><h2>Creating an OAuth 2.0 Client ID on the Google Cloud Platform</h2><p>The first thing we need to do is go to the <a href="https://console.cloud.google.com/projectcreate">Google Cloud Platform to create a project</a>. The name of the project doesn&#x27;t matter particularly; although it can be helpful to name the project to align with the API you&#x27;re intending to consume. That&#x27;s what we&#x27;ll do here as we plan to integrate with the Google Calendar API:</p><p><img src="../static/blog/2021-09-10-google-apis-authentication-with-typescript/google-cloud-platform-create-project.png" alt="Screenshot of the Create Project screen in the Google Cloud Platform"/></p><p>The project is the container in which the OAuth 2.0 Client ID will be housed. Now we&#x27;ve created the project, let&#x27;s go to the <a href="https://console.cloud.google.com/apis/credentials">credentials screen</a> and create an OAuth Client ID using the Create Credentials dropdown:</p><p><img src="../static/blog/2021-09-10-google-apis-authentication-with-typescript/create-credentials.png" alt="Screenshot of the Create Credentials dropdown in the Google Cloud Platform"/></p><p>You&#x27;ll likely have to create an OAuth consent screen before you can create the OAuth Client ID. Going through the journey of doing that feels a little daunting as many questions have to be answered. This is because the consent screen can be used for a variety of purposes beyond the API authentication we&#x27;re looking at today.</p><p>When challenged, you can generally accept the defaults and proceed. The user type you&#x27;ll require will be &quot;External&quot;:</p><p><img src="../static/blog/2021-09-10-google-apis-authentication-with-typescript/oauth-consent-screen.png" alt="Screenshot of the OAuth consent screen in the Google Cloud Platform"/></p><p>You&#x27;ll also be required to create an app registration - all that&#x27;s really required here is a name (which can be anything) and your email address:</p><p><img src="../static/blog/2021-09-10-google-apis-authentication-with-typescript/app-registration.png" alt="Screenshot of the OAuth consent screen in the Google Cloud Platform"/></p><p>You don&#x27;t need to worry about scopes. You can either plan to publish the app, or alternately set yourself up to be a test user - you&#x27;ll need to do one of these in order that you can authenticate with the app. Continuing to the end of the journey should provide you with the OAuth consent screen which you need in order that you may then create the OAuth Client ID.</p><p>Creating the OAuth Client ID is slightly confusing as the &quot;Application type&quot; required is &quot;TVs and Limited Input devices&quot;.</p><p><img src="../static/blog/2021-09-10-google-apis-authentication-with-typescript/create-oauth-client-id-type.png" alt="Screenshot of the create OAuth Client ID screen in the Google Cloud Platform"/></p><p>We&#x27;re using this type of application as we want to acquire a <a href="https://oauth.net/2/grant-types/refresh-token/">refresh token</a> which we&#x27;ll be able to use in future to aquire access tokens which will be used to access the Google APIs.</p><p>Once it&#x27;s created, you&#x27;ll be able to download the Client ID from the Google Cloud Platform:</p><p><img src="../static/blog/2021-09-10-google-apis-authentication-with-typescript/oauth-client-id.png" alt="Screenshot of the create OAuth Client ID screen in the Google Cloud Platform"/></p><p>When you download it, it should look something like this:</p><pre><code class="language-json">{
  &quot;installed&quot;: {
    &quot;client_id&quot;: &quot;CLIENT_ID&quot;,
    &quot;project_id&quot;: &quot;PROJECT_ID&quot;,
    &quot;auth_uri&quot;: &quot;https://accounts.google.com/o/oauth2/auth&quot;,
    &quot;token_uri&quot;: &quot;https://oauth2.googleapis.com/token&quot;,
    &quot;auth_provider_x509_cert_url&quot;: &quot;https://www.googleapis.com/oauth2/v1/certs&quot;,
    &quot;client_secret&quot;: &quot;CLIENT_SECRET&quot;,
    &quot;redirect_uris&quot;: [&quot;urn:ietf:wg:oauth:2.0:oob&quot;, &quot;http://localhost&quot;]
  }
}
</code></pre><p>You&#x27;ll need the <code>client_id</code>, <code>client_secret</code> and <code>redirect_uris</code> - but keep them in a safe place and don&#x27;t commit <code>client_id</code> and <code>client_secret</code> to source control!</p><h2>Acquiring a refresh token</h2><p>Now we&#x27;ve got our <code>client_id</code> and <code>client_secret</code>, we&#x27;re ready to write a simple node command line application which we can use to obtain a refresh token. This is actually a multi-stage process that will end up looking like this:</p><ul><li>Provide the Google authentication provider with the <code>client_id</code> and <code>client_secret</code>, in return it will provide an authentication URL.</li><li>Open the authentication URL in the browser and grant consent, the provider will hand over a code.</li><li>Provide the Google authentication provider with the <code>client_id</code>, <code>client_secret</code> and the code, it will acquire and provide users with a refresh token.</li></ul><p>Let&#x27;s start coding. We&#x27;ll initialise a TypeScript Node project like so:</p><pre><code class="language-bash">mkdir src
cd src
npm init -y
npm install googleapis ts-node typescript yargs @types/yargs @types/node
npx tsc --init
</code></pre><p>We&#x27;ve added a number of dependencies that will allow us to write a TypeScript Node command line application. We&#x27;ve also added a dependency to the <a href="https://www.npmjs.com/package/googleapis"><code>googleapis</code></a> package which describes itself as:</p><blockquote><p>Node.js client library for using Google APIs. Support for authorization and authentication with OAuth 2.0, API Keys and JWT tokens is included.</p></blockquote><p>We&#x27;re going to make use of the OAuth 2.0 part. We&#x27;ll start our journey by creating a file called <code>google-api-auth.ts</code>:</p><pre><code class="language-ts">import { getArgs, makeOAuth2Client } from &#x27;./shared&#x27;;

async function getToken() {
  const { clientId, clientSecret, code } = await getArgs();
  const oauth2Client = makeOAuth2Client({ clientId, clientSecret });

  if (code) await getRefreshToken(code);
  else getAuthUrl();

  async function getAuthUrl() {
    const url = oauth2Client.generateAuthUrl({
      // &#x27;online&#x27; (default) or &#x27;offline&#x27; (gets refresh_token)
      access_type: &#x27;offline&#x27;,

      // scopes are documented here: https://developers.google.com/identity/protocols/oauth2/scopes#calendar
      scope: [
        &#x27;https://www.googleapis.com/auth/calendar&#x27;,
        &#x27;https://www.googleapis.com/auth/calendar.events&#x27;,
      ],
    });

    console.log(`Go to this URL to acquire a refresh token:\n\n${url}\n`);
  }

  async function getRefreshToken(code: string) {
    const token = await oauth2Client.getToken(code);
    console.log(token);
  }
}

getToken();
</code></pre><p>And a common file named <code>shared.ts</code> which <code>google-api-auth.ts</code> imports and which we&#x27;ll re-use later:</p><pre><code class="language-ts">import { google } from &#x27;googleapis&#x27;;
import yargs from &#x27;yargs/yargs&#x27;;
const { hideBin } = require(&#x27;yargs/helpers&#x27;);

export async function getArgs() {
  const argv = await Promise.resolve(yargs(hideBin(process.argv)).argv);

  const clientId = argv[&#x27;clientId&#x27;] as string;
  const clientSecret = argv[&#x27;clientSecret&#x27;] as string;

  const code = argv.code as string | undefined;
  const refreshToken = argv.refreshToken as string | undefined;
  const test = argv.test as boolean;

  if (!clientId) throw new Error(&#x27;No clientId &#x27;);
  console.log(&#x27;We have a clientId&#x27;);

  if (!clientSecret) throw new Error(&#x27;No clientSecret&#x27;);
  console.log(&#x27;We have a clientSecret&#x27;);

  if (code) console.log(&#x27;We have a code&#x27;);
  if (refreshToken) console.log(&#x27;We have a refreshToken&#x27;);

  return { code, clientId, clientSecret, refreshToken, test };
}

export function makeOAuth2Client({
  clientId,
  clientSecret,
}: {
  clientId: string;
  clientSecret: string;
}) {
  return new google.auth.OAuth2(
    /* YOUR_CLIENT_ID */ clientId,
    /* YOUR_CLIENT_SECRET */ clientSecret,
    /* YOUR_REDIRECT_URL */ &#x27;urn:ietf:wg:oauth:2.0:oob&#x27;
  );
}
</code></pre><p>The <code>getToken</code> function above does these things:</p><ol><li>If given a <code>client_id</code> and <code>client_secret</code> it will obtain an authentication URL.</li><li>If given a <code>client_id</code>, <code>client_secret</code> and <code>code</code> it will obtain a refresh token (scoped to access the Google Calendar API).</li></ol><p>We&#x27;ll add an entry to our <code>package.json</code> which will allow us to run our console app:</p><pre><code class="language-json">    &quot;google-api-auth&quot;: &quot;ts-node google-api-auth.ts&quot;
</code></pre><p>Now we&#x27;re ready to acquire the refresh token. We&#x27;ll run the following command (substituting in the appropriate values):</p><p><code>npm run google-api-auth -- --clientId CLIENT_ID --clientSecret CLIENT_SECRET</code></p><p>Click on the URL that is generated in the console, it should open up a consent screen in the browser which looks like this:</p><p><img src="../static/blog/2021-09-10-google-apis-authentication-with-typescript/grant-consent.png" alt="Screenshot of the consent screen"/></p><p>Authenticate and grant consent and you should get a code:</p><p><img src="../static/blog/2021-09-10-google-apis-authentication-with-typescript/auth-code.png" alt="Screenshot of the generated code"/></p><p>Then (quickly) paste the acquired code into the following command:</p><p><code>npm run google-api-auth -- --clientId CLIENT_ID --clientSecret CLIENT_SECRET --code THISISTHECODE</code></p><p>The <code>refresh_token</code> (alongside much else) will be printed to the console. Grab it and put it somewhere secure. Again, no storing in source control!</p><p>It&#x27;s worth taking a moment to reflect on what we&#x27;ve done. We&#x27;ve acquired a refresh token which involved a certain amount of human interaction. We&#x27;ve had to run a console command, do some work in a browser and run another commmand. You wouldn&#x27;t want to do this repeatedly because it involves human interaction. Intentionally it cannot be automated. However, once you&#x27;ve acquired the refresh token, you can use it repeatedly until it expires (which may be never or at least years in the future). So once you have the refresh token, and you&#x27;ve stored it securely, you have what you need to be able to automate an API interaction.</p><h2>Accessing the Google Calendar API</h2><p>Let&#x27;s test out our refresh token by attempting to access the Google Calendar API. We&#x27;ll create a <code>calendar.ts</code> file</p><pre><code class="language-ts">import { google } from &#x27;googleapis&#x27;;
import { getArgs, makeOAuth2Client } from &#x27;./shared&#x27;;

async function makeCalendarClient() {
  const { clientId, clientSecret, refreshToken } = await getArgs();
  const oauth2Client = makeOAuth2Client({ clientId, clientSecret });
  oauth2Client.setCredentials({
    refresh_token: refreshToken,
  });

  const calendarClient = google.calendar({
    version: &#x27;v3&#x27;,
    auth: oauth2Client,
  });
  return calendarClient;
}

async function getCalendar() {
  const calendarClient = await makeCalendarClient();

  const { data: calendars, status } = await calendarClient.calendarList.list();

  if (status === 200) {
    console.log(&#x27;calendars&#x27;, calendars);
  } else {
    console.log(&#x27;there was an issue...&#x27;, status);
  }
}

getCalendar();
</code></pre><p>The <code>getCalendar</code> function above uses the <code>client_id</code>, <code>client_secret</code> and <code>refresh_token</code> to access the Google Calendar API and retrieve the list of calendars.</p><p>We&#x27;ll add an entry to our <code>package.json</code> which will allow us to run this function:</p><pre><code class="language-json">    &quot;calendar&quot;: &quot;ts-node calendar.ts&quot;,
</code></pre><p>Now we&#x27;re ready to test <code>calendar.ts</code>. We&#x27;ll run the following command (substituting in the appropriate values):</p><p><code>npm run calendar -- --clientId CLIENT_ID --clientSecret CLIENT_SECRET --refreshToken REFRESH_TOKEN</code></p><p>When we run for the first time, we may encounter a self explanatory message which tells us that we need enable the calendar API for our application:</p><pre><code>(node:31563) UnhandledPromiseRejectionWarning: Error: Google Calendar API has not been used in project 77777777777777 before or it is disabled. Enable it by visiting https://console.developers.google.com/apis/api/calendar-json.googleapis.com/overview?project=77777777777777 then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry.
</code></pre><p>Once enabled, we can run successfully for the first time. Consequently we should see something like this showing up in the console:</p><p><img src="../static/blog/2021-09-10-google-apis-authentication-with-typescript/calendars-response.png" alt="Screenshot of calendars list response in the console"/></p><p>This demonstrates that we&#x27;re successfully integrating with a Google API using our refresh token.</p><h2>Today the Google Calendar API, tomorrow the (Google API) world!</h2><p>What we&#x27;ve demonstrated here is integrating with the Google Calendar API. However, that is not the limit of what we can do. As we discussed earlier, Google has more than two hundred APIs we can interact with, and the key to that interaction is following the same steps for authentication that this post outlines.</p><p>Let&#x27;s imagine that we want to integrate with the YouTube API or the GMail API. We&#x27;d be able to follow the steps in this post, using different <a href="https://developers.google.com/identity/protocols/oauth2/scopes#calendar">scopes for the refresh token appropriate to the API</a>, and build an integration against that API. <a href="https://developers.google.com/apis-explorer">Take a look at the available APIs</a> here.</p><p>The approach outlined by this post is the key to integrating with a multitude of Google APIs. Happy integrating!</p><p>The idea of this was sparked by <a href="https://martinfowler.com/articles/command-line-google.html">Martin Fowler&#x27;s post</a> on the topic which comes from a Ruby angle.</p><p><a href="https://blog.logrocket.com/how-to-authenticate-access-google-apis-using-oauth-2-0/">This post was originally published on LogRocket.</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Bicep: syntax highlighting with PrismJS (and Docusaurus)]]></title>
            <link>https://blog.johnnyreilly.com/2021/08/19/bicep-syntax-highlighting-with-prismjs</link>
            <guid>Bicep: syntax highlighting with PrismJS (and Docusaurus)</guid>
            <pubDate>Thu, 19 Aug 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Bicep is an amazing language, it's also very new. If you want to write attractive code snippets about Bicep, you can by using PrismJS (and Docusaurus). This post shows you how.]]></description>
            <content:encoded><![CDATA[<p>Bicep is an amazing language, it&#x27;s also very new. If you want to write attractive code snippets about Bicep, you can by using PrismJS (and Docusaurus). This post shows you how.</p><p><img src="../static/blog/2021-08-19-bicep-syntax-highlighting-with-prismjs/bicep-syntax-highlighting-with-prismjs.png" alt="title image reading &quot;Publish Azure Static Web Apps with Bicep and Azure DevOps&quot; and some Azure logos"/></p><h2>Syntax highlighting</h2><p>I&#x27;ve been writing blog posts about Bicep for a little while. I was frustrated that the code snippets were entirely unhighlighted. I&#x27;m keen my posts are as readable as possible, and so I <a href="https://github.com/PrismJS/prism/pull/3027">looked into adding support to PrismJS</a> which is what <a href="https://docusaurus.io/">Docusaurus</a> uses to power syntax highlighting.</p><p>Whilst my regex fu is amateur at best, happily <a href="https://github.com/RunDevelopment">Michael Schmidt</a> of the PrismJS family is considerably better. He took the support I added and <a href="https://github.com/PrismJS/prism/pull/3028">made it much better</a>.</p><h2>Docusaurus meet Bicep</h2><p>If you have any code snippets that start with three backticks and the word <code>bicep</code>...</p><pre><code>```bicep
// code goes here...
</code></pre><p>... then ideally you&#x27;d like to see some syntax highlighting in your post. Since Bicep isn&#x27;t &quot;in the box&quot; for Docusaurus you need to <a href="https://docusaurus.io/docs/next/markdown-features/code-blocks#supported-languages">explicitly opt into support like so:</a></p><pre><code class="language-js">    prism: {
      additionalLanguages: [&quot;powershell&quot;, &quot;csharp&quot;, &quot;docker&quot;, &quot;bicep&quot;],
    },
</code></pre><p>Above you can see a snippet from my own <a href="https://github.com/johnnyreilly/blog.johnnyreilly.com/blob/b2df93efb72adc32d9f45de4f727e890e59a4919/blog-website/docusaurus.config.js#L185"><code>docusaurus.config.js</code></a> which adds Bicep, alongside the other additional languages I use.</p><p>With this in place, you would typically get all the syntax highlighting support you need.</p><h2>Early adoption workaround</h2><p>I&#x27;m writing this post before the latest version of PrismJS has shipped. As such, Bicep support isn&#x27;t available by default yet. But if you&#x27;re an early adopter, you can get support right now. The secret is adding a <code>resolutions</code> section to your <code>package.json</code> which points to the GitHub Repo <a href="https://github.com/PrismJS/prism">where Prism lives</a>:</p><pre><code class="language-json">  &quot;resolutions&quot;: {
    &quot;prismjs&quot;: &quot;PrismJS/prism&quot;
  },
</code></pre><p>This will mean that Yarn (if you&#x27;re using Docusaurus you&#x27;re probably using Yarn) pulls <code>prismjs</code> directly from GitHub, as demonstrated by the <code>yarn.lock</code> file:</p><pre><code>prismjs@PrismJS/prism, prismjs@^1.23.0:
  version &quot;1.24.1&quot;
  resolved &quot;https://codeload.github.com/PrismJS/prism/tar.gz/59f449d33dc9fd19302f21aad95fc0b5028ac830&quot;
</code></pre><h2>What does it look like?</h2><p>Finally, let&#x27;s see if works. Here&#x27;s a Bicep code snippet that I borrowed from <a href="/2021/08/19/bicep-syntax-highlighting-with-prismjs">an earlier post</a>:</p><pre><code class="language-bicep">param repositoryUrl string
param repositoryBranch string

param location string = &#x27;westeurope&#x27;
param skuName string = &#x27;Free&#x27;
param skuTier string = &#x27;Free&#x27;

param appName string

resource staticWebApp &#x27;Microsoft.Web/staticSites@2020-12-01&#x27; = {
  name: appName
  location: location
  tags: tagsObj
  sku: {
    name: skuName
    tier: skuTier
  }
  properties: {
    // The provider, repositoryUrl and branch fields are required for successive deployments to succeed
    // for more details see: https://github.com/Azure/static-web-apps/issues/516
    provider: &#x27;DevOps&#x27;
    repositoryUrl: repositoryUrl
    branch: repositoryBranch
    buildProperties: {
      skipGithubActionWorkflowGeneration: true
    }
  }
}

output deployment_token string = listSecrets(staticWebApp.id, staticWebApp.apiVersion).properties.apiKey
</code></pre><p>As you can see, it&#x27;s delightfully highlighted by PrismJS. Enjoy!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Publish Azure Static Web Apps with Bicep and Azure DevOps]]></title>
            <link>https://blog.johnnyreilly.com/2021/08/15/bicep-azure-static-web-apps-azure-devops</link>
            <guid>Publish Azure Static Web Apps with Bicep and Azure DevOps</guid>
            <pubDate>Sun, 15 Aug 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[This post demonstrates how to deploy Azure Static Web Apps using Bicep and Azure DevOps. It includes a few workarounds for the "Provider is invalid. Cannot change the Provider. Please detach your static site first if you wish to use to another deployment provider." issue.]]></description>
            <content:encoded><![CDATA[<p>This post demonstrates how to deploy <a href="https://docs.microsoft.com/en-us/azure/static-web-apps/overview">Azure Static Web Apps</a> using Bicep and Azure DevOps. It includes a few workarounds for the <a href="https://github.com/Azure/static-web-apps/issues/516">&quot;Provider is invalid. Cannot change the Provider. Please detach your static site first if you wish to use to another deployment provider.&quot; issue</a>.</p><p><img src="../static/blog/2021-08-15-bicep-azure-static-web-apps-azure-devops/title-image.png" alt="title image reading &quot;Publish Azure Static Web Apps with Bicep and Azure DevOps&quot; and some Azure logos"/></p><h2>Bicep template</h2><p>The first thing we&#x27;re going to do is create a folder where our Bicep file for deploying our Azure Static Web App will live:</p><pre><code class="language-bash">mkdir infra/static-web-app -p
</code></pre><p>Then we&#x27;ll create a <code>main.bicep</code> file:</p><pre><code class="language-bicep">param repositoryUrl string
param repositoryBranch string

param location string = &#x27;westeurope&#x27;
param skuName string = &#x27;Free&#x27;
param skuTier string = &#x27;Free&#x27;

param appName string

resource staticWebApp &#x27;Microsoft.Web/staticSites@2020-12-01&#x27; = {
  name: appName
  location: location
  sku: {
    name: skuName
    tier: skuTier
  }
  properties: {
    // The provider, repositoryUrl and branch fields are required for successive deployments to succeed
    // for more details see: https://github.com/Azure/static-web-apps/issues/516
    provider: &#x27;DevOps&#x27;
    repositoryUrl: repositoryUrl
    branch: repositoryBranch
    buildProperties: {
      skipGithubActionWorkflowGeneration: true
    }
  }
}

output deployment_token string = listSecrets(staticWebApp.id, staticWebApp.apiVersion).properties.apiKey
</code></pre><p>There&#x27;s some things to draw attention to in the code above:</p><ol><li>The <code>provider</code>, <code>repositoryUrl</code> and <code>branch</code> fields are required for successive deployments to succeed. In our case we&#x27;re deploying via Azure DevOps and so our provider is <code>&#x27;DevOps&#x27;</code>. For more details, <a href="https://github.com/Azure/static-web-apps/issues/516">look at this issue</a>.</li><li>We&#x27;re creating a <code>deployment_token</code> which we&#x27;ll need in order that we can deploy into the Azure Static Web App resource.</li></ol><h2>Static Web App</h2><p>In order that we can test out Azure Static Web Apps, what we need is a static web app. You could use pretty much anything here; we&#x27;re going to use Docusaurus. We&#x27;ll execute this single command:</p><pre><code class="language-bash">npx @docusaurus/init@latest init static-web-app classic
</code></pre><p>Which will scaffold a Docusaurus site in a folder named <code>static-web-app</code>. We don&#x27;t need to change it any further; let&#x27;s just see if we can deploy it.</p><h2>Azure Pipeline</h2><p>We&#x27;re going to add an <code>azure-pipelines.yml</code> file which Azure DevOps can use to power a pipeline:</p><pre><code class="language-yml">trigger:
  - main

pool:
  vmImage: ubuntu-latest

steps:
  - checkout: self
    submodules: true

  - bash: az bicep build --file infra/static-web-app/main.bicep
    displayName: &#x27;Compile Bicep to ARM&#x27;

  - task: AzureResourceManagerTemplateDeployment@3
    name: DeployStaticWebAppInfra
    displayName: Deploy Static Web App infra
    inputs:
      deploymentScope: Resource Group
      azureResourceManagerConnection: $(serviceConnection)
      subscriptionId: $(subscriptionId)
      action: Create Or Update Resource Group
      resourceGroupName: $(azureResourceGroup)
      location: $(location)
      templateLocation: Linked artifact
      csmFile: &#x27;infra/static-web-app/main.json&#x27; # created by bash script
      overrideParameters: &gt;-
        -repositoryUrl $(repo)
        -repositoryBranch $(Build.SourceBranchName)
        -appName $(staticWebAppName)
      deploymentMode: Incremental
      deploymentOutputs: deploymentOutputs

  - task: PowerShell@2
    name: &#x27;SetDeploymentOutputVariables&#x27;
    displayName: &#x27;Set Deployment Output Variables&#x27;
    inputs:
      targetType: inline
      script: |
        $armOutputObj = &#x27;$(deploymentOutputs)&#x27; | ConvertFrom-Json
        $armOutputObj.PSObject.Properties | ForEach-Object {
          $keyname = $_.Name
          $value = $_.Value.value

          # Creates a standard pipeline variable
          Write-Output &quot;##vso[task.setvariable variable=$keyName;issecret=true]$value&quot;

          # Display keys in pipeline
          Write-Output &quot;output variable: $keyName&quot;
        }
      pwsh: true

  - task: AzureStaticWebApp@0
    name: DeployStaticWebApp
    displayName: Deploy Static Web App
    inputs:
      app_location: &#x27;static-web-app&#x27;
      # api_location: &#x27;api&#x27; # we don&#x27;t have an API
      output_location: &#x27;build&#x27;
      azure_static_web_apps_api_token: $(deployment_token) # captured from deploymentOutputs
</code></pre><p>When the pipeline is run, it does the following:</p><ol><li>Compiles our Bicep into an ARM template</li><li>Deploys the compiled ARM template to Azure</li><li>Captures the deployment outputs (essentially the <code>deployment_token</code>) and converts them into variables to use in the pipeline</li><li>Deploys our Static Web App using the <code>deployment_token</code></li></ol><p>The pipeline depends upon a number of variables:</p><ul><li><code>azureResourceGroup</code> - the name of your resource group in Azure where the app will be deployed</li><li><code>location</code> - where your app is deployed, eg <code>northeurope</code></li><li><code>repo</code> - the URL of your repository in Azure DevOps, eg <a href="https://dev.azure.com/johnnyreilly/_git/azure-static-web-apps">https://dev.azure.com/johnnyreilly/_git/azure-static-web-apps</a></li><li><code>serviceConnection</code> - the name of your AzureRM service connection in Azure DevOps</li><li><code>staticWebAppName</code> - the name of your static web app, eg <code>azure-static-web-apps-johnnyreilly</code></li><li><code>subscriptionId</code> - your Azure subscription id from the <a href="https://portal.azure.com">Azure Portal</a></li></ul><p>A successful pipeline looks something like this:</p><p><img src="../static/blog/2021-08-15-bicep-azure-static-web-apps-azure-devops/successful-azure-pipelines-run-screenshot.png" alt="Screenshot of successfully running Azure Pipeline"/></p><p>What you might notice is that the <code>AzureStaticWebApp</code> is itself installing and building our application. This is handled by <a href="https://github.com/Microsoft/Oryx">Microsoft Oryx</a>. The upshot of this is that we don&#x27;t need to manually run <code>npm install</code> and <code>npm build</code> ourselves; the <code>AzureStaticWebApp</code> task will take care of it for us.</p><p>Finally, let&#x27;s see if we&#x27;ve deployed something successfully...</p><p><img src="../static/blog/2021-08-15-bicep-azure-static-web-apps-azure-devops/deployed-azure-static-web-app-screenshot.png" alt="Screenshot of deployed Azure Static Web App"/></p><p>We have! It&#x27;s worth noting that you&#x27;ll likely want to give your Azure Static Web App a lovelier URL, and perhaps even put it behind Azure Front Door as well.</p><h2><code>Provider is invalid</code> workaround 2</h2><p><a href="https://www.linkedin.com/in/shaneneff/">Shane Neff</a> was attempting to follow the instructions in this post and encountered issues. He shared his struggles with me as he encountered the <a href="https://github.com/Azure/static-web-apps/issues/516">&quot;Provider is invalid. Cannot change the Provider. Please detach your static site first if you wish to use to another deployment provider.&quot; issue</a>.</p><p>He was good enough to share his solution as well, which is inserting this task at the start of the pipeline (before the <code>az bicep build</code> step):</p><pre><code class="language-yml">- task: AzureCLI@2
  inputs:
    azureSubscription: &#x27;&lt;name of your service connection&gt;&#x27;
    scriptType: &#x27;bash&#x27;
    scriptLocation: &#x27;inlineScript&#x27;
    inlineScript: &#x27;az staticwebapp disconnect -n &lt;name of your app&gt;&#x27;
</code></pre><p>I haven&#x27;t had the problems that Shane has had myself, but I wanted to share his fix for the people out there who almost certainly are bumping on this.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[TypeScript 4.4 and more readable code]]></title>
            <link>https://blog.johnnyreilly.com/2021/08/14/typescript-4-4-more-readable-code</link>
            <guid>TypeScript 4.4 and more readable code</guid>
            <pubDate>Sat, 14 Aug 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[An exciting feature is shipping with TypeScript 4.4. It has the name "Control Flow Analysis of Aliased Conditions" which is quite a mouthful. This post unpacks what this feature is, and demonstrates the contribution it makes to improving the readability of code.]]></description>
            <content:encoded><![CDATA[<p>An exciting feature is shipping with TypeScript 4.4. It has the name <a href="https://devblogs.microsoft.com/typescript/announcing-typescript-4-4-beta/#cfa-aliased-conditions">&quot;Control Flow Analysis of Aliased Conditions&quot;</a> which is quite a mouthful. This post unpacks what this feature is, and demonstrates the contribution it makes to improving the readability of code.</p><h2>Updated 30th September 2021</h2><p>This blog evolved to become a talk:</p><iframe width="560" height="315" src="https://www.youtube.com/embed/LxZx3ycrxI0" title="YouTube video player" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"></iframe><h2>Indirect type narrowing via <code>const</code></h2><p>On June 24th 2021, an issue on the TypeScript GitHub repository with the title &quot;Indirect type narrowing via <code>const</code>&quot; was closed by <a href="https://www.twitter.com/ahejlsberg">Anders Hejlsberg</a>. The issue had been open since 2016 and it was closed as it was covered by <a href="https://github.com/microsoft/TypeScript/pull/44730">a pull request addressing control flow analysis of aliased conditional expressions and discriminants</a>.</p><p>It&#x27;s fair to say that the TypeScript community was very excited about this, both judging from reactions on the issue:</p><p><a href="https://github.com/microsoft/TypeScript/issues/12184#issuecomment-867928408"><img src="../static/blog/2021-08-14-typescript-4-4-more-readable-code/reactions-on-github.png" alt="Screenshot of reactions on GitHub"/></a></p><p>And also the general delight on Twitter:</p><p><a href="https://www.twitter.com/johnny_reilly/status/1408162514504933378"><img src="../static/blog/2021-08-14-typescript-4-4-more-readable-code/reactions-on-twitter.png" alt="Screenshot of reactions on Twitter"/></a></p><p>What Zeh said is a great explanation of the significance of this feature:</p><blockquote><p>Lack of type narrowing with consts made me repeat code, or avoid helpfully namef consts, too many times</p></blockquote><p>With this feature we&#x27;re going to have the possibility of more readable code, and less repetition. That&#x27;s amazing!</p><h2>The code we would like to write</h2><p>Rather than starting with an explanation of what this new language feature is, let&#x27;s instead start from the position of writing some code and seeing what&#x27;s possible with TypeScript 4.4 that we couldn&#x27;t tackle previously.</p><p>Here&#x27;s a simple function that adds all the parameters it receives and returns the total. It&#x27;s a tolerant function and will allow people to supply numbers in the form of strings as well; so it would successfully process <code>&#x27;2&#x27;</code> as it would <code>2</code>. This is, of course, a slightly contrived example, but should be useful for demonstrating the new feature.</p><pre><code class="language-ts">function add(...thingsToAdd: (string | number)[]): number {
  let total = 0;
  for (const thingToAdd of thingsToAdd) {
    if (typeof thingToAdd === &#x27;string&#x27;) {
      total += Number(thingToAdd);
    } else {
      total += thingToAdd;
    }
  }
  return total;
}

console.log(add(1, &#x27;7&#x27;, &#x27;3&#x27;, 9));
</code></pre><p><a href="https://www.typescriptlang.org/play?ts=4.3.5#code/GYVwdgxgLglg9mABAQwCaoBQDodQBYxgDmAzgCpwCC6AXIhiVAE6FGIA+iYIAtgEYBTJgEoA2gF1hdbvyGIA3gChEKxABsBURFDhRkaxAF5EABgDcy1cDhN6EBI20FiFaqkRxgT1uSrphCpaqqjBeGFAAngAOAp7eLn7uhsmIAOSMLMSpAUrBeao6egYA1MYAcryCTOHORK7+FvkqAL6IAmokAoFNeYX6iKXxdYmNTc1BiOPBTJogTEh9ahbjivZgJHAaWGpwRBhomACMADRpAOypp6kAzJeIAJzCwkA">Try it out in the TypeScript playground.</a></p><p>If we look at this function, whilst it works, it&#x27;s not super expressive. The <code>typeof thingToAdd === &#x27;string&#x27;</code> performs two purposes:</p><ol><li>It narrows the type from <code>string | number</code> to <code>string</code></li><li>It branches the logic, such that the <code>string</code> can be coerced into a <code>number</code> and added to the total.</li></ol><p>You can infer this from reading the code. However, what if we were to re-write it to capture intent? Let&#x27;s try creating a <code>shouldCoerceToNumber</code> constant which expresses the action we need to take:</p><pre><code class="language-ts">function add(...thingsToAdd: (string | number)[]): number {
  let total = 0;
  for (const thingToAdd of thingsToAdd) {
    const shouldCoerceToNumber = typeof thingToAdd === &#x27;string&#x27;;
    if (shouldCoerceToNumber) {
      total += Number(thingToAdd);
    } else {
      total += thingToAdd;
    }
  }
  return total;
}

console.log(add(1, &#x27;7&#x27;, &#x27;3&#x27;, 9));
</code></pre><p><a href="https://www.typescriptlang.org/play?ts=4.3.5#code/GYVwdgxgLglg9mABAQwCaoBQDodQBYxgDmAzgCpwCC6AXIhiVAE6FGIA+iYIAtgEYBTJgEoA2gF1hdbvyGIA3gChEKxABsBURFDhRkaxAF5EABgDcy1cDhN6EBI20FiFaqkRxgT1uSrphCpaqqvZgjiR4cCBqqADCcEIQAhQAcryCtsZQAJ4ADgKe3i5+7oZliADkjCzEFRbBwTBeDJHRcQlMSanpQgFKDQPauvqIANTGabJMGPisrv71gwC+iAJqJAKBgw06egbjRUTzqIsDS0GI58FMmiBMSLv6FueKoSRwGlhqcEQYaJgARgANJUAOwVEEVADMEMQAE5hMIgA">Try it out in the TypeScript playground.</a></p><p>This is valid code; however TypeScript 4.3 is choking with an error:</p><p><img src="../static/blog/2021-08-14-typescript-4-4-more-readable-code/doesnt-work-in-typescript-4-3.png" alt="Screenshot of the TypeScript playground running TypeScript 4.3 and throwing an error on our new code"/></p><p>The error being surfaced is:</p><blockquote><p><code>Operator &#x27;+=&#x27; cannot be applied to types &#x27;number&#x27; and &#x27;string | number&#x27;.(2365)</code></p></blockquote><p>What&#x27;s happening here, is TypeScript <em>does not remember</em> that <code>shouldCoerceToNumber</code> represents a type narrowing of <code>thingToAdd</code> from <code>string | number</code> to <code>string</code>. So the type of <code>thingToAdd</code> remains unchanged from <code>string | number</code> when we write code that depends upon it.</p><p>This has terrible consequences. It means we can&#x27;t write this more expressive code that we&#x27;re interested in, and would be better for maintainers of our codebase. And this is what TypeScript 4.4, with our new feature, unlocks. Let&#x27;s change the playground to use TypeScript 4.4 instead:</p><p><img src="../static/blog/2021-08-14-typescript-4-4-more-readable-code/does-work-in-typescript-4-4.png" alt="Screenshot of the TypeScript playground running TypeScript 4.4 and working with our new code - it shows the `thingToAdd` variable has been narrowed to a `string`"/></p><p><a href="https://www.typescriptlang.org/play?ts=4.4.0-beta#code/GYVwdgxgLglg9mABAQwCaoBQDodQBYxgDmAzgCpwCC6AXIhiVAE6FGIA+iYIAtgEYBTJgEoA2gF1hdbvyGIA3gChEKxABsBURFDhRkaxAF5EABgDcy1cDhN6EBI20FiFaqkRxgT1uSrphCpaqqvZgjiR4cCBqqADCcEIQAhQAcryCtsZQAJ4ADgKe3i5+7oZliADkjCzEFRbBwTBeDJHRcQlMSanpQgFKDQPauvqIANTGabJMGPisrv71gwC+iAJqJAKBgw06egbjRUTzqIsDS0GI58FMmiBMSLv6FueKoSRwGlhqcEQYaJgARgANJUAOwVEEVADMEMQAE5hMIgA">Try it out in the TypeScript playground.</a></p><p>Delightfully, we no longer have errors now we&#x27;ve made the switch. And as the screenshot shows, the <code>thingToAdd</code> variable has been narrowed to a <code>string</code>. This is because Control Flow Analysis of Aliased Conditions is now in play.</p><p>So we&#x27;re now writing more expressive code, and TypeScript is willing us on our way.</p><h2>Read more</h2><p>This feature is a tremendous addition to the TypeScript language. It should have a significant long-term positive impact on how people write code with TypeScript.</p><p>To read more, do check out the excellent <a href="https://devblogs.microsoft.com/typescript/announcing-typescript-4-4-beta/#cfa-aliased-conditions">TypeScript 4.4 beta release notes</a>. There&#x27;s also some other exciting feature shipping with this release as well. Thanks very much to the TypeScript team for once again improving the language, and making a real contribution to people being able to write readable code.</p><p><a href="https://blog.logrocket.com/typescript-4-4-and-more-readable-code/">This post was originally published on LogRocket.</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[TypeScript, abstract classes, and constructors]]></title>
            <link>https://blog.johnnyreilly.com/2021/08/01/typescript-abstract-classes-and-constructors</link>
            <guid>TypeScript, abstract classes, and constructors</guid>
            <pubDate>Sun, 01 Aug 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[TypeScript has the ability to define classes as abstract. This means they cannot be instantiated directly, only non-abstract subclasses can be. Let's take a look at what this means when it comes to constructor usage.]]></description>
            <content:encoded><![CDATA[<p>TypeScript has the ability to define classes as abstract. This means they cannot be instantiated directly, only non-abstract subclasses can be. Let&#x27;s take a look at what this means when it comes to constructor usage.</p><h2>Making a scratchpad</h2><p>In order that we can dig into this, let&#x27;s create ourselves a scratchpad project to work with. We&#x27;re going to create a node project and install TypeScript as a dependency.</p><pre><code class="language-bash">mkdir ts-abstract-constructors
cd ts-abstract-constructors
npm init --yes
npm install typescript @types/node --save-dev
</code></pre><p>We now have a <code>package.json</code> file set up. We need to initialise a TypeScript project as well:</p><pre><code>npx tsc --init
</code></pre><p>This will give us a <code>tsconfig.json</code> file that will drive configuration of TypeScript. By default TypeScript transpiles to an older version of JavaScript that predates classes. So we&#x27;ll update the config to target a newer version of the language that does include them:</p><pre><code class="language-json">    &quot;target&quot;: &quot;es2020&quot;,
    &quot;lib&quot;: [&quot;es2020&quot;],
</code></pre><p>Let&#x27;s create ourselves a TypeScript file called <code>index.ts</code>. The name is not significant; we just need a file to develop in.</p><p>Finally we&#x27;ll add a script to our <code>package.json</code> that compiles our TypeScript to JavaScript, and then runs the JS with node:</p><pre><code class="language-json">&quot;start&quot;: &quot;tsc --project \&quot;.\&quot; &amp;&amp; node index.js&quot;
</code></pre><h2>Making an abstract class</h2><p>Now we&#x27;re ready. Let&#x27;s add an abstract class with a constructor to our <code>index.ts</code> file:</p><pre><code class="language-ts">abstract class ViewModel {
  id: string;

  constructor(id: string) {
    this.id = id;
  }
}
</code></pre><p>Consider the <code>ViewModel</code> class above. Let&#x27;s say we&#x27;re building some kind of CRUD app, we&#x27;ll have different views. Each of those views will have a corresponding viewmodel which is a subclass of the <code>ViewModel</code> abstract class. The <code>ViewModel</code> class has a mandatory <code>id</code> parameter in the constructor. This is to ensure that every viewmodel has an <code>id</code> value. If this were a real app, <code>id</code> would likely be the value with which an entity was looked up in some kind of database.</p><p>Importantly, all subclasses of <code>ViewModel</code> should either:</p><ul><li><p>not implement a constructor at all, leaving the base class constructor to become the default constructor of the subclass <em>or</em></p></li><li><p>implement their own constructor which invokes the <code>ViewModel</code> base class constructor.</p></li></ul><h2>Taking our abstract class for a spin</h2><p>Now we have it, let&#x27;s see what we can do with our abstract class. First of all, can we instantiate our abstract class? We shouldn&#x27;t be able to do this:</p><pre><code>const viewModel = new ViewModel(&#x27;my-id&#x27;);

console.log(`the id is: ${viewModel.id}`);
</code></pre><p>And sure enough, running <code>npm start</code> results in the following error (which is also being reported by our editor; VS Code).</p><pre><code class="language-shell">index.ts:9:19 - error TS2511: Cannot create an instance of an abstract class.

const viewModel = new ViewModel(&#x27;my-id&#x27;);
</code></pre><p><img src="../static/blog/2021-08-01-typescript-abstract-classes-and-constructors/vs-code-abstract-screenshot.png" alt="Screenshot of &quot;Cannot create an instance of an abstract class.&quot; error in VS Code"/></p><p>Tremendous. However, it&#x27;s worth remembering that <code>abstract</code> is a TypeScript concept. When we compile our TS, although it&#x27;s throwing a compilation error, it still transpiles an <code>index.js</code> file that looks like this:</p><pre><code class="language-js">&#x27;use strict&#x27;;
class ViewModel {
  constructor(id) {
    this.id = id;
  }
}
const viewModel = new ViewModel(&#x27;my-id&#x27;);
console.log(`the id is: ${viewModel.id}`);
</code></pre><p>As we can see, there&#x27;s no mention of <code>abstract</code>; it&#x27;s just a straightforward <code>class</code>. In fact, if we directly execute the file with <code>node index.js</code> we can see an output of:</p><pre><code>the id is: my-id
</code></pre><p>So the transpiled code is valid JavaScript even if the source code isn&#x27;t valid TypeScript. This all reminds us that <code>abstract</code> is a TypeScript construct.</p><h2>Subclassing without a new constructor</h2><p>Let&#x27;s now create our first subclass of <code>ViewModel</code> and attempt to instantiate it:</p><pre><code class="language-ts">class NoNewConstructorViewModel extends ViewModel {}

// error TS2554: Expected 1 arguments, but got 0.
const viewModel1 = new NoNewConstructorViewModel();

const viewModel2 = new NoNewConstructorViewModel(&#x27;my-id&#x27;);
</code></pre><p><img src="../static/blog/2021-08-01-typescript-abstract-classes-and-constructors/vs-code-no-new-constructor.png" alt="Screenshot of &quot;error TS2554: Expected 1 arguments, but got 0.&quot; error in VS Code"/></p><p>As the TypeScript compiler tells us, the second of these instantiations is legitimate as it relies upon the constructor from the base class as we&#x27;d hope. The first is not as there is no parameterless constructor.</p><h2>Subclassing with a new constructor</h2><p>Having done that, let&#x27;s try subclassing and implementing a new constructor which has two parameters (to differentiate from the constructor we&#x27;re overriding):</p><pre><code class="language-ts">class NewConstructorViewModel extends ViewModel {
  data: string;
  constructor(id: string, data: string) {
    super(id);
    this.data = data;
  }
}

// error TS2554: Expected 2 arguments, but got 0.
const viewModel3 = new NewConstructorViewModel();

// error TS2554: Expected 2 arguments, but got 1.
const viewModel4 = new NewConstructorViewModel(&#x27;my-id&#x27;);

const viewModel5 = new NewConstructorViewModel(&#x27;my-id&#x27;, &#x27;important info&#x27;);
</code></pre><p><img src="../static/blog/2021-08-01-typescript-abstract-classes-and-constructors/vs-code-new-constructor.png" alt="Screenshot of &quot;error TS2554: Expected 1 arguments, but got 1.&quot; error in VS Code"/></p><p>Again, only one of the attempted instantiations is legitimate. <code>viewModel3</code> is not as there is no parameterless constructor. <code>viewModel4</code> is not as we have overridden the base class constructor with our new one that has two parameters. Hence <code>viewModel5</code> is our &quot;Goldilocks&quot; instantiation; it&#x27;s just right!</p><p>It&#x27;s also worth noting that we&#x27;re calling <code>super</code> in the <code>NewConstructorViewModel</code> constructor. This invokes the constructor of the <code>ViewModel</code> base (or &quot;super&quot;) class. TypeScript enforces that we pass the appropriate arguments (in our case a single <code>string</code>).</p><h2>Wrapping it up</h2><p>We&#x27;ve seen that TypeScript ensures correct usage of constructors when we have an abstract class. Importantly, all subclasses of abstract classes either:</p><ul><li><p>do not implement a constructor at all, leaving the base class constructor (the abstract constructor) to become the default constructor of the subclass <em>or</em></p></li><li><p>implement their own constructor which invokes the base (or &quot;super&quot;) class constructor with the correct arguments.</p></li></ul><p><a href="https://blog.logrocket.com/typescript-abstract-classes-and-constructors/">This post was originally published on LogRocket.</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Directory.Build.props: C# 9 for all your projects]]></title>
            <link>https://blog.johnnyreilly.com/2021/07/14/directory-build-props-c-sharp-9-for-all</link>
            <guid>Directory.Build.props: C# 9 for all your projects</guid>
            <pubDate>Wed, 14 Jul 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[.NET Core can make use of C# 9 by making some changes to your .csproj files. There is a way to opt all projects in a solution into this behaviour in a single place, through using a Directory.Build.props file and / or a Directory.Build.targets file. Here's how to do it.]]></description>
            <content:encoded><![CDATA[<p>.NET Core can make use of C# 9 by making some changes to your <code>.csproj</code> files. There is a way to opt all projects in a solution into this behaviour in a <em>single</em> place, through using a <code>Directory.Build.props</code> file and / or a <code>Directory.Build.targets</code> file. Here&#x27;s how to do it.</p><p><img src="../static/blog/2021-07-14-directory-build-props-c-sharp-9-for-all/title-image.png" alt="title image showing name of post and the C# logo"/></p><h2>&quot;have you the good news about <code>Directory.Build.props</code>&quot;?</h2><p><a href="./2021-07-01-c-sharp-9-azure-functions-in-process.md">I wrote recently about using C# 9 with in-process Azure Functions.</a> What that amounted to, was using C# 9 with .NET Core.</p><p>One of the best things about blogging, is all that you get to learn along the way. After I put up that post, <a href="https://twitter.com/danielearwicker">Daniel Earwicker</a> was kind enough to send this message:</p><p><a href="https://twitter.com/danielearwicker/status/1412678642203828226"><img src="../static/blog/2021-07-14-directory-build-props-c-sharp-9-for-all/daniel-earwicker-tweet.png" alt="title image showing name of post and the C# logo"/></a></p><p>I was intrigued that Daniel was able to configure all the projects in a solution to use the same approach using some strange incantations named <code>Directory.Build.props</code> and <code>Directory.Build.targets</code>. <a href="https://docs.microsoft.com/en-us/visualstudio/msbuild/customize-your-build?view=vs-2019#directorybuildprops-and-directorybuildtargets">Microsoft describes them thusly</a>:</p><blockquote><p>Prior to MSBuild version 15, if you wanted to provide a new, custom property to projects in your solution, you had to manually add a reference to that property to every project file in the solution. Or, you had to define the property in a <code>.props</code> file and then explicitly import the <code>.props</code> file in every project in the solution, among other things.</p><p>However, now you can add a new property to every project in one step by defining it in a single file called <code>Directory.Build.props</code> in the root folder that contains your source.</p></blockquote><p>Let&#x27;s see if we can put it to use.</p><h2><code>Directory.Build.props</code>: C# 9 for all</h2><p>So, rather than us updating each of our <code>.csproj</code> files, we should be able to create a <code>Directory.Build.props</code> file to sit alongside our <code>.sln</code> file in the root of our source code. We&#x27;ll add this into the file:</p><pre><code class="language-xml">&lt;Project&gt;
 &lt;PropertyGroup&gt;
    &lt;!-- use C# 9 --&gt;
    &lt;LangVersion&gt;9.0&lt;/LangVersion&gt;
 &lt;/PropertyGroup&gt;
 &lt;ItemGroup&gt;
    &lt;!-- allows some C# 9 support with .NET Core 3.1 https://github.com/manuelroemer/IsExternalInit --&gt;
    &lt;PackageReference Include=&quot;IsExternalInit&quot; Version=&quot;1.0.1&quot;&gt;
      &lt;IncludeAssets&gt;runtime; build; native; contentfiles; analyzers; buildtransitive&lt;/IncludeAssets&gt;
      &lt;PrivateAssets&gt;all&lt;/PrivateAssets&gt;
    &lt;/PackageReference&gt;
  &lt;/ItemGroup&gt;
&lt;/Project&gt;
</code></pre><p>Now we&#x27;re free to add projects into the solution, which will <em>already</em> support C# 9 without us taking any further steps. It&#x27;s as simple as that! Thanks to Daniel for sharing this super handy tip. ❤️🌻</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[webpack? esbuild? Why not both?]]></title>
            <link>https://blog.johnnyreilly.com/2021/07/11/webpack-esbuild-why-not-both</link>
            <guid>webpack? esbuild? Why not both?</guid>
            <pubDate>Sun, 11 Jul 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Builds can be made faster using tools like esbuild. However, if you're invested in webpack but would still like to take advantage of speedier builds, there is a way. This post takes us through using esbuild alongside webpack using esbuild-loader.]]></description>
            <content:encoded><![CDATA[<p>Builds can be made faster using tools like <a href="https://github.com/evanw/esbuild">esbuild</a>. However, if you&#x27;re invested in <a href="https://github.com/webpack/webpack">webpack</a> but would still like to take advantage of speedier builds, there is a way. This post takes us through using esbuild alongside webpack using <a href="https://github.com/privatenumber/esbuild-loader">esbuild-loader</a>.</p><p><img src="../static/blog/2021-07-11-webpack-esbuild-why-not-both/webpack-esbuild-why-not-both.jpg" alt="A screenshot of the &quot;why not both&quot; meme adapted to include webpack and esbuild"/></p><h2>Web development</h2><p>With apologies to those suffering from JavaScript fatigue, once again the world of web development is evolving. It&#x27;s long been common practice to run your JavaScript and TypeScript through some kind of Node.js based build tool, like webpack or rollup.js. These tools are written in the same language they compile to; JavaScript (or TypeScript). The new kids on the blog are tools like <a href="https://github.com/evanw/esbuild">esbuild</a>, <a href="https://github.com/vitejs/vite">Vite</a> and <a href="https://github.com/swc-project/swc">swc</a>. The significant difference between these and their predecessors is that they are written in languages like Go and Rust. Go and Rust enjoy far greater performance than JavaScript. This translates into significantly faster builds. If you&#x27;d like to read about esbuild directly there&#x27;s a <a href="https://blog.logrocket.com/fast-javascript-bundling-with-esbuild/">great post</a> about it.</p><p>These new tools are transformative and represent a likely future of build tooling for the web. In the long term, the likes of esbuild, Vite and friends may well come to displace the current standard build tools. So the webpacks, the rollups and so on.</p><p>However, that’s the long term. There’s a lot of projects out there that are already heavily invested in their current build tooling. Mostly webpack. Migrating to a new build tool is no small piece of work. New projects might start with Vite, but existing ones are less likely to be ported. There’s a reason webpack is so popular. It does a lot of things very well indeed. It&#x27;s battle tested on large projects; it&#x27;s mature and it handles many use cases.</p><p>So if you’re a team that wants to have faster builds, but doesn’t have the time to go through a big migration... Is there anything you can do? Yes. There’s a middle ground to be explored. There’s a relatively new project named <a href="https://github.com/privatenumber/esbuild-loader">esbuild-loader</a> developed by <a href="https://twitter.com/privatenumbr">hiroki osame</a>. It&#x27;s a webpack loader built on top of esbuild. It allows users to swap out <code>ts-loader</code> or <code>babel-loader</code> with itself, and massively improve build speeds.</p><p>To declare an interest here, I&#x27;m the primary maintainer of <a href="https://github.com/TypeStrong/ts-loader">ts-loader</a>; a popular TypeScript loader that is commonly used with webpack. However, I feel strongly that the important thing here is developer productivity. As Node.js-based projects, <code>ts-loader</code> and <code>babel-loader</code> will never be able to compete with <code>esbuild-loader</code> in the same way. As a language, Go really, uh, goes!</p><p>Whilst esbuild may not work for all use cases, it will for the majority. As such <code>esbuild-loader</code> represents a middle ground; and an early way to get access to the increased build speed that esbuild offers <em>without</em> saying goodbye to webpack. This post will look at using <code>esbuild-loader</code> in your webpack setup.</p><h2>Migrating an existing project to esbuild</h2><p>It&#x27;s very straightforward to migrate a project which uses either <code>babel-loader</code> or <code>ts-loader</code> to <code>esbuild-loader</code>. You install the dependency:</p><pre><code class="language-bash">npm i -D esbuild-loader
</code></pre><p>Then if we are currently using <code>babel-loader</code>, we make this change to our <code>webpack.config.js</code>:</p><pre><code class="language-diff">  module.exports = {
    module: {
      rules: [
-       {
-         test: /\.js$/,
-         use: &#x27;babel-loader&#x27;,
-       },
+       {
+         test: /\.js$/,
+         loader: &#x27;esbuild-loader&#x27;,
+         options: {
+           loader: &#x27;jsx&#x27;,  // Remove this if you&#x27;re not using JSX
+           target: &#x27;es2015&#x27;  // Syntax to compile to (see options below for possible values)
+         }
+       },

        ...
      ],
    },
  }
</code></pre><p>Or if we&#x27;re using <code>ts-loader</code>, we make this change to our <code>webpack.config.js</code>:</p><pre><code class="language-diff">  module.exports = {
    module: {
      rules: [
-       {
-         test: /\.tsx?$/,
-         use: &#x27;ts-loader&#x27;
-       },
+       {
+         test: /\.tsx?$/,
+         loader: &#x27;esbuild-loader&#x27;,
+         options: {
+           loader: &#x27;tsx&#x27;,  // Or &#x27;ts&#x27; if you don&#x27;t need tsx
+           target: &#x27;es2015&#x27;
+         }
+       },

        ...
      ]
    },
  }
</code></pre><h2>Creating a baseline application</h2><p>Let&#x27;s try <code>esbuild-loader</code> out in practice. We&#x27;re going to create a new React application using <a href="https://create-react-app.dev/">Create React App</a>:</p><pre><code class="language-bash">npx create-react-app my-app --template typescript
</code></pre><p>This will scaffold out a new React application using TypeScript in the <code>my-app</code> directory. It&#x27;s worth knowing that Create React App uses <code>babel-loader</code> behind the scenes.</p><p>CRA also uses the <a href="https://github.com/TypeStrong/fork-ts-checker-webpack-plugin">fork-ts-checker-webpack-plugin</a> to provide TypeScript type checking. This is very useful, as esbuild <em>just</em> does transpilation and <a href="https://esbuild.github.io/faq/#upcoming-roadmap">does not intend to provide type checking support</a>. So it&#x27;s tremendous we still have that plugin in place as otherwise we would lose type checking.</p><p>So we can understand the advantage of moving to esbuild, we first need a baseline to understand what performance looks like with babel-loader. We&#x27;ll run <code>time npm run build</code> to execute a build of our simple app:</p><p><img src="../static/blog/2021-07-11-webpack-esbuild-why-not-both/create-react-app-raw.png" alt="A screenshot of the completed build for Create React App"/></p><p>Our complete build, TypeScript type checking, transpilation, minification and so on, all took 22.08 seconds. The question now is, what will happen if we drop esbuild into the mix?</p><h2>CRACO</h2><p>One way to customise a Create React App build is by running <code>npm run eject</code> and then customising the code that CRA pumps out. Doing so is fine, but it means you can&#x27;t keep track with CRA&#x27;s evolution. An alternative is to use a tool like <a href="https://github.com/gsoft-inc/craco">CRACO</a> which allows us to tweak configuration in place. It describes itself this way:</p><blockquote><p><em>C</em>reate <em>R</em>eact <em>A</em>pp <em>C</em>onfiguration <em>O</em>verride is an easy and comprehensible configuration layer for create-react-app.</p></blockquote><p>We&#x27;re going to use CRACO, so we&#x27;ll add <code>esbuild-loader</code> and CRACO as dependencies:</p><pre><code class="language-bash">npm install @craco/craco esbuild-loader --save-dev
</code></pre><p>Then we&#x27;ll swap over our various <code>scripts</code> in our <code>package.json</code> to use <code>CRACO</code>:</p><pre><code class="language-json">&quot;start&quot;: &quot;craco start&quot;,
&quot;build&quot;: &quot;craco build&quot;,
&quot;test&quot;: &quot;craco test&quot;,
</code></pre><p>Our app now uses CRACO, but we haven&#x27;t yet configured it. So we&#x27;ll add a <code>craco.config.js</code> file to the root of our project. This is where we swap out <code>babel-loader</code> for <code>esbuild-loader</code>:</p><pre><code class="language-js">const {
  addAfterLoader,
  removeLoaders,
  loaderByName,
  getLoaders,
  throwUnexpectedConfigError,
} = require(&#x27;@craco/craco&#x27;);
const { ESBuildMinifyPlugin } = require(&#x27;esbuild-loader&#x27;);

const throwError = (message) =&gt;
  throwUnexpectedConfigError({
    packageName: &#x27;craco&#x27;,
    githubRepo: &#x27;gsoft-inc/craco&#x27;,
    message,
    githubIssueQuery: &#x27;webpack&#x27;,
  });

module.exports = {
  webpack: {
    configure: (webpackConfig, { paths }) =&gt; {
      const { hasFoundAny, matches } = getLoaders(
        webpackConfig,
        loaderByName(&#x27;babel-loader&#x27;)
      );
      if (!hasFoundAny) throwError(&#x27;failed to find babel-loader&#x27;);

      console.log(&#x27;removing babel-loader&#x27;);
      const { hasRemovedAny, removedCount } = removeLoaders(
        webpackConfig,
        loaderByName(&#x27;babel-loader&#x27;)
      );
      if (!hasRemovedAny) throwError(&#x27;no babel-loader to remove&#x27;);
      if (removedCount !== 2)
        throwError(&#x27;had expected to remove 2 babel loader instances&#x27;);

      console.log(&#x27;adding esbuild-loader&#x27;);

      const tsLoader = {
        test: /\.(js|mjs|jsx|ts|tsx)$/,
        include: paths.appSrc,
        loader: require.resolve(&#x27;esbuild-loader&#x27;),
        options: {
          loader: &#x27;tsx&#x27;,
          target: &#x27;es2015&#x27;,
        },
      };

      const { isAdded: tsLoaderIsAdded } = addAfterLoader(
        webpackConfig,
        loaderByName(&#x27;url-loader&#x27;),
        tsLoader
      );
      if (!tsLoaderIsAdded) throwError(&#x27;failed to add esbuild-loader&#x27;);
      console.log(&#x27;added esbuild-loader&#x27;);

      console.log(&#x27;adding non-application JS babel-loader back&#x27;);
      const { isAdded: babelLoaderIsAdded } = addAfterLoader(
        webpackConfig,
        loaderByName(&#x27;esbuild-loader&#x27;),
        matches[1].loader // babel-loader
      );
      if (!babelLoaderIsAdded)
        throwError(&#x27;failed to add back babel-loader for non-application JS&#x27;);
      console.log(&#x27;added non-application JS babel-loader back&#x27;);

      console.log(&#x27;replacing TerserPlugin with ESBuildMinifyPlugin&#x27;);
      webpackConfig.optimization.minimizer = [
        new ESBuildMinifyPlugin({
          target: &#x27;es2015&#x27;,
        }),
      ];

      return webpackConfig;
    },
  },
};
</code></pre><p>So what&#x27;s happening here? The script looks for <code>babel-loader</code> usages in the default Create React App config. There will be two; one for TypeScript / JavaScript application code (we want to replace this) and one for non application JavaScript code. It&#x27;s not too clear what non application JavaScript code there is or can be, so we&#x27;ll leave it in place; it may be important. Significantly, the code we care about is the application code.</p><p>You cannot remove a <em>single</em> loader using <code>CRACO</code>, so instead we&#x27;ll remove both and we&#x27;ll add back the non application JavaScript <code>babel-loader</code>. We&#x27;ll also add <code>esbuild-loader</code> with the <code>{ loader: &#x27;tsx&#x27;, target: &#x27;es2015&#x27; }</code> option set (to ensure we can process JSX/TSX).</p><p>Finally we&#x27;ll swap out using Terser for JavaScript minification for esbuild as well.</p><p>Our migration is complete. The next time we build we&#x27;ll have Create React App running using <code>esbuild-loader</code> <em>without</em> having ejected. Once again we&#x27;ll run <code>time npm run build</code> to execute a build of our simple app and determine how long it takes:</p><p><img src="../static/blog/2021-07-11-webpack-esbuild-why-not-both/create-react-app-esbuild.png" alt="A screenshot of the completed build for Create React App with esbuild"/></p><p>Our complete build, TypeScript type checking, transpilation, minification and so on, all took 13.85 seconds. By migrating to <code>esbuild-loader</code> we&#x27;ve reduced our overall compilation time by approximately one third; this is a tremendous improvement!</p><p>As your codebase scales and your application grows, compilation time can skyrocket also. With <code>esbuild-loader</code> you should get ongoing benefits to your build time.</p><p><a href="https://blog.logrocket.com/webpack-or-esbuild-why-not-both/">This post was originally published on LogRocket.</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Output connection strings and keys from Azure Bicep]]></title>
            <link>https://blog.johnnyreilly.com/2021/07/07/output-connection-strings-and-keys-from-azure-bicep</link>
            <guid>Output connection strings and keys from Azure Bicep</guid>
            <pubDate>Wed, 07 Jul 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[If we're provisioning resources in Azure with Bicep, we may have a need to acquire the connection strings and keys of our newly deployed infrastructure. For example, the connection strings of an event hub or the access keys of a storage account. Perhaps we'd like to use them to run an end-to-end test, perhaps we'd like to store these secrets somewhere for later consumption. This post shows how to do that using Bicep and the listKeys helper. Optionally it shows how we could consume this in Azure Pipelines.]]></description>
            <content:encoded><![CDATA[<p>If we&#x27;re provisioning resources in Azure with Bicep, we may have a need to acquire the connection strings and keys of our newly deployed infrastructure. For example, the connection strings of an event hub or the access keys of a storage account. Perhaps we&#x27;d like to use them to run an end-to-end test, perhaps we&#x27;d like to store these secrets somewhere for later consumption. This post shows how to do that using Bicep and the <code>listKeys</code> helper. Optionally it shows how we could consume this in Azure Pipelines.</p><p>An alternative approach would be permissioning our pipeline to access the resources directly. You can read about that approach <a href="2021-09-12-permissioning-azure-pipelines-bicep-role-assignments.md">here</a>.</p><p><img src="../static/blog/2021-07-07-output-connection-strings-and-keys-from-azure-bicep/title-image.jpg" alt="image which contains the blog title"/></p><h2>Event Hub connection string</h2><p>First of all, let&#x27;s provision an Azure Event Hub. This involves deploying an event hub namespace, an event hub in that namespace and an authorization rule. The following Bicep will do this for us:</p><pre><code class="language-bicep">// Create an event hub namespace

var eventHubNamespaceName = &#x27;evhns-demo&#x27;

resource eventHubNamespace &#x27;Microsoft.EventHub/namespaces@2021-01-01-preview&#x27; = {
  name: eventHubNamespaceName
  location: resourceGroup().location
  sku: {
    name: &#x27;Standard&#x27;
    tier: &#x27;Standard&#x27;
    capacity: 1
  }
  properties: {
    zoneRedundant: true
  }
}

// Create an event hub inside the namespace

var eventHubName = &#x27;evh-demo&#x27;

resource eventHubNamespaceName_eventHubName &#x27;Microsoft.EventHub/namespaces/eventhubs@2021-01-01-preview&#x27; = {
  parent: eventHubNamespace
  name: eventHubName
  properties: {
    messageRetentionInDays: 7
    partitionCount: 1
  }
}

// Grant Listen and Send on our event hub

resource eventHubNamespaceName_eventHubName_ListenSend &#x27;Microsoft.EventHub/namespaces/eventhubs/authorizationRules@2021-01-01-preview&#x27; = {
  parent: eventHubNamespaceName_eventHubName
  name: &#x27;ListenSend&#x27;
  properties: {
    rights: [
      &#x27;Listen&#x27;
      &#x27;Send&#x27;
    ]
  }
  dependsOn: [
    eventHubNamespace
  ]
}
</code></pre><p>When this is deployed to Azure, it will result in creating something like this:</p><p><img src="../static/blog/2021-07-07-output-connection-strings-and-keys-from-azure-bicep/event-hub-connection-string.png" alt="screenshot of event hub connection strings in the Azure Portal"/></p><p>As we can see, there are connection strings available which can be used to access the event hub. How do we get a connection string that we can play with? It&#x27;s easily achieved by appending the following to our Bicep:</p><pre><code class="language-bicep">// Determine our connection string

var eventHubNamespaceConnectionString = listKeys(eventHubNamespaceName_eventHubName_ListenSend.id, eventHubNamespaceName_eventHubName_ListenSend.apiVersion).primaryConnectionString

// Output our variables

output eventHubNamespaceConnectionString string = eventHubNamespaceConnectionString
output eventHubName string = eventHubName
</code></pre><p>What we&#x27;re doing here is using the <a href="https://docs.microsoft.com/en-us/azure/azure-resource-manager/bicep/bicep-functions-resource#list"><code>listKeys</code></a> helper on our authorization rule and retrieving the handy <code>primaryConnectionString</code>, which is then exposed as an output variable.</p><h2>Storage Account connection string</h2><p>We&#x27;d like to obtain a connection string for a storage account also. Let&#x27;s put together a Bicep file that creates a storage account and a container therein. (Incidentally, it&#x27;s fairly common to have a storage account provisioned alongside an event hub to facilitate reading from an event hub.)</p><pre><code class="language-bicep">// Create a storage account

var storageAccountName = &#x27;stdemo&#x27;

resource eventHubNamespaceName_storageAccount &#x27;Microsoft.Storage/storageAccounts@2021-02-01&#x27; = {
  name: storageAccountName
  location: resourceGroup().location
  sku: {
    name: &#x27;Standard_LRS&#x27;
    tier: &#x27;Standard&#x27;
  }
  kind: &#x27;StorageV2&#x27;
  properties: {
    networkAcls: {
      bypass: &#x27;AzureServices&#x27;
      defaultAction: &#x27;Allow&#x27;
    }
    accessTier: &#x27;Hot&#x27;
    allowBlobPublicAccess: false
    minimumTlsVersion: &#x27;TLS1_2&#x27;
    allowSharedKeyAccess: true
  }
}

// create a container inside that storage account

var blobContainerName = &#x27;test-container&#x27;

resource storageAccountName_default_containerName &#x27;Microsoft.Storage/storageAccounts/blobServices/containers@2021-02-01&#x27; = {
  name: &#x27;${storageAccountName}/default/${blobContainerName}&#x27;
  dependsOn: [
    eventHubNamespaceName_storageAccount
  ]
}
</code></pre><p>When this is deployed to Azure, it will result in creating something like this:</p><p><img src="../static/blog/2021-07-07-output-connection-strings-and-keys-from-azure-bicep/storage-account-access-keys.png" alt="screenshot of storage account access keys in the Azure Portal"/></p><p>Again we can see, there are connection strings available in the Azure Portal, which can be used to access the storage account. However, things aren&#x27;t quite as simple as previously; in that there doesn&#x27;t seem to be a way to directly acquire a connection string. What we can do, is acquire a key; and construct ourselves a connection string with that. Here&#x27;s how:</p><pre><code class="language-bicep">// Determine our connection string

var blobStorageConnectionString = &#x27;DefaultEndpointsProtocol=https;AccountName=${eventHubNamespaceName_storageAccount.name};EndpointSuffix=${environment().suffixes.storage};AccountKey=${listKeys(eventHubNamespaceName_storageAccount.id, eventHubNamespaceName_storageAccount.apiVersion).keys[0].value}&#x27;

// Output our variable

output blobStorageConnectionString string = blobStorageConnectionString
output blobContainerName string = blobContainerName
</code></pre><p>If you just wanted to know how to acquire connection strings from Bicep then you can stop now; we&#x27;re done! But if you&#x27;re curious on how the Bicep might connect to <del>the shoulder</del> Azure Pipelines... Read on.</p><h2>From Bicep to Azure Pipelines</h2><p>If we put together our snippets above into a single Bicep file it would look like this:</p><pre><code class="language-bicep">// Create an event hub namespace

var eventHubNamespaceName = &#x27;evhns-demo&#x27;

resource eventHubNamespace &#x27;Microsoft.EventHub/namespaces@2021-01-01-preview&#x27; = {
  name: eventHubNamespaceName
  location: resourceGroup().location
  sku: {
    name: &#x27;Standard&#x27;
    tier: &#x27;Standard&#x27;
    capacity: 1
  }
  properties: {
    zoneRedundant: true
  }
}

// Create an event hub inside the namespace

var eventHubName = &#x27;evh-demo&#x27;

resource eventHubNamespaceName_eventHubName &#x27;Microsoft.EventHub/namespaces/eventhubs@2021-01-01-preview&#x27; = {
  parent: eventHubNamespace
  name: eventHubName
  properties: {
    messageRetentionInDays: 7
    partitionCount: 1
  }
}

// Grant Listen and Send on our event hub

resource eventHubNamespaceName_eventHubName_ListenSend &#x27;Microsoft.EventHub/namespaces/eventhubs/authorizationRules@2021-01-01-preview&#x27; = {
  parent: eventHubNamespaceName_eventHubName
  name: &#x27;ListenSend&#x27;
  properties: {
    rights: [
      &#x27;Listen&#x27;
      &#x27;Send&#x27;
    ]
  }
  dependsOn: [
    eventHubNamespace
  ]
}

// Create a storage account

var storageAccountName = &#x27;stdemo&#x27;

resource eventHubNamespaceName_storageAccount &#x27;Microsoft.Storage/storageAccounts@2021-02-01&#x27; = {
  name: storageAccountName
  location: resourceGroup().location
  sku: {
    name: &#x27;Standard_LRS&#x27;
    tier: &#x27;Standard&#x27;
  }
  kind: &#x27;StorageV2&#x27;
  properties: {
    networkAcls: {
      bypass: &#x27;AzureServices&#x27;
      defaultAction: &#x27;Allow&#x27;
    }
    accessTier: &#x27;Hot&#x27;
    allowBlobPublicAccess: false
    minimumTlsVersion: &#x27;TLS1_2&#x27;
    allowSharedKeyAccess: true
  }
}

// create a container inside that storage account

var blobContainerName = &#x27;test-container&#x27;

resource storageAccountName_default_containerName &#x27;Microsoft.Storage/storageAccounts/blobServices/containers@2021-02-01&#x27; = {
  name: &#x27;${storageAccountName}/default/${blobContainerName}&#x27;
  dependsOn: [
    eventHubNamespaceName_storageAccount
  ]
}

// Determine our connection strings

var blobStorageConnectionString       = &#x27;DefaultEndpointsProtocol=https;AccountName=${eventHubNamespaceName_storageAccount.name};EndpointSuffix=${environment().suffixes.storage};AccountKey=${listKeys(eventHubNamespaceName_storageAccount.id, eventHubNamespaceName_storageAccount.apiVersion).keys[0].value}&#x27;
var eventHubNamespaceConnectionString = listKeys(eventHubNamespaceName_eventHubName_ListenSend.id, eventHubNamespaceName_eventHubName_ListenSend.apiVersion).primaryConnectionString

// Output our variables

output blobStorageConnectionString string = blobStorageConnectionString
output blobContainerName string = blobContainerName
output eventHubNamespaceConnectionString string = eventHubNamespaceConnectionString
output eventHubName string = eventHubName
</code></pre><p>This might be consumed in an Azure Pipeline that looks like this:</p><pre><code class="language-yml">- bash: az bicep build --file infra/our-test-app/main.bicep
  displayName: &#x27;Compile Bicep to ARM&#x27;

- task: AzureResourceManagerTemplateDeployment@3
  name: DeploySharedInfra
  displayName: Deploy Shared ARM Template
  inputs:
    deploymentScope: Resource Group
    azureResourceManagerConnection: ${{ parameters.serviceConnection }}
    subscriptionId: $(subscriptionId)
    action: Create Or Update Resource Group
    resourceGroupName: $(azureResourceGroup)
    location: $(location)
    templateLocation: Linked artifact
    csmFile: &#x27;infra/our-test-app/main.json&#x27; # created by bash script
    deploymentMode: Incremental
    deploymentOutputs: deployOutputs

- task: PowerShell@2
  name: &#x27;SetOutputVariables&#x27;
  displayName: &#x27;Set Output Variables&#x27;
  inputs:
    targetType: inline
    script: |
      $armOutputObj = &#x27;$(deployOutputs)&#x27; | ConvertFrom-Json
      $armOutputObj.PSObject.Properties | ForEach-Object {
        $keyname = $_.Name
        $value = $_.Value.value

        # Creates a standard pipeline variable
        Write-Output &quot;##vso[task.setvariable variable=$keyName;]$value&quot;

        # Creates an output variable
        Write-Output &quot;##vso[task.setvariable variable=$keyName;issecret=true;isOutput=true]$value&quot;

        # Display keys in pipeline
        Write-Output &quot;output variable: $keyName&quot;
      }
    pwsh: true
</code></pre><p>Above we can see:</p><ul><li>the Bicep get compiled to ARM</li><li>the ARM is deployed to Azure, with <code>deploymentOutputs</code> being passed out at the end</li><li>the outputs are turned into secret output variables inside the pipeline (the names of which are printed to the console)</li></ul><p>With the above in place, we now have all of our variables in place; <code>blobStorageConnectionString</code>, <code>blobContainerName</code>, <code>eventHubNamespaceConnectionString</code> and <code>eventHubName</code>. These could now be consumed in whatever way is useful. Consider the following:</p><pre><code class="language-yml">- task: UseDotNet@2
  displayName: &#x27;Install .NET Core SDK 3.1.x&#x27;
  inputs:
    packageType: &#x27;sdk&#x27;
    version: 3.1.x

- task: DotNetCoreCLI@2
  displayName: &#x27;dotnet run eventhub test&#x27;
  inputs:
    command: &#x27;run&#x27;
    arguments: &#x27;eventhub test --eventHubNamespaceConnectionString &quot;$(eventHubNamespaceConnectionString)&quot; --eventHubName &quot;$(eventHubName)&quot; --blobStorageConnectionString &quot;$(blobStorageConnectionString)&quot; --blobContainerName &quot;$(blobContainerName)&quot;&#x27;
    workingDirectory: &#x27;$(Build.SourcesDirectory)/OurTestApp&#x27;
</code></pre><p>Here we run a .NET application and pass it our connection strings. Please note, there&#x27;s nothing .NET specific about what we&#x27;re doing above - it could be any kind of application, bash script or similar that consumes our connection strings. The significant thing is that we can acquire connection strings in an automated fashion, for use in whichever manner pleases us.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[C# 9 in-process Azure Functions]]></title>
            <link>https://blog.johnnyreilly.com/2021/07/01/c-sharp-9-azure-functions-in-process</link>
            <guid>C# 9 in-process Azure Functions</guid>
            <pubDate>Thu, 01 Jul 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[C# 9 has some amazing features. Azure Functions are have two modes: isolated and in-process. Whilst isolated supports .NET 5 (and hence C# 9), in-process supports .NET Core 3.1 (C# 8). This post shows how we can use C# 9 with in-process Azure Functions running on .NET Core 3.1.]]></description>
            <content:encoded><![CDATA[<p>C# 9 has some amazing features. Azure Functions are have two modes: isolated and in-process. Whilst isolated supports .NET 5 (and hence C# 9), in-process supports .NET Core 3.1 (C# 8). This post shows how we can use C# 9 with in-process Azure Functions running on .NET Core 3.1.</p><p><img src="../static/blog/2021-07-01-c-sharp-9-azure-functions-in-process/title-image.png" alt="title image showing name of post and the Azure Functions logo"/></p><h2>Azure Functions: in-process and isolated</h2><p>Historically .NET Azure Functions have been in-process. This changed with .NET 5 where a new model was introduced named &quot;isolated&quot;. <a href="https://techcommunity.microsoft.com/t5/apps-on-azure/net-on-azure-functions-roadmap/ba-p/2197916">To quote from the roadmap</a>:</p><blockquote><p>Running in an isolated process decouples .NET functions from the Azure Functions host—allowing us to more easily support new .NET versions and address pain points associated with sharing a single process.</p></blockquote><p>However, the initial launch of isolated functions <a href="https://docs.microsoft.com/en-us/azure/azure-functions/dotnet-isolated-process-guide#differences-with-net-class-library-functions">does not have the full level of functionality enjoyed by in-process functions</a>. This will happen, according the roadmap:</p><blockquote><p>Long term, our vision is to have full feature parity out of process, bringing many of the features that are currently exclusive to the in-process model to the isolated model. We plan to begin delivering improvements to the isolated model after the .NET 6 general availability release.</p></blockquote><p>In the future, in-process functions will be retired in favour of isolated functions. However, it will be .NET 7 (scheduled to ship in November 2022) before that takes place:</p><p><img src="../static/blog/2021-07-01-c-sharp-9-azure-functions-in-process/dotnet-functions-roadmap.png" alt="the Azure Functions roadmap image illustrating the future of .NET functions taken from https://techcommunity.microsoft.com/t5/apps-on-azure/net-on-azure-functions-roadmap/ba-p/2197916"/></p><p>As the image taken from the roadmap shows, when .NET 5 shipped, it did not support in-process Azure Functions. When .NET 6 ships in November, it should.</p><p>In the meantime, we would like to use C# 9.</p><h2>Setting up a C# 8 project</h2><p>We&#x27;re have the <a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-run-local">Azure Functions Core Tools</a> installed, so let&#x27;s create a new function project:</p><pre><code class="language-bash">func new --worker-runtime dotnet --template &quot;Http Trigger&quot; --name &quot;HelloRecord&quot;
</code></pre><p>The above command scaffolds out a .NET Core 3.1 Azure function project which contains a single Azure function. The <code>--worker-runtime dotnet</code> parameter is what causes an in-process .NET Core 3.1 function being created. You should have a <code>.csproj</code> file that looks like this:</p><pre><code class="language-xml">&lt;Project Sdk=&quot;Microsoft.NET.Sdk&quot;&gt;
  &lt;PropertyGroup&gt;
    &lt;TargetFramework&gt;netcoreapp3.1&lt;/TargetFramework&gt;
    &lt;AzureFunctionsVersion&gt;v3&lt;/AzureFunctionsVersion&gt;
  &lt;/PropertyGroup&gt;
  &lt;ItemGroup&gt;
    &lt;PackageReference Include=&quot;Microsoft.NET.Sdk.Functions&quot; Version=&quot;3.0.11&quot; /&gt;
  &lt;/ItemGroup&gt;
  &lt;ItemGroup&gt;
    &lt;None Update=&quot;host.json&quot;&gt;
      &lt;CopyToOutputDirectory&gt;PreserveNewest&lt;/CopyToOutputDirectory&gt;
    &lt;/None&gt;
    &lt;None Update=&quot;local.settings.json&quot;&gt;
      &lt;CopyToOutputDirectory&gt;PreserveNewest&lt;/CopyToOutputDirectory&gt;
      &lt;CopyToPublishDirectory&gt;Never&lt;/CopyToPublishDirectory&gt;
    &lt;/None&gt;
  &lt;/ItemGroup&gt;
&lt;/Project&gt;
</code></pre><p>We&#x27;re running with C# 8 and .NET Core 3.1 at this point. What does it take to get us to C# 9?</p><h2>What does it take to get to C# 9?</h2><p>There&#x27;s a <a href="https://www.reddit.com/r/csharp/comments/kiplz8/can_i_use_c90_with_aspnet_core_31/">great post on Reddit addressing using C# 9 with .NET Core 3.1 which says:</a></p><blockquote><p>You can use <code>&lt;LangVersion&gt;9.0&lt;/LangVersion&gt;</code>, and VS even includes support for suggesting a language upgrade.</p><p>However, there are three categories of features in C#:</p><ol><li><p>features that are entirely part of the compiler. Those will work.</p></li><li><p>features that require BCL additions. Since you&#x27;re on the older BCL, those will need to be backported. For example, to use init; and record, you can use <a href="https://github.com/manuelroemer/IsExternalInit">https://github.com/manuelroemer/IsExternalInit</a>.</p></li><li><p>features that require runtime additions. Those cannot be added at all. For example, default interface members in C# 8, and covariant return types in C# 9.</p></li></ol></blockquote><p>Of the above, 1 and 2 add a tremendous amount of value. The features of 3 are great, but more niche. Speaking personally, I care a great deal about <a href="https://docs.microsoft.com/en-us/dotnet/csharp/whats-new/csharp-9#record-types">Record types</a>. So let&#x27;s apply this.</p><h2>Adding C# 9 to the in-process function</h2><p>To get C# into the mix, we want to make two changes:</p><ul><li>add a <code>&lt;LangVersion&gt;9.0&lt;/LangVersion&gt;</code> to the <code>&lt;PropertyGroup&gt;</code> element of our <code>.csproj</code> file</li><li>add a package reference to the <a href="https://github.com/manuelroemer/IsExternalInit"><code>IsExternalInit</code></a></li></ul><p>The applied changes look like this:</p><pre><code class="language-diff">&lt;Project Sdk=&quot;Microsoft.NET.Sdk&quot;&gt;
  &lt;PropertyGroup&gt;
    &lt;TargetFramework&gt;netcoreapp3.1&lt;/TargetFramework&gt;
+    &lt;LangVersion&gt;9.0&lt;/LangVersion&gt;
    &lt;AzureFunctionsVersion&gt;v3&lt;/AzureFunctionsVersion&gt;
  &lt;/PropertyGroup&gt;
  &lt;ItemGroup&gt;
    &lt;PackageReference Include=&quot;Microsoft.NET.Sdk.Functions&quot; Version=&quot;3.0.11&quot; /&gt;
+    &lt;PackageReference Include=&quot;IsExternalInit&quot; Version=&quot;1.0.1&quot; PrivateAssets=&quot;all&quot; /&gt;
  &lt;/ItemGroup&gt;
  &lt;ItemGroup&gt;
    &lt;None Update=&quot;host.json&quot;&gt;
      &lt;CopyToOutputDirectory&gt;PreserveNewest&lt;/CopyToOutputDirectory&gt;
    &lt;/None&gt;
    &lt;None Update=&quot;local.settings.json&quot;&gt;
      &lt;CopyToOutputDirectory&gt;PreserveNewest&lt;/CopyToOutputDirectory&gt;
      &lt;CopyToPublishDirectory&gt;Never&lt;/CopyToPublishDirectory&gt;
    &lt;/None&gt;
  &lt;/ItemGroup&gt;
&lt;/Project&gt;
</code></pre><p>If we used <code>dotnet add package IsExternalInit</code>, we might be using a different syntax in the <code>.csproj</code>. Be not afeard - that won&#x27;t affect usage.</p><h2>Making a C# 9 program</h2><p>Now we can theoretically use C# 9.... Let&#x27;s use C# 9. We&#x27;ll tweak our <code>HelloRecord.cs</code> file, add in a simple <code>record</code> named <code>MessageRecord</code> and tweak the <code>Run</code> method to use it:</p><pre><code class="language-cs">using System;
using System.IO;
using System.Threading.Tasks;
using Microsoft.AspNetCore.Mvc;
using Microsoft.Azure.WebJobs;
using Microsoft.Azure.WebJobs.Extensions.Http;
using Microsoft.AspNetCore.Http;
using Microsoft.Extensions.Logging;
using Newtonsoft.Json;

namespace tmp
{
    public record MessageRecord(string message);

    public static class HelloRecord
    {
        [FunctionName(&quot;HelloRecord&quot;)]
        public static async Task&lt;IActionResult&gt; Run(
            [HttpTrigger(AuthorizationLevel.Function, &quot;get&quot;, &quot;post&quot;, Route = null)] HttpRequest req,
            ILogger log)
        {
            log.LogInformation(&quot;C# HTTP trigger function processed a request.&quot;);

            string name = req.Query[&quot;name&quot;];

            string requestBody = await new StreamReader(req.Body).ReadToEndAsync();
            dynamic data = JsonConvert.DeserializeObject(requestBody);
            name = name ?? data?.name;

            var responseMessage = new MessageRecord(string.IsNullOrEmpty(name)
                ? &quot;This HTTP triggered function executed successfully. Pass a name in the query string or in the request body for a personalized response.&quot;
                : $&quot;Hello, {name}. This HTTP triggered function executed successfully.&quot;);

            return new OkObjectResult(responseMessage);
        }
    }
}
</code></pre><p>If we kick off our function with <code>func start</code>:</p><p><img src="../static/blog/2021-07-01-c-sharp-9-azure-functions-in-process/calling-hello-record.png" alt="screenshot of the output of the HelloRecord function"/></p><p>We can see we can compile, and output is as we might expect and hope. Likewise if we try and debug in VS Code, we can:</p><p><img src="../static/blog/2021-07-01-c-sharp-9-azure-functions-in-process/debugging-hello-record.png" alt="screenshot of the output of the HelloRecord function"/></p><h2>Best before...</h2><p>So, we&#x27;ve now a way to use C# 9 (or most of it) with in-process .NET Core 3.1 apps. This should serve until .NET 6 ships in November 2021 and we&#x27;re able to use C# 9 by default.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[React 18 and TypeScript]]></title>
            <link>https://blog.johnnyreilly.com/2021/06/30/react-18-and-typescript</link>
            <guid>React 18 and TypeScript</guid>
            <pubDate>Wed, 30 Jun 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[React 18 alpha has been released; but can we use it with TypeScript? The answer is "yes", but you need to do a couple of things to make that happen. This post will show you what to do.]]></description>
            <content:encoded><![CDATA[<p><a href="https://reactjs.org/blog/2021/06/08/the-plan-for-react-18.html">React 18 alpha has been released</a>; but can we use it with TypeScript? The answer is &quot;yes&quot;, but you need to do a couple of things to make that happen. This post will show you what to do.</p><h2>Creating a React App with TypeScript</h2><p>Let&#x27;s create ourselves a vanilla React TypeScript app with <a href="https://create-react-app.dev/">Create React App</a>:</p><pre><code class="language-shell">yarn create react-app my-app --template typescript
</code></pre><p>Now let&#x27;s upgrade the version of React to <code>@next</code>:</p><pre><code class="language-shell">yarn add react@next react-dom@next
</code></pre><p>Which will leave you with entries in the <code>package.json</code> which use React 18. It will likely look something like this:</p><pre><code class="language-json">    &quot;react&quot;: &quot;^18.0.0-alpha-e6be2d531&quot;,
    &quot;react-dom&quot;: &quot;^18.0.0-alpha-e6be2d531&quot;,
</code></pre><p>If we run <code>yarn start</code> we&#x27;ll find ourselves running a React 18 app. Exciting!</p><h2>Using the new APIs</h2><p>So let&#x27;s try using <a href="https://github.com/reactwg/react-18/discussions/5"><code>ReactDOM.createRoot</code></a> API. It&#x27;s this API that opts our application into using new features of React 18. We&#x27;ll open up <code>index.tsx</code> and make this change:</p><pre><code class="language-diff">-ReactDOM.render(
-  &lt;React.StrictMode&gt;
-    &lt;App /&gt;
-  &lt;/React.StrictMode&gt;,
-  document.getElementById(&#x27;root&#x27;)
-);
+const root = ReactDOM.createRoot(document.getElementById(&#x27;root&#x27;));
+
+root.render(
+  &lt;React.StrictMode&gt;
+    &lt;App /&gt;
+  &lt;/React.StrictMode&gt;
+);
</code></pre><p>If we were running JavaScript alone, this would work. However, because we&#x27;re using TypeScript as well, we&#x27;re now confronted with an error:</p><blockquote><p><code>Property &#x27;createRoot&#x27; does not exist on type &#x27;typeof import(&quot;/code/my-app/node_modules/@types/react-dom/index&quot;)&#x27;. TS2339</code></p></blockquote><p><img src="../static/blog/2021-06-30-react-18-and-typescript/createNode-error.png" alt="a screenshot of the Property &#x27;createRoot&#x27; does not exist error"/></p><p>This is the TypeScript compiler complaining that it doesn&#x27;t know anything about <code>ReactDOM.createRoot</code>. This is because the type definitions that are currently in place in our application don&#x27;t have that API defined.</p><p>Let&#x27;s upgrade our type definitions:</p><pre><code class="language-shell">yarn add @types/react @types/react-dom
</code></pre><p>We might reasonably hope that everything should work now. Alas it does not. The same error is presenting. TypeScript is not happy.</p><h2>Telling TypeScript about the new APIs</h2><p>If we take a look at the <a href="https://github.com/DefinitelyTyped/DefinitelyTyped/pull/53685">PR that added support for the APIs</a>, we&#x27;ll find some tips. If you look at one of the <a href="https://github.com/DefinitelyTyped/DefinitelyTyped/blob/a07e9cfb005682fb6be0a2e85113eac131c3006f/types/react/next.d.ts"><code>next.d.ts</code></a> you&#x27;ll find this info, courtesy of <a href="https://twitter.com/sebsilbermann">Sebastian Silbermann</a>:</p><pre><code class="language-ts">/**
 * These are types for things that are present in the upcoming React 18 release.
 *
 * Once React 18 is released they can just be moved to the main index file.
 *
 * To load the types declared here in an actual project, there are three ways. The easiest one,
 * if your `tsconfig.json` already has a `&quot;types&quot;` array in the `&quot;compilerOptions&quot;` section,
 * is to add `&quot;react/next&quot;` to the `&quot;types&quot;` array.
 *
 * Alternatively, a specific import syntax can to be used from a typescript file.
 * This module does not exist in reality, which is why the {} is important:
 *
 * ```ts
 * import {} from &#x27;react/next&#x27;
 * ```
 *
 * It is also possible to include it through a triple-slash reference:
 *
 * ```ts
 * /// &lt;reference types=&quot;react/next&quot; /&gt;
 * ```
 *
 * Either the import or the reference only needs to appear once, anywhere in the project.
 */
</code></pre><p>Let&#x27;s try the first item on the list. We&#x27;ll edit our <code>tsconfig.json</code> and add a new entry to the <code>&quot;compilerOptions&quot;</code> section:</p><pre><code class="language-json">    &quot;types&quot;: [&quot;react/next&quot;, &quot;react-dom/next&quot;]
</code></pre><p>If we restart our build with <code>yarn start</code> we&#x27;re now presented with a <em>different</em> error:</p><blockquote><p><code>Argument of type &#x27;HTMLElement | null&#x27; is not assignable to parameter of type &#x27;Element | Document | DocumentFragment | Comment&#x27;. Type &#x27;null&#x27; is not assignable to type &#x27;Element | Document | DocumentFragment | Comment&#x27;. TS2345</code></p></blockquote><p><img src="../static/blog/2021-06-30-react-18-and-typescript/null_is_not_assignable-error.png" alt="a screenshot of the null is not assignable error"/></p><p>Now this is actually nothing to do with issues with our new React type definitions. They are fine. This is TypeScript saying &quot;it&#x27;s not guaranteed that <code>document.getElementById(&#x27;root&#x27;)</code> returns something that is not <code>null</code>... since we&#x27;re in <code>strictNullChecks</code> mode you need to be sure <code>root</code> is not null&quot;.</p><p>We&#x27;ll deal with that by testing we do have an element in play before invoking <code>ReactDOM.createRoot</code>:</p><pre><code class="language-diff">-const root = ReactDOM.createRoot(document.getElementById(&#x27;root&#x27;));
+const rootElement = document.getElementById(&#x27;root&#x27;);
+if (!rootElement) throw new Error(&#x27;Failed to find the root element&#x27;);
+const root = ReactDOM.createRoot(rootElement);
</code></pre><p>Now that change is made, we have a working React 18 application, using TypeScript. Enjoy!</p><p><a href="https://blog.logrocket.com/how-to-use-typescript-with-react-18-alpha/">This post was originally published on LogRocket.</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Azure Functions and .NET 5: Query params, Dependency Injection, Bicep & Build]]></title>
            <link>https://blog.johnnyreilly.com/2021/06/11/azure-functions-dotnet-5-query-params-di-bicep</link>
            <guid>Azure Functions and .NET 5: Query params, Dependency Injection, Bicep &amp; Build</guid>
            <pubDate>Fri, 11 Jun 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[The upgrade of Azure Functions from .NET Core 3.1 to .NET 5 is significant. This post shows part of the upgrade: Query params, Dependency Injection, Bicep & Build]]></description>
            <content:encoded><![CDATA[<p>The upgrade of Azure Functions from .NET Core 3.1 to .NET 5 is significant. There&#x27;s an excellent <a href="https://codetraveler.io/2021/05/28/creating-azure-functions-using-net-5/">guide</a> for the general steps required to perform the upgrade. However there&#x27;s a number of (unrelated) items which are not covered by that post:</p><ul><li>Query params</li><li>Dependency Injection</li><li>Bicep</li><li>Build</li></ul><p>This post will show how to tackle these.</p><p><img src="../static/blog/2021-06-11-azure-functions-dotnet-5-query-params-di-bicep/title-image.png" alt="title image showing name of post and the Azure Functions logo"/></p><h2>Query params</h2><p>As part of the move to .NET 5 functions, we say goodbye to <a href="https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.http.httprequest?view=aspnetcore-5.0"><code>HttpRequest</code></a> and hello to <a href="https://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.functions.worker.http.httprequestdata?view=azure-dotnet"><code>HttpRequestData</code></a>. Now <code>HttpRequest</code> had a useful <a href="https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.http.httprequest.query?view=aspnetcore-5.0#Microsoft_AspNetCore_Http_HttpRequest_Query"><code>Query</code></a> property which allowed for the simple extraction of query parameters like so.</p><pre><code class="language-cs">var from = req.Query[&quot;from&quot;]
</code></pre><p><code>HttpRequestData</code> has no such property. However, it&#x27;s straightforward to make our own. It&#x27;s simply a matter of using <a href="https://docs.microsoft.com/en-us/dotnet/api/system.web.httputility.parsequerystring?view=net-5.0"><code>System.Web.HttpUtility.ParseQueryString</code></a> on <code>req.Url.Query</code> and using that:</p><pre><code class="language-cs">var query = System.Web.HttpUtility.ParseQueryString(req.Url.Query);
var from = query[&quot;from&quot;]
</code></pre><h2>Dependency Injection, local development and Azure Application Settings</h2><p>Dependency Injection is a much more familiar shape in .NET 5 if you&#x27;re familiar with .NET Core web apps. Once again we have a <code>Program.cs</code> file. To get the configuration built in such a way to support both local development and when deployed to Azure, there&#x27;s a few things to do. When deployed to Azure you&#x27;ll likely want to read from Azure Application Settings:</p><p><img src="../static/blog/2021-06-11-azure-functions-dotnet-5-query-params-di-bicep/application-settings.png" alt="screenshot of Azure Application Settings"/></p><p>To tackle both of these, you&#x27;ll want to use <code>AddJsonFile</code> and <code>AddEnvironmentVariables</code> in <code>ConfigureAppConfiguration</code>. A final <code>Program.cs</code> might look something like this:</p><pre><code class="language-cs">using System;
using System.Threading.Tasks;
using Microsoft.Extensions.Configuration;
using Microsoft.Extensions.DependencyInjection;
using Microsoft.Extensions.Hosting;

namespace MyApp
{
    public class Program
    {
        public static Task Main(string[] args)
        {
            var host = new HostBuilder()
                .ConfigureAppConfiguration(configurationBuilder =&gt;
                    configurationBuilder
                        .AddCommandLine(args)
                        // below is for local development
                        .AddJsonFile(&quot;local.settings.json&quot;, optional: true, reloadOnChange: true)
                        // below is what you need to read Application Settings in Azure
                        .AddEnvironmentVariables()
                )
                .ConfigureFunctionsWorkerDefaults()
                .ConfigureServices(services =&gt;
                {
                    services.AddLogging();
                    services.AddHttpClient();
                })
                .Build();

            return host.RunAsync();
        }
    }
}
</code></pre><p>With this approach in place, when the application runs, it should construct a configuration driven by all the providers required to run our application.</p><h2>Bicep</h2><p>When it comes to deploying to Azure via <a href="https://github.com/Azure/bicep">Bicep</a>, there&#x27;s some small tweaks required:</p><ul><li><code>appSettings.FUNCTIONS_WORKER_RUNTIME</code> becomes <code>dotnet-isolated</code></li><li><code>linuxFxVersion</code> becomes <code>DOTNET-ISOLATED|5.0</code></li></ul><p>Applied to the resource itself the diff looks like this:</p><pre><code class="language-diff">resource functionAppName_resource &#x27;Microsoft.Web/sites@2018-11-01&#x27; = {
  name: functionAppName
  location: location
  tags: tags_var
  kind: &#x27;functionapp,linux&#x27;
  identity: {
    type: &#x27;SystemAssigned&#x27;
  }
  properties: {
    serverFarmId: appServicePlanName_resource.id
    siteConfig: {
      http20Enabled: true
      remoteDebuggingEnabled: false
      minTlsVersion: &#x27;1.2&#x27;
      appSettings: [
        {
          name: &#x27;FUNCTIONS_EXTENSION_VERSION&#x27;
          value: &#x27;~3&#x27;
        }
        {
          name: &#x27;FUNCTIONS_WORKER_RUNTIME&#x27;
-          value: &#x27;dotnet&#x27;
+          value: &#x27;dotnet-isolated&#x27;
        }
        {
          name: &#x27;AzureWebJobsStorage&#x27;
          value: &#x27;DefaultEndpointsProtocol=https;AccountName=${storageAccountName};AccountKey=${listKeys(resourceId(&#x27;Microsoft.Storage/storageAccounts&#x27;, storageAccountName), &#x27;2019-06-01&#x27;).keys[0].value};EndpointSuffix=${environment().suffixes.storage}&#x27;
        }
      ]
      connectionStrings: [
        {
          name: &#x27;TableStorageConnectionString&#x27;
          connectionString: &#x27;DefaultEndpointsProtocol=https;AccountName=${storageAccountName};AccountKey=${listKeys(resourceId(&#x27;Microsoft.Storage/storageAccounts&#x27;, storageAccountName), &#x27;2019-06-01&#x27;).keys[0].value};EndpointSuffix=${environment().suffixes.storage}&#x27;
        }
      ]
-      linuxFxVersion: &#x27;DOTNETCORE|LTS&#x27;
+      linuxFxVersion: &#x27;DOTNET-ISOLATED|5.0&#x27;
      ftpsState: &#x27;Disabled&#x27;
      managedServiceIdentityId: 1
    }
    clientAffinityEnabled: false
    httpsOnly: true
  }
}
</code></pre><h2>Building .NET 5 functions</h2><p>Before signing off, there&#x27;s one more thing to slip in. When attempting to build .NET 5 Azure Functions with the .NET SDK <em>alone</em>, you&#x27;ll encounter this error:</p><pre><code>The framework &#x27;Microsoft.NETCore.App&#x27;, version &#x27;3.1.0&#x27; was not found.
</code></pre><p>Docs on this seem to be pretty short. The closest I came to docs was <a href="https://stackoverflow.com/questions/66938752/net-5-the-framework-microsoft-netcore-app-version-3-1-0-was-not-found/66938753#66938753">this comment on Stack Overflow</a>:</p><blockquote><p>To build .NET 5 functions, the .NET Core 3 SDK is required. So this must be installed alongside the 5.0.x sdk.</p></blockquote><p>So with Azure Pipelines you might have have something that looks like this:</p><pre><code class="language-yml">stages:
  - stage: build
    displayName: build
    pool:
      vmImage: &#x27;ubuntu-latest&#x27;
    jobs:
      - job: BuildAndTest
        displayName: &#x27;Build and Test&#x27;
        steps:
          # we need .NET Core SDK 3.1 too!
          - task: UseDotNet@2
            displayName: &#x27;Install .NET Core SDK 3.1&#x27;
            inputs:
              packageType: &#x27;sdk&#x27;
              version: 3.1.x

          - task: UseDotNet@2
            displayName: &#x27;Install .NET SDK 5.0&#x27;
            inputs:
              packageType: &#x27;sdk&#x27;
              version: 5.0.x

          - task: DotNetCoreCLI@2
            displayName: &#x27;function app test&#x27;
            inputs:
              command: test

          - task: DotNetCoreCLI@2
            displayName: &#x27;function app build&#x27;
            inputs:
              command: build
              arguments: &#x27;--configuration Release --output $(Build.ArtifactStagingDirectory)/MyApp&#x27;

          - task: DotNetCoreCLI@2
            displayName: &#x27;function app publish&#x27;
            inputs:
              command: publish
              arguments: &#x27;--configuration Release --output $(Build.ArtifactStagingDirectory)/MyApp /p:SourceRevisionId=$(Build.SourceVersion)&#x27;
              publishWebProjects: false
              modifyOutputPath: false
              zipAfterPublish: true

          - publish: $(Build.ArtifactStagingDirectory)/MyApp
            artifact: functionapp
</code></pre><p>Have fun building .NET 5 functions!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Azurite and Table Storage in a dev container]]></title>
            <link>https://blog.johnnyreilly.com/2021/05/15/azurite-and-table-storage-dev-container</link>
            <guid>Azurite and Table Storage in a dev container</guid>
            <pubDate>Sat, 15 May 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[It's great to be able to develop locally without needing a "real" database to connect to. Azurite is an Azure Storage emulator which exists to support just that. This post demonstrates how to run Azurite v3 in a dev container, such that you can access the Table Storage API, which is currently in preview.]]></description>
            <content:encoded><![CDATA[<p>It&#x27;s great to be able to develop locally without needing a &quot;real&quot; database to connect to. <a href="https://github.com/Azure/Azurite">Azurite</a> is an Azure Storage emulator which exists to support just that. This post demonstrates how to run Azurite v3 in a <a href="https://code.visualstudio.com/docs/remote/containers">dev container</a>, such that you can access the Table Storage API, which is currently in preview.</p><h2>Azurite in VS Code</h2><p><a href="https://github.com/Azure/Azurite/releases/tag/v3.12.0">Azurite v3.12.0</a> recently shipped, and with it came:</p><blockquote><p>Preview of Table Service in npm package and docker image. (Visual Studio Code extension doesn&#x27;t support Table Service in this release)</p></blockquote><p>You&#x27;ll note that whilst there&#x27;s a VS Code extension for Azurite, it doesn&#x27;t have support for the Table Service yet. However, we do have it available in the form of a Docker image. So whilst we may not be able to directly use the Table APIs of Azurite in VS Code, what we could do instead is use a dev container.</p><p>We&#x27;ll start by making ourselves a new directory and open VS Code in that location:</p><pre><code class="language-bash">mkdir azurite-devcontainer
code azurite-devcontainer
</code></pre><p>We&#x27;re going to initialise a dev container there for function apps based upon <a href="https://github.com/microsoft/vscode-dev-containers/tree/master/containers/azure-functions-dotnetcore-3.1">the example Azure Functions &amp; C# - .NET Core 3.1 container</a>. We&#x27;ll use it later to test our Azurite connectivity. To do that let&#x27;s create ourselves a <code>.devcontainer</code> directory:</p><pre><code class="language-bash">mkdir .devcontainer
</code></pre><p>And inside there we&#x27;ll add a <code>devcontainer.json</code>:</p><pre><code class="language-json">// For format details, see https://aka.ms/devcontainer.json. For config options, see the README at:
// https://github.com/microsoft/vscode-dev-containers/tree/v0.177.0/containers/azure-functions-dotnetcore-3.1
{
  &quot;name&quot;: &quot;Azurite and Azure Functions &amp; C# - .NET Core 3.1&quot;,
  &quot;dockerComposeFile&quot;: &quot;docker-compose.yml&quot;,
  &quot;service&quot;: &quot;app&quot;,
  &quot;workspaceFolder&quot;: &quot;/workspace&quot;,
  &quot;forwardPorts&quot;: [7071],

  // Set *default* container specific settings.json values on container create.
  &quot;settings&quot;: {
    &quot;terminal.integrated.defaultProfile.linux&quot;: &quot;/bin/bash&quot;
  },

  // Add the IDs of extensions you want installed when the container is created.
  &quot;extensions&quot;: [
    &quot;ms-azuretools.vscode-azurefunctions&quot;,
    &quot;ms-dotnettools.csharp&quot;
  ],

  // Use &#x27;postCreateCommand&#x27; to run commands after the container is created.
  // &quot;postCreateCommand&quot;: &quot;dotnet restore&quot;,

  // Comment out connect as root instead. More info: https://aka.ms/vscode-remote/containers/non-root.
  &quot;remoteUser&quot;: &quot;vscode&quot;
}
</code></pre><p>As we can see, we&#x27;re referencing a <code>docker-compose.yml</code> file; let&#x27;s add that:</p><pre><code class="language-yml">version: &#x27;3&#x27;

services:
  app:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        # On Linux, you may need to update USER_UID and USER_GID below if not your local UID is not 1000.
        USER_UID: 10000
        USER_GID: 10000

    init: true
    volumes:
      - ..:/workspace:cached

    # Overrides default command so things don&#x27;t shut down after the process ends.
    command: sleep infinity

    # Uncomment the next line to use a non-root user for all processes.
    user: vscode

  # run azurite and expose the relevant ports
  azurite:
    image: &#x27;mcr.microsoft.com/azure-storage/azurite&#x27;
    ports:
      - &#x27;10000:10000&#x27;
      - &#x27;10001:10001&#x27;
      - &#x27;10002:10002&#x27;
</code></pre><p>It consists of two services; <code>app</code> and <code>azurite</code>. <code>azurite</code> is the Docker image of Azurite, which exposes the Azurite ports so <code>app</code> can access it. Note the name of <code>azurite</code>; that will turn out to be significant later. We&#x27;re actually only going to use the Table Storage port of <code>10002</code>, but this would allow us to use Blobs and Queues also. The <code>azurite</code> service is effectively going to be executing this command for us when it runs:</p><pre><code class="language-bash">docker run -p 10000:10000 -p 10001:10001 -p 10002:10002 mcr.microsoft.com/azure-storage/azurite
</code></pre><p>Now let&#x27;s look at <code>app</code>. This is our Azure Functions container. It references a <code>Dockerfile</code> which we need to add:</p><pre><code class="language-dockerfile"># Find the Dockerfile for mcr.microsoft.com/azure-functions/dotnet:3.0-dotnet3-core-tools at this URL
# https://github.com/Azure/azure-functions-docker/blob/master/host/3.0/buster/amd64/dotnet/dotnet-core-tools.Dockerfile
FROM mcr.microsoft.com/azure-functions/dotnet:3.0-dotnet3-core-tools
</code></pre><p>We now have ourselves a dev container! VS Code should prompt us to reopen inside the container:</p><p><img src="../static/blog/2021-05-15-azurite-and-table-storage-dev-container/dev-container-start.gif" alt="The dev container starting"/></p><h2>Make a function app</h2><p>Now we&#x27;re inside our container, we&#x27;re going to make ourselves a function app that will use Azurite. Let&#x27;s fire up the terminal in VS Code and create a function app containing a simple HTTP function:</p><pre><code class="language-bash">mkdir src
cd src
func init TableStorage --dotnet
cd TableStorage
</code></pre><p>We need to add a package for the APIs which interact with Table Storage:</p><pre><code class="language-bash">dotnet restore
dotnet add package Microsoft.Azure.Cosmos.Table --version 1.0.8
</code></pre><p>The name is somewhat misleading, as it&#x27;s both for Cosmos <em>and</em> for Table Storage. Famously, naming things is hard 😉.</p><p>Our mission is to be able to write and read from Azurite Table Storage. We need something to read and write that we care about. I like to visit <a href="https://www.kew.org/kew-gardens">Kew Gardens</a> and so let&#x27;s imagine ourselves a system which tracks visitors to Kew.</p><p>We&#x27;re going to add a class called <code>KewGardensVisit</code>:</p><pre><code class="language-cs">using System;
using Microsoft.Azure.Cosmos.Table;

namespace TableStorage
{
    public class KewGardenVisit : TableEntity
    {
        public KewGardenVisit() {}
        public KewGardenVisit(DateTime arrivedAt, string memberId)
        {
            PartitionKey = arrivedAt.ToString(&quot;yyyy-MM-dd&quot;, System.Globalization.CultureInfo.InvariantCulture);
            RowKey = memberId;

            ArrivedAt = arrivedAt;
        }

        public DateTime ArrivedAt { get; set; }
    }
}
</code></pre><p>Now we have our entity, let&#x27;s add a class called <code>HelloAzuriteTableStorage</code> which will contain functions which interact with the storage:</p><pre><code class="language-cs">using System;
using System.Linq;
using System.Threading.Tasks;
using Microsoft.AspNetCore.Mvc;
using Microsoft.Azure.WebJobs;
using Microsoft.Azure.WebJobs.Extensions.Http;
using Microsoft.AspNetCore.Http;
using Microsoft.Extensions.Logging;
using Microsoft.Azure.Cosmos.Table;

namespace TableStorage
{
    public static class HelloAzuriteTableStorage
    {
        // Note how we&#x27;re addressing our azurite service
        const string AZURITE_TABLESTORAGE_CONNECTIONSTRING =
            &quot;DefaultEndpointsProtocol=http;&quot; +
            &quot;AccountName=devstoreaccount1;&quot; +
            &quot;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;&quot; +
            &quot;BlobEndpoint=http://azurite:10000/devstoreaccount1;&quot; +
            &quot;QueueEndpoint=http://azurite:10001/devstoreaccount1;&quot; +
            &quot;TableEndpoint=http://azurite:10002/devstoreaccount1;&quot;;
        const string TABLE_NAME = &quot;KewGardenVisits&quot;;

        [FunctionName(&quot;SaveVisit&quot;)]
        public static async Task&lt;IActionResult&gt; RunSaveVisit(
            [HttpTrigger(AuthorizationLevel.Anonymous, &quot;get&quot;, &quot;post&quot;, Route = null)] HttpRequest req,
            ILogger log)
        {
            try
            {
                var table = await GetTable(log);

                // Create the InsertOrReplace table operation
                var insertOrMergeOperation = TableOperation.InsertOrMerge(new KewGardenVisit(
                    arrivedAt: DateTime.UtcNow,
                    memberId: &quot;JR123456743921&quot;
                ));

                // Execute the operation.
                TableResult result = await table.ExecuteAsync(insertOrMergeOperation);
                KewGardenVisit savedTelemetry = result.Result as KewGardenVisit;

                if (result.RequestCharge.HasValue)
                    log.LogInformation(&quot;Request Charge of InsertOrMerge Operation: &quot; + result.RequestCharge);

                return new OkObjectResult(savedTelemetry);

            }
            catch (Exception e)
            {
                log.LogError(e, &quot;Problem saving&quot;);
            }

            return new BadRequestObjectResult(&quot;There was an issue&quot;);
        }

        [FunctionName(&quot;GetTodaysVisits&quot;)]
        public static async Task&lt;IActionResult&gt; RunGetTodaysVisits(
            [HttpTrigger(AuthorizationLevel.Anonymous, &quot;get&quot;, Route = null)] HttpRequest req,
            ILogger log)
        {
            try
            {
                var table = await GetTable(log);

                var snowOnTheAdoTelemetries = table.CreateQuery&lt;KewGardenVisit&gt;()
                    .Where(x =&gt; x.PartitionKey == DateTime.UtcNow.ToString(&quot;yyyy-MM-dd&quot;, System.Globalization.CultureInfo.InvariantCulture))
                    .ToArray();

                return new OkObjectResult(snowOnTheAdoTelemetries);

            }
            catch (Exception e)
            {
                log.LogError(e, &quot;Problem loading&quot;);
                return new BadRequestObjectResult(&quot;There was an issue&quot;);
            }
        }

        private static async Task&lt;CloudTable&gt; GetTable(ILogger log)
        {
            // Construct a new TableClient using a TableSharedKeyCredential.
            var storageAccount = CloudStorageAccount.Parse(AZURITE_TABLESTORAGE_CONNECTIONSTRING); ;

            // Create a table client for interacting with the table service
            CloudTableClient tableClient = storageAccount.CreateCloudTableClient(new TableClientConfiguration());

            // Create a table client for interacting with the table service
            CloudTable table = tableClient.GetTableReference(TABLE_NAME);
            if (await table.CreateIfNotExistsAsync())
                log.LogInformation(&quot;Created Table named: {0}&quot;, TABLE_NAME);
            else
                log.LogInformation(&quot;Table {0} already exists&quot;, TABLE_NAME);

            return table;
        }
    }
}
</code></pre><p>There&#x27;s a couple of things to draw attention to here:</p><ul><li><p><code>AZURITE_TABLESTORAGE_CONNECTIONSTRING</code> - this mega string is based upon the <a href="https://github.com/Azure/Azurite#connection-strings">Azurite connection string docs</a>. The account name and key are the <a href="https://github.com/Azure/Azurite#default-storage-account">Azurite default storage accounts</a>. You&#x27;ll note we target <code>TableEndpoint=http://azurite:10002/devstoreaccount1</code>. The <code>azurite</code> here is replacing the standard <code>127.0.0.1</code> where Azurite typically listens. This <code>azurite</code> name comes from the name of our service in the <code>docker-compose.yml</code> file.</p></li><li><p>We&#x27;re creating two functions <code>SaveVisit</code> and <code>GetTodaysVisits</code>. <code>SaveVisit</code> creates an entry in our storage to represent someone&#x27;s visit. It&#x27;s a hardcoded value representing me, and we&#x27;re exposing a write operation at a <code>GET</code> endpoint which is not very RESTful. But this is a demo and Roy Fielding would forgive us. <code>GetTodaysVisits</code> allows us to read back the visits that have happened today.</p></li></ul><p>Let&#x27;s see if it works by entering <code>func start</code> and browsing to <code>http://localhost:7071/api/savevisit</code></p><p><img src="../static/blog/2021-05-15-azurite-and-table-storage-dev-container/savevisits.png" alt="a screenshot of the response from the savevisits endpoint"/></p><p>Looking good. Now let&#x27;s see if we can query them at <code>http://localhost:7071/api/gettodaysvisits</code>:</p><p><img src="../static/blog/2021-05-15-azurite-and-table-storage-dev-container/gettodaysvisits.png" alt="a screenshot of the response from the gettodaysvisits endpoint"/></p><p>Disco.</p><h2>Can we swap out Azurite for The Real Thing™️?</h2><p>You may be thinking <em>&quot;This is great! But in the end I need to write to Azure Table Storage itself; not Azurite.&quot;</em></p><p>That&#x27;s a fair point. Fortunately, it&#x27;s only the connection string that determines where you read and write to. It would be fairly easy to dependency inject the appropriate connection string, or indeed a service that is connected to the storage you wish to target. If you want to make that happen, you can.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Create a Pipeline with the Azure DevOps API]]></title>
            <link>https://blog.johnnyreilly.com/2021/05/08/create-pipeline-with-azure-devops-api</link>
            <guid>Create a Pipeline with the Azure DevOps API</guid>
            <pubDate>Sat, 08 May 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Creating an Azure Pipeline using the Azure DevOps REST API is possible, but badly documented. This post goes through how to do this.]]></description>
            <content:encoded><![CDATA[<p>Creating an Azure Pipeline using the Azure DevOps REST API is possible, but badly documented. This post goes through how to do this.</p><h2>curling a pipeline</h2><p>The <a href="https://docs.microsoft.com/en-us/rest/api/azure/devops/pipelines/pipelines/create?view=azure-devops-rest-6.1">documentation</a> for creating an Azure Pipeline using the Azure DevOps API is somewhat lacking. However it isn&#x27;t actually too hard, you just need the recipe.</p><p>Here&#x27;s a curl to make you a pipeline:</p><pre><code class="language-bash">curl  --user &#x27;&#x27;:&#x27;PERSONAL_ACCESS_TOKEN&#x27; --header &quot;Content-Type: application/json&quot; --header &quot;Accept:application/json&quot; https://dev.azure.com/organisation-name/sandbox/_apis/pipelines?api-version=6.1-preview.1 -d @makepipeline.json
</code></pre><p>Looking at the above there&#x27;s two things you need:</p><ol><li>A personal access token. You can make one of those here: <a href="https://dev.azure.com/organisation-name/_usersSettings/tokens">https://dev.azure.com/organisation-name/_usersSettings/tokens</a> (where <code>organisation-name</code> is the name of your organisation)</li><li>A <code>makepipeline.json</code> file, which contains the details of the pipeline you want to create:</li></ol><pre><code class="language-json">{
  &quot;folder&quot;: null,
  &quot;name&quot;: &quot;pipeline-made-by-api&quot;,
  &quot;configuration&quot;: {
    &quot;type&quot;: &quot;yaml&quot;,
    &quot;path&quot;: &quot;/azure-pipelines.yml&quot;,
    &quot;repository&quot;: {
      &quot;id&quot;: &quot;guid-of-repo-id&quot;,
      &quot;name&quot;: &quot;my-repo&quot;,
      &quot;type&quot;: &quot;azureReposGit&quot;
    }
  }
}
</code></pre><p>Let&#x27;s talk through the significant properties above:</p><ul><li><code>folder</code> - can be <code>null</code> if you&#x27;d like the pipeline to be created in the root of Pipelines; otherwise provide the folder name. Incidentally a <code>null</code> will be translated into a value of <code>\\</code> which appears to be the magic value which represents the root.</li><li><code>name</code> - your pipeline needs a name</li><li><code>path</code> - this is the path to the yaml pipelines file in the repo. Note we&#x27;re creating the pipeline itself here; what&#x27;s actually in the pipeline sits in that file.</li><li><code>repository.id</code> - this is the guid that represents the repo you&#x27;re creating the pipeline for. You can find this out by going to your equivalent <a href="https://dev.azure.com/organisation-name/project-name/_settings/repositories">https://dev.azure.com/organisation-name/project-name/_settings/repositories</a> (substituting in appropriate values) and looking up your repository there.</li><li><code>repository.name</code> - the name of your repo</li></ul><p>When you execute your curl you should be returned some JSON along these lines:</p><pre><code class="language-json">{
  &quot;_links&quot;: {
    &quot;self&quot;: {
      &quot;href&quot;: &quot;https://dev.azure.com/organisation-name/2184049d-8bc4-484a-91e6-00fca6b5b19f/_apis/pipelines/975?revision=1&quot;
    },
    &quot;web&quot;: {
      &quot;href&quot;: &quot;https://dev.azure.com/organisation-name/2184049d-8bc4-484a-91e6-00fca6b5b19f/_build/definition?definitionId=975&quot;
    }
  },
  &quot;configuration&quot;: {
    &quot;path&quot;: &quot;/azure-pipelines.yml&quot;,
    &quot;repository&quot;: {
      &quot;id&quot;: &quot;9a72560d-1622-4016-93dd-32ac85b96d03&quot;,
      &quot;type&quot;: &quot;azureReposGit&quot;
    },
    &quot;type&quot;: &quot;yaml&quot;
  },
  &quot;url&quot;: &quot;https://dev.azure.com/organisation-name/2184049d-8bc4-484a-91e6-00fca6b5b19f/_apis/pipelines/975?revision=1&quot;,
  &quot;id&quot;: 975,
  &quot;revision&quot;: 1,
  &quot;name&quot;: &quot;pipeline-made-by-api&quot;,
  &quot;folder&quot;: &quot;\\&quot;
}
</code></pre><p>And inside Azure DevOps you&#x27;ll now have a shiny new pipeline:</p><p><img src="../static/blog/2021-05-08-create-pipeline-with-azure-devops-api/new-pipeline.png" alt="The new pipeline"/></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Blog Archive for Docusaurus]]></title>
            <link>https://blog.johnnyreilly.com/2021/05/01/blog-archive-for-docusaurus</link>
            <guid>Blog Archive for Docusaurus</guid>
            <pubDate>Sat, 01 May 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Docusaurus doesn't ship with "blog archive" functionality. By which I mean, something that allows you to look at an overview of your historic blog posts. It turns out it is fairly straightforward to implement your own. This post does just that.]]></description>
            <content:encoded><![CDATA[<p>Docusaurus doesn&#x27;t ship with &quot;blog archive&quot; functionality. By which I mean, something that allows you to look at an overview of your historic blog posts. It turns out it is fairly straightforward to implement your own. This post does just that.</p><p><img src="../static/blog/2021-05-01-blog-archive-for-docusaurus/docusaurus-blog-archive.png" alt="Docusaurus blog archive"/></p><h2>Update 2021-09-01</h2><p>As of <a href="https://github.com/facebook/docusaurus/releases/tag/v2.0.0-beta.6">v2.0.0-beta.6</a>, Docusauras <em>does</em> ship with blog archive functionality that lives at the <code>archive</code> route. This is down to the work of <a href="https://github.com/gabrielcsapo">Gabriel Csapo</a> in <a href="https://github.com/facebook/docusaurus/pull/5428">this PR</a>.</p><p>If you&#x27;d like to know how to build your own, read on... But you may not need to!</p><h2>Blogger&#x27;s blog archive</h2><p>I recently went through the exercise of <a href="./2021-03-15-from-blogger-to-docusaurus.md">migrating my blog from Blogger to Docusaurus</a>. I found that <a href="https://docusaurus.io/">Docusaurus</a> was a tremendous platform upon which to build a blog, but it was missing a feature from Blogger that I valued highly; the blog archive:</p><p><img src="../static/blog/2021-05-01-blog-archive-for-docusaurus/blogger-blog-archive-small.png" alt="Blogger blog archive"/></p><p>The blog archive is a way by which you can browse through your historic blog posts. A place where you can see all that you&#x27;ve written and when. I find this very helpful. I didn&#x27;t really want to make the jump without having something like that around.</p><h2>Handrolling a Docusaurus blog archive</h2><p>Let&#x27;s create our own blog archive in the land of the Docusaurus.</p><p>We&#x27;ll create a new page under the <code>pages</code> directory called <code>blog-archive.js</code> and we&#x27;ll add a link to it in our <code>docusaurus.config.js</code>:</p><pre><code class="language-json">    navbar: {
      // ...
      items: [
        // ...
        { to: &quot;blog-archive&quot;, label: &quot;Blog Archive&quot;, position: &quot;left&quot; },
        // ...
      ],
    },
</code></pre><h2>Obtaining the blog data</h2><p>This page will be powered by webpack&#x27;s <a href="https://webpack.js.org/guides/dependency-management/#requirecontext"><code>require.context</code></a> function. <code>require.context</code> allows us to use webpack to obtain all of the blog modules:</p><pre><code class="language-js">require.context(&#x27;../../blog&#x27;, false, /.md/);
</code></pre><p>The code snippet above looks in the <code>blog</code> directory for files / modules ending with the suffix <code>&quot;.md&quot;</code>. Each one of these represents a blog post. The function returns a <code>context</code> object, which contains all of the data about these modules.</p><p>By reducing over that data we can construct an array of objects called <code>allPosts</code> that could drive a blog archive screen. Let&#x27;s do this below, and we&#x27;ll use <a href="https://www.typescriptlang.org/docs/handbook/jsdoc-supported-types.html">TypeScripts JSDoc support</a> to type our JavaScript:</p><pre><code class="language-tsx">/**
 * @typedef {Object} BlogPost - creates a new type named &#x27;BlogPost&#x27;
 * @property {string} date - eg &quot;2021-04-24T00:00:00.000Z&quot;
 * @property {string} formattedDate - eg &quot;April 24, 2021&quot;
 * @property {string} title - eg &quot;The Service Now API and TypeScript Conditional Types&quot;
 * @property {string} permalink - eg &quot;/2021/04/24/service-now-api-and-typescript-conditional-types&quot;
 */

/** @type {BlogPost[]} */
const allPosts = ((ctx) =&gt; {
  /** @type {string[]} */
  const blogpostNames = ctx.keys();

  return blogpostNames.reduce(
    (blogposts, blogpostName, i) =&gt; {
      const module = ctx(blogpostName);
      const { date, formattedDate, title, permalink } = module.metadata;
      return [
        ...blogposts,
        {
          date,
          formattedDate,
          title,
          permalink,
        },
      ];
    },
    /** @type {string[]}&gt;} */ []
  );
})(require.context(&#x27;../../blog&#x27;, false, /.md/));
</code></pre><p>Observe the <code>metadata</code> property in the screenshot below:</p><p><img src="../static/blog/2021-05-01-blog-archive-for-docusaurus/require.context.png" alt="require.context"/></p><p>This gives us a flavour of the data available in the modules and shows how we pull out the bits that we need; <code>date</code>, <code>formattedDate</code>, <code>title</code> and <code>permalink</code>.</p><h2>Presenting it</h2><p>Now we have our data in the form of <code>allPosts</code>, let&#x27;s display it. We&#x27;d like to break it up into posts by year, which we can do by reducing and looking at the <code>date</code> property which is an ISO-8601 style date string taking a format that begins <code>yyyy-mm-dd</code>:</p><pre><code class="language-tsx">const postsByYear = allPosts.reduceRight((posts, post) =&gt; {
  const year = post.date.split(&#x27;-&#x27;)[0];
  const yearPosts = posts.get(year) || [];
  return posts.set(year, [post, ...yearPosts]);
}, /** @type {Map&lt;string, BlogPost[]&gt;}&gt;} */ new Map());

const yearsOfPosts = Array.from(postsByYear, ([year, posts]) =&gt; ({
  year,
  posts,
}));
</code></pre><p>Now we&#x27;re ready to blast it onto the screen. We&#x27;ll create two components:</p><ul><li><code>Year</code> - which is a list of the posts for a given year and</li><li><code>BlogArchive</code> - which is the overall page and maps over <code>yearsOfPosts</code> to render <code>Year</code>s</li></ul><pre><code class="language-tsx">function Year(
  /** @type {{ year: string; posts: BlogPost[]}} */ { year, posts }
) {
  return (
    &lt;div className={clsx(&#x27;col col--4&#x27;, styles.feature)}&gt;
      &lt;h3&gt;{year}&lt;/h3&gt;
      &lt;ul&gt;
        {posts.map((post) =&gt; (
          &lt;li key={post.date}&gt;
            &lt;Link to={post.permalink}&gt;
              {post.formattedDate} - {post.title}
            &lt;/Link&gt;
          &lt;/li&gt;
        ))}
      &lt;/ul&gt;
    &lt;/div&gt;
  );
}

function BlogArchive() {
  return (
    &lt;Layout title=&quot;Blog Archive&quot;&gt;
      &lt;header className={clsx(&#x27;hero hero--primary&#x27;, styles.heroBanner)}&gt;
        &lt;div className=&quot;container&quot;&gt;
          &lt;h1 className=&quot;hero__title&quot;&gt;Blog Archive&lt;/h1&gt;
          &lt;p className=&quot;hero__subtitle&quot;&gt;Historic posts&lt;/p&gt;
        &lt;/div&gt;
      &lt;/header&gt;
      &lt;main&gt;
        {yearsOfPosts &amp;&amp; yearsOfPosts.length &gt; 0 &amp;&amp; (
          &lt;section className={styles.features}&gt;
            &lt;div className=&quot;container&quot;&gt;
              &lt;div className=&quot;row&quot;&gt;
                {yearsOfPosts.map((props, idx) =&gt; (
                  &lt;Year key={idx} {...props} /&gt;
                ))}
              &lt;/div&gt;
            &lt;/div&gt;
          &lt;/section&gt;
        )}
      &lt;/main&gt;
    &lt;/Layout&gt;
  );
}
</code></pre><h2>Bringing it all together</h2><p>We&#x27;re finished! We have a delightful looking blog archive plumbed into our blog:</p><p><img src="../static/blog/2021-05-01-blog-archive-for-docusaurus/docusaurus-blog-archive.png" alt="Docusaurus blog archive"/></p><p>It is possible that a blog archive may become natively available in Docusaurus in future. If you&#x27;re interested in this, you can track <a href="https://github.com/facebook/docusaurus/issues/4431">this issue</a>.</p><p>Here&#x27;s the final code - which you can see <a href="https://blog.johnnyreilly.com/blog-archive">powering this screen</a>. And you can see the code that backs it <a href="https://github.com/johnnyreilly/blog.johnnyreilly.com/blob/main/blog-website/src/pages/blog-archive.js">here</a>:</p><pre><code class="language-tsx">import React from &#x27;react&#x27;;
import clsx from &#x27;clsx&#x27;;
import Layout from &#x27;@theme/Layout&#x27;;
import Link from &#x27;@docusaurus/Link&#x27;;
import styles from &#x27;./styles.module.css&#x27;;

/**
 * @typedef {Object} BlogPost - creates a new type named &#x27;BlogPost&#x27;
 * @property {string} date - eg &quot;2021-04-24T00:00:00.000Z&quot;
 * @property {string} formattedDate - eg &quot;April 24, 2021&quot;
 * @property {string} title - eg &quot;The Service Now API and TypeScript Conditional Types&quot;
 * @property {string} permalink - eg &quot;/2021/04/24/service-now-api-and-typescript-conditional-types&quot;
 */

/** @type {BlogPost[]} */
const allPosts = ((ctx) =&gt; {
  /** @type {string[]} */
  const blogpostNames = ctx.keys();

  return blogpostNames.reduce(
    (blogposts, blogpostName, i) =&gt; {
      const module = ctx(blogpostName);
      const { date, formattedDate, title, permalink } = module.metadata;
      return [
        ...blogposts,
        {
          date,
          formattedDate,
          title,
          permalink,
        },
      ];
    },
    /** @type {string[]}&gt;} */ []
  );
})(require.context(&#x27;../../blog&#x27;, false, /.md/));

const postsByYear = allPosts.reduceRight((posts, post) =&gt; {
  const year = post.date.split(&#x27;-&#x27;)[0];
  const yearPosts = posts.get(year) || [];
  return posts.set(year, [post, ...yearPosts]);
}, /** @type {Map&lt;string, BlogPost[]&gt;}&gt;} */ new Map());

const yearsOfPosts = Array.from(postsByYear, ([year, posts]) =&gt; ({
  year,
  posts,
}));

function Year(
  /** @type {{ year: string; posts: BlogPost[]}} */ { year, posts }
) {
  return (
    &lt;div className={clsx(&#x27;col col--4&#x27;, styles.feature)}&gt;
      &lt;h3&gt;{year}&lt;/h3&gt;
      &lt;ul&gt;
        {posts.map((post) =&gt; (
          &lt;li key={post.date}&gt;
            &lt;Link to={post.permalink}&gt;
              {post.formattedDate} - {post.title}
            &lt;/Link&gt;
          &lt;/li&gt;
        ))}
      &lt;/ul&gt;
    &lt;/div&gt;
  );
}

function BlogArchive() {
  return (
    &lt;Layout title=&quot;Blog Archive&quot;&gt;
      &lt;header className={clsx(&#x27;hero hero--primary&#x27;, styles.heroBanner)}&gt;
        &lt;div className=&quot;container&quot;&gt;
          &lt;h1 className=&quot;hero__title&quot;&gt;Blog Archive&lt;/h1&gt;
          &lt;p className=&quot;hero__subtitle&quot;&gt;Historic posts&lt;/p&gt;
        &lt;/div&gt;
      &lt;/header&gt;
      &lt;main&gt;
        {yearsOfPosts &amp;&amp; yearsOfPosts.length &gt; 0 &amp;&amp; (
          &lt;section className={styles.features}&gt;
            &lt;div className=&quot;container&quot;&gt;
              &lt;div className=&quot;row&quot;&gt;
                {yearsOfPosts.map((props, idx) =&gt; (
                  &lt;Year key={idx} {...props} /&gt;
                ))}
              &lt;/div&gt;
            &lt;/div&gt;
          &lt;/section&gt;
        )}
      &lt;/main&gt;
    &lt;/Layout&gt;
  );
}

export default BlogArchive;
</code></pre>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Service Now API and TypeScript Conditional Types]]></title>
            <link>https://blog.johnnyreilly.com/2021/04/24/service-now-api-and-typescript-conditional-types</link>
            <guid>The Service Now API and TypeScript Conditional Types</guid>
            <pubDate>Sat, 24 Apr 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[The Service Now REST API is an API which allows you to interact with Service Now. It produces different shaped results based upon the sysparmdisplayvalue query parameter. This post looks at how we can model these API results with TypeScripts conditional types. The aim being to minimise repetition whilst remaining strongly typed. This post is specifically about the Service Now API, but the principles around conditional type usage are generally applicable.]]></description>
            <content:encoded><![CDATA[<p>The <a href="https://docs.servicenow.com/bundle/paris-application-development/page/build/applications/concept/api-rest.html">Service Now REST API</a> is an API which allows you to interact with Service Now. It produces different shaped results based upon the <a href="https://docs.servicenow.com/bundle/paris-application-development/page/integrate/inbound-rest/concept/c_TableAPI.html#c_TableAPI__table-GET"><code>sysparm_display_value</code> query parameter</a>. This post looks at how we can model these API results with TypeScripts conditional types. The aim being to minimise repetition whilst remaining strongly typed. This post is specifically about the Service Now API, but the principles around conditional type usage are generally applicable.</p><p><img src="../static/blog/2021-04-24-service-now-api-and-typescript-conditional-types/ts-ervice-now.png" alt="Service Now and TypeScript"/></p><h2>The power of a query parameter</h2><p>There is a query parameter which many endpoints in Service Nows Table API support named <code>sysparm_display_value</code>. The docs describe it thus:</p><blockquote><p>Data retrieval operation for reference and choice fields.
Based on this value, retrieves the display value and/or the actual value from the database.</p><p>Valid values:</p><ul><li><code>true</code>: Returns the display values for all fields.</li><li><code>false</code>: Returns the actual values from the database.</li><li><code>all</code>: Returns both actual and display value</li></ul></blockquote><p>Let&#x27;s see what that looks like when it comes to loading a Change Request. Consider the following curls:</p><pre><code class="language-shell"># sysparm_display_value=all
curl &quot;https://ourcompanyinstance.service-now.com/api/now/table/change_request?sysparm_query=number=CHG0122585&amp;sysparm_limit=1&amp;sysparm_display_value=all&quot; --request GET --header &quot;Accept:application/json&quot; --user &#x27;API_USERNAME&#x27;:&#x27;API_PASSWORD&#x27; | jq &#x27;.result[0] | { state, sys_id, number, requested_by, reason }&#x27;

# sysparm_display_value=true
curl &quot;https://ourcompanyinstance.service-now.com/api/now/table/change_request?sysparm_query=number=CHG0122585&amp;sysparm_limit=1&amp;sysparm_display_value=true&quot; --request GET --header &quot;Accept:application/json&quot; --user &#x27;API_USERNAME&#x27;:&#x27;API_PASSWORD&#x27; | jq &#x27;.result[0] | { state, sys_id, number, requested_by, reason }&#x27;

# sysparm_display_value=false
curl &quot;https://ourcompanyinstance.service-now.com/api/now/table/change_request?sysparm_query=number=CHG0122585&amp;sysparm_limit=1&amp;sysparm_display_value=false&quot; --request GET --header &quot;Accept:application/json&quot; --user &#x27;API_USERNAME&#x27;:&#x27;API_PASSWORD&#x27; | jq &#x27;.result[0] | { state, sys_id, number, requested_by, reason }&#x27;
</code></pre><p>When executed, they each load the same Change Request from Service Now with a different value for <code>sysparm_display_value</code>. You&#x27;ll notice there&#x27;s some <a href="https://stedolan.github.io/jq/"><code>jq</code></a> in the mix as well. This is because there&#x27;s a <em>lot</em> of data in a Change Request. Rather than display everything, we&#x27;re displaying a subset of fields. The first curl has a <code>sysparm_display_value</code> value of <code>all</code>, the second <code>false</code> and the third <code>true</code>. What do the results look like?</p><p><code>sysparm_display_value=all</code>:</p><pre><code class="language-json">{
  &quot;state&quot;: {
    &quot;display_value&quot;: &quot;Closed&quot;,
    &quot;value&quot;: &quot;3&quot;
  },
  &quot;sys_id&quot;: {
    &quot;display_value&quot;: &quot;4d54d7481b37e010d315cbb5464bcb95&quot;,
    &quot;value&quot;: &quot;4d54d7481b37e010d315cbb5464bcb95&quot;
  },
  &quot;number&quot;: {
    &quot;display_value&quot;: &quot;CHG0122595&quot;,
    &quot;value&quot;: &quot;CHG0122595&quot;
  },
  &quot;requested_by&quot;: {
    &quot;display_value&quot;: &quot;Sally Omer&quot;,
    &quot;link&quot;: &quot;https://ourcompanyinstance.service-now.com/api/now/table/sys_user/b15cf3ebdbe11300f196f3651d961999&quot;,
    &quot;value&quot;: &quot;b15cf3ebdbe11300f196f3651d961999&quot;
  },
  &quot;reason&quot;: {
    &quot;display_value&quot;: null,
    &quot;value&quot;: &quot;&quot;
  }
}
</code></pre><p><code>sysparm_display_value=true</code>:</p><pre><code class="language-json">{
  &quot;state&quot;: &quot;Closed&quot;,
  &quot;sys_id&quot;: &quot;4d54d7481b37e010d315cbb5464bcb95&quot;,
  &quot;number&quot;: &quot;CHG0122595&quot;,
  &quot;requested_by&quot;: {
    &quot;display_value&quot;: &quot;Sally Omer&quot;,
    &quot;link&quot;: &quot;https://ourcompanyinstance.service-now.com/api/now/table/sys_user/b15cf3ebdbe11300f196f3651d961999&quot;
  },
  &quot;reason&quot;: null
}
</code></pre><p><code>sysparm_display_value=false</code>:</p><pre><code class="language-json">{
  &quot;state&quot;: &quot;3&quot;,
  &quot;sys_id&quot;: &quot;4d54d7481b37e010d315cbb5464bcb95&quot;,
  &quot;number&quot;: &quot;CHG0122595&quot;,
  &quot;requested_by&quot;: {
    &quot;link&quot;: &quot;https://ourcompanyinstance.service-now.com/api/now/table/sys_user/b15cf3ebdbe11300f196f3651d961999&quot;,
    &quot;value&quot;: &quot;b15cf3ebdbe11300f196f3651d961999&quot;
  },
  &quot;reason&quot;: &quot;&quot;
}
</code></pre><p>As you can see, we have the same properties being returned each time, but with a different shape. Let&#x27;s call out some interesting highlights:</p><ul><li><code>requested_by</code> is <em>always</em> an object which contains <code>link</code>. It may also contain <code>value</code> and <code>display_value</code> depending upon <code>sysparm_display_value</code></li><li><code>state</code>, <code>sys_id</code>, <code>number</code> and <code>reason</code> are objects containing <code>value</code> and <code>display_value</code> when <code>sysparm_display_value</code> is <code>all</code>. Otherwise, the value of <code>value</code> or <code>display_value</code> is surfaced up directly; not in an object.</li><li>most values are strings, even if they represent another data type. So <code>state.value</code> is always a stringified number. The only exception to this rule is <code>reason.display_value</code> which can be <code>null</code></li></ul><h2>Type Definition time</h2><p>We want to create type definitions for these API results. We could of course create three different results, but that would involve duplication. Boo! It&#x27;s worth bearing in mind we&#x27;re looking at a subset of five properties in this example. In reality, there are many, many properties on a Change Request. Whilst this example is for a subset, if we wanted to go on to create the full type definition the duplication would become very impractical.</p><p>What can we do? Well, if all of the underlying properties were of the same type, we could use a generic and be done. But given the underlying types can vary, that&#x27;s not going to work. We can achieve this though through using a combination of generics and conditional types.</p><p>Let&#x27;s begin by creating a string literal type of the possible values of <code>sysparm_display_value</code>:</p><pre><code class="language-ts">export type DisplayValue = &#x27;all&#x27; | &#x27;true&#x27; | &#x27;false&#x27;;
</code></pre><h2>Making a <code>PropertyValue</code> type</h2><p>Next we need to create a type that models the object with <code>display_value</code> and <code>value</code> properties.</p><p>:::info a type for state, sys_id, number and reason</p><ul><li><code>state</code>, <code>sys_id</code>, <code>number</code> and <code>reason</code> are objects containing <code>value</code> and <code>display_value</code> when <code>sysparm_display_value</code> is <code>&#x27;all&#x27;</code>. Otherwise, the value of <code>value</code> or <code>display</code> is surfaced up directly; not in an object.</li><li>most values are strings, even if they represent another data type. So <code>state.value</code> is always a stringified number. The only exception to this rule is <code>reason.display_value</code> which can be <code>null</code></li></ul><p>:::</p><pre><code class="language-ts">export interface ValueAndDisplayValue&lt;TValue = string, TDisplayValue = string&gt; {
  display_value: TDisplayValue;
  value: TValue;
}
</code></pre><p>Note that this is a generic property with a default type of <code>string</code> for both <code>display_value</code> and <code>value</code>. Most of the time, <code>string</code> is the type in question so it&#x27;s great that TypeScript allows us to cut down on the amount of syntax we use.</p><p>Now we&#x27;re going to create our first conditional type:</p><pre><code class="language-ts">export type PropertyValue&lt;
  TAllTrueFalse extends DisplayValue,
  TValue = string,
  TDisplayValue = string
&gt; = TAllTrueFalse extends &#x27;all&#x27;
  ? ValueAndDisplayValue&lt;TValue, TDisplayValue&gt;
  : TAllTrueFalse extends &#x27;true&#x27;
  ? TDisplayValue
  : TValue;
</code></pre><p>The <code>PropertyValue</code> will either be a <code>ValueAndDisplayValue</code>, a <code>TDisplayValue</code> or a <code>TValue</code>, depending upon whether <code>PropertyValue</code> is <code>&#x27;all&#x27;</code>, <code>&#x27;true&#x27;</code> or <code>&#x27;false&#x27;</code> respectively. That&#x27;s hard to grok. Let&#x27;s look at an example of each of those cases using the <code>reason</code> property, which allows a <code>TValue</code> of <code>string</code> and a <code>TDisplayValue</code> of <code>string | null</code>:</p><pre><code class="language-ts">const reasonAll: PropertyValue&lt;&#x27;all&#x27;, string, string | null&gt; = {
  display_value: null,
  value: &#x27;&#x27;,
};
const reasonTrue: PropertyValue&lt;&#x27;true&#x27;, string, string | null&gt; = null;
const reasonFalse: PropertyValue&lt;&#x27;false&#x27;, string, string | null&gt; = &#x27;&#x27;;
</code></pre><p>Consider the type on the left and the value on the right. We&#x27;re successfully modelling our <code>PropertyValue</code>s. I&#x27;ve deliberately picked an edge case example to push our conditional type to its limits.</p><h2>Service Now Change Request States</h2><p>Let&#x27;s look at another usage. We&#x27;ll create a type that repesents the possible values of a Change Request&#x27;s <code>state</code> in Service Now. Do take a moment to appreciate these values. Many engineers were lost in the numerous missions to obtain these rare and secret enums. Alas, the Service Now API docs have some significant gaps.</p><pre><code class="language-ts">/** represents the possible Change Request &quot;State&quot; values in Service Now */
export const STATE = {
  NEW: &#x27;-5&#x27;,
  ASSESS: &#x27;-4&#x27;,
  SENT_FOR_APPROVAL: &#x27;-3&#x27;,
  SCHEDULED: &#x27;-2&#x27;,
  APPROVED: &#x27;-1&#x27;,
  WAITING: &#x27;1&#x27;,
  IN_PROGRESS: &#x27;2&#x27;,
  COMPLETE: &#x27;3&#x27;,
  ERROR: &#x27;4&#x27;,
  CLOSED: &#x27;7&#x27;,
} as const;

export type State = typeof STATE[keyof typeof STATE];
</code></pre><p>By combining <code>State</code> and <code>PropertyValue</code>, we can strongly type the <code>state</code> property of Change Requests. Consider:</p><pre><code class="language-ts">const stateAll: PropertyValue&lt;&#x27;all&#x27;, State&gt; = {
  display_value: &#x27;Closed&#x27;,
  value: &#x27;3&#x27;,
};
const stateTrue: PropertyValue&lt;&#x27;true&#x27;, State&gt; = &#x27;Closed&#x27;;
const stateFalse: PropertyValue&lt;&#x27;false&#x27;, State&gt; = &#x27;3&#x27;;
</code></pre><p>With that in place, let&#x27;s turn our attention to our other natural type that the <code>requested_by</code> property demonstrates.</p><h2>Making a <code>LinkValue</code> type</h2><p>:::info a type for requested_by</p><p><code>requested_by</code> is <em>always</em> an object which contains <code>link</code>. It may also contain <code>value</code> and <code>display_value</code> depending upon <code>sysparm_display_value</code></p><p>:::</p><pre><code class="language-ts">interface Link {
  link: string;
}

/** when TAllTrueFalse is &#x27;false&#x27; */
export interface LinkAndValue extends Link {
  value: string;
}

/** when TAllTrueFalse is &#x27;true&#x27; */
export interface LinkAndDisplayValue extends Link {
  display_value: string;
}

/** when TAllTrueFalse is &#x27;all&#x27; */
export interface LinkValueAndDisplayValue
  extends LinkAndValue,
    LinkAndDisplayValue {}
</code></pre><p>The three types above model the different scenarios. Now we need a conditional type to make use of them:</p><pre><code class="language-ts">export type LinkValue&lt;TAllTrueFalse extends DisplayValue&gt; =
  TAllTrueFalse extends &#x27;all&#x27;
    ? LinkValueAndDisplayValue
    : TAllTrueFalse extends &#x27;true&#x27;
    ? LinkAndDisplayValue
    : LinkAndValue;
</code></pre><p>This is hopefully simpler to read than the <code>PropertyValue</code> type, and if you look at the examples below you can see what usage looks like:</p><pre><code class="language-ts">const requested_byAll: LinkValue&lt;&#x27;all&#x27;&gt; = {
  display_value: &#x27;Sally Omer&#x27;,
  link: &#x27;https://ourcompanyinstance.service-now.com/api/now/table/sys_user/b15cf3ebdbe11300f196f3651d961999&#x27;,
  value: &#x27;b15cf3ebdbe11300f196f3651d961999&#x27;,
};
const requested_byTrue: LinkValue&lt;&#x27;true&#x27;&gt; = {
  display_value: &#x27;Sally Omer&#x27;,
  link: &#x27;https://ourcompanyinstance.service-now.com/api/now/table/sys_user/b15cf3ebdbe11300f196f3651d961999&#x27;,
};
const requested_byFalse: LinkValue&lt;&#x27;false&#x27;&gt; = {
  link: &#x27;https://ourcompanyinstance.service-now.com/api/now/table/sys_user/b15cf3ebdbe11300f196f3651d961999&#x27;,
  value: &#x27;b15cf3ebdbe11300f196f3651d961999&#x27;,
};
</code></pre><h2>Making our complete type</h2><p>With these primitives in place, we can now build ourself a (cut-down) type that models a Change Request:</p><pre><code class="language-ts">export interface ServiceNowChangeRequest&lt;TAllTrueFalse extends DisplayValue&gt; {
  state: PropertyValue&lt;TAllTrueFalse, State&gt;;
  sys_id: PropertyValue&lt;TAllTrueFalse&gt;;
  number: PropertyValue&lt;TAllTrueFalse&gt;;
  requested_by: LinkValue&lt;TAllTrueFalse&gt;;
  reason: PropertyValue&lt;TAllTrueFalse, string, string | null&gt;;
  // there are *way* more properties in reality
}
</code></pre><p>This is a generic type which will accept <code>&#x27;all&#x27;</code>, <code>&#x27;true&#x27;</code> or <code>&#x27;false&#x27;</code> and will use that type to drive the type of the properties <em>inside</em> the object. And now we have successfully typed our Service Now Change Request, thanks to TypeScript&#x27;s conditional types.</p><p>To test it out, let&#x27;s take the JSON responses we got back from our curls at the start, and see if we can make <code>ServiceNowChangeRequest</code>s with them.</p><pre><code class="language-ts">const changeRequestFalse: ServiceNowChangeRequest&lt;&#x27;false&#x27;&gt; = {
  state: &#x27;3&#x27;,
  sys_id: &#x27;4d54d7481b37e010d315cbb5464bcb95&#x27;,
  number: &#x27;CHG0122595&#x27;,
  requested_by: {
    link: &#x27;https://ourcompanyinstance.service-now.com/api/now/table/sys_user/b15cf3ebdbe11300f196f3651d961999&#x27;,
    value: &#x27;b15cf3ebdbe11300f196f3651d961999&#x27;,
  },
  reason: &#x27;&#x27;,
};

const changeRequestTrue: ServiceNowChangeRequest&lt;&#x27;true&#x27;&gt; = {
  state: &#x27;Closed&#x27;,
  sys_id: &#x27;4d54d7481b37e010d315cbb5464bcb95&#x27;,
  number: &#x27;CHG0122595&#x27;,
  requested_by: {
    display_value: &#x27;Sally Omer&#x27;,
    link: &#x27;https://ourcompanyinstance.service-now.com/api/now/table/sys_user/b15cf3ebdbe11300f196f3651d961999&#x27;,
  },
  reason: null,
};

const changeRequestAll: ServiceNowChangeRequest&lt;&#x27;all&#x27;&gt; = {
  state: {
    display_value: &#x27;Closed&#x27;,
    value: &#x27;3&#x27;,
  },
  sys_id: {
    display_value: &#x27;4d54d7481b37e010d315cbb5464bcb95&#x27;,
    value: &#x27;4d54d7481b37e010d315cbb5464bcb95&#x27;,
  },
  number: {
    display_value: &#x27;CHG0122595&#x27;,
    value: &#x27;CHG0122595&#x27;,
  },
  requested_by: {
    display_value: &#x27;Sally Omer&#x27;,
    link: &#x27;https://ourcompanyinstance.service-now.com/api/now/table/sys_user/b15cf3ebdbe11300f196f3651d961999&#x27;,
    value: &#x27;b15cf3ebdbe11300f196f3651d961999&#x27;,
  },
  reason: {
    display_value: null,
    value: &#x27;&#x27;,
  },
};
</code></pre><p>We can! Do take a look at this in the <a href="https://www.typescriptlang.org/play?#code/KYDwDg9gTgLgBDAnmYcAiBLAzmANgQ0QDV9cBXVAXjgHJTca4AfWmKCxlmgM1K2BoBuAFDDQkWHAwA7GMCi8AxqhLlgAQWkATTDgLFSFADwAVVRTjUsbGQHMANHBO68hc1TjWodgHxwA3sJwwXBa2K6IAPoAbobAAFxOLvruIiFwsWqJZnEiAL6i4tDwSChwAApQECiwBmpGQSEm6ri4JuzAAGJ8qKBy2ljo4Slx9o3BOWqWnjbSDuNJw25x0152wn7Uza3tFN24-HB9wAO09DQLAPxw7po6S3XGkxSOzg-uPgvZLW0d+4fHU40NgcK6LPTLNRfJypUSKCDSaxwKDAfBYBE-RKVaryJDuIx0Vo0RxrOYk2a2ZhwaRkVqbAILABEYQhUUyFEZiRprTG6UZ7OAnLgjMZwgK8MR8BRaIRuwSFSqNTxcQJIIE5O8ZJmmspLG5uHp+pEEqR0vR0n+8uxSsewAJvAO6u1dg1dip+vpNCEogA9AAqP3I4BgFH8WSDGAAC1QkCwWAwACNcKgAMKR-BzVAAJWAAEcKEjGQBlGD4OSMjJxQYyOBF+TRDDKOAAOQgAHc4H6fWJwMU4Cb4EXmiYAKLTQLpZsjgDqiRoAFoAKzEhbqItFkfrufzgAsK-SG+bJkinQA8lnIupyuUs6eiOoADLbgDM+5CRZTAAkR2gAKoPn9twAJjfYIrxvO9ANoecAEZQLgad1AASRMJDmwAcTnODeRCNDIgg9Cs03Is5xAnDghTU8AFlygA0c51fci4BHLNbyzOc9yYlMH1PDc0DnAB2Fc8jgNF+wRawRB7CQSmQVASzLDxSmACBuFrYcRwAbQAa2ARBVIQOSDKHdRRwAXSkgcZkUzEFRxWp8UJBhHAUuR6QnEJmQeGI4iFRkU1wCB+C0RkmP5XzEkZZ9RTyY0JPgaxFLlLFFVxW1VQ6Yla1LNzpn8wLgsZOLJWsuRLRS+zlXqHgeiy1zgHpKKitEGQ5AUfAmwfGRtIZdJcG6xJSVsfJfQDOA22jaQnB+OVLSkQYasdRgu2kvtWvkJRUC66RtLudwjhAfotEGbaeo84IBUGikRuEf1Awmk5pp2P4enm1hMs7bsikkdb2s67q7mSSELEBY64FO3qQhZCIfKyZ05huu7xsmp7fj2V7sDOIlPtWn7ZA2jqtu625tCB20DqOk6Ae0dxHFOwH3hWfwClx2SylO-FtjRrpXtBwYyY+aYudm3nDpOMGnIudJrg5uIGdZdxoWFl7HQp8WFrVKWQhl6n7gVuJoXpmncjheKg3zYBrGALRIgTRBbNl6rzncplof0WGOUiloMFrCAAFt5FCpl+p2vzIxgGAwCweIfR9GRoktuRFES2w7AAOn4KAG2UedpHbNP4T9n18DADAfTztsfVLJNgB9LBECwSIyEzn0ExgxdFG4Z9gATLQE2AGCYOfAAGYfuBggBOAA2Lup8XGCtGnyeJ-wIO+QFPy247rue77geh9H8fp9n+fF6n5fV7FYrTTzAs5Btu3kvB4mVWBTKXb5N3CA9wUvf632A5QDXp5EO2kw4RyjjHOO0gE5W2TqWVOcwM71kbMAXO+dC7F1LuXdsVd8A1zrg3JuLct6d27r3fug8R5j0njPZ8c8F5LwnivGKwgrIogtlbB+iByrPx2o5B0-AaAfxAd1cBkdo6x3jonYA8D8CINsMgrOqD0FtgLv7LBZcK54IIfXRuzd5Ct3bmQ3elCD40OPvQ0+TCWFhQ3pFUhO8KH72oUfOhDCz4XxilJb68BfqbVrCg5QrY2xpgzLYYAOZOEwFMDNFWAIxanAFnEPw51SpWlSg5FUyt0aOhcjlBqaQQh6MiBgLQFUbSczibk-gPginBBpH7fuUAKlpSqc9GphSFgcLvtbW2iBEiOyeNUnmjo6ndNROaVpWT6g5NGfwV0WohrulpAaG6VlFDpkzFEu+vC6zKOCe2MJ2zb6J3tLVERjJErlkitFHCVyiFlL8juLQi4XkCR3AADhggmZ8AlgDDxgsPLQz5jEJgTG8qeO4EyKATJ8xcwDGSNOaX5L86FAVASAoueFiKekyO4UKNJjJQHiMgVImBMi5EKKUdnNBFd1FFxLlo3B1dkyEP0SQ4xTi95UMPrQk+jDz7MNXnYiKwpHHkJ5eYtxArPHCtFMEPI9yzQIj8t402JVNnhMiac6wT99m0pCcciJOyzlvw4Jc65v9hQBSCtbRFJSnmRReW8rQHzvm-P+YC4FoKO7gshdC2FOL7nIsDpFNFGKsXBsaIyPFXD+mEtdt5exwpvYAMDmFElkVw4SKgdIuBKd06Z1paohlmicGV1ZbXEpBioBGO3pKsxrj+VWMFV4xoSqY0qukEKfUYoNVIi1Sc6JtkDWoKNVsk1uqYmS3pMIDyVyCmJs-smsV+U7UhVFWoPy0UO33MdSFRIRKv5sjXS695Xyfl-IBUCkFYKIU7ihTCuFCKt2e2FOet1l7PU3p9fegNz6cV7pjaGoBR6k2sh-qiz86KYKYuxa+pkKb-IwcjQhhVcBO3BFjdOvpdtl2eRPVBv+Psiz+wzcHMR2aIGSOgbApOhakHFpUfSzBTKK06LZTWzlDbTEuL5ZYjxNiRVIbXRKvjvKLHuOsUKlhwHsPdoI9hojKb9RvutSKDtYogA">TypeScript playground</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ts-loader goes webpack 5]]></title>
            <link>https://blog.johnnyreilly.com/2021/04/20/ts-loader-goes-webpack-5</link>
            <guid>ts-loader goes webpack 5</guid>
            <pubDate>Tue, 20 Apr 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[ts-loader has just released v9.0.0. This post goes through what this release is all about, and what it took to ship this version. For intrigue, it includes a brief scamper into my mental health along the way. Some upgrades go smoothly - this one had some hiccups. But we'll get into that.]]></description>
            <content:encoded><![CDATA[<p><code>ts-loader</code> has just released <a href="https://github.com/TypeStrong/ts-loader/releases/tag/v9.0.0">v9.0.0</a>. This post goes through what this release is all about, and what it took to ship this version. For intrigue, it includes a brief scamper into my mental health along the way. Some upgrades go smoothly - this one had some hiccups. But we&#x27;ll get into that.</p><p><img src="../static/blog/2021-04-20-ts-loader-goes-webpack-5/ts-loader-9.png" alt="hello world bicep"/></p><h2>One big pull request</h2><p>As of v8, <code>ts-loader</code> supported webpack 4 and webpack 5. However the webpack 5 support was best efforts, and not protected by any automated tests. <code>ts-loader</code> has two test packs:</p><ol><li>A <a href="https://github.com/TypeStrong/ts-loader/tree/main/test/comparison-tests#readme">comparison test pack</a> that compares transpilation and webpack compilation output with known outputs.</li><li>An <a href="https://github.com/TypeStrong/ts-loader/tree/main/test/execution-tests#readme">execution test pack</a> that executes Karma test packs written in TypeScript using <code>ts-loader</code>.</li></ol><p>The test packs were tightly coupled to webpack 4 (and in the case of the comparison test pack, that&#x27;s unavoidable). The mission was to port <code>ts-loader</code> to be built against (and have an automated test pack that ran against) webpack 5.</p><p>This ended up being a <a href="https://github.com/TypeStrong/ts-loader/pull/1251">very big pull request</a>. Work on it started back in February 2021 and we&#x27;re shipping now in April of 2021. I&#x27;d initially expected it would take a couple of days at most. I had underestimated.</p><p>A number of people collaborated on this PR, either with code, feedback, testing or even just responding to questions. So I&#x27;d like to say thank you to:</p><ul><li><a href="https://github.com/JonWallsten">John Wallsten</a> - who did a lot of the work swapping <code>ts-loader</code> over to webpack 5 APIs</li><li><a href="https://github.com/appzuka">Nick Excell</a></li><li><a href="https://github.com/andrewbranch">Andrew Branch</a></li><li><a href="https://github.com/alexander-akait">Alexander Akait</a> - who provided webpack 5 expertise and ideas</li><li><a href="https://github.com/sokra">Tobias Koppers</a> - who got me out of a hole - more on that later</li></ul><h2>What&#x27;s changed</h2><p>Let&#x27;s go through what&#x27;s different in v9. There&#x27;s two breaking changes:</p><ul><li>The minimum webpack version supported is now webpack 5. This simplifies the codebase, which previously had to if/else the various API registrations based on the version of webpack being used.</li><li>The minimum node version supported is now node 12. <a href="https://nodejs.org/en/about/releases/">Node 10 reaches end of life status at the end of April 2021.</a></li></ul><p>An interesting aspect of migrating to building against webpack 5 was dropping the dependency upon <a href="https://www.npmjs.com/package/@types/webpack"><code>@types/webpack</code></a> in favour of the types that now ship with webpack 5 itself. This was a mostly great experience; however we discovered some missing pieces.</p><p>Most notably, the <code>LoaderContext</code> <a href="https://github.com/webpack/webpack/blob/03961f33912ab6735d470b870eacff678735a9ed/lib/NormalModule.js#L424">wasn&#x27;t strongly typed</a>. <code>LoaderContext</code> is the value of <code>this</code> in the context of a running loader function. So it is probably the most interesting and important type from the perspective of a loader author.</p><p>Historically we used our own definition which had been adapted from the one in <code>@types/webpack</code>. <a href="https://github.com/webpack/webpack/issues/13162">I&#x27;ve looked into the possibility of a type being exposed in webpack itself.</a> However, it turns out, <a href="https://github.com/webpack/webpack/pull/13164#issuecomment-821410359">it&#x27;s complicated - with the <code>LoaderContext</code> type being effectively created across two packages</a>. The type is initially created in <code>webpack</code> and then augmented later in <code>loader-runner</code>, prior to being supplied to loaders. You can read more on that <a href="https://github.com/webpack/webpack/pull/13164#issuecomment-821410359">here</a>.</p><p>For now we&#x27;ve opted to stick with keeping <a href="https://github.com/TypeStrong/ts-loader/pull/1251/commits/acbc71feed91fe14ec065dd9d31081af7a492f47">an interface in <code>ts-loader</code></a> that models what arrives in the loader when executed. We have freshened it up somewhat, to model the webpack 5 world.</p><p>Alongside these changes, a <a href="https://github.com/TypeStrong/ts-loader/pull/1251/files#diff-7ae45ad102eab3b6d7e7896acd08c427a9b25b346470d7bc6507b6481575d519">number of dependencies were upgraded</a>.</p><h2>The hole</h2><p>By the 19th of February most of the work was done. However, <a href="https://github.com/TypeStrong/ts-loader/pull/1251#issuecomment-781967959">we were experiencing different behaviour between Linux and Windows in our comparison test pack</a>.</p><p>As far as I was aware, we were doing all the appropriate work to ensure <code>ts-loader</code> and our test packs worked cross platform. But we were still experiencing problems whenever we ran the test pack on Windows. I&#x27;d done no end of tweaking but nothing worked. I couldn&#x27;t explain it. I couldn&#x27;t fix it. I was finding that tough to deal with.</p><p>I really want to be transparent about the warts and all aspect of open source software development. It is like all other types of software development; sometimes things go wrong and it can be tough to work out why. Right then, I was really quite unhappy. Things weren&#x27;t working code-wise and I was at a loss to say why. This is not something that I dig.</p><p>I also wasn&#x27;t sleeping amazingly at this point. It was winter and we&#x27;d been in lockdown in the UK for three months; as the COVID-19 pandemic ground relentlessly on. I love my family dearly. I really do. With that said, having my children around whilst I attempted to work was remarkably tough. I love those guys but, woah, was it stressful.</p><p>I was feeling at a low ebb. And I wasn&#x27;t sure what to do next. So, feeling tired and pretty fed up, I took a break.</p><h2>&quot;Anybody down there?&quot;</h2><p>Time passed. In March <a href="https://github.com/alexander-akait">Alexander Akait</a> checked in to see how things were going and volunteered to help. He also <a href="https://github.com/TypeStrong/ts-loader/pull/1251#issuecomment-799531375">suggested what turned out to be the fix</a>; namely replacing usage of <code>&#x27;\&#x27;</code> with <code>&#x27;/&#x27;</code> in the assets supplied back to webpack. But crucially I implemented this wrong. Observe <a href="https://github.com/TypeStrong/ts-loader/pull/1251/commits/4bcc5c9623acfd7ffbaf028781a8353b37243804">this commit</a>:</p><pre><code class="language-ts">const assetPath = path
  .relative(compilation.compiler.outputPath, outputFile.name)
  // According to @alexander-akait we should always &#x27;/&#x27; https://github.com/TypeStrong/ts-loader/pull/1251#issuecomment-799606985
  .replace(/\//g, &#x27;/&#x27;);
</code></pre><p>If you look closely at the <code>replace</code> you&#x27;ll see that I&#x27;m globally replacing <code>&#x27;/&#x27;</code> with <code>&#x27;/&#x27;</code> <em>rather</em> than globally replacing <code>&#x27;\&#x27;</code> with <code>&#x27;/&#x27;</code>. The wasted time this caused... I could weep.</p><p>I generally thrashed around for a bit after this. Going in circles, like a six year old swimming wearing one armband. Then <a href="https://github.com/TypeStrong/ts-loader/pull/1251#issuecomment-805143890">Tobias kindly volunteered to help</a>. This much I&#x27;ve learned from a career in software: if talented people offer their assistance, grab it with both hands!</p><p>I&#x27;d been trying be as &quot;learn in public&quot; as possible about the issues I was facing on the pull request. The idea being, to surface the problems in a public forum where others can read and advise. And also to attempt a textual kind of <a href="https://en.wikipedia.org/wiki/Rubber_duck_debugging">rubber duck debugging</a>.</p><p>When Tobias pitched in, I wanted to make it as easy as possible for him to help. So I wrote up <a href="https://github.com/TypeStrong/ts-loader/pull/1251#issuecomment-805181069">a full description of what had changed</a>. What the divergent behaviour in test packs looked like. I shared my speculation for what might be causing the issue (I was wrong by the way). Finally I provided a simple way to get up and running with the broken code. The easier I could make it for others to collaborate on this, I figured, the greater the likelihood of an answer. Tobias got to an answer quickly:</p><blockquote><p>The problem is introduced due to some normalization logic in the test case: see <a href="https://github.com/TypeStrong/ts-loader/pull/1273">#1273</a></p><p>While the PR fixes the problem, I think the paths should be normalized earlier in the pipeline to make this normalization code unnecessary. Note that asset names should have only <code>/</code> as they are filenames and not paths. Only absolute paths have <code>\</code>.</p></blockquote><p>Tobias had raised a PR which introduced a workaround to resolved things in the test pack. This made me happy. More than that, he also identified that the issue lay in <code>ts-loader</code> itself. This caused me to look again at the changes I&#x27;d made, including my <code>replace</code> addition. <a href="https://github.com/TypeStrong/ts-loader/pull/1251#issuecomment-805907212">With fresh eyes, I now realised this was a bug</a>, and <a href="https://github.com/TypeStrong/ts-loader/pull/1251/commits/427714e43519289bb5745ca078133d1ace8fc2c1">fixed</a> it.</p><p>I found then that I could revert Tobias&#x27; workaround and still have passing tests. Result!</p><h2>Release details</h2><p>Now that we&#x27;ve got there; we&#x27;ve shipped. You can get the latest version of <code>ts-loader</code> on <a href="https://www.npmjs.com/package/ts-loader/v/9.0.0">npm</a> and you can find the release details on <a href="https://github.com/TypeStrong/ts-loader/releases/tag/v9.0.0">GitHub</a>.</p><p>Thanks everyone - I couldn&#x27;t have done it without your help. 🌻❤️</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Hello World Bicep]]></title>
            <link>https://blog.johnnyreilly.com/2021/04/10/hello-world-bicep</link>
            <guid>Hello World Bicep</guid>
            <pubDate>Sat, 10 Apr 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Bicep makes Azure Resource Management a great deal simpler than ARM templates. The selling point here is grokkability. This post takes a look at the "Hello World" example recently added to the Bicep repo to appreciate quite what a difference it makes.]]></description>
            <content:encoded><![CDATA[<p>Bicep makes Azure Resource Management a great deal simpler than ARM templates. The selling point here is grokkability. This post takes a look at the <a href="https://github.com/Azure/bicep/pull/2011">&quot;Hello World&quot; example recently added to the Bicep repo</a> to appreciate quite what a difference it makes.</p><p><img src="../static/blog/2021-04-10-hello-world-bicep/hello-world-bicep.png" alt="hello world bicep"/></p><h2>More than configuration</h2><p>The <a href="https://github.com/Azure/bicep/tree/187d4d2047dc83c69695ba79761f552bcb00c319/docs/examples/000/01-hello-world">&quot;Hello World&quot;</a> added to the Bicep repo by <a href="https://github.com/ChristopherGLewis">Chris Lewis</a> illustrates the simplest usage of Bicep:</p><blockquote><p>This bicep file takes a <code>yourName</code> parameter and adds that to a <code>hello</code> variable and returns the concatenated string as an ARM output.</p></blockquote><p>This is, when you consider it, the very essence of a computer program. Taking an input, doing some computation and providing an output. When I think about ARM templates, (and because Bicep is transpiled into ARM templates I mentally bracket the two together) I tend to think about resources being deployed. I focus on <em>configuration</em>, not <em>computation</em></p><p>This is an imperfect mental model. ARM templates can do so much more than deploy by slinging strings and numbers. Thanks to the wealth of <a href="https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/template-functions">template functions</a> that exist they have much more power. They can do computation.</p><p>The Hello World example focuses just on computation.</p><h2>From terse to verbose</h2><p>The Hello World example is made up of two significant files:</p><ol><li><code>main.bicep</code> - the bicep code</li><li><code>main.json</code> - the ARM template compiled from the Bicep file</li></ol><p>The <a href="https://github.com/Azure/bicep/blob/187d4d2047dc83c69695ba79761f552bcb00c319/docs/examples/000/01-hello-world/main.bicep"><code>main.bicep</code></a> file amounts to 3 lines of code (I have omitted the comment line):</p><pre><code class="language-bicep">param yourName string
var hello = &#x27;Hello World! - Hi&#x27;

output helloWorld string = &#x27;${hello} ${yourName}&#x27;
</code></pre><ul><li>the first line takes the <em>input</em> of <code>yourName</code></li><li>the second line declares a <code>hello</code> variable</li><li>the third line <em>computes</em> the new value of <code>helloWorld</code> based upon <code>hello</code> and <code>yourName</code>, then passes it as <em>output</em></li></ul><p>Gosh is it ever simple. It&#x27;s easy to read and it&#x27;s simple to understand. Even if you don&#x27;t know Bicep, if you&#x27;ve experience in another language you can likely guess what&#x27;s happening.</p><p>Let&#x27;s compare this with the <a href="https://github.com/Azure/bicep/blob/187d4d2047dc83c69695ba79761f552bcb00c319/docs/examples/000/01-hello-world/main.json"><code>main.json</code></a> that <code>main.bicep</code> is transpiled into:</p><pre><code class="language-json">{
  &quot;$schema&quot;: &quot;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#&quot;,
  &quot;contentVersion&quot;: &quot;1.0.0.0&quot;,
  &quot;metadata&quot;: {
    &quot;_generator&quot;: {
      &quot;name&quot;: &quot;bicep&quot;,
      &quot;version&quot;: &quot;dev&quot;,
      &quot;templateHash&quot;: &quot;6989941473549654446&quot;
    }
  },
  &quot;parameters&quot;: {
    &quot;yourName&quot;: {
      &quot;type&quot;: &quot;string&quot;
    }
  },
  &quot;functions&quot;: [],
  &quot;variables&quot;: {
    &quot;hello&quot;: &quot;Hello World! - Hi&quot;
  },
  &quot;resources&quot;: [],
  &quot;outputs&quot;: {
    &quot;helloWorld&quot;: {
      &quot;type&quot;: &quot;string&quot;,
      &quot;value&quot;: &quot;[format(&#x27;{0} {1}&#x27;, variables(&#x27;hello&#x27;), parameters(&#x27;yourName&#x27;))]&quot;
    }
  }
}
</code></pre><p>The above ARM template expresses exactly the same thing as the Bicep alternative. But that 3 lines of logic has become 27 lines of JSON. We&#x27;ve lost something in the transition. Intent is no longer clear. We&#x27;ve gone from something easy to reason about, to something that is hard to reason about. You need to think a lot less to write the Bicep alternative and that&#x27;s a <em>good</em> thing.</p><p>I was chatting to someone recently who expressed it well by saying:</p><blockquote><p>ARM is the format that the resource providers understand, so really it’s the Azure equivalent of Assembler – and I don’t know anyone who enjoys coding in Assembler.</p></blockquote><p>This is a great example of the value that Bicep provides. If you&#x27;d like to play with the Hello World a little, why not <a href="https://aka.ms/bicepdemo#eJzT1w9OzC3ISVXISM3JyVcozy/KSeEqSCxKzFWozC8t8kvMTVUoLinKzEvnKkssgqqyVVD3ADPCQcoVFXQVPDLVubjyS0sKSksgasAyUJ0g9SrVYOFaBZVqmLm16gCvlitr">take it for a spin in the Bicep playground</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Bicep meet Azure Pipelines 2]]></title>
            <link>https://blog.johnnyreilly.com/2021/03/23/bicep-meet-azure-pipelines-2</link>
            <guid>Bicep meet Azure Pipelines 2</guid>
            <pubDate>Tue, 23 Mar 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Last time I wrote about how to use the Azure CLI to run Bicep within the context of an Azure Pipeline. The solution was relatively straightforward, and involved using az deployment group create in a task. There's an easier way.]]></description>
            <content:encoded><![CDATA[<p><a href="./2021-03-20-bicep-meet-azure-pipelines.md">Last time</a> I wrote about how to use the Azure CLI to run Bicep within the context of an Azure Pipeline. The solution was relatively straightforward, and involved using <code>az deployment group create</code> in a task. There&#x27;s an easier way.</p><p><img src="../static/blog/2021-03-23-bicep-meet-azure-pipelines-2/bicep-meet-azure-pipelines.png" alt="Bicep meet Azure Pipelines"/></p><h2>The easier way</h2><p>The target reader of the previous post was someone who was already using <code>AzureResourceManagerTemplateDeployment@3</code> in an Azure Pipeline to deploy an ARM template. Rather than replacing your existing <code>AzureResourceManagerTemplateDeployment@3</code> tasks, all you need do is insert a prior <code>bash</code> step that compiles the Bicep to ARM, which your existing template can then process. It looks like this:</p><pre><code class="language-yml">- bash: az bicep build --file infra/app-service/azuredeploy.bicep
  displayName: &#x27;Compile Bicep to ARM&#x27;
</code></pre><p>This will take your Bicep template of <code>azuredeploy.bicep</code>, transpile it into an ARM template named <code>azuredeploy.json</code> which a subsequent <code>AzureResourceManagerTemplateDeployment@3</code> task can process. Since this is just exercising the Azure CLI, using <code>bash</code> is not required; powershell etc would also be fine; it&#x27;s just required that the Azure CLI is available in a pipeline.</p><p>In fact this simple task could even be a one-liner if you didn&#x27;t fancy using the <code>displayName</code>. (Though I say keep it; optimising for readability is generally a good shout.) A full pipeline could look like this:</p><pre><code class="language-yml">- bash: az bicep build --file infra/app-service/azuredeploy.bicep
  displayName: &#x27;Compile Bicep to ARM&#x27;

- task: AzureResourceManagerTemplateDeployment@3
  displayName: &#x27;Deploy Hello Azure ARM&#x27;
  inputs:
    azureResourceManagerConnection: &#x27;$(azureSubscription)&#x27;
    action: Create Or Update Resource Group
    resourceGroupName: &#x27;$(resourceGroupName)&#x27;
    location: &#x27;North Europe&#x27;
    templateLocation: Linked artifact
    csmFile: &#x27;infra/app-service/azuredeploy.json&#x27; # created by bash script
    csmParametersFile: &#x27;infra/app-service/azuredeploy.parameters.json&#x27;
    deploymentMode: Incremental
    deploymentOutputs: resourceGroupDeploymentOutputs
    overrideParameters: -applicationName $(Build.Repository.Name)

- pwsh: |
    $outputs = ConvertFrom-Json &#x27;$(resourceGroupDeploymentOutputs)&#x27;
    foreach ($output in $outputs.PSObject.Properties) {
        Write-Host &quot;##vso[task.setvariable variable=RGDO_$($output.Name)]$($output.Value.value)&quot;
    }
  displayName: &#x27;Turn ARM outputs into variables&#x27;
</code></pre><p>And when it&#x27;s run, it may result in something along these lines:</p><p><img src="../static/blog/2021-03-23-bicep-meet-azure-pipelines-2/azure-pipeline-with-bicep.png" alt="Bicep in an Azure Pipeline"/></p><p>So if you want to get using Bicep right now with minimal effort, this an on ramp that could work for you! Props to <a href="https://twitter.com/foldr">Jamie McCrindle</a> for suggesting this.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Bicep meet Azure Pipelines]]></title>
            <link>https://blog.johnnyreilly.com/2021/03/20/bicep-meet-azure-pipelines</link>
            <guid>Bicep meet Azure Pipelines</guid>
            <pubDate>Sat, 20 Mar 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Bicep is a terser and more readable alternative language to ARM templates. Running ARM templates in Azure Pipelines is straightforward. However, there isn't yet a first class experience for running Bicep in Azure Pipelines. This post demonstrates an approach that can be used until a Bicep task is available.]]></description>
            <content:encoded><![CDATA[<p><a href="https://github.com/Azure/bicep">Bicep</a> is a terser and more readable alternative language to ARM templates. Running ARM templates in Azure Pipelines is straightforward. However, there isn&#x27;t yet a first class experience for running Bicep in Azure Pipelines. This post demonstrates an approach that can be used until a Bicep task is available.</p><p><img src="../static/blog/2021-03-20-bicep-meet-azure-pipelines/bicep-meet-azure-pipelines.png" alt="Bicep meet Azure Pipelines"/></p><h2>Bicep: mostly ARMless</h2><p>If you&#x27;ve been working with Azure and infrastructure as code, you&#x27;ll likely have encountered <a href="https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/overview">ARM templates</a>. They&#x27;re a domain specific language that lives inside JSON, used to define the infrastructure that is deployed to Azure; App Services, Key Vaults and the like.</p><p>ARM templates are quite verbose and not the easiest thing to read. This is a consequence of being effectively a language nestled inside another language. Bicep is an alternative language which is far more readable. Bicep transpiles down to ARM templates, in the same way that TypeScript transpiles down to JavaScript.</p><p>Bicep is quite new, but already it enjoys feature parity with ARM templates (as of <a href="https://github.com/Azure/bicep/releases/tag/v0.3.1">v0.3</a>) and ships as part of the <a href="https://github.com/MicrosoftDocs/azure-docs-cli/blob/master/docs-ref-conceptual/release-notes-azure-cli.md#arm-1">Azure CLI</a>. However, as Bicep is new, it doesn&#x27;t yet have a dedicated Azure Pipelines task for deployment. This should exist in future, perhaps as soon as the <a href="https://github.com/Azure/bicep/issues/1341">v0.4 release</a>. In the meantime there&#x27;s an alternative way to achieve this which we&#x27;ll go through.</p><h2>App Service with Bicep</h2><p>Let&#x27;s take a simple Bicep file, <code>azuredeploy.bicep</code>, which is designed to deploy an App Service resource to Azure. It looks like this:</p><pre><code class="language-bicep">@description(&#x27;Tags that our resources need&#x27;)
param tags object = {
  costCenter: &#x27;todo: replace&#x27;
  environment: &#x27;todo: replace&#x27;
  application: &#x27;todo: replace with app name&#x27;
  description: &#x27;todo: replace&#x27;
  managedBy: &#x27;ARM&#x27;
}

@minLength(2)
@description(&#x27;Base name of the resource such as web app name and app service plan&#x27;)
param applicationName string

@description(&#x27;Location for all resources.&#x27;)
param location string = resourceGroup().location

@description(&#x27;The SKU of App Service Plan&#x27;)
param sku string

var appServicePlanName_var = &#x27;plan-${applicationName}-${tags.environment}&#x27;
var linuxFxVersion = &#x27;DOTNETCORE|5.0&#x27;
var fullApplicationName_var = &#x27;app-${applicationName}-${uniqueString(applicationName)}&#x27;

resource appServicePlanName &#x27;Microsoft.Web/serverfarms@2019-08-01&#x27; = {
  name: appServicePlanName_var
  location: location
  sku: {
    name: sku
  }
  kind: &#x27;linux&#x27;
  tags: {
    CostCenter: tags.costCenter
    Environment: tags.environment
    Description: tags.description
    ManagedBy: tags.managedBy
  }
  properties: {
    reserved: true
  }
}

resource fullApplicationName &#x27;Microsoft.Web/sites@2018-11-01&#x27; = {
  name: fullApplicationName_var
  location: location
  kind: &#x27;app&#x27;
  tags: {
    CostCenter: tags.costCenter
    Environment: tags.environment
    Description: tags.description
    ManagedBy: tags.managedBy
  }
  properties: {
    serverFarmId: appServicePlanName.id
    clientAffinityEnabled: true
    siteConfig: {
      appSettings: []
      linuxFxVersion: linuxFxVersion
      alwaysOn: false
      ftpsState: &#x27;Disabled&#x27;
      http20Enabled: true
      minTlsVersion: &#x27;1.2&#x27;
      remoteDebuggingEnabled: false
    }
    httpsOnly: true
  }
  identity: {
    type: &#x27;SystemAssigned&#x27;
  }
}

output fullApplicationName string = fullApplicationName_var
</code></pre><p>When transpiled down to an ARM template, this Bicep file more than doubles in size:</p><ul><li><code>azuredeploy.bicep</code> - 1782 bytes</li><li><code>azuredeploy.json</code> - 3863 bytes</li></ul><p>This tells you something of the advantage of Bicep. The template comes with an associated <code>azuredeploy.parameters.json</code> file:</p><pre><code class="language-json">{
  &quot;$schema&quot;: &quot;https://schema.management.azure.com/schemas/2019-04-01/deploymentParameters.json#&quot;,
  &quot;contentVersion&quot;: &quot;1.0.0.0&quot;,
  &quot;parameters&quot;: {
    &quot;tags&quot;: {
      &quot;value&quot;: {
        &quot;costCenter&quot;: &quot;8888&quot;,
        &quot;environment&quot;: &quot;stg&quot;,
        &quot;application&quot;: &quot;hello-azure&quot;,
        &quot;description&quot;: &quot;App Service for hello-azure&quot;,
        &quot;managedBy&quot;: &quot;ARM&quot;
      }
    },
    &quot;sku&quot;: {
      &quot;value&quot;: &quot;B1&quot;
    }
  }
}
</code></pre><p>It&#x27;s worth remembering that you can use the same parameters files with Bicep that you can use with ARM templates. This is great for minimising friction when it comes to migrating.</p><h2>Bicep in <code>azure-pipelines.yml</code></h2><p>Now we have our Bicep file, we want to execute it from the context of an Azure Pipeline. If we were working directly with the ARM template we&#x27;d likely have something like this in place:</p><pre><code class="language-yml">- task: AzureResourceManagerTemplateDeployment@3
  displayName: &#x27;Deploy Hello Azure ARM&#x27;
  inputs:
    azureResourceManagerConnection: &#x27;$(azureSubscription)&#x27;
    action: Create Or Update Resource Group
    resourceGroupName: &#x27;$(resourceGroupName)&#x27;
    location: &#x27;North Europe&#x27;
    templateLocation: Linked artifact
    csmFile: &#x27;infra/app-service/azuredeploy.json&#x27;
    csmParametersFile: &#x27;infra/app-service/azuredeploy.parameters.json&#x27;
    deploymentMode: Incremental
    deploymentOutputs: resourceGroupDeploymentOutputs
    overrideParameters: -applicationName $(Build.Repository.Name)

- pwsh: |
    $outputs = ConvertFrom-Json &#x27;$(resourceGroupDeploymentOutputs)&#x27;
    foreach ($output in $outputs.PSObject.Properties) {
        Write-Host &quot;##vso[task.setvariable variable=RGDO_$($output.Name)]$($output.Value.value)&quot;
    }
  displayName: &#x27;Turn ARM outputs into variables&#x27;
</code></pre><p>There&#x27;s two tasks above. The first is the native task for ARM deployments which takes our ARM template and our parameters and deploys them. The second task takes the output variables from the first task and converts them into Azure Pipeline variables such that they can be referenced later in the pipeline. In this case this variablifies our <code>fullApplicationName</code> output.</p><p>There is, as yet, no <code>BicepTemplateDeployment@1</code>. <a href="https://github.com/Azure/bicep/issues/1341">Though it&#x27;s coming</a>. In the meantime, the marvellous <a href="https://twitter.com/adotfrank">Alex Frankel</a> <a href="https://github.com/Azure/bicep/issues/1341#issuecomment-802010110">advised</a>:</p><blockquote><p>I&#x27;d recommend using the <a href="https://docs.microsoft.com/azure/devops/pipelines/tasks/deploy/azure-cli?view=azure-devops">Azure CLI task</a> to deploy. As long as that task is updated to Az CLI version 2.20 or later, it will automatically install the bicep CLI when calling <code>az deployment group create -f main.bicep</code>.</p></blockquote><p>Let&#x27;s give it a go!</p><pre><code class="language-yml">- task: AzureCLI@2
  displayName: &#x27;Deploy Hello Azure Bicep&#x27;
  inputs:
    azureSubscription: &#x27;$(azureSubscription)&#x27;
    scriptType: bash
    scriptLocation: inlineScript
    inlineScript: |
      az --version

      echo &quot;az deployment group create --resource-group &#x27;$(resourceGroupName)&#x27; --name appservicedeploy&quot;
      az deployment group create --resource-group &#x27;$(resourceGroupName)&#x27; --name appservicedeploy \
        --template-file infra/app-service/azuredeploy.bicep \
        --parameters infra/app-service/azuredeploy.parameters.json \
        --parameters applicationName=&#x27;$(Build.Repository.Name)&#x27;

      echo &quot;az deployment group show --resource-group &#x27;$(resourceGroupName)&#x27; --name appservicedeploy&quot;
      deploymentoutputs=$(az deployment group show --resource-group &#x27;$(resourceGroupName)&#x27; --name appservicedeploy \
        --query properties.outputs)

      echo &#x27;convert outputs to variables&#x27;
      echo $deploymentoutputs | jq -c &#x27;. | to_entries[] | [.key, .value.value]&#x27; |
        while IFS=$&quot;\n&quot; read -r c; do
          outputname=$(echo &quot;$c&quot; | jq -r &#x27;.[0]&#x27;)
          outputvalue=$(echo &quot;$c&quot; | jq -r &#x27;.[1]&#x27;)
          echo &quot;setting variable RGDO_$outputname=$outputvalue&quot;
          echo &quot;##vso[task.setvariable variable=RGDO_$outputname]$outputvalue&quot;
        done
</code></pre><p>The above is just a single Azure CLI task (as advised). It invokes <code>az deployment group create</code> passing the relevant parameters. It then acquires the output properties using <code>az deployment group show</code>. Finally it once again converts these outputs to Azure Pipeline variables with some <a href="https://stedolan.github.io/jq/"><code>jq</code></a> smarts.</p><p>This works right now, and running it results in something like the output below. So if you&#x27;re excited about Bicep and don&#x27;t want to wait for 0.4 to start moving on this, then this can get you going. To track the progress of the custom task, <a href="https://github.com/Azure/bicep/issues/1341">keep an eye on this issue</a>.</p><p><img src="../static/blog/2021-03-20-bicep-meet-azure-pipelines/bicep-in-a-pipeline.png" alt="Bicep in an Azure Pipeline"/></p><h2>Update: an even simpler alternative</h2><p>There is even a simpler way to do this which I discovered subsequent to writing this. <a href="./2021-03-23-bicep-meet-azure-pipelines-2.md">Have a read</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[RSS update; we moved to Docusaurus]]></title>
            <link>https://blog.johnnyreilly.com/2021/03/17/rss-update-we-moved-to-docusaurus</link>
            <guid>RSS update; we moved to Docusaurus</guid>
            <pubDate>Wed, 17 Mar 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[My blog lived happily on Blogger for the past decade. It's now built with Docusaurus and hosted on GitHub Pages. To understand the why, read my last post. This post serves purely to share details of feed updates for RSS / Atom subscribers.]]></description>
            <content:encoded><![CDATA[<p>My blog lived happily on <a href="https://icanmakethiswork.blogspot.com/">Blogger</a> for the past decade. It&#x27;s now built with <a href="https://v2.docusaurus.io/">Docusaurus</a> and hosted on <a href="https://pages.github.com/">GitHub Pages</a>. To understand the why, <a href="./2021-03-15-from-blogger-to-docusaurus.md">read my last post</a>. This post serves purely to share details of feed updates for RSS / Atom subscribers.</p><p>The Atom feed at this location no longer exists: <a href="https://blog.johnnyreilly.com/feeds/posts/default">https://blog.johnnyreilly.com/feeds/posts/default</a></p><p>The following feeds are new and different:</p><ul><li>RSS - <a href="https://blog.johnnyreilly.com/rss.xml">https://blog.johnnyreilly.com/rss.xml</a></li><li>Atom - <a href="https://blog.johnnyreilly.com/atom.xml">https://blog.johnnyreilly.com/atom.xml</a></li></ul><p>The new format might mess with any feed reader you have set up. I do apologise for the friction; hopefully it shouldn&#x27;t cause you too much drama.</p><p>Finally, all historic links should continue to work with the new site; redirects have been implemented.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[From Blogger to Docusaurus]]></title>
            <link>https://blog.johnnyreilly.com/2021/03/15/from-blogger-to-docusaurus</link>
            <guid>From Blogger to Docusaurus</guid>
            <pubDate>Mon, 15 Mar 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Docusaurus is, amongst other things, a Markdown powered blogging platform. My blog has lived happily on Blogger for the past decade. I'm considering moving, but losing my historic content as part of the move was never an option. This post goes through what it would look like to move from Blogger to Docusaurus without losing your content.]]></description>
            <content:encoded><![CDATA[<p><a href="https://v2.docusaurus.io/">Docusaurus</a> is, amongst other things, a Markdown powered blogging platform. My blog has lived happily on <a href="https://www.blogger.com/">Blogger</a> for the past decade. I&#x27;m considering moving, but losing my historic content as part of the move was never an option. This post goes through what it would look like to move from Blogger to Docusaurus <em>without</em> losing your content.</p><p>It is imperative that the world never forgets what I was doing with jQuery in 2012.</p><h2>Blog as code</h2><p>Everything is better when it&#x27;s code. Infrastructure as code. Awesome right? So naturally &quot;blog as code&quot; must be better than just a blog. More seriously, <a href="https://en.wikipedia.org/wiki/Markdown">Markdown</a> is a tremendous documentation format. Simple, straightforward and, like Goldilocks, &quot;just right&quot;. For a long time I&#x27;ve written everything as Markdown. My years of toil down the Open Source mines have preconditioned me to be very MD-disposed.</p><p>I started out writing this blog a long time ago as pure HTML. Not the smoothest of writing formats. At some point I got into the habit of spinning up a new repo in GitHub for a new blogpost, writing it in Markdown and piping it through a variety of tools to convert it into HTML for publication on Blogger. As time passed I felt I&#x27;d be a lot happier if I wasn&#x27;t creating a repo each time. What if I did all my blogging in a single repo and used that as the code that represented my blog?</p><p>Just having that thought laid the seeds for what was to follow:</p><ol><li>An investigation into importing my content from Blogger into a GitHub repo</li><li>An experimental port to Docusaurus</li><li>The automation of publication to Docusaurus and Blogger</li></ol><p>We&#x27;re going to go through 1 and 2 now. But before we do that, let&#x27;s create ourselves a Docusaurus site for our blog:</p><pre><code>npx @docusaurus/init@latest init blog-website classic
</code></pre><h2>I want everything</h2><p>The first thing to do, was obtain my blog content. This is a mass of HTML that lived inside Blogger&#x27;s database. (One assumes they have a database; I haven&#x27;t actually checked.) There&#x27;s a &quot;Back up content&quot; option inside Blogger to allow this:</p><p><img src="../static/blog/2021-03-15-from-blogger-to-docusaurus/blogger-back-up-your-content.png" alt="Download content from Blogger"/></p><p>It provides you with an XML file with a dispiritingly small size. Ten years blogging? You&#x27;ll get change out of 4Mb it turns out.</p><h2>From HTML in XML to Markdown</h2><p>We now want to take that XML and:</p><ul><li>Extract each blog post (and it&#x27;s associated metadata; title / tags and whatnot)</li><li>Convert the HTML content of each blog post from HTML to Markdown and save it as a <code>.md</code> file</li><li>Download the images used in the blogpost so they can be stored in the repo alongside</li></ul><p>To do this we&#x27;re going to whip up a smallish TypeScript console app. Let&#x27;s initialise it with the packages we&#x27;re going to need:</p><pre><code>mkdir from-blogger-to-docusaurus
cd from-blogger-to-docusaurus
npx typescript --init
yarn init
yarn add @types/axios @types/he @types/jsdom @types/node @types/showdown axios fast-xml-parser he jsdom showdown ts-node typescript
</code></pre><p>We&#x27;re using:</p><ul><li><a href="https://github.com/NaturalIntelligence/fast-xml-parser"><code>fast-xml-parser</code></a> to parse XML</li><li><a href="https://github.com/mathiasbynens/he"><code>he</code></a>, <a href="https://github.com/jsdom/jsdom">jsdom</a> and <a href="https://github.com/showdownjs/showdown">showdown</a> to convert HTML to Markdown</li><li><a href="https://github.com/axios/axios"><code>axios</code></a> to download images</li><li><a href="https://github.com/microsoft/TypeScript"><code>typescript</code></a> to code in and <a href="https://github.com/TypeStrong/ts-node"><code>ts-node</code></a> to make our TypeScript Node.js console app.</li></ul><p>Now we have all the packages we need, it&#x27;s time to write our script.</p><pre><code class="language-ts">import fs from &#x27;fs&#x27;;
import path from &#x27;path&#x27;;
import showdown from &#x27;showdown&#x27;;
import he from &#x27;he&#x27;;
import jsdom from &#x27;jsdom&#x27;;
import axios from &#x27;axios&#x27;;
import fastXmlParser from &#x27;fast-xml-parser&#x27;;

const bloggerXmlPath = &#x27;./blog-03-13-2021.xml&#x27;;
const docusaurusDirectory = &#x27;../blog-website&#x27;;
const notMarkdownable: string[] = [];

async function fromXmlToMarkDown() {
  const posts = await getPosts();

  for (const post of posts) {
    await makePostIntoMarkDownAndDownloadImages(post);
  }
  if (notMarkdownable.length)
    console.log(
      &#x27;These blog posts could not be turned into MarkDown - go find out why!&#x27;,
      notMarkdownable
    );
}

async function getPosts(): Promise&lt;Post[]&gt; {
  const xml = await fs.promises.readFile(bloggerXmlPath, &#x27;utf-8&#x27;);

  const options = {
    attributeNamePrefix: &#x27;@_&#x27;,
    attrNodeName: &#x27;attr&#x27;, //default is &#x27;false&#x27;
    textNodeName: &#x27;#text&#x27;,
    ignoreAttributes: false,
    ignoreNameSpace: false,
    allowBooleanAttributes: true,
    parseNodeValue: true,
    parseAttributeValue: true,
    trimValues: true,
    cdataTagName: &#x27;__cdata&#x27;, //default is &#x27;false&#x27;
    cdataPositionChar: &#x27;\\c&#x27;,
    parseTrueNumberOnly: false,
    arrayMode: true, //&quot;strict&quot;
    attrValueProcessor: (val: string, attrName: string) =&gt;
      he.decode(val, { isAttributeValue: true }), //default is a=&gt;a
    tagValueProcessor: (val: string, tagName: string) =&gt; he.decode(val), //default is a=&gt;a
  };

  const traversalObj = fastXmlParser.getTraversalObj(xml, options);
  const blog = fastXmlParser.convertToJson(traversalObj, options);

  const postsRaw = blog.feed[0].entry.filter(
    (entry: any) =&gt;
      entry.category.some(
        (category: any) =&gt;
          category.attr[&#x27;@_term&#x27;] ===
          &#x27;http://schemas.google.com/blogger/2008/kind#post&#x27;
      ) &amp;&amp;
      entry.link.some(
        (link: any) =&gt;
          link.attr[&#x27;@_href&#x27;] &amp;&amp; link.attr[&#x27;@_type&#x27;] === &#x27;text/html&#x27;
      ) &amp;&amp;
      entry.published &lt; &#x27;2021-03-07&#x27;
  );

  const posts: Post[] = postsRaw.map((entry: any) =&gt; {
    return {
      title: entry.title[0][&#x27;#text&#x27;],
      content: entry.content[0][&#x27;#text&#x27;],
      published: entry.published,
      link: entry.link.find(
        (link: any) =&gt;
          link.attr[&#x27;@_href&#x27;] &amp;&amp; link.attr[&#x27;@_type&#x27;] === &#x27;text/html&#x27;
      )
        ? entry.link.find(
            (link: any) =&gt;
              link.attr[&#x27;@_href&#x27;] &amp;&amp; link.attr[&#x27;@_type&#x27;] === &#x27;text/html&#x27;
          ).attr[&#x27;@_href&#x27;]
        : undefined,
      tags:
        Array.isArray(entry.category) &amp;&amp;
        entry.category.some(
          (category: any) =&gt;
            category.attr[&#x27;@_scheme&#x27;] === &#x27;http://www.blogger.com/atom/ns#&#x27;
        )
          ? entry.category
              .filter(
                (category: any) =&gt;
                  category.attr[&#x27;@_scheme&#x27;] ===
                    &#x27;http://www.blogger.com/atom/ns#&#x27; &amp;&amp;
                  category.attr[&#x27;@_term&#x27;] !== &#x27;constructor&#x27;
              ) // &#x27;constructor&#x27; will make docusaurus choke
              .map((category: any) =&gt; category.attr[&#x27;@_term&#x27;])
          : [],
    };
  });

  for (const post of posts) {
    const { content, ...others } = post;
    console.log(others, content.length);
    if (!content || !others.title || !others.published)
      throw new Error(&#x27;No content&#x27;);
  }

  return posts.filter((post) =&gt; post.link);
}

async function makePostIntoMarkDownAndDownloadImages(post: Post) {
  const converter = new showdown.Converter({
    ghCodeBlocks: true,
  });
  const linkSections = post.link.split(&#x27;/&#x27;);
  const linkSlug = linkSections[linkSections.length - 1];
  const filename =
    post.published.substr(0, 10) + &#x27;-&#x27; + linkSlug.replace(&#x27;.html&#x27;, &#x27;.md&#x27;);

  const contentProcessed = post.content
    // remove stray &lt;br /&gt; tags
    .replace(/&lt;br\s*\/?&gt;/gi, &#x27;\n&#x27;)
    // translate &lt;code class=&quot;lang-cs&quot; into &lt;code class=&quot;language-cs&quot;&gt; to be showdown friendly
    .replace(/code class=&quot;lang-/gi, &#x27;code class=&quot;language-&#x27;);

  const images: string[] = [];
  const dom = new jsdom.JSDOM(contentProcessed);
  let markdown = &#x27;&#x27;;
  try {
    markdown = converter
      .makeMarkdown(contentProcessed, dom.window.document)
      // bigger titles
      .replace(/#### /g, &#x27;## &#x27;)

      // &lt;div style=&quot;width:100%;height:0;padding-bottom:56%;position:relative;&quot;&gt;&lt;iframe src=&quot;https://giphy.com/embed/l7JDTHpsXM26k&quot; width=&quot;100%&quot; height=&quot;100%&quot; style=&quot;position:absolute&quot; frameBorder=&quot;0&quot; class=&quot;giphy-embed&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

      // The mechanism below extracts the underlying iframe
      .replace(/&lt;div.*(&lt;iframe.*&quot;&gt;).*&lt;\/div&gt;/g, (replacer) =&gt; {
        const dom = new jsdom.JSDOM(replacer);
        const iframe = dom?.window?.document?.querySelector(&#x27;iframe&#x27;);
        return iframe?.outerHTML ?? &#x27;&#x27;;
      })

      // The mechanism below strips class and style attributes from iframes - react hates them
      .replace(/&lt;iframe.*&lt;\/iframe&gt;/g, (replacer) =&gt; {
        const dom = new jsdom.JSDOM(replacer);
        const iframe = dom?.window?.document?.querySelector(&#x27;iframe&#x27;);
        iframe?.removeAttribute(&#x27;class&#x27;);
        iframe?.removeAttribute(&#x27;style&#x27;);
        return iframe?.outerHTML ?? &#x27;&#x27;;
      })

      // capitalise appropriately
      .replace(/frameBorder/g, &#x27;frameBorder&#x27;)
      .replace(/allowfullscreen/g, &#x27;allowFullScreen&#x27;)
      .replace(/charset/g, &#x27;charSet&#x27;)

      // Deals with these:
      // [![null](&lt;https://4.bp.blogspot.com/-b9-GrL0IXaY/Xmqj4GRhKXI/AAAAAAAAT5s/ZoceUInSY5EWXeCr2LkGV9Zvea8S6-mUgCPcBGAYYCw/s640/hello_world_idb_keyval.png&gt; =640x484)](&lt;https://4.bp.blogspot.com/-b9-GrL0IXaY/Xmqj4GRhKXI/AAAAAAAAT5s/ZoceUInSY5EWXeCr2LkGV9Zvea8S6-mUgCPcBGAYYCw/s1600/hello_world_idb_keyval.png&gt;)We successfully wrote something into IndexedDB, read it back and printed that value to the console. Amazing!
      .replace(
        /\[!\[null\]\(&lt;(.*?)&gt;\)/g,
        (match) =&gt;
          `![](${match.slice(match.indexOf(&#x27;&lt;&#x27;) + 1, match.indexOf(&#x27;&gt;&#x27;))})\n\n`
      )

      // Blogger tends to put images in HTML that looks like this:
      // &lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-UwrtZigWg78/YDqN82KbjVI/AAAAAAAAZTE/Umezr1MGQicnxMMr5rQHD4xKINg9fasDACLcBGAsYHQ/s783/traffic-to-app-service.png&quot; style=&quot;display: block; padding: 1em 0; text-align: center; &quot;&gt;&lt;img alt=&quot;traffic to app service&quot; border=&quot;0&quot; width=&quot;600&quot; data-original-height=&quot;753&quot; data-original-width=&quot;783&quot; src=&quot;https://1.bp.blogspot.com/-UwrtZigWg78/YDqN82KbjVI/AAAAAAAAZTE/Umezr1MGQicnxMMr5rQHD4xKINg9fasDACLcBGAsYHQ/s600/traffic-to-app-service.png&quot;&gt;&lt;/a&gt;&lt;/div&gt;

      // The mechanism below extracts the underlying image path and it&#x27;s alt text
      .replace(/&lt;div.*(&lt;img.*&quot;&gt;).*&lt;\/div&gt;/g, (replacer) =&gt; {
        const div = new jsdom.JSDOM(replacer);
        const img = div?.window?.document?.querySelector(&#x27;img&#x27;);
        const alt = img?.getAttribute(&#x27;alt&#x27;) ?? &#x27;&#x27;;
        const src = img?.getAttribute(&#x27;src&#x27;) ?? &#x27;&#x27;;

        if (src) images.push(src);

        return `![${alt}](${src})`;
      });
  } catch (e) {
    console.log(post.link);
    console.log(e);
    notMarkdownable.push(post.link);
    return;
  }

  const imageDirectory = filename.replace(&#x27;.md&#x27;, &#x27;&#x27;);
  for (const url of images) {
    try {
      const localUrl = await downloadImage(url, imageDirectory);
      markdown = markdown.replace(url, &#x27;../static/blog/&#x27; + localUrl);
    } catch (e) {
      console.error(`Failed to download ${url}`);
    }
  }

  const content = `---
title: &quot;${post.title}&quot;
author: John Reilly
author_url: https://github.com/johnnyreilly
author_image_url: https://avatars.githubusercontent.com/u/1010525?s=400&amp;u=294033082cfecf8ad1645b4290e362583b33094a&amp;v=4
tags: [${post.tags.join(&#x27;, &#x27;)}]
hide_table_of_contents: false
---
${markdown}
`;

  await fs.promises.writeFile(
    path.resolve(docusaurusDirectory, &#x27;blog&#x27;, filename),
    content
  );
}

async function downloadImage(url: string, directory: string) {
  console.log(`Downloading ${url}`);
  const pathParts = new URL(url).pathname.split(&#x27;/&#x27;);
  const filename = pathParts[pathParts.length - 1];
  const directoryTo = path.resolve(
    docusaurusDirectory,
    &#x27;static&#x27;,
    &#x27;blog&#x27;,
    directory
  );
  const pathTo = path.resolve(
    docusaurusDirectory,
    &#x27;static&#x27;,
    &#x27;blog&#x27;,
    directory,
    filename
  );

  if (!fs.existsSync(directoryTo)) {
    fs.mkdirSync(directoryTo);
  }

  const writer = fs.createWriteStream(pathTo);

  const response = await axios({
    url,
    method: &#x27;GET&#x27;,
    responseType: &#x27;stream&#x27;,
  });

  response.data.pipe(writer);

  return new Promise&lt;string&gt;((resolve, reject) =&gt; {
    writer.on(&#x27;finish&#x27;, () =&gt; resolve(directory + &#x27;/&#x27; + filename));
    writer.on(&#x27;error&#x27;, reject);
  });
}

interface Post {
  title: string;
  content: string;
  published: string;
  link: string;
  tags: string[];
}

// do it!
fromXmlToMarkDown();
</code></pre><p>To summarise what the script does, it:</p><ul><li>parses the blog XML into an array of <code>Post</code>s</li><li>each post is then converted from HTML into Markdown, a Docusaurus header is created and prepended, then the file is saved to the <code>blog-website/blog</code> directory</li><li>the images of each post are downloaded with Axios and saved to the <code>blog-website/static/blog/{POST NAME}</code> directory</li></ul><h2>Bringing it all together</h2><p>To run the script, we add the following script to the <code>package.json</code>:</p><pre><code>  &quot;scripts&quot;: {
    &quot;start&quot;: &quot;ts-node index.ts&quot;
  },
</code></pre><p>And have ourselves a merry little <code>yarn start</code> to kick off the process. In a very short period of time, if you crack open the <code>blogs</code> directory of your Docusaurus site you&#x27;ll see a collection of Markdown files which represent your blog and are ready to power Docusaurus:</p><p><img src="../static/blog/2021-03-15-from-blogger-to-docusaurus/blogs-as-markdown.png" alt="Markdown files"/></p><p>I have slightly papered over some details here. For my own case I discovered that I hadn&#x27;t always written perfect HTML when blogging. I had to go in and fix the HTML in a number of historic blogs such that the mechanism would work. I also learned that a number of my screenshots that I use to illustrate posts have vanished from Blogger at some point. This makes me all the more convinced that storing your blog in a repo is a good idea. Things should not &quot;go missing&quot;.</p><p>Congratulations! We&#x27;re now the proud owners of a Docusaurus blog site based upon our Blogger content that looks something like this:</p><p><img src="../static/blog/2021-03-15-from-blogger-to-docusaurus/docusaurus.png" alt="Blog in Docusaurus"/></p><h2>Making the move?</h2><p>Now that I&#x27;ve got the content, I&#x27;m theoretically safe to migrate from Blogger to Docusaurus. I&#x27;m pondering this now and I have come up with a checklist of criteria to satisfy before I do. You can have a read of the <a href="https://github.com/johnnyreilly/blog.johnnyreilly.com#migrating-to-docusauras">criteria here</a>.</p><p>Odds are, I&#x27;m likely to make the move; it&#x27;s probably just a matter of time.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Managed Identity, Azure SQL and Entity Framework]]></title>
            <link>https://blog.johnnyreilly.com/2021/03/10/managed-identity-azure-sql-entity-framework</link>
            <guid>Managed Identity, Azure SQL and Entity Framework</guid>
            <pubDate>Wed, 10 Mar 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Managed Identity offers a very secure way for applications running in Azure to connect to Azure SQL databases. It's an approach that does not require code changes; merely configuration of connection string and associated resources. Hence it has a good developer experience. Importantly, it allows us to avoid exposing our database to username / password authentication, and hence making it a tougher target for bad actors.]]></description>
            <content:encoded><![CDATA[<p>Managed Identity offers a very secure way for applications running in Azure to connect to Azure SQL databases. It&#x27;s an approach that does not require code changes; merely configuration of connection string and associated resources. Hence it has a good developer experience. Importantly, it allows us to avoid exposing our database to username / password authentication, and hence making it a tougher target for bad actors.</p><p>This post talks us through using managed identity for connecting to Azure SQL.</p><h2><code>Integrated Security=true</code></h2><p>Everyone is deploying to the cloud. Few are the organisations that view deployment to data centers they manage as the future. This is generally a good thing, however in the excitement of the new, it&#x27;s possible to forget some of the good properties that &quot;on premise&quot; deployment afforded when it came to connectivity and authentication.</p><p>I speak of course, of our old friend <code>Integrated Security=true</code>. When you seek to connect a web application to a database, you&#x27;ll typically use some kind of database connection string. And back in the day, it may have looked something like this:</p><pre><code>Data Source=myServer;Initial Catalog=myDB;Integrated Security=true;
</code></pre><p>The above provides a database server, a database and also <code>Integrated Security=true</code>. When you see <code>Integrated Security=true</code>, what you&#x27;re essentially looking at is an instruction to use the identity that an application is running under (typically called a &quot;service account&quot;) as the authentication credential to secure access to the database. Under the covers, this amounts to <a href="https://docs.microsoft.com/en-us/dotnet/framework/data/adonet/sql/authentication-in-sql-server">Windows Authentication</a>.</p><p>The significant thing about this approach is that it is more secure than using usernames and passwords in the connection string. If you have to use username and password to authenticate, then you need to persist them somewhere - so you need to make sure that&#x27;s secure. Also, if someone manages to acquire that username and password, they&#x27;re free to get access to the database and do malicious things.</p><p>Bottom line: the less you are sharing authentication credentials, the better your security. Integrated Security is a harder nut to crack than username and password. The thing to note about the above phrase is &quot;Windows Authentication&quot;. Web Apps in Azure / AWS etc do not typically use Windows Authentication when it comes to connecting to the database. Connecting with username / password is far more common.</p><p>What if there was a way to have the developer experience of <code>Integrated Security=true</code> without needing to use Windows Authentication? There is.</p><h2>Managed Identity</h2><p>The docs express the purpose of <a href="https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview">managed identity</a> well:</p><blockquote><p>A common challenge for developers is the management of secrets and credentials to secure communication between different services. On Azure, managed identities eliminate the need for developers having to manage credentials by providing an identity for the Azure resource in Azure AD and using it to obtain Azure Active Directory (Azure AD) tokens</p></blockquote><p>Historically a certain amount of ceremony was required to use managed identity to connect to a database, and could involve augmenting a <code>DbContext</code> like so:</p><pre><code class="language-cs">public MyDbContext(DbContextOptions options) : base(options) {
    var conn = (Microsoft.Data.SqlClient.SqlConnection)Database.GetDbConnection();
    var credential = new DefaultAzureCredential();
    var token = credential
        .GetToken(
            new Azure.Core.TokenRequestContext(new[] { &quot;https://database.windows.net/.default&quot; })
        );
    conn.AccessToken = token.Token;
}
</code></pre><p>This mechanism works, and has the tremendous upside of no longer requiring credentials be passed in a connection string. However, as you can see this isn&#x27;t the simplest of setups. And also, what if you don&#x27;t want to use managed identity when you&#x27;re developing locally? This approach has baggage and forces us to make code changes.</p><h2>Connection String alone</h2><p>The wonderful aspect of the original <code>Integrated Security=true</code> approach, was that there were no code changes required; one need only supply the connection string. Just configuration.</p><p>This is now possible with Azure SQL thanks to <a href="https://github.com/dotnet/SqlClient/pull/730">this PR</a> to the <a href="https://www.nuget.org/packages/Microsoft.Data.SqlClient/">Microsoft.Data.SqlClient</a> nuget package. (Incidentally, <a href="https://devblogs.microsoft.com/dotnet/introducing-the-new-microsoftdatasqlclient/">Microsoft.Data.SqlClient is the successor to System.Data.SqlClient.</a>)</p><p>Support for connection string managed identities <a href="https://github.com/dotnet/SqlClient/blob/master/release-notes/2.1/2.1.0.md#Azure-Active-Directory-Managed-Identity-authentication">shipped with v2.1</a>. Connection strings can look slightly different depending on the type of managed identity you&#x27;re using:</p><pre><code>// For System Assigned Managed Identity
&quot;Server:{serverURL}; Authentication=Active Directory MSI; Initial Catalog={db};&quot;

// For System Assigned Managed Identity
&quot;Server:{serverURL}; Authentication=Active Directory Managed Identity; Initial Catalog={db};&quot;

// For User Assigned Managed Identity
&quot;Server:{serverURL}; Authentication=Active Directory MSI; User Id={ObjectIdOfManagedIdentity}; Initial Catalog={db};&quot;

// For User Assigned Managed Identity
&quot;Server:{serverURL}; Authentication=Active Directory Managed Identity; User Id={ObjectIdOfManagedIdentity}; Initial Catalog={db};&quot;
</code></pre><p>Regardless of the approach, you can see that none of the connection strings have credentials in them. And that&#x27;s special.</p><h2>Usage with Entity Framework Core 5</h2><p>If you&#x27;re using Entity Framework Core, you might be struggling to get this working and encountering strange error messages. In my ASP.NET project I had a dependendency on
<a href="https://www.nuget.org/packages/Microsoft.EntityFrameworkCore.SqlServer/5.0.4">Microsoft.EntityFrameworkCore.SqlServer@5</a>.</p><p><img src="../static/blog/2021-03-10-managed-identity-azure-sql-entity-framework/entity-framework-core-nuget.png" alt="Microsoft.EntityFrameworkCore.SqlServer@5 in NuGet"/></p><p>If you look close above, you&#x27;ll see that the package has a dependency on Microsoft.Data.SqlClient, but crucially on 2.0.1 or greater. So if <code>dotnet</code> has installed a version of Microsoft.Data.SqlClient which is <em>less</em> than 2.1 then the functionality required will not be installed. The resolution is simple, ensure that the required version is installed:</p><pre><code>dotnet add package Microsoft.Data.SqlClient --version 2.1.2
</code></pre><p>The version which we want to use is 2.1 (or greater) and fortunately that is compatible with Entity Framework Core 5. Incidentally, when Entity Framework Core 6 ships it will no longer be necessary to manually specify this dependency as it already requires <a href="mailto:Microsoft.Data.SqlClient@2.1">Microsoft.Data.SqlClient@2.1</a> as a minimum.</p><h2>User Assigned Managed Identity</h2><p>If you&#x27;re using user assigned managed identity, you&#x27;ll need to supply the object id of your managed identity, which you can find in the <a href="https://portal.azure.com/">Azure Portal</a>:</p><p><img src="../static/blog/2021-03-10-managed-identity-azure-sql-entity-framework/managed-identity-object-id.png" alt="Managed Identity object id"/></p><p>You can configure this in ARM as well, but cryptically, the object id goes by the nom de plume of <code>principalId</code> (thanks to my partner in crime <a href="https://github.com/jmccor99">John McCormick</a> for puzzling that out):</p><pre><code class="language-json">&quot;CONNECTIONSTRINGS__OURDBCONNECTION&quot;: &quot;[concat(&#x27;Server=tcp:&#x27;, parameters(&#x27;sqlServerName&#x27;) , &#x27;.database.windows.net,1433;Initial Catalog=&#x27;, parameters(&#x27;sqlDatabaseName&#x27;),&#x27;;Authentication=Active Directory MSI&#x27;,&#x27;;User Id=&#x27;, reference(resourceId(&#x27;Microsoft.ManagedIdentity/userAssignedIdentities/&#x27;, parameters(&#x27;managedIdentityName&#x27;)), &#x27;2018-11-30&#x27;).principalId)]&quot;
</code></pre><p>That&#x27;s it! With managed identity handling your authentication you can sleep easy, knowing you should be in a better place security wise.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[NSwag: TypeScript and CSharp client generation based on an API]]></title>
            <link>https://blog.johnnyreilly.com/2021/03/06/generate-typescript-and-csharp-clients-with-nswag</link>
            <guid>NSwag: TypeScript and CSharp client generation based on an API</guid>
            <pubDate>Sat, 06 Mar 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Generating clients for APIs is a tremendous way to reduce the amount of work you have to do when you're building a project. Why handwrite that code when it can be auto-generated for you quickly and accurately by a tool like NSwag? To quote the docs:]]></description>
            <content:encoded><![CDATA[<p>Generating clients for APIs is a tremendous way to reduce the amount of work you have to do when you&#x27;re building a project. Why handwrite that code when it can be auto-generated for you quickly and accurately by a tool like <a href="https://github.com/RicoSuter/NSwag">NSwag</a>? To quote the docs:</p><blockquote><p>The NSwag project provides tools to generate OpenAPI specifications from existing ASP.NET Web API controllers and client code from these OpenAPI specifications. The project combines the functionality of Swashbuckle (OpenAPI/Swagger generation) and AutoRest (client generation) in one toolchain.</p></blockquote><p>There&#x27;s some great posts out there that show you how to generate the clients with NSwag using an <code>nswag.json</code> file directly from a .NET project.</p><p>However, what if you want to use NSwag purely for its client generation capabilities? You may have an API written with another language / platform that exposes a Swagger endpoint, that you simply wish to create a client for. How do you do that? Also, if you want to do some special customisation of the clients you&#x27;re generating, you may find yourself struggling to configure that in <code>nswag.json</code>. In that case, it&#x27;s possible to hook into NSwag directly to do this with a simple .NET console app.</p><p>This post will:</p><ul><li>Create a .NET API which exposes a Swagger endpoint. (Alternatively, you could use any other Swagger endpoint; <a href="https://blog.logrocket.com/documenting-your-express-api-with-swagger/">for example an Express API</a>.)</li><li>Create a .NET console app which can create both TypeScript and CSharp clients from a Swagger endpoint.</li><li>Create a script which, when run, creates a TypeScript client.</li><li>Consume the API using the generated client in a simple TypeScript application.</li></ul><p>You will need both <a href="https://nodejs.org/en/">Node.js</a> and the <a href="https://dotnet.microsoft.com/download">.NET SDK</a> installed.</p><h2>Create an API</h2><p>We&#x27;ll now create an API which exposes a <a href="https://swagger.io/resources/open-api/">Swagger / Open API</a> endpoint. Whilst we&#x27;re doing that we&#x27;ll create a TypeScript React app which we&#x27;ll use later on. We&#x27;ll drop to the command line and enter the following commands which use the .NET SDK, node and the <code>create-react-app</code> package:</p><pre><code class="language-shell">mkdir src
cd src
npx create-react-app client-app --template typescript
mkdir server-app
cd server-app
dotnet new api -o API
cd API
dotnet add package NSwag.AspNetCore
</code></pre><p>We now have a .NET API with a dependency on NSwag. We&#x27;ll start to use it by replacing the <code>Startup.cs</code> that&#x27;s been generated with the following:</p><pre><code class="language-cs">using Microsoft.AspNetCore.Builder;
using Microsoft.AspNetCore.Hosting;
using Microsoft.Extensions.Configuration;
using Microsoft.Extensions.DependencyInjection;
using Microsoft.Extensions.Hosting;

namespace API
{
    public class Startup
    {
        const string ALLOW_DEVELOPMENT_CORS_ORIGINS_POLICY = &quot;AllowDevelopmentSpecificOrigins&quot;;
        const string LOCAL_DEVELOPMENT_URL = &quot;http://localhost:3000&quot;;

        public Startup(IConfiguration configuration)
        {
            Configuration = configuration;
        }

        public IConfiguration Configuration { get; }

        // This method gets called by the runtime. Use this method to add services to the container.
        public void ConfigureServices(IServiceCollection services)
        {

            services.AddControllers();

            services.AddCors(options =&gt; {
                options.AddPolicy(name: ALLOW_DEVELOPMENT_CORS_ORIGINS_POLICY,
                    builder =&gt; {
                        builder.WithOrigins(LOCAL_DEVELOPMENT_URL)
                            .AllowAnyMethod()
                            .AllowAnyHeader()
                            .AllowCredentials();
                    });
            });

            // Register the Swagger services
            services.AddSwaggerDocument();
        }

        // This method gets called by the runtime. Use this method to configure the HTTP request pipeline.
        public void Configure (IApplicationBuilder app, IWebHostEnvironment env)
        {
            if (env.IsDevelopment())
            {
                app.UseDeveloperExceptionPage();
            }
            else
            {
                app.UseExceptionHandler(&quot;/Error&quot;);
                app.UseHsts ();
                app.UseHttpsRedirection();
            }

            app.UseDefaultFiles();
            app.UseStaticFiles();

            app.UseRouting();

            app.UseAuthorization();

            // Register the Swagger generator and the Swagger UI middlewares
            app.UseOpenApi();
            app.UseSwaggerUi3();

            if (env.IsDevelopment())
                app.UseCors(ALLOW_DEVELOPMENT_CORS_ORIGINS_POLICY);

            app.UseEndpoints(endpoints =&gt;
            {
                endpoints.MapControllers();
            });
        }
    }
}
</code></pre><p>The significant changes in the above <code>Startup.cs</code> are:</p><ol><li>Exposing a Swagger endpoint with <code>UseOpenApi</code> and <code>UseSwaggerUi3</code>. NSwag will automagically create Swagger endpoints in your application for all your controllers. The .NET template ships with a <code>WeatherForecastController</code>.</li><li>Allowing <a href="https://docs.microsoft.com/en-us/aspnet/core/security/cors">Cross-Origin Requests (CORS)</a> which is useful during development (and will facilitate a demo later).</li></ol><p>Back in the root of our project we&#x27;re going to initialise an npm project. We&#x27;re going to use this to put in place a number of handy <a href="https://docs.npmjs.com/cli/v6/using-npm/scripts"><code>npm scripts</code></a> that will make our project easier to work with. So we&#x27;ll <code>npm init</code> and accept all the defaults.</p><p>Now we&#x27;re going add some dependencies which our scripts will use: <code>npm install cpx cross-env npm-run-all start-server-and-test</code></p><p>We&#x27;ll also add ourselves some <code>scripts</code> to our <code>package.json</code>:</p><pre><code class="language-json">&quot;scripts&quot;: {
    &quot;postinstall&quot;: &quot;npm run install:client-app &amp;&amp; npm run install:server-app&quot;,
    &quot;install:client-app&quot;: &quot;cd src/client-app &amp;&amp; npm install&quot;,
    &quot;install:server-app&quot;: &quot;cd src/server-app/API &amp;&amp; dotnet restore&quot;,
    &quot;build&quot;: &quot;npm run build:client-app &amp;&amp; npm run build:server-app&quot;,
    &quot;build:client-app&quot;: &quot;cd src/client-app &amp;&amp; npm run build&quot;,
    &quot;postbuild:client-app&quot;: &quot;cpx \&quot;src/client-app/build/**/*.*\&quot; \&quot;src/server-app/API/wwwroot/\&quot;&quot;,
    &quot;build:server-app&quot;: &quot;cd src/server-app/API &amp;&amp; dotnet build --configuration release&quot;,
    &quot;start&quot;: &quot;run-p start:client-app start:server-app&quot;,
    &quot;start:client-app&quot;: &quot;cd src/client-app &amp;&amp; npm start&quot;,
    &quot;start:server-app&quot;: &quot;cross-env ASPNETCORE_URLS=http://*:5000 ASPNETCORE_ENVIRONMENT=Development dotnet watch --project src/server-app/API run --no-launch-profile&quot;
  }
</code></pre><p>Let&#x27;s walk through what the above scripts provide us with:</p><ul><li>Running <code>npm install</code> in the root of our project will not only install dependencies for our root <code>package.json</code>, thanks to our <code>postinstall</code>, <code>install:client-app</code> and <code>install:server-app</code> scripts it will install the React app and .NET app dependencies as well.</li><li>Running <code>npm run build</code> will build our client and server apps.</li><li>Running <code>npm run start</code> will start both our React app and our .NET app. Our React app will be started at <a href="http://localhost:3000">http://localhost:3000</a>. Our .NET app will be started at <a href="http://localhost:5000">http://localhost:5000</a> (some environment variables are passed to it with <a href="https://github.com/kentcdodds/cross-env"><code>cross-env</code></a> ).</li></ul><p>Once <code>npm run start</code> has been run, you will find a Swagger endpoint at <a href="http://localhost:5000/swagger">http://localhost:5000/swagger</a>:</p><p><img src="../static/blog/2021-03-06-generate-typescript-and-csharp-clients-with-nswag/swagger.png" alt="swagger screenshot"/></p><h2>The client generator project</h2><p>Now we&#x27;ve scaffolded our Swagger-ed API, we want to put together the console app that will generate our typed clients.</p><pre><code class="language-shell">cd src/server-app
dotnet new console -o APIClientGenerator
cd APIClientGenerator
dotnet add package NSwag.CodeGeneration.CSharp
dotnet add package NSwag.CodeGeneration.TypeScript
dotnet add package NSwag.Core
</code></pre><p>We now have a console app with dependencies on the code generation portions of NSwag. Now let&#x27;s change up <code>Program.cs</code> to make use of this:</p><pre><code class="language-cs">using System;
using System.IO;
using System.Threading.Tasks;
using NJsonSchema;
using NJsonSchema.CodeGeneration.TypeScript;
using NJsonSchema.Visitors;
using NSwag;
using NSwag.CodeGeneration.CSharp;
using NSwag.CodeGeneration.TypeScript;

namespace APIClientGenerator
{
    class Program
    {
        static async Task Main(string[] args)
        {
            if (args.Length != 3)
                throw new ArgumentException(&quot;Expecting 3 arguments: URL, generatePath, language&quot;);

            var url = args[0];
            var generatePath = Path.Combine(Directory.GetCurrentDirectory(), args[1]);
            var language = args[2];

            if (language != &quot;TypeScript&quot; &amp;&amp; language != &quot;CSharp&quot;)
                throw new ArgumentException(&quot;Invalid language parameter; valid values are TypeScript and CSharp&quot;);

            if (language == &quot;TypeScript&quot;)
                await GenerateTypeScriptClient(url, generatePath);
            else
                await GenerateCSharpClient(url, generatePath);
        }

        async static Task GenerateTypeScriptClient(string url, string generatePath) =&gt;
            await GenerateClient(
                document: await OpenApiDocument.FromUrlAsync(url),
                generatePath: generatePath,
                generateCode: (OpenApiDocument document) =&gt;
                {
                    var settings = new TypeScriptClientGeneratorSettings();

                    settings.TypeScriptGeneratorSettings.TypeStyle = TypeScriptTypeStyle.Interface;
                    settings.TypeScriptGeneratorSettings.TypeScriptVersion = 3.5M;
                    settings.TypeScriptGeneratorSettings.DateTimeType = TypeScriptDateTimeType.String;

                    var generator = new TypeScriptClientGenerator(document, settings);
                    var code = generator.GenerateFile();

                    return code;
                }
            );

        async static Task GenerateCSharpClient(string url, string generatePath) =&gt;
            await GenerateClient(
                document: await OpenApiDocument.FromUrlAsync(url),
                generatePath: generatePath,
                generateCode: (OpenApiDocument document) =&gt;
                {
                    var settings = new CSharpClientGeneratorSettings
                    {
                        UseBaseUrl = false
                    };

                    var generator = new CSharpClientGenerator(document, settings);
                    var code = generator.GenerateFile();
                    return code;
                }
            );

        private async static Task GenerateClient(OpenApiDocument document, string generatePath, Func&lt;OpenApiDocument, string&gt; generateCode)
        {
            Console.WriteLine($&quot;Generating {generatePath}...&quot;);

            var code = generateCode(document);

            await System.IO.File.WriteAllTextAsync(generatePath, code);
        }
    }
}
</code></pre><p>We&#x27;ve created ourselves a simple .NET console application that creates TypeScript and CSharp clients for a given Swagger URL. It expects three arguments:</p><ul><li><code>url</code> <!-- -->-<!-- --> the url of the <code>swagger.json</code> file to generate a client for.</li><li><code>generatePath</code> <!-- -->-<!-- --> the path where the generated client file should be placed, relative to this project.</li><li><code>language</code> <!-- -->-<!-- --> the language of the client to generate; valid values are &quot;TypeScript&quot; and &quot;CSharp&quot;.</li></ul><p>To create a TypeScript client with it then we&#x27;d use the following command:</p><pre><code class="language-shell">dotnet run --project src/server-app/APIClientGenerator http://localhost:5000/swagger/v1/swagger.json src/client-app/src/clients.ts TypeScript
</code></pre><p>However, for this to run successfully, we&#x27;ll first have to ensure the API is running. It would be great if we had a single command we could run that would:</p><ul><li>bring up the API</li><li>generate a client</li><li>bring down the API</li></ul><p>Let&#x27;s make that.</p><h2>Building a &quot;make a client&quot; script</h2><p>In the root of the project we&#x27;re going to add the following <code>scripts</code>:</p><pre><code class="language-json">&quot;generate-client:server-app&quot;: &quot;start-server-and-test generate-client:server-app:serve http-get://localhost:5000/swagger/v1/swagger.json generate-client:server-app:generate&quot;,
    &quot;generate-client:server-app:serve&quot;: &quot;cross-env ASPNETCORE_URLS=http://*:5000 ASPNETCORE_ENVIRONMENT=Development dotnet run --project src/server-app/API --no-launch-profile&quot;,
    &quot;generate-client:server-app:generate&quot;: &quot;dotnet run --project src/server-app/APIClientGenerator http://localhost:5000/swagger/v1/swagger.json src/client-app/src/clients.ts TypeScript&quot;,
</code></pre><p>Let&#x27;s walk through what&#x27;s happening here. Running <code>npm run generate-client:server-app</code> will:</p><ul><li>Use the <a href="https://github.com/bahmutov/start-server-and-test"><code>start-server-and-test</code></a> package to spin up our server-app by running the <code>generate-client:server-app:serve</code> script.</li><li><code>start-server-and-test</code> waits for the Swagger endpoint to start responding to requests. When it does start responding, <code>start-server-and-test</code> runs the <code>generate-client:server-app:generate</code> script which runs our APIClientGenerator console app and provides it with the URL where our swagger can be found, the path of the file to generate and the language of &quot;TypeScript&quot;</li></ul><p>If you were wanting to generate a C# client (say if you were writing a <a href="https://blog.logrocket.com/js-free-frontends-blazor/">Blazor</a> app) then you could change the <code>generate-client:server-app:generate</code> script as follows:</p><pre><code class="language-json">&quot;generate-client:server-app:generate&quot;: &quot;dotnet run --project src/server-app/ApiClientGenerator http://localhost:5000/swagger/v1/swagger.json clients.cs CSharp&quot;,
</code></pre><h2>Consume our generated API client</h2><p>Let&#x27;s run the <code>npm run generate-client:server-app</code> command. It creates a <code>clients.ts</code> file which nestles nicely inside our <code>client-app</code>. We&#x27;re going to exercise that in a moment. First of all, let&#x27;s enable proxying from our <code>client-app</code> to our <code>server-app</code> following the instructions in the <a href="https://create-react-app.dev/docs/proxying-api-requests-in-development/">Create React App docs</a> and adding the following to our <code>client-app/package.json</code>:</p><pre><code class="language-json">&quot;proxy&quot;: &quot;http://localhost:5000&quot;
</code></pre><p>Now let&#x27;s start our apps with <code>npm run start</code>. We&#x27;ll then replace the contents of <code>App.tsx</code> with:</p><pre><code class="language-jsx">import React from &quot;react&quot;;
import &quot;./App.css&quot;;
import { WeatherForecast, WeatherForecastClient } from &quot;./clients&quot;;

function App() {
  const [weather, setWeather] = React.useState&lt;WeatherForecast[] | null&gt;();
  React.useEffect(() =&gt; {
    async function loadWeather() {
      const weatherClient = new WeatherForecastClient(/* baseUrl */ &quot;&quot;);
      const forecast = await weatherClient.get();
      setWeather(forecast);
    }
    loadWeather();
  }, [setWeather]);

  return (
    &lt;div className=&quot;App&quot;&gt;
      &lt;header className=&quot;App-header&quot;&gt;
        {weather ? (
          &lt;table&gt;
            &lt;thead&gt;
              &lt;tr&gt;
                &lt;th&gt;Date&lt;/th&gt;
                &lt;th&gt;Summary&lt;/th&gt;
                &lt;th&gt;Centigrade&lt;/th&gt;
                &lt;th&gt;Fahrenheit&lt;/th&gt;
              &lt;/tr&gt;
            &lt;/thead&gt;
            &lt;tbody&gt;
              {weather.map(({ date, summary, temperatureC, temperatureF }) =&gt; (
                &lt;tr key={date}&gt;
                  &lt;td&gt;{new Date(date).toLocaleDateString()}&lt;/td&gt;
                  &lt;td&gt;{summary}&lt;/td&gt;
                  &lt;td&gt;{temperatureC}&lt;/td&gt;
                  &lt;td&gt;{temperatureF}&lt;/td&gt;
                &lt;/tr&gt;
              ))}
            &lt;/tbody&gt;
          &lt;/table&gt;
        ) : (
          &lt;p&gt;Loading weather...&lt;/p&gt;
        )}
      &lt;/header&gt;
    &lt;/div&gt;
  );
}

export default App;
</code></pre><p>Inside the <code>React.useEffect</code> above you can see we create a new instance of the auto-generated <code>WeatherForecastClient</code>. We then call <code>weatherClient.get()</code> which sends the <code>GET</code> request to the server to acquire the data and provides it in a strongly typed fashion (<code>get()</code> returns an array of <code>WeatherForecast</code>). This is then displayed on the page like so:</p><p><img src="../static/blog/2021-03-06-generate-typescript-and-csharp-clients-with-nswag/use-generated-client.gif" alt="load data from server"/></p><p>As you an see we&#x27;re loading data from the server using our auto-generated client. We&#x27;re reducing the amount of code we have to write <em>and</em> we&#x27;re reducing the likelihood of errors.</p><p><em>This post was originally posted on <a href="https://blog.logrocket.com/generate-typescript-csharp-clients-nswag-api/">LogRocket</a>.</em></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Goodbye Client Affinity, Hello Data Protection with Azure]]></title>
            <link>https://blog.johnnyreilly.com/2021/02/27/goodbye-client-affinity-hello-data-protection-with-azure</link>
            <guid>Goodbye Client Affinity, Hello Data Protection with Azure</guid>
            <pubDate>Sat, 27 Feb 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[How to use ASP.NET Data Protection to remove the need for sticky sessions with Client Affinity]]></description>
            <content:encoded><![CDATA[<p>I&#x27;ve written lately about <a href="./2021-02-11-azure-app-service-health-checks-and-zero-downtime-deployments.md">zero downtime releases with Azure App Service</a>. Zero downtime releases are only successful if your authentication mechanism survives a new deployment. We looked in my last post at <a href="./2021-02-16-easy-auth-tokens-survive-releases-on-linux-azure-app-service.md">how to achieve this with Azure&#x27;s in-built authentication mechanism; Easy Auth</a>.</p><p>We&#x27;re now going to look at how the same goal can be achieved if your ASP.NET application is authenticating another way. We achieve this through use of the <a href="https://docs.microsoft.com/en-us/aspnet/core/security/data-protection/configuration/overview">ASP.NET Data Protection</a> system. Andrew Lock has written <a href="https://andrewlock.net/an-introduction-to-the-data-protection-system-in-asp-net-core/">an excellent walkthrough on the topic</a> and I encourage you to read it.</p><p>We&#x27;re interested in the ASP.NET data-protection system because it encrypts and decrypts sensitive data including the authentication cookie. It&#x27;s wonderful that the data protection does this, but at the same time it presents a problem. We would like to route traffic to <em>multiple</em> instances of our application… So traffic could go to instance 1, instance 2 of our app etc.</p><p><img src="../static/blog/2021-02-27-goodbye-client-affinity-hello-data-protection-with-azure/traffic-to-app-service.png" alt="traffic to app service"/></p><p>How can we ensure the different instances of our app can read the authentication cookies regardless of the instance that produced them? How can we ensure that instance 1 can read cookies produced by instance 2 and vice versa? And for that matter, we&#x27;d like all instances to be able to read cookies whether they were produced by an instance in a production or staging slot.</p><p>We&#x27;re aiming to avoid the use of &quot;sticky sessions&quot; and ARRAffinity cookies. These ensure that traffic is continually routed to the same instance. Routing to the same instance explicitly prevents us from stopping routing traffic to an old instance and starting routing to a new one.</p><p>With the data protection activated and multiple instances of your app service you immediately face the issue that different instances of the app will be unable to read cookies they did not create. This is the default behaviour of data protection. <a href="https://docs.microsoft.com/en-us/aspnet/core/host-and-deploy/web-farm?view=aspnetcore-5.0#data-protection">To quote the docs:</a></p><blockquote><p>Data Protection relies upon a set of cryptographic keys stored in a key ring. When the Data Protection system is initialized, it applies default settings that store the key ring locally. Under the default configuration, a unique key ring is stored on each node of the web farm. Consequently, each web farm node can&#x27;t decrypt data that&#x27;s encrypted by an app on any other node.</p></blockquote><p>The problem here is the data protection keys (the key ring) is being stored locally on each instance. What are the implications of this? Well, For example, instance 2 doesn&#x27;t have access to the keys instance 1 is using and so can&#x27;t decrypt instance 1 cookies.</p><h2>Sharing is caring</h2><p>What we need to do is move away from storing keys locally, and to storing it in a <em>shared</em> place instead. We&#x27;re going to store data protection keys in Azure Blob Storage and protect the keys with Azure Key Vault:</p><p><img src="../static/blog/2021-02-27-goodbye-client-affinity-hello-data-protection-with-azure/data-protection-zero-downtime.png" alt="persist keys to azure blob"/></p><p>All instances of the application can access the key ring and consequently sharing cookies is enabled. <a href="https://docs.microsoft.com/en-us/aspnet/core/security/data-protection/configuration/overview?view=aspnetcore-5.0#protectkeyswithazurekeyvault">As the documentation attests</a>, enabling this is fairly simple. It amounts to adding the following packages to your ASP.NET app:</p><ul><li><a href="https://www.nuget.org/packages/Azure.Extensions.AspNetCore.DataProtection.Blobs"><code>Azure.Extensions.AspNetCore.DataProtection.Blobs</code></a></li><li><a href="https://www.nuget.org/packages/Azure.Extensions.AspNetCore.DataProtection.Keys"><code>Azure.Extensions.AspNetCore.DataProtection.Keys</code></a></li></ul><p>And adding the following to the <code>ConfigureServices</code> in your ASP.NET app:</p><pre><code class="language-cs">services.AddDataProtection().SetApplicationName(&quot;OurWebApp&quot;)
        // azure credentials require storage blob contributor role permissions
        // eg https://my-storage-account.blob.core.windows.net/keys/key
        .PersistKeysToAzureBlobStorage(new Uri($&quot;https://{Configuration[&quot;StorageAccountName&quot;]}.blob.core.windows.net/keys/key&quot;), new DefaultAzureCredential())

        // azure credentials require key vault crypto role permissions
        // eg https://my-key-vault.vault.azure.net/keys/dataprotection
        .ProtectKeysWithAzureKeyVault(new Uri($&quot;https://{Configuration[&quot;KeyVaultName&quot;]}.vault.azure.net/keys/dataprotection&quot;), new DefaultAzureCredential());
</code></pre><p>In the above example you can see we&#x27;re passing the name of our Storage account and Key Vault via configuration.</p><p>There&#x27;s one more crucial piece of the puzzle here; and it&#x27;s role assignments, better known as permissions. Your App Service needs to be able to read and write to Azure Key Vault and the Azure Blob Storage. The permissions of <a href="https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#storage-blob-data-contributor">Storage Blob Data Contributor</a> and <a href="https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#key-vault-crypto-officer-preview">Key Vault Crypto Officer</a> are sufficient to enable this. (If you&#x27;d like to see what configuring that looks like via ARM templates then <a href="./2021-02-08-arm-templates-security-role-assignments.md">check out this post</a>.)</p><p>With this in place we&#x27;re able to route traffic to any instance of our application, secure in the knowledge that it will be able to read the cookies. Furthermore, we&#x27;ve enabled zero downtime releases as a direct consequence.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Making Easy Auth tokens survive releases on Linux Azure App Service]]></title>
            <link>https://blog.johnnyreilly.com/2021/02/16/easy-auth-tokens-survive-releases-on-linux-azure-app-service</link>
            <guid>Making Easy Auth tokens survive releases on Linux Azure App Service</guid>
            <pubDate>Tue, 16 Feb 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[I wrote recently about zero downtime deployments on Azure App Service. Many applications require authentication, and ours is no exception. In our case we're using Azure Active Directory facilitated by "Easy Auth" which provides authentication to our App Service.]]></description>
            <content:encoded><![CDATA[<p>I <a href="./2021-02-11-azure-app-service-health-checks-and-zero-downtime-deployments.md">wrote recently about zero downtime deployments on Azure App Service</a>. Many applications require authentication, and ours is no exception. In our case we&#x27;re using Azure Active Directory facilitated by <a href="https://docs.microsoft.com/en-us/azure/app-service/overview-authentication-authorization">&quot;Easy Auth&quot;</a> which provides authentication to our App Service.</p><p>Our app uses a Linux App Service. It&#x27;s worth knowing that Linux App Services run as a Docker container. As a consequence, Easy Auth works in a slightly different way; effectively as a middleware. <a href="https://docs.microsoft.com/en-us/azure/app-service/overview-authentication-authorization#on-containers">To quote the docs on Easy Auth</a>:</p><blockquote><p>This module handles several things for your app:</p><ul><li>Authenticates users with the specified provider</li><li>Validates, stores, and refreshes tokens</li><li>Manages the authenticated session</li><li>Injects identity information into request headers The module runs separately from your application code and is configured using app settings. No SDKs, specific languages, or changes to your application code are required.</li></ul><p>The authentication and authorization module runs in a separate container, isolated from your application code. Using what&#x27;s known as the <a href="https://docs.microsoft.com/en-us/azure/architecture/patterns/ambassador">Ambassador</a> pattern, it interacts with the incoming traffic to perform similar functionality as on Windows.</p></blockquote><p>However, <a href="https://social.msdn.microsoft.com/Forums/en-US/dde551b2-c86d-474b-b0a6-cc66163785a1/restarting-azure-app-service-on-linux-with-azure-active-directory-authentication-resets-authme#b59951ab-623a-4442-a221-80c157387bbe">Microsoft have acknowledged there is a potential bug in Easy Auth support at present</a>. When the app service is restarted, the stored tokens are removed, and <strong>authentication begins to fail</strong>. As you might well imagine, authentication similarly starts to fail when a new app service is introduced - as is the case during deployment.</p><p>This is really significant. You may well have &quot;zero downtime deployment&quot;, but it doesn&#x27;t amount to a hill of beans if the moment you&#x27;ve deployed your users find they&#x27;re effectively logged out. <a href="https://social.msdn.microsoft.com/Forums/en-US/dde551b2-c86d-474b-b0a6-cc66163785a1/restarting-azure-app-service-on-linux-with-azure-active-directory-authentication-resets-authme#b59951ab-623a-4442-a221-80c157387bbe">The advice from Microsoft</a> is to use <a href="https://docs.microsoft.com/en-gb/archive/blogs/jpsanders/azure-app-service-authentication-using-a-blob-storage-for-token-cache">Blob Storage for Token Cache</a>:</p><p><a href="https://twitter.com/cgillum">Chris Gillum</a> said in a <a href="https://cgillum.tech/2016/03/07/app-service-token-store/">blog on the topic</a>:</p><blockquote><p>you can provision an Azure Blob Storage container and configure your web app with a SaS URL (with read/write/list access) pointing to that blob container. This SaS URL can then be saved to the <code>WEBSITE_AUTH_TOKEN_CONTAINER_SASURL</code> app setting. When this app setting is present, all tokens will be stored in and fetched from the specified blob container.</p></blockquote><p>To turn that into something visual, what&#x27;s suggested is this:</p><p><img src="../static/blog/2021-02-16-easy-auth-tokens-survive-releases-on-linux-azure-app-service/easy-auth-zero-downtime-deployment.png" alt="diagram of Easy Auth with blog storage"/></p><h2>SaS-sy ARM Templates</h2><p>I have the good fortune to work with some very talented people. One of them, <a href="https://github.com/jmccor99">John McCormick</a> turned his hand to putting this proposed solution into <code>azure-pipelines.yml</code> and ARM template-land. First of all, let&#x27;s look at our <code>azure-pipelines.yml</code>. We add the following, prior to our deployment job:</p><pre><code class="language-yml">- job: SASGen
        displayName: Generate SAS Token

        steps:
          - task: AzurePowerShell@4
            name: ObtainSasTokenTask
            inputs:
              azureSubscription: $(serviceConnection)
              ScriptType: inlineScript
              Inline: |
                $startTime = Get-Date
                $expiryTime = $startTime.AddDays(90)
                $storageAcc = Get-AzStorageAccount -ResourceGroupName $(azureResourceGroup) -Name $(storageAccountName)
                $ctx = $storageAcc.Context
                $sas = New-AzStorageContainerSASToken -Context $ctx -Name &quot;tokens&quot; -Permission &quot;rwl&quot; -Protocol HttpsOnly -StartTime $startTime -ExpiryTime $expiryTime -FullUri
                Write-Host &quot;##vso[task.setvariable variable=sasToken;issecret=true;isOutput=true]$sas&quot;
              azurePowerShellVersion: &#x27;LatestVersion&#x27;

      - job: DeployAppARMTemplates
        variables:
          sasToken: $[dependencies.SASGen.outputs[&#x27;ObtainSasTokenTask.sasToken&#x27;] ]
        displayName: Deploy App ARM Templates
        dependsOn:
        - SASGen

        steps:
          - task: AzureResourceManagerTemplateDeployment@3
            displayName: Deploy app-service ARM Template
            inputs:
              deploymentScope: Resource Group
              azureResourceManagerConnection: $(serviceConnection)
              subscriptionId: $(subscriptionId)
              action: Create Or Update Resource Group
              resourceGroupName: $(azureResourceGroup)
              location: $(location)
              templateLocation: Linked artifact
              csmFile: &#x27;infra/app-service/azuredeploy.json&#x27;
              csmParametersFile: &#x27;infra/azuredeploy.parameters.json&#x27;
              overrideParameters: &gt;-
                -sasUrl $(sasToken)
              deploymentMode: Incremental
</code></pre><p>There&#x27;s two notable things happening above:</p><ol><li>In the <code>SASGen</code> job, a PowerShell script runs that <a href="https://docs.microsoft.com/en-us/powershell/module/az.storage/new-azstoragecontainersastoken?view=azps-5.5.0">generates a SaS token URL</a> with read, write and list permissions that will last for 90 days. (Incidentally, there is a way to do this via <a href="https://stackoverflow.com/a/56127006/761388">ARM templates, and without PowerShell</a> <!-- -->-<!-- --> but alas it didn&#x27;t seem to work when we experimented with it.)</li><li>The generated (secret) token URL (<code>sasUrl</code>) is passed as a parameter to our App Service ARM template. The ARM template sets an appsetting for the app service:</li></ol><pre><code class="language-json">{
    &quot;apiVersion&quot;: &quot;2020-09-01&quot;,
    &quot;name&quot;: &quot;appsettings&quot;,
    &quot;type&quot;: &quot;config&quot;,
    &quot;properties&quot;: {
        &quot;WEBSITE_AUTH_TOKEN_CONTAINER_SASURL&quot;: &quot;[parameters(&#x27;sasUrl&#x27;)]&quot;
    }
},
</code></pre><p>If you google <code>WEBSITE_AUTH_TOKEN_CONTAINER_SASURL</code> you will not find a geat deal. Documentation is short. What you will find is <a href="http://jsandersblog.azurewebsites.net/2017/08/10/azure-app-service-authentication-using-a-blob-storage-for-token-cache/">Jeff Sanders excellent blog on the topic</a>. It is, in terms of content, it has some commonality with this post; except in Jeff&#x27;s example he&#x27;s manually implementing the workaround in the Azure Portal.</p><h2>What&#x27;s actually happening?</h2><p>With this in place, every time someone logs into your app a JSON token is written to the storage like so:</p><p><img src="../static/blog/2021-02-16-easy-auth-tokens-survive-releases-on-linux-azure-app-service/token.png" alt="token in storage account"/></p><p>If you take the trouble to look inside you&#x27;ll find something like this tucked away:</p><pre><code class="language-json">{
  &quot;encrypted&quot;: true,
  &quot;tokens&quot;: {
    &quot;aad&quot;: &quot;herewith_a_very_very_long_encrypted_token&quot;
  },
  &quot;version&quot;: 1
}
</code></pre><p>With this in place, you can safely restart your app service and / or deploy a new one, safe in the knowledge that the tokens will live on in the storage account, and that consequently you will not be unauthenticating users.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Azure App Service, Health checks and zero downtime deployments]]></title>
            <link>https://blog.johnnyreilly.com/2021/02/11/azure-app-service-health-checks-and-zero-downtime-deployments</link>
            <guid>Azure App Service, Health checks and zero downtime deployments</guid>
            <pubDate>Thu, 11 Feb 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[I've been working recently on zero downtime deployments using Azure App Service. They're facilitated by a combination of Health checks and deployment slots. This post will talk about why this is important and how it works.]]></description>
            <content:encoded><![CDATA[<p>I&#x27;ve been working recently on zero downtime deployments using Azure App Service. They&#x27;re facilitated by a combination of <a href="https://docs.microsoft.com/en-us/azure/app-service/monitor-instances-health-check">Health checks</a> and <a href="https://docs.microsoft.com/en-us/azure/app-service/deploy-staging-slots">deployment slots</a>. This post will talk about why this is important and how it works.</p><h2>Why zero downtime deployments?</h2><p>Historically (and for many applications, currently) deployment results in downtime. A period of time during the release where an application is not available to users whilst the new version is deployed. There are a number of downsides to releases with downtime:</p><ol><li>Your users cannot use your application. This will frustrate them and make them sad.</li><li>Because you&#x27;re a kind person and you want your users to be happy, you&#x27;ll optimise to make their lives better. You&#x27;ll release when the fewest users are accessing your application. It will likely mean you&#x27;ll end up working late, early or at weekends.</li><li>Again because you want to reduce impact on users, you&#x27;ll release less often. This means that every release will bring with it a greater collection of changes. This is turn will often result in a large degree of focus on manually testing each release, to reduce the likelihood of bugs ending up in users hands. This is a noble aim, but it drags the teams focus away from shipping.</li></ol><p>Put simply: downtime in releases impacts customer happiness and leads to reduced pace for teams. It&#x27;s a vicious circle.</p><p>But if we turn it around, what does it look like if releases have <em>no</em> downtime at all?</p><ol><li>Your users can always use your application. This will please them.</li><li>Your team is now safe to release at any time, day or night. They will likely release more often as a consequence.</li><li>If your team has sufficient automated testing in place, they&#x27;re now in a position where they can move to <a href="https://www.atlassian.com/continuous-delivery/principles/continuous-integration-vs-delivery-vs-deployment">Continuous Deployment</a>.</li><li>Releases become boring. This is good. They &quot;just work™️&quot; and so the team can focus instead on building the cool features that are going to make users lives even better.</li></ol><h2>Manual zero downtime releases with App Services</h2><p>App Services have the ability to scale out. To <a href="https://azure.microsoft.com/en-us/blog/scaling-up-and-scaling-out-in-windows-azure-web-sites/">quote the docs</a>:</p><blockquote><p>A scale out operation is the equivalent of creating multiple copies of your web site and adding a load balancer to distribute the demand between them. When you scale out ... there is no need to configure load balancing separately since this is already provided by the platform.</p></blockquote><p>As you can see, scaling out works by having multiple instances of your app. Deployment slots are exactly this, but with an extra twist. If you add a deployment slot to your App Service, then you <strong>no longer deploy to production</strong>. Instead you deploy to your staging slot. Your staging slot is accessible in the same way your production slot is accessible. So whilst your users may go to <a href="https://my-glorious-app.io">https://my-glorious-app.io</a>, your staging slot may live at <a href="https://my-glorious-app-stage.azurewebsites.net">https://my-glorious-app-stage.azurewebsites.net</a> instead. Because this is accessible, this is testable. You are in a position to test the deployed application before making it generally available.</p><p><img src="../static/blog/2021-02-11-azure-app-service-health-checks-and-zero-downtime-deployments/app-service-with-slots.png" alt="diagram of network traffic going to various App Service Deployment Slots"/></p><p>Once you&#x27;re happy that everything looks good, you can &quot;swap slots&quot;. What this means, is the version of the app living in the staging slot, gets moved into the production slot. So that which lived at <a href="https://my-glorious-app-stage.azurewebsites.net">https://my-glorious-app-stage.azurewebsites.net</a> moves to <a href="https://my-glorious-app.io">https://my-glorious-app.io</a>. For a more details on what that involves <a href="https://docs.microsoft.com/en-us/azure/app-service/deploy-staging-slots#what-happens-during-a-swap">read this</a>. The significant take home is this: there is no downtime. Traffic stops being routed to the old instance and starts being routed to the new one. It&#x27;s as simple as that.</p><p>I should mention at this point that there&#x27;s a <a href="https://opensource.com/article/17/5/colorful-deployments">number of zero downtime strategies out there</a> and slots can help support a number of these. This includes canary deployments, where a subset of traffic is routed to the new version prior to it being opened out more widely. In our case, we&#x27;re looking at rolling deployments, where we replace the currently running instances of our application with the new ones; but it&#x27;s worth being aware that there are other strategies that slots can facilitate.</p><p>So what does it look like when slots swap? Well, to test that out, we swapped slots on our two App Service instances. We repeatedly CURLed our apps <a href="./2021-01-29-surfacing-azure-pipelines-build-info-in-an-aspnet-react-app.md"><code>api/build</code></a> endpoint that exposes the build information; to get visibility around which version of our app we were routing traffic to. This is what we saw:</p><pre><code>Thu Jan 21 11:51:51 GMT 2021
{&quot;buildNumber&quot;:&quot;20210121.5&quot;,&quot;buildId&quot;:&quot;17992&quot;,&quot;commitHash&quot;:&quot;c2122919df54bfa6a0d20bceb9f06890f822b26e&quot;}
Thu Jan 21 11:51:54 GMT 2021
{&quot;buildNumber&quot;:&quot;20210121.6&quot;,&quot;buildId&quot;:&quot;18015&quot;,&quot;commitHash&quot;:&quot;062ac1488fcf1737fe1dbab0d05c095786218f30&quot;}
Thu Jan 21 11:51:57 GMT 2021
{&quot;buildNumber&quot;:&quot;20210121.5&quot;,&quot;buildId&quot;:&quot;17992&quot;,&quot;commitHash&quot;:&quot;c2122919df54bfa6a0d20bceb9f06890f822b26e&quot;}
Thu Jan 21 11:52:00 GMT 2021
{&quot;buildNumber&quot;:&quot;20210121.6&quot;,&quot;buildId&quot;:&quot;18015&quot;,&quot;commitHash&quot;:&quot;062ac1488fcf1737fe1dbab0d05c095786218f30&quot;}
Thu Jan 21 11:52:03 GMT 2021
{&quot;buildNumber&quot;:&quot;20210121.6&quot;,&quot;buildId&quot;:&quot;18015&quot;,&quot;commitHash&quot;:&quot;062ac1488fcf1737fe1dbab0d05c095786218f30&quot;}
Thu Jan 21 11:52:05 GMT 2021
{&quot;buildNumber&quot;:&quot;20210121.6&quot;,&quot;buildId&quot;:&quot;18015&quot;,&quot;commitHash&quot;:&quot;062ac1488fcf1737fe1dbab0d05c095786218f30&quot;}
Thu Jan 21 11:52:08 GMT 2021
{&quot;buildNumber&quot;:&quot;20210121.5&quot;,&quot;buildId&quot;:&quot;17992&quot;,&quot;commitHash&quot;:&quot;c2122919df54bfa6a0d20bceb9f06890f822b26e&quot;}
Thu Jan 21 11:52:10 GMT 2021
{&quot;buildNumber&quot;:&quot;20210121.6&quot;,&quot;buildId&quot;:&quot;18015&quot;,&quot;commitHash&quot;:&quot;062ac1488fcf1737fe1dbab0d05c095786218f30&quot;}
Thu Jan 21 11:52:12 GMT 2021
{&quot;buildNumber&quot;:&quot;20210121.5&quot;,&quot;buildId&quot;:&quot;17992&quot;,&quot;commitHash&quot;:&quot;c2122919df54bfa6a0d20bceb9f06890f822b26e&quot;}
Thu Jan 21 11:52:15 GMT 2021
{&quot;buildNumber&quot;:&quot;20210121.6&quot;,&quot;buildId&quot;:&quot;18015&quot;,&quot;commitHash&quot;:&quot;062ac1488fcf1737fe1dbab0d05c095786218f30&quot;}
Thu Jan 21 11:52:17 GMT 2021
{&quot;buildNumber&quot;:&quot;20210121.6&quot;,&quot;buildId&quot;:&quot;18015&quot;,&quot;commitHash&quot;:&quot;062ac1488fcf1737fe1dbab0d05c095786218f30&quot;}
</code></pre><p>The first new version of our application showed up in a production slot at 11:51:54, and the last old version showed up at 11:52:12. So it took a total of 15 seconds to complete the transition from hitting only instances of the old application to hitting only instances of the new application. During that 15 seconds either old or new versions of the application would be serving traffic. Significantly, there was always a version of the application returning responses.</p><p>This is <em>very</em> exciting! We have zero downtime deployments!</p><h2>Rollbacks for bonus points</h2><p>We now have the new version of the app (<code>buildNumber: 20210121.6</code>) in the production slot, and the old version of the app (<code>buildNumber: 20210121.5</code>) in the staging slot.</p><p>Slots have a tremendous rollback story. If it emerges that there was some uncaught issue in your release and you&#x27;d like to revert to the previous version, you can! Just as we swapped just now to move <code>buildNumber: 20210121.6</code> from the staging slot to the production slot and <code>buildNumber: 20210121.5</code> the other way, we can swap right back and revert our release like so:</p><p><img src="../static/blog/2021-02-11-azure-app-service-health-checks-and-zero-downtime-deployments/app-service-with-slots-and-build-number.png" alt="diagram of network traffic going to various App Service Deployment Slots exposing build number"/></p><p>Once again users going to <a href="https://my-glorious-app.io">https://my-glorious-app.io</a> are hitting <code>buildNumber: 20210121.5</code>.</p><p>This is also <em>very</em> exciting! We have zero downtime deployments <em>and</em> rollbacks!</p><h2>Automated zero downtime releases with Health checks</h2><p>The final piece of the puzzle here automation. You&#x27;re a sophisticated team, you&#x27;ve put a great deal of energy into automating your tests. You don&#x27;t want your release process to be manual for this very reason; you trust your test coverage. You want to move to Continuous Deployment.</p><p>Fortunately, automating swapping slots is a breeze with <code>azure-pipelines.yml</code>. Consider the following:</p><pre><code class="language-yml">- job: DeployApp
        displayName: Deploy app
        dependsOn:
        - DeployARMTemplates

        steps:
        - download: current
          artifact: webapp

        - task: AzureWebApp@1
          displayName: &#x27;Deploy Web Application&#x27;
          inputs:
            azureSubscription: $(serviceConnection)
            resourceGroupName: $(azureResourceGroup)
            appName: $(appServiceName)
            package: $(Pipeline.Workspace)/webapp/**/*.zip
            slotName: stage
            deployToSlotOrASE: true
            deploymentMethod: auto

      - job: SwapSlots
        displayName: Swap Slots
        dependsOn:
        - DeployApp

        steps:
          - task: AzureAppServiceManage@0
            displayName: Swap Slots
            inputs:
              action: &#x27;Swap Slots&#x27;
              azureSubscription: $(serviceConnection)
              resourceGroupName: $(azureResourceGroup)
              webAppName: $(appServiceName)
              SourceSlot: &#x27;stage&#x27;
</code></pre><p>The first job here, deploys our previously built <code>webapp</code> to the <code>stage</code> slot. The second job swaps the slot.</p><p>When I first considered this, the question rattling around in the back of my mind was this: how does App Service know when it&#x27;s safe to swap? What if we swap before our app has fully woken up and started serving responses?</p><p>It so happens that using <a href="https://docs.microsoft.com/en-us/azure/app-service/monitor-instances-health-check">Health checks, App Service caters for this beautifully</a>. A health check endpoint is a URL in your application which, when hit, checks the dependencies of your application. &quot;Is the database accessible?&quot; &quot;Are the APIs I depend upon accessible?&quot; The diagram from the docs expresses it very well:</p><p><img src="../static/blog/2021-02-11-azure-app-service-health-checks-and-zero-downtime-deployments/health-check-failure-diagram.png" alt="diagram of traffic hitting the health check endpoint"/></p><p>This approach is very similar to <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">liveness, readiness and startup probes in Kubernetes</a>. To make use of Health checks, in our ARM template for our App Service we have configured a <code>healthCheckPath</code>:</p><pre><code class="language-json">&quot;siteConfig&quot;: {
    &quot;linuxFxVersion&quot;: &quot;[parameters(&#x27;linuxFxVersion&#x27;)]&quot;,
    &quot;alwaysOn&quot;: true,
    &quot;http20Enabled&quot;: true,
    &quot;minTlsVersion&quot;: &quot;1.2&quot;,
    &quot;healthCheckPath&quot;: &quot;/api/health&quot;,
    //...
}
</code></pre><p>This tells App Service where to look to check the health. The health check endpoint itself is provided by the <code>MapHealthChecks</code> in our <code>Startup.cs</code> of our .NET application:</p><pre><code class="language-cs">app.UseEndpoints(endpoints =&gt; {
    endpoints.MapControllerRoute(
        name: &quot;default&quot;,
        pattern: &quot;{controller}/{action=Index}/{id?}&quot;);

    endpoints.MapHealthChecks(&quot;/api/health&quot;);
});
</code></pre><p>You read a full list of all the ways App Service uses Health checks <a href="https://docs.microsoft.com/en-us/azure/app-service/monitor-instances-health-check#what-app-service-does-with-health-checks">here</a>. Pertinent for zero downtime deployments is this:</p><blockquote><p>when scaling up or out, App Service pings the Health check path to ensure new instances are ready.</p></blockquote><p>This is the magic sauce. App Service doesn&#x27;t route traffic to an instance until it&#x27;s given the thumbs up that it&#x27;s ready in the form of passing health checks. This is excellent; it is this that makes automated zero downtime releases a reality.</p><p>Props to the various Azure teams that have made this possible; I&#x27;m very impressed by the way in which the Health checks and slots can be combined together to support some tremendous use cases.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Azure RBAC: role assignments and ARM templates]]></title>
            <link>https://blog.johnnyreilly.com/2021/02/08/arm-templates-security-role-assignments</link>
            <guid>Azure RBAC: role assignments and ARM templates</guid>
            <pubDate>Mon, 08 Feb 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[This post is about Azure's role assignments and ARM templates. Role assignments can be thought of as "permissions for Azure".]]></description>
            <content:encoded><![CDATA[<p>This post is about Azure&#x27;s role assignments and ARM templates. Role assignments can be thought of as &quot;permissions for Azure&quot;.</p><p>If you&#x27;re deploying to Azure, there&#x27;s a good chance you&#x27;re using <a href="https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/overview">ARM templates</a> to do so. Once you&#x27;ve got past &quot;Hello World&quot;, you&#x27;ll probably find yourself in a situation when you&#x27;re deploying multiple types of resource to make your solution. For instance, you may be deploying an <a href="https://docs.microsoft.com/en-us/azure/app-service/quickstart-arm-template?pivots=platform-linux#review-the-template">App Service</a> alongside <a href="https://docs.microsoft.com/en-us/azure/templates/microsoft.keyvault/vaults">Key Vault</a> and <a href="https://docs.microsoft.com/en-us/azure/templates/microsoft.storage/storageaccounts">Storage</a>.</p><p>One of the hardest things when it comes to deploying software and having it work, is permissions. Without adequate permissions configured, the most beautiful code can do <em>nothing</em>. Incidentally, this is a good thing. We&#x27;re deploying to the web; many people are there, not all good. As a different kind of web-head once said:</p><p><img src="../static/blog/2021-02-08-arm-templates-security-role-assignments/with-great-power-comes-great-responsibility.jpg" alt="Spider-man saying with great power, comes great responsibility"/></p><p>Azure has great power and <a href="https://docs.microsoft.com/en-us/azure/security/fundamentals/identity-management-best-practices#use-role-based-access-control">suggests you use it wisely</a>.</p><blockquote><p>Access management for cloud resources is critical for any organization that uses the cloud. <a href="https://docs.microsoft.com/en-us/azure/role-based-access-control/overview">Azure role-based access control (Azure RBAC)</a> helps you manage who has access to Azure resources, what they can do with those resources, and what areas they have access to.</p><p>Designating groups or individual roles responsible for specific functions in Azure helps avoid confusion that can lead to human and automation errors that create security risks. Restricting access based on the <a href="https://en.wikipedia.org/wiki/Need_to_know">need to know</a> and <a href="https://en.wikipedia.org/wiki/Principle_of_least_privilege">least privilege</a> security principles is imperative for organizations that want to enforce security policies for data access.</p></blockquote><p>This is good advice. With that in mind, how can we ensure that the different resources we&#x27;re deploying to Azure can talk to one another?</p><h2>Role (up for your) assignments</h2><p>The answer is roles. There&#x27;s a number of roles that exist in Azure that can be assigned to users, groups, service principals and managed identities. In our own case we&#x27;re using managed identity for our resources. What we can do is use <a href="https://docs.microsoft.com/en-us/azure/role-based-access-control/overview#how-azure-rbac-works">&quot;role assignments&quot;</a> to give our managed identity access to given resources. <a href="https://twitter.com/ArLucaID">Arturo Lucatero</a> gives a great short explanation of this:</p><iframe width="560" height="315" src="https://www.youtube.com/embed/Dzhm-garKBM" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"></iframe><p>Whilst this explanation is delightfully simple, the actual implementation when it comes to ARM templates is a little more involved. Because now it&#x27;s time to talk &quot;magic&quot; GUIDs. Consider the following truncated ARM template, which gives our managed identity (and hence our App Service which uses this identity) access to Key Vault and Storage:</p><pre><code class="language-json">{
  &quot;$schema&quot;: &quot;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#&quot;,
  // ...
  &quot;variables&quot;: {
    // ...
    &quot;managedIdentity&quot;: &quot;[concat(&#x27;mi-&#x27;, parameters(&#x27;applicationName&#x27;), &#x27;-&#x27;, parameters(&#x27;environment&#x27;), &#x27;-&#x27;, &#x27;001&#x27;)]&quot;,
    &quot;appInsightsName&quot;: &quot;[concat(&#x27;appi-&#x27;, parameters(&#x27;applicationName&#x27;), &#x27;-&#x27;, parameters(&#x27;environment&#x27;), &#x27;-&#x27;, &#x27;001&#x27;)]&quot;,
    &quot;keyVaultName&quot;: &quot;[concat(&#x27;kv-&#x27;, parameters(&#x27;applicationName&#x27;), &#x27;-&#x27;, parameters(&#x27;environment&#x27;), &#x27;-&#x27;, &#x27;001&#x27;)]&quot;,
    &quot;storageAccountName&quot;: &quot;[concat(&#x27;st&#x27;, parameters(&#x27;applicationName&#x27;), parameters(&#x27;environment&#x27;), &#x27;001&#x27;)]&quot;,
    &quot;storageBlobDataContributor&quot;: &quot;[subscriptionResourceId(&#x27;Microsoft.Authorization/roleDefinitions&#x27;, &#x27;ba92f5b4-2d11-453d-a403-e96b0029c9fe&#x27;)]&quot;,
    &quot;keyVaultSecretsOfficer&quot;: &quot;[subscriptionResourceId(&#x27;Microsoft.Authorization/roleDefinitions&#x27;, &#x27;b86a8fe4-44ce-4948-aee5-eccb2c155cd7&#x27;)]&quot;,
    &quot;keyVaultCryptoOfficer&quot;: &quot;[subscriptionResourceId(&#x27;Microsoft.Authorization/roleDefinitions&#x27;, &#x27;14b46e9e-c2b7-41b4-b07b-48a6ebf60603&#x27;)]&quot;,
    &quot;uniqueRoleGuidKeyVaultSecretsOfficer&quot;: &quot;[guid(resourceId(&#x27;Microsoft.KeyVault/vaults&#x27;,  variables(&#x27;keyVaultName&#x27;)), variables(&#x27;keyVaultSecretsOfficer&#x27;), resourceId(&#x27;Microsoft.KeyVault/vaults&#x27;, variables(&#x27;keyVaultName&#x27;)))]&quot;,
    &quot;uniqueRoleGuidKeyVaultCryptoOfficer&quot;: &quot;[guid(resourceId(&#x27;Microsoft.KeyVault/vaults&#x27;,  variables(&#x27;keyVaultName&#x27;)), variables(&#x27;keyVaultCryptoOfficer&#x27;), resourceId(&#x27;Microsoft.KeyVault/vaults&#x27;, variables(&#x27;keyVaultName&#x27;)))]&quot;,
    &quot;uniqueRoleGuidStorageAccount&quot;: &quot;[guid(resourceId(&#x27;Microsoft.Storage/storageAccounts&#x27;,  variables(&#x27;storageAccountName&#x27;)), variables(&#x27;storageBlobDataContributor&#x27;), resourceId(&#x27;Microsoft.Storage/storageAccounts&#x27;, variables(&#x27;storageAccountName&#x27;)))]&quot;
  },
  &quot;resources&quot;: [
    {
      &quot;type&quot;: &quot;Microsoft.ManagedIdentity/userAssignedIdentities&quot;,
      &quot;name&quot;: &quot;[variables(&#x27;managedIdentity&#x27;)]&quot;,
      &quot;apiVersion&quot;: &quot;2018-11-30&quot;,
      &quot;location&quot;: &quot;[parameters(&#x27;location&#x27;)]&quot;
    },
    // ...
    {
      &quot;type&quot;: &quot;Microsoft.Storage/storageAccounts/providers/roleAssignments&quot;,
      &quot;apiVersion&quot;: &quot;2020-04-01-preview&quot;,
      &quot;name&quot;: &quot;[concat(variables(&#x27;storageAccountName&#x27;), &#x27;/Microsoft.Authorization/&#x27;, variables(&#x27;uniqueRoleGuidStorageAccount&#x27;))]&quot;,
      &quot;dependsOn&quot;: [
        &quot;[resourceId(&#x27;Microsoft.ManagedIdentity/userAssignedIdentities&#x27;, variables(&#x27;managedIdentity&#x27;))]&quot;
      ],
      &quot;properties&quot;: {
        &quot;roleDefinitionId&quot;: &quot;[variables(&#x27;storageBlobDataContributor&#x27;)]&quot;,
        &quot;principalId&quot;: &quot;[reference(resourceId(&#x27;Microsoft.ManagedIdentity/userAssignedIdentities/&#x27;, variables(&#x27;managedIdentity&#x27;)), &#x27;2018-11-30&#x27;).principalId]&quot;,
        &quot;scope&quot;: &quot;[resourceId(&#x27;Microsoft.Storage/storageAccounts&#x27;, variables(&#x27;storageAccountName&#x27;))]&quot;,
        &quot;principalType&quot;: &quot;ServicePrincipal&quot;
      }
    },
    {
      &quot;type&quot;: &quot;Microsoft.KeyVault/vaults/providers/roleAssignments&quot;,
      &quot;apiVersion&quot;: &quot;2018-01-01-preview&quot;,
      &quot;name&quot;: &quot;[concat(variables(&#x27;keyVaultName&#x27;), &#x27;/Microsoft.Authorization/&#x27;, variables(&#x27;uniqueRoleGuidKeyVaultSecretsOfficer&#x27;))]&quot;,
      &quot;dependsOn&quot;: [
        &quot;[resourceId(&#x27;Microsoft.ManagedIdentity/userAssignedIdentities&#x27;, variables(&#x27;managedIdentity&#x27;))]&quot;
      ],
      &quot;properties&quot;: {
        &quot;roleDefinitionId&quot;: &quot;[variables(&#x27;keyVaultSecretsOfficer&#x27;)]&quot;,
        &quot;principalId&quot;: &quot;[reference(resourceId(&#x27;Microsoft.ManagedIdentity/userAssignedIdentities/&#x27;, variables(&#x27;managedIdentity&#x27;)), &#x27;2018-11-30&#x27;).principalId]&quot;,
        &quot;scope&quot;: &quot;[resourceId(&#x27;Microsoft.KeyVault/vaults&#x27;, variables(&#x27;keyVaultName&#x27;))]&quot;,
        &quot;principalType&quot;: &quot;ServicePrincipal&quot;
      }
    },
    {
      &quot;type&quot;: &quot;Microsoft.KeyVault/vaults/providers/roleAssignments&quot;,
      &quot;apiVersion&quot;: &quot;2018-01-01-preview&quot;,
      &quot;name&quot;: &quot;[concat(variables(&#x27;keyVaultName&#x27;), &#x27;/Microsoft.Authorization/&#x27;, variables(&#x27;uniqueRoleGuidKeyVaultCryptoOfficer&#x27;))]&quot;,
      &quot;dependsOn&quot;: [
        &quot;[resourceId(&#x27;Microsoft.ManagedIdentity/userAssignedIdentities&#x27;, variables(&#x27;managedIdentity&#x27;))]&quot;
      ],
      &quot;properties&quot;: {
        &quot;roleDefinitionId&quot;: &quot;[variables(&#x27;keyVaultCryptoOfficer&#x27;)]&quot;,
        &quot;principalId&quot;: &quot;[reference(resourceId(&#x27;Microsoft.ManagedIdentity/userAssignedIdentities/&#x27;, variables(&#x27;managedIdentity&#x27;)), &#x27;2018-11-30&#x27;).principalId]&quot;,
        &quot;scope&quot;: &quot;[resourceId(&#x27;Microsoft.KeyVault/vaults&#x27;, variables(&#x27;keyVaultName&#x27;))]&quot;,
        &quot;principalType&quot;: &quot;ServicePrincipal&quot;
      }
    }
  ]
}
</code></pre><p>Let&#x27;s take a look at these three variables:</p><pre><code class="language-json">&quot;storageBlobDataContributor&quot;: &quot;[subscriptionResourceId(&#x27;Microsoft.Authorization/roleDefinitions&#x27;, &#x27;ba92f5b4-2d11-453d-a403-e96b0029c9fe&#x27;)]&quot;,
&quot;keyVaultSecretsOfficer&quot;: &quot;[subscriptionResourceId(&#x27;Microsoft.Authorization/roleDefinitions&#x27;, &#x27;b86a8fe4-44ce-4948-aee5-eccb2c155cd7&#x27;)]&quot;,
&quot;keyVaultCryptoOfficer&quot;: &quot;[subscriptionResourceId(&#x27;Microsoft.Authorization/roleDefinitions&#x27;, &#x27;14b46e9e-c2b7-41b4-b07b-48a6ebf60603&#x27;)]&quot;,
</code></pre><p>The three variables above contain the subscription resource ids for the roles <a href="https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#storage-blob-data-contributor">Storage Blob Data Contributor</a>, <a href="https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#key-vault-secrets-officer-preview">Key Vault Secrets Officer</a> and <a href="https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#key-vault-crypto-officer-preview">Key Vault Crypto Officer</a>. The first question on your mind is likely: &quot;what is <code>ba92f5b4-2d11-453d-a403-e96b0029c9fe</code> and where does it come from?&quot; Great question! Well, each of these GUIDs represents a built-in role in Azure RBAC. The <code>ba92f5b4-2d11-453d-a403-e96b0029c9fe</code> represents the Storage Blob Data Contributor role.</p><p>How can I look these up? Well, there&#x27;s two ways; <a href="https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles">there&#x27;s an article which documents them here</a> or you could crack open the <a href="https://azure.microsoft.com/en-gb/features/cloud-shell/">Cloud Shell</a> and look up a role by GUID like so:</p><pre><code class="language-ps">Get-AzRoleDefinition | ? {$_.id -eq &quot;ba92f5b4-2d11-453d-a403-e96b0029c9fe&quot; }

Name             : Storage Blob Data Contributor
Id               : ba92f5b4-2d11-453d-a403-e96b0029c9fe
IsCustom         : False
Description      : Allows for read, write and delete access to Azure Storage blob containers and data
Actions          : {Microsoft.Storage/storageAccounts/blobServices/containers/delete, Microsoft.Storage/storageAccounts/blobServices/containers/read,
                   Microsoft.Storage/storageAccounts/blobServices/containers/write, Microsoft.Storage/storageAccounts/blobServices/generateUserDelegationKey/action}
NotActions       : {}
DataActions      : {Microsoft.Storage/storageAccounts/blobServices/containers/blobs/delete, Microsoft.Storage/storageAccounts/blobServices/containers/blobs/read,
                   Microsoft.Storage/storageAccounts/blobServices/containers/blobs/write, Microsoft.Storage/storageAccounts/blobServices/containers/blobs/move/action…}
NotDataActions   : {}
AssignableScopes : {/}
</code></pre><p>Or by name like so:</p><pre><code class="language-ps">Get-AzRoleDefinition | ? {$_.name -like &quot;*Crypto Officer*&quot; }

Name             : Key Vault Crypto Officer
Id               : 14b46e9e-c2b7-41b4-b07b-48a6ebf60603
IsCustom         : False
Description      : Perform any action on the keys of a key vault, except manage permissions. Only works for key vaults that use the &#x27;Azure role-based access control&#x27; permission model.
Actions          : {Microsoft.Authorization/*/read, Microsoft.Insights/alertRules/*, Microsoft.Resources/deployments/*, Microsoft.Resources/subscriptions/resourceGroups/read…}
NotActions       : {}
DataActions      : {Microsoft.KeyVault/vaults/keys/*}
NotDataActions   : {}
AssignableScopes : {/}
</code></pre><p>As you can see, the <code>Actions</code> section of the output above (and in even more detail on the <a href="https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles">linked article</a>) provides information about what the different roles can do. So if you&#x27;re looking to enable one Azure resource to talk to another, you should be able to refer to these to identify a role that you might want to use.</p><h2>Creating a role assignment</h2><p>So now we understand how you identify the roles in question, let&#x27;s take the final leap and look at assigning those roles to our managed identity. For each role assignment, you&#x27;ll need a <code>roleAssignments</code> resource defined that looks like this:</p><pre><code class="language-json">{
  &quot;type&quot;: &quot;Microsoft.KeyVault/vaults/providers/roleAssignments&quot;,
  &quot;apiVersion&quot;: &quot;2018-01-01-preview&quot;,
  &quot;name&quot;: &quot;[concat(variables(&#x27;keyVaultName&#x27;), &#x27;/Microsoft.Authorization/&#x27;, variables(&#x27;uniqueRoleGuidKeyVaultCryptoOfficer&#x27;))]&quot;,
  &quot;dependsOn&quot;: [
    &quot;[resourceId(&#x27;Microsoft.ManagedIdentity/userAssignedIdentities&#x27;, variables(&#x27;managedIdentity&#x27;))]&quot;
  ],
  &quot;properties&quot;: {
    &quot;roleDefinitionId&quot;: &quot;[variables(&#x27;keyVaultCryptoOfficer&#x27;)]&quot;,
    &quot;principalId&quot;: &quot;[reference(resourceId(&#x27;Microsoft.ManagedIdentity/userAssignedIdentities/&#x27;, variables(&#x27;managedIdentity&#x27;)), &#x27;2018-11-30&#x27;).principalId]&quot;,
    &quot;scope&quot;: &quot;[resourceId(&#x27;Microsoft.KeyVault/vaults&#x27;, variables(&#x27;keyVaultName&#x27;))]&quot;,
    &quot;principalType&quot;: &quot;ServicePrincipal&quot;
  }
}
</code></pre><p>Let&#x27;s go through the above, significant property by significant property (it&#x27;s also worth checking the official reference <a href="https://docs.microsoft.com/en-us/azure/templates/microsoft.authorization/roleassignments">here</a>):</p><ul><li><code>type</code> <!-- -->-<!-- --> the type of role assignment we want to create, for a key vault it&#x27;s <code>&quot;Microsoft.KeyVault/vaults/providers/roleAssignments&quot;</code>, for storage it&#x27;s <code>&quot;Microsoft.Storage/storageAccounts/providers/roleAssignments&quot;</code>. The pattern is that it&#x27;s the resource type, followed by <code>&quot;/providers/roleAssignments&quot;</code>.</li><li><code>dependsOn</code> <!-- -->-<!-- --> before we can create a role assignment, we need the service principal we desire to permission (in our case a managed identity) to exist</li><li><code>properties.roleDefinitionId</code> <!-- -->-<!-- --> the role that we&#x27;re assigning, provided as an id. So for this example it&#x27;s the <code>keyVaultCryptoOfficer</code> variable, which was earlier defined as <code>[subscriptionResourceId(&#x27;Microsoft.Authorization/roleDefinitions&#x27;, &#x27;ba92f5b4-2d11-453d-a403-e96b0029c9fe&#x27;)]</code>. (Note the use of the GUID)</li><li><code>properties.principalId</code> <!-- -->-<!-- --> the id of the principal we&#x27;re adding permissions for. In our case this is a managed identity (a type of service principal).</li><li><code>properties.scope</code> <!-- -->-<!-- --> we&#x27;re modifying another resource; our key vault isn&#x27;t defined in this ARM template and we want to specify the resource we&#x27;re granting permissions to.</li><li><code>properties.principalType</code> <!-- -->-<!-- --> the type of principal that we&#x27;re creating an assignment for; in our this is <code>&quot;ServicePrincipal&quot;</code> <!-- -->-<!-- --> our managed identity.</li></ul><p>There is an alternate approach that you can use where the <code>type</code> is <code>&quot;Microsoft.Authorization/roleAssignments&quot;</code>. Whilst this also works, it displayed errors in the <a href="https://marketplace.visualstudio.com/items?itemName=msazurermtools.azurerm-vscode-tools">Azure tooling for VS Code</a>. As such, we&#x27;ve opted not to use that approach in our ARM templates.</p><p>Many thanks to the awesome <a href="https://github.com/jmccor99">John McCormick</a> who wrangled permissions with me until we bent Azure RBAC to our will.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ASP.NET, Serilog and Application Insights]]></title>
            <link>https://blog.johnnyreilly.com/2021/01/30/aspnet-serilog-and-application-insights</link>
            <guid>ASP.NET, Serilog and Application Insights</guid>
            <pubDate>Sat, 30 Jan 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[If you're deploying an ASP.NET application to Azure App Services, there's a decent chance you'll also be using the fantastic Serilog and will want to plug it into Azure's Application Insights.]]></description>
            <content:encoded><![CDATA[<p>If you&#x27;re deploying an ASP.NET application to Azure App Services, there&#x27;s a decent chance you&#x27;ll also be using the fantastic <a href="https://serilog.net/">Serilog</a> and will want to plug it into Azure&#x27;s <a href="https://docs.microsoft.com/en-us/azure/azure-monitor/app/app-insights-overview">Application Insights</a>.</p><p>This post will show you how it&#x27;s done, and it&#x27;ll also build upon the <a href="2021-01-29-surfacing-azure-pipelines-build-info-in-an-aspnet-react-app.md">build info work from our previous post</a>. In what way? Great question. Well logs are a tremendous diagnostic tool. If you have logs which display some curious behaviour, and you&#x27;d like to replicate that in another environment, you really want to take exactly that version of the codebase out to play. Our last post introduced build info into our application in the form of our <code>AppVersionInfo</code> class that looks something like this:</p><pre><code class="language-json">{
  &quot;buildNumber&quot;: &quot;20210130.1&quot;,
  &quot;buildId&quot;: &quot;123456&quot;,
  &quot;branchName&quot;: &quot;main&quot;,
  &quot;commitHash&quot;: &quot;7089620222c30c1ad88e4b556c0a7908ddd34a8e&quot;
}
</code></pre><p>We&#x27;d initially exposed an endpoint in our application which surfaced up this information. Now we&#x27;re going to take that self same information and bake it into our log messages by making use of <a href="https://github.com/serilog/serilog/wiki/Enrichment">Serilog&#x27;s enrichment functionality</a>. Build info and Serilog&#x27;s enrichment are the double act your logging has been waiting for.</p><h2>Let&#x27;s plug it together</h2><p>We&#x27;re going to need a number of Serilog dependencies added to our <code>.csproj</code>:</p><pre><code class="language-xml">&lt;PackageReference Include=&quot;Serilog.AspNetCore&quot; Version=&quot;3.4.0&quot; /&gt;
&lt;PackageReference Include=&quot;Serilog.Enrichers.Environment&quot; Version=&quot;2.1.3&quot; /&gt;
&lt;PackageReference Include=&quot;Serilog.Enrichers.Thread&quot; Version=&quot;3.1.0&quot; /&gt;
&lt;PackageReference Include=&quot;Serilog.Sinks.ApplicationInsights&quot; Version=&quot;3.1.0&quot; /&gt;
&lt;PackageReference Include=&quot;Serilog.Sinks.Async&quot; Version=&quot;1.4.0&quot; /&gt;
</code></pre><p>The earlier in your application lifetime you get logging wired up, the happier you will be. Earlier, means more information when you&#x27;re diagnosing issues. So we want to start in our <code>Program.cs</code>; <code>Startup.cs</code> would be just <em>way</em> too late.</p><pre><code class="language-cs">public class Program {
    const string APP_NAME = &quot;MyAmazingApp&quot;;

    public static int Main(string[] args) {
        AppVersionInfo.InitialiseBuildInfoGivenPath(Directory.GetCurrentDirectory());
        LoggerConfigurationExtensions.SetupLoggerConfiguration(APP_NAME, AppVersionInfo.GetBuildInfo());

        try
        {
            Log.Information(&quot;Starting web host&quot;);
            CreateHostBuilder(args).Build().Run();
            return 0;
        }
        catch (Exception ex)
        {
            Log.Fatal(ex, &quot;Host terminated unexpectedly&quot;);
            return 1;
        }
        finally
        {
            Log.CloseAndFlush();
        }
    }

    public static IHostBuilder CreateHostBuilder(string[] args) =&gt;
        Host.CreateDefaultBuilder(args)
            .UseSerilog((hostBuilderContext, services, loggerConfiguration) =&gt; {
                loggerConfiguration.ConfigureBaseLogging(APP_NAME, AppVersionInfo.GetBuildInfo());
                loggerConfiguration.AddApplicationInsightsLogging(services, hostBuilderContext.Configuration);
            })
            .ConfigureWebHostDefaults(webBuilder =&gt; {
                webBuilder
                    .UseStartup&lt;Startup&gt;();
            });
}
</code></pre><p>If you look at the code above you&#x27;ll see that the first line of code that executes is <code>AppVersionInfo.InitialiseBuildInfoGivenPath</code>. This initialises our <code>AppVersionInfo</code> so we have meaningful build info to pump into our logs. The next thing we do is to configure Serilog with <code>LoggerConfigurationExtensions.SetupLoggerConfiguration</code>. This provides us with a configured logger so we are free to log any issues that take place during startup. (Incidentally, after startup you&#x27;ll likely inject an <code>ILogger</code> into your classes rather than using the static <code>Log</code> directly.)</p><p>Finally, we call <code>CreateHostBuilder</code> which in turn calls <code>UseSerilog</code> to plug Serilog into ASP.NET. If you take a look inside the body of <code>UseSerilog</code> you&#x27;ll see we configure the logging of ASP.NET (in the same way we did for Serilog) and we hook into Application Insights as well. There&#x27;s been a number of references to <code>LoggerConfigurationExtensions</code>. Let&#x27;s take a look at it:</p><pre><code class="language-cs">internal static class LoggerConfigurationExtensions {
    internal static void SetupLoggerConfiguration(string appName, BuildInfo buildInfo) {
        Log.Logger = new LoggerConfiguration()
            .ConfigureBaseLogging(appName, buildInfo)
            .CreateLogger();
    }

    internal static LoggerConfiguration ConfigureBaseLogging(
        this LoggerConfiguration loggerConfiguration,
        string appName,
        BuildInfo buildInfo
    ) {
        loggerConfiguration
            .MinimumLevel.Debug()
            .MinimumLevel.Override(&quot;Microsoft&quot;, LogEventLevel.Information)
            // AMAZING COLOURS IN THE CONSOLE!!!!
            .WriteTo.Async(a =&gt; a.Console(theme: AnsiConsoleTheme.Code))
            .Enrich.FromLogContext()
            .Enrich.WithMachineName()
            .Enrich.WithThreadId()
            // Build information as custom properties
            .Enrich.WithProperty(nameof(buildInfo.BuildId), buildInfo.BuildId)
            .Enrich.WithProperty(nameof(buildInfo.BuildNumber), buildInfo.BuildNumber)
            .Enrich.WithProperty(nameof(buildInfo.BranchName), buildInfo.BranchName)
            .Enrich.WithProperty(nameof(buildInfo.CommitHash), buildInfo.CommitHash)
            .Enrich.WithProperty(&quot;ApplicationName&quot;, appName);

        return loggerConfiguration;
    }

    internal static LoggerConfiguration AddApplicationInsightsLogging(this LoggerConfiguration loggerConfiguration, IServiceProvider services, IConfiguration configuration)
    {
        if (!string.IsNullOrWhiteSpace(configuration.GetValue&lt;string&gt;(&quot;APPINSIGHTS_INSTRUMENTATIONKEY&quot;)))
        {
            loggerConfiguration.WriteTo.ApplicationInsights(
                services.GetRequiredService&lt;TelemetryConfiguration&gt;(),
                TelemetryConverter.Traces);
        }

        return loggerConfiguration;
    }
}
</code></pre><p>If we take a look at the <code>ConfigureBaseLogging</code> method above, we can see that our logs are being enriched with the build info, property by property. We&#x27;re also giving ourselves a beautifully coloured console thanks to Serilog&#x27;s glorious <a href="https://github.com/serilog/serilog-sinks-console#themes">theme support</a>:</p><p><img src="../static/blog/2021-01-30-aspnet-serilog-and-application-insights/coloured-console.png" alt="screenshot of the console featuring coloured output"/></p><p>Take a moment to admire the salmon pinks. Is it not lovely?</p><p>Finally we come to the main act. Plugging in Application Insights is as simple as dropping in <code>loggerConfiguration.WriteTo.ApplicationInsights</code> into our configuration. You&#x27;ll note that this depends upon the existence of an application setting of <code>APPINSIGHTS_INSTRUMENTATIONKEY</code> - this is the secret sauce that we need to be in place so we can pipe logs merrily to Application Insights. So you&#x27;ll need this configuration in place so this works.</p><p><img src="../static/blog/2021-01-30-aspnet-serilog-and-application-insights/application-insights-properties.png" alt="screenshot of application insights with our output"/></p><p>As you can see, we now have the likes of <code>BuildNumber</code>, <code>CommitHash</code> and friends visible on each log. Happy diagnostic days!</p><p>I&#x27;m indebted to the marvellous <a href="https://twitter.com/MarcelMichau">Marcel Michau</a> who showed me how to get the fiddlier parts of how to get Application Insights plugged in the right way. Thanks chap!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Azure Pipelines Build Info in an ASP.NET React app]]></title>
            <link>https://blog.johnnyreilly.com/2021/01/29/surfacing-azure-pipelines-build-info-in-an-aspnet-react-app</link>
            <guid>Azure Pipelines Build Info in an ASP.NET React app</guid>
            <pubDate>Fri, 29 Jan 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[How do you answer the question: "what version of my application is running in Production right now?" This post demonstrates how to surface the build metadata that represents the version of your app, from your app using Azure Pipelines and ASP.NET.]]></description>
            <content:encoded><![CDATA[<p>How do you answer the question: &quot;what version of my application is running in Production right now?&quot; This post demonstrates how to surface the build metadata that represents the version of your app, from your app using Azure Pipelines and ASP.NET.</p><p>Many is the time where I&#x27;ve been pondering over why something isn&#x27;t working as expected and burned a disappointing amount of time before realising that I&#x27;m playing with an old version of an app. Wouldn&#x27;t it be great give our app a way to say: &quot;Hey! I&#x27;m version 1.2.3.4 of your app; built from this commit hash, I was built on Wednesday, I was the nineth build that day and I was built from the <code>main</code> branch. And I&#x27;m an Aries.&quot; Or something like that.</p><p>This post was inspired by <a href="https://www.hanselman.com/blog/adding-a-git-commit-hash-and-azure-devops-build-number-and-build-id-to-an-aspnet-website">Scott Hanselman&#x27;s similar post on the topic</a>. Ultimately this ended up going in a fairly different direction and so seemed worthy of a post of its own.</p><p>A particular difference is that this is targeting SPAs. Famously, cache invalidation is hard. It&#x27;s possible for the HTML/JS/CSS of your app to be stale due to aggressive caching. So we&#x27;re going to make it possible to see build information for both when the SPA (or &quot;client&quot;) is built, as well as when the .NET app (or &quot;server&quot;) is built. We&#x27;re using a specific type of SPA here; a <a href="https://reactjs.org/">React</a> SPA built with <a href="https://www.typescriptlang.org/">TypeScript</a> and <a href="https://material-ui.com/">Material UI</a>, however the principles here are general; you could surface this up any which way you choose.</p><h2>Putting build info into <code>azure-pipelines.yml</code></h2><p>The first thing we&#x27;re going to do is to inject our build details into two identical <code>buildinfo.json</code> files; one that sits in the server codebase and which will be used to drive the server build information, and one that sits in the client codebase to drive the client equivalent. They&#x27;ll end up looking something like this:</p><pre><code class="language-json">{
  &quot;buildNumber&quot;: &quot;20210130.1&quot;,
  &quot;buildId&quot;: &quot;123456&quot;,
  &quot;branchName&quot;: &quot;main&quot;,
  &quot;commitHash&quot;: &quot;7089620222c30c1ad88e4b556c0a7908ddd34a8e&quot;
}
</code></pre><p>We generate this by adding the following <code>yml</code> to the beginning of our <code>azure-pipelines.yml</code> (crucially before the client or server build take place):</p><pre><code class="language-yml">- script: |
            echo -e -n &quot;{\&quot;buildNumber\&quot;:\&quot;$(Build.BuildNumber)\&quot;,\&quot;buildId\&quot;:\&quot;$(Build.BuildId)\&quot;,\&quot;branchName\&quot;:\&quot;$(Build.SourceBranchName)\&quot;,\&quot;commitHash\&quot;:\&quot;$(Build.SourceVersion)\&quot;}&quot; &gt; &quot;$(Build.SourcesDirectory)/src/client-app/src/buildinfo.json&quot;
            echo -e -n &quot;{\&quot;buildNumber\&quot;:\&quot;$(Build.BuildNumber)\&quot;,\&quot;buildId\&quot;:\&quot;$(Build.BuildId)\&quot;,\&quot;branchName\&quot;:\&quot;$(Build.SourceBranchName)\&quot;,\&quot;commitHash\&quot;:\&quot;$(Build.SourceVersion)\&quot;}&quot; &gt; &quot;$(Build.SourcesDirectory)/src/server-app/Server/buildinfo.json&quot;
          displayName: &quot;emit build details as JSON&quot;
          failOnStderr: true
</code></pre><p>As you can see, we&#x27;re placing the following variables that are available at build time in Azure Pipelines, into the <code>buildinfo.json</code>:</p><ul><li><code>BuildNumber</code> - The name of the completed build; which usually takes the form of a date in the <code>yyyyMMdd</code> format, suffixed by <code>.x</code> where <code>x</code> is a number that increments representing the number of builds that have taken place on the given day.</li><li><code>BuildId</code> - The ID of the record for the completed build.</li><li><code>SourceVersion</code> - This is the commit hash of the source code in Git</li><li><code>SourceBranchName</code> - The name of the branch in Git.</li></ul><p><a href="https://docs.microsoft.com/en-us/azure/devops/pipelines/build/variables?view=azure-devops&amp;tabs=yaml#build-variables-devops-services">There&#x27;s many variables available in Azure Pipelines that can be used</a> - we&#x27;ve picked out the ones most interesting to us.</p><h2>Surfacing the server build info</h2><p>Our pipeline is dropping the <code>buildinfo.json</code> over pre-existing stub <code>buildinfo.json</code> files in both our client and server codebases. The stub files look like this:</p><pre><code class="language-json">{
  &quot;buildNumber&quot;: &quot;yyyyMMdd.x&quot;,
  &quot;buildId&quot;: &quot;xxxxxx&quot;,
  &quot;branchName&quot;: &quot;&quot;,
  &quot;commitHash&quot;: &quot;LOCAL_BUILD&quot;
}
</code></pre><p>In our .NET app, the <code>buildinfo.json</code> file has been dropped in the root of the app. And as luck would have it, all JSON files are automatically included in a .NET build and so it will be available at runtime. We want to surface this file through an API, and we also want to use it to stamp details into our logs.</p><p>So we need to parse the file, and for that we&#x27;ll use this:</p><pre><code class="language-cs">using System;
using System.IO;
using System.Text.Json;

namespace Server {
    public record BuildInfo(string BranchName, string BuildNumber, string BuildId, string CommitHash);

    public static class AppVersionInfo {
        private const string _buildFileName = &quot;buildinfo.json&quot;;
        private static BuildInfo _fileBuildInfo = new(
            BranchName: &quot;&quot;,
            BuildNumber: DateTime.UtcNow.ToString(&quot;yyyyMMdd&quot;) + &quot;.0&quot;,
            BuildId: &quot;xxxxxx&quot;,
            CommitHash: $&quot;Not yet initialised - call {nameof(InitialiseBuildInfoGivenPath)}&quot;
        );

        public static void InitialiseBuildInfoGivenPath(string path) {
            var buildFilePath = Path.Combine(path, _buildFileName);
            if (File.Exists(buildFilePath)) {
                try {
                    var buildInfoJson = File.ReadAllText(buildFilePath);
                    var buildInfo = JsonSerializer.Deserialize&lt;BuildInfo&gt;(buildInfoJson, new JsonSerializerOptions {
                        PropertyNamingPolicy = JsonNamingPolicy.CamelCase
                    });
                    if (buildInfo == null) throw new Exception($&quot;Failed to deserialise {_buildFileName}&quot;);

                    _fileBuildInfo = buildInfo;
                } catch (Exception) {
                    _fileBuildInfo = new BuildInfo(
                        BranchName: &quot;&quot;,
                        BuildNumber: DateTime.UtcNow.ToString(&quot;yyyyMMdd&quot;) + &quot;.0&quot;,
                        BuildId: &quot;xxxxxx&quot;,
                        CommitHash: &quot;Failed to load build info from buildinfo.json&quot;
                    );
                }
            }
        }

        public static BuildInfo GetBuildInfo() =&gt; _fileBuildInfo;
    }
}
</code></pre><p>The above code reads the <code>buildinfo.json</code> file and deserialises it into a <code>BuildInfo</code> record which is then surfaced up by the <code>GetBuildInfo</code> method. We initialise this at the start of our <code>Program.cs</code> like so:</p><pre><code class="language-cs">public static int Main(string[] args) {
    AppVersionInfo.InitialiseBuildInfoGivenPath(Directory.GetCurrentDirectory());
    // Now we&#x27;re free to call AppVersionInfo.GetBuildInfo()
    // ....
}
</code></pre><p>Now we need a controller to surface this information up. We&#x27;ll add ourselves a <code>BuildInfoController.cs</code>:</p><pre><code class="language-cs">using Microsoft.AspNetCore.Authorization;
using Microsoft.AspNetCore.Mvc;

namespace Server.Controllers {
    [ApiController]
    public class BuildInfoController : ControllerBase {
        [AllowAnonymous]
        [HttpGet(&quot;api/build&quot;)]
        public BuildInfo GetBuild() =&gt; AppVersionInfo.GetBuildInfo();
    }
}
</code></pre><p>This exposes an <code>api/build</code> endpoint in our .NET app that, when hit, will display the following JSON:</p><p><img src="../static/blog/2021-01-29-azure-pipelines-build-info-in-an-aspnet-react-app/api-build-screenshot.png" alt="screenshot of api/build output"/></p><h2>Surfacing the client build info</h2><p>Our server now lets the world know which version it is running and this is tremendous. Now let&#x27;s make our client do the same.</p><p>Very little is required to achieve this. Again we have a <code>buildinfo.json</code> sat in the root of our codebase. We&#x27;re able to import it as a module in TypeScript because we&#x27;ve set the following property in our <code>tsconfig.json</code>:</p><pre><code class="language-json">&quot;resolveJsonModule&quot;: true,
</code></pre><p>As a consequence, consumption is as simple as:</p><pre><code>import clientBuildInfo from &#x27;./buildinfo.json&#x27;;
</code></pre><p>Which provides us with a <code>clientBuildInfo</code> which TypeScript automatically derives as this type:</p><pre><code>type ClientBuildInfo = {
    buildNumber: string;
    buildId: string;
    branchName: string;
    commitHash: string;
}
</code></pre><p>How you choose to use that information is entirely your choice. We&#x27;re going to add ourselves an &quot;about&quot; screen in our app, which displays both client info (loaded using the mechanism above) and server info (<code>fetch</code>ed from the <code>/api/build</code> endpoint).</p><pre><code class="language-tsx">import {
  Card,
  CardContent,
  CardHeader,
  createStyles,
  Grid,
  makeStyles,
  Theme,
  Typography,
  Zoom,
} from &#x27;@material-ui/core&#x27;;
import React from &#x27;react&#x27;;
import clientBuildInfo from &#x27;../../buildinfo.json&#x27;;
import { projectsPurple } from &#x27;../shared/colors&#x27;;
import { Loading } from &#x27;../shared/Loading&#x27;;
import { TransitionContainer } from &#x27;../shared/TransitionContainer&#x27;;

const useStyles = (cardColor: string) =&gt;
  makeStyles((theme: Theme) =&gt;
    createStyles({
      card: {
        padding: theme.spacing(0),
        backgroundColor: cardColor,
        color: theme.palette.common.white,
        minHeight: theme.spacing(28),
      },
      avatar: {
        backgroundColor: theme.palette.getContrastText(cardColor),
        color: cardColor,
      },
      main: {
        padding: theme.spacing(2),
      },
    })
  )();

type Styles = ReturnType&lt;typeof useStyles&gt;;

const AboutPage: React.FC = () =&gt; {
  const [serverBuildInfo, setServerBuildInfo] =
    React.useState&lt;typeof clientBuildInfo&gt;();

  React.useEffect(() =&gt; {
    fetch(&#x27;/api/build&#x27;)
      .then((response) =&gt; response.json())
      .then(setServerBuildInfo);
  }, []);

  const classes = useStyles(projectsPurple);

  return (
    &lt;TransitionContainer&gt;
      &lt;Grid container spacing={3}&gt;
        &lt;Grid item xs={12} sm={12} container alignItems=&quot;center&quot;&gt;
          &lt;Grid item&gt;
            &lt;Typography variant=&quot;h4&quot; component=&quot;h1&quot;&gt;
              About
            &lt;/Typography&gt;
          &lt;/Grid&gt;
        &lt;/Grid&gt;
      &lt;/Grid&gt;
      &lt;Grid container spacing={1}&gt;
        &lt;BuildInfo
          classes={classes}
          title=&quot;Client Version&quot;
          {...clientBuildInfo}
        /&gt;
      &lt;/Grid&gt;
      &lt;br /&gt;
      &lt;Grid container spacing={1}&gt;
        {serverBuildInfo ? (
          &lt;BuildInfo
            classes={classes}
            title=&quot;Server Version&quot;
            {...serverBuildInfo}
          /&gt;
        ) : (
          &lt;Loading /&gt;
        )}
      &lt;/Grid&gt;
    &lt;/TransitionContainer&gt;
  );
};

interface Props {
  classes: Styles;
  title: string;
  branchName: string;
  buildNumber: string;
  buildId: string;
  commitHash: string;
}

const BuildInfo: React.FC&lt;Props&gt; = ({
  classes,
  title,
  branchName,
  buildNumber,
  buildId,
  commitHash,
}) =&gt; (
  &lt;Zoom mountOnEnter unmountOnExit in={true}&gt;
    &lt;Card className={classes.card}&gt;
      &lt;CardHeader title={title} /&gt;
      &lt;CardContent className={classes.main}&gt;
        &lt;Typography variant=&quot;body1&quot; component=&quot;p&quot;&gt;
          &lt;b&gt;Build Number&lt;/b&gt; {buildNumber}
        &lt;/Typography&gt;
        &lt;Typography variant=&quot;body1&quot; component=&quot;p&quot;&gt;
          &lt;b&gt;Build Id&lt;/b&gt; {buildId}
        &lt;/Typography&gt;
        &lt;Typography variant=&quot;body1&quot; component=&quot;p&quot;&gt;
          &lt;b&gt;Branch Name&lt;/b&gt; {branchName}
        &lt;/Typography&gt;
        &lt;Typography variant=&quot;body1&quot; component=&quot;p&quot;&gt;
          &lt;b&gt;Commit Hash&lt;/b&gt; {commitHash}
        &lt;/Typography&gt;
      &lt;/CardContent&gt;
    &lt;/Card&gt;
  &lt;/Zoom&gt;
);

export default AboutPage;
</code></pre><p>When the above page is viewed it looks like this:</p><p><img src="../static/blog/2021-01-29-azure-pipelines-build-info-in-an-aspnet-react-app/about-page.png" alt="screenshot of our web app surfacing up the build information"/></p><p>And that&#x27;s it! Our app is clearly telling us what version is being run, both on the server and in the client. Thanks to Scott Hanselman for his work which inspired this.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Azure Easy Auth and Roles with .NET and Microsoft.Identity.Web]]></title>
            <link>https://blog.johnnyreilly.com/2021/01/17/azure-easy-auth-and-roles-with-net-and-microsoft-identity-web</link>
            <guid>Azure Easy Auth and Roles with .NET and Microsoft.Identity.Web</guid>
            <pubDate>Sun, 17 Jan 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[I wrote recently about how to get Azure Easy Auth to work with roles. This involved borrowing the approach used by MaximeRouiller.Azure.AppService.EasyAuth.]]></description>
            <content:encoded><![CDATA[<p><a href="./2021-01-14-azure-easy-auth-and-roles-with-dotnet-and-core.md">I wrote recently about how to get Azure Easy Auth to work with roles</a>. This involved borrowing the approach used by <a href="https://github.com/MaximRouiller/MaximeRouiller.Azure.AppService.EasyAuth">MaximeRouiller.Azure.AppService.EasyAuth</a>.</p><p>As a consequence of writing that post I came to learn that official support for Azure Easy Auth had landed in October 2020 in v1.2 of <a href="https://github.com/AzureAD/microsoft-identity-web/wiki/1.2.0#integration-with-azure-app-services-authentication-of-web-apps-running-with-microsoftidentityweb">Microsoft.Identity.Web</a>. This was great news; I was delighted.</p><p>However, it turns out that the same authorization issue that <code>MaximeRouiller.Azure.AppService.EasyAuth</code> suffers from, is visited upon <code>Microsoft.Identity.Web</code> as well.</p><h2>Getting set up</h2><p>We&#x27;re using a .NET 5 project, running in an Azure App Service (Linux). In our <code>.csproj</code> we have:</p><pre><code class="language-xml">&lt;PackageReference Include=&quot;Microsoft.Identity.Web&quot; Version=&quot;1.4.1&quot; /&gt;
</code></pre><p>In our <code>Startup.cs</code> we&#x27;re using:</p><pre><code class="language-cs">public void ConfigureServices(IServiceCollection services) {
    //...
    services.AddMicrosoftIdentityWebAppAuthentication(Configuration);
    //...
}

public void Configure(IApplicationBuilder app, IWebHostEnvironment env) {
    //...
    app.UseAuthentication();
    app.UseAuthorization();
    //...
}
</code></pre><h2>You gotta <code>roles</code> with it</h2><p>Whilst the authentication works, authorization does not. So whilst my app knows who I am - the authorization is not working with relation to <strong>roles</strong>.</p><p>When directly using <code>Microsoft.Identity.Web</code> when running locally, we see these claims:</p><pre><code class="language-json">[
  // ...
  {
    &quot;type&quot;: &quot;http://schemas.microsoft.com/ws/2008/06/identity/claims/role&quot;,
    &quot;value&quot;: &quot;Administrator&quot;
  },
  {
    &quot;type&quot;: &quot;http://schemas.microsoft.com/ws/2008/06/identity/claims/role&quot;,
    &quot;value&quot;: &quot;Reader&quot;
  }
  // ...
]
</code></pre><p>However, we get different behaviour with EasyAuth; it provides roles related claims with a <strong>different type</strong>:</p><pre><code class="language-json">[
  // ...
  {
    &quot;type&quot;: &quot;roles&quot;,
    &quot;value&quot;: &quot;Administrator&quot;
  },
  {
    &quot;type&quot;: &quot;roles&quot;,
    &quot;value&quot;: &quot;Reader&quot;
  }
  // ...
]
</code></pre><p>This means that roles related authorization <em>does not work</em> with Easy Auth:</p><pre><code class="language-cs">[Authorize(Roles = &quot;Reader&quot;)]
[HttpGet(&quot;api/reader&quot;)]
public string GetWithReader() =&gt;
    &quot;this is a secure endpoint that users with the Reader role can access&quot;;
</code></pre><p>This is because .NET is looking for claims with a <code>type</code> of <code>&quot;http://schemas.microsoft.com/ws/2008/06/identity/claims/role&quot;</code> and not finding them with Easy Auth.</p><h2>Claims transformation FTW</h2><p>There is a way to work around this issue .NET using <code>IClaimsTransformation</code>. This is a poorly documented feature, but fortunately <a href="https://gunnarpeipman.com/aspnet-core-adding-claims-to-existing-identity/">Gunnar Peipman&#x27;s blog does a grand job of explaining it</a>.</p><p>Inside our <code>Startup.cs</code> I&#x27;ve registered a claims transformer:</p><pre><code class="language-cs">services.AddScoped&lt;IClaimsTransformation, AddRolesClaimsTransformation&gt;();
</code></pre><p>And that claims transformer looks like this:</p><pre><code class="language-cs">public class AddRolesClaimsTransformation : IClaimsTransformation {
    private readonly ILogger&lt;AddRolesClaimsTransformation&gt; _logger;

    public AddRolesClaimsTransformation(ILogger&lt;AddRolesClaimsTransformation&gt; logger) {
        _logger = logger;
    }

    public Task&lt;ClaimsPrincipal&gt; TransformAsync(ClaimsPrincipal principal) {
        var mappedRolesClaims = principal.Claims
            .Where(claim =&gt; claim.Type == &quot;roles&quot;)
            .Select(claim =&gt; new Claim(ClaimTypes.Role, claim.Value))
            .ToList();

        // Clone current identity
        var clone = principal.Clone();

        if (clone.Identity is not ClaimsIdentity newIdentity) return Task.FromResult(principal);

        // Add role claims to cloned identity
        foreach (var mappedRoleClaim in mappedRolesClaims)
            newIdentity.AddClaim(mappedRoleClaim);

        if (mappedRolesClaims.Count &gt; 0)
            _logger.LogInformation(&quot;Added roles claims {mappedRolesClaims}&quot;, mappedRolesClaims);
        else
            _logger.LogInformation(&quot;No roles claims added&quot;);

        return Task.FromResult(clone);
    }
}
</code></pre><p>The class above creates a new principal with <code>&quot;roles&quot;</code> claims mapped across to <code>&quot;http://schemas.microsoft.com/ws/2008/06/identity/claims/role&quot;</code>. This is enough to get .NET treating roles the way you&#x27;d hope.</p><p><a href="https://github.com/AzureAD/microsoft-identity-web/issues/881">I&#x27;ve raised an issue against the <code>Microsoft.Identity.Web</code> repo</a> about this. Perhaps one day this workaround will no longer be necessary.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Azure Easy Auth and Roles with .NET (and .NET Core)]]></title>
            <link>https://blog.johnnyreilly.com/2021/01/14/azure-easy-auth-and-roles-with-dotnet-and-core</link>
            <guid>Azure Easy Auth and Roles with .NET (and .NET Core)</guid>
            <pubDate>Thu, 14 Jan 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[If this post is interesting to you, you may also want to look at this one where we try to use Microsoft.Identity.Web for the same purpose.]]></description>
            <content:encoded><![CDATA[<p><em>If this post is interesting to you, you may also want to <a href="./2021-01-17-azure-easy-auth-and-roles-with-net-and-microsoft-identity-web.md">look at this one where we try to use Microsoft.Identity.Web for the same purpose.</a></em></p><p>Azure has a feature which is intended to allow Authentication and Authorization to be applied outside of your application code. It&#x27;s called <a href="https://docs.microsoft.com/en-us/azure/app-service/overview-authentication-authorization">&quot;Easy Auth&quot;</a>. Unfortunately, in the context of App Services it doesn&#x27;t work with .NET Core and .NET. Perhaps it would be better to say: of the various .NETs, it supports .NET Framework. <a href="https://docs.microsoft.com/en-us/azure/app-service/overview-authentication-authorization#userapplication-claims">To quote the docs</a>:</p><blockquote><p>At this time, ASP.NET Core does not currently support populating the current user with the Authentication/Authorization feature. However, some <a href="https://github.com/MaximRouiller/MaximeRouiller.Azure.AppService.EasyAuth">3rd party, open source middleware components</a> do exist to help fill this gap.</p></blockquote><p>Thanks to <a href="https://twitter.com/MaximRouiller">Maxime Rouiller</a> there&#x27;s a way forward here. However, as I was taking this for a spin today, I discovered another issue.</p><h2>Where are our roles?</h2><p>Consider the following .NET controller:</p><pre><code class="language-cs">[Authorize(Roles = &quot;Administrator,Reader&quot;)]
[HttpGet(&quot;api/admin-reader&quot;)]
public string GetWithAdminOrReader() =&gt;
    &quot;this is a secure endpoint that users with the Administrator or Reader role can access&quot;;

[Authorize(Roles = &quot;Administrator&quot;)]
[HttpGet(&quot;api/admin&quot;)]
public string GetWithAdmin() =&gt;
    &quot;this is a secure endpoint that users with the Administrator role can access&quot;;

[Authorize(Roles = &quot;Reader&quot;)]
[HttpGet(&quot;api/reader&quot;)]
public string GetWithReader() =&gt;
    &quot;this is a secure endpoint that users with the Reader role can access&quot;;
</code></pre><p>The three endpoints above restrict access based upon roles. However, even with Maxime&#x27;s marvellous shim in the mix, authorization doesn&#x27;t work when deployed to an Azure App Service. Why? Well, it comes down to how roles are mapped to claims.</p><p>Let&#x27;s back up a bit. First of all we&#x27;ve added a dependency to our project:</p><pre><code class="language-shell">dotnet add package MaximeRouiller.Azure.AppService.EasyAuth
</code></pre><p>Next we&#x27;ve updated our <code>Startup.ConfigureServices</code> such that it looks like this:</p><pre><code class="language-cs">if (Env.IsDevelopment()) {
    services.AddMicrosoftIdentityWebAppAuthentication(Configuration);
else
    services.AddAuthentication(&quot;EasyAuth&quot;).AddEasyAuthAuthentication((o) =&gt; { });
</code></pre><p>With the above in place, either the Microsoft Identity platform will directly be used for authentication, or Maxime&#x27;s package will be used as the default authentication scheme. The driver for this is <code>Env</code> which is an <code>IHostEnvironment</code> that was injected to the <code>Startup.cs</code>. Running locally, both authentication and authorization will work. However, deployed to an Azure App Service, only authentication will work.</p><p>It turns out that directly using the Microsoft Identity platform, we see roles claims coming through like so:</p><pre><code class="language-json">[
  // ...
  {
    &quot;type&quot;: &quot;http://schemas.microsoft.com/ws/2008/06/identity/claims/role&quot;,
    &quot;value&quot;: &quot;Administrator&quot;
  },
  {
    &quot;type&quot;: &quot;http://schemas.microsoft.com/ws/2008/06/identity/claims/role&quot;,
    &quot;value&quot;: &quot;Reader&quot;
  }
  // ...
]
</code></pre><p>But in Azure we see roles claims showing up with a different <code>type</code>:</p><pre><code class="language-json">[
  // ...
  {
    &quot;type&quot;: &quot;roles&quot;,
    &quot;value&quot;: &quot;Administrator&quot;
  },
  {
    &quot;type&quot;: &quot;roles&quot;,
    &quot;value&quot;: &quot;Reader&quot;
  }
  // ...
]
</code></pre><p>This is the crux of the problem; .NET and .NET Core are looking in a different place for roles.</p><h2>Role up, role up!</h2><p>There wasn&#x27;t an obvious way to make this work with Maxime&#x27;s package. So we ended up lifting the source code of Maxime&#x27;s package and tweaking it. Take a look:</p><pre><code class="language-cs">using Microsoft.AspNetCore.Authentication;
using Microsoft.Extensions.DependencyInjection;
using Microsoft.Extensions.Logging;
using Microsoft.Extensions.Options;
using System;
using System.Collections.Generic;
using System.Linq;
using System.Security.Claims;
using System.Text.Encodings.Web;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;

/// &lt;summary&gt;
/// Based on https://github.com/MaximRouiller/MaximeRouiller.Azure.AppService.EasyAuth
/// Essentially EasyAuth only supports .NET Framework: https://docs.microsoft.com/en-us/azure/app-service/app-service-authentication-how-to#access-user-claims
/// This allows us to get support for Authentication and Authorization (using roles) with .NET
/// &lt;/summary&gt;
namespace EasyAuth {
    public static class EasyAuthAuthenticationBuilderExtensions {
        public static AuthenticationBuilder AddEasyAuthAuthentication(
            this IServiceCollection services) =&gt;
            services.AddAuthentication(&quot;EasyAuth&quot;).AddEasyAuthAuthenticationScheme(o =&gt; { });

        public static AuthenticationBuilder AddEasyAuthAuthenticationScheme(
            this AuthenticationBuilder builder,
            Action&lt;EasyAuthAuthenticationOptions&gt; configure) =&gt;
                builder.AddScheme&lt;EasyAuthAuthenticationOptions, EasyAuthAuthenticationHandler&gt;(
                    &quot;EasyAuth&quot;,
                    &quot;EasyAuth&quot;,
                    configure);
    }

    public class EasyAuthAuthenticationOptions : AuthenticationSchemeOptions {
        public EasyAuthAuthenticationOptions() {
            Events = new object();
        }
    }

    public class EasyAuthAuthenticationHandler : AuthenticationHandler&lt;EasyAuthAuthenticationOptions&gt; {
        public EasyAuthAuthenticationHandler(
            IOptionsMonitor&lt;EasyAuthAuthenticationOptions&gt; options,
            ILoggerFactory logger,
            UrlEncoder encoder,
            ISystemClock clock)
            : base(options, logger, encoder, clock) {
        }

        protected override Task&lt;AuthenticateResult&gt; HandleAuthenticateAsync() {
            try {
                var easyAuthEnabled = string.Equals(Environment.GetEnvironmentVariable(&quot;WEBSITE_AUTH_ENABLED&quot;, EnvironmentVariableTarget.Process), &quot;True&quot;, StringComparison.InvariantCultureIgnoreCase);
                if (!easyAuthEnabled) return Task.FromResult(AuthenticateResult.NoResult());

                var easyAuthProvider = Context.Request.Headers[&quot;X-MS-CLIENT-PRINCIPAL-IDP&quot;].FirstOrDefault();
                var msClientPrincipalEncoded = Context.Request.Headers[&quot;X-MS-CLIENT-PRINCIPAL&quot;].FirstOrDefault();
                if (string.IsNullOrWhiteSpace(easyAuthProvider) ||
                    string.IsNullOrWhiteSpace(msClientPrincipalEncoded))
                    return Task.FromResult(AuthenticateResult.NoResult());

                var decodedBytes = Convert.FromBase64String(msClientPrincipalEncoded);
                var msClientPrincipalDecoded = System.Text.Encoding.Default.GetString(decodedBytes);
                var clientPrincipal = JsonSerializer.Deserialize&lt;MsClientPrincipal&gt;(msClientPrincipalDecoded);
                if (clientPrincipal == null) return Task.FromResult(AuthenticateResult.NoResult());

                var mappedRolesClaims = clientPrincipal.Claims
                    .Where(claim =&gt; claim.Type == &quot;roles&quot;)
                    .Select(claim =&gt; new Claim(ClaimTypes.Role, claim.Value))
                    .ToList();

                var claims = clientPrincipal.Claims.Select(claim =&gt; new Claim(claim.Type, claim.Value)).ToList();
                claims.AddRange(mappedRolesClaims);

                var principal = new ClaimsPrincipal();
                principal.AddIdentity(new ClaimsIdentity(claims, clientPrincipal.AuthenticationType, clientPrincipal.NameType, clientPrincipal.RoleType));

                var ticket = new AuthenticationTicket(principal, easyAuthProvider);
                var success = AuthenticateResult.Success(ticket);
                Context.User = principal;

                return Task.FromResult(success);
            } catch (Exception ex) {
                return Task.FromResult(AuthenticateResult.Fail(ex));
            }
        }
    }

    public class MsClientPrincipal {
        [JsonPropertyName(&quot;auth_typ&quot;)]
        public string? AuthenticationType { get; set; }
        [JsonPropertyName(&quot;claims&quot;)]
        public IEnumerable&lt;UserClaim&gt; Claims { get; set; } = Array.Empty&lt;UserClaim&gt;();
        [JsonPropertyName(&quot;name_typ&quot;)]
        public string? NameType { get; set; }
        [JsonPropertyName(&quot;role_typ&quot;)]
        public string? RoleType { get; set; }
    }

    public class UserClaim {
        [JsonPropertyName(&quot;typ&quot;)]
        public string Type { get; set; } = string.Empty;
        [JsonPropertyName(&quot;val&quot;)]
        public string Value { get; set; } = string.Empty;
    }
}
</code></pre><p>There&#x27;s a number of changes in the above code to Maxime&#x27;s package. Three changes that are not significant and one that is. First the insignificant changes:</p><ol><li>It uses <a href="https://docs.microsoft.com/en-us/dotnet/standard/serialization/system-text-json-how-to?pivots=dotnet-5-0"><code>System.Text.Json</code></a> in place of JSON.NET</li><li>It uses <a href="./2020-12-20-nullable-reference-types-csharp-strictnullchecks.md">C#s nullable reference types</a></li><li>It changes the extension method signature such that instead of entering <code>services.AddAuthentication().AddEasyAuthAuthentication((o) =&gt; { })</code> we now need only enter <code>services.AddEasyAuthAuthentication()</code></li></ol><p>Now the significant change:</p><p>Where the middleware encounters claims in the <code>X-MS-CLIENT-PRINCIPAL</code> header with the <code>Type</code> of <code>&quot;roles&quot;</code> it creates brand new claims for each, with the same <code>Value</code> but with the official <code>Type</code> supplied by <code>ClaimsTypes.Role</code> of <code>&quot;http://schemas.microsoft.com/ws/2008/06/identity/claims/role&quot;</code>. The upshot of this, is that when the processed claims are inspected in Azure they now look more like this:</p><pre><code class="language-json">[
  // ...
  {
    &quot;type&quot;: &quot;roles&quot;,
    &quot;value&quot;: &quot;Administrator&quot;
  },
  {
    &quot;type&quot;: &quot;roles&quot;,
    &quot;value&quot;: &quot;Reader&quot;
  },
  // ...
  {
    &quot;type&quot;: &quot;http://schemas.microsoft.com/ws/2008/06/identity/claims/role&quot;,
    &quot;value&quot;: &quot;Administrator&quot;
  },
  {
    &quot;type&quot;: &quot;http://schemas.microsoft.com/ws/2008/06/identity/claims/role&quot;,
    &quot;value&quot;: &quot;Reader&quot;
  }
]
</code></pre><p>As you can see, we now have both the originally supplied roles <em>as well</em> as roles of the type that .NET and .NET Core expect. Consequently, roles based behaviour starts to work. Thanks to Maxime for his fine work on the initial solution. It would be tremendous if neither the code in this blog post nor Maxime&#x27;s shim were required. Still, until that glorious day!</p><h2>Update: Potential ways forward</h2><p>When I was tweeting this post, Maxime was good enough to respond and suggest that this may be resolved within Azure itself in future:</p><blockquote><p>Oh, so that&#x27;s why they removed the name? 😲😜 Jokes aside, we hope that this package won&#x27;t be necessary for the future. I know that <a href="https://twitter.com/mattchenderson?ref_src=twsrc%5Etfw">@mattchenderson</a> is part of a working group to update Easy Auth. Might want to make sure you follow him as well. 😁</p><p>— Maxime Rouiller (@MaximRouiller) <a href="https://twitter.com/MaximRouiller/status/1349804324713615366?ref_src=twsrc%5Etfw">January 14, 2021</a></p></blockquote><p>There&#x27;s a prospective PR that would add an event to Maxime&#x27;s API. If something along these lines was merged, then my workaround would no longer be necessary. Follow the PR <a href="https://github.com/MaximRouiller/MaximeRouiller.Azure.AppService.EasyAuth/pull/13">here</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[react-query: strongly typing useQueries]]></title>
            <link>https://blog.johnnyreilly.com/2021/01/03/strongly-typing-react-query-s-usequeries</link>
            <guid>react-query: strongly typing useQueries</guid>
            <pubDate>Sun, 03 Jan 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[react-query has a weakly typed hook named useQueries. It's possible to turn that into a strong typed hook; this post shows you how.]]></description>
            <content:encoded><![CDATA[<p><code>react-query</code> has a weakly typed hook named <code>useQueries</code>. It&#x27;s possible to turn that into a strong typed hook; this post shows you how.</p><p><img src="../static/blog/2021-01-03-strongly-typing-react-query-s-usequeries/strongly-typing-usequeries.png" alt="title image that says &quot;react-query: strongly typings useQueries&quot;"/></p><p>If you haven&#x27;t used <a href="https://react-query.tanstack.com/"><code>react-query</code></a> then I heartily recommend it. It provides (to quote the docs):</p><blockquote><p>Hooks for fetching, caching and updating asynchronous data in React</p></blockquote><p>With version 3 of <code>react-query</code>, a new hook was added: <a href="https://react-query.tanstack.com/reference/useQueries"><code>useQueries</code></a>. This hook allows you fetch a variable number of queries at the same time. An example of what usage looks like is this (<a href="https://react-query.tanstack.com/guides/parallel-queries#dynamic-parallel-queries-with-usequeries">borrowed from the excellent docs</a>):</p><pre><code class="language-tsx">function App({ users }) {
  const userQueries = useQueries(
    users.map((user) =&gt; {
      return {
        queryKey: [&#x27;user&#x27;, user.id],
        queryFn: () =&gt; fetchUserById(user.id),
      };
    })
  );
}
</code></pre><p>Whilst <code>react-query</code> is written in TypeScript, the way that <code>useQueries</code> is presently written strips the types that are supplied to it. Consider <a href="https://github.com/tannerlinsley/react-query/blob/d25ab3ef8260ea1c02b52b7082c3ce4120596b31/src/react/useQueries.ts#L8">the signature of the <code>useQueries</code></a>:</p><pre><code class="language-ts">export function useQueries(queries: UseQueryOptions[]): UseQueryResult[] {
</code></pre><p>This returns an array of <a href="https://github.com/tannerlinsley/react-query/blob/d25ab3ef8260ea1c02b52b7082c3ce4120596b31/src/react/types.ts#L42"><code>UseQueryResult</code></a>:</p><pre><code class="language-ts">export type UseQueryResult&lt;
  TData = unknown,
  TError = unknown
&gt; = UseBaseQueryResult&lt;TData, TError&gt;;
</code></pre><p>As you can see, no type parameters are passed to <code>UseQueryResult</code> in the <code>useQueries</code> signature and so it takes the default types of <code>unknown</code>. This forces the consumer to either assert the type that they believe to be there, or to use type narrowing to ensure the type. The former approach exposes a possibility of errors (the user can specify incorrect types) and the latter approach requires our code to perform type narrowing operations which are essentially unnecessary (the type hasn&#x27;t changed since it was returned; it&#x27;s simply been discarded).</p><p>What if there was a way to strongly type <code>useQueries</code> so we neither risked specifying incorrect types, nor wasted precious lines of code and CPU cycles performing type narrowing? There is my friends, read on!</p><h2><code>useQueriesTyped</code> - a strongly typed wrapper for <code>useQueries</code></h2><p>It&#x27;s possible to wrap the <code>useQueries</code> hook with our own <code>useQueriesTyped</code> hook which exposes a strongly typed API. It looks like this:</p><pre><code class="language-ts">import { useQueries, UseQueryOptions, UseQueryResult } from &#x27;react-query&#x27;;

type Awaited&lt;T&gt; = T extends PromiseLike&lt;infer U&gt; ? Awaited&lt;U&gt; : T;

export function useQueriesTyped&lt;TQueries extends readonly UseQueryOptions[]&gt;(
  queries: [...TQueries]
): {
  [ArrayElement in keyof TQueries]: UseQueryResult&lt;
    TQueries[ArrayElement] extends { select: infer TSelect }
      ? TSelect extends (data: any) =&gt; any
        ? ReturnType&lt;TSelect&gt;
        : never
      : Awaited&lt;
          ReturnType&lt;
            NonNullable&lt;
              Extract&lt;TQueries[ArrayElement], UseQueryOptions&gt;[&#x27;queryFn&#x27;]
            &gt;
          &gt;
        &gt;
  &gt;;
} {
  // eslint-disable-next-line @typescript-eslint/no-explicit-any
  return useQueries(
    queries as UseQueryOptions&lt;unknown, unknown, unknown&gt;[]
  ) as any;
}
</code></pre><p>Let&#x27;s unpack this. The first and most significant thing to note here is that <code>queries</code> moves from being <code>UseQueryOptions[]</code> to being <code>TQueries extends readonly UseQueryOptions[]</code> <!-- -->-<!-- --> far more fancy! The reason for this change is we want the type parameters to flow through on an element by element basis in the supplied array. <a href="https://www.typescriptlang.org/docs/handbook/release-notes/typescript-4-0.html#variadic-tuple-types">TypeScript 4&#x27;s variadic tuple types</a> should allow us to support this. So the new array signature looks like this:</p><pre><code class="language-ts">queries: [...TQueries];
</code></pre><p>Where <code>TQueries</code> is</p><pre><code class="language-ts">TQueries extends readonly UseQueryOptions[]
</code></pre><p>What this means is, that each element of the rest parameters array must have a type of <code>readonly UseQueryOptions</code>. Otherwise the compiler will shout at us (and rightly so).</p><p>So that&#x27;s what&#x27;s coming in.... What&#x27;s going out? Well the return type of <code>useQueriesTyped</code> is the tremendously verbose:</p><pre><code class="language-ts">{
  [ArrayElement in keyof TQueries]: UseQueryResult&lt;
    TQueries[ArrayElement] extends { select: infer TSelect }
      ? TSelect extends (data: any) =&gt; any
        ? ReturnType&lt;TSelect&gt;
        : never
      : Awaited&lt;
          ReturnType&lt;
            NonNullable&lt;
              Extract&lt;TQueries[ArrayElement], UseQueryOptions&gt;[&#x27;queryFn&#x27;]
            &gt;
          &gt;
        &gt;
  &gt;
}
</code></pre><p>Let&#x27;s walk this through. First of all we&#x27;ll look at this bit:</p><pre><code class="language-ts">{ [ArrayElement in keyof TQueries]: /* the type has been stripped to protect your eyes */ }
</code></pre><p>On the face of it, it looks like we&#x27;re returning an <code>Object</code>, not an <code>Array</code>. There&#x27;s nuance here; <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array">JavaScript <code>Array</code>s are <code>Object</code>s</a>.</p><p>More specifically, by approaching the signature this way, we can acquire the <code>ArrayElement</code> type which represents each of the keys of the array. Consider this array:</p><pre><code class="language-ts">[1, &#x27;two&#x27;, new Date()];
</code></pre><p>For the above, <code>ArrayElement</code> would take the values <code>0</code>, <code>1</code> and <code>2</code>. And this is going to prove useful in a moment as we&#x27;re going to index into our <code>TQueries</code> object to surface up the return types for each element of our return array from there.</p><p>Now let&#x27;s look at the return type for each element. The signature of that looks like this:</p><pre><code class="language-ts">UseQueryResult&lt;
    TQueries[ArrayElement] extends { select: infer TSelect }
      ? TSelect extends (data: any) =&gt; any
        ? ReturnType&lt;TSelect&gt;
        : never
      : Awaited&lt;
          ReturnType&lt;
            NonNullable&lt;
              Extract&lt;TQueries[ArrayElement], UseQueryOptions&gt;[&#x27;queryFn&#x27;]
            &gt;
          &gt;
        &gt;
  &gt;
</code></pre><p>Gosh... Well there&#x27;s a lot going on here. Let&#x27;s start in the middle and work our way out.</p><pre><code class="language-ts">TQueries[ArrayElement];
</code></pre><p>The above code indexes into our <code>TQueries</code> array for each element of our strongly typed indexer <code>ArrayElement</code>. So it might resolve the first element of an array to <code>{ queryKey: &#x27;key1&#x27;, queryFn: () =&amp;gt; 1 }</code>, for example. Next:</p><pre><code class="language-ts">Extract &lt; TQueries[ArrayElement], UseQueryOptions &gt; [&#x27;queryFn&#x27;];
</code></pre><p>We&#x27;re now taking the type of each element provided, and grabbing the type of the <code>queryFn</code> property. It&#x27;s this type which contains the type of the data that will be passed back, that we want to make use of. So for an examples of <code>[{ queryKey: &#x27;key1&#x27;, queryFn: () =&amp;gt; 1 }, { queryKey: &#x27;key2&#x27;, queryFn: () =&amp;gt; &#x27;two&#x27; }, { queryKey: &#x27;key3&#x27;, queryFn: () =&amp;gt; new Date() }]</code> we&#x27;d have the type: <code>const result: [() =&amp;gt; number, () =&amp;gt; string, () =&amp;gt; Date]</code>.</p><pre><code class="language-ts">NonNullable&lt;Extract&lt;TQueries[ArrayElement], UseQueryOptions&gt;[&#x27;queryFn&#x27;]&gt;
</code></pre><p>The next stage is using <code>NonNullable</code> on our <code>queryFn</code>, given that on <code>UseQueryOptions</code> it&#x27;s an optional type. In our use case it is not optional / nullable and so we need to enforce that.</p><pre><code class="language-ts">ReturnType&lt;NonNullable&lt;Extract&lt;TQueries[ArrayElement], UseQueryOptions&gt;[&#x27;queryFn&#x27;]&gt;&gt;
</code></pre><p>Now we want to get the return type of our <code>queryFn</code> <!-- -->-<!-- --> as that&#x27;s the data type we&#x27;re interested. So we use TypeScript&#x27;s <code>ReturnType</code> for that.</p><pre><code class="language-ts">ReturnType&lt;NonNullable&lt;Extract&lt;TQueries[ArrayElement], UseQueryOptions&gt;[&#x27;queryFn&#x27;]&gt;&gt;
</code></pre><p>Here we&#x27;re using <a href="https://devblogs.microsoft.com/typescript/announcing-typescript-4-1/#recursive-conditional-types">TypeScript 4.1&#x27;s recursive conditional types</a> to unwrap a <code>Promise</code> (or not) to the relevant type. This allows us to get the actual type we&#x27;re interested in, as opposed to the <code>Promise</code> of that type. Finally we have the type we need! So we can do this:</p><pre><code class="language-ts">type Awaited&lt;T&gt; = T extends PromiseLike&lt;infer U&gt; ? Awaited&lt;U&gt; : T;

Awaited&lt;ReturnType&lt;NonNullable&lt;Extract&lt;TQueries[ArrayElement], UseQueryOptions&gt;[&#x27;queryFn&#x27;]&gt;&gt;&gt;
</code></pre><p>It&#x27;s at this point where we reach a conditional type in our type definition. Essentially, we have two different typing behaviours in play:</p><ol><li>Where we&#x27;re inferring the return type of the query</li><li>Where we&#x27;re inferring the return type of a <code>select</code>. A <code>select</code> option can be used to transform or select a part of the data returned by the query function. It has the signature: <code>select: (data: TData) =&gt; TSelect</code></li></ol><p>We&#x27;ve been unpacking the first of these so far. Now we encounter the conditional type that chooses between them:</p><pre><code class="language-ts">TQueries[ArrayElement] extends { select: infer TSelect }
      ? TSelect extends (data: any) =&gt; any
        ? ReturnType&lt;TSelect&gt;
        : never
      : Awaited&lt; /*...*/ &gt;
  &gt;
</code></pre><p>What&#x27;s happening here is:</p><ul><li>if a query includes a <code>select</code> option, we infer what that is and then subsequently extract the return type of the <code>select</code>.</li><li>otherwise we use the query return type (as we we&#x27;ve previously examined)</li></ul><p>Finally, whichever type we end up with, we supply that type as a parameter to <code>UseQueryResult</code>. And that is what is going to surface up our types to our users.</p><h2>Usage</h2><p>So what does using our <code>useQueriesTyped</code> hook look like?</p><p>Well, supplying <code>queryFn</code>s with different signatures looks like this:</p><pre><code class="language-ts">const result = useQueriesTyped(
  { queryKey: &#x27;key1&#x27;, queryFn: () =&gt; 1 },
  { queryKey: &#x27;key2&#x27;, queryFn: () =&gt; &#x27;two&#x27; }
);
// const result: [QueryObserverResult&lt;number, unknown&gt;, QueryObserverResult&lt;string, unknown&gt;]

if (result[0].data) {
  // number
}
if (result[1].data) {
  // string
}
</code></pre><p>As you can see, we&#x27;re being returned a <code>Tuple</code> and the exact types are flowing through.</p><p>Next let&#x27;s look at a <code>.map</code> example with identical types in our supplied array:</p><pre><code class="language-ts">const resultWithAllTheSameTypes = useQueriesTyped(
  ...[1, 2].map((x) =&gt; ({ queryKey: `${x}`, queryFn: () =&gt; x }))
);
// const resultWithAllTheSameTypes: QueryObserverResult&lt;number, unknown&gt;[]

if (resultWithAllTheSameTypes[0].data) {
  // number
}
</code></pre><p>The return type of <code>number</code> is flowing through for each element.</p><p>Finally let&#x27;s look at how <code>.map</code> handles arrays with different types of elements:</p><pre><code class="language-ts">const resultWithDifferentTypes = useQueriesTyped(
  ...[1, &#x27;two&#x27;, new Date()].map((x) =&gt; ({ queryKey: `${x}`, queryFn: () =&gt; x }))
);
//const resultWithDifferentTypes: QueryObserverResult&lt;string | number | Date, unknown&gt;[]

if (resultWithDifferentTypes[0].data) {
  // string | number | Date
}

if (resultWithDifferentTypes[1].data) {
  // string | number | Date
}

if (resultWithDifferentTypes[2].data) {
  // string | number | Date
}
</code></pre><p>Admittedly this last example is a somewhat unlikely scenario. But again we can see the types flowing through - though further narrowing would be required here to get to the exact type.</p><h2>In the box?</h2><p>It&#x27;s great that we can wrap <code>useQueries</code> to get a strongly typed experience. It would be tremendous if this functionality was available by default. <a href="https://github.com/tannerlinsley/react-query/pull/1527">There&#x27;s a discussion going on around this</a>. It&#x27;s possible that this wrapper may no longer need to exist, and that would be amazing. In the meantime; enjoy!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Create React App with ts-loader and CRACO]]></title>
            <link>https://blog.johnnyreilly.com/2021/01/02/create-react-app-with-ts-loader-and-craco</link>
            <guid>Create React App with ts-loader and CRACO</guid>
            <pubDate>Sat, 02 Jan 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Create React App is a fantastic way to get up and running building a web app with React. It also supports using TypeScript with React. Simply entering the following:]]></description>
            <content:encoded><![CDATA[<p><a href="https://create-react-app.dev/">Create React App</a> is a fantastic way to get up and running building a web app with React. It also supports using TypeScript with React. Simply entering the following:</p><pre><code class="language-shell">npx create-react-app my-app --template typescript
</code></pre><p>Will give you a great TypeScript React project to get building with. There&#x27;s two parts to the TypeScript support that exist:</p><ol><li>Transpilation AKA &quot;turning our TypeScript into JavaScript&quot;. Back since <a href="https://devblogs.microsoft.com/typescript/typescript-and-babel-7/">Babel 7 launched, Babel has enjoyed great support for transpiling TypeScript into JavaScript</a>. Create React App leverages this; using the Babel webpack loader, <a href="https://github.com/babel/babel-loader">babel-loader</a>, for transpilation.</li><li>Type checking AKA &quot;seeing if our code compiles&quot;. Create React App uses the <a href="https://github.com/TypeStrong/fork-ts-checker-webpack-plugin"><code>fork-ts-checker-webpack-plugin</code></a> to run the TypeScript type checker on a separate process and report any issues that may exist.</li></ol><p>This is a great setup and works very well for the majority of use cases. However, what if we&#x27;d like to tweak this setup? What if we&#x27;d like to swap out <code>babel-loader</code> for <code>ts-loader</code> for compilation purposes? Can we do that?</p><p>Yes you can! And that&#x27;s what we&#x27;re going to do using a tool named <a href="https://github.com/gsoft-inc/craco"><code>CRACO</code></a> <!-- -->-<!-- --> the pithy shortening of &quot;Create React App Configuration Override&quot;. This is a tool that allows us to:</p><blockquote><p>Get all the benefits of create-react-app and customization without using &#x27;eject&#x27; by adding a single <code>craco.config.js</code> file at the root of your application and customize your eslint, babel, postcss configurations and many more.</p></blockquote><h2><del><code>babel-loader</code></del> <code>ts-loader</code></h2><p>So let&#x27;s do the swap. First of all we&#x27;re going to need to add <code>CRACO</code> and <code>ts-loader</code> to our project:</p><pre><code class="language-shell">npm install @craco/craco ts-loader --save-dev
</code></pre><p>Then we&#x27;ll swap over our various <code>scripts</code> in our <code>package.json</code> to use <code>CRACO</code>:</p><pre><code class="language-json">&quot;start&quot;: &quot;craco start&quot;,
&quot;build&quot;: &quot;craco build&quot;,
&quot;test&quot;: &quot;craco test&quot;,
</code></pre><p>Finally we&#x27;ll add a <code>craco.config.js</code> file to the root of our project. This is where we swap out <code>babel-loader</code> for <code>ts-loader</code>:</p><pre><code class="language-js">const {
  addAfterLoader,
  removeLoaders,
  loaderByName,
  getLoaders,
  throwUnexpectedConfigError,
} = require(&#x27;@craco/craco&#x27;);

const throwError = (message) =&gt;
  throwUnexpectedConfigError({
    packageName: &#x27;craco&#x27;,
    githubRepo: &#x27;gsoft-inc/craco&#x27;,
    message,
    githubIssueQuery: &#x27;webpack&#x27;,
  });

module.exports = {
  webpack: {
    configure: (webpackConfig, { paths }) =&gt; {
      const { hasFoundAny, matches } = getLoaders(
        webpackConfig,
        loaderByName(&#x27;babel-loader&#x27;)
      );
      if (!hasFoundAny) throwError(&#x27;failed to find babel-loader&#x27;);

      console.log(&#x27;removing babel-loader&#x27;);
      const { hasRemovedAny, removedCount } = removeLoaders(
        webpackConfig,
        loaderByName(&#x27;babel-loader&#x27;)
      );
      if (!hasRemovedAny) throwError(&#x27;no babel-loader to remove&#x27;);
      if (removedCount !== 2)
        throwError(&#x27;had expected to remove 2 babel loader instances&#x27;);

      console.log(&#x27;adding ts-loader&#x27;);

      const tsLoader = {
        test: /\.(js|mjs|jsx|ts|tsx)$/,
        include: paths.appSrc,
        loader: require.resolve(&#x27;ts-loader&#x27;),
        options: { transpileOnly: true },
      };

      const { isAdded: tsLoaderIsAdded } = addAfterLoader(
        webpackConfig,
        loaderByName(&#x27;url-loader&#x27;),
        tsLoader
      );
      if (!tsLoaderIsAdded) throwError(&#x27;failed to add ts-loader&#x27;);
      console.log(&#x27;added ts-loader&#x27;);

      console.log(&#x27;adding non-application JS babel-loader back&#x27;);
      const { isAdded: babelLoaderIsAdded } = addAfterLoader(
        webpackConfig,
        loaderByName(&#x27;ts-loader&#x27;),
        matches[1].loader // babel-loader
      );
      if (!babelLoaderIsAdded)
        throwError(&#x27;failed to add back babel-loader for non-application JS&#x27;);
      console.log(&#x27;added non-application JS babel-loader back&#x27;);

      return webpackConfig;
    },
  },
};
</code></pre><p>So what&#x27;s happening here? The script looks for <code>babel-loader</code> usages in the default Create React App config. There will be two; one for TypeScript / JavaScript application code (we want to replace this) and one for non application JavaScript code. I&#x27;m actually not too clear what non application JavaScript code there is or can be, but we&#x27;ll leave it in place; it may be important.</p><p>You cannot remove a <em>single</em> loader using <code>CRACO</code>, so instead we&#x27;ll remove both and we&#x27;ll add back the non application JavaScript <code>babel-loader</code>. We&#x27;ll also add <code>ts-loader</code> with the <code>transpileOnly: true</code> option set (to ensure <code>ts-loader</code> doesn&#x27;t do type checking).</p><p>Now the next time we run <code>npm start</code> we&#x27;ll have Create React App running using <code>ts-loader</code> and <em>without</em> having ejected. If we want to adjust the options of <code>ts-loader</code> further then we&#x27;re completely at liberty to do so, adjusting the <code>options</code> in our <code>craco.config.js</code>.</p><p>If you value debugging your original source code rather than the transpiled JavaScript, remember to set the <code>&quot;sourceMap&quot;: true</code> property in your <code>tsconfig.json</code>.</p><p>Finally, if we wanted to go even further, we could remove the <code>fork-ts-checker-webpack-plugin</code> and move <code>ts-loader</code> to use <code>transpileOnly: false</code> so it performs type checking also. However, generally it may be better to stay with the setup with post outlines for performance reasons.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Azure Pipelines meet Jest]]></title>
            <link>https://blog.johnnyreilly.com/2020/12/30/azure-pipelines-meet-jest</link>
            <guid>Azure Pipelines meet Jest</guid>
            <pubDate>Wed, 30 Dec 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[This post explains how to integrate the tremendous test runner Jest with the continuous integration platform Azure Pipelines. Perhaps we're setting up a new project and we've created a new React app with Create React App. This ships with Jest support out of the box. How do we get that plugged into Pipelines such that:]]></description>
            <content:encoded><![CDATA[<p>This post explains how to integrate the tremendous test runner <a href="https://jestjs.io/">Jest</a> with the continuous integration platform <a href="https://azure.microsoft.com/en-gb/services/devops/pipelines/?nav=min">Azure Pipelines</a>. Perhaps we&#x27;re setting up a new project and we&#x27;ve created a new React app with <a href="https://create-react-app.dev/">Create React App</a>. This ships with Jest support out of the box. How do we get that plugged into Pipelines such that:</p><ol><li>Tests run as part of our pipeline</li><li>A failing test fails the build</li><li>Test results are reported in Azure Pipelines UI?</li></ol><h2>Tests run as part of our pipeline</h2><p>First of all, lets get the tests running. Crack open your <code>azure-pipelines.yml</code> file and, in the appropriate place add the following:</p><pre><code class="language-yml">- task: Npm@1
  displayName: npm run test
  inputs:
    command: &#x27;custom&#x27;
    workingDir: &#x27;src/client-app&#x27;
    customCommand: &#x27;run test&#x27;
</code></pre><p>The above will, when run, trigger a <code>npm run test</code> in the <code>src/client-app</code> folder of my project (it&#x27;s here where my React app lives). You&#x27;d imagine this would just work™️ - but life is not that simple. This is because Jest, by default, runs in watch mode. This is blocking and so not appropriate for CI.</p><p>In our <code>src/client-app/package.json</code> let&#x27;s create a new script that runs the tests but <em>not</em> in watch mode:</p><pre><code class="language-json">&quot;test:ci&quot;: &quot;npm run test -- --watchAll=false&quot;,
</code></pre><p>and switch our <code>azure-pipelines.yml</code> to use it:</p><pre><code class="language-yml">- task: Npm@1
  displayName: npm run test
  inputs:
    command: &#x27;custom&#x27;
    workingDir: &#x27;src/client-app&#x27;
    customCommand: &#x27;run test:ci&#x27;
</code></pre><p>Boom! We&#x27;re now running tests as part of our pipeline. And also, failing tests will fail the build, because of Jest&#x27;s default behaviour of exiting with status code 1 on failed tests.</p><h2>Tests results are reported in Azure Pipelines UI</h2><p>Pipelines has a really nice UI for reporting test results. If you&#x27;re using something like .NET then you&#x27;ll find that test results just magically show up there. We&#x27;d like that for our Jest tests as well. And we can have it.</p><p>The way we achieve this is by:</p><ol><li>Producing test results in a format that can be subsequently processed</li><li>Using those test results to publish to Azure Pipelines</li></ol><p>The way that you configure Jest test output is through usage of <a href="https://jestjs.io/docs/en/cli#--reporters"><code>reporters</code></a>. However, Create React App doesn&#x27;t support these. However that&#x27;s not an issue, as the marvellous <a href="https://twitter.com/dan_abramov">Dan Abramov</a> demonstrates <a href="https://github.com/facebook/create-react-app/issues/2474#issuecomment-306340526">here</a>.</p><p>We need to install the <a href="https://github.com/jest-community/jest-junit"><code>jest-junit</code></a> package to our <code>client-app</code>:</p><pre><code>npm install jest-junit --save-dev
</code></pre><p>And we&#x27;ll tweak our <code>test:ci</code> script to use the <code>jest-junit</code> reporter as well:</p><pre><code class="language-json">&quot;test:ci&quot;: &quot;npm run test -- --watchAll=false --reporters=default --reporters=jest-junit&quot;,
</code></pre><p>We also need to add some configuration to our <code>package.json</code> in the form of a <code>jest-junit</code> element:</p><pre><code class="language-json">&quot;jest-junit&quot;: {
        &quot;suiteNameTemplate&quot;: &quot;{filepath}&quot;,
        &quot;outputDirectory&quot;: &quot;.&quot;,
        &quot;outputName&quot;: &quot;junit.xml&quot;
    }
</code></pre><p>The above configuration will use the name of the test file as the suite name in the results, which should speed up the tracking down of the failing test. The other values specify where the test results should be published to, in this case the root of our <code>client-app</code> with the filename <code>junit.xml</code>.</p><p>Now our CI is producing our test results, how do we get them into Pipelines? For that we need the <a href="https://docs.microsoft.com/en-us/azure/devops/pipelines/tasks/test/publish-test-results?view=azure-devops&amp;tabs=trx%2Cyaml">Publish test results task</a> and a new step in our <code>azure-pipelines.yml</code> <em>after</em> our <code>npm run test</code> step:</p><pre><code class="language-yml">- task: Npm@1
  displayName: npm run test
  inputs:
    command: &#x27;custom&#x27;
    workingDir: &#x27;src/client-app&#x27;
    customCommand: &#x27;run test:ci&#x27;

- task: PublishTestResults@2
  displayName: &#x27;supply npm test results to pipelines&#x27;
  condition: succeededOrFailed() # because otherwise we won&#x27;t know what tests failed
  inputs:
    testResultsFiles: &#x27;src/client-app/junit.xml&#x27;
</code></pre><p>This will read the test results from our <code>src/client-app/junit.xml</code> file and pump them into Pipelines. Do note that we&#x27;re <em>always</em> running this step; so if the previous step failed (as it would in the case of a failing test) we still pump out the details of what that failure was. Like so:</p><p><img src="../static/blog/2020-12-30-azure-pipelines-meet-jest/test-and-publish-steps.png" alt="screenshot of test results being published to Azure Pipelines regardless of passing or failing tests"/></p><p>And that&#x27;s it! Azure Pipelines and Jest integrated.</p><p><img src="../static/blog/2020-12-30-azure-pipelines-meet-jest/test-results.png" alt="screenshot of test results published to Azure Pipelines"/></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[dotnet-format: Prettier your CSharp with lint-staged and husky]]></title>
            <link>https://blog.johnnyreilly.com/2020/12/22/prettier-your-csharp-with-dotnet-format-and-lint-staged</link>
            <guid>dotnet-format: Prettier your CSharp with lint-staged and husky</guid>
            <pubDate>Tue, 22 Dec 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[Consistent formatting in a codebase is a good thing. We can achieve this in dotnet using dotnet format, used in combination with the npm packages husky and lint-staged. This post shows how.]]></description>
            <content:encoded><![CDATA[<p>Consistent formatting in a codebase is a good thing. We can achieve this in dotnet using <code>dotnet format</code>, used in combination with the npm packages <code>husky</code> and <code>lint-staged</code>. This post shows how.</p><p><img src="../static/blog/2020-12-22-prettier-your-csharp-with-dotnet-format-and-lint-staged/title-image.png" alt="title image reading &quot;dotnet-format: Prettier your CSharp with lint-staged and husky&quot; and the dotnet-format logo"/></p><h2>Update 17/09/2021</h2><p>This has been updated to work with the latest versions of <code>lint-staged</code> and <code>husky</code>.</p><h2>Why format?</h2><p>Consistent formatting makes code less confusing to newcomers and it allows whoever is working on the codebase to reliably focus on the task at hand. Not &quot;fixing curly braces because Janice messed them up with her last commit&quot;. (A <code>git commit</code> message that would be tragic in so many ways.)</p><p>Once we&#x27;ve agreed that we want to have consistent formatting, we want it to be enforced. Enter, stage left, <a href="https://prettier.io/">Prettier</a>, the fantastic tool for formatting code. It rocks; I&#x27;ve been using on my JavaScript / TypeScript for the longest time. But what about C#? Well, there is a <a href="https://github.com/warrenseine/prettier-plugin-csharp">Prettier plugin for C#</a>.... Sort of. It appears to be abandoned and contains the worrying message in the <code>README.md</code>:</p><blockquote><p>Please note that this plugin is under active development, and might not be ready to run on production code yet. It will break your code.</p></blockquote><p>Not a ringing endorsement.</p><h2><code>dotnet-format</code>: a new hope</h2><p><a href="https://twitter.com/margaridagp">Margarida Pereira</a> recently pointed me in the direction of <a href="https://github.com/dotnet/format"><code>dotnet-format</code></a> which is a formatter for .NET. It&#x27;s a .NET tool which:</p><blockquote><p>is a code formatter for dotnet that applies style preferences to a project or solution. Preferences will be read from an <code>.editorconfig</code> file, if present, otherwise a default set of preferences will be used.</p></blockquote><p>It can be installed with:</p><pre><code class="language-shell">dotnet tool install -g dotnet-format
</code></pre><p>The <a href="https://github.com/dotnet/format/issues/648#issuecomment-614905524">VS Code C# extension will make use of this formatter</a>, we just need to set the following in our <code>settings.json</code>:</p><pre><code class="language-json">&quot;omnisharp.enableRoslynAnalyzers&quot;: true,
&quot;omnisharp.enableEditorConfigSupport&quot;: true
</code></pre><h2>Customising our formatting</h2><p>If we&#x27;d like to deviate from the <a href="https://docs.microsoft.com/en-us/dotnet/fundamentals/code-analysis/code-style-rule-options">default formatting options</a> then create ourselves an <code>.editorconfig</code> file in the root of our project. Let&#x27;s say we prefer more of the <a href="https://en.wikipedia.org/wiki/Indentation_style#K&amp;R_style">K &amp; R style</a> approach to braces instead of the C# default of <a href="https://en.wikipedia.org/wiki/Indentation_style#Allman_style">Allman style</a>. To make <code>dotnet-format</code> use that we&#x27;d set the following:</p><pre><code class="language-ini"># Remove the line below if you want to inherit .editorconfig settings from higher directories
root = true

# See https://github.com/dotnet/format/blob/master/docs/Supported-.editorconfig-options.md for reference
[*.cs]
csharp_new_line_before_open_brace = none
csharp_new_line_before_catch = false
csharp_new_line_before_else = false
csharp_new_line_before_finally = false
csharp_new_line_before_members_in_anonymous_types = false
csharp_new_line_before_members_in_object_initializers = false
csharp_new_line_between_query_expression_clauses = true
</code></pre><p>With this in place it&#x27;s K &amp; R all the way baby!</p><h2><code>lint-staged</code> / <code>husky</code> integration</h2><p>It&#x27;s become somewhat standard to use the marvellous <a href="https://github.com/typicode/husky"><code>husky</code></a> and <a href="https://github.com/okonet/lint-staged"><code>lint-staged</code></a> to enforce code quality. To quote the docs:</p><blockquote><p>Run linters against staged git files and don&#x27;t let 💩 slip into our code base!</p></blockquote><p>To add this to our (otherwise C# codebase), we&#x27;re going to need a <code>package.json</code> file:</p><pre><code class="language-sh">npm init --yes
</code></pre><p>We&#x27;ll install <code>husky</code> and <code>lint-staged</code>:</p><pre><code class="language-sh">npx husky-init &amp;&amp; npm install
npm install lint-staged --save-dev
</code></pre><p>We should have a new file living at <code>.husky/pre-commit</code> which is our pre-commit hook.</p><p>Within that file we should replace <code>npm test</code> with <code>npx lint-staged --relative</code>. This is the command that will be run on commit. <code>lint-staged</code> will be run and we&#x27;re specifying <code>relative</code> so that <strong>relative</strong> file paths will be used. This is important as <code>dotnet format</code>&#x27;s <code>--include</code> accepts &quot;a list of relative file or folder paths to include in formatting&quot;. <strong>Absolute paths (the default) won&#x27;t work - and if we pass them to <code>dotnet format</code>, it will not format the files.</strong></p><p>Finally we add the following entry to the <code>package.json</code>:</p><pre><code class="language-json">  &quot;lint-staged&quot;: {
    &quot;*.cs&quot;: &quot;dotnet format --include&quot;
  }
</code></pre><p>This is the task that will be invoked by <code>lint-staged</code> against files with a <code>.cs</code> suffix on commit. When <code>lint-staged</code> runs, it will pass a list of relative file paths to <code>dotnet format</code>. So if we&#x27;d staged two files it might end up executing a command like this:</p><p><code>dotnet format --include src/server-app/Server/Controllers/UserController.cs src/server-app/Server/Controllers/WeatherForecastController.cs</code></p><p>We should end up with a <code>package.json</code> that looks something like this:</p><pre><code class="language-json">{
  &quot;name&quot;: &quot;app&quot;,
  &quot;version&quot;: &quot;1.0.0&quot;,
  &quot;description&quot;: &quot;[![Shared Build Status](https://dev.azure.com/investec/maas/_apis/build/status/shared?repoName=maas)](https://dev.azure.com/investec/maas/_build/latest?definitionId=1128&amp;repoName=maas)&quot;,
  &quot;main&quot;: &quot;index.js&quot;,
  &quot;dependencies&quot;: {
    &quot;husky&quot;: &quot;^7.0.2&quot;
  },
  &quot;devDependencies&quot;: {
    &quot;husky&quot;: &quot;^7.0.0&quot;,
    &quot;lint-staged&quot;: &quot;^11.1.2&quot;
  },
  &quot;scripts&quot;: {
    &quot;test&quot;: &quot;echo \&quot;Error: no test specified\&quot; &amp;&amp; exit 1&quot;,
    &quot;prepare&quot;: &quot;husky install&quot;
  },
  &quot;lint-staged&quot;: {
    &quot;*.cs&quot;: &quot;dotnet format --include&quot;
  },
  &quot;repository&quot;: {
    &quot;type&quot;: &quot;git&quot;,
    &quot;url&quot;: &quot;https://investec@dev.azure.com/investec/maas/_git/maas&quot;
  },
  &quot;keywords&quot;: [],
  &quot;author&quot;: &quot;&quot;,
  &quot;license&quot;: &quot;ISC&quot;
}
</code></pre><p>By and large we don&#x27;t have to think about this; the important take home is that we&#x27;re now enforcing standardised formatting for all C# files upon commit. Everything that goes into the codebase will be formatted in a consistent fashion.</p><h2>CSharpier - update 16/05/2021</h2><p>There is an alternative to the CSharp Prettier project. It&#x27;s being worked on by
<a href="https://github.com/belav">Bela VanderVoort</a> and it goes by the name of <a href="https://github.com/belav/csharpier">csharpier</a>. When comparing CSharpier and dotnet-format, Bela put it like this:</p><blockquote><p>I could see CSharpier being the non-configurable super opinionated formatter and dotnet-format being for the people that do want to have options.</p></blockquote><p>Check it out!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Make Microsoft.Identity.Web respond with 403 forbidden instead of a 302 redirect]]></title>
            <link>https://blog.johnnyreilly.com/2020/12/21/how-to-make-azure-ad-403</link>
            <guid>Make Microsoft.Identity.Web respond with 403 forbidden instead of a 302 redirect</guid>
            <pubDate>Mon, 21 Dec 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[By default Microsoft.Identity.Web responds to unauthorized requests with a 302 (redirect). Do you want a 403 (forbidden) instead? Here's how.]]></description>
            <content:encoded><![CDATA[<p>By default <code>Microsoft.Identity.Web</code> responds to unauthorized requests with a 302 (redirect). Do you want a 403 (forbidden) instead? Here&#x27;s how.</p><p>If you&#x27;re using the tremendous <a href="https://docs.microsoft.com/en-us/azure/active-directory/develop/scenario-web-app-sign-user-app-configuration?tabs=aspnetcore">Azure Active Directory for authentication with ASP.NET</a> then there&#x27;s a good chance you&#x27;re using the <a href="https://github.com/AzureAD/microsoft-identity-web"><code>Microsoft.Identity.Web</code></a> library. It&#x27;s this that allows us to drop the following statement into the <code>ConfigureServices</code> method of our <code>Startup</code> class:</p><pre><code class="language-cs">services.AddMicrosoftIdentityWebAppAuthentication(Configuration);
</code></pre><p>Which (combined with configuration in our <code>appsettings.json</code> files) hooks us up with Azure AD for authentication. This is 95% awesome. The 5% is what we&#x27;re here for. Here&#x27;s a screenshot of the scenario that troubles us:</p><p><img src="../static/blog/2020-12-21-how-to-make-azure-ad-403/AccessDenied.png" alt="a screenshot of Chrome Devtools showing a 302"/></p><p>We&#x27;ve made a request to <code>/WeatherForecast</code>; a secured endpoint (a controller decorated with the <code>Authorize</code> attribute). We&#x27;re authenticated; the app knows who we are. But we&#x27;re not authorized / allowed to access this endpoint. We don&#x27;t have permission. The HTTP specification caters directly for this scenario with <a href="https://tools.ietf.org/html/rfc7231#section-6.5.3">status code <code>403 Forbidden</code></a>:</p><blockquote><p>The 403 (Forbidden) status code indicates that the server understood the request but refuses to authorize it.</p></blockquote><p>However, <code>Microsoft.Identity.Web</code> is ploughing another furrow. Instead of returning <code>403</code>, it&#x27;s returning <code>302 Found</code> and redirecting the browser to <code>https://localhost:5001/Account/AccessDenied?ReturnUrl=%2FWeatherForecast</code>. Now the intentions here are <em>great</em>. If you wanted to implement a page in your application at that endpoint that displayed some kind of useful message it would be really useful. However, what if you want the more HTTP-y behaviour instead? In the case of a HTTP request triggered by JavaScript (typical for Single Page Applications) then this redirect isn&#x27;t that helpful. JavaScript doesn&#x27;t really know what to do with the <code>302</code> and whilst you could code around this, it&#x27;s not desirable.</p><p>We want <code>403</code> - we don&#x27;t want <code>302</code>.</p><h2>Give us <code>403</code></h2><p>You can have this behaviour by dropping the following code after your <code>services.AddMicrosoftIdentityWebAppAuthentication</code>:</p><pre><code class="language-cs">services.Configure&lt;CookieAuthenticationOptions&gt;(CookieAuthenticationDefaults.AuthenticationScheme, options =&gt;
{
    options.Events.OnRedirectToAccessDenied = new Func&lt;RedirectContext&lt;CookieAuthenticationOptions&gt;, Task&gt;(context =&gt;
    {
        context.Response.StatusCode = StatusCodes.Status403Forbidden;
        return context.Response.CompleteAsync();
    });
});
</code></pre><p>This code hijacks the redirect to AccessDenied and transforms it into a <code>403</code> instead. Tremendous! What does this look like?</p><p><img src="../static/blog/2020-12-21-how-to-make-azure-ad-403/Forbidden.png" alt="a screenshot of Chrome Devtools showing a 403"/></p><p>This is the behaviour we want!</p><h2>Extra customisation bonus points</h2><p>You may want to have some nuance to the way you handle unauthorized requests. Because of the nature of <code>OnRedirectToAccessDenied</code> this is entirely possible; you have complete access to the requests coming in which you can use to direct behaviour. To take a single example, let&#x27;s say we want to direct normal browsing behaviour (AKA humans clicking about in Chrome) which is not authorized to a given screen, otherwise provide <code>403</code>s. What would that look like?</p><pre><code class="language-cs">services.Configure&lt;CookieAuthenticationOptions&gt;(CookieAuthenticationDefaults.AuthenticationScheme, options =&gt;
{
    options.Events.OnRedirectToAccessDenied = new Func&lt;RedirectContext&lt;CookieAuthenticationOptions&gt;, Task&gt;(context =&gt;
    {
        var isRequestForHtml = context.Request.Headers[&quot;Accept&quot;].ToString().Contains(&quot;text/html&quot;);
        if (isRequestForHtml) {
            context.Response.StatusCode = StatusCodes.Status302Found;
            context.Response.Headers[&quot;Location&quot;] = &quot;/unauthorized&quot;;
        }
        else {
            context.Response.StatusCode = StatusCodes.Status403Forbidden;
        }

        return context.Response.CompleteAsync();
    });
});
</code></pre><p>So above, we check the request <code>Accept</code> headers and see if they contain <code>&quot;text/html&quot;</code>; which we&#x27;re using as a signal that the request came from a users browsing. (This may not be bulletproof; better suggestions gratefully received.) If the request does contain a <code>&quot;text/html&quot;``Accept</code> header then we redirect the client to an <code>/unauthorized</code> screen, otherwise we return <code>403</code> as we did before. Super flexible and powerful!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Nullable reference types; CSharp's very own strictNullChecks]]></title>
            <link>https://blog.johnnyreilly.com/2020/12/20/nullable-reference-types-csharp-strictnullchecks</link>
            <guid>Nullable reference types; CSharp's very own strictNullChecks</guid>
            <pubDate>Sun, 20 Dec 2020 00:00:00 GMT</pubDate>
            <description><![CDATA['Tis the season to play with new compiler settings! I'm a very keen TypeScript user and have been merrily using strictNullChecks since it shipped. I was dimly aware that C# was also getting a similar feature by the name of nullable reference types.]]></description>
            <content:encoded><![CDATA[<p>&#x27;Tis the season to play with new compiler settings! I&#x27;m a very keen TypeScript user and have been merrily using <a href="https://www.typescriptlang.org/docs/handbook/release-notes/typescript-2-0.html#--strictnullchecks"><code>strictNullChecks</code></a> since it shipped. I was dimly aware that C# was also getting a similar feature by the name of <a href="https://docs.microsoft.com/en-us/dotnet/csharp/tutorials/nullable-reference-types">nullable reference types</a>.</p><p>It&#x27;s only now that I&#x27;ve got round to taking at look at this marvellous feature. I thought I&#x27;d share what moving to nullable reference types looked like for me; and what code changes I found myself making as a consequence.</p><h2>Turning on nullable reference types</h2><p>To turn on nullable reference types in a C# project you should pop open the <code>.csproj</code> file and ensure it contains a <code>&lt;Nullable&gt;enable&lt;/Nullable&gt;</code>. So if you had a .NET Core 3.1 codebase it might look like this:</p><pre><code class="language-xml">&lt;PropertyGroup&gt;
    &lt;TargetFramework&gt;netcoreapp3.1&lt;/TargetFramework&gt;
    &lt;Nullable&gt;enable&lt;/Nullable&gt;
&lt;/PropertyGroup&gt;
</code></pre><p>When you compile from this point forward, possible null reference types are reported as warnings. Consider this C#:</p><pre><code class="language-cs">[ApiController]
public class UserController : ControllerBase
{
    private readonly ILogger&lt;UserController&gt; _logger;

    public UserController(ILogger&lt;UserController&gt; logger)
    {
        _logger = logger;
    }

    [AllowAnonymous]
    [HttpGet(&quot;UserName&quot;)]
    public string GetUserName()
    {
        if (User.Identity.IsAuthenticated) {
            _logger.LogInformation(&quot;{User} is getting their username&quot;, User.Identity.Name);
            return User.Identity.Name;
        }

        _logger.LogInformation(&quot;The user is not authenticated&quot;);
        return null;
    }
}
</code></pre><p>A <code>dotnet build</code> results in this:</p><pre><code class="language-shell">dotnet build --configuration release

Microsoft (R) Build Engine version 16.7.1+52cd83677 for .NET
Copyright (C) Microsoft Corporation. All rights reserved.

  Determining projects to restore...
  Restored /Users/jreilly/code/app/src/server-app/Server/app.csproj (in 471 ms).
Controllers/UserController.cs(38,24): warning CS8603: Possible null reference return. [/Users/jreilly/code/app/src/server-app/Server/app.csproj]
Controllers/UserController.cs(42,20): warning CS8603: Possible null reference return. [/Users/jreilly/code/app/src/server-app/Server/app.csproj]
  app -&gt; /Users/jreilly/code/app/src/server-app/Server/bin/release/netcoreapp3.1/app.dll
  app -&gt; /Users/jreilly/code/app/src/server-app/Server/bin/release/netcoreapp3.1/app.Views.dll

Build succeeded.

Controllers/UserController.cs(38,24): warning CS8603: Possible null reference return. [/Users/jreilly/code/app/src/server-app/Server/app.csproj]
Controllers/UserController.cs(42,20): warning CS8603: Possible null reference return. [/Users/jreilly/code/app/src/server-app/Server/app.csproj]
    2 Warning(s)
    0 Error(s)
</code></pre><p>You see the two <code>&quot;Possible null reference return.&quot;</code> warnings? Bingo</p><h2>Really make it hurt</h2><p>This is good - information is being surfaced up. But it&#x27;s a warning. I could ignore it. I like compilers to get really up in my face and force me to make a change. I&#x27;m not into warnings; I&#x27;m into errors. Know what works for you. If you&#x27;re similarly minded, you can upgrade nullable reference warnings to errors by tweaking the <code>.csproj</code> a touch further. Add yourself a <code>&lt;WarningsAsErrors&gt;nullable&lt;/WarningsAsErrors&gt;</code> element. So maybe your <code>.csproj</code> now looks like this:</p><pre><code class="language-xml">&lt;PropertyGroup&gt;
    &lt;TargetFramework&gt;netcoreapp3.1&lt;/TargetFramework&gt;
    &lt;Nullable&gt;enable&lt;/Nullable&gt;
    &lt;WarningsAsErrors&gt;nullable&lt;/WarningsAsErrors&gt;
&lt;/PropertyGroup&gt;
</code></pre><p>And a <code>dotnet build</code> will result in this:</p><pre><code class="language-shell">dotnet build --configuration release

Microsoft (R) Build Engine version 16.7.1+52cd83677 for .NET
Copyright (C) Microsoft Corporation. All rights reserved.

  Determining projects to restore...
  Restored /Users/jreilly/code/app/src/server-app/Server/app.csproj (in 405 ms).
Controllers/UserController.cs(38,24): error CS8603: Possible null reference return. [/Users/jreilly/code/app/src/server-app/Server/app.csproj]
Controllers/UserController.cs(42,20): error CS8603: Possible null reference return. [/Users/jreilly/code/app/src/server-app/Server/app.csproj]

Build FAILED.

Controllers/UserController.cs(38,24): error CS8603: Possible null reference return. [/Users/jreilly/code/app/src/server-app/Server/app.csproj]
Controllers/UserController.cs(42,20): error CS8603: Possible null reference return. [/Users/jreilly/code/app/src/server-app/Server/app.csproj]
    0 Warning(s)
    2 Error(s)
</code></pre><p>Yay! Errors!</p><h2>What do they mean?</h2><p>&quot;<code>Possible null reference return</code>&quot; isn&#x27;t the clearest of errors. What does that actually amount to? Well, it amounts to the compiler saying &quot;you&#x27;re a liar! (maybe)&quot;. Let&#x27;s look again at the code where this error is reported:</p><pre><code class="language-cs">[AllowAnonymous]
[HttpGet(&quot;UserName&quot;)]
public string GetUserName()
{
    if (User.Identity.IsAuthenticated) {
        _logger.LogInformation(&quot;{User} is getting their username&quot;, User.Identity.Name);
        return User.Identity.Name;
    }

    _logger.LogInformation(&quot;The user is not authenticated&quot;);
    return null;
}
</code></pre><p>We&#x27;re getting that error reported where we&#x27;re returning <code>null</code> and where we&#x27;re returning <code>User.Identity.Name</code> which <em>may</em> be <code>null</code>. And we&#x27;re getting that because as far as the compiler is concerned <code>string</code> has changed. Before we turned on nullable reference types the compiler considered <code>string</code> to mean <code>string</code> <em>OR</em><code>null</code>. Now, <code>string</code> means <code>string</code>.</p><p>This is the same sort of behaviour as TypeScripts <code>strictNullChecks</code>. With TypeScript, before you turn on <code>strictNullChecks</code>, as far as the compiler is concerned, <code>string</code> means <code>string</code><em>OR</em><code>null</code><em>OR</em><code>undefined</code> (JavaScript didn&#x27;t feel one null-ish value was enough and so has two - don&#x27;t ask). Once <code>strictNullChecks</code> is on, <code>string</code> means <code>string</code>.</p><p>It&#x27;s a lot clearer. And that&#x27;s why the compiler is getting antsy. The method signature is <code>string</code>, but it can see <code>null</code> potentially being returned. It doesn&#x27;t like it. By and large that&#x27;s good. We want the compiler to notice this as that&#x27;s the entire point. We want to catch accidental <code>null</code>s before they hit a user. This is <em>great</em>! However, what do you do if have a method (as we do) that legitimately returns a <code>string</code> or <code>null</code>?</p><h2>Widening the type to include <code>null</code></h2><p>We change the signature from this:</p><pre><code class="language-cs">public string GetUserName()
</code></pre><p>To this:</p><pre><code class="language-cs">public string? GetUserName()
</code></pre><p>That&#x27;s right, the simple addition of <code>?</code> marks a reference type (like a string) as potentially being <code>null</code>. Adding that means that we&#x27;re potentially returning <code>null</code>, but we&#x27;re sure about it; there&#x27;s intention here - it&#x27;s not accidental. Wonderful!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[azure-pipelines-task-lib and isOutput setVariable]]></title>
            <link>https://blog.johnnyreilly.com/2020/12/09/azure-pipelines-task-lib-and-isoutput-setvariable</link>
            <guid>azure-pipelines-task-lib and isOutput setVariable</guid>
            <pubDate>Wed, 09 Dec 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[Some blog posts are insightful treatises on the future of web development, some are "here's how I solved my problem". This is most assuredly the latter.]]></description>
            <content:encoded><![CDATA[<p>Some blog posts are insightful treatises on the future of web development, some are &quot;here&#x27;s how I solved my problem&quot;. This is most assuredly the latter.</p><p>I&#x27;m writing an <a href="https://docs.microsoft.com/en-us/azure/devops/extend/develop/add-build-task?view=azure-devops">custom pipelines task extension for Azure Pipelines</a>. It&#x27;s written with TypeScript and the <a href="https://github.com/microsoft/azure-pipelines-task-lib">azure-pipelines-task-lib</a>.</p><p>The pipeline needs to output a variable. Azure Pipelines does that using the <code>setvariable</code> command combined with <a href="https://docs.microsoft.com/en-us/azure/devops/pipelines/process/variables?view=azure-devops&amp;tabs=yaml%2Cbatch#set-a-multi-job-output-variable">isOutput=true</a>. This looks something like this: <code>##vso[task.setvariable variable=myOutputVar;isOutput=true]this is the value&quot;</code>.</p><p>The bad news is that the lib <a href="https://github.com/microsoft/azure-pipelines-task-lib/issues/688">doesn&#x27;t presently support <code>isOutput=true</code></a>. Gosh it makes me sad. Hopefully in future it will be resolved. But what now?</p><p>For now we can hack ourselves a workaround:</p><pre><code class="language-ts">import * as tl from &#x27;azure-pipelines-task-lib/task&#x27;;
import * as tcm from &#x27;azure-pipelines-task-lib/taskcommand&#x27;;
import * as os from &#x27;os&#x27;;

/**
 * Sets a variable which will be output as well.
 *
 * @param     name    name of the variable to set
 * @param     val     value to set
 * @param     secret  whether variable is secret.  Multi-line secrets are not allowed.  Optional, defaults to false
 * @returns   void
 */
export function setOutputVariable(
  name: string,
  val: string,
  secret = false
): void {
  // use the implementation of setVariable to set all the internals,
  // then subsequently set the output variable manually
  tl.setVariable(name, val, secret);

  const varValue = val || &#x27;&#x27;;

  // write the command
  // see https://docs.microsoft.com/en-us/azure/devops/pipelines/process/variables?view=azure-devops&amp;tabs=yaml%2Cbatch#set-a-multi-job-output-variable
  _command(
    &#x27;task.setvariable&#x27;,
    {
      variable: name || &#x27;&#x27;,
      isOutput: &#x27;true&#x27;,
      issecret: (secret || false).toString(),
    },
    varValue
  );
}

const _outStream = process.stdout;

function _writeLine(str: string): void {
  _outStream.write(str + os.EOL);
}

function _command(command: string, properties: any, message: string) {
  const taskCmd = new tcm.TaskCommand(command, properties, message);
  _writeLine(taskCmd.toString());
}
</code></pre><p>The above is effectively a wrapper for the existing <a href="https://github.com/microsoft/azure-pipelines-task-lib/blob/90e9cde0e509cba77185a80ef3af2fc898fb026c/node/task.ts#L162"><code>setVariable</code></a>. However, once it&#x27;s called into the initial implementation, <code>setOutputVariable</code> then writes out the same variable once more, but this time bolting on <code>isOutput=true</code>.</p><p>Finally, I&#x27;ve raised a PR to see if <code>isOutput</code> can be added directly to the library. <a href="https://github.com/microsoft/azure-pipelines-task-lib/pull/691">You can track progress on that here.</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Visual Studio Marketplace: images in Markdown!]]></title>
            <link>https://blog.johnnyreilly.com/2020/11/28/images-in-markdown-for-azure-devops-marketplace</link>
            <guid>Visual Studio Marketplace: images in Markdown!</guid>
            <pubDate>Sat, 28 Nov 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[Publish your README.md and associated images to Visual Studio Marketplace.]]></description>
            <content:encoded><![CDATA[<p>I&#x27;ve recently found myself developing <a href="https://docs.microsoft.com/en-us/azure/devops/extend/develop/add-build-task?view=azure-devops">custom pipelines task extensions for Azure DevOps</a>. The extensions being developed end up in the <a href="https://marketplace.visualstudio.com/azuredevops">Azure DevOps Marketplace</a>. What you see there when you look at existing extensions is some pretty lovely documentation.</p><p><img src="../static/blog/2020-11-28-images-in-markdown-for-azure-devops-marketplace/azure-devops-marketplace.png" alt="screenshot of a rich Markdown powered screen with images in Visual Studio Marketplace"/></p><h2>How can our tasks look as lovely?</h2><p>That, my friends, is the question to answer. Good documentation is key to success. Here&#x27;s the ask: when a custom task is installed it becomes available in the marketplace, we want it to:</p><ul><li>contain documentation</li><li>that documentation should support images... For a picture, famously, speaks a thousand words</li></ul><h2>Mark(Down) our manifest</h2><p>To get documentation showing up in the marketplace, we need to take a look at the <code>vss-extension.json</code> file which lies at the root of our extension folder. It&#x27;s a kind of manifest file and is documented <a href="https://docs.microsoft.com/en-us/azure/devops/extend/develop/manifest?view=azure-devops">here</a>.</p><p><a href="https://docs.microsoft.com/en-us/azure/devops/extend/develop/manifest?view=azure-devops#discovery-attributes">Tucked away in the docs, you&#x27;ll find mention of a <code>content</code> property and the words:</a></p><blockquote><p>Dictionary of content files that describe your extension to users... Each file is assumed to be in <a href="https://help.github.com/articles/github-flavored-markdown/">GitHub Flavored Markdown format</a>. The path of each item is the path to the markdown file in the extension. Valid keys: <code>details</code>.</p></blockquote><p>This means we can have a Markdown file in our repo which documents our task. To stay consistent with most projects, a solid choice is to use the <code>README.md</code> that sits in the root of the project to this end.</p><p>So the simple addition of this:</p><pre><code class="language-json">{
  //...
  &quot;content&quot;: {
    &quot;details&quot;: {
      &quot;path&quot;: &quot;README.md&quot;
    }
  }
  //...
}
</code></pre><p>Gives us documentation in the marketplace. Yay!</p><h2>Now the images...</h2><p>If we are referencing images in our <code>README.md</code> then, as it stands right now, they won&#x27;t show up in the marketplace. It&#x27;ll be broken link city. Imagine some Markdown like this:</p><pre><code class="language-md">![alt text](images/screenshot.png)
</code></pre><p>This is entirely correct and supported, but won&#x27;t work by default. This is because these images need to be specified in the <a href="https://docs.microsoft.com/en-us/azure/devops/extend/develop/manifest?view=azure-devops#files"><code>files</code> property</a> of the <code>vss-extension.json</code>.</p><pre><code class="language-json">{
  //...
  &quot;content&quot;: {
    &quot;details&quot;: {
      &quot;path&quot;: &quot;README.md&quot;
    }
  },
  &quot;files&quot;: [
    {
      &quot;path&quot;: &quot;images&quot;,
      &quot;addressable&quot;: true
    }
  ]
  //...
}
</code></pre><p>Consider the above; the <code>path</code> of <code>images</code> includes everything inside the <code>images</code> folder in the task. However, it&#x27;s crucial that the <a href="https://docs.microsoft.com/en-us/azure/devops/extend/develop/manifest?view=azure-devops#properties-1"><code>&quot;addressable&quot;: true</code></a> is present as well. It&#x27;s this that makes the files in this <code>path</code> URL-addressable. And without that, the images won&#x27;t be displayed.</p><p>That&#x27;s it! We&#x27;re done! We can have rich, image inclusive, documentation in our custom tasks.</p><p>A final note: it&#x27;s possible to specify individual files rather than whole paths in the <code>files</code> directory and you might want to do that if you&#x27;re being very careful around file size. There is a maximum size for a custom task and it&#x27;s easy to breach it. But by and large I find that &quot;allowlisting&quot; a single directory is easier.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Bulletproof uniq with TypeScript generics (yay code reviews!)]]></title>
            <link>https://blog.johnnyreilly.com/2020/11/14/bulletproof-uniq-with-typescript</link>
            <guid>Bulletproof uniq with TypeScript generics (yay code reviews!)</guid>
            <pubDate>Sat, 14 Nov 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[Never neglect the possibilities of a code review. There are times when you raise a PR and all you want is for everyone to hit approve so you can merge, merge and ship, ship! This can be a missed opportunity. For as much as I'd like to imagine my code is perfect, it's patently not. There's always scope for improvement.]]></description>
            <content:encoded><![CDATA[<p>Never neglect the possibilities of a code review. There are times when you raise a PR and all you want is for everyone to hit approve so you can merge, merge and ship, ship! This can be a missed opportunity. For as much as I&#x27;d like to imagine my code is perfect, it&#x27;s patently not. There&#x27;s always scope for improvement.</p><h2>&quot;What&#x27;s this?&quot;</h2><p>This week afforded me that opportunity. I was walking through a somewhat complicated PR on a call and someone said &quot;what&#x27;s this?&quot;. They&#x27;d spotted an expression much like this in my code:</p><pre><code class="language-ts">const myValues = [...new Set(allTheValuesSupplied)];
</code></pre><p>What is that? Well, it&#x27;s a number of things:</p><ol><li><a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Set#Remove_duplicate_elements_from_the_array">It&#x27;s a way to get the unique values in a collection.</a></li><li>It&#x27;s a pro-tip and a coding BMX trick.</li></ol><p>What do I mean? Well, this is indeed a technique for getting the unique values in a collection. But it relies upon you knowing a bunch of things:</p><ul><li><a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Set"><code>Set</code></a> contains unique values. If you add multiple identical values, only a single value will be stored.</li><li>The <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Set/Set"><code>Set</code> constructor</a> takes <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Iteration_protocols#The_iterable_protocol">iterable objects</a>. This means we can <code>new</code> up a <code>Set</code> with an array that we want to &quot;unique-ify&quot; and we will have a <code>Set</code> that contains those unique values.</li><li>If you want to go on to do filtering / mapping etc on your unique values, you&#x27;ll need to get them out of the <code>Set</code>. This is because (regrettably) ECMAScript iterables don&#x27;t implicitly support these operations and neither are methods such as these part of the <code>Set</code> API. The easiest way to do that is to <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Spread_syntax">spread</a> into a new array which you can then operate upon.</li></ul><p>I have this knowledge. Lots of people have this knowledge. But whilst this may be the case, using this technique goes against what I would generally consider to be a good tenet of programming: comprehensibility. When you read this code above, it doesn&#x27;t immediately tell you what it&#x27;s doing. This is a strike against it.</p><p>Further to that, it&#x27;s &quot;noisy&quot;. Even if the reader does have this knowledge, as they digest the code, they have to mentally unravel it. &quot;Oh it&#x27;s a <code>Set</code>, we&#x27;re passing in values, then spreading it out, it&#x27;s probably intended to get the unique values.... Right, cool, cool.... Continue!&quot;</p><iframe src="https://giphy.com/embed/4NnSe87mg3h25JYIDh" width="100%" height="100%" frameBorder="0"></iframe><p><a href="https://twitter.com/margaridagp">Margarida Pereira</a> explicitly called this out and I found myself agreeing. Let&#x27;s make a <code>uniq</code> function!</p><h2><code>uniq</code> v1</h2><p>I wrote a very simple <code>uniq</code> function which looked like this:</p><pre><code class="language-ts">/**
 * Return the unique values found in the passed iterable
 */
function uniq&lt;TElement&gt;(iterableToGetUniqueValuesOf: Iterable&lt;TElement&gt;) {
  return [...new Set(iterableToGetUniqueValuesOf)];
}
</code></pre><p>Usage of this was simple:</p><pre><code class="language-ts">uniq([1, 1, 1, 3, 1, 1, 2]); // produces [1, 3, 2]
uniq([&#x27;John&#x27;, &#x27;Guida&#x27;, &#x27;Ollie&#x27;, &#x27;Divya&#x27;, &#x27;John&#x27;]); // produces [&quot;John&quot;, &quot;Guida&quot;, &quot;Ollie&quot;, &quot;Divya&quot;]
</code></pre><p>And I thought this was tremendous. I committed and pushed. I assumed there was no more to be done. Guida (Margarida) then made this very helpful comment:</p><blockquote><p>BTW, I found a big bold warning that <code>new Set()</code> compares objects by reference (unless they&#x27;re primitives) so it might be worth adding a comment to warn people that uniq/distinct compares objects by reference: <a href="https://codeburst.io/javascript-array-distinct-5edc93501dc4">https://codeburst.io/javascript-array-distinct-5edc93501dc4</a></p></blockquote><p>She was right! If a caller was to, say, pass a collection of objects to <code>uniq</code> then they&#x27;d end up highly disappointed. Consider:</p><pre><code class="language-ts">uniq([{ name: &#x27;John&#x27; }, { name: &#x27;John&#x27; }]); // produces [{ name: &quot;John&quot; }, { name: &quot;John&quot; }]
</code></pre><p>We can do better!</p><h2><code>uniq</code> v2</h2><p>I like compilers shouting at me. Or more accurately, I like compilers telling me when something isn&#x27;t valid / supported / correct. I wanted <code>uniq</code> to mirror the behaviour of <code>Set</code> <!-- -->-<!-- --> to only support primitives such as <code>string</code>, <code>number</code> etc. So I made a new version of <code>uniq</code> that hardened up the generic contraints:</p><pre><code class="language-ts">/**
 * Return the unique values found in the passed iterable
 */
function uniq&lt;TElement extends string | number | bigint | boolean | symbol&gt;(
  iterableToGetUniqueValuesOf: Iterable&lt;TElement&gt;
) {
  return [...new Set(iterableToGetUniqueValuesOf)];
}
</code></pre><p>With this in place, the compiler started shouting in the most helpful way. When I re-attemped <code>[{ name: &quot;John&quot; }, { name: &quot;John&quot; }]</code> the compiler hit me with:</p><p><code>Argument of type &#x27;{ name: string; }[]&#x27; is not assignable to parameter of type &#x27;Iterable&amp;lt;string | number | bigint | boolean | symbol&amp;gt;&#x27;.</code></p><p><a href="https://www.typescriptlang.org/play?#code/FAYw9gdgzmA2CmA6WYDmAKArhAlgR3QG0BvAAggEMBbeALlICIApMACwgdIF8AaUsyjXrM2HbgF0AlJNCQYCZGiy4ChEewZ91HKTOAB6AFSHgpQ6QBK8AC6YAThFLXW8UtnyZXANwqxPUUgAzMGwAE1IcR2dXAAcKKCh4cJxreDsKACMEU0N9YEDsEGscSDcVAB4AFQBRBBoIa1J4AA9UiFCAqGs7SNRSAB9yTCoMtIHSDJxUSMbBjLA4eApHQagATxG4AD50U1J9lLTMhEqwAHEbAFUVTwA1X38AeUD6AElU9Kz4Ktr4eustsBJPw9vs7DZ7I5CIgYRB4AB3UgAZRs6EOnxO5yuN3g9z88Cgz0k4gA3MAuMAgA">Take a look.</a></p><p>This is good. This is descriptive code that only allows legitimate inputs. It should lead to less confusion and a reduced likelihood of issues in Production. It&#x27;s also a nice example of how code review can result in demonstrably better code. Thanks Guida!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Throttling data requests with React Hooks]]></title>
            <link>https://blog.johnnyreilly.com/2020/11/10/throttle-data-requests-with-react-hooks</link>
            <guid>Throttling data requests with React Hooks</guid>
            <pubDate>Tue, 10 Nov 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[When an application loads data, typically relatively few HTTP requests will be made. For example, if we imagine we're making a student administration application, then a "view" screen might make a single HTTP request to load that student's data before displaying it.]]></description>
            <content:encoded><![CDATA[<p>When an application loads data, typically relatively few HTTP requests will be made. For example, if we imagine we&#x27;re making a student administration application, then a &quot;view&quot; screen might make a single HTTP request to load that student&#x27;s data before displaying it.</p><p>Occasionally there&#x27;s a need for an application to make a large number of HTTP requests. Consider a reporting application which loads data and then aggregates it for presentation purposes.</p><p>This need presents two interesting problems to solve:</p><ol><li>how do we load data gradually?</li><li>how do we present loading progress to users?</li></ol><p>This post will talk about how we can tackle these and demonstrate using a custom React Hook.</p><h2>Let&#x27;s bring Chrome to its knees</h2><p>We&#x27;ll begin our journey by spinning up a TypeScript React app with <a href="https://create-react-app.dev/">Create React App</a>:</p><pre><code class="language-shell">npx create-react-app throttle-requests-react-hook --template typescript
</code></pre><p>Because we&#x27;re going to be making a number of asynchronous calls, we&#x27;re going to simplify the code by leaning on the widely used <a href="https://github.com/streamich/react-use"><code>react-use</code></a> for a <a href="https://github.com/streamich/react-use/blob/master/docs/useAsync.md"><code>useAsync</code></a> hook.</p><pre><code class="language-shell">cd throttle-requests-react-hook
yarn add react-use
</code></pre><p>We&#x27;ll replace the <code>App.css</code> file with this:</p><pre><code class="language-css">.App {
  text-align: center;
}

.App-header {
  background-color: #282c34;
  min-height: 100vh;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  font-size: calc(10px + 2vmin);
  color: white;
}

.App-labelinput &gt; * {
  margin: 0.5em;
  font-size: 24px;
}

.App-link {
  color: #61dafb;
}

.App-button {
  font-size: calc(10px + 2vmin);
  margin-top: 0.5em;
  padding: 1em;
  background-color: cornflowerblue;
  color: #ffffff;
  text-align: center;
}

.App-progress {
  padding: 1em;
  background-color: cadetblue;
  color: #ffffff;
}

.App-results {
  display: flex;
  flex-wrap: wrap;
}

.App-results &gt; * {
  padding: 1em;
  margin: 0.5em;
  background-color: darkblue;
  flex: 1 1 300px;
}
</code></pre><p>Then we&#x27;ll replace the <code>App.tsx</code> contents with this:</p><pre><code class="language-tsx">import React, { useState } from &#x27;react&#x27;;
import { useAsync } from &#x27;react-use&#x27;;
import &#x27;./App.css&#x27;;

function use10_000Requests(startedAt: string) {
  const responses = useAsync(async () =&gt; {
    if (!startedAt) return;

    // make 10,000 unique HTTP requests
    const results = await Promise.all(
      Array.from(Array(10_000)).map(async (_, index) =&gt; {
        const response = await fetch(
          `/manifest.json?querystringValueToPreventCaching=${startedAt}_request-${index}`
        );
        const json = await response.json();
        return json;
      })
    );

    return results;
  }, [startedAt]);

  return responses;
}

function App() {
  const [startedAt, setStartedAt] = useState(&#x27;&#x27;);
  const responses = use10_000Requests(startedAt);

  return (
    &lt;div className=&quot;App&quot;&gt;
      &lt;header className=&quot;App-header&quot;&gt;
        &lt;h1&gt;The HTTP request machine&lt;/h1&gt;
        &lt;button
          className=&quot;App-button&quot;
          onClick={(_) =&gt; setStartedAt(new Date().toISOString())}
        &gt;
          Make 10,000 requests
        &lt;/button&gt;
        {responses.loading &amp;&amp; &lt;div&gt;{progressMessage}&lt;/div&gt;}
        {responses.error &amp;&amp; &lt;div&gt;Something went wrong&lt;/div&gt;}
        {responses.value &amp;&amp; (
          &lt;div className=&quot;App-results&quot;&gt;
            {responses.value.length} requests completed successfully
          &lt;/div&gt;
        )}
      &lt;/header&gt;
    &lt;/div&gt;
  );
}

export default App;
</code></pre><p>The app that we&#x27;ve built is very simple; it&#x27;s a button which, when you press it, fires 10,000 HTTP requests in parallel using the <a href="https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API">Fetch API</a>. The data being requested in this case is an arbitrary JSON file; the <code>manifest.json</code>. If you look closely you&#x27;ll see we&#x27;re doing some querystring tricks with our URL to avoid getting cached data.</p><p>In fact, for this demo we&#x27;re not interested in the results of these HTTP requests; rather we&#x27;re interested in how the browser copes with this approach. (Spoiler: not well!) It&#x27;s worth considering that requesting a text file from a server running on the same machine as the browser should be fast.</p><p>So we&#x27;ll run <code>yarn start</code> and go to <a href="http://localhost:3000">http://localhost:3000</a> to get to the app. Running with Devtools open results in the following unhappy affair:</p><p><img src="../static/blog/2020-11-10-throttle-data-requests-with-react-hooks/i-want-it-all.gif"/></p><p>The GIF above has been edited significantly for length. In reality it took 20 seconds for the first request to be fired, prior to that Chrome was unresponsive. When requests did start to fire, a significant number failed with <code>net::ERR_INSUFFICIENT_RESOURCES</code>. Further to that, those requests that were fired sat in &quot;Stalled&quot; state prior to being executed. This is a consequence of <a href="https://developers.google.com/web/tools/chrome-devtools/network/reference#timing">Chrome limiting the number of connections - all browsers do this</a>:</p><blockquote><p>There are already six TCP connections open for this origin, which is the limit. Applies to HTTP/1.0 and HTTP/1.1 only.</p></blockquote><p>In summary, the problems with the current approach are:</p><ol><li>the browser becoming unresponsive</li><li>failing HTTP requests due to insufficient resources</li><li>no information displayed to the user around progress</li></ol><h2>Throttle me this</h2><p>Instead of hammering the browser by firing all the requests at once, we could instead implement a throttle. A throttle is a mechanism which allows you to limit the rate at which operations are performed. In this case we want to limit the rate at which HTTP requests are made. A throttle will tackle problems 1 and 2 - essentially keeping the browser free and easy and ensuring that requests are all successfully sent. We also want to keep our users informed around how progress is going. It&#x27;s time to unveil the <code>useThrottleRequests</code> hook:</p><pre><code class="language-ts">import { useMemo, useReducer } from &#x27;react&#x27;;
import { AsyncState } from &#x27;react-use/lib/useAsync&#x27;;

/** Function which makes a request */
export type RequestToMake = () =&gt; Promise&lt;void&gt;;

/**
 * Given an array of requestsToMake and a limit on the number of max parallel requests
 * queue up those requests and start firing them
 * - inspired by Rafael Xavier&#x27;s approach here: https://stackoverflow.com/a/48007240/761388
 *
 * @param requestsToMake
 * @param maxParallelRequests the maximum number of requests to make - defaults to 6
 */
async function throttleRequests(
  requestsToMake: RequestToMake[],
  maxParallelRequests = 6
) {
  // queue up simultaneous calls
  const queue: Promise&lt;void&gt;[] = [];
  for (let requestToMake of requestsToMake) {
    // fire the async function, add its promise to the queue,
    // and remove it from queue when complete
    const promise = requestToMake().then((res) =&gt; {
      queue.splice(queue.indexOf(promise), 1);
      return res;
    });
    queue.push(promise);

    // if the number of queued requests matches our limit then
    // wait for one to finish before enqueueing more
    if (queue.length &gt;= maxParallelRequests) {
      await Promise.race(queue);
    }
  }
  // wait for the rest of the calls to finish
  await Promise.all(queue);
}

/**
 * The state that represents the progress in processing throttled requests
 */
export type ThrottledProgress&lt;TData&gt; = {
  /** the number of requests that will be made */
  totalRequests: number;
  /** the errors that came from failed requests */
  errors: Error[];
  /** the responses that came from successful requests */
  values: TData[];
  /** a value between 0 and 100 which represents the percentage of requests that have been completed (whether successfully or not) */
  percentageLoaded: number;
  /** whether the throttle is currently processing requests */
  loading: boolean;
};

function createThrottledProgress&lt;TData&gt;(
  totalRequests: number
): ThrottledProgress&lt;TData&gt; {
  return {
    totalRequests,
    percentageLoaded: 0,
    loading: false,
    errors: [],
    values: [],
  };
}

/**
 * A reducing function which takes the supplied `ThrottledProgress` and applies a new value to it
 */
function updateThrottledProgress&lt;TData&gt;(
  currentProgress: ThrottledProgress&lt;TData&gt;,
  newData: AsyncState&lt;TData&gt;
): ThrottledProgress&lt;TData&gt; {
  const errors = newData.error
    ? [...currentProgress.errors, newData.error]
    : currentProgress.errors;

  const values = newData.value
    ? [...currentProgress.values, newData.value]
    : currentProgress.values;

  const percentageLoaded =
    currentProgress.totalRequests === 0
      ? 0
      : Math.round(
          ((errors.length + values.length) / currentProgress.totalRequests) *
            100
        );

  const loading =
    currentProgress.totalRequests === 0
      ? false
      : errors.length + values.length &lt; currentProgress.totalRequests;

  return {
    totalRequests: currentProgress.totalRequests,
    loading,
    percentageLoaded,
    errors,
    values,
  };
}

type ThrottleActions&lt;TValue&gt; =
  | {
      type: &#x27;initialise&#x27;;
      totalRequests: number;
    }
  | {
      type: &#x27;requestSuccess&#x27;;
      value: TValue;
    }
  | {
      type: &#x27;requestFailed&#x27;;
      error: Error;
    };

/**
 * Create a ThrottleRequests and an updater
 */
export function useThrottleRequests&lt;TValue&gt;() {
  function reducer(
    throttledProgressAndState: ThrottledProgress&lt;TValue&gt;,
    action: ThrottleActions&lt;TValue&gt;
  ): ThrottledProgress&lt;TValue&gt; {
    switch (action.type) {
      case &#x27;initialise&#x27;:
        return createThrottledProgress(action.totalRequests);

      case &#x27;requestSuccess&#x27;:
        return updateThrottledProgress(throttledProgressAndState, {
          loading: false,
          value: action.value,
        });

      case &#x27;requestFailed&#x27;:
        return updateThrottledProgress(throttledProgressAndState, {
          loading: false,
          error: action.error,
        });
    }
  }

  const [throttle, dispatch] = useReducer(
    reducer,
    createThrottledProgress&lt;TValue&gt;(/** totalRequests */ 0)
  );

  const updateThrottle = useMemo(() =&gt; {
    /**
     * Update the throttle with a successful request
     * @param values from request
     */
    function requestSucceededWithData(value: TValue) {
      return dispatch({
        type: &#x27;requestSuccess&#x27;,
        value,
      });
    }

    /**
     * Update the throttle upon a failed request with an error message
     * @param error error
     */
    function requestFailedWithError(error: Error) {
      return dispatch({
        type: &#x27;requestFailed&#x27;,
        error,
      });
    }

    /**
     * Given an array of requestsToMake and a limit on the number of max parallel requests
     * queue up those requests and start firing them
     * - based upon https://stackoverflow.com/a/48007240/761388
     *
     * @param requestsToMake
     * @param maxParallelRequests the maximum number of requests to make - defaults to 6
     */
    function queueRequests(
      requestsToMake: RequestToMake[],
      maxParallelRequests = 6
    ) {
      dispatch({
        type: &#x27;initialise&#x27;,
        totalRequests: requestsToMake.length,
      });

      return throttleRequests(requestsToMake, maxParallelRequests);
    }

    return {
      queueRequests,
      requestSucceededWithData,
      requestFailedWithError,
    };
  }, [dispatch]);

  return {
    throttle,
    updateThrottle,
  };
}
</code></pre><p>The <code>useThrottleRequests</code> hook returns 2 properties:</p><ul><li><p><code>throttle</code> <!-- -->-<!-- --> a <code>ThrottledProgress&amp;lt;TData&amp;gt;</code> that contains the following data:</p><ul><li><code>totalRequests</code> <!-- -->-<!-- --> the number of requests that will be made</li><li><code>errors</code> <!-- -->-<!-- --> the errors that came from failed requests</li><li><code>values</code> <!-- -->-<!-- --> the responses that came from successful requests</li><li><code>percentageLoaded</code> <!-- -->-<!-- --> a value between 0 and 100 which represents the percentage of requests that have been completed (whether successfully or not)</li><li><code>loading</code> <!-- -->-<!-- --> whether the throttle is currently processing requests</li></ul></li><li><p><code>updateThrottle</code> <!-- -->-<!-- --> an object which exposes 3 functions:</p><ul><li><code>queueRequests</code> <!-- -->-<!-- --> the function to which you pass the requests that should be queued and executed in a throttled fashion</li><li><code>requestSucceededWithData</code> <!-- -->-<!-- --> the function which is called if a request succeeds to provide the data</li><li><code>requestFailedWithError</code> <!-- -->-<!-- --> the function which is called if a request fails to provide the error</li></ul></li></ul><p>That&#x27;s a lot of words to describe our <code>useThrottleRequests</code> hook. Let&#x27;s look at what it looks like by migrating our <code>use10_000Requests</code> hook to (no pun intended) use it. Here&#x27;s a new implementation of <code>App.tsx</code>:</p><pre><code class="language-tsx">import React, { useState } from &#x27;react&#x27;;
import { useAsync } from &#x27;react-use&#x27;;
import { useThrottleRequests } from &#x27;./useThrottleRequests&#x27;;
import &#x27;./App.css&#x27;;

function use10_000Requests(startedAt: string) {
  const { throttle, updateThrottle } = useThrottleRequests();
  const [progressMessage, setProgressMessage] = useState(&#x27;not started&#x27;);

  useAsync(async () =&gt; {
    if (!startedAt) return;

    setProgressMessage(&#x27;preparing&#x27;);

    const requestsToMake = Array.from(Array(10_000)).map(
      (_, index) =&gt; async () =&gt; {
        try {
          setProgressMessage(`loading ${index}...`);

          const response = await fetch(
            `/manifest.json?querystringValueToPreventCaching=${startedAt}_request-${index}`
          );
          const json = await response.json();

          updateThrottle.requestSucceededWithData(json);
        } catch (error) {
          console.error(`failed to load ${index}`, error);
          updateThrottle.requestFailedWithError(error);
        }
      }
    );

    await updateThrottle.queueRequests(requestsToMake);
  }, [startedAt, updateThrottle, setProgressMessage]);

  return { throttle, progressMessage };
}

function App() {
  const [startedAt, setStartedAt] = useState(&#x27;&#x27;);

  const { progressMessage, throttle } = use10_000Requests(startedAt);

  return (
    &lt;div className=&quot;App&quot;&gt;
      &lt;header className=&quot;App-header&quot;&gt;
        &lt;h1&gt;The HTTP request machine&lt;/h1&gt;
        &lt;button
          className=&quot;App-button&quot;
          onClick={(_) =&gt; setStartedAt(new Date().toISOString())}
        &gt;
          Make 10,000 requests
        &lt;/button&gt;
        {throttle.loading &amp;&amp; &lt;div&gt;{progressMessage}&lt;/div&gt;}
        {throttle.values.length &gt; 0 &amp;&amp; (
          &lt;div className=&quot;App-results&quot;&gt;
            {throttle.values.length} requests completed successfully
          &lt;/div&gt;
        )}
        {throttle.errors.length &gt; 0 &amp;&amp; (
          &lt;div className=&quot;App-results&quot;&gt;
            {throttle.errors.length} requests errored
          &lt;/div&gt;
        )}
      &lt;/header&gt;
    &lt;/div&gt;
  );
}

export default App;
</code></pre><p>Looking at the new <code>use10_000Requests</code> hook, there&#x27;s a few subtle differences to our prior implementation. First of all, we&#x27;re now exposing the <code>throttle</code>; a <code>ThrottleProgress&amp;lt;TData&amp;gt;</code>. Our updated hook also exposes a <code>progressMessage</code> which is a simple <code>string</code> stored with <code>useState</code> that we update as our throttle runs. In truth the information being surfaced here isn&#x27;t that interesting. The <code>progressMessage</code> is in place just to illustrate that you could capture some data from your requests as they complete for display purposes; a running total for instance.</p><p>So, how does our new hook approach perform?</p><p><img src="../static/blog/2020-11-10-throttle-data-requests-with-react-hooks/i-want-it-all-with-hook.gif"/></p><p>Very well indeed! Please note that the above GIF has again been edited for brevity. If we look back at the problems we faced with the prior approach, how do we compare?</p><ol><li><del>the browser becoming unresponsive</del> <!-- -->-<!-- --> the browser remains responsive.</li><li><del>failing HTTP requests due to insufficient resources</del> <!-- -->-<!-- --> the browser does not experience failing HTTP requests.</li><li><del>no information displayable to the user around progress</del> <!-- -->-<!-- --> details of progress are displayed to the user throughout.</li></ol><p>Tremendous!</p><h2>What shall we build?</h2><p>Our current example is definitely contrived. Let&#x27;s try and apply our <code>useThrottleRequests</code> hook to a more realistic scenario. We&#x27;re going to build an application which, given a repo on GitHub, lists all the contributors blogs. (You can specify a blog URL on your GitHub profile; many people use this to specify their Twitter profile.)</p><p>We can build this thanks to the excellent <a href="https://docs.github.com/en/free-pro-team@latest/rest">GitHub REST API</a>. It exposes two endpoints of interest given our goal.</p><h3>1<!-- -->.<!-- --> List repository contributors</h3><p><a href="https://docs.github.com/en/free-pro-team@latest/rest/reference/repos#list-repository-contributors">List repository contributors</a> lists contributors to the specified repository at this URL: <code>GET https://api.github.com/repos/{owner}/{repo}/contributors</code>. The response is an array of objects, crucially featuring a <code>url</code> property that points to the user in question&#x27;s API endpoint:</p><pre><code class="language-js">[
  // ...
  {
    // ...
    url: &#x27;https://api.github.com/users/octocat&#x27;,
    // ...
  },
  // ...
];
</code></pre><h3>2<!-- -->.<!-- --> Get a user</h3><p><a href="https://docs.github.com/en/free-pro-team@latest/rest/reference/users#get-a-user">Get a user</a> is the API that the <code>url</code> property above is referring to. When called it returns an object representing the publicly available information about a user:</p><pre><code class="language-js">{
  // ...
  &quot;name&quot;: &quot;The Octocat&quot;,
  // ...
  &quot;blog&quot;: &quot;https://github.blog&quot;,
  // ...
}
</code></pre><h2>Blogging devs v1.0</h2><p>We&#x27;re now ready to build our blogging devs app; let&#x27;s replace the existing <code>App.tsx</code> with:</p><pre><code class="language-tsx">import React, { useCallback, useMemo, useState } from &#x27;react&#x27;;
import { useAsync } from &#x27;react-use&#x27;;
import { useThrottleRequests } from &#x27;./useThrottleRequests&#x27;;
import &#x27;./App.css&#x27;;

type GitHubUser = { name: string; blog?: string };

function timeout(ms: number) {
  return new Promise((resolve) =&gt; setTimeout(resolve, ms));
}

function useContributors(contributorsUrlToLoad: string) {
  const { throttle, updateThrottle } = useThrottleRequests&lt;GitHubUser&gt;();
  const [progressMessage, setProgressMessage] = useState(&#x27;&#x27;);

  useAsync(async () =&gt; {
    if (!contributorsUrlToLoad) return;

    setProgressMessage(&#x27;loading contributors&#x27;);

    // load contributors from GitHub
    const contributorsResponse = await fetch(contributorsUrlToLoad);
    const contributors: { url: string }[] = await contributorsResponse.json();

    setProgressMessage(`loading ${contributors.length} contributors...`);

    // For each entry in result, retrieve the given user from GitHub
    const requestsToMake = contributors.map(({ url }, index) =&gt; async () =&gt; {
      try {
        setProgressMessage(
          `loading ${index} / ${contributors.length}: ${url}...`
        );

        const response = await fetch(url);
        const json: GitHubUser = await response.json();

        // wait for 1 second before completing the request
        // - makes for better demos
        await timeout(1000);

        updateThrottle.requestSucceededWithData(json);
      } catch (error) {
        console.error(`failed to load ${url}`, error);
        updateThrottle.requestFailedWithError(error);
      }
    });

    await updateThrottle.queueRequests(requestsToMake);

    setProgressMessage(&#x27;&#x27;);
  }, [contributorsUrlToLoad, updateThrottle, setProgressMessage]);

  return { throttle, progressMessage };
}

function App() {
  // The owner and repo to query; we&#x27;re going to default
  // to using DefinitelyTyped as an example repo as it
  // is one of the most contributed to repos on GitHub
  const [owner, setOwner] = useState(&#x27;DefinitelyTyped&#x27;);
  const [repo, setRepo] = useState(&#x27;DefinitelyTyped&#x27;);
  const handleOwnerChange = useCallback(
    (event: React.ChangeEvent&lt;HTMLInputElement&gt;) =&gt;
      setOwner(event.target.value),
    [setOwner]
  );
  const handleRepoChange = useCallback(
    (event: React.ChangeEvent&lt;HTMLInputElement&gt;) =&gt; setRepo(event.target.value),
    [setRepo]
  );

  const contributorsUrl = `https://api.github.com/repos/${owner}/${repo}/contributors`;

  const [contributorsUrlToLoad, setUrlToLoad] = useState(&#x27;&#x27;);
  const { progressMessage, throttle } = useContributors(contributorsUrlToLoad);

  const bloggers = useMemo(
    () =&gt; throttle.values.filter((contributor) =&gt; contributor.blog),
    [throttle]
  );

  return (
    &lt;div className=&quot;App&quot;&gt;
      &lt;header className=&quot;App-header&quot;&gt;
        &lt;h1&gt;Blogging devs&lt;/h1&gt;

        &lt;p&gt;
          Show me the{&#x27; &#x27;}
          &lt;a
            className=&quot;App-link&quot;
            href={contributorsUrl}
            target=&quot;_blank&quot;
            rel=&quot;noopener noreferrer&quot;
          &gt;
            contributors for {owner}/{repo}
          &lt;/a&gt;{&#x27; &#x27;}
          who have blogs.
        &lt;/p&gt;

        &lt;div className=&quot;App-labelinput&quot;&gt;
          &lt;label htmlFor=&quot;owner&quot;&gt;GitHub Owner&lt;/label&gt;
          &lt;input
            id=&quot;owner&quot;
            type=&quot;text&quot;
            value={owner}
            onChange={handleOwnerChange}
          /&gt;
          &lt;label htmlFor=&quot;repo&quot;&gt;GitHub Repo&lt;/label&gt;
          &lt;input
            id=&quot;repo&quot;
            type=&quot;text&quot;
            value={repo}
            onChange={handleRepoChange}
          /&gt;
        &lt;/div&gt;

        &lt;button
          className=&quot;App-button&quot;
          onClick={(e) =&gt; setUrlToLoad(contributorsUrl)}
        &gt;
          Load bloggers from GitHub
        &lt;/button&gt;

        {progressMessage &amp;&amp; (
          &lt;div className=&quot;App-progress&quot;&gt;{progressMessage}&lt;/div&gt;
        )}

        {throttle.percentageLoaded &gt; 0 &amp;&amp; (
          &lt;&gt;
            &lt;h3&gt;Behold {bloggers.length} bloggers:&lt;/h3&gt;
            &lt;div className=&quot;App-results&quot;&gt;
              {bloggers.map((blogger) =&gt; (
                &lt;div key={blogger.name}&gt;
                  &lt;div&gt;{blogger.name}&lt;/div&gt;
                  &lt;a
                    className=&quot;App-link&quot;
                    href={blogger.blog}
                    target=&quot;_blank&quot;
                    rel=&quot;noopener noreferrer&quot;
                  &gt;
                    {blogger.blog}
                  &lt;/a&gt;
                &lt;/div&gt;
              ))}
            &lt;/div&gt;
          &lt;/&gt;
        )}

        {throttle.errors.length &gt; 0 &amp;&amp; (
          &lt;div className=&quot;App-results&quot;&gt;
            {throttle.errors.length} requests errored
          &lt;/div&gt;
        )}
      &lt;/header&gt;
    &lt;/div&gt;
  );
}

export default App;
</code></pre><p>The application gives users the opportunity to enter the organisation and repository of a GitHub project. Then, when the button is clicked, it:</p><ul><li>loads the contributors</li><li>for each contributor it loads the individual user (separate HTTP request for each)</li><li>as it loads it communicates how far through the loading progress it has got</li><li>as users are loaded, it renders a tile for each user with a listed blog</li></ul><p>Just to make the demo a little clearer we&#x27;ve artificially slowed the duration of each request by a second. What does it look like when you put it together? Well like this:</p><p><img src="../static/blog/2020-11-10-throttle-data-requests-with-react-hooks/blogging-devs.gif"/></p><p>We have built a React Hook which allows us to:</p><ul><li>gradually load data</li><li>without blocking the UI of the browser</li><li>and which provides progress data to keep users informed.</li></ul><p><a href="https://blog.logrocket.com/throttling-data-requests-with-react-hooks/">This post was originally published on LogRocket.</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Azure DevOps Client for Node.js - working around limitations]]></title>
            <link>https://blog.johnnyreilly.com/2020/10/31/azure-devops-node-api-missing-episodes</link>
            <guid>Azure DevOps Client for Node.js - working around limitations</guid>
            <pubDate>Sat, 31 Oct 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[The Azure DevOps Client library for Node.js has limitations and missing features, such as the ability to paginate git refs and create wiki posts. This post details some of these issues and illustrates a workaround using the Azure DevOps REST API.]]></description>
            <content:encoded><![CDATA[<p>The Azure DevOps Client library for Node.js has limitations and missing features, such as the ability to paginate git refs and create wiki posts. This post details some of these issues and illustrates a workaround using the Azure DevOps REST API.</p><p><img src="../static/blog/2020-10-31-azure-devops-node-api-missing-episodes/title-image.png" alt="A title image that reads &quot;Azure DevOps Client for Node.js - working around limitations&quot;"/></p><h2>The Azure DevOps REST API and Client Libraries</h2><p>I&#x27;ve been taking a good look at the <a href="https://docs.microsoft.com/en-us/rest/api/azure/devops/?view=azure-devops-rest-6.1">REST API for Azure DevOps</a>. I&#x27;m delighted to say that it&#x27;s a very full API. However, there&#x27;s quirks.</p><p>I&#x27;m writing a tool that interrogates Azure DevOps in order that it can construct release documentation. That release documentation we would like to publish to the project wiki.</p><p>To make integration with Azure DevOps even easier, the ADO team have put a good amount of work into <a href="https://docs.microsoft.com/en-us/rest/api/azure/devops/?view=azure-devops-rest-6.1#client-libraries">client libraries</a> that allow you to code in your language of choice. In my case I&#x27;m writing a Node.js tool (using TypeScript) and happily the client lib for Node is written and published with TypeScript too. Tremendous! However, there is a &quot;but&quot; coming....</p><h2><code>GitApi</code> and <code>WikiApi</code> shortcomings</h2><p>As I&#x27;ve been using the Node client lib, I&#x27;ve found minor quirks. Such as the <a href="https://github.com/microsoft/azure-devops-node-api/issues/415"><code>GitApi.getRefs</code> missing the pagination parts of the API</a>.</p><p>Whilst the <code>GitApi</code> was missing some parameters on a method, the <code>WikiApi</code> was <a href="https://github.com/microsoft/azure-devops-node-api/issues/416">missing whole endpoints, such as the Pages - Create Or Update</a> one. The various <a href="https://github.com/microsoft/azure-devops-node-api/blob/master/CONTRIBUTING.md#general-contribution-guide">client libraries are auto-generated</a> which makes contribution a difficult game. The lovely <a href="https://github.com/vtbassmatt">Matt Cooper</a> has <a href="https://github.com/microsoft/azure-devops-node-api/issues/415#issuecomment-717991914">alerted the team</a></p><blockquote><p>These clients are generated from the server-side controllers, and at a glance, I don&#x27;t understand why those two parameters weren&#x27;t included. Full transparency, we don&#x27;t dedicate a lot of cycles here, but I will get it on the team&#x27;s radar to investigate/improve.</p></blockquote><p>In the meantime, I still had a tool to write.</p><h2>Handrolled Wiki API</h2><p>Whilst the Node.js client lib was missing some crucial pieces, there did seem to be a way forward. Using the API directly; not using the client lib to do our HTTP and using <a href="https://github.com/axios/axios">axios</a> instead. Happily the types we needed were still available for be leveraged.</p><p>Looking at the docs it seemed it ought to be simple:</p><p><a href="https://docs.microsoft.com/en-us/rest/api/azure/devops/?view=azure-devops-rest-6.1#assemble-the-request">https://docs.microsoft.com/en-us/rest/api/azure/devops/?view=azure-devops-rest-6.1#assemble-the-request</a></p><p>But when I attempted this I found my requests erroring out with 203 Non-Authoritative Informations. It didn&#x27;t make sense. I couldn&#x27;t get a single request to be successful, they all failed. It occurred to me that the answer was hiding in <code>node_modules</code>. I&#x27;d managed to make successful requests to the API using the client lib. What was it doing that I wasn&#x27;t?</p><p>The answer ended up being an authorization one-liner:</p><pre><code class="language-ts">const request = await axios({
        url,
        headers: {
            Accept: &#x27;application/json&#x27;,
            &#x27;Content-Type&#x27;: &#x27;application/json&#x27;,
            // This!
            Authorization: `Basic ${Buffer.from(`PAT:${adoPersonalAccessToken}`).toString(&#x27;base64&#x27;)}`,
            &#x27;X-TFS-FedAuthRedirect&#x27;: &#x27;Suppress&#x27;,
        },
    });
}
</code></pre><p>With this in hand everything started to work and I found myself able to write my own clients to fill in the missing pieces from the client lib:</p><pre><code class="language-ts">import axios from &#x27;axios&#x27;;
import {
  WikiPage,
  WikiPageCreateOrUpdateParameters,
  WikiType,
} from &#x27;azure-devops-node-api/interfaces/WikiInterfaces&#x27;;
import { IWikiApi } from &#x27;azure-devops-node-api/WikiApi&#x27;;

async function getWikiPage({
  adoUrl,
  adoProject,
  adoPat,
  wikiId,
  path,
}: {
  adoUrl: string;
  adoProject: string;
  adoPat: string;
  wikiId: string;
  path: string;
}) {
  try {
    const url = `${makeBaseApiUrl({
      adoUrl,
      adoProject,
    })}/wiki/wikis/${wikiId}/pages?${apiVersion}&amp;path=${path}&amp;includeContent=True&amp;recursionLevel=full`;
    const request = await axios({
      url,
      headers: makeHeaders(adoPat),
    });

    const page: WikiPage = request.data;
    return page;
  } catch (error) {
    return undefined;
  }
}

async function createWikiPage({
  adoUrl,
  adoProject,
  adoPat,
  wikiId,
  path,
  data,
}: {
  adoUrl: string;
  adoProject: string;
  adoPat: string;
  wikiId: string;
  path: string;
  data: WikiPageCreateOrUpdateParameters;
}) {
  try {
    const url = `${makeBaseApiUrl({
      adoUrl,
      adoProject,
    })}/wiki/wikis/${wikiId}/pages?${apiVersion}&amp;path=${path}`;

    const request = await axios({
      method: &#x27;PUT&#x27;,
      url,
      headers: makeHeaders(adoPat),
      data,
    });

    const newPage: WikiPage = request.data;
    return newPage;
  } catch (error) {
    return undefined;
  }
}

const apiVersion = &#x27;api-version=6.0&#x27;;

/**
 * Create the headers necessary to ake Azure DevOps happy
 * @param adoPat Personal Access Token from ADO
 */
function makeHeaders(adoPat: string) {
  return {
    Accept: &#x27;application/json&#x27;,
    &#x27;Content-Type&#x27;: &#x27;application/json&#x27;,
    Authorization: `Basic ${Buffer.from(`PAT:${adoPat}`).toString(&#x27;base64&#x27;)}`,
    &#x27;X-TFS-FedAuthRedirect&#x27;: &#x27;Suppress&#x27;,
  };
}

/**
 * eg https://dev.azure.com/{organization}/{project}/_apis
 */
function makeBaseApiUrl({
  adoUrl,
  adoProject,
}: {
  adoUrl: string;
  adoProject: string;
}) {
  return `${adoUrl}/${adoProject}/_apis`;
}
</code></pre><p>With this I was able to write code like this:</p><pre><code class="language-ts">let topLevelPage = await getWikiPage({
  adoUrl,
  adoProject,
  adoPat,
  wikiId,
  path: config.wikiTopLevelName,
});

if (!topLevelPage)
  topLevelPage = await createWikiPage({
    adoUrl,
    adoProject,
    adoPat,
    wikiId,
    path: config.wikiTopLevelName,
    data: { content: &#x27;&#x27; },
  });
</code></pre><p>and the wikis were ours!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Safari: The Mysterious Case of the Empty Download]]></title>
            <link>https://blog.johnnyreilly.com/2020/10/19/safari-empty-download-content-type</link>
            <guid>Safari: The Mysterious Case of the Empty Download</guid>
            <pubDate>Mon, 19 Oct 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[Safari wants a Content-Type header in responses. Even if the response is Content-Length: 0. Without this, Safari can attempt to trigger an empty download. Don't argue; just go with it; some browsers are strange.]]></description>
            <content:encoded><![CDATA[<p>Safari wants a <code>Content-Type</code> header in responses. Even if the response is <code>Content-Length: 0</code>. Without this, Safari can attempt to trigger an empty download. Don&#x27;t argue; just go with it; some browsers are strange.</p><h2>The longer version</h2><p>Every now and then a mystery presents itself. A puzzle which just doesn&#x27;t make sense and yet stubbornly continues to exist. I happened upon one of these the other day and to say it was frustrating does it no justice at all.</p><p>It all came back to the default iOS and Mac browser; Safari. When our users log into our application, they are redirected to a shared login provider which, upon successful authentication, hands over a cookie containing auth details and redirects back to our application. A middleware in our app reads what it needs from the cookie and then creates a cookie of its own which is to be used throughout the session. As soon as the cookie is set, the page refreshes and the app boots up in an authenticated state.</p><p>That&#x27;s the background. This mechanism had long been working fine with Chrome (which the majority of our users browse with), Edge, Firefox and Internet Explorer. But we started to get reports from Safari users that, once they&#x27;d supplied their credentials, they&#x27;d not be authenticated and redirected back to our application. Instead they&#x27;d be prompted to download an empty document and the redirect would not take place.</p><p>As a team we could not fathom why this should be the case; it just didn&#x27;t make sense. There followed hours of experimentation before <a href="https://twitter.com/hennie_spies">Hennie</a> noticed something. It was at the point when the redirect back to our app from the login provider took place. Specifically the initial response that came back which contained our custom cookie and a <code>Refresh: 0</code> header to trigger a refresh in the browser. There was no content in the response, save for headers. It was <code>Content-Length: 0</code> all the way.</p><p>Hennie noticed that there was no <code>Content-Type</code> set and wondered if that was significant. It didn&#x27;t seem like it would be a necessary header given there was no content. But Safari reckons not with logic. As an experiment we tried setting the response header to <code>Content-Type: text/html</code>. It worked! No mystery download, no failed redirect (which it turned out was actually a successful redirect which wasn&#x27;t being surfaced in Safari&#x27;s network request tab).</p><p>It appears that always providing a <code>Content-Type</code> header in your responses is wise if only for the case of Safari. In fact, it&#x27;s generally unlikely that this won&#x27;t be set anyway, but it can happen as we have experienced. Hopefully we&#x27;ve suffered so you don&#x27;t have to.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Autofac 6, integration tests and .NET generic hosting]]></title>
            <link>https://blog.johnnyreilly.com/2020/10/02/autofac-6-integration-tests-and-generic-hosting</link>
            <guid>Autofac 6, integration tests and .NET generic hosting</guid>
            <pubDate>Fri, 02 Oct 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[I blogged a little while ago around to support integration tests using Autofac. This was specific to Autofac but documented a workaround for a long standing issue with ConfigureTestContainer that was introduced into .NET core 3.0 which affects all third-party containers that use ConfigureTestContainer in their tests.]]></description>
            <content:encoded><![CDATA[<p>I <a href="./2020-05-21-autofac-webapplicationfactory-integration-tests.md">blogged a little while ago around to support integration tests using Autofac</a>. This was specific to Autofac but documented a workaround for a <a href="https://github.com/dotnet/aspnetcore/issues/14907">long standing issue with <code>ConfigureTestContainer</code> that was introduced into .NET core 3.0</a> which affects <a href="https://docs.microsoft.com/en-us/aspnet/core/fundamentals/dependency-injection?view=aspnetcore-3.1#default-service-container-replacement">all third-party containers</a> that use <code>ConfigureTestContainer</code> in their tests.</p><p><img src="../static/blog/2020-10-02-autofac-6-integration-tests-and-generic-hosting/autofac-integration-tests.png" alt="A title image for the blog featuring the Autofac logo"/></p><p>I&#x27;ll not repeat the contents of the previous post - it all still stands. However, with Autofac 6 the approach documented there will cease to work. This is because the previous approach relied upon <code>ContainerBuilder</code> not being sealed. <a href="https://github.com/autofac/Autofac/issues/1120">As of Autofac 6 it is.</a></p><p>Happily the tremendous <a href="https://twitter.com/evocationist">Alistair Evans</a> came up with an <a href="https://github.com/autofac/Autofac/issues/1207#issuecomment-701961371">alternative approach</a> which is listed below:</p><pre><code class="language-cs">/// &lt;summary&gt;
/// Based upon https://github.com/dotnet/AspNetCore.Docs/tree/master/aspnetcore/test/integration-tests/samples/3.x/IntegrationTestsSample
/// &lt;/summary&gt;
/// &lt;typeparam name=&quot;TStartup&quot;&gt;&lt;/typeparam&gt;
public class AutofacWebApplicationFactory&lt;TStartup&gt; : WebApplicationFactory&lt;TStartup&gt; where TStartup : class
{
    protected override IHost CreateHost(IHostBuilder builder)
    {
        builder.UseServiceProviderFactory&lt;ContainerBuilder&gt;(new CustomServiceProviderFactory());
        return base.CreateHost(builder);
    }
}

/// &lt;summary&gt;
/// Based upon https://github.com/dotnet/aspnetcore/issues/14907#issuecomment-620750841 - only necessary because of an issue in ASP.NET Core
/// &lt;/summary&gt;
public class CustomServiceProviderFactory : IServiceProviderFactory&lt;ContainerBuilder&gt;
{
    private AutofacServiceProviderFactory _wrapped;
    private IServiceCollection _services;

    public CustomServiceProviderFactory()
    {
        _wrapped = new AutofacServiceProviderFactory();
    }

    public ContainerBuilder CreateBuilder(IServiceCollection services)
    {
        // Store the services for later.
        _services = services;

        return _wrapped.CreateBuilder(services);
    }

    public IServiceProvider CreateServiceProvider(ContainerBuilder containerBuilder)
    {
        var sp = _services.BuildServiceProvider();
#pragma warning disable CS0612 // Type or member is obsolete
        var filters = sp.GetRequiredService&lt;IEnumerable&lt;IStartupConfigureContainerFilter&lt;ContainerBuilder&gt;&gt;&gt;();
#pragma warning restore CS0612 // Type or member is obsolete

        foreach (var filter in filters)
        {
            filter.ConfigureContainer(b =&gt; { })(containerBuilder);
        }

        return _wrapped.CreateServiceProvider(containerBuilder);
    }
}
</code></pre><p>Using this in place of the previous approach should allow you continue running your integration tests with Autofac 6. Thanks Alistair!</p><h2>Concern for third-party containers</h2><p>Whilst this gets us back up and running, <a href="https://github.com/autofac/Autofac/issues/1207#issuecomment-702250044">Alistair pointed out that this approach depends upon a deprecated interface</a>. This is the <a href="https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.hosting.istartupconfigurecontainerfilter-1.configurecontainer?view=aspnetcore-3.1"><code>IStartupConfigureContainerFilter</code></a> which <a href="https://github.com/dotnet/aspnetcore/pull/11505">has been marked as <code>Obsolete</code> since mid 2019</a>. What this means is, at some point, this approach will stop working.</p><p>The marvellous David Fowler has said that <a href="https://github.com/autofac/Autofac/issues/1207#issuecomment-702361608"><code>ConfigureTestContainer</code> issue should be resolved in .NET</a>. However it&#x27;s worth noting that this has been an issue since .NET Core 3 shipped and unfortunately the wonderful <a href="https://github.com/dotnet/aspnetcore/issues/14907#issuecomment-702287717">Chris Ross has advised that it&#x27;s not likely to be fixed for .NET 5</a>.</p><p>I&#x27;m very keen this does get resolved in .NET. Building tests upon an <code>Obsolete</code> attribute doesn&#x27;t fill me with confidence. I&#x27;m a long time user of Autofac and I&#x27;d like to continue to be. Here&#x27;s hoping that&#x27;s made possible by a fix landing in .NET. If this is something you care about, it may be worth upvoting / commenting on <a href="https://github.com/dotnet/aspnetcore/issues/14907">the issue in GitHub</a> so the team are aware of desire around this being resolved.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Why your team needs a newsfeed]]></title>
            <link>https://blog.johnnyreilly.com/2020/09/04/why-your-team-needs-newsfeed</link>
            <guid>Why your team needs a newsfeed</guid>
            <pubDate>Fri, 04 Sep 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[I'm part of a team that builds an online platform. I'm often preoccupied by how to narrow the gap between our users and "us" - the people that build the platform. It's important we understand how people use and interact with what we've built. If we don't then we're liable to waste our time and energy building the wrong things. Or the wrong amount of the right things.]]></description>
            <content:encoded><![CDATA[<p>I&#x27;m part of a team that builds an online platform. I&#x27;m often preoccupied by how to narrow the gap between our users and &quot;us&quot; - the people that build the platform. It&#x27;s important we understand how people use and interact with what we&#x27;ve built. If we don&#x27;t then we&#x27;re liable to waste our time and energy building the wrong things. Or the wrong amount of the right things.</p><p>On a recent holiday I spent a certain amount of time pondering how to narrow the gap between our user and us. We have lots of things that help us; we use various analytics tools like <a href="https://mixpanel.com/">mixpanel</a>, we&#x27;ve got a mini analytics platform of our own, we have teams notifications that pop up client feedback and so on. They are all great, but they&#x27;re somewhat disparate; they don&#x27;t give us a clear insight as to who uses our platform and how they do so. The information is there, but it&#x27;s tough to grok. It doesn&#x27;t make for a joined up story.</p><p>Reaching around for how to solve this I had an idea: what if our platform had a newsfeed? The kind of thing that social media platforms the likes of Twitter and Facebook have used to great effect; a stream of mini-activities which show how the community interacts with the product. People logging in and browsing around, using features on the platform. If we could see this in near real time we&#x27;d be brought closer to our users; we&#x27;d have something that would help us have real empathy and understanding. We&#x27;d see our product as the stories of users interacting with it.</p><h2>How do you build a newsfeed?</h2><p>This was an experiment that seemed worth pursuing. So I decided to build a proof of concept and see what happened. Now I intended to put the &quot;M&quot; into MVP with this; I went in with a number of intentional constraints:</p><ol><li>The news feed wouldn&#x27;t auto update (users have the F5 key for that)</li><li>We&#x27;d host the newsfeed in our own mini analytics platform (which is already used by the team to understand how people use the platform)</li><li>News stories wouldn&#x27;t be stored anywhere; we&#x27;d generate them on the fly by querying various databases / APIs. The cost of this would be that our news stories wouldn&#x27;t be &quot;persistent&quot;; you wouldn&#x27;t be able to address them with a URL; there&#x27;d be no way to build &quot;like&quot; or &quot;share&quot; functionality.</li></ol><p>All of the above constraints are, importantly, reversable decisions. If we want auto update it could be built later. If we want the newsfeed to live somewhere else we could move it. If we wanted news stories to be persisted then we could do that.</p><h2>Implementation</h2><p>With these constraints in mind, I turned my attention to the implementation. I built a <code>NewsFeedService</code> that would be queried for news stories. The interface I decided to build looked like this:</p><pre><code>NewsFeedService.getNewsFeed(from: Date, to: Date): NewsFeed

type NewsFeed {
    startedAt: Date;
    ended at: Date;
    stories: NewsStory[];
}

type NewsStory {
    /** When the story happened */
    happenedAt: Date;
    /** A code that represents the type of story this is; eg USER_SESSION */
    storyCode: string
    /** The story details in markdown format */
    story: string;
}
</code></pre><p>Each query to <code>NewsFeedService.getNewsFeed</code> would query various databases / APIs related to our product, looking for interesting events. Whether it be users logging in, users performing some kind of action, whatever. For each interested event a news story like this would be produced:</p><blockquote><p>Jane Smith logged in at 10:03am for 25 minutes. They placed <a href="https://my-glorious-platform.io/orders/janes-order">an order</a> worth £3,000.</p></blockquote><p>Now the killer feature here is <a href="https://en.wikipedia.org/wiki/Markdown#:~:text=Markdown%20is%20a%20lightweight%20markup,using%20a%20plain%20text%20editor.">Markdown</a>. Our stories are written in Markdown. Why is Markdown cool? Well <a href="https://web.archive.org/web/20040402182332/http://daringfireball.net/projects/markdown/">to quote the creators of Markdown</a>:</p><blockquote><p>Markdown allows you to write using an easy-to-read, easy-to-write plain text format, then convert it to structurally valid XHTML (or HTML).</p></blockquote><p>This crucially includes the ability to include links. This was significant because I want us to be able to be able to click on pieces of information in the stories and be taken to the relevant place in the platform to see more details. Just as you see status updates on, for example, Twitter which lead you on to more details:</p><blockquote><p>This is the history of <a href="https://twitter.com/DefinitelyTyped?ref_src=twsrc%5Etfw">@DefinitelyTyped</a>: <a href="https://t.co/AY6s3bWnKP">https://t.co/AY6s3bWnKP</a> Thanks to <a href="https://twitter.com/SeaRyanC?ref_src=twsrc%5Etfw">@SeaRyanC</a> &amp; <a href="https://twitter.com/drosenwasser?ref_src=twsrc%5Etfw">@drosenwasser</a> of the <a href="https://twitter.com/typescript?ref_src=twsrc%5Etfw">@typescript</a> team, <a href="https://twitter.com/blakeembrey?ref_src=twsrc%5Etfw">@blakeembrey</a> inventor of typings, <a href="https://twitter.com/vvakame?ref_src=twsrc%5Etfw">@vvakame</a>, <a href="https://twitter.com/_stevefenton?ref_src=twsrc%5Etfw">@<!-- -->_<!-- -->stevefenton</a>, <a href="https://twitter.com/basarat?ref_src=twsrc%5Etfw">@basarat</a>, and of course <a href="https://twitter.com/borisyankov?ref_src=twsrc%5Etfw">@borisyankov</a> for telling me their parts of the story❤️🌻</p><p>— John Reilly (@johnny_reilly) <a href="https://twitter.com/johnny_reilly/status/1181542739994976256?ref_src=twsrc%5Etfw">October 8, 2019</a></p></blockquote><script src="https://platform.twitter.com/widgets.js" charSet="utf-8"></script><p>Again consider this example news story:</p><blockquote><p>Jane Smith logged in at 10:03am for 25 minutes. They placed <a href="https://my-glorious-platform.io/orders/janes-order">an order</a> worth £3,000.</p></blockquote><p>Consider that story but without a link. It&#x27;s not the same is it? A newsfeed without links would be missing a trick. Markdown gives us links. And happily due to my extensive work down the open source mines, I speak it like a native.</p><p>The first consumer of the newsfeed was to be our own mini analytics platform, which is a React app. Converting the markdown stories to React is a solved problem thanks to the wonderful <a href="https://github.com/rexxars/react-markdown">react-markdown</a>. You can simply sling Markdown at it and out comes HTML. Et voilà a news feed!</p><h2>What&#x27;s next?</h2><p>So that&#x27;s it! We&#x27;ve built a (primitive) news feed. We can now see in real time how are users are getting on. We&#x27;re closer to them, we understand them better as a consequence. If we want to take it further there&#x27;s a number of things we could do:</p><ol><li>We could make the feed auto-update</li><li>We could push news stories to other destinations. Markdown is a gloriously portable format which can be used in a variety of environments. For instance the likes of Slack and <a href="./2019-12-18-teams-notification-webhooks.md">Teams</a> accept it and apps like these are generally open on people&#x27;s desktops and phones all the time anyway. Another way to narrow the gap between us and and our users.</li></ol><p>It&#x27;s very exciting!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Devcontainers AKA performance in a secure sandbox]]></title>
            <link>https://blog.johnnyreilly.com/2020/08/09/devcontainers-aka-performance-in-secure</link>
            <guid>Devcontainers AKA performance in a secure sandbox</guid>
            <pubDate>Sun, 09 Aug 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[Many corporate machines arrive in engineers hands with a preponderance of pre-installed background tools; from virus checkers to backup utilities to port blockers; the list is long.]]></description>
            <content:encoded><![CDATA[<p>Many corporate machines arrive in engineers hands with a preponderance of pre-installed background tools; from virus checkers to backup utilities to port blockers; the list is long.</p><p>The reason that these tools are installed is generally noble. However, the implementation can often be problematic. The tools may be set up in such a way as they impact and interfere with one another. Really powerful machines with 8 CPUs and hardy SSDs can be slowed to a crawl. Put simply: the good people responsible for ensuring security are rarely encouraged to incentivise performance alongside it. And so don&#x27;t.</p><p>The unfortunate consequence of considering the role of security without regard to performance is this: sluggish computers. The further consequence (and this is the one I want you to think long and hard about) is <em>low developer productivity</em>. And that sucks. It impacts what an organisation is able to do, how fast an organisation is able to move. Put simply: it can be the difference between success and failure.</p><p>The most secure computer is off. But you won&#x27;t ship much with it. Encouraging your organisation to consider tackling security with performance in mind is worthwhile. It&#x27;s a long game though. In the meantime what can we do?</p><h2>&quot;Hide from the virus checkers<!-- -->*<!-- -->*<!-- -->*<!-- --> in a devcontainer&quot;</h2><p>Devcontainers, the infrastructure as code equivalent for developing software, have an underappreciated quality: unlocking your machine&#x27;s performance.</p><p>Devcontainers are isolated secure sandboxes in which you can build software. To quote the <a href="https://code.visualstudio.com/docs/remote/containers">docs</a>:</p><blockquote><p>A <code>devcontainer.json</code> file in your project tells VS Code how to access (or create) a development container with a well-defined tool and runtime stack. This container can be used to run an application or to sandbox tools, libraries, or runtimes needed for working with a codebase.</p><p>Workspace files are mounted from the local file system or copied or <em>cloned into the container</em>.</p></blockquote><p>We&#x27;re going to set up a devcontainer to code an ASP.NET Core application with a JavaScript (well TypeScript) front end. If there&#x27;s one thing that&#x27;s sure to catch a virus checkers beady eye, it&#x27;s <code>node_modules</code>. <code>node_modules</code> contains more files than a black hole has mass. Consider a project with 5,000 source files. One trusty <code>yarn</code> later and the folder now has a tidy 250,000 files. The virus checker is now really sitting up and taking notice.</p><p>Our project has a <code>git commit</code> hook set up with <a href="https://github.com/typicode/husky">Husky</a> that formats our TypeScript files with <a href="https://prettier.io/">Prettier</a>. Every commit the files are formatted to align with the project standard. With all the virus checkers in place a <code>git commit</code> takes around 45 seconds. Inside a devcontainer we can drop this to 5 seconds. That&#x27;s nine times faster. I&#x27;ll repeat that: that&#x27;s <strong>nine times faster</strong>!</p><p>The &quot;cloned into a container&quot; above is key to what we&#x27;re going to do. We&#x27;re <em>not</em> going to mount our local file system into the devcontainer. Oh no. We&#x27;re going to build a devcontainer with ASP.NET CORE and JavaScript in. Then, inside there, we&#x27;re going to clone our repo. Then we can develop, build and debug all inside the container. It will feel like we&#x27;re working on our own machine because VS Code does such a damn fine job. In reality, we&#x27;re connecting to another computer (a Linux computer to boot) that is running in isolation to our own. In our case that machine is sharing our hardware; but that&#x27;s just an implementation detail. It could be anywhere (and in the future may well be).</p><h2>Make me a devcontainer...</h2><p>Enough talk... We&#x27;re going to need a <code>.devcontainer/devcontainer.json</code>:</p><pre><code class="language-json">{
  &quot;name&quot;: &quot;my devcontainer&quot;,
  &quot;dockerComposeFile&quot;: &quot;../docker-compose.devcontainer.yml&quot;,
  &quot;service&quot;: &quot;my-devcontainer&quot;,
  &quot;workspaceFolder&quot;: &quot;/workspace&quot;,

  // Set *default* container specific settings.json values on container create.
  &quot;settings&quot;: {
    &quot;terminal.integrated.shell.linux&quot;: &quot;/bin/zsh&quot;
  },

  // Add the IDs of extensions you want installed when the container is created.
  &quot;extensions&quot;: [
    &quot;ms-dotnettools.csharp&quot;,
    &quot;dbaeumer.vscode-eslint&quot;,
    &quot;esbenp.prettier-vscode&quot;,
    &quot;ms-mssql.mssql&quot;,
    &quot;eamodio.gitlens&quot;,
    &quot;ms-azuretools.vscode-docker&quot;,
    &quot;k--kato.docomment&quot;,
    &quot;Leopotam.csharpfixformat&quot;
  ],

  // Use &#x27;postCreateCommand&#x27; to clone the repo into the workspace folder when the devcontainer starts
  // and copy in the .env file
  &quot;postCreateCommand&quot;: &quot;git clone git@github.com:my-org/my-repo.git . &amp;&amp; cp /.env /workspace/.env&quot;

  // &quot;remoteUser&quot;: &quot;vscode&quot;
}
</code></pre><p>Now the <code>docker-compose.devcontainer.yml</code> which lives in the root of the project. It provisions a SQL Server container (using the official image) and our devcontainer:</p><pre><code>version: &quot;3.7&quot;
services:
  my-devcontainer:
    image: my-devcontainer
    build:
      context: .
      dockerfile: Dockerfile.devcontainer
    command: /bin/zsh -c &quot;while sleep 1000; do :; done&quot;
    volumes:
      # mount .zshrc from home - make sure it doesn&#x27;t contain Windows line endings
      - ~/.zshrc:/root/.zshrc

    # user: vscode
    ports:
      - &quot;5000:5000&quot;
      - &quot;8080:8080&quot;
    environment:
      - CONNECTIONSTRINGS__MYDATABASECONNECTION
    depends_on:
      - db
  db:
    image: mcr.microsoft.com/mssql/server:2019-latest
    privileged: true
    ports:
      - 1433:1433
    environment:
      SA_PASSWORD: &quot;Your_password123&quot;
      ACCEPT_EULA: &quot;Y&quot;
</code></pre><p>The devcontainer will be built with the <code>Dockerfile.devcontainer</code> in the root of our repo. It relies upon your SSH keys and a <code>.env</code> file being available to be copied in:</p><pre><code>#-----------------------------------------------------------------------------------------------------------
# Based upon: https://github.com/microsoft/vscode-dev-containers/tree/master/containers/dotnetcore
#-----------------------------------------------------------------------------------------------------------
ARG VARIANT=&quot;3.1-bionic&quot;
FROM mcr.microsoft.com/dotnet/core/sdk:${VARIANT}

# Because MITM certificates
COPY ./docker/certs/. /usr/local/share/ca-certificates/
ENV NODE_EXTRA_CA_CERTS=/usr/local/share/ca-certificates/mitm.pem
RUN update-ca-certificates

# This Dockerfile adds a non-root user with sudo access. Use the &quot;remoteUser&quot;
# property in devcontainer.json to use it. On Linux, the container user&#x27;s GID/UIDs
# will be updated to match your local UID/GID (when using the dockerFile property).
# See https://aka.ms/vscode-remote/containers/non-root-user for details.
ARG USERNAME=vscode
ARG USER_UID=1000
ARG USER_GID=$USER_UID

# Options for common package install script
ARG INSTALL_ZSH=&quot;true&quot;
ARG UPGRADE_PACKAGES=&quot;true&quot;
ARG COMMON_SCRIPT_SOURCE=&quot;https://raw.githubusercontent.com/microsoft/vscode-dev-containers/master/script-library/common-debian.sh&quot;
ARG COMMON_SCRIPT_SHA=&quot;dev-mode&quot;

# Settings for installing Node.js.
ARG INSTALL_NODE=&quot;true&quot;
ARG NODE_SCRIPT_SOURCE=&quot;https://raw.githubusercontent.com/microsoft/vscode-dev-containers/master/script-library/node-debian.sh&quot;
ARG NODE_SCRIPT_SHA=&quot;dev-mode&quot;

# ARG NODE_VERSION=&quot;lts/*&quot;
ARG NODE_VERSION=&quot;14&quot;
ENV NVM_DIR=/usr/local/share/nvm

# Have nvm create a &quot;current&quot; symlink and add to path to work around https://github.com/microsoft/vscode-remote-release/issues/3224
ENV NVM_SYMLINK_CURRENT=true
ENV PATH=${NVM_DIR}/current/bin:${PATH}

# Configure apt and install packages
RUN apt-get update \
    &amp;&amp; export DEBIAN_FRONTEND=noninteractive \
    #
    # Verify git, common tools / libs installed, add/modify non-root user, optionally install zsh
    &amp;&amp; apt-get -y install --no-install-recommends curl ca-certificates 2&gt;&amp;1 \
    &amp;&amp; curl -sSL ${COMMON_SCRIPT_SOURCE} -o /tmp/common-setup.sh \
    &amp;&amp; ([ &quot;${COMMON_SCRIPT_SHA}&quot; = &quot;dev-mode&quot; ] || (echo &quot;${COMMON_SCRIPT_SHA} */tmp/common-setup.sh&quot; | sha256sum -c -)) \
    &amp;&amp; /bin/bash /tmp/common-setup.sh &quot;${INSTALL_ZSH}&quot; &quot;${USERNAME}&quot; &quot;${USER_UID}&quot; &quot;${USER_GID}&quot; &quot;${UPGRADE_PACKAGES}&quot; \
    #
    # Install Node.js
    &amp;&amp; curl -sSL ${NODE_SCRIPT_SOURCE} -o /tmp/node-setup.sh \
    &amp;&amp; ([ &quot;${NODE_SCRIPT_SHA}&quot; = &quot;dev-mode&quot; ] || (echo &quot;${COMMON_SCRIPT_SHA} */tmp/node-setup.sh&quot; | sha256sum -c -)) \
    &amp;&amp; /bin/bash /tmp/node-setup.sh &quot;${NVM_DIR}&quot; &quot;${NODE_VERSION}&quot; &quot;${USERNAME}&quot; \
    #
    # Clean up
    &amp;&amp; apt-get autoremove -y \
    &amp;&amp; apt-get clean -y \
    &amp;&amp; rm -f /tmp/common-setup.sh /tmp/node-setup.sh \
    &amp;&amp; rm -rf /var/lib/apt/lists/* \
    #
    # Workspace
    &amp;&amp; mkdir workspace \
    &amp;&amp; chown -R ${NONROOT_USER}:root workspace


# Install Vim
RUN apt-get update &amp;&amp; apt-get install -y \
    vim \
    &amp;&amp; rm -rf /var/lib/apt/lists/*

# Set up a timezone in the devcontainer - necessary for anything timezone dependent
ENV TZ=Europe/London
RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime &amp;&amp; echo $TZ &gt; /etc/timezone \
 &amp;&amp; apt-get update \
 &amp;&amp; apt-get install --no-install-recommends -y \
    apt-utils \
    tzdata  \
 &amp;&amp; apt-get autoremove -y \
 &amp;&amp; apt-get clean -y \
 &amp;&amp; rm -rf /var/lib/apt/lists/*

ENV DOTNET_RUNNING_IN_CONTAINER=true

# Copy across SSH keys so you can git clone
RUN mkdir /root/.ssh
RUN chmod 700 /root/.ssh

COPY .ssh/id_rsa /root/.ssh
RUN chmod 600 /root/.ssh/id_rsa

COPY .ssh/id_rsa.pub /root/.ssh
RUN chmod 644 /root/.ssh/id_rsa.pub

COPY .ssh/known_hosts /root/.ssh
RUN chmod 644 /root/.ssh/known_hosts

# Disable initial git clone prompt
RUN echo &quot;StrictHostKeyChecking no&quot; &gt;&gt; /etc/ssh/ssh_config

# Copy across .env file so you can customise environment variables
# This will be copied into the root of the repo post git clone
COPY .env /.env
RUN chmod 644 /.env

# Install dotnet entity framework tools
RUN dotnet tool install dotnet-ef --tool-path /usr/local/bin --version 3.1.2
</code></pre><p>With this devcontainer you&#x27;re good to go for an ASP.NET Core / JavaScript developer setup that is blazing fast! Remember to fire up Docker and give it goodly access to the resources of your host machine. All the CPUs, lots of memory and all the performance that there ought to be.</p><p><em>*<!-- --> &quot;virus checkers&quot; is a euphemism here for all the background tools that may be running. It was that or calling them &quot;we are legion&quot;</em></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Devcontainers and SSL interception]]></title>
            <link>https://blog.johnnyreilly.com/2020/07/11/devcontainers-and-ssl-interception</link>
            <guid>Devcontainers and SSL interception</guid>
            <pubDate>Sat, 11 Jul 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[Devcontainers are cool. They are the infrastructure as code equivalent for developing software.]]></description>
            <content:encoded><![CDATA[<p><a href="https://code.visualstudio.com/docs/remote/containers">Devcontainers</a> are cool. They are the infrastructure as code equivalent for developing software.</p><p>Imagine your new starter joins the team, you&#x27;d like them to be contributing code on <em>day 1</em>. But if the first thing that happens is you hand them a sheaf of paper upon which are the instructions for how to get their machines set up for development, well, maybe it&#x27;s going to be a while. But if your project has a devcontainer then you&#x27;re off to the races. One trusty <code>git clone</code>, fire up VS Code and they can get going.</p><p>That&#x27;s the dream right?</p><p>I&#x27;ve recently been doing some work getting a project I work on set up with a devcontainer. As I&#x27;ve worked on that I&#x27;ve become aware of some of the hurdles that might hamper your adoption of devcontainers in a corporate environment.</p><h2>Certificates: I&#x27;m starting with the man in the middle</h2><p>It is a common practice in company networks to perform <a href="https://docs.citrix.com/en-us/citrix-adc/13/forward-proxy/ssl-interception.html">SSL interception</a>. Not SSL inception; that&#x27;d be more fun.</p><iframe src="https://giphy.com/embed/l7JDTHpsXM26k" width="100%" height="100%" frameBorder="0"></iframe><p>SSL interception is the practice of installing a &quot;man-in-the-middle&quot; (MITM) CA certificate on users machines. When SSL traffic takes place from a users machine, it goes through a proxy. That proxy performs the SSL on behalf of that user and, if it&#x27;s happy, supplies another certificate back to the users machine which satisfies the MITM CA certificate. So rather than seeing, for example, Google&#x27;s certificate from <a href="https://google.com">https://google.com</a> you&#x27;d see the one resulting from the SSL interception. You can read more <a href="https://security.stackexchange.com/questions/107542/is-it-common-practice-for-companies-to-mitm-https-traffic">here</a>.</p><p>Now this is a little known and less understood practice. I barely understand it myself. Certificates are <em>hard</em>. Even having read the above you may be none the wiser about why this is relevant. Let&#x27;s get to the broken stuff.</p><h2>&quot;Devcontainers don&#x27;t work at work!&quot;</h2><p>So, you&#x27;re ready to get going with your first devcontainer. You fire up the <a href="https://github.com/Microsoft/vscode-dev-containers">vscode-dev-containers</a> repo and find the container that&#x27;s going to work for you. Copy pasta the <code>.devcontainer</code> into your repo, install the <a href="https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.vscode-remote-extensionpack">Remote Development</a> extension into VS Code and enter the <code>Remote-Containers: Reopen Folder in Container</code>. Here comes the future!</p><p>But when it comes to performing SSL inside the devcontainer, trouble awaits. Here&#x27;s what a <code>yarn install</code> results in:</p><pre><code>yarn install v1.22.4
[1/4] Resolving packages...
[2/4] Fetching packages...
error An unexpected error occurred: &quot;https://registry.yarnpkg.com/@octokit/core/-/core-2.5.0.tgz: self signed certificate in certificate chain&quot;.
</code></pre><p>Oh no!</p><p>Gosh but it&#x27;s okay - you&#x27;re just bumping on the SSL interception. Why though? Well it&#x27;s like this: when you fire up your devcontainer it builds a new Docker container. It&#x27;s as well to imagine the container as a virtual operating system. So what&#x27;s the difference between this operating system and the one our machine is running? Well a number of things, but crucially our host operating system has the MITM CA certificate installed. So when we SSL, we have the certificate that will match up with what the proxy sends back to us certificate-wise. And inside our trusty devcontainer we don&#x27;t have that. Hence the sadness.</p><h2>Devcontainer + MITM cert = working</h2><p>We need to do two things to get this working:</p><ol><li>Acquire the requisite CA certificate(s) from your friendly neighbourhood networking team. Place them in a <code>certs</code> folder inside your repo, in the <code>.devcontainer</code> folder.</li><li>Add the following lines to your <code>.devcontainer/Dockerfile</code>, just after the initial <code>FROM</code> statement:</li></ol><pre><code># Because MITM certificates
COPY certs/. /usr/local/share/ca-certificates/
ENV NODE_EXTRA_CA_CERTS=/usr/local/share/ca-certificates/mitm.pem
RUN update-ca-certificates
</code></pre><p>Which does the following:</p><ul><li>Copies the certs into the devcontainer</li><li>This is a Node example and so we set an environment variable called <a href="https://nodejs.org/api/cli.html#cli_node_extra_ca_certs_file"><code>NODE_EXTRA_CA_CERTS</code></a> which points to the path of your MITM CA certificate file inside your devcontainer.</li><li>updates the directory <code>/etc/ssl/certs</code> to hold SSL certificates and generates <code>ca-certificates.crt</code></li></ul><p>With these in place then you should be able to build your devcontainer with no SSL trauma. Enjoy!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Task.WhenAll / Select is a footgun 👟🔫]]></title>
            <link>https://blog.johnnyreilly.com/2020/06/21/taskwhenall-select-is-footgun</link>
            <guid>Task.WhenAll / Select is a footgun 👟🔫</guid>
            <pubDate>Sun, 21 Jun 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[This post differs from my typical fayre. Most often I write "here's how to do a thing". This is not that. It's more "don't do this thing I did". And maybe also, "how can we avoid a situation like this happening again in future?". On this topic I very much don't have all the answers - but by putting my thoughts down maybe I'll learn and maybe others will educate me. I would love that!]]></description>
            <content:encoded><![CDATA[<p>This post differs from my typical fayre. Most often I write &quot;here&#x27;s how to do a thing&quot;. This is not that. It&#x27;s more &quot;don&#x27;t do this thing I did&quot;. And maybe also, &quot;how can we avoid a situation like this happening again in future?&quot;. On this topic I very much don&#x27;t have all the answers - but by putting my thoughts down maybe I&#x27;ll learn and maybe others will educate me. I would love that!</p><h2>Doing things that don&#x27;t scale</h2><p>The platform that I work on once had zero users. We used to beg people to log in and see what we had built. Those days are (happily) but a memory. We&#x27;re getting popular.</p><p>As our platform has grown in popularity it has revealed some bad choices we made. Approaches that look fine on the surface (and that work just dandy when you have no users) may start to cause problems as your number of users grows.</p><p>I wanted to draw attention to one approach in particular that impacted us severely. In this case &quot;impacted us severely&quot; is a euphemism for &quot;brought the site down and caused a critical incident&quot;.</p><p>You don&#x27;t want this to happen to you. Trust me. So, what follows is a cautionary tale. The purpose of which is simply this: reader, do you have code of this ilk in your codebase? If you do: out, damn&#x27;d spot! out, I say!</p><h2>So cool, so terrible</h2><p>I love LINQ. I love a declarative / functional style of coding. It appeals to me on some gut level. I find it tremendously readable. Read any C# of mine and the odds are pretty good that you&#x27;ll find some LINQ in the mix.</p><p>Imagine this scenario: you have a collection of user ids. You want to load the details of each user represented by their id from an API. You want to bag up all of those users into some kind of collection and send it back to the calling code.</p><p>Reading that, if you&#x27;re like me, you&#x27;re imagining some kind of map operation which loads the user details for each user id. Something like this:</p><pre><code class="language-cs">var users = userIds.Select(userId =&gt; GetUserDetails(userId)).ToArray(); // users is User[]
</code></pre><p>Lovely. But you&#x27;ll note that I&#x27;m loading users from an API. Oftentimes, APIs are asynchronous. Certainly, in my case they were. So rather than calling a <code>GetUserDetails</code> function I found myself calling a <code>GetUserDetailsAsync</code> function, behind which an HTTP request is being sent and, later, a response is being returned.</p><p>So how do we deal with this? <a href="https://docs.microsoft.com/en-us/dotnet/api/system.threading.tasks.task.whenall?view=netcore-3.1#System_Threading_Tasks_Task_WhenAll__1_System_Collections_Generic_IEnumerable_System_Threading_Tasks_Task___0___"><code>Task.WhenAll</code></a> my friends!</p><pre><code class="language-cs">var userTasks = userIds.Select(userId =&gt; GetUserDetailsAsync(userId));
var users = await Task.WhenAll(tasks); // users is User[]
</code></pre><p>It worked great! Right up until to the point where it didn&#x27;t. These sorts of shenanigans were fine when we had a minimal number of users... But there came a point where problems arose. It got to the point where that simple looking mapping operation became a cause of many, many, <em>many</em> HTTP requests being fired concurrently. Then bad things started to happen. Not only did we realise we were launching a denial of service attack on the API we were consuming, we were bringing our own application to collapse.</p><p>Not a proud day.</p><h2>What is the problem?</h2><p>Through log analysis, code reading and speculation, (with the help of the invaluable <a href="https://www.linkedin.com/in/robert-grzankowski-53618114">Robski</a>) we came to realise that the cause of our woes was the <code>Task.WhenAll</code> / <code>Select</code> combination. Exercising that codepath was a surefire way to bring the application to its knees.</p><p>As I read around on the topic I happened upon <a href="https://www.twitter.com/mark_heath">Mark Heath</a>&#x27;s excellent list of <a href="https://markheath.net/post/async-antipatterns">Async antipatterns</a>. Number #6 on the list is &quot;Excessive parallelization&quot;. It describes a nearly identical scenario to my own:</p><blockquote><p>Now, this does &quot;work&quot;, but what if there were 10,000 orders? We&#x27;ve flooded the thread pool with thousands of tasks, potentially preventing other useful work from completing. If <code>ProcessOrderAsync</code> makes downstream calls to another service like a database or a microservice, we&#x27;ll potentially overload that with too high a volume of calls.</p></blockquote><p>We&#x27;re definitely overloading the API we&#x27;re consuming with too high a volume of calls. I have to admit that I&#x27;m less clear on the direct reason that a <code>Task.WhenAll</code> / <code>Select</code> combination could prove fatal to our application. Mark suggests this approach will flood the thread pool with tasks. As I read around on <code>async</code> and <code>await</code> it&#x27;s repeated again and again that a <code>Task</code> is not the same thing as a <code>Thread</code>. I have to hold my hands up here and say that I don&#x27;t understand the implementation of <code>async</code> / <code>await</code> in C# well enough. <a href="https://docs.microsoft.com/en-us/dotnet/standard/async-in-depth#deeper-dive-into-tasks-for-an-io-bound-operation">These docs are helpful but I still don&#x27;t think the penny has fully dropped for me yet.</a> I will continue to read.</p><p>One thing we learned as we debugged the production k8s pod was that, prior to its collapse, our app appeared to be opening up 1 million connections to the API we were consuming. Which seemed a bit much. Worthy of investigation. It&#x27;s worth saying that we&#x27;re not certain this is exactly what is happening; we have less instrumentation in place than we&#x27;d like. But some fancy wc grepping on Robski&#x27;s behalf suggested this was the case.</p><h2>What will we change in future?</h2><p>A learning that came out of this for us was this: we need more metrics exposed. We don&#x27;t understand our application&#x27;s behaviour under load as well as we&#x27;d like. So we&#x27;re planning to do some work with <a href="https://www.app-metrics.io/">App Metrics</a> and <a href="https://grafana.com/">Grafana</a> so we&#x27;ve a better idea of how our application performs. If you want to improve something, first measure it.</p><p>Another fly in the ointment was that we were unable to reproduce the issue when running locally. It&#x27;s worth saying here that I develop on a Windows machine and, when deployed, our application runs in a (Linux) Docker container. So there&#x27;s a difference and a distance between our development experience and our running one.</p><p>I&#x27;m planning to migrate to developing in a <a href="https://code.visualstudio.com/docs/remote/containers">devcontainer</a> where that&#x27;s possible. That should narrow the gap between our production experience and our development one. Reducing the difference between the two is always useful as it means you&#x27;re less likely to get different behaviour (ie &quot;problems&quot;) in production as compared to development. I&#x27;m curious as to whether I&#x27;ll be able to replicate that behaviour in a devcontainer.</p><h2>What did we do right now?</h2><p>To solve the immediate issue we were able to pivot away to a completely different approach. We moved aggregation from our ASP.NET Core web application to our TypeScript / React client with a (pretty sweet) custom hook. The topic for a subsequent blog post.</p><p>Moving to a different approach solved my immediate issue. But it left me puzzling. What was actually going wrong? Is it thread pool exhaustion? Is it something else? So many possibilities!</p><p>If anyone has any insights they&#x27;d like to share that would be incredible! I&#x27;ve also <a href="https://stackoverflow.com/questions/62490098/task-whenall-with-select-is-a-footgun-but-why/62490705">asked a question on Stack Overflow</a> which has kindly had answers from generous souls. <a href="https://twitter.com/jamesskimming">James Skimming</a>&#x27;s answer lead me to <a href="https://www.stevejgordon.co.uk/httpclient-connection-pooling-in-dotnet-core">Steve Gordon&#x27;s excellent post on connection pooling</a> which I&#x27;m still absorbing and seems like it could be relevant.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Autofac, WebApplicationFactory and integration tests]]></title>
            <link>https://blog.johnnyreilly.com/2020/05/21/autofac-webapplicationfactory-integration-tests</link>
            <guid>Autofac, WebApplicationFactory and integration tests</guid>
            <pubDate>Thu, 21 May 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[Updated 2nd Oct 2020: for an approach that works with Autofac 6 and ConfigureTestContainer see this post.]]></description>
            <content:encoded><![CDATA[<p><strong>Updated 2nd Oct 2020:</strong> <em>for an approach that works with Autofac 6 and <code>ConfigureTestContainer</code> see <a href="./2020-10-02-autofac-6-integration-tests-and-generic-hosting.md">this post</a>.</em></p><p><img src="../static/blog/2020-05-21-autofac-webapplicationfactory-integration-tests/autofac-webapplicationfactory-tests.png" alt="A title image for the blog featuring the Autofac logo"/></p><p>This is one of those occasions where I&#x27;m not writing up my own work so much as my discovery after in depth googling.</p><p>Integration tests with ASP.NET Core are the best. They spin up an in memory version of your application and let you fire requests at it. They&#x27;ve gone through a number of iterations since ASP.NET Core has been around. You may also be familiar with the <code>TestServer</code> approach of earlier versions. For some time, the advised approach has been using <a href="https://docs.microsoft.com/en-us/aspnet/core/test/integration-tests?view=aspnetcore-3.1#basic-tests-with-the-default-webapplicationfactory"><code>WebApplicationFactory</code></a>.</p><p>What makes this approach particularly useful / powerful is that you can swap out dependencies of your running app with fakes / stubs etc. Just like unit tests! But potentially more useful because they run your whole app and hence give you a greater degree of confidence. What does this mean? Well, imagine you changed a piece of middleware in your application; this could potentially break functionality. Unit tests would probably not reveal this. Integration tests would.</p><p>There is a fly in the ointment. A hair in the gazpacho. ASP.NET Core ships with dependency injection in the box. It has its own Inversion of Control container which is perfectly fine. However, many people are accustomed to using other IOC containers such as <a href="https://autofac.org/">Autofac</a>.</p><p>What&#x27;s the problem? Well, swapping out dependencies registered using ASP.NET Core&#x27;s IOC requires using a hook called <a href="https://docs.microsoft.com/en-us/aspnet/core/test/integration-tests?view=aspnetcore-3.1#inject-mock-services"><code>ConfigureTestServices</code></a>. There&#x27;s an equivalent hook for swapping out services registered using a custom IOC container: <a href="https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.testhost.webhostbuilderextensions.configuretestcontainer?view=aspnetcore-3.0"><code>ConfigureTestContainer</code></a>. Unfortunately, there is a bug in ASP.NET Core as of version 3.0: <a href="https://github.com/dotnet/aspnetcore/issues/14907">When using GenericHost, in tests <code>ConfigureTestContainer</code> is not executed</a></p><p>This means you cannot swap out dependencies that have been registered with Autofac and the like. According to the tremendous <a href="https://www.twitter.com/davidfowl">David Fowler</a> of the ASP.NET team, <a href="https://github.com/dotnet/aspnetcore/issues/14907#issuecomment-592102145">this will hopefully be resolved</a>.</p><p>In the meantime, <a href="https://github.com/dotnet/aspnetcore/issues/14907#issuecomment-620750841">there&#x27;s a workaround thanks to various commenters on the thread</a>. Instead of using <code>WebApplicationFactory</code> directly, subclass it and create a custom <code>AutofacWebApplicationFactory</code> (the name is not important). This custom class overrides the behavior of <code>ConfigureServices</code> and <code>CreateHost</code> with a <code>CustomServiceProviderFactory</code>:</p><pre><code class="language-cs">namespace My.Web.Tests.Helpers {
    /// &lt;summary&gt;
    /// Based upon https://github.com/dotnet/AspNetCore.Docs/tree/master/aspnetcore/test/integration-tests/samples/3.x/IntegrationTestsSample
    /// &lt;/summary&gt;
    /// &lt;typeparam name=&quot;TStartup&quot;&gt;&lt;/typeparam&gt;
    public class AutofacWebApplicationFactory&lt;TStartup&gt; : WebApplicationFactory&lt;TStartup&gt; where TStartup : class {
        protected override void ConfigureWebHost(IWebHostBuilder builder) {
            builder.ConfigureServices(services =&gt; {
                    services.AddSingleton&lt;IAuthorizationHandler&gt;(new PassThroughPermissionedRolesHandler());
                })
                .ConfigureTestServices(services =&gt; {
                }).ConfigureTestContainer&lt;Autofac.ContainerBuilder&gt;(builder =&gt; {
                    // called after Startup.ConfigureContainer
                });
        }

        protected override IHost CreateHost(IHostBuilder builder) {
            builder.UseServiceProviderFactory(new CustomServiceProviderFactory());
            return base.CreateHost(builder);
        }
    }

    /// &lt;summary&gt;
    /// Based upon https://github.com/dotnet/aspnetcore/issues/14907#issuecomment-620750841 - only necessary because of an issue in ASP.NET Core
    /// &lt;/summary&gt;
    public class CustomServiceProviderFactory : IServiceProviderFactory&lt;CustomContainerBuilder&gt; {
        public CustomContainerBuilder CreateBuilder(IServiceCollection services) =&gt; new CustomContainerBuilder(services);

        public IServiceProvider CreateServiceProvider(CustomContainerBuilder containerBuilder) =&gt;
        new AutofacServiceProvider(containerBuilder.CustomBuild());
    }

    public class CustomContainerBuilder : Autofac.ContainerBuilder {
        private readonly IServiceCollection services;

        public CustomContainerBuilder(IServiceCollection services) {
            this.services = services;
            this.Populate(services);
        }

        public Autofac.IContainer CustomBuild() {
            var sp = this.services.BuildServiceProvider();
#pragma warning disable CS0612 // Type or member is obsolete
            var filters = sp.GetRequiredService&lt;IEnumerable&lt;IStartupConfigureContainerFilter&lt;Autofac.ContainerBuilder&gt;&gt;&gt;();
#pragma warning restore CS0612 // Type or member is obsolete

            foreach (var filter in filters) {
                filter.ConfigureContainer(b =&gt; { }) (this);
            }

            return this.Build();
        }
    }
}
</code></pre><p>I&#x27;m going to level with you; I don&#x27;t understand all of this code. I&#x27;m not au fait with the inner workings of ASP.NET Core or Autofac but I can tell you what this allows. With this custom <code>WebApplicationFactory</code> in play you get <code>ConfigureTestContainer</code> back in the mix! You get to write code like this:</p><pre><code class="language-cs">using System;
using System.Net;
using System.Net.Http.Headers;
using System.Threading.Tasks;
using FakeItEasy;
using FluentAssertions;
using Microsoft.AspNetCore.TestHost;
using Microsoft.Extensions.DependencyInjection;
using Xunit;
using Microsoft.Extensions.Options;
using Autofac;
using System.Net.Http;
using Newtonsoft.Json;

namespace My.Web.Tests.Controllers
{
    public class MyControllerTests : IClassFixture&lt;AutofacWebApplicationFactory&lt;My.Web.Startup&gt;&gt; {
        private readonly AutofacWebApplicationFactory&lt;My.Web.Startup&gt; _factory;

        public MyControllerTests(
            AutofacWebApplicationFactory&lt;My.Web.Startup&gt; factory
        ) {
            _factory = factory;
        }

        [Fact]
        public async Task My() {
            var fakeSomethingService = A.Fake&lt;IMySomethingService&gt;();
            var fakeConfig = Options.Create(new MyConfiguration {
                SomeConfig = &quot;Important thing&quot;,
                OtherConfigMaybeAnEmailAddress = &quot;johnny_reilly@hotmail.com&quot;
            });

            A.CallTo(() =&gt; fakeSomethingService.DoSomething(A&lt;string&gt;.Ignored))
                .Returns(Task.FromResult(true));

            void ConfigureTestServices(IServiceCollection services) {
                services.AddSingleton(fakeConfig);
            }

            void ConfigureTestContainer(ContainerBuilder builder) {
                builder.RegisterInstance(fakeSomethingService);
            }

            var client = _factory
                .WithWebHostBuilder(builder =&gt; {
                    builder.ConfigureTestServices(ConfigureTestServices);
                    builder.ConfigureTestContainer&lt;Autofac.ContainerBuilder&gt;(ConfigureTestContainer);
                })
                .CreateClient();

            // Act
            var request = StringContent(&quot;{\&quot;sommat\&quot;:\&quot;to see\&quot;}&quot;);
            request.Headers.ContentType = MediaTypeHeaderValue.Parse(&quot;application/json&quot;);
            var response = await client.PostAsync(&quot;/something/submit&quot;, request);

            // Assert
            response.StatusCode.Should().Be(HttpStatusCode.OK);

            A.CallTo(() =&gt; fakeSomethingService.DoSomething(A&lt;string&gt;.Ignored))
                .MustHaveHappened();
        }

    }
}
</code></pre>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[From react-window to react-virtual]]></title>
            <link>https://blog.johnnyreilly.com/2020/05/10/from-react-window-to-react-virtual</link>
            <guid>From react-window to react-virtual</guid>
            <pubDate>Sun, 10 May 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[The tremendous Tanner Linsley recently released react-virtual. react-virtual provides "hooks for virtualizing scrollable elements in React".]]></description>
            <content:encoded><![CDATA[<p>The tremendous <a href="https://twitter.com/tannerlinsley">Tanner Linsley</a> recently released <a href="https://github.com/tannerlinsley/react-virtual"><code>react-virtual</code></a>. <code>react-virtual</code> provides &quot;hooks for virtualizing scrollable elements in React&quot;.</p><p>I was already using the (also excellent) <a href="https://github.com/bvaughn/react-window"><code>react-window</code></a> for this purpose. <code>react-window</code> does the virtualising job and does it very well indeed However, I was both intrigued by the lure of the new shiny thing. I&#x27;ve also never been the biggest fan of <code>react-window</code>&#x27;s API. So I tried switching over from <code>react-window</code> to <code>react-virtual</code> as an experiment. To my delight, the experiment went so well I didn&#x27;t look back!</p><p>What did I get out of the switch?</p><ul><li>Simpler code / nicer developer ergonomics. The API for <code>react-virtual</code> allowed me to simplify my code and lose a layer of components.</li><li>TypeScript support in the box</li><li>Improved perceived performance. I didn&#x27;t run any specific tests to quantify this, but I can say that the same functionality now feels snappier.</li></ul><p>I tweeted my delight at this and Tanner asked if there was commit diff I could share. I couldn&#x27;t as it&#x27;s a private codebase, but I thought it could form the basis of a blogpost.</p><blockquote><p>Nice! Do you have a commit diff we could see?</p><p>— Tanner Linsley ⚛️ (@tannerlinsley) <a href="https://twitter.com/tannerlinsley/status/1259503283103608832?ref_src=twsrc%5Etfw">May 10, 2020</a></p></blockquote><script src="https://platform.twitter.com/widgets.js" charSet="utf-8"></script><p>In case you hadn&#x27;t guessed, this is that blog post...</p><h2>Make that change</h2><p>So what does the change look like? Well first remove <code>react-window</code> from your project:</p><pre><code>yarn remove react-window @types/react-window
</code></pre><p>Add the dependency to <code>react-virtual</code>:</p><pre><code>yarn add react-virtual
</code></pre><p>Change your imports from:</p><pre><code class="language-ts">import { FixedSizeList, ListChildComponentProps } from &#x27;react-window&#x27;;
</code></pre><p>to:</p><pre><code class="language-ts">import { useVirtual } from &#x27;react-virtual&#x27;;
</code></pre><p>Change your component code from:</p><pre><code class="language-ts">type ImportantDataListProps = {
  classes: ReturnType&lt;typeof useStyles&gt;;
  importants: ImportantData[];
};

const ImportantDataList: React.FC&lt;ImportantDataListProps&gt; = React.memo(
  (props) =&gt; (
    &lt;FixedSizeList
      height={400}
      width={&#x27;100%&#x27;}
      itemSize={80}
      itemCount={props.importants.length}
      itemData={props}
    &gt;
      {RenderRow}
    &lt;/FixedSizeList&gt;
  )
);

type ListItemProps = {
  classes: ReturnType&lt;typeof useStyles&gt;;
  importants: ImportantData[];
};

function RenderRow(props: ListChildComponentProps) {
  const { index, style } = props;
  const { importants, classes } = props.data as ListItemProps;
  const important = importants[index];

  return (
    &lt;ListItem button style={style} key={index}&gt;
      &lt;ImportantThing classes={classes} important={important} /&gt;
    &lt;/ListItem&gt;
  );
}
</code></pre><p>Of the above you can delete the <code>ListItemProps</code> type and the associate <code>RenderRow</code> function. You won&#x27;t need them again! There&#x27;s no longer a need to pass down data to the child element and then extract it for usage; it all comes down into a single simpler component.</p><p>Replace the <code>ImportantDataList</code> component with this:</p><pre><code class="language-ts">const ImportantDataList: React.FC&lt;ImportantDataListProps&gt; = React.memo(
  (props) =&gt; {
    const parentRef = React.useRef&lt;HTMLDivElement&gt;(null);

    const rowVirtualizer = useVirtual({
      size: props.importants.length,
      parentRef,
      estimateSize: React.useCallback(() =&gt; 80, []), // This is just a best guess
      overscan: 5,
    });

    return (
      &lt;div
        ref={parentRef}
        style={{
          width: `100%`,
          height: `500px`,
          overflow: &#x27;auto&#x27;,
        }}
      &gt;
        &lt;div
          style={{
            height: `${rowVirtualizer.totalSize}px`,
            width: &#x27;100%&#x27;,
            position: &#x27;relative&#x27;,
          }}
        &gt;
          {rowVirtualizer.virtualItems.map((virtualRow) =&gt; (
            &lt;div
              key={virtualRow.index}
              ref={virtualRow.measureRef}
              className={props.classes.hoverRow}
              style={{
                position: &#x27;absolute&#x27;,
                top: 0,
                left: 0,
                width: &#x27;100%&#x27;,
                height: `${virtualRow.size}px`,
                transform: `translateY(${virtualRow.start}px)`,
              }}
            &gt;
              &lt;ImportantThing
                classes={props.classes}
                important={props.importants[virtualRow.index]}
              /&gt;
            &lt;/div&gt;
          ))}
        &lt;/div&gt;
      &lt;/div&gt;
    );
  }
);
</code></pre><p>And you are done! Thanks Tanner for this tremendous library!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Up to the clouds!]]></title>
            <link>https://blog.johnnyreilly.com/2020/04/04/up-to-clouds</link>
            <guid>Up to the clouds!</guid>
            <pubDate>Sat, 04 Apr 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[This last four months has been quite the departure for me. Most typically I find myself building applications; for this last period of time I've been taking the platform that I work on, and been migrating it from running on our on premise servers to running in the cloud.]]></description>
            <content:encoded><![CDATA[<p>This last four months has been quite the departure for me. Most typically I find myself building applications; for this last period of time I&#x27;ve been taking the platform that I work on, and been migrating it from running on our on premise servers to running in the cloud.</p><p>This turned out to be much more difficult than I&#x27;d expected and for reasons that often surprised me. We knew where we wanted to get to, but not all of what we&#x27;d need to do to get there. So many things you can only learn by doing. Whilst these experiences are still fresh in my mind I wanted to document some of the challenges we faced.</p><h2>The mission</h2><p>At the start of January, the team decided to make a concerted effort to take our humble ASP.NET Core application and migrate it to the cloud. We sat down with some friends from the DevOps team who are part of our organisation. We&#x27;re fortunate in that these marvellous people are very talented engineers indeed. It was going to be a collaboration between our two teams of budding cloudmongers that would make this happen.</p><p>Now our application is young. It is not much more than a year old. However it is growing <em>fast</em>. And as we did the migration from on premise to the cloud, that wasn&#x27;t going to stop. Development of the application was to continue as is, shipping new versions daily. Without impeding that, we were to try and get the application migrated to the cloud.</p><p>I would liken it to boarding a speeding train, fighting your way to the front, taking the driver hostage and then diverting the train onto a different track. It was challenging. Really, really challenging.</p><p>So many things had to change for us to get from on premise servers to the cloud, all the while keeping our application a going (and shipping) concern. Let&#x27;s go through them one by one.</p><h2>Kubernetes and Docker</h2><p>Our application was built using ASP.NET Core. A technology that is entirely cloud friendly (that&#x27;s one of the reasons we picked it). We were running on a collection of hand installed, hand configured Windows servers. That had to change. We wanted to move our application to run on Kubernetes; so we didn&#x27;t have to manually configure servers. Rather k8s would manage the provisioning and deployment of containers running our application. Worth saying now: I knew <em>nothing</em> about Kubernetes. Or nearly nothing. I learned a bunch along the way, but, as I&#x27;ve said, this was a collaboration between our team and the mighty site reliability engineers of the DevOps team. They knew a <em>lot</em> about this k8s stuff and moreoften than not, our team stood back and let them work their magic.</p><p>In order that we could migrate to running in k8s, we first needed to containerise our application. We needed a <code>Dockerfile</code>. There followed a good amount of experimentation as we worked out how to build ourselves images. There&#x27;s an art to building an optimal Docker image.</p><p>So that we can cover a lot of ground, this post will remain relatively high level. So here&#x27;s a number of things that we encountered along the way that are worth considering:</p><ul><li>Multi-stage builds were an absolute necessity for us. We&#x27;d build the front end of our app (React / TypeScript) using one stage with a <a href="https://hub.docker.com/_/node">Node base image</a>. Then we&#x27;d build our app using a <a href="https://hub.docker.com/_/microsoft-dotnet-core-sdk/">.NET Core SDK base image</a>. Finally, we&#x27;d use a <a href="https://hub.docker.com/_/microsoft-dotnet-core-aspnet">ASP.Net</a> image to run the app; copying in the output of previous stages.</li><li>Our application accesses various SQL Server databases. We struggled to get our application to connect to them. The issue related to the SSL configuration of our runner image. The fix was simple but frustrating; use a <code>-bionic</code> image as it has the configuration you need. We found that gem <a href="https://github.com/dotnet/SqlClient/issues/222#issuecomment-535802822">here</a>.</li><li>Tests. Automated tests. We want to run them in our build; but how? Once more multi-stage builds to the rescue. We&#x27;d build our application, then in a separate stage we&#x27;d run the tests; copying in the app from the build stage. If the tests failed, the build failed. If they passed then the intermediate stage containing the tests would be discarded by Docker. No unnecessary bloat of the image; all that testing goodness still; now in containerised form!</li></ul><h2>Jenkins</h2><p>Our on premise world used TeamCity for our continuous integration needs and Octopus for deployment. We liked these tools well enough; particularly Octopus. However, the DevOps team were very much of the mind that we should be use Jenkins instead. And <a href="https://jenkins.io/doc/book/pipeline/">Pipeline</a>. It was here that we initially struggled. To quote the docs:</p><blockquote><p>Jenkins Pipeline (or simply &quot;Pipeline&quot; with a capital &quot;P&quot;) is a suite of plugins which supports implementing and integrating continuous delivery pipelines into Jenkins.</p></blockquote><p>Whilst continuous delivery is super cool, and is something our team was interested in, we weren&#x27;t ready for it yet. We didn&#x27;t yet have the kind of automated testing in place that gave us the confidence that we&#x27;d need to move to it. One day, but not today. For now there was still some manual testing done on each release, prior to shipping. Octopus suited us very well here as it allowed us to deploy, on demand, a build of our choice to a given environment. So the question was: what to do? Fortunately the immensely talented Aby Egea came up with a mechanism that supported that very notion. A pipeline that would, optionally, deploy our build to a specified environment. So we were good!</p><p>One thing we got to really appreciate about Jenkins was that the build is scripted with a <a href="https://jenkins.io/doc/book/pipeline/jenkinsfile/">Jenkinsfile</a>. This was in contrast to our TeamCity world where it was all manually configured. <a href="https://jenkins.io/projects/jcasc/">Configuration as code</a> is truly a wonderful thing as your build pipeline becomes part of your codebase; open for everyone to see and understand. If anyone wants to change the build pipeline it has to get code reviewed like everything else. It was as code in our <code>Jenkinsfile</code> that the deployment mechanism lived.</p><h2>Vault</h2><p>Another thing that we used Octopus for was secrets. Applications run on configuration; these are settings that drive the behaviour of your application. A subset of configuration is &quot;secrets&quot;. Secrets are configuration that can&#x27;t be stored in source code; they would represent a risk if they did. For instance a database connection string. We&#x27;d been merrily using Octopus for this; as Octopus deploys an application to a server it enriches the <code>appsettings.json</code> file with any required secrets.</p><p>Without Octopus in the mix, how were we to handle our secrets? The answer is with <a href="https://www.vaultproject.io/">Hashicorp Vault</a>. We&#x27;d store our secrets in there and, thanks to clever work by <a href="https://uk.linkedin.com/in/robert-grzankowski-53618114">Robski</a> of the DevOps team, when our container was brought up by Kubernetes, it would mount into the filesystem an <code>appsettings.Vault.json</code> file which we read thanks to our trusty friend <a href="https://docs.microsoft.com/en-us/aspnet/core/fundamentals/configuration/?view=aspnetcore-3.1#json-configuration-provider"><code>.AddJsonFile</code></a> with <code>optional: true</code>. (As the file didn&#x27;t exist in our development environment.)</p><p>Hey presto! Safe secrets in k8s.</p><h2>Networking</h2><p>Our on premise servers sat on the company network. They could see <em>everything</em> that there was to see. All the other servers around them on the network, bleeping and blooping. The opposite was true in AWS. There was nothing to see. Nothing to access. As it should be. It&#x27;s safer that way should a machine become compromised. For each database and each API our application depended upon, we needed to specifically allowlist access.</p><h2>Kerberos</h2><p>There&#x27;s always a fly in the ointment. A nasty surprise on a dark night. Ours was realising that our application depended upon an API that was secured using <a href="https://docs.microsoft.com/en-us/iis/configuration/system.webserver/security/authentication/windowsauthentication/">Windows Authentication</a>. Our application was accessing it by running under a service account which had been permissioned to access it. However, in AWS, our application wasn&#x27;t running as under a service account on the company network. Disappointingly, in the short term the API was not going to support an alternate authentication mechanism.</p><p>What to do? Honestly it wasn&#x27;t looking good. We were considering proxying through one of our Windows servers just to get access to that API. I was tremendously disappointed. At this point our hero arrived; one <a href="https://twitter.com/foldr">JMac</a> hacked together a Kerberos sidecar approach one weekend. You can see a similar approach <a href="https://github.com/edseymour/kinit-sidecar">here</a>. This got us to a point that allowed us to access the API we needed to.</p><p>I&#x27;m kind of amazed that there isn&#x27;t better documentation out there around have a Kerberos sidecar in a k8s setup. Tragically Windows Authentication is a widely used authentication mechanism. That being the case, having good docs to show how you can get a Kerberos sidecar in place would likely greatly advance the ability of enterprises to migrate to the cloud. The best docs I&#x27;ve found are <a href="https://blog.openshift.com/kerberos-sidecar-container/">here</a>. It is super hard though. <em>So hard!</em></p><h2>Hangfire</h2><p>We were using <a href="https://docs.microsoft.com/en-us/aspnet/core/fundamentals/host/hosted-services?view=aspnetcore-3.1&amp;tabs=visual-studio">Hosted Services</a> to perform background task running in our app. The nature of our background tasks meant that it was important to only run a single instance of a background task at a time. Or bad things would happen. This was going to become a problem since we had ambitions to be able to horizontally scale our application; to add new pods as running our app as demand determined.</p><p>So we started to use <a href="https://www.hangfire.io/">Hangfire</a> to perform task running in our app. With Hangfire, when a job is picked up it gets locked so other servers can&#x27;t pick it up. That&#x27;s what we need.</p><p>Hangfire is pretty awesome. However it turns out that there&#x27;s quirks when you move to a containerised environment. We have a number of recurring jobs that are scheduled to run at certain dates and times. In order that Hangfire can ascertain what time it is, it needs a timezone. It turns out that timezones on Windows != timezones in Docker / Linux.</p><p>This was a problem because, as we limbered up for the great migration, we were trying to run our cloud implementation side by side with our on premise one. And Windows picked a fight with Linux over timezones. You can see others bumping into this condition <a href="https://github.com/HangfireIO/Hangfire/issues/1268">here</a>. We learned this the hard way; jobs mysteriously stopping due to timezone related errors. Windows Hangfire not able to recognise Linux Hangfire timezones and vica versa.</p><p>The TL;DR is that we had to do a hard switch with Hangfire; it couldn&#x27;t run side by side. Not the end of the world, but surprising.</p><h2>Azure Active Directory Single Sign-On</h2><p>Historically our application had used two modes of authentication; Windows Authentication and cookies. Windows Authentication doesn&#x27;t generally play nicely with Docker. It&#x27;s doable, but it&#x27;s not the hill you want to die on. So we didn&#x27;t; we swapped out Windows Authentication for <a href="https://docs.microsoft.com/en-us/azure/active-directory/manage-apps/what-is-single-sign-on">Azure AD SSO</a> and didn&#x27;t look back.</p><p>We also made some changes so our app would support cookies auth alongside Azure AD auth; <a href="https://blog.johnnyreilly.com/2020/03/dual-boot-authentication-with-aspnetcore.html">I&#x27;ve written about this previously</a>.</p><h2>Do the right thing and tell people about it</h2><p>We&#x27;re there now; we&#x27;ve made the move. It was a difficult journey but one worth making; it sets up our platform for where we want to take it in the future. Having infrastructure as code makes all kinds of approaches possible that weren&#x27;t before. Here&#x27;s some things we&#x27;re hoping to get out of the move:</p><ul><li>blue green deployments - shipping without taking down our platform</li><li>provision environments on demand - currently we have a highly contended situation when it comes to test environments. With k8s and AWS we can look at spinning up environments as we need them and throwing them away also</li><li>autoscaling for need - we can start to look at spinning up new containers in times of high load and removing excessive containers in times of low load</li></ul><p>We&#x27;ve also become more efficient as a team. We are no longer maintaining servers, renewing certificates, installing software, RDPing onto boxes. All that time and effort we can plough back into making awesome experiences for our users.</p><p>There&#x27;s a long list of other benefits and it&#x27;s very exciting indeed! It&#x27;s not enough for us to have done this though. It&#x27;s important that we tell the story of what we&#x27;ve done and how and why we&#x27;ve done it. That way people have empathy for the work. Also they can start to think about how they could start to reap similar benefits themselves. By talking to others about the road we&#x27;ve travelled, we can save them time and help them to travel a similar road. This is good for them and it&#x27;s good for us; it helps our relationships and it helps us all to move forwards together.</p><p>A rising tide lifts all boats. By telling others about our journey, we raise the water level. Up to the clouds!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Offline storage in a PWA]]></title>
            <link>https://blog.johnnyreilly.com/2020/03/29/offline-storage-in-pwa</link>
            <guid>Offline storage in a PWA</guid>
            <pubDate>Sun, 29 Mar 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[When you are building any kind of application it's typical to want to store information which persists beyond a single user session. Sometimes that will be information that you'll want to live in some kind of centralised database, but not always.]]></description>
            <content:encoded><![CDATA[<p>When you are building any kind of application it&#x27;s typical to want to store information which persists beyond a single user session. Sometimes that will be information that you&#x27;ll want to live in some kind of centralised database, but not always.</p><p>Also, you may want that data to still be available if your user is offline. Even if they can&#x27;t connect to the network, the user may still be able to use the app to do meaningful tasks; but the app will likely require a certain amount of data to drive that.</p><p>How can we achieve this in the context of a PWA?</p><h2>The problem with <code>localStorage</code></h2><p>If you were building a classic web app you&#x27;d probably be reaching for <a href="https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage"><code>Window.localStorage</code></a> at this point. <code>Window.localStorage</code> is a long existing API that stores data beyond a single session. It has a simple API and is very easy to use. However, it has a couple of problems:</p><ol><li><code>Window.localStorage</code> is synchronous. Not a tremendous problem for every app, but if you&#x27;re building something that has significant performance needs then this could become an issue.</li><li><code>Window.localStorage</code> cannot be used in the context of a <code>Worker</code> or a <code>ServiceWorker</code>. The APIs are not available there.</li><li><code>Window.localStorage</code> stores only <code>string</code>s. Given <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/JSON/stringify"><code>JSON.stringify</code></a> and <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/JSON/parse"><code>JSON.parse</code></a> that&#x27;s not a big problem. But it&#x27;s an inconvenience.</li></ol><p>The second point here is the significant one. If we&#x27;ve a need to access our offline data in the context of a <code>ServiceWorker</code> (and if you&#x27;re offline you&#x27;ll be using a <code>ServiceWorker</code>) then what do you do?</p><h2>IndexedDB to the rescue?</h2><p>Fortunately, <code>localStorage</code> is not the only game in town. There&#x27;s alternative offline storage mechanism available in browsers with the curious name of <a href="https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API">IndexedDB</a>. To quote the docs:</p><blockquote><p>IndexedDB is a transactional database system, like an SQL-based RDBMS. However, unlike SQL-based RDBMSes, which use fixed-column tables, IndexedDB is a JavaScript-based object-oriented database. IndexedDB lets you store and retrieve objects that are indexed with a key; any objects supported by the structured clone algorithm can be stored. You need to specify the database schema, open a connection to your database, and then retrieve and update data within a series of transactions.</p></blockquote><p>It&#x27;s clear that IndexedDB is <em>very</em> powerful. But it doesn&#x27;t sound very simple. A further look at the <a href="https://github.com/mdn/to-do-notifications/blob/8b3e1708598e42062b0136608b1c5fbb66520f0a/scripts/todo.js#L48">MDN example</a> of how to interact with IndexedDB does little to remove that thought.</p><p>We&#x27;d like to be able to access data offline; but in a simple fashion. Like we could with <code>localStorage</code> which has a wonderfully straightforward API. If only someone would build an astraction on top of IndexedDB to make our lives easier...</p><p>Someone did.</p><h2>IDB-Keyval to the rescue!</h2><p>The excellent <a href="https://twitter.com/jaffathecake">Jake Archibald</a> of Google has written <a href="https://github.com/jakearchibald/idb-keyval">IDB-Keyval</a> which is:</p><blockquote><p>A super-simple-small promise-based keyval store implemented with IndexedDB</p></blockquote><p>The API is essentially equivalent to <code>localStorage</code> with a few lovely differences:</p><ol><li>The API is promise based; all functions return a <code>Promise</code>; this makes it a non-blocking API.</li><li>The API is not restricted to <code>string</code>s as <code>localStorage</code> is. To quote the docs: <em>this is IDB-backed, you can store anything structured-clonable (numbers, arrays, objects, dates, blobs etc)</em></li><li>Because this is abstraction built on top of IndexedDB, it can be used both in the context of a typical web app and also in a <code>Worker</code> or a <code>ServiceWorker</code> if required.</li></ol><h2>Simple usage</h2><p>Let&#x27;s take a look at what usage of <code>IDB-Keyval</code> might be like. For that we&#x27;re going to need an application. It would be good to be able to demonstrate both simple usage and also how usage in the context of an application might look.</p><p>Let&#x27;s spin up a TypeScript React app with <a href="https://create-react-app.dev/">Create React App</a>:</p><pre><code class="language-shell">npx create-react-app offline-storage-in-a-pwa --template typescript
</code></pre><p>This creates us a simple app. Now let&#x27;s add IDB-Keyval to it:</p><pre><code class="language-shell">yarn add idb-keyval
</code></pre><p>Then, let&#x27;s update the <code>index.tsx</code> file to add a function that tests using IDB-Keyval:</p><pre><code class="language-tsx">import React from &#x27;react&#x27;;
import ReactDOM from &#x27;react-dom&#x27;;
import { set, get } from &#x27;idb-keyval&#x27;;
import &#x27;./index.css&#x27;;
import App from &#x27;./App&#x27;;
import * as serviceWorker from &#x27;./serviceWorker&#x27;;

ReactDOM.render(&lt;App /&gt;, document.getElementById(&#x27;root&#x27;));

serviceWorker.register();

async function testIDBKeyval() {
  await set(&#x27;hello&#x27;, &#x27;world&#x27;);
  const whatDoWeHave = await get(&#x27;hello&#x27;);
  console.log(
    `When we queried idb-keyval for &#x27;hello&#x27;, we found: ${whatDoWeHave}`
  );
}

testIDBKeyval();
</code></pre><p>As you can see, we&#x27;ve added a <code>testIDBKeyval</code> function which does the following:</p><ol><li>Adds a value of <code>&#x27;world&#x27;</code> to IndexedDB using IDB-Keyval for the key of <code>&#x27;hello&#x27;</code></li><li>Queries IndexedDB using IDB-Keyval for the key of <code>&#x27;hello&#x27;</code> and stores it in the variable <code>whatDoWeHave</code></li><li>Logs out what we found.</li></ol><p>You&#x27;ll also note that <code>testIDBKeyval</code> is an <code>async</code> function. This is so that we can use <code>await</code> when we&#x27;re interacting with IDB-Keyval. Given that its API is <code>Promise</code> based, it is <code>await</code> friendly. Where you&#x27;re performing more than an a single asynchronous operation at a time, it&#x27;s often valuable to use <code>async</code> / <code>await</code> to increase the readability of your codebase.</p><p>What happens when we run our application with <code>yarn start</code>? Let&#x27;s do that and take a look at the devtools:</p><p><img src="../static/blog/2020-03-29-offline-storage-in-pwa/hello_world_idb_keyval.png"/></p><p>We successfully wrote something into IndexedDB, read it back and printed that value to the console. Amazing!</p><h2>Usage in React</h2><p>What we&#x27;ve done so far is slightly abstract. It would be good to implement a real-world use case. Let&#x27;s create an application which gives users the choice between using a &quot;Dark mode&quot; version of the app or not. To do that we&#x27;ll replace our <code>App.tsx</code> with this:</p><pre><code class="language-tsx">import React, { useState } from &#x27;react&#x27;;
import &#x27;./App.css&#x27;;

const sharedStyles = {
  height: &#x27;30rem&#x27;,
  fontSize: &#x27;5rem&#x27;,
  textAlign: &#x27;center&#x27;,
} as const;

function App() {
  const [darkModeOn, setDarkModeOn] = useState(true);
  const handleOnChange = ({ target }: React.ChangeEvent&lt;HTMLInputElement&gt;) =&gt;
    setDarkModeOn(target.checked);

  const styles = {
    ...sharedStyles,
    ...(darkModeOn
      ? {
          backgroundColor: &#x27;black&#x27;,
          color: &#x27;white&#x27;,
        }
      : {
          backgroundColor: &#x27;white&#x27;,
          color: &#x27;black&#x27;,
        }),
  };

  return (
    &lt;div style={styles}&gt;
      &lt;input
        type=&quot;checkbox&quot;
        value=&quot;darkMode&quot;
        checked={darkModeOn}
        id=&quot;darkModeOn&quot;
        name=&quot;darkModeOn&quot;
        style={{ width: &#x27;3rem&#x27;, height: &#x27;3rem&#x27; }}
        onChange={handleOnChange}
      /&gt;
      &lt;label htmlFor=&quot;darkModeOn&quot;&gt;Use dark mode?&lt;/label&gt;
    &lt;/div&gt;
  );
}

export default App;
</code></pre><p>When you run the app you can see how it works:</p><p><img src="../static/blog/2020-03-29-offline-storage-in-pwa/use-dark-mode.gif"/></p><p>Looking at the code you&#x27;ll be able to see that this is implemented using React&#x27;s <code>useState</code> hook. So any user preference selected will be lost on a page refresh. Let&#x27;s see if we can take this state and move it into IndexedDB using <code>IDB-Keyval</code>.</p><p>We&#x27;ll change the code like so:</p><pre><code class="language-tsx">import React, { useState, useEffect } from &#x27;react&#x27;;
import { set, get } from &#x27;idb-keyval&#x27;;
import &#x27;./App.css&#x27;;

const sharedStyles = {
  height: &#x27;30rem&#x27;,
  fontSize: &#x27;5rem&#x27;,
  textAlign: &#x27;center&#x27;,
} as const;

function App() {
  const [darkModeOn, setDarkModeOn] = useState&lt;boolean | undefined&gt;(undefined);

  useEffect(() =&gt; {
    get&lt;boolean&gt;(&#x27;darkModeOn&#x27;).then((value) =&gt;
      // If a value is retrieved then use it; otherwise default to true
      setDarkModeOn(value ?? true)
    );
  }, [setDarkModeOn]);

  const handleOnChange = ({ target }: React.ChangeEvent&lt;HTMLInputElement&gt;) =&gt; {
    setDarkModeOn(target.checked);

    set(&#x27;darkModeOn&#x27;, target.checked);
  };

  const styles = {
    ...sharedStyles,
    ...(darkModeOn
      ? {
          backgroundColor: &#x27;black&#x27;,
          color: &#x27;white&#x27;,
        }
      : {
          backgroundColor: &#x27;white&#x27;,
          color: &#x27;black&#x27;,
        }),
  };

  return (
    &lt;div style={styles}&gt;
      {darkModeOn === undefined ? (
        &lt;&gt;Loading preferences...&lt;/&gt;
      ) : (
        &lt;&gt;
          &lt;input
            type=&quot;checkbox&quot;
            value=&quot;darkMode&quot;
            checked={darkModeOn}
            id=&quot;darkModeOn&quot;
            name=&quot;darkModeOn&quot;
            style={{ width: &#x27;3rem&#x27;, height: &#x27;3rem&#x27; }}
            onChange={handleOnChange}
          /&gt;
          &lt;label htmlFor=&quot;darkModeOn&quot;&gt;Use dark mode?&lt;/label&gt;
        &lt;/&gt;
      )}
    &lt;/div&gt;
  );
}

export default App;
</code></pre><p>The changes here are:</p><ol><li><code>darkModeOn</code> is now initialised to <code>undefined</code> and the app displays a loading message until <code>darkModeOn</code> has a value.</li><li>The app attempts to app load a value from IDB-Keyval with the key <code>&#x27;darkModeOn&#x27;</code> and set <code>darkModeOn</code> with the retrieved value. If no value is retrieved then it sets <code>darkModeOn</code> to <code>true</code>.</li><li>When the checkbox is changed, the corresponding value is both applied to <code>darkModeOn</code> and saved to IDB-Keyval with the key <code>&#x27;darkModeOn&#x27;</code></li></ol><p>As you can see, this means that we are persisting preferences beyond page refresh in a fashion that will work both online <em>and</em> offline!</p><p><img src="../static/blog/2020-03-29-offline-storage-in-pwa/use-dark-mode-with-idb-keyval.gif"/></p><h2>Usage as a React hook</h2><p>Finally it&#x27;s time for bonus points. Wouldn&#x27;t it be nice if we could move this functionality into a reusable React hook? Let&#x27;s do it!</p><p>Let&#x27;s create a new <code>usePersistedState.ts</code> file:</p><pre><code class="language-ts">import { useState, useEffect, useCallback } from &#x27;react&#x27;;
import { set, get } from &#x27;idb-keyval&#x27;;

export function usePersistedState&lt;TState&gt;(
  keyToPersistWith: string,
  defaultState: TState
) {
  const [state, setState] = useState&lt;TState | undefined&gt;(undefined);

  useEffect(() =&gt; {
    get&lt;TState&gt;(keyToPersistWith).then((retrievedState) =&gt;
      // If a value is retrieved then use it; otherwise default to defaultValue
      setState(retrievedState ?? defaultState)
    );
  }, [keyToPersistWith, setState, defaultState]);

  const setPersistedValue = useCallback(
    (newValue: TState) =&gt; {
      setState(newValue);
      set(keyToPersistWith, newValue);
    },
    [keyToPersistWith, setState]
  );

  return [state, setPersistedValue] as const;
}
</code></pre><p>This new hook is modelled after the API of <a href="https://reactjs.org/docs/hooks-reference.html#usestate"><code>useState</code></a> and is named <code>usePersistentState</code>. It requires that a key be supplied which is the key that will be used to save the data. It also requires a default value to use in the case that nothing is found during the lookup.</p><p>It returns (just like <code>useState</code>) a stateful value, and a function to update it. Finally, let&#x27;s switch over our <code>App.tsx</code> to use our shiny new hook:</p><pre><code class="language-tsx">import React from &#x27;react&#x27;;
import &#x27;./App.css&#x27;;
import { usePersistedState } from &#x27;./usePersistedState&#x27;;

const sharedStyles = {
  height: &#x27;30rem&#x27;,
  fontSize: &#x27;5rem&#x27;,
  textAlign: &#x27;center&#x27;,
} as const;

function App() {
  const [darkModeOn, setDarkModeOn] = usePersistedState&lt;boolean&gt;(
    &#x27;darkModeOn&#x27;,
    true
  );

  const handleOnChange = ({ target }: React.ChangeEvent&lt;HTMLInputElement&gt;) =&gt;
    setDarkModeOn(target.checked);

  const styles = {
    ...sharedStyles,
    ...(darkModeOn
      ? {
          backgroundColor: &#x27;black&#x27;,
          color: &#x27;white&#x27;,
        }
      : {
          backgroundColor: &#x27;white&#x27;,
          color: &#x27;black&#x27;,
        }),
  };

  return (
    &lt;div style={styles}&gt;
      {darkModeOn === undefined ? (
        &lt;&gt;Loading preferences...&lt;/&gt;
      ) : (
        &lt;&gt;
          &lt;input
            type=&quot;checkbox&quot;
            value=&quot;darkMode&quot;
            checked={darkModeOn}
            id=&quot;darkModeOn&quot;
            name=&quot;darkModeOn&quot;
            style={{ width: &#x27;3rem&#x27;, height: &#x27;3rem&#x27; }}
            onChange={handleOnChange}
          /&gt;
          &lt;label htmlFor=&quot;darkModeOn&quot;&gt;Use dark mode?&lt;/label&gt;
        &lt;/&gt;
      )}
    &lt;/div&gt;
  );
}

export default App;
</code></pre><h2>Conclusion</h2><p>This post has demonstrate how a web application or a PWA can safely store data that is persisted between sessions using native browser capabilities easily. IndexedDB powered the solution we&#x27;ve built. We used used <a href="https://github.com/jakearchibald/idb-keyval">IDB-Keyval</a> for the delightful and familiar abstraction it offers over IndexedDB. It&#x27;s allowed us to come up with a solution with a similarly lovely API. It&#x27;s worth knowing that there are alternatives to IDB-Keyval available such as <a href="https://github.com/localForage/localForage">localForage</a>. If you are building for older browsers which may lack good IndexedDB support then this would be a good choice. But be aware that with greater backwards compatibility comes greater download size. Do consider this and make the tradeoffs that make sense for you.</p><p>Finally, I&#x27;ve finished this post illustrating what usage would look like in a React context. Do be aware that there&#x27;s nothing React specific about our offline storage mechanism. So if you&#x27;re rolling with Vue, Angular or something else entirely: <em>this is for you too</em>! Offline storage is a feature that provide much greater user experiences. Please do consider making use of it in your applications.</p><p><a href="https://blog.logrocket.com/offline-storage-for-pwas/">This post was originally published on LogRocket.</a></p><p><a href="https://github.com/johnnyreilly/offline-storage-in-a-pwa">The source code for this project can be found here.</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Dual boot authentication with ASP.NET]]></title>
            <link>https://blog.johnnyreilly.com/2020/03/22/dual-boot-authentication-with-aspnetcore</link>
            <guid>Dual boot authentication with ASP.NET</guid>
            <pubDate>Sun, 22 Mar 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[This is a post about having two kinds of authentication working at the same time in ASP.Net Core. But choosing which authentication method to use dynamically at runtime; based upon the criteria of your choice.]]></description>
            <content:encoded><![CDATA[<p>This is a post about having two kinds of authentication working at the same time in ASP.Net Core. But choosing which authentication method to use dynamically at runtime; based upon the criteria of your choice.</p><p>Already this sounds complicated; let&#x27;s fix that. Perhaps I should describe my situation to you. I&#x27;ve an app which has two classes of user. One class, let&#x27;s call them &quot;customers&quot; (because... uh... they&#x27;re customers). The customers access our application via a public facing website. Traffic rolls through Cloudflare and into our application. The public facing URL is something fancy like <a href="https://mega-app.com">https://mega-app.com</a>. That&#x27;s one class of user.</p><p>The other class of user we&#x27;ll call &quot;our peeps&quot;; because they are <em>us</em>. We use the app that we build. Traffic from &quot;us&quot; comes from a different hostname; only addressable on our network. So URLs from requests that we make are more along the lines of <a href="https://strictly4mypeeps.io">https://strictly4mypeeps.io</a>.</p><p>So far, so uncontroversial. Now it starts to get interesting. Our customers log into our application using their super secret credentials. It&#x27;s <a href="https://docs.microsoft.com/en-us/aspnet/core/security/authentication/cookie?view=aspnetcore-3.1#create-an-authentication-cookie">cookie based authentication</a>. But for our peeps we do something different. Having to enter your credentials each time you use the app is friction. It gets in the way. So for us we have <a href="https://docs.microsoft.com/en-us/aspnet/core/security/authentication/azure-active-directory/?view=aspnetcore-3.1">Azure AD</a> in the mix. Azure AD is how we authenticate ourselves; and that means we don&#x27;t spend 5% of each working day entering credentials.</p><h2>Let us speak of the past</h2><p>Now our delightful little application grew up in a simpler time. A time where you went to the marketplace, picked out some healthy looking servers, installed software upon them, got them attached to the internet, deployed an app onto them and said &quot;hey presto, we&#x27;re live!&quot;.</p><p>Way back when, we had some servers on the internet, that&#x27;s how our customers got to our app. Our peeps, us, we went to other servers that lived on our network. So we had multiple instances of our app, deployed to different machines. The ones on the internet were configured to use cookie based auth, the ones on our internal network were Azure AD.</p><p>As I said, a simpler time.</p><h2>A new hope</h2><p>We&#x27;ve been going through the process of cloudifying our app. Bye, bye servers, hello <a href="https://www.docker.com/">Docker</a> and <a href="https://kubernetes.io/">Kubernetes</a>. So exciting! As we change the way our app is built and deployed; we&#x27;ve been thinking about whether the choices we make still make sense.</p><p>When it came to authentication, my initial thoughts were to continue the same road we&#x27;re travelling; just in containers and pods. So where we had &quot;internal&quot; servers, we&#x27;d have &quot;internal&quot; pods, and where we&#x27;d have &quot;external&quot; servers we&#x27;d have external pods. I had the good fortune to be working with the amazingly talented <a href="https://uk.linkedin.com/in/robert-grzankowski-53618114">Robski</a>. Robski knows far more about K8s and networking than I&#x27;m ever likely to. He&#x27;d regularly say things like &quot;ingress&quot; and &quot;MTLS&quot; whilst I stared blankly at him. He definitely knows stuff.</p><p>Robski challenged my plans. &quot;We don&#x27;t need it. Have one pod that does both sorts of auth. If you do that, your implementation is simpler and scaling is more straightforward. You&#x27;ll only need half the pods because you won&#x27;t need internal <em>and</em> external ones; one pod can handle both sets of traffic. You&#x27;ll save money.&quot;</p><p>I loved the idea but I didn&#x27;t think that ASP.Net Core supported it. &quot;It&#x27;s just not a thing Robski; ASP.Net Core doesn&#x27;t suppport it.&quot; Robski didn&#x27;t believe me. That turned out to a <em>very good thing</em>. There followed a period of much googling and experimentation. One day of hunting in, I was still convinced there was no way to do it that would allow me to look in the mirror without self loathing. Then Robski sent me this:</p><p><img src="../static/blog/2020-03-22-dual-boot-authentication-with-aspnetcore/robski-dynamic-auth.png" alt="screenshot of WhatsApp message with a link in it"/></p><p>It was a link to the amazing <a href="https://twitter.com/davidfowl">David Fowler</a> talking about <a href="https://github.com/aspnet/Security/issues/1469#issuecomment-335027005">some API I&#x27;d never heard of called <code>SchemeSelector</code></a>. It turned out that this was the starting point for exactly what we needed; a way to dynamically select an authentication scheme at runtime.</p><h2>Show me the code</h2><p>This API did end up landing in ASP.Net Core, but with the name <code>ForwardDefaultSelector</code>. Not the most descriptive of names and I&#x27;ve struggled to find any documentation on it at all. What I did discover was <a href="https://stackoverflow.com/a/51897159/761388">an answer on StackOverflow by the marvellous Barbara Post</a>. I was able to take the approach Barbara laid out and use it to my own ends. I ended up with this snippet of code added to my <code>Startup.ConfigureServices</code>:</p><pre><code class="language-cs">services
    .AddAuthentication(sharedOptions =&gt; {
        sharedOptions.DefaultScheme = &quot;WhichAuthDoWeUse&quot;;
        sharedOptions.DefaultAuthenticateScheme = &quot;WhichAuthDoWeUse&quot;;
        sharedOptions.DefaultChallengeScheme = &quot;WhichAuthDoWeUse&quot;;
    })
    .AddPolicyScheme(&quot;WhichAuthDoWeUse&quot;, &quot;Azure AD or Cookies&quot;, options =&gt; {
        options.ForwardDefaultSelector = context =&gt; {
            var (isExternalRequest, requestUrl) = context.Request.GetIsExternalRequestAndDomain();
            if (isExternalRequest) {
                _logger.LogInformation(
                    &quot;Request ({RequestURL}) has come from external domain ({Domain}) so using Cookie Authentication&quot;,
                    requestUrl, ExternalBaseUrl);

                return CookieAuthenticationDefaults.AuthenticationScheme;
           }

           _logger.LogInformation(
               &quot;Request ({RequestURL}) has not come from external domain ({Domain}) so using Azure AD Authentication&quot;,
               requestUrl, ExternalBaseUrl);

            return AzureADDefaults.AuthenticationScheme;
        };
    })
    .AddAzureAD(options =&gt; {
        Configuration.Bind(&quot;AzureAd&quot;, options);
    })
    .AddCookie(options =&gt; {
        options.Cookie.SecurePolicy = CookieSecurePolicy.Always;
        options.Cookie.SameSite = SameSiteMode.Strict;
        options.Cookie.HttpOnly = true;
        options.Events.OnRedirectToAccessDenied = (context) =&gt; {
            context.Response.StatusCode = Microsoft.AspNetCore.Http.StatusCodes.Status401Unauthorized;
            return Task.CompletedTask;
        };

        options.Events.OnRedirectToLogin = (context) =&gt; {
            context.Response.StatusCode = Microsoft.AspNetCore.Http.StatusCodes.Status401Unauthorized;
            return Task.CompletedTask;
        };
    });
</code></pre><p>If you look at this code it&#x27;s doing these things:</p><ol><li>Registering three types of authentication: Cookie, Azure AD and &quot;WhichAuthDoWeUse&quot;</li><li>Registers the default <code>Scheme</code> to be &quot;WhichAuthDoWeUse&quot;.</li></ol><p>&quot;WhichAuthDoWeUse&quot; is effectively an <code>if</code> statement that says, <em>&quot;if this is an external <code>Request</code> use Cookies authentication, otherwise use Azure AD&quot;</em>. Given that &quot;WhichAuthDoWeUse&quot; is the default scheme, this code runs for each request, to determine which authentication method to use.</p><p>Alongside this mechanism I added these extension methods:</p><pre><code class="language-cs">using System;
using Microsoft.AspNetCore.Http;
using Microsoft.AspNetCore.Http.Extensions;

namespace My.App.Auth {
    public static class AuthExtensions {
        public const string ExternalBaseUrl = &quot;https://mega-app.com&quot;;
        public const string InternalBaseUrl = &quot;https://strictly4mypeeps.io&quot;;

        /// &lt;summary&gt;
        /// Determines if a request is an &quot;external&quot; URL (eg begins &quot;https://mega-app.com&quot;)
        /// or an &quot;internal&quot; URL (eg begins &quot;https://strictly4mypeeps.io&quot;)
        /// &lt;/summary&gt;
        public static (bool, string) GetIsExternalRequestAndDomain(this HttpRequest request) {
            var (requestUrl, domain) = GetRequestUrlAndDomain(request);

            var isExternalUrl = domain == ExternalBaseUrl;

            var isUnknownPath = domain == null; // This scenario is extremely unlikely but has been observed once during testing so we will cater for it

            var isExternalRequest = isExternalUrl || isUnknownPath; // If unknown we&#x27;ll treat as &quot;external&quot; for a safe fallback

            return (isExternalRequest, requestUrl);
        }

        /// &lt;summary&gt;
        /// Determines if a request is an &quot;external&quot; URL (eg begins &quot;https://mega-app.com&quot;)
        /// or an &quot;internal&quot; URL (eg begins &quot;https://strictly4mypeeps.io&quot;)
        /// &lt;/summary&gt;
        public static (bool, string) GetIsInternalRequestAndDomain(this HttpRequest request) {
            var (requestUrl, domain) = GetRequestUrlAndDomain(request);

            var isInternalRequest = domain == InternalBaseUrl;

            return (isInternalRequest, requestUrl);
        }

        private static (string, string) GetRequestUrlAndDomain(HttpRequest request) {
            string requestUrl = null;
            string domain = null;
            if (request.Host.HasValue) {
                requestUrl = request.GetEncodedUrl();
                domain = new Uri(requestUrl).GetLeftPart(UriPartial.Authority);
            }

            return (requestUrl, domain);
        }
    }
}
</code></pre><p>Finally, I updated the <code>SpaController.cs</code> (which serves initial requests to our Single Page Application) to cater for having two types of Auth in play:</p><pre><code class="language-cs">        /// &lt;summary&gt;
        /// ASP.NET will try and load the index.html using the FileServer if we don&#x27;t have a route
        /// here to match `/`. These attributes can&#x27;t be on Index or the spa fallback doesn&#x27;t work
        /// Note: this is almost perfect except that if someone actually calls /index.html they&#x27;ll get
        /// the FileServer one, not the one from this file.
        /// &lt;/summary&gt;
        [HttpGet(&quot;/&quot;)]
        [AllowAnonymous]
        public async Task&lt;IActionResult&gt; SpaFallback([FromQuery] string returnUrl) {
            var redirectUrlIfUserIsInternalAndNotAuthenticated = GetRedirectUrlIfUserIsInternalAndNotAuthenticated(returnUrl);

            if (redirectUrlIfUserIsInternalAndNotAuthenticated != null)
                return LocalRedirect(redirectUrlIfUserIsInternalAndNotAuthenticated);

            return await Index(); // Index just serves up our SPA index.html
        }

        /// &lt;summary&gt;
        /// SPA landing with authorisation - this endpoint will typically not be directly navigated to by a user;
        /// rather it will be redirected to from the IndexWithoutAuthorisation and SpaFallback actions above
        /// in the case where a user is *not* authenticated but has come from an internal URL eg https://strictlyformypeeps.io
        /// &lt;/summary&gt;
        [HttpGet(&quot;/login-with-azure-ad&quot;)]
        [Authorize]
        public async Task&lt;IActionResult&gt; IndexWithAuthorisation()
        {
            return await Index(); // Index just serves up our SPA index.html
        }

        /// &lt;summary&gt;
        /// This method returns a RedirectURL if a request is coming from an internal URL
        /// eg https://int.prd.our.cloud and is not authenticated.  In this case
        /// we likely want to trigger authentication by redirecting to an authorized endpoint
        /// &lt;/summary&gt;
        string GetRedirectUrlIfUserIsInternalAndNotAuthenticated(string returnUrl)
        {
            // If a user is authenticated then we don&#x27;t need to trigger authentication
            var isAuthenticated = User?.Identity?.Name != null;
            if (isAuthenticated)
                return null;

            // This scenario is extremely unlikely but has been observed once during testing so we will cater for it
            var (isInternalRequest, requestUrl) = Request.GetIsInternalRequestAndDomain();

            if (isInternalRequest) {
                var redirectUrl = $&quot;/login-with-azure-ad{(string.IsNullOrEmpty(returnUrl) ? &quot;&quot; : &quot;?returnUrl=&quot; + WebUtility.UrlEncode(returnUrl))}&quot;;
                _logger.LogInformation(
                    &quot;Request ({RequestURL}) has come from internal domain ({InternalDomain}) but is not authenticated; redirecting to {RedirectURL}&quot;,
                    requestUrl, AuthExtensions.InternalBaseUrl, redirectUrl);

                return redirectUrl;
            }

            return null;
        }
</code></pre><p>The code above allows anonymous requests to land in our app through the <code>AllowAnonymous</code> attribute. However, it checks the request when it comes in to see if:</p><ol><li>It&#x27;s an internal request (i.e. the Request URL starts &quot;<a href="https://strictly4mypeeps.io/%22">https://strictly4mypeeps.io/&quot;</a>)</li><li>The current user is <em>not</em> authenticated.</li></ol><p>In this case the user is redirected to the <a href="https://strictly4mypeeps.io/login-with-azure-ad">https://strictly4mypeeps.io/login-with-azure-ad</a> route which is decorated with the <code>Authorize</code> attribute. This will trigger authentication for our unauthenticated internal users and drive them through the Azure AD login process.</p><h2>The mystery of no documentation</h2><p>I&#x27;m so surprised that this approach hasn&#x27;t yet been better documented on the (generally superb) ASP.Net Core docs. It&#x27;s such a potentially useful approach; and in our case, money saving too! I hope the official docs feature something on this in future. If they do, and I&#x27;ve just missed it (possible!) then please hit me up in the comments.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Web Workers, comlink, TypeScript and React]]></title>
            <link>https://blog.johnnyreilly.com/2020/02/21/web-workers-comlink-typescript-and-react</link>
            <guid>Web Workers, comlink, TypeScript and React</guid>
            <pubDate>Fri, 21 Feb 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[JavaScript is famously single threaded. However, if you're developing for the web, you may well know that this is not quite accurate. There are Web Workers:]]></description>
            <content:encoded><![CDATA[<p>JavaScript is famously single threaded. However, if you&#x27;re developing for the web, you may well know that this is not quite accurate. There are <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Using_web_workers"><code>Web Workers</code></a>:</p><blockquote><p>A worker is an object created using a constructor (e.g. <code>Worker()</code>) that runs a named JavaScript file — this file contains the code that will run in the worker thread; workers run in another global context that is different from the current window.</p></blockquote><p>Given that there is a way to use other threads for background processing, why doesn&#x27;t this happen all the time? Well there&#x27;s a number of reasons; not the least of which is the ceremony involved in interacting with Web Workers. Consider the following example that illustrates moving a calculation into a worker:</p><pre><code class="language-js">// main.js
function add2NumbersUsingWebWorker() {
  const myWorker = new Worker(&#x27;worker.js&#x27;);

  myWorker.postMessage([42, 7]);
  console.log(&#x27;Message posted to worker&#x27;);

  myWorker.onmessage = function (e) {
    console.log(&#x27;Message received from worker&#x27;, e.data);
  };
}

add2NumbersUsingWebWorker();

// worker.js
onmessage = function (e) {
  console.log(&#x27;Worker: Message received from main script&#x27;);
  const result = e.data[0] * e.data[1];
  if (isNaN(result)) {
    postMessage(&#x27;Please write two numbers&#x27;);
  } else {
    const workerResult = &#x27;Result: &#x27; + result;
    console.log(&#x27;Worker: Posting message back to main script&#x27;);
    postMessage(workerResult);
  }
};
</code></pre><p><em>This is not simple.</em> It&#x27;s hard to understand what&#x27;s happening. Also, this approach only supports a single method call. I&#x27;d much rather write something that looked more like this:</p><pre><code class="language-js">// main.js
function add2NumbersUsingWebWorker() {
  const myWorker = new Worker(&#x27;worker.js&#x27;);

  const total = myWorker.add2Numbers([42, 7]);
  console.log(&#x27;Message received from worker&#x27;, total);
}

add2NumbersUsingWebWorker();

// worker.js
export function add2Numbers(firstNumber, secondNumber) {
  const result = firstNumber + secondNumber;
  return isNaN(result) ? &#x27;Please write two numbers&#x27; : &#x27;Result: &#x27; + result;
}
</code></pre><p>There&#x27;s a way to do this using a library made by Google called <a href="https://github.com/GoogleChromeLabs/comlink">comlink</a>. This post will demonstrate how we can use this. We&#x27;ll use TypeScript and webpack. We&#x27;ll also examine how to integrate this approach into a React app.</p><h2>A use case for a Web Worker</h2><p>Let&#x27;s make ourselves a TypeScript web app. We&#x27;re going to use <code>create-react-app</code> for this:</p><pre><code class="language-shell">npx create-react-app webworkers-comlink-typescript-react --template typescript
</code></pre><p>Create a <code>takeALongTimeToDoSomething.ts</code> file alongside <code>index.tsx</code>:</p><pre><code class="language-ts">export function takeALongTimeToDoSomething() {
  console.log(&#x27;Start our long running job...&#x27;);
  const seconds = 5;
  const start = new Date().getTime();
  const delay = seconds * 1000;

  while (true) {
    if (new Date().getTime() - start &gt; delay) {
      break;
    }
  }
  console.log(&#x27;Finished our long running job&#x27;);
}
</code></pre><p>To <code>index.tsx</code> add this code:</p><pre><code class="language-ts">import { takeALongTimeToDoSomething } from &#x27;./takeALongTimeToDoSomething&#x27;;

// ...

console.log(&#x27;Do something&#x27;);
takeALongTimeToDoSomething();
console.log(&#x27;Do another thing&#x27;);
</code></pre><p>When our application runs we see this behaviour:</p><p><img src="../static/blog/2020-02-21-web-workers-comlink-typescript-and-react/blocking.gif"/></p><p>The app starts and logs <code>Do something</code> and <code>Start our long running job...</code> to the console. It then blocks the UI until the <code>takeALongTimeToDoSomething</code> function has completed running. During this time the screen is empty and unresponsive. This is a poor user experience.</p><h2>Hello <code>worker-plugin</code> and <code>comlink</code></h2><p>To start using comlink we&#x27;re going to need to eject our <code>create-react-app</code> application. The way <code>create-react-app</code> works is by giving you a setup that handles a high percentage of the needs for a typical web app. When you encounter an unsupported use case, you can run the <code>yarn eject</code> command to get direct access to the configuration of your setup.</p><p>Web Workers are not that commonly used in day to day development at present. Consequently there isn&#x27;t yet a &quot;plug&#x27;n&#x27;play&quot; solution for workers supported by <code>create-react-app</code>. There&#x27;s a number of potential ways to support this use case and you can track the various discussions happening against <code>create-react-app</code> that covers this. For now, let&#x27;s eject with:</p><pre><code>yarn eject
</code></pre><p>Then let&#x27;s install the packages we&#x27;re going to be using:</p><ul><li><a href="https://github.com/GoogleChromeLabs/worker-plugin"><code>worker-plugin</code></a> <!-- -->-<!-- --> this webpack plugin automatically compiles modules loaded in Web Workers</li><li><code>comlink</code> <!-- -->-<!-- --> this library provides the RPC-like experience that we want from our workers</li></ul><pre><code>yarn add comlink worker-plugin
</code></pre><p>We now need to tweak our <code>webpack.config.js</code> to use the <code>worker-plugin</code>:</p><pre><code class="language-js">const WorkerPlugin = require(&#x27;worker-plugin&#x27;);

// ....

    plugins: [
      new WorkerPlugin(),

// ....
</code></pre><p>Do note that there&#x27;s a number of <code>plugins</code> statements in the <code>webpack.config.js</code>. You want the top level one; look out for the <code>new HtmlWebpackPlugin</code> statement and place your <code>new WorkerPlugin(),</code> before that.</p><h2>Workerize our slow process</h2><p>Now we&#x27;re ready to take our long running process and move it into a worker. Inside the <code>src</code> folder, create a new folder called <code>my-first-worker</code>. Our worker is going to live in here. Into this folder we&#x27;re going to add a <code>tsconfig.json</code> file:</p><pre><code>{
  &quot;compilerOptions&quot;: {
    &quot;strict&quot;: true,
    &quot;target&quot;: &quot;esnext&quot;,
    &quot;module&quot;: &quot;esnext&quot;,
    &quot;lib&quot;: [
      &quot;webworker&quot;,
      &quot;esnext&quot;
    ],
    &quot;moduleResolution&quot;: &quot;node&quot;,
    &quot;noUnusedLocals&quot;: true,
    &quot;sourceMap&quot;: true,
    &quot;allowJs&quot;: false,
    &quot;baseUrl&quot;: &quot;.&quot;
  }
}
</code></pre><p>This file exists to tell TypeScript that this is a Web Worker. Do note the <code>&quot;lib&quot;: [ &quot;webworker&quot;</code> usage which does exactly that.</p><p>Alongside the <code>tsconfig.json</code> file, let&#x27;s create an <code>index.ts</code> file. This will be our worker:</p><pre><code class="language-ts">import { expose } from &#x27;comlink&#x27;;
import { takeALongTimeToDoSomething } from &#x27;../takeALongTimeToDoSomething&#x27;;

const exports = {
  takeALongTimeToDoSomething,
};
export type MyFirstWorker = typeof exports;

expose(exports);
</code></pre><p>There&#x27;s a number of things happening in our small worker file. Let&#x27;s go through this statement by statement:</p><pre><code class="language-ts">import { expose } from &#x27;comlink&#x27;;
</code></pre><p>Here we&#x27;re importing the <code>expose</code> method from comlink. Comlink’s goal is to make <em>expose</em>d values from one thread available in the other. The <code>expose</code> method can be viewed as the comlink equivalent of <code>export</code>. It is used to export the RPC style signature of our worker. We&#x27;ll see it&#x27;s use later.</p><pre><code class="language-ts">import { takeALongTimeToDoSomething } from &#x27;../takeALongTimeToDoSomething&#x27;;
</code></pre><p>Here we&#x27;re going to import our <code>takeALongTimeToDoSomething</code> function that we wrote previously, so we can use it in our worker.</p><pre><code class="language-ts">const exports = {
  takeALongTimeToDoSomething,
};
</code></pre><p>Here we&#x27;re creating the public facing API that we&#x27;re going to expose.</p><pre><code class="language-ts">export type MyFirstWorker = typeof exports;
</code></pre><p>We&#x27;re going to want our worker to be strongly typed. This line creates a type called <code>MyFirstWorker</code> which is derived from our <code>exports</code> object literal.</p><pre><code class="language-ts">expose(exports);
</code></pre><p>Finally we expose the <code>exports</code> using comlink. We&#x27;re done; that&#x27;s our worker finished. Now let&#x27;s consume it. Let&#x27;s change our <code>index.tsx</code> file to use it. Replace our import of <code>takeALongTimeToDoSomething</code>:</p><pre><code class="language-ts">import { takeALongTimeToDoSomething } from &#x27;./takeALongTimeToDoSomething&#x27;;
</code></pre><p>With an import of <code>wrap</code> from comlink that creates a local <code>takeALongTimeToDoSomething</code> function that wraps interacting with our worker:</p><pre><code class="language-ts">import { wrap } from &#x27;comlink&#x27;;

function takeALongTimeToDoSomething() {
  const worker = new Worker(&#x27;./my-first-worker&#x27;, {
    name: &#x27;my-first-worker&#x27;,
    type: &#x27;module&#x27;,
  });
  const workerApi = wrap&lt;import(&#x27;./my-first-worker&#x27;).MyFirstWorker&gt;(worker);
  workerApi.takeALongTimeToDoSomething();
}
</code></pre><p>Now we&#x27;re ready to demo our application using our function offloaded into a Web Worker. It now behaves like this:</p><p><img src="../static/blog/2020-02-21-web-workers-comlink-typescript-and-react/non-blocking.gif"/></p><p>There&#x27;s a number of exciting things to note here:</p><ol><li>The application is now non-blocking. Our long running function is now not preventing the UI from updating</li><li>The functionality is lazily loaded via a <code>my-first-worker.chunk.worker.js</code> that has been created by the <code>worker-plugin</code> and <code>comlink</code>.</li></ol><h2>Using Web Workers in React</h2><p>The example we&#x27;ve showed so far demostrates how you could use Web Workers and why you might want to. However, it&#x27;s a far cry from a real world use case. Let&#x27;s take the next step and plug our Web Worker usage into our React application. What would that look like? Let&#x27;s find out.</p><p>We&#x27;ll return <code>index.tsx</code> back to it&#x27;s initial state. Then we&#x27;ll make a simple adder function that takes some values and returns their total. To our <code>takeALongTimeToDoSomething.ts</code> module let&#x27;s add:</p><pre><code class="language-ts">export function takeALongTimeToAddTwoNumbers(number1: number, number2: number) {
  console.log(&#x27;Start to add...&#x27;);
  const seconds = 5;
  const start = new Date().getTime();
  const delay = seconds * 1000;
  while (true) {
    if (new Date().getTime() - start &gt; delay) {
      break;
    }
  }
  const total = number1 + number2;
  console.log(&#x27;Finished adding&#x27;);
  return total;
}
</code></pre><p>Let&#x27;s start using our long running calculator in a React component. We&#x27;ll update our <code>App.tsx</code> to use this function and create a simple adder component:</p><pre><code class="language-tsx">import React, { useState } from &#x27;react&#x27;;
import &#x27;./App.css&#x27;;
import { takeALongTimeToAddTwoNumbers } from &#x27;./takeALongTimeToDoSomething&#x27;;

const App: React.FC = () =&gt; {
  const [number1, setNumber1] = useState(1);
  const [number2, setNumber2] = useState(2);

  const total = takeALongTimeToAddTwoNumbers(number1, number2);

  return (
    &lt;div className=&quot;App&quot;&gt;
      &lt;h1&gt;Web Workers in action!&lt;/h1&gt;

      &lt;div&gt;
        &lt;label&gt;Number to add: &lt;/label&gt;
        &lt;input
          type=&quot;number&quot;
          onChange={(e) =&gt; setNumber1(parseInt(e.target.value))}
          value={number1}
        /&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;label&gt;Number to add: &lt;/label&gt;
        &lt;input
          type=&quot;number&quot;
          onChange={(e) =&gt; setNumber2(parseInt(e.target.value))}
          value={number2}
        /&gt;
      &lt;/div&gt;
      &lt;h2&gt;Total: {total}&lt;/h2&gt;
    &lt;/div&gt;
  );
};

export default App;
</code></pre><p>When you try it out you&#x27;ll notice that entering a single digit locks the UI for 5 seconds whilst it adds the numbers. From the moment the cursor stops blinking to the moment the screen updates the UI is non-responsive:</p><p><img src="../static/blog/2020-02-21-web-workers-comlink-typescript-and-react/blocking-react.gif"/></p><p>So far, so classic. Let&#x27;s Web Workerify this!</p><p>We&#x27;ll update our <code>my-first-worker/index.ts</code> to import this new function:</p><pre><code class="language-ts">import { expose } from &#x27;comlink&#x27;;
import {
  takeALongTimeToDoSomething,
  takeALongTimeToAddTwoNumbers,
} from &#x27;../takeALongTimeToDoSomething&#x27;;

const exports = {
  takeALongTimeToDoSomething,
  takeALongTimeToAddTwoNumbers,
};
export type MyFirstWorker = typeof exports;

expose(exports);
</code></pre><p>Alongside our <code>App.tsx</code> file let&#x27;s create an <code>App.hooks.ts</code> file.</p><pre><code class="language-ts">import { wrap, releaseProxy } from &#x27;comlink&#x27;;
import { useEffect, useState, useMemo } from &#x27;react&#x27;;

/**
 * Our hook that performs the calculation on the worker
 */
export function useTakeALongTimeToAddTwoNumbers(
  number1: number,
  number2: number
) {
  // We&#x27;ll want to expose a wrapping object so we know when a calculation is in progress
  const [data, setData] = useState({
    isCalculating: false,
    total: undefined as number | undefined,
  });

  // acquire our worker
  const { workerApi } = useWorker();

  useEffect(() =&gt; {
    // We&#x27;re starting the calculation here
    setData({ isCalculating: true, total: undefined });

    workerApi
      .takeALongTimeToAddTwoNumbers(number1, number2)
      .then((total) =&gt; setData({ isCalculating: false, total })); // We receive the result here
  }, [workerApi, setData, number1, number2]);

  return data;
}

function useWorker() {
  // memoise a worker so it can be reused; create one worker up front
  // and then reuse it subsequently; no creating new workers each time
  const workerApiAndCleanup = useMemo(() =&gt; makeWorkerApiAndCleanup(), []);

  useEffect(() =&gt; {
    const { cleanup } = workerApiAndCleanup;

    // cleanup our worker when we&#x27;re done with it
    return () =&gt; {
      cleanup();
    };
  }, [workerApiAndCleanup]);

  return workerApiAndCleanup;
}

/**
 * Creates a worker, a cleanup function and returns it
 */
function makeWorkerApiAndCleanup() {
  // Here we create our worker and wrap it with comlink so we can interact with it
  const worker = new Worker(&#x27;./my-first-worker&#x27;, {
    name: &#x27;my-first-worker&#x27;,
    type: &#x27;module&#x27;,
  });
  const workerApi = wrap&lt;import(&#x27;./my-first-worker&#x27;).MyFirstWorker&gt;(worker);

  // A cleanup function that releases the comlink proxy and terminates the worker
  const cleanup = () =&gt; {
    workerApi[releaseProxy]();
    worker.terminate();
  };

  const workerApiAndCleanup = { workerApi, cleanup };

  return workerApiAndCleanup;
}
</code></pre><p>The <code>useWorker</code> and <code>makeWorkerApiAndCleanup</code> functions make up the basis of a shareable worker hooks approach. It would take very little work to paramaterise them so this could be used elsewhere. That&#x27;s outside the scope of this post but would be extremely straightforward to accomplish.</p><p>Time to test! We&#x27;ll change our <code>App.tsx</code> to use the new <code>useTakeALongTimeToAddTwoNumbers</code> hook:</p><pre><code class="language-tsx">import React, { useState } from &#x27;react&#x27;;
import &#x27;./App.css&#x27;;
import { useTakeALongTimeToAddTwoNumbers } from &#x27;./App.hooks&#x27;;

const App: React.FC = () =&gt; {
  const [number1, setNumber1] = useState(1);
  const [number2, setNumber2] = useState(2);

  const total = useTakeALongTimeToAddTwoNumbers(number1, number2);

  return (
    &lt;div className=&quot;App&quot;&gt;
      &lt;h1&gt;Web Workers in action!&lt;/h1&gt;

      &lt;div&gt;
        &lt;label&gt;Number to add: &lt;/label&gt;
        &lt;input
          type=&quot;number&quot;
          onChange={(e) =&gt; setNumber1(parseInt(e.target.value))}
          value={number1}
        /&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;label&gt;Number to add: &lt;/label&gt;
        &lt;input
          type=&quot;number&quot;
          onChange={(e) =&gt; setNumber2(parseInt(e.target.value))}
          value={number2}
        /&gt;
      &lt;/div&gt;
      &lt;h2&gt;
        Total:{&#x27; &#x27;}
        {total.isCalculating ? (
          &lt;em&gt;Calculating...&lt;/em&gt;
        ) : (
          &lt;strong&gt;{total.total}&lt;/strong&gt;
        )}
      &lt;/h2&gt;
    &lt;/div&gt;
  );
};

export default App;
</code></pre><p>Now our calculation takes place off the main thread and the UI is no longer blocked!</p><p><img src="../static/blog/2020-02-21-web-workers-comlink-typescript-and-react/non-blocking-react.gif"/></p><p><a href="https://blog.logrocket.com/integrating-web-workers-in-a-react-app-with-comlink/">This post was originally published on LogRocket.</a></p><p><a href="https://github.com/johnnyreilly/webworkers-comlink-typescript-react">The source code for this project can be found here.</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[From create-react-app to PWA]]></title>
            <link>https://blog.johnnyreilly.com/2020/01/31/from-create-react-app-to-pwa</link>
            <guid>From create-react-app to PWA</guid>
            <pubDate>Fri, 31 Jan 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[Progressive Web Apps are a (terribly named) wonderful idea. You can build an app once using web technologies which serves all devices and form factors. It can be accessible over the web, but also surface on the home screen of your Android / iOS device. That app can work offline, have a splash screen when it launches and have notifications too.]]></description>
            <content:encoded><![CDATA[<p>Progressive Web Apps are a (terribly named) wonderful idea. You can build an app <em>once</em> using web technologies which serves all devices and form factors. It can be accessible over the web, but also surface on the home screen of your Android / iOS device. That app can work offline, have a splash screen when it launches and have notifications too.</p><p>PWAs can be a money saver for your business. The alternative, should you want an app experience for your users, is building the same application using three different technologies (one for web, one for Android and one for iOS). When you take this path it&#x27;s hard to avoid a multiplication of cost and complexity. It often leads to dividing up the team as each works on a different stack. It&#x27;s common to lose a certain amount of focus as a consequence. PWAs can help here; they are a compelling alternative, not just from a developers standpoint, but from a resourcing one too.</p><p>However, the downside of PWAs is that they are more complicated than normal web apps. Writing one from scratch is just less straightforward than a classic web app. There are easy onramps to building a PWA that help you fall into the pit of success. This post will highlight one of these. How you can travel from zero to a PWA of your very own using React and TypeScript.</p><p>This post presumes knowledge of:</p><ul><li>React</li><li>TypeScript</li><li>Node</li></ul><h2>From console to web app</h2><p>To create our PWA we&#x27;re going to use <a href="https://create-react-app.dev/"><code>create-react-app</code></a>. This excellent project has long had inbuilt support for making PWAs. In recent months that support has matured to a very satisfactory level. To create ourselves a TypeScript React app using <code>create-react-app</code> enter this <code>npx</code> command at the console:</p><pre><code class="language-shell">npx create-react-app pwa-react-typescript --template typescript
</code></pre><p>This builds you a react web app built with TypeScript; it can be tested locally with:</p><pre><code class="language-shell">cd pwa-react-typescript
yarn start
</code></pre><h2>From web app to PWA</h2><p>From web app to PWA is incredibly simple; it’s just a question of opting in to offline behaviour. If you open up the <code>index.tsx</code> file in your newly created project you&#x27;ll find this code:</p><pre><code class="language-ts">// If you want your app to work offline and load faster, you can change
// unregister() to register() below. Note this comes with some pitfalls.
// Learn more about service workers: https://bit.ly/CRA-PWA
serviceWorker.unregister();
</code></pre><p>As the hint suggests, swap <code>serviceWorker.unregister()</code> for <code>serviceWorker.register()</code> and you now have a PWA. Amazing! What does this mean? Well to <a href="https://create-react-app.dev/docs/making-a-progressive-web-app/#why-opt-in">quote the docs</a>:</p><blockquote><ul><li>All static site assets are cached so that your page loads fast on subsequent visits, regardless of network connectivity (such as 2G or 3G). Updates are downloaded in the background.</li><li>Your app will work regardless of network state, even if offline. This means your users will be able to use your app at 10,000 feet and on the subway.</li></ul><p>... it will take care of generating a service worker file that will automatically precache all of your local assets and keep them up to date as you deploy updates. The service worker will use a <a href="https://developers.google.com/web/fundamentals/instant-and-offline/offline-cookbook/#cache-falling-back-to-network">cache-first strategy</a>for handling all requests for local assets, including <a href="https://developers.google.com/web/fundamentals/primers/service-workers/high-performance-loading#first_what_are_navigation_requests">navigation requests</a> for your HTML, ensuring that your web app is consistently fast, even on a slow or unreliable network.</p></blockquote><p>Under the bonnet, <code>create-react-app</code> is achieving this through the use of technology called <a href="https://developers.google.com/web/tools/workbox">&quot;Workbox&quot;</a>. Workbox describes itself as:</p><blockquote><p>a set of libraries and Node modules that make it easy to cache assets and take full advantage of features used to build <a href="https://developers.google.com/web/progressive-web-apps/">Progressive Web Apps</a>.</p></blockquote><p>The good folks of Google are aware that writing your own PWA can be tricky. There&#x27;s much new behaviour to configure and be aware of; it&#x27;s easy to make mistakes. Workbox is there to help ease the way forward by implementing default strategies for caching / offline behaviour which can be controlled through configuration.</p><p>A downside of the usage of <code>Workbox</code> in <code>create-react-app</code> is that (as with most things <code>create-react-app</code>) there&#x27;s little scope for configuration of your own if the defaults don&#x27;t serve your purpose. This may change in the future, indeed <a href="https://github.com/facebook/create-react-app/pull/5369">there&#x27;s an open PR that adds this support</a>.</p><h2>Icons and splash screens and A2HS, oh my!</h2><p>But it&#x27;s not just an offline experience that makes this a PWA. Other important factors are:</p><ul><li>That the app can be added to your home screen (A2HS AKA &quot;installed&quot;).</li><li>That the app has a name and an icon which can be customised.</li><li>That there&#x27;s a splash screen displayed to the user as the app starts up.</li></ul><p>All of the above is &quot;in the box&quot; with <code>create-react-app</code>. Let&#x27;s start customizing these.</p><p>First of all, we&#x27;ll give our app a name. Fire up <code>index.html</code> and replace <code>&amp;lt;title&amp;gt;React App&amp;lt;/title&amp;gt;</code> with <code>&amp;lt;title&amp;gt;My PWA&amp;lt;/title&amp;gt;</code>. (Feel free to concoct a more imaginative name than the one I&#x27;ve suggested.) Next open up <code>manifest.json</code> and replace:</p><pre><code class="language-json">&quot;short_name&quot;: &quot;React App&quot;,
  &quot;name&quot;: &quot;Create React App Sample&quot;,
</code></pre><p>with:</p><pre><code class="language-json">&quot;short_name&quot;: &quot;My PWA&quot;,
  &quot;name&quot;: &quot;My PWA&quot;,
</code></pre><p>Your app now has a name. The question you might be asking is: what is this <code>manifest.json</code> file? Well to <a href="https://developers.google.com/web/fundamentals/web-app-manifest">quote the good folks of Google</a>:</p><blockquote><p>The <a href="https://developer.mozilla.org/en-US/docs/Web/Manifest">web app manifest</a> is a simple JSON file that tells the browser about your web application and how it should behave when &#x27;installed&#x27; on the user&#x27;s mobile device or desktop. Having a manifest is required by Chrome to show the <a href="https://developers.google.com/web/fundamentals/app-install-banners/">Add to Home Screen prompt</a>.</p><p>A typical manifest file includes information about the app name, icons it should use, the start_url it should start at when launched, and more.</p></blockquote><p>So the <code>manifest.json</code> is essentially metadata about your app. Here&#x27;s what it should look like right now:</p><pre><code class="language-json">{
  &quot;short_name&quot;: &quot;My PWA&quot;,
  &quot;name&quot;: &quot;My PWA&quot;,
  &quot;icons&quot;: [
    {
      &quot;src&quot;: &quot;favicon.ico&quot;,
      &quot;sizes&quot;: &quot;64x64 32x32 24x24 16x16&quot;,
      &quot;type&quot;: &quot;image/x-icon&quot;
    },
    {
      &quot;src&quot;: &quot;logo192.png&quot;,
      &quot;type&quot;: &quot;image/png&quot;,
      &quot;sizes&quot;: &quot;192x192&quot;
    },
    {
      &quot;src&quot;: &quot;logo512.png&quot;,
      &quot;type&quot;: &quot;image/png&quot;,
      &quot;sizes&quot;: &quot;512x512&quot;
    }
  ],
  &quot;start_url&quot;: &quot;.&quot;,
  &quot;display&quot;: &quot;standalone&quot;,
  &quot;theme_color&quot;: &quot;#000000&quot;,
  &quot;background_color&quot;: &quot;#ffffff&quot;
}
</code></pre><p>You can use the above properties (and others not yet configured) to control how your app behaves. For instance, if you want to replace icons your app uses then it&#x27;s a simple matter of:</p><ul><li>placing new logo files in the <code>public</code> folder</li><li>updating references to them in the <code>manifest.json</code></li><li>finally, for older Apple devices, updating the <code>&amp;lt;link rel=&quot;apple-touch-icon&quot; ... /&amp;gt;</code> in the <code>index.html</code>.</li></ul><h2>Where are we?</h2><p>So far, we have a basic PWA in place. It&#x27;s installable. You can run it locally and develop it with <code>yarn start</code>. You can build it for deployment with <code>yarn build</code>.</p><p>What this isn&#x27;t, is recognisably a web app. In the sense that it doesn&#x27;t have support for different pages / URLs. We&#x27;re typically going to want to break up our application this way. Let&#x27;s do that now. We&#x27;re going to use <a href="https://github.com/ReactTraining/react-router"><code>react-router</code></a>; the de facto routing solution for React. To add it to our project (and the required type definitions for TypeScript) we use:</p><pre><code>yarn add react-router-dom @types/react-router-dom
</code></pre><p>Now let&#x27;s split up our app into a couple of pages. We&#x27;ll replace the existing <code>App.tsx</code> with this:</p><pre><code class="language-tsx">import React from &#x27;react&#x27;;
import { BrowserRouter as Router, Switch, Route, Link } from &#x27;react-router-dom&#x27;;
import About from &#x27;./About&#x27;;
import Home from &#x27;./Home&#x27;;

const App: React.FC = () =&gt; (
  &lt;Router&gt;
    &lt;nav&gt;
      &lt;ul&gt;
        &lt;li&gt;
          &lt;Link to=&quot;/&quot;&gt;Home&lt;/Link&gt;
        &lt;/li&gt;
        &lt;li&gt;
          &lt;Link to=&quot;/about&quot;&gt;About&lt;/Link&gt;
        &lt;/li&gt;
      &lt;/ul&gt;
    &lt;/nav&gt;
    &lt;Switch&gt;
      &lt;Route path=&quot;/about&quot;&gt;
        &lt;About /&gt;
      &lt;/Route&gt;
      &lt;Route path=&quot;/&quot;&gt;
        &lt;Home /&gt;
      &lt;/Route&gt;
    &lt;/Switch&gt;
  &lt;/Router&gt;
);

export default App;
</code></pre><p>This will be our root page. It has the responsiblity of using <code>react-router</code> to render the pages we want to serve, and also to provide the links that allow users to navigate to those pages. In making our changes we&#x27;ll have broken our test (which checked for a link we&#x27;ve now deleted), so we&#x27;ll fix it like so:</p><p>Replace the <code>App.test.tsx</code> with this:</p><pre><code class="language-tsx">import React from &#x27;react&#x27;;
import { render } from &#x27;@testing-library/react&#x27;;
import App from &#x27;./App&#x27;;

test(&#x27;renders about link&#x27;, () =&gt; {
  const { getByText } = render(&lt;App /&gt;);
  const linkElement = getByText(/about/i);
  expect(linkElement).toBeInTheDocument();
});
</code></pre><p>You&#x27;ll have noticed that in our new <code>App.tsx</code> we import two new components (or pages); <code>About</code> and <code>Home</code>. Let&#x27;s create those. First <code>About.tsx</code>:</p><pre><code class="language-tsx">import React from &#x27;react&#x27;;

const About: React.FC = () =&gt; &lt;h1&gt;This is a PWA&lt;/h1&gt;;

export default About;
</code></pre><p>Then <code>Home.tsx</code>:</p><pre><code class="language-tsx">import React from &#x27;react&#x27;;

const Home: React.FC = () =&gt; &lt;h1&gt;Welcome to your PWA!&lt;/h1&gt;;

export default Home;
</code></pre><h2>Code splitting</h2><p>Now we&#x27;ve split up our app into multiple sections, we&#x27;re going to split the code too. A good way to improve loading times for PWAs is to ensure that the code is not built into big files. At the moment our app builds a <code>single-file.js</code>. If you run <code>yarn build</code> you&#x27;ll see what this looks like:</p><pre><code>47.88 KB  build/static/js/2.89bc6648.chunk.js
  784 B     build/static/js/runtime-main.9c116153.js
  555 B     build/static/js/main.bc740179.chunk.js
  269 B     build/static/css/main.5ecd60fb.chunk.css
</code></pre><p>Notice the <code>build/static/js/main.bc740179.chunk.js</code> file. This is our <code>single-file.js</code>. It represents the compiled output of building the TypeScript files that make up our app. It will grow and grow as our app grows, eventually becoming problematic from a user loading speed perspective.</p><p><code>create-react-app</code> is built upon webpack. There is excellent support for code splitting in webpack and hence <a href="https://reactjs.org/docs/code-splitting.html#code-splitting">create-react-app supports it by default</a>. Let&#x27;s apply it to our app. Again we&#x27;re going to change <code>App.tsx</code>.</p><p>Where we previously had:</p><pre><code class="language-tsx">import About from &#x27;./About&#x27;;
import Home from &#x27;./Home&#x27;;
</code></pre><p>Let&#x27;s replace with:</p><pre><code class="language-tsx">const About = lazy(() =&gt; import(&#x27;./About&#x27;));
const Home = lazy(() =&gt; import(&#x27;./Home&#x27;));
</code></pre><p>This is the syntax to lazily load components in React. You&#x27;ll note that it internally uses the <a href="https://github.com/tc39/proposal-dynamic-import">dynamic <code>import()</code> syntax</a> which webpack uses as a &quot;split point&quot;.</p><p>Let&#x27;s also give React something to render whilst it waits for the dynamic imports to be resolved. Just inside our <code>&amp;lt;Router&amp;gt;</code> component we&#x27;ll add a <code>&amp;lt;Suspense&amp;gt;</code> component too:</p><pre><code class="language-tsx">&lt;Router&gt;
  &lt;Suspense fallback={&lt;div&gt;Loading...&lt;/div&gt;}&gt;{/*...*/}&lt;/Suspense&gt;
&lt;/Router&gt;
</code></pre><p>The <code>&amp;lt;Suspense&amp;gt;</code> component will render the <code>&amp;lt;div&amp;gt;Loading...&amp;lt;/div&amp;gt;</code> whilst it waits for a routes code to be dynamically loaded. So our final <code>App.tsx</code> component ends up looking like this:</p><pre><code class="language-tsx">import React, { lazy, Suspense } from &#x27;react&#x27;;
import { BrowserRouter as Router, Switch, Route, Link } from &#x27;react-router-dom&#x27;;
const About = lazy(() =&gt; import(&#x27;./About&#x27;));
const Home = lazy(() =&gt; import(&#x27;./Home&#x27;));

const App: React.FC = () =&gt; (
  &lt;Router&gt;
    &lt;Suspense fallback={&lt;div&gt;Loading...&lt;/div&gt;}&gt;
      &lt;nav&gt;
        &lt;ul&gt;
          &lt;li&gt;
            &lt;Link to=&quot;/&quot;&gt;Home&lt;/Link&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;Link to=&quot;/about&quot;&gt;About&lt;/Link&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/nav&gt;
      &lt;Switch&gt;
        &lt;Route path=&quot;/about&quot;&gt;
          &lt;About /&gt;
        &lt;/Route&gt;
        &lt;Route path=&quot;/&quot;&gt;
          &lt;Home /&gt;
        &lt;/Route&gt;
      &lt;/Switch&gt;
    &lt;/Suspense&gt;
  &lt;/Router&gt;
);

export default App;
</code></pre><p>This is now a code split application. How can we tell? If we run <code>yarn build</code> again we&#x27;ll see something like this:</p><pre><code>47.88 KB          build/static/js/2.89bc6648.chunk.js
  1.18 KB (+428 B)  build/static/js/runtime-main.415ab5ea.js
  596 B (+41 B)     build/static/js/main.e60948bb.chunk.js
  269 B             build/static/css/main.5ecd60fb.chunk.css
  233 B             build/static/js/4.0c85e1cb.chunk.js
  228 B             build/static/js/3.eed49094.chunk.js
</code></pre><p>Note that we now have multiple <code>*.chunk.js</code> files. Our initial <code>main.*.chunk.js</code> and then <code>3.*.chunk.js</code> representing <code>Home.tsx</code> and <code>4.*.chunk.js</code> representing <code>About.tsx</code>.</p><p>As we continue to build out our app from this point we&#x27;ll have a great approach in place to ensure that users load files as they need to and that those files should not be too large. Great performance which will scale.</p><h2>Deploy your PWA</h2><p>Now that we have our basic PWA in place, let&#x27;s deploy it so the outside world can appreciate it. We&#x27;re going to use <a href="https://www.netlify.com/">Netlify</a> for this.</p><p>The source code of our PWA lives on GitHub here: <a href="https://github.com/johnnyreilly/pwa-react-typescript">https://github.com/johnnyreilly/pwa-react-typescript</a></p><p>We&#x27;re going to log into Netlify, click on the &quot;Create a new site&quot; option and select GitHub as the provider. We&#x27;ll need to authorize Netlify to access our GitHub.</p><p><img src="../static/blog/2020-01-31-from-create-react-app-to-pwa/netlify-auth.png"/></p><p>You may need to click the &quot;Configure Netlify on GitHub&quot; button to grant permissions for Netlify to access your repo like so:</p><p><img src="../static/blog/2020-01-31-from-create-react-app-to-pwa/netlify-repo-permissions.png"/></p><p>Then you can select your repo from within Netlify. All of the default settings that Netlify provides should work for our use case:</p><p><img src="../static/blog/2020-01-31-from-create-react-app-to-pwa/netlify-deploy-settings.png"/></p><p>Let&#x27;s hit the magic &quot;Deploy site&quot; button! In a matter of minutes you&#x27;ll find that Netlify has deployed your PWA.</p><p><img src="../static/blog/2020-01-31-from-create-react-app-to-pwa/netlify-deployed.png"/></p><p>If we browse to the URL provided by Netlify we&#x27;ll be able to see the deployed PWA in action. (You also have the opportunity to set up a custom domain name that you would typically want outside of a simple demo such as this.) Importantly this will be served over HTTPS which will allow our Service Worker to operate.</p><p>Now that we know it&#x27;s there, let&#x27;s see how what we&#x27;ve built holds up according to the professionals. We&#x27;re going to run the Google Chrome Developer Tools Audit against our PWA:</p><p><img src="../static/blog/2020-01-31-from-create-react-app-to-pwa/pwa-audit.png"/></p><p>That is a good start for our PWA!</p><p><a href="https://blog.logrocket.com/from-create-react-app-to-pwa/">This post was originally published on LogRocket.</a></p><p><a href="https://github.com/johnnyreilly/pwa-react-typescript">The source code for this project can be found here.</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[LICENSE to kill your PWA]]></title>
            <link>https://blog.johnnyreilly.com/2020/01/21/license-to-kill-your-pwa</link>
            <guid>LICENSE to kill your PWA</guid>
            <pubDate>Tue, 21 Jan 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[Update: 26/01/2020 - LICENSE to kill revoked!]]></description>
            <content:encoded><![CDATA[<h2>Update: 26/01/2020 - LICENSE to kill revoked!</h2><p>Following the original publication of this post I received this tweet suggesting we should change the behaviour of the underlying <code>terser-webpack-plugin</code>:</p><blockquote><p>Send a PR to change the name to .LICENSE.txt by default.</p><p>— Tobias Koppers (@wSokra) <a href="https://twitter.com/wSokra/status/1220069497660411904?ref_src=twsrc%5Etfw">January 22, 2020</a></p></blockquote><script src="https://platform.twitter.com/widgets.js" charSet="utf-8"></script><p>That seemed like an excellent idea! I raised <a href="https://github.com/webpack-contrib/terser-webpack-plugin/pull/210">this PR</a> which changes the behaviour such that instead of <code>.LICENSE</code> files being produced, <code>.LICENSE.txt</code> files are pumped out instead. Crucially they are IIS (and other servers) friendly. The great news is that future users of webpack / create-react-app etc will not face this problem at all; result!</p><h2>The tragedy</h2><p>Recently my beloved PWA died. I didn&#x27;t realise it at first. It wasn&#x27;t until a week or so after the tragedy that I realised he&#x27;d gone. In his place was the stale memory of service workers gone by. Last week&#x27;s code; cached and repeatedly served up to a disappointed audience. Terrible news.</p><p>What had happened? What indeed. The problem was quirky and (now that I know the answer) I&#x27;m going to share it with you. Because it&#x27;s entirely non-obvious.</p><h2>The mystery</h2><p>Once I realised that I was repeatedly being served up an old version of my PWA, I got to wondering.... Why? What&#x27;s happening? What&#x27;s wrong? What did I do? I felt bad. I stared at the ceiling. I sighed and opened my Chrome devtools. With no small amount of sadness I went to the <code>Application</code> tab, hit <code>Service Workers</code> and then <code>Unregister</code>.</p><p>Then I hit refresh and took a look at console. I saw this:</p><p><img src="../static/blog/2020-01-21-license-to-kill-your-pwa/LICENSE%2Bcannot%2Bbe%2Bcached.png"/></p><p>What does this mean? Something about a &quot;bad-precaching-response&quot;. And apparently this was happening whilst trying to load this resource: <code>/static/js/6.20102e99.chunk.js.LICENSE?__WB_REVISION__=e2fc36</code></p><p>This <code>404</code> was preventing pre-caching from executing successfully. This was what was killing my PWA. This was the perpetrator. How to fix this? Read on!</p><h2>The investigation</h2><p>Time to find out what&#x27;s going on. I dropped that URL into my browser to see what would happen. <code>404</code> city man:</p><p><img src="../static/blog/2020-01-21-license-to-kill-your-pwa/LICENSE%2Bfile%2Bscrewing%2Bme%2Bover.png"/></p><p>So, to disk. I took a look at what was actually on the server in that location. Sure enough, the file existed. When I opened it up I found this:</p><pre><code class="language-js">/**
 * A better abstraction over CSS.
 *
 * @copyright Oleg Isonen (Slobodskoi) / Isonen 2014-present
 * @website https://github.com/cssinjs/jss
 * @license MIT
 */

/*
object-assign
(c) Sindre Sorhus
@license MIT
*/

/** @license React v16.12.0
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v16.12.0
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v0.18.0
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v16.12.0
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */
</code></pre><p>What is this? Well, as the name of the file suggests, it&#x27;s licenses. For some reason, my build was scraping the licenses from the head of some of my files and placing them in a separate <code>6.20102e99.chunk.js.LICENSE</code> file. Doing some more digging I happened upon <a href="https://github.com/facebook/create-react-app/issues/6441">this discussion against the <code>create-react-app</code></a> project. It&#x27;s worth saying, that my PWA was an ejected <code>create-react-app</code> project.</p><p>It turned out the the issue was related to the <a href="https://github.com/webpack-contrib/terser-webpack-plugin"><code>terser-webpack-plugin</code></a>. The default behaviour performs this kind of license file extraction. The app was being served by an IIS server and it wasn&#x27;t configured to support the <code>.LICENSE</code> file type.</p><h2>The resolution</h2><p>The simplest solution was simply this: wave goodbye to <code>LICENSE</code> files. If you haven&#x27;t ejected from your <code>create-react-app</code> then this might be a problem. But since I had, I was able to make this tweak to the terser settings in the <code>webpack.config.js</code>:</p><pre><code class="language-js">new TerserPlugin({
    /* TURN OFF LICENSE FILES - SEE https://github.com/facebook/create-react-app/issues/6441 */
    extractComments: false,
    /* TURN OFF LICENSE FILES - Tweak by John Reilly */
    terserOptions: {
        // ....
</code></pre><p>And with this we say goodbye to our <code>404</code>s and hello to a resurrected PWA!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[EF Core 3.1 breaks left join with no navigation property]]></title>
            <link>https://blog.johnnyreilly.com/2020/01/02/ef-core-31-breaks-left-join-with-no-navigation-property</link>
            <guid>EF Core 3.1 breaks left join with no navigation property</guid>
            <pubDate>Thu, 02 Jan 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[Just recently my team took on the challenge of upgrading our codebase from .NET Core 2.2 to .NET Core 3.1. Along the way we encountered a quirky issue which caused us much befuddlement. Should you be befuddled too, then maybe this can help you.]]></description>
            <content:encoded><![CDATA[<p>Just recently my team took on the challenge of upgrading our codebase from .NET Core 2.2 to .NET Core 3.1. Along the way we encountered a quirky issue which caused us much befuddlement. Should you be befuddled too, then maybe this can help you.</p><p>Whilst running our app, we started encountering an error with an Entity Framework Query that looked like this:</p><pre><code class="language-cs">var stuffWeCareAbout = await context.Things
    .Include(thing =&gt; thing.ThisIsFine)
    .Include(thing =&gt; thing.Problematic)
    .Where(thing =&gt; thing.CreatedOn &gt; startFromThisTime &amp;&amp; thing.CreatedOn &lt; endAtThisTime)
    .OrderByDescending(thing =&gt; thing.CreatedOn)
    .ToArrayAsync();
</code></pre><h2>Join me!</h2><p>As EF Core tried to join from the <code>Things</code> table to the <code>Problematic</code> table (some obfuscation in table names here), that which worked in .NET Core 2.2 was <em>not</em> working in .NET Core 3.1. Digging into the issue, we discovered EF Core was generating an invalid <code>LEFT JOIN</code>:</p><pre><code class="language-sql">fail: Microsoft.EntityFrameworkCore.Database.Command[20102]
      Failed executing DbCommand (18ms) [Parameters=[@__startFromThisTime_0=&#x27;?&#x27; (DbType = DateTime2), @__endAtThisTime_1=&#x27;?&#x27; (DbType = DateTime2)], CommandType=&#x27;Text&#x27;, CommandTimeout=&#x27;30&#x27;]
      SELECT [o].[ThingId], [o].[AnonymousId], [o].[CreatedOn],  [o].[Status], [o].[UpdatedOn], [o0].[Id], [o0].[ThingId], [o0].[Name], [o1].[ThingId], [o1].[Status], [o1].[CreatedOn], [o1].[ThingThingId], [o1].[SentOn]
      FROM [Things] AS [o]
      LEFT JOIN [ThisIsFines] AS [o0] ON [o].[ThingId] = [o0].[ThingId]
      LEFT JOIN [Problematic] AS [o1] ON [o].[ThingId] = [o1].[ThingThingId]
      WHERE ([o].[CreatedOn] @__startFromThisTime_0) AND ([o].[CreatedOn] &lt; @__endAtThisTime_1)
      ORDER BY [o].[CreatedOn] DESC, [o].[ThingId], [o1].[ThingId], [o1].[Status]
Microsoft.EntityFrameworkCore.Database.Command: Error: Failed executing DbCommand (18ms) [Parameters=[@__startFromThisTime_0=&#x27;?&#x27; (DbType = DateTime2), @__endAtThisTime_1=&#x27;?&#x27; (DbType = DateTime2)], CommandType=&#x27;Text&#x27;, CommandTimeout=&#x27;30&#x27;]
SELECT [o].[ThingId], [o].[AnonymousId], [o].[CreatedOn], [o].[Status], [o].[UpdatedOn], [o0].[Id], [o0].[ThingId], [o0].[Name], [o1].[ThingId], [o1].[Status], [o1].[CreatedOn], [o1].[ThingThingId], [o1].[SentOn]
FROM [Things] AS [o]
LEFT JOIN [ThisIsFines] AS [o0] ON [o].[ThingId] = [o0].[ThingId]
LEFT JOIN [Problematic] AS [o1] ON [o].[ThingId] = [o1].[ThingThingId]
WHERE ([o].[CreatedOn] @__startFromThisTime_0) AND ([o].[CreatedOn] &lt; @__endAtThisTime_1)
ORDER BY [o].[CreatedOn] DESC, [o].[ThingId], [o1].[ThingId], [o1].[Status]
</code></pre><p>Do you see it? Probably not; it took us a while too... The issue lay here:</p><pre><code class="language-sql">LEFT JOIN [Problematic] AS [o1] ON [o].[ThingId] = [o1].[ThingThingId]
</code></pre><p>This should actually have been:</p><pre><code class="language-sql">LEFT JOIN [Problematic] AS [o1] ON [o].[ThingId] = [o1].[ThingId]
</code></pre><p>For some reason EF Core was looking for <code>ThingThingId</code> where it should have looked for <code>ThingId</code>. But why?</p><h2>Navigation properties to the rescue!</h2><p>This was the <code>Problematic</code> class:</p><pre><code class="language-cs">using System;
using System.ComponentModel.DataAnnotations;
using System.ComponentModel.DataAnnotations.Schema;

namespace Treasury.Data.Entities
{
    public class Problematic
    {
        [ForeignKey(&quot;Thing&quot;)]
        [Required]
        public Guid ThingId { get; set; }
        [Required]
        public DateTime CreatedOn { get; set; }
        public DateTime SentOn { get; set; }
    }
}
</code></pre><p>If you look closely you&#x27;ll see it has a <code>ForeignKey</code> but <em>no</em> accompanying Navigation property. So let&#x27;s add one:</p><pre><code class="language-cs">using System;
using System.ComponentModel.DataAnnotations;
using System.ComponentModel.DataAnnotations.Schema;

namespace Our.App
{
    public class Problematic
    {
        [ForeignKey(&quot;Thing&quot;)]
        [Required]
        public Guid ThingId { get; set; }
        [Required]
        public DateTime CreatedOn { get; set; }
        public DateTime SentOn { get; set; }

        /* THIS NAVIGATION PROPERTY IS WHAT WE NEEDED!!! */
        public virtual Thing Thing { get; set; }
    }
}
</code></pre><p>With this in place our app starts generating the SQL we need.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Teams notification webhooks]]></title>
            <link>https://blog.johnnyreilly.com/2019/12/18/teams-notification-webhooks</link>
            <guid>Teams notification webhooks</guid>
            <pubDate>Wed, 18 Dec 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[Teams notifications are mighty useful. You can send them using Markdown via a webhook.]]></description>
            <content:encoded><![CDATA[<p>Teams notifications are mighty useful. You can send them using Markdown via a webhook.</p><p>This post will explain the following:</p><ol><li>How you can automate the sending of notifications using Teams.</li><li>How Teams supports Markdown in notifications.</li><li>How you can use ASP.Net Core to automate sending notifications.</li></ol><h2>Notifications via Webhooks</h2><p>Now, it&#x27;s not obvious from Teams that there is a simple webhooks integration for Teams, but there is. It&#x27;s tucked away under &quot;Connectors&quot;. If you want to create a webhook of your own, find your team, your channel, click on the menu, then connectors and create a hook. Like so:</p><p><img src="../static/blog/2019-12-18-teams-notification-webhooks/teams-webhook-connector.gif" alt="animation of setting up a webhook connector in Teams"/></p><p>With the URL you&#x27;ve just obtained, you are now free to send notifications to that channel via a simple <code>curl</code>:</p><pre><code class="language-shell">curl -H &quot;Content-Type: application/json&quot; -d &quot;{\&quot;text\&quot;: \&quot;Hello World\&quot;}&quot; https://outlook.office.com/webhook/big-long-guid1/IncomingWebhook/big-long-guid2
</code></pre><h2>Markdown</h2><p>Let&#x27;s see if we can make this more interesting. It turns out that the the webhook can receive JSON as the body of the payload. And there&#x27;s 3 properties we&#x27;d like our JSON to contain:</p><ol><li><code>title</code> - this is optional and is the title of your notification if supplied.</li><li><code>textFormat</code> - provide the value <code>&quot;markdown&quot;</code> and then...</li><li><code>text</code> - provide your markdown notification content!</li></ol><p>So if we have a notification payload file called <code>down.json</code>:</p><pre><code class="language-json">{
  &quot;title&quot;: &quot;Your Notification Title&quot;,
  &quot;textFormat&quot;: &quot;markdown&quot;,
  &quot;text&quot;: &quot;*Wow*\nThis is [markdown](https://en.wikipedia.org/wiki/Markdown)!\n![do a little dance!](https://media.giphy.com/media/YJ5OlVLZ2QNl6/giphy.gif)\n**Huzzah**!&quot;
}
</code></pre><p>We can trigger it with this <code>curl</code>:</p><pre><code class="language-shell">curl -H &quot;Content-Type: application/json&quot; -d @down.json https://outlook.office.com/webhook/big-long-guid1/IncomingWebhook/big-long-guid2
</code></pre><p>As you can see from the example above, you can use all the qualities of Markdown that you know and love. Text, bold text, italics, links and even images too. It&#x27;s <em>great</em>!</p><p><img src="../static/blog/2019-12-18-teams-notification-webhooks/teams-notification.gif" alt="animation of Teams notification"/></p><h2>ASP.Net Core</h2><p>Finally, I wanted to illustrate just how simple the WebHooks API makes plugging notifications into an existing app. In our case we&#x27;re going to use ASP.Net Core, but really there&#x27;s nothing particular about how we&#x27;re going to do this.</p><p>Here&#x27;s a class called <code>TeamsNotificationService</code>. It exposes 2 methods:</p><ul><li><code>SendNotification</code> which allows the consumer to just provide a <code>title</code> and a <code>message</code> - you could consume this from anywhere in your app and use it to publish the notification of your choice.</li><li><code>SendExcitingNotification</code> which actually uses <code>SendNotification</code> and illustrates how you might provide an exciting notification to publish out.</li></ul><pre><code class="language-cs">using System;
using System.Collections.Generic;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Threading.Tasks;

namespace My.Services {
    public interface ITeamsNotificationService {
        Task SendNotification(string title, string message);
        Task SendExcitingNotification(Guid someAppId, string person);
    }

    public class TeamsNotificationService : ITeamsNotificationService {

        // in Startup.ConfigureServices you&#x27;re going to want to add this line:
        // services.AddHttpClient(TeamsNotificationService.TEAMS_NOTIFIER_CLIENT);

        public const string TEAMS_NOTIFIER_CLIENT = &quot;TEAMS_NOTIFIER_CLIENT&quot;;

        private readonly ILogger&lt;TeamsNotificationService&gt; logger;
        private readonly IHttpClientFactory _clientFactory;


        public TeamsNotificationService(
            ILogger&lt;TeamsNotificationService&gt; logger,
            IHttpClientFactory clientFactory
        ) {
            _logger = logger;
            _clientFactory = clientFactory;
        }

        private HttpClient CreateClient() {
            var client = _clientFactory.CreateClient(TEAMS_NOTIFIER);

            client.DefaultRequestHeaders.Clear();
            client.DefaultRequestHeaders.Accept.Clear();
            client.DefaultRequestHeaders.Accept.Add(new MediaTypeWithQualityHeaderValue(&quot;application/json&quot;));

            return client;
        }

        public async Task SendNotification(string title, string message) {
            try {
                var client = CreateClient();

                var messageContents = string.IsNullOrEmpty(title)
                    ? new JsonContent(new { text = message, textFormat = &quot;markdown&quot; })
                    : new JsonContent(new { title = title, text = message, textFormat = &quot;markdown&quot; });

                var webhookUrl = &quot;https://outlook.office.com/webhook/big-long-guid1/IncomingWebhook/big-long-guid2&quot;;
                var response = await client.PostAsync(webhookUrl, messageContents);

                _logger.LogInformation(&quot;Sent {title} notification to Teams using {url}; received this response: {responseStatusCode}&quot;, title, url, response.StatusCode);
            }
            catch (Exception exc) {
                _logger.LogError(exc, $&quot;Failed to send {title} notification to Teams&quot;);
            }
        }

        public async Task SendExcitingNotification(Guid someAppId, string person) {
            var celebration = GetCelebration();
            await SendNotification(
                title: &quot;Incredible Thing Alert!&quot;,
                message: $@&quot;**{person}** has done something incredible! &amp;#x1F44B;

![celebration time!]({celebration})

[Go see for yourself](https://my.app/some-page/{someAppId})&quot;
            );
        }

        string GetCelebration() =&gt; GetRandomItem(_celebrations);
        string GetRandomItem(string[] arrayOfStrings) =&gt; arrayOfStrings[new Random().Next(0, arrayOfStrings.Length)];

        string[] _celebrations = new string[] {
            &quot;https://media.giphy.com/media/KYElw07kzDspaBOwf9/giphy.gif&quot;,
            &quot;https://media.giphy.com/media/GStLeae4F7VIs/giphy.gif&quot;,
            &quot;https://media.giphy.com/media/NbXTwsoD7hvag/giphy.gif&quot;,
            &quot;https://media.giphy.com/media/d86kftzaeizO8/giphy.gif&quot;,
            &quot;https://media.giphy.com/media/YJ5OlVLZ2QNl6/giphy.gif&quot;,
            &quot;https://media.giphy.com/media/kyLYXonQYYfwYDIeZl/giphy.gif&quot;,
            &quot;https://media.giphy.com/media/KYElw07kzDspaBOwf9/giphy.gif&quot;,
            &quot;https://media.giphy.com/media/6nuiJjOOQBBn2/giphy.gif&quot;,
            &quot;https://media.giphy.com/media/hZj44bR9FVI3K/giphy.gif&quot;,
            &quot;https://media.giphy.com/media/31lPv5L3aIvTi/giphy.gif&quot;
        };
    }
}
</code></pre><p>It&#x27;s as simple as that 😄</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Definitely Typed: The Movie]]></title>
            <link>https://blog.johnnyreilly.com/2019/10/08/definitely-typed-movie</link>
            <guid>Definitely Typed: The Movie</guid>
            <pubDate>Tue, 08 Oct 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[I'd like to tell you a story. It's the tale of the ecosystem that grew up around a language: TypeScript. TypeScript is, for want of a better description, JavaScript after a trip to Saville Row. Essentially the same language, but a little more together, a little less wild west. JS with a decent haircut and a new suit. These days, the world seems to be written in TypeScript. And when you pause to consider just how young the language is, well, that's kind of amazing.]]></description>
            <content:encoded><![CDATA[<p>I&#x27;d like to tell you a story. It&#x27;s the tale of the ecosystem that grew up around a language: TypeScript. TypeScript is, for want of a better description, JavaScript after a trip to Saville Row. Essentially the same language, but a little more together, a little less wild west. JS with a decent haircut and a new suit. These days, the world seems to be written in TypeScript. And when you pause to consider just how young the language is, well, that&#x27;s kind of amazing.</p><p>Who could have predicted it would end up like this? When I was a boy I remember coming down the stairs in my childhood home. Shuffling to the edge of each step on my bottom before thumping down to the one beneath. When I look at those same stairs now they&#x27;re so small. I barely notice the difference between one step and the next. But back then each step seemed giant, each one so far apart. Definitely Typed had any number of steps in its evolution. They all seemed so significant then; whereas now they&#x27;re just a memory. Let&#x27;s remember together…</p><p><img src="../static/blog/2019-10-08-definitely-typed-movie/title-image.png" alt="A title image that reads &quot;Definitely Typed: The Movie&quot;"/></p><h2>Prolog(ue)</h2><p>When it was first unveiled to the world by Anders Hejlsberg back in 2012, there was nothing to suggest TypeScript was going to be seismic in its effects. The language brought two important things to the table. First of all, the ability to write JavaScript with optional static typing (imagine this as &quot;belts and braces&quot; for JS). The second feature was interoperability with existing JavaScript.</p><p>The reason TypeScript has the traction that it does, is a consequence of the latter feature. The JavaScript ecosystem was already a roaring success by 2012. Many useful libraries were out there, authored in vanilla JavaScript. jQuery, Backbone, Knockout were all going concerns. People were building things.</p><p>Wisely, having TypeScript able to work with existing JavaScript libraries was a goal of the language right from the off. This made sense; otherwise it would have been like unveiling Netflix to the world whilst saying &quot;sorry you can&#x27;t use a television set to watch this&quot;. Remember, JS was great as is - people wanted static typing so they could be more productive and so they could sleep better at night. (&quot;Oh wait, did I write that unit test to check all the properties? Dammit, it&#x27;s 3am!&quot;) If TypeScript had hove onto the scene requiring that everything was written <em>in</em> TypeScript then I would not be writing this. It didn&#x27;t.</p><p>Interoperability was made possible by the concept of &quot;type definitions&quot;. Analogous to header files in C, these are TypeScript files with a <code>.d.ts</code> suffix that tell the compiler about an existing JavaScript library which is in scope. This means you can write TypeScript and use jQuery or <!-- -->[insert your favourite library name here]<!-- -->. Even though they are not written in TypeScript.</p><p>At the time of the initial TypeScript announcement (v0.8.1) there was no concept of a repository of type definitions. I mean, there was every chance that TypeScript wasn&#x27;t going to be a big deal. Success wasn&#x27;t guaranteed. But it happened. You&#x27;re reading this in a world where Definitely Typed is one of the most popular repos on GitHub and where type definitions from it are published out to npm for consumption by developers greedy for static types. A world where the TypeScript team has pretty much achieved its goal of &quot;types on every desk&quot;.</p><p>I want to tell you the story of the history of type definitions in the TypeScript world. I&#x27;m pretty well placed to do this since I&#x27;ve been involved since the early days. Others involved have been kind enough to give me their time and tell me their stories. There&#x27;s likely to be errors and omissions, and that&#x27;s on me. It&#x27;s an amazing tale though; I&#x27;m fortunate to get to tell it.</p><h2>The First Type Definition</h2><p>I was hanging out for something like TypeScript. I&#x27;d been busily developing rich client applications in JS and, whilst I loved the language, I was dearly missing static typing. All the things broke all of the time and I wanted help. I wanted a compiler to take me by the hand and say &quot;hey John, you just did a silly thing. Don&#x27;t do it John; you&#x27;ll only be filled with regret...&quot;. The TypeScript team wrote that compiler.</p><p>When TypeScript was announced, it was important that the world could see that interop with JS was a first class citizen. Accordingly, a jQuery type definition was demonstrated as well. At the time, jQuery was the number one JavaScript library downloaded on the internet. So naturally it was the obvious choice for a demo. The type definition was fairly rough and ready but it worked. <a href="https://channel9.msdn.com/posts/Anders-Hejlsberg-Introducing-TypeScript">You can see Anders Hejlsberg showing off the jQuery definition 45 minutes into this presentation introducing TypeScript.</a></p><p>Consumption was straightforward, if perhaps quirky. You took the <code>jquery.d.ts</code> file, copied it into your project location. Back then, to let the compiler know that a JS library had come to the party you had to use a kind of comment pragma in the header of your TypeScript files. For example: <code>/// &lt;reference path=&quot;jquery/jquery.d.ts&quot; /&gt;</code>. This let TypeScript know that the type definition living at that path was relevant for the current script and it should scope it in.</p><p>There was no discussion of “how do we type the world”? Even if they wanted to, the TypeScript team didn&#x27;t really have the resources at that point to support this. They&#x27;d got as far as they had on the person power of four or five developers and some testers as well. There was a problem clearly waiting to be solved. As luck would have it, in Bulgaria a man named Boris Yankov had been watching the TypeScript announcement.</p><h2>Boris Yankov</h2><p><img src="../static/blog/2019-10-08-definitely-typed-movie/boris_yankov.jpeg" alt="photograph of Boris Yankov looking mean, moody and magnificent"/></p><p>Boris Yankov was a handsome thirty year old man, living in the historic Bulgarian city of Plovdiv. He was swarthy with dark hair; like Ben Affleck if had been hanging out in Eastern Europe for a couple of years.</p><p>Boris was a backend developer who&#x27;d found himself doing more and more frontend. More JavaScript. He was accustomed to C# on the backend with static typing a-gogo. From his point of view JS was brittle. It was super easy to break things and have no idea until runtime that you&#x27;d done so. It seemed so backward. He was ready for something TypeScript shaped.</p><p>&quot;What people forget is how different it was back then. Microsoft made this announcement, but probably most of the people that were listening were part of the MS ecosystem. I certainly was. Remember, back then if you had a Mac or did Linux you probably didn&#x27;t think about MS too much.&quot;</p><p>Boris thought TypeScript just seemed like this interesting and weird thing that Microsoft were doing. He was excited by types; he was missing them and there was a real need there. A problem to solve. There were already people trying to address this. But the attempts so far had been underwhelming. Boris had encountered Google Closure Compiler; a tool built by Google which, amongst other things, introduces some measure of type safety to JavaScript by reading annotations in JSDoc format. Boris viewed GCC as a tentative first step. <a href="https://github.com/google/closure-compiler/wiki/Annotating-JavaScript-for-the-Closure-Compiler">One which lead the way for things like TypeScript and Flow to follow.</a></p><p>The other aspect of TypeScript that excited Boris was transpilation. Transpilation is the term coined to describe what TypeScript does when it comes to emit output. It takes in TypeScript and pumps out JavaScript. The question is: what sort of JavaScript? One choice the TypeScript team could have made was just having the compiler stripping out types from the codebase. If it worked that way then you&#x27;d get out the JavaScript equivalent of the TypeScript you wrote. You wrote a <code>class</code>? TypeScript emits a <code>class</code>; just; one shorn of types and interfaces.</p><p>The TypeScript team made a different choice. They wrote the compiler such that the user could write ES6 style TypeScript syntax and have the TypeScript compiler transpile that down to ES5 or even ES3 syntax. This made TypeScript a much more interesting proposition than it already was, for a couple of reasons.</p><p>ES6 had been in the works for some time at this point. The release was shaping up to be the biggest incremental change to JavaScript that had so far happened. Or that would ever happen. Prior to this, JavaScript had experienced no small amount of tension and disagreement as it sought to evolve and develop. These played out in the form of the abandoned fourth edition of the language. There were arguments, harsh words, public disagreements and finally a failure to ship ECMAScript 4. In an alternate universe this was the end of the road for JavaScript. However, in our universe JavaScript got another throw of the dice.</p><p>It&#x27;s telling that ES5 was for a long time known also as ES3.1; reflecting that it was initially planned to be the stepping stone between ES3 and ES4. In reality it ended up being the stepping stone between ES3 and ES6. As it turned out, it was a vital one too, <a href="https://en.m.wikipedia.org/wiki/ECMAScript">it allowed the TC39 to recalibrate after a very public shelving of plans.</a></p><p>The band was back together (albeit with a new rhythm section) and ES6 was going to be <em>massive</em>. JavaScript was going to get new constructs such as <code>Map</code>, <code>Set</code>, new scoping possibilities with <code>let</code> and <code>const</code>, <code>Promise</code>s which paved the way for new kinds of async programming, the contentious <code>class</code>es…. And who can forget where they were when they first heard about &quot;fat&quot; arrow functions?</p><p>People salivated at the idea of it all. Such new shiny toys! But how could we use them? Whilst all this new hotness was on the way, where could you actually run your new style code? Complete browser implementations of ES6 wouldn&#x27;t start to materialise until 2018. Given the slowness of people to upgrade and the need to support the lowest common denominator of browser this could have meant that all the excitement was trapped in a never tomorrow situation.</p><p>Back to TypeScript. The team had a solution for this issue. In their wisdom, the TypeScript team allowed us to write ES6 TypeScript and the compiler could (with some limitations) transpile it down to ES3 JavaScript. The audacity of this was immense. The TypeScript team brought the future back to the past. What&#x27;s more, they made it work in Internet Explorer 6. Now that&#x27;s rock&#x27;n&#x27;roll. It&#x27;s nothing short of miraculous!</p><p>The significance of transpilation to TypeScript cannot be overstated.</p><p>You might be thinking to yourself, &quot;that&#x27;s just Babel, right?&quot; Right. It&#x27;s just that Babel didn&#x27;t exist then. 6to5 was still an idea waiting for Sebastian McKenzie to think of. Even if you were kind of &quot;meh&quot; on types, the attraction of using a tool which allowed you to use new JavaScript constructs without breaking your customers was a significant draw. People may have come for types, but once they&#x27;d experienced the joy of a lexically bound <code>this</code> in a fat arrow function they were <em>never</em> going back.</p><p>Success has many parents. TypeScript is a successful project. One reason for this is that it&#x27;s an excellent product that fills a definite need. Another reason is one that can&#x27;t be banked upon; timing. TypeScript has enjoyed phenomenal timing. Appearing just when JavaScript was going off like a rocket and having the twin benefits of types and future JS today when nothing else offered anything close, that&#x27;s perfect timing. It got people&#x27;s curiosity. Now it got Boris&#x27;s attention.</p><h2>Definitely Typed</h2><p><img src="../static/blog/2019-10-08-definitely-typed-movie/dt-logo-smallish.png" alt="The Definitely Typed logo"/></p><p>Boris had been feeling unproductive. He would build applications in JS and watch them unaccountably break as he made simple tweaks to them. He was constantly changing things, breaking them, fixing them and hoping he hadn&#x27;t broken something else along the way. It was exhausting. He saw the promise in what TypeScript was offering and decided to give it a go.</p><p>It was great. He fired up Visual Studio and converted a <code>.js</code> file to end with the mystical TypeScript suffix of <code>.ts</code>. In front of his eyes, red squiggly lines started to appear here and there in his code. As he looked at the visual noise he could see this was TypeScript delivering on its promise. It was finding the bugs he hadn&#x27;t spotted. These migrations were also addictive; the more information you could feed the compiler, the more problems it found. Boris felt it was time to start writing type definitions, whatever they were.</p><p>Boris quickly learned how to write a type definition and set to work. Most libraries weren&#x27;t well documented and so he found himself reading the source code of libraries he used in order that he could write the definitions. At first, the definitions were just files dropped in his ASP.NET MVC projects that he copied around. That wasn&#x27;t going to scale; there needed to be somewhere he could go to grab type definitions when he needed them. And so on October 5th 2012 he created a repository under his profile at GitHub called &quot;DefinitelyTyped&quot;: <a href="https://github.com/DefinitelyTyped/DefinitelyTyped/commit/647369a322be470d84f8d226e297267a7d1a0796">https://github.com/DefinitelyTyped/DefinitelyTyped/commit/647369a322be470d84f8d226e297267a7d1a0796</a></p><p>Boris took his type definitions and put them into this repository. Were you ever curious what the first definition added was? Close your eyes and think... You might imagine it was the (then number one JavaScript library on the web) jQuery. In fact it was Modernizr. Then Underscore followed, and then jQuery. Take a look:</p><p><a href="https://github.com/DefinitelyTyped/DefinitelyTyped/commits?after=4a4cf23ff4301835a45bb138bbb62bf5f0759255+699&amp;author=borisyankov">https://github.com/DefinitelyTyped/DefinitelyTyped/commits?after=4a4cf23ff4301835a45bb138bbb62bf5f0759255+699&amp;author=borisyankov</a></p><p><img src="../static/blog/2019-10-08-definitely-typed-movie/Initial-CommitsDefinitelyTyped.png" alt="A screenshot of the initial commits to Definitely Typed on GitHub"/></p><p>It wasn&#x27;t complicated; it was just a folder with subfolders underneath; each folder representing a project. One for jQuery, one for jQuery UI, one for Knockout.... You get the idea. It&#x27;s not so different now.</p><p>Boris had laid simple but dependable foundations. Definitely Typed had been born.</p><h2>How Do You Test a Type Definition?</h2><p>Boris was careful too. Right from the first type definition he added tests alongside them. Now tests for a type definition were a conundrum. How do you write a test for interfaces that don&#x27;t exist in the runtime environment? Code that is expunged as part of the compilation process. Well, the answer Boris came to was this: a compilation test.</p><p>Someone once said: compilation is the first unit test... But it&#x27;s a doozy. They&#x27;re right. The value you get from compilation, from a computer checking the assertions your code makes, is significant. Simply put, it takes a large amount of tests to get the same level of developer confidence. Computers are wonderful at attention to detail in a way that puts even the most anally retentive human being to shame.</p><p>So if Boris had written a definition called <code>mylib.d.ts</code>, he&#x27;d write a file that exercises this type definition. A <code>mylib.tests.ts</code> if you will. This file would contain code that exercises the type definition in the way that it should correctly be used. This is code that will never be executed in the way that tests normally are; a test program is never actually run. Rather these tests exist solely for compilation time. (In much the same way that TypeScript types only exist for compilation time.) Boris&#x27;s plan was this: no compilation errors in <code>mylib.tests.ts</code> represents passing tests. Compilation errors in <code>mylib.tests.ts</code> represents failing tests. It was functional, brutal and also beautiful in it&#x27;s simplicity.</p><p>So, imagine your definition looked like this:</p><pre><code class="language-ts">declare function turnANumberIntoAString (numberToMakeStringOutOf: number): string
</code></pre><p>You might write a compilation test that looks like this:</p><pre><code class="language-ts">const itIsAString: string = turnANumberIntoAString(42);
</code></pre><p>This test ensures that you can use your function in the way you&#x27;d expect. It returns the types you&#x27;d desire (a <code>string</code> in this case) and it accepts the parameters you&#x27;d expect (a single <code>number</code> for this example). If someone changed the definition in future, such that a different type was returned or a different set of parameters was required it would break the test. The test code wouldn&#x27;t compile anymore. That&#x27;s the nature of our &quot;test&quot;. It&#x27;s blunt but effective.</p><p>This is the very first test committed to Definitely Typed; a test for Modernizr.</p><p><a href="https://github.com/DefinitelyTyped/DefinitelyTyped/blob/a976315cacbcfc151d5f57b25f4325dc7deca6f2/Tests/modernizr.ts">https://github.com/DefinitelyTyped/DefinitelyTyped/blob/a976315cacbcfc151d5f57b25f4325dc7deca6f2/Tests/modernizr.ts</a></p><p>This idea represents what tests look like throughout Definitely Typed today. They&#x27;re now &quot;run&quot; as part of Continuous Integration and type definitions are tested in concert with one another to ensure that a change to one type definition doesn&#x27;t break another. But there is nothing fundamentally different in place today to what Boris originally came up with.</p><h2>Independence</h2><p>Very quickly, Definitely Typed became a known project. People like Steve Fenton (author of the first book about TypeScript) were vocal supporters of the project. The TypeScript team talked up the project and were entirely supportive of its existence. In fact, at every given opportunity Anders Hejlsberg would sing its praises. For a while you could guarantee that any TypeScript talk by Anders would include a variant of &quot;this guy called Boris started a project called Definitely Typed&quot;. The impression he gave was that he was kind of amazed, and thoroughly delighted, the project existed.</p><p>The TypeScript team were completely uninvolved with Definitely Typed. That in itself is worth considering. The perception of Microsoft by developers generally in 2012 was at best, highly suspicious. &quot;Embrace, extend, extinguish&quot; - a strategy attributed to MS was very much a current perspective. This was born out in online comments and conversations at meetups. <a href="https://news.ycombinator.com/item?id=4597716">The Hacker News comments on the TypeScript release were a mixed bag.</a> The reaction on social media was rather less generous. Certainly it was harsh enough to prompt Scott Hanselman to write something of <a href="https://www.hanselman.com/blog/WhyDoesTypeScriptHaveToBeTheAnswerToAnything.aspx">a defence of TypeScripts right to exist</a>.</p><p>Given that TypeScript had arrived with the promise of transforming the JavaScript developer experience, the developer community was understandably cautious. Was Microsoft doing a good or ill? Could they be trusted? There were already signs that MS was changing. For example, it had been shipping open source libraries such as jQuery with ASP.Net MVC for some time. Microsoft was starting to engage with the world of open source software.</p><p>How Microsoft interacted with the (very open source driven) JS community was going to be key to the success (or not) of TypeScript. What happened with the establishment of Definitely Typed very much indicated TypeScripts direction of travel.</p><p>On day one of its existence, Boris took type definitions written by Microsoft and made them available via Definitely Typed. A ballsy move. It would have been completely possible for MS to object to this. They didn&#x27;t.</p><p>People like Diullei Gomes started submitting pull requests to improve the existing definitions and add new ones. Diullei even wrote the first command line tooling which allowed people to install type definitions: TSD. Within a surprisingly short period, DT had become the default home of type definitions on the web. There were briefly alternative Definitely Typed styled collections of type definitions elsewhere on GitHub but they didn&#x27;t last.</p><p>This all happened completely independently of the TypeScript team. Definitely Typed existing actually allowed TypeScript itself to prosper. It was worth persevering with this bleeding edge language because of the interoperability Definitely Typed was providing to the community. So the hands off attitude of MS was both surprising and encouraging. It showed trust of the community; something that hadn&#x27;t hitherto been a commonly noted characteristic of MS.</p><p>Boris started adding contributors to Definitely Typed to help him with the work. Definitely Typed was no longer a one man band, it had taken an important step. It was built and maintained by an increasing number of creative and generous people. All motivated by a simple aim: the best developer experience when working with TypeScript and existing JS libraries.</p><h2>Basarat Ali Syed</h2><p><img src="../static/blog/2019-10-08-definitely-typed-movie/basarat.jpg" alt="A photograph of Basarat"/></p><p>Basarat Ali Syed was a 27 year old who had recently moved to Melbourne, Australia from Pakistan. You might know of him for a number of reasons, not least being the TypeScript equivalent of Jon Skeet. That, incidentally, is not a coincidence. Basarat had watched Jon Skeet&#x27;s impressive work, being <em>the</em> gold standard in C# answers and thought &quot;there&#x27;s something worth emulating here&quot;.</p><p>Bas was working for a startup who had a JS frontend. About six months before TypeScript was announced to the world he watched Anders Hejlsberg do a presentation on JavaScript which included Anders saying to the audience &quot;don&#x27;t you just wish you had type safety?&quot; with a twinkle in his eye. TypeScript was of course well underway by this time; just not yet public. Bas remembered the comment and, when TypeScript was announced, he was ready. He made it his personal mission to be the goto person answering questions about TypeScript on Stack Overflow.</p><p>In those early days of TypeScript, if you put a question about TypeScript onto Stack Overflow there was a very good chance that Bas would answer it. And Bas was more helpful than your typical SO answerer. Not only would he provide helpful commentary and useful guidance, he would often find him answering &quot;yeah, the problem isn&#x27;t your code, it&#x27;s the type definition. It needs improvement. In fact, I&#x27;ve raised a PR to fix it here…&quot;</p><p>Boris saw the drip, drip of Basarat PRs turning into a flood. So, very quickly, he invited Basarat join Definitely Typed. Now Bas could not just suggest changes, he could ensure they were made. Step by step the quality of type definitions improved.</p><p>Basarat describes himself as a &quot;serial OSS contributor and mover on-er&quot;. It&#x27;s certainly true. As well as his Stack Overflow work, he&#x27;s been someone involved in the early days of any number of open source projects. Not just Definitely Typed. Bas also worked on the TypeScript port of the JavaScript task runner; Grunt TS. He met up with Pete Hunt (he of React) at a Decompress conference and together they hacked together a POC webpack TypeScript loader. (That POC ultimately lead to James Brantly creating ts-loader which I maintain.) Bas wrote the atom-typescript plugin which offers first class support for TypeScript in Atom. Not content with that he went on to write a full blown editor of his own called alm-tools.</p><p>This is not an exhaustive list of his achievements and already I&#x27;m tired. Besides this he wrote the TypeScript Deep Dive book and the VS Code TypeScript God extension. And more.</p><p>Bas had the level of self knowledge required to realise that getting others involved was key to the success of open source projects. Particularly given that he knew he had a predilection to eventually move on, to work on other things. So Bas kept his eyes open and welcomed in new maintainers for projects he was working on. Bas&#x27; actions in particular were to be crucial. Bas grew the Definitely Typed team; he invited others in, he got people involved.</p><p>On December 28th 2013 Basarat decided that a regular contributor to Definitely Typed might be a potential team member. Bas opened up Twitter and sent a Direct Message to John Reilly.</p><p><img src="../static/blog/2019-10-08-definitely-typed-movie/2019-10-02%2B21_51_58-basarat%2B_%2BTwitter.png" alt="A screenshot of direct message Basarat sent to John Reilly in Twitter"/></p><h2>John Reilly</h2><p><img src="../static/blog/2019-10-08-definitely-typed-movie/johnny_reilly.jpg" alt="A photograph of John Reilly"/></p><p>That&#x27;s me. Or <a href="https://twitter.com/johnny_reilly">johnny_reilly on Twitter</a> and <a href="https://github.com/johnnyreilly">johnnyreilly</a> on GitHub (as John Papa and I have learned to our chagrin; GitHub don&#x27;t support the &quot;<!-- -->_<!-- -->&quot; character in usernames). Relatively few people call me Johnny. I&#x27;m named that online because back when I applied for an email address, someone had already bagsied john<!-- -->_<a href="mailto:reilly@popularemailhotness.com.">reilly@popularemailhotness.com.</a> So rather than sully my handle with a number or a middle name I settled for johnny_reilly. I haven&#x27;t looked back and have generally tried to keep that nom de plume wherever I lay my hat online.</p><p>In contrast to others I was a relatively late starter to TypeScript. I was intrigued right from the initial announcement, but held off from properly getting my hands dirty until generics was added to the language in 0.9. (This predisposition towards generics in a language perhaps explains why I didn&#x27;t get too far with Golang.)</p><p>At that point I was working in London for a private equity house. It was based in the historic and affluent area of St James. St James is an interesting part of London, caught midway between the Government, Buckingham Palace and the heart of the West End. It&#x27;s old fashioned, dripping with money and physically delightful. It&#x27;s the sort of place film crews dash towards when they&#x27;re called upon to show old fashioned London in all its pomp. It rocks.</p><p>My team hated JavaScript. Absolutely loathed it. I was the solo voice saying &quot;but it&#x27;s really cool!&quot; whilst they all but burned effigies of Brendan Eich in each code review. However, to my delight (and their abject horror) the project we were working on could only be implemented using JS. Essentially the house wanted an application offering rich interactivity which had to be a web app. So… JS. We were coding then with a combination of jQuery and Knockout JS. And, in large part due to the majority of the team being unfamiliar with JS, we were shipping bugs. The kind of bugs that could be caught by a compiler. By static typing. Not to put too fine a point on it; by TypeScript.</p><p>So I proposed an experiment: &quot;Let&#x27;s take one screen and develop it with TypeScript. Let&#x27;s leave the rest of the app as is; JavaScript as usual. And then once we&#x27;re done with that screen let&#x27;s see how we feel about it. TypeScript might not be that great. But that&#x27;s fine, if it isn&#x27;t we&#x27;ll take the generated JS, keep that and throw away the TypeScript. Deal?&quot;</p><p>The team were on board and, one sprint review later, we decided that all future JS functionality would be implemented with TypeScript. We were in!</p><p>From day one of using TypeScript I was in love. I had the functionality of JavaScript, the future semantics of JavaScript and I was making less mistakes. Our team had become more productive. We were shipping faster and more reliably with fewer errors. People were noticing; our reputation as a team was improving, in part due to our usage of TypeScript. We had a jetpack.</p><p>However. I wasn&#x27;t satisfied. As I tapped away at my keyboard I found type definitions to be… imperfect. And that niggled. Did it ever niggle. By then <a href="https://github.com/staxmanade">Jason Jarrett</a> had wired up Definitely Typed packages to be published out to Nuget. Devs using ASP.NET MVC 4 (as I then was) were busily installing type definitions alongside AutoFac and other dependencies. Whilst most of those dependencies arrived like polished diamonds, finished products ready to be plugged into the project and start adding value. The type definitions by contrast felt very beta. And of course, they were. TypeScript was beta. The definitions reflected the newness of the language.</p><p>I could make it better.</p><p>I started submitting pull requests. The first problem I decided to solve was IntelliSense. I wanted IntelliSense for jQuery. If you went to <a href="https://api.jquery.com">https://api.jquery.com</a> there was rich documentation for every method jQuery exposed. I wanted to see that documentation inside Visual Studio as I coded. If I keyed in <code>$.appendTo(</code> I wanted VS to be filled with the content from <a href="https://api.jquery.com/appendTo/">https://api.jquery.com/appendTo/</a> . That was my mission. For each overload of the method I&#x27;d add something akin to this to the type definition file:</p><pre><code class="language-ts">/**
 * Insert every element in the set of matched elements to the end of the target.
 *
 * @param value A selector, element, HTML string, array of elements, or jQuery
 *              object; the matched set of elements will be inserted at the end
 *              of the element(s) specified by this parameter.
 */
appendTo(target: string): JQuery;
</code></pre><p>It was a tedious task plugging it all in, but the pleasure I got from having rich IntelliSense in VS more than made up for it to me. Along the way I added and fixed sections of the jQuery API that hadn&#x27;t been implemented, or had been implemented incorrectly. It got to a point where jQuery was a good example of what a type definition should look like. That remains the case to this day; surprisingly few type definitions enjoy the JSDoc richness of jQuery. <a href="https://blog.johnnyreilly.com/2014/05/typescript-jsdoc-and-intellisense.html">I have tried to encourage more use of this with blog posts code reviews and the like, but it&#x27;s never got the traction I&#x27;d hoped.</a></p><p>I&#x27;m fairly relentless when I put my mind to something. I work very hard to make things come to pass. What this meant at one point was the Definitely Typed maintainers receiving multiple PRs a day. Which prompted Bas to wonder &quot;I wonder if he&#x27;d like to join us?&quot;</p><p>I happily accepted Bas&#x27; invitation and soon found myself reading this email:</p><blockquote><p>From: Bas</p><p>Sent: 28 December 2013 11:47</p><p>To: Boris Yankov; johnny<!-- -->_<a href="mailto:reilly@hotmail.com">reilly@hotmail.com</a>; Bas; vvakame; Bart van der Schoor; Diullei Gomes; steve fenton; Jason Jarret Subject: DefinitelyTyped team introduction</p><p>Dear All,</p><p>Meet John Reilly (github : <a href="https://github.com/johnnyreilly">https://github.com/johnnyreilly</a> , twitter : <a href="https://twitter.com/johnny%5C_reilly">https://twitter.com/johnny\_reilly</a>) who will be helping with Definitely Typed definitions.</p><p>Boris manages the project and he can add you as a collaborator.</p><p>Additional team member introductions:</p><p>Admin : Boris Yankov</p><p>TSD package manager : <a href="https://github.com/DefinitelyTyped/tsd">https://github.com/DefinitelyTyped/tsd</a> : Diullei / Bart van der Schoor</p><p>NUGET: <a href="https://github.com/DefinitelyTyped/NugetAutomation">https://github.com/DefinitelyTyped/NugetAutomation</a> : Json Jarret</p><p>Passionate TypeScript users like yourself: Wakame, Myself and SteveFenton .</p><p>Cheers, Bas (Basarat)</p></blockquote><p>Some of those names you&#x27;ll recognise; some perhaps not. Jason Jarrett wrote the Nuget distribution mechanism for type definitions that ended up existing for far longer than anyone (least of all Jason) anticipated. Steve Fenton was largely a cheerleader for Definitely Typed in its early days. Diullei and Bart, amongst other things, worked on the initial command line tooling for DT: TSD.</p><p>After being powered up in Definitely Typed, my contributions only increased. Anything that I was using in my day to day work, I wanted to have an amazing TypeScript experience. I wanted the language to thrive and I was pretty sure I could help by trying to get users the best-in-class developer experience as they used JS libraries. I&#x27;ve always found good developer experience a strong motivation; the idea being, if someone loves their tools, they&#x27;ll do great work. The end customer (of whatever they&#x27;re building) gets a better product sooner. Great developer experience is a force multiplier for building software.</p><h2>Policy time</h2><p>TypeScript was now at version 0.9.1. Still very much beta. Back then every release was breaking. Breaking. Very much with a capital &quot;B&quot;.</p><p>TypeScript had, since the very early days, made a commitment to track the ECMAScript standard. All JavaScript is valid TypeScript. However, there was briefly a period where this might not have been so. One of the things people most remember from the initial release is that they could now write classes. These were already the standardised classes of ES6 but it almost wasn&#x27;t to be. For a brief period there had been consideration of doing something subtly different. In fact Anders would describe the TypeScript team&#x27;s journey towards embracing the standards as a tale tinged with regret. In doing so they&#x27;d had to say goodbye to a different implementation of classes which he&#x27;d preferred but which they&#x27;d ditched because they weren&#x27;t standard.</p><p>Alongside differences like this there were other delineations. Types had different names in the past which, as time went by, were renamed to align with standards. <code>boolean</code> was originally <code>bool</code> for instance; likely a reflection of Anders involvement with C#.</p><p>These sorts of changes, alongside any number of others, meant that each release of TypeScript sometimes entirely broke the definitions in Definitely Typed. Most notable was the 0.9.1 -&gt; 0.9.5 migration. This was both an exercise in serious pain endurance and also a testament to the already strong commitment to TypeScript that existed. The reason people were willing to put the effort in to keep these migrations going was because they believed it was worth it. They believed in TypeScript. This PR is testament to that: <a href="https://github.com/DefinitelyTyped/DefinitelyTyped/pull/1385">https://github.com/DefinitelyTyped/DefinitelyTyped/pull/1385</a></p><p>A level of flux meant that for a long time Definitely Typed committed only to support the latest version of TypeScript and the latest version of packages therein. These days it&#x27;s not so brutal, but then it had to be as a matter of necessity.</p><p>The compiler was changing too fast and there were too few people involved to allow for any realistic alternative. As is often the case in software development, it was &quot;good enough&quot;. Any other choice would probably have increased the workload of maintainers to a point where the project would no longer be a going concern. It was a choice with downsides; trade-offs. But it was the choice that best served the future of Definitely Typed and TypeScript.</p><h2>Masahiro Wakame</h2><p><img src="../static/blog/2019-10-08-definitely-typed-movie/masahiro_wakame.jpg" alt="A photograph of Masahiro Wakame"/></p><p>Time passed. Autumn turned into winter, winter into spring. TypeScript reached 1.0. It wasn&#x27;t beta anymore. As each release came, the changes in the compiler became more gradual. This was a blessing for the Definitely Typed team. The projects popularity was ticking up and up. New definitions were added each day. The trickle of issues and PRs had become a stream, then a river. A river very much ready to burst its banks.</p><p>It was taking its toll. Inside Definitely Typed roles were shifting. Boris was starting to step back from day to day reviewing of PRs. New members were joining the project, like Igor Oleinikov. But the pace was insatiable.</p><p>Some people left the project entirely, burned out by the never ending issues and PRs. Basarat started contributing less, beginning to turn his attention to one of his many sidejams. Fortunately, it turned out that before Basarat stepped back, he had done a very fine thing. In Tokyo, Japan was a 28 year old developer named Masahiro Wakame.</p><p>Mas was using JS to build the web applications he worked on. But ECMAScript 5 wasn&#x27;t hitting the mark for him. For a time Masahiro used CoffeeScript (Jeremy Ashkenas Ruby style JS alternative). He liked it, but, as he put it: &quot;I was shooting my foot everyday&quot;. Looking out for that elusive solution he landed on Dart. It looked amazing. But it wasn&#x27;t ECMAScript. Masahiro worried he&#x27;d be locked in. He&#x27;d built some libraries and a testing framework using Dart. But he didn&#x27;t feel he could suggest that his company adopted it; it was too different and only he knew it. He was left with the &quot;what if I go under a bus?&quot; problem. If he left the company, his colleagues would find it hard to move away from using Dart. This made him very hesitant. He didn&#x27;t feel he could justify the choice.</p><p>Then Masahiro heard about TypeScript. Like Goldilocks and the three bears, this third language sounded just right. He loved the type safety. It also had a compelling proposition: the transpiled JS that TypeScript generated was human readable and idiomatic. Generating idiomatic JS as opposed to some kind of strange byte code was a goal of the language from the early days, as Anders Hejlsberg would repeatedly explain. This generation of &quot;real JS&quot; made test driving TypeScript a low risk proposition. One that appealed to the likes of Masahiro. No lock-in. You decide TypeScript isn&#x27;t for you? Fine. Take the generated JS files and shake the TypeScript dust off your sandals. Masahiro consequently went all in on TypeScript. This was his bet. And he was going to cover his bet by trying to make the ecosystem even stronger.</p><p>Masahiro started out trying to improve the testing framework in DT; sending in pull requests. Before too long, Basarat messaged him to say &quot;do you wanna become a committer?&quot; <a href="https://github.com/DefinitelyTyped/DefinitelyTyped/pull/1358">Masahiro became a committer.</a></p><p>It turned out that MH had a special qualities that DT was going to sorely need: he was willing and able to review PR after PR, day after day. His stamina was incredible.</p><p>Whilst it may not have been obvious from the outside, by now Definitely Typed was a slightly troubled project. The speed at which issues and PRs landed was relentless. Anyone who had once set GitHub to &quot;watching&quot; for Definitely Typed soon unsubscribed. It was becoming unmanageable. And whilst almost everyone else in the project was in the process of burning out / moving on / stepping back and similar, Masahiro kept going. He kept showing up. He kept reviewing. He kept merging. At his peek he was spending 2 hours a day, every day, glued to his screen in Tokyo and reviewing PRs for GitHub. The pulse of Definitely Typed may have slowed. But Masahiro kept the heart beating.</p><p>As Masahiro kept the lights on, in a hotel room in Buenos Aires an Australian named Blake Embrey was making plans...</p><h2>Blake Embrey</h2><p><img src="../static/blog/2019-10-08-definitely-typed-movie/blake_embrey.jpeg" alt="A photograph of Blake Embrey"/></p><p>Blake was a 21 year old Australian. He was a nomadic developer, travelling around the world and working remotely. He travelled from country to country armed with a suitcase and his trusty MacBook in search of WiFi. He found himself dialing into standups from cafés in Vietnam at 1am to provide Jira updates, coding from airports as he criss-crossed the globe. It was an unusual life.</p><p>A friend showed Blake TypeScript somewhere around the TypeScript 1.2 era. He was interested. He was mostly working on backend NodeJS at the time. He could see the potential that TypeScript had to help him. Around the TypeScript 1.5 era Blake started to take a really good look at what was possible. From his vantage point, there was good and there was also bad. And he thought he could help.</p><p>As a module author and developer, he loved TypeScript. It allowed him to write, publish and consume 100% type-safe JavaScript. Features like autocompletion, type validation and ES6 features became part of his typical workflow. It was so good!</p><p>However, one step in this development lifecycle was broken to his mind. The problem was module shaped. Yeah, modules. Wade into the controversy!</p><p>Thanks again to Basarat, Blake soon became a DT contributor. Of all the contributors to Definitely Typed, Blake was the first one who was looking hard at the module problem. This was because whilst he wanted TypeScript to solve the same problems as everyone else, he wanted to solve them in a world of package dependencies. He wanted to solve for Node.</p><p>Now it&#x27;s worth taking a moment to draw a comparison between web development then, and web development now. Because it&#x27;s changed. The phrase &quot;web development fatigue&quot; exists with good reason. Web development in 2014 as compared to web development in 2019 is a very different proposition. Historically, JavaScript has not had a good story around modularisation. The language meandered forwards without ever gaining an official approach to modularisation until ES6. So for twenty years, if you wanted to write a large JS application you had to think hard about how to solve this problem. And even when modules were nailed down it was longer still until module loading was standardised.</p><p>But that didn&#x27;t stop us. JavaScript apps were still being built on the frontend and the backend. On the frontend an approach to modularisation emerged called the Asynchronous Module Definition. It had some adoption but in the main that wasn&#x27;t how people rolled. The frontend was generally a sea of global variables. People would write programs that depended upon global variables and worked hard to make sure that they didn’t collide. Everything did this. Everything. Underscore? It was a global variable called <code>_</code>. jQuery? It was two global variables: <code>$</code> and <code>jQuery</code>. That&#x27;s just what people did. I&#x27;m a person. I did that. If you were there you probably did too.</p><p>On the server side, in Node JS land, a different standard had emerged: CommonJS. Unlike AMD, CommonJS was simply how the Node JS community worked. Everything was a CommonJS module. Alongside Node, npm was growing and growing. Exposing Node developers to a rich ecosystem of modules or packages that they could drop into their apps with merely a tap tap tap of <code>npm install super-cool-package</code> and then <code>var scp = require(&#x27;super-cool-package&#x27;)</code>.</p><p>And therein, as the Bard would have it, lay the rub. You see, in the frontend it was simpler. Uglier but simpler. By and large, the global variables were fine. They weren&#x27;t beautiful but they were functional. It may have impaired the development of frontend apps, but it certainly didn&#x27;t stop it.</p><p>And since a design goal of TypeScript was to meet JavaScript developers where they were and try and make their lives better, the initial focus of Definitely Typed was necessarily types that existed in the global namespace. So <code>jquery.d.ts</code> would declare global <code>$</code> and <code>jQuery</code> variables and underneath them all the jQuery methods and variables that were implemented. Alongside jQuery, maybe an application would have jQuery UI which would extend the <code>$</code> variable and add extra functionality. In addition maybe there&#x27;d be a couple of jQuery plugins in play too. (It&#x27;s worth saying that jQuery was the crack cocaine of web development back in the day. People just couldn&#x27;t get enough.)</p><p>TypeScript catered for this world by allowing type definitions to extend interfaces created by other definition files. The focus of most of the Definitely Typed contributors up to this point was frontend and hence DT was an ocean of global type definitions.</p><p>Of course, this is not what the frontend world looks like these days. The frontend now is all about npm thanks to tools like Browserify, webpack, Rollup and the like. Client and server side development is mighty similar these days. Or at least, it&#x27;s swimming in more of the same waters. There&#x27;s a good TypeScript story to tell about this as well. But there wasn&#x27;t always. Back to Blake.</p><p>Blake had published a bunch of modules on npm. But no one had ever been able to consume the type definitions from them. Why was that? Well, without delving into great detail it comes down to type definitions of a package generally conflicting with type definitions that a user installs themselves.</p><p>This essentially came down to how TSD worked and what Definitely Typed contained. TSD was a pretty simple tool; by and large it worked by copying files from Definitely Typed into a users project. The files copied would contain type definitions which contained global types. So even though you cared solely about external modules, because of Definitely Typed you found yourself installing globals alongside which lead to conflicts between different type definitions. Different type definitions punching it out whilst the TypeScript compiler stood in between ineffectually shouting &quot;leave it alone mate - it&#x27;s not worth it!&quot;</p><p>How could we have a world where external modules and global were treated distinctly? Blake had ideas… Plan one was to rewrite TSD to support external modules; the type of modules that were standard in Node land. After working hard on that for some time, Blake came to conclusion that solving global variables alongside external modules was a hard problem. A very hard problem. And perhaps that just running with external modules, <a href="https://github.com/DefinitelyTyped/tsd/issues/150">a new start if you will, represented the best way forwards</a>.</p><h2>Typings</h2><p><img src="../static/blog/2019-10-08-definitely-typed-movie/typings.jpg" alt="A screenshot of the Typings project"/></p><p>Blake made <a href="https://github.com/typings/typings">typings</a>. Typings was a number of things; it was a new command line tool to replace TSD, it was a new approach to distributing type definitions and it was a registry. But Typings was a registry which pointed out to the web. Typings installation was entirely decentralized and the typings themselves could be downloaded from almost anywhere - GitHub, NPM, Bower and even over HTTP or the filesystem. Those type definitions could be external modules or globals.</p><p>It was radical. From centralisation to decentralisation. As Blake described it:</p><blockquote><p>This decentralization solves the biggest pain point I see with maintaining DefinitelyTyped. How does an author of one typings package maintain their file in DefinitelyTyped when they get notifications on thousands of others? How do you make sure typings maintain quality when you have 1000s to review? The solution in typings is you don’t, the community does. If typings are incorrect, I can just write and install my own from wherever I want, something that TSD doesn’t really allow. There’s no merge or review process you need to wait for (300+ open pull requests!).</p><p>However, decentralization comes with the cost of discoverability. To solve this, a registry exists that maintains locations of where the best typing can currently be installed from, for any version. If there’s a newer typing, patches, or the old typing author has somehow disappeared, you can replace the entry with your own so people will be directed to your typings from now on.</p></blockquote><p>The world started to use Typings as the default CLI for type definitions. <code>typings.json</code> files started appearing in people&#x27;s repos. Typings allowed consumption of types both from the Typings registry and from Definitely Typed and so there was an easy on ramp for people to start using Typings.</p><p>Little by little, people started consuming type definitions that came from the typings registry rather than from Definitely Typed. Typings began to thrive whilst DT continued to choke. The community was beginning to diverge.</p><h2>The TypeScript Team</h2><p><img src="../static/blog/2019-10-08-definitely-typed-movie/TypeScriptTeam.jpg" alt="A photograph of the TypeScript team"/></p><p>Over in Seattle, the TypeScript team was thinking hard about the type definition ecosystem. About Definitely Typed and Typings. And about tooling and distribution.</p><p>At this point, there wasn&#x27;t a dedicated registry for type definitions. There was GitHub. By and large, all type definitions lived in GitHub. Since GitHub is a git based source control provider it was possible for it to be used as a makeshift registry. So that&#x27;s exactly what Definitely Typed and Typings were doing; piggy backing on GitHub and MacGyvering &quot;infrastructure&quot;. It worked.</p><p>There wasn&#x27;t a great versioning story. Definitely Typed just didn&#x27;t do versioning. The latest and greatest was supported. Nothing else. The Typings approach was more nuanced. It did have an approach for versioning. It supported it by dint of allowing a version number in the registry to point to a specific git hash in a repo. It was an elegant and smart approach. Blake Embrey was one sharp cat.</p><p>Innovative though it was, the decentralised Typings approach presented potential security risks as it pointed out to the web making auditing harder. Alongside this, The TypeScript team was pondering ways they could reduce friction for developers that wanted to use TypeScript.</p><p>By now, the JavaScript ecosystem had started to coalesce around npm as the registry du jour. Bower and jspm were starting to fade in popularity. NuGet (the .NET package manager) was no longer being encouraged as a place to house JS. npm was standard. TypeScript users found themselves using npm to install jQuery and reaching for tsd or Typings to install the associated type definitions. That&#x27;s two commands. With two package managers. Each with subtly different syntax. And then perhaps you had to fiddle with the <code>tsconfig.json</code> to get the compiler looking in the right places. It worked. But it didn&#x27;t feel …. idiomatic. It didn&#x27;t feel like TypeScript was meeting their users where they were.</p><p>The likes of Daniel Rosenwasser, Mohamed Hegazy and Ryan Cavanaugh found themselves pondering the problem. Alongside this, they were thinking more about what a first class module support experience in TypeScript would look like, motivated in part by the critical mass around npm, which was entirely module / package based.</p><p>That wasn’t the only thing on their minds; there was also the testing story. Definitely Typed had a straightforward testing story due to being a real mega-repo. Everything lived together and could be tested together. Thanks to the hard work of the Definitely Typed team this was already in place; every PR spun up Travis and tested all the type definitions individually and in concert with one another. Typings didn’t have this. What’s more, it would be hard to build. The decentralised nature of Typings meant that you’d need to build infrastructure to crawl the Typings registry, download the type definitions and then perform the tests. It was non-trivial and unlikely to be speedy.</p><p>There was one more factor in play. The TypeScript team were aware that for the longest time they&#x27;d been working on the language. But they&#x27;d become distant from one of the most significant aspects of how the language was used. They weren’t well enough informed about the rough edges in the type definition space. They weren’t feeling their users pain. They needed to address this and really there was only one thing to do... It was time for the TypeScript team to start eating their own dogfood.</p><h2>A Plan Emerges</h2><p>The TypeScript team reached out to Blake Embrey and started to talk about ways forward. They started collaborating over Slack.</p><p><img src="../static/blog/2019-10-08-definitely-typed-movie/typings_typescript_collaboration.jpg" alt="A screenshot of the collaboration on Slack"/></p><p>The TypeScript team had also been in contact with the Definitely Typed team. They were, at this point, aware that Definitely Typed was being kept going mainly due to the hard graft of Masahiro Wakame. As Daniel observed “vvakame was a champ”.</p><p>At this point I have to stick my own hand up and confess to thinking that Definitely Typed was not long for this world. Steve Ognibene (another DT member) and others were all feeling similarly. It seemed inevitable.</p><p><img src="../static/blog/2019-10-08-definitely-typed-movie/steveognibe.png" alt="A photograph of Steve Ognibe"/></p><p>The TypeScript team were about to change that. After talking, thinking, thinking and talking they put together a plan. It was going to change TypeScript and change Definitely Typed. It was also going to effectively end Typings.</p><p>It’s worth saying at this point that the TypeScript team didn’t enter into this lightly. They were hugely impressed by Typings. It was, to quote Daniel Rosenwasser, “an impressive piece of work”. It also had the most amazing command line experience. Everyone on the team felt that it was an incredible endeavour and had their proverbial hats off to Blake Embrey. But Definitely Typed had critical mass and, whilst it had known problems, they were problems that could be likely solved (or ameliorated) through automation. The Typings approach was very innovative, but it presented other issues which seemed harder to solve. The TypeScript team made a bet. They placed their money on Definitely Typed.</p><p>To remove friction in the type acquisition space they decided to change the compiler. It would now look out for a special scoped namespace on npm named @types. Type definitions from Definitely Typed would be published out to @types. They would land as type definition packages that matched the non @types package. This meant that TypeScript was now sharing the same infrastructure as the rest of the JS ecosystem: npm. And consequently, installation of a package like jQuery in a TypeScript workflow now looked like this: <code>npm install jquery @types/jquery</code>. One command, one tool, one registry.</p><p>They published their plans here: <a href="https://github.com/Microsoft/TypeScript/issues/9184">https://github.com/Microsoft/TypeScript/issues/9184</a></p><p>There was more. The TypeScript team had really enjoyed knowing that this open source project which ran completely independently from the TypeScript team existed. And whilst they were focused directly on the language that was reasonable. But with the changes that were being planned, TypeScript was about to start explicitly depending upon Definitely Typed. It had been unofficially true up until that point. But now it was different; TypeScript were going to automate publishing Definitely Typed packages to the special @types scope in npm which the TypeScript compiler gave preference to. TypeScript and Definitely Typed were going from dating to being engaged.</p><p>It was time for the TypeScript team to get involved.</p><p>The team committed to doing weekly rotations of a TypeScript team member working on Definitely Typed. Reviewing PRs, merging them and, crucially, helping with automation and testing.</p><p>TypeScript was now part of Definitely Typed. Definitely Typed was part of TypeScript.</p><h2>TypeScript 2.0 / Definitely Typed 2.0</h2><p>Blake was immensely disappointed. He&#x27;d put his heart and soul into Typings. It was a massive amount of work and he&#x27;d not only started a project, he&#x27;d started a community that he felt responsible for.</p><p>Although that work had arguably kickstarted the discussion of what the future of type acquisition in TypeScript should look like, Typings wouldn&#x27;t be coming along for the ride. It was a burner rocket, carrying the good ship TypeScript into outer orbit, dropping back to Earth once it&#x27;s job was done.</p><p>Very much, Blake had in mind all the people that had contributed to Typings. That all their work was going to be abandoned. He felt a sense of responsibility. It was both frustrating and heartbreaking.</p><p>When TypeScript 2.0 shipped, in the release announcement was the following statement: <a href="https://devblogs.microsoft.com/typescript/announcing-typescript-2-0/">https://devblogs.microsoft.com/typescript/announcing-typescript-2-0/</a></p><blockquote><p>We’d like to thank Blake Embrey for his work on Typings and helping us bring this solution forward.</p></blockquote><p>Blake really appreciated the recognition. In years to come Blake would come to feel that the decisions made were the right ones. That they lead to TypeScripts continued success and served the community well. But he has regrets. He says now &quot;I am disappointed we didn&#x27;t get to integrate the two philosophies for managing types. It hurt Typings registry contributors without a story in place, I didn’t want to let down and alienate potential contributors of type definitions.&quot;</p><p>A young Australian man had helped change the direction of TypeScript. It was time for him to take a well earned rest.</p><p>In the meantime, the TypeScript team was starting to get stuck into the work of giving Definitely Typed a make-over.</p><p><img src="../static/blog/2019-10-08-definitely-typed-movie/rotation.png" alt="Screenshot of the rota for Definitely Typed work for the TypeScript team"/></p><p>At this point, Definitely Typed had more than 500 open pull requests. Most of which had been open for a very long time. The most urgent and pressing problem was getting that down. The TypeScript team committed to, in perpetuity, a weekly rotation where one team member would review PRs. This would, in future, mean that PRs were handled in a timely fashion and that the number of open PRs was generally kept beneath 100.</p><p>Alongside this, changes were being made to the TypeScript compiler. In large part these related to enabling automatic type acquisition through the @types scope. To make that work, the TypeScript team realised pretty quickly that many of the type definitions would not work as is. Ryan wrote up this report:</p><p><img src="../static/blog/2019-10-08-definitely-typed-movie/RyansDefTypReport.png" alt="Ryan Cavanagh&#x27;s report on what to do in Definitely Typed"/></p><p>At this point in time there were around 1700 type definitions. Pretty much all of them required some massaging. Roughly speaking, with TS 2.0, the language was going to move from a name based type acquisition approach to a file based one. New features were added to TypeScript 2.0 such as the <code>export as namespace</code> syntax to support a type definition supporting both being used in modules (where there are <code>import</code> / <code>export</code>s) but also in script files (where there aren&#x27;t)</p><p>Ryan Cavanaugh put together scripts that migrated 1200 of the type definitions to TypeScript 2.0 syntax. The remaining 500 were delicately transitioned by hand by diligent TypeScript team members. It was a task of utter drudgery that still sparks flickers of PTSD in those who were involved. It was like being in the digital equivalent of a Dickensian workhouse.</p><p>This was one of the reasons why going with the centralised approach of Definitely Typed instead of the decentralised one of Typings was necessary. Because the TypeScript team were involved in DT they could help make things happen. They could do the hard work. In a decentralised world that wouldn&#x27;t be possible; everything would constantly be held up, waiting.</p><p>It took a long time to get the types 2.0 branch to a point where CI went green. All this time, merges we&#x27;re taking place between the master branch and the future one. It was hard, unglamorous work. As Ryan put it, &quot;I partied hard when CI went green for the first time on types 2.0.&quot;</p><p><img src="../static/blog/2019-10-08-definitely-typed-movie/types20goinggreen.png" alt="Screenshot of @types going green"/></p><p>The first and most obvious addition was the automation of TypeScript definitions being published out to npm.</p><p>Next came a solution for the &quot;notification flood&quot; issue. It was no longer feasible for a user to have Definitely Typed set up as &quot;watching&quot; in GitHub. That way lead an unstoppable deluge of information about issues and pull requests. The result of that was that users were generally unaware of changes / issues and so on. People, as much as they wanted to be, were becoming disconnected from the type definitions they were interested in in DT.</p><p>The solution for this problem was, as with so many problems, a bot. It would send notifications to the users who had historically worked on a type definition when someone sent a PR. This was hugely useful. It made it possible for people to become effective stewards of the type definitions they knew about. It meant people could effectively remain involved with DT; giving them targeted information. It was the solution to a communications problem.</p><p>As Ryan Cavanaugh put it when he looked back upon TypeScripts story, he had this to say: “Definitely Typed is the best thing that could exist from our perspective”.</p><p>He was speaking from the perspective of a TypeScript team member. He could as well be speaking for the developer world at large. Definitely Typed is an organic monster of open source goodness; bringing types to the world thanks to nearly 10,000 contributors. Each person of which has donated at least an hour or their time for the greater good. Far more than that in many cases. It’s incredible. I’m glad I get to be part of it. I never would have guessed it would have turned out like this.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Start Me Up: ts-loader meet .tsbuildinfo]]></title>
            <link>https://blog.johnnyreilly.com/2019/09/30/start-me-up-ts-loader-meet-tsbuildinfo</link>
            <guid>Start Me Up: ts-loader meet .tsbuildinfo</guid>
            <pubDate>Mon, 30 Sep 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[With TypeScript 3.4, a new behaviour landed and a magical new file type appeared; .tsbuildinfo]]></description>
            <content:encoded><![CDATA[<p>With TypeScript 3.4, <a href="https://www.typescriptlang.org/docs/handbook/release-notes/typescript-3-4.html">a new behaviour landed and a magical new file type appeared; <code>.tsbuildinfo</code></a></p><blockquote><p>TypeScript 3.4 introduces a new flag called <code>--incremental</code> which tells TypeScript to save information about the project graph from the last compilation. The next time TypeScript is invoked with <code>--incremental</code>, it will use that information to detect the least costly way to type-check and emit changes to your project.</p><p>...</p><p>These <code>.tsbuildinfo</code> files can be safely deleted and don’t have any impact on our code at runtime - they’re purely used to make compilations faster.</p></blockquote><p>This was all very exciting, but until the release of TypeScript 3.6 there were no APIs available to allow third party tools like <code>ts-loader</code> to hook into them. The wait is over! Because with TypeScript 3.6 the APIs landed: <a href="https://www.typescriptlang.org/docs/handbook/release-notes/typescript-3-6.html#apis-to-support---build-and---incremental">https://www.typescriptlang.org/docs/handbook/release-notes/typescript-3-6.html#apis-to-support---build-and---incremental</a></p><p>This was the handiwork of the very excellent <a href="https://twitter.com/sheetalkamat">@sheetalkamat</a> of the TypeScript team - you can see her PR here: <a href="https://github.com/microsoft/TypeScript/pull/31432">https://github.com/microsoft/TypeScript/pull/31432</a></p><p>What&#x27;s more, Sheetal took the PR for a test drive using <code>ts-loader</code>, and her hard work has just shipped with <code>&lt;a href=&quot;https://github.com/TypeStrong/ts-loader/releases/tag/v6.2.0&quot;&gt;v6.2.0&lt;/a&gt;</code>:</p><ul><li><a href="https://github.com/TypeStrong/ts-loader/pull/1012">https://github.com/TypeStrong/ts-loader/pull/1012</a></li><li><a href="https://github.com/TypeStrong/ts-loader/pull/1017">https://github.com/TypeStrong/ts-loader/pull/1017</a></li></ul><p>If you&#x27;re a <code>ts-loader</code> user, and you&#x27;re using TypeScript 3.6+ then you can get the benefit of this now. That is, if you make use of the <code>experimentalWatchApi: true</code> option. With this set:</p><ol><li><p>ts-loader will both emit and consume the <code>.tsbuildinfo</code> artefact.</p></li><li><p>This applies both when a project has <code>tsconfig.json</code> options <code>composite</code> or <code>incremental</code> set to <code>true</code>.</p></li><li><p>The net result of people using this should be faster cold starts in build time where a previous compilation has taken place.</p></li></ol><h2><code>ts-loader v7.0.0</code></h2><p>We would love for you to take this new functionality for a spin. Partly because we think it will make your life better. And partly because we&#x27;re planning to make using the watch API the default behaviour of <code>ts-loader</code> when we come to ship <code>v7.0.0</code>.</p><p>If you can take this for a spin before we make that change we&#x27;d be so grateful. Thanks so much to Sheetal for persevering away on this feature. It&#x27;s amazing work and so very appreciated.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Coming Soon: Definitely Typed]]></title>
            <link>https://blog.johnnyreilly.com/2019/09/14/coming-soon-definitely-typed</link>
            <guid>Coming Soon: Definitely Typed</guid>
            <pubDate>Sat, 14 Sep 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[A long time ago (well, 2012) in a galaxy far, far away (okay; Plovdiv, Bulgaria)....]]></description>
            <content:encoded><![CDATA[<p>A long time ago (well, 2012) in a galaxy far, far away (okay; Plovdiv, Bulgaria)....</p><p><a href="https://github.com/DefinitelyTyped/DefinitelyTyped">Definitely Typed</a> began!</p><p>This is a project that set out to provide type definitions for every JavaScript library that lacked them. An ambitious goal. Have you ever wondered what the story that lay behind it was?</p><p>Perhaps you know that the project was started by a shadowy figure named &quot;Boris Yankov&quot;. And maybe you know that the TypeScript team is now part of the Definitely Typed team. There&#x27;s a lot more to tell.</p><p>This autumn, I&#x27;d like to tell you the story of how Definitely Typed came to be what it is. From an individual commit in a repo that Boris created in 2012 to <a href="https://octoverse.github.com/projects">the number 10 project by contributions on GitHub in 2018</a>. I&#x27;m part of that story. Basarat Ali Syed is part of that story. Masahi Wakame too. Blake Embrey. Steve Fenton. Igor Oleinikov. It&#x27;s an amazing and unexpected tale. One that turns upon the actions of individuals. They changed your life and I&#x27;d love you to learn how.</p><p>So, coming soon to a blog post near you, is the story of Definitely Typed. It&#x27;s very exciting! Stay tuned...</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Symbiotic Definitely Typed]]></title>
            <link>https://blog.johnnyreilly.com/2019/08/17/symbiotic-definitely-typed</link>
            <guid>Symbiotic Definitely Typed</guid>
            <pubDate>Sat, 17 Aug 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[I did ponder calling this post "how to enable a good TypeScript developer experience for npm modules that aren't written in TypeScript"... Not exactly pithy though.]]></description>
            <content:encoded><![CDATA[<p>I did ponder calling this post &quot;how to enable a good TypeScript developer experience for npm modules that aren&#x27;t written in TypeScript&quot;... Not exactly pithy though.</p><p>Definitely Typed is the resource which allows developers to use TypeScript with existing JavaScript libraries that ship without their own type definitions.</p><p>DT began as a way to enable interop between JS and TS. When DT started, everything on npm was JavaScript. Over time it has become more common for libraries (eg <a href="https://github.com/mobxjs/mobx">Mobx</a> / <a href="https://github.com/angular/angular">Angular</a>) to be written (or rewritten) in TypeScript. For publishing, they are compiled down to JS with perfect type definitions generated from the TypeScript alongside the compiled JavaScript. These libraries do not need to exist in Definitely Typed anymore.</p><p>Another pattern that has emerged over time is that of type definitions being removed from Definitely Typed to live and be maintained alongside the libraries they support. An example of this is <a href="https://github.com/moment/moment">MomentJS</a>.</p><p>This week, I think for the first time, there emerged another approach. <a href="https://kentcdodds.com/">Kent C Dodds</a>&#x27; <code>react-testing-library</code> had started out with the MomentJS approach of hosting type definitions alongside the JavaScript source code. <a href="https://github.com/testing-library/react-testing-library/pull/437">Alex Krolic raised a PR which proposed removing the type definitions from the RTL repo in favor of having the community maintain them at DefinitelyTyped.</a></p><p>I&#x27;ll directly quote Kent&#x27;s explanation of the motivation for this:</p><blockquote><p>We were getting a lot of drive-by contributions to the TypeScript typings and many pull requests would either sit without being reviewed by someone who knows TypeScript well enough, or be merged by a maintainer who just hoped the contributor knew what they were doing. This resulted in a poor experience for TypeScript users who could experience type definition churn and delays, and it became a burden on project maintainers as well (most of us don&#x27;t know TypeScript very well). Moving the type definitions to DefinitelyTyped puts the maintenance in much more capable hands.</p></blockquote><p>I have to admit I was reticent about this idea in the first place. I like the idea that types ship with the package they support. It&#x27;s a good developer experience; users install your package and it works with TypeScript straight out of the box. However Alex&#x27;s PR addressed a real issue: what do you do when the authors of a package aren&#x27;t interested / equipped / don&#x27;t have the time to support TypeScript? Or don&#x27;t want to deal with the noise of TypeScript related PRs which aren&#x27;t relevant to them. What then?</p><p>Alex was saying, let&#x27;s not force it. Let the types and the library be maintained separately. This can and is done well already; React is a case in point. The React team does not work on the type definitions for React, that&#x27;s done (excellently) by a crew of dedicated React lovers in Definitely Typed.</p><p>It&#x27;s a fair point. The thing that was sad about this move was that the developer experience was going to have more friction. Users would have to <code>yarn add -D @testing-library/react</code> and then subsequently <code>yarn add -D @types/testing-library__react</code> to get the types.</p><p>This two step process isn&#x27;t the end of the world, but it does make it marginally harder for TypeScript users to get up and running. It reduces the developer joy. As a side note, this is made more unlovely by <code>@testing-library/react</code> being a scoped package. <a href="https://stackoverflow.com/questions/47296731/how-can-i-install-typescript-declarations-for-scoped-namespaced-packages-via-ty">Types for a scoped package have a quirky convention for publishing.</a> A fictional scoped package of <code>@foo/bar</code> would be published to npm as: <code>@types/foo__bar</code>. This is functional but non-obvious; it&#x27;s tricky to discover. A two step process instead of a one step process is a non-useful friction that it would be great to eliminate.</p><p>Fortunately, Kent and <a href="https://github.com/FredyC">Daniel K</a> had one of these moments:</p><p><img src="../static/blog/2019-08-17-symbiotic-definitely-typed/hang-on-lads-ive-got-a-great-idea.jpg"/></p><p>Kent suggested that at the same time as dropping the type definitions that were shipped with the library, we try making <code>@types/testing-library__react</code> a dependency of <code>@testing-library/react</code>. This would mean that people installing <code>@testing-library/react</code> would get <code>@types/testing-library__react</code> installed <em>automatically</em>. So from the developers point of view, it&#x27;s as though the type definitions shipped with the package directly.</p><p>To cut a long story short reader, that&#x27;s what happened. If you&#x27;re using <code>@testing-library/react</code> from 9.1.2 you&#x27;re getting Definitely Typed under the covers. This was <a href="https://github.com/testing-library/react-testing-library/pull/437#issuecomment-521763117">nicely illustrated by Kent</a> showing what the TypeScript consumption experience looked like before the Definitely Typed switch:</p><p><img src="../static/blog/2019-08-17-symbiotic-definitely-typed/RTL-9.1.1.png"/></p><p>And here&#x27;s what it looked like after:</p><p><img src="../static/blog/2019-08-17-symbiotic-definitely-typed/RTL-9.1.2.png"/></p><p>Identical! i.e it worked. I grant you this is one of the more boring before / after comparisons there is… But hopefully you can see it demonstrates that this is giving us exactly what we need.</p><p>To quote Kent once more:</p><blockquote><p>By adding the type definitions to the dependencies of React Testing Library, the experience for users is completely unchanged. So it&#x27;s a huge improvement for the maintenance of the type definitions without any breaking changes for the users of those definitions.</p></blockquote><p>This is clearly an approach that&#x27;s useful; it adds value. It would be tremendous to see other libraries that aren&#x27;t written in TypeScript but would like to enable a good TypeScript experience for those people that do use TS also adopting this approach.</p><h2>Update: Use a Loose Version Range in <code>package.json</code></h2><p>When I <a href="https://twitter.com/johnny_reilly/status/1162843916661592064">tweeted this article</a> it prompted this helpful response from <a href="https://twitter.com/atcb">Andrew Branch</a> of the TypeScript team:</p><blockquote><p>&gt;<!-- --> use a loose version range This is my advice as well and should probably be mentioned in the article TBH.</p><p>— Kent C. Dodds (@kentcdodds) <a href="https://twitter.com/kentcdodds/status/1162876792287293440?ref_src=twsrc%5Etfw">August 18, 2019</a></p></blockquote><script src="https://platform.twitter.com/widgets.js" charSet="utf-8"></script><p>Andrew makes the useful point that if you are adding support for TypeScript via an <code>@types/...</code> dependency then it&#x27;s wise to do so with a loose version range. <a href="https://github.com/testing-library/react-testing-library/blob/c4ba755e42938018ec67dbc716037cfafca15e03/package.json#L46">In the case of RTL we did it like this:</a></p><pre><code class="language-json">&quot;@types/testing-library__react&quot;: &quot;^9.1.0&quot;
</code></pre><p>i.e. Any type definition with a version of <code>9.1</code> or greater (whilst still lower than <code>10.0.0</code>) is considered valid. You could go even looser than that. If you really don&#x27;t want to think about TypeScript beyond adding the dependency then a completely loose version range would do:</p><pre><code class="language-json">&quot;@types/testing-library__react&quot;: &quot;*&quot;
</code></pre><p>This will always install the latest version of the <code>@types/testing-library__react</code> dependency and (importantly) allow users to override if there&#x27;s a problematic <code>@types/testing-library__react</code> out there. This level of looseness is not really advised though. As in the scenario when a library (and associated type definitions) do a major release, users of the old major would get the wrong definitions by default when installing or upgrading (in range).</p><p>Probably the most helpful approach is the approach followed by RTL; fixing the major version but allowing all minor and patch releases <em>inside</em> a major version.</p><h2>Update 2: Further Discussions!</h2><p>The technique used in this blog post sparked an interesting conversation with members of the TypeScript team when it was applied to <code>&lt;a href=&quot;https://github.com/testing-library/jest-dom&quot;&gt;https://github.com/testing-library/jest-dom&lt;/a&gt;</code>. <a href="https://github.com/testing-library/jest-dom/issues/123#issuecomment-523586977">The conversation can be read here</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ASP.NET Core authentication: hard-coding a claim in development]]></title>
            <link>https://blog.johnnyreilly.com/2019/08/02/asp-net-authentication-hard-coding-claims</link>
            <guid>ASP.NET Core authentication: hard-coding a claim in development</guid>
            <pubDate>Fri, 02 Aug 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[This post demonstrates how you can hard code user authentication claims in ASP.NET Core; a useful technique to facilate testing during development.]]></description>
            <content:encoded><![CDATA[<p>This post demonstrates how you can hard code user authentication claims in ASP.NET Core; a useful technique to facilate testing during development.</p><p>I was recently part of a hackathon team that put together an API in just 30 hours. We came second. (Not bitter, not bitter...)</p><p>We were moving pretty quickly during the hackathon and, when we came to the end of it, we had a working API which we were able to demo. The good news is that the API is going to graduate to be a product! We&#x27;re going to ship this. Before we can do that though, there&#x27;s a little tidy up to do.</p><p>The first thing I remembered / realised when I picked up the codebase again, was the shortcuts we&#x27;d made on the developer experience. We&#x27;d put the API together using ASP.Net Core. We&#x27;re handling authentication using JWTs which is nicely supported. When we&#x27;re deployed, an external facing proxy calls our application with the appropriate JWT and everything works as you&#x27;d hope.</p><p>The question is, what&#x27;s it like to develop against this on your laptop? Getting a JWT for when I&#x27;m debugging locally is too much friction. I want to be able to work on the problem at hand, going away to get a JWT each time is a timesuck. So what to do? Well, during the hackathon, we just commented out <code>[Authorize]</code> attributes and hardcoded user ids in our controllers. This works, but it&#x27;s a messy developer experience; it&#x27;s easy to forget to uncomment things you&#x27;ve commented and break things. There must be a better way.</p><p>The solution I landed on was this: in development mode (which we only use whilst debugging) we hardcode an authenticated user. The way our authentication works is that we have a claim on our principal called something like <code>&quot;our-user-id&quot;</code>, the value of which is our authenticated user id. So in the <code>ConfigureServices</code> method of our <code>Startup.cs</code> we have a conditional authentication registration like this:</p><pre><code class="language-cs">// Whilst developing, we don&#x27;t want to authenticate; we hardcode to a particular users id
if (Env.IsDevelopment()) {
    services.AddAuthentication(nameof(DevelopmentModeAuthenticationHandler))
        .AddScheme&lt;DevelopmentModeAuthenticationOptions, DevelopmentModeAuthenticationHandler&gt;(
            nameof(DevelopmentModeAuthenticationHandler),
            options =&gt; {
                options.UserIdToSetInClaims = &quot;this-is-a-user-id&quot;;
            }
        );
}
else {
    // The application typically uses this
    services.AddAuthentication(JwtBearerDefaults.AuthenticationScheme)
        .AddJwtBearer(options =&gt; {
            // ...
        });
}
</code></pre><p>As you can see, we&#x27;re using a special <code>DevelopmentModeAuthenticationHandler</code> authentication scheme in development mode, instead of JWT. As we register that, we declare the user id that we want to use. Whenever the app runs using the <code>DevelopmentModeAuthenticationHandler</code> auth, all requests will arrive using a principal with an <code>&quot;our-user-id&quot;</code> claim with a value of <code>&quot;this-is-a-user-id&quot;</code> (or whatever you&#x27;ve set it to.)</p><p>The <code>DevelopmentModeAuthenticationHandler</code> looks like this:</p><pre><code class="language-cs">using System.Collections.Generic;
using System.Security.Claims;
using System.Text.Encodings.Web;
using System.Threading.Tasks;
using Microsoft.AspNetCore.Authentication;
using Microsoft.Extensions.Logging;
using Microsoft.Extensions.Options;

namespace OurApp
{
    public class DevelopmentModeAuthenticationOptions : AuthenticationSchemeOptions
    {
        public string UserIdToSetInClaims { get; set; }
    }

    public class DevelopmentModeAuthenticationHandler : AuthenticationHandler&lt;DevelopmentModeAuthenticationOptions&gt; {
        private readonly ILoggingService _loggingService;

        public DevelopmentModeAuthenticationHandler(
            IOptionsMonitor&lt;DevelopmentModeAuthenticationOptions&gt; options,
            ILoggerFactory logger,
            UrlEncoder encoder,
            ISystemClock clock
        ) : base(options, logger, encoder, clock) {
        }

        protected override Task&lt;AuthenticateResult&gt; HandleAuthenticateAsync() {
            var claims = new List&lt;Claim&gt; { new Claim(&quot;our-user-id&quot;, Options.UserIdToSetInClaims) };

            var identity = new ClaimsIdentity(claims, nameof(DevelopmentModeAuthenticationHandler));
            var ticket = new AuthenticationTicket(new ClaimsPrincipal(identity), Scheme.Name);

            return Task.FromResult(AuthenticateResult.Success(ticket));
        }
    }
}
</code></pre><p>Now, developing locally is frictionless! We don&#x27;t comment out <code>[Authorize]</code> attributes, we don&#x27;t hard code user ids in controllers.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using TypeScript and ESLint with webpack (fork-ts-checker-webpack-plugin new feature!)]]></title>
            <link>https://blog.johnnyreilly.com/2019/07/13/typescript-and-eslint-meet-fork-ts-checker-webpack-plugin</link>
            <guid>Using TypeScript and ESLint with webpack (fork-ts-checker-webpack-plugin new feature!)</guid>
            <pubDate>Sat, 13 Jul 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[The fork-ts-checker-webpack-plugin has, since its inception, performed two classes of checking:]]></description>
            <content:encoded><![CDATA[<p>The <code>fork-ts-checker-webpack-plugin</code> has, since its inception, performed two classes of checking:</p><ol><li>Compilation errors which the TypeScript compiler surfaces up</li><li>Linting issues which TSLint reports</li></ol><p><a href="https://eslint.org/blog/2019/01/future-typescript-eslint">You may have caught the announcement that TSLint is being deprecated and ESLint is the future of linting in the TypeScript world.</a> This plainly has a bearing on linting in <code>fork-ts-checker-webpack-plugin</code>.</p><p><a href="https://github.com/TypeStrong/fork-ts-checker-webpack-plugin/pull/305">I&#x27;ve been beavering away at adding support for ESLint to the fork-ts-checker-webpack-plugin.</a> I&#x27;m happy to say, the plugin now supports ESLint. Do you want to get your arms all around ESLint with <code>fork-ts-checker-webpack-plugin</code>? Read on!</p><h2>How do you migrate from TSLint to ESLint?</h2><p>Well, first of all you need the latest and greatest <code>fork-ts-checker-webpack-plugin</code>. Support for ESLint shipped with <a href="https://github.com/TypeStrong/fork-ts-checker-webpack-plugin/releases/tag/v1.4.0">v1.4.0</a>.</p><p>You need to change the options you supply to the plugin in your <code>webpack.config.js</code> to look something like this:</p><pre><code class="language-js">new ForkTsCheckerWebpackPlugin({ eslint: true });
</code></pre><p>You&#x27;ll also need the various ESLint related packages to your <code>package.json</code>:</p><pre><code class="language-js">yarn add eslint @typescript-eslint/parser @typescript-eslint/eslint-plugin --dev
</code></pre><ul><li><code>eslint</code></li><li><code>@typescript-eslint/parser</code>: The parser that will allow ESLint to lint TypeScript code</li><li><code>@typescript-eslint/eslint-plugin</code>: A plugin that contains ESLint rules that are TypeScript specific</li></ul><p>If you want, you can pass options to ESLint using the <code>eslintOptions</code> option as well. These will be passed through to the underlying ESLint CLI Engine when it is instantiated. Docs on the supported options are <a href="https://eslint.org/docs/developer-guide/nodejs-api#cliengine">documented here</a>.</p><h2>Go Configure</h2><p>Now you&#x27;re ready to use ESLint, you just need to give it some configuration. Typically, an <code>.eslintrc.js</code> is what you want here.</p><pre><code class="language-js">const path = require(&#x27;path&#x27;);
module.exports = {
  parser: &#x27;@typescript-eslint/parser&#x27;, // Specifies the ESLint parser
  plugins: [&#x27;@typescript-eslint&#x27;],
  env: {
    browser: true,
    jest: true,
  },
  extends: [
    &#x27;plugin:@typescript-eslint/recommended&#x27;, // Uses the recommended rules from the @typescript-eslint/eslint-plugin
  ],
  parserOptions: {
    project: path.resolve(__dirname, &#x27;./tsconfig.json&#x27;),
    tsconfigRootDir: __dirname,
    ecmaVersion: 2018, // Allows for the parsing of modern ECMAScript features
    sourceType: &#x27;module&#x27;, // Allows for the use of imports
  },
  rules: {
    // Place to specify ESLint rules. Can be used to overwrite rules specified from the extended configs
    // e.g. &quot;@typescript-eslint/explicit-function-return-type&quot;: &quot;off&quot;,
    &#x27;@typescript-eslint/explicit-function-return-type&#x27;: &#x27;off&#x27;,
    &#x27;@typescript-eslint/no-unused-vars&#x27;: &#x27;off&#x27;,
  },
};
</code></pre><p>If you&#x27;re a React person (and I am!) then you&#x27;ll also need: <code>yarn add eslint-plugin-react</code>. Then enrich your <code>eslintrc.js</code> a little:</p><pre><code class="language-js">const path = require(&#x27;path&#x27;);
module.exports = {
  parser: &#x27;@typescript-eslint/parser&#x27;, // Specifies the ESLint parser
  plugins: [
    &#x27;@typescript-eslint&#x27;,
    &#x27;react&#x27;,
    // &#x27;prettier&#x27; commented as we don&#x27;t want to run prettier through eslint because performance
  ],
  env: {
    browser: true,
    jest: true,
  },
  extends: [
    &#x27;plugin:@typescript-eslint/recommended&#x27;, // Uses the recommended rules from the @typescript-eslint/eslint-plugin
    &#x27;prettier/@typescript-eslint&#x27;, // Uses eslint-config-prettier to disable ESLint rules from @typescript-eslint/eslint-plugin that would conflict with prettier
    // &#x27;plugin:react/recommended&#x27;, // Uses the recommended rules from @eslint-plugin-react
    &#x27;prettier/react&#x27;, // disables react-specific linting rules that conflict with prettier
    // &#x27;plugin:prettier/recommended&#x27; // Enables eslint-plugin-prettier and displays prettier errors as ESLint errors. Make sure this is always the last configuration in the extends array.
  ],
  parserOptions: {
    project: path.resolve(__dirname, &#x27;./tsconfig.json&#x27;),
    tsconfigRootDir: __dirname,
    ecmaVersion: 2018, // Allows for the parsing of modern ECMAScript features
    sourceType: &#x27;module&#x27;, // Allows for the use of imports
    ecmaFeatures: {
      jsx: true, // Allows for the parsing of JSX
    },
  },
  rules: {
    // Place to specify ESLint rules. Can be used to overwrite rules specified from the extended configs
    // e.g. &quot;@typescript-eslint/explicit-function-return-type&quot;: &quot;off&quot;,
    &#x27;@typescript-eslint/explicit-function-return-type&#x27;: &#x27;off&#x27;,
    &#x27;@typescript-eslint/no-unused-vars&#x27;: &#x27;off&#x27;,

    // These rules don&#x27;t add much value, are better covered by TypeScript and good definition files
    &#x27;react/no-direct-mutation-state&#x27;: &#x27;off&#x27;,
    &#x27;react/no-deprecated&#x27;: &#x27;off&#x27;,
    &#x27;react/no-string-refs&#x27;: &#x27;off&#x27;,
    &#x27;react/require-render-return&#x27;: &#x27;off&#x27;,

    &#x27;react/jsx-filename-extension&#x27;: [
      &#x27;warn&#x27;,
      {
        extensions: [&#x27;.jsx&#x27;, &#x27;.tsx&#x27;],
      },
    ], // also want to use with &quot;.tsx&quot;
    &#x27;react/prop-types&#x27;: &#x27;off&#x27;, // Is this incompatible with TS props type?
  },
  settings: {
    react: {
      version: &#x27;detect&#x27;, // Tells eslint-plugin-react to automatically detect the version of React to use
    },
  },
};
</code></pre><p>You can add Prettier into the mix too. You can see how it is used in the above code sample. But given the impact that has on performance I wouldn&#x27;t recommend it; hence it&#x27;s commented out. <a href="https://dev.to/robertcoopercode/using-eslint-and-prettier-in-a-typescript-project-53jb">There&#x27;s a good piece by Rob Cooper&#x27;s for more details on setting up Prettier and VS Code with TypeScript and ESLint.</a></p><h2>Performance and Power Tools</h2><p>It&#x27;s worth noting that support for TypeScript in ESLint is still brand new. As such, the rule of &quot;Make it Work, Make it Right, Make it Fast&quot; applies.... ESLint with TypeScript still has some performance issues which should be ironed out in the fullness of time. You can <a href="https://github.com/typescript-eslint/typescript-eslint/issues/389">track them here</a>.</p><p>This is important to bear in mind as, when I converted a large codebase over to using ESLint, I discovered that initial performance of linting was terribly slow. Something that&#x27;s worth doing right now is identifying which rules are costing you most timewise and tweaking based on whether you think they&#x27;re earning their keep.</p><p>The <a href="https://eslint.org/docs/developer-guide/working-with-rules#per-rule-performance"><code>TIMING</code> environment variable</a> can be used to provide a report on the relative cost performance wise of running each rule. A nice way to plug this into your workflow is to add the <code>cross-env</code> package to your project: <code>yarn add cross-env -D</code> and then add 2 scripts to your <code>package.json</code>:</p><pre><code>&quot;lint&quot;: &quot;eslint ./&quot;,
&quot;lint-rule-timings&quot;: &quot;cross-env TIMING=1 yarn lint&quot;
</code></pre><ul><li><code>lint</code> <!-- -->-<!-- --> just runs the linter standalone</li><li><code>lint-rule-timings</code> <!-- -->-<!-- --> does the same but with the <code>TIMING</code> environment variable set to 1 so a report will be generated.</li></ul><p>I&#x27;d advise, making use of <code>lint-rule-timings</code> to identify which rules are costing you performance and then turning <code>off</code> rules as you need to. Remember, different rules have different value.</p><p><a href="https://github.com/TypeStrong/ts-loader/pull/960">Finally, if you&#x27;d like to see how it&#x27;s done, here&#x27;s an example of porting from TSLint to ESLint.</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[TypeScript / webpack - you down with PnP? Yarn, you know me!]]></title>
            <link>https://blog.johnnyreilly.com/2019/06/07/typescript-webpack-you-down-with-pnp</link>
            <guid>TypeScript / webpack - you down with PnP? Yarn, you know me!</guid>
            <pubDate>Fri, 07 Jun 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[Yarn PnP is an innovation by the Yarn team designed to speed up module resolution by node. To quote the (excellent) docs:]]></description>
            <content:encoded><![CDATA[<p>Yarn PnP is an innovation by the Yarn team designed to speed up module resolution by node. To quote the <a href="https://yarnpkg.com/en/docs/pnp">(excellent) docs</a>:</p><blockquote><p>Plug’n’Play is an alternative installation strategy unveiled in September 2018...</p><p>The way regular installs work is simple: Yarn generates a <code>node_modules</code> directory that Node is then able to consume. In this context, Node doesn’t know the first thing about what a package is: it only reasons in terms of files. “Does this file exist here? No? Let’s look in the parent <code>node_modules</code> then. Does it exist here? Still no? Too bad… parent folder it is!” - and it does this until it matches something that matches one of the possibilities. That’s vastly inefficient.</p><p>When you think about it, Yarn knows everything about your dependency tree - it evens installs it! So why is Node tasked with locating your packages on the disk? Why don’t we simply query Yarn, and let it tell us where to look for a package X required by a package Y? That’s what Plug’n’Play (abbreviated PnP) is. Instead of generating a node_modules directory and leaving the resolution to Node, we now generate a single .pnp.js file and let Yarn tell us where to find our packages.</p></blockquote><p>Yarn has been worked upon, amongst others, by the excellent <a href="https://twitter.com/arcanis">Maël Nison</a>. You can hear him talking about it in person <a href="https://youtu.be/XePfzVs852s">in this talk at JSConfEU</a>.</p><p>Thanks particularly to Maël&#x27;s work, it&#x27;s possible to use Yarn PnP with TypeScript using webpack with <code>ts-loader</code> <em>and</em><code>fork-ts-checker-webpack-plugin</code>. This post intends to show you just how simple it is to convert a project that uses either to work with Yarn PnP.</p><h2>Vanilla <code>ts-loader</code></h2><p>Your project is built using standalone <code>ts-loader</code>; i.e. a simple setup that handles both transpilation and type checking.</p><p>First things first, add this property to your <code>package.json</code>: (this is only required if you are using Yarn 1; this tag will be optional starting from the v2, where projects will switch to PnP by default.)</p><pre><code>{
    &quot;installConfig&quot;: {
        &quot;pnp&quot;: true
    }
}
</code></pre><p>Also, because this is webpack, we&#x27;re going to need to add an extra dependency in the form of <code>pnp-webpack-plugin</code>:</p><pre><code>yarn add -D pnp-webpack-plugin
</code></pre><p>To quote the excellent docs, make the following amends to your <code>webpack.config.js</code>:</p><pre><code>const PnpWebpackPlugin = require(`pnp-webpack-plugin`);

module.exports = {
    module: {
        rules: [{
            test: /\.ts$/,
            loader: require.resolve(&#x27;ts-loader&#x27;),
            options: PnpWebpackPlugin.tsLoaderOptions(),
        }],
    },
    resolve: {
        plugins: [ PnpWebpackPlugin, ],
    },
    resolveLoader: {
        plugins: [ PnpWebpackPlugin.moduleLoader(module), ],
    },
};
</code></pre><p>If you have any options you want to pass to <code>ts-loader</code>, just pass them as parameter of <code>pnp-webpack-plugin</code>&#x27;s <code>tsLoaderOptions</code> function and it will take care of forwarding them properly. Behind the scenes the <code>tsLoaderOptions</code> function is providing <code>ts-loader</code> with the options necessary to switch into Yarn PnP mode.</p><p>Congratulations; you now have <code>ts-loader</code> functioning with Yarn PnP support!</p><h2><code>fork-ts-checker-webpack-plugin</code> with <code>ts-loader</code></h2><p>You may well be using <code>fork-ts-checker-webpack-plugin</code> to handle type checking whilst <code>ts-loader</code> gets on with the transpilation. This workflow is also supported using <code>pnp-webpack-plugin</code>. You&#x27;ll have needed to follow the same steps as the <code>ts-loader</code> setup. It&#x27;s just the <code>webpack.config.js</code> tweaks that will be different.</p><pre><code>const PnpWebpackPlugin = require(`pnp-webpack-plugin`);

module.exports = {
    plugins: {
        new ForkTsCheckerWebpackPlugin(PnpWebpackPlugin.forkTsCheckerOptions({
            useTypescriptIncrementalApi: false, // not possible to use this until: https://github.com/microsoft/TypeScript/issues/31056
        })),
    }
    module: {
        rules: [{
            test: /\.ts$/,
            loader: require.resolve(&#x27;ts-loader&#x27;),
            options: PnpWebpackPlugin.tsLoaderOptions({ transpileOnly: true }),
        }],
    },
    resolve: {
        plugins: [ PnpWebpackPlugin, ],
    },
    resolveLoader: {
        plugins: [ PnpWebpackPlugin.moduleLoader(module), ],
    },
};
</code></pre><p>Again if you have any options you want to pass to <code>ts-loader</code>, just pass them as parameter of <code>pnp-webpack-plugin</code>&#x27;s <code>tsLoaderOptions</code> function. As we&#x27;re using <code>fork-ts-checker-webpack-plugin</code> we&#x27;re going to want to stop <code>ts-loader</code> doing type checking with the <code>transpileOnly: true</code> option.</p><p>We&#x27;re now initialising <code>fork-ts-checker-webpack-plugin</code> with <code>pnp-webpack-plugin</code>&#x27;s <code>forkTsCheckerOptions</code> function. Behind the scenes the <code>forkTsCheckerOptions</code> function is providing the <code>fork-ts-checker-webpack-plugin</code> with the options necessary to switch into Yarn PnP mode.</p><p>And that&#x27;s it! You now have <code>ts-loader</code> and <code>fork-ts-checker-webpack-plugin</code> functioning with Yarn PnP support!</p><h2>Living on the Bleeding Edge</h2><p>Whilst you can happily develop and build using Yarn PnP, it&#x27;s worth bearing in mind that this is a new approach. As such, there&#x27;s some rough edges right now.</p><p>If you&#x27;re interested in Yarn PnP, it&#x27;s worth taking the v2 of Yarn (Berry) for a spin. You can find it here: <a href="https://github.com/yarnpkg/berry">https://github.com/yarnpkg/berry</a>. It&#x27;s where most of the Yarn PnP work happens, and it includes zip loading - two birds, one stone!</p><p>Because there isn&#x27;t first class support for Yarn PnP in TypeScript itself yet, you cannot make use of the Watch API through <code>fork-ts-checker-webpack-plugin</code>. (You can read about that issue <a href="https://github.com/microsoft/TypeScript/issues/31056">here</a>)</p><p>As you&#x27;ve likely noticed, the webpack configuration required makes for a noisy <code>webpack.config.js</code>. Further to that, VS Code (which is powered by TypeScript remember) has no support for Yarn PnP yet and so will present resolution errors to you. If you can ignore the sea of red squigglies all over your source files in the editor and just look at your webpack build you&#x27;ll be fine.</p><p>There is a tool called <code>PnPify</code> that adds support for PnP to TypeScript (in particular tsc). You can find more information here: <a href="https://yarnpkg.github.io/berry/advanced/pnpify">https://yarnpkg.github.io/berry/advanced/pnpify</a>. For tsc it would be:</p><pre><code>$&gt; yarn pnpify tsc [...]
</code></pre><p>The gist is that it simulates the existence of <code>node_modules</code> by leveraging the data from the PnP file. As such it&#x27;s not a perfect fix (<code>pnp-webpack-plugin</code> is a better integration), but it&#x27;s a very useful tool to have to unblock yourself when using a project that doesn&#x27;t support it.</p><p>PnPify actually allows us to use TypeScript in VSCode with PnP! Its documentation is here: <a href="https://yarnpkg.github.io/berry/advanced/pnpify#vscode-support">https://yarnpkg.github.io/berry/advanced/pnpify#vscode-support</a></p><p>All of these hindrances should hopefully be resolved in future. Ideally, one day a good developer experience can be the default experience. In the meantime, you can still dev - just be prepared for the rough edges. Here&#x27;s some useful resources to track the future of support:</p><ul><li>You can follow more on built in webpack support here: <a href="https://github.com/webpack/enhanced-resolve/issues/162">https://github.com/webpack/enhanced-resolve/issues/162</a></li><li>And on built in TypeScript support here: <a href="https://github.com/Microsoft/TypeScript/issues/18896">https://github.com/Microsoft/TypeScript/issues/18896</a></li><li>Finally, there it&#x27;s worth watching the <a href="https://github.com/nodejs/modules">nodejs/module</a> repository, which debates amongst other things how to properly integrate loaders with Node.</li></ul><p>This last one would be nice because:</p><ul><li>We&#x27;d stop having to patch require</li><li>We probably wouldn&#x27;t have to use yarn node if Node itself was able to find the loader somehow (such as if it was listed in the package.json metadata)</li></ul><p>Thanks to Maël for his tireless work on Yarn. To my mind Maël is certainly a candidate for the hardest worker in open source. I&#x27;ve been shamelessly borrowing his excellent docs for this post - thanks for writing so excellently Maël!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[TypeScript and high CPU usage - watch don't stare!]]></title>
            <link>https://blog.johnnyreilly.com/2019/05/23/typescript-and-high-cpu-usage-watch</link>
            <guid>TypeScript and high CPU usage - watch don't stare!</guid>
            <pubDate>Thu, 23 May 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[I'm one of the maintainers of the fork-ts-checker-webpack-plugin. Hi there!]]></description>
            <content:encoded><![CDATA[<p>I&#x27;m one of the maintainers of the <a href="https://github.com/Realytics/fork-ts-checker-webpack-plugin">fork-ts-checker-webpack-plugin</a>. Hi there!</p><p>Recently, various issues have been raised against create-react-app (which uses fork-ts-checker-webpack-plugin) as well as against the plugin itself. They&#x27;ve been related to the level of CPU usage in watch mode on idle; i.e. it&#x27;s high!</p><ul><li><a href="https://github.com/Realytics/fork-ts-checker-webpack-plugin/issues/236">https://github.com/Realytics/fork-ts-checker-webpack-plugin/issues/236</a></li><li><a href="https://github.com/facebook/create-react-app/issues/6792">https://github.com/facebook/create-react-app/issues/6792</a></li></ul><h2>Why High?</h2><p>Now, under the covers, the <code>fork-ts-checker-webpack-plugin</code> uses the TypeScript watch API.</p><p>The marvellous <a href="https://github.com/NeKJ">John</a> (not me - another John) did some digging and discovered the root cause came down to the way that the TypeScript watch API watches files:</p><blockquote><p>TS uses internally the <code>fs.watch</code> and <code>fs.watchFile</code> API functions of nodejs for their watch mode. The latter function <a href="https://nodejs.org/api/fs.html#fs_fs_watchfile_filename_options_listener">is even not recommended by nodejs documentation</a> for performance reasons, and urges to use <code>fs.watch</code> instead.</p><p><strong>NodeJS doc:</strong></p><blockquote><p>Using fs.watch() is more efficient than fs.watchFile and fs.unwatchFile. fs.watch should be used instead of fs.watchFile and fs.unwatchFile when possible.</p></blockquote></blockquote><h2>&quot;there is another&quot;</h2><p>John also found that there are other file watching behaviours offered by TypeScript. What&#x27;s more, the file watching behaviour is <em>configurable with an environment variable</em>. That&#x27;s right, if an environment variable called <code>TSC_WATCHFILE</code> is set, it controls the file watching approach used. Big news!</p><p>John did some rough benchmarking of the performance of the different options that be set on his PC running linux 64 bit. Here&#x27;s how it came out:</p><table><thead><tr><th>Value</th><th>CPU usage on idle</th></tr></thead><tbody><tr><td>TS default <em>(TSC_WATCHFILE not set)</em></td><td><strong>7<!-- -->.<!-- -->4%</strong></td></tr><tr><td>UseFsEventsWithFallbackDynamicPolling</td><td>0<!-- -->.<!-- -->2%</td></tr><tr><td>UseFsEventsOnParentDirectory</td><td>0<!-- -->.<!-- -->2%</td></tr><tr><td>PriorityPollingInterval</td><td><strong>6<!-- -->.<!-- -->2%</strong></td></tr><tr><td>DynamicPriorityPolling</td><td>0<!-- -->.<!-- -->5%</td></tr><tr><td>UseFsEvents</td><td>0<!-- -->.<!-- -->2%</td></tr></tbody></table><p>As you can see, the default performs poorly. On the other hand, an option like <code>UseFsEventsWithFallbackDynamicPolling</code> is comparative greasy lightning.</p><h2>workaround!</h2><p>To get this better experience into your world now, you could just set an environment variable on your machine. However, that doesn&#x27;t scale; let&#x27;s instead look at introducing the environment variable into your project explicitly.</p><p>We&#x27;re going to do this in a cross platform way using <code>&lt;a href=&quot;https://github.com/kentcdodds/cross-env&quot;&gt;cross-env&lt;/a&gt;</code>. This is a mighty useful utility by Kent C Dodds which allows you to set environment variables in a way that will work on Windows, Mac and Linux. Imagine it as the jQuery of the environment variables world :-)</p><p>Let&#x27;s add it as a <code>devDependency</code>:</p><pre><code>yarn add -D cross-env
</code></pre><p>Then take a look at your <code>package.json</code>. You&#x27;ve probably got a <code>start</code> script that looks something like this:</p><pre><code>&quot;start&quot;: &quot;webpack-dev-server --progress --color --mode development --config webpack.config.development.js&quot;,
</code></pre><p>Or if you&#x27;re a create-react-app user maybe this:</p><pre><code>&quot;start&quot;: &quot;react-scripts start&quot;,
</code></pre><p>Prefix your <code>start</code> script with <code>cross-env TSC_WATCHFILE=UseFsEventsWithFallbackDynamicPolling</code>. This will, when run, initialise an environment variable called <code>TSC_WATCHFILE</code> with the value <code>UseFsEventsWithFallbackDynamicPolling</code>. Then it will start your development server as it did before. When TypeScript is fired up by webpack it will see this environment variable and use it to configure the file watching behaviour to one of the more performant options.</p><p>So, in the case of a <code>create-react-app</code> user, your finished <code>start</code> script would look like this:</p><pre><code>&quot;start&quot;: &quot;cross-env TSC_WATCHFILE=UseFsEventsWithFallbackDynamicPolling react-scripts start&quot;,
</code></pre><h2>The Future</h2><p>There&#x27;s a possibility that the default watch behaviour may change in TypeScript in future. It&#x27;s currently under discussion, you can read more <a href="https://github.com/microsoft/TypeScript/issues/31048">here</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[react-select with less typing lag]]></title>
            <link>https://blog.johnnyreilly.com/2019/04/27/react-select-with-less-typing-lag</link>
            <guid>react-select with less typing lag</guid>
            <pubDate>Sat, 27 Apr 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[This is going out to all those people using react-select with 1000+ items to render. To those people typing into the select and saying out loud "it's so laggy.... This can't be... It's 2019... I mean, right?" To the people who read this GitHub issue top to bottom 30 times and still came back unsure of what to do. This is for you.]]></description>
            <content:encoded><![CDATA[<p>This is going out to all those people using <a href="https://react-select.com"><code>react-select</code></a> with 1000+ items to render. To those people typing into the select and saying out loud &quot;it&#x27;s <em>so</em> laggy.... This can&#x27;t be... It&#x27;s 2019... I mean, right?&quot; To the people who read this <a href="https://github.com/JedWatson/react-select/issues/3128">GitHub issue</a> top to bottom 30 times and still came back unsure of what to do. This is for you.</p><p>I&#x27;m lying. Mostly this goes out to me. I have a select box. I need it to render 2000+ items. I want it to be lovely. I want my users to be delighted as they use it. I want them to type in and (<em>this is the crucial part!</em>) for the control to feel responsive. Not laggy. Not like each keypress is going to Jupiter and back before it renders to the screen.</p><p>Amongst the various gems on the GitHub issue are shared CodeSandboxes illustrating ways to integrate react-select with react-window. That&#x27;s great and they do improve things. However, they don&#x27;t do much to improve the laggy typing feel. There&#x27;s <a href="https://github.com/JedWatson/react-select/issues/3128#issuecomment-431397942">brief mention</a> of a props tweak you can make to react-select; this:</p><pre><code class="language-js">filterOption={createFilter({ ignoreAccents: false })}
</code></pre><p>What does this do? Well, this improves the typing lag experience <em>massively</em>. For why? Well, <a href="https://github.com/JedWatson/react-select/blob/292bad3298f2cafad6767f2134bd79a9c27e4073/src/filters.js#L21">if you look at the code</a> you find that the default value is <code>ignoreAccents: true</code>. This default makes react-select invoke an expensive (and scary sounding) function called <a href="https://github.com/JedWatson/react-select/blob/292bad3298f2cafad6767f2134bd79a9c27e4073/src/diacritics.js#L90"><code>stripDiacritics</code></a>. Not once but twice. Ouchy. And this kills performance.</p><p>But if you&#x27;re okay with accents not being ignored (and <em>spoiler</em>: I am) then this is the option for you.</p><p>Here&#x27;s a CodeSandbox which also includes the <code>ignoreAccents: false</code> tweak. Enjoy!</p><p><a href="https://codesandbox.io/s/zn70lqp31m?fontsize=14"><img src="https://codesandbox.io/static/img/play-codesandbox.svg" alt="Edit johnnyreilly/react-window-with-react-select-less-laggy"/></a></p><pre><code class="language-js">import React, { Component } from &#x27;react&#x27;;
import ReactDOM from &#x27;react-dom&#x27;;
import Select, { createFilter } from &#x27;react-select&#x27;;
import { FixedSizeList as List } from &#x27;react-window&#x27;;

import &#x27;./styles.css&#x27;;

const options = [];
for (let i = 0; i &lt; 2500; i = i + 1) {
  options.push({ value: i, label: `Option ${i}` });
}

const height = 35;

class MenuList extends Component {
  render() {
    const { options, children, maxHeight, getValue } = this.props;
    const [value] = getValue();
    const initialOffset = options.indexOf(value) * height;

    return (
      &lt;List
        height={maxHeight}
        itemCount={children.length}
        itemSize={height}
        initialScrollOffset={initialOffset}
      &gt;
        {({ index, style }) =&gt; &lt;div style={style}&gt;{children[index]}&lt;/div&gt;}
      &lt;/List&gt;
    );
  }
}

const App = () =&gt; (
  &lt;Select
    filterOption={createFilter({ ignoreAccents: false })} // this makes all the difference!
    components={{ MenuList }}
    options={options}
  /&gt;
);

ReactDOM.render(&lt;App /&gt;, document.getElementById(&#x27;root&#x27;));
</code></pre>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Template Tricks for a Dainty DOM]]></title>
            <link>https://blog.johnnyreilly.com/2019/03/24/template-tricks-for-dainty-dom</link>
            <guid>Template Tricks for a Dainty DOM</guid>
            <pubDate>Sun, 24 Mar 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[I'm somewhat into code golf. Placing restrictions on what you're "allowed" to do in code and seeing what the happens as a result. I'd like to share with you something that came out of some recent dabblings.]]></description>
            <content:encoded><![CDATA[<p>I&#x27;m somewhat into code golf. Placing restrictions on what you&#x27;re &quot;allowed&quot; to do in code and seeing what the happens as a result. I&#x27;d like to share with you something that came out of some recent dabblings.</p><p>Typically I spend a good amount of time playing with TypeScript. Either working on build tools or making web apps with it. (Usually with a portion of React on the side.) This is something different.</p><p>I have a side project on the go which is essentially a mini analytics dashboard. For the purposes of this piece let&#x27;s call it &quot;StatsDash&quot;. When I was starting it I thought: let&#x27;s try something different. Let&#x27;s build StatsDash with HTML <em>only</em>. The actual HTML is hand cranked by me and generated in ASP.Net Core / C# using a combination of LINQ and string interpolation. (Who needs Razor? 😎) I&#x27;ll say it&#x27;s pretty fun - but the back end is not what I want to focus on.</p><p>I got something up and running pretty quickly in pure HTML. The first lesson I learned was this: HTML alone is hella ugly. So I relaxed my criteria; I allowed CSS to come play as long as I didn&#x27;t have to write any / much myself. There followed some experimentation with different CSS frameworks. For a while I rolled with Bootstrap (old school!), then Bulma and finally I settled on <a href="https://materializecss.com/">Materialized</a>. Materialized is a heavily inspired by Google&#x27;s Material Design and is hence quite beautiful. With my HTML and Materialize&#x27;s CSS we were rolling. Beautiful stats - no JS.</p><h2>&quot;Oh All Right; Just a Splash&quot;</h2><p>Lovely as things were, StatsDash quickly got to the point where there was too much information on the screen. It was time to make some changes. If data is to convey a message, it must first be comprehensible.</p><p>I needed a way to hide and show data as people interacted with StatsDash. I wanted to achieve this <em>without</em> starting to render on the client side and also without going back to the server each time.</p><p>If you want interactions in your UI all roads lead to JS. It&#x27;s certainly possible to do some tricks with CSS but that&#x27;s a round of code golf I&#x27;m ill equipped to play. So, I took a look at what Materialized had to offer. Usefully it has a <a href="https://materializecss.com/modals.html">Modal</a> component. With that in play I&#x27;d be able to separate the detailed information into different modals which the users could show and hide as required. Perfect!</p><p>It required a little JS. What&#x27;s a line or two between friends? Dear reader, I compromised once more.</p><h2>The DOM Bunker</h2><p>With my handy modals, StatsDash was now a one stop shop for a great deal of information. Info which took the form of DOM nodes. Lots of them. And by &quot;lots of them&quot; I want you to think along the lines of &quot;space is big, really big...&quot;.</p><p>This was impacting users. Clicking to open a modal resulted in a noticeable lag. It would take 2+ seconds for the browser to respond. Users found themselves clicking multiple times; wondering why nothing seemed to occur. In the end the modal would shuffle into view. However, this wasn&#x27;t the best experience. The lack of responsiveness was getting in the way of users enjoying all StatsDash had to offer.</p><p>Running an audit of StatsDash in Chrome DevTools there was no doubt we had a DOM problem:</p><p><img src="../static/blog/2019-03-24-template-tricks-for-dainty-dom/DOM-massive.png"/></p><p>What to do? I still didn&#x27;t want to go back to the server on each click in StatsDash. And I didn&#x27;t want to start writing rendering code on the client as well either. I have in the past mixed client and server side rendering and I know well that it&#x27;s a first class ticket to a confusing codebase.</p><h2>Smuggling DOM in Templates</h2><p>There&#x27;s a mechanism that supports this use case directly: the <code>&amp;lt;template&amp;gt;</code> element. <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/template">To quote MDN</a>:</p><blockquote><p>The HTML Content Template (<code>&amp;lt;template&amp;gt;</code>) element is a mechanism for holding client-side content that is not to be rendered when a page is loaded but may subsequently be instantiated during runtime using JavaScript.</p></blockquote><blockquote><p>Think of a template as a content fragment that is being stored for subsequent use in the document.</p></blockquote><p>This is <em>exactly</em> what I&#x27;m after. I can keep my rendering server side, but instead wrap content that isn&#x27;t immediately visible to users inside a <code>&amp;lt;template&amp;gt;</code> element and render that only when users need it.</p><p>So in the case of my modals (where most of my DOM lives), I can tuck the contents of each modal into a <code>&amp;lt;template&amp;gt;</code> element. Then, when the user clicks to open a modal we move that template content into the DOM so they can see it. Likewise, as they close a modal we can clear out the modal&#x27;s DOM content to ease the load on the dear old browser.</p><h2>&quot;That Sounds Complicated...&quot;</h2><p>It&#x27;s not. Let me show you how easily this is accomplished. First of all, wrap all your modal contents into <code>&amp;lt;template&amp;gt;</code> elements. They should look a little something like this:</p><pre><code class="language-html">&lt;div&gt;
  &lt;button data-target=&quot;modalId&quot; class=&quot;btn modal-trigger&quot;&gt;
    Open the Modal!
  &lt;/button&gt;

  &lt;template&gt;
    &lt;!--
        loads of DOM nodes
        --&gt;
  &lt;/template&gt;

  &lt;div id=&quot;modalId&quot; class=&quot;modal modal-fixed-footer&quot;&gt;&lt;/div&gt;
&lt;/div&gt;
</code></pre><p>Next, where you initialise your modals you need to make a little tweak:</p><pre><code class="language-js">document.addEventListener(&#x27;DOMContentLoaded&#x27;, function () {
  M.Modal.init(document.querySelectorAll(&#x27;.modal&#x27;), {
    onOpenStart: (modalDiv) =&gt; {
      const template = modalDiv.parentNode.querySelector(&#x27;template&#x27;);

      modalDiv.appendChild(document.importNode(template.content, true));
    },
    onCloseEnd: (modalDiv) =&gt; {
      while (modalDiv.firstChild) {
        modalDiv.removeChild(modalDiv.firstChild);
      }
    },
  });
});
</code></pre><p>That&#x27;s it! As you can see, before we open our modals, the <code>onOpenStart</code> callback will fire which creates the actual DOM elements based upon the <code>template</code>. And when the modals finish closing the <code>onCloseEnd</code> callback runs to remove those DOM elements once more.</p><p>For this minimal change, the client gets a dramatically different user experience. StatsDash went from super laggy to satisfyingly fast. Using <code>template</code>s, The number of initial DOM nodes dropped from more than <em>20,000</em> to <em>200</em>. That&#x27;s right 💯 times smaller!</p><h2>Do It Yourself</h2><p>The code examples above rely upon the Materialize modals. However the principles used here are broadly applicable. It&#x27;s easy for you to take the approach outlined here and apply it in a different situation.</p><p>If you&#x27;re interested in some of the other exciting things you can do with templates then I recommend <a href="https://www.html5rocks.com/en/tutorials/webcomponents/template/">Eric Bidelman&#x27;s post on the topic</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google Analytics API and ASP.Net Core]]></title>
            <link>https://blog.johnnyreilly.com/2019/03/22/google-analytics-api-and-aspnet-core</link>
            <guid>Google Analytics API and ASP.Net Core</guid>
            <pubDate>Fri, 22 Mar 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[I recently had need to be able to access the API for Google Analytics from ASP.Net Core. Getting this up and running turned out to be surprisingly tough because of an absence of good examples. So here it is; an example of how you can access a simple page access stat using the API:]]></description>
            <content:encoded><![CDATA[<p>I recently had need to be able to access the API for Google Analytics from ASP.Net Core. Getting this up and running turned out to be surprisingly tough because of an absence of good examples. So here it is; an example of how you can access a simple page access stat using <a href="https://www.nuget.org/packages/Google.Apis.AnalyticsReporting.v4/">the API</a>:</p><pre><code class="language-cs">async Task&lt;SomeKindOfDataStructure[]&gt; GetUsageFromGoogleAnalytics(DateTime startAtThisDate, DateTime endAtThisDate)
{
    // Create the DateRange object. Here we want data from last week.
    var dateRange = new DateRange
    {
        StartDate = startAtThisDate.ToString(&quot;yyyy-MM-dd&quot;),
        EndDate = endAtThisDate.ToString(&quot;yyyy-MM-dd&quot;)
    };
    // Create the Metrics and dimensions object.
    // var metrics = new List&lt;Metric&gt; { new Metric { Expression = &quot;ga:sessions&quot;, Alias = &quot;Sessions&quot; } };
    // var dimensions = new List&lt;Dimension&gt; { new Dimension { Name = &quot;ga:pageTitle&quot; } };
    var metrics = new List&lt;Metric&gt; { new Metric { Expression = &quot;ga:uniquePageviews&quot; } };
    var dimensions = new List&lt;Dimension&gt; {
        new Dimension { Name = &quot;ga:date&quot; },
        new Dimension { Name = &quot;ga:dimension1&quot; }
    };

    // Get required View Id from configuration
    var viewId = $&quot;ga:{&quot;[VIEWID]&quot;}&quot;;

    // Create the Request object.
    var reportRequest = new ReportRequest
    {
        DateRanges = new List&lt;DateRange&gt; { dateRange },
        Metrics = metrics,
        Dimensions = dimensions,
        FiltersExpression = &quot;ga:pagePath==/index.html&quot;,
        ViewId = viewId
    };

    var getReportsRequest = new GetReportsRequest {
        ReportRequests = new List&lt;ReportRequest&gt; { reportRequest }
    };

    //Invoke Google Analytics API call and get report
    var analyticsService = GetAnalyticsReportingServiceInstance();
    var response = await (analyticsService.Reports.BatchGet(getReportsRequest)).ExecuteAsync();

    var logins = response.Reports[0].Data.Rows.Select(row =&gt; new SomeKindOfDataStructure {
        Date = new DateTime(
            year: Convert.ToInt32(row.Dimensions[0].Substring(0, 4)),
            month: Convert.ToInt32(row.Dimensions[0].Substring(4, 2)),
            day: Convert.ToInt32(row.Dimensions[0].Substring(6, 2))),
        NumberOfLogins = Convert.ToInt32(row.Metrics[0].Values[0])
    })
    .OrderByDescending(login =&gt; login.Date)
    .ToArray();

    return logins;
}

/// &lt;summary&gt;
/// Intializes and returns Analytics Reporting Service Instance
/// &lt;/summary&gt;
AnalyticsReportingService GetAnalyticsReportingServiceInstance() {
    var googleAuthFlow = new GoogleAuthorizationCodeFlow(new GoogleAuthorizationCodeFlow.Initializer {
        ClientSecrets = new ClientSecrets {
            ClientId = &quot;[CLIENTID]&quot;,
            ClientSecret = &quot;[CLIENTSECRET]&quot;
        }
    });

    var responseToken = new TokenResponse {
        AccessToken = &quot;[ANALYTICSTOKEN]&quot;,
        RefreshToken = &quot;[REFRESHTOKEN]&quot;,
        Scope = AnalyticsReportingService.Scope.AnalyticsReadonly, //Read-only access to Google Analytics,
        TokenType = &quot;Bearer&quot;,
    };

    var credential = new UserCredential(googleAuthFlow, &quot;&quot;, responseToken);

    // Create the  Analytics service.
    return new AnalyticsReportingService(new BaseClientService.Initializer {
        HttpClientInitializer = credential,
        ApplicationName = &quot;my-super-applicatio&quot;,
    });
}
</code></pre><p>You can see above that you need various credentials to be able to use the API. You can acquire these by logging into GA. Enjoy!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[fork-ts-checker-webpack-plugin v1.0]]></title>
            <link>https://blog.johnnyreilly.com/2019/03/06/fork-ts-checker-webpack-plugin-v1</link>
            <guid>fork-ts-checker-webpack-plugin v1.0</guid>
            <pubDate>Wed, 06 Mar 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[It's time for the first major version of fork-ts-checker-webpack-plugin. It's been a long time coming :-)]]></description>
            <content:encoded><![CDATA[<p><a href="https://github.com/Realytics/fork-ts-checker-webpack-plugin/releases/tag/v1.0.0">It&#x27;s time for the first major version of <code>fork-ts-checker-webpack-plugin</code></a>. It&#x27;s been a long time coming :-)</p><h2>A Little History</h2><p>The <code>fork-ts-checker-webpack-plugin</code> was originally the handiwork of <a href="https://github.com/piotr-oles">Piotr Oleś</a>. He raised an issue with <a href="https://github.com/TypeStrong/ts-loader/issues/537"><code>ts-loader</code></a> suggesting it could be the McCartney to <code>ts-loader</code>&#x27;s Lennon:</p><blockquote><p>Hi everyone!</p><p>I&#x27;ve created webpack plugin: <a href="https://github.com/Realytics/fork-ts-checker-webpack-plugin">fork-ts-checker-webpack-plugin</a> that plays nicely with <code>ts-loader</code>. The idea is to compile project with <code>transpileOnly: true</code> and check types on separate process (async). With this approach, webpack build is not blocked by type checker and we have semantic check with fast incremental build. More info on github repo :)</p><p>So if you like it and you think it would be good to add some info in README.md about this plugin, I would be greatful.</p><p>Thanks :)</p></blockquote><p>We did like it. We did think it would be good. We took him up on his kind offer.</p><p>Since that time many people have had their paws on the <code>fork-ts-checker-webpack-plugin</code> codebase. We love them all.</p><h2>One Point Oh</h2><p>We could have had our first major release a long time ago. The idea first occurred when webpack 5 alpha appeared. &quot;Huh, look at that, a major version number.... Maybe we should do that?&quot; &quot;<em>Great</em> idea chap - do it!&quot; So here it is; fresh out the box: v1.0.0</p><p>There are actually no breaking changes that we&#x27;re aware of; users of 0.x <code>fork-ts-checker-webpack-plugin</code> should be be able to upgrade without any drama.</p><h2>Incremental Watch API on by Default</h2><p>Users of TypeScript 3+ may notice a performance improvement as by default the plugin now uses the <a href="https://github.com/Microsoft/TypeScript/pull/20234">incremental watch API</a> in TypeScript.</p><p>Should this prove problematic you can opt out of using it by supplying <code>useTypescriptIncrementalApi: false</code>. We are aware of an <a href="https://github.com/Realytics/fork-ts-checker-webpack-plugin/issues/219">issue with Vue and the incremental API</a>. We hope it will be fixed soon - a generous member of the community is taking a look. In the meantime, we will <em>not</em> default to using the incremental watch API when in Vue mode.</p><h2>Compatibility</h2><p>As it stands, the plugin supports webpack 2, 3, 4 and 5 alpha. It is compatible with TypeScript 2.1+ and TSLint 4+.</p><p>Right that&#x27;s it - enjoy it! And thanks everyone for contributing - we really dig your help. Much love.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ASP.NET Core: Proxying HTTP Requests with an AllowList]]></title>
            <link>https://blog.johnnyreilly.com/2019/02/22/aspnet-core-allowlist-proxying-http-requests</link>
            <guid>ASP.NET Core: Proxying HTTP Requests with an AllowList</guid>
            <pubDate>Fri, 22 Feb 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[This post demonstrates a mechanism for proxying HTTP requests in ASP.NET Core. It doesn't proxy all requests; it only proxies requests that match entries on an "allowlist" - so we only proxy the traffic that we've actively decided is acceptable as determined by taking the form of an expected URL and HTTP verb (GET / POST etc).]]></description>
            <content:encoded><![CDATA[<p>This post demonstrates a mechanism for proxying HTTP requests in ASP.NET Core. It doesn&#x27;t proxy all requests; it only proxies requests that match entries on an &quot;allowlist&quot; - so we only proxy the traffic that we&#x27;ve actively decided is acceptable as determined by taking the form of an expected URL and HTTP verb (GET / POST etc).</p><h2>Why do we need to proxy?</h2><p>Once upon a time there lived a young team who were building a product. They were ready to go live with their beta and so they set off on a journey to a mystical land they had heard tales of. This magical kingdom was called &quot;Production&quot;. However, Production was a land with walls and but one gate. That gate was jealously guarded by a defender named &quot;InfoSec&quot;. InfoSec was there to make sure that only the the right people, noble of thought and pure of deed were allowed into the promised land. InfoSec would ask questions like &quot;are you serving over HTTPS&quot; and &quot;what are you doing about cross site scripting&quot;?</p><p>The team felt they had good answers to InfoSec&#x27;s questions. However, just as they were about to step through the gate, InfoSec held up their hand and said &quot;your application wants to access a database... database access needs to take place on our own internal network. Not over the publicly accessible internet.&quot;</p><p>The team, with one foot in the air, paused. They swallowed and said &quot;can you give us five minutes?&quot;</p><p><img src="../static/blog/2019-02-22-aspnet-core-allowlist-proxying-http-requests/hang-on-lads-ive-got-a-great-idea.jpg" alt="image taken from the end of the classic movie &quot;The Italian Job&quot; of the bus hanging half off a mountainside"/></p><h2>The Proxy Regroup</h2><p>And so it came to pass that the teams product (which took the form of ASP.Net Core web application) had to be changed. Where once there had been a single application, there would now be two; one that lived on the internet (the <em>web</em> app) and one that lived on the companies private network (the <em>API</em> app). The API app would do all the database access. In fact the product team opted to move all significant operations into the API as well. This left the web app with two purposes:</p><ol><li>the straightforward serving of HTML, CSS, JS and images</li><li>the proxying of API calls through to the API app</li></ol><h2>Proxy Part 1</h2><p>In the early days of this proxying the team reached for <a href="https://github.com/twitchax/AspNetCore.Proxy"><code>AspNetCore.Proxy</code></a>. It&#x27;s a great open source project that allows you to proxy HTTP requests. It gives you complete control over the construction of proxy requests, so that you can have a request come into your API and end up proxying it to a URL with a completely different path on the proxy server.</p><h2>Proxy Part 2</h2><p>The approach offered by <code>AspNetCore.Proxy</code> is fantastically powerful in terms of control. However, we didn&#x27;t actually need that level of configurability. In fact, it resulted in us writing a great deal of boilerplate code. You see in our case we&#x27;d opted to proxy path for path, changing only the server name on each proxied request. So if a GET request came in going to <a href="https://web.app.com/api/version">https://web.app.com/api/version</a> then we would want to proxy it to a GET request to <a href="https://api.app.com/api/version">https://api.app.com/api/version</a>. You see? All we did was swap <a href="https://web.app.com">https://web.app.com</a> for <a href="https://api.app.com.">https://api.app.com.</a> Nothing more. We did that as a rule. We knew we <em>always</em> wanted to do just this.</p><p>So we ended up spinning up our own solution which allowed just the specification of paths we wanted to proxy with their corresponding HTTP verbs. Let&#x27;s talk through it. Usage of our approach ended up as a middleware within our web app&#x27;s <code>Startup.cs</code>:</p><pre><code class="language-cs">public void Configure(IApplicationBuilder app) {
    // ...

    app.UseProxyAllowList(
        // where ServerToProxyToBaseUrl is the server you want requests to be proxied to
        // eg &quot;https://the-server-we-proxy-to&quot;
        proxyAddressTweaker: (requestPath) =&gt; $&quot;{ServerToProxyToBaseUrl}{requestPath}&quot;,
        allowListProxyRoutes: new [] {
            // An anonymous request
            AllowListProxy.AnonymousRoute(&quot;api/version&quot;, HttpMethod.Get),

            // An authenticated request; to send this we must know who the user is
            AllowListProxy.Route(&quot;api/account/{accountId:int}/all-the-secret-info&quot;, HttpMethod.Get, HttpMethod.Post),
    });


    app.UseMvc();

    // ...
}
</code></pre><p>If you look at the code above you can see that we are proxing requests to a single server: <code>ServerToProxyToBaseUrl</code>. We&#x27;re also only proxying requests which match an entry on our allowlist (as represented by <code>allowListProxyRoutes</code>). So in this case we&#x27;re proxying two different requests:</p><ol><li><code>GET</code> requests to <code>api/version</code> are proxied through as <em>anonymous</em><code>GET</code> requests.</li><li><code>GET</code> and <code>POST</code> requests to <code>api/account/{accountId:int}/all-the-secret-info</code> are proxied through as <code>GET</code> and <code>POST</code> requests. These requests require that a user be authenticated first.</li></ol><p>The <code>AllowListProxy</code> proxy class we&#x27;ve been using looks like this:</p><pre><code class="language-cs">using System;
using System.Collections.Generic;
using System.Net.Http;

namespace My.Web.Proxy {
    public class AllowListProxy {
        public string Path { get; set; }
        public IEnumerable&lt;HttpMethod&gt; Methods { get; set; }
        public bool IsAnonymous { get; set; }

        private AllowListProxy(string path, bool isAnonymous, params HttpMethod[] methods) {
            if (methods == null || methods.Length == 0)
                throw new ArgumentException($&quot;You need at least a single HttpMethod to be specified for {path}&quot;);

            Path = path;
            IsAnonymous = isAnonymous;
            Methods = methods;
        }

        public static AllowListProxy Route(string path, params HttpMethod[] methods) =&gt;
            new AllowListProxy(path, isAnonymous: false, methods: methods);

        public static AllowListProxy AnonymousRoute(string path, params HttpMethod[] methods) =&gt;
            new AllowListProxy(path, isAnonymous: true, methods: methods);
    }
}
</code></pre><p>The middleware for proxying (our <code>UseProxyAllowList</code>) looks like this:</p><pre><code class="language-cs">using System;
using System.Collections.Generic;
using System.ComponentModel;
using System.Linq;
using System.Net.Http;
using System.Reflection;
using System.Threading.Tasks;
using Microsoft.AspNetCore.Authentication;
using Microsoft.AspNetCore.Builder;
using Microsoft.AspNetCore.Http;
using Microsoft.AspNetCore.Routing;
using Microsoft.Extensions.DependencyModel;
using Microsoft.Extensions.DependencyInjection;
using Serilog;

namespace My.Web.Proxy {
    public static class ProxyRouteExtensions {
        /// &lt;summary&gt;
        /// Middleware which proxies the supplied allowlist routes
        /// &lt;/summary&gt;
        public static void UseProxyAllowList(
            this IApplicationBuilder app,
            Func&lt;string, string&gt; proxyAddressTweaker,
            Action&lt;HttpContext, HttpRequestMessage&gt; preSendProxyRequestAction,
            IEnumerable&lt;AllowListProxy&gt; allowListProxyRoutes
        ) {
            app.UseRouter(builder =&gt; {
                foreach (var allowListProxy in allowListProxyRoutes) {
                    foreach (var method in allowListProxy.Methods) {
                        builder.MapMiddlewareVerb(method.ToString(), allowListProxy.Path, proxyApp =&gt; {
                            proxyApp.UseProxy_Challenge(allowListProxy.IsAnonymous);
                            proxyApp.UseProxy_Run(proxyAddressTweaker, preSendProxyRequestAction);
                        });
                    }
                }
            });
        }

        private static void UseProxy_Challenge(this IApplicationBuilder app, bool allowAnonymous) {
            app.Use((context, next) =&gt;
            {
                var routePath = context.Request.Path.Value;

                var weAreAuthenticatedOrWeDontNeedToBe =
                    context.User.Identity.IsAuthenticated || allowAnonymous;
                if (weAreAuthenticatedOrWeDontNeedToBe)
                    return next();

                return context.ChallengeAsync();
            });
        }

        private static void UseProxy_Run(
            this IApplicationBuilder app,
            Func&lt;string, string&gt; proxyAddressTweaker,
            Action&lt;HttpContext, HttpRequestMessage&gt; preSendProxyRequestAction
            )
        {
            app.Run(async context =&gt; {
                var proxyAddress = &quot;&quot;;
                try {
                    proxyAddress = proxyAddressTweaker(context.Request.Path.Value);

                    var proxyRequest = context.Request.CreateProxyHttpRequest(proxyAddress);

                    if (preSendProxyRequestAction != null)
                        preSendProxyRequestAction(context, proxyRequest);

                    var httpClients = context.RequestServices.GetService&lt;IHttpClients&gt;(); // IHttpClients is just a wrapper for HttpClient - insert your own here

                    var proxyResponse = await httpClients.SendRequestAsync(proxyRequest,
                            HttpCompletionOption.ResponseHeadersRead, context.RequestAborted)
                        .ConfigureAwait(false);

                    await context.CopyProxyHttpResponse(proxyResponse).ConfigureAwait(false);
                }
                catch (OperationCanceledException ex) {
                    if (ex.CancellationToken.IsCancellationRequested)
                        return;

                    if (!context.Response.HasStarted)
                    {
                        context.Response.StatusCode = 408;
                        await context.Response
                            .WriteAsync(&quot;Request timed out.&quot;);
                    }
                }
                catch (Exception e) {
                    if (!context.Response.HasStarted)
                    {
                        context.Response.StatusCode = 500;
                        await context.Response
                            .WriteAsync(
                                $&quot;Request could not be proxied.\n\n{e.Message}\n\n{e.StackTrace}.&quot;);
                    }
                }
            });
        }

        public static void AddOrReplaceHeader(this HttpRequestMessage request, string headerName, string headerValue) {
            // It&#x27;s possible for there to be multiple headers with the same name; we only want a single header to remain.  Our one.
            while (request.Headers.TryGetValues(headerName, out var existingAuthorizationHeader)) {
                request.Headers.Remove(headerName);
            }
            request.Headers.TryAddWithoutValidation(headerName, headerValue);
        }

        public static HttpRequestMessage CreateProxyHttpRequest(this HttpRequest request, string uriString) {
            var uri = new Uri(uriString + request.QueryString);

            var requestMessage = new HttpRequestMessage();
            var requestMethod = request.Method;
            if (!HttpMethods.IsGet(requestMethod) &amp;&amp;
                !HttpMethods.IsHead(requestMethod) &amp;&amp;
                !HttpMethods.IsDelete(requestMethod) &amp;&amp;
                !HttpMethods.IsTrace(requestMethod)) {
                var streamContent = new StreamContent(request.Body);
                requestMessage.Content = streamContent;
            }

            // Copy the request headers.
            if (requestMessage.Content != null)
                foreach (var header in request.Headers)
                    if (!requestMessage.Headers.TryAddWithoutValidation(header.Key, header.Value.ToArray()))
                        requestMessage.Content?.Headers.TryAddWithoutValidation(header.Key, header.Value.ToArray());

            requestMessage.Headers.Host = uri.Authority;
            requestMessage.RequestUri = uri;
            requestMessage.Method = new HttpMethod(request.Method);

            return requestMessage;
        }

        public static async Task CopyProxyHttpResponse(this HttpContext context, HttpResponseMessage responseMessage) {
            var response = context.Response;

            response.StatusCode = (int) responseMessage.StatusCode;
            foreach (var header in responseMessage.Headers) {
                response.Headers[header.Key] = header.Value.ToArray();
            }

            if (responseMessage.Content != null) {
                foreach (var header in responseMessage.Content.Headers) {
                    response.Headers[header.Key] = header.Value.ToArray();
                }
            }

            response.Headers.Remove(&quot;transfer-encoding&quot;);

            using(var responseStream = await responseMessage.Content.ReadAsStreamAsync().ConfigureAwait(false)) {
                await responseStream.CopyToAsync(response.Body, 81920, context.RequestAborted).ConfigureAwait(false);
            }
        }
    }
}
</code></pre><p>This works out to be a flexible and simple approach to allowlist proxying.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[TypeScript and webpack: Watch It]]></title>
            <link>https://blog.johnnyreilly.com/2019/01/13/typescript-and-webpack-watch-it</link>
            <guid>TypeScript and webpack: Watch It</guid>
            <pubDate>Sun, 13 Jan 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[All I ask for is a compiler and a tight feedback loop. Narrowing the gap between making a change to a program and seeing the effect of that is a productivity boon. The TypeScript team are wise cats and dig this. They've taken strides to improve the developer experience of TypeScript users by introducing a "watch" API which can be leveraged by other tools. To quote the docs:]]></description>
            <content:encoded><![CDATA[<p>All I ask for is a compiler and a tight feedback loop. Narrowing the gap between making a change to a program and seeing the effect of that is a productivity boon. The TypeScript team are wise cats and dig this. They&#x27;ve taken strides to improve the developer experience of TypeScript users by <a href="https://github.com/Microsoft/TypeScript/wiki/Using-the-Compiler-API#writing-an-incremental-program-watcher">introducing a &quot;watch&quot; API which can be leveraged by other tools</a>. To quote the docs:</p><blockquote><p>TypeScript 2.7 introduces two new APIs: one for creating &quot;watcher&quot; programs that provide set of APIs to trigger rebuilds, and a &quot;builder&quot; API that watchers can take advantage of... This can speed up large projects with many files.</p></blockquote><p>Recently the wonderful <a href="https://github.com/0xorial">0xorial</a> <a href="https://github.com/Realytics/fork-ts-checker-webpack-plugin/pull/198">opened a PR to add support for the watch API</a> to the <a href="https://github.com/Realytics/fork-ts-checker-webpack-plugin"><code>fork-ts-checker-webpack-plugin</code></a>.</p><p>I took this PR for a spin on a large project that I work on. With my machine, I was averaging 12 seconds between incremental builds. (I will charitably describe the machine in question as &quot;challenged&quot;; hobbled by one of the most aggressive virus checkers known to mankind. Fist bump InfoSec 🤜🤛😉) Switching to using the watch API dropped this to a mere 1.5 seconds!</p><h2>You Can Watch Too</h2><p>0xorial&#x27;s PR was merged toot suite and was been released as <a href="https://github.com/Realytics/fork-ts-checker-webpack-plugin/releases/tag/v1.0.0-alpha.2"><code>fork-ts-checker-webpack-plugin@1.0.0-alpha.2</code></a>. If you&#x27;d like to take this for a spin then you can. Just:</p><ol><li>Up your version of the plugin to <code>fork-ts-checker-webpack-plugin@next</code> in your <code>package.json</code></li><li>Add <code>useTypescriptIncrementalApi: true</code> to the plugin when you initialise it in your <code>webpack.config.js</code>.</li></ol><p>That&#x27;s it.</p><h2>Mary Poppins</h2><p>Sorry, I was trying to paint a word picture of something you might watch that was also comforting. Didn&#x27;t quite work...</p><p>Anyway, you might be thinking &quot;wait, just hold on a minute.... he said <code>@next</code> <!-- -->-<!-- --> I am <em>not</em> that bleeding edge.&quot; Well, it&#x27;s not like that. Don&#x27;t be scared.</p><p><code>fork-ts-checker-webpack-plugin</code> has merely been updated for webpack 5 (which is in alpha) and the <code>@next</code> reflects that. To be clear, the <code>@next</code> version of the plugin still supports (remarkably!) webpack 2, 3 and 4 as well as 5 alpha. Users of current and historic versions of webpack should feel safe using the <code>@next</code> version; for webpack 2, 3 and 4 expect stability. webpack 5 users should expect potential changes to align with webpack 5 as it progresses.</p><h2>Roadmap</h2><p>This is available now and we&#x27;d love for you to try it out. As you can see, at the moment it&#x27;s opt-in. You have to explicitly choose to use the new behaviour. Depending upon how testing goes, we may look to make this the default behaviour for the plugin in future (assuming users are running a high enough version of TypeScript). It would be great to hear from people if they have any views on that, or feedback in general.</p><p>Much ❤️ y&#x27;all. And many thanks to the very excellent <a href="https://github.com/0xorial">0xorial</a> for the hard work.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[GitHub Actions and Yarn]]></title>
            <link>https://blog.johnnyreilly.com/2019/01/05/github-actions-and-yarn</link>
            <guid>GitHub Actions and Yarn</guid>
            <pubDate>Sat, 05 Jan 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[I'd been meaning to automate the npm publishing of ts-loader for the longest time. I had attempted to use Travis to do this in the same way as fork-ts-checker-webpack-plugin. Alas using secure environment variables in Travis has unfortunate implications for ts-loader's test pack.]]></description>
            <content:encoded><![CDATA[<p>I&#x27;d been meaning to automate the npm publishing of <a href="https://github.com/TypeStrong/ts-loader"><code>ts-loader</code></a> for the longest time. I had attempted to use Travis to do this in the same way as <a href="https://github.com/Realytics/fork-ts-checker-webpack-plugin"><code>fork-ts-checker-webpack-plugin</code></a>. Alas using secure environment variables in Travis has unfortunate implications for ts-loader&#x27;s test pack.</p><p>Be not afeard. I&#x27;ve heard there&#x27;s a new shiny thing from GitHub that I could use instead... It&#x27;s a sign; I must use it!</p><p>GitHub Actions are still in beta. Technically Actions are <a href="https://developer.github.com/actions/creating-github-actions/">code run in Docker containers</a> in response to events. This didn&#x27;t mean a great deal to me until I started thinking about what I wanted to do with <code>ts-loader</code>&#x27;s publishing flow.</p><h2>Automate What?</h2><p>Each time I publish a release of <code>ts-loader</code> I execute the following node commands by hand:</p><ol><li><code>yarn install</code> <!-- -->-<!-- --> to install <code>ts-loader</code>&#x27;s dependencies</li><li><code>yarn build</code> <!-- -->-<!-- --> to build <code>ts-loader</code></li><li><code>yarn test</code> <!-- -->-<!-- --> to run <code>ts-loader</code>&#x27;s test packs</li><li><code>npm publish</code> <!-- -->-<!-- --> to publish the release of <code>ts-loader</code> to npm</li></ol><p>Having read up on GitHub Actions it seemed like they were born to handle this sort of task.</p><h2>GitHub Action for <code>npm</code></h2><p>I quickly discovered that someone out there <s>loves me</s></p><p>had <a href="https://github.com/actions/npm">already written a GitHub Action for <code>npm</code></a>.</p><p>The example in the <code>README.md</code> could be easily tweaked to meet my needs with one caveat: I had to use <code>npm</code> in place of <code>yarn</code>. I didn&#x27;t want to switch from <code>yarn</code>. What to do?</p><p>Well, remember when I said actions are code run in Docker containers? Another way to phrase that is to say: GitHub Actions are Docker images. Let&#x27;s look under the covers of the <code>npm</code> GitHub Action. As we peer inside the <a href="https://github.com/actions/npm/blob/e7aaefed7c9f2e83d493ff810f17fa5ccd7ed437/Dockerfile#L1"><code>Dockerfile</code></a> what do we find?</p><pre><code>FROM node:10-slim
</code></pre><p>Hmmmm.... Interesting. The base image of the <code>npm</code> GitHub Action is <code>node:10-slim</code>. Looking it up, it seems the <code>-slim</code> Docker images come with <a href="https://github.com/nodejs/docker-node/blob/master/Dockerfile-slim.template"><code>yarn</code> included</a>. Which means we should be able to use <code>yarn</code> inside the <code>npm</code> GitHub Action. Nice!</p><h2>GitHub Action for <code>npm</code> for <code>yarn</code></h2><p>Using <code>yarn</code> from the GitHub Action for <code>npm</code> is delightfully simple. Here&#x27;s what running <code>npm install</code> looks like:</p><pre><code># install with npm
action &quot;install&quot; {
  uses = &quot;actions/npm@1.0.0&quot;
  args = &quot;install&quot;
}
</code></pre><p>Pivoting to use <code>yarn install</code> instead of <code>npm install</code> is as simple as:</p><pre><code># install with yarn
action &quot;install&quot; {
  uses = &quot;actions/npm@1.0.0&quot;
  runs = &quot;yarn&quot;
  args = &quot;install&quot;
}
</code></pre><p>You can see we&#x27;ve introduced the <code>runs = &quot;yarn&quot;</code> and after that the <code>args</code> are whatever you need them to be.</p><h2>Going With The Workflow</h2><p>A GitHub Workflow that implements the steps I need would look like this:</p><pre><code>workflow &quot;build, test and publish on release&quot; {
  on = &quot;push&quot;
  resolves = &quot;publish&quot;
}

# install with yarn
action &quot;install&quot; {
  uses = &quot;actions/npm@1.0.0&quot;
  runs = &quot;yarn&quot;
  args = &quot;install&quot;
}

# build with yarn
action &quot;build&quot; {
  needs = &quot;install&quot;
  uses = &quot;actions/npm@1.0.0&quot;
  runs = &quot;yarn&quot;
  args = &quot;build&quot;
}

# test with yarn
action &quot;test&quot; {
  needs = &quot;build&quot;
  uses = &quot;actions/npm@1.0.0&quot;
  runs = &quot;yarn&quot;
  args = &quot;test&quot;
}

# filter for a new tag
action &quot;check for new tag&quot; {
  needs = &quot;Test&quot;
  uses = &quot;actions/bin/filter@master&quot;
  args = &quot;tag&quot;
}

# publish with npm
action &quot;publish&quot; {
  needs = &quot;check for new tag&quot;
  uses = &quot;actions/npm@1.0.0&quot;
  args = &quot;publish&quot;
  secrets = [&quot;NPM_AUTH_TOKEN&quot;]
}
</code></pre><p>As you can see, this is a direct automation of steps 1-4 I listed earlier. Since all these actions are executed in the same container, we can skip from <code>yarn</code> to <code>npm</code> with gay abandon.</p><p>What&#x27;s absolutely amazing is, when I got access to GitHub Actions <a href="https://github.com/TypeStrong/ts-loader/blob/master/.github/main.workflow">my hand crafted workflow</a> looked like it should work first time! I know, right? Don&#x27;t you love it when that happens? <a href="https://github.com/actions/bin/issues/13">Alas there&#x27;s presently a problem with filters in GitHub Actions</a>. But that&#x27;s by the by, if you&#x27;re just looking to use a GitHub Action with yarn instead of npm then you are home free.</p><h2>You Don&#x27;t Actually Need the npm GitHub Action</h2><p>You heard me right. Docker containers be Docker containers. You don&#x27;t actually need to use this:</p><pre><code>uses = &quot;actions/npm@1.0.0&quot;
</code></pre><p>You can use <em>any</em> Docker container which has node / npm installed! So if you&#x27;d like to use say node 11 instead you could just do this:</p><pre><code>uses = &quot;docker://node:11&quot;
</code></pre><p>Which would use the node 11 image on <a href="https://hub.docker.com/_/node">docker hub</a>.</p><p>Which is pretty cool. You know what&#x27;s even more incredible? Inside a workflow you can switch <code>uses</code> mid-workflow and keep the output. That&#x27;s right; you can have a work flow with say three actions running <code>uses = &quot;docker://node:11&quot;</code> and then a fourth running <code>uses = &quot;actions/npm@1.0.0&quot;</code>. That&#x27;s <em>so</em> flexible and powerful!</p><p>Thanks to <a href="https://github.com/mcolyer">Matt Colyer</a> and <a href="https://github.com/LandonSchropp">Landon Schropp</a> for <a href="https://github.com/actions/npm/issues/9">schooling me on the intricicies of GitHub Actions</a>. Much ❤</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[You Might Not Need thread-loader]]></title>
            <link>https://blog.johnnyreilly.com/2018/12/22/you-might-not-need-thread-loader</link>
            <guid>You Might Not Need thread-loader</guid>
            <pubDate>Sat, 22 Dec 2018 00:00:00 GMT</pubDate>
            <description><![CDATA[It all started with a GitHub issue. Ernst Ammann reported:]]></description>
            <content:encoded><![CDATA[<p>It all started with a GitHub issue. <a href="https://github.com/namics/webpack-config-plugins/issues/24">Ernst Ammann reported</a>:</p><blockquote><p>Without the thread-loader, compilation takes three to four times less time on changes. We could remove it.</p></blockquote><p>If you&#x27;re not aware of the <a href="https://github.com/namics/webpack-config-plugins"><code>webpack-config-plugins</code></a> project then I commend it to you. Famously, webpack configuration can prove tricky. <code>webpack-config-plugins</code> borrows the idea of presets from Babel. It provides a number of pluggable webpack configurations which give a best practice setup for different webpack use cases. So if you&#x27;re no expert with webpack and you want a good setup for building your TypeScript / Sass / JavaScript then <code>webpack-config-plugins</code> has got your back.</p><p>One of the people behind the project is the very excellent <a href="https://github.com/jantimon">Jan Nicklas</a> who is well known for his work on the <a href="https://github.com/jantimon/html-webpack-plugin"><code>html-webpack-plugin</code></a>.</p><p>It was Jan who responded to Ernst&#x27;s issue and decided to look into it.</p><h2>All I Want For Christmas is Faster Builds</h2><p>Everyone wants fast builds. I do. You do. We all do. <code>webpack-config-plugins</code> is about giving these to the user in a precooked package.</p><p>There&#x27;s a webpack loader called <a href="https://github.com/webpack-contrib/thread-loader"><code>thread-loader</code></a> which spawns multiple processes and splits up work between them. It was originally inspired by the work in the happypack project which does a similar thing.</p><p>I wrote <a href="https://medium.com/p/83cc568dea79">a blog post</a> some time ago which gave details about ways to speed up your TypeScript builds by combining the <a href="https://github.com/TypeStrong/ts-loader"><code>ts-loader</code></a> project (which I manage) with the <a href="https://github.com/Realytics/fork-ts-checker-webpack-plugin"><code>fork-ts-checker-webpack-plugin</code></a> project (which I&#x27;m heavily involved with).</p><p>That post was written back in the days of webpack 2 / 3. It advocated use of both <code>happypack</code> / <code>thread-loader</code> to drop your build times even further. As you&#x27;ll see, now that we&#x27;re well into the world of webpack 4 (with webpack 5 waiting in the wings) the advantage of <code>happypack</code> / <code>thread-loader</code> are no longer so profound.</p><p><code>webpack-config-plugins</code> follows the advice I set out in my post; it uses <code>thread-loader</code> in its pluggable configurations. Now, back to Ernst&#x27;s issue.</p><h2><code>thread-loader</code>: Infinity War</h2><p>Jan quickly identified the problem. He did that rarest of things; he read the documentation which said:</p><pre><code class="language-js">// timeout for killing the worker processes when idle
      // defaults to 500 (ms)
      // can be set to Infinity for watching builds to keep workers alive
      poolTimeout: 2000,
</code></pre><p>The <code>webpack-config-plugins</code> configurations (running in watch mode) were subject to the thread loaders being killed after 500ms. They got resurrected when they were next needed; but that&#x27;s not as instant as you might hope. Jan then did a test:</p><pre><code class="language-sh">(default pool - 30 runs - 1000 components ) average: 2.668068965517241
(no thread-loader - 30 runs - 1000 components ) average: 1.2674137931034484
(Infinity pool - 30 runs - 1000 components ) average: 1.371827586206896
</code></pre><p>This demonstrates that using <code>thread-loader</code> in watch mode with <code>poolTimeout: Infinity</code> performs significantly better than when it defaults to 500ms. But perhaps more significantly, not using <code>thread-loader</code> performs even better still.</p><h2>&quot;Maybe You&#x27;ve Thread Enough&quot;</h2><p>When I tested using <code>thread-loader</code> in watch mode with <code>poolTimeout: Infinity</code> on my own builds I got the same benefit Jan had. I also got <em>even</em> more benefit from dropping <code>thread-loader</code> entirely.</p><p>A likely reason for this benefit is that typically when you&#x27;re developing, you&#x27;re working on one file at a time. Hence you only transpile one file at a time:</p><p><img src="../static/blog/2018-12-22-you-might-not-need-thread-loader/ts-profile2.png"/></p><p>So there&#x27;s not a great deal of value that <code>thread-loader</code> can add here; mostly it&#x27;s twiddling thumbs and adding an overhead. <a href="https://github.com/webpack-contrib/thread-loader/blob/master/README.md#usage">To quote the docs:</a></p><blockquote><p>Each worker is a separate node.js process, which has an overhead of <!-- -->~<!-- -->600ms. There is also an overhead of inter-process communication.</p><p>Use this loader only for expensive operations!</p></blockquote><p>Now, my build is not your build. I can&#x27;t guarantee that you&#x27;ll get the same results as Jan and I experienced; but I would encourage you to investigate if you&#x27;re using <code>thread-loader</code> correctly and whether it&#x27;s actually helping you. In these days of webpack 4+ perhaps it isn&#x27;t.</p><p>There are still scenarios where <code>thread-loader</code> still provides an advantage. It can speed up production builds. It can speed up the initial startup of watch mode. <a href="https://github.com/webpack-contrib/thread-loader/pull/52">In fact Jan has subsequently actually improved the <code>thread-loader</code> to that specific end.</a> Yay Jan!</p><p>If this is all too much for you, and you want to hand off the concern to someone else then perhaps all of this serves as a motivation to just sit back, put your feet up and start using <a href="https://github.com/namics/webpack-config-plugins"><code>webpack-config-plugins</code></a> instead of doing your own configuration.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Cache Rules Everything Around Me]]></title>
            <link>https://blog.johnnyreilly.com/2018/12/10/cache-rules-everything-around-me</link>
            <guid>Cache Rules Everything Around Me</guid>
            <pubDate>Mon, 10 Dec 2018 00:00:00 GMT</pubDate>
            <description><![CDATA[One thing that ASP.Net Core really got right was caching. IMemoryCache is a caching implementation that does just what I want. I love it. I take it everywhere. I've introduced it to my family.]]></description>
            <content:encoded><![CDATA[<p>One thing that ASP.Net Core really got right was caching. <a href="https://docs.microsoft.com/en-us/aspnet/core/performance/caching/memory"><code>IMemoryCache</code></a> is a caching implementation that does just what I want. I love it. I take it everywhere. I&#x27;ve introduced it to my family.</p><h2>TimeSpan, TimeSpan Expiration Y&#x27;all</h2><p>To make usage of the <code>IMemoryCache</code> <em>even</em> more lovely I&#x27;ve written an extension method. I follow pretty much one cache strategy: <code>SetAbsoluteExpiration</code> and I just vary the expiration by an amount of time. This extension method implements that in a simple way; I call it <code>GetOrCreateForTimeSpanAsync</code> - catchy right? It looks like this:</p><pre><code class="language-cs">using System;
using System.Threading.Tasks;
using Microsoft.Extensions.Caching.Memory;

namespace My.Helpers {

    public static class CacheHelpers {

        public static async Task&lt;TItem&gt; GetOrCreateForTimeSpanAsync&lt;TItem&gt;(
            this IMemoryCache cache,
            string key,
            Func&lt;Task&lt;TItem&gt;&gt; itemGetterAsync,
            TimeSpan timeToCache
        ) {
            if (!cache.TryGetValue(key, out object result)) {
                result = await itemGetterAsync();
                if (result == null)
                    return default(TItem);

                var cacheEntryOptions = new MemoryCacheEntryOptions()
                    .SetAbsoluteExpiration(timeToCache);

                cache.Set(key, result, cacheEntryOptions);
            }

            return (TItem) result;
        }
    }
}
</code></pre><p>Usage looks like this:</p><pre><code class="language-cs">private Task&lt;SuperInterestingThing&gt; GetSuperInterestingThingFromCache(Guid superInterestingThingId) =&gt;
    _cache.GetOrCreateForTimeSpanAsync(
        key: $&quot;{nameof(MyClass)}:GetSuperInterestingThing:{superInterestingThingId}&quot;,
        itemGetterAsync: () =&gt; GetSuperInterestingThing(superInterestingThingId),
        timeToCache: TimeSpan.FromMinutes(5)
    );
</code></pre><p>This helper allows the consumer to provide three things:</p><ul><li>The <code>key</code> key for the item to be cached with</li><li>A <code>itemGetterAsync</code> which is the method that is used to retrieve a new value if an item cannot be found in the cache</li><li>A <code>timeToCache</code> which is the period of time that an item should be cached</li></ul><p>If an item can&#x27;t be looked up by the <code>itemGetterAsync</code> then <em>nothing</em> will be cached and a the <code>default</code> value of the expected type will be returned. This is important because lookups can fail, and there&#x27;s nothing worse than a lookup failing and you caching <code>null</code> as a result.</p><p>Go on, ask me how I know.</p><p>This is a simple, clear and helpful API which makes interacting with <code>IMemoryCache</code> even more lovely than it was. Peep it y&#x27;all.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Snapshot Testing for C#]]></title>
            <link>https://blog.johnnyreilly.com/2018/11/17/snapshot-testing-for-c</link>
            <guid>Snapshot Testing for C#</guid>
            <pubDate>Sat, 17 Nov 2018 00:00:00 GMT</pubDate>
            <description><![CDATA[If you're a user of Jest, you've no doubt heard of and perhaps made use of snapshot testing.]]></description>
            <content:encoded><![CDATA[<p>If you&#x27;re a user of Jest, you&#x27;ve no doubt heard of and perhaps made use of <a href="https://jestjs.io/docs/en/snapshot-testing">snapshot testing</a>.</p><p>Snapshot testing is an awesome tool that is generally discussed in the context of JavaScript React UI testing. But snapshot testing has a wider application than that. Essentially it is profoundly useful where you have functions which produce a complex structured output. It could be a React UI, it could be a list of FX prices. The type of data is immaterial; it&#x27;s the amount of it that&#x27;s key.</p><p>Typically there&#x27;s a direct correlation between the size and complexity of the output of a method and the length of the tests that will be written for it. Let&#x27;s say you&#x27;re outputting a class that contains 20 properties. Congratulations! You get to write 20 assertions in one form or another for each test case. Or a single assertion whereby you supply the expected output by hand specifying each of the 20 properties. Either way, that&#x27;s not going to be fun. And just imagine the time it would take to update multiple test cases if you wanted to change the behaviour of the method in question. Ouchy.</p><p>Time is money kid. What you need is snapshot testing. Say goodbye to handcrafted assertions and hello to JSON serialised output checked into source control. Let&#x27;s unpack that a little bit. The usefulness of snapshot testing that I want in C# is predominantly about removing the need to write and maintain multiple assertions. Instead you write tests that compare the output of a call to your method with JSON serialised output you&#x27;ve generated on a previous occasion.</p><p>This approach takes less time to write, less time to maintain and the solid readability of JSON makes it more likely you&#x27;ll pick up on bugs. It&#x27;s so much easier to scan JSON than it is a list of assertions.</p><h2>Putting the Snapshot into C#</h2><p>Now if you&#x27;re writing tests in JavaScript or TypeScript then Jest already has your back with CLI snapshot generation and <code>shouldMatchSnapshot</code>. However getting to nearly the same place in C# is delightfully easy. What are we going to need?</p><p>First up, a serializer which can take your big bad data structures and render them as JSON. Also we&#x27;ll use it to rehydrate our data structure into an object ready for comparison. We&#x27;re going to use <a href="https://www.newtonsoft.com/json">Json.NET</a>.</p><p>Next up we need a way to compare our outputs with our rehydrated snapshots - we need a C# <code>shouldMatchSnapshot</code>. There&#x27;s many choices out there, but for my money <a href="https://fluentassertions.com">Fluent Assertions</a> is king of the hill.</p><p>Finally we&#x27;re going to need Snapshot, a little helper utility I put together:</p><pre><code class="language-cs">using System;
using System.IO;
using Newtonsoft.Json;
using Newtonsoft.Json.Serialization;

namespace Test.Utilities {
    public static class Snapshot {
        private static readonly JsonSerializer StubSerializer = new JsonSerializer {
            ContractResolver = new CamelCasePropertyNamesContractResolver(),
            NullValueHandling = NullValueHandling.Ignore
        };

        private static JsonTextWriter MakeJsonTextWriter(TextWriter sw) =&gt; new JsonTextWriter(sw) {
            Formatting = Formatting.Indented,
            IndentChar = &#x27; &#x27;,
            Indentation = 2
        };

        /// &lt;summary&gt;
        /// Make yourself some JSON! Usage looks like this:
        /// Stubs.Make($&quot;{System.AppDomain.CurrentDomain.BaseDirectory}..\\..\\..\\data.json&quot;, myData);
        /// &lt;/summary&gt;
        public static void Make&lt;T&gt;(string stubPath, T data) {
            try {
                if (string.IsNullOrEmpty(stubPath))
                    throw new ArgumentNullException(nameof(stubPath));
                if (data == null)
                    throw new ArgumentNullException(nameof(data));

                using(var sw = new StreamWriter(stubPath))
                using(var writer = MakeJsonTextWriter(sw)) {
                    StubSerializer.Serialize(writer, data);
                }
            } catch (Exception exc) {
                throw new Exception($&quot;Failed to make {stubPath}&quot;, exc);
            }
        }

        public static string Serialize&lt;T&gt;(T data) {
            using (var sw = new StringWriter())
            using(var writer = MakeJsonTextWriter(sw)) {
                StubSerializer.Serialize(writer, data);
                return sw.ToString();
            }
        }

        public static string Load(string filename) {
            var content = new StreamReader(
                File.OpenRead(filename)
            ).ReadToEnd();

            return content;
        }
    }
}
</code></pre><p>Let&#x27;s look at the methods: <code>Make</code> and <code>Load</code>. Make is what we&#x27;re going to use to create our snapshots. Load is what we&#x27;re going to use to, uh, load our snapshots.</p><p>What does usage look like? Great question. Let&#x27;s go through the process of writing a C# snapshot test.</p><h2>Taking Snapshot for a Spin</h2><p>First of all, we&#x27;re going to need a method to test that outputs a data structure which is more than just a scalar value. Let&#x27;s use this:</p><pre><code class="language-cs">public class Leopard {
    public string Name { get; set; }
    public int Spots { get; set; }
}

public class LeopardService {
    public Leopard[] GetTheLeopards() {
        return new Leopard[] {
            new Leopard { Spots = 42, Name = &quot;Nimoy&quot; },
            new Leopard { Spots = 900, Name = &quot;Dotty&quot; }
        };
    }
}
</code></pre><p>Yes - our trusty <code>LeopardService</code>. As you can see, the <code>GetTheLeopards</code> method returns an array of <code>Leopard</code>s. For now, let&#x27;s write a test using <code>Snapshot</code>: (ours is an XUnit test; but <code>Snapshot</code> is agnostic of this)</p><pre><code class="language-cs">[Fact]
public void GetTheLeopards_should_return_expected_Leopards() {
    // Arrange
    var leopardService = new LeopardService();

    // Act
    var leopards = leopardService.GetTheLeopards();

    // UNCOMMENT THE LINE BELOW *ONLY* WHEN YOU WANT TO GENERATE THE SNAPSHOT
    Snapshot.Make($&quot;{System.AppDomain.CurrentDomain.BaseDirectory}..\\..\\..\\Snapshots\\leopardsSnapshot.json&quot;, leopards);

    // Assert
    var snapshotLeopards = JsonConvert.DeserializeObject&lt;leopard[]&gt;(Snapshot.Load(&quot;Snapshots/leopardsSnapshot.json&quot;));
    snapshotLeopards.Should().BeEquivalentTo(leopards);
}
&lt;/leopard[]&gt;
</code></pre><p>Before we run this for the first time we need to setup our testing project to be ready for snapshots. First of all we add a <code>Snapshot</code> folder to the test project. The we also add the following to the <code>.csproj</code>:</p><pre><code class="language-xml">&lt;ItemGroup&gt;
    &lt;Content Include=&quot;Snapshots\**&quot;&gt;
      &lt;CopyToOutputDirectory&gt;Always&lt;/CopyToOutputDirectory&gt;
    &lt;/Content&gt;
  &lt;/ItemGroup&gt;
</code></pre><p>This includes the snapshots in the compile output for when tests are being run.</p><p>Now let&#x27;s run the test. It will generate a <code>leopardsSnapshot.json</code> file:</p><pre><code class="language-json">[
  {
    &quot;name&quot;: &quot;Nimoy&quot;,
    &quot;spots&quot;: 42
  },
  {
    &quot;name&quot;: &quot;Dotty&quot;,
    &quot;spots&quot;: 900
  }
]
</code></pre><p>With our snapshot in place, we comment out the <code>Snapshot.Make...</code> line and we have a passing test. Let&#x27;s commit our code, push and go about our business.</p><h2>Time Passes...</h2><p>Someone decides that the implementation of <code>GetTheLeopards</code> needs to change. Defying expectations it seems that Dotty the leopard should now have 90 spots. I know... Business requirements, right?</p><p>If we make that change we&#x27;d ideally expect our trusty test to fail. Let&#x27;s see what happens:</p><pre><code>----- Test Execution Summary -----

Leopard.Tests.Services.LeopardServiceTests.GetTheLeopards_should_return_expected_Leopards:
    Outcome: Failed
    Error Message:
    Expected item[1].Spots to be 90, but found 900.
</code></pre><p>Boom! We are protected!</p><p>Since this is a change we&#x27;re completely happy with we want to update our <code>leopardsSnapshot.json</code> file. We could make our test pass by manually updating the JSON. That&#x27;d be fine. But why work when you don&#x27;t have to? Let&#x27;s uncomment our <code>Snapshot.Make...</code> line and run the test the once.</p><pre><code class="language-json">[
  {
    &quot;name&quot;: &quot;Nimoy&quot;,
    &quot;spots&quot;: 42
  },
  {
    &quot;name&quot;: &quot;Dotty&quot;,
    &quot;spots&quot;: 90
  }
]
</code></pre><p>That&#x27;s right, we have an updated snapshot! Minimal effort.</p><h2>Next Steps</h2><p>This is a basic approach to getting the goodness of snapshot testing in C#. It could be refined further. To my mind the uncommenting / commenting of code is not the most elegant way to approach this and so there&#x27;s some work that could be done around this area.</p><p>Happy snapshotting!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Making a Programmer]]></title>
            <link>https://blog.johnnyreilly.com/2018/10/27/making-a-programmer</link>
            <guid>Making a Programmer</guid>
            <pubDate>Sat, 27 Oct 2018 00:00:00 GMT</pubDate>
            <description><![CDATA[I recently had the good fortune to help run a coding bootcamp. The idea was simple: there are many people around us who are interested in programming but don't know where to start. Let's take some folk who do and share the knowledge.]]></description>
            <content:encoded><![CDATA[<p>I recently had the good fortune to help run a coding bootcamp. The idea was simple: there are many people around us who are interested in programming but don&#x27;t know where to start. Let&#x27;s take some folk who do and share the knowledge.</p><p>The bootcamp went tremendously! (Well, I say that... Frankly I had a blast. 😀 )</p><p>Coding padawans walked in at the start with laptops and questions, and six weeks later they left with the groundwork of development experience. We ran a session for an hour during lunchtime once a week. Between that, people would have the opportunity to learn online, do exercises and reach out to the facilitators and their fellow apprentices for help.</p><p>We&#x27;d never done this before. We were student teachers; learning how to teach as we ran the course. So what did we do? Are you curious? Read on, Macduff!</p><h2>Code Review</h2><p>It&#x27;s worth saying now that we started our course with a plan: the plan was that we would be ready to change the plan. Or to put it another way, we were ready to pivot as we went.</p><p>We (by which I mean myself and the other course organisers) are interested in feedback. Sitting back and saying &quot;Hey! We did this thing.... What do you think about it?&quot; Because sometimes your plans are great. Do more of that hotness! But also, not all your ideas pan out... Maybe bail on those guys. Finally, never forget: other folk have brain tickling notions too.... We&#x27;re with Picasso on this: good artists copy; great artists steal.</p><p>We&#x27;re heavily invested in feedback in both what we build and how we build it. So we were totally going to apply this to doing something we&#x27;d never done before. So seized were we of this that we made feedback part of the session. For the last five minutes each week we&#x27;d run a short retrospective. We&#x27;d stick up happy, sad and &quot;meh&quot; emojis to the wall, hand out post-its and everyone got to stick up their thoughts.</p><p><img src="../static/blog/2018-10-27-making-a-programmer/not-so-sure-about-this-feedback.jpg"/></p><p>From that we learned what was working, what wasn&#x27;t and when we were very lucky there were suggestions too. We listened to all the feedback and the next week&#x27;s session would be informed by what we&#x27;d just learned.</p><h2>Merging to Master</h2><p>So, what did we end up with? What did our coding bootcamp look like?</p><p>Well, to start each session we kicked off with an icebreaker. We very much wanted the sessions to be interactive experiences; we wanted them to feel playful and human. So an icebreaker was a good way to get things off on the right foot.</p><p>The IBs were connected with the subject at hand. For example: Human FizzBuzz. We took the classic interview question and applied it to wannabe coders. We explained the rules, and went round in a circle, each person was the next iteration of the loop. As each dev-in-training opened their mouths they had to say a number or &quot;Fizz&quot; or &quot;Buzz&quot; or &quot;FizzBuzz&quot;. (It turns out this is harder than you think; and makes for a surprisingly entertaining parlour game. I intend to do this at my next dinner party.)</p><p>After that we covered the rules of the game. (Yup, learning is a game and it&#x27;s a good &#x27;un.) Certainly the most important rule was this: <u>there are <strong><em>no</em></strong> stupid questions</u></p><p>. If people think there are, then they might be hesitant to ask. And any question benched is a learning opportunity lost. We don&#x27;t want that.</p><p>&quot;Ask any question!&quot; we said each week. Kudos to the people who have the courage to pipe up. We salute you! You&#x27;re likely putting voice to a common area of misunderstanding.</p><p>Then we&#x27;d move onto the main content. The initial plan was to make use of the excellent <a href="https://www.edx.org/learn/python">EdX Python course</a> Between each session our learners would do a module and then we&#x27;d come together and talk around that topic somewhat. Whilst this was a good initial plan it did make the learning experience somewhat passive and less interactive than we&#x27;d hoped.</p><p>One week we tried something different. It turns out that the amazing <a href="https://twitter.com/foldr">JMac</a> has quite the skill for writing programming exercises. Small coding challenges that people can tackle independently. JMac put together a <a href="https://repl.it/">repl.it</a> of exercises and encouraged the class to get stuck in. They did. So much so that at the end of the session it was hard to get everyone&#x27;s attention to let them know the session was over. They were in the zone. When we did finally disrupt their flow, the feedback was pretty unanimous: we&#x27;d hit paydirt.</p><p><img src="../static/blog/2018-10-27-making-a-programmer/we-dug-this-feedback.jpg"/></p><p>Consequently, that was the format going onwards. JMac would come up with a number of exercises for the class. Wisely they were constructed so that they gently levelled up in terms of complexity as you went on. You&#x27;d get the dopamine hit of satisfaction as you did the earliest challenges that would give you the confidence to tackle the more complex later problems. If peeps got stuck they could ask someone to advise them, a facilitator or a peer. Or they could google it.... Like any other dev.</p><p>Having the chance to talk with others when you&#x27;re stuck is fantastic. You can talk through a problem. The act of doing that is a useful exercise. When you talk through a problem out loud you can unlock your understanding and often get to the point where you can tackle this yourself. This is <a href="https://en.wikipedia.org/wiki/Rubber_duck_debugging">rubber duck debugging</a>. Any dev does this in their everyday; it makes complete sense to have it as part of a coding bootcamp.</p><p>We learned that it was useful, very useful, to have repitition in the exercises. Repitition. Repitition. Repitition. As the exercises started each week they would typically begin by recapping and repeating the content covered the previous week. The best way to learn is to practice. It&#x27;s not for nothing the Karate Kid had to &quot;wax on, wax off&quot;.</p><p>Finally, we did this together. The course wasn&#x27;t run by one person; we had a gang! We had three facilitators who helped to run the sessions; JMac, Jonesy and myself. We also had the amazing <a href="https://twitter.com/janicewarden">Janice</a> who handled the general organisation and logistics. And made us laugh. A lot. This was obviously great from a camaraderie and sharing the load perspective. It turns out that having that number of facilitators in the session meant that everyone who needed help could get it. It&#x27;s worth noting that having more than a single facilitator is useful in terms of the dynamic it creates. You can bounce things off one another; you can use each other for examples and illustrations. You can crack each other up. Done well it reduces the instructor / learner divide and that breaking down of barriers is something worth seeking.</p><h2>RTM</h2><p>We&#x27;ve run a bootcamp once now. Where we are is informed by the experience we&#x27;ve just had. A different group of learners may well have resulted in a slightly different format; though I have a feeling not overly dissimilar. We feel pretty sure that what we&#x27;ve got is pretty solid. That said, just as the attendees are learning about development, we&#x27;re still learning about learning!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Brand New Fonting Awesomeness]]></title>
            <link>https://blog.johnnyreilly.com/2018/10/07/font-awesome-brand-icons-react</link>
            <guid>Brand New Fonting Awesomeness</guid>
            <pubDate>Sun, 07 Oct 2018 00:00:00 GMT</pubDate>
            <description><![CDATA[Love me some Font Awesome. Absolutely wonderful. However, I came a cropper when following the instructions on using the all new Font Awesome 5 with React. The instructions for standard icons work fine. But if you want to use brand icons then this does not help you out much. There's 2 problems:]]></description>
            <content:encoded><![CDATA[<p>Love me some <a href="https://fontawesome.com">Font Awesome</a>. Absolutely wonderful. However, I came a cropper when following the instructions <a href="https://fontawesome.com/how-to-use/on-the-web/using-with/react">on using the all new Font Awesome 5 with React</a>. The instructions for standard icons work <em>fine</em>. But if you want to use brand icons then this does not help you out much. There&#x27;s 2 problems:</p><ol><li>Font Awesome&#x27;s brand icons are not part of <code>&lt;a href=&quot;https://www.npmjs.com/package/@fortawesome/free-solid-svg-icons&quot;&gt;@fortawesome/free-solid-svg-icons&lt;/a&gt;</code> package</li><li>The method of icon usage illustrated (i.e. with the <code>FontAwesomeIcon</code> component) doesn&#x27;t work. It doesn&#x27;t render owt.</li></ol><h2>Brand Me Up Buttercup</h2><p>You want brands? Well you need the <code>&lt;a href=&quot;https://www.npmjs.com/package/@fortawesome/free-brands-svg-icons&quot;&gt;@fortawesome/free-brands-svg-icons&lt;/a&gt;</code>. Obvs, right?</p><pre><code class="language-sh">yarn add @fortawesome/fontawesome-svg-core
yarn add @fortawesome/free-brands-svg-icons
yarn add @fortawesome/react-fontawesome
</code></pre><p>Now usage:</p><pre><code class="language-jsx">import * as React from &#x27;react&#x27;;
import { FontAwesomeIcon } from &#x27;@fortawesome/react-fontawesome&#x27;;
import { faReact } from &#x27;@fortawesome/free-brands-svg-icons&#x27;;

export const Framework = () =&gt; (
  &lt;div&gt;
    Favorite Framework: &lt;FontAwesomeIcon icon={faReact} /&gt;
  &lt;/div&gt;
);
</code></pre><p>Here we&#x27;ve ditched the &quot;library / magic-string&quot; approach from the documentation for one which explicitly imports and uses the required icons. I suspect this will be good for tree-shaking as well but, hand-on-heart, I haven&#x27;t rigorously tested that. I&#x27;m not sure why the approach I&#x27;m using isn&#x27;t documented actually. Mysterious! I&#x27;ve seen no ill-effects from using it but perhaps YMMV. Proceed with caution...</p><h2>Update: It is documented!</h2><p>Yup - information on this approach is out there; but it&#x27;s less obvious than you might hope. <a href="https://github.com/FortAwesome/react-fontawesome#explicit-import">Read all about it here.</a> For what it&#x27;s worth, the explicit import approach seems to be playing second fiddle to the library / magic-string one. I&#x27;m not too sure why. For my money, explicit imports are clearer, less prone to errors and better setup for optimisation. Go figure...</p><p>Feel free to set me straight in the comments!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ts-loader Project References: First Blood]]></title>
            <link>https://blog.johnnyreilly.com/2018/09/23/ts-loader-project-references-first-blood</link>
            <guid>ts-loader Project References: First Blood</guid>
            <pubDate>Sun, 23 Sep 2018 00:00:00 GMT</pubDate>
            <description><![CDATA[So project references eh? They shipped with TypeScript 3. We've just shipped initial support for project references in ts-loader v5.2.0. All the hard work was done by the amazing Andrew Branch. In fact I'd recommend taking a gander at the PR. Yay Andrew!]]></description>
            <content:encoded><![CDATA[<p>So <a href="https://www.typescriptlang.org/docs/handbook/project-references.html">project references</a> eh? They shipped with <a href="https://blogs.msdn.microsoft.com/typescript/2018/07/30/announcing-typescript-3-0/#project-references">TypeScript 3</a>. We&#x27;ve just shipped initial support for project references in <a href="https://github.com/TypeStrong/ts-loader/releases/tag/v5.2.0"><code>ts-loader v5.2.0</code></a>. All the hard work was done by the amazing <a href="https://twitter.com/atcb">Andrew Branch</a>. In fact I&#x27;d recommend taking a gander at <a href="https://github.com/TypeStrong/ts-loader/pull/817">the PR</a>. Yay Andrew!</p><p>This post will take us through the nature of the support for project references in ts-loader now and what we hope the future will bring. It <strike>rips off shamelessly</strike></p><p>borrows from the <a href="https://github.com/TypeStrong/ts-loader#projectreferences-boolean-defaultfalse"><code>README.md</code></a> documentation that Andrew wrote as part of the PR. Because I am not above stealing.</p><h2>TL;DR</h2><p>Using project references currently requires building referenced projects outside of ts-loader. We don’t want to keep it that way, but we’re releasing what we’ve got now. To try it out, you’ll need to pass <code>projectReferences: true</code> to <code>loaderOptions</code>.</p><h2>Like <code>tsc</code>, but <em>not</em> like <code>tsc --build</code></h2><p>ts-loader has partial support for <a href="https://www.typescriptlang.org/docs/handbook/project-references.html">project references</a> in that it will <em>load</em> dependent composite projects that are already built, but will not currently <em>build/rebuild</em> those upstream projects. The best way to explain exactly what this means is through an example. Say you have a project with a project reference pointing to the <code>lib/</code> directory:</p><pre><code class="language-sh">tsconfig.json
app.ts
lib/
  tsconfig.json
  niftyUtil.ts
</code></pre><p>And we’ll assume that the root <code>tsconfig.json</code> has <code>{ &quot;references&quot;: { &quot;path&quot;: &quot;lib&quot; } }</code>, which means that any import of a file that’s part of the <code>lib</code> sub-project is treated as a reference to another project, not just a reference to a TypeScript file. Before discussing how ts-loader handles this, it’s helpful to review at a really basic level what <code>tsc</code> itself does here. If you were to run <code>tsc</code> on this tiny example project, the build would fail with the error:</p><pre><code class="language-sh">error TS6305: Output file &#x27;lib/niftyUtil.d.ts&#x27; has not been built from source file &#x27;lib/niftyUtil.ts&#x27;.
</code></pre><p>Using project references actually instructs <code>tsc</code><em>not</em> to build anything that’s part of another project from source, but rather to look for any <code>.d.ts</code> and <code>.js</code> files that have already been generated from a previous build. Since we’ve never built the project in <code>lib</code> before, those files don’t exist, so building the root project fails. Still just thinking about how <code>tsc</code> works, there are two options to make the build succeed: either run <code>tsc -p lib/tsconfig.json</code><em>first</em>, or simply run <code>tsc --build</code>, which will figure out that <code>lib</code> hasn’t been built and build it first for you.</p><p>Ok, so how is that relevant to ts-loader? Because the best way to think about what ts-loader does with project references is that it acts like <code>tsc</code>, but <em>not</em> like <code>tsc --build</code>. If you run ts-loader on a project that’s using project references, and any upstream project hasn’t been built, you’ll get the exact same <code>error TS6305</code> that you would get with <code>tsc</code>. If you modify a source file in an upstream project and don’t rebuild that project, <code>ts-loader</code> won’t have any idea that you’ve changed anything—it will still be looking at the output from the last time you <em>built</em> that file.</p><h2>“Hey, don’t you think that sounds kind of useless and terrible?”</h2><p>Well, sort of. You can consider it a work-in-progress. It’s true that on its own, as of today, ts-loader doesn’t have everything you need to take advantage of project references in webpack. In practice, though, <em>consuming</em> upstream projects and <em>building</em> upstream projects are somewhat separate concerns. Building them will likely come in a future release. For background, see the <a href="https://github.com/TypeStrong/ts-loader/issues/815">original issue</a>.</p><h2><code>outDir</code> Windows problemo.</h2><p>At the moment, composite projects built using the <a href="https://www.typescriptlang.org/docs/handbook/compiler-options.html"><code>outDir</code> compiler option</a> cannot be consumed using ts-loader on Windows. If you try to, ts-loader throws a &quot;<code>has not been built from source file</code>&quot; error. <a href="https://github.com/TypeStrong/ts-loader/pull/817#issuecomment-422245998">You can see Andrew and I puzzling over it in the PR.</a> We don&#x27;t know why yet; it&#x27;s possible there&#x27;s a bug in <code>tsc</code>. It&#x27;s more likely there&#x27;s a bug in <code>ts-loader</code>. Hopefully it&#x27;s going to get solved at some point. (Hey, maybe you&#x27;re the one to solve it!) Either way, we didn&#x27;t want to hold back from releasing. So if you&#x27;re building on Windows then avoid building <code>composite</code> projects using <code>outDir</code>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Semantic Versioning and Definitely Typed]]></title>
            <link>https://blog.johnnyreilly.com/2018/09/15/semantic-versioning-and-definitely-typed</link>
            <guid>Semantic Versioning and Definitely Typed</guid>
            <pubDate>Sat, 15 Sep 2018 00:00:00 GMT</pubDate>
            <description><![CDATA[This a tale of things that are and things that aren't. It's a tale of semantic versioning, the lack thereof and heartbreak. It's a story of terror and failing builds. But it has a bittersweet ending wherein our heroes learn a lesson and understand the need for compromise. We all come out better and wiser people. Hopefully there's something for everybody; let's start with an exciting opener and see where it goes...]]></description>
            <content:encoded><![CDATA[<p>This a tale of things that are and things that aren&#x27;t. It&#x27;s a tale of semantic versioning, the lack thereof and heartbreak. It&#x27;s a story of terror and failing builds. But it has a bittersweet ending wherein our heroes learn a lesson and understand the need for compromise. We all come out better and wiser people. Hopefully there&#x27;s something for everybody; let&#x27;s start with an exciting opener and see where it goes...</p><h2>Definitely Typed</h2><p>This is often the experience people have of using type definitions from Definitely Typed:</p><p><img src="../static/blog/2018-09-15-ivan-drago-and-definitely-typed/i-must-break-you.jpg" alt="Ivan Drago saying &quot;I must break you&quot;"/></p><p>Specifically, people are used to the idea of semantic versioning and expect it from types published to npm by Definitely Typed. They wait in vain. <a href="./2017-02-14-typescript-types-and-repeatable-builds.md">I&#x27;ve written before about the Definitely Typed / @types semantic version compromise.</a> And I wanted to talk about it a little further as (watching the issues raised on DT) I don&#x27;t think the message has quite got out there. To summarise:</p><ol><li><p>npm is built on top of <a href="http://semver.org/">semantic versioning</a> and they <a href="https://docs.npmjs.com/getting-started/semantic-versioning">take it seriously</a>. When a package is published it should be categorised as a major release (breaking changes), a minor release (extra functionality which is backwards compatible) or a patch release (backwards compatible bug fixes).</p></li><li><p>Definitely Typed publishes type definitions to npm under the <code>@types</code> namespace</p></li><li><p>To make consumption of type definitions easier, the versioning of a type definition package will seek to emulate the versioning of the npm package it supports. For example, right now <code>&lt;a href=&quot;https://www.npmjs.com/package/react-router&quot;&gt;react-router&lt;/a&gt;</code>&#x27;s latest version is <code>4.3.1</code>. The corresponding type definition <code>&lt;a href=&quot;https://www.npmjs.com/package/@types/react-router&quot;&gt;@types/react-router&lt;/a&gt;</code>&#x27;s latest version is <code>4.0.31</code>. (It&#x27;s fairly common for type definition versions to lag behind the package they type.)</p><p>If there&#x27;s a breaking change to the <code>react-router</code> type definition then the new version published will have a version number that begins <code>&quot;4.0.&quot;</code>. If you are relying on semantic versioning this will break you.</p></li></ol><h2>I Couldn&#x27;t Help But Notice Your Pain</h2><p>If you&#x27;re reading this and can&#x27;t quite believe that @types would be so inconsiderate as to break the conventions of the ecosystem it lives in, I understand. But hopefully you can see there are reasons for this. In the end, being able to use npm as a delivery mechanism for versioned type definitions associated with another package has a cost; that cost is semantic versioning for the type definitions themselves. It wasn&#x27;t a choice taken lightly; it&#x27;s a pragmatic compromise.</p><p>&quot;But what about my failing builds? Fine, people are going to change type definitions, but why should I burn because of their choices?&quot;</p><p>Excellent question. Truly. Well here&#x27;s my advice: don&#x27;t expect semantic versioning where there is none. Use specific package versions. You can do that directly with your <code>package.json</code>. For example replace something like this: <code>&quot;@types/react-router&quot;: &quot;^4.0.0&quot;</code> with a specific version number: <code>&quot;@types/react-router&quot;: &quot;4.0.31&quot;</code>. With this approach it&#x27;s a specific activity to upgrade your type definitions. A chore if you will; but a chore that guarantees builds will not fail unexpectedly due to changing type defs.</p><p>My own personal preference is <a href="https://yarnpkg.com/lang/en/">yarn</a>. Mother, I&#x27;m in love with a <code>yarn.lock</code> file. It is the alternative npm client that came out of Facebook. It pins the exact versions of all packages used in your <code>yarn.lock</code> file and guarantees to install the same versions each time. Problem solved; and it even allows me to keep the semantic versioning in my <code>package.json</code> as is.</p><p>This has some value in that when I upgrade I probably want to upgrade to a newer version following the semantic versioning convention. I should just expect that I&#x27;ll need to check valid compilation when I do so. yarn even has it&#x27;s own built in utility that tells you when things are out of date: <code>yarn outdated</code>:</p><p><img src="../static/blog/2018-09-15-ivan-drago-and-definitely-typed/yarn-outdated.png" alt="Screenshot of outdated dependencies in yarn"/></p><p>So lovely.</p><h2>You Were Already Broken - I Just Showed You How</h2><p>Before I finish I wanted to draw out one reason why breaking changes can be a reason for happiness. Because sometimes your code is wrong. An update to a type definition may highlight that. This is analogous to when the TypeScript compiler ships a new version. When I upgrade to a newer version of TypeScript it lights up errors in my codebase that I hadn&#x27;t spotted. Yay compiler!</p><p>An example of this is <a href="https://github.com/DefinitelyTyped/DefinitelyTyped/pull/28868">a PR I submitted to DefinitelyTyped earlier this week</a>. This PR changed how <code>react-router</code> models the parameters of a <code>Match</code>. Until now, an object was expected; the user could define any object they liked. However, <code>react-router</code> will only produce <code>string</code> values for a parameter. <a href="https://github.com/ReactTraining/react-router/blob/34ff1f8077d95edf01e9d5ca8ea4708b8d0290e2/packages/react-router/modules/matchPath.js#L36">If you look at the underlying code it&#x27;s nothing more than an <code>exec</code> on a regular expression.</a></p><p>My PR enforces this at type level by changing this:</p><pre><code class="language-ts">export interface match&lt;P&gt; {
  params: P;
  ...
}
</code></pre><p>To this</p><pre><code class="language-ts">export interface match&lt;Params extends { [K in keyof Params]?: string } = {}&gt; {
  params: Params;
</code></pre><p>So any object definition supplied must have <code>string</code> values (and you don&#x27;t actually need to supply an object definition; that&#x27;s optional now).</p><p>I expected this PR to break people <a href="https://github.com/DefinitelyTyped/DefinitelyTyped/issues/28894">and it did</a>. But this is a useful break. If they were relying upon their parameters to be types other than strings they would be experiencing some unexpected behaviour. In fact, it&#x27;s exactly this that prompted my PR in the first place. A colleague had defined his parameters as <code>number</code>s and couldn&#x27;t understand why they weren&#x27;t behaving like <code>number</code>s. Because they weren&#x27;t <code>number</code>s! And wonderfully, this will now be caught at compile time; not runtime. Yay!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using TypeScript and webpack alias: goodbye relative paths]]></title>
            <link>https://blog.johnnyreilly.com/2018/08/21/typescript-webpack-alias-goodbye-relative-paths</link>
            <guid>Using TypeScript and webpack alias: goodbye relative paths</guid>
            <pubDate>Tue, 21 Aug 2018 00:00:00 GMT</pubDate>
            <description><![CDATA[This post shows how you can use TypeScript with webpack alias to move away from using relative paths in your import statements.]]></description>
            <content:encoded><![CDATA[<p>This post shows how you can use TypeScript with webpack <code>alias</code> to move away from using relative paths in your <code>import</code> statements.</p><h2>Long relative paths</h2><p>I write a lot of TypeScript. Because I like modularity, I split up my codebases into discreet modules and <code>import</code> from them as necessary.</p><p>Take a look at this <code>import</code>:</p><pre><code class="language-ts">import * as utils from &#x27;../../../../../../../shared/utils&#x27;;
</code></pre><p>Now take a look at this import:</p><pre><code class="language-ts">import * as utils from &#x27;shared/utils&#x27;;
</code></pre><p>Which do you prefer? If the answer was &quot;the first&quot; then read no further. You have all you need, go forth and be happy. If the answer was &quot;the second&quot; then stick around; I can help!</p><h2>TypeScript</h2><p>There&#x27;s been a solution for this in TypeScript-land for some time. You can read the detail <a href="https://www.typescriptlang.org/docs/handbook/module-resolution.html#path-mapping">in the &quot;path mapping&quot; docs here</a>.</p><p>Let&#x27;s take a slightly simpler example; we have a folder structure that looks like this:</p><pre><code class="language-console">projectRoot
├── components
│ └── page.tsx (imports &#x27;../shared/utils&#x27;)
├── shared
│ ├── folder1
│ └── folder2
│ └── utils.ts
└── tsconfig.json
</code></pre><p>We would like <code>page.tsx</code> to import <code>&#x27;shared/utils&#x27;</code> instead of <code>&#x27;../shared/utils&#x27;</code>. We can, if we augment our <code>tsconfig.json</code> with the following properties:</p><pre><code class="language-json">{
  &quot;compilerOptions&quot;: {
    &quot;baseUrl&quot;: &quot;.&quot;,
    &quot;paths&quot;: {
      &quot;components/*&quot;: [&quot;components/*&quot;],
      &quot;shared/*&quot;: [&quot;shared/*&quot;]
    }
  }
}
</code></pre><p>Then we can use option 2. We can happily write:</p><pre><code class="language-ts">import * as utils from &#x27;shared/utils&#x27;;
</code></pre><p>My code compiles, yay.... Ship it!</p><p>Let&#x27;s not get over-excited. Actually, we&#x27;re only part-way there; you can compile this code with the TypeScript compiler.... But is that enough?</p><p>I bundle my TypeScript with <a href="https://github.com/TypeStrong/ts-loader">ts-loader</a> and webpack. If I try and use my new exciting import statement above with my build system then disappointment is in my future. webpack will be all like &quot;import whuuuuuuuut?&quot;</p><p>You see, webpack doesn&#x27;t know what we told the TypeScript compiler in the <code>tsconfig.json</code>. Why would it? It was our little secret.</p><h2>webpack <code>resolve.alias</code> to the rescue!</h2><p>This same functionality has existed in webpack for a long time; actually much longer than it has existed in TypeScript. It&#x27;s the <a href="https://webpack.js.org/configuration/resolve/#resolve-alias"><code>resolve.alias</code></a> functionality.</p><p>So, looking at that I should be able to augment my <code>webpack.config.js</code> like so:</p><pre><code class="language-js">module.exports = {
  //...
  resolve: {
    alias: {
      components: path.resolve(process.cwd(), &#x27;components/&#x27;),
      shared: path.resolve(process.cwd(), &#x27;shared/&#x27;),
    },
  },
};
</code></pre><p>And now both webpack and TypeScript are up to speed with how to resolve modules.</p><h2>DRY with the <a href="https://github.com/dividab/tsconfig-paths-webpack-plugin"><code>tsconfig-paths-webpack-plugin</code></a></h2><p>When I look at the <code>tsconfig.json</code> and the <code>webpack.config.js</code> something occurs to me: I don&#x27;t like to repeat myself. As well as that, I don&#x27;t like to repeat myself. It&#x27;s so... Repetitive.</p><p>The declarations you make in the <code>tsconfig.json</code> are re-stated in the <code>webpack.config.js</code>. Who wants to maintain two sets of code where one would do? Not me.</p><p>Fortunately, you don&#x27;t have to. There&#x27;s the <a href="https://github.com/dividab/tsconfig-paths-webpack-plugin"><code>tsconfig-paths-webpack-plugin</code></a> for webpack which will do the job for you. You can replace your verbose <code>resolve.alias</code> with this:</p><pre><code class="language-ts">module.exports = {
  //...
  resolve: {
    plugins: [
      new TsconfigPathsPlugin({
        /*configFile: &quot;./path/to/tsconfig.json&quot; */
      }),
    ],
  },
};
</code></pre><p>This does the hard graft of reading your <code>tsconfig.json</code> and translating path mappings into webpack <code>alias</code>es. From this point forward, you need only edit the <code>tsconfig.json</code> and everything else will just work.</p><p>Thanks to <a href="https://github.com/jonaskello">Jonas Kello</a>, author of the plugin; it&#x27;s tremendous! Thanks also to <a href="https://twitter.com/TheLarkInn">Sean Larkin</a> and <a href="https://github.com/s-panferov">Stanislav Panferov</a> (of <a href="https://github.com/s-panferov/awesome-typescript-loader">awesome-typescript-loader</a>) who together worked on the original plugin that I understand the <code>tsconfig-paths-webpack-plugin</code> is based on. Great work!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Azure App Service: nested configuration for ASP.NET running in Web App for Containers using Application Settings]]></title>
            <link>https://blog.johnnyreilly.com/2018/07/28/azure-app-service-web-app-containers-asp-net-nested-configuration</link>
            <guid>Azure App Service: nested configuration for ASP.NET running in Web App for Containers using Application Settings</guid>
            <pubDate>Sat, 28 Jul 2018 00:00:00 GMT</pubDate>
            <description><![CDATA[How can we configure an ASP.NET application with nested properties Azure App Service Web App for Containers using Application Settings in Azure? Colons don't work.]]></description>
            <content:encoded><![CDATA[<p>How can we configure an ASP.NET application with nested properties <a href="https://azure.microsoft.com/en-gb/services/app-service/containers/">Azure App Service Web App for Containers</a> using Application Settings in Azure? Colons don&#x27;t work.</p><h2>Containers on App Service</h2><p>App Services have long been a super simple way to spin up a web app in Azure. The barrier to entry is low, maintenance is easy. It just works. App Services recently got a turbo boost in the form of <a href="https://docs.microsoft.com/en-us/azure/app-service/containers/app-service-linux-intro">Azure App Service on Linux</a>. Being able to deploy to Linux is exciting enough; but another reason this is notable because <a href="https://docs.microsoft.com/en-us/azure/app-service/containers/tutorial-custom-docker-image">you can deploy Docker images that will be run as app services</a>.</p><p>I cannot over-emphasise just how easy this makes getting a Docker image into Production. Yay Azure!</p><h2>The Mystery of Configuration</h2><p>Applications need configuration. ASP.Net Core applications are typically configured by an <code>appsettings.json</code> file which might look like so:</p><pre><code class="language-json">{
  &quot;Parent&quot;: {
    &quot;ChildOne&quot;: &quot;I&#x27;m a little teapot&quot;,
    &quot;ChildTwo&quot;: &quot;Short and stout&quot;
  }
}
</code></pre><p>With a classic App Service you could override a setting in the <code>appsettings.json</code> by updating &quot;Application settings&quot; within the Azure portal. You&#x27;d do this in the style of creating an Application setting called <code>Parent:ChildOne</code> or <code>Parent:ChildTwo</code>. To be clear: using colons to target a specific piece of config.</p><p><img src="../static/blog/2018-07-28-azure-app-service-web-app-containers-asp-net-nested-configuration/appservice_classic.png" alt="screenshot of an App Service Application Settings in the Azure Portal, nested properties configured using colons"/></p><p>You can read about this approach <a href="https://blogs.msdn.microsoft.com/waws/2018/06/12/asp-net-core-settings-for-azure-app-service/">here</a>. Now there&#x27;s something I want you to notice; consider the colons below:</p><p><img src="../static/blog/2018-07-28-azure-app-service-web-app-containers-asp-net-nested-configuration/appservice_colons_fine.png" alt="screenshot of an App Service specific Application Setting nested property configured using colons - all good"/></p><p>If you try and follow the same steps when you&#x27;re using Web App for Containers / i.e. <a href="https://docs.microsoft.com/en-us/azure/app-service/containers/app-service-linux-intro">a Docker image deployed to an Azure App Service on Linux </a> you <strong>cannot</strong> use colons:</p><p><img src="../static/blog/2018-07-28-azure-app-service-web-app-containers-asp-net-nested-configuration/appservice_container_colons_bad.png" alt="screenshot of a Web App for Containers specific Application Setting nested property configured using colons - errors"/></p><p>When you hover over the error you see this message: <code>This field can only contain letters, numbers (0-9), periods (&quot;.&quot;), and underscores (&quot;_&quot;)</code>. Using <code>.</code> does not work alas.</p><h2>How do we configure without colons?</h2><p>It&#x27;s simple. Where you would use <code>:</code> on a classic App Service, you should use a <code>__</code> (double underscore) on an App Service with containers. So <code>Parent__ChildOne</code> instead of <code>Parent:ChildOne</code>. It&#x27;s as simple as that.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Cypress and Auth0]]></title>
            <link>https://blog.johnnyreilly.com/2018/07/09/cypress-and-auth0</link>
            <guid>Cypress and Auth0</guid>
            <pubDate>Mon, 09 Jul 2018 00:00:00 GMT</pubDate>
            <description><![CDATA[Cypress is a fantastic way to write UI tests for your web apps. Just world class. Wait, no. Galaxy class. I'm going to go one further: universe class. You get my drift.]]></description>
            <content:encoded><![CDATA[<p><a href="https://www.cypress.io/">Cypress</a> is a fantastic way to write UI tests for your web apps. Just world class. Wait, no. Galaxy class. I&#x27;m going to go one further: universe class. You get my drift.</p><p>Here&#x27;s a pickle for you. You have functionality that lies only behind the walled garden of authentication. You want to write tests for these capabilities. Assuming that authentication takes place within your application that&#x27;s no great shakes. Authentication is part of your app; it&#x27;s no big deal using Cypress to automate logging in.</p><p>Auth is a serious business and, as Cypress is best in class for UI testing, I&#x27;ll say that Auth0 is romping home with the same title in the auth-as-a-service space. My app is using Auth0 for authentication. What&#x27;s important to note about this is the flow. Typically when using auth-as-a-service, the user is redirected to the auth provider&#x27;s site to authenticate and then be redirected back to the application post-login.</p><p><a href="https://github.com/brian-mann">Brian Mann</a> (of Cypress fame) has been <a href="https://github.com/cypress-io/cypress/issues/1342#issuecomment-366747803">fairly clear when talking about testing with this sort of authentication flow</a>:</p><blockquote><p>You&#x27;re trying to test SSO - and we have recipes showing you exactly how to do this.</p><p>Also best practice is never to visit or test 3rd party sites not under your control. You don&#x27;t control <code>microsoftonline</code>, so there&#x27;s no reason to use the UI to test this. You can programmatically test the integration between it and your app with <code>cy.request</code> <!-- -->-<!-- --> which is far faster, more reliable, and still gives you 100% confidence.</p></blockquote><p>I want to automate logging into Auth0 from my Cypress tests. But hopefully in a good way. Not a bad way. Wouldn&#x27;t want to make Brian sad.</p><h2>Commanding Auth0</h2><p>To automate our login, we&#x27;re going to use the <a href="https://github.com/auth0/auth0.js">auth0-js client library</a>. This is the same library the application uses; but we&#x27;re going to do something subtly different with it.</p><p>The application uses <a href="https://github.com/auth0/auth0.js#api"><code>authorize</code></a> to log users in. This function redirects the user into the Auth0 lock screen, and then, post authentication, redirects the user back to the application with a token in the URL. The app parses the token (using the auth0 client library) and sets the token and the expiration of said token in the browser sessionStorage.</p><p>What we&#x27;re going to do is automate our login by using <code>login</code> instead. First of all, we need to add <code>auth0-js</code> as a dependency of our e2e tests:</p><pre><code class="language-js">yarn add auth0-js --dev
</code></pre><p>Next, we&#x27;re going to create ourselves a custom command called loginAsAdmin:</p><pre><code class="language-js">const auth0 = require(&#x27;auth0-js&#x27;);

Cypress.Commands.add(&#x27;loginAsAdmin&#x27;, (overrides = {}) =&gt; {
  Cypress.log({
    name: &#x27;loginAsAdminBySingleSignOn&#x27;,
  });

  const webAuth = new auth0.WebAuth({
    domain: &#x27;my-super-duper-domain.eu.auth0.com&#x27;, // Get this from https://manage.auth0.com/#/applications and your application
    clientID: &#x27;myclientid&#x27;, // Get this from https://manage.auth0.com/#/applications and your application
    responseType: &#x27;token id_token&#x27;,
  });

  webAuth.client.login(
    {
      realm: &#x27;Username-Password-Authentication&#x27;,
      username: &#x27;mytestemail@something.co.uk&#x27;,
      password: &#x27;SoVeryVeryVery$ecure&#x27;,
      audience: &#x27;myaudience&#x27;, // Get this from https://manage.auth0.com/#/apis and your api, use the identifier property
      scope: &#x27;openid email profile&#x27;,
    },
    function (err, authResult) {
      // Auth tokens in the result or an error
      if (authResult &amp;&amp; authResult.accessToken &amp;&amp; authResult.idToken) {
        const token = {
          accessToken: authResult.accessToken,
          idToken: authResult.idToken,
          // Set the time that the access token will expire at
          expiresAt: authResult.expiresIn * 1000 + new Date().getTime(),
        };

        window.sessionStorage.setItem(
          &#x27;my-super-duper-app:storage_token&#x27;,
          JSON.stringify(token)
        );
      } else {
        console.error(&#x27;Problem logging into Auth0&#x27;, err);
        throw err;
      }
    }
  );
});
</code></pre><p>This command logs in using the <code>auth0-js</code> API and then sets the result into <code>sessionStorage</code> in the same way that our app does. This allows our app to read the value out of <code>sessionStorage</code> and use it. We&#x27;re also going to put together one other command:</p><pre><code class="language-js">Cypress.Commands.add(&#x27;visitHome&#x27;, (overrides = {}) =&gt; {
  cy.visit(&#x27;/&#x27;, {
    onBeforeLoad: (win) =&gt; {
      win.sessionStorage.clear();
    },
  });
});
</code></pre><p>This visits the root of our application and wipes the <code>sessionStorage</code>. This is necessary because Cypress doesn&#x27;t clear down <code>sessionStorage</code> between tests. (<a href="https://github.com/cypress-io/cypress/issues/413">That&#x27;s going to change though.</a>)</p><h2>Using It</h2><p>Let&#x27;s write a test that uses our new commands to see if it gets access to our admin functionality:</p><pre><code class="language-js">describe(&#x27;access secret admin functionality&#x27;, () =&gt; {
  it(&#x27;should be able to navigate to&#x27;, () =&gt; {
    cy.visitHome()
      .loginAsAdmin()
      .get(&#x27;[href=&quot;/secret-adminny-stuff&quot;]&#x27;) // This link should only be visible to admins
      .click()
      .url()
      .should(&#x27;contain&#x27;, &#x27;secret-adminny-stuff/&#x27;); // non-admins should be redirected away from this url
  });
});
</code></pre><p>Well, the test looks good but it&#x27;s failing. If I fire up the Chrome Dev Tools in Cypress (did I mention that Cypress is absolutely fabulous?) then I see this response tucked away in the network tab:</p><pre><code class="language-json">{error: &quot;unauthorized_client&quot;,…} error : &quot;unauthorized_client&quot; error_description : &quot;Grant type &#x27;http://auth0.com/oauth/grant-type/password-realm&#x27; not allowed for the client.&quot;
</code></pre><p>Hmmm... So sad. If you go to <a href="https://manage.auth0.com/#/applications">https://manage.auth0.com/#/applications</a>, select your application, <code>Show Advanced Settings</code> and <code>Grant Types</code> you&#x27;ll see a <code>Password</code> option is unselected.</p><p>Select it, Save Changes and try again.</p><p><img src="../static/blog/2018-07-09-cypress-and-auth0/auth0-enable-password-grant-type.png"/></p><p>You now have a test which automates your Auth0 login using Cypress and goes on to test your application functionality with it!</p><h2>One More Thing...</h2><p>It&#x27;s worth saying that it&#x27;s worth setting up different tenants in Auth0 to support your testing scenarios. This is generally a good idea so you can separate your testing accounts from Production accounts. Further to that, you don&#x27;t need to have your Production setup supporting the <code>Password``Grant Type</code>.</p><p>Also, if you&#x27;re curious about what the application under test is like then read <a href="./2018-01-14-auth0-typescript-and-aspnet-core.md">this</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[VSTS and EF Core Migrations]]></title>
            <link>https://blog.johnnyreilly.com/2018/06/24/vsts-and-ef-core-migrations</link>
            <guid>VSTS and EF Core Migrations</guid>
            <pubDate>Sun, 24 Jun 2018 00:00:00 GMT</pubDate>
            <description><![CDATA[Let me start by telling you a dirty secret. I have an ASP.Net Core project that I build with VSTS. It is deployed to Azure through a CI / CD setup in VSTS. That part I'm happy with. Proud of even. Now to the sordid hiddenness: try as I might, I've never found a nice way to deploy Entity Framework database migrations as part of the deployment flow. So I have [blushes with embarrassment] been using the Startup of my ASP.Net core app to run the migrations on my database. There. I said it. You all know. Absolutely filthy. Don't judge me.]]></description>
            <content:encoded><![CDATA[<p>Let me start by telling you a dirty secret. I have an ASP.Net Core project that I build with VSTS. It is deployed to Azure through a CI / CD setup in VSTS. That part I&#x27;m happy with. Proud of even. Now to the sordid hiddenness: try as I might, I&#x27;ve never found a nice way to deploy Entity Framework database migrations as part of the deployment flow. So I have <!-- -->[blushes with embarrassment]<!-- --> been using the <code>Startup</code> of my ASP.Net core app to run the migrations on my database. There. I said it. You all know. Absolutely filthy. Don&#x27;t judge me.</p><p>If you care to google, you&#x27;ll find various discussions around this, and various ways to tackle it. Most of which felt like too much hard work and so I never attempted.</p><p>It&#x27;s also worth saying that being on VSTS made me less likely to give these approaches a go. Why? Well, the feedback loop for debugging a CI / CD setup is truly sucky. Make a change. Wait for it to trickle through the CI / CD flow (10 mins at least). Spot a problem, try and fix. Start waiting again. Repeat until you succeed. Or, if you&#x27;re using the free tier of VSTS, repeat until you run out of build minutes. You have a limited number of build minutes per month with VSTS. Last time I fiddled with the build, I bled my way through a full month&#x27;s minutes in 2 days. I have now adopted the approach of only playing with the setup in the last week of the month. That way if I end up running out of minutes, at least I&#x27;ll roll over to the new allowance in a matter of days.</p><p>Digression over. I could take the guilt of my EF migrations secret no longer, I decided to try and tackle it another way. I used the approach suggested by <a href="https://github.com/broersa">Andre Broers</a><a href="https://github.com/aspnet/EntityFrameworkCore/issues/9841#issuecomment-395712061">here</a>:</p><blockquote><p>I worked around by adding a dotnetcore consoleapp project where I run the migration via the Context. In the Build I build this consoleapp in the release I execute it.</p></blockquote><h2>Console Yourself</h2><p>First things first, we need a console app added to our solution. Fire up PowerShell in the root of your project and:</p><pre><code class="language-console">md MyAwesomeProject.MigrateDatabase
cd .\MyAwesomeProject.MigrateDatabase\
dotnet new console
</code></pre><p>Next we need that project to know about Entity Framework and also our DbContext (which I store in a dedicated project):</p><pre><code class="language-console">dotnet add package Microsoft.EntityFrameworkCore.Design
dotnet add package Microsoft.EntityFrameworkCore.SqlServer
dotnet add reference ..\MyAwesomeProject.Database\MyAwesomeProject.Database.csproj
</code></pre><p>Add our new project to our solution: (I always forget to do this)</p><pre><code class="language-console">cd ../
dotnet sln add .\MyAwesomeProject.MigrateDatabase\MyAwesomeProject.MigrateDatabase.csproj
</code></pre><p>You should now be the proud possessor of a <code>.csproj</code> file that looks like this:</p><pre><code class="language-xml">&lt;Project Sdk=&quot;Microsoft.NET.Sdk&quot;&gt;

  &lt;PropertyGroup&gt;
    &lt;OutputType&gt;Exe&lt;/OutputType&gt;
    &lt;TargetFramework&gt;netcoreapp2.1&lt;/TargetFramework&gt;
  &lt;/PropertyGroup&gt;

  &lt;ItemGroup&gt;
    &lt;PackageReference Include=&quot;Microsoft.EntityFrameworkCore.Design&quot; Version=&quot;2.1.1&quot; /&gt;
    &lt;PackageReference Include=&quot;Microsoft.EntityFrameworkCore.SqlServer&quot; Version=&quot;2.1.1&quot; /&gt;
  &lt;/ItemGroup&gt;

  &lt;ItemGroup&gt;
    &lt;ProjectReference Include=&quot;..\MyAwesomeProject.Database\MyAwesomeProject.Database.csproj&quot; /&gt;
  &lt;/ItemGroup&gt;

&lt;/Project&gt;
</code></pre><p>Replace the contents of the <code>Program.cs</code> file with this:</p><pre><code class="language-cs">using System;
using System.IO;
using MyAwesomeProject.Database;
using Microsoft.EntityFrameworkCore;

namespace MyAwesomeProject.MigrateDatabase {
    class Program {
        // Example usage:
        // dotnet MyAwesomeProject.MigrateDatabase.dll &quot;Server=(localdb)\\mssqllocaldb;Database=MyAwesomeProject;Trusted_Connection=True;&quot;
        static void Main(string[] args) {
            if (args.Length == 0)
                throw new Exception(&quot;No connection string supplied!&quot;);

            var myAwesomeProjectConnectionString = args[0];

            // Totally optional debug information
            Console.WriteLine(&quot;About to migrate this database:&quot;);
            var connectionBits = myAwesomeProjectConnectionString.Split(&quot;;&quot;);
            foreach (var connectionBit in connectionBits) {
                if (!connectionBit.StartsWith(&quot;Password&quot;, StringComparison.CurrentCultureIgnoreCase))
                    Console.WriteLine(connectionBit);
            }

            try {
                var optionsBuilder = new DbContextOptionsBuilder&lt;MyAwesomeProjectContext&gt;();
                optionsBuilder.UseSqlServer(myAwesomeProjectConnectionString);

                using(var context = new MyAwesomeProjectContext(optionsBuilder.Options)) {
                    context.Database.Migrate();
                }
                Console.WriteLine(&quot;This database is migrated like it&#x27;s the Serengeti!&quot;);
            } catch (Exception exc) {
                var failedToMigrateException = new Exception(&quot;Failed to apply migrations!&quot;, exc);
                Console.WriteLine($&quot;Didn&#x27;t succeed in applying migrations: {exc.Message}&quot;);
                throw failedToMigrateException;
            }
        }
    }
}
</code></pre><p>This code takes the database connection string passed as an argument, spins up a db context with that, and migrates like it&#x27;s the Serengeti.</p><h2>Build It!</h2><p>The next thing we need is to ensure that this is included as part of the build process in VSTS. The following commands need to be run during the build to include the MigrateDatabase project in the build output in a <code>MigrateDatabase</code> folder:</p><pre><code class="language-cs">cd MyAwesomeProject.MigrateDatabase
dotnet build
dotnet publish --configuration Release --output $(build.artifactstagingdirectory)/MigrateDatabase
</code></pre><p>There&#x27;s various ways to accomplish this which I wont reiterate now. <a href="./2018-06-16-vsts-yaml-up.md">I recommend YAML</a>.</p><h2>Deploy It!</h2><p>Now to execute our console app as part of the deployment process we need to add a CommandLine task to our VSTS build definition. It should execute the following command:</p><pre><code class="language-cs">dotnet MyAwesomeProject.MigrateDatabase.dll &quot;$(ConnectionStrings.MyAwesomeProjectDatabaseConnection)&quot;
</code></pre><p>In the following folder:</p><pre><code class="language-cs">$(System.DefaultWorkingDirectory)/my-awesome-project-YAML/drop/MigrateDatabase
</code></pre><p>Do note that the command uses the <code>ConnectionStrings.MyAwesomeProjectDatabaseConnection</code> variable which you need to create and set to the value of your connection string.</p><p><img src="../static/blog/2018-06-24-vsts-and-ef-core-migrations/Screenshot%2B2018-06-24%2B10.55.27.png"/></p><h2>Give It A Whirl</h2><p>Let&#x27;s find out what happens when the rubber hits the road. I&#x27;ll add a new entity to my database project:</p><pre><code class="language-cs">using System;

namespace MyAwesomeProject.Database.Entities {
    public class NewHotness {
        public Guid NewHotnessId { get; set; }
    }
}
</code></pre><p>And reference it in my DbContext:</p><pre><code class="language-cs">using MyAwesomeProject.Database.Entities;
using Microsoft.EntityFrameworkCore;

namespace MyAwesomeProject.Database {
    public class MyAwesomeProjectContext : DbContext {
        public MyAwesomeProjectContext(DbContextOptions&lt;MyAwesomeProjectContext&gt; options) : base(options) { }

        // ...

        public DbSet&lt;NewHotness&gt; NewHotnesses { get; set; }

        // ...
    }
}
</code></pre><p>Let&#x27;s let EF know by adding a migration to my project:</p><pre><code class="language-cs">dotnet ef migrations add TestOurMigrationsApproach
</code></pre><p>Commit my change, push it to VSTS, wait for the build to run and a deployment to take place.... Okay. It&#x27;s done. Looks good.</p><p><img src="../static/blog/2018-06-24-vsts-and-ef-core-migrations/Screenshot%2B2018-06-24%2B09.02.22.png"/></p><p>Let&#x27;s take a look in the database:</p><pre><code class="language-console">select * from NewHotnesses
go
</code></pre><p><img src="../static/blog/2018-06-24-vsts-and-ef-core-migrations/Screenshot%2B2018-06-24%2B08.59.00.png"/></p><p>It&#x27;s there! We are migrating our database upon deployment; and not in our ASP.Net Core app itself. I feel a burden lifted.</p><h2>Wrapping Up</h2><p>The EF Core team are aware of the lack of guidance around deploying migrations and have recently announced plans to fix that in the docs. You can track the progress of this issue <a href="https://github.com/aspnet/EntityFramework.Docs/issues/691">here</a>. There&#x27;s good odds that once they come out with this I&#x27;ll find there&#x27;s a better way than the approach I&#x27;ve outlined in this post. Until that glorious day!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[VSTS... YAML up!]]></title>
            <link>https://blog.johnnyreilly.com/2018/06/16/vsts-yaml-up</link>
            <guid>VSTS... YAML up!</guid>
            <pubDate>Sat, 16 Jun 2018 00:00:00 GMT</pubDate>
            <description><![CDATA[For the longest time I've been using the likes of Travis and AppVeyor to build open source projects that I work on. They rock. I've also recently been dipping my toes back in the water of Visual Studio Team Services. VSTS offers a whole stack of stuff, but my own area of interest has been the Continuous Integration / Continuous Deployment offering.]]></description>
            <content:encoded><![CDATA[<p>For the longest time I&#x27;ve been using the likes of <a href="https://travis-ci.org/">Travis</a> and <a href="https://www.appveyor.com/">AppVeyor</a> to build open source projects that I work on. They rock. I&#x27;ve also recently been dipping my toes back in the water of <a href="https://www.visualstudio.com/team-services/">Visual Studio Team Services</a>. VSTS offers a whole stack of stuff, but my own area of interest has been the Continuous Integration / Continuous Deployment offering.</p><p>Historically I have been underwhelmed by the CI proposition of Team Foundation Server / VSTS. It was difficult to debug, difficult to configure, difficult to understand. If it worked... Great! If it didn&#x27;t (and it often didn&#x27;t), you were toast. But things done changed! I don&#x27;t know when it happened, but VSTS is now super configurable. You add tasks / configure them, build and you&#x27;re done! It&#x27;s really nice.</p><p>However, there&#x27;s been something I&#x27;ve been missing from Travis, AppVeyor et al. Keeping my build script with my code. Travis has <code>.travis.yml</code>, AppVeyor has <code>appveyor.yml</code>. VSTS, what&#x27;s up?</p><h2>The New Dawn</h2><p>Up until now, really not much. It just wasn&#x27;t possible. Until it was:</p><blockquote><p>If you prefer a build definition in YAML then we’re currently hard at work on that. You can enable it as a preview feature: <a href="https://t.co/hau9Sv8brf">https://t.co/hau9Sv8brf</a></p><p>— Martin Woodward (@martinwoodward) <a href="https://twitter.com/martinwoodward/status/970250739510534144?ref_src=twsrc%5Etfw">March 4, 2018</a></p></blockquote><script src="https://platform.twitter.com/widgets.js" charSet="utf-8"></script><p>When I started testing it out I found things to like and some things I didn&#x27;t understand. Crucially, my CI now builds based upon <code>.vsts-ci.yml</code>. YAML baby!</p><h2>It Begins!</h2><p>You can get to &quot;Hello World&quot; by looking at <a href="https://docs.microsoft.com/en-us/vsts/pipelines/build/yaml?view=vsts">the docs here</a> and <a href="https://github.com/Microsoft/vsts-agent/blob/master/docs/preview/yamlgettingstarted.md">the examples here</a>. But what you really want is your existing build, configured in the UI, exported to YAML. That doesn&#x27;t seem to quite exist, but there&#x27;s something that gets you part way. Take a look:</p><p><img src="../static/blog/2018-06-16-vsts-yaml-up/vsts-screenshot-of-restore-task.png" alt="screenshot of restore task in VSTS"/></p><p>If you notice, in the top right of the screen, each task now allows you click on a new &quot;View YAML&quot; button. It&#x27;s kinda <a href="https://en.wikipedia.org/wiki/Ronseal">Ronseal</a>:</p><p><img src="../static/blog/2018-06-16-vsts-yaml-up/vsts-screenshot-of-copy-to-clipboard.png" alt="screenshot of copy to clipboard in VSTS"/></p><p>Using this hotness you can build yourself a <code>.vsts-ci.yml</code> file task by task.</p><h2>A Bump in the Road</h2><p>If you look closely at the message above you&#x27;ll see there&#x27;s a message about an undefined variable.</p><pre><code class="language-yml">#Your build definition references an undefined variable named ‘Parameters.RestoreBuildProjects’. Create or edit the build definition for this YAML file, define the variable on the Variables tab. See https://go.microsoft.com/fwlink/?linkid=865972
steps:
  - task: DotNetCoreCLI@2
    displayName: Restore
    inputs:
      command: restore
      projects: &#x27;$(Parameters.RestoreBuildProjects)&#x27;
</code></pre><p>Try as I might, I couldn&#x27;t locate <code>Parameters.RestoreBuildProjects</code>. So no working CI build for me. Then I remembered <a href="https://github.com/zerdos">Zoltan Erdos</a>. He&#x27;s hard to forget. Or rather, I remembered an idea of his which I will summarise thusly: &quot;Have a <code>package.json</code> in the root of your repo, use the <code>scripts</code> for individual tasks and you have a cross platform task runner&quot;.</p><p>This is a powerful idea and one I decided to put to work. My project is React and TypeScript on the front end, and ASP.Net Core on the back. I wanted a <code>package.json</code> in the root of the repo which I could install dependencies, build, test and publish my whole app. I could call into that from my <code>.vsts-ci.yml</code> file. Something like this:</p><pre><code class="language-json">{
  &quot;name&quot;: &quot;my-amazing-project&quot;,
  &quot;version&quot;: &quot;1.0.0&quot;,
  &quot;author&quot;: &quot;John Reilly &lt;johnny_reilly@hotmail.com&gt;&quot;,
  &quot;license&quot;: &quot;MIT&quot;,
  &quot;private&quot;: true,
  &quot;scripts&quot;: {
    &quot;preinstall&quot;: &quot;yarn run install:clientapp &amp;&amp; yarn run install:web&quot;,
    &quot;install:clientapp&quot;: &quot;cd MyAmazingProject.ClientApp &amp;&amp; yarn install&quot;,
    &quot;install:web&quot;: &quot;dotnet restore&quot;,
    &quot;prebuild&quot;: &quot;yarn install&quot;,
    &quot;build&quot;: &quot;yarn run build:clientapp &amp;&amp; yarn run build:web&quot;,
    &quot;build:clientapp&quot;: &quot;cd MyAmazingProject.ClientApp &amp;&amp; yarn run build&quot;,
    &quot;build:web&quot;: &quot;dotnet build --configuration Release&quot;,
    &quot;postbuild&quot;: &quot;yarn test&quot;,
    &quot;test&quot;: &quot;yarn run test:clientapp &amp;&amp; yarn run test:web&quot;,
    &quot;test:clientapp&quot;: &quot;cd MyAmazingProject.ClientApp &amp;&amp; yarn test&quot;,
    &quot;test:web&quot;: &quot;cd MyAmazingProject.Web.Tests &amp;&amp; dotnet test&quot;,
    &quot;publish:web&quot;: &quot;cd MyAmazingProject.Web &amp;&amp; dotnet publish MyAmazingProject.Web.csproj --configuration Release&quot;
  }
}
&lt;/johnny_reilly@hotmail.com&gt;
</code></pre><p>It doesn&#x27;t matter if I have &quot;an undefined variable named ‘Parameters.RestoreBuildProjects’&quot;. I now have no need to use all the individual tasks in a build. I can convert them into a couple of scripts in my <code>package.json</code>. So here&#x27;s where I&#x27;ve ended up for now. I&#x27;ve a <code>.vsts-ci.yml</code> file which looks like this:</p><pre><code class="language-yml">queue: Hosted VS2017

steps:
  - task: geeklearningio.gl-vsts-tasks-yarn.yarn-installer-task.YarnInstaller@2
    displayName: install yarn itself
    inputs:
      checkLatest: true
  - task: geeklearningio.gl-vsts-tasks-yarn.yarn-task.Yarn@2
    displayName: yarn build and test
    inputs:
      Arguments: build
  - task: geeklearningio.gl-vsts-tasks-yarn.yarn-task.Yarn@2
    displayName: yarn publish:web
    inputs:
      Arguments: &#x27;run publish:web --output $(build.artifactstagingdirectory)/MyAmazingProject&#x27;
  - task: PublishBuildArtifacts@1
    displayName: publish build artifact
    inputs:
      PathtoPublish: &#x27;$(build.artifactstagingdirectory)&#x27;
</code></pre><p>This file does the following:</p><ol><li>Installs yarn. (By the way VSTS, what&#x27;s with not having yarn installed by default? I&#x27;ll say this for the avoidance of doubt: in the npm cli space: yarn has won.)</li><li>Install our dependencies, build the front end and back end, run all the tests. Effectively <code>yarn build</code>.</li><li>Publish our web app to a directory. Effectively <code>yarn run publish:web</code>. This is only separate because we want to pass in the output directory and so it&#x27;s just easier for it to be a separate step.</li><li>Publish the build artefact to TFS. (This will go on to be picked up by the continuous deployment mechanism and published out to Azure.)</li></ol><p>I much prefer this to what I had before. I feel there&#x27;s much more that can be done here as well. I&#x27;m looking forward to the continuous deployment piece becoming scriptable too.</p><p>Thanks to Zoltan and props to the TFVS team!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Compromising: A Guide for Developers]]></title>
            <link>https://blog.johnnyreilly.com/2018/05/13/compromising-guide-for-developers</link>
            <guid>Compromising: A Guide for Developers</guid>
            <pubDate>Sun, 13 May 2018 00:00:00 GMT</pubDate>
            <description><![CDATA[It is a truth universally acknowledged, that a single developer, will not be short of an opinion. Opinions on tabs vs spaces. Upon OOP vs FP. Upon classes vs functions. Just opinions, opinions, opinions. Opinions that are felt with all the sincerity of a Witchfinder General. And, alas, not always the same level of empathy.]]></description>
            <content:encoded><![CDATA[<p>It is a truth universally acknowledged, that a single developer, will not be short of an opinion. Opinions on tabs vs spaces. Upon OOP vs FP. Upon <code>class</code>es vs <code>function</code>s. Just opinions, opinions, opinions. Opinions that are felt with all the sincerity of a Witchfinder General. And, alas, not always the same level of empathy.</p><p>Given the wealth of strongly felt desires, it&#x27;s kind of amazing that developers ever manage to work together. It&#x27;s rare to find a fellow dev that agrees entirely with your predilections. So how do people ever get past the &quot;you don&#x27;t use semi-colons; what&#x27;s wrong with you&quot;? Well, not easily to be honest. It involves compromise.</p><h2>On Compromise</h2><p>We&#x27;ve all been in the position where we realise that there&#x27;s something we don&#x27;t like in a codebase. The ordering of members in a <code>class</code>, naming conventions, a lack of tests... Something.</p><p>Then comes the moment of trepidation. You suggest a change. You suggest difference. It&#x27;s time to find out if you&#x27;re working with psychopaths. It&#x27;s not untypical to find that you just have to go with the flow.</p><ul><li>&quot;You&#x27;ve been using 3 spaces?&quot;</li><li>&quot;Yes we use 3 spaces.&quot;</li><li>&quot;Okay... So we&#x27;ll be using 3 spaces...&quot; <!-- -->[backs away carefully]</li></ul><p>I&#x27;ve been in this position so many times I&#x27;ve learned to adapt. It helps that I&#x27;m a malleable sort anyway. But what if there were another way?</p><h2>Weighting Opinion</h2><p>Sometimes your opinion is... Well.... Just an opinion. Other opinions are legitimate. At least in theory. If you can acknowledge that, you already have a level of self knowledge not gifted to all in the dev community. If you&#x27;re able to get that far I feel there&#x27;s something you might want to consider.</p><p>Let me frame this up: there&#x27;s a choice to be made around an approach that could be used in a codebase. There are 2 camps in the team; 1 camp advocating for 1 approach. The other for a different approach. Either one is functionally legitimate. They work. It&#x27;s just a matter of preference of choice. How do you choose now? Let&#x27;s look at a technique for splitting the difference.</p><p>Voting helps. But let&#x27;s say 50% of the team wants 1 approach and 50% wants the other. What then? Or, to take a more interesting idea, what say 25% want 1 approach and 75% want the other? If it&#x27;s just 1 person, 1 vote then the 75% wins and that&#x27;s it.</p><p>But before we all move on, let&#x27;s consider another factor. How much do people care? What if the 25% are really, really invested in the choice they&#x27;re advocating for and the 75% just have a mild preference? From that point forwards the 25% are likely going to be less happy. Maybe they&#x27;ll even burn inside. They&#x27;re certainly going to be less productive.</p><p>It&#x27;s because of situations like this that weighting votes becomes useful. Out of 5, how much do you care? If one person cares &quot;5 out of 5&quot; and the other three are &quot;1 out of 5&quot;.... Well go with the 25% It matters to them and that it matters to them should matter to you.</p><p>I&#x27;ll contend that rolling like this makes for more content, happier and more productive teams. Making strength of feeling a factor in choices reduces friction and increases the peace.</p><p><img src="../static/blog/2018-05-13-compromising-guide-for-developers/Bestival_2008_Increase_the_Peace_banner.jpg"/></p><p>I&#x27;ve only recently discovered this technique and I can&#x27;t claim credit for it. I learned it from the awesome <a href="https://twitter.com/foldr">Jamie McCrindle</a>. I commend to you! Be happier!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using Reflection to Identify Unwanted Dependencies]]></title>
            <link>https://blog.johnnyreilly.com/2018/04/28/using-reflection-to-identify-unwanted-dependencies</link>
            <guid>Using Reflection to Identify Unwanted Dependencies</guid>
            <pubDate>Sat, 28 Apr 2018 00:00:00 GMT</pubDate>
            <description><![CDATA[I having a web app which is fairly complex. It's made up of services, controllers and all sorts of things. So far, so unremarkable. However, I needed to ensure that the controllers did not attempt to access the database via any of their dependencies. Or their dependencies, dependencies. Or their dependencies. You get my point.]]></description>
            <content:encoded><![CDATA[<p>I having a web app which is fairly complex. It&#x27;s made up of services, controllers and all sorts of things. So far, so unremarkable. However, I needed to ensure that the controllers did not attempt to access the database via any of their dependencies. Or their dependencies, dependencies. Or their dependencies. You get my point.</p><p>The why is not important here. What&#x27;s significant is the idea of walking a dependency tree and identifying, via a reflection based test, when such unwelcome dependencies occur, and where.</p><p>When they do occur the test should fail, like this:</p><pre><code class="language-sh">[xUnit.net 00:00:01.6766691]     My.Web.Tests.HousekeepingTests.My_Api_Controllers_do_not_depend_upon_the_database [FAIL]
[xUnit.net 00:00:01.6782295]       Expected dependsUponTheDatabase.Any() to be False because My.Api.Controllers.ThingyController depends upon the database through My.Data.Services.OohItsAService, but found True.
</code></pre><p>What follows is an example of how you can accomplish this. It is exceedingly far from the most beautiful code I&#x27;ve ever written. But it works. One reservation I have about it is that it doesn&#x27;t use the Dependency Injection mechanism used at runtime (AutoFac). If I had more time I would amend the code to use that instead; it would become an easier test to read if I did. Also it would better get round the limitations of the code below. Essentially the approach relies on the assumption of there being 1 interface and 1 implementation. That&#x27;s often not true in complex systems. But this is good enough to roll with for now.</p><pre><code class="language-cs">using System;
using System.Collections.Generic;
using System.Linq;
using System.Reflection;
using FluentAssertions;
using My.Data;
using My.Web.Controllers;
using Xunit;

namespace My.Web.Tests {
    public class OiYouThereGetOutTests {
        [Fact]
        public void My_Controllers_do_not_depend_upon_the_database() {
            var myConcreteTypes = GetMyAssemblies()
                .SelectMany(assembly =&gt; assembly.GetTypes())
                .ToArray();

            var controllerTypes = typeof(My.Web.Startup).Assembly.GetTypes()
                .Where(myWebType =&gt;
                    myWebType != typeof(Microsoft.AspNetCore.Mvc.Controller) &amp;&amp;
                    typeof(Microsoft.AspNetCore.Mvc.Controller).IsAssignableFrom(myWebType));

            foreach (var controllerType in controllerTypes) {
                var allTheTypes = GetDependentTypes(controllerType, myConcreteTypes);
                allTheTypes.Count.Should().BeGreaterThan(0);
                var dependsUponTheDatabase = allTheTypes.Where(keyValue =&gt; keyValue.Key == typeof(MyDbContext));
                dependsUponTheDatabase.Any().Should().Be(false, because: $&quot;{controllerType} depends upon the database through {string.Join(&quot;, &quot;, dependsUponTheDatabase.Select(dod =&gt; dod.Value))}&quot;);
            }
        }

        private static Dictionary&lt;Type, Type&gt; GetDependentTypes(Type type, Type[] typesToCheck, Dictionary&lt;Type, Type&gt; typesSoFar = null) {
            var types = typesSoFar ?? new Dictionary&lt;Type, Type&gt;();
            foreach (var constructor in type.GetConstructors().Where(ctor =&gt; ctor.IsPublic)) {
                foreach (var parameter in constructor.GetParameters()) {
                    if (parameter.ParameterType.IsInterface) {
                        if (parameter.ParameterType.IsGenericType) {
                            foreach (var genericType in parameter.ParameterType.GenericTypeArguments) {
                                AddIfMissing(types, genericType, type);
                            }
                        } else {
                            var typesImplementingInterface = TypesImplementingInterface(parameter.ParameterType, typesToCheck);
                            foreach (var typeImplementingInterface in typesImplementingInterface) {
                                AddIfMissing(types, typeImplementingInterface, type);
                                AddIfMissing(types, GetDependentTypes(typeImplementingInterface, typesToCheck, types).Keys.ToList(), type);
                            }
                        }
                    } else {
                        AddIfMissing(types, parameter.ParameterType, type);
                        AddIfMissing(types, GetDependentTypes(parameter.ParameterType, typesToCheck, types).Keys.ToList(), type);
                    }
                }
            }
            return types;
        }

        private static void AddIfMissing(Dictionary&lt;Type, Type&gt; types, Type typeToAdd, Type parentType) {
            if (!types.Keys.Contains(typeToAdd))
                types.Add(typeToAdd, parentType);
        }

        private static void AddIfMissing(Dictionary&lt;Type, Type&gt; types, IList&lt;Type&gt; typesToAdd, Type parentType) {
            foreach (var typeToAdd in typesToAdd) {
                AddIfMissing(types, typeToAdd, parentType);
            }
        }

        private static Type[] TypesImplementingInterface(Type interfaceType, Type[] typesToCheck) =&gt;
            typesToCheck.Where(type =&gt; !type.IsInterface &amp;&amp; interfaceType.IsAssignableFrom(type)).ToArray();

        private static bool IsRealClass(Type testType) =&gt;
            testType.IsAbstract == false &amp;&amp;
            testType.IsGenericType == false &amp;&amp;
            testType.IsGenericTypeDefinition == false &amp;&amp;
            testType.IsInterface == false;

        private static Assembly[] GetMyAssemblies() =&gt;
            AppDomain
            .CurrentDomain
            .GetAssemblies()
            // Not strictly necessary but it reduces the amount of types returned
            .Where(assembly =&gt; assembly.GetName().Name.StartsWith(&quot;My&quot;))
            .ToArray();
    }
}
</code></pre>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[It's Not Dead 2: mobx-react-devtools and the undead]]></title>
            <link>https://blog.johnnyreilly.com/2018/03/26/its-not-dead-2-mobx-react-devtools-and-the-undead</link>
            <guid>It's Not Dead 2: mobx-react-devtools and the undead</guid>
            <pubDate>Mon, 26 Mar 2018 00:00:00 GMT</pubDate>
            <description><![CDATA[I spent today digging through our webpack 4 config trying to work out why a production bundle contained code like this:]]></description>
            <content:encoded><![CDATA[<p>I spent today digging through our webpack 4 config trying to work out why a production bundle contained code like this:</p><pre><code class="language-js">if(&quot;production&quot;!==e.env.NODE_ENV){//...
</code></pre><p>My expectation was that with webpack 4 and <code>&#x27;mode&#x27;: &#x27;production&#x27;</code> this meant that behind the scenes all <code>process.env.NODE_ENV</code> statements should be converted to <code>&#x27;production&#x27;</code>. Subsequently Uglify would automatically get its groove on with the resulting <code>if(&quot;production&quot;!==&quot;production&quot;) ...</code> and et voilà!... Strip the dead code.</p><p>It seemed that was not the case. I was seeing (regrettably) undead code. And who here actually likes the undead?</p><h2>Who Betrayed Me?</h2><p>My beef was with webpack. It done did me wrong. Or... So I thought. webpack did nothing wrong. It is pure and good and unjustly complained about. It was my other love: <a href="https://github.com/mobxjs/mobx">mobx</a>. Or to be more specific: <a href="https://github.com/mobxjs/mobx-react-devtools">mobx-react-devtools</a>.</p><p>It turns out that the way you use <code>mobx-react-devtools</code> reliably makes the difference. It&#x27;s the cause of the stray <code>(&quot;production&quot;!==e.env.NODE_ENV)</code> statements in our bundle output. After a <strong>long</strong> time I happened upon <a href="https://github.com/mobxjs/mobx-react-devtools/issues/66#issuecomment-365151531">this issue</a> which contained a gem by one <a href="https://github.com/gilesbutler">Giles Butler</a>. His suggested way to reference <code>mobx-react-devtools</code> is (as far as I can tell) the solution!</p><p>On a dummy project I had the <code>mobx-react-devtools</code> advised code in place:</p><pre><code class="language-js">import * as React from &#x27;react&#x27;;
import { Layout } from &#x27;./components/layout&#x27;;
import DevTools from &#x27;mobx-react-devtools&#x27;;

export const App: React.SFC&lt;{}&gt; = (_props) =&gt; (
  &lt;div className=&quot;ui container&quot;&gt;
    &lt;Layout /&gt;
    {process.env.NODE_ENV !== &#x27;production&#x27; ? (
      &lt;DevTools position={{ bottom: 20, right: 20 }} /&gt;
    ) : null}
  &lt;/div&gt;
);
</code></pre><p>With this I had a build size of 311kb. Closer examination of my bundle revealed that my <code>bundle.js</code> was riddled with <code>(&quot;production&quot;!==e.env.NODE_ENV)</code> statements. Sucks, right?</p><p>Then I tried this instead:</p><pre><code class="language-js">import * as React from &#x27;react&#x27;;
import { Layout } from &#x27;./components/layout&#x27;;
const { Fragment } = React;

const DevTools =
  process.env.NODE_ENV !== &#x27;production&#x27;
    ? require(&#x27;mobx-react-devtools&#x27;).default
    : Fragment;

export const App: React.SFC&lt;{}&gt; = (_props) =&gt; (
  &lt;div className=&quot;ui container&quot;&gt;
    &lt;Layout /&gt;
    &lt;DevTools position={{ bottom: 20, right: 20 }} /&gt;
  &lt;/div&gt;
);
</code></pre><p>With this approach I got a build size of 191kb. This was thanks to the dead code being actually stripped. That&#x27;s a saving of 120kb!</p><h2>Perhaps We Change the Advice?</h2><p>There&#x27;s a suggestion that the README should be changed to reflect this advice - until that happens, I wanted to share this solution. Also, I&#x27;ve a nagging feeling that I&#x27;ve missed something pertinent here; if someone knows something that I should... Tell me please!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Uploading Images to Cloudinary with the Fetch API]]></title>
            <link>https://blog.johnnyreilly.com/2018/03/25/uploading-images-to-cloudinary-with-fetch</link>
            <guid>Uploading Images to Cloudinary with the Fetch API</guid>
            <pubDate>Sun, 25 Mar 2018 00:00:00 GMT</pubDate>
            <description><![CDATA[I was recently checking out a very good post which explained how to upload images using React Dropzone and SuperAgent to Cloudinary.]]></description>
            <content:encoded><![CDATA[<p>I was recently checking out a <a href="https://css-tricks.com/image-upload-manipulation-react/">very good post</a> which explained how to upload images using <a href="https://github.com/react-dropzone/react-dropzone">React Dropzone</a> and <a href="https://github.com/visionmedia/superagent">SuperAgent</a> to <a href="https://cloudinary.com/">Cloudinary</a>.</p><p>It&#x27;s a brilliant post; you should totally read it. Even if you hate images, uploads and JavaScript. However, there was one thing in there that I didn&#x27;t want; SuperAgent. It&#x27;s lovely but I&#x27;m a <a href="https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API">Fetch</a> guy. That&#x27;s just how I roll. The question is, how do I do the below using Fetch?</p><pre><code class="language-js">handleImageUpload(file) {
    let upload = request.post(CLOUDINARY_UPLOAD_URL)
                     .field(&#x27;upload_preset&#x27;, CLOUDINARY_UPLOAD_PRESET)
                     .field(&#x27;file&#x27;, file);

    upload.end((err, response) =&gt; {
      if (err) {
        console.error(err);
      }

      if (response.body.secure_url !== &#x27;&#x27;) {
        this.setState({
          uploadedFileCloudinaryUrl: response.body.secure_url
        });
      }
    });
  }
</code></pre><p>Well it actually took me longer to work out than I&#x27;d like to admit. But now I have, let me save you the bother. To do the above using Fetch you just need this:</p><pre><code class="language-js">handleImageUpload(file) {
    const formData = new FormData();
    formData.append(&quot;file&quot;, file);
    formData.append(&quot;upload_preset&quot;, CLOUDINARY_UPLOAD_PRESET); // Replace the preset name with your own

    fetch(CLOUDINARY_UPLOAD_URL, {
      method: &#x27;POST&#x27;,
      body: formData
    })
      .then(response =&gt; response.json())
      .then(data =&gt; {
        if (data.secure_url !== &#x27;&#x27;) {
          this.setState({
            uploadedFileCloudinaryUrl: data.secure_url
          });
        }
      })
      .catch(err =&gt; console.error(err))
  }
</code></pre><p>To get a pre-canned project to try this with take a look at <a href="https://github.com/damonbauer/react-cloudinary">Damon&#x27;s repo</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[It's Not Dead: webpack and dead code elimination limitations]]></title>
            <link>https://blog.johnnyreilly.com/2018/03/07/its-not-dead-webpack-and-dead-code</link>
            <guid>It's Not Dead: webpack and dead code elimination limitations</guid>
            <pubDate>Wed, 07 Mar 2018 00:00:00 GMT</pubDate>
            <description><![CDATA[Every now and then you can be surprised. Your assumptions turn out to be wrong.]]></description>
            <content:encoded><![CDATA[<p>Every now and then you can be surprised. Your assumptions turn out to be wrong.</p><p>Webpack has long supported the notion of dead code elimination. webpack facilitates this through use of the <code>DefinePlugin</code>. The compile time value of <code>process.env.NODE_ENV</code> is set either to <code>&#x27;production&#x27;</code> or something else. If it&#x27;s set to <code>&#x27;production&#x27;</code> then some dead code hackery can happen. <a href="https://reactjs.org/docs/optimizing-performance.html#webpack">Libraries like React make use of this to serve up different, and crucially smaller, production builds.</a></p><p>A (pre-webpack 4) production config file will typically contain this code:</p><pre><code class="language-js">new webpack.DefinePlugin({
    &#x27;process.env.NODE_ENV&#x27;: JSON.stringify(&#x27;production&#x27;)
}),
new UglifyJSPlugin(),
</code></pre><p>The result of the above config is that webpack will inject the value &#x27;production&#x27; everywhere in the codebase where a <code>process.env.NODE_ENV</code> can be found. (In fact, as of webpack 4 setting this magic value is out-of-the-box behaviour for Production mode; yay the #0CJS!)</p><p>What this means is, if you&#x27;ve written:</p><pre><code class="language-js">if (process.env.NODE_ENV !== &#x27;production&#x27;) {
  // Do a development mode only thing
}
</code></pre><p>webpack can and will turn this into</p><pre><code class="language-js">if (&#x27;production&#x27; !== &#x27;production&#x27;) {
  // Do a development mode only thing
}
</code></pre><p>The <a href="https://github.com/webpack-contrib/uglifyjs-webpack-plugin">UglifyJSPlugin</a> is there to minify the JavaScript in your bundles. As an added benefit, this plugin is smart enough to know that <code>&#x27;production&#x27; !== &#x27;production&#x27;</code> is always <code>false</code>. And because it&#x27;s smart, it chops the code. Dead code elimated.</p><p>You can read more about this <a href="https://webpack.js.org/guides/production/#specify-the-environment">in the webpack docs</a>.</p><h2>Limitations</h2><p>Given what I&#x27;ve said, consider the following code:</p><pre><code class="language-js">export class Config {
  // Other properties

  get isDevelopment() {
    return process.env.NODE_ENV !== &#x27;production&#x27;;
  }
}
</code></pre><p>This is a config class that exposes the expression <code>process.env.NODE_ENV !== &#x27;production&#x27;</code> with the friendly name <code>isDevelopment</code>. You&#x27;d think that dead code elimination would be your friend here. It&#x27;s not.</p><p>My personal expection was that dead code elimination would treat <code>Config.isDevelopment</code> and the expression <code>process.env.NODE_ENV !== &#x27;production&#x27;</code> identically. Because they&#x27;re identical.</p><p>However, this turns out not to be the case. Dead code elimination works just as you would hope when using the expression <code>process.env.NODE_ENV !== &#x27;production&#x27;</code> directly in code. However webpack <strong>only</strong> performs dead code elimination for the <strong>direct</strong> usage of the <code>process.env.NODE_ENV !== &#x27;production&#x27;</code> expression. I&#x27;ll say that again: if you want dead code elimination then use the injected values; not an encapsulated version of them. It turns out you cannot rely on webpack flowing values through and performing dead code elimination on that basis.</p><p>The TL;DR: if you want to elimate dead code then <!-- -->*<!-- -->always<!-- -->*<!-- --> use <code>process.env.NODE_ENV !== &#x27;production&#x27;</code>; don&#x27;t abstract it. It doesn&#x27;t work.</p><p>UglifyJS is smart. But not that smart.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ts-loader 4 / fork-ts-checker-webpack-plugin 0.4]]></title>
            <link>https://blog.johnnyreilly.com/2018/02/25/ts-loader-400-fork-ts-checker-webpack</link>
            <guid>ts-loader 4 / fork-ts-checker-webpack-plugin 0.4</guid>
            <pubDate>Sun, 25 Feb 2018 00:00:00 GMT</pubDate>
            <description><![CDATA[webpack 4 has shipped!]]></description>
            <content:encoded><![CDATA[<p>webpack 4 has shipped!</p><h2><code>ts-loader</code></h2><p>The <a href="https://github.com/TypeStrong/ts-loader"><code>ts-loader</code></a> 4 is available too. For details see our release <a href="https://github.com/TypeStrong/ts-loader/releases/tag/v4.0.0">here</a>. To start using <code>ts-loader</code> 4:</p><ul><li>When using <code>yarn</code>: <code>yarn add ts-loader@4.1.0 -D</code></li><li>When using <code>npm</code>: <code>npm install ts-loader@4.1.0 -D</code></li></ul><p>Remember to use this in concert with the webpack 4. To see a working example take a look at <a href="https://github.com/johnnyreilly/ts-loader/tree/master/examples/vanilla">the &quot;vanilla&quot; example</a>.</p><h2><code>fork-ts-checker-webpack-plugin</code></h2><p>There&#x27;s more! You may like to use the <code>&lt;a href=&quot;https://github.com/Realytics/fork-ts-checker-webpack-plugin&quot;&gt;fork-ts-checker-webpack-plugin&lt;/a&gt;</code>, (aka the ts-loader turbo-booster). The webpack compatible version has been <a href="https://github.com/Realytics/fork-ts-checker-webpack-plugin/releases/tag/v0.4.1">released to npm as 0.4.1</a>:</p><ul><li>When using <code>yarn</code>: <code>yarn add fork-ts-checker-webpack-plugin@0.4.1 -D</code></li><li>When using <code>npm</code>: <code>npm install fork-ts-checker-webpack-plugin@0.4.1 -D</code></li></ul><p>To see a working example take a look at <a href="https://github.com/johnnyreilly/ts-loader/tree/master/examples/fork-ts-checker">the &quot;fork-ts-checker&quot; example</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Finding webpack 4 (use a Map)]]></title>
            <link>https://blog.johnnyreilly.com/2018/01/29/finding-webpack-4-use-map</link>
            <guid>Finding webpack 4 (use a Map)</guid>
            <pubDate>Mon, 29 Jan 2018 00:00:00 GMT</pubDate>
            <description><![CDATA[Update: 03/02/2018]]></description>
            <content:encoded><![CDATA[<h2>Update: 03/02/2018</h2><p>Tobias Koppers has written a migration guide for plugins / loaders as well - take a read <a href="https://medium.com/webpack/webpack-4-migration-guide-for-plugins-loaders-20a79b927202">here</a>. It&#x27;s very useful.</p><h2>webpack 4</h2><p>webpack 4 is on the horizon. <a href="https://medium.com/webpack/webpack-4-beta-try-it-today-6b1d27d7d7e2">The beta dropped last Friday</a>. So what do you, as a plugin / loader author need to do? What needs to change to make your loader / plugin webpack 4 friendly?</p><p>This is a guide that should inform you about the changes you might need to make. It&#x27;s based on my own experiences migrating <a href="https://github.com/TypeStrong/ts-loader"><code>ts-loader</code></a> and the <a href="https://github.com/Realytics/fork-ts-checker-webpack-plugin"><code>fork-ts-checker-webpack-plugin</code></a>. If you&#x27;d like to see this in action then take a look at the PRs related to these. The ts-loader PR can be found <a href="https://github.com/TypeStrong/ts-loader/pull/710">here</a>. The fork-ts-checker-webpack-plugin PR can be found <a href="https://github.com/Realytics/fork-ts-checker-webpack-plugin/pull/93">here</a>.</p><h2>Plugins</h2><p>One of the notable changes to webpack with v4 is the change to the plugin architecture. In terms of implications it&#x27;s worth reading the comments made by <a href="https://twitter.com/wsokra">Tobias Koppers</a><a href="https://github.com/webpack/webpack/issues/6244#issuecomment-357502113">here</a> and <a href="https://github.com/webpack/webpack/issues/6064#issuecomment-349405474">here</a>.</p><p>Previously, if your plugin was tapping into a compiler hook you&#x27;d write code that looked something like this:</p><pre><code class="language-js">this.compiler.plugin(&#x27;watch-close&#x27;, () =&gt; {
  // do your thing here
});
</code></pre><p>With webpack 4 things done changed. You&#x27;d now write something like this:</p><pre><code class="language-js">this.compiler.hooks.watchClose.tap(
  &#x27;name-to-identify-your-plugin-goes-here&#x27;,
  () =&gt; {
    // do your thing here
  }
);
</code></pre><p>Hopefully that&#x27;s fairly clear; we&#x27;re using the new <code>hooks</code> property and tapping into our event of choice by <code>camelCasing</code> what was previously <code>kebab-cased</code>. So in this case <code>plugin(&#x27;watch-close&#x27; =&amp;gt; hooks.watchClose.tap</code>.</p><p>In the example above we were attaching to a sync hook. Now let&#x27;s look at an async hook:</p><pre><code class="language-js">this.compiler.plugin(&#x27;watch-run&#x27;, (watching, callback) =&gt; {
  // do your thing here
  callback();
});
</code></pre><p>This would change to be:</p><pre><code class="language-js">this.compiler.hooks.watchRun.tapAsync(
  &#x27;name-to-identify-your-plugin-goes-here&#x27;,
  (compiler, callback) =&gt; {
    // do your thing here
    callback();
  }
);
</code></pre><p>Note that rather than using <code>tap</code> here, we&#x27;re using <code>tapAsync</code>. If you&#x27;re more into promises there&#x27;s a <code>tapPromise</code> you could use instead.</p><h2>Custom Hooks</h2><p>Prior to webpack 4, you could use your own custom hooks within your plugin. Usage was as simple as this:</p><pre><code class="language-js">this.compiler.applyPluginsAsync(&#x27;fork-ts-checker-service-before-start&#x27;, () =&gt; {
  // do your thing here
});
</code></pre><p>You can still use custom hooks with webpack 4, but there&#x27;s a little more ceremony involved. Essentially, you need to tell webpack up front what you&#x27;re planning. Not hard, I promise you.</p><p>First of all, you&#x27;ll need to add the package <a href="https://www.npmjs.com/package/tapable"><code>tapable</code></a> as a dependency. Then, inside your plugin you&#x27;ll need to import the type of hook that you want to use; in the case of the <code>fork-ts-checker-webpack-plugin</code> we used both a sync and an async hook:</p><pre><code class="language-js">const AsyncSeriesHook = require(&#x27;tapable&#x27;).AsyncSeriesHook;
const SyncHook = require(&#x27;tapable&#x27;).SyncHook;
</code></pre><p>Then, inside your <code>apply</code> method you need to register your hooks:</p><pre><code class="language-js">if (
  this.compiler.hooks.forkTsCheckerServiceBeforeStart ||
  this.compiler.hooks.forkTsCheckerCancel ||
  // other hooks...
  this.compiler.hooks.forkTsCheckerEmit
) {
  throw new Error(&#x27;fork-ts-checker-webpack-plugin hooks are already in use&#x27;);
}
this.compiler.hooks.forkTsCheckerServiceBeforeStart = new AsyncSeriesHook([]);

this.compiler.hooks.forkTsCheckerCancel = new SyncHook([]);
// other sync hooks...
this.compiler.hooks.forkTsCheckerDone = new SyncHook([]);
</code></pre><p>If you&#x27;re interested in backwards compatibility then you should use the <code>_pluginCompat</code> to wire that in:</p><pre><code class="language-js">this.compiler._pluginCompat.tap(&#x27;fork-ts-checker-webpack-plugin&#x27;, (options) =&gt; {
  switch (options.name) {
    case &#x27;fork-ts-checker-service-before-start&#x27;:
      options.async = true;
      break;
    case &#x27;fork-ts-checker-cancel&#x27;:
    // other sync hooks...
    case &#x27;fork-ts-checker-done&#x27;:
      return true;
  }
  return undefined;
});
</code></pre><p>With your registration in place, you just need to replace your calls to <code>compiler.applyPlugins(&#x27;sync-hook-name&#x27;, </code> and <code>compiler.applyPluginsAsync(&#x27;async-hook-name&#x27;, </code> with calls to <code>compiler.hooks.syncHookName.call(</code> and <code>compiler.hooks.asyncHookName.callAsync(</code>. So to migrate our <code>fork-ts-checker-service-before-start</code> hook we&#x27;d write:</p><pre><code class="language-js">this.compiler.hooks.forkTsCheckerServiceBeforeStart.callAsync(() =&gt; {
  // do your thing here
});
</code></pre><h2>Loaders</h2><p>Loaders are impacted by the changes to the plugin architecture. Mostly this means applying the same plugin changes as discussed above. <code>ts-loader</code> hooks into 2 plugin events:</p><pre><code class="language-js">loader._compiler.plugin(&#x27;after-compile&#x27; /* callback goes here */);
loader._compiler.plugin(&#x27;watch-run&#x27; /* callback goes here */);
</code></pre><p>With webpack 4 these become:</p><pre><code class="language-js">loader._compiler.hooks.afterCompile.tapAsync(
  &#x27;ts-loader&#x27; /* callback goes here */
);
loader._compiler.hooks.watchRun.tapAsync(&#x27;ts-loader&#x27; /* callback goes here */);
</code></pre><p>Note again, we&#x27;re using the string <code>&quot;ts-loader&quot;</code> to identify our loader.</p><h2>I need a <code>Map</code></h2><p>When I initially ported to webpack 4, <code>ts-loader</code> simply wasn&#x27;t working. In the end I tied this down to problems in our <code>watch-run</code> callback. There&#x27;s 2 things of note here.</p><p>Firstly, as per <a href="https://github.com/webpack/webpack/releases/tag/v4.0.0-beta.0">the changelog</a>, the <code>watch-run</code> hook now has the <code>Compiler</code> as the first parameter. Previously this was a subproperty on the supplied <code>watching</code> parameter. So swapping over to use the compiler directly was necessary. Incidentally, <code>ts-loader</code> previously made use of the <code>watching.startTime</code> property that was supplied in webpack&#x27;s 1, 2 and 3. It seems to be coping without it; so hopefully that&#x27;s fine.</p><p>Secondly, with webpack 4 it&#x27;s &quot;ES2015 all the things!&quot; That is to say, with webpack now requiring a minimum of node 6, the codebase is free to start using ES2015. So if you&#x27;re a consumer of <code>compiler.fileTimestamps</code> (and <code>ts-loader</code> is) then it&#x27;s time to make a change to cater for the different API that a <code>Map</code> offers instead of indexing into an object literal with a <code>string</code> key.</p><p>What this means is, code that would once have looked like this:</p><pre><code class="language-js">Object.keys(watching.compiler.fileTimestamps)
  .filter(
    (filePath) =&gt;
      watching.compiler.fileTimestamps[filePath] &gt; lastTimes[filePath]
  )
  .forEach((filePath) =&gt; {
    lastTimes[filePath] = times[filePath];
    // ...
  });
</code></pre><p>Now looks more like this:</p><pre><code class="language-js">for (const [filePath, date] of compiler.fileTimestamps) {
  if (date &gt; lastTimes.get(filePath)) {
    continue;
  }

  lastTimes.set(filePath, date);
  // ...
}
</code></pre><h2>Happy Porting!</h2><p>I hope your own port to webpack 4 goes well. Do let me know if there&#x27;s anything I&#x27;ve missed out / any inaccuracies etc and I&#x27;ll update this guide.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[webpack 4 - ts-loader / fork-ts-checker-webpack-plugin betas]]></title>
            <link>https://blog.johnnyreilly.com/2018/01/28/webpack-4-ts-loader-fork-ts-checker</link>
            <guid>webpack 4 - ts-loader / fork-ts-checker-webpack-plugin betas</guid>
            <pubDate>Sun, 28 Jan 2018 00:00:00 GMT</pubDate>
            <description><![CDATA[The first webpack 4 beta dropped on Friday. Very exciting! Following hot on the heels of those announcements, I've some news to share too. Can you guess what it is?]]></description>
            <content:encoded><![CDATA[<p><a href="https://medium.com/webpack/webpack-4-beta-try-it-today-6b1d27d7d7e2">The first webpack 4 beta dropped on Friday</a>. Very exciting! Following hot on the heels of those announcements, I&#x27;ve some news to share too. Can you guess what it is?</p><h2><code>ts-loader</code></h2><p>Yes! The <a href="https://github.com/TypeStrong/ts-loader"><code>ts-loader</code></a> beta to work with webpack 4 is available. To get hold of the beta:</p><ul><li>When using <code>yarn</code>: <code>yarn add ts-loader@4.0.0-beta.0 -D</code></li><li>When using <code>npm</code>: <code>npm install ts-loader@4.0.0-beta.0 -D</code></li></ul><p>Remember to use this in concert with the webpack 4 beta. To see a working example take a look at <a href="https://github.com/johnnyreilly/ts-loader/tree/master/examples/vanilla">the &quot;vanilla&quot; example</a>.</p><h2><code>fork-ts-checker-webpack-plugin</code></h2><p>There&#x27;s more! You may like to use the <code>&lt;a href=&quot;https://github.com/Realytics/fork-ts-checker-webpack-plugin&quot;&gt;fork-ts-checker-webpack-plugin&lt;/a&gt;</code>, (which goes lovely with <code>ts-loader</code> and a biscuit). There is a beta available for that too:</p><ul><li>When using <code>yarn</code>: <code>yarn add johnnyreilly/fork-ts-checker-webpack-plugin#4.0.0-beta.1 -D</code></li><li>When using <code>npm</code>: <code>npm install johnnyreilly/fork-ts-checker-webpack-plugin#4.0.0-beta.1 -D</code></li></ul><p>To see a working example take a look at <a href="https://github.com/johnnyreilly/ts-loader/tree/master/examples/fork-ts-checker">the &quot;fork-ts-checker&quot; example</a>.</p><h2>PRs</h2><p>If you would like to track the progress of these betas then I encourage you to take a look at the PRs they were built from. The ts-loader PR can be found <a href="https://github.com/TypeStrong/ts-loader/pull/710">here</a>. The fork-ts-checker-webpack-plugin PR can be found <a href="https://github.com/Realytics/fork-ts-checker-webpack-plugin/pull/93">here</a>.</p><p>These are betas so things may change further; though hopefully not significantly.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Auth0, TypeScript and ASP.NET Core]]></title>
            <link>https://blog.johnnyreilly.com/2018/01/14/auth0-typescript-and-aspnet-core</link>
            <guid>Auth0, TypeScript and ASP.NET Core</guid>
            <pubDate>Sun, 14 Jan 2018 00:00:00 GMT</pubDate>
            <description><![CDATA[Most applications I write have some need for authentication and perhaps authorisation too. In fact, most apps most people write fall into that bracket. Here's the thing: Auth done well is a \big\ chunk of work. And the minute you start thinking about that you almost invariably lose focus on the thing you actually want to build and ship.]]></description>
            <content:encoded><![CDATA[<p>Most applications I write have some need for authentication and perhaps authorisation too. In fact, most apps most people write fall into that bracket. Here&#x27;s the thing: Auth done well is a <!-- -->*<!-- -->big<!-- -->*<!-- --> chunk of work. And the minute you start thinking about that you almost invariably lose focus on the thing you actually want to build and ship.</p><p>So this Christmas I decided it was time to take a look into offloading that particular problem onto someone else. I knew there were third parties who provided Auth-As-A-Service - time to give them a whirl. On the recommendation of a friend, I made Auth0 my first port of call. Lest you be expecting a full breakdown of the various players in this space, let me stop you now; I liked Auth0 so much I strayed no further. Auth0 kicks AAAS. (I&#x27;m so sorry)</p><h2>What I wanted to build</h2><p>My criteria for &quot;auth success&quot; was this:</p><ul><li>I want to build a SPA, specifically a React SPA. Ideally, I shouldn&#x27;t need a back end of my own at all</li><li>I want to use TypeScript on my client.</li></ul><p>But, for when I do implement a back end:</p><ul><li>I want that to be able to use the client side&#x27;s Auth tokens to allow access to Auth routes on my server.</li><li>‎I want to able to identify the user, given the token, to provide targeted data</li><li>Oh, and I want to use .NET Core 2 for my server.</li></ul><p>And in achieving all of the I want to add minimal code to my app. Not War and Peace. My code should remain focused on doing what it does.</p><h2>Boil a Plate</h2><p>I ended up with unqualified ticks for all my criteria, but it took some work to find out. I will say that Auth0 do travel the extra mile in terms of getting you up and running. When you create a new Client in Auth0 you&#x27;re given the option to download a quick start using the technology of your choice.</p><p>This was a massive plus for me. I took the quickstart provided and ran with it to get me to the point of meeting my own criteria. You can use this boilerplate for your own ends. Herewith, a walkthrough:</p><h2>The Walkthrough</h2><p>Fork and clone the repo at this location: <a href="https://github.com/johnnyreilly/auth0-react-typescript-asp-net-core">https://github.com/johnnyreilly/auth0-react-typescript-asp-net-core</a>.</p><p>What have we got? 2 folders, ClientApp contains the React app, Web contains the ASP.NET Core app. Now we need to get setup with Auth0 and customise our config.</p><h2>Setup Auth0</h2><p>Here&#x27;s how to get the app set up with Auth0; you&#x27;re going to need to sign up for a (free) Auth0 account. Then login into Auth0 and go to the management portal.</p><h3>Client</h3><ul><li>Create a Client with the name of your choice and use the Single Page Web Applications template.</li><li>From the new Client Settings page take the Domain and Client ID and update the similarly named properties in the <code>appsettings.Development.json</code> and <code>appsettings.Production.json</code> files with these settings.</li><li>To the Allowed Callback URLs setting add the URLs: <code>http://localhost:3000/callback,http://localhost:5000/callback</code> <!-- -->-<!-- --> the first of these faciliates running in Debug mode, the second in Production mode. If you were to deploy this you&#x27;d need to add other callback URLs in here too.</li></ul><h3>API</h3><ul><li>Create an API with the name of your choice (I recommend the same as the Client to avoid confusion), an identifier which can be anything you like; I like to use the URL of my app but it&#x27;s your call.</li><li>From the new API Settings page take the Identifier and update the Audience property in the <code>appsettings.Development.json</code> and <code>appsettings.Production.json</code> files with that value.</li></ul><h2>Running the App</h2><h3>Production build</h3><p>Build the client app with <code>yarn build</code> in the <code>ClientApp</code> folder. (Don&#x27;t forget to <code>yarn install</code> first.) Then, in the <code>Web</code> folder <code>dotnet restore</code>, <code>dotnet run</code> and open your browser to <a href="http://localhost:5000"><code>http://localhost:5000</code></a></p><h3>Debugging</h3><p>Run the client app using webpack-dev-server using <code>yarn start</code> in the <code>ClientApp</code> folder. Fire up VS Code in the root of the repo and hit F5 to debug the server. Then open your browser to <a href="http://localhost:3000"><code>http://localhost:3000</code></a></p><h2>The Tour</h2><p>When you fire up the app you&#x27;re presented with &quot;you are not logged in!&quot; message and the option to login. Do it, it&#x27;ll take you to the Auth0 &quot;lock&quot; screen where you can sign up / login. Once you do that you&#x27;ll be asked to confirm access:</p><p><img src="../static/blog/2018-01-14-auth0-typescript-and-aspnet-core/Screenshot%2B2018-01-13%2B18.40.21.png"/></p><p>All this is powered by Auth0&#x27;s <a href="https://www.npmjs.com/package/auth0-js">auth0-js</a> npm package. (Excellent type definition files are available from Definitely Typed; I&#x27;m using the <a href="https://www.npmjs.com/package/@types/auth0-js">@types/auth0-js</a> package DT publishes.) Usage of which is super simple; it exposes an <code>authorize</code> method that when called triggers the Auth0 lock screen. Once you&#x27;ve &quot;okayed&quot; you&#x27;ll be taken back to the app which will use the <code>parseHash</code> method to extract the access token that Auth0 has provided. Take a look at how our <code>authStore</code> makes use of auth0-js: (don&#x27;t be scared; it uses mobx - but you could use anything)</p><h3>authStore.ts</h3><pre><code class="language-ts">import { Auth0UserProfile, WebAuth } from &#x27;auth0-js&#x27;;
import { action, computed, observable, runInAction } from &#x27;mobx&#x27;;
import { IAuth0Config } from &#x27;../../config&#x27;;
import { StorageFacade } from &#x27;../storageFacade&#x27;;

interface IStorageToken {
  accessToken: string;
  idToken: string;
  expiresAt: number;
}

const STORAGE_TOKEN = &#x27;storage_token&#x27;;

export class AuthStore {
  @observable.ref auth0: WebAuth;
  @observable.ref userProfile: Auth0UserProfile;
  @observable.ref token: IStorageToken;

  constructor(config: IAuth0Config, private storage: StorageFacade) {
    this.auth0 = new WebAuth({
      domain: config.domain,
      clientID: config.clientId,
      redirectUri: config.redirectUri,
      audience: config.audience,
      responseType: &#x27;token id_token&#x27;,
      scope: &#x27;openid email profile do:admin:thing&#x27;, // the do:admin:thing scope is custom and defined in the scopes section of our API in the Auth0 dashboard
    });
  }

  initialise() {
    const token = this.parseToken(this.storage.getItem(STORAGE_TOKEN));
    if (token) {
      this.setSession(token);
    }
    this.storage.addEventListener(this.onStorageChanged);
  }

  parseToken(tokenString: string) {
    const token = JSON.parse(tokenString || &#x27;{}&#x27;);
    return token;
  }

  onStorageChanged = (event: StorageEvent) =&gt; {
    if (event.key === STORAGE_TOKEN) {
      this.setSession(this.parseToken(event.newValue));
    }
  };

  @computed get isAuthenticated() {
    // Check whether the current time is past the
    // access token&#x27;s expiry time
    return this.token &amp;&amp; new Date().getTime() &lt; this.token.expiresAt;
  }

  login = () =&gt; {
    this.auth0.authorize();
  };

  handleAuthentication = () =&gt; {
    this.auth0.parseHash((err, authResult) =&gt; {
      if (authResult &amp;&amp; authResult.accessToken &amp;&amp; authResult.idToken) {
        const token = {
          accessToken: authResult.accessToken,
          idToken: authResult.idToken,
          // Set the time that the access token will expire at
          expiresAt: authResult.expiresIn * 1000 + new Date().getTime(),
        };

        this.setSession(token);
      } else if (err) {
        // tslint:disable-next-line:no-console
        console.log(err);
        alert(`Error: ${err.error}. Check the console for further details.`);
      }
    });
  };

  @action
  setSession(token: IStorageToken) {
    this.token = token;
    this.storage.setItem(STORAGE_TOKEN, JSON.stringify(token));
  }

  getAccessToken = () =&gt; {
    const accessToken = this.token.accessToken;
    if (!accessToken) {
      throw new Error(&#x27;No access token found&#x27;);
    }
    return accessToken;
  };

  @action
  loadProfile = async () =&gt; {
    const accessToken = this.token.accessToken;
    if (!accessToken) {
      return;
    }

    this.auth0.client.userInfo(accessToken, (err, profile) =&gt; {
      if (err) {
        throw err;
      }

      if (profile) {
        runInAction(() =&gt; (this.userProfile = profile));
        return profile;
      }

      return undefined;
    });
  };

  @action
  logout = () =&gt; {
    // Clear access token and ID token from local storage
    this.storage.removeItem(STORAGE_TOKEN);

    this.token = null;
    this.userProfile = null;
  };
}
</code></pre><p>Once you&#x27;re logged in the app offers you more in the way of navigation options. A &quot;Profile&quot; screen shows you the details your React app has retrieved from Auth0 about you. This is backed by the <code>client.userInfo</code> method on <code>auth0-js</code>. There&#x27;s also a &quot;Ping&quot; screen which is where your React app talks to your ASP.NET Core server. The screenshot below illustrates the result of hitting the &quot;Get Private Data&quot; button:</p><p><img src="../static/blog/2018-01-14-auth0-typescript-and-aspnet-core/Screenshot%2B2018-01-13%2B18.47.49.png"/></p><p>The &quot;Get Server to Retrieve Profile Data&quot; button is interesting as it illustrates that the server can get access to your profile data as well. There&#x27;s nothing insecure here; it gets the details using the access token retrieved from Auth0 by the ClientApp and passed to the server. It&#x27;s the API we set up in Auth0 that is in play here. The app uses the Domain and the access token to talk to Auth0 like so:</p><h3>UserController.cs</h3><pre><code class="language-cs">// Retrieve the access_token claim which we saved in the OnTokenValidated event
    var accessToken = User.Claims.FirstOrDefault(c =&gt; c.Type == &quot;access_token&quot;).Value;

    // If we have an access_token, then retrieve the user&#x27;s information
    if (!string.IsNullOrEmpty(accessToken))
    {
        var domain = _config[&quot;Auth0:Domain&quot;];
        var apiClient = new AuthenticationApiClient(domain);
        var userInfo = await apiClient.GetUserInfoAsync(accessToken);

        return Ok(userInfo);
    }
</code></pre><p>We can also access the <code>sub</code> claim, which uniquely identifies the user:</p><h3>UserController.cs</h3><pre><code class="language-cs">// We&#x27;re not doing anything with this, but hey! It&#x27;s useful to know where the user id lives
    var userId = User.Claims.FirstOrDefault(c =&gt; c.Type == System.Security.Claims.ClaimTypes.NameIdentifier).Value; // our userId is the sub value
</code></pre><p>The reason our ASP.NET Core app works with Auth0 and that we have access to the access token here in the first place is because of our startup code:</p><h3>Startup.cs</h3><pre><code class="language-cs">public void ConfigureServices(IServiceCollection services)
    {
        var domain = $&quot;https://{Configuration[&quot;Auth0:Domain&quot;]}/&quot;;
        services.AddAuthentication(options =&gt;
        {
            options.DefaultAuthenticateScheme = JwtBearerDefaults.AuthenticationScheme;
            options.DefaultChallengeScheme = JwtBearerDefaults.AuthenticationScheme;
        }).AddJwtBearer(options =&gt;
        {
            options.Authority = domain;
            options.Audience = Configuration[&quot;Auth0:Audience&quot;];
            options.Events = new JwtBearerEvents
            {
                OnTokenValidated = context =&gt;
                {
                    if (context.SecurityToken is JwtSecurityToken token)
                    {
                        if (context.Principal.Identity is ClaimsIdentity identity)
                        {
                            identity.AddClaim(new Claim(&quot;access_token&quot;, token.RawData));
                        }
                    }

                    return Task.FromResult(0);
                }
            };
        });

        // ....
</code></pre><h2>Authorization</h2><p>We&#x27;re pretty much done now; just one magic button to investigate: &quot;Get Admin Data&quot;. If you presently try and access the admin data you&#x27;ll get a <code>403 Forbidden</code>. It&#x27;s forbidden because that endpoint relies on the <code>&quot;do:admin:thing&quot;</code> scope in our claims:</p><h3>UserController.cs</h3><pre><code class="language-cs">[Authorize(Scopes.DoAdminThing)]
    [HttpGet(&quot;api/userDoAdminThing&quot;)]
    public IActionResult GetUserDoAdminThing()
    {
        return Ok(&quot;Admin endpoint&quot;);
    }
</code></pre><h3>Scopes.cs</h3><pre><code class="language-cs">public static class Scopes
    {
         // the do:admin:thing scope is custom and defined in the scopes section of our API in the Auth0 dashboard
        public const string DoAdminThing = &quot;do:admin:thing&quot;;
    }
</code></pre><p>This wired up in our ASP.NET Core app like so:</p><h3>Startup.cs</h3><pre><code class="language-cs">services.AddAuthorization(options =&gt;
    {
        options.AddPolicy(Scopes.DoAdminThing, policy =&gt; policy.Requirements.Add(new HasScopeRequirement(Scopes.DoAdminThing, domain)));
    });

    // register the scope authorization handler
    services.AddSingleton&lt;iauthorizationhandler, hasscopehandler=&quot;&quot;&gt;();
&lt;/iauthorizationhandler,&gt;
</code></pre><h3>HasScopeHandler.cs</h3><pre><code class="language-cs">public class HasScopeHandler : AuthorizationHandler&lt;hasscoperequirement&gt;
    {
        protected override Task HandleRequirementAsync(AuthorizationHandlerContext context, HasScopeRequirement requirement)
        {
            // If user does not have the scope claim, get out of here
            if (!context.User.HasClaim(c =&gt; c.Type == &quot;scope&quot; &amp;&amp; c.Issuer == requirement.Issuer))
                return Task.CompletedTask;

            // Split the scopes string into an array
            var scopes = context.User.FindFirst(c =&gt; c.Type == &quot;scope&quot; &amp;&amp; c.Issuer == requirement.Issuer).Value.Split(&#x27; &#x27;);

            // Succeed if the scope array contains the required scope
            if (scopes.Any(s =&gt; s == requirement.Scope))
                context.Succeed(requirement);

            return Task.CompletedTask;
        }
    }
&lt;/hasscoperequirement&gt;
</code></pre><p>The reason we&#x27;re 403ing at present is because when our <code>HasScopeHandler</code> executes, <code>requirement.Scope</code> has the value of <code>&quot;do:admin:thing&quot;</code> and our <code>scopes</code> do not contain that value. To add it, go to your API in the Auth0 management console and add it:</p><p><img src="../static/blog/2018-01-14-auth0-typescript-and-aspnet-core/Screenshot%2B2018-01-14%2B08.26.54.png"/></p><p>Note that you can control how this scope is acquired using &quot;Rules&quot; in the Auth0 management portal.</p><p>You won&#x27;t be able to access the admin endpoint yet because you&#x27;re still rocking with the old access token; pre-newly-added scope. But when you next login to Auth0 you&#x27;ll see a prompt like this:</p><p><img src="../static/blog/2018-01-14-auth0-typescript-and-aspnet-core/Screenshot%2B2018-01-14%2B08.32.59.png"/></p><p>Which demonstrates that you&#x27;re being granted an extra scope. With your new shiny access token you can now access the oh-so-secret Admin endpoint.</p><p>I had some more questions about Auth0 as I&#x27;m still new to it myself. To see my question (and the very helpful answer!) go here: <a href="https://community.auth0.com/questions/13786/get-user-data-server-side-what-is-a-good-approach">https://community.auth0.com/questions/13786/get-user-data-server-side-what-is-a-good-approach</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ts-loader 2017 retrospective]]></title>
            <link>https://blog.johnnyreilly.com/2017/12/24/ts-loader-2017-retrospective</link>
            <guid>ts-loader 2017 retrospective</guid>
            <pubDate>Sun, 24 Dec 2017 00:00:00 GMT</pubDate>
            <description><![CDATA[2017 is drawing to a close, and it's been a big, big year in webpack-land. It's been a big year for ts-loader too. At the start of the year v1.3.3 was the latest version available, officially supporting webpack 1. (Old school!) We end the year with ts-loader sitting pretty at v3.2.0 and supporting webpack 2 and 3.]]></description>
            <content:encoded><![CDATA[<p>2017 is drawing to a close, and it&#x27;s been a big, big year in webpack-land. It&#x27;s been a big year for <code>ts-loader</code> too. At the start of the year v1.3.3 was the latest version available, officially supporting webpack 1. (Old school!) We end the year with <code>ts-loader</code> sitting pretty at v3.2.0 and supporting webpack 2 and 3.</p><p>Many releases were shipped and that was down to a whole bunch of folk. People helped out with bug fixes, features, advice and docs improvements. <strong>All of these help.</strong><code>ts-loader</code> wouldn&#x27;t be where it is without you so thanks to everyone that helped out - you rock!</p><p><img src="https://avatars.githubusercontent.com/christiantinauer"/></p><p><img src="https://avatars.githubusercontent.com/Pajn"/></p><p><img src="https://avatars.githubusercontent.com/maier49"/></p><p><img src="https://avatars.githubusercontent.com/false"/></p><p><img src="https://avatars.githubusercontent.com/roddypratt"/></p><p><img src="https://avatars.githubusercontent.com/ldrick"/></p><p><img src="https://avatars.githubusercontent.com/mattlewis92"/></p><p><img src="https://avatars.githubusercontent.com/Venryx"/></p><p><img src="https://avatars.githubusercontent.com/WillMartin"/></p><p><img src="https://avatars.githubusercontent.com/Loilo"/></p><p><img src="https://avatars.githubusercontent.com/Brooooooklyn"/></p><p><img src="https://avatars.githubusercontent.com/mengxy"/></p><p><img src="https://avatars.githubusercontent.com/bsouthga"/></p><p><img src="https://avatars.githubusercontent.com/zinserjan"/></p><p><img src="https://avatars.githubusercontent.com/sokra"/></p><p><img src="https://avatars.githubusercontent.com/vhqtvn"/></p><p><img src="https://avatars.githubusercontent.com/HerringtonDarkholme"/></p><p><img src="https://avatars.githubusercontent.com/johnnyreilly"/></p><p><img src="https://avatars.githubusercontent.com/jbrantly"/></p><p><img src="https://avatars.githubusercontent.com/octref"/></p><p><img src="https://avatars.githubusercontent.com/rhyek"/></p><p><img src="https://avatars.githubusercontent.com/develar"/></p><p><img src="https://avatars.githubusercontent.com/donaldpipowitch"/></p><p><img src="https://avatars.githubusercontent.com/schmuli"/></p><p><img src="https://avatars.githubusercontent.com/longlho"/></p><p><img src="https://avatars.githubusercontent.com/Igorbek"/></p><p><img src="https://avatars.githubusercontent.com/aindlq"/></p><p><img src="https://avatars.githubusercontent.com/wearymonkey"/></p><p><img src="https://avatars.githubusercontent.com/bancek"/></p><p><img src="https://avatars.githubusercontent.com/mredbishop"/></p><p>I&#x27;m really grateful to all of you. Thanks so much! (Apologies for those I&#x27;ve missed anyone out - I know there&#x27;s more still.)</p><h2><code>fork-ts-checker-webpack-plugin</code> build speed improvements</h2><p>Alongside other&#x27;s direct contributions to <code>ts-loader</code>, other projects improved the experience of using <code>ts-loader</code>. <a href="https://github.com/piotr-oles">Piotr Oleś</a> dropped his <code>&lt;a href=&quot;https://github.com/Realytics/fork-ts-checker-webpack-plugin&quot;&gt;fork-ts-checker-webpack-plugin&lt;/a&gt;</code> this year which nicely increased build speed when used with <code>ts-loader</code>.</p><p>That opened up the possibility of adding <a href="https://github.com/amireh/happypack">HappyPack</a> support. I had the good fortune to work with webpack&#x27;s <a href="https://github.com/sokra">Tobias Koppers</a> and ExtraHop&#x27;s <a href="https://github.com/abirmingham">Alex Birmingham</a> on <a href="https://www.extrahop.com/company/blog/2017/extrahop-webpack-accelerating-build-times/">improving TypeScript build speed further</a>.</p><p>So what does the future hold?</p><h2>ts-loader 4.0 (Live webpack or Die Hard)</h2><p>The web marches on and webpack gallops alongside. Here&#x27;s what&#x27;s in the pipeline for ts-loader in 2018:</p><h3>Start using the new watch API</h3><p><a href="https://github.com/Microsoft/TypeScript/pull/20234">A new watch API is being made available in the TypeScript API</a>. We have <a href="https://github.com/TypeStrong/ts-loader/pull/685">a PR</a> from the amazing <a href="https://github.com/sheetalkamat">Sheetal Nandi</a> which adds support to ts-loader. Given that&#x27;s quite a big PR we want to merge that before anything else lands. The watch API is still being finalised but once it lands in TypeScript we&#x27;ll look to merge the PR and ship a new version of <code>ts-loader</code>.</p><h3>Drop custom module resolution</h3><p>Historically <code>ts-loader</code> has had it&#x27;s own module resolution mechanism in place. We&#x27;re going to look to move to use the TypeScript mechanism instead. The old module resolution be deprecated but will remain available behind a flag for a time. In future we&#x27;ll look to drop the old mechanism entirely.</p><h3>Drop support for TypeScript 2.3 and below</h3><p>The codebase can be made simpler if we drop support for older versions of TypeScript so that&#x27;s what we plan to do with our next breaking changes release.</p><h3>webpack v4 is in alpha now</h3><p>If any changes need to happen to ts-loader to support webpack 4 then they will be. Personally I&#x27;m planning to help out with <code>&lt;a href=&quot;https://github.com/Realytics/fork-ts-checker-webpack-plugin&quot;&gt;fork-ts-checker-webpack-plugin&lt;/a&gt;</code> as there will likely be some changes required there.</p><h3><code>contextAsConfigBasePath</code> will be replaced with a <code>context</code></h3><p>The option that landed in the last month doesn&#x27;t quite achieve the aims of the original PR&#x27;s author <a href="https://github.com/christiantinauer">Christian Tinauer</a>. Consequently it&#x27;s going to be replaced with a new option. This is queued up and ready to go <a href="https://github.com/TypeStrong/ts-loader/pull/688">here</a>.</p><h3><code>reportFiles</code> option to be added</h3><p><a href="https://github.com/freeman">Michel Rasschaert</a> is presently working on adding a <code>reportFiles</code> option to <code>ts-loader</code>. You can see the PR in progress <a href="https://github.com/TypeStrong/ts-loader/pull/701">here</a>.</p><h2>Merry Christmas!</h2><p>You can expect to see the first releases of ts-loader 4.0 in 2018. In the meantime, I&#x27;d like to wish you Merry Christmas and a Happy New Year! And once more, thanks and thanks again to all you generous people who help build <code>ts-loader</code>. You&#x27;re wonderful and so I&#x27;m glad you do what you do... joyeux Noel!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The TypeScript webpack PWA]]></title>
            <link>https://blog.johnnyreilly.com/2017/11/19/the-typescript-webpack-pwa</link>
            <guid>The TypeScript webpack PWA</guid>
            <pubDate>Sun, 19 Nov 2017 00:00:00 GMT</pubDate>
            <description><![CDATA[So, there you sit, conflicted. You've got a lovely build setup; it's a thing of beauty. Precious, polished like a diamond, sharpened like a circular saw. There at the core of your carefully crafted setup sits webpack. Heaving, mysterious... powerful.]]></description>
            <content:encoded><![CDATA[<p>So, there you sit, conflicted. You&#x27;ve got a lovely build setup; it&#x27;s a thing of beauty. Precious, polished like a diamond, sharpened like a circular saw. There at the core of your carefully crafted setup sits webpack. Heaving, mysterious... powerful.</p><p>There&#x27;s more. Not only are you sold on webpack, you&#x27;re all in TypeScript too. But now you&#x27;ve heard tell of &quot;Progressive Web Applications&quot; and &quot;Service Workers&quot;.... And you want to be dealt in. You want to build web apps that work offline. It can&#x27;t work can it? Your build setup&#x27;s going to be like the creature in the episode where they&#x27;ve taken one too many jumps and it&#x27;s gone into the foetal position.</p><p>So this is the plan kids. Let&#x27;s take a simple TypeScript, webpack setup and make it a PWA. Like Victoria Wood said...</p><h2><a href="https://youtu.be/lNU5KVa_Tu8">Let&#x27;s Do It Tonight</a></h2><p>How to begin? Well first comes the plagiarism; <a href="https://github.com/TypeStrong/ts-loader/tree/master/examples/core-js">here&#x27;s a simple TypeScript webpack setup</a>. Rob it. Stick a gun to its head and order it onto your hard drive. <code>yarn install</code> to pick up your dependencies and then <code>yarn start</code> to see what you&#x27;ve got. Something like this:</p><p><img src="../static/blog/2017-11-19-the-typescript-webpack-pwa/Screenshot%2B2017-11-19%2B18.29.15.png"/></p><p>Beautiful right? And if we <code>yarn build</code> we end up with a simple output:</p><p><img src="../static/blog/2017-11-19-the-typescript-webpack-pwa/Screenshot%2B2017-11-19%2B18.34.12.png"/></p><p>To test what we&#x27;ve built out we want to use a simple web server to serve up the <code>dist</code> folder. I&#x27;ve got the npm package <a href="https://www.npmjs.com/package/http-server">http-server</a> installed globally for just such an eventuality. So let&#x27;s <code>http-server ./dist</code> and I&#x27;m once again looking at our simple app; it looks exactly the same as when I <code>yarn start</code>. Smashing. What would we see if we were offline? Well thanks to the magic of Chrome DevTools we can find out. Offline and refresh our browser...</p><p><img src="../static/blog/2017-11-19-the-typescript-webpack-pwa/Screenshot%2B2017-11-19%2B20.05.19.png"/></p><p>Not very user friendly. Once we&#x27;re done, we should be able to refresh and still see our app.</p><h2><a href="https://youtu.be/UODX_pYpVxk">Work(box) It</a></h2><p><a href="https://developers.google.com/web/tools/workbox/">Workbox</a> is a project that makes the setting up of Service Workers (aka the magic that powers PWAs) easier. It supports webpack use cases through the <a href="https://www.npmjs.com/package/workbox-webpack-plugin">workbox-webpack-plugin</a>; so let&#x27;s give it a whirl. Incidentally, there&#x27;s a <a href="https://developers.google.com/web/tools/workbox/get-started/webpack">cracking example</a> on the Workbox site.</p><p><code>yarn add workbox-webpack-plugin --dev</code> adds the plugin to our project. To make use of it, punt your way over to the <code>webpack.production.config.js</code> and add an entry for the plugin. We also need to set the <code>hash</code> parameter of the html-webpack-plugin to be false; if it&#x27;s true it&#x27;ll cause problems for the ServiceWorker.</p><pre><code class="language-js">const WorkboxPlugin = require(&#x27;workbox-webpack-plugin&#x27;);

//...

module.exports = {
  //...

  plugins: [
    //...

    new HtmlWebpackPlugin({
      hash: false,
      inject: true,
      template: &#x27;src/index.html&#x27;,
      minify: {
        removeComments: true,
        collapseWhitespace: true,
        removeRedundantAttributes: true,
        useShortDoctype: true,
        removeEmptyAttributes: true,
        removeStyleLinkTypeAttributes: true,
        keepClosingSlash: true,
        minifyJS: true,
        minifyCSS: true,
        minifyURLs: true,
      },
    }),

    new WorkboxPlugin({
      // we want our service worker to cache the dist directory
      globDirectory: &#x27;dist&#x27;,
      // these are the sorts of files we want to cache
      globPatterns: [&#x27;**/*.{html,js,css,png,svg,jpg,gif,json}&#x27;],
      // this is where we want our ServiceWorker to be created
      swDest: path.resolve(&#x27;dist&#x27;, &#x27;sw.js&#x27;),
      // these options encourage the ServiceWorkers to get in there fast
      // and not allow any straggling &quot;old&quot; SWs to hang around
      clientsClaim: true,
      skipWaiting: true,
    }),
  ],

  //...
};
</code></pre><p>With this in place, <code>yarn build</code> will generate a ServiceWorker. Now to alter our code to register it. Open up <code>index.tsx</code> and add this to the end of the file:</p><pre><code class="language-js">if (&#x27;serviceWorker&#x27; in navigator) {
  window.addEventListener(&#x27;load&#x27;, () =&gt; {
    navigator.serviceWorker
      .register(&#x27;/sw.js&#x27;)
      .then((registration) =&gt; {
        // tslint:disable:no-console
        console.log(&#x27;SW registered: &#x27;, registration);
      })
      .catch((registrationError) =&gt; {
        console.log(&#x27;SW registration failed: &#x27;, registrationError);
      });
  });
}
</code></pre><p>Put it together and...</p><h2>What Have We Got?</h2><p>Let&#x27;s <code>yarn build</code> again.</p><p><img src="../static/blog/2017-11-19-the-typescript-webpack-pwa/Screenshot%2B2017-11-19%2B21.55.18.png"/></p><p>Oooohh look! A service worker is with us. Does it work? Let&#x27;s find out... <code>http-server ./dist</code> Browse to <a href="http://localhost:8080">http://localhost:8080</a> and let&#x27;s have a look at the console.</p><p><img src="../static/blog/2017-11-19-the-typescript-webpack-pwa/Screenshot%2B2017-11-19%2B21.34.54.png"/></p><p>Looks very exciting. So now the test; let&#x27;s go offline and refresh:</p><p><img src="../static/blog/2017-11-19-the-typescript-webpack-pwa/Screenshot%2B2017-11-19%2B22.01.37.png"/></p><p>You are looking at the 200s of success. You&#x27;re now running with webpack and TypeScript and you have built a Progressive Web Application. Feel good about life.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[TypeScript Definitions, webpack and Module Types]]></title>
            <link>https://blog.johnnyreilly.com/2017/10/20/typescript-definitions-webpack-and-module-types</link>
            <guid>TypeScript Definitions, webpack and Module Types</guid>
            <pubDate>Fri, 20 Oct 2017 00:00:00 GMT</pubDate>
            <description><![CDATA[A funny thing happened on the way to the registry the other day. Something changed in an npm package I was using and confusion arose. You can read my unfiltered confusion here but here's the slightly clearer explanation.]]></description>
            <content:encoded><![CDATA[<p>A funny thing happened on the way to the registry the other day. Something changed in an npm package I was using and confusion arose. You can read my unfiltered confusion <a href="https://github.com/Microsoft/TypeScript/issues/18791">here</a> but here&#x27;s the slightly clearer explanation.</p><h2>The TL;DR</h2><p>When modules are imported, your loader will decide which module format it wants to use. CommonJS / AMD etc. The loader decides. It&#x27;s important that the export is of the same &quot;shape&quot; regardless of the module format. For 2 reasons:</p><ol><li>You want to be able to reliably use the module regardless of the choice that your loader has made for which export to use.</li><li>Because when it comes to writing type definition files for modules, there is support for a <em>single</em> external definition. Not one for each module format.</li></ol><p><img src="../static/blog/2017-10-20-typescript-definitions-webpack-and-module-types/one-definition-to-rule-them-all.jpg"/></p><h2>The DR</h2><p>Once upon a time we decided to use <a href="https://github.com/MikeMcl/big.js/">big.js</a> in our project. It&#x27;s popular and my old friend <a href="https://twitter.com/nycdotnet">Steve Ognibene</a> apparently originally wrote the type definitions which can be found <a href="https://github.com/DefinitelyTyped/DefinitelyTyped/tree/master/types/big.js">here</a>. Then the definitions were updated by <a href="https://github.com/googol">Miika Hänninen</a>. And then there was pain.</p><h2>UMD / CommonJS <!-- -->*<!-- -->*<!-- -->and<!-- -->*<!-- -->*<!-- --> Global exports oh my!</h2><p>My usage code was as simple as this:</p><pre><code class="language-js">import * as BigJs from &#x27;big.js&#x27;;
const lookABigJs = new BigJs(1);
</code></pre><p>If you execute it in a browser it works. It makes me a <code>Big</code>. However the TypeScript compiler is <!-- -->*<!-- -->*<!-- -->not<!-- -->*<!-- -->*<!-- --> happy. No siree. Nope. It&#x27;s bellowing at me:</p><pre><code class="language-ts">[ts] Cannot use &#x27;new&#x27; with an expression whose type lacks a call or construct signature.
</code></pre><p>So I think: &quot;Huh! I guess Miika just missed something off when he updated the definition files. No bother. I&#x27;ll fix it.&quot; I take a look at how <code>big.js</code> exposes itself to the outside world. At the time, thusly:</p><pre><code class="language-js">//AMD.
if (typeof define === &#x27;function&#x27; &amp;&amp; define.amd) {
  define(function () {
    return Big;
  });

  // Node and other CommonJS-like environments that support module.exports.
} else if (typeof module !== &#x27;undefined&#x27; &amp;&amp; module.exports) {
  module.exports = Big;
  module.exports.Big = Big;
  //Browser.
} else {
  global.Big = Big;
}
</code></pre><p>Now, we were using webpack as our script bundler / loader. webpack is supersmart; it can take all kinds of module formats. So although it&#x27;s more famous for supporting CommonJS, it can roll with AMD. That&#x27;s exactly what&#x27;s happening here. When webpack encounters the above code, it goes with the AMD export. So at runtime, <code>import * as BigJs from &#x27;big.js&#x27;;</code> lands up resolving to the <code>return Big;</code> above.</p><p>Now this turns out to be super-relevant. I took a look at the relevant portion of the definition file and found this:</p><pre><code class="language-js">export const Big: BigConstructor;
</code></pre><p>Which tells me that <code>Big</code> is being exported as a subproperty of the module. That makes sense; that lines up with the <code>module.exports.Big = Big;</code> statement in the the big.js source code. There&#x27;s a &quot;gotcha&quot; coming; can you guess what it is?</p><p>The problem is that our type definition is not exposing <code>Big</code> as a default export. So even though it&#x27;s there; TypeScript won&#x27;t let us use it. What&#x27;s killing us further is that webpack is loading the AMD export which <em>doesn&#x27;t</em> have <code>Big</code> as a subproperty of the module. It only has it as a default.</p><p><a href="https://twitter.com/kitsonk">Kitson Kelly</a> expressed the problem well when he said:</p><blockquote><p>there is a different shape depending on which loader is being used and I am not sure that makes a huge amount of sense. The AMD shape is different than the CommonJS shape. While that is technically possible, that feels like that is an issue.</p></blockquote><h2>One Definition to Rule Them All</h2><p>He&#x27;s right; it is an issue. From a TypeScript perspective there is no way to write a definition file that allows for different module &quot;shapes&quot; depending upon the module type. If you really wanted to do that you&#x27;re reduced to writing multiple definition files. That&#x27;s blind alley anyway; what you want is a module to expose itself with the same &quot;shape&quot; regardless of the module type. What you want is this:</p><p><code>AMD === CommonJS === Global</code></p><p>And that&#x27;s what we now have! Thanks to <a href="https://github.com/mikemcl">Michael McLaughlin</a>, author of big.js, <a href="https://github.com/MikeMcl/big.js/pull/87#issuecomment-332663587">version 4.0 unified the export shape of the package</a>. Miika Hänninen submitted another <a href="https://github.com/DefinitelyTyped/DefinitelyTyped/pull/20096">PR</a> which fixed up the type definitions. And once again the world is a beautiful place!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Working with Extrahop on webpack and ts-loader]]></title>
            <link>https://blog.johnnyreilly.com/2017/10/19/working-with-extrahop-on-webpack-and-ts</link>
            <guid>Working with Extrahop on webpack and ts-loader</guid>
            <pubDate>Thu, 19 Oct 2017 00:00:00 GMT</pubDate>
            <description><![CDATA[I'm quite proud of this//www.extrahop.com/company/blog/2017/extrahop-webpack-accelerating-build-times/]]></description>
            <content:encoded><![CDATA[<p>I&#x27;m quite proud of this: <a href="https://www.extrahop.com/company/blog/2017/extrahop-webpack-accelerating-build-times/">https://www.extrahop.com/company/blog/2017/extrahop-webpack-accelerating-build-times/</a></p><p>If you didn&#x27;t know, I spend a good amount of my spare time hacking on open source software. You may not know what that is. I would describe OSS as software made with ❤ by people, for other people to use.</p><p>You are currently reading this on a platform that was built using OSS. It&#x27;s all around you, every day. It&#x27;s on your phone, on your computer, on your TV. It&#x27;s everywhere.</p><p>It&#x27;s my hobby, it&#x27;s part of my work. This specifically was one of those tremendously rare occasions when I got paid directly to work on my hobby, with people much brighter than me. It was brilliant. I loved it; it was a privilege.</p><p>Here&#x27;s to Open Source!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[fork-ts-checker-webpack-plugin code clickability]]></title>
            <link>https://blog.johnnyreilly.com/2017/09/12/fork-ts-checker-webpack-plugin-code</link>
            <guid>fork-ts-checker-webpack-plugin code clickability</guid>
            <pubDate>Tue, 12 Sep 2017 00:00:00 GMT</pubDate>
            <description><![CDATA[My name is John Reilly and I'm a VS Code addict. There I said it. I'm also a big fan of TypeScript and webpack. I've recently switched to using the awesome fork-ts-checker-webpack-plugin to speed up my builds.]]></description>
            <content:encoded><![CDATA[<p>My name is John Reilly and I&#x27;m a VS Code addict. There I said it. I&#x27;m also a big fan of TypeScript and webpack. I&#x27;ve recently switched to using the awesome <a href="https://www.npmjs.com/package/fork-ts-checker-webpack-plugin"><code>fork-ts-checker-webpack-plugin</code></a> to speed up my builds.</p><p>One thing I love is using VS Code both as my editor and my terminal. Using the fork-ts-checker-webpack-plugin I noticed a problem when TypeScript errors showed up in the terminal:</p><p><img src="../static/blog/2017-09-12-fork-ts-checker-webpack-plugin-code/Screenshot%2B2017-09-12%2B06.12.25.png"/></p><p>Take a look at the red file location in the console above. What&#x27;s probably not obvious from the above screenshot is that it is <strong>not clickable</strong>. I&#x27;m used to being able to click on link in the console and bounce straight to the error location. It&#x27;s a really productive workflow; see a problem, click on it, be taken to the cause, fix it.</p><p>I want to click on &quot;<code>C:/source/ts-loader/examples/fork-ts-checker/src/fileWithError.ts(2,7)</code>&quot; and have VS Code open up <code>fileWithError.ts</code>, ideally at line 2 and column 7. But here it&#x27;s not working. Why?</p><p>Well, I initially got this slightly wrong; I thought it was about the formatting of the file path. It is. I thought that having the line number and column number in parentheses after the path (eg <code>&quot;(2,7)&quot;</code>) was screwing over VS Code. It isn&#x27;t. Something else is. Look closely at the screenshot; what do you see? Do you notice how the colour of the line number / column number is different to the path? In the words of <a href="https://youtu.be/281jMxOvP5k">Delbert Wilkins</a>: that&#x27;s crucial.</p><p>Yup, the colour change between the path and the line number / column number is the problem. I&#x27;ve submitted a <a href="https://github.com/Realytics/fork-ts-checker-webpack-plugin/pull/48">PR to fix this</a> that I hope will get merged. In the meantime you can avoid this issue by dropping this code into your <code>webpack.config.js</code>:</p><pre><code class="language-js">var chalk = require(&#x27;chalk&#x27;);
var os = require(&#x27;os&#x27;);

function clickableFormatter(message, useColors) {
  var colors = new chalk.constructor({ enabled: useColors });
  var messageColor = message.isWarningSeverity()
    ? colors.bold.yellow
    : colors.bold.red;
  var fileAndNumberColor = colors.bold.cyan;
  var codeColor = colors.grey;
  return [
    messageColor(message.getSeverity().toUpperCase() + &#x27; in &#x27;) +
      fileAndNumberColor(
        message.getFile() +
          &#x27;(&#x27; +
          message.getLine() +
          &#x27;,&#x27; +
          message.getCharacter() +
          &#x27;)&#x27;
      ) +
      messageColor(&#x27;:&#x27;),

    codeColor(message.getFormattedCode() + &#x27;: &#x27;) + message.getContent(),
  ].join(os.EOL);
}

module.exports = {
  // Other config...
  module: {
    rules: [
      {
        test: /\.tsx?$/,
        loader: &#x27;ts-loader&#x27;,
        options: { transpileOnly: true },
      },
    ],
  },
  resolve: {
    extensions: [&#x27;.ts&#x27;, &#x27;.tsx&#x27;, &#x27;js&#x27;],
  },
  plugins: [
    new ForkTsCheckerWebpackPlugin({ formatter: clickableFormatter }), // Here we get our clickability back
  ],
};
</code></pre><p>With that in place, what do you we have? This:</p><p><img src="../static/blog/2017-09-12-fork-ts-checker-webpack-plugin-code/Screenshot%2B2017-09-12%2B06.35.48.png"/></p><p>VS Code clickability; it&#x27;s a beautiful thing.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[TypeScript + Webpack: Super Pursuit Mode]]></title>
            <link>https://blog.johnnyreilly.com/2017/09/07/typescript-webpack-super-pursuit-mode</link>
            <guid>TypeScript + Webpack: Super Pursuit Mode</guid>
            <pubDate>Thu, 07 Sep 2017 00:00:00 GMT</pubDate>
            <description><![CDATA[This post also featured as a webpack Medium publication.]]></description>
            <content:encoded><![CDATA[<p><em><a href="https://medium.com/webpack/typescript-webpack-super-pursuit-mode-83cc568dea79">This post also featured as a webpack Medium publication</a>.</em></p><p>If you&#x27;re like me then you&#x27;ll like TypeScript and you&#x27;ll like module bundling with webpack. You may also like speedy builds. That&#x27;s completely understandable. The fact of the matter is, you sacrifice a bit of build speed to have webpack in the mix. Wouldn&#x27;t it be great if we could even up the difference?</p><p>I&#x27;m the primary maintainer of ts-loader, a TypeScript loader for webpack. Just recently a couple of PRs were submitted that said, in other words: ts-loader is like this:</p><p><img src="../static/blog/2017-09-07-typescript-webpack-super-pursuit-mode/KITT.jpg"/></p><p>But it could be like this:</p><p><img src="../static/blog/2017-09-07-typescript-webpack-super-pursuit-mode/webkitt.jpg"/></p><p>Apologies for the image quality above; there appear to be no high quality pictures out there of KITT in Super Pursuit Mode for me to defame with <a href="https://github.com/plemont">Garan Jenkin</a>&#x27;s atrocious puns.</p><h2>fork-ts-checker-webpack-plugin</h2><p><a href="https://github.com/TypeStrong/ts-loader/issues/537">&quot;Faster type checking with forked process&quot;</a> read the enticing name of the issue. It turned out to be <a href="https://github.com/piotr-oles">Piotr Oleś</a> (<a href="https://twitter.com/OlesDev">@OlesDev</a>) telling the world about his beautiful creation. He&#x27;d put together a mighty fine plugin that can be used alongside ts-loader called the <a href="https://github.com/Realytics/fork-ts-checker-webpack-plugin">fork-ts-checker-webpack-plugin</a>. The name is a bit of a mouthful but the purpose is mouth-watering. To quote the README, it is a:</p><blockquote><p>Webpack plugin that runs typescript type checker on a separate process.</p></blockquote><p>What does this mean and how does this fit with ts-loader? Well, ts-loader does 2 jobs:</p><ol><li>It transpiles your TypeScript into JavaScript and hands it off to webpack</li><li>It collects any TypeScript compilation errors and reports them to webpack</li></ol><p>What this plugin does is say, &quot;forget about #2 - we&#x27;ve got this.&quot; It removes the responsibility for type checking from ts-loader, so the only work ts-loader does is transpilation. In the meantime, the all important type checking is still happening. To be honest, there would be little reason to recommend this approach otherwise. The difference is <code>fork-ts-checker-webpack-plugin</code> is doing the heavy lifting <strong>in a separate process</strong>. This provides a nice performance boost to your workflow. ts-loader is doing <strong>less</strong> and that&#x27;s a <u>good thing</u></p><p>.</p><p>The approach used here is similar to that employed by awesome-typescript-loader. ATL is another TypeScript loader for webpack by the excellent <a href="https://github.com/s-panferov">Stanislav Panferov</a>. ATL also has a technique for performing typechecking in a forked process. fork-ts-checker-webpack-plugin was an effort by Piotr to implement something similar but with improved incremental build performance.</p><p>How do we use it? Add fork-ts-checker-webpack-plugin as a <code>devDependency</code> of your project and then amend the <code>webpack.config.js</code> to set ts-loader into <code>transpileOnly</code> mode and drop the plugin into the mix:</p><pre><code class="language-js">var ForkTsCheckerWebpackPlugin = require(&#x27;fork-ts-checker-webpack-plugin&#x27;);

var webpackConfig = {
  // other config...
  context: __dirname, // to automatically find tsconfig.json
  module: {
    rules: [
      {
        test: /\.tsx?$/,
        loader: &#x27;ts-loader&#x27;,
        options: {
          // disable type checker - we will use it in fork plugin
          transpileOnly: true,
        },
      },
    ],
  },
  plugins: [new ForkTsCheckerWebpackPlugin()],
};
</code></pre><p>If you&#x27;d like to see an example of how to use the plugin then take a look at a <a href="https://github.com/TypeStrong/ts-loader/tree/master/examples/fork-ts-checker">simple example</a> and a <a href="https://github.com/TypeStrong/ts-loader/tree/master/examples/react-babel-karma-gulp-fork-ts-checker">more involved one</a>.</p><h2>HappyPack</h2><p>Not so long ago I didn&#x27;t know what <strike>happyness</strike></p><p><a href="https://github.com/amireh/happypack">HappyPack</a> was. &quot;Happiness in the form of faster webpack build times.&quot; That&#x27;s what it is.</p><blockquote><p>HappyPack makes webpack builds faster by allowing you to transform multiple files in parallel.</p></blockquote><p>It does this by spinning up multiple threads, each with their own loaders inside. We wanted to do this with ts-loader; to have multiple instances of ts-loader running. Work can then be divided up across these separate loaders. Isn&#x27;t multi-threading great?</p><p>ts-loader did not initially play nicely with HappyPack; essentially this is because ts-loader touches parts of webpack&#x27;s API that HappyPack replaces. The entirely wonderful <a href="https://github.com/aindlq">Artem Kozlov</a> submitted a <a href="https://github.com/TypeStrong/ts-loader/pull/547">PR which added HappyPack support to ts-loader</a>. Support essentially amounts to switching ts-loader to run in <code>transpileOnly</code> mode and ensuring that there is no attempt to talk to parts of the webpack API that HappyPack removes.</p><p>It would be hard to recommend using HappyPack as is because, as with <code>transpileOnly</code> mode you lose all typechecking. Where it becomes worthwhile is where it is combined with the fork-ts-checker-webpack-plugin so you keep the typechecking.</p><p>Enough with the chitter chatter; how can we achieve this? Add HappyPack as a <code>devDependency</code> of your project and then amend the <code>webpack.config.js</code> as follows:</p><pre><code class="language-js">var HappyPack = require(&#x27;happypack&#x27;);
var ForkTsCheckerWebpackPlugin = require(&#x27;fork-ts-checker-webpack-plugin&#x27;);

module.exports = {
  // other config...
  context: __dirname, // to automatically find tsconfig.json
  module: {
    rules: [
      {
        test: /\.tsx?$/,
        exclude: /node_modules/,
        loader: &#x27;happypack/loader?id=ts&#x27;,
      },
    ],
  },
  plugins: [
    new HappyPack({
      id: &#x27;ts&#x27;,
      threads: 2,
      loaders: [
        {
          path: &#x27;ts-loader&#x27;,
          query: { happyPackMode: true },
        },
      ],
    }),
    new ForkTsCheckerWebpackPlugin({ checkSyntacticErrors: true }),
  ],
};
</code></pre><p>Note that the ts-loader options are now configured via the HappyPack <code>query</code> and that we&#x27;re setting ts-loader with the <code>happyPackMode</code> option set.</p><p>There&#x27;s one other thing to note which is important; we&#x27;re now passing the <code>checkSyntacticErrors</code> option to the fork plugin. This ensures that the plugin checks for both syntactic errors (eg <code>const array = [{} {}];</code>) and semantic errors (eg <code>const x: number = &#x27;1&#x27;;</code>). By default the plugin only checks for semantic errors. This is because when ts-loader is used with <code>transpileOnly</code> set, ts-loader will still report syntactic errors. But when used in <code>happyPackMode</code> it does not.</p><p>If you&#x27;d like to see an example of how to use HappyPack then once again we have a <a href="https://github.com/TypeStrong/ts-loader/tree/master/examples/happypack">simple example</a> and a <a href="https://github.com/TypeStrong/ts-loader/tree/master/examples/react-babel-karma-gulp-happypack">more involved one</a>.</p><h2><code>thread-loader</code> <!-- -->+<!-- --> <code>cache-loader</code></h2><p>You might have some reservations about using HappyPack. First of all the quirky configuration required makes your webpack config rather less comprehensible. Also, HappyPack is not officially blessed by webpack. It is a side project developed externally from webpack and there&#x27;s no guarantees that new versions of webpack won&#x27;t break it. Neither of these are reasons not to use HappyPack but they are things to bear in mind.</p><p>What if there were a way to parallelise our builds which dealt with these issues? Well, there is! By using <a href="https://github.com/webpack-contrib/thread-loader">thread-loader</a> and <a href="https://github.com/webpack-contrib/cache-loader">cache-loader</a> in combination you can both feel happy that you&#x27;re using an official webpack workflow and you can have a config that&#x27;s less confusing.</p><p>What would that config look like? This:</p><pre><code class="language-js">var ForkTsCheckerWebpackPlugin = require(&#x27;fork-ts-checker-webpack-plugin&#x27;);

module.exports = {
  // other config...
  context: __dirname, // to automatically find tsconfig.json
  module: {
    rules: {
      test: /\.tsx?$/,
      use: [
        { loader: &#x27;cache-loader&#x27; },
        {
          loader: &#x27;thread-loader&#x27;,
          options: {
            // there should be 1 cpu for the fork-ts-checker-webpack-plugin
            workers: require(&#x27;os&#x27;).cpus().length - 1,
          },
        },
        {
          loader: &#x27;ts-loader&#x27;,
          options: {
            happyPackMode: true, // IMPORTANT! use happyPackMode mode to speed-up compilation and reduce errors reported to webpack
          },
        },
      ],
    },
  },
  plugins: [new ForkTsCheckerWebpackPlugin({ checkSyntacticErrors: true })],
};
</code></pre><p>As you can see the configuration is much cleaner than with HappyPack. Interestingly ts-loader still needs to run in &quot;<code>happyPackMode</code>&quot; and that&#x27;s because thread-loader is essentially behaving in the same fashion as with HappyPack and so ts-loader needs to behave in the same way. Probably ts-loader should have a more generic flag name than &quot;<code>happyPackMode</code>&quot;. (Famously, naming things is hard; so if you&#x27;ve a good idea, tell me!)</p><p>These loaders are new and so tread carefully. My own experiences have been pretty positive but your mileage may vary. Do note that, as with HappyPack, the thread-loader is highly configurable.</p><p>If you&#x27;d like to see an example of how to use thread-loader and cache-loader then once again we have a <a href="https://github.com/TypeStrong/ts-loader/tree/master/examples/thread-loader">simple example</a> and a <a href="https://github.com/TypeStrong/ts-loader/tree/master/examples/react-babel-karma-gulp-thread-loader">more involved one</a>.</p><h2>All This Could Be Yours...</h2><blockquote><p>Wow! It looks like we can cut our build time by 4 minutes! <a href="https://twitter.com/hashtag/webpack?src=hash">#<!-- -->webpack</a><a href="https://twitter.com/typescriptlang">@typescriptlang</a> // cc <a href="https://twitter.com/johnny_reilly">@johnny_reilly</a><a href="https://t.co/gjvy9SLBAT">pic.twitter.com/gjvy9SLBAT</a></p><p>— Donald Pipowitch (@PipoPeperoni) <a href="https://twitter.com/PipoPeperoni/status/878148978356834304">June 23, 2017</a></p></blockquote><script src="//platform.twitter.com/widgets.js" charSet="utf-8"></script><p>In this post we&#x27;re improving build speeds with TypeScript and webpack in 3 ways:</p><dl><dt>fork-ts-checker-webpack-plugin</dt><dd>With this plugin in play ts-loader only performs transpilation. ts-loader is doing less so the build is faster.</dd><dt>HappyPack</dt><dd>With HappyPack in the mix, the build is parallelised. That parallelisation means the build is faster.</dd><dt>thread-loader / cache-loader</dt><dd>With thread-loader and cache-loader, again the build is parallelised and the build is faster.</dd></dl><iframe src="https://giphy.com/embed/Bo2WsocASVBm0" width="240" height="180" frameBorder="0"></iframe>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Oh the Glamour of Open Source]]></title>
            <link>https://blog.johnnyreilly.com/2017/08/30/oh-glamour-of-open-source</link>
            <guid>Oh the Glamour of Open Source</guid>
            <pubDate>Wed, 30 Aug 2017 00:00:00 GMT</pubDate>
            <description><![CDATA[Here's how my life panned out in the early hours of Wednesday 30th September 2017:]]></description>
            <content:encoded><![CDATA[<p>Here&#x27;s how my life panned out in the early hours of Wednesday 30th September 2017:</p><dl><dt>2 am</dt><dd>awoken by Lisette having a nightmare</dd><dt>3 am</dt><dd>gave up hope of getting back to sleep upstairs and headed for the sofa</dd><dt>4 am</dt><dd>still not asleep and discovered a serious gap in an open source project I help out with</dd><dt>4:30 am</dt><dd> come up with idea for a fix</dd><dt>4:45 am</dt><dd> accidentally delete a repo that I and many others care about from GitHub</dd><dt>4:50 am</dt><dd> recover said repo from backups (sweet mercy how could I be so stupid?)</dd><dt>4:55 am</dt><dd> actually succeed in cloning the repo I want to hack on </dd><dt>5:30 am</dt><dd> implement fix and <a href="https://github.com/Realytics/fork-ts-checker-webpack-plugin/pull/43">send PR</a></dd><dt>5:35 am</dt><dd> go for a walk round the river</dd><dt>6:30 am</dt><dd> realise I didn&#x27;t submit a test for the changed functionality</dd><dt>6:35 am</dt><dd> write test only to discover I can&#x27;t run the test pack on Windows</dd><dt>6:40 am</dt><dd> add test to PR anyway so I can see test results when Travis runs on each commit.</dd><dt>7 am</dt><dd>despair at the duration of my feedback loop, totally fail to get my tests to pass</dd><dt>7:10 am</dt><dd> stub my toe really badly on a train set Benjamin has been busily assembling beneath my feet</dd><dt>7:11 am</dt><dd> give in and literally beg the project owner in Paris to fix the tests for me. He takes pity on me and agrees. Possibly because I gave him emoji tulips 🌷</dd><dt>7:12 am</dt><dd> feel like a slight failure and profoundly tired.</dd></dl><p>Oh the glamour of open source.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Karma: From PhantomJS to Headless Chrome]]></title>
            <link>https://blog.johnnyreilly.com/2017/08/27/karma-from-phantomjs-to-headless-chrome</link>
            <guid>Karma: From PhantomJS to Headless Chrome</guid>
            <pubDate>Sun, 27 Aug 2017 00:00:00 GMT</pubDate>
            <description><![CDATA[Like pretty much everyone else I've been using PhantomJS to run my JavaScript (or compiled-to-JS) unit tests. It's been great. So when I heard the news that PhantomJS was dead I was genuinely sad. However, the King is dead.... Long live the King! For there is a new hope; it's called Chrome Headless . It's not a separate version of Chrome; rather the ability to run Chrome without a UI is now baked into Google's favourite browser as of v59. (For those history buffs I might as well be clear: the main reason PhantomJS died is because Chrome Headless was in the works.)]]></description>
            <content:encoded><![CDATA[<p>Like pretty much everyone else I&#x27;ve been using PhantomJS to run my JavaScript (or compiled-to-JS) unit tests. It&#x27;s been great. So when I heard the news that <a href="https://news.ycombinator.com/item?id=14105489">PhantomJS was dead</a> I was genuinely sad. However, the King is dead.... Long live the King! For there is a new hope; it&#x27;s called <a href="https://developers.google.com/web/updates/2017/04/headless-chrome">Chrome Headless </a>. It&#x27;s not a separate version of Chrome; rather the ability to run Chrome without a UI is now baked into Google&#x27;s favourite browser as of v59. (For those history buffs I might as well be clear: the main reason PhantomJS died is because Chrome Headless was in the works.)</p><h2>Making the Switch</h2><p>As long as you&#x27;re running Chrome v59 or greater then you can switch. I&#x27;ve just made ts-loader&#x27;s execution test pack run with Chrome Headless instead of PhantomJS and I&#x27;ve rarely been happier. Honest. Some context: the execution test pack runs Jasmine unit tests via the <a href="https://karma-runner.github.io/1.0/index.html">Karma test runner</a>. The move was surprisingly easy and you can see just how minimal it was in the PR <a href="https://github.com/TypeStrong/ts-loader/pull/611/files">here</a>. If you want to migrate a test that runs tests via Karma then this will take you through what you need to do.</p><h2><code>package.json</code></h2><p>You no longer need <code>phantomjs-prebuilt</code> as a dev dependency of your project. That&#x27;s the PhantomJS browser disappearing in the rear view mirror. Next we need to replace <code>karma-phantomjs-launcher</code> with <code>karma-chrome-launcher</code>. These packages are responsible for firing up the browser that the tests are run in and we no longer want to invoke PhantomJS; we&#x27;re Chrome all the way baby.</p><h2><code>karma.conf.js</code></h2><p>You need to tell Karma to use Chrome Headless instead of PhantomJS. You do that by replacing</p><pre><code class="language-js">browsers: [ &#x27;PhantomJS&#x27; ],
</code></pre><p>with</p><pre><code class="language-js">browsers: [ &#x27;ChromeHeadless&#x27; ],
</code></pre><p>That&#x27;s it; job done!</p><h2>Continuous Integration</h2><p>There&#x27;s always one more thing isn&#x27;t there? Yup, ts-loader has CI builds that run on <a href="https://ci.appveyor.com/project/JohnReilly/ts-loader/branch/master">Windows with AppVeyor</a> and <a href="https://travis-ci.org/TypeStrong/ts-loader">Linux with Travis</a>. The AppVeyor build went green on the first run; that&#x27;s because Chrome is installed by default in the AppVeyor build environment. (yay!)</p><p>Travis went red. (boooo!) Travis doesn&#x27;t have Chrome installed by default. But it&#x27;s no biggie; you just need to tweak your <code>.travis.yml</code> like so:</p><pre><code class="language-yml">dist: trusty
addons:
  chrome: stable
</code></pre><p>This includes Chrome in the Travis build environment. Green. Boom!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A Haiku on the Problem with SemVer: Us]]></title>
            <link>https://blog.johnnyreilly.com/2017/07/29/a-haiku-on-problem-with-semver-us</link>
            <guid>A Haiku on the Problem with SemVer: Us</guid>
            <pubDate>Sat, 29 Jul 2017 00:00:00 GMT</pubDate>
            <description><![CDATA[Version numbers wrong We release breaking changes We don't know we do]]></description>
            <content:encoded><![CDATA[<p>Version numbers wrong We release breaking changes We don&#x27;t know we do</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Dynamic import: I've been awaiting you...]]></title>
            <link>https://blog.johnnyreilly.com/2017/07/02/dynamic-import-ive-been-await-ing-you</link>
            <guid>Dynamic import: I've been awaiting you...</guid>
            <pubDate>Sun, 02 Jul 2017 00:00:00 GMT</pubDate>
            <description><![CDATA[One of the most exciting features to ship with TypeScript 2.4 was support for the dynamic import expression. To quote the release blog post:]]></description>
            <content:encoded><![CDATA[<p>One of the most exciting features to ship with TypeScript 2.4 was support for the dynamic import expression. To quote the <a href="https://blogs.msdn.microsoft.com/typescript/2017/06/27/announcing-typescript-2-4/#dynamic-import-expressions">release blog post</a>:</p><blockquote><p>Dynamic <code>import</code> expressions are a new feature in ECMAScript that allows you to asynchronously request a module at any arbitrary point in your program. These modules come back as <code>Promise</code>s of the module itself, and can be <code>await</code>-<!-- -->ed in an async function, or can be given a callback with <code>.then</code>.</p><p>...</p><p>Many bundlers have support for automatically splitting output bundles (a.k.a. “code splitting”) based on these <code>import()</code> expressions, so consider using this new feature with the <code>esnext</code> module target. Note that this feature won’t work with the <code>es2015</code> module target, since the feature is anticipated for ES2018 or later.</p></blockquote><p>As the post makes clear, this adds support for a very bleeding edge ECMAScript feature. This is not fully standardised yet; it&#x27;s currently at <a href="https://github.com/tc39/proposals">stage 3</a> on the TC39 proposals list. That means it&#x27;s at the <a href="https://tc39.github.io/process-document/">Candidate</a> stage and is unlikely to change further. If you&#x27;d like to read more about it then take a look at the official proposal <a href="https://github.com/tc39/proposal-dynamic-import">here</a>.</p><p>Whilst this is super-new, we are still able to use this feature. We just have to jump through a few hoops first.</p><h2>TypeScript Setup</h2><p>First of all, you need to install TypeScript 2.4. With that in place you need to make some adjustments to your <code>tsconfig.json</code> in order that the relevant compiler switches are flipped. What do you need? First of all you need to be targeting ECMAScript 2015 as a minimum. That&#x27;s important specifically because ES2015 contained <code>Promise</code>s which is what dynamic <code>import</code>s produce. The second thing you need is to target the module type of <code>esnext</code>. You&#x27;re likely targeting <code>es2015</code> now, <code>esnext</code> is that <strong>plus</strong> dynamic <code>import</code>s.</p><p>Here&#x27;s a <code>tsconfig.json</code> I made earlier which has the relevant settings set:</p><pre><code class="language-json">{
  &quot;compilerOptions&quot;: {
    &quot;allowSyntheticDefaultImports&quot;: true,
    &quot;lib&quot;: [&quot;dom&quot;, &quot;es2015&quot;],
    &quot;target&quot;: &quot;es2015&quot;,
    &quot;module&quot;: &quot;esnext&quot;,
    &quot;moduleResolution&quot;: &quot;node&quot;,
    &quot;noImplicitAny&quot;: true,
    &quot;noUnusedLocals&quot;: true,
    &quot;noUnusedParameters&quot;: true,
    &quot;removeComments&quot;: false,
    &quot;preserveConstEnums&quot;: true,
    &quot;sourceMap&quot;: true,
    &quot;skipLibCheck&quot;: true
  }
}
</code></pre><h2>Babel Setup</h2><p>At the time of writing, browser support for dynamic <code>import</code> is non-existent. This will likely be the case for some time but it needn&#x27;t hold us back. Babel can step in here and compile our super-new JS into JS that will run in our browsers today.</p><p>You&#x27;ll need to decide for yourself how much you want Babel to do for you. In my case I&#x27;m targeting old school browsers which don&#x27;t yet support ES2015. You may not need to. However, the one thing that you&#x27;ll certainly need is the <a href="https://babeljs.io/docs/plugins/syntax-dynamic-import/">Syntax Dynamic Import</a> plugin. It&#x27;s this that allows Babel to process dynamic <code>import</code> statements.</p><p>These are the options I&#x27;m passing to Babel:</p><pre><code class="language-js">var babelOptions = {
  plugins: [&#x27;syntax-dynamic-import&#x27;],
  presets: [
    [
      &#x27;es2015&#x27;,
      {
        modules: false,
      },
    ],
  ],
};
</code></pre><p>You&#x27;re also going to need something that actually execute the <code>import</code>s. In my case I&#x27;m using webpack...</p><h2>webpack</h2><p>webpack 2 supports <a href="https://webpack.js.org/api/module-methods/#import-"><code>import()</code></a>. So if you webpack set up with <a href="https://github.com/TypeStrong/ts-loader">ts-loader</a> (or awesome-typescript-loader etc), chaining into <a href="https://github.com/babel/babel-loader">babel-loader</a> you should find you have a setup that supports dynamic <code>import</code>. That means a <code>webpack.config.js</code> that looks something like this:</p><pre><code class="language-js">var path = require(&#x27;path&#x27;);
var webpack = require(&#x27;webpack&#x27;);

var babelOptions = {
  plugins: [&#x27;syntax-dynamic-import&#x27;],
  presets: [
    [
      &#x27;es2015&#x27;,
      {
        modules: false,
      },
    ],
  ],
};

module.exports = {
  entry: &#x27;./app.ts&#x27;,
  output: {
    filename: &#x27;bundle.js&#x27;,
  },
  module: {
    rules: [
      {
        test: /\.ts(x?)$/,
        exclude: /node_modules/,
        use: [
          {
            loader: &#x27;babel-loader&#x27;,
            options: babelOptions,
          },
          {
            loader: &#x27;ts-loader&#x27;,
          },
        ],
      },
      {
        test: /\.js$/,
        exclude: /node_modules/,
        use: [
          {
            loader: &#x27;babel-loader&#x27;,
            options: babelOptions,
          },
        ],
      },
    ],
  },
  resolve: {
    extensions: [&#x27;.ts&#x27;, &#x27;.tsx&#x27;, &#x27;.js&#x27;],
  },
};
</code></pre><h2>ts-loader example</h2><p>I&#x27;m one of the maintainers of <a href="https://github.com/TypeStrong/ts-loader">ts-loader</a> which is a TypeScript loader for webpack. When support for dynamic <code>import</code>s landed I wanted to add a test to cover usage of the new syntax with ts-loader.</p><p>We have 2 test packs for ts-loader, one of which is our &quot;execution&quot; test pack. It is so named because it works by spinning up webpack with ts-loader and then using <a href="https://github.com/karma-runner/karma">karma</a> to execute a set of tests. Each &quot;test&quot; in our execution test pack is actually a mini-project with its own test suite (generally <a href="https://jasmine.github.io/">jasmine</a> but that&#x27;s entirely configurabe). Each complete with its own <code>webpack.config.js</code>, <code>karma.conf.js</code> and either a <code>typings.json</code> or <code>package.json</code> for bringing in dependencies. So it&#x27;s a full test of whether code slung with ts-loader and webpack actually executes when the output is plugged into a browser.</p><p>This is the test pack for dynamic <code>import</code>s:</p><pre><code class="language-js">import a from &quot;../src/a&quot;;
import b from &quot;../src/b&quot;;

describe(&quot;app&quot;, () =&gt; {
  it(&quot;a to be &#x27;a&#x27; and b to be &#x27;b&#x27; (classic)&quot;, () =&gt; {
    expect(a).toBe(&quot;a&quot;);
    expect(b).toBe(&quot;b&quot;);
  });

  it(&quot;import results in a module with a default export&quot;, done =&gt; {
    import(&quot;../src/c&quot;).then(c =&gt; {
      // .default is the default export
      expect(c.default).toBe(&quot;c&quot;);

      done();
    }
  });

  it(&quot;import results in a module with an export&quot;, done =&gt; {
    import(&quot;../src/d&quot;).then(d =&gt; {
      // .default is the default export
      expect(d.d).toBe(&quot;d&quot;);

      done();
    }
  });

  it(&quot;await import results in a module with a default export&quot;, async done =&gt; {
    const c = await import(&quot;../src/c&quot;);

    // .default is the default export
    expect(c.default).toBe(&quot;c&quot;);

    done();
  });

  it(&quot;await import results in a module with an export&quot;, async done =&gt; {
    const d = await import(&quot;../src/d&quot;);

    expect(d.d).toBe(&quot;d&quot;);

    done();
  });
});
</code></pre><p>As you can see, it&#x27;s possible to use the dynamic <code>import</code> as a <code>Promise</code> directly. Alternatively, it&#x27;s possible to consume the imported module using TypeScripts support for <code>async</code> / <code>await</code>. For my money the latter option makes for much clearer code.</p><p>If you&#x27;re looking for a complete example of how to use the new syntax then you could do worse than taking the existing test pack and tweaking it to your own ends. The only change you&#x27;d need to make is to strip out the <code>resolveLoader</code> statements in <code>webpack.config.js</code> and <code>karma.conf.js</code>. (They exist to lock the test in case to the freshly built ts-loader stored locally. You&#x27;ll not need this.)</p><p>You can find the test in question <a href="https://github.com/TypeStrong/ts-loader/tree/master/test/execution-tests/2.4.1_babel-importCodeSplitting">here</a>. Happy code splitting!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Windows Defender Step Away From npm]]></title>
            <link>https://blog.johnnyreilly.com/2017/06/11/windows-defender-step-away-from-npm</link>
            <guid>Windows Defender Step Away From npm</guid>
            <pubDate>Sun, 11 Jun 2017 00:00:00 GMT</pubDate>
            <description><![CDATA[Update 18/06/2017]]></description>
            <content:encoded><![CDATA[<h2>Update 18/06/2017</h2><p>Whilst things did improve by fiddling with Windows Defender it wasn&#x27;t a 100% fix which makes me wary. Interestingly, VS Code was always open when I did experience the issue and I haven&#x27;t experienced it when it&#x27;s been closed. So it may be the cause. I&#x27;ve opened <a href="https://github.com/Microsoft/vscode/issues/28593">an issue for this against the VS Code repo</a> <!-- -->-<!-- --> it sounds like other people may be affected as I was. Perhaps this is VS Code and not Windows Defender. Watch that space...</p><h2>Update 12/07/2017</h2><p>The issue was VS Code. The bug has now been fixed and shipped last night with <a href="https://code.visualstudio.com/updates/v1_14">VS Code 1.14.0</a>. Yay!</p><hr/><p>I&#x27;ve recently experienced many of my <code>npm install</code>s failing for no consistent reason. The error message would generally be something along the lines of:</p><pre><code class="language-sh">npm ERR! Error: EPERM: operation not permitted, rename &#x27;C:\dev\training\drrug\node_modules\.staging\@exponent\ngrok-fc327f2a&#x27; -&gt; &#x27;C:\dev\training\drrug\node_modules\@exponent\ngrok&#x27;
</code></pre><p>I spent a good deal of time changing the versions of node and npm I was running; all seemingly to no avail. Regular flakiness which I ascribed to node / npm. I was starting to give up when I read of <a href="https://github.com/react-community/create-react-native-app/issues/191#issuecomment-304073970">other people experiencing similar issues</a>. Encouragingly <a href="https://github.com/fmeira">Fernando Meira</a> suggested a solution:</p><blockquote><p>I got the same problem just doing an npm install. Run with antivirus disabled (if you use Windows Defender, turn off Real-Time protection and Cloud-based protection). That worked for me!</p></blockquote><p>I didn&#x27;t really expect this to work - Windows Defender has been running in the background of my Windows 10 laptop since I&#x27;ve had it. There&#x27;s been no problems with npm installs up until a week or so ago. But given the experience I and others have had I thought I should put it out there: it looks like Windows Defender has it in for npm. Go figure.</p><p>Alas Windows Defender doesn&#x27;t stay dead for long; it&#x27;s like a zombie that rises from the grave no matter how many times you kill it. So you might want to try configuring it to ignore node.exe:</p><p><img src="../static/blog/2017-06-11-windows-defender-step-away-from-npm/Screenshot%2B2017-06-11%2B15.05.47.png"/></p><p>Or switching to Linux...</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[TypeScript: Spare the Rod, Spoil the Code]]></title>
            <link>https://blog.johnnyreilly.com/2017/05/20/typescript-spare-rod-spoil-code</link>
            <guid>TypeScript: Spare the Rod, Spoil the Code</guid>
            <pubDate>Sat, 20 May 2017 00:00:00 GMT</pubDate>
            <description><![CDATA[I've recently started a new role. Perhaps unsurprisingly, part of the technology stack is TypeScript. A couple of days into the new codebase I found a bug. Well, I say I found a bug, TypeScript and VS Code found the bug - I just let everyone else know.]]></description>
            <content:encoded><![CDATA[<p>I&#x27;ve recently started a new role. Perhaps unsurprisingly, part of the technology stack is TypeScript. A couple of days into the new codebase I found a bug. Well, I say I found a bug, TypeScript and VS Code found the bug - I just let everyone else know.</p><p>The flexibility that TypeScript offers in terms of compiler settings is second to none. You can turn up the dial of strictness to your hearts content. Or down. I&#x27;m an &quot;up&quot; man myself.</p><p>The project that I am working on has the dial set fairly low; it&#x27;s pretty much using the default compiler values which are (sensibly) not too strict. I have to say this makes sense for helping people get on board with using TypeScript. Start from a point of low strictness and turn it up when you&#x27;re ready. As you might have guessed, I cranked the dial up on day one on my own machine. I should say that as I did this, I didn&#x27;t foist this on the project at large - I kept it just to my build... I&#x27;m not <!-- -->*<strong>that</strong>*<!-- --> guy!</p><p>I made the below changes to the <code>tsconfig.json</code> file. Details of what each of these settings does can be found in the documentation <a href="https://www.typescriptlang.org/docs/handbook/compiler-options.html">here</a>.</p><pre><code class="language-json">&quot;noImplicitAny&quot;: true,
    &quot;noImplicitThis&quot;: true,
    &quot;noUnusedLocals&quot;: true,
    &quot;noImplicitReturns&quot;: true,
    &quot;noUnusedParameters&quot;: true,
</code></pre><p>I said I found a bug. The nature of the bug was an unused variable; a variable was created in a function but then not used. Here&#x27;s a super simple example:</p><pre><code class="language-ts">function sayHi(name: string) {
  const greeting = `Hi ${name}`;
  return name;
}
</code></pre><p>It&#x27;s an easy mistake to make. I&#x27;ve made this mistake before myself. But with the <code>noUnusedLocals</code> compiler setting in place it&#x27;s now an easy mistake to catch; VS Code lets you know loud and clear:</p><p><img src="../static/blog/2017-05-20-typescript-spare-rod-spoil-code/Screenshot%2B2017-05-20%2B05.58.54.png"/></p><p>The other compiler settings will similarly highlight simple mistakes it&#x27;s possible to make and I&#x27;d recommend using them. I should say I&#x27;ve written this from the perspective of a VS Code user, but this really applies generally to TypeScript usage. So whether you&#x27;re an <a href="http://alm.tools/">alm.tools</a> guy, a WebStorm gal or something else entirely then this too can be yours!</p><p>I&#x27;d also say that the <code>strictNullChecks</code> compiler setting is worth looking into. However, switching an already established project to using that can involve fairly extensive code changes and will also require a certain amount of education of, and buy in from, your team. So whilst I&#x27;d recommend it too, I&#x27;d save that one until last.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Setting Build Version Using AppVeyor and ASP.Net Core]]></title>
            <link>https://blog.johnnyreilly.com/2017/04/25/setting-build-version-using-appveyor</link>
            <guid>Setting Build Version Using AppVeyor and ASP.Net Core</guid>
            <pubDate>Tue, 25 Apr 2017 00:00:00 GMT</pubDate>
            <description><![CDATA[AppVeyor has support for setting the version of a binary during a build. However - this deals with the classic ASP.Net world of AssemblyInfo. I didn't find any reference to support for doing the same with dot net core. Remember, dot net core relies upon a &lt;Version&gt; or a &lt;VersionPrefix&gt; setting in the .csproj file. Personally, &lt;Version&gt; is my jam.]]></description>
            <content:encoded><![CDATA[<p>AppVeyor has <a href="https://www.appveyor.com/docs/build-configuration/#assemblyinfo-patching">support for setting the version of a binary during a build</a>. However - this deals with the classic ASP.Net world of <code>AssemblyInfo</code>. I didn&#x27;t find any reference to support for doing the same with dot net core. Remember, dot net core <a href="https://docs.microsoft.com/en-us/dotnet/articles/core/tools/project-json-to-csproj#version">relies upon a <code>&amp;lt;Version&amp;gt;</code> or a <code>&amp;lt;VersionPrefix&amp;gt;</code> setting in the <code>.csproj</code> file</a>. Personally, <code>&amp;lt;Version&amp;gt;</code> is my jam.</p><p>However, coming up with your own bit of powershell that stamps the version during the build is a doddle; here we go:</p><pre><code class="language-ps">Param($projectFile, $buildNum)

$content = [IO.File]::ReadAllText($projectFile)

$regex = new-object System.Text.RegularExpressions.Regex (&#x27;(&lt;version&gt;)([\d]+.[\d]+.[\d]+)(.[\d]+)(&lt;\/Version&gt;)&#x27;,
         [System.Text.RegularExpressions.RegexOptions]::MultiLine)

$version = $null
$match = $regex.Match($content)
if($match.Success) {
    # from &quot;&lt;version&gt;1.0.0.0&lt;/version&gt;&quot; this will extract &quot;1.0.0&quot;
    $version = $match.groups[2].value
}

# suffix build number onto $version. eg &quot;1.0.0.15&quot;
$version = &quot;$version.$buildNum&quot;

# update &quot;&lt;version&gt;1.0.0.0&lt;/version&gt;&quot; to &quot;&lt;version&gt;$version&lt;/version&gt;&quot;
$content = $regex.Replace($content, &#x27;${1}&#x27; + $version + &#x27;${4}&#x27;)

# update csproj file
[IO.File]::WriteAllText($projectFile, $content)

# update AppVeyor build
Update-AppveyorBuild -Version $version
&lt;/version&gt;
</code></pre><p>You can invoke this script as part of the build process in AppVeyor by adding something like this to your <code>appveyor.yml</code>.</p><pre><code class="language-yml">before_build:
  - ps: .\ModifyVersion.ps1 $env:APPVEYOR_BUILD_FOLDER\src\Proverb.Web\Proverb.Web.csproj $env:APPVEYOR_BUILD_NUMBER
</code></pre><p>It will keep the first 3 parts of the version in your <code>.csproj</code> (eg &quot;1.0.0&quot;) and suffix on the build number supplied by AppVeyor.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[I'm looking for work!]]></title>
            <link>https://blog.johnnyreilly.com/2017/03/30/im-looking-for-work</link>
            <guid>I'm looking for work!</guid>
            <pubDate>Thu, 30 Mar 2017 00:00:00 GMT</pubDate>
            <description><![CDATA[My name is John Reilly. I'm a full stack developer based in London, UK. I'm just coming to the end of a contract (due to finish in April 2017) and I'm starting to look for my next role.]]></description>
            <content:encoded><![CDATA[<p>My name is John Reilly. I&#x27;m a full stack developer based in London, UK. I&#x27;m just coming to the end of a contract (due to finish in April 2017) and I&#x27;m starting to look for my next role.</p><p>I have more than 15 years experience developing software commercially. I&#x27;ve worked in a number of industries including telecoms, advertising, technology (I worked at Microsoft for a time) and, of course, finance. The bulk of my experience is in the finance sector. I&#x27;ve provided consultancy services, building and maintaining applications for both large and small companies; from enterprise to startup.</p><p>My most recent work has been full stack web work; using React on the front end and SignalR (ASP.Net) on the back end. I&#x27;m pragmatic about the tools that I use to deliver software solutions and not tied to any particular technology. That said, I&#x27;ve gravitated towards the handiwork of <a href="https://en.wikipedia.org/wiki/Anders_Hejlsberg">Anders Hejlsberg</a>; starting out with Delphi and being both an early C# and TypeScript adopter. I&#x27;ve built everything from high volume trade feeds with no UI beyond a log file, WinForms apps for call centres, to fully fledged rich web applications with a heavy emphasis on UX.</p><p>I enjoy the challenges of understanding problems and coming up with useful solutions to them. I&#x27;m thrilled when something I&#x27;ve built makes someone&#x27;s life easier. I love to learn and to share my knowledge; both in person and also through writing this blog. (This is the first time I&#x27;ve used a post to seek work.)</p><p>In my spare time I&#x27;m involved with various open source projects including <a href="https://github.com/typestrong/ts-loader">ts-loader</a> and <a href="https://github.com/DefinitelyTyped/DefinitelyTyped">DefinitelyTyped</a> (<a href="https://github.com/orgs/DefinitelyTyped/people">member of the core team</a>). Get in contact with me if you&#x27;re interested in learning more about me. Mail me at <a href="mailto:johnny_reilly@hotmail.com">johnny_reilly@hotmail.com</a> and I can provide you with a CV. You can also find me on <a href="https://github.com/johnnyreilly">GitHub</a>.</p><h2>Update 25/04/2016: Position Filled</h2><p>I&#x27;m happy to say that I&#x27;ve lined up work for the next 6 months or so. Once again I&#x27;ll be working in the financial services industry with one interesting twist. <a href="https://blog.johnnyreilly.com/2014/02/wpf-and-mystic-meg-or-playing.html">In a blog post ages ago I bet that native apps would start to be replaced with SPAs.</a> This has started to happen. I&#x27;ve started to see companies taking a &quot;web-first-and-only&quot; approach to building apps. In that vein, that&#x27;s exactly what I&#x27;m off to build.</p><p>As a result of publishing this blog post I&#x27;ve had some interesting conversations with companies and got to think hard about the direction the industry is taking. I remain excited by JavaScript / TypeScript and React. I&#x27;m hopeful of the possibilities offered by the container world of Docker etc. I&#x27;m enjoying .NET Core and have very high hopes for it. I remain curious about Web Assembly.</p><p>Before I sign off, I know at some point I&#x27;ll be looking for work once again. If there&#x27;s a system you&#x27;d like built, if there&#x27;s some mentoring and training you&#x27;d like done or if you&#x27;d just like to have a conversation I&#x27;m always available to talk. Drop me a line at <a href="mailto:johnny_reilly@hotmail.com">johnny_reilly@hotmail.com</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Debugging ASP.Net Core in VS or Code]]></title>
            <link>https://blog.johnnyreilly.com/2017/03/28/debugging-aspnet-core-in-vs-or-code</link>
            <guid>Debugging ASP.Net Core in VS or Code</guid>
            <pubDate>Tue, 28 Mar 2017 00:00:00 GMT</pubDate>
            <description><![CDATA[I've been using Visual Studio for a long time. Very good it is too. However, it is heavyweight; it does far more than I need. What I really want when I'm working is a fast snappy editor, with intellisense and debugging. What I've basically described is VS Code. It rocks and has long become my go-to editor for TypeScript.]]></description>
            <content:encoded><![CDATA[<p>I&#x27;ve been using Visual Studio for a long time. Very good it is too. However, it is heavyweight; it does far more than I need. What I really want when I&#x27;m working is a fast snappy editor, with intellisense and debugging. What I&#x27;ve basically described is <a href="https://code.visualstudio.com/">VS Code</a>. It rocks and has long become my go-to editor for TypeScript.</p><p>Since I&#x27;m a big C# fan as well I was delighted that editing C# was also possible in Code. What I want now is to be able to debug ASP.Net Core in Visual Studio OR VS Code. Can it be done? Let&#x27;s see....</p><p>I fire up Visual Studio and <code>File -&amp;gt; New Project</code> (yes it&#x27;s a verb now). Select .NET Core and then ASP.Net Core Web Application. OK. We&#x27;ll go for a Web Application. Let&#x27;s not bother with authentication. OK. Wait a couple of seconds and Visual Studio serves up a new project. Hit F5 and we&#x27;re debugging in Visual Studio.</p><p>So far, so straightforward. What will VS Code make of this?</p><p>I cd my way to the root of my new ASP.Net Core Web Application and type the magical phrase &quot;code .&quot;. Up it fires. I feel lucky, let&#x27;s hit &quot;F5&quot;. Huh, a dropdown shows up saying <code>&quot;Select Environment&quot;</code> and offering me the options of Chrome and Node. Neither do I want. It&#x27;s about this time I remember this is a clean install of VS Code and doesn&#x27;t yet have the C# extension installed. In fact, if I open a C# file it up it tells me and recommends that I install. Well that&#x27;s nice. I take it up on the kind offer; install and reload.</p><p>When it comes back up I see the following entries in the &quot;output&quot; tab:</p><pre><code class="language-ts">Updating C# dependencies...
Platform: win32, x86_64 (win7-x64)

Downloading package &#x27;OmniSharp (.NET 4.6 / x64)&#x27; (20447 KB) .................... Done!
Downloading package &#x27;.NET Core Debugger (Windows / x64)&#x27; (39685 KB) .................... Done!

Installing package &#x27;OmniSharp (.NET 4.6 / x64)&#x27;
Installing package &#x27;.NET Core Debugger (Windows / x64)&#x27;

Finished
</code></pre><p>Note that mention of &quot;debugger&quot; there? Sounds super-promising. There&#x27;s also some prompts: <code>&quot;There are unresolved dependencies from &#x27;WebApplication1/WebApplication1.csproj&#x27;. Please execute the restore command to continue&quot;</code></p><p>So it wants me to <code>dotnet restore</code>. It&#x27;s even offering to do that for me! Have at you; I let it.</p><pre><code class="language-ts">Welcome to .NET Core!
---------------------
Learn more about .NET Core @ https://aka.ms/dotnet-docs. Use dotnet --help to see available commands or go to https://aka.ms/dotnet-cli-docs.

Telemetry
--------------
The .NET Core tools collect usage data in order to improve your experience. The data is anonymous and does not include command-line arguments. The data is collected by Microsoft and shared with the community.
You can opt out of telemetry by setting a DOTNET_CLI_TELEMETRY_OPTOUT environment variable to 1 using your favorite shell.
You can read more about .NET Core tools telemetry @ https://aka.ms/dotnet-cli-telemetry.

Configuring...
-------------------
A command is running to initially populate your local package cache, to improve restore speed and enable offline access. This command will take up to a minute to complete and will only happen once.
Decompressing Decompressing 100% 4026 ms
Expanding 100% 34814 ms
  Restoring packages for c:\Source\Debugging\WebApplication1\WebApplication1\WebApplication1.csproj...
  Restoring packages for c:\Source\Debugging\WebApplication1\WebApplication1\WebApplication1.csproj...
  Restore completed in 734.05 ms for c:\Source\Debugging\WebApplication1\WebApplication1\WebApplication1.csproj.
  Generating MSBuild file c:\Source\Debugging\WebApplication1\WebApplication1\obj\WebApplication1.csproj.nuget.g.props.
  Writing lock file to disk. Path: c:\Source\Debugging\WebApplication1\WebApplication1\obj\project.assets.json
  Restore completed in 1.26 sec for c:\Source\Debugging\WebApplication1\WebApplication1\WebApplication1.csproj.

  NuGet Config files used:
      C:\Users\johnr\AppData\Roaming\NuGet\NuGet.Config
      C:\Program Files (x86)\NuGet\Config\Microsoft.VisualStudio.Offline.config

  Feeds used:
      https://api.nuget.org/v3/index.json
      C:\Program Files (x86)\Microsoft SDKs\NuGetPackages\
Done: 0.
</code></pre><p>The other prompt says <code>&quot;Required assets to build and debug are missing from &#x27;WebApplication1&#x27;. Add them?&quot;</code>. This also sounds very promising and I give it the nod. This creates a <code>.vscode</code> directory and 2 enclosed files; <code>launch.json</code> and <code>tasks.json</code>.</p><p>So lets try that F5 thing again... http://localhost:5000/ is now serving the same app. That looks pretty good. So lets add a breakpoint to the <code>HomeController</code> and see if we can hit it:</p><p><img src="../static/blog/2017-03-28-debugging-aspnet-core-in-vs-or-code/firstgo.png"/></p><p>Well I can certainly add a breakpoint but all those red squigglies are unnerving me. Let&#x27;s clean the slate. If you want to simply do that in VS Code hold down <code>CTRL+SHIFT+P</code> and then type &quot;reload&quot;. Pick &quot;Reload window&quot;. A couple of seconds later we&#x27;re back in and Code is looking much happier. Can we hit our breakpoint?</p><p><img src="../static/blog/2017-03-28-debugging-aspnet-core-in-vs-or-code/secondgo.png"/></p><p>Yes we can! So you&#x27;re free to develop in either Code or VS; the choice is yours. I think that&#x27;s pretty awesome - and well done to all the peeople behind Code who&#x27;ve made this a pretty seamless experience!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Under the Duck: An Afternoon in Open Source]]></title>
            <link>https://blog.johnnyreilly.com/2017/02/23/under-duck-afternoon-in-open-source</link>
            <guid>Under the Duck: An Afternoon in Open Source</guid>
            <pubDate>Thu, 23 Feb 2017 00:00:00 GMT</pubDate>
            <description><![CDATA[Have you ever wondered what happens behind the scenes of open source projects? One that I'm involved with is ts-loader; a TypeScript loader for webpack. Yesterday was an interesting day in the life of ts-loader and webpack; things unexpectedly broke. Oh and don't worry, they're fixed now.]]></description>
            <content:encoded><![CDATA[<p>Have you ever wondered what happens behind the scenes of open source projects? One that I&#x27;m involved with is <a href="https://github.com/typestrong/ts-loader">ts-loader</a>; a TypeScript loader for webpack. Yesterday was an interesting day in the life of ts-loader and webpack; things unexpectedly broke. Oh and don&#x27;t worry, they&#x27;re fixed now.</p><p>How things panned out reflects well on the webpack community. I thought it might be instructive to take a look at the legs furiously paddling underneath the duck of open source. What follows is a minute by minute account of my life on the afternoon of Wednesday 22nd February 2017:</p><h3>3:55pm</h3><p>I&#x27;m sat at my desk in the City of London. I have to leave at 4pm to go to the dentist. I&#x27;m working away on a project which is built and bundled using ts-loader and webpack. However, having just npm installed and tried to spin up webpack in watch mode, I discover that everything is broken. Watch mode is not working - there&#x27;s an error being thrown in ts-loader. It&#x27;s to do with a webpack property called <code>mtimes</code>. ts-loader depends upon it and it looks like it is no longer always passed through. Go figure. ### 4:01pm</p><p>I&#x27;ve got to go. I&#x27;m 15 minutes from Bank station. So, I grab my bag and scarper out the door. On my phone I notice <a href="https://github.com/TypeStrong/ts-loader/issues/479">an issue</a> has been raised - other people are being affected by the problem too. As I trot down the various alleys that lead to the station I wonder whether I can work around this issue. Using GitHub to fork, edit code and submit a PR on a mobile phone is possible. Just. But it&#x27;s certainly not easy...</p><p><a href="https://github.com/TypeStrong/ts-loader/pull/481">My PR is in</a>, the various test packs are starting to execute somewhere out there in Travis and Appveyor-land. Then I notice <a href="https://github.com/mredbishop">Ed Bishop</a> has submitted a <a href="https://github.com/TypeStrong/ts-loader/pull/480">near identical PR</a>. Yay Ed! I&#x27;m always keen to encourage people to contribute and so I intend to merge that PR rather than my own.</p><h3>16:12</h3><p>Rubbish. The Waterloo and City Line is out of action. I need to get across London to reach Waterloo or I&#x27;ll miss my appointment. It&#x27;s time to start running....</p><iframe width="560" height="315" src="https://www.youtube.com/embed/4IBGernmtKA" frameBorder="0"></iframe><h3>16:15</h3><p>It&#x27;s rather nagging at me that behaviour has changed without warning. This has been reliably in place the entire time I&#x27;ve been involved with ts-loader / webpack. Why now? I don&#x27;t see any obvious mentions on the webpack GitHub repo. So I head over to the webpack Slack channel and ask: (conversation slightly abridged)</p><blockquote><h4>johnny_reilly</h4><p>Hey all, has something happened to <code>mtimes</code>? Behaviour seems to have changed - now undefined occasionally during watch mode. A PR has been raised against ts-loader to work around this <a href="https://github.com/TypeStrong/ts-loader/pull/480#issuecomment-281714600">https://github.com/TypeStrong/ts-loader/pull/480#issuecomment-281714600</a></p><p>However I&#x27;m wondering if this should actually be merged given behaviour has changed unexpectedly</p><h4>sokra</h4><p>ah...</p><p>i removed it. I thought it was unused.</p><h4>johnny_reilly</h4><p>It&#x27;s definitely not!</p><h4>sokra</h4><p>it&#x27;s not in the public API^^</p><p>Any reason why you are not using <code>getTimes()</code>?</p><p>...</p><h4>johnny_reilly</h4><p>Okay, I&#x27;m on a train and won&#x27;t be near a computer for a while. ts-loader is presently broken because it depends on mtimes. Would it be possible for you to add this back at least for now. I&#x27;m aware many people depend on ts-loader and are now broken. #### sokra</p><p>sure, I readd it but deprecate it.</p><p>...</p><h4>sean.larkin</h4><p>@sokra is this the change you just made for that watchpack bug fix? Or unlrelated, just wanted to track if I didn&#x27;t already have the change/issue #### sokra</p><p><a href="https://github.com/webpack/watchpack/pull/48">https://github.com/webpack/watchpack/pull/48</a></p><h4>johnny_reilly</h4><p>This is what the present code does:</p><pre><code class="language-js">const watcher =
  watching.compiler.watchFileSystem.watcher ||
  watching.compiler.watchFileSystem.wfs.watcher;
</code></pre><p>And then <code>.mtimes</code></p><p>Should I be able to do <code>.getTimes()</code> instead?</p><h4>sokra</h4><p>actually you can&#x27;t rely on <code>watchFileSystem</code> being <code>NodeJsWatchFileSystem</code>. But this is another topic</p><p>...</p><p>but yes</p><h4>johnny_reilly</h4><p>Thanks @sokra - when I get to a keyboard I&#x27;ll swap <code>mtimes</code> for <code>getTimes()</code> and report back.</p></blockquote><h3>17:28</h3><p>Despite various trains being out of action / missing in action I&#x27;ve made it to the dentists; phew! I go in for my checkup and plan to take a look at the issue later that evening. In the meantime I&#x27;ve hoping that Tobias (<a href="https://twitter.com/wsokra">Sokra</a>) will get chance to republish so that ts-loader users aren&#x27;t too impacted.</p><h3>18:00</h3><p>Done at the dentist and I&#x27;m heading home. Whilst I&#x27;ve been opening wide and squinting at the ceiling, <a href="https://blogs.msdn.microsoft.com/typescript/2017/02/22/announcing-typescript-2-2/">TypeScript 2.2 has shipped</a>. Whilst this is super exciting, according to Greenkeeper, <a href="https://github.com/TypeStrong/ts-loader/pull/483">the new version has broken the build</a>. Arrrrghhhh...</p><p>I start to look into this and realise we&#x27;re not broken because of TypeScript 2.2; we were broken because of the <code>mtimes</code>. Tobias has now re-added <code>mtimes</code> and published. With that in place I requeue a build and.... drum roll.... we&#x27;re green!</p><p>The good news just keeps on coming as <a href="https://twitter.com/bancek">Luka Zakrajšek</a> has submitted a <a href="https://github.com/TypeStrong/ts-loader/pull/482">PR which uses <code>getTimes()</code> in place of <code>mtimes</code></a>. And the tests pass. Awesome! MERGE. I just need to cut a release and we&#x27;re done.</p><h3>18:15</h3><p>I&#x27;m home. My youngest son has been suffering from chicken pox all week and as a result my wife has been in isolation, taking care of him. We chat whilst the boys watch Paw Patrol as the bath runs. I flick open the laptop and start doing the various housekeeping tasks around cutting a release. This is interrupted by various bathtime / bedtime activities and I abandon work for now.</p><h3>19:30</h3><p>The boys are down and I get on with the release; updating the changelog, bumping the version number and running the tests. For various reasons this takes longer than it normally does.</p><h3>20:30</h3><p>Finally we&#x27;re there; ts-loader 2.0.1 ships: <a href="https://github.com/TypeStrong/ts-loader/releases/tag/v2.0.1">https://github.com/TypeStrong/ts-loader/releases/tag/v2.0.1</a>.</p><p>I&#x27;m tremendously grateful to everyone that helped out - thank you all!</p><blockquote><p>ts-loader 2.0.1 has shipped; thanks <a href="https://twitter.com/wSokra">@wsokra</a><a href="https://twitter.com/bancek">@bancek</a> and @mredbishop <a href="https://t.co/I00c7sJyFo">https://t.co/I00c7sJyFo</a><a href="https://twitter.com/hashtag/typescript?src=hash">#<!-- -->typescript</a></p><p>— John Reilly (@johnny_reilly) <a href="https://twitter.com/johnny_reilly/status/834515296077627392">February 22, 2017</a></p></blockquote><script src="//platform.twitter.com/widgets.js" charSet="utf-8"></script>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[@types is rogue]]></title>
            <link>https://blog.johnnyreilly.com/2017/02/14/typescript-types-and-repeatable-builds</link>
            <guid>@types is rogue</guid>
            <pubDate>Tue, 14 Feb 2017 00:00:00 GMT</pubDate>
            <description><![CDATA[Or perhaps I should call this "@types and repeatable builds"....]]></description>
            <content:encoded><![CDATA[<p>Or perhaps I should call this &quot;@types and repeatable builds&quot;....</p><p>The other day, on a React / TypeScript project I work on, the nightly CI build started failing. But nothing had changed in the project... What gives? After digging I discovered the reason; spome of the type definitions which my project depends upon had changed. Why did this break my build? Let’s learn some more...</p><p>We acquire type definitions via npm. Type definitions from Definitely Typed are published to npm by an <a href="https://github.com/Microsoft/types-publisher">automated process</a> and they are all published under the @types namespace on npm. So, the <a href="https://www.npmjs.com/package/react">react type definition</a> is published as the <a href="https://www.npmjs.com/package/@types/react">@types/react</a> package, the node type definition is published as the <a href="https://www.npmjs.com/package/@types/node">@types/node</a> package. The hip bone&#x27;s connected to the thigh bone. You get the picture.</p><p>The npm ecosystem is essentially built on top of <a href="http://semver.org/">semantic versioning</a> and they <a href="https://docs.npmjs.com/getting-started/semantic-versioning">take it seriously</a>. Essentially, when a package is published it should be categorised as a major release (breaking changes), a minor release (extra functionality which is backwards compatible) or a patch release (backwards compatible bug fixes).</p><p>Now we get to the meat of the matter: @types is rogue. You cannot trust the version numbers on @types packages to respect semantic versioning. They don&#x27;t.</p><p>The main reason for this is that when it comes to versioning, the @types type definition essentially looks to mirror the version of the package they are seeking to type. <em>THIS MEANS THE TYPE DEFINITION CANNOT DO ITS OWN SEMANTIC VERSIONING.</em> A simple change in a type definition can lead to breakages in consuming code. That&#x27;s what happened to me. Let&#x27;s say an exported interface name changes; all code that relies upon the old name will now break. You see? Pain.</p><h2>How do we respond to this?</h2><p>My own take has been to pin the version numbers of @types packages; fixing to specific definitions. No <code>&quot;~&quot;</code> or <code>&quot;^&quot;</code> for my <code>@types devDependencies</code>.</p><p>No respect semantic versioning? No problem. You can go much further with repeatable builds and made use of <a href="https://code.facebook.com/posts/1840075619545360">facebook&#x27;s new npm client yarn</a> and <a href="https://yarnpkg.com/blog/2016/11/24/lockfiles-for-all/">lockfiles</a> (very popular BTW) but I haven&#x27;t felt the need yet. This should be ample for now.</p><p>The other question that may be nagging at your subconscious is this: what’s an easy way to know when new packages are available for my project dependencies? Well, the <code>Get-Package -Updates</code> (nuget hat tip) for npm that I’d recommend is this: <a href="https://www.npmjs.com/package/npm-check-updates">npm-check-updates</a>. It does the job wonderfully.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Hands-free HTTPS]]></title>
            <link>https://blog.johnnyreilly.com/2017/02/01/hands-free-https</link>
            <guid>Hands-free HTTPS</guid>
            <pubDate>Wed, 01 Feb 2017 00:00:00 GMT</pubDate>
            <description><![CDATA[I have had a \*great\* week. You? Take a look at this blog. Can you see what I can see? Here's a clue:]]></description>
            <content:encoded><![CDATA[<p>I have had a <!-- -->*<strong>great</strong>*<!-- --> week. You? Take a look at this blog. Can you see what I can see? Here&#x27;s a clue:</p><p><img src="../static/blog/2017-02-01-hands-free-https/Screenshot%2B2017-01-29%2B14.45.57.png"/></p><p>Yup, look at the top left hand corner.... see that beautiful padlock? Yeah - that&#x27;s what&#x27;s thrilled me. You see I have a dream; that one day on the red hills of the internet, the sons of former certificates and the sons of former certificate authorities will be able to sit down together at the table of HTTPS. Peace, love and TLS for all.</p><p>The world is turning and slowly but surely HTTPS is becoming the default of the web. <a href="https://security.googleblog.com/2014/08/https-as-ranking-signal_6.html">Search results get ranked higher if they&#x27;re HTTPS.</a><a href="https://en.wikipedia.org/wiki/HTTP/2#Encryption">HTTP/2 is, to all intents and purposes, a HTTPS-only game.</a><a href="https://developer.mozilla.org/en/docs/Web/API/Service_Worker_API">Service Workers are HTTPS-only.</a></p><p>I care about all of these. So it&#x27;s <em>essential</em> that I have HTTPS. But. But. But... Certificates, the administration that goes with them. It&#x27;s boring. I mean, it just is. I want to be building interesting apps, I don&#x27;t want to be devoting my time to acquiring certificates and fighting my way through the (never simple) administration of them. I&#x27;m dimly aware that there&#x27;s free certificates to be had thanks to the fine work of <a href="https://letsencrypt.org/">LetsEncrypt</a>. I believe that work is being done on reduce the onerous admin burden as well. And that&#x27;s great. But I&#x27;m still avoiding it...</p><p>What if I told you you could have HTTPS on your blog, on your Azure websites, on your anywhere.... <em>FOR FREE. IN FIVE MINUTES?</em>. Well, you can thanks to <a href="https://www.cloudflare.com/">CloudFlare</a>. I did; you should too.</p><p>This is where I point you off to a number of resources to help you on your HTTPS way:</p><ol><li><a href="https://www.troyhunt.com/how-to-get-your-ssl-for-free-on-shared/">Read Troy Hunt&#x27;s &quot;How to get your SSL for free on a Shared Azure website with CloudFlare&quot;</a></li><li><a href="https://www.pluralsight.com/courses/cloudflare-security-getting-started">Watch Troy Hunt&#x27;s Pluralsight course &quot;Getting Started with CloudFlare™ Security&quot;</a></li><li><a href="https://www.cloudflare.com/">Go to Cloudflare&#x27;s website and sign up</a></li></ol><p>It just works. And that makes me very happy indeed.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[webpack: resolveLoader / alias with query / options]]></title>
            <link>https://blog.johnnyreilly.com/2017/01/06/webpack-resolveloader-alias-with-query</link>
            <guid>webpack: resolveLoader / alias with query / options</guid>
            <pubDate>Fri, 06 Jan 2017 00:00:00 GMT</pubDate>
            <description><![CDATA[Sometimes you write a post for the ages. Sometimes you write one you hope is out of date before you hit "publish". This is one of those.]]></description>
            <content:encoded><![CDATA[<p>Sometimes you write a post for the ages. Sometimes you write one you hope is out of date before you hit &quot;publish&quot;. This is one of those.</p><p>There&#x27;s a <a href="https://github.com/webpack/enhanced-resolve/issues/41">bug</a> in webpack&#x27;s enhanced-resolve. It means that you cannot configure an aliased loader using the <code>query</code> (or <code>options</code> in the webpack 2 nomenclature). Let me illustrate; consider the following code:</p><pre><code class="language-js">module.exports = {
  // ...
  module: {
    loaders: [
      {
        test: /\.ts$/,
        loader: &#x27;ts-loader&#x27;,
        query: {
            entryFileIsJs: true
        }
      }
    ]
  }
}

module.exports.resolveLoader = { alias: { &#x27;ts-loader&#x27;: require(&#x27;path&#x27;).join(__dirname, &quot;../../index.js&quot;)
</code></pre><p>At the time of writing, if you alias a loader as above, then the <code>query</code> / <code>options</code> will <!-- -->*<em>not</em>*<!-- --> be passed along. This is bad, particularly given the requirement in webpack 2 that configuration is no longer possible through extending the <a href="https://webpack.js.org/guides/migrating/#loader-configuration-is-through-options"><code>webpack.config.js</code></a>. So what to do? Well, when this was a problem previously the marvellous <a href="https://www.twitter.com/jbrantly">James Brantly</a> had a <a href="https://github.com/webpack/webpack/issues/1289#issuecomment-125767499">workaround</a>. I&#x27;ve taken that and run with it:</p><pre><code class="language-js">var config = {
  // ...
  module: {
    loaders: [
      {
        test: /\.ts$/,
        loader: &#x27;ts-loader&#x27;,
        query: {
          entryFileIsJs: true,
        },
      },
    ],
  },
};

module.exports = config;

var loaderAliasPath = require(&#x27;path&#x27;).join(__dirname, &#x27;../../../index.js&#x27;);
var rules = config.module.loaders || config.module.rules;
rules.forEach(function (rule) {
  var options = rule.query || rule.options;
  rule.loader = rule.loader.replace(
    &#x27;ts-loader&#x27;,
    loaderAliasPath + (options ? &#x27;?&#x27; + JSON.stringify(options) : &#x27;&#x27;)
  );
});
</code></pre><p>This approach stringifies the <code>query</code> / <code>options</code> and suffixes it to the aliased path. This works as long as the options you&#x27;re passing are JSON-able (yes it&#x27;s a word).</p><p>As I said earlier; hopefully by the time you read this the workaround will no longer be necessary again. But just in case....</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[webpack: configuring a loader with query / options]]></title>
            <link>https://blog.johnnyreilly.com/2017/01/01/webpack-configuring-loader-with-query</link>
            <guid>webpack: configuring a loader with query / options</guid>
            <pubDate>Sun, 01 Jan 2017 00:00:00 GMT</pubDate>
            <description><![CDATA[webpack 2 is on it's way. As one of the maintainers of ts-loader I've been checking out that ts-loader works with webpack 2. It does: phew!]]></description>
            <content:encoded><![CDATA[<p><a href="https://medium.com/webpack/webpack-2-2-the-release-candidate-2e614d05d75f#.ntniu44u6">webpack 2 is on it&#x27;s way</a>. As one of the maintainers of <a href="https://github.com/TypeStrong/ts-loader/">ts-loader</a> I&#x27;ve been checking out that ts-loader works with webpack 2. It does: phew!</p><p>ts-loader has a continuous integration build that runs against webpack 1. When webpack 2 ships we&#x27;re planning to move to running CI against webpack 2. However, webpack 2 has some breaking changes. The one that&#x27;s particularly of relevance to our test packs is that a strict schema is now enforced for <code>webpack.config.js</code> with webpack 2. This has been the case since webpack 2 hit beta 23. Check the <a href="https://github.com/webpack/webpack/pull/2974">PR that added it</a>. You can see some of the <a href="https://github.com/webpack/webpack/issues/3018">frankly tortured discussion that this generated as well</a>.</p><p>Let&#x27;s all take a moment and realise that working on open source is sometimes a rather painful experience. Take a breath. Breathe out. Ready to carry on? Great.</p><p>There are 2 ways to configure loader options for ts-loader (and in fact this stands for most loaders). Loader options can be set either using a <code>query</code> when specifying the loader or through the <code>ts</code> (insert the name of alternative loaders here) property in the <code>webpack.config.js</code>.</p><p>The implicatations of the breaking change are: with webpack 2 you can <strong>no longer</strong> configure ts-loader (or any other loader) with a <code>ts</code> (insert the name of alternative loaders here) property in the <code>webpack.config.js</code>. It <strong>must</strong> be done through the <code>query</code> / <code>options</code>. The following code is no longer valid with webpack 2:</p><pre><code class="language-js">module.exports = {
  ...
  module: {
    loaders: [{
      test: /\.tsx?$/,
      loader: &#x27;ts-loader&#x27;
    }]
  },
  // specify option using `ts` property - **only do this if you are using webpack 1**
  ts: {
    transpileOnly: false
  }
}
</code></pre><p>This change means that we have needed to adjust how our test pack works. We can no longer make use of <code>ts</code> for configuration. Since I wasn&#x27;t terribly aware of <code>query</code> I thought it made sense to share my learnings.</p><h2>What exactly is <code>query</code> / <code>options</code>?</h2><p>Good question. Well, strictly speaking it&#x27;s 2 possible things; both ways to configure a webpack loader. Classically <code>query</code> was a string which could be appended to the name of the loader much like a <code>&lt;a href=&quot;https://en.wikipedia.org/wiki/Query_string&quot;&gt;query string&lt;/a&gt;</code> but actually with <a href="https://github.com/webpack/loader-utils#parsequery">greater powers</a>:</p><pre><code class="language-js">module.exports = {
  ...
  module: {
    loaders: [{
      test: /\.tsx?$/,
      loader: &#x27;ts-loader?&#x27; + JSON.stringify({
        transpileOnly: false
      })
    }]
  }
}
</code></pre><p>But it can also be a separately specified object that&#x27;s supplied alongside a loader (I understand this is relatively new behaviour):</p><pre><code class="language-js">module.exports = {
  ...
  module: {
    loaders: [{
      test: /\.tsx?$/,
      loader: &#x27;ts-loader&#x27;
      query: {
        transpileOnly: false
      }
    }]
  }
}
</code></pre><h2>webpack 2 is coming - look busy!</h2><p>So if you&#x27;re planning to move to webpack 2, be aware of this breaking change. You can start moving to using configuration via query right now with webpack 1. You don&#x27;t need to be using webpack 2 to make the jump. So jump!</p><p>Finally, and by way of a PS, <code>query</code> is renamed to <code>options</code> in webpack 2; a much better name to my mind. There&#x27;s actually a bunch of other renames on the way as well - check out the <a href="https://webpack.js.org/guides/migrating/#module-loaders-is-now-module-rules">migration guide</a> for more on this. The important thing to note is that <strong>the old names work in webpack 2</strong>. But you should plan to move to the new naming at some point as they&#x27;ll likely disappear when webpack 3 ships.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using ts-loader with webpack 2]]></title>
            <link>https://blog.johnnyreilly.com/2016/12/19/using-ts-loader-with-webpack-2</link>
            <guid>Using ts-loader with webpack 2</guid>
            <pubDate>Mon, 19 Dec 2016 00:00:00 GMT</pubDate>
            <description><![CDATA[Hands up, despite being one of the maintainers of ts-loader (a TypeScript loader for webpack) I have not been tracking webpack v2. My reasons? Well, I'm keen on cutting edge but bleeding edge is often not a ton of fun as dealing with regularly breaking changes is frustrating. I'm generally happy to wait for things to settle down a bit before leaping aboard. However, webpack 2 RC'd last week and so it's time to take a look!]]></description>
            <content:encoded><![CDATA[<p>Hands up, despite being one of the maintainers of <a href="https://github.com/TypeStrong/ts-loader">ts-loader</a> (a TypeScript loader for webpack) I have not been tracking webpack v2. My reasons? Well, I&#x27;m keen on cutting edge but bleeding edge is often not a ton of fun as dealing with regularly breaking changes is frustrating. I&#x27;m generally happy to wait for things to settle down a bit before leaping aboard. However, <a href="https://github.com/webpack/webpack/releases/tag/v2.2.0-rc.0">webpack 2 RC&#x27;d last week</a> and so it&#x27;s time to take a look!</p><h2>Porting our example</h2><p>Let&#x27;s take <a href="https://github.com/TypeStrong/ts-loader/tree/master/examples/webpack1-gulp-react-flux-babel-karma">ts-loader&#x27;s webpack 1 example</a> and try and port it to webpack 2. Will it work? Probably; I&#x27;m aware of other people using ts-loader with webpack 2. It&#x27;ll be a voyage of discovery. Like Darwin on the Beagle, I shall document our voyage for a couple of reasons:</p><ul><li>I&#x27;m probably going to get some stuff wrong. That&#x27;s fine; one of the best ways to learn is to make mistakes. So do let me know where I go wrong.</li><li>I&#x27;m doing this based on what I&#x27;ve read in the new docs; they&#x27;re very much a work in progress and the mistakes I make here may lead to those docs improving even more. That matters; <strong>documentation matters</strong>. I&#x27;ll be leaning heavily on the <a href="https://webpack.js.org/guides/migrating/">Migrating from v1 to v2</a> guide.</li></ul><p>So here we go. Our example is one which uses TypeScript for static typing and uses Babel to transpile from ES-super-modern (yes - it&#x27;s a thing) to ES-older-than-that. Our example also uses React; but that&#x27;s somewhat incidental. It only uses webpack for typescript / javascript and karma. It uses gulp to perform various other tasks; so if you&#x27;re reliant on webpack for less / sass compilation etc then I have no idea whether that works.</p><p>First of all, let&#x27;s install the latest RC of webpack:</p><pre><code class="language-ts">npm install webpack@2.2.0-rc.1 --save-dev
</code></pre><h2><code>webpack.config.js</code></h2><p>Let&#x27;s look at our existing <code>webpack.config.js</code>:</p><pre><code class="language-js">&#x27;use strict&#x27;;

var path = require(&#x27;path&#x27;);

module.exports = {
  cache: true,
  entry: {
    main: &#x27;./src/main.tsx&#x27;,
    vendor: [&#x27;babel-polyfill&#x27;, &#x27;fbemitter&#x27;, &#x27;flux&#x27;, &#x27;react&#x27;, &#x27;react-dom&#x27;],
  },
  output: {
    path: path.resolve(__dirname, &#x27;./dist/scripts&#x27;),
    filename: &#x27;[name].js&#x27;,
    chunkFilename: &#x27;[chunkhash].js&#x27;,
  },
  module: {
    loaders: [
      {
        test: /\.ts(x?)$/,
        exclude: /node_modules/,
        loader:
          &#x27;babel-loader?presets[]=es2016&amp;presets[]=es2015&amp;presets[]=react!ts-loader&#x27;,
      },
      {
        test: /\.js$/,
        exclude: /node_modules/,
        loader: &#x27;babel&#x27;,
        query: {
          presets: [&#x27;es2016&#x27;, &#x27;es2015&#x27;, &#x27;react&#x27;],
        },
      },
    ],
  },
  plugins: [],
  resolve: {
    extensions: [&#x27;&#x27;, &#x27;.webpack.js&#x27;, &#x27;.web.js&#x27;, &#x27;.ts&#x27;, &#x27;.tsx&#x27;, &#x27;.js&#x27;],
  },
};
</code></pre><p>There&#x27;s a number of things we need to do here. First of all, we can get rid of the empty extension under resolve; I understand that&#x27;s unnecessary now. Also, I&#x27;m going to get rid of <code>&#x27;.webpack.js&#x27;</code> and <code>&#x27;.web.js&#x27;</code>; I never used them anyway. Also, just having <code>&#x27;babel&#x27;</code> as a loader won&#x27;t fly anymore. We need that suffix as well.</p><p>Now I could start renaming <code>loaders</code> to <code>rules</code> as the terminology is changing. But I&#x27;d like to deal with that later since I know the old school names are still supported at present. More interestingly, I seem to remember hearing that one of the super exciting things about webpack is that it supports modules directly now. (I think that&#x27;s supposed to be good for tree-shaking but I&#x27;m not totally certain.)</p><p>Initially I thought I was supposed to switch to a custom babel preset called <code>&lt;a href=&quot;https://www.npmjs.com/package/babel-preset-es2015-webpack&quot;&gt;babel-preset-es2015-webpack&lt;/a&gt;</code>. However it has a big &quot;DEPRECATED&quot; mark at the top and it says I should just use <code>babel-preset-es2015</code> (which I already am) with the following option specified:</p><pre><code class="language-js">{
    &quot;presets&quot;: [
        [
            &quot;es2015&quot;,
            {
                &quot;modules&quot;: false
            }
        ]
    ]
}
</code></pre><p>Looking at our existing config you&#x27;ll note that for <code>js</code> files we&#x27;re using <code>query</code> (<code>options</code> in the new world I understand) to configure babel usage. We&#x27;re using <a href="https://webpack.github.io/docs/using-loaders.html#query-parameters">query parameters</a> for <code>ts</code> files. I have <em>zero</em> idea how to configure preset options using query parameters. Fiddling with <code>query</code> / <code>options</code> didn&#x27;t seem to work. So, I&#x27;ve decided to abandon using query entirely and drop in a <code>&lt;a href=&quot;http://babeljs.io/docs/usage/babelrc/&quot;&gt;.babelrc&lt;/a&gt;</code> file using our presets combined with the <code>&lt;a href=&quot;https://babeljs.io/docs/plugins/#plugin-preset-options&quot;&gt;modules&lt;/a&gt;</code> setting:</p><pre><code class="language-js">{
   &quot;presets&quot;: [
      &quot;react&quot;,
      [
         &quot;es2015&quot;,
         {
            &quot;modules&quot;: false
         }
      ],
      &quot;es2016&quot;
   ]
}
</code></pre><p>As an aside; apparently these are applied in reverse order. So <code>es2016</code> is applied first, <code>es2015</code> second and <code>react</code> third. I&#x27;m not totally certain this is correct; the <code>&lt;a href=&quot;http://babeljs.io/docs/usage/babelrc/&quot;&gt;.babelrc&lt;/a&gt; docs</code> are a little unclear.</p><p>With our query options extracted we&#x27;re down to a simpler <code>webpack.config.js</code>:</p><pre><code class="language-js">&#x27;use strict&#x27;;

var path = require(&#x27;path&#x27;);

module.exports = {
  cache: true,
  entry: {
    main: &#x27;./src/main.tsx&#x27;,
    vendor: [&#x27;babel-polyfill&#x27;, &#x27;fbemitter&#x27;, &#x27;flux&#x27;, &#x27;react&#x27;, &#x27;react-dom&#x27;],
  },
  output: {
    path: path.resolve(__dirname, &#x27;./dist/scripts&#x27;),
    filename: &#x27;[name].js&#x27;,
    chunkFilename: &#x27;[chunkhash].js&#x27;,
  },
  module: {
    loaders: [
      {
        test: /\.ts(x?)$/,
        exclude: /node_modules/,
        loader: &#x27;babel-loader!ts-loader&#x27;,
      },
      {
        test: /\.js$/,
        exclude: /node_modules/,
        loader: &#x27;babel-loader&#x27;,
      },
    ],
  },
  plugins: [],
  resolve: {
    extensions: [&#x27;.ts&#x27;, &#x27;.tsx&#x27;, &#x27;.js&#x27;],
  },
};
</code></pre><h2><code>plugins</code></h2><p>In our example the <code>plugins</code> section of our <code>webpack.config.js</code> is extended in a separate process. Whilst we&#x27;re developing we also set the <code>debug</code> flag to be <code>true</code>. <a href="https://webpack.js.org/guides/migrating/#debug">It seems we need to introduce a <code>LoaderOptionsPlugin</code> to do this for us.</a></p><p>As we introduce our <code>LoaderOptionsPlugin</code> we also need to make sure that we provide it with <code>options</code>. How do I know this? Well <a href="https://github.com/TypeStrong/ts-loader/issues/283">someone raised an issue against ts-loader</a>. I don&#x27;t think this is actually an issue with ts-loader; I think it&#x27;s just a webpack 2 thing. I could be wrong; answers on a postcard please.</p><p>Either way, to get up and running we just need the <code>LoaderOptionsPlugin</code> in play. Consequently, most of what follows in our <code>webpack.js</code> file is unchanged:</p><pre><code class="language-js">// .....

var webpackConfig = require(&#x27;../webpack.config.js&#x27;);
var packageJson = require(&#x27;../package.json&#x27;);

// .....

function buildProduction(done) {
  // .....

  myProdConfig.plugins = myProdConfig.plugins.concat(
    // .....

    // new webpack.optimize.DedupePlugin(), Not a thing anymore apparently
    new webpack.optimize.UglifyJsPlugin(),

    // I understand this here matters...
    // but it doesn&#x27;t seem to make any difference; perhaps I&#x27;m missing something?
    new webpack.LoaderOptionsPlugin({
      minimize: true,
      debug: false,
    }),

    failPlugin
  );

  // .....
}

function createDevCompiler() {
  var myDevConfig = webpackConfig;
  myDevConfig.devtool = &#x27;inline-source-map&#x27;;
  // myDevConfig.debug = true; - not allowed in webpack 2

  myDevConfig.plugins = myDevConfig.plugins.concat(
    new webpack.optimize.CommonsChunkPlugin({
      name: &#x27;vendor&#x27;,
      filename: &#x27;vendor.js&#x27;,
    }),
    new WebpackNotifierPlugin({
      title: &#x27;Webpack build&#x27;,
      excludeWarnings: true,
    }),

    // this is the Webpack 2 hotness!
    new webpack.LoaderOptionsPlugin({
      debug: true,
      options: myDevConfig,
    })
    // it ends here - there wasn&#x27;t much really....
  );

  // create a single instance of the compiler to allow caching
  return webpack(myDevConfig);
}

// .....
</code></pre><h2><code>LoaderOptionsPlugin</code> we hardly new ya</h2><p>After a little more experimentation it seems that the <code>LoaderOptionsPlugin</code> is not necessary at all for our own use case. In fact it&#x27;s probably not best practice to get used to using it as it&#x27;s only intended to live a short while whilst people move from webpack 1 to webpack 2. In that vein let&#x27;s tweak our <code>webpack.js</code> file once more:</p><pre><code class="language-js">function buildProduction(done) {
  // .....

  myProdConfig.plugins = myProdConfig.plugins.concat(
    // .....

    new webpack.optimize.UglifyJsPlugin({
      compress: {
        warnings: true,
      },
    }),

    failPlugin
  );

  // .....
}

function createDevCompiler() {
  var myDevConfig = webpackConfig;
  myDevConfig.devtool = &#x27;inline-source-map&#x27;;

  myDevConfig.plugins = myDevConfig.plugins.concat(
    new webpack.optimize.CommonsChunkPlugin({
      name: &#x27;vendor&#x27;,
      filename: &#x27;vendor.js&#x27;,
    }),
    new WebpackNotifierPlugin({ title: &#x27;Webpack build&#x27;, excludeWarnings: true })
  );

  // create a single instance of the compiler to allow caching
  return webpack(myDevConfig);
}

// .....
</code></pre><h2><code>karma.conf.js</code></h2><p>Finally Karma. Our <code>karma.conf.js</code> with webpack 1 looked like this:</p><pre><code class="language-js">/* eslint-disable no-var, strict */
&#x27;use strict&#x27;;

var webpackConfig = require(&#x27;./webpack.config.js&#x27;);

module.exports = function (config) {
  // Documentation: https://karma-runner.github.io/0.13/config/configuration-file.html
  config.set({
    browsers: [&#x27;PhantomJS&#x27;],

    files: [
      // This ensures we have the es6 shims in place and then loads all the tests
      &#x27;test/main.js&#x27;,
    ],

    port: 9876,

    frameworks: [&#x27;jasmine&#x27;],

    logLevel: config.LOG_INFO, //config.LOG_DEBUG

    preprocessors: {
      &#x27;test/main.js&#x27;: [&#x27;webpack&#x27;, &#x27;sourcemap&#x27;],
    },

    webpack: {
      devtool: &#x27;inline-source-map&#x27;,
      debug: true,
      module: webpackConfig.module,
      resolve: webpackConfig.resolve,
    },

    webpackMiddleware: {
      quiet: true,
      stats: {
        colors: true,
      },
    },

    // reporter options
    mochaReporter: {
      colors: {
        success: &#x27;bgGreen&#x27;,
        info: &#x27;cyan&#x27;,
        warning: &#x27;bgBlue&#x27;,
        error: &#x27;bgRed&#x27;,
      },
    },
  });
};
</code></pre><p>We just need to chop out the <code>debug</code> statement from the <code>webpack</code> section like so:</p><pre><code class="language-js">module.exports = function(config) {

  // .....

    webpack: {
      devtool: &#x27;inline-source-map&#x27;,
      module: webpackConfig.module,
      resolve: webpackConfig.resolve
    },

  // .....

  });
};
</code></pre><h2>Compare and contrast</h2><p>We now have a repo that works with webpack 2 rc 1. Yay! If you&#x27;d like to see it then take a look <a href="https://github.com/TypeStrong/ts-loader/tree/master/examples/webpack2-gulp-react-flux-babel-karma">here</a>.</p><p>I thought I&#x27;d compare performance / output size of compiling with webpack 1 to webpack 2. First of all in debug / development mode:</p><pre><code class="language-ts">// webpack 1

Version: webpack 1.14.0
Time: 5063ms
    Asset     Size  Chunks             Chunk Names
  main.js  37.2 kB       0  [emitted]  main
vendor.js  2.65 MB       1  [emitted]  vendor

// webpack 2

Version: webpack 2.2.0-rc.1
Time: 5820ms
    Asset     Size  Chunks                    Chunk Names
  main.js  38.7 kB       0  [emitted]         main
vendor.js  2.63 MB       1  [emitted]  [big]  vendor
</code></pre><p>Size and compilation time is not massively different from webpack 1 to webpack 2. It&#x27;s all about the same. I&#x27;m not sure if that&#x27;s to be expected or not.... Though I&#x27;ve a feeling in production mode I&#x27;m supposed to feel the benefits of tree shaking so let&#x27;s have a go:</p><pre><code class="language-ts">// webpack 1

Version: webpack 1.14.0
Time: 5788ms
                         Asset     Size  Chunks             Chunk Names
  main.269c66e1bc13b7426cee.js  10.5 kB       0  [emitted]  main
vendor.269c66e1bc13b7426cee.js   231 kB       1  [emitted]  vendor

// webpack 2

Version: webpack 2.2.0-rc.1
Time: 5659ms
                         Asset     Size  Chunks             Chunk Names
  main.33e0d70eeec29206e9b6.js  9.22 kB       0  [emitted]  main
vendor.33e0d70eeec29206e9b6.js   233 kB       1  [emitted]  vendor
</code></pre><p>To my surprise this looks pretty much unchanged before and after as well. This may be a sign I have missed something crucial out. Or maybe that&#x27;s to be expected. Do give me a heads up if I&#x27;ve missed something...</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[webpack: syncing the enhanced-resolve]]></title>
            <link>https://blog.johnnyreilly.com/2016/12/11/webpack-syncing-enhanced-resolve</link>
            <guid>webpack: syncing the enhanced-resolve</guid>
            <pubDate>Sun, 11 Dec 2016 00:00:00 GMT</pubDate>
            <description><![CDATA[Like Captain Ahab I resolve to sync the white whale that is webpack's enhanced-resolve... English you say? Let me start again:]]></description>
            <content:encoded><![CDATA[<p>Like Captain Ahab I resolve to sync the white whale that is webpack&#x27;s <code>&lt;a href=&quot;https://github.com/webpack/enhanced-resolve&quot;&gt;enhanced-resolve&lt;/a&gt;</code>... English you say? Let me start again:</p><p>So, you&#x27;re working on a webpack loader. (In my case the typescript loader; <code>&lt;a href=&quot;https://github.com/TypeStrong/ts-loader&quot;&gt;ts-loader&lt;/a&gt;</code>) You have need of webpack&#x27;s resolve capabilities. You dig around and you discover that that superpower is lodged in the very heart of the enhanced-resolve package. Fantastic. But wait, there&#x27;s more: your needs are custom. You need a sync, not an async resolver. (Try saying that quickly.) You regard the description of <code>enhanced-resolve</code> with some concern:</p><blockquote><p>&quot;Offers an async require.resolve function. It&#x27;s highly configurable.&quot;</p></blockquote><p>Well that doesn&#x27;t sound too promising. Let&#x27;s have a look at the docs. Ah. Hmmm. You know how it goes with webpack. Why document anything clearly when people could just guess wildly until they near insanity and gibber? Right? It&#x27;s well established that webpack&#x27;s attitude to docs has been traditionally akin to Gordon Gecko&#x27;s view on lunch.</p><p><img src="../static/blog/2016-12-11-webpack-syncing-enhanced-resolve/documentation-is-for-wimps.png"/></p><p>In all fairness, things are beginning to change on that front. In fact the <a href="https://webpack.js.org/">new docs</a> look very promising. But regrettably, the docs on the enhanced-resolve repo are old school. Which is to say: opaque. However, I&#x27;m here to tell you that if a sync resolver is your baby then, contrary to appearances, <code>enhanced-resolve</code> has your back.</p><h2>Sync, for lack of a better word, is good</h2><p>Nestled inside enhanced-resolve is the <code>&lt;a href=&quot;https://github.com/webpack/enhanced-resolve/blob/3f3f4cd1fcbafa1e98c3c6470fed1277817ed607/lib/ResolverFactory.js&quot;&gt;ResolverFactory.js&lt;/a&gt;</code> which can be used to make a resolver. However, you can supply it with a million options and that&#x27;s just like giving someone a gun with a predilection for feet.</p><p>What you want is an example of how you could make a sync resolver. Well, surprise surprise it&#x27;s right in front of your nose. Tucked away in <code>&lt;a href=&quot;https://github.com/webpack/enhanced-resolve/blob/3f3f4cd1fcbafa1e98c3c6470fed1277817ed607/lib/node.js&quot;&gt;node.js&lt;/a&gt;</code> (I do <!-- -->*<strong>not</strong>*<!-- --> get the name) is exactly what you&#x27;re after. It contains a number of factory functions which will construct a ready-made resolver for you; sync or async. Perfect! So here&#x27;s how I&#x27;m rolling:</p><pre><code class="language-js">const node = require(&#x27;enhanced-resolve/lib/node&#x27;);

function makeSyncResolver(options) {
  return node.create.sync(options.resolve);
}

const resolveSync = makeSyncResolver(loader.options);
</code></pre><p>The loader options used above you&#x27;ll be familiar with as the <code>resolve</code> section of your <code>webpack.config.js</code>. You can read more about them <a href="https://github.com/webpack/enhanced-resolve/blob/master/README.md">here</a> and <a href="https://webpack.js.org/configuration/resolve/">here</a>.</p><p>What you&#x27;re left with at this point is a function; a <code>resolveSync</code> function if you will that takes 3 arguments:</p><dl><dt>context</dt><dd>I don&#x27;t know what this is. So when using the function I just supply <code>undefined</code>; and that seems to be OK. Weird, right?</dd><dt>path</dt><dd>This is the path to your code (I think). So, a valid value to supply - handily lifted from the ts-loader test pack - would be: <code>C:\source\ts-loader\.test\babel-issue92</code></dd><dt>request</dt><dd>The actual module you&#x27;re interested in; so using the same test the relevant value would be <code>./submodule/submodule</code></dd></dl><p>Put it all together and what have you got?</p><pre><code class="language-js">const resolvedFileName = resolveSync(
  undefined,
  &#x27;C:source\ts-loader.test\babel-issue92&#x27;,
  &#x27;./submodule/submodule&#x27;
);

// resolvedFileName: C:\source\ts-loader\.test\babel-issue92\submodule\submodule.tsx
</code></pre><p>Boom.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[My Subconscious is a Better Developer Than I Am]]></title>
            <link>https://blog.johnnyreilly.com/2016/11/12/my-subconscious-is-better-developer</link>
            <guid>My Subconscious is a Better Developer Than I Am</guid>
            <pubDate>Sat, 12 Nov 2016 00:00:00 GMT</pubDate>
            <description><![CDATA[Occasionally I flatter myself that I'm alright at this development lark. Such egotistical talk is foolish. What makes me pause even more when I consider the proposition is this: my subconscious is a better developer than I am.]]></description>
            <content:encoded><![CDATA[<p>Occasionally I flatter myself that I&#x27;m alright at this development lark. Such egotistical talk is foolish. What makes me pause even more when I consider the proposition is this: my subconscious is a better developer than I am.</p><p>What&#x27;s this fellow talking about?</p><p>There&#x27;s two of me. Not identical twins; masquerading as a single man (spoiler: I am not a Christopher Nolan movie). No. There&#x27;s me, the chap who&#x27;s tapping away at his keyboard and solving a problem. And there&#x27;s the other chap too.</p><p>I have days when I&#x27;m working away at something and I&#x27;ll hit a brick wall. I produce solutions that work but are not elegant. I&#x27;m not proud of them. Or worse, I fail to come up with something that solves the problem I&#x27;m facing. So I go home. I see my family, I have some food, I do something else. I context switch. I go to sleep.</p><p>When I awake, sometimes (not always) I&#x27;ll have waiting in my head a better solution. I can see the solution in my head. I can turn it over and compare it to what, if anything, I currently have and see the reasons the new approach is better. Great, right? Up to a point.</p><p>What concerns me is this: I didn&#x27;t work this out from first principles. The idea arrived sight unseen in my head. It totally works but whose work actually is it? I feel like I&#x27;m taking credit for someone else&#x27;s graft. This is probably why I&#x27;m so keen on the MIT License. Don&#x27;t want to be caught out.</p><p>I think I&#x27;d like it better if I was a better developer than my subconscious. I&#x27;d come up with the gold and mock the half baked ideas he shows me in the morning. Alas it is not to be.</p><p>I draw some comfort from the knowledge that I&#x27;m not alone in my experience. I&#x27;ve chatted to other devs in the same boat. There&#x27;s probably two of you as well. Amarite? There&#x27;s probably three of Jon Skeet; each more brilliant than the last...</p><p><img src="../static/blog/2016-11-12-my-subconscious-is-better-developer/beingjohnm.png" alt="a poster from the film Being John Malkovich"/></p><p>PS I posted this to Hacker News and <a href="https://news.ycombinator.com/item?id=12942461">the comments left by people are pretty fascinating</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[But you can't die... I love you!]]></title>
            <link>https://blog.johnnyreilly.com/2016/11/01/but-you-cant-die-i-love-you-ts-loader</link>
            <guid>But you can't die... I love you!</guid>
            <pubDate>Tue, 01 Nov 2016 00:00:00 GMT</pubDate>
            <description><![CDATA[That's how I was feeling on the morning of October 6th 2016. I'd been feeling that way for some time. The target of my concern? ts-loader. ts-loader is a loader for webpack; the module bundler. ts-loader allows you use TypeScript with webpack. I'd been a merry user of it for at least a year or so. But, at that point, all was not well in the land of ts-loader. Come with me and I'll tell you a story...]]></description>
            <content:encoded><![CDATA[<p>That&#x27;s how I was feeling on the morning of October 6th 2016. I&#x27;d been feeling that way for some time. The target of my concern? <a href="https://github.com/TypeStrong/ts-loader">ts-loader</a>. ts-loader is a loader for <a href="https://webpack.github.io/">webpack; the module bundler</a>. ts-loader allows you use TypeScript with webpack. I&#x27;d been a merry user of it for at least a year or so. But, at that point, all was not well in the land of ts-loader. Come with me and I&#x27;ll tell you a story...</p><h2>Going Red</h2><p>At some point, I became a member of the <a href="https://github.com/TypeStrong">TypeStrong</a> organisation on GitHub. I&#x27;m honestly not entirely sure how. I think it may have been down to the very excellent <a href="https://github.com/basarat">Basarat</a> (he of <a href="http://alm.tools/">ALM</a> / <a href="https://github.com/TypeStrong/atom-typescript">atom-typescript</a> / the list goes on fame) but I couldn&#x27;t clearly say.</p><p>Either way, <a href="https://github.com/jbrantly">James Brantly</a>&#x27;s ts-loader was also one of TypeStrong&#x27;s projects. Since I used it, I occasionally contributed. Not much to be honest; mostly it was documentation tweaks. I mean I never really looked at the main code at all. It worked (thanks to other people). I just plugged it into my projects and ploughed on my merry way. I liked it. It was well established; with friendly maintainers. It had a continuous integration test pack that ran against multiple versions of TypeScript on both Windows and Linux. I trusted it. Then one day the continuous integration tests went red. And stayed red.</p><p>This is where we came in. On the morning of October 6th I was mulling what to do about this. I knew there was another alternative out there (awesome-typescript-loader) but I was a little wary of it. My understanding of ATL was that it targeted webpack 2.0 which has long been in beta. Where I ply my trade (mostly developing software for the financial sector in the City of London) beta is not a word that people trust. They don&#x27;t do beta. What&#x27;s more I was quite happy with ts-loader; I didn&#x27;t want to switch if I didn&#x27;t have to. I also rather suspected (rightly) that there wasn&#x27;t much wrong; ts-loader just needed a little bit of love. So I thought: I bet I can help here.</p><h2>The Statement of Intent</h2><p>So that evening I raised <a href="https://github.com/TypeStrong/ts-loader/issues/296">an issue against ts-loader</a>. Not a &quot;sort it out chap&quot; issue. No. That wouldn&#x27;t be terribly helpful. I raised a &quot;here&#x27;s how I can help&quot; issue. I present an abridged version below:</p><blockquote><p>Okay here&#x27;s the deal; I&#x27;ve been using ts-loader for a long time but my contributions up until now have mostly been documentation. Fixing of tests etc. As the commit history shows this is <a href="https://github.com/jbrantly">@jbrantly</a>&#x27;s baby and kudos to him.</p><p>He&#x27;s not been able to contribute much of late and since he&#x27;s the main person who&#x27;s worked on ts-loader not much has happened for a while; the code is a bit stale. As I&#x27;m a member of TypeStrong I&#x27;m going to have a go at improving the state of the project. I&#x27;m going to do this as carefully as I can. This issue is intended as a meta issue to make it visible what I&#x27;m plannning to do / doing.</p><p>My immediate goal is to get a newer version of ts-loader built and shipped. Essentially all the bug fixes / tweaks since the last release should ship.</p><p>...</p><p>I don&#x27;t have npm publish rights for ts-loader. Fortunately both <a href="https://github.com/jbrantly">@jbrantly</a> and <a href="https://github.com/blakeembrey">@blakeembrey</a> do - and hopefully one of them will either be able to help out with a publish or let me have the requisite rights to do it.</p><p>I can&#x27;t promise this is all going to work; I&#x27;ve got a limited amount of spare time I&#x27;m afraid. Whatever happens it&#x27;s going to take me a little while. But I&#x27;m going to see where I can take this. Best foot forward! Please bear with me...</p></blockquote><p>I did wonder what would happen next. This happened next:</p><blockquote><p>My <a href="https://twitter.com/hashtag/opensourceguilt?src=hash">#<!-- -->opensourceguilt</a> has been lifted thanks to <a href="https://twitter.com/johnny_reilly">@johnny_reilly</a> stepping up to take over ts-loader. Thanks man!</p><p>— James Brantly (@jbrantly) <a href="https://twitter.com/jbrantly/status/785931975064444928">October 11, 2016</a></p></blockquote><script src="//platform.twitter.com/widgets.js" charSet="utf-8"></script><h2>Caretaker, not <a href="https://en.wikipedia.org/wiki/Benevolent_dictator_for_life">BDFL</a></h2><p>So that&#x27;s how it came to pass that I became the present main caretaker of ts-loader. James very kindly gave me the rights to publish to npm and soon enough I did. I fixed up the existing integration test pack; made it less brittle. I wrote a new integration test pack (that performs a different sort of testing; execution rather than comparison). I merged pull requests, I closed issues. I introduced a regression (whoops!), a community member helped me fix it (thanks <a href="https://github.com/dopare">Mike Mazmanyan</a>!). In the last month ts-loader has shipped 6 times.</p><p>The thing that matters most in the last paragraph are the phrases &quot;I merged pull requests&quot; and &quot;a community member helped me fix it&quot;. I&#x27;m wary of one man bands; you should be to. I want projects to be a thing communally built and maintained. If I go under a bus I want someone else to be able to carry on without me. So be part of this; I want you to help!</p><p>I&#x27;ve got plans to do a lot more. I&#x27;m in the process of <a href="https://github.com/TypeStrong/ts-loader/pull/343">refactoring ts-loader to make it more modular and hence easier for others to contribute</a>. (Also it must be said, refactoring something is an excellent way to try and learn a codebase.) Version 1.0 of ts-loader should ship this week.</p><p>I&#x27;m working with <a href="https://github.com/HerringtonDarkholme">Herrington Darkholme</a> (awesome name BTW!) to <a href="https://github.com/TypeStrong/ts-loader/issues/270">add a hook-in point</a> that will allow ts-loader to support <a href="http://vuejs.org/">vuejs</a>. Stuff is happening and will continue to. But don&#x27;t be shy; be part of this! ts-loader awaits your PRs and is happy to have as many caretakers as possible!</p><p><img src="../static/blog/2016-11-01-but-you-cant-die-i-love-you-ts-loader/caretaker.png"/></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[React Component Curry]]></title>
            <link>https://blog.johnnyreilly.com/2016/10/05/react-component-curry</link>
            <guid>React Component Curry</guid>
            <pubDate>Wed, 05 Oct 2016 00:00:00 GMT</pubDate>
            <description><![CDATA[Everyone loves curry don't they? I don't know about you but I'm going for one on Friday.]]></description>
            <content:encoded><![CDATA[<p>Everyone loves curry don&#x27;t they? I don&#x27;t know about you but I&#x27;m going for one on Friday.</p><p>When React 0.14 shipped, it came with a new way to write React components. Rather than as an ES2015 class or using <code>React.createClass</code> there was now another way: stateless functional components.</p><p>These are components which have no state (the name gives it away) and a simple syntax; they are a function which takes your component props as a single parameter and they return JSX. Think of them as the render method of a standard component just with props as a parameter.</p><p>The advantage of these components is that they can reduce the amount of code you have to write for a component which requires no state. This is even more true if you&#x27;re using ES2015 syntax as you have arrow functions and destructuring to help.Embrace the terseness!</p><h2>Mine&#x27;s a Balti</h2><p>There is another advantage of this syntax. If you have a number of components which share similar implementation you can easily make component factories by currying:</p><pre><code class="language-jsx">function iconMaker(fontAwesomeClassName: string) {
  return (props) =&gt; &lt;i className={`fa ${fontAwesomeClassName}`} /&gt;;
}

const ThumbsUpIcon = iconMaker(&#x27;fa-thumbs-up&#x27;);
const TrophyIcon = iconMaker(&#x27;fa-trophy&#x27;);

// Somewhere in else inside a render function:

&lt;p&gt;
  This is totally &lt;ThumbsUpIcon /&gt;
  .... You should win a &lt;TrophyIcon /&gt;
&lt;/p&gt;;
</code></pre><p>So our <code>iconMaker</code> is a function which, when called with a <a href="http://fontawesome.io/">Font Awesome</a> class name produces a function which, when invoked, will return a the HTML required to render that icon. This is a super simple example, a bhaji if you will, but you can imagine how useful this technique can be when you&#x27;ve more of a banquet in mind.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[TypeScript 2.0, ES2016 and Babel]]></title>
            <link>https://blog.johnnyreilly.com/2016/09/22/typescript-20-es2016-and-babel</link>
            <guid>TypeScript 2.0, ES2016 and Babel</guid>
            <pubDate>Thu, 22 Sep 2016 00:00:00 GMT</pubDate>
            <description><![CDATA[TypeScript 2.0 has shipped! Naturally I'm excited. For some time I've been using TypeScript to emit ES2015 code which I pass onto Babel to transpile to ES "old school". You can see how here.]]></description>
            <content:encoded><![CDATA[<p><a href="https://blogs.msdn.microsoft.com/typescript/2016/09/22/announcing-typescript-2-0/">TypeScript 2.0 has shipped!</a> Naturally I&#x27;m excited. For some time I&#x27;ve been using TypeScript to emit ES2015 code which I pass onto Babel to transpile to ES &quot;old school&quot;. You can see how <a href="https://blog.johnnyreilly.com/2015/12/es6-typescript-babel-react-flux-karma.html">here</a>.</p><p>Merely upgrading my <code>package.json</code> to use <code>&quot;typescript&quot;: &quot;^2.0.3&quot;</code> from <code>&quot;typescript&quot;: &quot;^1.8.10&quot;</code> was painless. TypeScript now supports ES2016 (the previous major release 1.8 supported ES2015). I wanted to move on from writing ES2015 to writing ES2016 using my chosen build process. Fortunately, it&#x27;s supported. Phew. However, due to some advances in ecmascript feature modularisation within the TypeScript compiler the upgrade path is slightly different. I figured that I&#x27;d just be able to update the <code>&lt;a href=&quot;https://www.typescriptlang.org/docs/handbook/compiler-options.html&quot;&gt;target&lt;/a&gt;</code> in my <code>tsconfig.json</code> to <code>&quot;es2016&quot;</code> from <code>&quot;es2015&quot;</code>, add in the ES2016 preset for Babel and jobs a good &#x27;un. Not so. There were a few more steps to follow. Here&#x27;s the recipe:</p><h2><code>tsconfig.json</code> changes</h2><p>Well, there&#x27;s no <code>&quot;es2016&quot;</code> target for TypeScript. You carry on with a target of <code>&quot;es2015&quot;</code>. What I need is a new entry: <code>&quot;lib&quot;: [&quot;dom&quot;, &quot;es2015&quot;, &quot;es2016&quot;]</code>. This tells the compiler that we&#x27;re expecting to be emitting to an environment which supports a browser (<code>&quot;dom&quot;</code>), and both ES2016 and ES2015. Our &quot;environment&quot; is Babel and it&#x27;s going to pick up the baton from this point. My complete <code>tsconfig.json</code> looks like this:</p><pre><code class="language-json">{
  &quot;compileOnSave&quot;: false,
  &quot;compilerOptions&quot;: {
    &quot;allowSyntheticDefaultImports&quot;: true,
    &quot;lib&quot;: [&quot;dom&quot;, &quot;es2015&quot;, &quot;es2016&quot;],
    &quot;jsx&quot;: &quot;preserve&quot;,
    &quot;module&quot;: &quot;es2015&quot;,
    &quot;moduleResolution&quot;: &quot;node&quot;,
    &quot;noEmitOnError&quot;: false,
    &quot;noImplicitAny&quot;: true,
    &quot;preserveConstEnums&quot;: true,
    &quot;removeComments&quot;: false,
    &quot;suppressImplicitAnyIndexErrors&quot;: true,
    &quot;target&quot;: &quot;es2015&quot;
  }
}
</code></pre><h2>Babel changes</h2><p>I needed the Babel preset for ES2016; with a quick <code>&lt;a href=&quot;https://www.npmjs.com/package/babel-preset-es2016&quot;&gt;npm install --save-dev babel-preset-es2016&lt;/a&gt;</code> that was sorted. Now just to kick Webpack into gear...</p><h2>Webpack changes</h2><p>My webpack config plugs together TypeScript and Babel with the help of <a href="https://www.npmjs.com/package/ts-loader">ts-loader</a> and <a href="https://www.npmjs.com/package/babel-loader">babel-loader</a>. It allows the transpilation of my (few) JavaScript files so I can write ES2015. However, mainly it allows the transpilation of my (many) TypeScript files so I can write ES2015-flavoured TypeScript. I&#x27;ll now tweak the <code>loaders</code> so they cater for ES2016 as well.</p><pre><code class="language-js">var webpack = require(&#x27;webpack&#x27;);

module.exports = {
  // ....

  module: {
    loaders: [
      {
        // Now transpiling ES2016 TS
        test: /\.ts(x?)$/,
        exclude: /node_modules/,
        loader:
          &#x27;babel-loader?presets[]=es2016&amp;presets[]=es2015&amp;presets[]=react!ts-loader&#x27;,
      },
      {
        // Now transpiling ES2016 JS
        test: /\.js$/,
        exclude: /node_modules/,
        loader: &#x27;babel&#x27;,
        query: {
          presets: [&#x27;es2016&#x27;, &#x27;es2015&#x27;, &#x27;react&#x27;],
        },
      },
    ],
  },

  // ....
};
</code></pre><h2>Wake Up and Smell the Jasmine</h2><p>And we&#x27;re there; it works. How do I know? Well; here&#x27;s the proof:</p><pre><code class="language-ts">it(&#x27;Array.prototype.includes works&#x27;, () =&gt; {
  const result = [1, 2, 3].includes(2);
  expect(result).toBe(true);
});

it(&#x27;Exponentiation operator works&#x27;, () =&gt; {
  expect(1 ** 2 === Math.pow(1, 2)).toBe(true);
});
</code></pre><p>Much love to the TypeScript team for an awesome job; I can&#x27;t wait to get stuck into some of the exciting new features of TypeScript 2.0. <code>strictNullChecks</code> FTW!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Integration Tests with SQL Server Database Snapshots]]></title>
            <link>https://blog.johnnyreilly.com/2016/09/12/integration-tests-with-sql-server</link>
            <guid>Integration Tests with SQL Server Database Snapshots</guid>
            <pubDate>Mon, 12 Sep 2016 00:00:00 GMT</pubDate>
            <description><![CDATA[Once More With Feeling]]></description>
            <content:encoded><![CDATA[<h2>Once More With Feeling</h2><p>This is a topic that I have written about <a href="https://blog.johnnyreilly.com/2014/01/integration-testing-with-entity.html">before</a>.... But not well. I recently had cause to dust down my notes on how to use snapshotting in your integration tests. To my dismay, referring back to my original blog post was less helpful than I&#x27;d hoped. Now I&#x27;ve cracked the enigma code that my original scribings turned out to be, it&#x27;s time to turn my relearnings back into something genuinely useful.</p><h2>What&#x27;s the Scenario?</h2><p>You have a test database. You want to write integration tests. So what&#x27;s the problem? Well, these tests will add records, delete records, update records within the tables of the database. They will mutate the data. And that&#x27;s exactly what they ought to do; they&#x27;re testing that our code uses the database in the way we would hope and expect.</p><p>So how do we handle this? Well, we could handle this by writing code at the end of each test that is responsible for reverting the database back to the state that it was in at the start of the test. So if we had a test that added a record and tested it, we&#x27;d need the test to be responsible for removing that record before any subsequent tests run. Now that&#x27;s a totally legitimate approach but it adds tax. Each test becomes more complicated and requires more code.</p><p>So what&#x27;s another approach? Perhaps we could take a backup of our database before our first test runs. Then, at the end of each test, we could restore our backup to roll the database back to its initial state. Perfect, right? Less code to write, less scope for errors. So what&#x27;s the downside? Backups are slowwwww. Restores likewise. We could be waiting minutes between each test that runs. That&#x27;s not acceptable.</p><p>There is another way though: <a href="https://msdn.microsoft.com/en-us/library/ms175158.aspx">database snapshots</a> <!-- -->-<!-- --> a feature that&#x27;s been nestling inside SQL Server for a goodly number of years. For our use case, to all intents and purposes, database snapshots offers the same functionality as backups and restores. You can backup a database (take a snapshot of a database at a point in time), you can restore a database (roll back the database to the point of the snapshot). More importantly, you can do either operation in <!-- -->*<em>under a second</em>*<!-- -->. As it happens, Microsoft advocate using this approach themselves:</p><blockquote><p>In a testing environment, it can be useful when repeatedly running a test protocol for the database to contain identical data at the start of each round of testing. Before running the first round, an application developer or tester can create a database snapshot on the test database. After each test run, the database can be quickly returned to its prior state by reverting the database snapshot.</p></blockquote><p>Sold!</p><h2>Talk is cheap, show me the code</h2><p>In the end it comes down to 3 classes; <code>DatabaseSnapshot.cs</code> which does the actual snapshotting work and 2 classes that make use of it.</p><h3>DatabaseSnapshot.cs</h3><p>This is our <code>DatabaseSnapshot</code> class. Isn&#x27;t it pretty?</p><pre><code class="language-cs">using System.Data;
using System.Data.SqlClient;

namespace Testing.Shared
{
    public class DatabaseSnapshot
    {
        private readonly string _dbName;
        private readonly string _dbSnapShotPath;
        private readonly string _dbSnapShotName;
        private readonly string _dbConnectionString;

        public DatabaseSnapshot(string dbName, string dbSnapshotPath, string dbSnapshotName, string dbConnectionString)
        {
            _dbName = dbName;
            _dbSnapshotPath = dbSnapshotPath;
            _dbSnapshotName = dbSnapshotName;
            _dbConnectionString = dbConnectionString;
        }

        public void CreateSnapshot()
        {
            if (!System.IO.Directory.Exists(_dbSnapshotPath))
                System.IO.Directory.CreateDirectory(_dbSnapshotPath);

            var sql = $&quot;CREATE DATABASE { _dbSnapshotName } ON (NAME=[{ _dbName }], FILENAME=&#x27;{ _dbSnapshotPath }{ _dbSnapshotName }&#x27;) AS SNAPSHOT OF [{_dbName }]&quot;;

            ExecuteSqlAgainstMaster(sql);
        }

        public void DeleteSnapshot()
        {
            var sql = $&quot;DROP DATABASE { _dbSnapshotName }&quot;;

            ExecuteSqlAgainstMaster(sql);
        }

        public void RestoreSnapshot()
        {
            var sql = &quot;USE master;\r\n&quot; +

                $&quot;ALTER DATABASE {_dbName} SET SINGLE_USER WITH ROLLBACK IMMEDIATE;\r\n&quot; +

                $&quot;RESTORE DATABASE {_dbName}\r\n&quot; +
                $&quot;FROM DATABASE_SNAPSHOT = &#x27;{ _dbSnapshotName }&#x27;;\r\n&quot; +

                $&quot;ALTER DATABASE {_dbName} SET MULTI_USER;\r\n&quot;;

            ExecuteSqlAgainstMaster(sql);
        }

        private void ExecuteSqlAgainstMaster(string sql, params SqlParameter[] parameters)
        {
            using (var conn = new SqlConnection(_dbConnectionString))
            {
                conn.Open();
                var cmd = new SqlCommand(sql, conn) { CommandType = CommandType.Text };
                cmd.Parameters.AddRange(parameters);
                cmd.ExecuteNonQuery();
                conn.Close();
            }
        }
    }
}
</code></pre><p>It exposes 3 methods:</p><dl><dt>CreateSnapshot</dt><dd>This method creates the snapshot of the database. We will run this right at the start, before any of our tests run.</dd><dt>DeleteSnapshot</dt><dd>Deletes the snapshot we created. We will run this at the end, after all our tests have finished running.</dd><dt>RestoreSnapshot</dt><dd>Restores the database back to the snapshot we took earlier. We run this after each test has completed. This method relies on a connection to the database (perhaps unsurprisingly). It switches the database in use away from the database that is being restored prior to actually running the restore. It happens to shift to the master database (I believe that&#x27;s entirely incidental; although I haven&#x27;t tested).</dd></dl><h3>SetupAndTeardown.cs</h3><p>This class is responsible for setting up the snapshot we&#x27;re going to use in our tests right before any of the tests have run (in the <code>FixtureSetup</code> method). It&#x27;s also responsible for deleting the snapshot once all the tests have finished running (in the <code>FixtureTearDown</code> method). It should be noted that in this example I&#x27;m using NUnit and this class is written to depend on the hooks NUnit exposes for running code at the very beginning and end of the test cycle. All test frameworks have these hooks; if you&#x27;re using something other than NUnit then it&#x27;s just a case of swapping in the relevant attribute (everything tends to attribute driven in the test framework world).</p><pre><code class="language-cs">using NUnit.Framework;

namespace Testing.Shared
{
   [SetUpFixture]
   public class SetupAndTeardown
   {
      public static DatabaseSnapshot DatabaseSnapshot;

      [SetUp]
      public void FixtureSetup()
      {
         DatabaseSnapshot = new DatabaseSnapshot(&quot;MyDbName&quot;, &quot;C:\\&quot;, &quot;MySnapshot&quot;, &quot;Data Source=.;initial catalog=MyDbName;integrated security=True;&quot;);

         try
         {
            // Try to delete the snapshot in case it was left over from aborted test runs
            DatabaseSnapshot.DeleteSnapShot();
         }
         catch { /* this should fail with snapshot does not exist */ }

         DatabaseSnapshot.CreateSnapShot();
      }

      [TearDown]
      public void FixtureTearDown()
      {
         DatabaseSnapshot.DeleteSnapShot();
      }
   }
}
</code></pre><h3>TestBase.cs</h3><p>All of our test classes are made to inherit from this class:</p><pre><code class="language-cs">using NUnit.Framework;

namespace Testing.Shared
{
   public class TestBase
   {
      [TearDown]
      public void TearDown()
      {
         SetupAndTeardown.DatabaseSnapshot.RestoreSnapShot();
      }
   }
}
</code></pre><p>Which restores the database back to the snapshot position at the end of each test. And that... Is that!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Ternary Operator <3 Destructuring]]></title>
            <link>https://blog.johnnyreilly.com/2016/08/19/the-ternary-operator-meets-destructuring</link>
            <guid>The Ternary Operator &lt;3 Destructuring</guid>
            <pubDate>Fri, 19 Aug 2016 00:00:00 GMT</pubDate>
            <description><![CDATA[I'm addicted to the ternary operator. For reasons I can't explain, I cannot get enough of:]]></description>
            <content:encoded><![CDATA[<p>I&#x27;m addicted to the <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Conditional_Operator">ternary operator</a>. For reasons I can&#x27;t explain, I cannot get enough of:</p><pre><code class="language-js">const thisOrThat = someCondition ? &#x27;this&#x27; : &#x27;or that&#x27;;
</code></pre><p>The occasion regularly arises where I need to turn my lovely terse code into an if statement in order to set 2 variables instead of 1. I&#x27;ve been heartbroken; I hate doing:</p><pre><code class="language-ts">let legWear: string, coat: boolean;
if (weather === &#x27;good&#x27;) {
  legWear = &#x27;shorts&#x27;;
  coat = false;
} else {
  legWear = &#x27;jeans&#x27;;
  coat = true;
}
</code></pre><p>Just going from setting one variable to setting two has been really traumatic:</p><ul><li>I&#x27;ve had do stop using <code>const</code> and moved to <code>let</code>. This has made my code less &quot;truthful&quot; in the sense that I never intend to reassign these variables again; they are intended to be immutable.</li><li>I&#x27;ve gone from 1 line of code to <em>9 lines of code</em>. That&#x27;s 9x the code for increasing the number of variables in play by 1. That&#x27;s... heavy.</li><li>This third point only applies if you&#x27;re using TypeScript (and I am): I have to specify the types of my variables up front if I want type safety.</li></ul><p>ES2015 gives us another option. We can move back to the ternary operator if we change the return type of each branch to be an object sharing the same signature. Then, using destructuring, we can pull out those object properties into <code>const</code>s:</p><pre><code class="language-ts">const { legWear, coat } =
  weather === &#x27;good&#x27;
    ? { legWear: &#x27;shorts&#x27;, coat: false }
    : { legWear: &#x27;jeans&#x27;, coat: true };
</code></pre><p>With this approach we&#x27;re keeping usage of <code>const</code> instead of <code>let</code> and we&#x27;re only marginally increasing the amount of code we&#x27;re writing. If you&#x27;re using TypeScript you&#x27;re back to being able to rely on the compiler correctly inferring your types; you don&#x27;t need to specify. Awesome.</p><h2>Crowdfund You A Tuple</h2><p>I thought I was done and then I saw this:</p><blockquote><p><a href="https://twitter.com/johnny_reilly">@johnny_reilly</a> even neater with tuples: const <!-- -->[str, num]<!-- --> = test ? <!-- -->[&quot;yes&quot;, 100]<!-- --> : <!-- -->[&quot;no&quot;, 50]<!-- -->;</p><p>— Illustrated Pamphlet (@Rickenhacker) <a href="https://twitter.com/Rickenhacker/status/766913766323781632">August 20, 2016</a></p></blockquote><script src="//platform.twitter.com/widgets.js" charSet="utf-8"></script><p><a href="https://twitter.com/Rickenhacker">Daniel</a> helpfully points out that there&#x27;s an even terser syntax available to us:</p><pre><code class="language-ts">const [legWear, coat] =
  weather === &#x27;good&#x27; ? [&#x27;shorts&#x27;, false] : [&#x27;jeans&#x27;, true];
</code></pre><p>The above is ES2015 array destructuring. We get exactly the same effect but it&#x27;s a little terser as we don&#x27;t have to repeat the prop names as we do when using object destructuring. From a TypeScript perspective the assignment side of the above is a <a href="https://github.com/Microsoft/TypeScript/pull/428">Tuple</a> which allows our type inference to flow through in the manner we&#x27;d hope.</p><p>Lovely. Thanks!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Understanding Webpack's DefinePlugin (and using with TypeScript)]]></title>
            <link>https://blog.johnnyreilly.com/2016/07/23/using-webpacks-defineplugin-with-typescript</link>
            <guid>Understanding Webpack's DefinePlugin (and using with TypeScript)</guid>
            <pubDate>Sat, 23 Jul 2016 00:00:00 GMT</pubDate>
            <description><![CDATA[I've been searching for a way to describe what the DefinePlugin actually does. The docs say\*:]]></description>
            <content:encoded><![CDATA[<p>I&#x27;ve been searching for a way to describe what the DefinePlugin actually does. The <a href="https://github.com/webpack/docs/wiki/list-of-plugins#defineplugin">docs</a> say<!-- -->*<!-- -->:</p><blockquote><p>Define free variables. Useful for having development builds with debug logging or adding global constants.</p></blockquote><sub>\* Actually that should read &quot;used to say&quot;. I&#x27;ve made some changes to the official docs.... (Surprisingly easy to do that by the way; it&#x27;s just a wiki you can edit at will.)</sub><p>I think I would describe it thusly: the DefinePlugin allows you to create global constants which can be <em>configured at compile time</em>. I find this very useful for allowing different behaviour between development builds and release builds. This post will demonstrate usage of this approach, talk about what&#x27;s actually happening and how to get this working nicely with TypeScript.</p><p>If you just want to see this in action then take a look at this <a href="https://github.com/johnnyreilly/poorclaresarundel/">repo</a> and keep your eyes open for usage of <a href="https://github.com/johnnyreilly/poorclaresarundel/search?utf8=%E2%9C%93&amp;q=__VERSION__"><code>__VERSION__</code></a> and <a href="https://github.com/johnnyreilly/poorclaresarundel/search?utf8=%E2%9C%93&amp;q=__IN_DEBUG__"><code>__IN_DEBUG__</code></a>.</p><h2>What Globals?</h2><p>For our example we want to define 2 global constants; a string called <code>__VERSION__</code> and a boolean called <code>__IN_DEBUG__</code>. The names are deliberately wacky to draw attention to the fact that these are not your everyday, common-or-garden variables. Them&#x27;s &quot;special&quot;. These constants will be initialised with different values depending on whether we are in a debug build or a production build. Usage of these constants in our code might look like this:</p><pre><code class="language-ts">if (__IN_DEBUG__) {
  console.log(`This app is version ${__VERSION__}`);
}
</code></pre><p>So, if <code>__IN_DEBUG__</code> is set to <code>true</code> this code would log out to the console the version of the app.</p><h2>Configuring our Globals</h2><p>To introduce these constants to webpack we&#x27;re going to add this to our webpack configuration:</p><pre><code class="language-ts">var webpack = require(&#x27;webpack&#x27;);

// ...

plugins: [
  new webpack.DefinePlugin({
    __IN_DEBUG__: JSON.stringify(false),
    __VERSION__: JSON.stringify(&#x27;1.0.0.&#x27; + Date.now()),
  }),
  // ...
];
// ...
</code></pre><p>What&#x27;s going on here? Well, each key of the object literal above represents one of our global constants. When you look at the value, just imagine each outer <code>JSON.stringify( ... )</code> is not there. It&#x27;s just noise. Imagine instead that you&#x27;re seeing this:</p><pre><code class="language-ts">__IN_DEBUG__: false,
          __VERSION__: &#x27;1.0.0.&#x27; + Date.now()
</code></pre><p>A little clearer, right? <code>__IN_DEBUG__</code> is given the boolean value <code>false</code> and <code>__VERSION__</code> is given the string value of <code>1.0.0.</code> plus the ticks off of <code>Date.now()</code>. What&#x27;s happening here is well explained in Pete Hunt&#x27;s excellent <a href="https://github.com/petehunt/webpack-howto#6-feature-flags">webpack howto</a>: &quot;definePlugin takes raw strings and inserts them&quot;. <code>JSON.stringify</code> facilitates this; it produces a string representation of a value that can be inlined into code. When the inlining takes place the actual output would be something like this:</p><pre><code class="language-ts">if (false) {
  // Because at compile time, __IN_DEBUG__ === false
  console.log(`This app is version ${&#x27;1.0.0.1469268116580&#x27;}`); // And __VERSION__ === &quot;1.0.0.1469268116580&quot;
}
</code></pre><p>And if you&#x27;ve got some <a href="https://github.com/mishoo/UglifyJS">UglifyJS</a> or similar in the mix then, in the example above, this would actually strip out the statement above entirely since it&#x27;s clearly a <a href="https://en.wikipedia.org/wiki/NOP">NOOP</a>. Yay the dead code removal! If <code>__IN_DEBUG__</code> was <code>false</code> then (perhaps obviously) this statement would be left in place as it wouldn&#x27;t be dead code.</p><h2>TypeScript and Define</h2><p>The final piece of the puzzle is making TypeScript happy. It doesn&#x27;t know anything about our global constants. So we need to tell it:</p><pre><code class="language-ts">declare var __IN_DEBUG__: boolean;
declare var __VERSION__: string;
</code></pre><p>And that&#x27;s it. Compile time constants are a go!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Creating an ES2015 Map from an Array in TypeScript]]></title>
            <link>https://blog.johnnyreilly.com/2016/06/02/create-es2015-map-from-array-in-typescript</link>
            <guid>Creating an ES2015 Map from an Array in TypeScript</guid>
            <pubDate>Thu, 02 Jun 2016 00:00:00 GMT</pubDate>
            <description><![CDATA[I'm a great lover of ES2015's Map. However, just recently I tumbled over something I find a touch inconvenient about how you initialise a new Map from the contents of an Array in TypeScript.]]></description>
            <content:encoded><![CDATA[<p>I&#x27;m a great lover of ES2015&#x27;s <code>&lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Map&quot;&gt;Map&lt;/a&gt;</code>. However, just recently I tumbled over something I find a touch inconvenient about how you initialise a new <code>Map</code> from the contents of an <code>Array</code> in TypeScript.</p><h2>This Doesn&#x27;t Work</h2><p>We&#x27;re going try to something like this: (pilfered from the MDN docs)</p><pre><code class="language-ts">var kvArray = [
  [&#x27;key1&#x27;, &#x27;value1&#x27;],
  [&#x27;key2&#x27;, &#x27;value2&#x27;],
];

// Use the regular Map constructor to transform a 2D key-value Array into a map
var myMap = new Map(kvArray);
</code></pre><p>Simple enough right? Well I&#x27;d rather assumed that I should be able to do something like this in TypeScript:</p><pre><code class="language-ts">const iAmAnArray [
  { value: &quot;value1&quot;, text: &quot;hello&quot; }
  { value: &quot;value2&quot;, text: &quot;map&quot; }
];

const iAmAMap = new Map&lt;string, string&gt;(
  iAmAnArray.map(x =&gt; [x.value, x.text])
);
</code></pre><p>However, to my surprise this errored out with:</p><pre><code>[ts] Argument of type &#x27;string[][]&#x27; is not assignable to parameter of type &#x27;Iterable&lt;[string, string]&gt;&#x27;.
  Types of property &#x27;[Symbol.iterator]&#x27; are incompatible.
    Type &#x27;() =&gt; IterableIterator&lt;string[]&gt;&#x27; is not assignable to type &#x27;() =&gt; Iterator&lt;[string, string]&gt;&#x27;.
      Type &#x27;IterableIterator&lt;string[]&gt;&#x27; is not assignable to type &#x27;Iterator&lt;[string, string]&gt;&#x27;.
        Types of property &#x27;next&#x27; are incompatible.
          Type &#x27;(value?: any) =&gt; IteratorResult&lt;string[]&gt;&#x27; is not assignable to type &#x27;(value?: any) =&gt; IteratorResult&lt;[string, string]&gt;&#x27;.
            Type &#x27;IteratorResult&lt;string[]&gt;&#x27; is not assignable to type &#x27;IteratorResult&lt;[string, string]&gt;&#x27;.
              Type &#x27;string[]&#x27; is not assignable to type &#x27;[string, string]&#x27;.
                Property &#x27;0&#x27; is missing in type &#x27;string[]&#x27;.
</code></pre><p>Disappointing right? It&#x27;s expecting <code>Iterable&amp;lt;[string, string]&amp;gt;</code> and an <code>Array</code> with 2 elements that are strings is <em>not</em> inferred to be that.</p><h2>This Does</h2><p>It emerges that there is a way to do this though; you just need to give the compiler a clue. You need to include a type assertion of <code> as [string, string]</code> which tells the compiler that what you&#x27;ve just declared is a <code>Tuple</code> of <code>string</code> and <code>string</code>. (Please note that <code>[string, string]</code> corresponds to the types of the <code>Key</code> and <code>Value</code> of your <code>Map</code> and should be set accordingly.)</p><p>So a working version of the code looks like this:</p><pre><code class="language-ts">const iAmAnArray [
  { value: &quot;value1&quot;, text: &quot;hello&quot; }
  { value: &quot;value2&quot;, text: &quot;map&quot; }
];

const iAmAMap = new Map&lt;string, string&gt;(
  iAmAnArray.map(x =&gt; [x.value, x.text] as [string, string])
);
</code></pre><p>Or, to be terser, this:</p><pre><code class="language-ts">const iAmAnArray [
  { value: &quot;value1&quot;, text: &quot;hello&quot; }
  { value: &quot;value2&quot;, text: &quot;map&quot; }
];

const iAmAMap = new Map( // Look Ma!  No type annotations
  iAmAnArray.map(x =&gt; [x.value, x.text] as [string, string])
);
</code></pre><p>I&#x27;ve raised this as an issue with the TypeScript team; you can find details <a href="https://github.com/Microsoft/TypeScript/issues/8936">here</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Mysterious Case of Webpack, Angular and jQuery]]></title>
            <link>https://blog.johnnyreilly.com/2016/05/24/the-mysterious-case-of-webpack-angular-and-jquery</link>
            <guid>The Mysterious Case of Webpack, Angular and jQuery</guid>
            <pubDate>Tue, 24 May 2016 00:00:00 GMT</pubDate>
            <description><![CDATA[You may know that Angular ships with a cutdown version of jQuery called jQLite. It's still possible to use the full-fat jQuery; to quote the docs:]]></description>
            <content:encoded><![CDATA[<p>You may know that <a href="https://docs.angularjs.org/api/ng/function/angular.element">Angular ships with a cutdown version of jQuery called jQLite</a>. It&#x27;s still possible to use the full-fat jQuery; to quote the docs:</p><blockquote><p>To use <code>jQuery</code>, simply ensure it is loaded before the <code>angular.js</code> file.</p></blockquote><p>Now the wording rather implies that you&#x27;re not using any module loader / bundler. Rather that all files are being loaded via <code>script</code> tags and relies on the global variables that result from that. True enough, if you take a look at the <a href="https://github.com/angular/angular.js/blob/eaa1119d4252bed08dfa42f984ef9502d0f02775/src/Angular.js#L1791">Angular source</a> you can see how this works:</p><pre><code class="language-ts">// bind to jQuery if present;
var jqName = jq();
jQuery = isUndefined(jqName)
  ? window.jQuery // use jQuery (if present)
  : !jqName
  ? undefined // use jqLite
  : window[jqName]; // use jQuery specified by `ngJq`
</code></pre><p>Amongst other things it looks for a <code>jQuery</code> variable which has been placed onto the <code>window</code> object. If it is found then jQuery is used; if it is not then it&#x27;s <code>jqLite</code> all the way.</p><h2>But wait! I&#x27;m using webpack</h2><p>Me too! And one of the reasons is that we get to move away from reliance upon the global scope and towards proper modularisation. So how do we get Angular to use jQuery given the code we&#x27;ve seen above? Well, your first thought might be to <code>npm install</code> yourself some <code>jQuery</code> and then make sure you&#x27;ve got something like this in your entry file:</p><pre><code class="language-ts">import &#x27;jquery&#x27;; // This&#x27;ll fix it... Right?
import * as angular from &#x27;angular&#x27;;
</code></pre><p>Wrong.</p><h2>You need the <code>ProvidePlugin</code></h2><p>In your <code>webpack.config.js</code> you need to add the following entry to your plugins:</p><pre><code class="language-ts">new webpack.ProvidePlugin({
          &quot;window.jQuery&quot;: &quot;jquery&quot;
      }),
</code></pre><p>This uses the webpack <code>&lt;a href=&quot;https://github.com/webpack/docs/wiki/list-of-plugins#provideplugin&quot;&gt;ProvidePlugin&lt;/a&gt;</code> and, at the point of webpackification (© 2016 John Reilly) all references in the code to <code>window.jQuery</code> will be replaced with a reference to the webpack module that contains jQuery. So when you look at the bundled file you&#x27;ll see that the code that checks the <code>window</code> object for <code>jQuery</code> has become this:</p><pre><code class="language-ts">jQuery = isUndefined(jqName)
  ? __webpack_provided_window_dot_jQuery // use jQuery (if present)
  : !jqName
  ? undefined // use jqLite
  : window[jqName]; // use jQuery specified by `ngJq`
</code></pre><p>That&#x27;s right; webpack is providing Angular with jQuery whilst still <em>not</em> placing a <code>jQuery</code> variable onto the <code>window</code>. Neat huh?</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Inlining Angular Templates with WebPack and TypeScript]]></title>
            <link>https://blog.johnnyreilly.com/2016/05/13/inlining-angular-templates-with-webpack</link>
            <guid>Inlining Angular Templates with WebPack and TypeScript</guid>
            <pubDate>Fri, 13 May 2016 00:00:00 GMT</pubDate>
            <description><![CDATA[This technique actually applies to pretty much any web stack where you have to supply templates; it just so happens that I'm using Angular 1.x in this case. Also I have an extra technique which is useful to handle the ng-include scenario.]]></description>
            <content:encoded><![CDATA[<p>This technique actually applies to pretty much any web stack where you have to supply templates; it just so happens that I&#x27;m using Angular 1.x in this case. Also I have an extra technique which is useful to handle the <a href="https://docs.angularjs.org/api/ng/directive/ngInclude">ng-include</a> scenario.</p><h2>Preamble</h2><p>For some time I&#x27;ve been using webpack to bundle my front end. I write ES6 TypeScript; import statements and all. This is all sewn together using the glorious <a href="https://www.npmjs.com/package/ts-loader">ts-loader</a> to compile and emit ES6 code which is handed off to the wonderful <a href="https://www.npmjs.com/package/babel-loader">babel-loader</a> which transpiles it to ESold code. All with full source map support. It&#x27;s wonderful.</p><p>However, up until now I&#x27;ve been leaving Angular to perform the relevant http requests at runtime when it needs to pull in templates. That works absolutely fine but my preference is to preload those templates. In fact I&#x27;ve <a href="http://blog.johnnyreilly.com/2015/02/using-gulp-in-asp-net-instead-of-web-optimization.html">written before</a> about using the <a href="https://www.npmjs.com/package/gulp-angular-templatecache">gulp angular template cache</a> to achieve just that aim.</p><p>So I was wondering; in this modular world what would be the equivalent approach? Sure I could still use the gulp angular template cache approach but I would like something a little more deliberate and a little less magic. Also, I&#x27;ve discovered (to my cost) that when using the existing approach, it&#x27;s possible to break the existing implementation without realising it; only finding out there&#x27;s a problem in Production when unexpected http requests start happening. Finding these problems out at compile time rather than runtime is always to be strived for. So how?</p><h2><a href="https://www.npmjs.com/package/raw-loader">raw-loader</a>!</h2><p>raw-loader allows you load file content using <code>require</code> statements. This works well with the use case of inlining html. So I drop it into my <code>webpack.config.js</code> like so:</p><pre><code class="language-js">var path = require(&#x27;path&#x27;);

module.exports = {
  cache: true,
  entry: {
    main: &#x27;./src/main.ts&#x27;,

    vendor: [
      &#x27;babel-polyfill&#x27;,
      &#x27;angular&#x27;,
      &#x27;angular-animate&#x27;,
      &#x27;angular-sanitize&#x27;,
      &#x27;angular-ui-bootstrap&#x27;,
      &#x27;angular-ui-router&#x27;,
    ],
  },
  output: {
    path: path.resolve(__dirname, &#x27;./dist/scripts&#x27;),
    filename: &#x27;[name].js&#x27;,
    chunkFilename: &#x27;[chunkhash].js&#x27;,
  },
  module: {
    loaders: [
      {
        test: /\.ts(x?)$/,
        exclude: /node_modules/,
        loader: &#x27;babel-loader?presets[]=es2015!ts-loader&#x27;,
      },
      {
        test: /\.js$/,
        exclude: /node_modules/,
        loader: &#x27;babel&#x27;,
        query: {
          presets: [&#x27;es2015&#x27;],
        },
      },
      {
        // THIS IS THE MAGIC!
        test: /\.html$/,
        exclude: /node_modules/,
        loader: &#x27;raw&#x27;,
      },
    ], // THAT WAS THE MAGIC!
  },
  plugins: [
    // ....
  ],
  resolve: {
    extensions: [&#x27;&#x27;, &#x27;.ts&#x27;, &#x27;.tsx&#x27;, &#x27;.js&#x27;],
  },
};
</code></pre><p>With this in place, if someone requires a file with the <code>html</code> suffix then raw-loader comes in. So now we can swap this:</p><pre><code class="language-js">$stateProvider.state(&#x27;state1&#x27;, {
  url: &#x27;/state1&#x27;,
  templateUrl: &#x27;partials/state1.html&#x27;,
});
</code></pre><p>For this:</p><pre><code class="language-js">$stateProvider.state(&#x27;state1&#x27;, {
  url: &#x27;/state1&#x27;,
  template: require(&#x27;./partials/state1.html&#x27;),
});
</code></pre><p>Now initially TypeScript is going to complain about your <code>require</code> statement. That&#x27;s fair; outside of node-land it doesn&#x27;t know what <code>require</code> is. No bother, you just need to drop in a one line simple definition file to sort this out; let me present <code>webpack-require.d.ts</code>:</p><pre><code class="language-ts">declare var require: (filename: string) =&gt; any;
</code></pre><p>You&#x27;ve now inlined your template. And for bonus points, if you were to make a mistake in your path then webpack would shout at you at compile time; which is a <em>good, good</em> thing.</p><h2>ng-include</h2><p>The one use case that this doesn&#x27;t cover is where your templates import other templates through use of the <a href="https://docs.angularjs.org/api/ng/directive/ngInclude">ng-include</a> directive. They will still trigger http requests as the templates are served. The simple way to prevent that is by priming the angular <code>&lt;a href=&quot;https://docs.angularjs.org/api/ng/service/$templateCache&quot;&gt;$templateCache&lt;/a&gt;</code> like so:</p><pre><code class="language-js">app.run([
  &#x27;$templateCache&#x27;,
  ($templateCache: ng.ITemplateCacheService) =&gt; {
    $templateCache.put(&#x27;justSome.html&#x27;, require(&#x27;./justSome.html&#x27;));
    // Other templates go here...
  },
]);
</code></pre><p>Now when the app spins up it already has everything it needs pre-cached.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Instant Stubs with JSON.Net (just add hot water)]]></title>
            <link>https://blog.johnnyreilly.com/2016/04/25/instant-stubs-with-jsonnet</link>
            <guid>Instant Stubs with JSON.Net (just add hot water)</guid>
            <pubDate>Mon, 25 Apr 2016 00:00:00 GMT</pubDate>
            <description><![CDATA[I'd like you to close your eyes and imagine a scenario. You're handed a prototype system. You're told it works. It has no documentation. It has 0 unit tests. The hope is that you can take it on, refactor it, make it better and (crucially) not break it. Oh, and you don't really understand what the code does or why it does it either; information on that front is, alas, sorely lacking.]]></description>
            <content:encoded><![CDATA[<p>I&#x27;d like you to close your eyes and imagine a scenario. You&#x27;re handed a prototype system. You&#x27;re told it works. It has no documentation. It has 0 unit tests. The hope is that you can take it on, refactor it, make it better and (crucially) not break it. Oh, and you don&#x27;t really understand what the code does or why it does it either; information on that front is, alas, sorely lacking.</p><p>This has happened to me; it&#x27;s alas not that unusual. The common advice handed out in this situation is: &quot;add unit tests before you change it&quot;. That&#x27;s good advice. We need to take the implementation that embodies the correctness of the system and create unit tests that set that implementation in stone. However, what say the system that you&#x27;re hoping to add tests to takes a number of large and complex inputs from some external source and produces a similarly large and complex output?</p><p>You could start with integration tests. They&#x27;re good but slow and crucially they depend upon the external inputs being available and unchanged (which is perhaps unlikely). What you could do (what I have done) is debug a working working system. At each point that an input is obtained I have painstakingly transcribed the data which allows me to subsequently hand code stub data. There comes a point when this is plainly untenable; it&#x27;s just too much data to transcribe. At this point the temptation is to think &quot;it&#x27;s okay; I can live without the tests. I&#x27;ll just be super careful with my refactoring... It&#x27;ll be fine It&#x27;ll be fine It&#x27;ll be fine It&#x27;ll be fine&quot;.</p><p>Actually, it probably won&#x27;t be fine. And even if it is (miracles do happen) you&#x27;re going to be fairly stressed as you wonder if you&#x27;ve been careful enough. What if there was another way? A way that wasn&#x27;t quite so hard but that allowed you to add tests without requiring 3 months hand coding....</p><h2>Instant Stubs</h2><p>What I&#x27;ve come up with is a super simple utility class for creating stubs / fakes. (I&#x27;m aware the naming of such things <a href="http://martinfowler.com/articles/mocksArentStubs.html">can be a little contentious</a>.)</p><pre><code class="language-cs">using Newtonsoft.Json;
using System;
using System.IO;

namespace MakeFakeData.UnitTests
{
  public static class Stubs
  {
    private static JsonSerializer _serializer = new JsonSerializer { NullValueHandling = NullValueHandling.Ignore };

    public static void Make&lt;T&gt;(string stubPath, T data)
    {
      try
      {
        if (string.IsNullOrEmpty(stubPath))
          throw new ArgumentNullException(nameof(stubPath));
        if (data == null)
          throw new ArgumentNullException(nameof(data));

        using (var sw = new StreamWriter(stubPath))
        using (var writer = new JsonTextWriter(sw) {
            Formatting = Formatting.Indented,
            IndentChar = &#x27; &#x27;,
            Indentation = 2})
        {
          _serializer.Serialize(writer, data);
        }
      }
      catch (Exception exc)
      {
        throw new Exception($&quot;Failed to make {stubPath}&quot;, exc);
      }
    }

    public static T Load&lt;T&gt;(string stubPath)
    {
      try
      {
        if (string.IsNullOrEmpty(stubPath))
          throw new ArgumentNullException(nameof(stubPath));

        using (var file = File.OpenText(stubPath))
        using (var reader = new JsonTextReader(file))
        {
          return _serializer.Deserialize&lt;T&gt;(reader);
        }
      }
      catch (Exception exc)
      {
        throw new Exception($&quot;Failed to load {stubPath}&quot;, exc);
      }
    }
  }
}
</code></pre><p>As you can see this class uses <a href="http://www.newtonsoft.com/json">JSON.Net</a> and exposes 2 methods:</p><dl><dt>Make</dt><dd>Takes a given piece of data and uses JSON.Net to serialise it as JSON to a file. (nb I choose to format the JSON for readability and exclude null values; both totally optional)</dd><dt>Load</dt><dd>Takes the given path and loads the associated JSON file and deserialises it back into an object.</dd></dl><p>The idea is this: we take our working implementation and, wherever it extracts data from an external source, we insert a temporary statement like this:</p><pre><code class="language-cs">var data = _dataService.GetComplexData();

    // Just inserted so we can generate the stub data...
    Stubs.Make($&quot;{System.AppDomain.CurrentDomain.BaseDirectory}\\data.json&quot;, data);
</code></pre><p>The next time you run the implementation you&#x27;ll find the app generates a <code>data.json</code> file containing the complex data serialized to JSON. Strip out your <code>Stubs.Make</code> statements from the implementation and we&#x27;re ready for the next stage.</p><h2>Using your JSON</h2><p>What you need to do now is to take the new and shiny <code>data.json</code> file and move it to your unit test project. It needs to be included within the unit test project. Also, for each JSON file you have, the <code>Build Action</code> in VS needs to be set to <code>Content</code> and the <code>Copy to Output Directory</code> to <code>Copy if newer</code>.</p><p>Then within your unit tests you can write code like this:</p><pre><code class="language-ts">var dummyData = Stubs.Load&lt;ComplexDataType&gt;(&#x27;Stubs/data.json&#x27;);
</code></pre><p>Which pulls in your data from the JSON file and deserialises it into the original types. With this in hand you can plug together a unit test based on an existing implementation which depends on external data much faster than the hand-cranked method of old.</p><p>Finally, before the wildebeest of TDD descend upon me howling and wailing, let me say again; I anticipate this being useful when you&#x27;re trying to add tests to something that already exists but is untested. Clearly it would be better not to be in this situaion in the first place.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Elvis and King Concat]]></title>
            <link>https://blog.johnnyreilly.com/2016/03/22/elvis-and-king-concat</link>
            <guid>Elvis and King Concat</guid>
            <pubDate>Tue, 22 Mar 2016 00:00:00 GMT</pubDate>
            <description><![CDATA[I hate LINQ's Enumerable.Concat when bringing together IEnumerables. Not the behaviour (I love that!) but rather how code ends up looking when you use it. Consider this:]]></description>
            <content:encoded><![CDATA[<p>I hate LINQ&#x27;s <code>&lt;a href=&quot;https://msdn.microsoft.com/en-us/library/bb302894%28v=vs.110%29.aspx?f=255&amp;amp;MSPPError=-2147217396&quot;&gt;Enumerable.Concat&lt;/a&gt;</code> when bringing together <code>IEnumerable</code>s. Not the behaviour (I love that!) but rather how code ends up looking when you use it. Consider this:</p><pre><code class="language-cs">var concatenated = myCollection?.Select(x =&gt; new ConcatObj(x)) ?? new ConcatObj[0].Concat(
   myOtherCollection?.Select(x =&gt; new ConcatObj(x)) ?? new ConcatObj[0]
);
</code></pre><p>In this example I&#x27;m bringing together 2 collections, either of which may be null (more on that later). I think we can all agree this doesn&#x27;t represent a world of readability. I&#x27;ve also had to create a custom class <code>ConcatObj</code> because you can&#x27;t create an empty array for an anonymous type in C#.</p><h2>Attempt #1: <code>ConcatMany</code></h2><p>After toying around with a bunch of different ideas I created this extension method:</p><pre><code class="language-cs">public static class FunctionalExtensions
{
    public static IEnumerable&lt;T&gt; ConcatMany&lt;T&gt;(
        this IEnumerable&lt;T&gt; original,
        params IEnumerable&lt;T&gt;[] enumerablesToConcat) =&gt; original.Concat(
            enumerablesToConcat.Where(e =&gt; e != null).SelectMany(c =&gt; c)
        );
}
</code></pre><p>Thanks to the joy of <code>params</code> this extension allows me to bring together multiple IEnumerables into a single one but has the advantage of considerably cleaner calling code:</p><pre><code class="language-cs">var concatenated = Enumerable.Empty&lt;ConcatObj&gt;().ConcatMany(
    myCollection?.Select(x =&gt; new ConcatObj(x)),
    myOtherCollection?.Select(x =&gt; new ConcatObj(x))
    );
</code></pre><p>For my money this is more readable and intent is clearer. Particularly as the number of contributing IEnumerables goes up. The downside is that I can’t use anonymous objects because you need to tell the compiler what the type is when using <code>&lt;a href=&quot;https://msdn.microsoft.com/en-us/library/bb341042%28v=vs.110%29.aspx?f=255&amp;amp;MSPPError=-2147217396&quot;&gt;Enumerable.Empty&lt;/a&gt;</code>.</p><p>Wouldn&#x27;t it be nice to have both:</p><ol><li>Readable code and</li><li>Anonymous objects?</li></ol><h2>Attempt #2: <code>EnumerableExtensions.Create</code></h2><p>After batting round a few ideas (thanks Matt) I settled on this implementation:</p><pre><code class="language-cs">public static class EnumerableExtensions
{
    public static IEnumerable&lt;TSource&gt; Create&lt;TSource&gt;(params IEnumerable&lt;TSource&gt;[] enumerables)
    {
        return Concat(enumerables.Where(e =&gt; e != null));
    }

    private static IEnumerable&lt;TSource&gt; Concat&lt;TSource&gt;(IEnumerable&lt;IEnumerable&lt;TSource&gt;&gt; enumerables)
    {
        foreach (var enumerable in enumerables)
        {
            foreach (var item in enumerable)
            {
                yield return item;
            }
        }
    }
}
</code></pre><p>Which allows for calling code like this:</p><pre><code class="language-cs">var concatenated = EnumerableExtensions.Create(
    myCollection?.Select(x =&gt; new { Anonymous = x.Types }),
    myOtherCollection?.Select(x =&gt; new { Anonymous = x.Types })
    );
</code></pre><p>That&#x27;s right; anonymous types are back! Strictly speaking the <code>Concat</code> method above could be converted into a single <code>SelectMany</code> (and boy does ReSharper like telling me) but I&#x27;m quite happy with it as is. And to be honest, I so rarely get to use <code>yield</code> in my own code; I thought it might be nice to give it a whirl 😊</p><h2>What Gives Elvis?</h2><p>If you look closely at the implementation you&#x27;ll notice that I purge all <code>null</code>s when I&#x27;m bringing together the <code>Enumerable</code>s. For why? Some may legitimately argue this is a bad idea. However, there is method in my &quot;bad practice&quot;.</p><p>I&#x27;ve chosen to treat <code>null</code> as &quot;not important&quot; for this use case. I&#x27;m doing this because it emerges that ASP.NET MVC deserialises empty collections as nulls. (See <a href="http://aspnetwebstack.codeplex.com/SourceControl/latest#src/System.Web.Mvc/ValueProviderResult.cs">here</a> and play spot the <code>return null;</code>) Which is a pain. But thanks to the null purging behaviour of <code>EnumerableExtensions.Create</code> I can trust in the <a href="https://csharp.today/c-6-features-null-conditional-and-and-null-coalescing-operators/">null-conditional (Elvis)</a> operator to not do me wrong.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Atom - Recovering from Corrupted Packages]]></title>
            <link>https://blog.johnnyreilly.com/2016/03/17/atom-recovering-from-corrupted-packages</link>
            <guid>Atom - Recovering from Corrupted Packages</guid>
            <pubDate>Thu, 17 Mar 2016 00:00:00 GMT</pubDate>
            <description><![CDATA[Every now and then when I try and update my packages in Atom I find this glaring back at me:]]></description>
            <content:encoded><![CDATA[<p>Every now and then when I try and update my packages in <a href="https://atom.io/">Atom</a> I find this glaring back at me:</p><p><img src="../static/blog/2016-03-17-atom-recovering-from-corrupted-packages/Screenshot%2B2016-03-17%2B06.17.03.png"/></p><p>Ug. The problem is that my atom packages have become corrupt. Quite how I couldn&#x27;t say. But that&#x27;s the problem. Atom, as I know from bitter experience, will not recover from this. It just sits there feeling sorry for itself. However, getting back to where you belong is simpler than you imagine:</p><ol><li><p>Shutdown Atom</p></li><li><p>In the file system go to <code>[Your name]/.atom</code> (and bear in mind this is Windows; Macs / Linux may be different) <img src="../static/blog/2016-03-17-atom-recovering-from-corrupted-packages/Screenshot%2B2016-03-17%2B06.17.53.png"/></p></li><li><p>You&#x27;ll see an <code>.apm</code> folder that contains all your packages. Delete this.</p></li></ol><p>When you next fire up Atom these packages will automagically come back but this time they shouldn&#x27;t be corrupt. Instead you should see the happiness of normality restored:</p><p><img src="../static/blog/2016-03-17-atom-recovering-from-corrupted-packages/Screenshot%2B2016-03-17%2B06.23.18.png"/></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[TFS 2012 meet PowerShell, Karma and BuildNumber]]></title>
            <link>https://blog.johnnyreilly.com/2016/03/04/tfs-2012-meet-powershell-karma-and-buildnumber</link>
            <guid>TFS 2012 meet PowerShell, Karma and BuildNumber</guid>
            <pubDate>Fri, 04 Mar 2016 00:00:00 GMT</pubDate>
            <description><![CDATA[To my lasting regret, TFS 2012 has no direct support for PowerShell. Such a shame as PowerShell scripts can do a lot of heavy lifting in a build process. Well, here we're going to brute force TFS 2012 into running PowerShell scripts. And along the way we'll also get Karma test results publishing into TFS 2012 as an example usage. Nice huh? Let's go!]]></description>
            <content:encoded><![CDATA[<p>To my lasting regret, TFS 2012 has no direct support for PowerShell. Such a shame as PowerShell scripts can do a lot of heavy lifting in a build process. Well, here we&#x27;re going to brute force TFS 2012 into running PowerShell scripts. And along the way we&#x27;ll also get Karma test results publishing into TFS 2012 as an example usage. Nice huh? Let&#x27;s go!</p><h2>PowerShell via <code>csproj</code></h2><p>It&#x27;s time to hack the <code>csproj</code> (or whatever project file you have) again. We&#x27;re going to add an <code>AfterBuild</code> target to the end of the file. This target will be triggered after the build completes (as the name suggests):</p><pre><code class="language-xml">&lt;Target Name=&quot;AfterBuild&quot;&gt;
    &lt;Message Importance=&quot;High&quot; Text=&quot;AfterBuild: PublishUrl = $(PublishUrl), BuildUri = $(BuildUri), Configuration = $(Configuration), ProjectDir = $(ProjectDir), TargetDir = $(TargetDir), TargetFileName = $(TargetFileName), BuildNumber = $(BuildNumber), BuildDefinitionName = $(BuildDefinitionName)&quot; /&gt;
    &lt;Exec Command=&quot;powershell.exe -NonInteractive -ExecutionPolicy RemoteSigned &quot;&amp; &#x27;$(ProjectDir)AfterBuild.ps1&#x27; &#x27;$(Configuration)&#x27; &#x27;$(ProjectDir)&#x27; &#x27;$(TargetDir)&#x27; &#x27;$(PublishUrl)&#x27; &#x27;$(BuildNumber)&#x27; &#x27;$(BuildDefinitionName)&#x27;&quot;&quot; /&gt;
  &lt;/Target&gt;
</code></pre><p>There&#x27;s 2 things happening in this target:</p><ol><li>A message is printed out during compilation which contains details of the various compile time variables. This is nothing more than a <code>console.log</code> statement really; it&#x27;s useful for debugging and so I keep it around. You&#x27;ll notice one of them is called <code>$(BuildNumber)</code>; more on that later.</li><li>A command is executed; PowerShell! This invokes PowerShell with the <code>-NonInteractive</code> and <code>-ExecutionPolicy RemoteSigned</code> flags. It passes a script to be executed called <code>AfterBuild.ps1</code> that lives in the root of the project directory. To that script a number of parameters are supplied; compile time variables that we may use in the script.</li></ol><h2>Where&#x27;s my <code>BuildNumber</code> and <code>BuildDefinitionName</code>?</h2><p>So you&#x27;ve checked in your changes and kicked off a build on the server. You&#x27;re picking over the log messages and you&#x27;re thinking: &quot;Where&#x27;s my <code>BuildNumber</code>?&quot;. Well, TFS 2012 does not have that set as a variable at compile time by default. This stumped me for a while but thankfully I wasn&#x27;t the only person wondering... As ever, <a href="http://stackoverflow.com/a/7330453/761388">Stack Overflow had the answer</a>. So, deep breath, it&#x27;s time to hack the TFS build template file.</p><p>Checkout the <code>DefaultTemplate.11.1.xaml</code> file from TFS and open it in your text editor of choice. It&#x27;s <em>find and replace</em> time! (There are probably 2 instances that need replacement.) Perform a <em>find</em> for the below</p><pre><code class="language-js">[String.Format(&amp;quot;/p:SkipInvalidConfigurations=true {0}&amp;quot;, MSBuildArguments)]
</code></pre><p>And <em>replace</em> it with this:</p><pre><code class="language-js">[
  String.Format(
    &#x27;/p:SkipInvalidConfigurations=true /p:BuildNumber={1} /p:BuildDefinitionName={2} {0}&#x27;,
    MSBuildArguments,
    BuildDetail.BuildNumber,
    BuildDetail.BuildDefinition.Name
  ),
];
</code></pre><p>Pretty long line eh? Let&#x27;s try breaking that up to make it more readable: (but remember in the XAML it needs to be a one liner)</p><pre><code class="language-js">[String.Format(&quot;/p:SkipInvalidConfigurations=true
    /p:BuildNumber={1}
    /p:BuildDefinitionName={2} {0}&quot;, MSBuildArguments, BuildDetail.BuildNumber, BuildDetail.BuildDefinition.Name)]
</code></pre><p>We&#x27;re just adding 2 extra parameters of <code>BuildNumber</code> and <code>BuildDefinitionName</code> to the invocation of MSBuild. Once we&#x27;ve checked this back in, <code>BuildNumber</code> and <code>BuildDefinitionName</code> will be available on future builds. Yay! <em>Important! You must have a build name that does not feature spaces; probably there&#x27;s a way to pass spaces here but I&#x27;m not sure what it is.</em></p><h2><code>AfterBuild.ps1</code></h2><p>You can use your <code>AfterBuild.ps1</code> script to do any number of things. In my case I&#x27;m going to use MSTest to publish some test results which have been generated by Karma into TFS:</p><pre><code class="language-ps">param ([string]$configuration, [string]$projectDir, [string]$targetDir, [string]$publishUrl, [string]$buildNumber, [string] $buildDefinitionName)

$ErrorActionPreference = &#x27;Stop&#x27;
Clear

function PublishTestResults([string]$resultsFile) {
 Write-Host &#x27;PublishTests&#x27;
 $mstest = &#x27;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE\MSTest.exe&#x27;

 Write-Host &quot;Using $mstest at $pwd&quot;
 Write-Host &quot;Publishing: $resultsFile&quot;

 &amp; $mstest /publishresultsfile:$resultsFile /publish:http://my-tfs-server:8080/tfs /teamproject:MyProject /publishbuild:$buildNumber /platform:&#x27;Any CPU&#x27; /flavor:Release
}

function FailBuildIfThereAreTestFailures([string]$resultsFile) {
 $results = [xml](GC $resultsFile)
 $outcome = $results.TestRun.ResultSummary.outcome
 $fgColor = if($outcome -eq &quot;Failed&quot;) { &quot;Red&quot; } else { &quot;Green&quot; }
 $total = $results.TestRun.ResultSummary.Counters.total
 $passed = $results.TestRun.ResultSummary.Counters.passed
 $failed = $results.TestRun.ResultSummary.Counters.failed

 $failedTests = $results.TestRun.Results.UnitTestResult | Where-Object { $_.outcome -eq &quot;Failed&quot; }

 Write-Host Test Results: $outcome -ForegroundColor $fgColor -BackgroundColor &quot;Black&quot;
 Write-Host Total tests: $total
 Write-Host Passed: $passed
 Write-Host Failed: $failed
 Write-Host

 $failedTests | % { Write-Host Failed test: $_.testName
  Write-Host $_.Output.ErrorInfo.Message
  Write-Host $_.Output.ErrorInfo.StackTrace }

 Write-Host

 if($outcome -eq &quot;Failed&quot;) {
  Write-Host &quot;Failing build as there are broken tests&quot;
  $host.SetShouldExit(1)
 }
}

function Run() {
  Write-Host &quot;Running AfterBuild.ps1 using Configuration: $configuration, projectDir: $projectDir, targetDir: $targetDir, publishUrl: $publishUrl, buildNumber: $buildNumber, buildDefinitionName: $buildDefinitionName&quot;

 if($buildNumber) {
  $resultsFile = &quot;$projectDir\test-results.trx&quot;
  PublishTestResults $resultsFile
  FailBuildIfThereAreTestFailures $resultsFile
 }
}

# Off we go...
Run
</code></pre><p>Assuming we have a build number this script kicks off the <code>PublishTestResults</code> function above. So we won&#x27;t attempt to publish test results when compiling in Visual Studio on our dev machine. The script looks for <code>MSTest.exe</code> in a certain location on disk (the default VS 2015 installation location in fact; it may be installed elsewhere on your build machine). MSTest is then invoked and passed a file called <code>test-results.trx</code> which is is expected to live in the root of the project. All being well, the test results will be registered with the running build and will be visible when you look at test results in TFS.</p><p>Finally in <code>FailBuildIfThereAreTestFailures</code> we parse the <code>test-results.trx</code> file (and for this I&#x27;m totally in the debt of <a href="https://gist.github.com/davidroberts63/5655441">David Robert&#x27;s helpful Gist</a>). We write out the results to the host so it&#x27;ll show up in the MSBuild logs. Also, and this is crucial, if there are any failures we fail the build by exiting PowerShell with a code of 1. We are deliberately swallowing any error that Karma raises earlier when it detects failed tests. We do this because we want to publish the failed test results to TFS <em>before</em> we kill the build.</p><h2>Bonus Karma: <code>test-results.trx</code></h2><p>If you&#x27;ve read a <a href="https://blog.johnnyreilly.com/2016/02/visual-studio-tsconfigjson-and-external.html">previous post of mine</a> you&#x27;ll be aware that it&#x27;s possible to get MSBuild to kick off npm build tasks. Specifically I have MSBuild kicking off an <code>npm run build</code>. My <code>package.json</code> looks like this:</p><pre><code class="language-json">&quot;scripts&quot;: {
    &quot;test&quot;: &quot;karma start --reporters mocha,trx --single-run --browsers PhantomJS&quot;,
    &quot;clean&quot;: &quot;gulp delete-dist-contents&quot;,
    &quot;watch&quot;: &quot;gulp watch&quot;,
    &quot;build&quot;: &quot;gulp build&quot;,
    &quot;postbuild&quot;: &quot;npm run test&quot;
  },
</code></pre><p>You can see that the <code>postbuild</code> hook kicks off the <code>test</code> script in turn. And that <code>test</code> script kicks off a single run of karma. I&#x27;m not going to go over setting up Karma at all here; there are other posts out there that cover that admirably. But I wanted to share news of the <a href="https://www.npmjs.com/package/karma-trx-reporter">karma trx reporter</a>. This reporter is the thing that produces our <code>test-results.trx</code> file; trx being the format that TFS likes to deal with.</p><p>So now we&#x27;ve got a PowerShell hook into our build process (something very useful in itself) which we are using to publish Karma test results into TFS 2012. They said it couldn&#x27;t be done. They were wrong. Huzzah!!!!!!!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Creating Angular UI Routes in the Controller]]></title>
            <link>https://blog.johnnyreilly.com/2016/02/29/creating-angular-ui-routes-in-controller</link>
            <guid>Creating Angular UI Routes in the Controller</guid>
            <pubDate>Mon, 29 Feb 2016 00:00:00 GMT</pubDate>
            <description><![CDATA[So you're creating a link with the Angular UI Router. You're passing more than a few parameters and it's getting kinda big. Something like this:]]></description>
            <content:encoded><![CDATA[<p>So you&#x27;re creating a link with the Angular UI Router. You&#x27;re passing more than a few parameters and it&#x27;s getting kinda big. Something like this:</p><pre><code class="language-xml">&lt;a class=&quot;contains-icon&quot;
      ui-sref=&quot;Entity.Edit({ entityId: (vm.selectedEntityId ? vm.selectedEntityId: null), initialData: vm.initialData })&quot;&gt;
        &lt;i class=&quot;fa fa-pencil&quot;&gt;&lt;/i&gt;Edit
   &lt;/a&gt;
</code></pre><p>See? It&#x27;s too long to fit on the screen without wrapping. It&#x27;s clearly mad and bad.</p><p>Generally I try to keep the logic in a view to a minimum. It makes the view harder to read, it makes behaviour of the app harder to reason about. Also, it&#x27;s not testable and (if you&#x27;re using some kind of static typing like TypeScript) it is entirely out of the realms that a compiler can catch. So what to do? Move the URL generation to the controller. That&#x27;s what I decided to do after I had a typo in my view which I didn&#x27;t catch until post-commit.</p><h2><code>ui-sref</code> in the Controller</h2><p>Actually, that&#x27;s not exactly what you want to do. If you look at the <a href="http://angular-ui.github.io/ui-router/site/#/api/ui.router.state.directive:ui-sref">Angular UI Router docs</a> you will see that <code>ui-sref</code> is:</p><blockquote><p>...a directive that binds a link (<code>&amp;lt;a&amp;gt;</code> tag) to a state. If the state has an associated URL, the directive will automatically generate &amp; update the href attribute via the <a href="http://angular-ui.github.io/ui-router/site/#/api/ui.router.state.$state#methods_href"><code>$state.href()</code></a> method.</p></blockquote><p>So what we actually want to do is use the <code>$state.href()</code> method in our controller. To take our example above we&#x27;ll create another method on our controller called <code>getEditUrl</code></p><pre><code class="language-js">export class EntityController {
  $state: angular.ui.IStateService;

  static $inject = [&#x27;$state&#x27;];
  constructor($state: angular.ui.IStateService) {
    this.$state = $state;
  }

  //... Other stuff

  getEditUrl() {
    return this.$state.href(&#x27;Entity.Edit&#x27;, {
      selectedEntityId: this.selectedEntityId ? this.selectedEntityId : null,
      initialData: this.initialData,
    });
  }
}
</code></pre><p>You can see I&#x27;m using TypeScript here; but feel free to strip out the type annotations and go with raw ES6 classes; that&#x27;ll still give you testability if not static typing.</p><p>Now we&#x27;ve added the <code>getEditUrl</code> method we just need to reference it in our view:</p><pre><code class="language-xml">&lt;a class=&quot;contains-icon&quot; ng-href=&quot;{{vm.getEditUrl()}}&quot;&gt;&lt;i class=&quot;fa fa-pencil&quot;&gt;&lt;/i&gt;Edit&lt;/a&gt;
</code></pre><p>Note we&#x27;ve ditched usage of the <code>ui-sref</code> directive and gone with Angular&#x27;s native <code>&lt;a href=&quot;https://docs.angularjs.org/api/ng/directive/ngHref&quot;&gt;ng-href&lt;/a&gt;</code>. Within that directive we execute our <code>getEditUrl</code> as an expression which gives us our route. As a bonus, our view is much less cluttered and comprehensible as a result. How lovely.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Visual Studio, tsconfig.json and external TypeScript compilation]]></title>
            <link>https://blog.johnnyreilly.com/2016/02/19/visual-studio-tsconfigjson-and-external</link>
            <guid>Visual Studio, tsconfig.json and external TypeScript compilation</guid>
            <pubDate>Fri, 19 Feb 2016 00:00:00 GMT</pubDate>
            <description><![CDATA[TypeScript first gained support for tsconfig.json back with the 1\.5 release. However, to my lasting regret and surprise Visual Studio will not be gaining meaningful support for it until TypeScript 1.8 ships. However, if you want it now, it's already available to use in beta.]]></description>
            <content:encoded><![CDATA[<p>TypeScript first gained support for <a href="https://github.com/Microsoft/TypeScript/wiki/tsconfig.json"><code>tsconfig.json</code></a> back with the <a href="https://blogs.msdn.microsoft.com/typescript/2015/07/20/announcing-typescript-1-5/">1<!-- -->.<!-- -->5 release</a>. However, to my lasting regret and surprise Visual Studio will not be gaining meaningful support for it until <a href="https://github.com/Microsoft/TypeScript/wiki/What%27s-new-in-TypeScript#improved-support-for-tsconfigjson-in-visual-studio-2015">TypeScript 1.8</a> ships. However, if you want it now, it&#x27;s already available to use in <a href="https://blogs.msdn.microsoft.com/typescript/2016/01/28/announcing-typescript-1-8-beta/">beta</a>.</p><p>I&#x27;ve already leapt aboard. Whilst there&#x27;s a number of bugs in the beta it&#x27;s still totally usable. So use it.</p><h2>External TypeScript Compilation and the VS build</h2><p>Whilst <code>tsconfig.json</code> is useful and super cool it has limitations. It allows you to deactivate compilation upon file saving using <a href="https://github.com/Microsoft/TypeScript/issues/2326#issuecomment-178294169"><code>compileOnSave</code></a>. <a href="https://github.com/Microsoft/TypeScript/issues/7091">What it doesn&#x27;t allow is deactivation of the TypeScript compilation that happens as part of a Visual Studio build.</a> That may not matter for the vanilla workflow of just dropping TypeScript files in a Visual Studio web project and having VS invoke the TypeScript compilation. However it comes to matter when your workflow deviates from the norm, as mine does. Using external compilation of TypeScript within Visual Studio is a little tricky. My own use case is somewhat atypical but perhaps not uncommon.</p><p>I&#x27;m working on a project which has been built using TypeScript since TS 0.9. Not surprisingly, this started off using the default Visual Studio / TypeScript workflow. Active development on the project ceased around 9 months ago. Now it&#x27;s starting up again. It&#x27;s a reasonable sized web app and the existing functionality is, in the main, fine. But the users want to add some new screens.</p><p>Like any developer, I want to build with the latest and greatest. In my case, this means I want to write modular ES6 using TypeScript. With this approach my code can be leaner and I have less script ordering drama in my life. (Yay import statements!) This can be done by bringing together webpack, TypeScript (<a href="https://github.com/TypeStrong/ts-loader">ts-loader</a>) and <a href="http://babeljs.io/">Babel</a> (<a href="https://github.com/babel/babel-loader">babel-loader</a>). There&#x27;s an example of this approach <a href="https://blog.johnnyreilly.com/2015/12/es6-typescript-babel-react-flux-karma.html">here</a>. Given the size of the existing codebase I&#x27;d rather leave the legacy TypeScript as is and isolate my new approach to the screens I&#x27;m going to build. Obviously I&#x27;d like to have a common build process for all the codebase at some point but I&#x27;ve got a deadline to meet and so a half-old / half-new approach is called for (at least for the time being).</p><h2>Goodbye TypeScript Compilation in VS</h2><p>Writing modular ES6 TypeScript which is fully transpiled to old-school JS is <em>not possible</em> using the Visual Studio tooling at present. For what it&#x27;s worth I think that SystemJS compilation may make this more possible in the future but I don&#x27;t really know enough about it to be sure. That&#x27;s why I&#x27;m bringing webpack / Babel into the mix right now. I don&#x27;t want Visual Studio to do anything for the ES6 code; I don&#x27;t want it to compile. I want to deactivate my TypeScript compilation for the ES6 code. I can&#x27;t do this from the <code>tsconfig.json</code> so I&#x27;m in a bit of a hole. What to do?</p><p>Well, as of (I think) TypeScript 1.7 it&#x27;s possible to deactivate TypeScript compilation in Visual Studio. To <a href="https://github.com/Microsoft/TypeScript/issues/2294#issuecomment-129367578">quote</a>:</p><blockquote><p>there is an easier way to disable TypeScriptCompile:</p><p>Just add <code>&amp;lt;TypeScriptCompileBlocked&amp;gt;true&amp;lt;/TypeScriptCompileBlocked&amp;gt;</code> to the <code>.csproj</code>, e.g. in the first <code>&amp;lt;PropertyGroup&amp;gt;</code>.</p></blockquote><p>Awesomeness!</p><p>But wait, this means that the legacy TypeScript isn&#x27;t being compiled any longer. Bear in mind, I&#x27;m totally happy with the existing / legacy TypeScript compilation. Nooooooooooooooo!!!!!!!!!!!!!!!</p><h2>Hello TypeScript Compilation outside VS</h2><p>Have no fear, I gotcha. What we&#x27;re going to do is ensure that Visual Studio triggers 2 external TypeScript builds as part of its own build process:</p><ul><li>The modular ES6 TypeScript (new)</li><li>The legacy TypeScript (old)</li></ul><p>How do we do this? Through the magic of build targets. We need to add this to our <code>.csproj</code>: (I add it near the end; I&#x27;m not sure if location matters though)</p><pre><code class="language-xml">&lt;PropertyGroup&gt;
    &lt;CompileDependsOn&gt;
      $(CompileDependsOn);
      WebClientBuild;
    &lt;/CompileDependsOn&gt;
    &lt;CleanDependsOn&gt;
      $(CleanDependsOn);
      WebClientClean
    &lt;/CleanDependsOn&gt;
    &lt;CopyAllFilesToSingleFolderForPackageDependsOn&gt;
      CollectGulpOutput;
      CollectLegacyTypeScriptOutput;
      $(CopyAllFilesToSingleFolderForPackageDependsOn);
    &lt;/CopyAllFilesToSingleFolderForPackageDependsOn&gt;
    &lt;CopyAllFilesToSingleFolderForMsdeployDependsOn&gt;
      CollectGulpOutput;
      CollectLegacyTypeScriptOutput;
      $(CopyAllFilesToSingleFolderForPackageDependsOn);
    &lt;/CopyAllFilesToSingleFolderForMsdeployDependsOn&gt;
  &lt;/PropertyGroup&gt;
  &lt;Target Name=&quot;WebClientBuild&quot;&gt;
    &lt;Exec Command=&quot;npm install&quot; /&gt;
    &lt;Exec Command=&quot;npm run build-legacy-typescript&quot; /&gt;
    &lt;Exec Command=&quot;npm run build -- --mode $(ConfigurationName)&quot; /&gt;
  &lt;/Target&gt;
  &lt;Target Name=&quot;WebClientClean&quot;&gt;
    &lt;Exec Command=&quot;npm run clean&quot; /&gt;
  &lt;/Target&gt;
  &lt;Target Name=&quot;CollectGulpOutput&quot;&gt;
    &lt;ItemGroup&gt;
      &lt;_CustomFiles Include=&quot;dist\**\*&quot; /&gt;
      &lt;FilesForPackagingFromProject Include=&quot;%(_CustomFiles.Identity)&quot;&gt;
        &lt;DestinationRelativePath&gt;dist\%(RecursiveDir)%(Filename)%(Extension)&lt;/DestinationRelativePath&gt;
      &lt;/FilesForPackagingFromProject&gt;
    &lt;/ItemGroup&gt;
    &lt;Message Text=&quot;CollectGulpOutput list: %(_CustomFiles.Identity)&quot; /&gt;
  &lt;/Target&gt;
  &lt;Target Name=&quot;CollectLegacyTypeScriptOutput&quot;&gt;
    &lt;ItemGroup&gt;
      &lt;_CustomFiles Include=&quot;Scripts\**\*.js&quot; /&gt;
      &lt;FilesForPackagingFromProject Include=&quot;%(_CustomFiles.Identity)&quot;&gt;
        &lt;DestinationRelativePath&gt;Scripts\%(RecursiveDir)%(Filename)%(Extension)&lt;/DestinationRelativePath&gt;
      &lt;/FilesForPackagingFromProject&gt;
    &lt;/ItemGroup&gt;
    &lt;Message Text=&quot;CollectLegacyTypeScriptOutput list: %(_CustomFiles.Identity)&quot; /&gt;
  &lt;/Target&gt;
</code></pre><p>There&#x27;s a few things going on here; let&#x27;s take them one by one.</p><h2>The <code>WebClientBuild</code> Target</h2><p>This target triggers our external builds. One by one it runs the following commands:</p><dl><dt><code>npm install</code></dt><dd>Installs the npm packages.</dd><dt><code>npm run build-legacy-typescript</code></dt><dd>Runs the <code>&quot;build-legacy-typescript&quot;</code><code>script</code> in our <code>package.json</code></dd><dt><code>npm run build -- --mode $(ConfigurationName)</code></dt><dd>Runs the <code>&quot;build&quot;</code><code>script</code> in our <code>package.json</code> and passes through a <code>mode</code> parameter of either <code>&quot;Debug&quot;</code> or <code>&quot;Release&quot;</code> from MSBuild - indicating whether we&#x27;re creating a debug or a release build.</dd></dl><p>As you&#x27;ve no doubt gathered, I&#x27;m following the convention of using the <code>scripts</code> element of my <code>package.json</code> as repository for the various build tasks I might have for a web project. It looks like this:</p><pre><code class="language-json">{
  // ...
  &quot;scripts&quot;: {
    &quot;test&quot;: &quot;karma start --reporters mocha,junit --single-run --browsers PhantomJS&quot;,
    &quot;build-legacy-typescript&quot;: &quot;tsc -v&amp;&amp;tsc --project Scripts&quot;,
    &quot;clean&quot;: &quot;gulp delete-dist-contents&quot;,
    &quot;watch&quot;: &quot;gulp watch&quot;,
    &quot;build&quot;: &quot;gulp build&quot;
  }
  // ...
}
</code></pre><p>As you can see, <code>&quot;build-legacy-typescript&quot;</code> invokes <code>tsc</code> (which is registered as a <code>devDependency</code>) to print out the version of the compiler. Then it invokes <code>tsc</code> again using the <a href="https://github.com/Microsoft/TypeScript/wiki/Compiler-Options"><code>project</code></a> flag directly on the <code>Scripts</code> directory. This is where the legacy TypeScript and its associated <code>tsconfig.json</code> resides. Et voilá, the old / existing TypeScript is compiled just as it was previously by VS itself.</p><p>Next, the <code>&quot;build&quot;</code> invokes a <code>gulp</code> task called, descriptively, <code>&quot;build&quot;</code>. This task caters for our brand new codebase of modular ES6 TypeScript. When run, this task will invoke webpack, copy static files, build less etc. Quick digression: you can see there&#x27;s a <code>&quot;watch&quot;</code> script that does the same thing on a file-watching basis; I use that during development.</p><h2>The <code>WebClientClean</code> Target</h2><p>The task that runs to clean up artefacts created by <code>WebClientBuild</code>.</p><h2>The <code>CollectLegacyTypeScriptOutput</code> and <code>CollectGulpOutput</code> Targets</h2><p>Since we&#x27;re compiling our TypeScript outside of VS we need to tell MSBuild / MSDeploy about the externally compiled assets in order that they are included in the publish pipeline. Here I&#x27;m standing on the shoulders of <a href="http://www.codecadwallader.com/2015/03/15/integrating-gulp-into-your-tfs-builds-and-web-deploy/">Steve Cadwallader&#x27;s excellent post</a>. Thanks Steve!</p><p><code>CollectLegacyTypeScriptOutput</code> and <code>CollectGulpOutput</code> respectively include all the built files contained in the <code>&quot;Scripts&quot;</code> and <code>&quot;dist&quot;</code> folders when a publish takes place. You don&#x27;t need this for when you&#x27;re building on your own machine but if you&#x27;re looking to publish (either from your machine or from TFS) then you will need exactly this. Believe me that last sentence was typed with a memory of <em>great</em> pain and frustration.</p><p>So in the end, as far as TypeScript is concerned, I&#x27;m using Visual Studio solely as an editor. It&#x27;s the hooks in the <code>.csproj</code> that ensure that compilation happens. It seems a little quirky that we still need to have the original TypeScript targets in the <code>.csproj</code> file as well; but it works. That&#x27;s all that matters.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[TFS 2012, .NET 4.5 and C# 6]]></title>
            <link>https://blog.johnnyreilly.com/2016/02/01/tfs-2012-net-45-and-c-6</link>
            <guid>TFS 2012, .NET 4.5 and C# 6</guid>
            <pubDate>Mon, 01 Feb 2016 00:00:00 GMT</pubDate>
            <description><![CDATA[So, you want to use C# 6 language features and you’re working on an older project that’s still rocking .NET 4.5. Well, with some caveats, you can.]]></description>
            <content:encoded><![CDATA[<p>So, you want to use C# 6 language features and you’re working on an older project that’s still rocking .NET 4.5. Well, with <a href="http://stackoverflow.com/a/28921749/761388">some caveats</a>, you can.</p><p>The new compiler will compile targeting older framework versions. Well that’s all lovely; let’s all go home.</p><p>Now. What say you’ve got an old, old build server? It’s TFS 2012 Update 2, creaking away, still glad to alive and kind of wondering why it hasn’t been upgraded or retired. This is where you want to compile .NET 4.5 from C# 6. Well it can be done. Here’s how it’s done:</p><ol><li><p>Install Visual Studio 2015 on the build server (I’m told this can be achieved using <a href="https://www.microsoft.com/en-us/download/details.aspx?id=48159">Microsoft Build Tools 2015</a> but I haven’t tried it myelf so caveat emptor)</p></li><li><p>set the <code>MSBuild Arguments</code> in the build definition to <code>/p:VisualStudioVersion=14.0</code> (i.e. Visual Studio 2015 mode) <img src="../static/blog/2016-02-01-tfs-2012-net-45-and-c-6/EditBuildConfiguration.png"/></p></li><li><p>in each project that uses C# 6 syntax, install the NuGet package <a href="https://www.nuget.org/packages/Microsoft.Net.Compilers">Microsoft.Net.Compilers</a> with a quick <code>install-package Microsoft.Net.Compilers</code></p></li></ol><p>That’s it; huzzah! String interpolation here I come…</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Coded UI and the Curse of the Docking Station]]></title>
            <link>https://blog.johnnyreilly.com/2016/01/14/coded-ui-and-curse-of-docking-station</link>
            <guid>Coded UI and the Curse of the Docking Station</guid>
            <pubDate>Thu, 14 Jan 2016 00:00:00 GMT</pubDate>
            <description><![CDATA[I’ve a love / hate relationship with Coded UI. Well hate / hate might be more accurate. Hate perhaps married with a very grudging respect still underpinned by a wary bitterness. Yes, that’s about the size of it. Why? Well, when Coded UI works, it’s fab. But it’s flaky as anything. Anybody who’s used the technology is presently nodding sagely and holding back the tears. It’s all a bit... tough.]]></description>
            <content:encoded><![CDATA[<p>I’ve a love / hate relationship with Coded UI. Well hate / hate might be more accurate. Hate perhaps married with a very grudging respect still underpinned by a wary bitterness. Yes, that’s about the size of it. Why? Well, when Coded UI works, it’s fab. But it’s flaky as anything. Anybody who’s used the technology is presently nodding sagely and holding back the tears. It’s all a bit... tough.</p><p>I’ve recently discovered another quirk to add to the list. Docking stations. I was back working on a project which had a Coded UI test suite. I’d heard tell that there were problems with the tests and was just taking a look at them. The first hurdle I fell at was getting the tests to run locally. The tests had first been developed on a standard desktop build and, as much as this can ever be said of Coded UI tests, they worked. However, the future had happened. The company in question was no longer using the old school desktop towers. Nope, they’d reached for the sky and equipped the whole office with Surface Pro 3’s, hot desks, docking stations and big, big monitors. It looked terribly flash.</p><p>Coded UI was not happy.</p><p>The <code>Mouse.Click</code> behaviour wasn’t working. Most tests need the ability for users to click on buttons, dropdowns etc. That’s part of a normal UI. And so it was with these tests. This is where they fell over. The reason they fell over at this point didn’t become clear for a while. It wasn’t until we tried tweaking our implementation of the tests that we realised what was happening. The tests normally found buttons / dropdowns etc on the screen and then attempted to perform a <code>Mouse.Click</code> upon them. We changed the implementation to be subtly different. Instead of just clicking on the element we amended the test to move the mouse to the button and then perform the click.</p><p>Aha!</p><p>Rather than steadily moving towards an element and clicking, the pointer was swerving like a drunk man crossing the road at 3am. It completely missed the element it was aiming for and clicked upon a seemingly random area of the screen. This is Coded UI doing “pin the tail on the donkey”.</p><p>After more time than I&#x27;d like to admit I happened upon the solution. I tended to dock my Surface and then tune my monitor resolution to the one most optimal for coding. (ie really high res.) This is what messes with Coded UI&#x27;s head; the resolution change. If I wanted to be able to run tests successfully all I had to do was switch back to the resolution I initially booted with. Alternately I could restart my computer so it launched with the resolution I was presently using.</p><p>Once you do follow this guidance Coded UI has a moment of clarity, gets sober and starts <code>Mouse.Click</code>-<!-- -->ing like a pro.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[UseStaticFiles for ASP.Net Framework]]></title>
            <link>https://blog.johnnyreilly.com/2016/01/01/usestaticfiles-for-aspnet-vold</link>
            <guid>UseStaticFiles for ASP.Net Framework</guid>
            <pubDate>Fri, 01 Jan 2016 00:00:00 GMT</pubDate>
            <description><![CDATA[This is a guide on how not to expose all your static files to the world at large when working with the ASP.Net Framework. How to move from a blocklisting approach to a allowlisting approach.]]></description>
            <content:encoded><![CDATA[<p>This is a guide on how <em>not</em> to expose all your static files to the world at large when working with the ASP.Net Framework. How to move from a blocklisting approach to a allowlisting approach.</p><p>Not clear? Stick around; I&#x27;ll get better. Oh and that&#x27;s not all, we&#x27;ve also got.... drumroll:</p><h2>Support for <a href="https://html.spec.whatwg.org/multipage/browsers.html#the-history-interface">HTML5 History API</a>!</h2><p>What that means, in as close to English as I can get it, is real URLs for Single Page Applications. None of that hash-based routing malarkey. So, <code>https://i-am-your-domain.com/i-am-your-route</code> rather than <code>https://i-am-your-domain.com/&lt;em&gt;#/&lt;/em&gt;i-am-your-route</code>. (For a more in depth look at the different sorts of routing SPA&#x27;s can use then take a look at the <a href="http://rackt.org/history/stable/GettingStarted.html">excellent docs</a> by the folk behind <a href="https://github.com/rackt/react-router">React Router</a>. These concepts are not React specific and can be applied to any SPA technology.)</p><h2><code>UseStaticFiles</code></h2><p>You may be aware that historically ASP.Net has been somewhat unusual in its approach to serving static files. Essentially, all the files in a project are theoretically servable. Okay, that&#x27;s not entirely true; things like the <code>web.config</code> files etc are not going to be handed over to someone browsing your site. But other files that you might well want kept away from prying eyes may be. So your <a href="http://www.typescriptlang.org/">TypeScript</a> files, your <a href="http://lesscss.org/">Less</a> files are all up for grabs unless you take action to block access to them. This is, and has always been, bad.</p><p>The ASP.Net team know this and things are changing with ASP.Net 5. With the new stack you have to say &quot;these are the static files we want people to access&quot; in the form of an <code>&lt;a href=&quot;https://msdn.microsoft.com/en-us/library/dn782589(v=vs.113).aspx&quot;&gt;app.UseStaticFiles()&lt;/a&gt;</code> declaration. This is mighty similar to how you do things in other frameworks such as <a href="http://expressjs.com/en/starter/static-files.html">Express</a>. To quote the <a href="https://docs.asp.net/en/latest/fundamentals/static-files.html#serving-static-files">docs</a>:</p><blockquote><p>By default, static files are stored in the webroot of your project. The location of the webroot is defined in the project’s <code>project.json</code> file where the default is wwwroot.</p><pre><code class="language-json">&quot;webroot&quot;: &quot;wwwroot&quot;
</code></pre><p>Static files can be stored in any folder under the webroot and accessed with a relative path to that root. For example, when you create a default Web application project using Visual Studio, there are several folders created within the webroot folder - <code>css</code>, <code>images</code> and <code>js</code>. In order to directly access an image in the images subfolder, the URL would look like the following:</p><p><code>http://&amp;lt;yourApp&amp;gt;/images/&amp;lt;imageFileName&amp;gt;</code></p></blockquote><p>So how do we get this behaviour with ASP.Net vOld? Well, it&#x27;s just a matter of <code>web.config</code> URL rewrite twiddling:</p><pre><code class="language-xml">&lt;configuration&gt;
  &lt;!-- other config --&gt;

  &lt;system.webServer&gt;
    &lt;rewrite&gt;
      &lt;rules&gt;
        &lt;rule name=&quot;Map empty URLs to the index.html&quot;&gt;
          &lt;match url=&quot;^$&quot; /&gt;
          &lt;action type=&quot;Rewrite&quot; url=&quot;/index.html&quot; /&gt;
        &lt;/rule&gt;
        &lt;rule name=&quot;Map all requests with a &#x27;.&#x27; in to the &#x27;build&#x27; directory&quot; stopProcessing=&quot;true&quot;&gt;
          &lt;match url=&quot;^(.*[.].*)$&quot; /&gt;
          &lt;action type=&quot;Rewrite&quot; url=&quot;/build/{R:1}&quot; /&gt;
        &lt;/rule&gt;
      &lt;/rules&gt;
    &lt;/rewrite&gt;
  &lt;/system.webServer&gt;
&lt;/configuration&gt;
</code></pre><p>My <code>webroot</code> is named <code>build</code> rather than <code>wwwroot</code>. The 2 URL rewrite rules above do the following:</p><dl><dt>Map empty URLs to the index.html</dt><dd>Empty URLs (ie the URL for the root of your site) are mapped to <code>index.html</code>. The <code>index.html</code> in the <code>build</code> folder is the home page of this particular site and the next rule will make sure that we hit that. (Since we haven&#x27;t set <code>stopProcessing</code> to <code>true</code> for this particular rule the next rule will be processed after this one.)</dd><dt>Map all requests with a &#x27;.&#x27; in to the &#x27;build&#x27; directory</dt><dd>All URLs with a &quot;.&quot; in the title (including <code>index.html</code>) are redirected to the <code>build</code> folder. All static files have a &quot;.&quot; in them because filenames have suffixes. This essentially means all requests for files are served from the <code>build</code> folder. In this case we have set <code>stopProcessing</code> to <code>true</code> which means that any URLs that matched this rule will be not be affected by any subsequent rules.</dd></dl><p>So if anyone sneakily tries to sneakily browse to say, <code>http://&amp;lt;yourApp&amp;gt;/js/app.ts</code> then they&#x27;ll be nicely redirected to the non-existent <code>build/js/app.ts</code>. 404 in your face!</p><h2>&quot;I am SPArtucus&quot;</h2><p>When you have a Single Page Application you want the same web experience as a server side rendered web app. What I mean by this is: routing just works. You want people to be able to go to <code>https://i-am-your-domain.com/i-am-your-route</code> and get your site at the specified route. Happily, whether you&#x27;re using React Router, Angular UI Router or something else, the client side is covered. They can be configured to detect the route that you enter at and serve up the SPA in the relevant state. But you have to meet them halfway; the server needs to do its bit.</p><p>When a URL is requested that doesn&#x27;t look like a request for a static file, let&#x27;s make the (reasonable) assumption that this is a route URL and serve up the shell SPA page. So, for my own example of an Angular 1.x app I want the server to hand over <code>/build/index.html</code>.</p><p>This is the magic that makes real URLs and SPAs work. Provided the client hasn&#x27;t requested a static file, every request to the server will be responded to with our very own &quot;I am SPArtucus&quot;; the shell SPA page. This is catered for by the addition of another new rule to our <code>web.config</code>:</p><pre><code class="language-xml">&lt;configuration&gt;
  &lt;!-- other config --&gt;

  &lt;system.webServer&gt;
    &lt;rewrite&gt;
      &lt;rules&gt;
        &lt;rule name=&quot;Map empty URLs to the index.html&quot;&gt;
          &lt;match url=&quot;^$&quot; /&gt;
          &lt;action type=&quot;Rewrite&quot; url=&quot;/index.html&quot; /&gt;
        &lt;/rule&gt;
        &lt;rule name=&quot;Map all requests with a &#x27;.&#x27; in to the &#x27;build&#x27; directory&quot; stopProcessing=&quot;true&quot;&gt;
          &lt;match url=&quot;^(.*[.].*)$&quot; /&gt;
          &lt;action type=&quot;Rewrite&quot; url=&quot;/build/{R:1}&quot; /&gt;
        &lt;/rule&gt;
        &lt;rule name=&quot;Map all other URLs to the index.html - this to support our SPA routes&quot;&gt;
          &lt;match url=&quot;^.*$&quot; /&gt;
          &lt;action type=&quot;Rewrite&quot; url=&quot;/build/index.html&quot; /&gt;
        &lt;/rule&gt;
      &lt;/rules&gt;
    &lt;/rewrite&gt;
  &lt;/system.webServer&gt;
&lt;/configuration&gt;
</code></pre><dl><dt>Map all other URLs to the index.html - this to support our SPA routes</dt><dd>Our new rule says &quot;whatever URL turns up, if it hasn&#x27;t been catered for by an existing rule, well it must be a SPA route, so we&#x27;ll serve up the shell SPA page of <code>build/index.html</code>&quot;.</dd></dl><h2>Data! Data! Data!.. I can&#x27;t make bricks without clay.</h2><p>Sherlock Holmes was onto something; most applications are nothing without data. What you&#x27;ve got at present is an application that carefully restricts access to static files and, for all other requests, serves up our shell SPA page. So let&#x27;s relax our final rule a little to make data access a thing:</p><pre><code class="language-xml">&lt;configuration&gt;
  &lt;!-- other config --&gt;

  &lt;system.webServer&gt;
    &lt;rewrite&gt;
      &lt;rules&gt;
        &lt;rule name=&quot;Map empty URLs to the index.html&quot;&gt;
          &lt;match url=&quot;^$&quot; /&gt;
          &lt;action type=&quot;Rewrite&quot; url=&quot;/index.html&quot; /&gt;
        &lt;/rule&gt;
        &lt;rule name=&quot;Map all requests with a &#x27;.&#x27; in to the &#x27;build&#x27; directory&quot; stopProcessing=&quot;true&quot;&gt;
          &lt;match url=&quot;^(.*[.].*)$&quot; /&gt;
          &lt;action type=&quot;Rewrite&quot; url=&quot;/build/{R:1}&quot; /&gt;
        &lt;/rule&gt;
        &lt;rule name=&quot;Map non-api URLs to the index.html - this to support our SPA routes&quot;&gt;
          &lt;match url=&quot;^(api/).*$&quot; negate=&quot;true&quot; ignoreCase=&quot;true&quot; /&gt;
          &lt;action type=&quot;Rewrite&quot; url=&quot;/build/index.html&quot; /&gt;
        &lt;/rule&gt;
      &lt;/rules&gt;
    &lt;/rewrite&gt;
  &lt;/system.webServer&gt;
&lt;/configuration&gt;
</code></pre><dl><dt>Map non-api URLs to the index.html - this to support our SPA routes</dt><dd>This amended rule says &quot;whatever URL turns up, <em>unless it begins <code>&quot;api/&quot;</code></em>, if it hasn&#x27;t been catered for by an existing rule, well it must be a SPA route, so we&#x27;ll serve up the shell SPA page of <code>build/index.html</code>&quot;.</dd></dl><p>This allows us to perform data access by prefixing all the Web API routes with <code>&quot;api/&quot;</code>. I&#x27;ve picked this because it is the default location for ASP.Net Web API routes. It is (like most things) entirely configurable. To see a working implementation of this complete approach then take a look at the PoorClaresAngular project <a href="https://github.com/johnnyreilly/poorclaresarundel/tree/15e7d4ddc0f1c06fe326b44c3bdc71ceb554bf73">here</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Live Reload Considered Harmful]]></title>
            <link>https://blog.johnnyreilly.com/2015/12/20/live-reload-considered-harmful</link>
            <guid>Live Reload Considered Harmful</guid>
            <pubDate>Sun, 20 Dec 2015 00:00:00 GMT</pubDate>
            <description><![CDATA[I've seen it go by many names; live reload, hot reload, browser sync... the list goes on. It's been the subject of a million demos. It's the focus of a thousand npm packages. Someone tweaks a file and... wait for it... doesn't have to refresh their browser to see the changes... The future is now!]]></description>
            <content:encoded><![CDATA[<p>I&#x27;ve seen it go by many names; <a href="http://livereload.com/">live reload</a>, hot reload, <a href="https://browsersync.io/">browser sync</a>... the list goes on. It&#x27;s been the subject of a million demos. It&#x27;s the focus of a thousand npm packages. Someone tweaks a file and... wait for it... <em>doesn&#x27;t have to refresh their browser to see the changes</em>... The future is now!</p><p>Forgive me the sarcasm, but I have come to the conclusion that whilst live reload is impressive... for my own purposes, it is not actually that useful. It certainly shouldn&#x27;t be the default goto that it seems to have become.</p><p>Hear me out people, I may be the voice crying out in the wilderness but I&#x27;m right dammit.</p><p><img src="../static/blog/2015-12-20-live-reload-considered-harmful/tumblr_mxjpcobvcg...6_r2_250-4abb938.gif"/></p><h2>Why is Live Reload a Thing?</h2><p>What is live reload? Well having to hit F5 after you&#x27;ve made a change... That seems like such hard work right? To quote <a href="http://haacked.com/archive/2011/12/13/better-git-with-powershell.aspx/">Phil Haack</a>:</p><blockquote><p>... we’re software developers.... It’s time to AWW TOE MATE!</p></blockquote><p>Yup, automation. Anything that a developer can theoretically automate.... will be automated. Usually this is a good thing but automation can be addictive. And on this occasion it&#x27;s time for an intervention.</p><p>What else could be the attraction? Well, this is speculation but I would say that the implementation actually has something to do with it. Live reload is almost invariably powered by <a href="https://en.wikipedia.org/wiki/WebSocket">WebSockets</a> and they are certainly cool. Developers I know what you are like. You&#x27;re attracted by the new shiny thing. You can&#x27;t resist the allure of WS. And there with live reload idling away in the background you&#x27;re all bleeding edge. I can say all this because this is exactly what I am like.</p><h2>Why is Live Reload a BAD Thing?</h2><p>Well the OCD part of me is instinctively repelled by the extra <code>script</code> tag of alien code that live reload foists upon your app. How very dare that <code>&amp;lt;script src=&quot;http://localhost:35729/livereload.js?snipver=1&quot;&amp;gt;&amp;lt;/script&amp;gt;</code> push its way into my pristine DOM. It&#x27;s an outrage.</p><p>Perhaps a more convincing rationale is how useful it is to have 2 different versions of your app up on screen at the same time. I like to try things out when I&#x27;m working. I get a screen working one way and then I tweak and play with my implementation. I have the app of 10 minutes ago sat side by side with the newly adjusted one. Assess, compare and and declare a winner. That&#x27;s so useful and live reload does away with it. That&#x27;s a problem.</p><p>Finally, I&#x27;m an obsessive &#x27;Ctrl-S&#x27;-er. I&#x27;ve been burned by unsaved changes too many times. I&#x27;m saving every couple of keypresses. With live reload that usually means I have the noise of a dead application in the corner of my eye as LR obsessively forces the latest brokenness upon me. That sucks.</p><p>I&#x27;ve no doubt there are situations where live reload is useful. But for my money that&#x27;s the exception rather than the rule. Let the madness end now. Just say &quot;no&quot;, kids.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ES6 + TypeScript + Babel + React + Flux + Karma: The Secret Recipe]]></title>
            <link>https://blog.johnnyreilly.com/2015/12/16/es6-typescript-babel-react-flux-karma</link>
            <guid>ES6 + TypeScript + Babel + React + Flux + Karma: The Secret Recipe</guid>
            <pubDate>Wed, 16 Dec 2015 00:00:00 GMT</pubDate>
            <description><![CDATA[I wrote a while ago about how I was using some different tools in a current project:]]></description>
            <content:encoded><![CDATA[<p>I wrote <a href="https://blog.johnnyreilly.com/2015/09/things-done-changed.html">a while ago</a> about how I was using some different tools in a current project:</p><ul><li>React with JSX</li><li>Flux</li><li>ES6 with Babel</li><li>Karma for unit testing</li></ul><p>I have fully come to love and appreciate all of the above. I really like working with them. However. There was still an ache in my soul and a thorn in my side. Whilst I love the syntax of ES6 and even though I&#x27;ve come to appreciate the clarity of JSX, I have been missing something. Perhaps you can guess? It&#x27;s static typing.</p><p>It&#x27;s actually been really good to have the chance to work without it because it&#x27;s made me realise what a productivity boost having static typing actually is. The number of silly mistakes burning time that a compiler could have told me.... Sigh.</p><p>But the pain is over. The dark days are gone. It&#x27;s possible to have strong typing, courtesy of TypeScript, plugged into this workflow. It&#x27;s yours for the taking. Take it. Take it now!</p><h2>What a Guy Wants</h2><p>I decided a couple of months ago what I wanted to have in my setup:</p><ol><li>I want to be able to write React / JSX in TypeScript. Naturally I couldn&#x27;t achieve that by myself but handily the TypeScript team decided to add support for JSX with <a href="https://blogs.msdn.com/b/typescript/archive/2015/09/16/announcing-typescript-1-6.aspx">TypeScript 1.6</a>. Ooh yeah.</li><li>I wanted to be able to write ES6. When I realised <a href="https://github.com/Microsoft/TypeScript/issues/3956">the approach for writing ES6 and having the transpilation handled by TypeScript wasn&#x27;t clear</a> I had another idea. I thought <a href="https://github.com/Microsoft/TypeScript/issues/4765">&quot;what if I write ES6 and hand off the transpilation to Babel?&quot;</a> i.e. Use TypeScript for type checking, not for transpilation. I realised that <a href="http://www.jbrantly.com/es6-modules-with-typescript-and-webpack/#configuringwebpack">James Brantly had my back</a> here already. Enter <a href="https://webpack.github.io/">Webpack</a> and <a href="https://github.com/TypeStrong/ts-loader">ts-loader</a>.</li><li>Debugging. Being able to debug my code is non-negotiable for me. If I can&#x27;t debug it I&#x27;m less productive. (I&#x27;m also bitter and twisted inside.) I should say that I wanted to be able to debug my <em>original</em> source code. Thanks to the magic of <a href="https://docs.google.com/document/d/1U1RGAehQwRypUTovF1KRlpiOFze0b-_2gc6fAH0KY0k/edit?usp=sharing">sourcemaps</a>, that mad thing is possible.</li><li>Karma for unit testing. I&#x27;ve become accustomed to writing my tests in ES6 and running them on a continual basis with <a href="https://karma-runner.github.io/0.13/index.html">Karma</a>. This allows for a rather good debugging story as well. I didn&#x27;t want to lose this when I moved to TypeScript. I didn&#x27;t.</li></ol><p>So I&#x27;ve talked about what I want and I&#x27;ve alluded to some of the solutions that there are. The question now is how to bring them all together. This post is, for the most part, going to be about correctly orchestrating a number of <a href="http://gulpjs.com/">gulp tasks</a> to achieve the goals listed above. If you&#x27;re after the <a href="https://en.wikipedia.org/wiki/Blue_Peter">Blue Peter &quot;here&#x27;s one I made earlier&quot;</a> moment then take a look at <a href="https://github.com/Microsoft/TypeScriptSamples/tree/master/es6-babel-react-flux-karma">the es6-babel-react-flux-karma repo</a> in the <a href="https://github.com/Microsoft/TypeScriptSamples">Microsoft/TypeScriptSamples repo on Github</a>.</p><h2>gulpfile.js</h2><pre><code class="language-js">/* eslint-disable no-var, strict, prefer-arrow-callback */
&#x27;use strict&#x27;;

var gulp = require(&#x27;gulp&#x27;);
var gutil = require(&#x27;gulp-util&#x27;);
var connect = require(&#x27;gulp-connect&#x27;);
var eslint = require(&#x27;gulp-eslint&#x27;);
var webpack = require(&#x27;./gulp/webpack&#x27;);
var staticFiles = require(&#x27;./gulp/staticFiles&#x27;);
var tests = require(&#x27;./gulp/tests&#x27;);
var clean = require(&#x27;./gulp/clean&#x27;);
var inject = require(&#x27;./gulp/inject&#x27;);

var lintSrcs = [&#x27;./gulp/**/*.js&#x27;];

gulp.task(&#x27;delete-dist&#x27;, function (done) {
  clean.run(done);
});

gulp.task(&#x27;build-process.env.NODE_ENV&#x27;, function () {
  process.env.NODE_ENV = &#x27;production&#x27;;
});

gulp.task(
  &#x27;build-js&#x27;,
  [&#x27;delete-dist&#x27;, &#x27;build-process.env.NODE_ENV&#x27;],
  function (done) {
    webpack.build().then(function () {
      done();
    });
  }
);

gulp.task(
  &#x27;build-other&#x27;,
  [&#x27;delete-dist&#x27;, &#x27;build-process.env.NODE_ENV&#x27;],
  function () {
    staticFiles.build();
  }
);

gulp.task(&#x27;build&#x27;, [&#x27;build-js&#x27;, &#x27;build-other&#x27;, &#x27;lint&#x27;], function () {
  inject.build();
});

gulp.task(&#x27;lint&#x27;, function () {
  return gulp.src(lintSrcs).pipe(eslint()).pipe(eslint.format());
});

gulp.task(&#x27;watch&#x27;, [&#x27;delete-dist&#x27;], function () {
  process.env.NODE_ENV = &#x27;development&#x27;;
  Promise.all([
    webpack.watch(), //,
    //less.watch()
  ])
    .then(function () {
      gutil.log(
        &#x27;Now that initial assets (js and css) are generated inject will start...&#x27;
      );
      inject.watch(postInjectCb);
    })
    .catch(function (error) {
      gutil.log(&#x27;Problem generating initial assets (js and css)&#x27;, error);
    });

  gulp.watch(lintSrcs, [&#x27;lint&#x27;]);
  staticFiles.watch();
  tests.watch();
});

gulp.task(&#x27;watch-and-serve&#x27;, [&#x27;watch&#x27;], function () {
  postInjectCb = stopAndStartServer;
});

var postInjectCb = null;
var serverStarted = false;
function stopAndStartServer() {
  if (serverStarted) {
    gutil.log(&#x27;Stopping server&#x27;);
    connect.serverClose();
    serverStarted = false;
  }
  startServer();
}

function startServer() {
  gutil.log(&#x27;Starting server&#x27;);
  connect.server({
    root: &#x27;./dist&#x27;,
    port: 8080,
  });
  serverStarted = true;
}
</code></pre><p>Let&#x27;s start picking this apart; what do we actually have here? Well, we have 2 gulp tasks that I want you to notice:</p><dl><dt>build</dt><dd><p>This is likely the task you would use when deploying. It takes all of your source code, builds it, provides cache-busting filenames (eg <code>main.dd2fa20cd9eac9d1fb2f.js</code>), injects your shell SPA page with references to the files and deploys everything to the <code>./dist/</code> directory. So that&#x27;s TypeScript, static assets like images and CSS all made ready for Production.</p><p>The build task also implements <a href="https://facebook.github.io/react/blog/2015/09/10/react-v0.14-rc1.html">this advice</a>:</p><blockquote cite="https://facebook.github.io/react/blog/2015/09/10/react-v0.14-rc1.html">When deploying your app, set the <code>NODE_ENV</code> environment variable to <code>production</code> to use the production build of React which does not include the development warnings and runs significantly faster. </blockquote></dd><dt>watch-and-serve</dt><dd><p>This task represents &quot;development mode&quot; or &quot;debug mode&quot;. It&#x27;s what you&#x27;ll likely be running as you develop your app. It does the same as the build task but with some important distinctions.</p><ul><li>As well as building your source it also runs your tests using Karma</li><li>This task is not triggered on a once-only basis, rather your files are watched and each tweak of a file will result in a new build and a fresh run of your tests. Nice eh?</li><li>It spins up a simple web server and serves up the contents of <code>./dist</code> (i.e. your built code) in order that you can easily test out your app.</li><li>In addition, whilst it builds your source it does <em>not</em> minify your code and it emits sourcemaps. For why? For debugging! You can go to <code><a href="http://localhost:8080/">http://localhost:8080/</a></code> in your browser of choice, fire up the dev tools and you&#x27;re off to the races; debugging like gangbusters. It also doesn&#x27;t bother to provide cache-busting filenames as Chrome dev tools are smart enough to not cache localhost.</li><li>Oh and Karma.... If you&#x27;ve got problems with a failing test then head to <code><a href="http://localhost:9876/">http://localhost:9876/</a></code> and you can debug the tests in your dev tools.</li><li>Finally, it runs ESLint in the console. Not all of my files are TypeScript; essentially the build process (aka &quot;gulp-y&quot;) files are all vanilla JS. So they&#x27;re easily breakable. ESLint is there to provide a little reassurance on that front.</li></ul></dd></dl><p>Now let&#x27;s dig into each of these in a little more detail</p><h2>WebPack</h2><p>Let&#x27;s take a look at what&#x27;s happening under the covers of <code>webpack.build()</code> and <code>webpack.watch()</code>.</p><p><a href="https://github.com/webpack/webpack">WebPack</a> with <a href="https://github.com/TypeStrong/ts-loader">ts-loader</a> and <a href="https://github.com/babel/babel-loader">babel-loader</a> is what we&#x27;re using to compile our ES6 TypeScript. ts-loader uses the TypeScript compiler to, um, compile TypeScript and emit ES6 code. This is then passed on to the babel-loader which transpiles it from ES6 down to ES-old-school. It all gets brought together in 2 files; <code>main.js</code> which contains the compiled result of the code written by us and <code>vendor.js</code> which contains the compiled result of 3rd party / vendor files. The reason for this separation is that vendor files are likely to change fairly rarely whilst our own code will constantly be changing. This separation allows for quicker compile times upon file changes as, for the most part, the vendor files will not need to included in this process.</p><p>Our <code>gulpfile.js</code> above uses the following task:</p><pre><code class="language-js">&#x27;use strict&#x27;;

var gulp = require(&#x27;gulp&#x27;);
var gutil = require(&#x27;gulp-util&#x27;);
var webpack = require(&#x27;webpack&#x27;);
var WebpackNotifierPlugin = require(&#x27;webpack-notifier&#x27;);
var webpackConfig = require(&#x27;../webpack.config.js&#x27;);

function buildProduction(done) {
  // modify some webpack config options
  var myProdConfig = Object.create(webpackConfig);
  myProdConfig.output.filename = &#x27;[name].[hash].js&#x27;;

  myProdConfig.plugins = myProdConfig.plugins.concat(
    // make the vendor.js file with cachebusting filename
    new webpack.optimize.CommonsChunkPlugin({
      name: &#x27;vendor&#x27;,
      filename: &#x27;vendor.[hash].js&#x27;,
    }),
    new webpack.optimize.DedupePlugin(),
    new webpack.optimize.UglifyJsPlugin()
  );

  // run webpack
  webpack(myProdConfig, function (err, stats) {
    if (err) {
      throw new gutil.PluginError(&#x27;webpack:build&#x27;, err);
    }
    gutil.log(
      &#x27;[webpack:build]&#x27;,
      stats.toString({
        colors: true,
      })
    );

    if (done) {
      done();
    }
  });
}

function createDevCompiler() {
  // show me some sourcemap love people
  var myDevConfig = Object.create(webpackConfig);
  myDevConfig.devtool = &#x27;inline-source-map&#x27;;
  myDevConfig.debug = true;

  myDevConfig.plugins = myDevConfig.plugins.concat(
    // Make the vendor.js file
    new webpack.optimize.CommonsChunkPlugin({
      name: &#x27;vendor&#x27;,
      filename: &#x27;vendor.js&#x27;,
    }),
    new WebpackNotifierPlugin({ title: &#x27;Webpack build&#x27;, excludeWarnings: true })
  );

  // create a single instance of the compiler to allow caching
  return webpack(myDevConfig);
}

function buildDevelopment(done, devCompiler) {
  // run webpack
  devCompiler.run(function (err, stats) {
    if (err) {
      throw new gutil.PluginError(&#x27;webpack:build-dev&#x27;, err);
    }
    gutil.log(
      &#x27;[webpack:build-dev]&#x27;,
      stats.toString({
        chunks: false, // dial down the output from webpack (it can be noisy)
        colors: true,
      })
    );

    if (done) {
      done();
    }
  });
}

function bundle(options) {
  var devCompiler;

  function build(done) {
    if (options.shouldWatch) {
      buildDevelopment(done, devCompiler);
    } else {
      buildProduction(done);
    }
  }

  if (options.shouldWatch) {
    devCompiler = createDevCompiler();

    gulp.watch(&#x27;src/**/*&#x27;, function () {
      build();
    });
  }

  return new Promise(function (resolve, reject) {
    build(function (err) {
      if (err) {
        reject(err);
      } else {
        resolve(&#x27;webpack built&#x27;);
      }
    });
  });
}

module.exports = {
  build: function () {
    return bundle({ shouldWatch: false });
  },
  watch: function () {
    return bundle({ shouldWatch: true });
  },
};
</code></pre><p>Hopefully this is fairly self-explanatory; essentially <code>buildDevelopment</code> performs the development build (providing sourcemap support) and <code>buildProduction</code> builds for Production (providing minification support). Both are driven by this <code>webpack.config.js</code>:</p><pre><code class="language-js">/* eslint-disable no-var, strict, prefer-arrow-callback */
&#x27;use strict&#x27;;

var path = require(&#x27;path&#x27;);

module.exports = {
  cache: true,
  entry: {
    // The entry point of our application; the script that imports all other scripts in our SPA
    main: &#x27;./src/main.tsx&#x27;,

    // The packages that are to be included in vendor.js
    vendor: [&#x27;babel-polyfill&#x27;, &#x27;events&#x27;, &#x27;flux&#x27;, &#x27;react&#x27;],
  },

  // Where the output of our compilation ends up
  output: {
    path: path.resolve(__dirname, &#x27;./dist/scripts&#x27;),
    filename: &#x27;[name].js&#x27;,
    chunkFilename: &#x27;[chunkhash].js&#x27;,
  },

  module: {
    loaders: [
      {
        // The loader that handles ts and tsx files.  These are compiled
        // with the ts-loader and the output is then passed through to the
        // babel-loader.  The babel-loader uses the es2015 and react presets
        // in order that jsx and es6 are processed.
        test: /\.ts(x?)$/,
        exclude: /node_modules/,
        loader: &#x27;babel-loader?presets[]=es2015&amp;presets[]=react!ts-loader&#x27;,
      },
      {
        // The loader that handles any js files presented alone.
        // It passes these to the babel-loader which (again) uses the es2015
        // and react presets.
        test: /\.js$/,
        exclude: /node_modules/,
        loader: &#x27;babel&#x27;,
        query: {
          presets: [&#x27;es2015&#x27;, &#x27;react&#x27;],
        },
      },
    ],
  },
  plugins: [],
  resolve: {
    // Files with the following extensions are fair game for webpack to process
    extensions: [&#x27;&#x27;, &#x27;.webpack.js&#x27;, &#x27;.web.js&#x27;, &#x27;.ts&#x27;, &#x27;.tsx&#x27;, &#x27;.js&#x27;],
  },
};
</code></pre><h2>Inject</h2><p>Your compiled output needs to be referenced from some kind of HTML page. So we&#x27;ve got this:</p><pre><code class="language-html">&lt;!DOCTYPE html&gt;
&lt;html lang=&quot;en&quot;&gt;
  &lt;head&gt;
    &lt;meta charset=&quot;utf-8&quot; /&gt;
    &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot; /&gt;
    &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot; /&gt;

    &lt;title&gt;ES6 + Babel + React + Flux + Karma: The Secret Recipe&lt;/title&gt;

    &lt;!-- inject:css --&gt;
    &lt;!-- endinject --&gt;
    &lt;link
      rel=&quot;stylesheet&quot;
      href=&quot;https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css&quot;
    /&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;div id=&quot;content&quot;&gt;&lt;/div&gt;
    &lt;!-- inject:js --&gt;
    &lt;!-- endinject --&gt;
  &lt;/body&gt;
&lt;/html&gt;
</code></pre><p>Which is no more than a boilerplate HTML page with a couple of key features:</p><ul><li>a single <code>&amp;lt;div /&amp;gt;</code> element in the <code>&amp;lt;body /&amp;gt;</code> which is where our React app is going to be rendered.</li><li><code>&amp;lt;!-- inject:css --&amp;gt;</code> and <code>&amp;lt;!-- inject:js --&amp;gt;</code> placeholders where css and js is going to be injected by <code>gulp-inject</code>.</li><li>a single <code>&amp;lt;link /&amp;gt;</code> to the Bootstrap CDN. This sample app doesn&#x27;t actually serve up any css generated as part of the project. It could but it doesn&#x27;t. When it comes to injection time no css will actually be injected. This has been left in place as, more typically, a project would have some styling served up.</li></ul><p>This is fed into our inject task in <code>inject.build()</code> and <code>inject.watch()</code>. They take css and javascript and, using our shell template, create a new page which has the css and javascript dropped into their respective placeholders:</p><pre><code class="language-js">&#x27;use strict&#x27;;

var gulp = require(&#x27;gulp&#x27;);
var inject = require(&#x27;gulp-inject&#x27;);
var glob = require(&#x27;glob&#x27;);

function injectIndex(options) {
  var postInjectCb = options.postInjectCb;
  var postInjectCbTriggerId = null;
  function run() {
    var target = gulp.src(&#x27;./src/index.html&#x27;);
    var sources = gulp.src(
      [
        //&#x27;./dist/styles/main*.css&#x27;,
        &#x27;./dist/scripts/vendor*.js&#x27;,
        &#x27;./dist/scripts/main*.js&#x27;,
      ],
      { read: false }
    );

    return target
      .on(&#x27;end&#x27;, function () {
        // invoke postInjectCb after 1s
        if (postInjectCbTriggerId || !postInjectCb) {
          return;
        }

        postInjectCbTriggerId = setTimeout(function () {
          postInjectCb();
          postInjectCbTriggerId = null;
        }, 1000);
      })
      .pipe(
        inject(sources, {
          ignorePath: &#x27;/dist/&#x27;,
          addRootSlash: false,
          removeTags: true,
        })
      )
      .pipe(gulp.dest(&#x27;./dist&#x27;));
  }

  var jsCssGlob = &#x27;dist/**/*.{js,css}&#x27;;

  function checkForInitialFilesThenRun() {
    glob(jsCssGlob, function (er, files) {
      var filesWeNeed = [
        &#x27;dist/scripts/main&#x27;,
        &#x27;dist/scripts/vendor&#x27; /*, &#x27;dist/styles/main&#x27;*/,
      ];

      function fileIsPresent(fileWeNeed) {
        return files.some(function (file) {
          return file.indexOf(fileWeNeed) !== -1;
        });
      }

      if (filesWeNeed.every(fileIsPresent)) {
        run(&#x27;initial build&#x27;);
      } else {
        checkForInitialFilesThenRun();
      }
    });
  }

  checkForInitialFilesThenRun();

  if (options.shouldWatch) {
    gulp.watch(jsCssGlob, function (evt) {
      if (evt.path &amp;&amp; evt.type === &#x27;changed&#x27;) {
        run(evt.path);
      }
    });
  }
}

module.exports = {
  build: function () {
    return injectIndex({ shouldWatch: false });
  },
  watch: function (postInjectCb) {
    return injectIndex({ shouldWatch: true, postInjectCb: postInjectCb });
  },
};
</code></pre><p>This also triggers the server to serve up the new content.</p><h2>Static Files</h2><p>Your app will likely rely on a number of static assets; images, fonts and whatnot. This script picks up the static assets you&#x27;ve defined and places them in the <code>dist</code> folder ready for use:</p><pre><code class="language-js">&#x27;use strict&#x27;;

var gulp = require(&#x27;gulp&#x27;);
var cache = require(&#x27;gulp-cached&#x27;);

var targets = [
  // In my own example I don&#x27;t use any of the targets below, they
  // are included to give you more of a feel of how you might use this
  { description: &#x27;FONTS&#x27;, src: &#x27;./fonts/*&#x27;, dest: &#x27;./dist/fonts&#x27; },
  { description: &#x27;STYLES&#x27;, src: &#x27;./styles/*&#x27;, dest: &#x27;./dist/styles&#x27; },
  { description: &#x27;FAVICON&#x27;, src: &#x27;./favicon.ico&#x27;, dest: &#x27;./dist&#x27; },
  { description: &#x27;IMAGES&#x27;, src: &#x27;./images/*&#x27;, dest: &#x27;./dist/images&#x27; },
];

function copy(options) {
  // Copy files from their source to their destination
  function run(target) {
    gulp
      .src(target.src)
      .pipe(cache(target.description))
      .pipe(gulp.dest(target.dest));
  }

  function watch(target) {
    gulp.watch(target.src, function () {
      run(target);
    });
  }

  targets.forEach(run);

  if (options.shouldWatch) {
    targets.forEach(watch);
  }
}

module.exports = {
  build: function () {
    return copy({ shouldWatch: false });
  },
  watch: function () {
    return copy({ shouldWatch: true });
  },
};
</code></pre><h2>Karma</h2><p>Finally, we&#x27;re ready to get our tests set up to run continually with Karma. <code>tests.watch()</code> triggers the following task:</p><pre><code class="language-js">&#x27;use strict&#x27;;

var Server = require(&#x27;karma&#x27;).Server;
var path = require(&#x27;path&#x27;);
var gutil = require(&#x27;gulp-util&#x27;);

module.exports = {
  watch: function () {
    // Documentation: https://karma-runner.github.io/0.13/dev/public-api.html
    var karmaConfig = {
      configFile: path.join(__dirname, &#x27;../karma.conf.js&#x27;),
      singleRun: false,

      plugins: [
        &#x27;karma-webpack&#x27;,
        &#x27;karma-jasmine&#x27;,
        &#x27;karma-mocha-reporter&#x27;,
        &#x27;karma-sourcemap-loader&#x27;,
        &#x27;karma-phantomjs-launcher&#x27;,
        &#x27;karma-phantomjs-shim&#x27;,
      ], // karma-phantomjs-shim only in place until PhantomJS hits 2.0 and has function.bind
      reporters: [&#x27;mocha&#x27;],
    };

    new Server(karmaConfig, karmaCompleted).start();

    function karmaCompleted(exitCode) {
      gutil.log(&#x27;Karma has exited with:&#x27;, exitCode);
      process.exit(exitCode);
    }
  },
};
</code></pre><p>When running in watch mode it&#x27;s possible to debug the tests by going to: <code>&lt;a href=&quot;http://localhost:9876/&quot;&gt;http://localhost:9876/&lt;/a&gt;</code>. It&#x27;s also possible to run the tests standalone with a simple <code>npm run test</code>. Running them like this also outputs the results to an <a href="http://stackoverflow.com/q/442556/761388">XML file in JUnit format</a>; this can be useful for integrating into CI solutions that don&#x27;t natively pick up test results.</p><p>Whichever approach we use for running tests, we use the following <code>karma.conf.js</code> file to configure Karma:</p><pre><code class="language-js">/* eslint-disable no-var, strict */
&#x27;use strict&#x27;;

var webpackConfig = require(&#x27;./webpack.config.js&#x27;);

module.exports = function (config) {
  // Documentation: https://karma-runner.github.io/0.13/config/configuration-file.html
  config.set({
    browsers: [&#x27;PhantomJS&#x27;],

    files: [
      &#x27;test/import-babel-polyfill.js&#x27;, // This ensures we have the es6 shims in place from babel
      &#x27;test/**/*.tests.ts&#x27;,
      &#x27;test/**/*.tests.tsx&#x27;,
    ],

    port: 9876,

    frameworks: [&#x27;jasmine&#x27;, &#x27;phantomjs-shim&#x27;],

    logLevel: config.LOG_INFO, //config.LOG_DEBUG

    preprocessors: {
      &#x27;test/import-babel-polyfill.js&#x27;: [&#x27;webpack&#x27;, &#x27;sourcemap&#x27;],
      &#x27;src/**/*.{ts,tsx}&#x27;: [&#x27;webpack&#x27;, &#x27;sourcemap&#x27;],
      &#x27;test/**/*.tests.{ts,tsx}&#x27;: [&#x27;webpack&#x27;, &#x27;sourcemap&#x27;],
    },

    webpack: {
      devtool: &#x27;eval-source-map&#x27;, //&#x27;inline-source-map&#x27;, - inline-source-map doesn&#x27;t work at present
      debug: true,
      module: webpackConfig.module,
      resolve: webpackConfig.resolve,
    },

    webpackMiddleware: {
      quiet: true,
      stats: {
        colors: true,
      },
    },

    // reporter options
    mochaReporter: {
      colors: {
        success: &#x27;bgGreen&#x27;,
        info: &#x27;cyan&#x27;,
        warning: &#x27;bgBlue&#x27;,
        error: &#x27;bgRed&#x27;,
      },
    },

    junitReporter: {
      outputDir: &#x27;test-results&#x27;, // results will be saved as $outputDir/$browserName.xml
      outputFile: undefined, // if included, results will be saved as $outputDir/$browserName/$outputFile
      suite: &#x27;&#x27;,
    },
  });
};
</code></pre><p>As you can see, we&#x27;re still using our webpack configuration from earlier to configure much of how the transpilation takes place.</p><p>And that&#x27;s it; we have a workflow for developing in TypeScript using React with tests running in an automated fashion. I appreciated this has been a rather long blog post but I hope I&#x27;ve clarified somewhat how this all plugs together and works. Do leave a comment if you think I&#x27;ve missed something.</p><h2>Babel 5 -&gt; Babel 6</h2><p>This post has actually been sat waiting to be published for some time. I&#x27;d got this solution up and running with Babel 5. Then they shipped Babel 6 and (as is the way with &quot;breaking changes&quot;) <a href="https://phabricator.babeljs.io/T2864">broke sourcemap support</a> and thus torpedoed this workflow. Happily that&#x27;s now <a href="https://github.com/babel/babel/pull/3108">been resolved</a>. But if you should experience any wonkiness - it&#x27;s worth checking that you&#x27;re using the latest and greatest of Babel 6.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[IQueryable... IEnumerable... Hmmm...]]></title>
            <link>https://blog.johnnyreilly.com/2015/11/30/iqueryable-ienumerable-hmmm</link>
            <guid>IQueryable... IEnumerable... Hmmm...</guid>
            <pubDate>Mon, 30 Nov 2015 00:00:00 GMT</pubDate>
            <description><![CDATA[So there I was, tip-tapping away at my keyboard when I became aware of the slowly loudening noise of a debate. It wasn't about poverty, war, civil rights or anything like that. No; this was far more contentious. It was about the behaviour of IQueryable&lt;T&gt; when mixed with IEnumerable&lt;T&gt;. I know, right, how could I not get involved?]]></description>
            <content:encoded><![CDATA[<p>So there I was, tip-tapping away at my keyboard when I became aware of the slowly loudening noise of a debate. It wasn&#x27;t about poverty, war, civil rights or anything like that. No; this was far more contentious. It was about the behaviour of <code>&lt;a href=&quot;https://msdn.microsoft.com/en-gb/library/bb351562(v=vs.100).aspx&quot;&gt;IQueryable&amp;lt;T&amp;gt;&lt;/a&gt;</code> when mixed with <code>&lt;a href=&quot;https://msdn.microsoft.com/en-gb/library/9eekhta0(v=vs.100).aspx&quot;&gt;IEnumerable&amp;lt;T&amp;gt;&lt;/a&gt;</code>. I know, right, how could I not get involved?</p><p>The code that was being debated was a database query that was being facilitated by Entity Framework. Now let me ask you a question: what is the problem with the methods below?</p><pre><code class="language-cs">private IEnumerable&lt;Sage&gt; GetSagesWithSayings()
{
    IQueryable&lt;Sage&gt; sageWithSayings =
        from s in DbContext.Sages.Include(x =&gt; x.Sayings)
        select s;

    return sageWithSayings;
}

public IEnumerable&lt;Sage&gt; GetSagesWithSayingsBornWithinTheLast100Years()
{
    var aHundredYearsAgo = DateTime.Now.AddYears(-100);
    var sageWithSayings = GetSagesWithSayings().Where(x =&gt; x.DateOfBirth &gt; aHundredYearsAgo);

    return sageWithSayings;
}
</code></pre><p>I&#x27;ve rather emphasised the problem by expressly declaring types in the <code>GetSagesWithSayings</code> method. More typically the <code>IQueryable&amp;lt;Sage&amp;gt;</code> would be hiding itself beneath a <code>var</code> making the problem less obvious. But you get the point; it&#x27;s something to do with an <code>IQueryable&amp;lt;Sage&amp;gt;</code> being passed back as an <code>IEnumerable&amp;lt;Sage&amp;gt;</code>.</p><p>The debate was raging around what this piece of code (or one much like it) actually did. One side positing &quot;it&#x27;ll get every record from the database and then throw away what it doesn&#x27;t need in C#-land...&quot; The opposing view being &quot;are you sure about that? Doesn&#x27;t it just get the records from the last hundred years from the database?&quot;</p><p>So it comes down the SQL that ends up being generated. On the one hand it&#x27;s going to get everything from the Sages table...</p><pre><code class="language-sql">select ...
from Sages ...
</code></pre><p>Or does it include a filter clause as well?</p><pre><code class="language-sql">select ...
from Sages ...
where DateOfBirth &gt; &#x27;1915-11-30&#x27;
</code></pre><p>You probably know the answer... It gets everything. Every record is brought back from the database and those that are older than 100 years are then casually thrown away. So kinda wasteful. That&#x27;s the problem. But why? And what does that tell us?</p><h2>LINQ to Objects vs LINQ to ... ?</h2><blockquote><p>The term &quot;LINQ to Objects&quot; refers to the use of LINQ queries with any <code>IEnumerable</code> or <code>IEnumerable&amp;lt;T&amp;gt;</code> collection directly, without the use of an intermediate LINQ provider or API such as LINQ to SQL or LINQ to XML.</p></blockquote><blockquote><p>The <code>&lt;a href=&quot;https://msdn.microsoft.com/en-gb/library/bb351562(v=vs.100).aspx&quot;&gt;IQueryable&amp;lt;T&amp;gt;&lt;/a&gt;</code> interface is intended for implementation by query providers.</p><p>This interface inherits the <code>&lt;a href=&quot;https://msdn.microsoft.com/en-gb/library/9eekhta0(v=vs.100).aspx&quot;&gt;IEnumerable&amp;lt;T&amp;gt;&lt;/a&gt;</code> interface so that if it represents a query, the results of that query can be enumerated. Enumeration forces the expression tree associated with an <code>&lt;a href=&quot;https://msdn.microsoft.com/en-gb/library/bb351562(v=vs.100).aspx&quot;&gt;IQueryable&amp;lt;T&amp;gt;&lt;/a&gt;</code> object to be executed. Queries that do not return enumerable results are executed when the <code>&lt;a href=&quot;https://msdn.microsoft.com/en-gb/library/bb549414(v=vs.100).aspx&quot;&gt;Execute&amp;lt;TResult&amp;gt;(Expression)&lt;/a&gt;</code> method is called.</p><p>The definition of &quot;executing an expression tree&quot; is specific to a query provider. For example, it may involve translating the expression tree to a query language appropriate for an underlying data source.</p></blockquote><p>I know - check me out with my &quot;quotes&quot;.</p><p>Now, <code>IEnumerable</code> and <code>IQueryable</code> are similar; for instance they are both considered &quot;lazy&quot; as they offer deferred execution. But there is an important difference between <code>IEnumerable</code> and <code>IQueryable</code>; namely that <code>IQueryable</code> hands off information about a query to another provider in order that they may decide how to do the necessary work. <code>IEnumerable</code> does not; its work is done in memory by operating on the data it has.</p><p>So let&#x27;s apply this to our issue. We have an <code>IQueryable&amp;lt;Sage&amp;gt;</code> and we return it as an <code>IEnumerable&amp;lt;Sage&amp;gt;</code>. By doing this we haven&#x27;t changed the underlying type; it&#x27;s still an <code>IQueryable&amp;lt;Sage&amp;gt;</code>. But by upcasting to <code>IEnumerable&amp;lt;Sage&amp;gt;</code> we have told the compiler that we don&#x27;t have an <code>IQueryable&amp;lt;Sage&amp;gt;</code>. We&#x27;ve lied. I trust you&#x27;re feeling guilty.</p><p>No doubt whoever raised you told you not to tell lies. This was probably the very situation they had in mind. The implications of our dirty little fib come back to haunt us when we start to chain on subsequent filters. So when we perform our filter of <code>.Where(x =&amp;gt; x.DateOfBirth &amp;gt; aHundredYearsAgo)</code> the compiler isn&#x27;t going to get LINQ to Entities&#x27;s extension methods in on this. No, it&#x27;s going to get the LINQ to object extension methods instead.</p><p>This is the cause of our problem. When it comes to execution we&#x27;re not getting the database to do the heavy lifting because we&#x27;ve moved away from using <code>IQueryable</code>.</p><h2>Fixing the Problem</h2><p>There are 2 courses of action open to you. The obvious course of action (and 99% of the time what you&#x27;d look to do) is change the signature of the `` method to return an IQueryable like so:</p><pre><code class="language-cs">private IQueryable&lt;Sage&gt; GetSagesWithSayings()
    var sageWithSayings = // I prefer &#x27;var&#x27;, don&#x27;t you?
        from s in DbContext.Sages.Include(x =&gt; x.Sayings)
        select s;

    return sageWithSayings;
}
</code></pre><p>The other alternative is what I like to think of as &quot;the escape hatch&quot;: <code>&lt;a href=&quot;https://msdn.microsoft.com/en-gb/library/bb353734(v=vs.100).aspx&quot;&gt;AsQueryable&lt;/a&gt;</code>. This takes an <code>IEnumerable</code>, checks if it&#x27;s actually an <code>IQueryable</code> slumming it and casts back to that if it is. You might use this in a situation where you didn&#x27;t have control over the data access code. Using it looks like this: (and would work whether <code>GetSagesWithSayings</code> was returning <code>IEnumerable</code><em>or</em><code>IQueryable</code>)</p><pre><code class="language-cs">public IEnumerable&lt;Sage&gt; GetSagesWithSayingsBornWithinTheLast100Years()
{
    var aHundredYearsAgo = DateTime.Now.AddYears(-100);
    var sageWithSayings =GetSagesWithSayings().AsQueryable().Where(x =&gt; x.DateOfBirth &gt; aHundredYearsAgo);

    return sageWithSayings;
}
</code></pre>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Names Have Been Changed...]]></title>
            <link>https://blog.johnnyreilly.com/2015/10/23/the-names-have-been-changed</link>
            <guid>The Names Have Been Changed...</guid>
            <pubDate>Fri, 23 Oct 2015 00:00:00 GMT</pubDate>
            <description><![CDATA[...to protect my wallet.]]></description>
            <content:encoded><![CDATA[<p>...to protect my wallet.</p><p>Subsequent to this blog getting <a href="http://blog.johnnyreilly.com/2014/12/whats-in-a-name.html">a proper domain name a year ago</a> it&#x27;s now got a new one. That&#x27;s right, <code>blog.icanmakethiswork.io</code> is dead! Long live <code>blog.johnnyreilly.com</code>!</p><p>There&#x27;s nothing particularly exciting about this, it&#x27;s more that <code>.io</code> domain names are <em>wayyyyy</em> expensive. And also I noticed that johnnyreilly.com was available. By an accident of history I&#x27;ve ended up either being johnny_reilly or johnnyreilly online. (&quot;<a href="mailto:johnreilly@hotmail.com">johnreilly@hotmail.com</a>&quot; was already taken back in 2000 and &quot;johnny<!-- -->_<a href="mailto:reilly@hotmail.com">reilly@hotmail.com</a>&quot; was available. I&#x27;ve subsequently become <a href="https://twitter.com/johnny_reilly">@johnny_reilly</a> on Twitter, <a href="https://github.com/johnnyreilly">johnnyreilly</a> on GitHub so I guess you could say it&#x27;s stuck.)</p><p>So I thought I&#x27;d kill 2 birds with one stone and make the switch. I&#x27;ve set up a redirect on <a href="http://blog.icanmakethiswork.io">blog.icanmakethiswork.io</a> and so, anyone who goes to the old site should be 301&#x27;d over here. At least until my old domain name expires. Last time it&#x27;ll change I promise. Well.... until next time anyway...</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[jQuery Validation Globalize hits 1.0]]></title>
            <link>https://blog.johnnyreilly.com/2015/10/05/jquery-validation-globalize-hits-10</link>
            <guid>jQuery Validation Globalize hits 1.0</guid>
            <pubDate>Mon, 05 Oct 2015 00:00:00 GMT</pubDate>
            <description><![CDATA[This is just a quick post - the tl;dr is this: jQuery Validation Globalize has been ported to Globalize 1.x. Yay! In one of those twists of fate I'm not actually using this plugin in my day job anymore but I thought it might be useful to other people. So here you go. You can read more about this plugin in an older post and you can see a demo of it in action here.]]></description>
            <content:encoded><![CDATA[<p>This is just a quick post - the tl;dr is this: jQuery Validation Globalize has been ported to Globalize 1.x. Yay! In one of those twists of fate I&#x27;m not actually using this plugin in my day job anymore but I thought it might be useful to other people. So here you go. You can read more about this plugin in an <a href="https://blog.johnnyreilly.com/2012/09/globalize-and-jquery-validate.html">older post</a> and you can see a demo of it in action <a href="http://johnnyreilly.github.io/jQuery.Validation.Unobtrusive.Native/AdvancedDemo/Globalize.html">here</a>.</p><p>The code did not change drastically - essentially it was just a question of swapping <code>parseFloat</code> for <code>parseNumber</code> and <code>parseDate</code> for a slightly different <code>parseDate</code>. So, we went from this:</p><pre><code class="language-js">(function ($, Globalize) {
  // Clone original methods we want to call into
  var originalMethods = {
    min: $.validator.methods.min,
    max: $.validator.methods.max,
    range: $.validator.methods.range,
  };

  // Tell the validator that we want numbers parsed using Globalize

  $.validator.methods.number = function (value, element) {
    var val = Globalize.parseFloat(value);
    return this.optional(element) || $.isNumeric(val);
  };

  // Tell the validator that we want dates parsed using Globalize

  $.validator.methods.date = function (value, element) {
    var val = Globalize.parseDate(value);
    return this.optional(element) || val instanceof Date;
  };

  // Tell the validator that we want numbers parsed using Globalize,
  // then call into original implementation with parsed value

  $.validator.methods.min = function (value, element, param) {
    var val = Globalize.parseFloat(value);
    return originalMethods.min.call(this, val, element, param);
  };

  $.validator.methods.max = function (value, element, param) {
    var val = Globalize.parseFloat(value);
    return originalMethods.max.call(this, val, element, param);
  };

  $.validator.methods.range = function (value, element, param) {
    var val = Globalize.parseFloat(value);
    return originalMethods.range.call(this, val, element, param);
  };
})(jQuery, Globalize);
</code></pre><p>To this:</p><pre><code class="language-js">(function ($, Globalize) {
  // Clone original methods we want to call into
  var originalMethods = {
    min: $.validator.methods.min,
    max: $.validator.methods.max,
    range: $.validator.methods.range,
  };

  // Globalize options - initially just the date format used for parsing
  // Users can customise this to suit them
  $.validator.methods.dateGlobalizeOptions = {
    dateParseFormat: { skeleton: &#x27;yMd&#x27; },
  };

  // Tell the validator that we want numbers parsed using Globalize
  $.validator.methods.number = function (value, element) {
    var val = Globalize.parseNumber(value);
    return this.optional(element) || $.isNumeric(val);
  };

  // Tell the validator that we want dates parsed using Globalize
  $.validator.methods.date = function (value, element) {
    var val = Globalize.parseDate(
      value,
      $.validator.methods.dateGlobalizeOptions.dateParseFormat
    );
    return this.optional(element) || val instanceof Date;
  };

  // Tell the validator that we want numbers parsed using Globalize,
  // then call into original implementation with parsed value

  $.validator.methods.min = function (value, element, param) {
    var val = Globalize.parseNumber(value);
    return originalMethods.min.call(this, val, element, param);
  };

  $.validator.methods.max = function (value, element, param) {
    var val = Globalize.parseNumber(value);
    return originalMethods.max.call(this, val, element, param);
  };

  $.validator.methods.range = function (value, element, param) {
    var val = Globalize.parseNumber(value);
    return originalMethods.range.call(this, val, element, param);
  };
})(jQuery, Globalize);
</code></pre><p>All of which is pretty self-explanatory. The only thing I&#x27;d like to draw out is that Globalize 0.1.x didn&#x27;t force you to specify a date parsing format and, as I recall, would attempt various methods of parsing. For that reason jQuery Validation Globalize 1.0 exposes a <code>$.validator.methods.dateGlobalizeOptions</code> which allows you to specify the data parsing format you want to use. This means, should you be using a different format than the out of the box one then you can tweak it like so:</p><pre><code class="language-js">$.validator.methods.dateGlobalizeOptions.dateParseFormat = // your data parsing format goes here...
</code></pre><p>Theoretically, this functionality could be tweaked to allow the user to specify multiple possible date parsing formats to attempt. I&#x27;m not certain if that&#x27;s a good idea though, so it remains unimplemented for now.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Definitely Typed Shouldn't Exist]]></title>
            <link>https://blog.johnnyreilly.com/2015/09/23/authoring-npm-modules-with-typescript</link>
            <guid>Definitely Typed Shouldn't Exist</guid>
            <pubDate>Wed, 23 Sep 2015 00:00:00 GMT</pubDate>
            <description><![CDATA[OK - the title's total clickbait but stay with me; there's a point here.]]></description>
            <content:encoded><![CDATA[<p>OK - the title&#x27;s total clickbait but stay with me; there&#x27;s a point here.</p><p>I&#x27;m a member of the Definitely Typed team - and hopefully I won&#x27;t be kicked out for writing this. My point is this: <code>.d.ts</code> files should live with the package they provide typing information for, in npm / GitHub etc. Not separately. TypeScript 1.6 has just been released. Yay! In the <a href="https://blogs.msdn.com/b/typescript/archive/2015/09/16/announcing-typescript-1-6.aspx">release blog post</a> it says this:</p><blockquote><p>We’ve changed module resolution when doing CommonJS output to work more closely to how Node does module resolution. If a module name is non-relative, we now follow these steps to find the associated typings:</p><ol><li>Check in <code>node_modules</code> for <code>&amp;lt;module name&amp;gt;.d.ts</code></li><li>Search <code>node_modules\&amp;lt;module name&amp;gt;\package.json</code> for a <code>typings</code> field</li><li>Look for <code>node_modules\&amp;lt;module name&amp;gt;\index.d.ts</code></li><li>Then we go one level higher and repeat the process</li></ol><p><strong>Please note:</strong> when we search through node_modules, we assume these are the packaged node modules which have type information and a corresponding <code>.js</code> file. As such, we resolve only <code>.d.ts</code> files (not <code>.ts</code> file) for non-relative names.</p><p>Previously, we treated all module names as relative paths, and therefore we would never properly look in node_modules... We will continue to improve module resolution, including improvements to AMD, in upcoming releases.</p></blockquote><p>The TL;DR is this: consuming npm packages which come with definition files should JUST WORK™... npm is now a first class citizen in TypeScriptLand. So everyone who has a package on npm should now feel duty bound to include a <code>.d.ts</code> when they publish and Definitely Typed can shut up shop. Simple right?</p><h2>Wrong!</h2><p>Yeah, it&#x27;s never going to happen. Surprising as it is, there are many people who are quite happy without TypeScript in their lives (I know - mad right?). These poor unfortunates are unlikely to ever take the extra steps necessary to write definition files. For this reason, there will probably <em>always</em> be a need for a provider of typings such as Definitely Typed. As well as that, the vast majority of people using TypeScript probably don&#x27;t use npm to manage dependencies. There are, however, an increasing number of users who are using npm. Some (like me) may even be using tools like <a href="http://browserify.org/">Browserify</a> (with the <a href="https://github.com/smrq/tsify">TSIFY plugin</a>) or <a href="https://webpack.github.io/">WebPack</a> (with the <a href="https://github.com/jbrantly/ts-loader">TS loader</a>) to bring it all together. My feeling is that, over time, using npm will become more common; particularly given the improvements being made to module resolution in the language.</p><p>An advantage of shipping typings with an npm package is this: those typings should accurately describe their accompanying package. In Definitely Typed we only aim to support the latest and greatest typings. So if you find yourself looking for the typings of an older version of a package you&#x27;re going to have to pick your way back through the history of a <code>.d.ts</code> file and hope you happen upon the version you&#x27;re looking for. Not a fantastic experience.</p><p>So I guess what I&#x27;m saying is this: if you&#x27;re an npm package author then it would be fantastic to start shipping a package with typings in the box. If you&#x27;re using npm to consume packages then using Definitely Typed ought to be the second step you might take after installing a package; the step you only need to take if the package doesn&#x27;t come with typings. Using DT should be a fallback, not a default.</p><h2>Authoring npm modules with TypeScript</h2><p>Yup - that&#x27;s what this post is actually about. See how I lured you in with my mild trolling and pulled the old switcheroo? That&#x27;s edutainment my friend. So, how do we write npm packages in TypeScript and publish them with their typings? Apparently Gandhi <a href="http://www.nytimes.com/2011/08/30/opinion/falser-words-were-never-spoken.html?_r=0">didn&#x27;t actually say</a> &quot;Be the change you wish to see in the world.&quot; Which is a shame. But anyway, I&#x27;m going to try and embrace the sentiment here.</p><p>Not so long ago I wrote a small npm module called <a href="https://www.npmjs.com/package/globalize-so-what-cha-want">globalize-so-what-cha-want</a>. It is used to determine what parts of Globalize 1.x you need depending on the modules you&#x27;re planning to use. It also, contains a little demo UI / online tool written in React which powers <a href="http://johnnyreilly.github.io/globalize-so-what-cha-want/">this</a>.</p><p>For this post, the purpose of the package is rather irrelevant. And even though I&#x27;ve just told you about it, I want you to pretend that the online tool doesn&#x27;t exist. Pretend I never mentioned it.</p><p>What is relevant, and what I want you to think about, is this: I wrote globalize-so-what-cha-want in plain old, honest to goodness JavaScript. Old school.</p><p><a href="https://www.youtube.com/watch?v=V4YPFHyGWaY&amp;feature=youtu.be&amp;t=49s">But, my love of static typing could be held in abeyance for only so long.</a> Once the initial package was written, unit tested and published I got the itch. THIS SHOULD BE WRITTEN IN TYPESCRIPT!!! Well, it didn&#x27;t have to be but I wanted it to be. Despite having used TypeScript since the early days I&#x27;d only been using it for front end work; not for writing npm packages. My mission was clear: port globalize-so-what-cha-want to TypeScript and re-publish to npm.</p><h2>Port, port, port!!!</h2><p>At this point globalize-so-what-cha-want consisted of a single <code>index.js</code> file in the root of the package. My end goal was to end up with that file still sat there, but now generated from TypeScript. Alongside it I wanted to see a <code>index.d.ts</code> which was generated from the same TypeScript.</p><p><code>index.js</code><a href="https://github.com/johnnyreilly/globalize-so-what-cha-want/tree/6cce84289134a555fe8462247b43eddb051303e3">before</a> looked like this:</p><pre><code class="language-js">/* jshint varstmt: false, esnext: false */
var DEPENDENCY_TYPES = {
  SHARED_JSON: &#x27;Shared JSON (used by all locales)&#x27;,
  LOCALE_JSON: &#x27;Locale specific JSON (supplied for each locale)&#x27;,
};

var moduleDependencies = {
  core: {
    dependsUpon: [],
    cldrGlobalizeFiles: [
      &#x27;cldr.js&#x27;,
      &#x27;cldr/event.js&#x27;,
      &#x27;cldr/supplemental.js&#x27;,
      &#x27;globalize.js&#x27;,
    ],
    json: [
      {
        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,
        dependency: &#x27;cldr/supplemental/likelySubtags.json&#x27;,
      },
    ],
  },

  currency: {
    dependsUpon: [&#x27;number&#x27;, &#x27;plural&#x27;],
    cldrGlobalizeFiles: [&#x27;globalize/currency.js&#x27;],
    json: [
      {
        dependencyType: DEPENDENCY_TYPES.LOCALE_JSON,
        dependency: &#x27;cldr/main/{locale}/currencies.json&#x27;,
      },
      {
        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,
        dependency: &#x27;cldr/supplemental/currencyData.json&#x27;,
      },
    ],
  },

  date: {
    dependsUpon: [&#x27;number&#x27;],
    cldrGlobalizeFiles: [&#x27;globalize/date.js&#x27;],
    json: [
      {
        dependencyType: DEPENDENCY_TYPES.LOCALE_JSON,
        dependency: &#x27;cldr/main/{locale}/ca-gregorian.json&#x27;,
      },
      {
        dependencyType: DEPENDENCY_TYPES.LOCALE_JSON,
        dependency: &#x27;cldr/main/{locale}/timeZoneNames.json&#x27;,
      },
      {
        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,
        dependency: &#x27;cldr/supplemental/timeData.json&#x27;,
      },
      {
        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,
        dependency: &#x27;cldr/supplemental/weekData.json&#x27;,
      },
    ],
  },

  message: {
    dependsUpon: [&#x27;core&#x27;],
    cldrGlobalizeFiles: [&#x27;globalize/message.js&#x27;],
    json: [],
  },

  number: {
    dependsUpon: [&#x27;core&#x27;],
    cldrGlobalizeFiles: [&#x27;globalize/number.js&#x27;],
    json: [
      {
        dependencyType: DEPENDENCY_TYPES.LOCALE_JSON,
        dependency: &#x27;cldr/main/{locale}/numbers.json&#x27;,
      },
      {
        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,
        dependency: &#x27;cldr/supplemental/numberingSystems.json&#x27;,
      },
    ],
  },

  plural: {
    dependsUpon: [&#x27;core&#x27;],
    cldrGlobalizeFiles: [&#x27;globalize/plural.js&#x27;],
    json: [
      {
        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,
        dependency: &#x27;cldr/supplemental/plurals.json&#x27;,
      },
      {
        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,
        dependency: &#x27;cldr/supplemental/ordinals.json&#x27;,
      },
    ],
  },

  relativeTime: {
    dependsUpon: [&#x27;number&#x27;, &#x27;plural&#x27;],
    cldrGlobalizeFiles: [&#x27;globalize/relative-time.js&#x27;],
    json: [
      {
        dependencyType: DEPENDENCY_TYPES.LOCALE_JSON,
        dependency: &#x27;cldr/main/{locale}/dateFields.json&#x27;,
      },
    ],
  },
};

function determineRequiredCldrData(globalizeOptions) {
  return determineRequired(
    globalizeOptions,
    _populateDependencyCurrier(&#x27;json&#x27;, function (json) {
      return json.dependency;
    })
  );
}

function determineRequiredCldrGlobalizeFiles(globalizeOptions) {
  return determineRequired(
    globalizeOptions,
    _populateDependencyCurrier(
      &#x27;cldrGlobalizeFiles&#x27;,
      function (cldrGlobalizeFile) {
        return cldrGlobalizeFile;
      }
    )
  );
}

function determineRequired(globalizeOptions, populateDependencies) {
  var modules = Object.keys(globalizeOptions);
  modules.forEach(function (module) {
    if (!moduleDependencies[module]) {
      throw new TypeError(&quot;There is no &#x27;&quot; + module + &quot;&#x27; module&quot;);
    }
  });

  var requireds = [];
  modules.forEach(function (module) {
    if (globalizeOptions[module]) {
      populateDependencies(module, requireds);
    }
  });

  return requireds;
}

function _populateDependencyCurrier(requiredArray, requiredArrayGetter) {
  var popDepFn = function (module, requireds) {
    var dependencies = moduleDependencies[module];

    dependencies.dependsUpon.forEach(function (requiredModule) {
      popDepFn(requiredModule, requireds);
    });

    dependencies[requiredArray].forEach(function (required) {
      var newRequired = requiredArrayGetter(required);
      if (requireds.indexOf(newRequired) === -1) {
        requireds.push(newRequired);
      }
    });

    return requireds;
  };

  return popDepFn;
}

module.exports = {
  determineRequiredCldrData: determineRequiredCldrData,
  determineRequiredCldrGlobalizeFiles: determineRequiredCldrGlobalizeFiles,
};
</code></pre><p>You can even kind of tell that it was written in JavaScript thanks to the jshint rules at the top.</p><p>I fired up Atom and created a new folder <code>src/lib</code> and inside there I created <code>index.ts</code> (yes, <code>index.js</code> renamed) and <code>tsconfig.json</code>. By the way, you&#x27;ll notice I&#x27;m not leaving Atom - I&#x27;m making use of the magnificent <a href="https://atom.io/packages/atom-typescript">atom-typescript</a> which you should totally be using too. It rocks.</p><p><img src="../static/blog/2015-09-23-authoring-npm-modules-with-typescript/Screenshot%2B2015-09-23%2B05.51.14.png"/></p><p>Now I&#x27;m not going to bore you with what I had to do to port the JS to TS (not much). If you&#x27;re interested, the source is <a href="https://github.com/johnnyreilly/globalize-so-what-cha-want/blob/master/src/lib/index.ts">here</a>. What&#x27;s more interesting is the <code>tsconfig.json</code> <!-- -->-<!-- --> as it&#x27;s this that is going to lead the generation of the JS and TS that we need:</p><pre><code class="language-json">{
  &quot;compileOnSave&quot;: true,
  &quot;compilerOptions&quot;: {
    &quot;module&quot;: &quot;commonjs&quot;,
    &quot;declaration&quot;: true,
    &quot;target&quot;: &quot;es5&quot;,
    &quot;noImplicitAny&quot;: true,
    &quot;suppressImplicitAnyIndexErrors&quot;: true,
    &quot;removeComments&quot;: false,
    &quot;preserveConstEnums&quot;: true,
    &quot;sourceMap&quot;: false,
    &quot;outDir&quot;: &quot;../../&quot;
  },
  &quot;files&quot;: [&quot;index.ts&quot;]
}
</code></pre><p>The things to notice are:</p><dl><dt>module</dt><dd>Publishing a commonjs module means it will play well with npm</dd><dt>declaration</dt><dd>This is what makes TypeScript generate <code>index.d.ts</code></dd><dt>outDir</dt><dd>We want to regenerate the <code>index.js</code> in the root (2 directories above this)</dd></dl><p>So now, what do we get when we build in Atom? Well, we&#x27;re generating an <code>&lt;a href=&quot;https://github.com/johnnyreilly/globalize-so-what-cha-want/blob/master/index.js&quot;&gt;index.js&lt;/a&gt;</code> file which looks like this:</p><pre><code class="language-js">var DEPENDENCY_TYPES = {
  SHARED_JSON: &#x27;Shared JSON (used by all locales)&#x27;,
  LOCALE_JSON: &#x27;Locale specific JSON (supplied for each locale)&#x27;,
};
var moduleDependencies = {
  core: {
    dependsUpon: [],
    cldrGlobalizeFiles: [
      &#x27;cldr.js&#x27;,
      &#x27;cldr/event.js&#x27;,
      &#x27;cldr/supplemental.js&#x27;,
      &#x27;globalize.js&#x27;,
    ],
    json: [
      {
        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,
        dependency: &#x27;cldr/supplemental/likelySubtags.json&#x27;,
      },
    ],
  },
  currency: {
    dependsUpon: [&#x27;number&#x27;, &#x27;plural&#x27;],
    cldrGlobalizeFiles: [&#x27;globalize/currency.js&#x27;],
    json: [
      {
        dependencyType: DEPENDENCY_TYPES.LOCALE_JSON,
        dependency: &#x27;cldr/main/{locale}/currencies.json&#x27;,
      },
      {
        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,
        dependency: &#x27;cldr/supplemental/currencyData.json&#x27;,
      },
    ],
  },
  date: {
    dependsUpon: [&#x27;number&#x27;],
    cldrGlobalizeFiles: [&#x27;globalize/date.js&#x27;],
    json: [
      {
        dependencyType: DEPENDENCY_TYPES.LOCALE_JSON,
        dependency: &#x27;cldr/main/{locale}/ca-gregorian.json&#x27;,
      },
      {
        dependencyType: DEPENDENCY_TYPES.LOCALE_JSON,
        dependency: &#x27;cldr/main/{locale}/timeZoneNames.json&#x27;,
      },
      {
        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,
        dependency: &#x27;cldr/supplemental/timeData.json&#x27;,
      },
      {
        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,
        dependency: &#x27;cldr/supplemental/weekData.json&#x27;,
      },
    ],
  },
  message: {
    dependsUpon: [&#x27;core&#x27;],
    cldrGlobalizeFiles: [&#x27;globalize/message.js&#x27;],
    json: [],
  },
  number: {
    dependsUpon: [&#x27;core&#x27;],
    cldrGlobalizeFiles: [&#x27;globalize/number.js&#x27;],
    json: [
      {
        dependencyType: DEPENDENCY_TYPES.LOCALE_JSON,
        dependency: &#x27;cldr/main/{locale}/numbers.json&#x27;,
      },
      {
        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,
        dependency: &#x27;cldr/supplemental/numberingSystems.json&#x27;,
      },
    ],
  },
  plural: {
    dependsUpon: [&#x27;core&#x27;],
    cldrGlobalizeFiles: [&#x27;globalize/plural.js&#x27;],
    json: [
      {
        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,
        dependency: &#x27;cldr/supplemental/plurals.json&#x27;,
      },
      {
        dependencyType: DEPENDENCY_TYPES.SHARED_JSON,
        dependency: &#x27;cldr/supplemental/ordinals.json&#x27;,
      },
    ],
  },
  relativeTime: {
    dependsUpon: [&#x27;number&#x27;, &#x27;plural&#x27;],
    cldrGlobalizeFiles: [&#x27;globalize/relative-time.js&#x27;],
    json: [
      {
        dependencyType: DEPENDENCY_TYPES.LOCALE_JSON,
        dependency: &#x27;cldr/main/{locale}/dateFields.json&#x27;,
      },
    ],
  },
};
function determineRequired(globalizeOptions, populateDependencies) {
  var modules = Object.keys(globalizeOptions);
  modules.forEach(function (module) {
    if (!moduleDependencies[module]) {
      throw new TypeError(&quot;There is no &#x27;&quot; + module + &quot;&#x27; module&quot;);
    }
  });
  var requireds = [];
  modules.forEach(function (module) {
    if (globalizeOptions[module]) {
      populateDependencies(module, requireds);
    }
  });
  return requireds;
}
function _populateDependencyCurrier(requiredArray, requiredArrayGetter) {
  var popDepFn = function (module, requireds) {
    var dependencies = moduleDependencies[module];
    dependencies.dependsUpon.forEach(function (requiredModule) {
      popDepFn(requiredModule, requireds);
    });
    dependencies[requiredArray].forEach(function (required) {
      var newRequired = requiredArrayGetter(required);
      if (requireds.indexOf(newRequired) === -1) {
        requireds.push(newRequired);
      }
    });
    return requireds;
  };
  return popDepFn;
}
/**
 * The string array returned will contain a list of the required cldr json data you need. I don&#x27;t believe ordering matters for the json but it is listed in the same dependency order as the cldr / globalize files are.
 *
 * @param options The globalize modules being used.
 */
function determineRequiredCldrData(globalizeOptions) {
  return determineRequired(
    globalizeOptions,
    _populateDependencyCurrier(&#x27;json&#x27;, function (json) {
      return json.dependency;
    })
  );
}
exports.determineRequiredCldrData = determineRequiredCldrData;
/**
 * The string array returned will contain a list of the required cldr / globalize files you need, listed in the order they are required.
 *
 * @param options The globalize modules being used.
 */
function determineRequiredCldrGlobalizeFiles(globalizeOptions) {
  return determineRequired(
    globalizeOptions,
    _populateDependencyCurrier(
      &#x27;cldrGlobalizeFiles&#x27;,
      function (cldrGlobalizeFile) {
        return cldrGlobalizeFile;
      }
    )
  );
}
exports.determineRequiredCldrGlobalizeFiles =
  determineRequiredCldrGlobalizeFiles;
</code></pre><p>Aside from one method moving internally and me adding some JSDoc, the only really notable change is the end of the file. TypeScript, when generating commonjs, doesn&#x27;t use the <code>module.exports = {}</code> approach. Rather, it drops exported functions onto the <code>exports</code> object as functions are exported. Functionally this is <em>identical</em>.</p><p>Now for our big finish: happily sat alongside is <code>index.js</code> is the <code>&lt;a href=&quot;https://github.com/johnnyreilly/globalize-so-what-cha-want/blob/master/index.d.ts&quot;&gt;index.d.ts&lt;/a&gt;</code> file:</p><pre><code class="language-ts">export interface Options {
  currency?: boolean;
  date?: boolean;
  message?: boolean;
  number?: boolean;
  plural?: boolean;
  relativeTime?: boolean;
}
/**
 * The string array returned will contain a list of the required cldr json data you need. I don&#x27;t believe ordering matters for the json but it is listed in the same dependency order as the cldr / globalize files are.
 *
 * @param options The globalize modules being used.
 */
export declare function determineRequiredCldrData(
  globalizeOptions: Options
): string[];
/**
 * The string array returned will contain a list of the required cldr / globalize files you need, listed in the order they are required.
 *
 * @param options The globalize modules being used.
 */
export declare function determineRequiredCldrGlobalizeFiles(
  globalizeOptions: Options
): string[];
</code></pre><p>We&#x27;re there, huzzah! This has been now published to npm - anyone consuming this package can use TypeScript straight out of the box. I really hope that publishing npm packages in this fashion becomes much more commonplace. Time will tell.</p><h2>PS I&#x27;m not the only one</h2><p>I was just about to hit &quot;publish&quot; when I happened upon <a href="https://twitter.com/basarat">Basarat</a>&#x27;s <a href="https://github.com/basarat/ts-npm-module">ts-npm-module</a> which is a project on GitHub which demo&#x27;s how to publish and consume TypeScript using npm. I&#x27;d say great minds think alike but I&#x27;m pretty sure Basarat&#x27;s mind is far greater than mine! (Cough, atom-typescript, cough.) Either way, it&#x27;s good to see validation for the approach I&#x27;m suggesting.</p><h2>PPS Update 23/09/2015 09:51</h2><p>One of the useful things about writing a blog is that you get to learn. Since I published I&#x27;ve become aware of a few things somewhat relevant to this post. First of all, there is still work ongoing in TypeScript land around this topic. Essentially there are problems resolving dependency conflicts when different dependencies have different versions - you can take part in the ongoing discussion <a href="https://github.com/Microsoft/TypeScript/issues/4665">here</a>. There&#x27;s also some useful resources to look at:</p><ul><li><a href="https://github.com/Microsoft/TypeScript/wiki/Typings-for-npm-packages">https://github.com/Microsoft/TypeScript/wiki/Typings-for-npm-packages</a></li><li><a href="https://basarat.gitbooks.io/typescript/content/docs/node/nodejs.html">https://basarat.gitbooks.io/typescript/content/docs/node/nodejs.html</a></li></ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Things Done Changed]]></title>
            <link>https://blog.johnnyreilly.com/2015/09/10/things-done-changed</link>
            <guid>Things Done Changed</guid>
            <pubDate>Thu, 10 Sep 2015 00:00:00 GMT</pubDate>
            <description><![CDATA[Some people fear change. Most people actually. I'm not immune to that myself, but not in the key area of technology. Any developer that fears change when it comes to the tools and languages that he / she is using is in the wrong business. Because what you're using to cut code today will not last. The language will evolve, the tools and frameworks that you love will die out and be replaced by new ones that are different and strange. In time, the language you feel you write as a native will fall out of favour, replaced by a new upstart.]]></description>
            <content:encoded><![CDATA[<p>Some people fear change. Most people actually. I&#x27;m not immune to that myself, but not in the key area of technology. Any developer that fears change when it comes to the tools and languages that he / she is using is in the <em>wrong</em> business. Because what you&#x27;re using to cut code today will not last. The language will evolve, the tools and frameworks that you love will die out and be replaced by new ones that are different and strange. In time, the language you feel you write as a native will fall out of favour, replaced by a new upstart.</p><p>My first gig was writing telecoms software using Delphi. I haven&#x27;t touched Delphi (or telecoms for that matter) for over 10 years now. Believe me, I grok that things change.</p><p>That is the developer&#x27;s lot. If you&#x27;re able to accept that then you&#x27;ll be just fine. For my part I&#x27;ve always rather relished the new and so I embrace it. However, I&#x27;ve met a surprising number of devs that are outraged when they realise that the language and tools they have used since their first job are not going to last. They do not go gentle into that good dawn. They rage, rage against the death of WebForms. My apologies to Dylan Thomas.</p><p>I recently started a new contract. This always brings a certain amount of change. This is part of the fun of contracting. However, the change was more significant in this case. As a result, the tools that I&#x27;ve been using for the last couple of months have been rather different to those that I&#x27;m used to. I&#x27;ve been outside my comfort zone. I&#x27;ve loved it. And now I want to reflect upon it. Because, in the words of Socrates, &quot;the unexamined life is not worth living&quot;.</p><h2>The Shock of the New (Toys)</h2><p>I&#x27;d been brought in to work on a full stack ASP.Net project. However, I&#x27;ve initially been working on a separate project which is <em>entirely</em> different. A web client app which has nothing to do with ASP.Net at all. It&#x27;s a greenfield app which is built using the following:</p><ol><li><a href="https://facebook.github.io/react/">React</a> / <a href="https://facebook.github.io/flux/docs/overview.html">Flux</a></li><li><a href="https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API">WebSockets</a> / <a href="https://developers.google.com/protocol-buffers/">Protocol Buffers</a></li><li><a href="http://browserify.org/">Browserify</a></li><li><a href="http://babeljs.io/">ES6 with Babel</a></li><li><a href="https://karma-runner.github.io">Karma</a></li><li><a href="http://gulpjs.com/">Gulp</a></li><li><a href="https://atom.io/">Atom</a></li></ol><p>Where to begin? Perhaps at the end - Atom.</p><h2>How Does it Feel to be on Your Own?</h2><p>When all around you, as far as the eye can see, are monitors displaying Visual Studio in all its grey glory whilst I was hammering away on Atom. It felt pretty good actually.</p><p>The app I was working on was a React / Flux app. You know what that means? JSX! At the time the project began Visual Studio did not have good editor support for JSX (something that the shipping of VS 2015 may have remedied but I haven&#x27;t checked). So, rather than endure a life dominated with red squigglies I jumped ship and moved over to using GitHub&#x27;s Atom to edit code.</p><p>I rather like it. Whilst VS is a full blown IDE, Atom is a text editor. A very pretty one. And crucially, one that can be extended by plugins of which there is a rich ecosystem. You want JSX support? <a href="https://atom.io/packages/jshint">There&#x27;s a plugin for that</a>. You want something that formats JSON nicely for you? <a href="https://atom.io/packages/pretty-json">There&#x27;s a plugin for that too</a>.</p><p>My only criticism of Atom really is that it doesn&#x27;t handle large files well and it crashes a lot. I&#x27;m quite surprised by both of these characteristics given that in contrast to VS it is so small. You&#x27;d think the basics would be better covered. Go figure. It still rocks though. It looks so sexy - how could I not like it?</p><h2>Someone to watch over me</h2><p>I&#x27;ve been using Gulp for a while now. It&#x27;s a great JavaScript task runner and incredibly powerful. Previously though, I&#x27;ve used it as part of a manual build step (even plumbing it into my csproj). With the current project I&#x27;ve moved over to using the watch functionality of gulp. So I&#x27;m scrapping triggering gulp tasks manually. Instead we have gulp configured to gaze lovingly at the source files and, when they change, re-run the build steps.</p><p>This is nice for a couple of reasons:</p><ul><li>When I want to test out the app the build is already done - I don&#x27;t have to wait for it to happen.</li><li>When I do bad things I find out faster. So I&#x27;ve got JSHint being triggered by my watch. If I write code that makes JSHint sad (and I haven&#x27;t noticed the warnings from the <a href="https://atom.io/packages/jshint">atom plugin</a>) then they&#x27;ll appear in the console. Likewise, my unit tests are continuously running in response to file changes (in an <a href="http://www.ncrunch.net/">ncrunch</a>-<!-- -->y sorta style) and so I know straight away if I&#x27;m breaking tests. Rather invaluable in the dynamic world of JavaScript.</li></ul><h2>Karma, Karma, Karma, Chameleon</h2><p>In the past, when using Visual Studio, it made sense to use the mighty <a href="http://mmanela.github.io/chutzpah/">Chutzpah</a> which allows you to run JS unit tests from within VS itself. I needed a new way to run my Jasmine unit tests. The obvious choice was Karma (built by the Angular team I think?). It&#x27;s really flexible.</p><p>You&#x27;re using Browserify? <a href="https://www.npmjs.com/package/karma-browserify">No bother</a>. You&#x27;re writing ES6 and transpiling with Babel? Not a problem. You want code coverage? <a href="https://www.npmjs.com/package/karma-coverage">That we can do</a>. You want an integration for TeamCity? <a href="https://www.npmjs.com/package/karma-teamcity-reporter">That too is sorted</a>....</p><p>Karma is fantastic. Fun fact: originally it was called Testacular. I kind of get why they changed the name but the world is all the duller for it. A side effect of the name change is that due to invalid search results I know a lot more about Buddhism than I used to.</p><h2>I cry Babel, Babel, look at me now</h2><p>Can you not wait for the future? Me neither. Even though it&#x27;s 2015 and Back to the Future II takes place in <a href="http://www.october212015.com/">only a month&#x27;s time</a>. So I&#x27;m not waiting for a million and one browsers to implement ES6 and IE 8 to finally die dammit. Nope, I have a plan and it&#x27;s <a href="http://babeljs.io/">Babel</a>. Transpile the tears away, write ES6 and have Babel spit out EStoday.</p><p>I&#x27;ve found this pretty addictive. Once you&#x27;ve started using ES6 features it&#x27;s hard to go back. Take <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Destructuring_assignment">destructuring</a> <!-- -->-<!-- --> I can&#x27;t get enough of it.</p><p>Whilst I love Babel, it has caused me some sadness as well. My beloved TypeScript is currently not in the mix, Babel is instead sat squarely where I want TS to be. I&#x27;m without static types and rather bereft. You can certainly live without them but having done so for a while I&#x27;m pretty clear now that static typing is a massive productivity win. You don&#x27;t have to hold the data structures that you&#x27;re working on in your head so much, code completion gives you what you need there, you can focus more on the problem that you&#x27;re trying to solve. You also burn less time on silly mistakes. If you accidentally change the return type of a function you&#x27;re likely to know straight away. Refactoring is so much harder without static types. I could go on.</p><p>All this goes to say: I want my static typing back. It wasn&#x27;t really an option to use TypeScript in the beginning because we were using JSX and TS didn&#x27;t support it. However! TypeScript is due to add support for JSX in <a href="https://blogs.msdn.com/b/typescript/archive/2015/09/02/announcing-typescript-1-6-beta-react-jsx-better-error-checking-and-more.aspx">TS 1.6 (currently in beta)</a>. I&#x27;ve plans to see if I can get TypeScript to emit ES6 and then keep using Babel to do the transpilation. Whether this will work, I don&#x27;t know yet. But it seems likely. So I&#x27;m hoping for a bright future.</p><h2>Browserify (there are no song lyrics that can be crowbarred in)</h2><p>Emitting scripts in the required order has been a perpetual pain for everyone in the web world for the longest time. I&#x27;ve taken about 5 different approaches to solving this problem over the years. None felt particularly good. So Browserify.</p><p>Browserify solves the problem of script ordering for you by allowing you to define an entry point to your application and getting you to write <code>require</code> (npm style) or <code>import</code> (<a href="http://exploringjs.com/es6/ch_modules.html">ES6 modules</a>) to bring in the modules that you need. This allows Browserify (which we&#x27;re using with Babel thanks to the <a href="https://github.com/babel/babelify">babelify transform</a>) to create a ginormous js file that contains the scripts served as needed. Thanks to the magic of source maps it also allows us to debug our original code (yup, the original ES6!) Browserify has the added bonus of allowing us free reign to pull in npm packages to use in our app without a great deal of ceremony.</p><p>Browserify is pretty fab - my only real reservation is that if you step outside the normal use cases you can quickly find yourself in deep water. Take for instance web workers. We were briefly looking into using them as an optimisation (breaking IO onto a separate process from the UI). A prime reason for backing away from this is that <a href="https://github.com/substack/webworkify/issues/14">Web Workers don&#x27;t play particularly well with Browserify</a>. And when you&#x27;ve got Babel (or <a href="https://github.com/babel/babelify">Babelify</a>) in the mix the problems just multiply. That apart, I really dig Browserify. I think I&#x27;d like to give WebPack a go as well as I understand it fulfills a similar purpose.</p><h2>WebSockets / Protocol Buffers</h2><p>The application I&#x27;m working on is plugging into an existing system which uses WebSockets for communication. Since WebSockets are native to the web we&#x27;ve been able to plumb these straight into our app. We&#x27;re also using Protocol Buffers as another optimisation; a way to save a few extra bytes from going down the wire. I don&#x27;t have much to say about either, just some observations really:</p><ol><li>WebSockets is a slightly different way of working - permanently open connections as opposed to the request / response paradigm of classic HTTP</li><li>WebSockets are wicked fast (due in part to those permanent connections). So performance is <em>amazing</em>. Fast like native, type amazing. In our case performance is pretty important and so this has been really great.</li></ol><h2>React / Flux</h2><p>Finally, React and Flux. I was completely new to these when I came onto the project and I quickly came to love them. There was a prejudice for me to overcome and that was JSX. When I first saw it I felt a little sick. &quot;Have we learned <em>NOTHING</em>???&quot; I wailed. &quot;Don&#x27;t we know that embedding strings in our controllers is a <em>BAD</em> thing?&quot; I was wrong. I had an epiphany. I discovered that JSX is not, as I first imagined, embedded HTML strings. Actually it&#x27;s syntactic sugar for object creation. A simple example:</p><pre><code class="language-jsx">var App;

// Some JSX:
var app = &lt;App version=&quot;1.0.0&quot; /&gt;;

// What that JSX transpiles into:
var app = React.createElement(App, { version: &#x27;1.0.0&#x27; });
</code></pre><p>Now that I&#x27;m well used to JSX and React I&#x27;ve really got to like it. I keep my views / components as dumb as possible and do all the complex logic in the stores. The stores are just standard JavaScript and so, pretty testable (simple Jasmine gives you all you need - I haven&#x27;t felt the need for <a href="https://facebook.github.io/jest/">Jest</a>). The components / views are also completely testable. I&#x27;d advise anyone coming to React afresh to make use of the <code>&lt;a href=&quot;https://facebook.github.io/react/docs/test-utils.html#shallow-rendering&quot;&gt;ReactShallowRenderer&lt;/a&gt;</code> for these purposes. This means you can test without a DOM - much better all round.</p><p>I don&#x27;t have a great deal to say about Flux; I think my views on it aren&#x27;t fully formed yet. I do really like predictability that unidirectional data flow gives you. However, I&#x27;m mindful that the app that I&#x27;ve been writing is very much about displaying data and doesn&#x27;t require much user input. I know that I&#x27;m living without 2-way data binding and I do wonder if I would come to miss it. Time will tell.</p><p>I really want to get back to static typing. That either means TypeScript (which I know and love) or Facebook&#x27;s Flow. (<a href="https://github.com/facebook/flow/issues/6">A Windows version of Flow is in the works</a>.) I&#x27;ll be very happy if I get either into the mix... Watch this space.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[(Top One, Nice One) Get Sorted]]></title>
            <link>https://blog.johnnyreilly.com/2015/08/13/top-one-nice-one-get-sorted</link>
            <guid>(Top One, Nice One) Get Sorted</guid>
            <pubDate>Thu, 13 Aug 2015 00:00:00 GMT</pubDate>
            <description><![CDATA[I was recently reading a post by Jaime González García which featured the following mind-bending proposition:]]></description>
            <content:encoded><![CDATA[<p>I was recently reading <a href="http://www.barbarianmeetscoding.com/blog/2015/07/09/mastering-the-arcane-art-of-javascript-mancy-for-c-sharp-developers-chapter-7-using-linq-in-javascript/">a post by Jaime González García</a> which featured the following mind-bending proposition:</p><blockquote><p>What if I told you that JavaScript has <a href="https://msdn.microsoft.com/en-us/library/bb397926.aspx">LINQ</a>??</p></blockquote><p>It got me thinking about one of favourite features of LINQ: <a href="http://www.dotnetperls.com/orderby-extension">ordering using OrderBy, ThenBy...</a> The ability to simply expose a collection of objects in a given order with a relatively terse and descriptive syntax. It is fantastically convenient, expressive and something I&#x27;ve been missing in JavaScript. But if Jaime is right... Well, let&#x27;s see what we can do.</p><h2>Sort</h2><p>JavaScript arrays have a <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/sort">sort</a> method. To quote MDN:</p><blockquote><p><code>arr.sort([compareFunction])</code>### <code>compareFunction</code></p><p>Optional. Specifies a function that defines the sort order. If omitted, the array is sorted according to each character&#x27;s Unicode code point value, according to the string conversion of each element.</p></blockquote><p>We want to use the <code>sort</code> function to introduce some LINQ-ish ordering goodness. Sort of. See what I did there?</p><p>Before we get going it&#x27;s worth saying that LINQ&#x27;s <code>OrderBy</code> and JavaScript&#x27;s <code>sort</code> are not the same thing. <code>sort</code> actually changes the order of the array. However, <code>OrderBy</code> returns an <code>IOrderedEnumerable</code> which when iterated returns the items of the collection in a particular order. An important difference. If preserving the original order of my array was important to me (spoiler: mostly it isn&#x27;t) then I could make a call to <code>&lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/slice&quot;&gt;slice&lt;/a&gt;</code> prior to calling <code>sort</code>.</p><p><code>sort</code> also returns the array to the caller which is nice for chaining and means we can use it in a similar fashion to the way we use <code>OrderBy</code>. With that in mind, we&#x27;re going to create comparer functions which will take a lambda / arrow function (ES6 alert!) and return a function which will compare based on the supplied lambda.</p><h2>String Comparer</h2><p>Let&#x27;s start with ordering by string properties:</p><pre><code class="language-js">function stringComparer(propLambda) {
  return (obj1, obj2) =&gt; {
    const obj1Val = propLambda(obj1) || &#x27;&#x27;;
    const obj2Val = propLambda(obj2) || &#x27;&#x27;;
    return obj1Val.localeCompare(obj2Val);
  };
}
</code></pre><p>We need some example data to sort: (I can only apologise for my lack of inspiration here)</p><pre><code class="language-js">const foodInTheHouse = [
  { what: &#x27;cake&#x27;, daysSincePurchase: 2 },
  { what: &#x27;apple&#x27;, daysSincePurchase: 8 },
  { what: &#x27;orange&#x27;, daysSincePurchase: 6 },
  { what: &#x27;apple&#x27;, daysSincePurchase: 2 },
];
</code></pre><p>If we were doing a sort by strings in LINQ we wouldn&#x27;t need to implement our own comparer. And the code we&#x27;d write would look something like this:</p><pre><code class="language-js">var foodInTheHouseSorted = foodInTheHouse.OrderBy((x) =&gt; x.what);
</code></pre><p>With that in mind, here&#x27;s how it would look to use our shiny and new <code>stringComparer</code>:</p><pre><code class="language-js">const foodInTheHouseSorted = foodInTheHouse.sort(stringComparer((x) =&gt; x.what));

// foodInTheHouseSorted: [
//   { what: &#x27;apple&#x27;,  daysSincePurchase: 8 },
//   { what: &#x27;apple&#x27;,  daysSincePurchase: 2 },
//   { what: &#x27;cake&#x27;,   daysSincePurchase: 2 },
//   { what: &#x27;orange&#x27;, daysSincePurchase: 6 }
// ]

// PS Don&#x27;t forget, for our JavaScript: foodInTheHouse === foodInTheHouseSorted
//    But for the LINQ:                 foodInTheHouse !=  foodInTheHouseSorted
//
//    However, if I&#x27;d done this:

const foodInTheHouseSlicedAndSorted = foodInTheHouse
  .slice()
  .sort(stringComparer((x) =&gt; x.what));

//    then:                             foodInTheHouse !== foodInTheHouseSlicedAndSorted
//
//    I shan&#x27;t mention this again.
</code></pre><h2>Number Comparer</h2><p>Well that&#x27;s strings sorted (quite literally). Now, what about numbers?</p><pre><code class="language-js">function numberComparer(propLambda) {
  return (obj1, obj2) =&gt; {
    const obj1Val = propLambda(obj1);
    const obj2Val = propLambda(obj2);
    if (obj1Val &gt; obj2Val) {
      return 1;
    } else if (obj1Val &lt; obj2Val) {
      return -1;
    }
    return 0;
  };
}
</code></pre><p>If we use the <code>numberComparer</code> on our original array it looks like this:</p><pre><code class="language-js">const foodInTheHouseSorted = foodInTheHouse.sort(
  numberComparer((x) =&gt; x.daysSincePurchase)
);

// foodInTheHouseSorted: [
//   { what: &#x27;cake&#x27;,   daysSincePurchase: 2 },
//   { what: &#x27;apple&#x27;,  daysSincePurchase: 2 },
//   { what: &#x27;orange&#x27;, daysSincePurchase: 6 },
//   { what: &#x27;apple&#x27;,  daysSincePurchase: 8 }
// ]
</code></pre><h2>Descending Into the Pit of Success</h2><p>Well this is all kinds of fabulous. But something&#x27;s probably nagging at you... What about <code>OrderByDescending</code>? What about when I want to sort in the reverse order? May I present the <code>reverse</code> function:</p><pre><code class="language-js">function reverse(comparer) {
  return (obj1, obj2) =&gt; comparer(obj1, obj2) * -1;
}
</code></pre><p>As the name suggests, this function takes a given comparer that&#x27;s handed to it and returns a function that inverts the results of executing that comparer. Clear as mud? A comparer can return 3 types of return values:</p><ul><li>0 - implies equality for <code>obj1</code> and <code>obj2</code></li><li>positive - implies <code>obj1</code> is greater than <code>obj2</code> by the ordering criterion</li><li>negative - implies <code>obj1</code> is less than <code>obj2</code> by the ordering criterion</li></ul><p>Our <code>reverse</code> function takes the comparer it is given and returns a new comparer that will return a positive value where the old one would have returned a negative and vica versa. (Equality is unaffected.) An alternative implementation would have been this:</p><pre><code class="language-js">function reverse(comparer) {
  return (obj1, obj2) =&gt; comparer(obj2, obj1);
}
</code></pre><p>Which is more optimal and even simpler as it just swaps the values supplied to the comparer. Whatever tickles your fancy. Either way, when used it looks like this:</p><pre><code class="language-js">const foodInTheHouseSorted = foodInTheHouse.sort(
  reverse(stringComparer((x) =&gt; x.what))
);

// foodInTheHouseSorted: [
//   { what: &#x27;orange&#x27;, daysSincePurchase: 6 },
//   { what: &#x27;cake&#x27;,   daysSincePurchase: 2 },
//   { what: &#x27;apple&#x27;,  daysSincePurchase: 8 },
//   { what: &#x27;apple&#x27;,  daysSincePurchase: 2 }
// ]
</code></pre><p>If you&#x27;d rather not have a function wrapping a function inline then you could create <code>stringComparerDescending</code>, a <code>numberComparerDescending</code> etc implementations. Arguably it might make for a nicer API. I&#x27;m not unhappy with the present approach myself and so I&#x27;ll leave it as is. But it&#x27;s an option.</p><h2><code>ThenBy</code></h2><p>So far we can sort arrays by strings, we can sort arrays by numbers and we can do either in descending order. It&#x27;s time to take it to the next level people. That&#x27;s right <code>ThenBy</code>; I want to be able to sort by one criteria and then by a subcriteria. So perhaps I want to eat the food in the house in alphabetical order, but if I have multiple apples I want to eat the ones I bought most recently first (because the other ones look old, brown and yukky). This may also be a sign I haven&#x27;t thought my life through, but it&#x27;s a choice that people make. People that I know. People I may have married.</p><p>It&#x27;s time to compose our comparers together. May I present... drum roll.... the <code>composeComparers</code> function:</p><pre><code class="language-js">function composeComparers(...comparers) {
  return (obj1, obj2) =&gt; {
    const comparer = comparers.find((c) =&gt; c(obj1, obj2) !== 0);
    return comparer ? comparer(obj1, obj2) : 0;
  };
}
</code></pre><p>This fine function takes any number of comparers that have been supplied to it. It then returns a comparer function which, when called, iterates through each of the original comparers and executes them until it finds one that returns a value that is not 0 (ie represents that the 2 items are not equal). It then sends that non-zero value back or if all was equal then sends back 0.</p><pre><code class="language-js">const foodInTheHouseSorted = foodInTheHouse.sort(
  composeComparers(
    stringComparer((x) =&gt; x.what),
    numberComparer((x) =&gt; x.daysSincePurchase)
  )
);

// foodInTheHouseSorted: [
//   { what: &#x27;apple&#x27;,  daysSincePurchase: 2 },
//   { what: &#x27;apple&#x27;,  daysSincePurchase: 8 },
//   { what: &#x27;cake&#x27;,   daysSincePurchase: 2 },
//   { what: &#x27;orange&#x27;, daysSincePurchase: 6 }
// ]
</code></pre><h2><code>composeComparers</code>: The Sequel</h2><p>I&#x27;m not gonna lie - I was feeling quite pleased with this approach. I shared it with my friend (and repeated colleague) <a href="http://blog.peterfoldi.com/">Peter Foldi</a>. The next day I found this in my inbox:</p><pre><code class="language-js">function composeComparers(...comparers) {
  return (obj1, obj2) =&gt;
    comparers.reduce((prev, curr) =&gt; prev || curr(obj1, obj2), 0);
}
</code></pre><p>Dammit he&#x27;s improved it. It&#x27;s down to 1 line of code, it doesn&#x27;t execute a non-zero returning comparer twice and it doesn&#x27;t rely on <code>&lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/find&quot;&gt;find&lt;/a&gt;</code> which only arrives with ES6. So if you wanted to backport to ES5 then this is a better choice.</p><p>The only criticism I can make of it is that it iterates through each of the comparers even when it doesn&#x27;t need to execute them. But that&#x27;s just carping really.</p><h2><code>composeComparers</code>: The Ultimate</h2><p>So naturally I thought I was done. Showing Peter&#x27;s improvements to the estimable Matthew Horsley I learned that this was not so. Because he reached for the keyboard and entered this:</p><pre><code class="language-js">function composeComparers(...comparers) {
  // README: &lt;a href=&quot;https://wiki.haskell.org/Function_composition&quot;&gt;https://wiki.haskell.org/Function_composition&lt;/a&gt;
  return comparers.reduce((prev, curr) =&gt; (a, b) =&gt; prev(a, b) || curr(a, b));
}
</code></pre><p>That&#x27;s right, he&#x27;s created a function which takes a number of comparers and reduced them up front into a single comparer function. This means that when the sort takes place there is no longer a need to iterate through the comparers, just execute them.</p><p>I know.</p><p><img src="../static/blog/2015-08-13-top-one-nice-one-get-sorted/3qknmj.jpg"/></p><p>I&#x27;ll get my coat...</p><script src="https://gist.github.com/johnnyreilly/22f7c05b02c2129b89ef.js"></script><h2>Update 08/10/2018: Now TypeScript</h2><p>You want to do this with TypeScript? Use this:</p><pre><code class="language-ts">type Comparer&lt;TObject&gt; = (obj1: TObject, obj2: TObject) =&gt; number;

export function stringComparer&lt;TObject&gt;(
  propLambda: (obj: TObject) =&gt; string
): Comparer&lt;TObject&gt; {
  return (obj1: TObject, obj2: TObject) =&gt; {
    const obj1Val = propLambda(obj1) || &#x27;&#x27;;
    const obj2Val = propLambda(obj2) || &#x27;&#x27;;
    return obj1Val.localeCompare(obj2Val);
  };
}

export function numberComparer&lt;TObject&gt;(
  propLambda: (obj: TObject) =&gt; number
): Comparer&lt;TObject&gt; {
  return (obj1: TObject, obj2: TObject) =&gt; {
    const obj1Val = propLambda(obj1);
    const obj2Val = propLambda(obj2);
    if (obj1Val &gt; obj2Val) {
      return 1;
    } else if (obj1Val &lt; obj2Val) {
      return -1;
    }
    return 0;
  };
}

export function reverse&lt;TObject&gt;(comparer: Comparer&lt;TObject&gt;) {
  return (obj1: TObject, obj2: TObject) =&gt; comparer(obj2, obj1);
}

export function composeComparers&lt;TObject&gt;(...comparers: Comparer&lt;TObject&gt;[]) {
  return comparers.reduce((prev, curr) =&gt; (a, b) =&gt; prev(a, b) || curr(a, b));
}
</code></pre>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Upgrading to Globalize 1.x for Dummies]]></title>
            <link>https://blog.johnnyreilly.com/2015/07/30/upgrading-to-globalize-1x-for-dummies</link>
            <guid>Upgrading to Globalize 1.x for Dummies</guid>
            <pubDate>Thu, 30 Jul 2015 00:00:00 GMT</pubDate>
            <description><![CDATA[Globalize has hit 1.0. Anyone who reads my blog will likely be aware that I'm a long time user of Globalize 0.1.x. I've been a little daunted by the leap that the move from 0.1.x to 1.x represents. It appears to be the very definition of "breaking changes". :-) But hey, this is Semantic Versioning being used correctly so how could I complain? Either way, I've decided to write up the migration here as I'm not expecting this to be easy.]]></description>
            <content:encoded><![CDATA[<p>Globalize has hit 1.0. Anyone who reads my blog will likely be aware that I&#x27;m a long time user of <a href="http://blog.icanmakethiswork.io/2012/05/globalizejs-number-and-date.html">Globalize 0.1.x</a>. I&#x27;ve been a little daunted by the leap that the move from 0.1.x to 1.x represents. It appears to be the very definition of &quot;breaking changes&quot;. :-) But hey, this is Semantic Versioning being used correctly so how could I complain? Either way, I&#x27;ve decided to write up the migration here as I&#x27;m not expecting this to be easy.</p><p>To kick things off I&#x27;ve set up a very <a href="https://github.com/johnnyreilly/globalize-migration/tree/v0.1.x">simple repo</a> that consists of a single page that depends upon Globalize 0.1.x to render a number and a date in German. It looks like this:</p><pre><code class="language-html">&lt;html&gt;
  &lt;head&gt;
    &lt;title&gt;Globalize demo&lt;/title&gt;
    &lt;link
      href=&quot;https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css&quot;
      rel=&quot;stylesheet&quot;
    /&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;div class=&quot;container-fluid&quot;&gt;
      &lt;h4&gt;Globalize demo for the &lt;em id=&quot;locale&quot;&gt;&lt;/em&gt; culture / locale&lt;/h4&gt;
      &lt;p&gt;
        This is a the number &lt;strong id=&quot;number&quot;&gt;&lt;/strong&gt; formatted by
        Globalize: &lt;strong id=&quot;numberFormatted&quot;&gt;&lt;/strong&gt;
      &lt;/p&gt;
      &lt;p&gt;
        This is a the number &lt;strong id=&quot;date&quot;&gt;&lt;/strong&gt; formatted by Globalize:
        &lt;strong id=&quot;dateFormatted&quot;&gt;&lt;/strong&gt;
      &lt;/p&gt;
    &lt;/div&gt;

    &lt;script src=&quot;bower_components/globalize/lib/globalize.js&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;bower_components/globalize/lib/cultures/globalize.culture.de-DE.js&quot;&gt;&lt;/script&gt;
    &lt;script&gt;
      var locale = &#x27;de-DE&#x27;;
      var number = 12345.67;
      var date = new Date(2012, 5, 15);

      Globalize.culture(locale);
      document.querySelector(&#x27;#locale&#x27;).innerText = locale;
      document.querySelector(&#x27;#number&#x27;).innerText = number;
      document.querySelector(&#x27;#date&#x27;).innerText = date;
      document.querySelector(&#x27;#numberFormatted&#x27;).innerText = Globalize.format(
        number,
        &#x27;n2&#x27;
      );
      document.querySelector(&#x27;#dateFormatted&#x27;).innerText = Globalize.format(
        date,
        &#x27;d&#x27;
      );
    &lt;/script&gt;
  &lt;/body&gt;
&lt;/html&gt;
</code></pre><p>When it&#x27;s run it looks like this:</p><p><img src="../static/blog/2015-07-30-upgrading-to-globalize-1x-for-dummies/Screenshot%2B2015-07-29%2B06.03.04.png"/></p><p>Let&#x27;s see how we go about migrating this super simple example.</p><h2>Update our Bower dependencies</h2><p>First things first, we want to move Globalize from 0.1.x to 1.x using Bower. To do that we update our <code>bower.json</code>:</p><pre><code class="language-js">&quot;dependencies&quot;: {
    &quot;globalize&quot;: &quot;^1.0.0&quot;
  }
</code></pre><p>Now we enter: <code>bower update</code>. And we&#x27;re off!</p><pre><code class="language-sh">bower globalize#^1.0.0          cached git://github.com/jquery/globalize.git#1.0.0
bower globalize#^1.0.0        validate 1.0.0 against git://github.com/jquery/globalize.git#^1.0.0
bower cldr-data#&gt;=25            cached git://github.com/rxaviers/cldr-data-bower.git#27.0.3
bower cldr-data#&gt;=25          validate 27.0.3 against git://github.com/rxaviers/cldr-data-bower.git#&gt;=25
bower cldrjs#0.4.1              cached git://github.com/rxaviers/cldrjs.git#0.4.1
bower cldrjs#0.4.1            validate 0.4.1 against git://github.com/rxaviers/cldrjs.git#0.4.1
bower globalize#^1.0.0         install globalize#1.0.0
bower cldr-data#&gt;=25           install cldr-data#27.0.3
bower cldrjs#0.4.1             install cldrjs#0.4.1

globalize#1.0.0 bower_components\globalize
├── cldr-data#27.0.3
└── cldrjs#0.4.1

cldr-data#27.0.3 bower_components\cldr-data

cldrjs#0.4.1 bower_components\cldrjs
└── cldr-data#27.0.3
</code></pre><p>This all looks happy enough. Except it&#x27;s actually not.</p><h2>We need fuel</h2><p>Or as I like to call it cldr-data. We just pulled down Globalize 1.x but we didn&#x27;t pull down the data that Globalize 1.x relies upon. This is one of the differences between Globalize 0.1.x and 1.x. Globalize 1.x does not include the &quot;culture&quot; data. By which I mean all the <code>globalize.culture.de-DE.js</code> type files. Instead Globalize 1.x relies upon <a href="http://cldr.unicode.org/">CLDR - Unicode Common Locale Data Repository</a>. It does this in the form of <a href="https://github.com/unicode-cldr/cldr-json">cldr-json</a>.</p><p>Now before you start to worry, you shouldn&#x27;t actually need to go and get this by yourself, the lovely <a href="https://github.com/rxaviers">Rafael Xavier de Souza</a> has saved you a job by putting together <a href="https://github.com/rxaviers/cldr-data-bower">Bower</a> and <a href="https://github.com/rxaviers/cldr-data-npm">npm</a> modules to do the hard work for you.</p><p>I&#x27;m using Bower for my client side package management and so I&#x27;ll use that. Looking at the Bower dependencies downloaded when I upgraded my package I can see there is a <code>cldr-data</code> package. Yay! However it appears to be missing the associated json files. Boo!</p><p>To the documentation Batman. It says you need a <code>.bowerrc</code> file in your repo which contains this:</p><pre><code class="language-js">{
  &quot;scripts&quot;: {
    &quot;preinstall&quot;: &quot;npm install cldr-data-downloader@0.2.x&quot;,
    &quot;postinstall&quot;: &quot;node ./node_modules/cldr-data-downloader/bin/download.js -i bower_components/cldr-data/index.json -o bower_components/cldr-data/&quot;
  }
}
</code></pre><p>Unfortunately, because I&#x27;ve already upgraded to v1 adding this file alone doesn&#x27;t do anything for me. To get round that I delete my <code>bower_components</code> folder and enter <code>bower install</code>. Boom!</p><pre><code>bower globalize#^1.0.0          cached git://github.com/jquery/globalize.git#1.0.0
bower globalize#^1.0.0        validate 1.0.0 against git://github.com/jquery/globalize.git#^1.0.0
bower cldrjs#0.4.1                        cached git://github.com/rxaviers/cldrjs.git#0.4.1
bower cldrjs#0.4.1                      validate 0.4.1 against git://github.com/rxaviers/cldrjs.git#0.4.1
bower cldr-data#&gt;=25                      cached git://github.com/rxaviers/cldr-data-bower.git#27.0.3
bower cldr-data#&gt;=25                    validate 27.0.3 against git://github.com/rxaviers/cldr-data-bower.git#&gt;=25
bower                                 preinstall npm install cldr-data-downloader@0.2.x
bower                                 preinstall cldr-data-downloader@0.2.3 node_modules\cldr-data-downloader
bower                                 preinstall ├── progress@1.1.8
bower                                 preinstall ├── q@1.0.1
bower                                 preinstall ├── request-progress@0.3.1 (throttleit@0.0.2)
bower                                 preinstall ├── nopt@3.0.3 (abbrev@1.0.7)
bower                                 preinstall ├── mkdirp@0.5.0 (minimist@0.0.8)
bower                                 preinstall ├── adm-zip@0.4.4
bower                                 preinstall ├── npmconf@2.0.9 (uid-number@0.0.5, ini@1.3.4, inherits@2.0.1, once@1.3.2, osenv@0.1.3, config-chain@1.1.9, semver@4.3.6)
bower                                 preinstall └── request@2.53.0 (caseless@0.9.0, forever-agent@0.5.2, aws-sign2@0.5.0, stringstream@0.0.4, tunnel-agent@0.4.1, oauth-sign@0.6.0, isstream@0.1.2, json-stringify-safe@5.0.1, qs@2.3.3, node-uuid@1.4.3, combined-stream@0.0.7, mime-types@2.0.14, form-data@0.2.0, tough-cookie@2.0.0, bl@0.9.4, http-signature@0.10.1, hawk@2.3.1)
bower globalize#^1.0.0                   install globalize#1.0.0
bower cldrjs#0.4.1                       install cldrjs#0.4.1
bower cldr-data#&gt;=25                     install cldr-data#27.0.3
bower                                postinstall node ./node_modules/cldr-data-downloader/bin/download.js -i bower_components/cldr-data/index.json -o bower_components/cldr-data/
bower                                postinstall GET `https://github.com/unicode-cldr/cldr-core/archive/27.0.3.zip`
bower                                postinstall GET `https://github.com/unicode-cldr/cldr-dates-modern/archive/27.0.3.zip`
bower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-buddhist-modern/archive/27.0.3.zip`
bower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-chinese-modern/archive/27.0.3.zip`
bower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-coptic-modern/archive/27.0.3.zip`
bower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-dangi-modern/archive/27.0.3.zip`
bower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-ethiopic-modern/archive/27.0.3.zip`
bower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-hebrew-modern/archive/27.0.3.zip`
bower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-indian-modern/archive/27.0.3.zip`
bower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-islamic-modern/archive/27.0.3.zip`
bower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-japanese-modern/archive/27.0.3.zip`
bower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-persian-modern/archive/27.0.3.zip`
bower                                postinstall GET `https://github.com/unicode-cldr/cldr-cal-roc-modern/archive/27.0.3.zip`
bower                                postinstall GET `https://github.com/unicode-cldr/cldr-localenames-modern/archive/27.0.3.zip`
bower                                postinstall GET `https://github.com/unicode-cldr/cldr-misc-modern/archive/27.0.3.zip`
bower                                postinstall GET `https://github.com/unicode-cldr/cldr-numbers-modern/archive/27.0.3.zip`
bower                                postinstall GET `https://github.com/unicode-cldr/cldr-segments-modern/archive/27.0.3.zip`
bower                                postinstall GET `https://github.com/unicode-cldr/cldr-units-modern/archive/27.0.3.zip`
bower                                postinstall Received 28728K total.
bower                                postinstall Received 28753K total.
bower                                postinstall Unpacking it into `./bower_components\cldr-data`

globalize#1.0.0 bower_components\globalize
├── cldr-data#27.0.3
└── cldrjs#0.4.1

cldrjs#0.4.1 bower_components\cldrjs
└── cldr-data#27.0.3

cldr-data#27.0.3 bower_components\cldr-data
</code></pre><p>That&#x27;s right - I&#x27;m golden. And if I didn&#x27;t want to do that I could have gone straight to the command line and entered this: (look familiar?)</p><pre><code>npm install cldr-data-downloader@0.2.x
node ./node_modules/cldr-data-downloader/bin/download.js -i bower_components/cldr-data/index.json -o bower_components/cldr-data/
</code></pre><h2>Some bitching and moaning.</h2><p>If, like me, you were a regular user of Globalize 0.1.x then you know that you needed very little to get going. As you can see from our example you just serve up <code>Globalize.js</code> and the culture files you are interested in (eg <code>globalize.culture.de-DE.js</code>). That&#x27;s it - you have all you need; job&#x27;s a good&#x27;un. This is all very convenient and entirely lovely.</p><p>Globalize 1.x has a different approach and one that (I have to be honest) I&#x27;m not entirely on board with. The thing that you need to know about the new Globalize is that <em>nothing comes for free</em>. It&#x27;s been completely modularised and <a href="https://github.com/jquery/globalize#pick-the-modules-you-need">you have to include extra libraries depending on the functionality you require.</a> On top of that you then have to work out the <a href="https://github.com/jquery/globalize#2-cldr-content">portions of the cldr data that you require for those modules</a> and supply them. This means that getting up and running with Globalize 1.x is much harder. Frankly I think it&#x27;s a little painful.</p><p>I realise this is a little <a href="https://en.wikipedia.org/wiki/Who_Moved_My_Cheese%3F">&quot;Who moved my cheese&quot;</a>. I&#x27;ll get over it. I do actually see the logic of this. It is certainly good that the culture date is not frozen in aspic but will evolve as the world does. But it&#x27;s undeniable that in our brave new world Globalize is no longer a doddle to pick up. Or at least right now.</p><h2>Take the modules and run</h2><p>So. What do we actually need? Well I&#x27;ve consulted the <a href="https://github.com/jquery/globalize#pick-the-modules-you-need">documentation</a> and I think I&#x27;m clear. Our simple demo cares about dates and numbers. So I&#x27;m going to guess that means I need:</p><ul><li><code>&lt;a href=&quot;https://github.com/jquery/globalize#core-module&quot;&gt;globalize.js&lt;/a&gt;</code></li><li><code>&lt;a href=&quot;https://github.com/jquery/globalize#date-module&quot;&gt;globalize/date.js&lt;/a&gt;</code></li><li><code>&lt;a href=&quot;https://github.com/jquery/globalize#number-module&quot;&gt;globalize/number.js&lt;/a&gt;</code></li></ul><p>On top of that I&#x27;m also going to need the various cldr dependencies too. That&#x27;s not all. Given that I&#x27;ve decided which modules I will use I now need to acquire the associated cldr data. According to the docs <a href="https://github.com/jquery/globalize#2-cldr-content">here</a> we&#x27;re going to need:</p><ul><li><code>cldr/supplemental/likelySubtags.json</code></li><li><code>cldr/main/&lt;i&gt;locale&lt;/i&gt;/ca-gregorian.json</code></li><li><code>cldr/main/&lt;i&gt;locale&lt;/i&gt;/timeZoneNames.json</code></li><li><code>cldr/supplemental/timeData.json</code></li><li><code>cldr/supplemental/weekData.json</code></li><li><code>cldr/main/locale/numbers.json</code></li><li><code>cldr/supplemental/numberingSystems.json</code></li></ul><p>Figuring that all out felt like really hard work. But I think that now we&#x27;re ready to do the actual migration.</p><h3>Update 30/08/2015: Globalize · So What&#x27;cha Want</h3><p>To make working out what you need when using Globalize I&#x27;ve built <a href="http://johnnyreilly.github.io/globalize-so-what-cha-want/">Globalize · So What&#x27;cha Want</a>. You&#x27;re so very welcome.</p><h2>The Actual Migration</h2><p>To do this I&#x27;m going to lean heavily upon <a href="https://github.com/jquery/globalize/blob/master/examples/plain-javascript/index.html">an example put together by Rafael</a>. The migrated code looks like this:</p><pre><code class="language-html">&lt;html&gt;
  &lt;head&gt;
    &lt;title&gt;Globalize demo&lt;/title&gt;
    &lt;link
      href=&quot;https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css&quot;
      rel=&quot;stylesheet&quot;
    /&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;div class=&quot;container-fluid&quot;&gt;
      &lt;h4&gt;Globalize demo for the &lt;em id=&quot;locale&quot;&gt;&lt;/em&gt; culture / locale&lt;/h4&gt;
      &lt;p&gt;
        This is a the number &lt;strong id=&quot;number&quot;&gt;&lt;/strong&gt; formatted by
        Globalize: &lt;strong id=&quot;numberFormatted&quot;&gt;&lt;/strong&gt;
      &lt;/p&gt;
      &lt;p&gt;
        This is a the number &lt;strong id=&quot;date&quot;&gt;&lt;/strong&gt; formatted by Globalize:
        &lt;strong id=&quot;dateFormatted&quot;&gt;&lt;/strong&gt;
      &lt;/p&gt;
    &lt;/div&gt;

    &lt;!-- First, we load Globalize&#x27;s dependencies (`cldrjs` and its supplemental module). --&gt;
    &lt;script src=&quot;bower_components/cldrjs/dist/cldr.js&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;bower_components/cldrjs/dist/cldr/event.js&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;bower_components/cldrjs/dist/cldr/supplemental.js&quot;&gt;&lt;/script&gt;

    &lt;!-- Next, we load Globalize and its modules. --&gt;
    &lt;script src=&quot;bower_components/globalize/dist/globalize.js&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;bower_components/globalize/dist/globalize/number.js&quot;&gt;&lt;/script&gt;

    &lt;!-- Load after globalize/number.js --&gt;
    &lt;script src=&quot;bower_components/globalize/dist/globalize/date.js&quot;&gt;&lt;/script&gt;

    &lt;script&gt;
      var locale = &#x27;de&#x27;;

      Promise.all([
        // Core
        fetch(&#x27;bower_components/cldr-data/supplemental/likelySubtags.json&#x27;),

        // Date
        fetch(
          &#x27;bower_components/cldr-data/main/&#x27; + locale + &#x27;/ca-gregorian.json&#x27;
        ),
        fetch(
          &#x27;bower_components/cldr-data/main/&#x27; + locale + &#x27;/timeZoneNames.json&#x27;
        ),
        fetch(&#x27;bower_components/cldr-data/supplemental/timeData.json&#x27;),
        fetch(&#x27;bower_components/cldr-data/supplemental/weekData.json&#x27;),

        // Number
        fetch(&#x27;bower_components/cldr-data/main/&#x27; + locale + &#x27;/numbers.json&#x27;),
        fetch(&#x27;bower_components/cldr-data/supplemental/numberingSystems.json&#x27;),
      ])
        .then(function (responses) {
          return Promise.all(
            responses.map(function (response) {
              return response.json();
            })
          );
        })
        .then(Globalize.load)
        .then(function () {
          var number = 12345.67;
          var date = new Date(2012, 5, 15);

          var globalize = Globalize(locale);
          document.querySelector(&#x27;#locale&#x27;).innerText = locale;
          document.querySelector(&#x27;#number&#x27;).innerText = number;
          document.querySelector(&#x27;#date&#x27;).innerText = date;
          document.querySelector(&#x27;#numberFormatted&#x27;).innerText =
            globalize.formatNumber(number, {
              minimumFractionDigits: 2,
              useGrouping: true,
            });
          document.querySelector(&#x27;#dateFormatted&#x27;).innerText =
            globalize.formatDate(date, {
              date: &#x27;medium&#x27;,
            });
        });
    &lt;/script&gt;
  &lt;/body&gt;
&lt;/html&gt;
</code></pre><p>By the way, I&#x27;m using <a href="http://jakearchibald.com/2015/thats-so-fetch/">fetch</a> and <a href="http://www.html5rocks.com/en/tutorials/es6/promises/">promises</a> to load the cldr-data. This isn&#x27;t mandatory - I use it because Chrome lets me. (I&#x27;m so bleeding edge.) Some standard jQuery ajax calls would do just as well. There&#x27;s an example of that approach <a href="https://github.com/jquery/globalize/blob/master/doc/cldr.md#how-do-i-load-cldr-data-into-globalize">here</a>.</p><h2>Observations</h2><p>We&#x27;ve gone from not a lot of code to... well, more than a little. A medium amount. Almost all of that extra code relates to getting Globalize 1.x to spin up so it&#x27;s ready to work. We&#x27;ve also gone from 2 HTTP requests to 13 which is unlucky for you. 6 of them took place via ajax after the page had loaded. It&#x27;s worth noting that we&#x27;re not even loading all of Globalize either. On top of that there&#x27;s the old order-of-loading shenanigans to deal with. All of these can be mitigated by introducing a custom build step of your own to concatenate and minify the associated cldr / Globalize files.</p><p>Loading the data via ajax isn&#x27;t mandatory by the way. If you wanted to you could create your own style of <code>globalize.culture.de.js</code> files which would allow you load the page without recourse to post-page load HTTP requests. Something like this Gulp task I&#x27;ve knocked up would do the trick:</p><pre><code class="language-js">gulp.task(&#x27;make-globalize-culture-de-js&#x27;, function () {
  var locale = &#x27;de&#x27;;
  var jsonWeNeed = [
    require(&#x27;./bower_components/cldr-data/supplemental/likelySubtags.json&#x27;),
    require(&#x27;./bower_components/cldr-data/main/&#x27; +
      locale +
      &#x27;/ca-gregorian.json&#x27;),
    require(&#x27;./bower_components/cldr-data/main/&#x27; +
      locale +
      &#x27;/timeZoneNames.json&#x27;),
    require(&#x27;./bower_components/cldr-data/supplemental/timeData.json&#x27;),
    require(&#x27;./bower_components/cldr-data/supplemental/weekData.json&#x27;),
    require(&#x27;./bower_components/cldr-data/main/&#x27; + locale + &#x27;/numbers.json&#x27;),
    require(&#x27;./bower_components/cldr-data/supplemental/numberingSystems.json&#x27;),
  ];

  var jsonStringWithLoad =
    &#x27;Globalize.load(&#x27; +
    jsonWeNeed
      .map(function (json) {
        return JSON.stringify(json);
      })
      .join(&#x27;, &#x27;) +
    &#x27;);&#x27;;

  var fs = require(&#x27;fs&#x27;);
  fs.writeFile(
    &#x27;./globalize.culture.&#x27; + locale + &#x27;.js&#x27;,
    jsonStringWithLoad,
    function (err) {
      if (err) {
        console.log(err);
      } else {
        console.log(&#x27;The file was created!&#x27;);
      }
    }
  );
});
</code></pre><p>The above is standard node/io type code by the way; just take the contents of the function and run in node and you should be fine. If you do use this approach then you&#x27;re very much back to the simplicity of Globalize 0.1.x&#x27;s approach.</p><p>And here is the page in all its post migration glory:</p><p><img src="../static/blog/2015-07-30-upgrading-to-globalize-1x-for-dummies/Screenshot%2B2015-07-30%2B20.21.19.png"/></p><p>It looks exactly the same except &#x27;de-DE&#x27; has become simply &#x27;de&#x27; (since that&#x27;s how the cldr rolls).</p><p>The migrated code is <a href="https://github.com/johnnyreilly/globalize-migration">there for the taking</a>. Make sure you remember to <code>bower install</code> <!-- -->-<!-- --> and you&#x27;ll need to host the demo on a simple server since it makes ajax calls.</p><p>Before I finish off I wanted to say &quot;well done!&quot; to all the people who have worked on Globalize. It&#x27;s an important project and I do apologise for my being a little critical of it here. I should say that I think it&#x27;s just the getting started that&#x27;s hard. Once you get over that hurdle you&#x27;ll be fine. Hopefully this post will help people do just that. Pip, pip!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[npm please stop hurting Visual Studio]]></title>
            <link>https://blog.johnnyreilly.com/2015/06/29/npm-please-stop-hurting-visual-studio</link>
            <guid>npm please stop hurting Visual Studio</guid>
            <pubDate>Mon, 29 Jun 2015 00:00:00 GMT</pubDate>
            <description><![CDATA[I don't know about you but I personally feel that the following sentence may well be the saddest in the English language:]]></description>
            <content:encoded><![CDATA[<p>I don&#x27;t know about you but I personally feel that the following sentence may well be the saddest in the English language:</p><p><code>2&amp;gt;ASPNETCOMPILER : error ASPRUNTIME: The specified path, file name, or both are too long. The fully qualified file name must be less than 260 characters, and the directory name must be less than 248 characters.</code></p><p>The message above would suggest there is some kind of ASP.Net issue going on. There isn&#x27;t - the problem actually lies with Windows. It&#x27;s <a href="http://blog.icanmakethiswork.io/2014/12/gulp-npm-long-paths-and-visual-studio-fight.html">not the first time it&#x27;s come up</a> but for those of you not aware there is something you need to know about Windows: <em>It handles long paths badly.</em></p><p>There&#x27;s a number of caveats which people may attach the above sentence. But essentially what I have said is true. And it becomes brutally apparent to you the moment you start using a few node / npm powered tools in your workflow. You will likely see that horrible message and you won&#x27;t be able to get much further forward. Sigh. I thought this was the future...</p><p>This post is about how to deal with the long path issue when using npm with Visual Studio. This should very much be a short term workaround as <a href="https://github.com/npm/npm/releases/tag/v3.0.0">npm 3.0</a> is planned to make long paths with npm a thing of the past. But until that golden dawn....</p><h2>The Latest Infraction</h2><p>I&#x27;m a big fan of Gulp and Bower. They rock. <a href="https://twitter.com/codecadwallader">Steve Cadwallader</a> wrote an excellent blog post about <a href="http://www.codecadwallader.com/2015/03/15/integrating-gulp-into-your-tfs-builds-and-web-deploy/">integrating Gulp into your Visual Studio build</a>. Essentially the Gist of his post is this: forget using <a href="https://visualstudiogallery.msdn.microsoft.com/8e1b4368-4afb-467a-bc13-9650572db708">Task Runner Explorer</a> to trigger your Gulp / Grunt jobs. No, actually plug it into the build process by tweaking your <code>.csproj</code> file. The first time I used this approach it was a dream come true. It just worked and I was a very happy man.</p><p>Since this approach was so marvellous I took a look at the demo / docs part of <a href="https://github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native">jQuery Validation Unobtrusive Native</a> with a view to applying it there. I originally wrote this back in 2013 and at the time used NuGet for both server and client side package management. I decided to migrate it to use Bower for the client side packages (which I planned to combine with a Gulp script which was going to pull out the required JS / CSS etc as needed). However it wasn&#x27;t the plain sailing I&#x27;d imagined. The actual switchover from NuGet to Bower was simple. Just a case of removing NuGet packages and adding their associated Bower counterpart. The problem came when the migration was done and I hit &quot;compile&quot;. That&#x27;s when I got to see <code>2&amp;gt;ASPNETCOMPILER : error ASPRUNTIME: The specified path, file name, or both are too long...</code> etc</p><p>For reasons that I don&#x27;t fully understand, Visual Studio is really upset by the presence in the project structure of one almighty long path. Oddly enough, not a path that&#x27;s actually part of the Visual Studio project in question at all. Rather one that has come along as a result of our Gulp / Bower / npm shenanigans. Quick as a flash, I whipped out Daniel Schroeder&#x27;s <a href="https://pathlengthchecker.codeplex.com/">Path Length Checker</a> to see where the problem lay:</p><p><img src="../static/blog/2015-06-29-npm-please-stop-hurting-visual-studio/bower-with-the-long-paths.png"/></p><p>And lo, the fault lay with Bower. Poor show, Bower, poor show.</p><h2>rimraf to the Rescue</h2><p><a href="https://github.com/isaacs/rimraf">rimraf</a> is &quot;the <a href="https://en.wikipedia.org/wiki/Rm_(Unix)">UNIX command</a><code>rm -rf</code> for node&quot;. (By the way, what is it with node and the pathological hatred of capital letters?)</p><p>What this means is: rimraf can delete. Properly. So let&#x27;s get it: <code>npm install -g rimraf</code>. Then at any time at the command line we can dispose of a long path in 2 shakes of lamb&#x27;s tail.</p><p>In my current situation the contents of the <code>node_modules</code> folder is causing me heartache. But with rimraf in play I can get rid of it with the magic words: <code>rimraf ./node_modules</code>. Alakazam! So let&#x27;s poke this command into the extra commands that I&#x27;ve already shoplifted from Steve&#x27;s blog post. I&#x27;ll end up with the following section of XML at the end of my <code>.csproj</code>:</p><pre><code class="language-xml">&lt;PropertyGroup&gt;
    &lt;CompileDependsOn&gt;
      $(CompileDependsOn);
      GulpBuild;
    &lt;/CompileDependsOn&gt;
    &lt;CleanDependsOn&gt;
      $(CleanDependsOn);
      GulpClean
    &lt;/CleanDependsOn&gt;
    &lt;CopyAllFilesToSingleFolderForPackageDependsOn&gt;
      CollectGulpOutput;
      $(CopyAllFilesToSingleFolderForPackageDependsOn);
    &lt;/CopyAllFilesToSingleFolderForPackageDependsOn&gt;
    &lt;CopyAllFilesToSingleFolderForMsdeployDependsOn&gt;
      CollectGulpOutput;
      $(CopyAllFilesToSingleFolderForPackageDependsOn);
    &lt;/CopyAllFilesToSingleFolderForMsdeployDependsOn&gt;
  &lt;/PropertyGroup&gt;
  &lt;Target Name=&quot;GulpBuild&quot;&gt;
    &lt;Exec Command=&quot;npm install&quot; /&gt;
    &lt;Exec Command=&quot;bower install&quot; /&gt;
    &lt;Exec Command=&quot;gulp&quot; /&gt;
    &lt;Exec Command=&quot;rimraf ./node_modules&quot; /&gt;
  &lt;/Target&gt;
  &lt;Target Name=&quot;GulpClean&quot;&gt;
    &lt;Exec Command=&quot;npm install&quot; /&gt;
    &lt;Exec Command=&quot;gulp clean&quot; /&gt;
    &lt;Exec Command=&quot;rimraf ./node_modules&quot; /&gt;
  &lt;/Target&gt;
  &lt;Target Name=&quot;CollectGulpOutput&quot;&gt;
    &lt;ItemGroup&gt;
      &lt;_CustomFiles Include=&quot;build\**\*&quot; /&gt;
      &lt;FilesForPackagingFromProject Include=&quot;%(_CustomFiles.Identity)&quot;&gt;
        &lt;DestinationRelativePath&gt;build\%(RecursiveDir)%(Filename)%(Extension)&lt;/DestinationRelativePath&gt;
      &lt;/FilesForPackagingFromProject&gt;
    &lt;/ItemGroup&gt;
    &lt;Message Text=&quot;CollectGulpOutput list: %(_CustomFiles.Identity)&quot; /&gt;
  &lt;/Target&gt;
</code></pre><p>So let&#x27;s focus on the important bits in the <code>GulpBuild</code> target:</p><ul><li><code>&amp;lt;Exec Command=&quot;npm install&quot; /&amp;gt;</code> <!-- -->-<!-- --> install the node packages our project uses as specified in <code>package.json</code>. This will include Gulp and Bower. The latter package is going to contain super-long, Windows wrecking paths.</li><li><code>&amp;lt;Exec Command=&quot;bower install&quot; /&amp;gt;</code> <!-- -->-<!-- --> install the bower packages specified in <code>bower.json</code> using Bower (which was installed by npm just now).</li><li><code>&amp;lt;Exec Command=&quot;gulp&quot; /&amp;gt;</code> <!-- -->-<!-- --> do a little dance, make a little love, copy a few files, get down tonight.</li><li><code>&amp;lt;Exec Command=&quot;rimraf ./node_modules&quot; /&amp;gt;</code> <!-- -->-<!-- --> remove the <code>node_modules</code> folder populated by the <code>npm install</code> command.</li></ul><p>With that addition of <code>rimraf ./node_modules</code> to the build phase the problem goes away. During each build a big, big Windows path is being constructed but then it&#x27;s wiped again before it has chance to upset anyone. I&#x27;ve also added the same to the <code>GulpClean</code> target.</p><p>You are very welcome.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Back to the Future with Code First Migrations]]></title>
            <link>https://blog.johnnyreilly.com/2015/06/19/Back-to-the-Future-with-Code-First-Migrations</link>
            <guid>Back to the Future with Code First Migrations</guid>
            <pubDate>Fri, 19 Jun 2015 00:00:00 GMT</pubDate>
            <description><![CDATA[Code First Migrations. They look a little like this in Visual Studio:]]></description>
            <content:encoded><![CDATA[<p>Code First Migrations. They look a little like this in Visual Studio:</p><p><img src="../static/blog/2015-06-19-Back-to-the-Future-with-Code-First-Migrations/Migrations.png"/></p><p>The thing I want you to notice about the image above is not the pithily named migrations. It isn&#x27;t the natty opacity on everything but the migration files (which I can assure you took me to the very limits of my <a href="http://www.gimp.org/">GIMP</a> expertise). No, whilst exciting in themselves what I want you to think about is <em>the order in which migrations are applied</em>. Essentially how the <code>__MigrationHistory</code> table in SQL Server ends up being populated in this manner:</p><p><img src="../static/blog/2015-06-19-Back-to-the-Future-with-Code-First-Migrations/MigrationHistory.png"/></p><p>Because, myself, I didn&#x27;t really think about this until it came time for me to try and change the ordering of some migrations manually. Do you know how migrations end up the order they do? I bet you don&#x27;t. But either way, let&#x27;s watch and see what happens to the pre-enlightenment me as I attempt to take a migration which appears <em>before</em> a migration I have created locally and move it to <em>after</em> that same migration.</p><h2>Great Scott! It&#x27;s clearly filename driven</h2><p>That&#x27;s right - it&#x27;s blindingly obvious to me. All I need do is take the migration I want to move forwards in time and rename it in Visual Studio. So take our old migration (&quot;2014 is so passé darling&quot;):</p><p><img src="../static/blog/2015-06-19-Back-to-the-Future-with-Code-First-Migrations/Screenshot%2B2015-06-19%2B13.07.50.png"/></p><p>And rename it to make it new and shiny (&quot;2015! Gorgeous - I love it sweetie!&quot;):</p><p><img src="../static/blog/2015-06-19-Back-to-the-Future-with-Code-First-Migrations/Screenshot%2B2015-06-19%2B13.08.46.png"/></p><p>Perfection right? Wrong! What you&#x27;ve done makes not the slightest jot of difference.</p><h2>Whoa, this is heavy! Gimme the project file</h2><p>How could I be so dim? I mean it makes perfect sense - before the days of <a href="http://blog.icanmakethiswork.io/2015/02/hey-tsconfigjson-where-have-you-been.html">TypeScript&#x27;s tsconfig.json </a> the default ordering of <code>*.ts</code> files being passed to the TypeScript compiler was determined by the ordering of the <code>*.ts</code> files in the <code>.csproj</code> file. It must be the same for Code First Migrations.</p><p>So, simply spin up <a href="https://notepad-plus-plus.org/">Notepad++</a> and let&#x27;s play hack the XML until each file is referenced in the required order.</p><p>Well, I&#x27;m glad we sorted that out. A quick test to reassure myself of my astuteness. Drum roll.... Fail!! Things are just as they were. Shame on you John Reilly, shame on you.</p><h2>Designer.cs... Your kids are gonna love it</h2><p><img src="../static/blog/2015-06-19-Back-to-the-Future-with-Code-First-Migrations/Screenshot%2B2015-06-19%2B13.35.40.png"/></p><p>I want you to look very carefully at this and tell me what you see. We&#x27;re looking at the mysterious <code>201508121401253_AddSagacityToSage.Designer.cs</code> file that sits underneath the main <code>201508121401253_AddSagacityToSage.cs</code> file. What could it be.... Give in?</p><p>The <code>IMigrationMetadata.Id</code> property is returning <code>&lt;u&gt;201408121401253&lt;/u&gt;_AddSagacityToSage</code>. That is the <em>old</em> date! Remember? The passé one. If you change that property to line up with the file name change you&#x27;re done. It works.</p><p><img src="../static/blog/2015-06-19-Back-to-the-Future-with-Code-First-Migrations/where-were-going.jpg"/></p><p>Let&#x27;s say it together: &quot;Automatic Migrations? Where we&#x27;re going, we don&#x27;t need Automatic Migrations.&quot;</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Angular UI Bootstrap Datepicker Weirdness]]></title>
            <link>https://blog.johnnyreilly.com/2015/05/23/angular-ui-bootstrap-datepicker-weirdness</link>
            <guid>Angular UI Bootstrap Datepicker Weirdness</guid>
            <pubDate>Sat, 23 May 2015 00:00:00 GMT</pubDate>
            <description><![CDATA[The Angular UI Bootstrap Datepicker is fan-dabby-dozy. But it has a ... pecularity. You can use the picker like this:]]></description>
            <content:encoded><![CDATA[<p>The <a href="https://angular-ui.github.io/bootstrap/#/datepicker">Angular UI Bootstrap Datepicker</a> is fan-dabby-dozy. But it has a ... pecularity. You can use the picker like this:</p><pre><code class="language-html">&lt;div ng-app=&quot;peskyDatepicker&quot;&gt;
  &lt;div ng-controller=&quot;DatepickerDemoCtrl as vm&quot;&gt;
    &lt;input
      type=&quot;text&quot;
      class=&quot;form-control&quot;
      datepicker-popup=&quot;mediumDate&quot;
      is-open=&quot;vm.valuationDatePickerIsOpen&quot;
      ng-click=&quot;vm.valuationDatePickerOpen()&quot;
      ng-model=&quot;vm.valuationDate&quot;
    /&gt;
  &lt;/div&gt;
&lt;/div&gt;
</code></pre><pre><code class="language-js">angular
  .module(&#x27;peskyDatepicker&#x27;, [&#x27;ui.bootstrap&#x27;])
  .controller(&#x27;DatepickerDemoCtrl&#x27;, [
    function () {
      var vm = this;

      vm.valuationDate = new Date();
      vm.valuationDatePickerIsOpen = false;

      vm.valuationDatePickerOpen = function () {
        this.valuationDatePickerIsOpen = true;
      };
    },
  ]);
</code></pre><p>The above code produces a textbox which, when clicked upon, renders the datepicker popup (which vanishes upon date selection). This works because the <code>ng-click</code> directive calls the <code>valuationDatePickerOpen</code> function on the controller which sets the <code>valuationDatePickerIsOpen</code> property to be <code>true</code> and that property happens to be bound to the <code>is-open</code> attribute. Your knee bone connected to your thigh bone, Your thigh bone connected to your hip bone... This makes sense. This works. Great.</p><p>But I want something a little prettier - I want to use the lovely calendar glyph to trigger the datepicker popup like in the docs. That should be really easy right? I just tweak the HTML to add a calendar button and the associated <code>ng-click=&quot;vm.valuationDatePickerOpen()&quot;</code>:</p><pre><code class="language-html">&lt;div ng-app=&quot;peskyDatepicker&quot;&gt;
  &lt;div ng-controller=&quot;DatepickerDemoCtrl as vm&quot;&gt;
    &lt;p class=&quot;input-group&quot;&gt;
      &lt;input
        type=&quot;text&quot;
        class=&quot;form-control&quot;
        datepicker-popup=&quot;mediumDate&quot;
        is-open=&quot;vm.valuationDatePickerIsOpen&quot;
        ng-click=&quot;vm.valuationDatePickerOpen()&quot;
        ng-model=&quot;vm.valuationDate&quot;
      /&gt;
      &lt;span class=&quot;input-group-btn&quot;&gt;
        &lt;button
          type=&quot;button&quot;
          class=&quot;btn btn-default&quot;
          ng-click=&quot;vm.valuationDatePickerOpen()&quot;
        &gt;
          &lt;i class=&quot;glyphicon glyphicon-calendar&quot;&gt;&lt;/i&gt;
        &lt;/button&gt;
      &lt;/span&gt;
    &lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
</code></pre><p>Miraculously, this <em>doesn&#x27;t</em> work. Which is strange - I mean it ought to... The same <code>ng-click</code> directive is sat on our new calendar button as is in place on the datepicker itself. So what&#x27;s happening? Well let&#x27;s do some investigation. If you take a look at the docs you&#x27;ll see that their example with the calendar glyph is subtly different to our own. Namely, when the opener function is invoked, the official docs pass along <code>$event</code>. To what end? Well, the docs opener function does something that our own does not. This:</p><pre><code class="language-js">$scope.open = function ($event) {
  $event.preventDefault();
  $event.stopPropagation();

  $scope.opened = true;
};
</code></pre><p>Ignore all the <code>$scope</code> malarkey - I want you to pay attention to what is happening with <code>$event</code>. <code>preventDefault</code> and <code>stopPropogation</code> are being called. This is probably relevant.</p><p>I decided to do a little experimentation. I created a Plunk which demonstrates the datepicker and uses <code>$watch</code> to track what happens to <code>valuationDatePickerIsOpen</code>. The Plunk featured 2 calendar glyphs - the left one doesn&#x27;t pass along <code>$event</code> to <code>valuationDatePickerOpen</code> when it is clicked and the right one does. When <code>$event</code> is passed we call <code>preventDefault</code> and <code>stopPropogation</code>.</p><iframe src="https://embed.plnkr.co/dJyF531w0QRGiAScRf15/preview" width="100%" height="450"></iframe><p>After a little experimentation of my own I discovered that calling <code>$event.stopPropogation()</code> is the magic bullet. Without that in place <code>valuationDatePickerIsOpen</code> gets set to <code>true</code> and then immediately back to <code>false</code> again. I do not know why. There may be an entirely sane reason for this - if so then please do post a comment and let me know. It wouldn&#x27;t hurt for the Angular UI Bootstrap Datepicker docs to mention this. <a href="https://github.com/angular-ui/bootstrap/issues/3705">Perhaps it&#x27;s time to submit a PR....</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[NgValidationFor Baby Steps]]></title>
            <link>https://blog.johnnyreilly.com/2015/05/11/ngvalidationfor-baby-steps</link>
            <guid>NgValidationFor Baby Steps</guid>
            <pubDate>Mon, 11 May 2015 00:00:00 GMT</pubDate>
            <description><![CDATA[I thought as I start the NgValidationFor project I'd journal my progress. I'm writing this with someone particular in mind welcome!]]></description>
            <content:encoded><![CDATA[<p>I thought as I start the <a href="http://blog.icanmakethiswork.io/2015/04/tonight-ill-start-open-source-project.html">NgValidationFor project</a> I&#x27;d journal my progress. I&#x27;m writing this with someone particular in mind: me. Specifically, me in 2 years who will no doubt wonder why I made some of the choices I did. Everyone else, move along now - nothing to see. Unless the inner workings of someone else&#x27;s mind are interesting to you... In which case: welcome!</p><h2>Getting up and running</h2><p>I&#x27;ve got a project on <a href="https://github.com/johnnyreilly/NgValidationFor">GitHub</a> and I&#x27;m starting to think about implementations. One thing that bit me on <a href="http://johnnyreilly.github.io/jQuery.Validation.Unobtrusive.Native/">jVUN</a> was being tied to a specific version of ASP.Net MVC. For each major release of ASP.Net MVC I needed separate builds / NuGet packages and the like. A pain. Particularly when it came to bug fixes for prior versions - the breaking changes with each version of MVC meant far more work was required when it came to shipping fixes for MVC 4 / MVC 3.</p><p>So with that in mind I&#x27;m going to try and limit my dependencies. I&#x27;m not saying I will never depend upon ASP.Net MVC - I may if I think it becomes useful to give the users a nicer API or if there&#x27;s another compelling reason. But to start with I&#x27;m just going to focus on the translation of data annotations to Angular validation directive attributes.</p><p>To that end I&#x27;m going to begin with just a class library and an associated test project. I&#x27;m going to try and minimise the dependencies that NgValidationFor has. At least initially I may even see if I can sensibly avoid depending on <code>System.Web</code> (mindful of the upcoming ASP.Net 5 changes). Let&#x27;s see.</p><p>A little time passes.......</p><h2>So what have we got?</h2><p>My first efforts have resulted in the implementation of the <code>&lt;a href=&quot;https://msdn.microsoft.com/en-us/library/system.componentmodel.dataannotations.requiredattribute(v=vs.110).aspx&quot;&gt;RequiredAttribute&lt;/a&gt;</code>. This is the <a href="https://github.com/johnnyreilly/NgValidationFor/tree/6cf862a7638d3ed933cd0e075a1807b1414847da">code right now</a>. It&#x27;s made up of:</p><ol><li>NgValidationFor.Core - the core part of the project which converts data annotations into AngularJS 1.x validation directive attributes.</li><li>NgValidationFor.Core.UnitTests - the unit tests for the core</li><li>NgValidationFor.Documentation - this is an ASP.Net MVC project which will become a documentation site for NgValidationFor. It also doubles as a way for me to try out NgValidationFor.</li><li>NgValidationFor.Documentation.UnitTests - unit tests for the documentation (there&#x27;s none yet as I&#x27;m still spiking - but when I&#x27;m a little clearer, they will be)</li></ol><p>How can it be used? Well fairly easily. Take this simple model:</p><pre><code class="language-cs">using System.ComponentModel.DataAnnotations;

namespace NgValidationFor.Documentation.Models
{
    public class RequiredDemoModel
    {
        [Required]
        public string RequiredField { get; set; }
    }
}
</code></pre><p>When used in an MVC View for which <code>RequiredDemoModel</code> is the Model, NgValiditionFor can be used thusly:</p><pre><code class="language-html">@using NgValidationFor.Core @using NgValidationFor.Documentation.Models @model
RequiredDemoModel &lt;input type=&quot;text&quot; name=&quot;userName&quot; ng-model=&quot;user.name&quot;
@Html.Raw(Model.GetAttributes(x =&gt; Model.RequiredField)) &gt;
</code></pre><p>Which results in this HTML:</p><pre><code class="language-html">&lt;input type=&quot;text&quot; name=&quot;userName&quot; ng-model=&quot;user.name&quot; required=&quot;required&quot; /&gt;
</code></pre><p>Tada!!!! It works.</p><h2>So what now?</h2><p>Yes it works, but I&#x27;m not going to pretend it&#x27;s pretty. I don&#x27;t like having to wrap the usage of NgValidationFor with <code>Html.Raw(...)</code>. I&#x27;m having to do that because <code>GetAttributes</code> returns a <code>string</code>. This string is then HTML encoded by MVC. To avoid my quotation marks turning into <code>&amp;amp;quot;</code> I need to actually be exposing an <code>&lt;a href=&quot;https://msdn.microsoft.com/en-us/library/system.web.ihtmlstring(v=vs.110).aspx&quot;&gt;IHtmlString&lt;/a&gt;</code>. So I&#x27;m going to need to depend upon <code>System.Web</code>. That&#x27;s not so bad - at least I&#x27;m not tied to a specific MVC version.</p><p>I&#x27;m not too keen on the implementation I&#x27;ve come up with for NgValidationFor either. It&#x27;s a single static method at the minute which does everything. It breaks the <a href="https://en.wikipedia.org/wiki/Single_responsibility_principle">Single Responsibility Priniciple</a> and the <a href="https://en.wikipedia.org/wiki/Open/closed_principle">Open/Closed Principle</a>. I need to take a look at that - I want people to be able to extend this and I need to think about a good and simple way to achieve that.</p><p>Finally, usage. <code>Model.GetAttributes(x =&amp;gt; Model.RequiredField)</code> feels wrong to me. I think I&#x27;m happy with having this used as an extension method but it needs to be clearer what&#x27;s happening. Perhaps <code>Model.NgValidationFor(x =&amp;gt; Model.RequiredField)</code> would be better. I need to try a few things out and come up with a nicer way to use NgValidationFor.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A tale of Angular, html5mode, ASP.Net MVC and ASP.Net Web API]]></title>
            <link>https://blog.johnnyreilly.com/2015/05/05/a-tale-of-angular-html5mode-aspnet-mvc</link>
            <guid>A tale of Angular, html5mode, ASP.Net MVC and ASP.Net Web API</guid>
            <pubDate>Tue, 05 May 2015 00:00:00 GMT</pubDate>
            <description><![CDATA[So. You want to kick hash based routing to the kerb. You want real URLs. You've read the HTML5 mode section of the Angular $location docs and you're good to go. It's just a matter of dropping $locationProvider.html5Mode(true) into your app initialisation right?]]></description>
            <content:encoded><![CDATA[<p>So. You want to kick hash based routing to the kerb. You want <em>real</em> URLs. You&#x27;ve read the HTML5 mode section of the <a href="https://docs.angularjs.org/guide/$location">Angular $location docs</a> and you&#x27;re good to go. It&#x27;s just a matter of dropping <code>$locationProvider.html5Mode(true)</code> into your app initialisation right?</p><p>Wrong.</p><p>You want your URLs to be shareable. If, when you copy the URL out of your browser and send it someone else, they do not get taken to the same position in the application as you do then I&#x27;ve got news for you: THAT&#x27;S NOT REALLY A URL. And just using <code>$locationProvider.html5Mode(true)</code> has done nothing useful for you. You want to ensure that, if the URL entered in the browser does not relate to a specific server-side end-point, the self-same HTML root page is <em>always</em> served up. Then Angular can load the correct resources for the URL you have entered and get you to the required state.</p><p>There are tips to be found in Angular UI&#x27;s <a href="https://github.com/angular-ui/ui-router/wiki/Frequently-Asked-Questions#how-to-configure-your-server-to-work-with-html5mode">How to: Configure your server to work with html5Mode</a> doc. However they required a little extra fiddling to get my ASP.Net back end working quite as I wanted. To save you pain, here are my cultural learnings.</p><h2>ASP.Net MVC</h2><p>I had an ASP.Net MVC app which I wanted to use <code>html5mode</code> with. To do this is simply a matter of tweaking your <code>RouteConfig.cs</code> like so:</p><pre><code class="language-cs">public class RouteConfig
    {
        public static void RegisterRoutes(RouteCollection routes)
        {
            routes.IgnoreRoute(&quot;{resource}.axd/{*pathInfo}&quot;);

            // Here go the routes that you still want to be able to hit
            routes.MapRoute(
                name: &quot;IAmARouteThatYouStillWantToHit&quot;,
                url: &quot;ThatsWhyIAmRegisteredFirst&quot;,
                defaults: new { controller = &quot;Hittable&quot;, action = &quot;Index&quot; }
            );

            // Everything else will hit Home/Index which serves up the root angular app page
            routes.MapRoute(
                name: &quot;Default&quot;,
                url: &quot;{*anything}&quot;, // THIS IS THE MAGIC!!!!
                defaults: new { controller = &quot;Home&quot;, action = &quot;Index&quot; }
            );
        }
</code></pre><p>With this in place my existing routes work just as I would hope. Any route that doesn&#x27;t fit that registered can be assumed to be <code>html5mode</code> related and will serve up the root angular app page as I&#x27;d hope.</p><h2>ASP.Net Web API</h2><p>Later I realised that the app in question was mostly static content. Certainly the root angular app page was and so it seemed wasteful to require an ASP.Net MVC controller to serve up that static content. So I stripped out MVC from the app entirely, choosing to serve raw HTML instead. For the dynamic parts I switched to using Web API. This was &quot;hittable&quot; as long as I had my <code>WebApiConfig.cs</code> and my <code>system.webServer</code> section in my <code>web.config</code> lined up correctly, viz:</p><pre><code class="language-cs">public static class WebApiConfig
    {
        public static void Register(HttpConfiguration config)
        {
            // Web API routes
            config.MapHttpAttributeRoutes();

            config.Routes.MapHttpRoute(
                name: &quot;DefaultApi&quot;,
                routeTemplate: &quot;api/{controller}/{id}&quot;,
                defaults: new { id = RouteParameter.Optional }
            );

            // other stuff
        }
    }
</code></pre><pre><code class="language-xml">&lt;configuration&gt;

    &lt;system.webServer&gt;

        &lt;defaultDocument&gt;
            &lt;files&gt;
                &lt;clear /&gt;
                &lt;add value=&quot;build/index.html&quot; /&gt; &lt;!-- This is the root document for the Angular app --&gt;
            &lt;/files&gt;
        &lt;/defaultDocument&gt;

        &lt;rewrite&gt;
            &lt;rules&gt;
                &lt;rule name=&quot;Main Rule&quot; stopProcessing=&quot;true&quot;&gt;
                    &lt;match url=&quot;.*&quot; /&gt;
                    &lt;conditions logicalGrouping=&quot;MatchAll&quot;&gt;
                        &lt;!-- Allows &quot;api/&quot; prefixed URLs to still hit Web API controllers
                             as defined in WebApiConfig --&gt;
                        &lt;add input=&quot;{REQUEST_URI}&quot; pattern=&quot;api/&quot; ignoreCase=&quot;true&quot; negate=&quot;true&quot; /&gt;

                        &lt;!-- Static files and directories can be served so partials etc can be loaded --&gt;
                        &lt;add input=&quot;{REQUEST_FILENAME}&quot; matchType=&quot;IsFile&quot; negate=&quot;true&quot; /&gt;
                        &lt;add input=&quot;{REQUEST_FILENAME}&quot; matchType=&quot;IsDirectory&quot; negate=&quot;true&quot; /&gt;
                    &lt;/conditions&gt;
                    &lt;action type=&quot;Rewrite&quot; url=&quot;/&quot; /&gt;
                &lt;/rule&gt;
            &lt;/rules&gt;
        &lt;/rewrite&gt;

    &lt;/system.webServer&gt;

&lt;/configuration&gt;
</code></pre><p>With this in place I can happily hit &quot;api&quot; prefixed URLs and still land on my Web API controllers whilst other URLs will serve up the root angular app page. Lovely.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Tonight I'll Start an Open Source Project...]]></title>
            <link>https://blog.johnnyreilly.com/2015/04/24/tonight-ill-start-open-source-project</link>
            <guid>Tonight I'll Start an Open Source Project...</guid>
            <pubDate>Fri, 24 Apr 2015 00:00:00 GMT</pubDate>
            <description><![CDATA[Further posts on this topic]]></description>
            <content:encoded><![CDATA[<h3>Further posts on this topic</h3><ul><li><a href="https://blog.johnnyreilly.com/2015/05/ngvalidationfor-baby-steps.html">NgValidationFor Baby Steps</a></li></ul><p>I&#x27;m excited. Are you? I&#x27;m babysitting for a friend, I&#x27;ve my laptop, time to kill and (crucially) an idea...</p><h2>The Idea</h2><p>You&#x27;re likely aware of the various form element directives that AngularJS offers. For instance the <a href="https://docs.angularjs.org/api/ng/directive/input">input directive</a>:</p><blockquote><p>HTML input element control. When used together with ngModel, it provides data-binding, input state control, and <em>validation</em>.</p></blockquote><p>You&#x27;ll notice that I emphasised the word &quot;validation&quot; there. That&#x27;s important - that&#x27;s my idea. I&#x27;m using AngularJS to build SPA&#x27;s and for the server side I&#x27;m using ASP.Net MVC / Web API. Crucially, my templates are actually ASP.Net MVC Partial Views. That&#x27;s key.</p><p>When I send data back from my SPA back to the server it gets unmarshalled / deserialized into a C# class (view model) of some kind. When data goes the other way it&#x27;s flowing back from a JSON&#x27;d view model and being used by my Angular code.</p><p>Now historically if I was building a fairly vanilla MVC app then I&#x27;d be making use of all the <code>TextboxFor</code> extension methods etc to generate my input elements. For example, with a view model like this:</p><pre><code class="language-cs">using System.ComponentModel.DataAnnotations;

namespace App.ViewModels
{
 public class RequiredModel
 {
  [Required]
  public string RequiredField{ get; set; }
 }
}
</code></pre><p>I&#x27;d have a view like this:</p><pre><code class="language-html">@model App.ViewModels.RequiredModel @using (Html.BeginForm()) {
&lt;div class=&quot;row&quot;&gt;
  @Html.LabelFor(x =&gt; x.TextBox, &quot;Something must be entered:&quot;)
  @Html.TextBoxFor(x =&gt; x.TextBox, true)
&lt;/div&gt;
}
</code></pre><p>And that would generate HTML like this:</p><pre><code class="language-html">&lt;form action=&quot;/Demo/Required&quot; method=&quot;post&quot;&gt;
  &lt;div class=&quot;row&quot;&gt;
    &lt;label for=&quot;TextBox&quot;&gt;Something must be entered:&lt;/label&gt;
    &lt;input
      data-msg-required=&quot;The TextBox field is required.&quot;
      data-rule-required=&quot;true&quot;
      id=&quot;TextBox&quot;
      name=&quot;TextBox&quot;
      type=&quot;text&quot;
      value=&quot;&quot;
    /&gt;
  &lt;/div&gt;
&lt;/form&gt;
</code></pre><p>If you look at the HTML you&#x27;ll see that the <code>Required</code> data annotations have been propogated into the HTML in the HTML in the form of <code>data-rule-*</code> and <code>data-msg-*</code> attributes. The code above is built using my <a href="http://johnnyreilly.github.io/jQuery.Validation.Unobtrusive.Native/">jQuery.Validation.Unobtrusive.Native project</a> which in turn was inspired by / based upon the <a href="http://bradwilson.typepad.com/blog/2010/10/mvc3-unobtrusive-validation.html">Unobtrusive Client Validation in ASP.NET MVC</a>. That&#x27;s right - I&#x27;ve done this before - or at least something quite like it.</p><p>There&#x27;s clearly a strong crossover between AngularJS&#x27;s input directive parameters and unobtrusive client validation. I&#x27;m planning to take the principles (and maybe some of the code) that I used on that project and see if I can&#x27;t make something useful with it here. <a href="https://blog.johnnyreilly.com/2014/08/angularjs-meet-aspnet-server-validation.html">Server side validation is jolly important</a> but I can probably save a few compute cycles on the server by making use of client side validation as well. If I&#x27;m right then I should able to come up with a mechanism that saves me from manually duplicating my server validation on the client.</p><h2>The Aim</h2><p>I want to be able to use HTML Helpers to propogate validation metadata from the server view models into angular form validation directive attributes. Quite a mouthful I know. What does that actually mean? Well I&#x27;ve got 2 ideas. Possibly I want to be able to code something like this:</p><pre><code class="language-html">@model App.ViewModels.RequiredModel @using (Html.BeginForm()) {
&lt;div class=&quot;row&quot;&gt;
  @Html.LabelFor(x =&gt; x.TextBox, &quot;Something must be entered:&quot;)
  @Html.NgTextBoxFor(x =&gt; x.TextBox)
&lt;/div&gt;
}
</code></pre><p>And have HTML like this generated:</p><pre><code class="language-html">&lt;form action=&quot;/Demo/Required&quot; method=&quot;post&quot;&gt;
  &lt;div class=&quot;row&quot;&gt;
    &lt;label for=&quot;TextBox&quot;&gt;Something must be entered:&lt;/label&gt;
    &lt;input
      ng-required=&quot;true&quot;
      id=&quot;TextBox&quot;
      name=&quot;TextBox&quot;
      type=&quot;text&quot;
      value=&quot;&quot;
    /&gt;
  &lt;/div&gt;
&lt;/form&gt;
</code></pre><p>The reservation I have about this approach is that it rather takes you away from the HTML. Yes it works (and to your seasoned MVC-er it will feel quite natural in some ways) but it feels rather heavy handed. But I&#x27;d like what I&#x27;m building to be easy for users to plug into existing code without a ton of rework. So, the other idea I&#x27;m toying with is having HTML helpers that just return a string of attributes. So if I had an angular form that looked like this:</p><pre><code class="language-html">&lt;div ng-controller=&quot;ExampleController&quot;&gt;
  &lt;form&gt;
    &lt;div class=&quot;row&quot;&gt;
      &lt;label
        &gt;Something must be entered:
        &lt;input name=&quot;RequiredField&quot; type=&quot;text&quot; value=&quot;&quot; /&gt;
      &lt;/label&gt;
    &lt;/div&gt;
  &lt;/form&gt;
&lt;/div&gt;
</code></pre><p>I could tweak it to push in the validation directive attributes like this:</p><pre><code class="language-html">@model App.ViewModels.RequiredModel
&lt;div ng-controller=&quot;ExampleController&quot;&gt;
  &lt;form&gt;
    &lt;div class=&quot;row&quot;&gt;
      &lt;label
        &gt;Something must be entered: &lt;input name=&quot;RequiredField&quot; type=&quot;text&quot;
        value=&quot;&quot; @Html.NgValidationFor(x =&gt; x.RequiredField) /&gt;
      &lt;/label&gt;
    &lt;/div&gt;
  &lt;/form&gt;
&lt;/div&gt;
</code></pre><p>And end up with HTML like this:</p><pre><code class="language-html">&lt;div ng-controller=&quot;ExampleController&quot;&gt;
  &lt;form&gt;
    &lt;div class=&quot;row&quot;&gt;
      &lt;label
        &gt;Something must be entered:
        &lt;input name=&quot;RequiredField&quot; type=&quot;text&quot; value=&quot;&quot; ng-required=&quot;true&quot; /&gt;
      &lt;/label&gt;
    &lt;/div&gt;
  &lt;/form&gt;
&lt;/div&gt;
</code></pre><p>This is a simplified example of course - it&#x27;s likely that any number of validation directive attributes might be returned from <code>NgValidationFor</code>. And crucially if these attributes were changed on the server view model then the validation changes would automatically end up in the client HTML with this approach.</p><h2>The Approach</h2><p>At least to start off with I&#x27;m going to aim at creating the second of my approaches. I may come back and implement the first at some point but I think the second is a better place to start.</p><p>I&#x27;m kind of surprised no-one else has built this already actually - but I&#x27;m not aware of anything. I&#x27;ve had a little duckduckgo around and found no takers. The closest I&#x27;ve come is the excellent <a href="http://www.breezejs.com/sites/all/apidocs/classes/Validator.html">BreezeJS</a>. BreezeJS does way more than I want it to - I&#x27;m planning to restrict the scope of this project to simply turning data annotations on my ASP.Net MVC server models into <code>ng-*</code> directive attributes in HTML. That&#x27;s it.</p><p>So, general housekeeping.... I&#x27;m going to host this project on <a href="http://www.github.com">GitHub</a>, I&#x27;m going to have Continuous Integration with <a href="http://www.appveyor.com/">AppVeyor</a> and I&#x27;m planning to publish this via <a href="http://www.nuget.org/">NuGet</a> (when and if I&#x27;ve created something useful).</p><p>I just need a name and I&#x27;ll begin. What shall I call it? Some options:</p><ul><li>Angular ASP.Net MVC Extensions</li><li>angular-aspnet-mvc-extensions</li><li>Angular MVC Element Extensions</li><li>Angular Validation Html Helpers</li><li>NgValidationFor (the name of the HTML helper I made up)</li></ul><p>Hmmmm.... None of them is particularly lighting my fire. The first four are all a bit <a href="https://en.wikipedia.org/wiki/Ronseal">RonSeal</a> <!-- -->-<!-- --> which is fine.... Ug. The last one... It&#x27;s a bit more pithy. Okay - I&#x27;ll go with &quot;NgValidationFor&quot; at least for now. If something better occurs I can always change my mind.</p><p><a href="https://github.com/johnnyreilly/NgValidationFor">And we&#x27;re off!</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How to activate your emoji keyboard on Android 5.0 (Lollipop)]]></title>
            <link>https://blog.johnnyreilly.com/2015/04/17/how-to-activate-your-emoji-keyboard-on</link>
            <guid>How to activate your emoji keyboard on Android 5.0 (Lollipop)</guid>
            <pubDate>Fri, 17 Apr 2015 00:00:00 GMT</pubDate>
            <description><![CDATA[A departure from from my normal content - I need to tell you about emoji! You'll probably already know about them - just imagine a emoticon but about 300,000 times better. They really add spice to to textual content. Oh and they're Japanese - which is also way cool.]]></description>
            <content:encoded><![CDATA[<p>A departure from from my normal content - I need to tell you about <a href="http://en.wikipedia.org/wiki/Emoji">emoji</a>! You&#x27;ll probably already know about them - just imagine a emoticon but about 300,000 times better. They really add spice to to textual content. Oh and they&#x27;re Japanese - which is also way cool.</p><p>Since I&#x27;ve discovered emoji I&#x27;ve felt a pressing need to have them on my (Android) phone. This is harder than you might imagine. But totally do-able.... Here&#x27;s how you get the emoji love on your Android Lollipop phone:</p><ul><li><p>goto settings (the cog)</p></li><li><p>select &quot;Language and Input&quot;</p></li><li><p>select your &quot;Current keyboard&quot; and then select the &quot;Choose keyboards&quot; option</p></li><li><p>look for a keyboard that says &quot;iWnn IME Japanese&quot;. Select it</p></li><li><p>drop back to the &quot;Language and Input&quot; menu where you will see &quot;iWnn IME Japanese&quot; is now there.</p></li><li><p>select it and deactivate &quot;Japanese&quot; and activate &quot;Emoji&quot; like this: <a href="https://4.bp.blogspot.com/-toFgqIFcTs4/VTC6JXxwmtI/AAAAAAAAA0s/OT7O7MdGvSc/s1600/Screenshot_2015-04-16-07-21-06-741564.png"><img src="https://4.bp.blogspot.com/-toFgqIFcTs4/VTC6JXxwmtI/AAAAAAAAA0s/OT7O7MdGvSc/s320/Screenshot_2015-04-16-07-21-06-741564.png"/></a></p></li><li><p>now you should find your keyboard contains a little globe icon. When you select it.... Emoji!!!! <a href="https://2.bp.blogspot.com/-xtdHdGuc6YU/VTC6I-_43tI/AAAAAAAAA0g/JnlckIUnS48/s1600/Screenshot_2015-04-16-07-23-56%257E2-739197.jpg"><img src="https://2.bp.blogspot.com/-xtdHdGuc6YU/VTC6I-_43tI/AAAAAAAAA0g/JnlckIUnS48/s320/Screenshot_2015-04-16-07-23-56%257E2-739197.jpg"/></a></p></li></ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[PartialView.ToString()]]></title>
            <link>https://blog.johnnyreilly.com/2015/03/20/partialview-tostring</link>
            <guid>PartialView.ToString()</guid>
            <pubDate>Fri, 20 Mar 2015 00:00:00 GMT</pubDate>
            <description><![CDATA[In the name of DRY I found myself puzzling how one could take a PartialViewResult and render it as a string. Simple, right?]]></description>
            <content:encoded><![CDATA[<p>In the name of <a href="https://en.wikipedia.org/wiki/Don%27t_repeat_yourself">DRY</a> I found myself puzzling how one could take a <code>PartialViewResult</code> and render it as a <code>string</code>. Simple, right?</p><p>In fact, in my head this was already a solved problem. I mean I&#x27;ve written about this <a href="http://blog.icanmakethiswork.io/2012/07/rendering-partial-view-to-string.html">before</a> already! Except I haven&#x27;t. Not really - what I did back then was link to what someone else had written and say &quot;yay! well done chap - like he said!&quot;. It turns out that was a bad move. That blog appears to be gone and so I&#x27;m back to where I was. Ug. Lesson learned.</p><h2>What are we trying to do?</h2><p>So, for the second time of asking, here is how to take a <code>PartialViewResult</code> and turn it into a <code>string</code>. It&#x27;s an invaluable technique to deal with certain scenarios.</p><p>In my own case I have a toolbar in my application that is first pushed into the UI in my <code>_Layout.cshtml</code> by means of a trusty <code>@Html.Action(&quot;Toolbar&quot;)</code>. I wanted to be able to re-use the <code>PartialViewResult</code> returned by <code>Toolbar</code> on my controller inside a <code>JSON</code> payload. And despite the title of this post, <code>PartialView.ToString()</code><em>doesn&#x27;t</em> quite cut the mustard. Obvious really, if it did then why would I be writing this and you be reading this?</p><p>The solution is actually fairly simple. And, purely for swank, I&#x27;m going to offer it you 3 ways. Whatever&#x27;s your poison.</p><h2>Inheritance (it&#x27;s so yesterday darling)</h2><p>Yes there was a time when everything was inheritance based. You were rewarded handsomely for making sure that was the case. However, times have changed and (with good reason) people tend to <a href="https://en.wikipedia.org/wiki/Composition_over_inheritance">favour composition over inheritance</a>. So, perhaps just for the memories, let first offer you the inheritance based approach:</p><pre><code class="language-cs">protected string ConvertPartialViewToString(PartialViewResult partialView)
{
  using (var sw = new StringWriter())
  {
    partialView.View = ViewEngines.Engines
      .FindPartialView(ControllerContext, partialView.ViewName).View;

    var vc = new ViewContext(
      ControllerContext, partialView.View, partialView.ViewData, partialView.TempData, sw);
    partialView.View.Render(vc, sw);

    var partialViewString = sw.GetStringBuilder().ToString();

    return partialViewString;
  }
}
</code></pre><p>The idea being that the above method is placed onto a base controller which your controllers subclass. Thus using this method inside one of the controllers is as simple as:</p><pre><code class="language-cs">var toolbarHtml = ConvertPartialViewToString(partialViewResult);
</code></pre><h2>Extension method (sexier syntax)</h2><p>So the next choice is implementing this as an extension method. Here&#x27;s my static class which adds <code>ConvertToString</code> onto <code>PartialViewResult</code>:</p><pre><code class="language-cs">using System.IO;
using System.Web.Mvc;

namespace My.Utilities.Extensions
{
  public static class PartialViewResultExtensions
  {
    public static string ConvertToString(this PartialViewResult partialView,
                                              ControllerContext controllerContext)
    {
      using (var sw = new StringWriter())
      {
        partialView.View = ViewEngines.Engines
          .FindPartialView(controllerContext, partialView.ViewName).View;

        var vc = new ViewContext(
          controllerContext, partialView.View, partialView.ViewData, partialView.TempData, sw);
        partialView.View.Render(vc, sw);

        var partialViewString = sw.GetStringBuilder().ToString();

        return partialViewString;
      }
    }
  }
}
</code></pre><p>I don&#x27;t know about you but I do love an extension method - it often makes for much more readable code. In this case we can use:</p><pre><code class="language-cs">var toolbarHtml = partialViewResult.ConvertToString(ControllerContext);
</code></pre><p>Which I think we can all agree is really rather lovely. Perhaps it would be more lovely if I didn&#x27;t have to pass <code>ControllerContext</code> <!-- -->-<!-- --> but hey! Still quite nice.</p><h2>Favouring Composition over Inheritance (testable)</h2><p>Although ASP.Net MVC was designed to be testable there are times when you think &quot;really? Can it be that hard?&quot;. In fact for a well thought through discussion on the topic I advise you read <a href="http://volaresystems.com/blog/post/2010/08/19/Dont-mock-HttpContext">this</a>. (I&#x27;m aware of the irony implicit in linking to another blog post in a blog post that I only wrote because I first linked to another blog which vanished.... Infinite recursion anybody?)</p><p>The conclusion of the linked blog post is twofold</p><ol><li>Don&#x27;t mock HTTPContext</li><li>Use the <a href="https://en.wikipedia.org/wiki/Facade_pattern">facade pattern</a> instead</li></ol><p>Having testable code is not a optional bauble in my view - it&#x27;s a necessity. So with my final approach that&#x27;s exactly what I&#x27;ll do.</p><pre><code class="language-cs">using System.Web.Mvc;

namespace My.Interfaces
{
  public interface IMvcInternals
  {
    string ConvertPartialViewToString(PartialViewResult partialView, ControllerContext controllerContext);
  }
}

// ....

using System.IO;
using System.Web.Mvc;
using My.Interfaces;

namespace My.Utilities
{
  public class MvcInternals : IMvcInternals
  {
    public string ConvertPartialViewToString(PartialViewResult partialView,
                                             ControllerContext controllerContext)
    {
      using (var sw = new StringWriter())
      {
        partialView.View = ViewEngines.Engines
          .FindPartialView(controllerContext, partialView.ViewName).View;

        var vc = new ViewContext(
          controllerContext, partialView.View, partialView.ViewData, partialView.TempData, sw);
        partialView.View.Render(vc, sw);

        var partialViewString = sw.GetStringBuilder().ToString();

        return partialViewString;
      }
    }
  }
}
</code></pre><p>So here I have a simple interface with a <code>ConvertPartialViewToString</code> method on it. This interface can be passed into a controller and then used like this:</p><pre><code class="language-cs">var toolbarHtml = _mvcInternals.ConvertPartialViewToString(partialViewResult, ControllerContext);
</code></pre><p>Ah... that&#x27;s the sweet mellifluous sound of easily testable code.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Hey tsconfig.json, where have you been all my life?]]></title>
            <link>https://blog.johnnyreilly.com/2015/02/27/hey-tsconfigjson-where-have-you-been</link>
            <guid>Hey tsconfig.json, where have you been all my life?</guid>
            <pubDate>Fri, 27 Feb 2015 00:00:00 GMT</pubDate>
            <description><![CDATA[Sometimes, you just miss things. Something seismic happens and you had no idea. So it was with tsconfig.json.]]></description>
            <content:encoded><![CDATA[<p>Sometimes, you just miss things. Something seismic happens and you had no idea. So it was with <code>tsconfig.json</code>.</p><p>This blog post started life with the name &quot;TypeScript: Some IDEs are more equal than others&quot;. I&#x27;d intended to use it summarise a discussion on the <a href="https://github.com/Microsoft/TypeScript/issues/1066">TypeScript GitHub repo</a> about implicit referencing including a fist shaken at the sky at the injustice of it all. But whilst I was writing it I dicovered things had changed without my knowledge. That&#x27;s a rather wonderful thing.</p><h2>Implicit Referencing</h2><p>Implicit referencing, if you&#x27;re not aware, is the thing that separates Visual Studio from all other IDEs / text editors. Implicit referencing means that in Visual Studio you don&#x27;t need to make use of comments at the head of each TypeScript file in order to tell the compiler where it can find the related TypeScript files.</p><p>The <code>reference</code> comments aren&#x27;t necessary when using Visual Studio because the VS project file is used to drive the files passed to the TypeScript compiler (tsc).</p><p>The upshot of this is that, at time of writing, you can generally look at a TypeScript codebase and tell whether it was written using Visual Studio by opening it up a file at random and eyeballing for something like this at the top:</p><pre><code class="language-ts">/// &lt;reference path=&quot;other-file.ts&quot; /&gt;
</code></pre><p><em>&quot;A-ha! They&#x27;re using &quot;reference&quot; comments Watson. From this I deduce that the individuals in question are using the internal module approach and using Visual Studio as their IDE. Elementary, my dear fellow, quite elementary.&quot;</em></p><p>This has important implications. Important I tell you, yes important! Well, important if you want to reduce the barriers between Visual Studio and everyone else. And I do. Whilst I love Visual Studio - it&#x27;s been my daily workhorse for many years - I also love stepping away from it and using something more stripped down. I also like working with other people without mandating that they need to use Visual Studio as well. In the words of Rodney King, &quot;can&#x27;t we all get along?&quot;.</p><h2>Cross-IDE TypeScript projects</h2><p>I feel I should be clear - you can already set up TypeScript projects to work regardless of IDE. But there&#x27;s friction. It&#x27;s not clear cut. You can see a full on discussion around this <a href="https://github.com/Microsoft/TypeScript/issues/1066">here</a> but in the end it comes down to making a choice between these 3 options:</p><ol><li>Set <div>false</div> in a project file. <a href="https://github.com/Microsoft/TypeScript/issues/1066#issuecomment-63727612">This flag effectively deactivates implicit referencing.</a> This approach requires that all developers (regardless of IDE) use <code>/// &amp;lt;reference</code>s to build context. Compiler options in VS can be controlled using the project file as is.</li><li>Using Visual Studio without any csproj tweaks. This approach requires that all files will need <code>/// &amp;lt;reference</code>s at their heads in order to build compilation context <em>outside</em> of Visual Studio. It&#x27;s possible that <code>/// &amp;lt;reference</code>s and the csproj could get out of line - care is required to avoid this. Compiler options in VS can be controlled using the project file as is.</li><li>Using just files in Visual Studio with <code>/// &amp;lt;reference</code>s to build compilation context. This scenario also requires that all developers (regardless of IDE) use <code>/// &amp;lt;reference</code>s to build context. In Visual Studio there will be no control over compiler options.</li></ol><p>As you can see - this is sub-optimal. But don&#x27;t worry - there&#x27;s a new sheriff in town....</p><h2><code>tsconfig.json</code></h2><p>I&#x27;d decided to give <a href="https://github.com/TypeStrong/atom-typescript">Atom TypeScript plugin</a> a go as I heard much enthusiastic noise about it. I fired it up and pointed it at a a TypeScript AngularJS project built in Visual Studio. I was mentally preparing myself for the job of adding all the /// references in when I suddenly noticed a file blinking at me: <img src="../static/blog/2015-02-27-hey-tsconfigjson-where-have-you-been/Screenshot%2B2015-02-27%2B16.05.29.png"/></p><p><code>tsconfig.json</code>? What&#x27;s that? Time to read <a href="https://github.com/TypeStrong/atom-typescript#project-support">the docs</a>:</p><blockquote><p>Supported via tsconfig.json (<a href="https://github.com/TypeStrong/atom-typescript/blob/master/docs/tsconfig.md">read more</a>) which is going to be the defacto Project file format for the next versions of TypeScript.</p></blockquote><p>&quot;read more&quot;? Oh yes indeedy - I think I will &quot;read more&quot;!</p><blockquote><p>A unified project format for TypeScript (<a href="https://github.com/Microsoft/TypeScript/pull/1692">see merged PR on Microsoft/TypeScript</a>). The TypeScript compiler (1.4 and above) only cares about compilerOptions and files. We add additional features to this <a href="https://github.com/Microsoft/TypeScript/issues/1955">with the typescript team&#x27;s approval to extend the file as long as we don&#x27;t conflict:</a></p><ul><li><a href="https://github.com/TypeStrong/atom-typescript/blob/e2fa67c4715189b71430f766ed9a92d9fb3255f9/lib/main/tsconfig/tsconfig.ts#L8-L35">compilerOptions</a> similar to what you would pass on the commandline to tsc.</li><li><a href="https://github.com/TypeStrong/atom-typescript/blob/master/docs/tsconfig.md#filesglob">filesGlob</a>: To make it easier for you to just add / remove files in your project we add filesGlob which accepts an array of glob / minimatch / RegExp patterns (similar to grunt)to specify source files.</li><li><a href="https://github.com/TypeStrong/atom-typescript/blob/master/docs/tsconfig.md#format">format</a>: Code formatting options</li><li><a href="https://github.com/TypeStrong/atom-typescript/blob/master/docs/tsconfig.md#version">version</a>: The TypeScript version</li></ul></blockquote><p>That&#x27;s right folks, we don&#x27;t need <code>/// &amp;lt;reference</code>s comments anymore. In a blinding flash of light it all changes. We&#x27;re going from the dark end of the street, to the bright side of the road. <code>tsconfig.json</code> is here to ease away the pain and make it all better. Let&#x27;s enjoy it while we can.</p><p>This change should ship with TypeScript 1.5 (hopefully) for those using Visual Studio. For those using Atom TypeScript (and as of today that&#x27;s includes me) the carnival celebrations can begin now!</p><p>Thanks to <a href="https://github.com/basarat">@basarat</a> who have quoted at length and <a href="https://smellegantcode.wordpress.com/">Daniel Earwicker</a> who is the reason that I came to discover <code>tsconfig.json</code>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using Gulp to inject scripts and styles tags directly into your HTML]]></title>
            <link>https://blog.johnnyreilly.com/2015/02/17/using-gulp-in-asp-net-instead-of-web-optimization</link>
            <guid>Using Gulp to inject scripts and styles tags directly into your HTML</guid>
            <pubDate>Tue, 17 Feb 2015 00:00:00 GMT</pubDate>
            <description><![CDATA[This is very probably the dullest title for a blog post I've ever come up with. Read on though folks - it's definitely going to pick up...]]></description>
            <content:encoded><![CDATA[<p>This is very probably the dullest title for a blog post I&#x27;ve ever come up with. Read on though folks - it&#x27;s definitely going to pick up...</p><p>I <a href="https://blog.johnnyreilly.com/2014/11/using-gulp-in-visual-studio-instead-of-web-optimization.html">wrote last year</a> about my first usage of Gulp in an ASP.Net project. I used Gulp to replace the Web Optimization functionality that is due to disappear when ASP.Net v5 ships. What I came up with was an approach that provided pretty much the same functionality; raw source in debug mode, bundling + minification in release mode.</p><p>It worked by having a launch page which was straight HTML. Embedded within this page was JavaScript that would, at runtime, load the required JavaScript / CSS and inject it dynamically into the document. This approach worked but it had a number of downsides:</p><ol><li>Each time you fired up the app the following sequence of events would happen: - jQuery would load (purely there to simplify the making of various startup AJAX calls)</li></ol><ul><li>the page would make an AJAX call to the server to load various startup data, including whether the app is running in debug or release mode</li><li>Depending on the result of the startup data either the debug or release package manifest would be loaded.</li><li>For each entry in the package manifest <code>script</code> and <code>link</code> tags would be created and added to the document. These would generate further requests to the server to load the resources.</li></ul><p>Quite a lot going on here isn&#x27;t there? Accordingly, initial startup time was slower than you might hope. 2. The &quot;F&quot; word: <a href="https://en.wikipedia.org/wiki/Flash_of_unstyled_content">FOUC</a>. Flash Of Unstyled Content - whilst all the hard work of the page load was going on (before the CSS had been loaded) the page would look rather ... bare. Not a terrible thing but none too slick either. 3. The gulpfile built both the debug and the release package each time it was run. This meant the gulp task generally did double the work that it needed to do.</p><p>I wanted to see if I could tackle these issues. I&#x27;ve recently been watching <a href="https://twitter.com/John_Papa">John Papa</a>&#x27;s excellent <a href="http://www.pluralsight.com/courses/javascript-build-automation-gulpjs">Pluralsight course on Gulp</a> and picked up a number of useful tips. With that in hand let&#x27;s see what we can come up with...</p><h2>Death to dynamic loading</h2><p>The main issue with the approach I&#x27;ve been using is the dynamic loading. It makes the app slower and more complicated. So the obvious solution is to have my gulpfile inject scripts and css into the template. To that end it&#x27;s <a href="https://www.npmjs.com/package/wiredep">wiredep</a> &amp; <a href="https://www.npmjs.com/package/gulp-inject">gulp-inject</a> to the rescue!</p><p>gulp-inject (as the name suggests) is used to inject <code>script</code> and <code>link</code> tags into source code. I&#x27;m using <a href="http://bower.io/">Bower</a> as my client side package manager and so I&#x27;m going to use wiredep to determine the vendor scripts I need. It will determine what packages my app is using from looking at my <code>bower.json</code>, and give me a list of file paths in <em>dependency order</em> (which I can then pass on to gulp-inject in combination with my own app script files). This means I don&#x27;t have to think about ordering bower dependencies myself and I no longer need to separately maintain a list of these files within my gulpfile.</p><p>So, let&#x27;s get the launch page (<code>index.html</code>) ready for gulp-inject:</p><pre><code class="language-html">&lt;!DOCTYPE html&gt;
&lt;html&gt;
  &lt;head&gt;
    &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge, chrome=1&quot; /&gt;
    &lt;style&gt;
      .ng-hide {
        display: none !important;
      }
    &lt;/style&gt;
    &lt;title ng-bind=&quot;title&quot;&gt;Proverb&lt;/title&gt;
    &lt;meta charset=&quot;utf-8&quot; /&gt;
    &lt;meta
      name=&quot;viewport&quot;
      content=&quot;width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no&quot;
    /&gt;

    &lt;!-- inject:css --&gt;
    &lt;!-- endinject --&gt;

    &lt;link rel=&quot;icon&quot; type=&quot;image/png&quot; href=&quot;content/images/icon.png&quot; /&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;div&gt;
      &lt;div ng-include=&quot;&#x27;app/layout/shell.html&#x27;&quot;&gt;&lt;/div&gt;
      &lt;div id=&quot;splash-page&quot; ng-show=&quot;false&quot; class=&quot;dissolve-animation&quot;&gt;
        &lt;div class=&quot;page-splash&quot;&gt;
          &lt;div class=&quot;page-splash-message&quot;&gt;Proverb&lt;/div&gt;

          &lt;div class=&quot;progress&quot;&gt;
            &lt;div
              class=&quot;progress-bar progress-bar-striped active&quot;
              role=&quot;progressbar&quot;
              style=&quot;width: 20%;&quot;
            &gt;
              &lt;span class=&quot;sr-only&quot;&gt;loading...&lt;/span&gt;
            &lt;/div&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;

    &lt;script src=&quot;https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js&quot;&gt;&lt;/script&gt;
    &lt;script&gt;
      window.jQuery ||
        document.write(&#x27;&lt;script src=&quot;/build/jquery.min.js&quot;&gt;\x3C/script&gt;&#x27;);
    &lt;/script&gt;

    &lt;!-- inject:js --&gt;
    &lt;!-- endinject --&gt;

    &lt;script&gt;
      (function () {
        // Load startup data from the server
        $.getJSON(&#x27;api/Startup&#x27;).done(function (startUpData) {
          angularApp.start({
            thirdPartyLibs: {
              moment: window.moment,
              toastr: window.toastr,
              underscore: window._,
            },
            appConfig: startUpData,
          });
        });
      })();
    &lt;/script&gt;
  &lt;/body&gt;
&lt;/html&gt;
</code></pre><p>The important thing to notice here are the <code>&amp;lt;!-- inject:css --&amp;gt;</code> and <code>&amp;lt;!-- inject:js --&amp;gt;</code> injection placeholders. It&#x27;s here that our script and style tags will be injected into the template. You&#x27;ll notice that jQuery is <em>not</em> being injected - and that&#x27;s because I&#x27;ve opted to use a CDN for jQuery and then only fallback to serving jQuery myself if the CDN fails.</p><p>The other thing to notice here is that our launch page has become oh so much simpler in comparison with the dynamic loading approach. Which is fab.</p><p>Now before we start looking at our gulpfile I want to split out the configuration into a standalone file called gulpfile.config.js:</p><pre><code class="language-js">var tsjsmapjsSuffix = &#x27;.{ts,js.map,js}&#x27;;

var bower = &#x27;bower_components/&#x27;;
var app = &#x27;app/&#x27;;

var config = {
  base: &#x27;.&#x27;,
  buildDir: &#x27;./build/&#x27;,
  debug: &#x27;debug&#x27;,
  release: &#x27;release&#x27;,
  css: &#x27;css&#x27;,

  bootFile: app + &#x27;index.html&#x27;,
  bootjQuery: bower + &#x27;jquery/dist/jquery.min.js&#x27;,

  // The fonts we want Gulp to process
  fonts: [bower + &#x27;fontawesome/fonts/*.*&#x27;],

  images: &#x27;images/**/*.{gif,jpg,png}&#x27;,

  // The scripts we want Gulp to process
  scripts: [
    // Bootstrapping
    app + &#x27;app&#x27; + tsjsmapjsSuffix,
    app + &#x27;config.route&#x27; + tsjsmapjsSuffix,

    // common Modules
    app + &#x27;common/common&#x27; + tsjsmapjsSuffix,
    app + &#x27;common/logger&#x27; + tsjsmapjsSuffix,
    app + &#x27;common/spinner&#x27; + tsjsmapjsSuffix,

    // common.bootstrap Modules
    app + &#x27;common/bootstrap/bootstrap.dialog&#x27; + tsjsmapjsSuffix,

    // directives
    app + &#x27;directives/**/*&#x27; + tsjsmapjsSuffix,

    // services
    app + &#x27;services/**/*&#x27; + tsjsmapjsSuffix,

    // controllers
    app + &#x27;about/**/*&#x27; + tsjsmapjsSuffix,
    app + &#x27;admin/**/*&#x27; + tsjsmapjsSuffix,
    app + &#x27;dashboard/**/*&#x27; + tsjsmapjsSuffix,
    app + &#x27;layout/**/*&#x27; + tsjsmapjsSuffix,
    app + &#x27;sages/**/*&#x27; + tsjsmapjsSuffix,
    app + &#x27;sayings/**/*&#x27; + tsjsmapjsSuffix,
  ],

  // The styles we want Gulp to process
  styles: [&#x27;content/styles.css&#x27;],

  wiredepOptions: {
    exclude: [/jquery/],
    ignorePath: &#x27;..&#x27;,
  },
};

config.debugFolder = config.buildDir + config.debug + &#x27;/&#x27;;
config.releaseFolder = config.buildDir + config.release + &#x27;/&#x27;;

config.templateFiles = [
  app + &#x27;**/*.html&#x27;,
  &#x27;!&#x27; + config.bootFile, // Exclude the launch page
];

module.exports = config;
</code></pre><p>Now to the meat of the matter - let me present the gulpfile:</p><pre><code class="language-js">/// &lt;vs AfterBuild=&#x27;default&#x27; /&gt;
var gulp = require(&#x27;gulp&#x27;);

// Include Our Plugins
var concat = require(&#x27;gulp-concat&#x27;);
var ignore = require(&#x27;gulp-ignore&#x27;);
var minifyCss = require(&#x27;gulp-minify-css&#x27;);
var uglify = require(&#x27;gulp-uglify&#x27;);
var rev = require(&#x27;gulp-rev&#x27;);
var del = require(&#x27;del&#x27;);
var path = require(&#x27;path&#x27;);
var templateCache = require(&#x27;gulp-angular-templatecache&#x27;);
var eventStream = require(&#x27;event-stream&#x27;);
var order = require(&#x27;gulp-order&#x27;);
var gulpUtil = require(&#x27;gulp-util&#x27;);
var wiredep = require(&#x27;wiredep&#x27;);
var inject = require(&#x27;gulp-inject&#x27;);

// Get our config
var config = require(&#x27;./gulpfile.config.js&#x27;);

/**
 * Get the scripts or styles the app requires by combining bower dependencies and app dependencies
 *
 * @param {string} jsOrCss Should be &quot;js&quot; or &quot;css&quot;
 */
function getScriptsOrStyles(jsOrCss) {
  var bowerScriptsAbsolute = wiredep(config.wiredepOptions)[jsOrCss];

  var bowerScriptsRelative = bowerScriptsAbsolute.map(
    function makePathRelativeToCwd(file) {
      return path.relative(&#x27;&#x27;, file);
    }
  );

  var appScripts = bowerScriptsRelative.concat(
    jsOrCss === &#x27;js&#x27; ? config.scripts : config.styles
  );

  return appScripts;
}

/**
 * Get the scripts the app requires
 */
function getScripts() {
  return getScriptsOrStyles(&#x27;js&#x27;);
}

/**
 * Get the styles the app requires
 */
function getStyles() {
  return getScriptsOrStyles(&#x27;css&#x27;);
}

/**
 * Get the scripts and the templates combined streams
 *
 * @param {boolean} isDebug
 */
function getScriptsAndTemplates(isDebug) {
  var options = isDebug ? { base: config.base } : undefined;
  var appScripts = gulp.src(getScripts(), options);

  //Get the view templates for $templateCache
  var templates = gulp
    .src(config.templateFiles)
    .pipe(templateCache({ module: &#x27;app&#x27;, root: &#x27;app/&#x27; }));

  var combined = eventStream.merge(appScripts, templates);

  return combined;
}

gulp.task(&#x27;clean&#x27;, function (cb) {
  gulpUtil.log(&#x27;Delete the build folder&#x27;);

  return del([config.buildDir], cb);
});

gulp.task(&#x27;boot-dependencies&#x27;, [&#x27;clean&#x27;], function () {
  gulpUtil.log(&#x27;Get dependencies needed for boot (jQuery and images)&#x27;);

  var jQuery = gulp.src(config.bootjQuery);
  var images = gulp.src(config.images, { base: config.base });

  var combined = eventStream
    .merge(jQuery, images)
    .pipe(gulp.dest(config.buildDir));

  return combined;
});

gulp.task(&#x27;inject-debug&#x27;, [&#x27;styles-debug&#x27;, &#x27;scripts-debug&#x27;], function () {
  gulpUtil.log(&#x27;Inject debug links and script tags into &#x27; + config.bootFile);

  var scriptsAndStyles = [].concat(getScripts(), getStyles());

  return gulp
    .src(config.bootFile)
    .pipe(
      inject(
        gulp
          .src(
            [
              config.debugFolder + &#x27;**/*.{js,css}&#x27;,
              &#x27;!build\\debug\\bower_components\\spin.js&#x27;, // Exclude weird spin js path
            ],
            { read: false }
          )
          .pipe(order(scriptsAndStyles))
      )
    )
    .pipe(gulp.dest(config.buildDir));
});

gulp.task(&#x27;inject-release&#x27;, [&#x27;styles-release&#x27;, &#x27;scripts-release&#x27;], function () {
  gulpUtil.log(&#x27;Inject release links and script tags into &#x27; + config.bootFile);

  return gulp
    .src(config.bootFile)
    .pipe(
      inject(gulp.src(config.releaseFolder + &#x27;**/*.{js,css}&#x27;, { read: false }))
    )
    .pipe(gulp.dest(config.buildDir));
});

gulp.task(&#x27;scripts-debug&#x27;, [&#x27;clean&#x27;], function () {
  gulpUtil.log(&#x27;Copy across all JavaScript files to build/debug&#x27;);

  return getScriptsAndTemplates(true).pipe(gulp.dest(config.debugFolder));
});

gulp.task(&#x27;scripts-release&#x27;, [&#x27;clean&#x27;], function () {
  gulpUtil.log(&#x27;Concatenate &amp; Minify JS for release into a single file&#x27;);

  return getScriptsAndTemplates(false)
    .pipe(ignore.exclude(&#x27;**/*.{ts,js.map}&#x27;)) // Exclude ts and js.map files - not needed in release mode
    .pipe(concat(&#x27;app.js&#x27;)) // Make a single file
    .pipe(uglify()) // Make the file titchy tiny small
    .pipe(rev()) // Suffix a version number to it
    .pipe(gulp.dest(config.releaseFolder)); // Write single versioned file to build/release folder
});

gulp.task(&#x27;styles-debug&#x27;, [&#x27;clean&#x27;], function () {
  gulpUtil.log(&#x27;Copy across all CSS files to build/debug&#x27;);

  return gulp
    .src(getStyles(), { base: config.base })
    .pipe(gulp.dest(config.debugFolder));
});

gulp.task(&#x27;styles-release&#x27;, [&#x27;clean&#x27;], function () {
  gulpUtil.log(&#x27;Copy across all files in config.styles to build/debug&#x27;);

  return gulp
    .src(getStyles())
    .pipe(concat(&#x27;app.css&#x27;)) // Make a single file
    .pipe(minifyCss()) // Make the file titchy tiny small
    .pipe(rev()) // Suffix a version number to it
    .pipe(gulp.dest(config.releaseFolder + &#x27;/&#x27; + config.css)); // Write single versioned file to build/release folder
});

gulp.task(&#x27;fonts-debug&#x27;, [&#x27;clean&#x27;], function () {
  gulpUtil.log(&#x27;Copy across all fonts in config.fonts to debug location&#x27;);

  return gulp
    .src(config.fonts, { base: config.base })
    .pipe(gulp.dest(config.debugFolder));
});

gulp.task(&#x27;fonts-release&#x27;, [&#x27;clean&#x27;], function () {
  gulpUtil.log(&#x27;Copy across all fonts in config.fonts to release location&#x27;);

  return gulp
    .src(config.fonts)
    .pipe(gulp.dest(config.releaseFolder + &#x27;/fonts&#x27;));
});

gulp.task(&#x27;build-debug&#x27;, [&#x27;boot-dependencies&#x27;, &#x27;inject-debug&#x27;, &#x27;fonts-debug&#x27;]);

gulp.task(&#x27;build-release&#x27;, [
  &#x27;boot-dependencies&#x27;,
  &#x27;inject-release&#x27;,
  &#x27;fonts-release&#x27;,
]);

// Use the web.config to determine whether the default task should create a debug or a release build
// If the web.config contains this: &#x27;&lt;compilation debug=&quot;true&quot;&#x27; then we do a default build, otherwise
// we do a release build.  It&#x27;s a little hacky but generally works
var fs = require(&#x27;fs&#x27;);
var data = fs.readFileSync(__dirname + &#x27;/web.config&#x27;, &#x27;UTF-8&#x27;);
var inDebug = !!data.match(/&lt;compilation debug=&quot;true&quot;/);

gulp.task(&#x27;default&#x27;, [inDebug ? &#x27;build-debug&#x27; : &#x27;build-release&#x27;]);
</code></pre><p>That&#x27;s a big old lump of code. So let&#x27;s go through this a task by task...</p><h3>clean</h3><p>Deletes the <code>build</code> folder so we have a clean slate to build into.</p><h3>boot-dependencies</h3><p>Copy across all files that are needed to allow the page to &quot;boot&quot; / startup. At present this is only jQuery and images.</p><h3>inject-debug and inject-release</h3><p>This is the magic. This picks up the launch page (<code>index.html</code>), takes the JavaScript and CSS and injects the corresponding <code>script</code> and <code>link</code> tags into the page and writing it to the <code>build</code> folder. Either the original source code or the bundled / minified equivalent will be used depending on whether it&#x27;s debug or release.</p><h3>scripts-debug and scripts-release</h3><p>Here we collect up the following:</p><ul><li>the Bower specified JavaScript files</li><li>the TypeScript + associated JavaScript files</li><li>and we use our template files to construct a <code>templates.js</code> file to prime the Angular template cache</li></ul><p>If it&#x27;s the scripts-debug task we copy all these files into the <code>build/debug</code> folder. If it&#x27;s the scripts-release task we also bundle, minify and strip the TypeScript out too and copy into the <code>build/release</code> folder.</p><h3>styles-debug and styles-release</h3><p>Here we collect up the following:</p><ul><li>the Bower specified CSS files</li><li>our own app CSS</li></ul><p>If it&#x27;s the styles-debug task we copy all these files into the <code>build/debug</code> folder. If it&#x27;s the styles-release task we also bundle and minify and copy into the <code>build/release</code> folder.</p><h3>fonts-debug and fonts-release</h3><p>Whether it&#x27;s the debug or the release build we copy across the font-awesome assets and place them in a location which works for the associated CSS (as the CSS will depend upon font-awesome).</p><h3>build-debug, build-release and default</h3><p>build-debug and build-release (as their name suggests) either perform a build for release or a build for debug. If you remember, the web optimization library in ASP.Net serves up the raw code (&quot;debug&quot; code) if the <code>compilation debug</code> flag in the <code>web.config</code> is set to <code>true</code>. If it is set to <code>false</code> then we get the bundled and minified code (&quot;release&quot; code) instead. Our default task tries its best to emulate this behaviour by doing a very blunt regex against the <code>web.config</code>. Simply, if it can match <code>&amp;lt;compilation debug=&quot;true&quot;</code> then it runs the debug build. Otherwise, the release build. It could be more elegant but there&#x27;s a dearth of XML readers on npm that support synchronous parsing (which you kinda need for this scenario).</p><p>What I intend to do soon is switch from using the web.config to drive the gulp build to using the approach outlined <a href="http://www.codecadwallader.com/2015/03/15/integrating-gulp-into-your-tfs-builds-and-web-deploy/">here</a>. Namely plugging the build directly into Visual Studio&#x27;s build process and using the type of build there.</p><p>Hopefully what I&#x27;ve written here makes it fairly clear how to use Gulp to directly inject scripts and styles directly into your HTML. If you want to look directly at the source then check out the Proverb.Web folder in <a href="https://github.com/johnnyreilly/proverb-offline">this repo</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Convent with Continuous Delivery]]></title>
            <link>https://blog.johnnyreilly.com/2015/02/11/the-convent-with-continuous-delivery</link>
            <guid>The Convent with Continuous Delivery</guid>
            <pubDate>Wed, 11 Feb 2015 00:00:00 GMT</pubDate>
            <description><![CDATA[I've done it. I've open sourced the website that I maintain for my aunt what is a nun. Because I think we can all agree that nuns need open source and continuous integration about as much as anyone else.]]></description>
            <content:encoded><![CDATA[<p>I&#x27;ve done it. I&#x27;ve open sourced the <a href="http://www.poorclaresarundel.org/">website that I maintain for my aunt what is a nun</a>. Because I think we can all agree that nuns need open source and continuous integration about as much as anyone else.</p><p>For a long time now I&#x27;ve been maintaining a website for one of my (many) aunts that is a Poor Clare. (<a href="https://en.wikipedia.org/wiki/Subtyping">That&#x27;s a subtype of &quot;nun&quot; you OO enthusiasts.</a>) It&#x27;s not a terribly exciting site - it&#x27;s mostly static content. It&#x27;s built with a combination of AngularJS / TypeScript / Bootstrap and ASP.Net MVC. It&#x27;s hosted on <a href="http://azure.microsoft.com/en-us/documentation/services/websites/">Azure Websites</a>. In fact I have written about it (slightly more cagily) before <a href="https://blog.johnnyreilly.com/2014/06/migrating-from-angularjs-to-angularts.html">here</a>.</p><p>I&#x27;ll say up front: presentation-wise the site is not a work of art. However the nuns seem pretty happy with it. (Or perhaps secretly they&#x27;re forgiving me the shonkiness and sparing my feelings - who can say?) If I put my mind to it the site could look much more lovely. But there&#x27;s only so much time I can spare - and that&#x27;s actually one of the reasons I&#x27;ve set up Continuous Delivery.</p><h2>Why on earth did you bother?</h2><p>Well, you&#x27;d be surprised how often tweaks can be requested. Sometimes it appears to be forgotten for months at a time, and then all of a sudden my inbox is daily filled with a list of minor alterations. You know, slight text changes and the like.</p><p>So what I was generally doing was getting home of an evening, waiting until the children were in bed, chomping down some food and then firing up Visual Studio to make the changes and hit &quot;Publish&quot;. Yes that&#x27;s right; I was essentially using Visual Studio to edit text files and push a website out to Azure. The very definition of using a sledgehammer to crack a nut I think we can all agree.</p><p>It occurred to me that if I had Continuous Delivery set up then I could make these tweaks and not have to worry about the site being published. Which would be nice. I wouldn&#x27;t need Visual Studio anymore - any text editor would do. Also nice. Finally, if the source control was accessible online then I could probably get away with doing most tweaks on my mobile phone whilst I was travelling home. Timesaver!</p><h2>How did you go about it?</h2><p>Since <a href="http://www.visualstudioonline.com">Visual Studio Online (then &quot;Team Foundation Service&quot;)</a> was released I have been using it to host the source code. So the obvious solution was to use the tools offered there to do the deployment. However, this wasn&#x27;t the smooth experience you might have hoped for. I had quite a frustrating afternoon trying things out before deciding it was becoming more trouble than it was worth. VSO appeared to make it supremely hard to customise builds.</p><p>Just recently though I have been having the most wonderful experience with <a href="http://www.appveyor.com/">AppVeyor</a>. AppVeyor market themselves as <em>&quot;#1 Continuous Delivery service for Windows&quot;</em> <!-- -->-<!-- --> I think they&#x27;re right. Their build process is entirely flexible and customisable. It is, in short, a joy to use. (The support is fantastic too - very helpful indeed. Go <a href="https://github.com/FeodorFitsner">Feodor</a>!)</p><p>If you look just below the header you&#x27;ll read a very important sentence: <em>&quot;Free for open-source projects&quot;</em>. You hear that? By the time I&#x27;d finished reading that sentence I&#x27;d decided that the Poor Clares website was about to become an open source project.</p><p>And now it is.</p><h2>Where is it?</h2><p>The source on <a href="https://github.com/johnnyreilly/poorclaresarundel">GitHub</a>. The builds and deployment are taken care of by <a href="https://ci.appveyor.com/project/JohnReilly/poorclaresarundel">AppVeyor</a>.</p><h2>Will you take pull requests?</h2><p>If they&#x27;re serious, then yes, certainly! My long term plan is to try and get the nuns set up as collaborators in GitHub. That way they can make their own minor tweaks without me getting involved.</p><p>On another front, I do wonder if open-sourcing Poor Clares, Arundel might have other hidden benefits. There&#x27;s a number of things I&#x27;m not too keen on in the code. Up until now I think my attitude was possibly &quot;it works so that&#x27;s good enough&quot;. It was only me aware of the shortcomings. Now it&#x27;s public I&#x27;ll probably have more of an incentive to tidy up the rough edges. That&#x27;s the theory anyway - Embarrassment Driven Development anyone? :-)</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[TypeScript: In Praise of Union Types]]></title>
            <link>https://blog.johnnyreilly.com/2015/01/20/typescript-using-functions-with-union-types</link>
            <guid>TypeScript: In Praise of Union Types</guid>
            <pubDate>Tue, 20 Jan 2015 00:00:00 GMT</pubDate>
            <description><![CDATA[(& How to Express Functions in UTs)]]></description>
            <content:encoded><![CDATA[<h2>(&amp; How to Express Functions in UTs)</h2><p>Have you heard the good news my friend? I refer, of course, to the shipping of TypeScript 1.4 and my <em>favourite</em> language feature since generics.... Union Types.</p><p>In the <a href="https://blogs.msdn.com/b/typescript/archive/2015/01/16/announcing-typescript-1-4.aspx">1<!-- -->.<!-- -->4 announcement</a> Jonathan Turner described Union Types thusly:</p><blockquote><p>JavaScript functions may take a number of possible argument types. Up to now, we’ve supported this using function overloads. Starting with TypeScript 1.4, we’ve generalized this capability and now allow you to specify that that a value is one of a number of different types using a union type:</p><pre><code class="language-ts">function f(x: number | number[]) {
  if (typeof x === &#x27;number&#x27;) {
    return x + 10;
  } else {
    // return sum of numbers
  }
}
</code></pre><p>Once you have a value of a union type, you can use a typeof and instanceof checks to use the value in a type-safe way. You&#x27;ll notice we use this in the above example and can treat x as a number type inside of the if-block.</p><p>Union types are a new kind of type and work any place you specify a type.</p></blockquote><p>Lovely right? But what&#x27;s missing? Well, to my mind, the most helpful aspect of Union Types. Definition file creation.</p><h2>A little history</h2><h3>That&#x27;s right - the days before Union Types are now &quot;history&quot; :-)</h3><p>When creating definition files (<code>*.d.ts</code>) in the past there was a problem with TypeScript. A limitation. JavaScript often relies on &quot;option bags&quot; to pass configuration into a method. An &quot;option bag&quot; is essentially a JavaScript object literal which contains properties which are used to perform configuration. A good example of this is the <code>route</code> parameter passed into Angular&#x27;s ngRoute <code>&lt;a href=&quot;https://docs.angularjs.org/api/ngRoute/provider/$routeProvider#when&quot;&gt;when&lt;/a&gt;</code> method.</p><p>I&#x27;d like to draw your attention to 2 of the properties that can be passed in (quoted from the documentation):</p><blockquote><ul><li><p>controller – <code>{(string|function()=}</code> – Controller fn that should be associated with newly created scope or the name of a registered controller if passed as a string.</p></li><li><p>template – <code>{string=|function()=}</code> – html template as a string or a function that returns an html template as a string which should be used by ngView or ngInclude directives. This property takes precedence over templateUrl.</p><p>If template is a function, it will be called with the following parameters:</p><p><code>{Array.&amp;lt;Object&amp;gt;}</code> <!-- -->-<!-- --> route parameters extracted from the current $location.path() by applying the current route</p></li></ul></blockquote><p>Both of these properties can be of more than 1 type.</p><ul><li><code>controller</code> can be a <code>string</code><em>or</em> a <code>function</code>.</li><li><code>template</code> can be a <code>string</code><em>or</em> a <code>function</code> that returns a <code>string</code> and has <code>$routeParams</code> as a parameter.</li></ul><p>There&#x27;s the rub. Whilst it was possible to overload functions in TypeScript pre 1.4, it was <u>not</u></p><p>possible to overload interface members. This meant the only way to model these sorts of properties was by seeking out a best common type which would fit all scenarios. This invariably meant using the <code>any</code> type. Whilst that worked it didn&#x27;t lend any consuming code a great deal of type safety. Let&#x27;s look at a truncated version of <code>&lt;a href=&quot;https://github.com/borisyankov/DefinitelyTyped/blob/c71628e0765eb8e240d8eabd2225f64ea2e2fdb8/angularjs/angular-route.d.ts&quot;&gt;angular-route.d.ts&lt;/a&gt;</code> for these properties prior to union types:</p><pre><code class="language-ts">declare module ng.route {
  // ...

  interface IRoute {
    /**
     * {(string|function()=}
     * Controller fn that should be associated with newly created scope or
     * the name of a registered controller if passed as a string.
     */
    controller?: any;

    /**
     * {string=|function()=}
     * Html template as a string or a function that returns an html template
     * as a string which should be used by ngView or ngInclude directives. This
     * property takes precedence over templateUrl.
     *
     * If template is a function, it will be called with the following parameters:
     *
     * {Array.&lt;Object&gt;} - route parameters extracted from the current
     * $location.path() by applying the current route
     */
    template?: any;

    // ...
  }

  // ...
}
</code></pre><p>It&#x27;s <code>any</code> city... Kind of sticks in the craw doesn&#x27;t it?</p><h2>A new dawn</h2><p>TypeScript 1.4 has shipped and Union Types are with us. We can do better than <code>any</code>. So what does <code>&lt;a href=&quot;https://github.com/borisyankov/DefinitelyTyped/blob/30ce45e0e706322f34608ab6fa5de141bba59c90/angularjs/angular-route.d.ts&quot;&gt;angular-route.d.ts&lt;/a&gt;</code> look like now we have Union Types?</p><pre><code class="language-ts">declare module ng.route {
  // ...

  interface IRoute {
    /**
     * {(string|function()=}
     * Controller fn that should be associated with newly created scope or
     * the name of a registered controller if passed as a string.
     */
    controller?: string | Function;

    /**
     * {string=|function()=}
     * Html template as a string or a function that returns an html template
     * as a string which should be used by ngView or ngInclude directives. This
     * property takes precedence over templateUrl.
     *
     * If template is a function, it will be called with the following parameters:
     *
     * {Array.&lt;Object&gt;} - route parameters extracted from the current
     * $location.path() by applying the current route
     */
    template?:
      | string
      | { ($routeParams?: ng.route.IRouteParamsService): string };

    // ...
  }

  // ...
}
</code></pre><p>With these changes in place we are now accurately modelling the <code>route</code> option bags in TypeScript. Hoorah!!!</p><p>Let&#x27;s dig in a little. If you look at the <code>controller</code> definition it&#x27;s pretty straightforward. <code>string|Function</code> <!-- -->-<!-- --> clearly the <code>controller</code> can be a <code>string</code><em>or</em> a <code>Function</code>. Simple.</p><p>Now let&#x27;s look at the <code>template</code> definition by itself:</p><pre><code class="language-ts">template?: string | { ($routeParams?: ng.route.IRouteParamsService) : string; }
</code></pre><p>As with the <code>controller</code> the <code>template</code> can be a string - that is pretty clear. But what&#x27;s that hovering on the other side of the &quot;<!-- -->|<!-- -->&quot;? What could <code>{ ($routeParams?: ng.route.IRouteParamsService) : string; }</code> be exactly?</p><p>Well, in a word, it&#x27;s a <code>Function</code>. The <code>controller</code> would allow any kind of function at all. However the <code>template</code> definition is deliberately more restrictive. This defines a function which must return a <code>string</code> and which receives an optional parameter of <code>$routeParams</code> of type <code>ng.route.IRouteParamsService</code>.</p><h2>State of the Union</h2><p>Hopefully you can now see just how useful Union Types are and how you can express specific sorts of function definitions as part of a Union Type.</p><p>The thing that prompted me first to write this post was seeing that there don&#x27;t appear to be any examples out there of how to express functions inside Union Types. I only landed on the syntax myself after a little experimentation in Visual Studio after I&#x27;d installed TS 1.4. I&#x27;ve started work on bringing Union Types to the typings inside <a href="https://github.com/borisyankov/DefinitelyTyped">DefinitelyTyped</a> and so you&#x27;ll start to see them appearing more and more. But since it&#x27;s rather &quot;hidden knowledge&quot; at present I wanted to do my bit to make it a little better known.</p><p>As <a href="https://twitter.com/Rickenhacker">Daniel</a> helpfully points out in the comments there is an alternate syntax - lambda style. So instead of this:</p><pre><code class="language-ts">template?: string | { ($routeParams?: ng.route.IRouteParamsService) : string; }
</code></pre><p>You could write this:</p><pre><code class="language-ts">template?: string | (($routeParams?: ng.route.IRouteParamsService) =&gt; string);
</code></pre><p>Just remember to place parentheses around the lambda to clearly delineate it.</p><h2>Bonfire of the Overloads</h2><p>Before I sign off I should mention the ability Union Types give you to define a much terser definition file. Basically the &quot;<!-- -->|<!-- -->&quot; operator makes for a bonfire of the overloads. Where you previously may have had 6 overloads for the same method (each with identical JSDoc) you now only need 1. Which is beautiful (and DRY).</p><p>It&#x27;s surprising just what a difference it makes. This is <code>&lt;a href=&quot;https://github.com/borisyankov/DefinitelyTyped/blob/9bd7fe69d98337db56144c3da131d413f5b7e895/jquery/jquery.d.ts&quot;&gt;jQuery.d.ts&lt;/a&gt;</code> last week (pre TypeScript 1.4). This is <code>&lt;a href=&quot;https://github.com/borisyankov/DefinitelyTyped/blob/9f64372a065541fe2b8f6c5c5cd9b55a1d631f19/jquery/jquery.d.ts&quot;&gt;jQuery.d.ts&lt;/a&gt;</code> now - with Union Types aplenty. Last week it was <!-- -->~<!-- -->4000 lines of code. This week it&#x27;s <!-- -->~<!-- -->3200 lines of code. With the same functionality. Union Types are <em>FANTASTIC</em>!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Deploying from ASP.Net MVC to GitHub Pages using AppVeyor part 2]]></title>
            <link>https://blog.johnnyreilly.com/2015/01/07/deploying-aspnet-mvc-to-github-pages-with-appveyor-part-2</link>
            <guid>Deploying from ASP.Net MVC to GitHub Pages using AppVeyor part 2</guid>
            <pubDate>Wed, 07 Jan 2015 00:00:00 GMT</pubDate>
            <description><![CDATA["Automation, automation, automation." Those were and are Tony Blair's priorities for keeping open source projects well maintained.]]></description>
            <content:encoded><![CDATA[<p>&quot;Automation, automation, automation.&quot; Those were and are Tony Blair&#x27;s priorities for keeping open source projects well maintained.</p><p>OK, that&#x27;s not quite true... But what is certainly true is that maintaining an open source project takes time. And there&#x27;s only so much free time that anyone has. For that reason, wherever you can it makes sense to <em>AUTOMATE</em>!</p><p><a href="https://blog.johnnyreilly.com/2014/12/deploying-aspnet-mvc-to-github-pages-with-appveyor-part-1.html">Last time</a> we looked at how you can take an essentially static ASP.Net MVC site (in this case my jVUNDemo documentation site) and generate an entirely static version using Wget. This static site has been pushed to <a href="https://pages.github.com/">GitHub Pages</a> and is serving as the documentation for <a href="http://johnnyreilly.github.io/jQuery.Validation.Unobtrusive.Native/">jQuery Validation Unobtrusive Native</a> (and for bonus points is costing me no money at all).</p><p>So what next? Well, automation clearly! If I make a change to jQuery Validation Unobtrusive Native then AppVeyor already bounds in and performs a <a href="https://ci.appveyor.com/project/JohnReilly/jquery-validation-unobtrusive-native">continuous integration build</a> for me. It picks up the <a href="https://github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native">latest source</a> from GitHub, pulls in my dependencies, performs a build and runs my tests. Lovely.</p><p>So the obvious thing to do is to take this process and plug in the generation of my static site and the publication thereof to GitHub pages. The minute a change is made to my project the documentation should be updated without me having to break sweat. That&#x27;s the goal.</p><h2>GitHub Personal Access Token</h2><p>In order to complete our chosen mission we&#x27;re going to need a GitHub Personal Access Token. We&#x27;re going to use it when we clone, update and push our GitHub Pages branch. To get one we biff over to Settings / Applications in GitHub and click the &quot;Generate New Token&quot; button.</p><p><img src="../static/blog/2015-01-07-deploying-aspnet-mvc-to-github-pages-with-appveyor-part-2/GitHubApplicationSettings.png"/></p><p>The token I&#x27;m using for my project has the following scopes selected:</p><p><img src="../static/blog/2015-01-07-deploying-aspnet-mvc-to-github-pages-with-appveyor-part-2/GitHub%2BPersonal%2BAccess%2BToken.png"/></p><h2><code>appveyor.yml</code></h2><p>With our token in hand we turn our attention to AppVeyor build configuration. This is possible using a file called <a href="http://www.appveyor.com/docs/build-configuration"><code>appveyor.yml</code></a> stored in the root of your repo. You can also use the AppVeyor web UI to do this. However, for the purposes of ease of demonstration I&#x27;m using the file approach. The <a href="https://github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native/blob/master/appveyor.yml">jQuery Validation Unobtrusive Native <code>appveyor.yml</code></a> looks like this:</p><pre><code class="language-yml">---
#---------------------------------#
#      general configuration      #
#---------------------------------#

# version format
version: 1.0.{build}

#---------------------------------#
#    environment configuration    #
#---------------------------------#

# environment variables
environment:
  GithubEmail: johnny_reilly@hotmail.com
  GithubUsername: johnnyreilly
  GithubPersonalAccessToken:
    secure: T4M/N+e/baksVoeWoYKPWIpfahOsiSFw/+Zc81VuThZmWEqmrRtgEHUyin0vCWhl

branches:
  only:
    - master

install:
  - ps: choco install wget

build:
  verbosity: minimal

after_test:
  - ps: ./makeStatic.ps1 $env:APPVEYOR_BUILD_FOLDER
  - ps: ./pushStatic.ps1 $env:APPVEYOR_BUILD_FOLDER $env:GithubEmail $env:GithubUsername $env:GithubPersonalAccessToken
</code></pre><p>There&#x27;s a number of things you should notice from the yml file:</p><ul><li>We create 3 environment variables: GithubEmail, GithubUsername and GithubPersonalAccessToken (more on this in a moment).</li><li>We only build the master branch.</li><li>We use <a href="https://chocolatey.org/packages/Wget">Chocolatey</a> to install Wget which is used by the <code>makeStatic.ps1</code> Powershell script.</li><li>After the tests have completed we run 2 Powershell scripts. First <code>&lt;a href=&quot;https://github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native/blob/master/makeStatic.ps1&quot;&gt;makeStatic.ps1&lt;/a&gt;</code> which builds the static version of our site. This is the exact same script we discussed in the previous post - we&#x27;re just passing it the build folder this time (one of AppVeyor&#x27;s environment variables). Second, we run <code>&lt;a href=&quot;https://github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native/blob/master/pushStatic.ps1&quot;&gt;pushStatic.ps1&lt;/a&gt;</code> which publishes the static site to GitHub Pages.</li></ul><p>We pass 4 arguments to <code>pushStatic.ps1</code>: the build folder, my email address, my username and my personal access token. For the sake of security the GithubPersonalAccessToken has been encrypted as indicated by the <code>secure</code> keyword. This is a capability available in AppVeyor <a href="https://ci.appveyor.com/tools/encrypt">here</a>.</p><p><img src="../static/blog/2015-01-07-deploying-aspnet-mvc-to-github-pages-with-appveyor-part-2/AppVeyor%2Bencrypt.png"/></p><p>This allows me to mask my personal access token rather than have it available as free text for anyone to grab.</p><h2><code>pushStatic.ps1</code></h2><p>Finally we can turn our attention to how our Powershell script <code>pushStatic.ps1</code> goes about pushing our changes up to GitHub Pages:</p><pre><code class="language-ps">param([string]$buildFolder, [string]$email, [string]$username, [string]$personalAccessToken)

Write-Host &quot;- Set config settings....&quot;
git config --global user.email $email
git config --global user.name $username
git config --global push.default matching

Write-Host &quot;- Clone gh-pages branch....&quot;
cd &quot;$($buildFolder)\..\&quot;
mkdir gh-pages
git clone --quiet --branch=gh-pages https://$($username):$($personalAccessToken)@github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native.git .\gh-pages\
cd gh-pages
git status

Write-Host &quot;- Clean gh-pages folder....&quot;
Get-ChildItem -Attributes !r | Remove-Item -Recurse -Force

Write-Host &quot;- Copy contents of static-site folder into gh-pages folder....&quot;
copy-item -path ..\static-site\* -Destination $pwd.Path -Recurse

git status
$thereAreChanges = git status | select-string -pattern &quot;Changes not staged for commit:&quot;,&quot;Untracked files:&quot; -simplematch
if ($thereAreChanges -ne $null) {
    Write-host &quot;- Committing changes to documentation...&quot;
    git add --all
    git status
    git commit -m &quot;skip ci - static site regeneration&quot;
    git status
    Write-Host &quot;- Push it....&quot;
    git push --quiet
    Write-Host &quot;- Pushed it good!&quot;
}
else {
    write-host &quot;- No changes to documentation to commit&quot;
}
</code></pre><p>So what&#x27;s happening here? Let&#x27;s break it down:</p><ul><li>Git is configured with the passed in username and email address.</li><li>A folder is created that sits alongside the build folder called &quot;gh-pages&quot;.</li><li>We clone the &quot;gh-pages&quot; branch of jQuery Validation Unobtrusive Native into our &quot;gh-pages&quot; directory. You&#x27;ll notice that we are using our GitHub personal access token in the clone URL itself.</li><li>We delete the contents of the &quot;gh-pages&quot; directory leaving it empty.</li><li>We copy across the contents of the &quot;static-site&quot; folder (created by <code>makeStatic.ps1</code>) into the &quot;gh-pages&quot;.</li><li>We use <code>git status</code> to check if there are any changes. (This method is completely effective but a little crude to my mind - there&#x27;s probably better approaches to this.... shout me in the comments if you have a suggestion.)</li><li>If we have no changes then we do nothing.</li><li>If we have changes then we stage them, commit them and push them to GitHub Pages. Then we sign off with an allusion to <a href="https://en.wikipedia.org/wiki/Push_It_(Salt-n-Pepa_song)">80&#x27;s East Coast hip-hop</a>... &#x27;Cos that&#x27;s how we roll.</li></ul><p>With this in place, any changes to the docs will be automatically published out to our &quot;gh-pages&quot; branch. Our documentation will always be up to date thanks to the goodness of AppVeyor&#x27;s Continuous Integration service.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Deploying from ASP.Net MVC to GitHub Pages using AppVeyor part 1]]></title>
            <link>https://blog.johnnyreilly.com/2014/12/29/deploying-aspnet-mvc-to-github-pages-with-appveyor-part-1</link>
            <guid>Deploying from ASP.Net MVC to GitHub Pages using AppVeyor part 1</guid>
            <pubDate>Mon, 29 Dec 2014 00:00:00 GMT</pubDate>
            <description><![CDATA[There's a small open source project I'm responsible for called jQuery Validation Unobtrusive Native. (A catchy name is a must for any good open source project. Alas I'm not quite meeting my own exacting standards on this particular point... I should have gone with my gut and called it "Livingstone" instead. Too late now...)]]></description>
            <content:encoded><![CDATA[<p>There&#x27;s a small open source project I&#x27;m responsible for called <a href="https://github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native">jQuery Validation Unobtrusive Native</a>. (A catchy name is a must for any good open source project. Alas I&#x27;m not quite meeting my own exacting standards on this particular point... I should have gone with my gut and called it &quot;Livingstone&quot; instead. Too late now...)</p><p>The project itself is fairly simple in purpose. It&#x27;s essentially a bridge between ASP.Net MVC&#x27;s inbuilt support for driving validation from data attributes and jQuery Validation&#x27;s native support for the same. It is, in the end, a collection of ASP.Net MVC HTML helper extensions. It is not massively complicated.</p><p>I believe it was Tony Blair that said &quot;documentation, documentation, documentation&quot; were his priorities for open source projects. Or maybe it was someone else... Anyway, the point stands. Documentation is supremely important if you want your project to be in any way useful to anyone other than yourself. A project, no matter how fantastic, which lacks decent documentation is a missed opportunity.</p><p>Anyway I&#x27;m happy to say that jQuery Validation Unobtrusive Native <em>has</em> documentation! And pretty good documentation at that. The documentation takes the form of the <a href="https://github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native/tree/master/jVUNDemo">jVUNDemo</a> project which is part of the jQuery Validation Unobtrusive Native repo. jVUNDemo is an ASP.Net MVC web application which is built on top of the jQuery Validation Unobtrusive Native helpers. It demonstrates the helpers in action and documents how you might go about using them. It looks a bit like this:</p><p><img src="../static/blog/2014-12-29-deploying-aspnet-mvc-to-github-pages-with-appveyor-part-1/Screenshot%2B2014-12-29%2B06.22.46.png"/></p><p>When I first put jVUNDemo together I hosted it on Azure so the world could see it in all it&#x27;s finery. And that worked just fine. However, there&#x27;s something you ought to know about me:</p><h2>I&#x27;m quite cheap</h2><p>No really, I am. My attention was grabbed by the essentially &quot;free&quot; nature of <a href="https://pages.github.com/">GitHub Pages</a>. I was immediately seized by the desire to somehow deploy jVUNDemo to GitHub Pages and roll around joyfully in all that lovely free hosting.</p><p>&quot;But&quot;, I hear you cry, &quot;you can&#x27;t deploy an ASP.Net MVC application to Git Hub Pages... It only hosts static sites!&quot; Quite right. Sort of. This is where I get to pull my ace of spades: jVUNDemo is not really an &quot;app&quot; so much as a static site. Yup, once the HTML that makes up each page is generated there isn&#x27;t any app like business to do. It&#x27;s just a collection of text and 1 screen demo&#x27;s really.</p><p>That being the case, there&#x27;s no reason why I shouldn&#x27;t be able to make use of GitHub Pages. So that&#x27;s what I decided to do. Whilst I was at it I also wanted to automate the deployment process. When I tweak jVUNDemo I don&#x27;t want to have to manually push out a new version of jVUNDemo to wherever it&#x27;s being hosted. No, I&#x27;m a developer so I&#x27;ll automate it.</p><p>I&#x27;ve broken this up into 2 posts. This first one will cover how you generate a static site from an ASP.Net MVC site. The second will be about how to use <a href="http://www.appveyor.com/">AppVeyor</a> to ensure that on each build your documentation is getting republished.</p><h2>You Wget me?</h2><p>So, static site generation. There&#x27;s a well known Unix utility called <a href="https://en.wikipedia.org/wiki/Wget">Wget</a> which covers exactly that ground and so we&#x27;re going to use it. It downloads and saves HTML, it recursively walks the links inside the site and grabs those pages too and it converts our routes so they are locally browsable (&quot;Demo/Required&quot; becomes &quot;Demo/Required.html&quot;).</p><p>You can use <a href="https://chocolatey.org/packages/Wget">Chocolatey</a> to get a copy of Wget. We&#x27;re also going to need IIS Express to host jVUNDemo whilst Wget converts it. Once jVUNDemo has been built successfully you should be be able to kick off the process like so:</p><pre><code class="language-ps">cd C:\projects\jquery-validation-unobtrusive-native
.\makeStatic.ps1 $pwd.path
</code></pre><p>This invokes the <code>makeStatic</code> Powershell script in the root of the jQuery Validation Unobtrusive Native repo:</p><pre><code class="language-ps">param([string]$buildFolder)

$jVUNDemo = &quot;$($buildFolder)\jVUNDemo&quot;
$staticSiteParentPath = (get-item $buildFolder).Parent.FullName
$staticSite = &quot;static-site&quot;
$staticSitePath = &quot;$($staticSiteParentPath)\$($staticSite)&quot;
$wgetLogPath = &quot;$($staticSiteParentPath)\wget.log&quot;
$port = 57612
$servedAt = &quot;http://localhost:$($port)/&quot;
write-host &quot;jVUNDemo location: $jVUNDemo&quot;
write-host &quot;static site parent location: $staticSiteParentPath&quot;
write-host &quot;static site location: $staticSitePath&quot;
write-host &quot;wget log path: $wgetLogPath&quot;

write-host &quot;Spin up jVUNDemo site at $($servedAt)&quot;
$process = Start-Process &#x27;C:\Program Files (x86)\IIS Express\iisexpress.exe&#x27; -NoNewWindow -ArgumentList &quot;/path:$($jVUNDemo) /port:$($port)&quot;

write-host &quot;Wait a moment for IIS to startup&quot;
Start-sleep -s 15

if (Test-Path $staticSitePath) {
    write-host &quot;Removing $($staticSitePath)...&quot;
    Remove-Item -path $staticSitePath -Recurse -Force
}

write-host &quot;Create static version of demo site here: $($staticSitePath)&quot;
Push-Location $staticSiteParentPath
# 2&gt;&amp;1 used to combine stderr and stdout
wget.exe --recursive --convert-links -E --directory-prefix=$staticSite --no-host-directories $servedAt &gt; $wgetLogPath 2&gt;&amp;1
write-host &quot;lastExitCode: $($lastExitCode)&quot;
cat $wgetLogPath
Pop-Location

write-host &quot;Shut down jVUNDemo site&quot;
stop-process -Name iisexpress

if (Test-Path $staticSitePath) {
    write-host &quot;Contents of $($staticSitePath)&quot;
    ls $staticSitePath
}
</code></pre><p>The above Powershell does the following:</p><ol><li>Starts up IIS Express serving jVUNDemo on http://localhost:57612/</li><li>Waits 15 seconds for IIS Express to get itself together (probably a shorter wait time would be sufficient)</li><li>Points Wget at jVUNDemo and bellows &quot;go!!!!&quot;</li><li>Wget downloads and saves the static version of jVUNDemo to a directory called &quot;static-site&quot;</li><li>Stops IIS Express</li><li>Prints out the contents of the &quot;static-site&quot; directory</li></ol><p>When run, it pumps something like this out:</p><pre><code>jVUNDemo location: C:\projects\jquery-validation-unobtrusive-native\jVUNDemo
static site parent location: C:\projects
static site location: C:\projects\static-site
wget log path: C:\projects\wget.log
Spin up jVUNDemo site at http://localhost:57612/
Wait a moment for IIS to startup
Create static version of demo site here: C:\projects\static-site
wget.exe : --2014-12-29 07:49:56--  http://localhost:57612/
Resolving localhost...
127.0.0.1
Connecting to localhost|127.0.0.1|:57612... connected.
HTTP request sent, awaiting response...
200 OK

..... lots of HTTP requests.....

Downloaded: 30 files, 1.0M in 0.09s (10.8 MB/s)
Converting static-site/Demo/CreditCard.html... 34-0
Converting static-site/Demo/Number.html... 34-0
Converting static-site/Demo/Range.html... 34-0
Converting static-site/Demo/Email.html... 34-0
Converting static-site/AdvancedDemo/CustomValidation.html... 35-0
Converting static-site/Demo/Date.html... 36-0
Converting static-site/Home/License.html... 27-0
Converting static-site/index.html... 29-0
Converting static-site/AdvancedDemo/Dynamic.html... 35-0
Converting static-site/Demo/MaxLengthMinLength.html... 34-0
Converting static-site/Demo/Required.html... 34-0
Converting static-site/AdvancedDemo.html... 32-0
Converting static-site/Demo/Remote.html... 35-0
Converting static-site/Demo/EqualTo.html... 34-0
Converting static-site/AdvancedDemo/Globalize.html... 38-0
Converting static-site/Demo/Url.html... 34-0
Converting static-site/Demo.html... 37-0
Converting static-site/Home/GettingStarted.html... 29-0
Converting static-site/Home/Download.html... 27-0
Converting static-site/AdvancedDemo/Tooltip.html... 34-0
Converted 20 files in 0.03 seconds.

Shut down jVUNDemo site
Contents of C:\projects\static-site


    Directory: C:\projects\static-site


Mode                LastWriteTime     Length Name
----                -------------     ------ ----
d----        12/29/2014   7:50 AM            AdvancedDemo
d----        12/29/2014   7:50 AM            Content
d----        12/29/2014   7:50 AM            Demo
d----        12/29/2014   7:50 AM            Home
d----        12/29/2014   7:50 AM            Scripts
-a---        12/29/2014   7:50 AM       5967 AdvancedDemo.html
-a---        12/29/2014   7:50 AM       6802 Demo.html
-a---        12/29/2014   7:47 AM      12862 favicon.ico
-a---        12/29/2014   7:50 AM       8069 index.html
</code></pre><p>And that&#x27;s it for part 1 my friends! You now have a static version of the ASP.Net MVC site to dazzle the world with. I should say for the purposes of full disclosure that there are 2 pages in the site which are not entirely &quot;static&quot; friendly. For these 2 pages I&#x27;ve put messages in that are displayed when the page is served up in a static format explaining the limitations. Their full glory can still be experienced by cloning the project and running locally.</p><p><a href="https://blog.johnnyreilly.com/2015/01/deploying-aspnet-mvc-to-github-pages-with-appveyor-part-2.html">Next time</a> we&#x27;ll take the mechanism detailed above and plug it into AppVeyor for some Continuous Integration happiness.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Gulp, npm, long paths and Visual Studio.... Fight!]]></title>
            <link>https://blog.johnnyreilly.com/2014/12/12/gulp-npm-long-paths-and-visual-studio-fight</link>
            <guid>Gulp, npm, long paths and Visual Studio.... Fight!</guid>
            <pubDate>Fri, 12 Dec 2014 00:00:00 GMT</pubDate>
            <description><![CDATA[How I managed to gulp-angular-templatecache working inside Visual Studio]]></description>
            <content:encoded><![CDATA[<h2><sub>How I managed to gulp-angular-templatecache working inside Visual Studio</sub></h2><p>Every now and then something bites you unexpectedly. After a certain amount of pain, the answer comes to you and you know you want to save others from falling into the same deathtrap.</p><p>There I was minding my own business and having a play with a Gulp plugin called <a href="https://www.npmjs.com/package/gulp-angular-templatecache">gulp-angular-templatecache</a>. If you&#x27;re not aware of it, it &quot;Concatenates and registers AngularJS templates in the $templateCache&quot;. I was planning to use it so that all the views in an <a href="https://github.com/johnnyreilly/proverb-offline">Angular app of mine</a> were loaded up-front rather than on demand. (It&#x27;s a first step in making an &quot;offline-first&quot; version of that particular app.)</p><p>I digress already. No sooner had I tapped in:</p><pre><code class="language-ps">npm install gulp-angular-templatecache --saveDev
</code></pre><p>Then I noticed my Visual Studio project was no longer compiling. It was dying a death on build with this error:</p><pre><code class="language-ps">ASPNETCOMPILER : error ASPRUNTIME: The specified path, file name, or both are too long. The fully qualified file name must be less than 260 characters, and the directory name must be less than 248 characters.
</code></pre><p>I was dimly aware that there were issues with the nested <a href="https://github.com/joyent/node/issues/6960">node_modules</a> leading to Windows-killing paths. This sounded just like that.... And it was! <code>gulp-angular-templatecache</code> had a dependency on <code>gulp-footer</code> which had a dependency on <code>lodash.assign</code> which had a dependency on <code>lodash._basecreatecallback</code> which had.... You see where I&#x27;m going? It seems that the lovely lodash has created the path from hell.</p><p>For reasons that aren&#x27;t particularly clear this kills Visual Studio&#x27;s build process. This is slightly surprising given that our rogue path is sat in the <code>node_modules</code> directory which isn&#x27;t part of the project in Visual Studio. That being the case you&#x27;d imagine that you could do what you liked there. But no, it seems VS is a delicate flower and we must be careful not to offend. Strange.</p><h2>It&#x27;s Workaround Time!</h2><p>After a <em>great deal</em> of digging I found the answer nestled in the middle of an <a href="http://stackoverflow.com/a/24144479/761388">answer on Stack Overflow</a>. To quote:</p><blockquote><p>If you will add &quot;lodash.bind&quot; module to your project&#x27;s package.json as dependency it will be installed in one level with gulp and not as gulp&#x27;s dependency</p></blockquote><p>That&#x27;s right, I just needed to tap enter this at the root of my project:</p><pre><code class="language-ps">npm install lodash.bind --saveDev
</code></pre><p>And all was sweetness and light once more - no more complaints from VS.</p><h2>The Future</h2><p>It looks like lodash are <a href="https://github.com/lodash/lodash-cli/issues/23">on course to address this issue</a>. So one day this this workaround won&#x27;t be necessary anymore which is good.</p><p>However, the general long path issue concerning node / npm hasn&#x27;t vanished for Windows users. Given VS 2015 is planning to make Gulp and Grunt 1st class citizens of Visual Studio I&#x27;m going to guess that sort of issue is likely to arise again and again for other packages. I&#x27;m hoping that means that someone will actually fix the underlying path issues that upset Windows with npm.</p><p>It sounds like npm are planning to take <a href="https://github.com/joyent/node/issues/6960#issuecomment-46704998">some steps</a> which is great. But I can&#x27;t be alone in having a slightly nagging feeling that Windows isn&#x27;t quite a first class citizen for node / io / npm yet. I really hope it will become one.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What's in a (Domain) Name?]]></title>
            <link>https://blog.johnnyreilly.com/2014/12/05/whats-in-a-name</link>
            <guid>What's in a (Domain) Name?</guid>
            <pubDate>Fri, 05 Dec 2014 00:00:00 GMT</pubDate>
            <description><![CDATA[The observant amongst you may have noticed that this blog has a brand new and shiny domain name! That's right, after happily trading under "icanmakethiswork.blogspot.com" for the longest time it's now "blog.icanmakethiswork.io". Trumpets and fanfare!]]></description>
            <content:encoded><![CDATA[<p>The observant amongst you may have noticed that this blog has a brand new and shiny domain name! That&#x27;s right, after happily trading under &quot;icanmakethiswork.blogspot.com&quot; for the longest time it&#x27;s now &quot;blog.icanmakethiswork.io&quot;. Trumpets and fanfare!</p><p>Why the change? Well let&#x27;s break that question down a little. First of all, why change at all? Secondly, why change to blog.icanmakethiswork.io?</p><h2>Why do things have to change at all?</h2><p>I mean, weren&#x27;t we happy? Wasn&#x27;t it all good? Well quite. For the record, I have no complaints of Blogger who have hosted my blog since it began. They&#x27;ve provided good tools and a good service and I&#x27;m happy with them.</p><p>That said, I&#x27;ve been toying with the idea for a while now of trying out a few other blogging solutions - possibly even hosting it myself. Whilst my plans are far from definite at the moment I&#x27;m aware that I don&#x27;t own icanmakethiswork.blogspot.com - I can&#x27;t take it with me. So if I want to make a move to change my blogging solution a first step is establishing my own domain name for my blog. I&#x27;ve done that now. If and when I up sticks, people will hopefully come with me as the URL for my blog should not change.</p><p>Also, in the back of my mind I&#x27;m aware that Google owns Blogger. Given their recent spate of closing services it&#x27;s certainly possible that the Google reaper could one day call for Blogger. So it makes sense to be ready to move my blog elsewhere should that day come.</p><h2>Why blog.icanmakethiswork.io?</h2><p>Why indeed? And why the &quot;.io&quot; suffix? Doesn&#x27;t that just make you a desperate follower of fashion?</p><p>Good questions all, and &quot;no, I hope not&quot;. My original plans were to use the domain name &quot;icanmakethiswork.com&quot;. icanmakethiswork was the name of the blog and it made sense to keep it in the URL. So off I went to register the domain name when to my surprise I discovered this:</p><p><img src="../static/blog/2014-12-05-whats-in-a-name/Screenshot%2B2014-12-05%2B05.39.00.png"/></p><p>My domain is being <a href="https://en.wikipedia.org/wiki/Cybersquatting">cybersquatted</a>! I mean.... What??!!!!</p><p>I started to wonder &quot;is there another icanmakethiswork out there&quot;? Am I not the <a href="http://youtu.be/z8f2mW1GFSI">one and only</a>? So I checked with DuckDuckGo (&quot;The search engine that doesn&#x27;t track you.&quot;) and look what I found:</p><p><img src="../static/blog/2014-12-05-whats-in-a-name/Screenshot%2B2014-12-05%2B05.41.59.png"/></p><p>A whole screen of me. Just me.</p><p>As of June 3rd 2014 someone has been sitting on my blog name. I was actually rather outraged by this. I became even more so as I discovered that there was a mechanism (not free) by which I could try and buy it off the squatter. I could instead be like my life idol Madonna and go to court to get it back. But frankly in this sense I&#x27;m more like Rachel Green in Friends; not litigous.</p><p>So that&#x27;s why I went for icanmakethiswork.io instead. Path of least resistance and all that. I&#x27;d still like icanmakethiswork.com to be mine but I&#x27;m not going to court and I&#x27;m not paying the squatter. Maybe one day I&#x27;ll get it. Who knows?</p><p>Either way, from now on this is blog.icanmakethiswork.io - please stick around!</p><h2>Is anything else going to change?</h2><p>Not for now, no.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Pretending to be someone you're not and the dark pit of despair]]></title>
            <link>https://blog.johnnyreilly.com/2014/11/26/Coded-UI-IE-11-and-the-runas-problem</link>
            <guid>Pretending to be someone you're not and the dark pit of despair</guid>
            <pubDate>Wed, 26 Nov 2014 00:00:00 GMT</pubDate>
            <description><![CDATA[(Coded UI, IE 11 and the "runas" problem)]]></description>
            <content:encoded><![CDATA[<h2><sub>(Coded UI, IE 11 and the &quot;runas&quot; problem)</sub></h2><p>&quot;I&#x27;m not angry, I&#x27;m just disappointed.&quot;</p><p>That&#x27;s kind of how I feel about Coded UI tests. It may well be that you&#x27;ve never heard of them - in my experience very few people seem to be aware of them. What are they? Well, I&#x27;ve never used <a href="http://www.seleniumhq.org/">Selenium</a> but as best I understand Coded UI is Microsoft&#x27;s own version of that. Namely it&#x27;s a way to automate testing, in my case browser-based testing. You can write a suite of tests that will spin up your application and test it out, going from screen to screen, URL to URL and asserting all is as you would expect.</p><p>The project that I&#x27;m currently working on has a pretty comprehensive set of tests covering the use of the application. Each night as the clock strikes midnight a lonely computer in the West End of London whirrs into life and runs the full suite. It takes about 8 hours and at the end a report slips into your inbox letting you know of any failures.</p><h2>Sounds brilliant right? How could someone not love this?</h2><p>Well a number of reasons. First of all, <em>it takes 8 hours</em>!!!! That&#x27;s a long time; I&#x27;d rather learn what I broke today rather than tomorrow.</p><p>Also, and this is probably more significant, Coded UI tests are pretty flaky. Let me qualify that. For a test to be particularly useful it has to be quick, repeatable and reliable. As I&#x27;ve said, Coded UI tests are not quick.</p><p>By their very nature integration tests (of which Coded UI tests are a type) can never be entirely reliably repeatable. They test your app in it&#x27;s entirety. So, for example, if a 3rd party service goes down for 5 minutes then you will get failed tests. You&#x27;ll burn time investigating these false positives.</p><p>Further to that, Coded UI tests are repeatable, except when they&#x27;re not. I&#x27;ve seen colleagues reduced to near tears by incredible sensitivity of Coded UI tests. Out of the box Coded UI tests appear to ship with the <a href="http://blog.codinghorror.com/the-works-on-my-machine-certification-program/">&quot;Works on my machine&quot;</a> guarantee. It requires far more effort that you&#x27;d expect to come up with tests that can be reliably expected to pass. They will fail for surprising reasons. For instance, did you know that using the 2.x branch of jQuery won&#x27;t work with Coded UI? <a href="https://connect.microsoft.com/VisualStudio/Feedback/Details/794841">Neither did I.</a> I&#x27;ve lost track of the time that has been wasted running the same test in multiple different environments trying to identify what exactly is upsetting Coded UI about the environment this time.</p><p>It is sad but true that with Coded UI tests you can spend an <em>enormous</em> amount of time maintaining the test pack on a day to day basis. As infrastructure and project dependencies are upgraded you will sadly discover Coded UI has once again gone into the foetal position and has to tempted back to normal functioning by whispering sweet nothings in it&#x27;s ear. (<em>&quot;It&#x27;s not true that they&#x27;ve ended support for Windows XP&quot; / &quot;IE 6 will live forever&quot;</em> and so on)</p><p>Coded UI also appears to be badly supported by Microsoft. Documentation is pretty sparse and, as we&#x27;ll come back to in a minute, Coded UI is sometimes broken or damaged by other products shipped by Microsoft. This makes it hard to have faith in Coded UI. Indeed, if you&#x27;re thinking of automating your QA testing my advice would be &quot;look into Selenium&quot;. Not because I&#x27;ve used it (I haven&#x27;t) but those I&#x27;ve met who have used Selenium and Coded UI say Selenium wins hands down.</p><h2>And yet, and yet...</h2><p>All of the above said, if you have a Coded UI test suite it can still pay dividends. Significant dividends. As I mentioned, my current project has a significant coverage of Coded UI tests. We&#x27;ve crawled over a lot of broken glass to put these together. But now they&#x27;re there it is undeniably useful.</p><p>Every now and then we&#x27;ll do a significant refactor of part of the application. For instance, we&#x27;ve entirely changed our persistence strategy in the app but been able to check the code in with a high degree of confidence gleaned from running our test suite using the refactored codebase.</p><p>Let me be clear: Coded UI tests can be useful.</p><h2>The &quot;runas&quot; Problem</h2><p>Long preamble over, this post is about how to work around the latest issue Coded UI has thrown in our direction. I call it the &quot;runas&quot; problem. Our application is a Knockout / ASP.Net MVC web app built to be used in an intranet environment. By that I mean that identity is handled by Active Directory / <a href="http://en.wikipedia.org/wiki/Integrated_Windows_Authentication">Windows Authentication</a>. When someone logs into our app we know who they are without them having to directly supply us with a username and password. No, by logging into their computer they have announced just who they are and Internet Explorer (for it is he) will pass along the credentials. (The app can be used with pretty much any browser but we&#x27;re only mandated to support IE 9+.)</p><p>In order that we can test the app we have a number of test accounts set up in Active Directory. These test accounts have been assigned various roles (viewer / editor / administrator etc). Our tests are designed to run using these accounts in order that all scenarios can be adequately tested.</p><p>To achieve this lofty goal the following code (or something very like it) is executed as the first step in any Coded UI test:</p><pre><code class="language-cs">string browserLocation = &quot;C:\\Program Files\\Internet Explorer\\iexplore.exe&quot;;
string url = &quot;http://localhost:12345/&quot;;
string username = &quot;test.editor&quot;;
string domain = &quot;theDomain&quot;;
var password = new SecureString();
foreach (char c in &quot;test.editor.password&quot;)
{
    password.AppendChar(c);
}

ApplicationUnderTest.Launch(browserLocation, null, url, username, password, domain);
</code></pre><p>What this does is fire up Internet Explorer as the supplied user of theDomain\test.editor, and navigate to the home page. With that as our starting place we could dependably then run a test as this test user. This was a solution not without quirks (on occasion Coded UI tests would &quot;stutter&quot; - repeating each keypress 3 times with calamitous effects). But generally, this worked.</p><p>Until that is either Visual Studio 2013 Update 3 or Internet Explorer 11 was installed. One of these (and it appears to be hotly contested) broke the ability to run the above code successfully. After these were installed running the above code resulted in the following error message:</p><blockquote><p>&quot;The application cannot be started. This could be due to one of the following reasons:</p><ol><li>Another instance of the application is already running and only one instance can be running at a time.</li><li>The application started another process and has now stopped. You may need to launch the process directly.</li><li>You do not have sufficient privileges for this application.&quot; File: C:\Program Files\Internet Explorer\iexplore.exe.&quot;</li></ol></blockquote><p>Lamentably, this was pretty much unresolvable and <a href="https://connect.microsoft.com/VisualStudio/feedbackdetail/view/949049/coded-ui-cannot-run-as-a-different-user-with-visual-studio-2013-update-3">logging it with Microsoft yielded nothing helpful</a>. This is what I mean about Coded UI being badly supported by Microsoft. Despite my best efforts to report this issue both to Connect and <a href="http://social.msdn.microsoft.com/Forums/vstudio/en-US/f48665e4-569a-4b67-9bdb-5522b2adffb2/cannot-run-coded-ui-tests-as-different-user-on-windows-81?forum=vsmantest#28c9decb-b579-4848-a7a9-f41c57584d59">elsewhere</a> and in the end nothing useful happened.</p><p>So what to do? I still have Coded UI tests, I still need to be able to run them. And crucially I need to be able to run them impersonating a different user. What to do indeed....</p><h2>The <strike>hack</strike></h2><p>workaround</p><p>After IE 11 / Visual Studio Update 3 / whatev&#x27;s was installed I was left with a setup that allowed me to run Coded UI tests, <u>but only</u></p><p>as the current user. On that basis I started looking into a little MVC jiggery pokery. All my controllers inherit from a single base controller. Inside there I placed the following extra override:</p><pre><code class="language-cs">public abstract class BaseController : System.Web.Mvc.Controller
{
  //...

  protected override void OnAuthorization(AuthorizationContext filterContext)
  {
#if DEBUG
    if (filterContext.HttpContext.IsDebuggingEnabled)// Is compilation debug=&quot;true&quot; set in the web.config?
    {
      var userToImpersonate = Session[&quot;UserToImpersonate&quot;] as string;
      if (!string.IsNullOrEmpty(userToImpersonate))
      {
        // userToImpersonate example: &quot;test.editor@theDomain.com&quot;
        filterContext.HttpContext.User = new RolePrincipal(new WindowsIdentity(userToImpersonate));
      }
    }
#endif
      base.OnAuthorization(filterContext);
  }

  //...
}
</code></pre><p>Each request will trigger this method as one of the first steps in the MVC pipeline. What it does is checks the <code>Session</code> for a user to impersonate. (Yes I&#x27;m as wary of Session as the next chap - but in this case it&#x27;s the right tool for the job.) If a user has been specified then it replaces the current user with the <code>Session</code> user. From this point forwards the app is effectively running as that user. That&#x27;s great!</p><p>In order that Coded UI can make use of this mechanism we need to introduce a &quot;hook&quot;. This is going to look a bit hacky - bear with me. Inside <code>Global.asax.cs</code> we&#x27;re going to add a <code>Session_Start</code> method:</p><pre><code class="language-cs">protected void Session_Start(object sender, EventArgs eventArgs)
{
#if DEBUG
    // If a user to impersonate has been supplied then add this user to the session
    // Impersonation will happen in the OnAuthorization method of our base MVC controller
    // Note, this is only allowed in debug mode - not in release mode
    // This exists purely to support coded ui tests
    if (Context.IsDebuggingEnabled)  // Is compilation debug=&quot;true&quot; set in the web.config?
    {
        var userToImpersonate = Request.QueryString[&quot;UserToImpersonate&quot;] as string;
        if (!string.IsNullOrEmpty(userToImpersonate))
        {
            Session.Add(&quot;UserToImpersonate&quot;, userToImpersonate);
        }
    }
#endif
}
</code></pre><p>For the first Request in a Session this checks the <code>QueryString</code> for a parameter called <code>UserToImpersonate</code>. If it&#x27;s found then it&#x27;s placed into <code>Session</code>. With this hook exposed we can now amend the first step that all our Coded UI tests follow:</p><pre><code class="language-cs">// Various lines commented out as doesn&#x27;t work with IE 11 - left as an example of how it could be done in the past
//string browserLocation = &quot;C:\\Program Files\\Internet Explorer\\iexplore.exe&quot;;
string url = &quot;http://localhost:12345/&quot;;
string username = &quot;test.editor&quot;;
string domain = &quot;theDomain.com&quot;;
//var password = new SecureString();
//foreach (char c in &quot;test.editor.password&quot;)
//{
//    password.AppendChar(c);
//}

//ApplicationUnderTest.Launch(browserLocation, null, url, username, password, domain);

// Suffixing url with UrlToImpersonate which will be picked up in Session_Start and used to impersonate
// in OnAuthorization in BaseController.  Also no longer using ApplicationUnderTest.Launch; switched to
// BrowserWindow.Launch
// No longer used parameters: browserLocation, password
var userToImpersonate = username + &quot;@&quot; + domain; // eg &quot;test.editor@theDomain.com&quot;
var urlWithUser = url + &quot;?UserToImpersonate=&quot; + HttpUtility.UrlEncode(userToImpersonate);
var browser = BrowserWindow.Launch(urlWithUser, &quot;-nomerge&quot;); // &quot;-nomerge&quot; flag forces a new session
</code></pre><p>As you can see we actually need less when we&#x27;re using this approach. We no longer need to directly specify the password or the browser location. And the user to impersonate is now passed in as the part of the initial URL used to launch the test.</p><p>Pay careful attention to the &quot;-nomerge&quot; flag that is passed in. This ensures that when another browser instance is opened a new session will be started. This is essential for &quot;multi-user&quot; tests that run tests for <em>different</em> users as part of the same test. It ensures that &quot;test.editor&quot; and &quot;test.different.editor&quot; can co-exist happily.</p><h2>What do I think of the workaround?</h2><p>This approach works reliably and dependably. More so than the original approach which on occasion wouldn&#x27;t work or would &quot;stutter&quot; keypresses. That&#x27;s the good news.</p><p>The not so good news is that this approach is, in my view, a bit of hack. I want you to know that this isn&#x27;t my ideal.</p><p>I <em>really</em> don&#x27;t like having to change the actual system code to facilitate the impersonation requirement. Naturally we only ship the release and not the debug builds to Production so the &quot;back door&quot; that this approach provides will not exist in our Production builds. It will only be accessible in our development environments and on our Coded UI test server. But it feels oh so wrong that there is an effective potential back door in the system now. Well, only if the stars were to align in a really terrible (and admittedly rather unlikely) way. But still, you take my point. Caveat emptor and all that. This is something of a cutdown example to illustrate the point. If anyone else intends to use this then I&#x27;d suggest doing more to safeguard your approach. Implementing impersonation allowlists so &quot;any&quot; user cannot be impersonated would be a sensible precaution to start with.</p><p>Perhaps this is just one more reason that I&#x27;m not that enamoured of Coded UI. Once again it is useful but I&#x27;ve had to compromise more than I&#x27;d like to keep it&#x27;s use. If anyone out there has a better solution I would <em>love</em> to hear from you.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using Gulp in Visual Studio instead of Web Optimization]]></title>
            <link>https://blog.johnnyreilly.com/2014/11/04/using-gulp-in-visual-studio-instead-of-web-optimization</link>
            <guid>Using Gulp in Visual Studio instead of Web Optimization</guid>
            <pubDate>Tue, 04 Nov 2014 00:00:00 GMT</pubDate>
            <description><![CDATA[Update 17/02/2015: I've taken the approach discussed in this post a little further - you can see here]]></description>
            <content:encoded><![CDATA[<h3>Update 17/02/2015: I&#x27;ve taken the approach discussed in this post a little further - you can see <a href="https://blog.johnnyreilly.com/2015/02/using-gulp-in-asp-net-instead-of-web-optimization.html">here</a></h3><p>I&#x27;ve used a number of tools to package up JavaScript and CSS in my web apps. <a href="http://getcassette.net/">Andrew Davey&#x27;s tremendous Cassette</a> has been really useful. Also good (although less powerful/magical) has been Microsoft&#x27;s very own <a href="https://www.nuget.org/packages/Microsoft.AspNet.Web.Optimization/">Microsoft.AspNet.Web.Optimization</a> that ships with MVC.</p><p>I was watching the <a href="http://youtu.be/NgbA2BxNweE?list=PL0M0zPgJ3HSftTAAHttA3JQU4vOjXFquF">ASP.NET Community Standup from October 7th, 2014</a> and learned that the ASP.Net team is not planning to migrate <a href="https://www.nuget.org/packages/Microsoft.AspNet.Web.Optimization/">Microsoft.AspNet.Web.Optimization</a> to the next version of ASP.Net. Instead they&#x27;re looking to make use of JavaScript task runners like <a href="http://gruntjs.com/">Grunt</a> and maybe <a href="http://gulpjs.com/">Gulp</a>. Perhaps you&#x27;re even dimly aware that they&#x27;ve been taking steps to make these runners more of a first class citizen in Visual Studio, hence the recent release of the new and groovy <a href="http://visualstudiogallery.msdn.microsoft.com/8e1b4368-4afb-467a-bc13-9650572db708">Task Runner Explorer</a>.</p><p>Gulp has been on my radar for a while now as has Grunt. By &quot;on my radar&quot; what I really mean is &quot;Hmmmm, I really need to learn this..... perhaps I could wait until the <a href="http://en.wikipedia.org/wiki/Videotape_format_war">Betamax vs VHS battles</a> are done? Oh never mind, here we go...&quot;.</p><p>My understanding is that Grunt and Gulp essentially do the same thing (run tasks in JavaScript) but have different approaches. Grunt is more about configuration, Gulp is more about code. At present Gulp also has a performance advantage as it does less IO than Grunt - though I understand that&#x27;s due to change in the future. But generally my preference is code over configuration. On that basis I decided that I was going to give Gulp first crack.</p><h2>Bub bye Web Optimization</h2><p>I already had a project that used <a href="https://github.com/johnnyreilly/Proverb">Web Optimization</a> to bundle JavaScript and CSS files. When debugging on my own machine Web Optimization served up the full JavaScript and CSS files. Thanks to the magic of source maps I was able to debug the TypeScript that created the JavaScript files too. Which was nice. When I deployed to production, Web Optimization minified and concatenated the JavaScript and CSS files. This meant I had a single HTTP request for JavaScript and a single HTTP request for CSS. This was also... nooice!</p><p>I took a copy of my existing project and created a <a href="https://github.com/johnnyreilly/Proverb-gulp">new repo for it on GitHub</a>. It was very simple in terms of bundling. It had a <code>BundleConfig</code> that created 2 bundles; 1 for JavaScript and 1 for CSS:</p><pre><code class="language-cs">using System.Web;
using System.Web.Optimization;

namespace Proverb.Web
{
    public class BundleConfig
    {
        // For more information on bundling, visit http://go.microsoft.com/fwlink/?LinkId=301862
        public static void RegisterBundles(BundleCollection bundles)
        {
            var angularApp = new ScriptBundle(&quot;~/angularApp&quot;).Include(

                // Vendor Scripts
                &quot;~/scripts/jquery-{version}.js&quot;,
                &quot;~/scripts/angular.js&quot;,
                &quot;~/scripts/angular-animate.js&quot;,
                &quot;~/scripts/angular-route.js&quot;,
                &quot;~/scripts/angular-sanitize.js&quot;,
                &quot;~/scripts/angular-ui/ui-bootstrap-tpls.js&quot;,

                &quot;~/scripts/toastr.js&quot;,
                &quot;~/scripts/moment.js&quot;,
                &quot;~/scripts/spin.js&quot;,
                &quot;~/scripts/underscore.js&quot;,

                // Bootstrapping
                &quot;~/app/app.js&quot;,
                &quot;~/app/config.route.js&quot;,

                // common Modules
                &quot;~/app/common/common.js&quot;,
                &quot;~/app/common/logger.js&quot;,
                &quot;~/app/common/spinner.js&quot;,

                // common.bootstrap Modules
                &quot;~/app/common/bootstrap/bootstrap.dialog.js&quot;
                );

            // directives
            angularApp.IncludeDirectory(&quot;~/app/directives&quot;, &quot;*.js&quot;, true);

            // services
            angularApp.IncludeDirectory(&quot;~/app/services&quot;, &quot;*.js&quot;, true);

            // controllers
            angularApp.IncludeDirectory(&quot;~/app/admin&quot;, &quot;*.js&quot;, true);
            angularApp.IncludeDirectory(&quot;~/app/about&quot;, &quot;*.js&quot;, true);
            angularApp.IncludeDirectory(&quot;~/app/dashboard&quot;, &quot;*.js&quot;, true);
            angularApp.IncludeDirectory(&quot;~/app/layout&quot;, &quot;*.js&quot;, true);
            angularApp.IncludeDirectory(&quot;~/app/sayings&quot;, &quot;*.js&quot;, true);
            angularApp.IncludeDirectory(&quot;~/app/sages&quot;, &quot;*.js&quot;, true);

            bundles.Add(angularApp);

            bundles.Add(new StyleBundle(&quot;~/Content/css&quot;).Include(
                &quot;~/content/ie10mobile.css&quot;,
                &quot;~/content/bootstrap.css&quot;,
                &quot;~/content/font-awesome.css&quot;,
                &quot;~/content/toastr.css&quot;,
                &quot;~/content/styles.css&quot;
            ));
        }
    }
}
</code></pre><p>I set myself a task. I wanted to be able to work in <!-- -->*<strong>exactly</strong>*<!-- --> the way I was working now. But using Gulp instead of Web Optimization. I wanted to lose the BundleConfig above and remove Web Optimization from my application, secure in the knowledge that I had lost nothing. Could it be done? Read on!</p><h2>Installing Gulp (and Associates)</h2><p>I fired up Visual Studio and looked for an excuse to use the Task Runner Explorer. The first thing I needed was Gulp. My machine already had Node and NPM installed so I went to the command line to install Gulp globally:</p><pre><code class="language-ps">npm install gulp -g
</code></pre><p>Now to start to plug Gulp into my web project. It was time to make the introductions: Visual Studio meet NPM. At the root of the web project I created a <code>package.json</code> file by executing the following command and accepting all the defaults:</p><pre><code class="language-ps">npm init
</code></pre><p>I wanted to add Gulp as a development dependency of my project: (&quot;Development&quot; because I only need to run tasks at development time. My app has no dependency on Gulp at runtime - at that point it&#x27;s just about serving up static files.)</p><pre><code class="language-ps">npm install gulp --save-dev
</code></pre><p>This installs gulp local to the project as a development dependency. As a result we now have a &quot;node_modules&quot; folder sat in our root which contains our node packages. Currently, as our <code>package.json</code> reveals, this is only gulp:</p><pre><code class="language-json">&quot;devDependencies&quot;: {
    &quot;gulp&quot;: &quot;^3.8.8&quot;
  }
</code></pre><p>It&#x27;s time to go to town. Let&#x27;s install all the packages we&#x27;re going to need to bundle and minify JavaScript and CSS:</p><pre><code class="language-sh">npm install gulp-concat gulp-uglify gulp-rev del path gulp-ignore gulp-asset-manifest gulp-minify-css --save-dev
</code></pre><p>This installs the packages as dev dependencies (as you&#x27;ve probably guessed) and leaves us with a list of dev dependencies like this:</p><pre><code class="language-json">&quot;devDependencies&quot;: {
    &quot;del&quot;: &quot;^0.1.3&quot;,
    &quot;gulp&quot;: &quot;^3.8.8&quot;,
    &quot;gulp-asset-manifest&quot;: &quot;0.0.5&quot;,
    &quot;gulp-concat&quot;: &quot;^2.4.1&quot;,
    &quot;gulp-ignore&quot;: &quot;^1.2.1&quot;,
    &quot;gulp-minify-css&quot;: &quot;^0.3.10&quot;,
    &quot;gulp-rev&quot;: &quot;^1.1.0&quot;,
    &quot;gulp-uglify&quot;: &quot;^1.0.1&quot;,
    &quot;path&quot;: &quot;^0.4.9&quot;
  }
</code></pre><h2>Making <code>gulpfile.js</code></h2><p>So now I was ready. I had everything I needed to replace my <code>BundleConfig.cs</code>. I created a new file called <code>gulpfile.js</code> in the root of my web project that looked like this:</p><pre><code class="language-js">/// &lt;vs AfterBuild=&#x27;default&#x27; /&gt;
var gulp = require(&#x27;gulp&#x27;);

// Include Our Plugins
var concat = require(&#x27;gulp-concat&#x27;);
var ignore = require(&#x27;gulp-ignore&#x27;);
var manifest = require(&#x27;gulp-asset-manifest&#x27;);
var minifyCss = require(&#x27;gulp-minify-css&#x27;);
var uglify = require(&#x27;gulp-uglify&#x27;);
var rev = require(&#x27;gulp-rev&#x27;);
var del = require(&#x27;del&#x27;);
var path = require(&#x27;path&#x27;);

var tsjsmapjsSuffix = &#x27;.{ts,js.map,js}&#x27;;
var excludetsjsmap = &#x27;**/*.{ts,js.map}&#x27;;

var bundleNames = { scripts: &#x27;scripts&#x27;, styles: &#x27;styles&#x27; };

var filesAndFolders = {
  base: &#x27;.&#x27;,
  buildBaseFolder: &#x27;./build/&#x27;,
  debug: &#x27;debug&#x27;,
  release: &#x27;release&#x27;,
  css: &#x27;css&#x27;,

  // The fonts we want Gulp to process
  fonts: [&#x27;./fonts/*.*&#x27;],

  // The scripts we want Gulp to process - adapted from BundleConfig
  scripts: [
    // Vendor Scripts
    &#x27;./scripts/angular.js&#x27;,
    &#x27;./scripts/angular-animate.js&#x27;,
    &#x27;./scripts/angular-route.js&#x27;,
    &#x27;./scripts/angular-sanitize.js&#x27;,
    &#x27;./scripts/angular-ui/ui-bootstrap-tpls.js&#x27;,

    &#x27;./scripts/toastr.js&#x27;,
    &#x27;./scripts/moment.js&#x27;,
    &#x27;./scripts/spin.js&#x27;,
    &#x27;./scripts/underscore.js&#x27;,

    // Bootstrapping
    &#x27;./app/app&#x27; + tsjsmapjsSuffix,
    &#x27;./app/config.route&#x27; + tsjsmapjsSuffix,

    // common Modules
    &#x27;./app/common/common&#x27; + tsjsmapjsSuffix,
    &#x27;./app/common/logger&#x27; + tsjsmapjsSuffix,
    &#x27;./app/common/spinner&#x27; + tsjsmapjsSuffix,

    // common.bootstrap Modules
    &#x27;./app/common/bootstrap/bootstrap.dialog&#x27; + tsjsmapjsSuffix,

    // directives
    &#x27;./app/directives/**/*&#x27; + tsjsmapjsSuffix,

    // services
    &#x27;./app/services/**/*&#x27; + tsjsmapjsSuffix,

    // controllers
    &#x27;./app/about/**/*&#x27; + tsjsmapjsSuffix,
    &#x27;./app/admin/**/*&#x27; + tsjsmapjsSuffix,
    &#x27;./app/dashboard/**/*&#x27; + tsjsmapjsSuffix,
    &#x27;./app/layout/**/*&#x27; + tsjsmapjsSuffix,
    &#x27;./app/sages/**/*&#x27; + tsjsmapjsSuffix,
    &#x27;./app/sayings/**/*&#x27; + tsjsmapjsSuffix,
  ],

  // The styles we want Gulp to process - adapted from BundleConfig
  styles: [
    &#x27;./content/ie10mobile.css&#x27;,
    &#x27;./content/bootstrap.css&#x27;,
    &#x27;./content/font-awesome.css&#x27;,
    &#x27;./content/toastr.css&#x27;,
    &#x27;./content/styles.css&#x27;,
  ],
};

filesAndFolders.debugFolder =
  filesAndFolders.buildBaseFolder + &#x27;/&#x27; + filesAndFolders.debug + &#x27;/&#x27;;
filesAndFolders.releaseFolder =
  filesAndFolders.buildBaseFolder + &#x27;/&#x27; + filesAndFolders.release + &#x27;/&#x27;;

/**
 * Create a manifest depending upon the supplied arguments
 *
 * @param {string} manifestName
 * @param {string} bundleName
 * @param {boolean} includeRelativePath
 * @param {string} pathPrepend
 */
function getManifest(
  manifestName,
  bundleName,
  includeRelativePath,
  pathPrepend
) {
  // Determine filename (&quot;./build/manifest-debug.json&quot; or &quot;./build/manifest-release.json&quot;
  var manifestFile =
    filesAndFolders.buildBaseFolder + &#x27;manifest-&#x27; + manifestName + &#x27;.json&#x27;;

  return manifest({
    bundleName: bundleName,
    includeRelativePath: includeRelativePath,
    manifestFile: manifestFile,
    log: true,
    pathPrepend: pathPrepend,
    pathSeparator: &#x27;/&#x27;,
  });
}

// Delete the build folder
gulp.task(&#x27;clean&#x27;, function (cb) {
  del([filesAndFolders.buildBaseFolder], cb);
});

// Copy across all files in filesAndFolders.scripts to build/debug
gulp.task(&#x27;scripts-debug&#x27;, [&#x27;clean&#x27;], function () {
  return gulp
    .src(filesAndFolders.scripts, { base: filesAndFolders.base })
    .pipe(gulp.dest(filesAndFolders.debugFolder));
});

// Create a manifest.json for the debug build - this should have lots of script files in
gulp.task(&#x27;manifest-scripts-debug&#x27;, [&#x27;scripts-debug&#x27;], function () {
  return gulp
    .src(filesAndFolders.scripts, { base: filesAndFolders.base })
    .pipe(ignore.exclude(&#x27;**/*.{ts,js.map}&#x27;)) // Exclude ts and js.map files from the manifest (as they won&#x27;t become script tags)
    .pipe(getManifest(filesAndFolders.debug, bundleNames.scripts, true));
});

// Copy across all files in filesAndFolders.styles to build/debug
gulp.task(&#x27;styles-debug&#x27;, [&#x27;clean&#x27;], function () {
  return gulp
    .src(filesAndFolders.styles, { base: filesAndFolders.base })
    .pipe(gulp.dest(filesAndFolders.debugFolder));
});

// Create a manifest.json for the debug build - this should have lots of style files in
gulp.task(
  &#x27;manifest-styles-debug&#x27;,
  [&#x27;styles-debug&#x27;, &#x27;manifest-scripts-debug&#x27;],
  function () {
    return (
      gulp
        .src(filesAndFolders.styles, { base: filesAndFolders.base })
        //.pipe(ignore.exclude(&quot;**/*.{ts,js.map}&quot;)) // Exclude ts and js.map files from the manifest (as they won&#x27;t become script tags)
        .pipe(getManifest(filesAndFolders.debug, bundleNames.styles, true))
    );
  }
);

// Concatenate &amp; Minify JS for release into a single file
gulp.task(&#x27;scripts-release&#x27;, [&#x27;clean&#x27;], function () {
  return (
    gulp
      .src(filesAndFolders.scripts)
      .pipe(ignore.exclude(&#x27;**/*.{ts,js.map}&#x27;)) // Exclude ts and js.map files - not needed in release mode

      .pipe(concat(&#x27;app.js&#x27;)) // Make a single file - if you want to see the contents then include the line below
      //.pipe(gulp.dest(releaseFolder))

      .pipe(uglify()) // Make the file titchy tiny small
      .pipe(rev()) // Suffix a version number to it
      .pipe(gulp.dest(filesAndFolders.releaseFolder))
  ); // Write single versioned file to build/release folder
});

// Create a manifest.json for the release build - this should just have a single file for scripts
gulp.task(&#x27;manifest-scripts-release&#x27;, [&#x27;scripts-release&#x27;], function () {
  return gulp
    .src(filesAndFolders.buildBaseFolder + filesAndFolders.release + &#x27;/*.js&#x27;)
    .pipe(getManifest(filesAndFolders.release, bundleNames.scripts, false));
});

// Copy across all files in filesAndFolders.styles to build/debug
gulp.task(&#x27;styles-release&#x27;, [&#x27;clean&#x27;], function () {
  return (
    gulp
      .src(filesAndFolders.styles)
      .pipe(concat(&#x27;app.css&#x27;)) // Make a single file - if you want to see the contents then include the line below
      //.pipe(gulp.dest(releaseFolder))

      .pipe(minifyCss()) // Make the file titchy tiny small
      .pipe(rev()) // Suffix a version number to it
      .pipe(
        gulp.dest(filesAndFolders.releaseFolder + &#x27;/&#x27; + filesAndFolders.css)
      )
  ); // Write single versioned file to build/release folder
});

// Create a manifest.json for the debug build - this should have a single style files in
gulp.task(
  &#x27;manifest-styles-release&#x27;,
  [&#x27;styles-release&#x27;, &#x27;manifest-scripts-release&#x27;],
  function () {
    return gulp
      .src(filesAndFolders.releaseFolder + &#x27;**/*.css&#x27;)
      .pipe(
        getManifest(
          filesAndFolders.release,
          bundleNames.styles,
          false,
          filesAndFolders.css + &#x27;/&#x27;
        )
      );
  }
);

// Copy across all fonts in filesAndFolders.fonts to both release and debug locations
gulp.task(&#x27;fonts&#x27;, [&#x27;clean&#x27;], function () {
  return gulp
    .src(filesAndFolders.fonts, { base: filesAndFolders.base })
    .pipe(gulp.dest(filesAndFolders.debugFolder))
    .pipe(gulp.dest(filesAndFolders.releaseFolder));
});

// Default Task
gulp.task(&#x27;default&#x27;, [
  &#x27;scripts-debug&#x27;,
  &#x27;manifest-scripts-debug&#x27;,
  &#x27;styles-debug&#x27;,
  &#x27;manifest-styles-debug&#x27;,
  &#x27;scripts-release&#x27;,
  &#x27;manifest-scripts-release&#x27;,
  &#x27;styles-release&#x27;,
  &#x27;manifest-styles-release&#x27;,
  &#x27;fonts&#x27;,
]);
</code></pre><h2>What <code>gulpfile.js</code> does</h2><p>This file does a number of things each time it is run. First of all it deletes any <code>build</code> folder in the root of the web project so we&#x27;re ready to build anew. Then it packages up files both for debug and for release mode. For debug it does the following:</p><ol><li>It copies the <code>ts</code>, <code>js.map</code> and <code>js</code> files declared in <code>filesAndFolders.scripts</code> to the <code>build/debug</code> folder preserving their original folder structure. (So, for example, <code>app/app.ts</code>, <code>app/app.js.map</code> and <code>app/app.js</code> will all end up at <code>build/debug/app/app.ts</code>, <code>build/debug/app/app.js.map</code> and <code>build/debug/app/app.js</code> respectively.) This is done to allow the continued debugging of the original TypeScript files when running in debug mode.</li><li>It copies the <code>css</code> files declared in <code>filesAndFolders.styles</code> to the <code>build/debug</code> folder preserving their original folder structure. (So <code>content/bootstrap.css</code> will end up at <code>build/debug/content/bootstrap.css</code>.)</li><li>It creates a <code>build/manifest-debug.json</code> file which contains details of all the script and style files that have been packaged up:</li></ol><pre><code class="language-json">{
  &quot;scripts&quot;: [
    &quot;scripts/angular.js&quot;,
    &quot;scripts/angular-animate.js&quot;,
    &quot;scripts/angular-route.js&quot;,
    &quot;scripts/angular-sanitize.js&quot;,
    &quot;scripts/angular-ui/ui-bootstrap-tpls.js&quot;,
    &quot;scripts/toastr.js&quot;,
    &quot;scripts/moment.js&quot;,
    &quot;scripts/spin.js&quot;,
    &quot;scripts/underscore.js&quot;,
    &quot;app/app.js&quot;,
    &quot;app/config.route.js&quot;,
    &quot;app/common/common.js&quot;,
    &quot;app/common/logger.js&quot;,
    &quot;app/common/spinner.js&quot;,
    &quot;app/common/bootstrap/bootstrap.dialog.js&quot;,
    &quot;app/directives/imgPerson.js&quot;,
    &quot;app/directives/serverError.js&quot;,
    &quot;app/directives/sidebar.js&quot;,
    &quot;app/directives/spinner.js&quot;,
    &quot;app/directives/waiter.js&quot;,
    &quot;app/directives/widgetClose.js&quot;,
    &quot;app/directives/widgetHeader.js&quot;,
    &quot;app/directives/widgetMinimize.js&quot;,
    &quot;app/services/datacontext.js&quot;,
    &quot;app/services/repositories.js&quot;,
    &quot;app/services/repository.sage.js&quot;,
    &quot;app/services/repository.saying.js&quot;,
    &quot;app/about/about.js&quot;,
    &quot;app/admin/admin.js&quot;,
    &quot;app/dashboard/dashboard.js&quot;,
    &quot;app/layout/shell.js&quot;,
    &quot;app/layout/sidebar.js&quot;,
    &quot;app/layout/topnav.js&quot;,
    &quot;app/sages/sageDetail.js&quot;,
    &quot;app/sages/sageEdit.js&quot;,
    &quot;app/sages/sages.js&quot;,
    &quot;app/sayings/sayingEdit.js&quot;,
    &quot;app/sayings/sayings.js&quot;
  ],
  &quot;styles&quot;: [
    &quot;content/ie10mobile.css&quot;,
    &quot;content/bootstrap.css&quot;,
    &quot;content/font-awesome.css&quot;,
    &quot;content/toastr.css&quot;,
    &quot;content/styles.css&quot;
  ]
}
</code></pre><p>For release our gulpfile works with the same resources but has a different aim. Namely to minimise the the number of HTTP requests, obfuscate the code and version the files produced to prevent caching issues. To achieve those lofty aims it does the following:</p><ol><li>It concatenates together all the <code>js</code> files declared in <code>filesAndFolders.scripts</code>, minifies them and writes them to a single <code>build/release/app-{xxxxx}.js</code> file (where <code>-{xxxxx}</code> represents a version created by gulp-rev).</li><li>It concatenates together all the <code>css</code> files declared in <code>filesAndFolders.styles</code>, minifies them and writes them to a single <code>build/release/css/app-{xxxxx}.css</code> file. The file is placed in a css subfolder because of relative paths specified in the CSS file.</li><li>It creates a <code>build/manifest-release.json</code> file which contains details of all the script and style files that have been packaged up:</li></ol><pre><code class="language-json">{
  &quot;scripts&quot;: [&quot;app-95d1e06d.js&quot;],
  &quot;styles&quot;: [&quot;css/app-1a6256ea.css&quot;]
}
</code></pre><p>As you can see, the number of files included are reduced down to 2; 1 for JavaScript and 1 for CSS.</p><p>Finally, for both the debug and release packages the contents of the <code>fonts</code> folder is copied across wholesale, preserving the original folder structure. This is because the CSS files contain relative references that point to the font files. If I had image files which were referenced by my CSS I&#x27;d similarly need to include these in the build process.</p><h2>Task Runner Explorer gets in on the action</h2><p>The eagle eyed amongst you will also have noticed a peculiar first line to our <code>gulpfile.js</code>:</p><pre><code class="language-js">/// &lt;vs AfterBuild=&#x27;default&#x27; /&gt;
</code></pre><p>This mysterious comment is actually how the Task Runner Explorer hooks our <code>gulpfile.js</code> into the Visual Studio build process. Our &quot;magic comment&quot; ensures that on the <code>AfterBuild</code> event, Task Runner Explorer runs the <code>default</code> task in our <code>gulpfile.js</code>. The reason we&#x27;re using the <code>AfterBuild</code> event rather than the <code>BeforeBuild</code> event is because our project contains TypeScript and we need the transpiled JavaScript to be created before we can usefully run our package tasks. If we were using JavaScript alone then that wouldn&#x27;t be an issue and either build event would do.</p><p><img src="../static/blog/2014-11-04-using-gulp-in-visual-studio-instead-of-web-optimization/Screenshot%2B2014-10-21%2B17.02.11.png"/></p><h2>How do I use this in my HTML?</h2><p>Well this is magnificent - we have a gulpfile that builds our debug and release packages. The question now is, how do we use it?</p><p>Web Optimization made our lives really easy. Up in my head I had a <code>@Styles.Render(&quot;~/Content/css&quot;)</code> which pushed out my CSS and down at the foot of the body tag I had a <code>@Scripts.Render(&quot;~/angularApp&quot;)</code> which pushed out my script tags. <code>Styles</code> and <code>Scripts</code> are server-side utilities. It would be very easy to write equivalent utility classes that, depending on whether we were in debug or not, read the appropriate <code>build/manifest-xxxxxx.json</code> file and served up either debug or release <code>style</code> / <code>script</code> tags.</p><p>That would be pretty simple - and for what it&#x27;s worth <!-- -->*<!-- -->*<!-- -->simple is <u>good</u></p><p>*<!-- -->*<!-- -->. But today I felt like a challenge. What say server side rendering had been outlawed? A draconian ruling had been passed and all you had to play with was HTML / JavaScript and a server API that served up JSON? What would you do then? (All fantasy I know... But go with me on this - it&#x27;s a journey.) Or more sensibly, what if you just want to remove some of the work your app is doing server-side to bundle and minify. Just serve up static assets instead. Spend less money in Azure why not?</p><p>Before I make all the changes let&#x27;s review where we were. I had a single MVC view which, in terms of bundles, CSS and JavaScript pretty much looked like this:</p><pre><code class="language-html">&lt;!DOCTYPE html&gt;
&lt;html&gt;
  &lt;head&gt;
    &lt;!-- ... --&gt;
    @Styles.Render(&quot;~/Content/css&quot;)
  &lt;/head&gt;
  &lt;body&gt;
    &lt;!-- ... --&gt;

    @Scripts.Render(&quot;~/angularApp&quot;)
    &lt;script&gt;
      (function () {
        $.getJSON(&#x27;@Url.Content(&quot;~/Home/StartApp&quot;)&#x27;).done(function (
          startUpData
        ) {
          var appConfig = $.extend({}, startUpData, {
            appRoot: &#x27;@Url.Content(&quot;~/&quot;)&#x27;,
            remoteServiceRoot: &#x27;@Url.Content(&quot;~/api/&quot;)&#x27;,
          });

          angularApp.start({
            thirdPartyLibs: {
              moment: window.moment,
              toastr: window.toastr,
              underscore: window._,
            },
            appConfig: appConfig,
          });
        });
      })();
    &lt;/script&gt;
  &lt;/body&gt;
&lt;/html&gt;
</code></pre><p>This is already more a complicated example than most peoples use cases. Essentially what&#x27;s happening here is both bundles are written out as part of the HTML and then, once the scripts have loaded the Angular app is bootstrapped with some configuration loaded from the server by a good old jQuery AJAX call.</p><p>After reading <a href="http://www.html5rocks.com/en/tutorials/speed/script-loading/">an article about script loading by the magnificently funny Jake Archibald</a> I felt ready. I cast my MVC view to the four winds and created myself a straight HTML file:</p><pre><code class="language-html">&lt;!DOCTYPE html&gt;
&lt;html&gt;
  &lt;head&gt;
    &lt;!-- ... --&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;!-- ... --&gt;

    &lt;script src=&quot;Scripts/jquery-2.1.1.min.js&quot;&gt;&lt;/script&gt;
    &lt;script&gt;
      (function () {
        var appConfig = {};
        var scriptsToLoad;

        /**
         * Handler which fires as each script loads
         */
        function onScriptLoad(event) {
          scriptsToLoad -= 1;

          // Now all the scripts are present start the app
          if (scriptsToLoad === 0) {
            angularApp.start({
              thirdPartyLibs: {
                moment: window.moment,
                toastr: window.toastr,
                underscore: window._,
              },
              appConfig: appConfig,
            });
          }
        }

        // Load startup data from the server
        $.getJSON(&#x27;api/Startup&#x27;).done(function (startUpData) {
          appConfig = startUpData;

          // Determine the assets folder depending upon whether in debug mode or not
          var buildFolder = appConfig.appRoot + &#x27;build/&#x27;;
          var debugOrRelease = appConfig.inDebug ? &#x27;debug&#x27; : &#x27;release&#x27;;
          var manifestFile =
            buildFolder + &#x27;manifest-&#x27; + debugOrRelease + &#x27;.json&#x27;;
          var outputFolder = buildFolder + debugOrRelease + &#x27;/&#x27;;

          // Load JavaScript and CSS listed in manifest file
          $.getJSON(manifestFile).done(function (manifest) {
            manifest.styles.forEach(function (href) {
              var link = document.createElement(&#x27;link&#x27;);

              link.rel = &#x27;stylesheet&#x27;;
              link.media = &#x27;all&#x27;;
              link.href = outputFolder + href;

              document.head.appendChild(link);
            });

            scriptsToLoad = manifest.scripts.length;
            manifest.scripts.forEach(function (src) {
              var script = document.createElement(&#x27;script&#x27;);

              script.onload = onScriptLoad;
              script.src = outputFolder + src;
              script.async = false;

              document.head.appendChild(script);
            });
          });
        });
      })();
    &lt;/script&gt;
  &lt;/body&gt;
&lt;/html&gt;
</code></pre><p>If you very carefully compare the HTML above the MVC view that it replaces you can see the commonalities. They are doing pretty much the same thing - the only real difference is the bootstrapping API. Previously it was an MVC endpoint at <code>/Home/StartApp</code>. Now it&#x27;s a Web API endpoint at <code>api/Startup</code>. Here&#x27;s how it works:</p><ol><li>A jQuery AJAX call kicks off a call to load the bootstrapping / app config data. Importantly this data includes whether the app is running in debug or not.</li><li>Depending on the <code>isDebug</code> flag the app either loads the <code>build/manifest-debug.json</code> or <code>build/manifest-release.json</code> manifest.</li><li>For each CSS file in the styles bundle a <code>link</code> element is created and added to the page.</li><li>For each JavaScript file in the scripts bundle a <code>script</code> element is created and added to the page.</li></ol><p>It&#x27;s worth pointing out that this also has a performance edge over Web Optimization as the assets are loaded asynchronously! (Yes I know it says <code>script.async = false</code> but that&#x27;s not what you think it is... Go read Jake&#x27;s article!)</p><p>To finish off I had to make a few tweaks to my <code>web.config</code>:</p><pre><code class="language-xml">&lt;!-- Allow ASP.Net to serve up JSON files --&gt;
    &lt;system.webServer&gt;
        &lt;staticContent&gt;
            &lt;mimeMap fileExtension=&quot;.json&quot; mimeType=&quot;application/json&quot;/&gt;
        &lt;/staticContent&gt;
    &lt;/system.webServer&gt;

    &lt;!-- The build folder (and it&#x27;s child folder &quot;debug&quot;) will not be cached.
         When people are debugging they don&#x27;t want to cache --&gt;
    &lt;location path=&quot;build&quot;&gt;
        &lt;system.webServer&gt;
            &lt;staticContent&gt;
                &lt;clientCache cacheControlMode=&quot;DisableCache&quot;/&gt;
            &lt;/staticContent&gt;
        &lt;/system.webServer&gt;
    &lt;/location&gt;

    &lt;!-- The release folder will be cached for a loooooong time
         When you&#x27;re in Production caching is your friend --&gt;
    &lt;location path=&quot;build/release&quot;&gt;
        &lt;system.webServer&gt;
            &lt;staticContent&gt;
                &lt;clientCache cacheControlMode=&quot;UseMaxAge&quot;/&gt;
            &lt;/staticContent&gt;
        &lt;/system.webServer&gt;
    &lt;/location&gt;
</code></pre><h2>I want to publish, how do I include my assets?</h2><p>It&#x27;s time for some <code>csproj</code> trickery. I must say I think I&#x27;ll be glad to see the back of project files when ASP.Net vNext ships. This is what you need:</p><pre><code class="language-xml">&lt;Target Name=&quot;AfterBuild&quot;&gt;
    &lt;ItemGroup&gt;
      &lt;!-- what ever is in the build folder should be included in the project --&gt;
      &lt;Content Include=&quot;build\**\*.*&quot; /&gt;
    &lt;/ItemGroup&gt;
  &lt;/Target&gt;
</code></pre><p>What&#x27;s happening here is that <!-- -->*<em>after</em>*<!-- --> a build Visual Studio considers the complete contents of the build folder to part of the project. It&#x27;s after the build because the folder will be deleted and reconstructed as part of the build.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Caching and Cache-Busting in AngularJS with HTTP interceptors]]></title>
            <link>https://blog.johnnyreilly.com/2014/10/06/caching-and-cache-busting-in-angularjs-with-http-interceptors</link>
            <guid>Caching and Cache-Busting in AngularJS with HTTP interceptors</guid>
            <pubDate>Mon, 06 Oct 2014 00:00:00 GMT</pubDate>
            <description><![CDATA[Loading On-Demand and Caching]]></description>
            <content:encoded><![CDATA[<h2>Loading On-Demand and Caching</h2><p><a href="http://icanmakethiswork.blogspot.com/2014/03/caching-and-cache-busting-with-requirejs.html">I&#x27;ve written before about my own needs for caching and cache-busting when using RequireJS.</a> Long story short, when I&#x27;m loading <em>static</em> resources (scripts / views etc) on demand from the server I want to do a little URL fiddling along the way. I want to do that to cater for these 2 scenarios:</p><ol><li><em>In Development</em> <!-- -->-<!-- --> I want my URLs for static resources to have a unique querystring with each request to ensure that resources are loaded afresh each time. (eg so a GET request URL might look like this: &quot;/app/layout/sidebar.html?v=IAmRandomYesRandomRandomIsWhatIAm58965782&quot;)</li><li><em>In Production</em> <!-- -->-<!-- --> I want my URLs for static resources to have a querystring with that is driven by the application version number. This means that static resources can potentially be cached with a given querystring - subsequent requests should result in a 304 status code (indicating “Not Modified”) and local cache should be used. But when a new version of the app is rolled out and the app version is incremented then the querystring will change and resources will be loaded anew. (eg a GET request URL might look like this: &quot;/app/layout/sidebar.html?v=1.0.5389.16180&quot;)</li></ol><h2>Loading Views in AngularJS Using this Approach</h2><p>I have exactly the same use cases when I&#x27;m using AngularJS for views. Out of the box with AngularJS 1.x views are loaded lazily (unlike controllers, services etc). For that reason I want to use the same approach I&#x27;ve outlined above to load my views. Also, I want to prepend my URLs with the root of my application - this allows me to cater for my app being deployed in a virtual folder.</p><p>It turns out that&#x27;s pretty easy thanks to <a href="https://docs.angularjs.org/api/ng/service/$http#interceptors">HTTP interceptors</a>. They allow you to step into the pipeline and access and modify requests and responses made by your application. When AngularJS loads a view it&#x27;s the HTTP service doing the heavy lifting. So to deal with my own use case, I just need to add in an HTTP interceptor that amends the get request. This is handled in the example that follows in the <code>configureHttpProvider</code> function: (The example that follows is TypeScript - though if you just chopped out the interface and the type declarations you&#x27;d find this is pretty much idiomatic JavaScript)</p><pre><code class="language-js">interface config {
  appRoot: string; // eg &quot;/&quot;
  inDebug: boolean; // eg true or false
  urlCacheBusterSuffix: string; // if in debug this might look like this: &quot;v=1412608547047&quot;,
  // if not in debug this might look like this: &quot;v=1.0.5389.16180&quot;
}

function configureHttpProvider() {
  // This is the name of our HTTP interceptor
  var serviceId = &#x27;urlInterceptor&#x27;;

  // We&#x27;re going to create a service factory which will be our HTTP interceptor
  // It will be injected with a config object which is represented by the config interface above
  app.factory(serviceId, [
    &#x27;$templateCache&#x27;,
    &#x27;config&#x27;,
    function ($templateCache: ng.ITemplateCacheService, config: config) {
      // We&#x27;re returning an object literal with a single function; the &quot;request&quot; function
      var service = {
        request: request,
      };

      return service;

      // Request will be called with a request config object which includes the URL which we will amend
      function request(requestConfig: ng.IRequestConfig) {
        // For the loading of HTML templates we want the appRoot to be prefixed to the path
        // and we want a suffix to either allow caching or prevent caching
        // (depending on whether in debug mode or not)
        if (
          requestConfig.method === &#x27;GET&#x27; &amp;&amp;
          endsWith(requestConfig.url, &#x27;.html&#x27;)
        ) {
          // If this has already been placed into a primed template cache then we should leave the URL as is
          // so that the version in templateCache is served.  If we tweak the URL then it will not be found
          var cachedAlready = $templateCache.get(requestConfig.url);
          if (!cachedAlready) {
            // THIS IS THE MAGIC!!!!!!!!!!!!!!!

            requestConfig.url =
              config.appRoot + requestConfig.url + config.urlCacheBusterSuffix;

            // WE NOW HAVE A URL WHICH IS CACHE-FRIENDLY FOR OUR PURPOSES - REJOICE!!!!!!!!!!!
          }
        }

        return requestConfig;
      }

      // &lt;a href=&quot;http://stackoverflow.com/a/2548133/761388&quot;&gt;a simple JavaScript string &quot;endswith&quot; implementation&lt;/a&gt;
      function endsWith(str: string, suffix: string) {
        return str.indexOf(suffix, str.length - suffix.length) !== -1;
      }
    },
  ]);

  // This adds our service factory interceptor into the pipeline
  app.config([
    &#x27;$httpProvider&#x27;,
    function ($httpProvider: ng.IHttpProvider) {
      $httpProvider.interceptors.push(serviceId);
    },
  ]);
}
</code></pre><p>This interceptor steps in and amends each ajax request when all the following conditions hold true:</p><ol><li>It&#x27;s a GET request.</li><li>It&#x27;s requesting a file that ends &quot;.html&quot; - a template basically.</li><li>The template cache does not already contain the template. I left this out at first and got bitten when I found that the contents of the template cache were being ignored for pre-primed templates. Ugly.</li></ol><h2>Interesting technique.... How do I apply it?</h2><p>Isn&#x27;t it always much more helpful when you can see an example of code in the context of which it is actually used? Course it is! If you want that then take a look at <a href="https://github.com/johnnyreilly/Proverb/blob/master/Proverb.Web/app/app.ts"><code>app.ts</code></a> on GitHub. And if you&#x27;d like the naked JavaScript well <a href="https://github.com/johnnyreilly/Proverb/blob/master/Proverb.Web/app/app.js">that&#x27;s there too</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[He tasks me; he heaps me.... I will wreak that MOQ upon him.]]></title>
            <link>https://blog.johnnyreilly.com/2014/10/03/he-tasks-me-he-heaps-me-i-will-wreak</link>
            <guid>He tasks me; he heaps me.... I will wreak that MOQ upon him.</guid>
            <pubDate>Fri, 03 Oct 2014 00:00:00 GMT</pubDate>
            <description><![CDATA[Enough with the horrific misquotes - this is about Moq and async (that's my slight justification for robbing Herman Melville).]]></description>
            <content:encoded><![CDATA[<p>Enough with the horrific misquotes - this is about Moq and async (that&#x27;s my slight justification for robbing Herman Melville).</p><p>It&#x27;s pretty straightforward to use Moq to do async testing thanks to it&#x27;s marvellous <code>ReturnsAsync</code> method. That means it&#x27;s really easy to test a class that consumes an async API. Below is an example of a class that does just that: (it so happens that this class is a Web API controller but that&#x27;s pretty irrelevant to be honest)</p><pre><code class="language-cs">namespace Proverb.Web.Controllers
{
    // ISageService included inline for ease of explanation
    public interface ISageService
    {
        Task&lt;int&gt; DeleteAsync(int id);
    }

    public class SageController : ApiController
    {
        ISageService _sageService;

        public SageController(ISageService userService)
        {
            _sageService = userService;
        }

        public async Task&lt;IHttpActionResult&gt; Delete(int id)
        {
            int deleteCount = await _sageService.DeleteAsync(id);

            if (deleteCount == 0)
                return NotFound();
            else
                return Ok();
        }
   }
}
</code></pre><p>To mock the <code>_sageService.DeleteAsync</code> method it&#x27;s as easy as this:</p><pre><code class="language-cs">namespace Proverb.Web.Tests.ASPNet.Controllers
{
    [TestClass]
    public class SageControllerTests
    {
        private Mock&lt;ISageService&gt; _sageServiceMock;
        private SageController _controller;

        [TestInitialize]
        public void Initialise()
        {
            _sageServiceMock = new Mock&lt;ISageService&gt;();

            _controller = new SageController(_sageServiceMock.Object);
        }

        [TestMethod]
        public async Task Delete_returns_a_NotFound()
        {
            _sageServiceMock
                .Setup(x =&gt; x.DeleteAsync(_sage.Id))
                .ReturnsAsync(0); // This makes me *so* happy...

            IHttpActionResult result = await _controller.Delete(_sage.Id);

            var notFound = result as NotFoundResult;
            Assert.IsNotNull(notFound);
            _sageServiceMock.Verify(x =&gt; x.DeleteAsync(_sage.Id));
        }

        [TestMethod]
        public async Task Delete_returns_an_Ok()
        {
            _sageServiceMock
                .Setup(x =&gt; x.DeleteAsync(_sage.Id))
                .ReturnsAsync(1); // I&#x27;m still excited now!

            IHttpActionResult result = await _controller.Delete(_sage.Id);

            var ok = result as OkResult;
            Assert.IsNotNull(ok);
            _sageServiceMock.Verify(x =&gt; x.DeleteAsync(_sage.Id));
        }
    }
}
</code></pre><h2>But wait.... What if there&#x27;s like... Nothing?</h2><p>Nope, I&#x27;m not getting into metaphysics. Something more simple. What if the <code>async</code> API you&#x27;re consuming returns just a <code>Task</code>? Not a <code>Task</code> of <code>int</code> but a simple old humble <code>Task</code>.</p><p>So to take our example we&#x27;re going from this:</p><pre><code class="language-cs">public interface ISageService
    {
        Task&lt;int&gt; DeleteAsync(int id);
    }
</code></pre><p>To this:</p><pre><code class="language-ts">public interface ISageService
    {
        Task DeleteAsync(int id);
    }
</code></pre><p>Your initial thought might be &quot;well that&#x27;s okay, I&#x27;ll just lop off the <code>ReturnsAsync</code> statements and I&#x27;m home free&quot;. That&#x27;s what I thought anyway.... And I was <!-- -->*<strong>WRONG</strong>*<!-- -->! A moments thought and you realise that there&#x27;s still a return type - it&#x27;s just <code>Task</code> now. What you want to do is something like this:</p><pre><code class="language-cs">_sageServiceMock
                .Setup(x =&gt; x.DeleteAsync(_sage.Id))
                .ReturnsAsync(void); // This&#x27;ll definitely work... Probably
</code></pre><p>No it won&#x27;t - <code>void</code> is not a real type and much as you might like it to, this is not going to work.</p><p>So right now you&#x27;re thinking, well Moq probably has my back - it&#x27;ll have something like <code>ReturnsTask</code>, right? Wrong! It&#x27;s intentional it turns out - there&#x27;s a discussion on <a href="https://github.com/Moq/moq4/issues/117">GitHub about the issue</a>. And in that discussion there&#x27;s just what we need. We can use <code>Task.Delay</code> or <code>Task.FromResult</code> alongside Moq&#x27;s good old <code>Returns</code> method and we&#x27;re home free!</p><h2>Here&#x27;s one I made earlier...</h2><pre><code class="language-cs">namespace Proverb.Web.Controllers
{
    // ISageService again included inline for ease of explanation
    public interface ISageService
    {
        Task DeleteAsync(int id);
    }

    public class SageController : ApiController
    {
        ISageService _sageService;

        public SageController(ISageService userService)
        {
            _sageService = userService;
        }

        public async Task&lt;IHttpActionResult&gt; Delete(int id)
        {
            await _sageService.DeleteAsync(id);

            return Ok();
        }
   }
}
</code></pre><pre><code class="language-cs">namespace Proverb.Web.Tests.ASPNet.Controllers
{
    [TestClass]
    public class SageControllerTests
    {
        private Mock&lt;ISageService&gt; _sageServiceMock;
        private SageController _controller;

        readonly Task TaskOfNowt = Task.Delay(0);
        // Or you could use this equally valid but slightly more verbose approach:
        //readonly Task TaskOfNowt = Task.FromResult&lt;object&gt;(null);

        [TestInitialize]
        public void Initialise()
        {
            _sageServiceMock = new Mock&lt;ISageService&gt;();

            _controller = new SageController(_sageServiceMock.Object);
        }

        [TestMethod]
        public async Task Delete_returns_an_Ok()
        {
            _sageServiceMock
                .Setup(x =&gt; x.DeleteAsync(_sage.Id))
                .Returns(TaskOfNowt); // Feels good doesn&#x27;t it?

            IHttpActionResult result = await _controller.Delete(_sage.Id);

            var ok = result as OkResult;
            Assert.IsNotNull(ok);
            _sageServiceMock.Verify(x =&gt; x.DeleteAsync(_sage.Id));
        }
    }
}
</code></pre>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Journalling the Migration of Jasmine Tests to TypeScript]]></title>
            <link>https://blog.johnnyreilly.com/2014/09/13/migrating-jasmine-tests-to-typescript</link>
            <guid>Journalling the Migration of Jasmine Tests to TypeScript</guid>
            <pubDate>Sat, 13 Sep 2014 00:00:00 GMT</pubDate>
            <description><![CDATA[I previously attempted to migrate my Jasmine tests from JavaScript to TypeScript. The last time I tried it didn't go so well and I bailed. Thank the Lord for source control. But feeling I shouldn't be deterred I decided to have another crack at it.]]></description>
            <content:encoded><![CDATA[<p>I previously attempted to migrate my Jasmine tests from JavaScript to TypeScript. The last time I tried it didn&#x27;t go so well and I bailed. Thank the Lord for source control. But feeling I shouldn&#x27;t be deterred I decided to have another crack at it.</p><p>I did manage it this time... Sort of. Unfortunately there was a problem which I discovered right at the end. An issue with the TypeScript / Visual Studio tooling. So, just to be clear, this is not a blog post of &quot;do this and it will work perfectly&quot;. On this occasion there will be some rough edges. This post exists, as much as anything else, as a record of the problems I experienced - I hope it will prove useful. Here we go:</p><h2>What to Migrate?</h2><p>I&#x27;m going to use one of the test files in my my side project <a href="https://github.com/johnnyreilly/Proverb">Proverb</a>. It&#x27;s the tests for an AngularJS controller called <code>sageDetail</code> <!-- -->-<!-- --> I&#x27;ve written about it <a href="http://icanmakethiswork.blogspot.co.uk/2014/09/unit-testing-angular-controller-with.html">before</a>. Here it is in all it&#x27;s JavaScript-y glory:</p><pre><code class="language-ts">describe(&#x27;Proverb.Web -&gt; app-&gt; controllers -&gt;&#x27;, function () {
  beforeEach(function () {
    module(&#x27;app&#x27;);
  });

  describe(&#x27;sageDetail -&gt;&#x27;, function () {
    var $rootScope,
      getById_deferred, // deferred used for promises
      $location,
      $routeParams_stub,
      common,
      datacontext, // controller dependencies
      sageDetailController; // the controller

    beforeEach(inject(function (
      _$controller_,
      _$rootScope_,
      _$q_,
      _$location_,
      _common_,
      _datacontext_
    ) {
      $rootScope = _$rootScope_;
      $q = _$q_;

      $location = _$location_;
      common = _common_;
      datacontext = _datacontext_;

      $routeParams_stub = { id: &#x27;10&#x27; };
      getById_deferred = $q.defer();

      spyOn(datacontext.sage, &#x27;getById&#x27;).and.returnValue(
        getById_deferred.promise
      );
      spyOn(common, &#x27;activateController&#x27;).and.callThrough();
      spyOn(common.logger, &#x27;getLogFn&#x27;).and.returnValue(
        jasmine.createSpy(&#x27;log&#x27;)
      );
      spyOn($location, &#x27;path&#x27;).and.returnValue(jasmine.createSpy(&#x27;path&#x27;));

      sageDetailController = _$controller_(&#x27;sageDetail&#x27;, {
        $location: $location,
        $routeParams: $routeParams_stub,
        common: common,
        datacontext: datacontext,
      });
    }));

    describe(&#x27;on creation -&gt;&#x27;, function () {
      it(&quot;controller should have a title of &#x27;Sage Details&#x27;&quot;, function () {
        expect(sageDetailController.title).toBe(&#x27;Sage Details&#x27;);
      });

      it(&#x27;controller should have no sage&#x27;, function () {
        expect(sageDetailController.sage).toBeUndefined();
      });

      it(&#x27;datacontext.sage.getById should be called&#x27;, function () {
        expect(datacontext.sage.getById).toHaveBeenCalledWith(10, true);
      });
    });

    describe(&#x27;activateController -&gt;&#x27;, function () {
      var sage_stub;
      beforeEach(function () {
        sage_stub = { name: &#x27;John&#x27; };
      });

      it(&#x27;should set sages to be the resolved promise values&#x27;, function () {
        getById_deferred.resolve(sage_stub);
        $rootScope.$digest(); // So Angular processes the resolved promise

        expect(sageDetailController.sage).toBe(sage_stub);
      });

      it(&quot;should log &#x27;Activated Sage Details View&#x27; and set title with name&quot;, function () {
        getById_deferred.resolve(sage_stub);
        $rootScope.$digest(); // So Angular processes the resolved promise

        expect(sageDetailController.log).toHaveBeenCalledWith(
          &#x27;Activated Sage Details View&#x27;
        );
        expect(sageDetailController.title).toBe(
          &#x27;Sage Details: &#x27; + sage_stub.name
        );
      });
    });

    describe(&#x27;gotoEdit -&gt;&#x27;, function () {
      var sage_stub;
      beforeEach(function () {
        sage_stub = { id: 20 };
      });

      it(&#x27;should set $location.path to edit URL&#x27;, function () {
        getById_deferred.resolve(sage_stub);
        $rootScope.$digest(); // So Angular processes the resolved promise

        sageDetailController.gotoEdit();

        expect($location.path).toHaveBeenCalledWith(
          &#x27;/sages/edit/&#x27; + sage_stub.id
        );
      });
    });
  });
});
</code></pre><h2>Off we go</h2><p>Righteo. Let&#x27;s flip the switch. <code>sageDetail.js</code> you shall go to the ball! One wave of my magic wand and <code>sageDetail.js</code> becomes <code>sageDetail.ts</code>... Alakazam!! Of course we&#x27;ve got to do the fiddling with the <code>csproj</code> file to include the dependent JavaScript files. (I&#x27;ll be very pleased when ASP.Net vNext ships and I don&#x27;t have to do this anymore....) So find this:</p><pre><code class="language-xml">&lt;TypeScriptCompile Include=&quot;app\sages\sageDetail.ts&quot; /&gt;
</code></pre><p>And add this:</p><pre><code class="language-xml">&lt;Content Include=&quot;app\sages\sageDetail.js&quot;&gt;
  &lt;DependentUpon&gt;sageDetail.ts&lt;/DependentUpon&gt;
&lt;/Content&gt;
&lt;Content Include=&quot;app\sages\sageDetail.js.map&quot;&gt;
  &lt;DependentUpon&gt;sageDetail.ts&lt;/DependentUpon&gt;
&lt;/Content&gt;
</code></pre><p>What next? I&#x27;ve a million red squigglies in my code. It&#x27;s &quot;could not find symbol&quot; city. Why? Typings! We need typings! So let&#x27;s begin - I&#x27;m needing the Jasmine typings for starters. So let&#x27;s hit NuGet and it looks like we need <a href="http://www.nuget.org/packages/jasmine.TypeScript.DefinitelyTyped/">this</a>:</p><p><code>Install-Package jasmine.TypeScript.DefinitelyTyped</code>That did no good at all. Still red squigglies. I&#x27;m going to hazard a guess that this is something to do with the fact my JavaScript Unit Test project doesn&#x27;t contain the various TypeScript artefacts that Visual Studio kindly puts into the web csproj for you. This is because I&#x27;m keeping my JavaScript tests in a separate project from the code being tested. Also, the Visual Studio TypeScript tooling seems to work on the assumption that TypeScript will only be used within a web project; not a test project. Well I won&#x27;t let that hold me back... Time to port the TypeScript artefacts in the web csproj over by hand. I&#x27;ll take this:</p><pre><code class="language-xml">&lt;Import Project=&quot;$(MSBuildExtensionsPath32)\Microsoft\VisualStudio\v$(VisualStudioVersion)\TypeScript\Microsoft.TypeScript.Default.props&quot; Condition=&quot;Exists(&#x27;$(MSBuildExtensionsPath32)\Microsoft\VisualStudio\v$(VisualStudioVersion)\TypeScript\Microsoft.TypeScript.Default.props&#x27;)&quot; /&gt;
</code></pre><p>And I&#x27;ll also take this</p><pre><code class="language-xml">&lt;PropertyGroup Condition=&quot;&#x27;$(Configuration)&#x27; == &#x27;Debug&#x27;&quot;&gt;
  &lt;TypeScriptNoImplicitAny&gt;True&lt;/TypeScriptNoImplicitAny&gt;
&lt;/PropertyGroup&gt;
&lt;Import Project=&quot;$(MSBuildExtensionsPath32)\Microsoft\VisualStudio\v$(VisualStudioVersion)\TypeScript\Microsoft.TypeScript.targets&quot; Condition=&quot;Exists(&#x27;$(MSBuildExtensionsPath32)\Microsoft\VisualStudio\v$(VisualStudioVersion)\TypeScript\Microsoft.TypeScript.targets&#x27;)&quot; /&gt;
</code></pre><p>Bingo bango - a difference. I no longer have red squigglies under the Jasmine statements (<code>describe</code>, <code>it</code> etc). But alas, I do everywhere else. One in particular draws my eye...</p><h2>Could not find symbol &#x27;$q&#x27;</h2><p>Once again TypeScript picks up the hidden bugs in my JavaScript:</p><pre><code class="language-ts">$q = _$q_;
</code></pre><p>That&#x27;s right it&#x27;s an implicit global. Quickly fixed:</p><pre><code class="language-ts">var $q = _$q_;
</code></pre><h2>Typings? Where we&#x27;re going, we need typings...</h2><p>We need more types. We&#x27;re going to need the types created by our application; our controllers / services / directives etc. As well that we need the types used in the creation of the app. So the Angular typings etc. Since we&#x27;re going to need to use <code>reference</code> statements to pull in the types created by our application I might as well use them to pull in the required definition files as well (eg <code>angular.d.ts</code>):</p><pre><code class="language-xml">/// &lt;reference path=&quot;../../../proverb.web/scripts/typings/angularjs/angular.d.ts&quot; /&gt;
/// &lt;reference path=&quot;../../../proverb.web/scripts/typings/angularjs/angular-mocks.d.ts&quot; /&gt;
/// &lt;reference path=&quot;../../../proverb.web/app/sages/sagedetail.ts&quot; /&gt;
/// &lt;reference path=&quot;../../../proverb.web/app/common/common.ts&quot; /&gt;
/// &lt;reference path=&quot;../../../proverb.web/app/services/datacontext.ts&quot; /&gt;
/// &lt;reference path=&quot;../../../proverb.web/app/services/repository.sage.ts&quot; /&gt;
</code></pre><p>Now we need to work our way through the &quot;variable &#x27;x&#x27; implicitly has an &#x27;any&#x27; type&quot; messages. One thing we need to do is to amend our original sageDetails.ts file so that the <code>sageDetailRouteParams</code> interface and <code>SageDetail</code> class are exported from the controllers module. We can&#x27;t use the types otherwise. Now we can add typings to our file - once finished it looks like this:</p><pre><code class="language-ts">/// &lt;reference path=&quot;../../../proverb.web/scripts/typings/angularjs/angular.d.ts&quot; /&gt;
/// &lt;reference path=&quot;../../../proverb.web/scripts/typings/angularjs/angular-mocks.d.ts&quot; /&gt;
/// &lt;reference path=&quot;../../../proverb.web/app/sages/sagedetail.ts&quot; /&gt;
/// &lt;reference path=&quot;../../../proverb.web/app/common/common.ts&quot; /&gt;
/// &lt;reference path=&quot;../../../proverb.web/app/services/datacontext.ts&quot; /&gt;
/// &lt;reference path=&quot;../../../proverb.web/app/services/repository.sage.ts&quot; /&gt;
describe(&#x27;Proverb.Web -&gt; app-&gt; controllers -&gt;&#x27;, function () {
  beforeEach(function () {
    module(&#x27;app&#x27;);
  });

  describe(&#x27;sageDetail -&gt;&#x27;, function () {
    var $rootScope: ng.IRootScopeService,
      // deferred used for promises
      getById_deferred: ng.IDeferred&lt;sage&gt;,
      // controller dependencies
      $location: ng.ILocationService,
      $routeParams_stub: controllers.sageDetailRouteParams,
      common: common,
      datacontext: datacontext,
      sageDetailController: controllers.SageDetail; // the controller

    beforeEach(inject(function (
      _$controller_: any,
      _$rootScope_: ng.IRootScopeService,
      _$q_: ng.IQService,
      _$location_: ng.ILocationService,
      _common_: common,
      _datacontext_: datacontext
    ) {
      $rootScope = _$rootScope_;
      var $q = _$q_;

      $location = _$location_;
      common = _common_;
      datacontext = _datacontext_;

      $routeParams_stub = { id: &#x27;10&#x27; };
      getById_deferred = $q.defer();

      spyOn(datacontext.sage, &#x27;getById&#x27;).and.returnValue(
        getById_deferred.promise
      );
      spyOn(common, &#x27;activateController&#x27;).and.callThrough();
      spyOn(common.logger, &#x27;getLogFn&#x27;).and.returnValue(
        jasmine.createSpy(&#x27;log&#x27;)
      );
      spyOn($location, &#x27;path&#x27;).and.returnValue(jasmine.createSpy(&#x27;path&#x27;));

      sageDetailController = _$controller_(&#x27;sageDetail&#x27;, {
        $location: $location,
        $routeParams: $routeParams_stub,
        common: common,
        datacontext: datacontext,
      });
    }));

    describe(&#x27;on creation -&gt;&#x27;, function () {
      it(&quot;controller should have a title of &#x27;Sage Details&#x27;&quot;, function () {
        expect(sageDetailController.title).toBe(&#x27;Sage Details&#x27;);
      });

      it(&#x27;controller should have no sage&#x27;, function () {
        expect(sageDetailController.sage).toBeUndefined();
      });

      it(&#x27;datacontext.sage.getById should be called&#x27;, function () {
        expect(datacontext.sage.getById).toHaveBeenCalledWith(10, true);
      });
    });

    describe(&#x27;activateController -&gt;&#x27;, function () {
      var sage_stub: sage;
      beforeEach(function () {
        sage_stub = {
          name: &#x27;John&#x27;,
          id: 10,
          username: &#x27;John&#x27;,
          email: &#x27;john@&#x27;,
          dateOfBirth: new Date(),
        };
      });

      it(&#x27;should set sages to be the resolved promise values&#x27;, function () {
        getById_deferred.resolve(sage_stub);
        $rootScope.$digest(); // So Angular processes the resolved promise

        expect(sageDetailController.sage).toBe(sage_stub);
      });

      it(&quot;should log &#x27;Activated Sage Details View&#x27; and set title with name&quot;, function () {
        getById_deferred.resolve(sage_stub);
        $rootScope.$digest(); // So Angular processes the resolved promise

        expect(sageDetailController.log).toHaveBeenCalledWith(
          &#x27;Activated Sage Details View&#x27;
        );
        expect(sageDetailController.title).toBe(
          &#x27;Sage Details: &#x27; + sage_stub.name
        );
      });
    });

    describe(&#x27;gotoEdit -&gt;&#x27;, function () {
      var sage_stub: sage;
      beforeEach(function () {
        sage_stub = {
          name: &#x27;John&#x27;,
          id: 20,
          username: &#x27;John&#x27;,
          email: &#x27;john@&#x27;,
          dateOfBirth: new Date(),
        };
      });

      it(&#x27;should set $location.path to edit URL&#x27;, function () {
        getById_deferred.resolve(sage_stub);
        $rootScope.$digest(); // So Angular processes the resolved promise

        sageDetailController.gotoEdit();

        expect($location.path).toHaveBeenCalledWith(
          &#x27;/sages/edit/&#x27; + sage_stub.id
        );
      });
    });
  });
});
</code></pre><h2>So That&#x27;s All Good...</h2><p>Except it&#x27;s not. When I run the tests using Chutzpah my <code>sageDetail</code> controller tests aren&#x27;t found. My spider sense is tingling. This is something to do with the <code>reference</code> statements. They&#x27;re throwing Chutzpah off. No bother, I can fix that with a quick tweak of the project file:</p><pre><code class="language-xml">&lt;PropertyGroup Condition=&quot;&#x27;$(Configuration)&#x27; == &#x27;Debug&#x27;&quot;&gt;
    &lt;TypeScriptNoImplicitAny&gt;True&lt;/TypeScriptNoImplicitAny&gt;
    &lt;TypeScriptRemoveComments&gt;True&lt;/TypeScriptRemoveComments&gt;
  &lt;/PropertyGroup&gt;
</code></pre><p>The TypeScript compiler will now strip comments; which includes the <code>reference</code> statements. Now my tests are detected <!-- -->*<strong>and</strong>*<!-- --> they run. Yay!</p><h2>Who Killed the TypeScript Language Service?</h2><p>Yup it&#x27;s dead. Whilst the compilation itself has no issues, take a look at the errors being presented for just one of the files back in the original web project:</p><p><img src="../static/blog/2014-09-13-migrating-jasmine-tests-to-typescript/Screenshot%2B2014-09-12%2B23.15.22.png"/></p><p>It looks like having one TypeScript project in a solution which uses <code>reference</code> comments somehow breaks the implicit referencing behaviour built into Visual Studio for other TypeScript projects in the solution. I can say this with some confidence as if I pull out the <code>reference</code> comments from the top of the test file that we&#x27;ve converted then it&#x27;s business as usual - the TypeScript Language Service lives once more. I&#x27;m sure you can see the problem here though: the TypeScript test file doesn&#x27;t compile. All rather unsatisfactory.</p><p>I suspect that if I added <code>reference</code> comments throughout the web project the TypeScript Language Service would be just fine. But I rather like the implicit referencing functionality so I&#x27;m not inclined to do that. After reaching something of a brick wall and thinking I had encountered a bug in the TypeScript Language service I <a href="https://github.com/Microsoft/TypeScript/issues/673">raised an issue on GitHub</a>.</p><h2>Solutions....</h2><p>Thanks to the help of <a href="https://github.com/mhegazy">Mohamed Hegazy</a> it emerged that the problem was down to missing <code>reference</code> comments in my <code>sageDetail</code> controller tests. One thing I had not considered was the 2 different ways each of my TypeScript projects were working:</p><ul><li>Proverb.Web uses the Visual Studio implicit referencing functionality. This means that I do not need to use <code>reference</code> comments in the TypeScript files in Proverb.Web.</li><li>Proverb.Web.JavaScript does <!-- -->*<strong>not</strong>*<!-- --> uses the implicit referencing functionality. It needs <code>reference</code> comments to resolve references.</li></ul><p>The important thing to take away from this (and the thing I had overlooked) was that Proverb.Web.JavaScript uses <code>reference</code> comments to pull in Proverb.Web TypeScript files. Those files have dependencies which are <!-- -->*<strong>not</strong>*<!-- --> stated using <code>reference</code> comments. So the compiler trips up when it tries to walk the dependency tree - there are no <code>reference</code> comments to be followed! So for example, <code>common.ts</code> has a dependency upon <code>logger.ts</code>. Fixing the TypeScript Language Service involves ensuring that the full dependency list is included in the <code>sageDetail</code> controller tests file, like so:</p><pre><code class="language-ts">/// &lt;reference path=&quot;../../../proverb.web/scripts/typings/angularjs/angular.d.ts&quot; /&gt;
/// &lt;reference path=&quot;../../../proverb.web/scripts/typings/angularjs/angular-mocks.d.ts&quot; /&gt;
/// &lt;reference path=&quot;../../../proverb.web/scripts/typings/angularjs/angular-route.d.ts&quot; /&gt;
/// &lt;reference path=&quot;../../../proverb.web/scripts/typings/toastr/toastr.d.ts&quot; /&gt;
/// &lt;reference path=&quot;../../../proverb.web/scripts/typings/underscore/underscore.d.ts&quot; /&gt;
/// &lt;reference path=&quot;../../../proverb.web/app/sages/sagedetail.ts&quot; /&gt;
/// &lt;reference path=&quot;../../../proverb.web/app/common/logger.ts&quot; /&gt;
/// &lt;reference path=&quot;../../../proverb.web/app/common/common.ts&quot; /&gt;
/// &lt;reference path=&quot;../../../proverb.web/app/services/datacontext.ts&quot; /&gt;
/// &lt;reference path=&quot;../../../proverb.web/app/services/repositories.ts&quot; /&gt;
/// &lt;reference path=&quot;../../../proverb.web/app/services/repository.sage.ts&quot; /&gt;
/// &lt;reference path=&quot;../../../proverb.web/app/services/repository.saying.ts&quot; /&gt;
/// &lt;reference path=&quot;../../../proverb.web/app/app.ts&quot; /&gt;
/// &lt;reference path=&quot;../../../proverb.web/app/config.route.ts&quot; /&gt;
</code></pre><p>With this in place you have a working solution, albeit one that is a little flaky. <a href="https://github.com/Microsoft/TypeScript/issues/673#issuecomment-56024348">An alternative solution was suggested by Noel Abrahams</a> which I quote here:</p><blockquote><p>Why not do the following?</p><ul><li>Compile Proverb.Web with --declarations and the option for combining output into a single file. This should create a Proverb.Web.d.ts in your output directory.</li><li>In Proverb.Web.Tests.JavaScript add a reference to this file.</li><li>Right-click Proverb.Web.Tests.JavaScript select &quot;Build Dependencies&quot; &gt; &quot;Project Dependencies&quot; and add a reference to Proverb.Web.</li></ul><p>I don&#x27;t think directly referencing TypeScript source files is a good idea, because it causes the file to be rebuilt every time the dependant project is compiled.</p></blockquote><p>Mohamed rather liked this solution. It looks like some more work is due to be done on the TypeScript tooling to make this less headache-y in future.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Unit Testing an Angular Controller with Jasmine]]></title>
            <link>https://blog.johnnyreilly.com/2014/09/10/unit-testing-angular-controller-with</link>
            <guid>Unit Testing an Angular Controller with Jasmine</guid>
            <pubDate>Wed, 10 Sep 2014 00:00:00 GMT</pubDate>
            <description><![CDATA[Anyone who reads my blog will know that I have been long in the habit of writing unit tests for my C# code. I'm cool like that. However, it took me a while to get up and running writing unit tests for my JavaScript code. I finally got there using a combination of Jasmine 2.0 and Chutzpah. (Jasmine being my test framework and Chutzpah being my test runner.)]]></description>
            <content:encoded><![CDATA[<p>Anyone who reads my blog will know that I have been long in the habit of writing unit tests for my C# code. I&#x27;m cool like that. However, it took me a while to get up and running writing unit tests for my JavaScript code. I finally <a href="http://icanmakethiswork.blogspot.com/2014/03/the-surprisingly-happy-tale-of-visual.html">got there</a> using a combination of Jasmine 2.0 and Chutzpah. (Jasmine being my test framework and Chutzpah being my test runner.)</p><p>I&#x27;m getting properly into the habit of testing my JavaScript. I won&#x27;t pretend it&#x27;s been particularly fun but I firmly believe it will end up being useful... That&#x27;s what I tell myself during the long dark tea-times of the soul anyway.</p><p>I have a side project called <a href="https://github.com/johnnyreilly/Proverb">Proverb</a>. It doesn&#x27;t do anything in particular - for the most part it&#x27;s a simple application that displays the collected wise sayings of a team that I used to be part of. There&#x27;s not much to it - a bit of CRUD, a dashboard. Not much more. Because of the project&#x27;s simplicity it&#x27;s ideal to use Proverb&#x27;s underlying idea when trying out new technologies / frameworks. <a href="http://en.wikipedia.org/wiki/Paul_Halmos">The best way to learn is to do</a>. So if I want to learn &quot;X&quot;, then building Proverb using &quot;X&quot; is a good way to go.</p><p>I digress already. I had a version of Proverb built using a combination of <a href="https://github.com/johnnyreilly/Proverb/tree/master/AngularTypeScript">AngularJS and TypeScript</a>. I had written the Angular side of Proverb without any tests. Now I was able to write JavaScript tests for my Angular code that&#x27;s just what I set out to do. It should prove something of a of <a href="http://en.wikipedia.org/wiki/Kata_(programming)">Code Kata</a> too.</p><p>Whilst I&#x27;m at it I thought it might prove helpful if I wrote up how I approached writing unit tests for a single Angular controller. So here goes.</p><h2>What I&#x27;m Testing</h2><p>I have an Angular controller called <code>sagesDetail</code>. It powers this screen:</p><p><img src="../static/blog/2014-09-10-unit-testing-angular-controller-with/sageDetailScreen.png"/></p><p><code>sagesDetail</code> is a very simple controller. It does these things:</p><ol><li>Load the &quot;sage&quot; (think of it as just a &quot;user&quot;) and make it available on the controller so it can be bound to the view.</li><li>Set the view title.</li><li>Log view activation.</li><li>Expose a <code>gotoEdit</code> method which, when called, redirects the user to the edit screen.</li></ol><p>The controller is written in TypeScript and looks like this:</p><h3>sagesDetail.ts</h3><pre><code class="language-ts">module controllers {
  &#x27;use strict&#x27;;

  var controllerId = &#x27;sageDetail&#x27;;

  interface sageDetailRouteParams extends ng.route.IRouteParamsService {
    id: string;
  }

  class SageDetail {
    log: loggerFunction;
    sage: sage;
    title: string;

    static $inject = [&#x27;$location&#x27;, &#x27;$routeParams&#x27;, &#x27;common&#x27;, &#x27;datacontext&#x27;];
    constructor(
      private $location: ng.ILocationService,
      private $routeParams: sageDetailRouteParams,
      private common: common,
      private datacontext: datacontext
    ) {
      this.sage = undefined;
      this.title = &#x27;Sage Details&#x27;;

      this.log = common.logger.getLogFn(controllerId);

      this.activate();
    }

    // Prototype methods

    activate() {
      var id = parseInt(this.$routeParams.id, 10);
      var dataPromises: ng.IPromise&lt;any&gt;[] = [
        this.datacontext.sage
          .getById(id, true)
          .then((data) =&gt; (this.sage = data)),
      ];

      this.common
        .activateController(dataPromises, controllerId, this.title)
        .then(() =&gt; {
          this.log(&#x27;Activated Sage Details View&#x27;);
          this.title = &#x27;Sage Details: &#x27; + this.sage.name;
        });
    }

    gotoEdit() {
      this.$location.path(&#x27;/sages/edit/&#x27; + this.sage.id);
    }
  }

  angular.module(&#x27;app&#x27;).controller(controllerId, SageDetail);
}
</code></pre><p>When compiled to JavaScript it looks like this:</p><h3>sageDetail.js</h3><pre><code class="language-js">var controllers;
(function (controllers) {
  &#x27;use strict&#x27;;

  var controllerId = &#x27;sageDetail&#x27;;

  var SageDetail = (function () {
    function SageDetail($location, $routeParams, common, datacontext) {
      this.$location = $location;
      this.$routeParams = $routeParams;
      this.common = common;
      this.datacontext = datacontext;
      this.sage = undefined;
      this.title = &#x27;Sage Details&#x27;;

      this.log = common.logger.getLogFn(controllerId);

      this.activate();
    }
    // Prototype methods
    SageDetail.prototype.activate = function () {
      var _this = this;
      var id = parseInt(this.$routeParams.id, 10);
      var dataPromises = [
        this.datacontext.sage.getById(id, true).then(function (data) {
          return (_this.sage = data);
        }),
      ];

      this.common
        .activateController(dataPromises, controllerId, this.title)
        .then(function () {
          _this.log(&#x27;Activated Sage Details View&#x27;);
          _this.title = &#x27;Sage Details: &#x27; + _this.sage.name;
        });
    };

    SageDetail.prototype.gotoEdit = function () {
      this.$location.path(&#x27;/sages/edit/&#x27; + this.sage.id);
    };
    SageDetail.$inject = [&#x27;$location&#x27;, &#x27;$routeParams&#x27;, &#x27;common&#x27;, &#x27;datacontext&#x27;];
    return SageDetail;
  })();

  angular.module(&#x27;app&#x27;).controller(controllerId, SageDetail);
})(controllers || (controllers = {}));
//# sourceMappingURL=sageDetail.js.map
</code></pre><h2>Now for the Tests</h2><p>I haven&#x27;t yet made the move of switching over my Jasmine tests from JavaScript to TypeScript. (It&#x27;s on my list but there&#x27;s only so many things you can do at once...) For that reason the tests you&#x27;ll see here are straight JavaScript. Below you will see the tests for the <code>sageDetail</code> controller.</p><p>I have put very comments in the test code to make clear the intent to you, dear reader. Annotated the life out of them. Naturally I wouldn&#x27;t expect a test to be so heavily annotated in a typical test suite - and you can be sure mine normally aren&#x27;t!</p><h3>Jasmine tests for sageDetail.js</h3><pre><code class="language-js">describe(&#x27;Proverb.Web -&gt; app-&gt; controllers -&gt;&#x27;, function () {
  // Before each test runs we&#x27;re going to need ourselves an Angular App to test - go fetch!
  beforeEach(function () {
    module(&#x27;app&#x27;); // module is an alias for &lt;a href=&quot;https://docs.angularjs.org/api/ngMock/function/angular.mock.module&quot;&gt;angular.mock.module&lt;/a&gt;
  });

  // Tests for the sageDetail controller
  describe(&#x27;sageDetail -&gt;&#x27;, function () {
    // Declare describe-scoped variables
    var $rootScope,
      getById_deferred, // deferred used for promises
      $location,
      $routeParams_stub,
      common,
      datacontext, // controller dependencies
      sageDetailController; // the controller

    // Before each test runs set up the controller using inject - an alias for &lt;a href=&quot;https://docs.angularjs.org/api/ngMock/function/angular.mock.inject&quot;&gt;angular.mock.inject&lt;/a&gt;
    beforeEach(inject(function (
      _$controller_,
      _$rootScope_,
      _$q_,
      _$location_,
      _common_,
      _datacontext_
    ) {
      // Note how each parameter is prefixed and suffixed with &quot;_&quot; - this an Angular nicety
      // which allows you to have variables in your tests with the original reference name.
      // So here we assign the injected parameters to the describe-scoped variables:
      $rootScope = _$rootScope_;
      $q = _$q_;
      $location = _$location_;
      common = _common_;
      datacontext = _datacontext_;

      // Our controller has a dependency on an &quot;id&quot; property passed on the $routeParams
      // We&#x27;re going to stub this out with a JavaScript object literal
      $routeParams_stub = { id: &#x27;10&#x27; };

      // Our controller depends on a promise returned from this function: datacontext.sage.getById
      // Well strictly speaking it also uses a promise for activateController but since the activateController
      // promise just wraps the getById promise it will be resolved when the getById promise is.
      // Here we create a deferred representing the getById promise which we can resolve as we need to
      getById_deferred = $q.defer();

      // set up a spy on datacontext.sage.getById and set it to return the promise of getById_deferred
      // this allows us to #1 detect that getById has been called
      // and #2 resolve / reject our promise as our test requires using getById_deferred
      spyOn(datacontext.sage, &#x27;getById&#x27;).and.returnValue(
        getById_deferred.promise
      );

      // set up a spy on common.activateController and set it to call through
      // this allows us to detect that activateController has been called whilst
      // maintaining existing controller functionality
      spyOn(common, &#x27;activateController&#x27;).and.callThrough();

      // set up spys on common.logger.getLogFn and $location.path so we can detect they have been called
      spyOn(common.logger, &#x27;getLogFn&#x27;).and.returnValue(
        jasmine.createSpy(&#x27;log&#x27;)
      );
      spyOn($location, &#x27;path&#x27;).and.returnValue(jasmine.createSpy(&#x27;path&#x27;));

      // create a sageDetail controller and inject the dependencies we have set up
      sageDetailController = _$controller_(&#x27;sageDetail&#x27;, {
        $location: $location,
        $routeParams: $routeParams_stub,
        common: common,
        datacontext: datacontext,
      });
    }));

    // Tests for the controller state at the point of the sageDetail controller&#x27;s creation
    // ie before the getById / activateController promises have been resolved
    // So this tests the constructor (function) and the activate function up to the point
    // of the promise calls
    describe(&#x27;on creation -&gt;&#x27;, function () {
      it(&quot;controller should have a title of &#x27;Sage Details&#x27;&quot;, function () {
        // tests this code has executed:
        // this.title = &quot;Sage Details&quot;;
        expect(sageDetailController.title).toBe(&#x27;Sage Details&#x27;);
      });

      it(&#x27;controller should have no sage&#x27;, function () {
        // tests this code has executed:
        // this.sage = undefined;
        expect(sageDetailController.sage).toBeUndefined();
      });

      it(&#x27;datacontext.sage.getById should be called&#x27;, function () {
        // tests this code has executed:
        // this.datacontext.sage.getById(id, true)
        expect(datacontext.sage.getById).toHaveBeenCalledWith(10, true);
      });
    });

    // Tests for the controller state at the point of the resolution of the getById promise
    // ie after the getById / activateController promises have been resolved
    // So this tests the constructor (function) and the activate function after the point
    // of the promise calls
    describe(&#x27;activateController -&gt;&#x27;, function () {
      var sage_stub;
      beforeEach(function () {
        // Create a sage stub which will be used when resolving the getById promise
        sage_stub = { name: &#x27;John&#x27; };
      });

      it(&#x27;should set sages to be the resolved promise values&#x27;, function () {
        // Resolve the getById promise with the sage stub
        getById_deferred.resolve(sage_stub);
        $rootScope.$digest(); // So Angular processes the resolved promise

        // tests this code has executed:
        // this.sage = data
        expect(sageDetailController.sage).toBe(sage_stub);
      });

      it(&quot;should log &#x27;Activated Sage Details View&#x27; and set title with name&quot;, function () {
        // Resolve the getById promise with the sage stub
        getById_deferred.resolve(sage_stub);
        $rootScope.$digest(); // So Angular processes the resolved promise

        // tests this code has executed:
        // this.log(&quot;Activated Sage Details View&quot;);
        // this.title = &quot;Sage Details: &quot; + this.sage.name;
        expect(sageDetailController.log).toHaveBeenCalledWith(
          &#x27;Activated Sage Details View&#x27;
        );
        expect(sageDetailController.title).toBe(
          &#x27;Sage Details: &#x27; + sage_stub.name
        );
      });
    });

    // Tests for the gotoEdit function on the controller
    // Note that this will only be called *after* a controller has been created
    // and it depends upon a sage having first been loaded
    describe(&#x27;gotoEdit -&gt;&#x27;, function () {
      var sage_stub;
      beforeEach(function () {
        // Create a sage stub which will be used when resolving the getById promise
        sage_stub = { id: 20 };
      });

      it(&#x27;should set $location.path to edit URL&#x27;, function () {
        // Resolve the getById promise with the sage stub
        getById_deferred.resolve(sage_stub);
        $rootScope.$digest(); // So Angular processes the resolved promise

        sageDetailController.gotoEdit();

        // tests this code has executed:
        // this.$location.path(&quot;/sages/edit/&quot; + this.sage.id);
        expect($location.path).toHaveBeenCalledWith(
          &#x27;/sages/edit/&#x27; + sage_stub.id
        );
      });
    });
  });
});
</code></pre>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Running JavaScript Unit Tests in AppVeyor]]></title>
            <link>https://blog.johnnyreilly.com/2014/09/06/running-javascript-unit-tests-in-appveyor</link>
            <guid>Running JavaScript Unit Tests in AppVeyor</guid>
            <pubDate>Sat, 06 Sep 2014 00:00:00 GMT</pubDate>
            <description><![CDATA[With a little help from Chutzpah...]]></description>
            <content:encoded><![CDATA[<h2>With a little help from Chutzpah...</h2><p><a href="http://www.appveyor.com">AppVeyor</a> (if you&#x27;re not aware of it) is a Continuous Integration provider. If you like, it&#x27;s plug-and-play CI for .NET developers. It&#x27;s lovely. And what&#x27;s more it&#x27;s <a href="http://www.appveyor.com/pricing">&quot;free for open-source projects with public repositories hosted on GitHub and BitBucket&quot;</a>. Boom! I recently hooked up 2 of my GitHub projects with AppVeyor. It took me all of... 10 minutes. If that? It really is <!-- -->*<strong>that</strong>*<!-- --> good.</p><p>But.... There had to be a &quot;but&quot; otherwise I wouldn&#x27;t have been writing the post you&#x27;re reading. For a little side project of mine called <a href="https://github.com/johnnyreilly/Proverb">Proverb</a> there were C# unit tests and there were JavaScript unit tests. And the JavaScript unit tests weren&#x27;t being run... No fair!!!</p><p><a href="https://chutzpah.codeplex.com/">Chutzpah</a> is a JavaScript test runner which at this point runs QUnit, Jasmine and Mocha JavaScript tests. I use the <a href="http://visualstudiogallery.msdn.microsoft.com/f8741f04-bae4-4900-81c7-7c9bfb9ed1fe">Visual Studio extension</a> to run Jasmine tests on my machine during development. I&#x27;ve also been able to use <a href="http://icanmakethiswork.blogspot.com/2014/03/the-surprisingly-happy-tale-of-visual.html">Chutzpah for CI purposes with Visual Studio Online / Team Foundation Server</a>. So what say we try and do the triple and make it work with AppVeyor too?</p><h2>NuGet me?</h2><p>In order that I could run Chutzpah I needed Chutzpah to be installed on the build machine. So I had 2 choices:</p><ol><li>Add Chutzpah direct to the repo</li><li>Add the <a href="http://www.nuget.org/packages/chutzpah">Chutzpah Nuget package</a> to the solution</li></ol><p>Unsurprisingly I chose #2 - much cleaner.</p><h2>Now to use Chutzpah</h2><p>Time to dust down the PowerShell. I created myself a &quot;before tests script&quot; and added it to my build. It looked a little something like this:</p><pre><code class="language-ps"># Locate Chutzpah

$ChutzpahDir = get-childitem chutzpah.console.exe -recurse | select-object -first 1 | select -expand Directory

# Run tests using Chutzpah and export results as JUnit format to chutzpah-results.xml

$ChutzpahCmd = &quot;$($ChutzpahDir)\chutzpah.console.exe $($env:APPVEYOR_BUILD_FOLDER)\AngularTypeScript\Proverb.Web.Tests.JavaScript /junit .\chutzpah-results.xml&quot;
Write-Host $ChutzpahCmd
Invoke-Expression $ChutzpahCmd

# Upload results to AppVeyor one by one

$testsuites = [xml](get-content .\chutzpah-results.xml)

$anyFailures = $FALSE
foreach ($testsuite in $testsuites.testsuites.testsuite) {
    write-host &quot; $($testsuite.name)&quot;
    foreach ($testcase in $testsuite.testcase){
        $failed = $testcase.failure
        $time = $testsuite.time
        if ($testcase.time) { $time = $testcase.time }
        if ($failed) {
            write-host &quot;Failed   $($testcase.name) $($testcase.failure.message)&quot;
            Add-AppveyorTest $testcase.name -Outcome Failed -FileName $testsuite.name -ErrorMessage $testcase.failure.message -Duration $time
            $anyFailures = $TRUE
        }
        else {
            write-host &quot;Passed   $($testcase.name)&quot;
            Add-AppveyorTest $testcase.name -Outcome Passed -FileName $testsuite.name -Duration $time
        }

    }
}

if ($anyFailures -eq $TRUE){
    write-host &quot;Failing build as there are broken tests&quot;
    $host.SetShouldExit(1)
}
</code></pre><p>What this does is:</p><ol><li>Run Chutzpah from the installed NuGet package location, passing in the location of my Jasmine unit tests. In the case of my project there is a <code>chutzpah.json</code> file in the project which dictates how Chutzpah should run the tests. Also, <a href="https://chutzpah.codeplex.com/wikipage?title=Command%20Line%20Options&amp;referringTitle=Documentation">the JUnit flag is also passed</a> in order that Chutzpah creates a <code>chutzpah-results.xml</code> file of test results in the JUnit format.</li><li>We iterate through test results and tell AppVeyor about the the test passes and failures using the <a href="http://www.appveyor.com/docs/build-worker-api">Build Worker API</a>.</li><li>If there have been any failed tests then we fail the build. If you look <a href="https://ci.appveyor.com/project/JohnReilly/proverb/build/1.0.17">here</a> you can see a deliberately failed build which demo&#x27;s that this works as it should.</li></ol><p>That&#x27;s a wrap - We now have CI which includes our JavaScript tests! That&#x27;s right we get to see beautiful screens like these:</p><p><img src="../static/blog/2014-09-06-running-javascript-unit-tests-in-appveyor/Screenshot%2B2014-09-06%2B21.43.15.png"/></p><p><img src="../static/blog/2014-09-06-running-javascript-unit-tests-in-appveyor/Screenshot%2B2014-09-06%2B21.49.38.png"/></p><h2>Thanks to...</h2><p>Thanks to Dan Jones, whose comments on <a href="http://help.appveyor.com/discussions/questions/390-running-jasmine-on-appveyor#comment_34433599">this discussion</a> provided a number of useful pointers which moved me in the right direction. And thanks to Feador Fitzner who has generously <a href="http://help.appveyor.com/discussions/questions/495-integrating-chutzpah-into-appveyor#comment_34447202">said AppVeyor will support JUnit in the future</a> which may simplify use of Chutzpah with AppVeyor even further.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[My Unrequited Love for Isolate Scope]]></title>
            <link>https://blog.johnnyreilly.com/2014/08/12/my-unrequited-love-for-isolate-scope</link>
            <guid>My Unrequited Love for Isolate Scope</guid>
            <pubDate>Tue, 12 Aug 2014 00:00:00 GMT</pubDate>
            <description><![CDATA[I wrote a little while ago about creating a directive to present server errors on the screen in an Angular application. In my own (not so humble opinion), it was really quite nice. I was particularly proud of my usage of isolate scope. However, pride comes before a fall.]]></description>
            <content:encoded><![CDATA[<p><a href="http://icanmakethiswork.blogspot.com/2014/08/angularjs-meet-aspnet-server-validation.html">I wrote a little while ago about creating a directive to present server errors on the screen in an Angular application</a>. In my own (not so humble opinion), it was really quite nice. I was particularly proud of my usage of isolate scope. However, pride comes before a fall.</p><p>It turns out that using isolate scope in a directive is not always wise. Or rather – not always possible. And this is why:</p><p><code>Error: [$compile:multidir] Multiple directives [datepickerPopup, serverError] asking for new/isolated scope on: &amp;lt;input name=&quot;sage.dateOfBirth&quot; class=&quot;col-xs-12 col-sm-9&quot; type=&quot;text&quot; value=&quot;&quot; ng-click=&quot;vm.dateOfBirthDatePickerOpen()&quot; server-error=&quot;vm.errors&quot; ng-model=&quot;vm.sage.dateOfBirth&quot; is-open=&quot;vm.dateOfBirthDatePickerIsOpen&quot; datepicker-popup=&quot;dd MMM yyyy&quot;&amp;gt; </code>Ug. What happened here? Well, I had a date field that I was using my serverError directive on. Nothing too controversial there. The problem came when I tried to plug in <a href="http://angular-ui.github.io/bootstrap/">UI Bootstrap’s datepicker</a> as well. That’s right the directives are fighting. Sad face.</p><p>To be more precise, it turns out that only one directive on an element is allowed to create an isolated scope. So if I want to use UI Bootstrap’s datepicker (and I do) – well my serverError directive is toast.</p><h2>A New Hope</h2><p>So ladies and gentlemen, let me present serverError 2.0 – this time without isolated scope:</p><h3>serverError.ts</h3><pre><code class="language-ts">(function () {
  &#x27;use strict&#x27;;

  var app = angular.module(&#x27;app&#x27;);

  // Plant a validation message to the right of the element when it is declared invalid by the server
  app.directive(&#x27;serverError&#x27;, [
    function () {
      // Usage:
      // &lt;input class=&quot;col-xs-12 col-sm-9&quot;
      //        name=&quot;sage.name&quot; ng-model=&quot;vm.sage.name&quot; server-error=&quot;vm.errors&quot; /&gt;

      var directive = {
        link: link,
        restrict: &#x27;A&#x27;,
        require: &#x27;ngModel&#x27;, // supply the ngModel controller as the 4th parameter in the link function
      };
      return directive;

      function link(
        scope: ng.IScope,
        element: ng.IAugmentedJQuery,
        attrs: ng.IAttributes,
        ngModelController: ng.INgModelController
      ) {
        // Extract values from attributes (deliberately not using isolated scope)
        var errorKey: string = attrs[&#x27;name&#x27;]; // eg &quot;sage.name&quot;
        var errorDictionaryExpression: string = attrs[&#x27;serverError&#x27;]; // eg &quot;vm.errors&quot;

        // Bootstrap alert template for error
        var template =
          &#x27;&lt;div class=&quot;alert alert-danger col-xs-9 col-xs-offset-2&quot; role=&quot;alert&quot;&gt;&lt;i class=&quot;glyphicon glyphicon-warning-sign larger&quot;&gt;&lt;/i&gt; %error%&lt;/div&gt;&#x27;;

        // Create an element to hold the validation message
        var decorator = angular.element(&#x27;&lt;div&gt;&lt;/div&gt;&#x27;);
        element.after(decorator);

        // Watch ngModelController.$error.server &amp; show/hide validation accordingly
        scope.$watch(
          safeWatch(() =&gt; ngModelController.$error.server),
          showHideValidation
        );

        function showHideValidation(serverError: boolean) {
          // Display an error if serverError is true otherwise clear the element
          var errorHtml = &#x27;&#x27;;
          if (serverError) {
            var errorDictionary: { [field: string]: string } = scope.$eval(
              errorDictionaryExpression
            );
            errorHtml = template.replace(
              /%error%/,
              errorDictionary[errorKey] || &#x27;Unknown error occurred...&#x27;
            );
          }
          decorator.html(errorHtml);
        }

        // wipe the server error message upon keyup or change events so can revalidate with server
        element.on(&#x27;keyup change&#x27;, (event) =&gt; {
          scope.$apply(() =&gt; {
            ngModelController.$setValidity(&#x27;server&#x27;, true);
          });
        });
      }
    },
  ]);

  // Thanks @Basarat! http://stackoverflow.com/a/24863256/761388
  function safeWatch&lt;T extends Function&gt;(expression: T) {
    return () =&gt; {
      try {
        return expression();
      } catch (e) {
        return null;
      }
    };
  }
})();
</code></pre><h3>serverError.js</h3><pre><code class="language-js">(function () {
  &#x27;use strict&#x27;;

  var app = angular.module(&#x27;app&#x27;);

  // Plant a validation message to the right of the element when it is declared invalid by the server
  app.directive(&#x27;serverError&#x27;, [
    function () {
      // Usage:
      // &lt;input class=&quot;col-xs-12 col-sm-9&quot;
      //        name=&quot;sage.name&quot; ng-model=&quot;vm.sage.name&quot; server-error=&quot;vm.errors&quot; /&gt;
      var directive = {
        link: link,
        restrict: &#x27;A&#x27;,
        require: &#x27;ngModel&#x27;,
      };
      return directive;

      function link(scope, element, attrs, ngModelController) {
        // Extract values from attributes (deliberately not using isolated scope)
        var errorKey = attrs[&#x27;name&#x27;];
        var errorDictionaryExpression = attrs[&#x27;serverError&#x27;];

        // Bootstrap alert template for error
        var template =
          &#x27;&lt;div class=&quot;alert alert-danger col-xs-9 col-xs-offset-2&quot; role=&quot;alert&quot;&gt;&lt;i class=&quot;glyphicon glyphicon-warning-sign larger&quot;&gt;&lt;/i&gt; %error%&lt;/div&gt;&#x27;;

        // Create an element to hold the validation message
        var decorator = angular.element(&#x27;&lt;div&gt;&lt;/div&gt;&#x27;);
        element.after(decorator);

        // Watch ngModelController.$error.server &amp; show/hide validation accordingly
        scope.$watch(
          safeWatch(function () {
            return ngModelController.$error.server;
          }),
          showHideValidation
        );

        function showHideValidation(serverError) {
          // Display an error if serverError is true otherwise clear the element
          var errorHtml = &#x27;&#x27;;
          if (serverError) {
            var errorDictionary = scope.$eval(errorDictionaryExpression);
            errorHtml = template.replace(
              /%error%/,
              errorDictionary[errorKey] || &#x27;Unknown error occurred...&#x27;
            );
          }
          decorator.html(errorHtml);
        }

        // wipe the server error message upon keyup or change events so can revalidate with server
        element.on(&#x27;keyup change&#x27;, function (event) {
          scope.$apply(function () {
            ngModelController.$setValidity(&#x27;server&#x27;, true);
          });
        });
      }
    },
  ]);

  // Thanks @Basarat! http://stackoverflow.com/a/24863256/761388
  function safeWatch(expression) {
    return function () {
      try {
        return expression();
      } catch (e) {
        return null;
      }
    };
  }
})();
</code></pre><p>This version of the serverError directive is from a users perspective identical to the previous version. But it doesn’t use isolated scope – this means it can be used in concert with other directives which do.</p><p>It works by pulling the <code>name</code> and <code>serverError</code> values off the attrs parameter. <code>name</code> is just a string - the value of which never changes so it can be used as is. <code>serverError</code> is an expression that represents the error dictionary that is used to store the server error messages. This is accessed through use of <code>scope.$eval</code> as an when it needs to.</p><h2>My Plea</h2><p>What I’ve outlined here works. I’ll admit that usage of <code>$eval</code> makes me feel a little bit dirty (I’ve got <a href="http://www.jslint.com/lint.html#evil">“eval is evil”</a> running through my head). Whilst it works, I’m not sure what I’ve done is necessarily best practice. After all <a href="https://docs.angularjs.org/guide/directive">the Angular docs themselves say</a>:</p><blockquote><p><strong>*<!-- -->Best Practice:</strong> Use the scope option to create isolate scopes when making components that you want to reuse throughout your app. <!-- -->*</p></blockquote><p>But as we’ve seen this isn’t always an option. I’ve written this post to document my own particular struggle and ask the question “is there a better way?” If you know then please tell me!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Getting more RESTful with Web API and IHttpActionResult]]></title>
            <link>https://blog.johnnyreilly.com/2014/08/08/getting-more-RESTful-with-Web-API</link>
            <guid>Getting more RESTful with Web API and IHttpActionResult</guid>
            <pubDate>Fri, 08 Aug 2014 00:00:00 GMT</pubDate>
            <description><![CDATA[Up until, well yesterday really, I tended to have my Web API action methods all returning 200's no matter what. Successful request? 200 for you sir! Some validation error in the model? 200 for you too ma'am - but I'll wrap up the validation errors and send them back too. Database error? 200 and and an error message.]]></description>
            <content:encoded><![CDATA[<p>Up until, well yesterday really, I tended to have my Web API action methods all returning <a href="http://en.wikipedia.org/wiki/HTTP_200#2xx_Success">200</a>&#x27;s no matter what. Successful request? 200 for you sir! Some validation error in the model? 200 for you too ma&#x27;am - but I&#x27;ll wrap up the validation errors and send them back too. Database error? 200 and and an error message.</p><p>It kind of looked like this (this example taken from a <a href="http://icanmakethiswork.blogspot.co.uk/2014/08/angularjs-meet-aspnet-server-validation.html">previous post</a>):</p><pre><code class="language-cs">public class SageController : ApiController
{
  // ...

  public IHttpActionResult Post(User sage)
  {
    if (!ModelState.IsValid) {

      return Ok(new {
        Success = false,
        Errors = ModelState.ToErrorDictionary()
      });
    }

    sage = _userService.Save(sage);

    return Ok(new {
      Success = true,
      Entity = sage
    });
  }

  // ...
}
</code></pre><p>Well I&#x27;m no RESTafarian but this felt a little... wrong. Like I wasn&#x27;t fully embracing the web. I didn&#x27;t want to have to include my own <code>Success</code> flag to indicate whether the request was good or not. I decided that I&#x27;d rather have it at least a little more webby. To that end, I decided I&#x27;d like to have 2xx success status codes for genuine success only and 4xx client error status codes for failures.</p><p>Lose the wrapper - embrace the web. This post is about doing just that.</p><h2>Web API 2 - Bad Job on on the BadRequest Helper</h2><p>Web API 2 ships with a whole host of API helper methods. Things like <code>Ok</code> (which you can see me using above) and <code>BadRequest</code>. <code>BadRequest</code> was what I had in mind to use in place of <code>Ok</code> where I had some kind of error I wanted to report to the client like so:</p><pre><code class="language-cs">public class SageController : ApiController
{
  // ...

  public IHttpActionResult Post(User sage)
  {
    if (!ModelState.IsValid) {

      return BadRequest(new  {
        Errors = ModelState.ToErrorDictionary()
      });
    }

    sage = _userService.Save(sage);

    return Ok(new {
      Entity = sage
    });
  }

  // ...
}
</code></pre><p>Looks good right? No more need for my <code>Success</code> flag. Terser. Less code is better code. Unfortunately the built in <code>BadRequest</code> helper method doesn&#x27;t have the flexibility of the <code>Ok</code> helper method - it doesn&#x27;t allow you to send anything back you want. Fortunately this is easily remedied with a short extension method for <code>ApiController</code>:</p><pre><code class="language-cs">using System.Net;
using System.Web.Http;
using System.Web.Http.Results;

namespace System.Web.Http
{
    public static class ControllerExtensions
    {
        public static IHttpActionResult BadRequest&lt;T&gt;(this ApiController controller, T obj)
        {
            return new NegotiatedContentResult&lt;T&gt;(HttpStatusCode.BadRequest, obj, controller);
        }
    }
}
</code></pre><p>With this in place I can then tweak my implementation to hook into the extension method:</p><pre><code class="language-cs">public class SageController : ApiController
{
  // ...

  public IHttpActionResult Post(User sage)
  {
    if (!ModelState.IsValid) {
      // See how we have &quot;this.&quot; before BadRequest so the Extension method is invoked
      return this.BadRequest(new  {
        Errors = ModelState.ToErrorDictionary()
      });
    }

    sage = _userService.Save(sage);

    return Ok(new {
      Entity = sage
    });
  }

  // ...
}
</code></pre><p>And now we have have an endpoint that serves up 2xx status codes or 4xx status codes just as I&#x27;d hoped. Obviously this change in the way my action methods are returning will have implications for the consuming client (in my case an app built using AngularJS and $q). Essentially I can now use my <code>then</code> to handle the successes and my <code>catch</code> to handle the errors.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AngularJS meet ASP.Net Server Validation]]></title>
            <link>https://blog.johnnyreilly.com/2014/08/01/angularjs-meet-aspnet-server-validation</link>
            <guid>AngularJS meet ASP.Net Server Validation</guid>
            <pubDate>Fri, 01 Aug 2014 00:00:00 GMT</pubDate>
            <description><![CDATA[So. You're using AngularJS to build your front end with ASP.Net running on the server side. You're a trustworthy dev - you know that validation on the client will only get you so far. You need to validate on the server.]]></description>
            <content:encoded><![CDATA[<p>So. You&#x27;re using AngularJS to build your front end with ASP.Net running on the server side. You&#x27;re a trustworthy dev - you know that validation on the client will only get you so far. You need to validate on the server.</p><p>My particular scenario is where you have a form which you are saving. Angular serves you well when it comes to hooking in your own client side validation. But it doesn&#x27;t really ship with anything that supports <strong>nicely</strong> presenting server side validation on the client. Invariably when you look around you find people duplicating their server side validation on the client and presenting all their server side validation in a <code>&amp;lt;div&amp;gt;</code> at the top of the screen.</p><p>This works but it&#x27;s not as helpful to the user as it might be. It groups together all the validation from the server into one place. What I want is field level validation from the server that&#x27;s presented on a field level basis on the screen. Like this:</p><p><img src="https://2.bp.blogspot.com/-vxr6_ajRNj8/U9tQjs3SS3I/AAAAAAAAAr8/1HoOUJdPO0w/s320/server-error.png"/></p><p>I know. <a href="http://en.wikipedia.org/wiki/Endymion_(poem)">A thing of beauty is a joy forever.</a> Let us travel together to this promised land...</p><h2>What do we need client side?</h2><p>Well, let&#x27;s start with a directive which I&#x27;ll call <code>serverError</code>. This plants a validation message just <em>after</em> the element being validated which is displayed when that element is declared invalid by the server. (That is to say when the <code>ngModel</code> has a <code>$error.server</code> set.) When the element is changed then the <code>$error.server</code> is unset in order that validation can be hidden and the form can be revalidated against the server.</p><p>I&#x27;m using TypeScript with Angular so for my JavaScript examples I&#x27;ll give you both the TypeScript which I originally wrote and the generated JavaScript as well.</p><h3>TypeScript</h3><pre><code class="language-ts">interface serverErrorScope extends ng.IScope {
    name: string;
    serverError: { [field: string]: string };
}

app.directive(&quot;serverError&quot;, [function () {

  // Usage:
  // &lt;input class=&quot;col-xs-12 col-sm-9&quot;
  //        name=&quot;sage.name&quot; ng-model=&quot;vm.sage.name&quot; server-error=&quot;vm.errors&quot; /&gt;
  var directive = {
    link: link,
    restrict: &quot;A&quot;,
    require: &quot;ngModel&quot;, // supply the ngModel controller as the 4th parameter in the link function
    scope: { // Pass in name and serverError to the scope
      name: &quot;@&quot;,
      serverError: &quot;=&quot;
    }
  };
  return directive;

  function link(scope: serverErrorScope, element: ng.IAugmentedJQuery, attrs: ng.IAttributes, ngModelController: ng.INgModelController) {

    // Bootstrap alert template for error
    var template = &#x27;&lt;div class=&quot;alert alert-danger&quot; role=&quot;alert&quot;&gt;&#x27; +
                               &#x27;&lt;i class=&quot;glyphicon glyphicon-warning-sign&quot;&gt;&lt;/i&gt; &#x27; +
                               &#x27;%error%&lt;/div&gt;&#x27;;

    // Create an element to hold the validation message
    var decorator = angular.element(&#x27;&lt;div&gt;&lt;/div&gt;&#x27;);
    element.after(decorator);

    // Watch ngModelController.$error.server &amp; show/hide validation accordingly
    scope.$watch(safeWatch(() =&gt; ngModelController.$error.server), showHideValidation);

    function showHideValidation(serverError: boolean) {

      // Display an error if serverError is true otherwise clear the element
      var errorHtml = &quot;&quot;;
      if (serverError) {
        // Aliasing serverError and name to make it more obvious what their purpose is
        var errorDictionary = scope.serverError;
        var errorKey = scope.name;
        errorHtml = template.replace(/%error%/, errorDictionary[errorKey] || &quot;Unknown error occurred...&quot;);
      }
      decorator.html(errorHtml);
    }

    // wipe the server error message upon keyup or change events so can revalidate with server
    element.on(&quot;keyup change&quot;, (event) =&gt; {
      scope.$apply(() =&gt; { ngModelController.$setValidity(&quot;server&quot;, true); });
    });
  }
}]);

// Thanks @Basarat! http://stackoverflow.com/a/24863256/761388
function safeWatch&lt;t extends=&quot;&quot; function=&quot;&quot;&gt;(expression: T) {
  return () =&gt; {
    try {
      return expression();
    }
    catch (e) {
      return null;
    }
  };
}
&lt;/t&gt;
</code></pre><h3>JavaScript</h3><pre><code class="language-js">app.directive(&#x27;serverError&#x27;, [
  function () {
    // Usage:
    // &lt;input class=&quot;col-xs-12 col-sm-9&quot;
    //        name=&quot;sage.name&quot; ng-model=&quot;vm.sage.name&quot; server-error=&quot;vm.errors&quot; /&gt;
    var directive = {
      link: link,
      restrict: &#x27;A&#x27;,
      require: &#x27;ngModel&#x27;,
      scope: {
        name: &#x27;@&#x27;,
        serverError: &#x27;=&#x27;,
      },
    };
    return directive;

    function link(scope, element, attrs, ngModelController) {
      // Bootstrap alert template for error
      var template =
        &#x27;&lt;div class=&quot;alert alert-danger&quot; role=&quot;alert&quot;&gt;&#x27; +
        &#x27;&lt;i class=&quot;glyphicon glyphicon-warning-sign&quot;&gt;&lt;/i&gt; &#x27; +
        &#x27;%error%&lt;/div&gt;&#x27;;

      // Create an element to hold the validation message
      var decorator = angular.element(&#x27;&lt;div&gt;&lt;/div&gt;&#x27;);
      element.after(decorator);

      // Watch ngModelController.$error.server &amp; show/hide validation accordingly
      scope.$watch(
        safeWatch(function () {
          return ngModelController.$error.server;
        }),
        showHideValidation
      );

      function showHideValidation(serverError) {
        // Display an error if serverError is true otherwise clear the element
        var errorHtml = &#x27;&#x27;;
        if (serverError) {
          // Aliasing serverError and name to make it more obvious what their purpose is
          var errorDictionary = scope.serverError;
          var errorKey = scope.name;
          errorHtml = template.replace(
            /%error%/,
            errorDictionary[errorKey] || &#x27;Unknown error occurred...&#x27;
          );
        }
        decorator.html(errorHtml);
      }

      // wipe the server error message upon keyup or change events so can revalidate with server
      element.on(&#x27;keyup change&#x27;, function (event) {
        scope.$apply(function () {
          ngModelController.$setValidity(&#x27;server&#x27;, true);
        });
      });
    }
  },
]);

// Thanks @Basarat! http://stackoverflow.com/a/24863256/761388
function safeWatch(expression) {
  return function () {
    try {
      return expression();
    } catch (e) {
      return null;
    }
  };
}
</code></pre><p>If you look closely at this directive you&#x27;ll see it is restricted to be used as an attribute and it depends on 2 things:</p><ol><li>The value that the <code>server-error</code> attribute is set to should be an object which will contain key / values where the keys represent fields that are being validated.</li><li>The element being validated must have a name property (which will be used to look up the validation message in the <code>server-error</code> error &quot;dictionary&quot;.</li></ol><p>Totally not clear, right? Let&#x27;s have an example. Here is my &quot;sageEdit&quot; screen which you saw the screenshot of earlier:</p><pre><code class="language-html">&lt;section class=&quot;mainbar&quot; ng-controller=&quot;sageEdit as vm&quot;&gt;
  &lt;section class=&quot;matter&quot;&gt;
    &lt;div class=&quot;container-fluid&quot;&gt;
      &lt;form name=&quot;form&quot; novalidate role=&quot;form&quot;&gt;
        &lt;div&gt;
          &lt;button
            class=&quot;btn btn-info&quot;
            ng-click=&quot;vm.save()&quot;
            ng-disabled=&quot;!vm.canSave&quot;
          &gt;
            &lt;i class=&quot;glyphicon glyphicon-save&quot;&gt;&lt;/i&gt;Save
          &lt;/button&gt;
          &lt;span ng-show=&quot;vm.hasChanges&quot; class=&quot;dissolve-animation ng-hide&quot;&gt;
            &lt;i class=&quot;glyphicon glyphicon-asterisk orange&quot;&gt;&lt;/i&gt;
          &lt;/span&gt;
        &lt;/div&gt;
        &lt;div class=&quot;widget wblue&quot;&gt;
          &lt;div data-cc-widget-header title=&quot;{{vm.title}}&quot;&gt;&lt;/div&gt;
          &lt;div class=&quot;widget-content form-horizontal&quot;&gt;
            &lt;div class=&quot;form-group&quot;&gt;
              &lt;label class=&quot;col-xs-12 col-sm-2&quot;&gt;Name&lt;/label&gt;
              &lt;input
                class=&quot;col-xs-12 col-sm-9&quot;
                name=&quot;sage.name&quot;
                ng-model=&quot;vm.sage.name&quot;
                server-error=&quot;vm.errors&quot;
              /&gt;
            &lt;/div&gt;
            &lt;div class=&quot;form-group&quot;&gt;
              &lt;label class=&quot;col-xs-12 col-sm-2&quot;&gt;Username&lt;/label&gt;
              &lt;input
                class=&quot;col-xs-12 col-sm-9&quot;
                name=&quot;sage.userName&quot;
                ng-model=&quot;vm.sage.userName&quot;
                server-error=&quot;vm.errors&quot;
              /&gt;
            &lt;/div&gt;
            &lt;div class=&quot;form-group&quot;&gt;
              &lt;label class=&quot;col-xs-12 col-sm-2&quot;&gt;Email&lt;/label&gt;
              &lt;input
                class=&quot;col-xs-12 col-sm-9&quot;
                type=&quot;email&quot;
                name=&quot;sage.email&quot;
                ng-model=&quot;vm.sage.email&quot;
                server-error=&quot;vm.errors&quot;
              /&gt;
            &lt;/div&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/form&gt;
    &lt;/div&gt;
  &lt;/section&gt;
&lt;/section&gt;
</code></pre><p>If you look closely at where <code>server-error</code> is used we have a name attribute set (eg &quot;sage.email&quot;) and we&#x27;re passing in something called <code>&lt;em&gt;vm.&lt;/em&gt;errors</code> as the <code>server-error</code> attribute value. That&#x27;s because we&#x27;re using the &quot;controller as&quot; syntax and our controller is called <code>vm</code>.</p><p>On that controller we&#x27;re going to have a dictionary style object called <code>errors</code>. If you wanted to you could put that object on the scope instead and omit the &quot;vm.&quot; prefix. You could call it <code>wrongThingsWhatISpottedWithYourModel</code> or <code>barry</code> <!-- -->-<!-- --> whatever floats your boat really. You get my point; it&#x27;s flexible.</p><p>Let&#x27;s take a look at our sageEdit Angular controller:</p><h3>TypeScript</h3><pre><code class="language-ts">module controllers {
  &#x27;use strict&#x27;;

  interface sageEditRouteParams extends ng.route.IRouteParamsService {
    id: number;
  }

  interface sageEditScope extends ng.IScope {
    form: ng.IFormController;
  }

  class SageEdit {
    errors: { [field: string]: string };
    log: loggerFunction;
    logError: loggerFunction;
    logSuccess: loggerFunction;
    sage: sage;
    title: string;

    private _isSaving: boolean;

    static $inject = [
      &#x27;$location&#x27;,
      &#x27;$routeParams&#x27;,
      &#x27;$scope&#x27;,
      &#x27;common&#x27;,
      &#x27;datacontext&#x27;,
    ];
    constructor(
      private $location: ng.ILocationService,
      private $routeParams: sageEditRouteParams,
      private $scope: sageEditScope,
      private common: common,
      private datacontext: datacontext
    ) {
      this.errors = {};
      this.log = common.logger.getLogFn(controllerId);
      this.logError = common.logger.getLogFn(controllerId, &#x27;error&#x27;);
      this.logSuccess = common.logger.getLogFn(controllerId, &#x27;success&#x27;);
      this.sage = undefined;
      this.title = &#x27;Sage Edit&#x27;;

      this._isSaving = false;

      this.activate();
    }

    // Prototype methods

    activate() {
      var id = this.$routeParams.id;
      var dataPromises: ng.IPromise&lt;any&gt;[] = [this.getSage(id)];

      this.common
        .activateController(dataPromises, controllerId, this.title)
        .then(() =&gt; {
          this.log(&#x27;Activated Sage Edit View&#x27;);
          this.title = &#x27;Sage Edit: &#x27; + this.sage.name;
        });
    }

    getSage(id: number) {
      return this.datacontext.sage.getById(id).then((sage) =&gt; {
        this.sage = sage;
      });
    }

    save() {
      this.errors = {}; //Wipe server errors
      this._isSaving = true;
      this.datacontext.sage.save(this.sage).then((response) =&gt; {
        if (response.success) {
          this.sage = response.entity;
          this.logSuccess(
            &#x27;Saved &#x27; + this.sage.name + &#x27; [&#x27; + this.sage.id + &#x27;]&#x27;
          );
          this.$location.path(&#x27;/sages/detail/&#x27; + this.sage.id);
        } else {
          this.logError(&#x27;Failed to save&#x27;, response.errors);

          angular.forEach(response.errors, (errors, field) =&gt; {
            (&lt;ng.INgModelController&gt;this.$scope.form[field]).$setValidity(
              &#x27;server&#x27;,
              false
            );
            this.errors[field] = errors.join(&#x27;,&#x27;);
          });
        }

        this._isSaving = false;
      });
    }

    // Properties

    get hasChanges(): boolean {
      return this.$scope.form.$dirty;
    }

    get canSave(): boolean {
      return this.hasChanges &amp;&amp; !this._isSaving &amp;&amp; this.$scope.form.$valid;
    }
  }

  var controllerId = &#x27;sageEdit&#x27;;
  angular.module(&#x27;app&#x27;).controller(controllerId, SageEdit);
}
</code></pre><h3>JavaScript</h3><pre><code class="language-js">var controllers;
(function (controllers) {
  &#x27;use strict&#x27;;

  var SageEdit = (function () {
    function SageEdit($location, $routeParams, $scope, common, datacontext) {
      this.$location = $location;
      this.$routeParams = $routeParams;
      this.$scope = $scope;
      this.common = common;
      this.datacontext = datacontext;
      this.errors = {};
      this.log = common.logger.getLogFn(controllerId);
      this.logError = common.logger.getLogFn(controllerId, &#x27;error&#x27;);
      this.logSuccess = common.logger.getLogFn(controllerId, &#x27;success&#x27;);
      this.sage = undefined;
      this.title = &#x27;Sage Edit&#x27;;

      this._isSaving = false;

      this.activate();
    }
    // Prototype methods
    SageEdit.prototype.activate = function () {
      var _this = this;
      var id = this.$routeParams.id;
      var dataPromises = [this.getSage(id)];

      this.common
        .activateController(dataPromises, controllerId, this.title)
        .then(function () {
          _this.log(&#x27;Activated Sage Edit View&#x27;);
          _this.title = &#x27;Sage Edit: &#x27; + _this.sage.name;
        });
    };

    SageEdit.prototype.getSage = function (id) {
      var _this = this;
      return this.datacontext.sage.getById(id).then(function (sage) {
        _this.sage = sage;
      });
    };

    SageEdit.prototype.save = function () {
      var _this = this;
      this.errors = {}; //Wipe server errors
      this._isSaving = true;
      this.datacontext.sage.save(this.sage).then(function (response) {
        if (response.success) {
          _this.sage = response.entity;
          _this.logSuccess(
            &#x27;Saved &#x27; + _this.sage.name + &#x27; [&#x27; + _this.sage.id + &#x27;]&#x27;
          );

          _this.$location.path(&#x27;/sages/detail/&#x27; + _this.sage.id);
        } else {
          _this.logError(&#x27;Failed to save&#x27;, response.errors);

          angular.forEach(response.errors, function (errors, field) {
            _this.$scope.form[field].$setValidity(&#x27;server&#x27;, false);
            _this.errors[field] = errors.join(&#x27;,&#x27;);
          });
        }

        _this._isSaving = false;
      });
    };

    Object.defineProperty(SageEdit.prototype, &#x27;hasChanges&#x27;, {
      // Properties
      get: function () {
        return this.$scope.form.$dirty;
      },
      enumerable: true,
      configurable: true,
    });

    Object.defineProperty(SageEdit.prototype, &#x27;canSave&#x27;, {
      get: function () {
        return this.hasChanges &amp;&amp; !this._isSaving &amp;&amp; this.$scope.form.$valid;
      },
      enumerable: true,
      configurable: true,
    });
    SageEdit.$inject = [
      &#x27;$location&#x27;,
      &#x27;$routeParams&#x27;,
      &#x27;$scope&#x27;,
      &#x27;common&#x27;,
      &#x27;datacontext&#x27;,
    ];
    return SageEdit;
  })();

  var controllerId = &#x27;sageEdit&#x27;;
  angular.module(&#x27;app&#x27;).controller(controllerId, SageEdit);
})(controllers || (controllers = {}));
</code></pre><p>Okay - this is a shedload of code and most of it isn&#x27;t relevant to you. I share it as I like to see things in context. Let&#x27;s focus in on the important bits that you should take away. Firstly, our controller has a property called <code>errors</code>.</p><p>Secondly, when we attempt to save our server sends back a JSON payload which, given a validation failure, looks something like this:</p><pre><code class="language-json">{
  &quot;success&quot;: false,
  &quot;errors&quot;: {
    &quot;sage.name&quot;: [&quot;The Name field is required.&quot;],
    &quot;sage.userName&quot;: [
      &quot;The UserName field is required.&quot;,
      &quot;The field UserName must be a string with a minimum length of 3 and a maximum length of 30.&quot;
    ],
    &quot;sage.email&quot;: [&quot;The Email field is not a valid e-mail address.&quot;]
  }
}
</code></pre><p>So let&#x27;s pare back our <code>save</code> function to the bare necessities (those simple bare necessities, forget about your worries and your strife...):</p><h3>TypeScript</h3><pre><code class="language-ts">save() {

      this.errors = {}; //Wipe server errors

      this.datacontext.sage.save(this.sage).then(response =&gt; {

        if (response.success) {
          this.sage = response.entity;
        }
        else {
          angular.forEach(response.errors, (errors, field) =&gt; {
            (&lt;ng.INgModelController&gt;this.$scope.form[field]).$setValidity(&quot;server&quot;, false);
            this.errors[field] = errors.join(&quot;,&quot;);
          });
        }
      });
    }
</code></pre><h3>JavaScript</h3><pre><code class="language-js">SageEdit.prototype.save = function () {
  var _this = this;
  this.errors = {}; //Wipe server errors
  this.datacontext.sage.save(this.sage).then(function (response) {
    if (response.success) {
      _this.sage = response.entity;
    } else {
      angular.forEach(response.errors, function (errors, field) {
        _this.$scope.form[field].$setValidity(&#x27;server&#x27;, false);
        _this.errors[field] = errors.join(&#x27;,&#x27;);
      });
    }
  });
};
</code></pre><p>At the point of save we wipe any server error messages that might be stored on the client. Then, if we receive back a payload with errors we store those errors and set the validity of the relevant form element to false. This will trigger the display of the message by our directive.</p><p>That&#x27;s us done for the client side. You&#x27;re no doubt now asking yourself this question:</p><h2>How can I get ASP.Net to send me this information?</h2><p>So glad you asked. We&#x27;ve a simple model that looks like this which has a number of data annotations:</p><pre><code class="language-cs">public class Sage
{
  public int Id { get; set; }

  [Required]
  public string Name { get; set; }

  [Required]
  [StringLength(30, MinimumLength = 3)]
  public string UserName { get; set; }

  [EmailAddress]
  public string Email { get; set; }
}
</code></pre><p>When we save we post back to a Web API controller that looks like this:</p><pre><code class="language-cs">public class SageController : ApiController
{
  // ...

  public IHttpActionResult Post(User sage)
  {
    if (!ModelState.IsValid) {

      return Ok(new
      {
        Success = false,
        Errors = ModelState.ToErrorDictionary()
      });
    }

    sage = _userService.Save(sage);

    return Ok(new
    {
      Success = true,
      Entity = sage
    });
  }

  // ...
}
</code></pre><p>As you can see, when <code>ModelState</code> is not valid we send back a dictionary of the <code>ModelState</code> error messages keyed by property name. We generate this with an extension method I wrote called <code>ToErrorDictionary</code>:</p><pre><code class="language-cs">public static class ModelStateExtensions
{
  public static Dictionary&lt;string, IEnumerable&lt;string&gt;&gt; ToErrorDictionary(
    this System.Web.Http.ModelBinding.ModelStateDictionary modelState, bool camelCaseKeyName = true)
  {
    var errors = modelState
      .Where(x =&gt; x.Value.Errors.Any())
      .ToDictionary(
        kvp =&gt; CamelCasePropNames(kvp.Key),
        kvp =&gt; kvp.Value.Errors.Select(e =&gt; e.ErrorMessage)
      );

    return errors;
  }

  private static string CamelCasePropNames(string propName)
  {
    var array = propName.Split(&#x27;.&#x27;);
    var camelCaseList = new string[array.Length];
    for (var i = 0; i &lt; array.Length; i++)
    {
      var prop = array[i];
      camelCaseList[i] = prop.Substring(0, 1).ToLower() + prop.Substring(1, prop.Length - 1);
    }
    return string.Join(&quot;.&quot;, camelCaseList);
  }
}
</code></pre><p>That&#x27;s it - your solution front to back. It would be quite easy to hook other types of validation in server-side (database level checks etc). I hope you find this useful.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[HotTowel-Angular meet TypeScript]]></title>
            <link>https://blog.johnnyreilly.com/2014/07/03/hottowel-angular-meet-typescript</link>
            <guid>HotTowel-Angular meet TypeScript</guid>
            <pubDate>Thu, 03 Jul 2014 00:00:00 GMT</pubDate>
            <description><![CDATA[I've recently ported John Papa's popular Hot Towel Angular SPA Template to TypeScript. Why? Because it was there.]]></description>
            <content:encoded><![CDATA[<p>I&#x27;ve recently ported John Papa&#x27;s popular <a href="https://github.com/johnpapa/HotTowel-Angular">Hot Towel Angular SPA Template</a> to TypeScript. Why? <a href="http://en.wikipedia.org/wiki/George_Mallory">Because it was there.</a></p><p>If you&#x27;d like to read more about HotTowel-Angular then have a read of <a href="http://www.johnpapa.net/hot-towel-angular/">John Papa&#x27;s post</a>. You can find my port on GitHub <a href="https://github.com/johnnyreilly/HotTowel-Angular-TypeScript">here</a>.</p><h2>What is this port you speak of?</h2><p>It is <strong>intentionally</strong> a &quot;bare bones&quot; port of the HotTowel-Angular JavaScript code across to TypeScript. It&#x27;s essentially the same code as John&#x27;s - just with added type annotations (and yes it is <code>noImplicitAny</code> compliant).</p><p>You could, if you wanted to, go much further. You could start using a whole host of TypeScripts functionality: modules / classes / arrow functions... the whole shebang. But my port is deliberately not that; I didn&#x27;t want to scare your horses... I wanted you to see how easy it is to move from JS to TS. And I&#x27;m standing on the shoulders of that great giant <a href="https://twitter.com/john_papa">John Papa</a> for that purpose.</p><p>If you wanted an example of how you might go further in an Angular port to TypeScript then you could take a look at my <a href="http://icanmakethiswork.blogspot.co.uk/2014/06/migrating-from-angularjs-to-angularts.html">previous post</a> on the topic.</p><h2>What&#x27;s in the repo?</h2><p>The repo contains the contents of HotTowel-Angular&#x27;s app folder, with each JavaScript file converted over to TypeScript. The compiled JavaScript files are also included so that you can compare just how similar the compiled JavaScript is to John&#x27;s original code.</p><p>In fact there are only 2 differences in the end:</p><h3>1<!-- -->.<!-- --> sidebar.js&#x27;s <code>getNavRoutes</code></h3><p>...had the filtering changed from this:</p><pre><code class="language-ts">return r.config.settings &amp;&amp; r.config.settings.nav;
</code></pre><p>to this:</p><pre><code class="language-ts">return r.config.settings &amp;&amp; r.config.settings.nav ? true : false;
</code></pre><p>This was necessary as TypeScript insists that the array <code>filter</code> predicate returns a <code>boolean</code>. John&#x27;s original method returns a number (<code>nav</code>&#x27;s value to be clear) which actually seems to work fine. My assumption is that JavaScript&#x27;s filter method is happy with a truth-y / false-y test which John&#x27;s implementation would satisfy.</p><h3>2<!-- -->.<!-- --> common.js&#x27;s <code>$broadcast</code></h3><p>...had to be given a rest parameter to satisfy the TS compiler. John&#x27;s original method exposed no parameters as it just forwards on whatever arguments are passed to it. This means that <code>$broadcast</code> has a bit of unused code in the head of the generated method:</p><pre><code class="language-js">var args = [];
for (var _i = 0; _i &lt; arguments.length - 0; _i++) {
  args[_i] = arguments[_i + 0];
}
</code></pre><h2>If you want to use this</h2><p>Then simply follow the instructions for installing <a href="https://github.com/johnpapa/HotTowel-Angular">HotTowel-Angular</a> and then drop this repo&#x27;s app folder over the one just created when HotTowel-Angular was installed. If you&#x27;re using Visual Studio then make sure that you include the new TS files into your project and give them the <code>BuildAction</code> of <code>TypeScriptCompile</code>.</p><p>You&#x27;ll need the following NuGet packages for the relevant DefinitelyTyped Typings:</p><pre><code class="language-ps">Install-Package angularjs.TypeScript.DefinitelyTyped
    Install-Package angular-ui-bootstrap.TypeScript.DefinitelyTyped
    Install-Package jquery.TypeScript.DefinitelyTyped
    Install-Package spin.TypeScript.DefinitelyTyped
    Install-Package toastr.TypeScript.DefinitelyTyped
</code></pre><p>And you&#x27;re good to go. If you&#x27;re not using Visual Studio then you may need to add in some <code>&amp;lt;reference path=&quot;angular.d.ts&quot; /&amp;gt;</code> etc. statements to the TypeScript files as well.</p><p>If you&#x27;re interested in the specific versions of the typings that I used then you can find them in the <code>packages.config</code> of the repo.</p><h2>Thanks</h2><p>To John Papa for creating HotTowel-Angular. Much love.</p><p>And my mum too... Just because.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A folk story wherein we shall find dates, DataAnnotations & data impedance mismatch]]></title>
            <link>https://blog.johnnyreilly.com/2014/06/20/dates-DataAnnotations-and-data-impedance-mismatch</link>
            <guid>A folk story wherein we shall find dates, DataAnnotations &amp; data impedance mismatch</guid>
            <pubDate>Fri, 20 Jun 2014 00:00:00 GMT</pubDate>
            <description><![CDATA[If you ever take a step back from what you're doing it can sometimes seem pretty abstract. Here's an example. I was looking at an issue in an app that I was supporting. The problem concerned a field which was to store a date value. Let's call it, for the sake of argument, valuation_date. (Clearly in reality the field name was entirely different... Probably.) This field was supposed to represent a specific date, like June 15th 2012 or 19th August 2014. To be clear, a date and \*not\* in any way, a time.]]></description>
            <content:encoded><![CDATA[<p>If you ever take a step back from what you&#x27;re doing it can sometimes seem pretty abstract. Here&#x27;s an example. I was looking at an issue in an app that I was supporting. The problem concerned a field which was to store a date value. Let&#x27;s call it, for the sake of argument, <code>valuation_date</code>. (Clearly in reality the field name was entirely different... Probably.) This field was supposed to represent a specific date, like June 15th 2012 or 19th August 2014. To be clear, a date and <!-- -->*<strong>not</strong>*<!-- --> in any way, a time.</p><p><code>valuation_date</code> was stored in a SQL database as a <code>&lt;a href=&quot;http://msdn.microsoft.com/en-gb/library/ms187819.aspx&quot;&gt;datetime&lt;/a&gt;</code>. That&#x27;s right a date with a time portion. I&#x27;ve encountered this sort of scenario many times on systems I&#x27;ve inherited. Although there is a <code>&lt;a href=&quot;http://msdn.microsoft.com/en-gb/library/bb630352.aspx&quot;&gt;date&lt;/a&gt;</code> type in SQL it&#x27;s pretty rarely used. I think it only shipped in SQL Server with 2008 which may go some way to explaining this. Anyway, I digress...</p><p><code>valuation_date</code> was read into a field in a C# application called <code>ValuationDate</code> which was of type <code>&lt;a href=&quot;http://msdn.microsoft.com/en-us/library/system.datetime.aspx&quot;&gt;DateTime&lt;/a&gt;</code>. As the name suggests this is also a date with a time portion. After a travelling through various layers of application this ended up being serialized as JSON and sent across the wire where it became a JavaScript variable by the name of <code>valuationDate</code> which had the type <code>&lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Date&quot;&gt;Date&lt;/a&gt;</code>. Despite the deceptive name this is also, you guessed it, a date with a time portion. (Fine naming work there JavaScript!)</p><p>You can probably guess where I&#x27;m going with this... Despite our (cough) rock solid naming convention, the situation had arisen where actual datetimes had snuck in. That&#x27;s right, in the wilds of production, records with <code>valuation_date</code>s with time components had been spotted. My mission was to hunt them, kill them and stop them reproducing...</p><h2>A Primitive Problem</h2><p>Dates is a sticky topic in many languages. As I mentioned, SQL Server has a <code>&lt;a href=&quot;http://msdn.microsoft.com/en-gb/library/bb630352.aspx&quot;&gt;date&lt;/a&gt;</code> data type. C# has <code>&lt;a href=&quot;http://msdn.microsoft.com/en-gb/library/system.datetime.aspx&quot;&gt;DateTime&lt;/a&gt;</code>. If you want to operate on Dates alone then you&#x27;re best off talking looking at Jon Skeet&#x27;s <a href="http://nodatime.org/">NodaTime</a> <!-- -->-<!-- --> though most people start with <code>DateTime</code> and stick with it. (After all, it&#x27;s native.) As to JavaScript, well primitive-wise there&#x27;s no alternative to <code>Date</code> <!-- -->-<!-- --> but <code>&lt;a href=&quot;http://momentjs.com/&quot;&gt;Moment.js&lt;/a&gt;</code> may help.</p><p>My point is that it is a long standing issue in the development world. We represent data in types that aren&#x27;t entirely meant for the purpose that they are used. It&#x27;s not just restricted to dates, numbers have a comparable story around the issue of <a href="http://csharpindepth.com/Articles/General/Decimal.aspx">decimals and doubles</a>. As a result of data type issues, developers experience problems. Like the one I was facing.</p><h2>An Attribute Solution</h2><p>The source of the problem turned out to be the string JavaScript <code>&lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Date&quot;&gt;Date constructor&lt;/a&gt;</code> in an earlier version of Internet Explorer. The fix was switching away from using the JavaScript Date constructor in favour of using Moment.js&#x27;s more dependable ability to parse strings into dates. Happy days we&#x27;re working once more! Some quick work to put together a SQL script to fix up the data and we have ourselves our patch!</p><p>But we didn&#x27;t want to get bitten again. We wanted ourselves a little <a href="http://dictionary.cambridge.org/dictionary/british/belt-and-braces">belts and braces</a>. What do do? Hang on a minute, lads – I&#x27;ve got a great idea... It&#x27;s <code>&lt;a href=&quot;http://msdn.microsoft.com/en-us/library/system.componentmodel.dataannotations.validationattribute(v=vs.110).aspx&quot;&gt;ValidationAttribute&lt;/a&gt;</code> time!</p><p>We whipped ourselves up an attribute that looked like this:</p><pre><code class="language-cs">using System;
using System.ComponentModel.DataAnnotations;
using System.Globalization;

namespace My.Attributes
{
    [AttributeUsage(AttributeTargets.Property | AttributeTargets.Field, Inherited = false, AllowMultiple = false)]
    public class DateOnlyAttribute: ValidationAttribute
    {
        protected override ValidationResult IsValid(object value, ValidationContext validationContext)
        {
            if (value != null)
            {
                if (value is DateTime)
                {
                    // Date but not Time check
                    var date = (DateTime) value;
                    if (date.TimeOfDay != TimeSpan.Zero)
                    {
                        return new ValidationResult(date.ToString(&quot;O&quot;, CultureInfo.InvariantCulture) + &quot; is not a date - it is a date with a time&quot;, new[] { validationContext.MemberName });
                    }
                }
                else
                {
                    return new ValidationResult(&quot;DateOnlyAttribute can only be used on DateTime? and DateTime&quot;, new[] { validationContext.MemberName });
                }
            }

            return ValidationResult.Success;
        }
    }
}
</code></pre><p>This attribute does 2 things:</p><ol><li>Most importantly it fails validation for any <code>DateTime</code> or <code>DateTime?</code> that includes a time portion. It only allows through DateTimes where the clock strikes midnight. It&#x27;s optimised for Cinderella.</li><li>It fails validation if the attribute is applied to any property which is not a <code>DateTime</code> or <code>DateTime?</code>.</li></ol><p>You can decorate <code>DateTime</code> or <code>DateTime?</code> properties on your model with this attribute like so:</p><pre><code class="language-cs">namespace My.Models
{
    public class ImAModelYouKnowWhatIMean
    {
        public int Id { get; set; }

        [DateOnlyAttribute]
        public DateTime ValuationDate { get; set; }

        // Other properties...
    }
}
</code></pre><p>And if you&#x27;re using MVC (or anything that makes use of the validation data annotations) then you&#x27;ll now find that you are nicely protected from DateTimes masquerading as dates. Should they show up you&#x27;ll find that <code>ModelState.IsValid</code> is false and you can kick them to the curb with alacrity!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Migrating from AngularJS to AngularTS - a walkthrough]]></title>
            <link>https://blog.johnnyreilly.com/2014/06/01/migrating-from-angularjs-to-angularts</link>
            <guid>Migrating from AngularJS to AngularTS - a walkthrough</guid>
            <pubDate>Sun, 01 Jun 2014 00:00:00 GMT</pubDate>
            <description><![CDATA[It started with nuns. Don't all good stories start that way? One of my (many) aunts is a Poor Clare nun. At some point in the distant past I was cajoled into putting together a simple website for her convent. This post is a walkthrough of how to migrate from AngularJS using JavaScript to AngularJS using TypeScript. It just so happens that the AngularJS app in question is the one that belongs to my mother's sister's convent.]]></description>
            <content:encoded><![CDATA[<p>It started with nuns. Don&#x27;t all good stories start that way? One of my (many) aunts is a Poor Clare nun. At some point in the distant past I was cajoled into putting together a simple website for her convent. This post is a walkthrough of how to migrate from AngularJS using JavaScript to AngularJS using TypeScript. It just so happens that the AngularJS app in question is the one that belongs to my mother&#x27;s sister&#x27;s convent.</p><h2>TL;DR - grab what you need</h2><p>For reference the complete &quot;before&quot; and &quot;after&quot; projects can be found on GitHub <a href="https://github.com/johnnyreilly/AngularJS2AngularTS">here</a>. This is available so people can see clearly what changes have been made in the migration.</p><p>The content of the site is available for <u>reference only</u></p><p>. (Not that I can really imagine people creating their own &quot;Poor Clares&quot; site and hawking it to convents around the globe but I thought I&#x27;d make the point.) It looks like this:</p><p><img src="https://2.bp.blogspot.com/-jUf3uryRdKk/U4w3VVMX04I/AAAAAAAAAnQ/6Pu84tk92S0/s1600/SisterGabriel.png"/></p><h2>Background</h2><p>I&#x27;ve been quietly maintaining this website / app for quite a while now. It&#x27;s a very simple site; 95% of it is static content about the convent. The one piece of actual functionality is a page which allows the user of the website to send a prayer request to the nuns at the convent:</p><p><img src="https://2.bp.blogspot.com/-DChKaPJu4eE/U4w4DPbwxCI/AAAAAAAAAnY/PPtSe_HzPCU/s1600/OurPrayer.png"/></p><p>Behind the scenes this sends 2 emails:</p><ul><li>The first back to the person who submitted the prayer request assuring them that they will be prayed for.</li><li>The second to the convent telling them the details of what the person would like prayer for.</li></ul><aside><em>It&#x27;s not accidental that I am not sharing the location of my aunt&#x27;s website in this post. Given the inherent mischievousness of most developers (I should know, I am one) I harbour a fear that readers of this post might go away and submit many an insincere prayer request (or worse) to the convent. If that&#x27;s you I don&#x27;t intend to help you. You&#x27;re clever, you&#x27;ll find the site if you are so minded. But please know that the nuns who read any of your prayer requests are wonderful people (nuns get a bad rep) and that they love you. They *<strong>will</strong>* pray for you. They&#x27;re good like that. I appeal to your better nature on this.</em></aside><p>Right now you are probably thinking this is an unusual post. Perhaps it is, but bear with me.</p><p>Over time the website has had many incarnations. It&#x27;s been table-based layout, it&#x27;s used Kendo UI, it&#x27;s used Bootstrap. It&#x27;s been static HTML, it&#x27;s been ASP.Net WebForms, it&#x27;s been ASP.Net MVC and it&#x27;s currently built using <strong>AngularJS</strong> with <strong>MVC</strong> on the back-end to handle bundling / minification and dispatching of emails.</p><p>I decided to migrate this AngularJS app to use TypeScript. As I did that I thought I&#x27;d document the process for anyone else who might be considering doing something similar. As it happens this is a particularly good candidate for migration as there&#x27;s a full unit test suite for the app (written with Jasmine). Once I&#x27;ve finished the migration these unit tests should pass, just as they do currently.</p><p>You are probably thinking to yourself &quot;but TypeScript is just about adding compile-time annotations right? How could the unit tests not pass after migration?&quot; Fair point, well made. Well that is generally true but I have something slightly different planned when we get to the controllers - you&#x27;ll see what I mean...</p><p>It&#x27;s also a good candidate for documenting a walkthrough as it&#x27;s a particularly small and simple Angular app. It consists of just <strong>3 controllers</strong>, <strong>2 services</strong> and <strong>1 app</strong>.</p><p>Before I kick off I thought I&#x27;d list a couple of guidelines / caveats on this post:</p><ul><li>I don&#x27;t intend to say much about the architecture of this application - I want to focus on the migration from JavaScript to TypeScript.</li><li>The choices that I make for the migration path do not necessarily reflect the &quot;one true way&quot;. Rather, they are pragmatic choices that I am making - there may be alternatives approaches here and there that could be used instead.</li><li>I love Visual Studio - it&#x27;s my IDE of choice and the one I am using as I perform the migration. Some of the points that I will make are Visual Studio specific - I will try and highlight that when appropriate.</li></ul><h2>Typings</h2><p>The first thing we&#x27;re going to need to get going are the Angular typing files which can be found on Definitely Typed <a href="https://github.com/borisyankov/DefinitelyTyped/tree/master/angularjs">here</a>. Since these typings are made available over <a href="https://www.nuget.org/packages/angularjs.TypeScript.DefinitelyTyped/">NuGet</a> I&#x27;m going to pull them in with a wave of my magic <code>Install-Package angularjs.TypeScript.DefinitelyTyped</code>.</p><p>As well as pulling in the typing files Visual Studio 2013 has also made some tweaks to my <code>PoorClaresAngular.csproj</code> file which it tells me about:</p><p><img src="https://4.bp.blogspot.com/-DZcJ-YANHAE/U4b6Yd4Zr7I/AAAAAAAAAlM/SYpK8RFSVgg/s1600/TypeScriptDialog.png"/></p><p>And these are the TypeScript specific additions that Visual Studio has made to <code>PoorClaresAngular.csproj</code>:</p><pre><code class="language-xml">&lt;Import
   Project=&quot;$(MSBuildExtensionsPath32)\Microsoft\VisualStudio\v$(VisualStudioVersion)\TypeScript\Microsoft.TypeScript.Default.props&quot;
   Condition=&quot;Exists(&#x27;$(MSBuildExtensionsPath32)\Microsoft\VisualStudio\v$(VisualStudioVersion)\TypeScript\Microsoft.TypeScript.Default.props&#x27;)&quot; /&gt;

  &lt;TypeScriptToolsVersion&gt;1.0&lt;/TypeScriptToolsVersion&gt;

  &lt;Import
   Project=&quot;$(MSBuildExtensionsPath32)\Microsoft\VisualStudio\v$(VisualStudioVersion)\TypeScript\Microsoft.TypeScript.targets&quot;
   Condition=&quot;Exists(&#x27;$(MSBuildExtensionsPath32)\Microsoft\VisualStudio\v$(VisualStudioVersion)\TypeScript\Microsoft.TypeScript.targets&#x27;)&quot; /&gt;
</code></pre><p>I&#x27;m going to add one extra of my own:</p><pre><code class="language-xml">&lt;TypeScriptNoImplicitAny&gt;True&lt;/TypeScriptNoImplicitAny&gt;
</code></pre><p>This prevents you having variables of type <code>any</code> in your TypeScript codebase without you implicitly specifying the type. You can live without this but I&#x27;ve found it&#x27;s useful to catch where you&#x27;re missing out on the benefit of static typing. Further to that, this option can be particularly useful when performing a migration. It will become obvious why this is the case as we go on.</p><p>I decline the kind opportunity to further search NuGet as I&#x27;m already on my way typing-wise. So let&#x27;s review what has happened. Below you can see the typing files that have been pulled in and that the project and packages files were amended:</p><p><img src="https://1.bp.blogspot.com/-mgEjfAnPp5I/U4b7wqDLkdI/AAAAAAAAAlY/MlfZA5c7TIs/s1600/AddedTypings.png"/></p><h2>Changing JS files to TS files</h2><p>This really should be as simple as changing all the JavaScript files underneath the <code>js</code> directory to have the suffix <code>ts</code>. So going from this:</p><p><img src="https://1.bp.blogspot.com/-El_425y9130/U4clVSYUO4I/AAAAAAAAAlo/Z-NmvPlOjiA/s1600/js.png"/></p><p>To this:</p><p><img src="https://2.bp.blogspot.com/-QMLxo7CnDV0/U4clbseUBsI/AAAAAAAAAlw/oRCZf5YqZUQ/s1600/ts.png"/></p><p>And if you&#x27;re not using Visual Studio it is. But if you are using Visual Studio there&#x27;s a certain amount of fiddling required to include the generated <code>.js</code> and <code>.js.map</code> files associated with each <code>.ts</code> file. The easiest (hah!) thing to do is to crack open the project and wherever you find a <code>&amp;lt;TypeScriptCompile Include=&quot;js\somePath.ts&quot; /&amp;gt;</code> to add in 2 <code>Content</code> statements, one for each generated file which states the dependency on the TypeScript file. For example:</p><pre><code class="language-xml">&lt;TypeScriptCompile Include=&quot;js\services\siteSectionService.ts&quot; /&gt;
    &lt;Content Include=&quot;js\services\siteSectionService.js&quot;&gt;
      &lt;DependentUpon&gt;siteSectionService.ts&lt;/DependentUpon&gt;
    &lt;/Content&gt;
    &lt;Content Include=&quot;js\services\siteSectionService.js.map&quot;&gt;
      &lt;DependentUpon&gt;siteSectionService.ts&lt;/DependentUpon&gt;
    &lt;/Content&gt;
</code></pre><p>It&#x27;s a bit of a pain to have to do this at the moment. Hopefully the Visual Studio tooling will catch up so this sort of tweaking becomes unnecessary.</p><h2>Recap</h2><p>So, where are we? Well, we&#x27;ve got our project ready for TypeScript, we&#x27;ve pulled in the Angular typings from Definitely Typed and we&#x27;ve turned all our JavaScript files in the <code>js</code> directory into TypeScript files.</p><p>Now we can actually start working through our TypeScript files and ensuring we&#x27;re all typed correctly. Please note that because I&#x27;m working in Visual Studio I get the benefit of implicit referencing; I don&#x27;t have to explicitly state the typing files each TypeScript file relies on at the head of the file (eg <code>/// &amp;lt;reference path=&quot;angularjs/angular.d.ts&quot; /&amp;gt;</code>). If you aren&#x27;t working in Visual Studio then you&#x27;d need to add these yourself.</p><h2>TypeScriptify <code>app.ts</code></h2><p>Opening up <code>app.ts</code> we&#x27;re presented with a few red squigglies:</p><p><img src="https://4.bp.blogspot.com/-91g1TEbkZd4/U4ctcYQqogI/AAAAAAAAAmI/qQzfzNAaPhA/s1600/app.ts.png"/></p><p>These red squigglies are the direct result of my earlier opting in to <code>NoImplicitAny</code>. So in my view it&#x27;s already paid for itself as it&#x27;s telling me where I could start using typings. So to get things working nicely I&#x27;ll give <code>$routeProvider</code> the type of <code>ng.route.IRouteProvider</code> and I&#x27;ll explicitly specify the type of <code>any</code> for the 2 <code>params</code> parameters:</p><pre><code class="language-ts">// ...
    function ($routeProvider: ng.route.IRouteProvider) {

        function getTheConventTemplateUrl(params: any) {
            var view = params.view || &quot;home&quot;;
            return &quot;partials/theConvent/&quot; + view + &quot;.html&quot;;
        }

        function getMainTemplateUrl(params: any) {
            var view = params.view || &quot;home&quot;;
            return &quot;partials/main/&quot; + view + &quot;.html&quot;;
        }

        // ...
    }
    // ...
</code></pre><h2>TypeScriptify <code>siteSectionService.ts</code></h2><p>Opening up <code>siteSectionService.ts</code> we&#x27;re only presented with a single squiggly, and for the same reason as last time:</p><p><img src="https://4.bp.blogspot.com/-aFd1JgtcLIU/U4cwBbs8N7I/AAAAAAAAAmU/x9GME8J5CMc/s1600/siteSectionService.ts.png"/></p><p>This error is easily remedied by giving <code>path</code> the type of <code>string</code>.</p><p>What&#x27;s more interesting / challenging is thinking about how we want to enforce the definition of <code>siteSectionService</code>. Remember, this is a service and as such it will be re-used elsewhere in the application (in both <code>navController</code> and <code>mainController</code>). What we need is an interface that describes what our (revealing module pattern) service exposes:</p><pre><code class="language-ts">&#x27;use strict&#x27;;

interface ISiteSectionService {
  getSiteSection: () =&gt; string;
  determineSiteSection: (path: string) =&gt; void;
}

angular.module(&#x27;poorClaresApp.services&#x27;).factory(
  &#x27;siteSectionService&#x27;,

  [
    // No dependencies at present
    function (): ISiteSectionService {
      var siteSection = &#x27;home&#x27;;

      function getSiteSection() {
        return siteSection;
      }

      function determineSiteSection(path: string) {
        var newSiteSection = &#x27;home&#x27;;

        if (path.indexOf(&#x27;/theConvent/&#x27;) !== -1) {
          newSiteSection = &#x27;theConvent&#x27;;
        } else if (path !== &#x27;/&#x27;) {
          newSiteSection = &#x27;main&#x27;;
        }

        siteSection = newSiteSection;
      }

      return {
        getSiteSection: getSiteSection,
        determineSiteSection: determineSiteSection,
      };
    },
  ]
);
</code></pre><p>As you can see the <code>ISiteSectionService </code> interface is marked as the return type of the function. This ensures that what we return from the function satisfies that definition. Also, it allows us to re-use that interface elsewhere (as we will do later).</p><h2>TypeScriptify <code>prayerRequestService.ts</code></h2><p>Opening up <code>prayerRequestService.ts</code> we&#x27;re again in <code>NoImplicitAny</code> country:</p><p><img src="https://4.bp.blogspot.com/-QfZUdnxu5oA/U4c0iI-JF3I/AAAAAAAAAmg/pbwlmGGbBjo/s1600/prayerRequestService.ts.png"/></p><p>This is fixed up by defining <code>$http</code> as <code>ng.IHttpService</code> and <code>email</code> and <code>prayFor</code> as <code>string</code>.</p><p>As with <code>siteSectionService</code> we need to create an interface to define what <code>prayerRequestService</code> exposes. This leaves us with this:</p><pre><code class="language-ts">&#x27;use strict&#x27;;

interface IPrayerRequestService {
  sendPrayerRequest: (
    email: string,
    prayFor: string
  ) =&gt; ng.IPromise&lt;{
    success: boolean;
    text: string;
  }&gt;;
}

angular.module(&#x27;poorClaresApp.services&#x27;).factory(
  &#x27;prayerRequestService&#x27;,

  [
    &#x27;$http&#x27;,
    function ($http: ng.IHttpService): IPrayerRequestService {
      var url = &#x27;/PrayerRequest&#x27;;

      function sendPrayerRequest(email: string, prayFor: string) {
        var params = { email: email, prayFor: prayFor };

        return $http.post(url, params).then(function (response) {
          return {
            success: response.data.success,
            text: response.data.text,
          };
        });
      }

      return {
        sendPrayerRequest: sendPrayerRequest,
      };
    },
  ]
);
</code></pre><h2>TypeScriptify <code>prayerRequestController.ts</code></h2><p>Opening up <code>prayerRequestController.ts</code> leads me to the conclusion that I have <strong>no interesting way left</strong> of telling you that we once more need to supply types for our parameters. Let&#x27;s take it as read that the same will happen on all remaining files as well eh? Hopefully by now it&#x27;s fairly clear that this option is useful, even if only for a migration. I say this because using it forces you to think about what typings should be applied to your code:</p><p><img src="https://3.bp.blogspot.com/-5-joMHeUrNE/U4c5tcYeoLI/AAAAAAAAAmw/qwl0Bjz21zA/s1600/prayerRequestController.png"/></p><p>We&#x27;ll define <code>$scope</code> as <code>ng.IScope</code>, <code>prayerRequestService</code> as <code>IPrayerRequestService</code> (which we created just now) and <code>prayerRequest</code> as <code>{ email: string; prayFor: string }</code>. Which leaves me with this:</p><pre><code class="language-ts">&#x27;use strict&#x27;;

angular.module(&#x27;poorClaresApp.controllers&#x27;).controller(
  &#x27;PrayerRequestController&#x27;,

  [
    &#x27;$scope&#x27;,
    &#x27;prayerRequestService&#x27;,
    function ($scope: ng.IScope, prayerRequestService: IPrayerRequestService) {
      var vm = this;

      vm.send = function (prayerRequest: { email: string; prayFor: string }) {
        vm.message = {
          success: true,
          text: &#x27;Sending...&#x27;,
        };

        prayerRequestService
          .sendPrayerRequest(prayerRequest.email, prayerRequest.prayFor)
          .then(function (response) {
            vm.message = {
              success: response.success,
              text: response.text,
            };
          })
          .then(null, function (error) {
            // IE 8 friendly alias for catch
            vm.message = {
              success: false,
              text: &#x27;Sorry your email was not sent&#x27;,
            };
          });
      };
    },
  ]
);
</code></pre><p>I could move on but let&#x27;s go for bonus points (and now you&#x27;ll see why the unit test suite is so handy). To quote the Angular documentation:</p><blockquote><p>In Angular, a Controller is a JavaScript constructor function that is used to augment the Angular Scope.</p></blockquote><p>So let&#x27;s see if we can swap over our vanilla contructor function for a TypeScript class. This will (in my view) better express the intention of the code. To do this I am essentially following the example laid down by my Definitely Typed colleague <a href="https://twitter.com/basarat">Basarat</a>. I highly recommend his <a href="https://www.youtube.com/watch?v=WdtVn_8K17E">screencast on the topic</a>. Also kudos to <a href="https://twitter.com/andrewdavey">Andrew Davey</a> whose <a href="http://aboutcode.net/2013/10/20/typescript-angularjs-controller-classes.html">post on the topic</a> also fed into this.</p><pre><code class="language-ts">&#x27;use strict&#x27;;

module poorClaresApp.controllers {
  class PrayerRequestController {
    static $inject = [&#x27;$scope&#x27;, &#x27;prayerRequestService&#x27;];
    constructor(
      private $scope: ng.IScope,
      private prayerRequestService: IPrayerRequestService
    ) {}

    message: { success: boolean; text: string };

    send(prayerRequest: { email: string; prayFor: string }) {
      this.message = {
        success: true,
        text: &#x27;Sending...&#x27;,
      };

      this.prayerRequestService
        .sendPrayerRequest(prayerRequest.email, prayerRequest.prayFor)
        .then((response) =&gt; {
          this.message = {
            success: response.success,
            text: response.text,
          };
        })
        .then(null, (error) =&gt; {
          // IE 8 friendly alias for catch
          this.message = {
            success: false,
            text: &#x27;Sorry your email was not sent&#x27;,
          };
        });
    }
  }

  angular
    .module(&#x27;poorClaresApp.controllers&#x27;)
    .controller(&#x27;PrayerRequestController&#x27;, PrayerRequestController);
}
</code></pre><p>My only reservation with this approach is that we have to declare the TypeScript class outside the <code>angular.module...</code> statement. To avoid cluttering up global scope I&#x27;ve placed our class in a module called <code>poorClaresApp.controllers</code> which maps nicely to our Angular module name. It would be nice if I could place the class definition in an <a href="http://en.wikipedia.org/wiki/Immediately-invoked_function_expression">IIFE</a> to completely keep this completely isolated but TypeScript doesn&#x27;t allow for that syntax (for reasons I&#x27;m unclear about - the output would be legal JavaScript).</p><p>For a small class this seems to add a little noise but as classes grow in complexity I think this approach will quickly start to pay dividends. There are a few things worth noting about the above approach:</p><ul><li>The required injectable parameters have moved into the class definition in the form of the <code>static $inject</code> statement. I personally like that this no longer sits outside the code it relates to.</li><li>Because we&#x27;re using TypeScript arrow functions (which preserve the outer &quot;this&quot; context) we are now free to dispose of the <code>var vm = this;</code> mechanism we&#x27;re were previously using for the same purpose. Much more intuitive code to my mind.</li><li>We are not actually using <code>$scope</code> at all in this controller - maybe it should be removed entirely in the long run.</li></ul><h2>TypeScriptify <code>navController.ts</code></h2><p><code>navController</code> can be simply converted like so:</p><pre><code class="language-ts">&#x27;use strict&#x27;;

interface INavControllerScope extends ng.IScope {
  isCollapsed: boolean;
  siteSection: string;
}

angular.module(&#x27;poorClaresApp.controllers&#x27;).controller(
  &#x27;NavController&#x27;,

  [
    &#x27;$scope&#x27;,
    &#x27;siteSectionService&#x27;,
    function (
      $scope: INavControllerScope,
      siteSectionService: ISiteSectionService
    ) {
      $scope.isCollapsed = true;
      $scope.siteSection = siteSectionService.getSiteSection();

      $scope.$watch(
        siteSectionService.getSiteSection,
        function (newValue, oldValue) {
          $scope.siteSection = newValue;
        }
      );
    },
  ]
);
</code></pre><p>I&#x27;d draw your attention to the creation of a the <code>INavControllerScope</code> interface that extends the default Angular $scope of <code>ng.IScope</code> with 2 extra properties.</p><p>Let&#x27;s also switch this over to the class based approach (there is less of a reason to on this occasion just looking at the size of the codebase but I&#x27;m all about consistency of approach):</p><pre><code class="language-ts">&#x27;use strict&#x27;;

module poorClaresApp.controllers {
  interface INavControllerScope extends ng.IScope {
    isCollapsed: boolean;
    siteSection: string;
  }

  class NavController {
    static $inject = [&#x27;$scope&#x27;, &#x27;siteSectionService&#x27;];
    constructor(
      private $scope: INavControllerScope,
      private siteSectionService: ISiteSectionService
    ) {
      $scope.isCollapsed = true;
      $scope.siteSection = siteSectionService.getSiteSection();

      $scope.$watch(
        siteSectionService.getSiteSection,
        function (newValue, oldValue) {
          $scope.siteSection = newValue;
        }
      );
    }
  }

  angular
    .module(&#x27;poorClaresApp.controllers&#x27;)
    .controller(&#x27;NavController&#x27;, NavController);
}
</code></pre><h2>TypeScriptify <code>mainController.ts</code></h2><p>Finally, <code>mainController</code> can be converted as follows:</p><pre><code class="language-ts">&#x27;use strict&#x27;;

angular.module(&#x27;poorClaresApp.controllers&#x27;).controller(
  &#x27;MainController&#x27;,

  [
    &#x27;$location&#x27;,
    &#x27;siteSectionService&#x27;,
    function (
      $location: ng.ILocationService,
      siteSectionService: ISiteSectionService
    ) {
      siteSectionService.determineSiteSection($location.path());
    },
  ]
);
</code></pre><p>Again it&#x27;s just a case of assigning the undeclared types. For completeness lets also switch this over to the class based approach:</p><pre><code class="language-ts">&#x27;use strict&#x27;;

module poorClaresApp.controllers {
  class MainController {
    static $inject = [&#x27;$location&#x27;, &#x27;siteSectionService&#x27;];
    constructor(
      private $location: ng.ILocationService,
      private siteSectionService: ISiteSectionService
    ) {
      siteSectionService.determineSiteSection($location.path());
    }
  }

  angular
    .module(&#x27;poorClaresApp.controllers&#x27;)
    .controller(&#x27;MainController&#x27;, MainController);
}
</code></pre><h2>Did it work? Drum roll...</h2><p>In unit tests we trust. Let&#x27;s run them...</p><p><img src="https://2.bp.blogspot.com/-re8aAJVtSDk/U4hYNPqKk9I/AAAAAAAAAnA/1Vu7ooQk1jw/s1600/UnitTestsPass.png"/></p><p>Success! I hope you found this useful.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Team Foundation Server, Continuous Integration and separate projects for JavaScript unit tests]]></title>
            <link>https://blog.johnnyreilly.com/2014/05/15/team-foundation-server-continuous-integration-and-javascript-unit-tests-in-unit-test-project</link>
            <guid>Team Foundation Server, Continuous Integration and separate projects for JavaScript unit tests</guid>
            <pubDate>Thu, 15 May 2014 00:00:00 GMT</pubDate>
            <description><![CDATA[Do you like to separate out your unit tests from the project you are testing? I imagine so. My own practice when creating a new project in Visual Studio is to create a separate unit test project alongside whose responsibility is to house unit tests for that new project.]]></description>
            <content:encoded><![CDATA[<p>Do you like to separate out your unit tests from the project you are testing? I imagine so. My own practice when creating a new project in Visual Studio is to create a separate unit test project alongside whose responsibility is to house unit tests for that new project.</p><p>When I check in code for that project I expect the continuous integration build to kick off and, as part of that, the unit tests to be run. When it comes to running .NET tests then Team Foundation Server (and it&#x27;s cloud counterpart Visual Studio Online) has your back. When it comes to running JavaScript tests then... not so much.</p><p>This post will set out:</p><ol><li>How to get JavaScript tests to run on TFS / VSO in a continuous integration scenario.</li><li>How to achieve this <!-- -->*<strong>without</strong>*<!-- --> having to include your tests as part of web project.</li></ol><p>To do this I will lean heavily (that&#x27;s fancy language for &quot;rip off entirely&quot;) on an <a href="https://blogs.msdn.com/b/visualstudioalm/archive/2012/07/09/javascript-unit-tests-on-team-foundation-service-with-chutzpah.aspx">excellent blog post by Mathew Aniyan</a> which covers point #1. My contribution is point #2.</p><h2>Points #1 and #2 in short order</h2><p>First of all, install Chutzpah on TFS / VSO. You can do this by following <a href="https://blogs.msdn.com/b/visualstudioalm/archive/2012/07/09/javascript-unit-tests-on-team-foundation-service-with-chutzpah.aspx">Steps 1 - 6 from Mathew Aniyan&#x27;s post</a>. Instead of following steps 7 and 8 create a new unit test project in your solution.</p><aside>This unit test project will effectively be a C# project that hosts no real C# code at all. Instead we&#x27;re going to use it to house JavaScript tests. If there is another way to have a separate project which TFS / VSO can pick up on and run tests in then please let me know. As far as I&#x27;m aware though, this is the only game in town.</aside><p><strong>Edit 29/05/2014:</strong> Matthew Manela (creator of Chutzpah) has confirmed that this is the correct approach - thanks chap!</p><blockquote><p><a href="https://twitter.com/johnny_reilly">@johnny_reilly</a> Nope that is pretty much what you need to do.</p><p>— Matthew Manela (@mmanela) <a href="https://twitter.com/mmanela/statuses/466962743400996864">May 15, 2014</a></p></blockquote><script src="//platform.twitter.com/widgets.js" charSet="utf-8"></script><p>To our unit test project add your JavaScript unit tests. These should be marked in Visual Studio with a Build Action of &quot;Content&quot; and a Copy to Output Directory of &quot;Do not copy&quot;. You should be able to run these tests locally using the Visual Studio Chutzpah extension - or indeed in some other JavaScript test runner. Then, and this is the important part, edit the csproj file of your unit test project and add this <code>Import Project</code> statement:</p><pre><code class="language-xml">&lt;Import Project=&quot;$(VSToolsPath)\WebApplications\Microsoft.WebApplication.targets&quot; Condition=&quot;&#x27;$(VSToolsPath)&#x27; != &#x27;&#x27;&quot; /&gt;
</code></pre><p>Ordering is important in this case. It matters that this new statement sits after the other <code>Import Project</code> statements. So you should end up with a csproj file that looks in part like this: (comments added by me for clarity)</p><pre><code class="language-xml">&lt;!-- Pre-existing Import Project statements start --&gt;
  &lt;Import Project=&quot;$(VSToolsPath)\TeamTest\Microsoft.TestTools.targets&quot; Condition=&quot;Exists(&#x27;$(VSToolsPath)\TeamTest\Microsoft.TestTools.targets&#x27;)&quot; /&gt;
  &lt;Import Project=&quot;$(MSBuildToolsPath)\Microsoft.CSharp.targets&quot; /&gt;
  &lt;!-- Pre-existing Import Project statements end --&gt;

  &lt;!-- New addition start --&gt;
  &lt;Import Project=&quot;$(VSToolsPath)\WebApplications\Microsoft.WebApplication.targets&quot; Condition=&quot;&#x27;$(VSToolsPath)&#x27; != &#x27;&#x27;&quot; /&gt;
  &lt;!-- New addition end --&gt;
</code></pre><p>Check in your amended csproj and the unit tests to TFS / VSO. You should see the JavaScript unit tests being run as part of the build.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[TypeScript, JSDoc and Intellisense]]></title>
            <link>https://blog.johnnyreilly.com/2014/05/05/typescript-jsdoc-and-intellisense</link>
            <guid>TypeScript, JSDoc and Intellisense</guid>
            <pubDate>Mon, 05 May 2014 00:00:00 GMT</pubDate>
            <description><![CDATA[Days of Yore]]></description>
            <content:encoded><![CDATA[<h2>Days of Yore</h2><p>It was my first job. The web was alive and well at this point but still very much in it&#x27;s infancy. Newspapers had only recently moved on from calling it &quot;the information superhighway&quot;. No-one was doing <em>real</em> programming for the web - the desktop was where it was at.</p><p>As for me, I was writing call centre software. It was all very exciting. Here was the idea: the phone on your desk would start ringing and through the magic of <a href="http://en.wikipedia.org/wiki/Telephony_Application_Programming_Interface">TAPI</a> our app would be presented with the telephone number of the dialer. It would then look up that telephone number in the appropriate CRM application and pop the callers details on the screen. You&#x27;d pick up the phone and bellow &quot;why hello Mr Jones!&quot; and either impress the caller with your incredible fore-knowledge of who had rung you or perhaps terrify them with our <a href="http://en.wikipedia.org/wiki/Nineteen_Eighty-Four">Brave New Orwellian World</a>.</p><p>My job was to work out how to call into the APIs of the various CRM applications / databases being used and extract the relevant information. So it goes without saying that I have spent a lot of time with badly documented APIs. Or in fact <em>undocumented</em> APIs. I know pain my friend...</p><p>Hours and days were spent debugging and walking APIs just to find out what they could do and what information they exposed. This, I need hardly say, was dull and tedious work. Having spent longer than I care to remember with no more information on an API than method names has left its mark on me. I am consequently keener than your average dev on documentation and intellisense. When you&#x27;ve stared at the coalface of the <a href="http://en.wikipedia.org/wiki/IBM_Notes">Lotus Notes</a> API for 2 weeks with only Dephi 3 as your constant companion you&#x27;d feel the same way too. (This was <a href="http://en.wikipedia.org/wiki/AltaVista">before the days of Google</a> and actually being able to find stuff on the internet.)</p><p>If you can convey information about the API that you&#x27;re building then I&#x27;d say you&#x27;re duty-bound to do so. Or at least that it&#x27;s good manners.</p><h2>Definitely Intellisensed</h2><p>When I started getting involved with the <a href="https://github.com/DefinitelyTyped">Definitely Typed project</a> my focus was on giving good Intellisense. Where there was documentation for an API I wanted to get that popping in front of users when they hit the &quot;.&quot; key:</p><p><img src="https://blogs.msdn.com/cfs-filesystemfile.ashx/__key/communityserver-blogs-components-weblogfiles/00-00-01-56-67/1200.JSDoc_5F00_in_5F00_VS.png"/></p><p>As the above screenshot demonstrates <a href="https://blogs.msdn.com/b/typescript/archive/2013/01/21/announcing-typescript-0-8-2.aspx">TypeScript supports Intellisense</a> through a slightly tweaked implementation of <a href="http://en.wikipedia.org/wiki/JSDoc">JSDoc</a>:</p><blockquote><p>With 0.8.2, the TypeScript compiler and tools now support JSDoc comments.</p><p>In the TypeScript implementation, because types are already part of the system, we allow the JSDoc type annotation to be elided, as in the example above.</p><p>You can now document a variety of language constructs (including classes, modules, interfaces, and functions) with comments that become part of the information displayed to the user. We’ve also started extending lib.d.ts, the default JS and DOM API library, with JSDoc comments.</p></blockquote><p>Partly as an exercise in getting better acquainted with TypeScript and partly responding to my instinctive need to have nicely documented APIs I decided to start adding JSDoc comments to the world&#x27;s most popular typings file <code>&lt;a href=&quot;https://github.com/borisyankov/DefinitelyTyped/blob/master/jquery/jquery.d.ts&quot;&gt;jquery.d.ts&lt;/a&gt;</code>.</p><div class="digression getOutOfMySight"><h4>Why <code>jquery.d.ts</code>?</h4><p>Well a number of reasons:</p><ol><li>I used <code>jquery.d.ts</code> already myself and I&#x27;m a firm believer in <a href="http://en.wikipedia.org/wiki/Eating_your_own_dog_food">eating your own dogfood</a></li><li>jQuery is well documented. I needed a source of information to power my JSDoc and <a href="//api.jquery.com">api.jquery.com</a> had my back.</li><li><code>jquery.d.ts</code> was widely used. Given how ubiquitous jQuery has become this typing file was unsurprisingly the most popular in the world. That was key for me as I wanted feedback - if I was making a mess of the typings I wanted someone to pitch in and tell me.</li></ol><p>Just to digress once more, points #2 and #3 turned out to be of particular note.</p><p>Concerning point #2, I did find the occasional <a href="https://github.com/borisyankov/DefinitelyTyped/pull/1471#issuecomment-31204115">error</a> or <a href="https://github.com/borisyankov/DefinitelyTyped/pull/1835#issuecomment-37533088">inconsistency</a> in the jQuery API documentation. These were definitely the exception rather than the rule though. And thanks to the very helpful <a href="https://github.com/dmethvin">Dave Methvin</a> these actually lead to <a href="https://github.com/jquery/api.jquery.com/pull/460">minor improvements to the jQuery API documentation</a>.</p><blockquote class="twitter-tweet" lang="en"><p><a href="https://twitter.com/search?q=%23TypeScript&amp;amp;src=hash">#TypeScript</a> definitions pointing out errors in JavaScript docs of a project <a href="https://twitter.com/search?q=%23Jquery&amp;amp;src=hash">#Jquery</a> : <a href="https://t.co/v6rzCdBwmi">https://t.co/v6rzCdBwmi</a> caught by <a href="https://twitter.com/johnny_reilly">@johnny_reilly</a></p>— basarat (@basarat) <a href="https://twitter.com/basarat/statuses/416309213430689792">December 26, 2013</a></blockquote><script src="//platform.twitter.com/widgets.js" charSet="utf-8"></script><p>Concerning point #3 I did indeed get feedback. As well as enriching <code>jquery.d.ts</code> with JSDoc goodness I also found myself fixing slight errors in the typings. Here and there I would find examples where <code>jquery.d.ts</code> was out of line the with API documentation. Where this was the case I would amend the typings to bring them into line - trying to make <code>jquery.d.ts</code> entirely API-compliant. This was <a href="https://github.com/borisyankov/DefinitelyTyped/issues/1499">not always popular</a>. But despite the heat it generated I think it ended up leading to a better typing file. I&#x27;m again grateful for Dave Methvin&#x27;s thoughtful contributions.</p></div><h2>Turning API documentation into JSDoc</h2><p>I wanted to take an example of API documentation and demonstrate how that can be applied to a typing file with particular focus on how JSDoc comments can be created to drive Intellisense. So let&#x27;s take everyone&#x27;s favourite jQuery method: <code>val</code>. The documentation of <code>val</code> can be found here: <a href="http://api.jquery.com/val">api.jquery.com/val</a></p><p>By the way, check out the <!-- -->*<em>entirely</em>*<!-- --> intuitive URL. Now you&#x27;ve clocked just how straightforward that is you&#x27;ve probably a fair idea how you could find pretty much any jQuery documentation you might need without recourse to Google. Brilliant!</p><p>Let&#x27;s take a look at what <code>val</code> looked like <a href="https://github.com/borisyankov/DefinitelyTyped/blob/c98eebb13724b5156f12318b68fc2d875ca6e4a3/jquery/jquery.d.ts#L364-L368">before JSDoc</a> in the first version of the typing available on GitHub. (By the way, remember the original <code>jquery.d.ts</code><a href="https://typescript.codeplex.com/sourcecontrol/latest#samples/jquery/jquery.d.ts"> came out of the TypeScript team</a>):</p><pre><code class="language-ts">val(): any;
    val(value: string[]): JQuery;
    val(value: string): JQuery;
    val(value: number): JQuery;
    val(func: (index: any, value: any) =&gt; any): JQuery;
</code></pre><p>And now let&#x27;s look at <code>jquery.d.ts</code><a href="https://github.com/borisyankov/DefinitelyTyped/blob/c259dba094121a389b41c573d5000dda7bdf2092/jquery/jquery.d.ts#L1494-L1545">after JSDoc</a>:</p><pre><code class="language-ts">/**
     * Get the current value of the first element in the set of matched elements.
     */
    val(): any;
    /**
     * Set the value of each element in the set of matched elements.
     *
     * @param value A string of text or an array of strings corresponding to the value of each matched element to set as selected/checked.
     */
    val(value: string): JQuery;
    /**
     * Set the value of each element in the set of matched elements.
     *
     * @param value A string of text or an array of strings corresponding to the value of each matched element to set as selected/checked.
     */
    val(value: string[]): JQuery;
    /**
     * Set the value of each element in the set of matched elements.
     *
     * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.
     */
    val(func: (index: number, value: string) =&gt; string): JQuery;
    /**
     * Set the value of each element in the set of matched elements.
     *
     * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.
     */
    val(func: (index: number, value: string[]) =&gt; string): JQuery;
    /**
     * Set the value of each element in the set of matched elements.
     *
     * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.
     */
    val(func: (index: number, value: number) =&gt; string): JQuery;
    /**
     * Set the value of each element in the set of matched elements.
     *
     * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.
     */
    val(func: (index: number, value: string) =&gt; string[]): JQuery;
    /**
     * Set the value of each element in the set of matched elements.
     *
     * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.
     */
    val(func: (index: number, value: string[]) =&gt; string[]): JQuery;
    /**
     * Set the value of each element in the set of matched elements.
     *
     * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.
     */
    val(func: (index: number, value: number) =&gt; string[]): JQuery;
</code></pre><p>Many changes yes? Let&#x27;s break it down a little.</p><h2>1<!-- -->.<!-- --> You have 20 seconds to comply (with the API)</h2><p>The first thing to note is the <code>number</code> setter method:</p><pre><code class="language-ts">val(value: number): JQuery;
</code></pre><p>Let&#x27;s have a look at the jQuery documentation for the simple setter:</p><blockquote><h2><a href="http://api.jquery.com/val/#val-value">.val( value )</a></h2><div><strong>value</strong></div><div>Type: <a href="http://api.jquery.com/Types/#String">String</a> or <a href="http://api.jquery.com/Types/#Array">Array</a></div><div>A string of text or an array of strings corresponding to the value of each matched element to set as selected/checked.</div></blockquote><p>See the problem? There is <!-- -->*<em>no</em>*<!-- --> <code>number</code> setter. The typings are wrong. So let&#x27;s remedy this:</p><pre><code class="language-ts">&lt;strike&gt;val(value: number): JQuery;&lt;/strike&gt;
</code></pre><h2>2<!-- -->.<!-- --> <code>String</code> and <code>Array of String</code> setters</h2><p>The documentation states that we have setters which accept <code>String</code> and <code>Array of String</code>. These are already modeled in the existing typings by the <code>string</code> and <code>string[]</code> overloads:</p><pre><code class="language-ts">val(value: string[]): JQuery;
    val(value: string): JQuery;
</code></pre><p>So let&#x27;s enrich these typings with some JSDoc:</p><pre><code class="language-ts">/**
     * Set the value of each element in the set of matched elements.
     *
     * @param value A string of text or an array of strings corresponding to the value of each matched element to set as selected/checked.
     */
    val(value: string): JQuery;
    /**
     * Set the value of each element in the set of matched elements.
     *
     * @param value A string of text or an array of strings corresponding to the value of each matched element to set as selected/checked.
     */
    val(value: string[]): JQuery;
</code></pre><p>If you look you can see we&#x27;ve added a related JSDoc style comment block prior to each overload. The first part of the comment (<em>&quot;Set the value of...&quot;</em>) is the overarching Intellisense that is displayed. Each of the <code>@param</code> statements represents each of the parameters and it&#x27;s associated comment. By comparing the <a href="http://api.jquery.com/val/#val-value">API documentation</a> to the JSDoc it&#x27;s pretty clear how the API has been transformed into useful JSDoc.</p><p><img src="https://2.bp.blogspot.com/-ljw2HiAp0qE/U2D915IIcaI/AAAAAAAAAkk/DVPv-TolEJw/s640/Intellisense-Setter-String.png"/></p><p>It&#x27;s worth noting that I could have taken the choice to customise the <code>@param value</code> comments based on the overload I was JSDoc-ing. Arguably it would have been more useful to have something like this instead:</p><pre><code class="language-ts">/**
     * Set the value of each element in the set of matched elements.
     *
     * @param value A string of text &lt;strike&gt;or an array of strings&lt;/strike&gt; corresponding to the value of each matched element to set as selected/checked.
     */
    val(value: string): JQuery;
    /**
     * Set the value of each element in the set of matched elements.
     *
     * @param value &lt;strike&gt;A string of text or&lt;/strike&gt; an array of strings corresponding to the value of each matched element to set as selected/checked.
     */
    val(value: string[]): JQuery;
</code></pre><p>After some pondering I decided not to take this approach, just to maintain that close relationship between <code>jquery.d.ts</code> and <a href="http://api.jquery.com/">api.jquery.com</a>. It&#x27;s open to debate how useful that relationship actually is so I thought I&#x27;d just highlight this as a choice I made.</p><h2>3<!-- -->.<!-- --> Getter</h2><p>The jQuery documentation for the getter looks like this:</p><blockquote><h2>[<span class="icon-link"></span></h2><span class="name">.val()</span><p>](<a href="http://api.jquery.com/val/#val">http://api.jquery.com/val/#val</a>)<span class="returns">Returns: <a href="http://api.jquery.com/Types/#String">String</a> or <a href="http://api.jquery.com/Types/#Number">Number</a> or <a href="http://api.jquery.com/Types/#Array">Array</a></span></p><p><strong>Description: </strong>Get the current value of the first element in the set of matched elements.</p></blockquote><p>So the <code>val()</code> overload can return a <code>string</code>, a <code>number</code> or a <code>string[]</code>. Unfortunately there is no real way to model that in TypeScript at present due to the absence of <a href="https://typescript.codeplex.com/workitem/1364">&quot;union types&quot;</a>. Union types are being <a href="https://typescript.codeplex.com/discussions/543598#PostDetailsCell_1239340">discussed at present</a> but in TypeScript v1.0 world the only viable approach is returning the <code>any</code> type. This implies <code>val()</code> returns any possible JavaScript value from <code>boolean</code> to <code>Function</code> and straight on &#x27;til morning. So clearly this isn&#x27;t accurate but importantly it also allows for the possibility of <code>val()</code> returning <code>string</code>, <code>number</code> or <code>string[]</code>.</p><p>The final getter typing with JSDoc applied ends up looking like this:</p><pre><code class="language-ts">/**
     * Get the current value of the first element in the set of matched elements.
     */
    val(): any;
</code></pre><p>As you can see the <em>&quot;Get the current value...&quot;</em> from the API docs has been used as the overarching Intellisense that is displayed for the getter.</p><p><img src="https://4.bp.blogspot.com/-7PG3jVXPWdM/U2D-BvxwYYI/AAAAAAAAAks/_ZTRQWBt7L8/s640/Intellisense-Getter.png"/></p><h2>4<!-- -->.<!-- --> The <code>Function</code> setter</h2><p>Finally we&#x27;re going to take a look at the <code>Function</code> setter which is documented as follows:</p><blockquote><h2>[<span class="icon-link"></span></h2><p>.val( function(index, value) )](<a href="http://api.jquery.com/val/#val-functionindex--value">http://api.jquery.com/val/#val-functionindex--value</a>)</p><div><strong>function(index, value)</strong></div><div>Type: <a href="http://api.jquery.com/Types/#Function">Function</a>()</div><div>A function returning the value to set. <code>this</code> is the current element. Receives the index position of the element in the set and the old value as arguments.</div></blockquote><p>If you cast your eyes back to the original typings for the <code>Function</code> setter you&#x27;ll see they look like this:</p><pre><code class="language-ts">val(func: (index: any, value: any) =&gt; any): JQuery;
</code></pre><p>This is a good start but it&#x27;s less accurate than it could be in a number of ways:</p><ol><li><code>index</code> is a <code>number</code> <!-- -->-<!-- --> we needn&#x27;t keep it as an <code>any</code></li><li><code>value</code> is the old value - we know from our getter that this can be a <code>string</code>, <code>number</code> or <code>string[]</code>. So we can lose the <code>any</code> in favour of overloads which specify different types for <code>value</code> in each.</li><li>The return value of the function is the value that should be set. We know from our other setters that the possible types allowed here are <code>string</code> and <code>string[]</code>. (And yes I&#x27;m as puzzled as you are that the getter can return a <code>number</code> but the setter can&#x27;t set one.) That being the case it makes sense for us to have overloads with functions that return both <code>string</code> and <code>string[]</code></li></ol><p>So, we&#x27;ve got a little tidy up to do for #1 and extra overloads to add for #2 and #3. We&#x27;re going to replace the single <code>Function</code> setter with 3 overloads to cater for #2. Then for #3 we&#x27;re going to take each of the 3 overloads we&#x27;ve just created and make 2 overloads place of each to handle the different return types. This will lead us with the grand total of 6 overloads to model our <code>Function</code> setter!</p><pre><code class="language-ts">/**
     * Set the value of each element in the set of matched elements.
     *
     * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.
     */
    val(func: (index: number, value: string) =&gt; string): JQuery;
    /**
     * Set the value of each element in the set of matched elements.
     *
     * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.
     */
    val(func: (index: number, value: string[]) =&gt; string): JQuery;
    /**
     * Set the value of each element in the set of matched elements.
     *
     * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.
     */
    val(func: (index: number, value: number) =&gt; string): JQuery;
    /**
     * Set the value of each element in the set of matched elements.
     *
     * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.
     */
    val(func: (index: number, value: string) =&gt; string[]): JQuery;
    /**
     * Set the value of each element in the set of matched elements.
     *
     * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.
     */
    val(func: (index: number, value: string[]) =&gt; string[]): JQuery;
    /**
     * Set the value of each element in the set of matched elements.
     *
     * @param func A function returning the value to set. this is the current element. Receives the index position of the element in the set and the old value as arguments.
     */
    val(func: (index: number, value: number) =&gt; string[]): JQuery;
</code></pre><p>A cursory glance shows that each of the overloads above shares the same JSDoc. Each has the <em>&quot;Set the value...&quot;</em> from the API docs as the overarching Intellisense that is displayed for the <code>Function</code> setter. And each has the same <code>@param func</code> comment as well.</p><p><img src="https://1.bp.blogspot.com/-9wwPOZIiwcs/U2D-Ngw9CrI/AAAAAAAAAk0/V32FCsotPTQ/s640/Intellisense-Setter-Function.png"/></p><h2>It could be you...</h2><p>This post is much longer than I ever intended it to be. But I wanted to show how easy it is to create typings with JSDoc to drive Intellisense. For no obvious reason people generally don&#x27;t make a great deal of use of JSDoc when creating typings. Perhaps the creators have no good source of documentation (a common problem). Or perhaps people are not even aware it&#x27;s a possibility - they don&#x27;t know about the TypeScript support of JSDoc. In case it&#x27;s the latter I think this post was worth writing.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[TypeScript this is what I want! (the unfortunate neglect of Instance Methods / callback functions)]]></title>
            <link>https://blog.johnnyreilly.com/2014/04/01/typescript-instance-methods</link>
            <guid>TypeScript this is what I want! (the unfortunate neglect of Instance Methods / callback functions)</guid>
            <pubDate>Tue, 01 Apr 2014 00:00:00 GMT</pubDate>
            <description><![CDATA[I was recently reading Jeff Walker's blog post "Why TypeScript Isn't the Answer". This is part of series in which Jeff goes through various compile-to-JavaScript technologies including TypeScript, CoffeeScript and Dart and explains his view of why he feels they don't quite hit the mark.]]></description>
            <content:encoded><![CDATA[<p>I was recently reading <a href="http://www.walkercoderanger.com/blog/2014/02/typescript-isnt-the-answer/">Jeff Walker&#x27;s blog post &quot;Why TypeScript Isn&#x27;t the Answer&quot;</a>. This is part of series in which Jeff goes through various compile-to-JavaScript technologies including TypeScript, CoffeeScript and Dart and explains his view of why he feels they don&#x27;t quite hit the mark.</p><p>As a user (and big fan) of TypeScript I read the post with interest and picked up on one particular issue that Jeff mentions:</p><blockquote><p>Classes make the unchanged behaviour of the <code>this</code> keyword more confusing. For example, in a class like <code>Greeter</code> from the <a href="http://www.typescriptlang.org/Playground">TypeScript playground</a>, the use of <code>this</code> is confusing:</p><pre><code class="language-ts">class Greeter {
  greeting: string;
  constructor(message: string) {
    this.greeting = message;
  }
  greet() {
    return &#x27;Hello, &#x27; + this.greeting;
  }
}
</code></pre><p>One can’t help but feel the <code>this</code> keyword in the methods of <code>Greeter</code> should always reference a <code>Greeter</code> instance. However, the semantics of this are unchanged from JavaScript:</p><pre><code class="language-js">var greeter = new Greeter(&#x27;world&#x27;);
var unbound = greeter.greet;
alert(unbound());
</code></pre><p>The above code displays “Hello, undefined” instead of the naively expected “Hello, world”.</p></blockquote><p>Now Jeff is quite correct in everything he says above. However, he&#x27;s also missing a trick. Or rather, he&#x27;s missing out on a very useful feature of TypeScript.</p><h2>Instance Methods to the Rescue!</h2><p>Still in the early days of TypeScript, the issue Jeff raises had already been identified. (And for what it&#x27;s worth, this issue wasn&#x27;t there by mistake - remember TypeScript is quite deliberately a &quot;superset of JavaScript&quot;.) Happily with the <a href="https://blogs.msdn.com/b/typescript/archive/2013/08/06/announcing-0-9-1.aspx">release of TypeScript 0.9.1</a> a nice remedy was included in the language in the form of &quot;Instance Methods&quot;.</p><p>Instance Methods are lexically scoped; bound to a specific instance of a JavaScript object. i.e. These methods are <!-- -->*<strong>not</strong>*<!-- --> vulnerable to the “Hello, undefined” issue Jeff raises. To quote the blog post:</p><blockquote><p>We&#x27;ve relaxed the restrictions on field initializers to now allow <code>&#x27;this&#x27;</code>. This means that classes can now contain both methods on the prototype, and <strong>callback functions on the instance</strong>. The latter are particularly useful when you want to use a member on the class as a callback function, as in the code above. This lets you mix-n-match between ‘closure’ style and ‘prototype’ style class member patterns easily.</p></blockquote><h2><code>Greeter</code> with Instance Methods</h2><p>So, if we take the <code>Greeter</code> example, how do we apply Instance Methods to it? Well, like this:</p><pre><code class="language-ts">class Greeter {
  greeting: string;
  constructor(message: string) {
    this.greeting = message;
  }
  greet = () =&gt; {
    return &#x27;Hello, &#x27; + this.greeting;
  };
}
</code></pre><p>Can you tell the difference? It&#x27;s subtle. That&#x27;s right; the mere swapping out of <code>()</code> with <code>= () =&amp;gt;</code> on the <code>greet</code> method takes us from a <code>prototype</code> method to an Instance Method.</p><p>Observant readers will have noticed that we are using TypeScript / <a href="https://developer.mozilla.org/en/docs/Web/JavaScript/Reference/arrow_functions">ES6&#x27;s Arrow Function syntax</a>. In fact with that in mind I could actually have gone super-terse if I was so inclined:</p><pre><code class="language-ts">class Greeter {
  greeting: string;
  constructor(message: string) {
    this.greeting = message;
  }
  greet = () =&gt; &#x27;Hello, &#x27; + this.greeting;
}
</code></pre><p>But either way, both of the above class declarations compile down to the following JavaScript:</p><pre><code class="language-js">var Greeter = (function () {
  function Greeter(message) {
    var _this = this;
    this.greet = function () {
      return &#x27;Hello, &#x27; + _this.greeting;
    };
    this.greeting = message;
  }
  return Greeter;
})();
</code></pre><p>Which differs from the pre-Instance Methods generated JavaScript:</p><pre><code class="language-js">var Greeter = (function () {
  function Greeter(message) {
    this.greeting = message;
  }
  Greeter.prototype.greet = function () {
    return &#x27;Hello, &#x27; + this.greeting;
  };
  return Greeter;
})();
</code></pre><p>As you can see the Instance Methods approach does <!-- -->*<strong>not</strong>*<!-- --> make use of the <code>prototype</code> on <code>Greeter</code> to add the method. (As the pre-Instance Methods <code>greet()</code> declaration did.) Instead it creates a function directly on the created object and internally uses the <code>_this</code> variable inside the Instance Methods. (<code>_this</code> being a previously captured instance of <code>this</code>.)</p><p>So with Instance Methods we can repeat Jeff&#x27;s experiment from earlier:</p><pre><code class="language-js">var greeter = new Greeter(&#x27;world&#x27;);
var bound = greeter.greet;
alert(bound());
</code></pre><p>But this time round the code displays “Hello, world” and no longer “Hello, undefined”.</p><h2>Update 02/04/2014 - mixing and matching <code>prototype</code> and Instance Methods</h2><p><a href="https://twitter.com/bgever">Bart Verkoeijen</a> made an excellent comment concerning the extra memory that Instance Methods require as opposed to <code>prototype</code> methods. Not everyone reads the comments and so I thought I&#x27;d add a little suffix to my post.</p><p>What I’ve come to realise is that it comes down to problem that you’re trying to solve. Instance methods are bulletproof in terms of relying on a specific instance of <code>this</code> regardless of how a method is invoked. But for many of my use cases that’s overkill. Let’s take the original (<code>prototype</code> methods) <code>Greeter</code> example:</p><pre><code class="language-js">var Greeter = (function () {
  function Greeter(message) {
    this.greeting = message;
  }
  Greeter.prototype.greet = function () {
    return &#x27;Hello, &#x27; + this.greeting;
  };
  return Greeter;
})();

var greeter = new Greeter(&#x27;world&#x27;);
var greeter2 = new Greeter(&#x27;universe&#x27;);

console.log(greeter.greet()); // Logs &quot;Hello, world&quot;
console.log(greeter2.greet()); // Logs &quot;Hello, universe&quot;
</code></pre><p>As you can see above, provided I invoke my <code>greet</code> method in the context of my created object then I can rely on <code>this</code> being what I would hope.</p><p>That being the case my general practice has not been to use exclusively Instance methods <!-- -->*<strong>or</strong>*<!-- --> <code>prototype</code> methods. What I tend to do is start out only with <code>prototype</code> methods on my classes and switch them over to be an Instance method if there is an actual need to ensure context. So my TypeScript classes tend to be a combination of <code>prototype</code> methods and Instance methods.</p><p>More often than not the <code>prototype</code> methods are just fine. It tends to be where an object is interacting with some kind of presentation framework (Knockout / Angular etc) or being invoked as part of a callback (eg AJAX scenarios) where I need Instance methods.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Surprisingly Happy Tale of Visual Studio Online, Continous Integration and Chutzpah]]></title>
            <link>https://blog.johnnyreilly.com/2014/03/17/the-surprisingly-happy-tale-of-visual</link>
            <guid>The Surprisingly Happy Tale of Visual Studio Online, Continous Integration and Chutzpah</guid>
            <pubDate>Mon, 17 Mar 2014 00:00:00 GMT</pubDate>
            <description><![CDATA[Going off piste]]></description>
            <content:encoded><![CDATA[<h2>Going off piste</h2><p>The post that follows is a slightly rambly affair which is pretty much my journal of the first steps of getting up and running with JavaScript unit testing. I will not claim that much of this blog is down to me. In fact in large part is me working my way through <a href="https://blogs.msdn.com/b/visualstudioalm/archive/2012/07/09/javascript-unit-tests-on-team-foundation-service-with-chutzpah.aspx">Mathew Aniyan&#x27;s excellent blog post on integrating Chutzpah with TFS</a>. But a few deviations from this post have made me think it worth keeping hold of this record for my benefit (if no-one else&#x27;s).</p><p>That&#x27;s the disclaimers out of the way now...</p><h2>...Try, try, try again...</h2><p>Getting started with JavaScript unit testing has not been the breeze I’d expected. Frankly I’ve found the docs out there not particularly helpful. But if at first you don&#x27;t succeed then try, try, try again.</p><p>So after a number of failed attempts I’m going to give it another go. <a href="http://www.hanselminutes.com/412/getting-started-with-javascript-unit-testing-with-jasmine-and-rushaine-mcbean">Rushaine McBean</a> says Jasmine is easiest so I&#x27;m going to follow her lead and give it a go.</p><p>Let’s new up a new (empty) ASP.NET project. Yes, I know ASP.NET has nothing to do with JavaScript unit testing but my end goal is to be able to run JS unit tests in Visual Studio and as part of Continuous Integration. Further to that, I&#x27;m anticipating a future where I have a solution that contains JavaScript unit tests and C# unit tests as well. It is indeed a bold and visionary Brave New World. We&#x27;ll see how far we get.</p><p>First up, download Jasmine from <a href="http://jasmine.github.io/">GitHub</a> <!-- -->-<!-- --> I&#x27;ll use <a href="https://github.com/pivotal/jasmine/blob/master/dist/jasmine-standalone-2.0.0.zip">v2.0</a>. Unzip it and fire up SpecRunner.html and whaddya know... It works!</p><p><img src="https://4.bp.blogspot.com/-M-Qct1e8Ofo/UxiT5wHICLI/AAAAAAAAAiY/tHUQemETCGI/s320/LookItWorksRightOutTheBox.png"/></p><p>As well it might. I’d be worried if it didn’t. So I’ll move the contents of the release package into my empty project. Now let’s see if we can get those tests running inside Visual Studio. I’d heard of <a href="https://chutzpah.codeplex.com/">Chutzpah</a> which describes itself thusly:</p><blockquote><p><em>“Chutzpah is an open source JavaScript test runner which enables you to run unit tests using QUnit, Jasmine, Mocha, CoffeeScript and TypeScript.” </em></p></blockquote><p>What I’m after is the Chutzpah test adapter for Visual Studio 2012/2013 which can be found <a href="http://visualstudiogallery.msdn.microsoft.com/f8741f04-bae4-4900-81c7-7c9bfb9ed1fe">here</a>. I download the VSIX and install. Pretty painless. Once I restart Visual Studio I can see my unit tests in the test explorer. Nice! Run them and...</p><p><img src="https://2.bp.blogspot.com/-Ns9-ZoCzyxU/UxiVe83GQAI/AAAAAAAAAik/9rJiv7c3gOA/s320/EverythingFails.png"/></p><p>All fail. This makes me sad. All the errors say “Can’t find variable: Player in file”. Hmmm. Why? Dammit I’m actually going to have to read the <a href="https://chutzpah.codeplex.com/wikipage?title=Chutzpah%20File%20References&amp;referringTitle=Documentation">documentation</a>... It turns out the issue can be happily resolved by adding these 3 references to the top of PlayerSpec.js:</p><pre><code class="language-js">/// &lt;reference path=&quot;../src/Player.js&quot; /&gt;
/// &lt;reference path=&quot;../src/Song.js&quot; /&gt;
/// &lt;reference path=&quot;SpecHelper.js&quot; /&gt;
</code></pre><p>Now the tests pass:</p><p><img src="https://1.bp.blogspot.com/-n020yJN-tpA/UxiWLRegm5I/AAAAAAAAAis/TJHqYn08MZ4/s320/EverythingPasses.png"/></p><p>The question is: can we get this working with Visual Studio Online?</p><p>Fortunately another has gone before me. Mathew Aniyan has written a <a href="https://blogs.msdn.com/b/visualstudioalm/archive/2012/07/09/javascript-unit-tests-on-team-foundation-service-with-chutzpah.aspx">superb blog post called &quot;Javascript Unit Tests on Team Foundation Service with Chutzpah&quot;</a>. Using this post as a guide (it was written 18 months ago which is frankly aeons in the world of the web) I&#x27;m hoping that I&#x27;ll be able to, without too many tweaks, get Javascript unit tests running on Team Foundation Service / Visual Studio Online ( / insert this weeks rebranding here).</p><p>First of all in Visual Studio Online I’ll create a new project called &quot;GettingStartedWithJavaScriptUnitTesting&quot; (using all the default options). Apparently <em>“Your project is created and your team is going to absolutely love this.”</em> Hmmmm... I think I’ll be judge of that.</p><p>Let&#x27;s navigate to the project. I&#x27;ll fire up Visual Studio by clicking on the “Open in Visual Studio” link. Once fired up and all the workspace mapping is sorted I’ll move my project into the GettingStartedWithJavaScriptUnitTesting folder that now exists on my machine and add this to source control.</p><p>Back to Mathew&#x27;s blog. It suggests renaming Chutzpah.VS2012.vsix to Chutzpah.VS2012.zip and checking certain files into TFS. I think Chutzpah has changed a certain amount since this was written. To be on the safe side I’ll create a new folder in the root of my project called Chutzpah.VS2012 and put the contents of Chutzpah.VS2012.zip in there and add it to TFS (being careful to ensure that no dll’s are excluded).</p><p>Then I&#x27;ll follow steps 3 and 4 from the blog post:</p><blockquote><p>*<!-- -->3<!-- -->.<!-- --> In Visual Studio, Open Team Explorer &amp; connect to Team Foundation Service. Bring up the Manage Build Controllers dialog. <!-- -->[Build –&gt; Manage Build Controllers]<!-- --> Select Hosted Build Controller Click on Properties button to bring up the Build Controller Properties dialog.</p><p>4<!-- -->.<!-- --> Change Version Control Path to custom Assemblies to refer to the folder where you checked in the binaries in step 2.</p><ul><li></li></ul></blockquote><p>In step 5 the blog tells me to edit my build definition. Well I don’t have one in this new project so let’s click on “New Build Definition”, create one and then follow step 5:</p><blockquote><p>*<!-- -->5<!-- -->.<!-- --> In Team Explorer, go to the Builds section and Edit your Build Definition which will run the javascript tests. Click on the Process tab Select the row named Automated Tests. Click on … button next to the value.</p><ul><li></li></ul></blockquote><p>Rather than following step 6 (which essentially nukes the running of any .NET tests you might have) I chose to add another row by clicking &quot;Add&quot;. In the dialog presented I changed the Test assembly specification to <!-- -->*<!-- -->*<!-- -->\<!-- -->*<!-- -->.js and checked the &quot;Fail build on test failure&quot; checkbox.</p><p><img src="https://3.bp.blogspot.com/-4lbMIsT9jFQ/Ux3ATwBrPgI/AAAAAAAAAjY/4XSY0u0RpOE/s320/AutomatedTests.png"/></p><p>Step 7 says:</p><blockquote><p>*<!-- -->7<!-- -->.<!-- --> Create your Web application in Visual Studio and add your Qunit or Jasmine unit tests to them. <u>Make sure that the js files (that contain the tests) are getting copied to the build output directory.</u></p><ul><li></li></ul></blockquote><p>The picture below step 7 suggests that you should be setting your test / spec files to have a <code>Copy to Output Directory</code> setting of <code>Copy always</code>. <strong>This did not work for me!!!</strong> Instead, setting a <code>Build Action</code> of <code>Content</code> and a <code>Copy to Output Directory</code> setting of <code>Do not copy</code> did work.</p><p>Finally I checked everything into source control and queued a build. I honestly did not expect this to work. It couldn’t be this easy could it? And...</p><p><img src="https://2.bp.blogspot.com/-gEDIyMV7M_g/Uxibt99tuwI/AAAAAAAAAi8/G4I6XQp0aN0/s320/ItOnlyBlimminWellWorked.png"/></p><p>Wow! It did! Here’s me cynically expecting some kind of “permission denied” error and it actually works! Brilliant! Look up in the cloud it says the same thing!</p><p><img src="https://2.bp.blogspot.com/-A67cTSkzIDg/Uxib6wVnaWI/AAAAAAAAAjE/ZwbUdBJmi0w/s320/InTheCloudToo.png"/></p><p>Fantastic!</p><p>I realise that I haven’t yet written a single JavaScript unit test of my own and so I’ve still a way to go. What I have done is quietened those voices in my head that said “there’s not too much point having a unit test suite that isn’t plugged into continuous integration”. Although it&#x27;s not documented here I&#x27;m happy to be able to report that I have been able to follow the self-same instructions for Team Foundation Service / Visual Studio Online and get CI working with TFS 2012 on our build server as well.</p><p>Having got up and running off the back of other peoples hard work I best try and write some of my own tests now....</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Knockout + Globalize = valueNumber Binding Handler]]></title>
            <link>https://blog.johnnyreilly.com/2014/03/11/knockout-globalize-valuenumber-binding</link>
            <guid>Knockout + Globalize = valueNumber Binding Handler</guid>
            <pubDate>Tue, 11 Mar 2014 00:00:00 GMT</pubDate>
            <description><![CDATA[I’ve long used Globalize for my JavaScript number formatting / parsing needs. In a current project I’m using Knockout for the UI. When it came to data-binding numeric values none of the default binding handlers seemed appropriate. What I wanted was a binding handler that:]]></description>
            <content:encoded><![CDATA[<p>I’ve long used <a href="https://github.com/jquery/globalize/">Globalize</a> for my JavaScript number formatting / parsing needs. In a current project I’m using Knockout for the UI. When it came to data-binding numeric values none of the default binding handlers seemed appropriate. What I wanted was a binding handler that:</p><ol><li>Was specifically purposed for dealing with numeric values</li><li>Handled the parsing / formatting for the current locale (and I naturally intended to use Globalize for this purpose)</li></ol><p>Like so much development we start by standing on the shoulders of giants. In this case it’s the fantastic <a href="https://twitter.com/RPNiemeyer">Ryan Niemeyer</a> who put up a <a href="http://stackoverflow.com/a/12647270/761388">post on StackOverflow</a> that got me on the right track.</p><p>Essentially his approach provides an “interceptor” mechanism that allows you to validate numeric data entry on input and format numeric data going out as well. Very nice. Into this I plugged Globalize to handle the parsing and formatting. I ended up with the “valueNumber” binding handler:</p><pre><code class="language-js">ko.bindingHandlers.valueNumber = {
  init: function (
    element,
    valueAccessor,
    allBindingsAccessor,
    viewModel,
    bindingContext
  ) {
    /**
     * Adapted from the KO hasfocus handleElementFocusChange function
     */
    function elementIsFocused() {
      var isFocused = false,
        ownerDoc = element.ownerDocument;
      if (&#x27;activeElement&#x27; in ownerDoc) {
        var active;
        try {
          active = ownerDoc.activeElement;
        } catch (e) {
          // IE9 throws if you access activeElement during page load
          active = ownerDoc.body;
        }
        isFocused = active === element;
      }

      return isFocused;
    }

    /**
     * Adapted from the KO hasfocus handleElementFocusChange function
     *
     * @param {boolean} isFocused whether the current element has focus
     */
    function handleElementFocusChange(isFocused) {
      elementHasFocus(isFocused);
    }

    var observable = valueAccessor(),
      properties = allBindingsAccessor(),
      elementHasFocus = ko.observable(elementIsFocused()),
      handleElementFocusIn = handleElementFocusChange.bind(null, true),
      handleElementFocusOut = handleElementFocusChange.bind(null, false);

    var interceptor = ko.computed({
      read: function () {
        var currentValue = ko.utils.unwrapObservable(observable);
        if (elementHasFocus()) {
          return !isNaN(currentValue) &amp;&amp;
            currentValue !== null &amp;&amp;
            currentValue !== undefined
            ? currentValue
                .toString()
                .replace(&#x27;.&#x27;, Globalize.findClosestCulture().numberFormat[&#x27;.&#x27;]) // Displays correct decimal separator for the current culture (so de-DE would format 1.234 as &quot;1,234&quot;)
            : null;
        } else {
          var format = properties.numberFormat || &#x27;n2&#x27;,
            formattedNumber = Globalize.format(currentValue, format);

          return formattedNumber;
        }
      },
      write: function (newValue) {
        var currentValue = ko.utils.unwrapObservable(observable),
          numberValue = Globalize.parseFloat(newValue);

        if (!isNaN(numberValue)) {
          if (numberValue !== currentValue) {
            // The value has changed so update the observable
            observable(numberValue);
          }
        } else if (newValue.length === 0) {
          if (properties.isNullable) {
            // If newValue is a blank string and the isNullable property has been set then nullify the observable
            observable(null);
          } else {
            // If newValue is a blank string and the isNullable property has not been set then set the observable to 0
            observable(0);
          }
        }
      },
    });

    ko.utils.registerEventHandler(element, &#x27;focus&#x27;, handleElementFocusIn);
    ko.utils.registerEventHandler(element, &#x27;focusin&#x27;, handleElementFocusIn); // For IE
    ko.utils.registerEventHandler(element, &#x27;blur&#x27;, handleElementFocusOut);
    ko.utils.registerEventHandler(element, &#x27;focusout&#x27;, handleElementFocusOut); // For IE

    if (element.tagName.toLowerCase() === &#x27;input&#x27;) {
      ko.applyBindingsToNode(element, { value: interceptor });
    } else {
      ko.applyBindingsToNode(element, { text: interceptor });
    }
  },
};
</code></pre><p>Using this binding handler you just need to drop in a <code>valueNumber</code> into your <code>data-bind</code> statement where you might previously have used a <code>value</code> binding. The binding also has a couple of nice hooks in place which you might find useful:</p><dl><dt>numberFormat (defaults to &quot;n2&quot;)</dt><dd>allows you to specify a format to display your number with. Eg, &quot;c2&quot; would display your number as a currency to 2 decimal places, &quot;p1&quot; would display your number as a percentage to 1 decimal place etc</dd><dt>isNullable (defaults to false)</dt><dd>specifies whether your number should be treated as nullable. If it&#x27;s not then clearing the elements value will set the underlying observable to 0.</dd></dl><p>Finally when the element gains focus / becomes active the full underlying value is displayed. (Kind of like Excel - like many an app, the one I&#x27;m working on started life as Excel and the users want to keep some of the nice aspects of Excel&#x27;s UI.) To take a scenario, let&#x27;s imagine we have an input element which is applying the &quot;n1&quot; format. The underlying value backing this is 1.234. The valueNumber binding displays this as &quot;1.2&quot; when the input does not have focus and when the element gains focus the full &quot;1.234&quot; is displayed. Credit where it’s due, this is thanks to <a href="http://stackoverflow.com/users/1105996/robert-westerlund">Robert Westerlund</a> who was kind enough to respond to a <a href="http://stackoverflow.com/a/22313546/761388">question of mine on StackOverflow</a>.</p><p>Finally, here’s a demo using the &quot;de-DE&quot; locale:</p><iframe width="100%" height="400" src="https://jsfiddle.net/johnny_reilly/jRt3k/embedded/result,js,html" allowfullscreen="" frameBorder="0"></iframe><h2>PS Globalize is a-changing</h2><p>The version of Globalize used in the binding handler is Globalize v0.1.1. This has been available in various forms for quite some time but as I write this the Globalize plugin is in the process of being ported to the <a href="http://cldr.unicode.org/">CLDR</a>. As part of that work it looks like the Globalize API will change. When that gets finalized I’ll try and come back and update this.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Caching and cache-busting with RequireJS]]></title>
            <link>https://blog.johnnyreilly.com/2014/03/05/caching-and-cache-busting-with-requirejs</link>
            <guid>Caching and cache-busting with RequireJS</guid>
            <pubDate>Wed, 05 Mar 2014 00:00:00 GMT</pubDate>
            <description><![CDATA[Having put together a demo of using TypeScript with RequireJS my attention turned quickly to caching. Or rather, IE forced me to think about caching.]]></description>
            <content:encoded><![CDATA[<p>Having put together a demo of using TypeScript with RequireJS my attention turned quickly to caching. Or rather, IE forced me to think about caching.</p><p>Everyone has their own workflow, their own tools. The things they like to use as they put things together. And for me I’m a Visual Studio man – it’s not everyone’s bag but I really like it. I find the JavaScript tooling is now really solid combined with IE and it (generally) makes me more productive. I want to use it. But, as you know, nothing is perfect...</p><p>So there I was, delighted with the TypeScript / RequireJS demo. It was working just lovely. I started investigating the debugging story. What would happen if I change a script file on the fly? When I refresh IE does it pick up the tweaks?</p><p>Let’s find out. I&#x27;ll open up alerter.ts and change this:</p><pre><code class="language-ts">var name = &#x27;John&#x27;;
</code></pre><p>to this:</p><pre><code class="language-js">var name = &#x27;Bobby&#x27;;
</code></pre><p>And <!-- -->*<strong>boom</strong>*<!-- -->! Nothing. I’ve refreshed IE and I’m expecting to see “Welcome to Code Camp, Bobby”. But I’m still reading “Welcome to Code Camp, John”... I bet Chrome wouldn’t do this to me... And I’m right! It doesn’t. I don’t want to get too much into the details of this but it looks like it comes down to Chrome sending an &quot;If-Modified-Since&quot; request header where IE does not. I’m pretty sure that IE could be configured to behave likewise but I’d rather not have to remember that. (And furthermore I don’t want to have to remind every person that works on the app to do that as well.)</p><p>This raises a number of issues but essentially it gets me to think about the sort of caching I want. Like most of you I have 2 significant use cases:</p><ol><li>Development</li><li>Production</li></ol><p>For Development I want any changes to JavaScript files to be picked up – I do <!-- -->*<strong>not</strong>*<!-- --> want caching. For Production I want caching in order that users have better performance / faster loading. If I ship a new version of the app to Production I also want users to pick up the new versions of a file and cache those.</p><h2>Research</h2><p>I did a little digging. The most useful information I found was <a href="http://stackoverflow.com/q/8315088/761388">a StackOverflow post on RequireJS and caching</a>. Actually I’d recommend anyone reading this to head over and read that from top to bottom. Read the question and all of the answers as well – pretty much everything will add to your understanding of RequireJS.</p><p>As with any set of answers there are different and conflicting views. <a href="http://stackoverflow.com/a/8479953/761388">Phil McCull’s (accepted) answer</a> was for my money the most useful. It pointed <a href="http://requirejs.org/docs/api.html#config-urlArgs">back to the RequireJS documentation</a>.</p><blockquote><p>*<!-- -->&quot;urlArgs: Extra query string arguments appended to URLs that RequireJS uses to fetch resources. Most useful to cache bust when the browser or server is not configured correctly. Example cache bust setting for urlArgs:</p><pre><code class="language-js">urlArgs: &#x27;bust=&#x27; + new Date().getTime();
</code></pre><p>During development it can be useful to use this, however be sure to remove it before deploying your code.&quot;</p><ul><li></li></ul></blockquote><p>Phil’s answer suggests using urlArgs <!-- -->*<strong>both</strong>*<!-- --> for Production and for Development in 2 different ways. Using what amounts to a random number in the Development environment (as in the official docs) for cache-busting. For the Production environment he suggests using a specific version number which allows for client-side caching between different build versions.</p><p>Full disclosure, this is not the approach favoured by James Burke (author of RequireJS). He doesn’t go into why in the RequireJS docs but has <a href="https://groups.google.com/forum/#!msg/requirejs/3E9dP_BSQoY/36ut2Gtko7cJ">elsewhere expounded on this</a>:</p><blockquote><p><em>For deployed assets, I prefer to put the version or hash for the whole build as a build directory, then just modify the baseUrl config used for the project to use that versioned directory as the baseUrl. Then no other files change, and it helps avoid some proxy issues where they may not cache an URL with a query string on it. </em></p></blockquote><p>I’m not so worried about the proxy caching issue. My users tend to be people who use the application repeatedly and so the caching I most care about is their local machine caching. From what I understand urlArgs will work fine in this scenario. Yes the downside of this approach is that some proxy servers may not cache these assets. That’s a shame but it’s not a dealbreaker for me. As I said, I still have client side caching.</p><p>If you want to go a little deeper I recommend reading <a href="http://www.stevesouders.com/blog/2008/08/23/revving-filenames-dont-use-querystring/">Steve Souders post</a> on the topic (in case you’re not aware Steve is Google’s Mr Performance). Interestingly, looking at the comments on the post it sounds like the lack of support for proxy caching with querystrings may that may be starting to change.</p><p>But either way, I’m happy with this approach. As I always say, if it’s good enough for Stack Overflow then it’s good enough for me:</p><p><img src="https://4.bp.blogspot.com/-pG0ahnzaPJM/UxcP7f6ENII/AAAAAAAAAhY/VVahRmEe5_0/s320/IfItsGoodEnoughForStackOverflow.png"/></p><h2>Implementation</h2><p>I’m going to start off using the demo from <a href="http://icanmakethiswork.blogspot.com/2014/02/typescript-and-requirejs-keep-it-simple.html">my last blog post</a> as a basis. Let’s take that and evolve it. As a result my solution is going to work with TypeScript and RequireJS (since the previous demo was about that) but the implementation I’m going to come up with would work as well with vanilla JS as it would with TypeScript compiled JS.</p><p>Let’s take a look at our index.html. First we’ll drop our usage of <code>main.ts</code> / <code>main.js</code> (our bootstrapper file that defines config and kicks off the &quot;app&quot;). We’ll pull out the use of <code>data-main</code> and instead, just after the reference to require we’ll add the contents of <code>main.js</code> much in <a href="http://requirejs.org/docs/api.html#config">the style of the RequireJS docs</a>. We’ll also include a urlArgs that as a cache-buster that uses the approach outlined <a href="http://requirejs.org/docs/api.html#config-urlArgs">in the RequireJS docs</a>:</p><pre><code class="language-html">&lt;script src=&quot;/scripts/require.js&quot;&gt;&lt;/script&gt;
&lt;script&gt;
  require.config({
    baseUrl: &#x27;/scripts&#x27;,
    paths: {
      jquery: &#x27;jquery-2.1.0&#x27;,
    },
    urlArgs: &#x27;v=&#x27; + new Date().getTime(),
  });

  require([&#x27;alerter&#x27;], function (alerter) {
    alerter.showMessage();
  });
&lt;/script&gt;
</code></pre><p>Spinning up the site all runs as you would expect. The question is: does this work as a cache-buster? Let’s tweak <code>alerter.ts</code> / <code>alerter.js</code>. And:</p><p><img src="https://1.bp.blogspot.com/-WTNrPPyeMTY/UxcRTQpqM3I/AAAAAAAAAhk/ICvFXxji3FY/s320/newDateSolution.png"/></p><p>Oh yeah! We’re cache-busting like gangbusters!</p><p>So now let’s comment out our existing urlArgs (which represents the Development solution from Phil’s answer) and replace it with a fixed value like this:</p><pre><code class="language-js">//urlArgs: &quot;v=&quot; +  (new Date()).getTime()
urlArgs: &#x27;v=1&#x27;;
</code></pre><p>This represents the Production solution from Phil’s answer. Now let’s run, refresh a couple of times and ensure that our fixed querystring value results in a 304 status code (indicating “Not Modified” and cached item used):</p><p><img src="https://4.bp.blogspot.com/-Yy138lKDkuE/UxcRr7EpiXI/AAAAAAAAAhs/QVPcUF-rdgw/s320/FixedQuerystring304.png"/></p><p>It does! Now let’s increment the value:</p><pre><code class="language-js">urlArgs: &#x27;v=2&#x27;;
</code></pre><p>When we refresh the browser this should result in 200 status codes (indicating the cached version has not been used and the client has picked up a new version from the server).</p><p><img src="https://2.bp.blogspot.com/-qx7Ya1MZNC8/UxcSBwKjM_I/AAAAAAAAAh0/aywHMXHUrwI/s320/NewFixedQuerystring200.png"/></p><p>Success! That’s our premise tested – both Development and Production scenarios. Now we want to turn this into a slightly more sophisticated reusable solution like this:</p><pre><code class="language-html">&lt;script src=&quot;/scripts/require.js&quot;&gt;&lt;/script&gt;
&lt;script&gt;
  var inDevelopment = true,
    version = &#x27;1&#x27;;

  require.config({
    baseUrl: &#x27;/scripts&#x27;,
    paths: {
      jquery: &#x27;jquery-2.1.0&#x27;,
    },
    urlArgs: &#x27;v=&#x27; + (inDevelopment ? new Date().getTime() : version),
  });

  require([&#x27;alerter&#x27;], function (alerter) {
    alerter.showMessage();
  });
&lt;/script&gt;
</code></pre><p>In the tweaked script above 2 variables are defined. The first is <code>inDevelopment</code> which models whether you are in the Development scenario (true) or the Production scenario (false). The second is <code>version</code> which represents the application version number. With this in place I can simply flip between the Development and Production scenario by changing the value of <code>inDevelopment</code>. And when a new version ships I can change the version number to force a cache refresh on the users.</p><p>What drives the values of <code>inDevelopment</code> / <code>version</code> is down to you. You could load the <code>inDevelopment</code> / <code>version</code> values from some application endpoint. You could hardcode them in your screen. The choices are yours. I’m going to finish off with a simple approach that I&#x27;ve found useful.</p><h2>Let’s get the server involved!</h2><p>I want the server to drive my urlArgs value. Why? Well this project happens to be an ASP.NET project which handily has the concept of Development / Production scenarios nicely modelled by the <a href="http://msdn.microsoft.com/en-us/library/s10awwz0(v=vs.85).aspx">web.config’s compilation debug flag</a>.</p><pre><code class="language-xml">&lt;configuration&gt;
  &lt;system.web&gt;
    &lt;compilation debug=&quot;true&quot; targetFramework=&quot;4.5&quot; /&gt;
    &lt;httpRuntime targetFramework=&quot;4.5&quot; /&gt;
  &lt;/system.web&gt;
&lt;/configuration&gt;
</code></pre><p>If debug is <code>true</code> then that reflects the Development scenario. If debug is <code>false</code> then that reflects the Production scenario.</p><p>So bearing that in mind I want to use the value of debug to drive my <code>urlArgs</code>. If I have my debug flag set to <code>true</code> I want to cache-bust all the way. Likewise, if debug is set to <code>false</code> then I want to serve up the version number so that caching is used until the version number changes. Time to break out the C#:</p><pre><code class="language-cs">namespace RequireJSandCaching
{
    public static class RequireJSHelpers
    {
        private static readonly bool _inDebug;
        private static readonly string _version;

        static RequireJSHelpers()
        {
            _inDebug = System.Web.HttpContext.Current.IsDebuggingEnabled;
            _version = (_inDebug)
                ? &quot;InDebug&quot;
                : System.Reflection.Assembly.GetExecutingAssembly().GetName().Version.ToString();
        }

        public static string Version
        {
            get
            {
                return (_inDebug)
                    ? System.DateTime.Now.Ticks.ToString()
                    : _version;
            }
        }
    }
}
</code></pre><p>This is a static helper class called <code>RequireJSHelpers</code>. It has a static constructor which initialises 2 fields. <code>_inDebug</code> is taken from <code>System.Web.HttpContext.Current.IsDebuggingEnabled</code> which exposes the compilation debug value. <code>_version</code> is initialised, when debug is <code>false</code>, to the version number of the dll (driven by this <code>AssemblyInfo.cs [assembly: AssemblyVersion(&quot;1.0.*&quot;)]</code> attribute)</p><p>There’s 1 property on this helper class called version. Depending on whether the app is in debug mode or not this attribute either exposes the application version or effectively the C# equivalent to JavaScript’s <code>(new Date()).getTime()</code>. (Well strictly speaking they have a different starting point in history but that’s by-the-by... Both are of equal value as cache-busters.)</p><p>You probably see where this is all going.</p><p>Let’s clone our <code>index.html</code> page and call it <code>serverUrlArgs.cshtml</code> (note the suffix). Let’s replace the script section with this:</p><pre><code class="language-html">&lt;script&gt;
  require.config({
    baseUrl: &#x27;/scripts&#x27;,
    paths: {
      jquery: &#x27;jquery-2.1.0&#x27;,
    },
    urlArgs: &#x27;v=@RequireJSandCaching.RequireJSHelpers.Version&#x27;,
  });

  require([&#x27;alerter&#x27;], function (alerter) {
    alerter.showMessage();
  });
&lt;/script&gt;
</code></pre><p>Which drives <code>urlArgs</code> from the <code>RequireJSHelpers.Version</code> property. If we fire it up now (with debug set to true in our web.config) then we see requests like this:</p><p><img src="https://1.bp.blogspot.com/-N9TIJO1jzU4/UxcUW8z2uaI/AAAAAAAAAiA/-vo6wVx2NoI/s320/DebugEqualsTrue.png"/></p><p>And if we set debug to false in our web.config then (after the initial requests have been cached) we see requests like this:</p><p><img src="https://4.bp.blogspot.com/-xv40UDHgJfk/UxcUe4SZrUI/AAAAAAAAAiI/euLoArWTPLw/s320/DebugEqualsFalse.png"/></p><p>This leaves us with a simple mechanism to drive our RequireJS caching. If debug is set to <code>true</code> in our <code>web.config</code> then Require will perform cache-busting. If debug is set to <code>false</code> then RequireJS will perform only version-changing cache-busting and will, whilst the version remains constant, support client-side caching.</p><p>Finished. In case it helps I’ve put the code for this <a href="https://github.com/johnnyreilly/RequireJSandCaching">up on GitHub</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[TypeScript and RequireJS (Keep It Simple)]]></title>
            <link>https://blog.johnnyreilly.com/2014/02/27/typescript-and-requirejs-keep-it-simple</link>
            <guid>TypeScript and RequireJS (Keep It Simple)</guid>
            <pubDate>Thu, 27 Feb 2014 00:00:00 GMT</pubDate>
            <description><![CDATA[I'm not the first to take a look at mixing TypeScript and RequireJS but I wanted to get it clear in my head. Also, I've always felt the best way to learn is to do. So here we go. I'm going to create a TypeScript and RequireJS demo based on John Papa's "Keep It Simple RequireJS Demo".]]></description>
            <content:encoded><![CDATA[<p>I&#x27;m not the first to take a look at mixing TypeScript and RequireJS but I wanted to get it clear in my head. Also, I&#x27;ve always felt the best way to learn is to do. So here we go. I&#x27;m going to create a TypeScript and RequireJS demo based on <a href="https://github.com/johnpapa/kis-requirejs-demo/">John Papa&#x27;s &quot;Keep It Simple RequireJS Demo&quot;</a>.</p><p>So let&#x27;s fire up Visual Studio 2013 and create a new ASP.NET Web Application called “RequireJSandTypeScript” (the empty project template is fine).</p><p>Add a new HTML file to the root called “index.html” and base it on “index3.html” from <a href="https://github.com/johnpapa/kis-requirejs-demo/blob/master/ModularDemo/index3.html">John Papa’s demo</a>:</p><pre><code class="language-html">&lt;!DOCTYPE html&gt;
&lt;html&gt;
  &lt;head&gt;
    &lt;title&gt;TypeScript with RequireJS&lt;/title&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;div&gt;
      &lt;h1&gt;TypeScript with RequireJS loading jQuery in Visual Studio land&lt;/h1&gt;
    &lt;/div&gt;

    &lt;!-- use jquery to load this message--&gt;
    &lt;p id=&quot;message&quot;&gt;&lt;/p&gt;

    &lt;!-- Shortcut to load require and then load main--&gt;
    &lt;script
      src=&quot;/scripts/require.js&quot;
      data-main=&quot;/scripts/main&quot;
      type=&quot;text/javascript&quot;
    &gt;&lt;/script&gt;
  &lt;/body&gt;
&lt;/html&gt;
</code></pre><p>John’s demo depends on jQuery and RequireJS (not too surprisingly) so let’s fire up Nuget and get them:</p><pre><code class="language-ps">Install-Package RequireJS
Install-Package jQuery
</code></pre><p>Whilst we’re at it, let’s get the Definitely Typed typings as well:</p><pre><code class="language-ps">Install-Package jQuery.TypeScript.DefinitelyTyped
</code></pre><p>To my surprise this popped up the following dialog:</p><p><img src="https://2.bp.blogspot.com/-rzhPvMSWRZ4/Uw9uJaGRz2I/AAAAAAAAAhI/sa6ZS1-fuPs/s320/TypeScriptConfigured.png"/></p><p>By &quot;Your project has been configured to support TypeScript.&quot; it means that the csproj file has had the following entries added:</p><pre><code class="language-xml">&lt;Project ToolsVersion=&quot;12.0&quot; DefaultTargets=&quot;Build&quot; xmlns=&quot;http://schemas.microsoft.com/developer/msbuild/2003&quot;&gt;
  &lt;Import Project=&quot;$(MSBuildExtensionsPath32)\Microsoft\VisualStudio\v$(VisualStudioVersion)\TypeScript\Microsoft.TypeScript.Default.props&quot; Condition=&quot;Exists(&#x27;$(MSBuildExtensionsPath32)\Microsoft\VisualStudio\v$(VisualStudioVersion)\TypeScript\Microsoft.TypeScript.Default.props&#x27;)&quot; /&gt;
  ...
  &lt;PropertyGroup&gt;
    ...
    &lt;TypeScriptToolsVersion&gt;0.9&lt;/TypeScriptToolsVersion&gt;
  &lt;/PropertyGroup&gt;
  ...
  &lt;Import Project=&quot;$(MSBuildExtensionsPath32)\Microsoft\VisualStudio\v$(VisualStudioVersion)\TypeScript\Microsoft.TypeScript.targets&quot; Condition=&quot;Exists(&#x27;$(MSBuildExtensionsPath32)\Microsoft\VisualStudio\v$(VisualStudioVersion)\TypeScript\Microsoft.TypeScript.targets&#x27;)&quot; /&gt;
  ...
&lt;/Project&gt;
</code></pre><p>I’m not sure when this tweak to the Visual Studio tooling was added was added. Perhaps it&#x27;s part of the <a href="https://blogs.msdn.com/b/typescript/archive/2014/02/25/announcing-typescript-1-0rc.aspx">TypeScript 1.0 RC release</a>; either way it’s pretty nice. Let&#x27;s press on.</p><p>Whilst we’re at it let’s make sure that we’re compiling to AMD (to be RequireJS friendly) by adding in the following csproj tweaks just before the Microsoft.TypeScript.targets Project import statement:</p><pre><code class="language-xml">&lt;PropertyGroup Condition=&quot;&#x27;$(Configuration)&#x27; == &#x27;Debug&#x27;&quot;&gt;
    &lt;TypeScriptModuleKind&gt;amd&lt;/TypeScriptModuleKind&gt;
  &lt;/PropertyGroup&gt;
  &lt;PropertyGroup Condition=&quot;&#x27;$(Configuration)&#x27; == &#x27;Release&#x27;&quot;&gt;
    &lt;TypeScriptModuleKind&gt;amd&lt;/TypeScriptModuleKind&gt;
  &lt;/PropertyGroup&gt;
</code></pre><p>Where was I? Oh yes, typings. So let’s get the RequireJS typings too:</p><pre><code class="language-ps">Install-Package requirejs.TypeScript.DefinitelyTyped
</code></pre><p>Right – looking at index.html we can see from the data-main tag that the first file loaded by RequireJS, our bootstrapper if you will, is main.js. So let’s add ourselves a main.ts based on <a href="https://github.com/johnpapa/kis-requirejs-demo/blob/master/ModularDemo/Scripts3/main.js">John&#x27;s example</a> (which will in turn generate a main.js):</p><pre><code class="language-ts">(function () {
  requirejs.config({
    baseUrl: &#x27;scripts&#x27;,
    paths: {
      jquery: &#x27;jquery-2.1.0&#x27;,
    },
  });

  require([&#x27;alerter&#x27;], (alerter) =&gt; {
    alerter.showMessage();
  });
})();
</code></pre><p>main.ts depends upon <a href="https://github.com/johnpapa/kis-requirejs-demo/blob/master/ModularDemo/Scripts3/alerter.js">alerter</a> so let’s add ourselves an alerter.ts as well:</p><pre><code class="language-ts">define(&#x27;alerter&#x27;, [&#x27;jquery&#x27;, &#x27;dataservice&#x27;], function ($, dataservice) {
  var name = &#x27;John&#x27;,
    showMessage = function () {
      var msg = dataservice.getMessage();

      $(&#x27;#message&#x27;).text(msg + &#x27;, &#x27; + name);
    };

  return {
    showMessage: showMessage,
  };
});
</code></pre><p>And a <a href="https://github.com/johnpapa/kis-requirejs-demo/blob/master/ModularDemo/Scripts3/dataservice.js">dataservice.ts</a>:</p><pre><code class="language-ts">define(&#x27;dataservice&#x27;, [], function () {
  var msg = &#x27;Welcome to Code Camp&#x27;,
    getMessage = function () {
      return msg;
    };

  return {
    getMessage: getMessage,
  };
});
</code></pre><p>That all compiles fine. But we’re missing a trick. We’re supposed to be using TypeScripts AMD support so let’s change the code to do just that. First dataservice.ts:</p><pre><code class="language-ts">var msg = &#x27;Welcome to Code Camp&#x27;;

export function getMessage() {
  return msg;
}
</code></pre><p>Then alerter.ts:</p><pre><code class="language-ts">import $ = require(&#x27;jquery&#x27;);
import dataservice = require(&#x27;dataservice&#x27;);

var name = &#x27;John&#x27;;

export function showMessage() {
  var msg = dataservice.getMessage();

  $(&#x27;#message&#x27;).text(msg + &#x27;, &#x27; + name);
}
</code></pre><p>I know both of the above look slightly different but if you look close you&#x27;ll see it&#x27;s really only boilerplate changes. The actual application code is unaffected. Finally, main.ts remains as it is and that&#x27;s us done; we have ourselves a working demo... Yay!</p><p>Thanks to John Papa for creating such a simple demo I could use as the basis for my own demo.</p><h2>Closing Thoughts</h2><p>Unfortunately there is no typing on the alerter reference within main.ts. To my knowledge there is no way to implicitly import the typings here – the only thing you can do is specify them manually. (By the way, if I&#x27;m wrong about this then please do set me straight!) That said, this is not so bad really since this main.ts file is essentially just a bootstrapper that kicks things off. All the other files contain the real application code and they have have typings a-go-go. So we&#x27;re happy.</p><h2>Finally for bonus points....</h2><p>I’ve included the js and js.map files in the project file as they don&#x27;t seem to be added into the project by Visual Studio when the TS file is created or when it is compiled for the first time. I&#x27;ve also ensured that these files are dependent upon the typescript files they were generated from.</p><pre><code class="language-xml">&lt;TypeScriptCompile Include=&quot;Scripts\alerter.ts&quot; /&gt;
    &lt;Content Include=&quot;Scripts\alerter.js&quot;&gt;
        &lt;DependentUpon&gt;alerter.ts&lt;/DependentUpon&gt;
    &lt;/Content&gt;
    &lt;Content Include=&quot;Scripts\alerter.js.map&quot;&gt;
        &lt;DependentUpon&gt;alerter.ts&lt;/DependentUpon&gt;
    &lt;/Content&gt;
    &lt;TypeScriptCompile Include=&quot;Scripts\dataservice.ts&quot; /&gt;
    &lt;Content Include=&quot;Scripts\dataservice.js&quot;&gt;
        &lt;DependentUpon&gt;dataservice.ts&lt;/DependentUpon&gt;
    &lt;/Content&gt;
    &lt;Content Include=&quot;Scripts\dataservice.js.map&quot;&gt;
        &lt;DependentUpon&gt;dataservice.ts&lt;/DependentUpon&gt;
    &lt;/Content&gt;
    &lt;TypeScriptCompile Include=&quot;Scripts\main.ts&quot; /&gt;
    &lt;Content Include=&quot;Scripts\main.js&quot;&gt;
        &lt;DependentUpon&gt;main.ts&lt;/DependentUpon&gt;
    &lt;/Content&gt;
    &lt;Content Include=&quot;Scripts\main.js.map&quot;&gt;
        &lt;DependentUpon&gt;main.ts&lt;/DependentUpon&gt;
    &lt;/Content&gt;
</code></pre><h2>Want the code for your very own?</h2><p>Well you can grab it from <a href="https://github.com/johnnyreilly/RequireJSandTypeScript">GitHub</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[WPF and Mystic Meg or Playing Futurologist]]></title>
            <link>https://blog.johnnyreilly.com/2014/02/12/wpf-and-mystic-meg-or-playing</link>
            <guid>WPF and Mystic Meg or Playing Futurologist</guid>
            <pubDate>Wed, 12 Feb 2014 00:00:00 GMT</pubDate>
            <description><![CDATA[Time for an unusual post. Most of what gets put down here is technical "how-to's". It's usually prompted by something I've been working on and serves, as much as anything else, as an aide-memoire. Not this time.]]></description>
            <content:encoded><![CDATA[<p>Time for an unusual post. Most of what gets put down here is technical &quot;how-to&#x27;s&quot;. It&#x27;s usually prompted by something I&#x27;ve been working on and serves, as much as anything else, as an aide-memoire. Not this time.</p><p>I’ve been watching the changes in the world of development of the last couple of years and I’ve come to a controversial conclusion... So I wanted to write about it. Hopefully I&#x27;ll be able to return to this in 5 years and say &quot;wow - I&#x27;m so insightful - almost visionary really&quot;. Or not. Either way, let&#x27;s put it out there - it&#x27;s sink or swim time. Ready for it? Here’s my bet: WPF will die.</p><p>Sounds dramatic right? OK - I&#x27;ve overstated my case just to get you to read on (I should work for the tabloids). Let me flesh this out a little. First of all, I think WPF is a fine technology - great apps are built with it. My personal favourite being the fantastic <a href="https://github.com/blog/1151-designing-github-for-windows">GitHub for Windows</a>. And actually I don&#x27;t think WPF will die at all. What I think will happen is that it will become a more niche way to build applications.</p><p>More broadly, I think that native client apps (be they Windows, Mac, iOS, Android etc) will eventually come to be replaced by <a href="http://en.wikipedia.org/wiki/Single-page_application">rich web apps / SPAs</a> of the Angular / Ember / Durandal ilk. I realise that at the moment that seems like a ludicrous statement – native apps are heavily used throughout enterprises worldwide and certainly will continue to be used and actively developed for at least the next 5 years.</p><p>But as the web comes to <a href="http://arstechnica.com/information-technology/2013/05/native-level-performance-on-the-web-a-brief-examination-of-asm-js/">perform like native</a>, as <a href="https://github.com/jashkenas/coffee-script/wiki/List-of-languages-that-compile-to-JS">JavaScript become a compile target</a>, as <a href="http://davidwalsh.name/canvas-demos">HTML 5 provides rich UI</a> and as <a href="https://developer.mozilla.org/en/docs/WebSockets">interactive communication becomes possible</a> I reckon this is a fairly probable scenario. Particularly when you consider the <a href="https://developer.mozilla.org/en-US/Apps/Reference">API work Firefox is doing around Firefox OS</a>. I could be wrong, but my expectation is that the day will come when people will have apps that they can access from anywhere, on any platform and those apps can be deployed without infrastructure having to push out new versions to each client machine.</p><p>The web undeniably has issues but I think it will likely win out. And the cost case for a single client app is pretty compelling to anyone funding a system.</p><p>As a dev I’m always working with an ever-evolving grab bag of technology whether it be front end, middle tier, database or services. In fact that will likely always be the case (change being the only constant in the world of software). But on the basis of my expectations I’m planning to always keep at least a toe in the world of web apps as a form of “career future-proofing”.</p><p>Going less broad again when I look at the Microsoft stack, I think XAML will live on for some time. Obviously Silverlight is no longer being actively developed but MS are using it in Windows 8 (Phone and WinJS) as well as WPF. But I do kind of wonder if it will become like a bit like VB.Net, still around, still in use, but slowly dropping off in terms of popularity. Particularly as you can write WinJS apps in HTML / CSS / JavaScript.</p><p>As I say, I could very much be wrong about all of this. I don’t know what your view of the future of the development landscape is? You may have a different insight? I’d be intrigued to know!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Integration Testing with Entity Framework and Snapshot Backups]]></title>
            <link>https://blog.johnnyreilly.com/2014/01/24/integration-testing-with-entity</link>
            <guid>Integration Testing with Entity Framework and Snapshot Backups</guid>
            <pubDate>Fri, 24 Jan 2014 00:00:00 GMT</pubDate>
            <description><![CDATA[I've written before about how unit testing Entity Framework is a contentious and sometimes pointless activity. The TL;DR is that LINQ-to-Objects != Linq-to-Entities and so if you want some useful tests around your data tier then integration tests that actually hit a database are what you want.]]></description>
            <content:encoded><![CDATA[<p>I&#x27;ve written before about how unit testing <a href="http://icanmakethiswork.blogspot.co.uk/2012/10/unit-testing-and-entity-framework-filth.html">Entity Framework is a contentious and sometimes pointless activity</a>. The TL;DR is that LINQ-to-Objects != Linq-to-Entities and so if you want some useful tests around your data tier then integration tests that actually hit a database are what you want.</p><p>However hitting an actual database is has serious implications. For a start you need a database server and you need a database. But the real issue lies around cleanup. When you write a test that amends data in the database you need the test to clean up after itself. If it doesn&#x27;t then the next test that runs may trip over the amended data and that&#x27;s your test pack instantly useless.</p><p>What you want is a way to wipe the slate clean - to return the database back to the state that it was in before your test ran. Kind of like a database restore - except that would be slow. And this is where <a href="http://technet.microsoft.com/en-us/library/ms189548(v=sql.105).aspx">SQL Server&#x27;s snapshot backups</a> have got your back. To quote MSDN:</p><blockquote><p>*<!-- -->Snapshot backups have the following primary benefits:</p><ul><li>A backup can be created quickly, typically measured in seconds, with little or no effect on the server.</li><li>A restore operation can be accomplished from a disk backup just as quickly.</li><li>Backup to tape can be accomplished by another host without an effect on the production system.</li><li><strong>A copy of a production database can be created instantly for reporting or testing.</strong></li></ul><ul><li></li></ul></blockquote><p>Just the ticket.</p><h2>Our Mission</h2><p>In this post I want to go through the process of taking an existing database, pointing Entity Framework at it, setting up some repositories and then creating an integration test pack that uses snapshot backups to cleanup after each test runs. The code detailed in this post is available in this <a href="https://github.com/johnnyreilly/SnapshotBackupsIntegrationTesting">GitHub repo</a> if you want to have a go yourself.</p><h2>We need a database</h2><p>You can find a whole assortment of databases <a href="https://msftdbprodsamples.codeplex.com/releases">here</a>. I&#x27;m going to use <a href="https://msftdbprodsamples.codeplex.com/wikipage?title=AWLTDocs">AdventureWorksLT</a> as it&#x27;s small and simple. So I&#x27;ll download <a href="https://msftdbprodsamples.codeplex.com/downloads/get/478217">this</a> and unzip it. I&#x27;ll drop <code>AdventureWorksLT2008R2_Data.mdf</code> and <code>AdventureWorksLT2008R2_log.LDF</code> in my data folder and attach AdventureWorksLT2008R2 to my database server. And now I have a database:</p><p><img src="https://1.bp.blogspot.com/-Nke8F6wYI4A/UuEeJ6C0XqI/AAAAAAAAAgg/tbuhu2TuOpg/s320/Database2.png"/></p><h2>Assemble me your finest DbContext</h2><p>Or in English: we want to point Entity Framework at our new shiny database. So let&#x27;s fire up Visual Studio (I&#x27;m using 2013) and create a new solution called &quot;AdventureWorks&quot;.</p><p>To our solution let&#x27;s add a new class library project called &quot;AdventureWorks.EntityFramework&quot;. And to that we&#x27;ll add an ADO.NET Entity Data Model which we&#x27;ll call &quot;AdventureWorks.edmx&quot;. When the wizard fires up we&#x27;ll use the &quot;Generate from database&quot; option, click Next and select &quot;New Connection&quot;. In the dialog we&#x27;ll select our newly attached AdventureWorksLT2008R2 database. We&#x27;ll leave the &quot;save entity connection settings in App.Config&quot; option selected and click Next. I&#x27;m going to use Entity Framework 6.0 - though I think that any version would do. I&#x27;m going to pull in all tables / store procs and views. And now Entity Framework is pointing at my database:</p><p><img src="https://3.bp.blogspot.com/-Sv_GPsqilao/UuElIcLCYaI/AAAAAAAAAgw/7ui-xpml8dk/s400/EDMX.png"/></p><h2>Let There be Repositories!</h2><p>In the name of testability let&#x27;s create a new project to house repositories called &quot;AdventureWorks.Repositories&quot;. I&#x27;m going to use <a href="http://odetocode.com/about/scott-allen">K. Scott Allen</a>&#x27;s fine <a href="http://msdn.microsoft.com/en-us/library/ff714955.aspx">article on MSDN</a> to create a very basic set of repositories wrapped in a unit of work.</p><p>In my new project I&#x27;ll add a reference to the <code>AdventureWorks.EntityFramework</code> project and create a new <code>IRepository</code> interface that looks like this:</p><pre><code class="language-cs">using System;
using System.Linq;
using System.Linq.Expressions;

namespace AdventureWorks.Repositories
{
    public interface IRepository&lt;T&gt; where T : class
    {
        IQueryable&lt;T&gt; FindAll();
        IQueryable&lt;T&gt; FindWhere(Expression&lt;Func&lt;T, bool&gt;&gt; predicate);
        T Add(T newEntity);
        T Remove(T entity);
    }
}
</code></pre><p>And a new <code>IUnitOfWork</code> interface that looks like this:</p><pre><code class="language-cs">using AdventureWorks.EntityFramework;

namespace AdventureWorks.Repositories
{
    public interface IUnitOfWork
    {
        public IRepository&lt;ErrorLog&gt; ErrorLogs { get; }
        public IRepository&lt;Address&gt; Addresses { get; }
        public IRepository&lt;Customer&gt; Customers { get; }
        public IRepository&lt;CustomerAddress&gt; CustomerAddresses { get; }
        public IRepository&lt;Product&gt; Products { get; }
        public IRepository&lt;ProductCategory&gt; ProductCategories { get; }
        public IRepository&lt;ProductDescription&gt; ProductDescriptions { get; }
        public IRepository&lt;ProductModel&gt; ProductModels { get; }
        public IRepository&lt;ProductModelProductDescription&gt; ProductModelProductDescriptions { get; }
        public IRepository&lt;SalesOrderDetail&gt; SalesOrderDetails { get; }
        public IRepository&lt;SalesOrderHeader&gt; SalesOrderHeaders { get; }
        public IRepository&lt;BuildVersion&gt; BuildVersions { get; }

        void Commit();
    }
}
</code></pre><p>Now for the implementation of <code>IRepository</code>. For this we&#x27;ll need a reference to Entity Framework in our project. Then we&#x27;ll create a class called <code>SqlRepository</code>:</p><pre><code class="language-cs">using System;
using System.Data.Entity;
using System.Linq;
using System.Linq.Expressions;

namespace AdventureWorks.Repositories
{
    public class SqlRepository&lt;T&gt; : IRepository&lt;T&gt; where T : class
    {
        public SqlRepository(DbContext context)
        {
            _dbSet = context.Set&lt;T&gt;();
        }

        public IQueryable&lt;T&gt; FindAll()
        {
            return _dbSet;
        }

        public IQueryable&lt;T&gt; FindWhere(Expression&lt;Func&lt;T, bool&gt;&gt; predicate)
        {
            return _dbSet.Where(predicate);
        }

        public T Add(T newEntity)
        {
            return _dbSet.Add(newEntity);
        }

        public T Remove(T entity)
        {
            return _dbSet.Remove(entity);
        }

        protected DbSet&lt;T&gt; _dbSet;
    }
}
</code></pre><p>And we also need the implementation of <code>IUnitOfWork</code>. So we&#x27;ll create a class called <code>SqlUnitOfWork</code>:</p><pre><code class="language-cs">using System;
using System.Linq;
using System.Data.Entity;
using AdventureWorks.EntityFramework;

namespace AdventureWorks.Repositories
{
    public class SqlUnitOfWork : IUnitOfWork
    {
        public SqlUnitOfWork()
        {
            _context = new AdventureWorksLT2008R2Entities();
        }

        public IRepository&lt;ErrorLog&gt; ErrorLogs
        {
            get
            {
                if (_errorLogs == null) _errorLogs = new SqlRepository&lt;ErrorLog&gt;(_context);
                return _errorLogs;
            }
        }

        public IRepository&lt;Address&gt; Addresses
        {
            get
            {
                if (_addresses == null) _addresses = new SqlRepository&lt;Address&gt;(_context);
                return _addresses;
            }
        }

        public IRepository&lt;Customer&gt; Customers
        {
            get
            {
                if (_customers == null) _customers = new SqlRepository&lt;Customer&gt;(_context);
                return _customers;
            }
        }

        public IRepository&lt;CustomerAddress&gt; CustomerAddresses
        {
            get
            {
                if (_customerAddresses == null) _customerAddresses = new SqlRepository&lt;CustomerAddress&gt;(_context);
                return _customerAddresses;
            }
        }

        public IRepository&lt;Product&gt; Products
        {
            get
            {
                if (_products == null) _products = new SqlRepository&lt;Product&gt;(_context);
                return _products;
            }
        }

        public IRepository&lt;ProductCategory&gt; ProductCategories
        {
            get
            {
                if (_productCategories == null) _productCategories = new SqlRepository&lt;ProductCategory&gt;(_context);
                return _productCategories;
            }
        }

        public IRepository&lt;ProductDescription&gt; ProductDescriptions
        {
            get
            {
                if (_productDescriptions == null) _productDescriptions = new SqlRepository&lt;ProductDescription&gt;(_context);
                return _productDescriptions;
            }
        }

        public IRepository&lt;ProductModel&gt; ProductModels
        {
            get
            {
                if (_productModels == null) _productModels = new SqlRepository&lt;ProductModel&gt;(_context);
                return _productModels;
            }
        }

        public IRepository&lt;ProductModelProductDescription&gt; ProductModelProductDescriptions
        {
            get
            {
                if (_productModelProductDescriptions == null) _productModelProductDescriptions = new SqlRepository&lt;ProductModelProductDescription&gt;(_context);
                return _productModelProductDescriptions;
            }
        }

        public IRepository&lt;SalesOrderDetail&gt; SalesOrderDetails
        {
            get
            {
                if (_salesOrderDetails == null) _salesOrderDetails = new SqlRepository&lt;SalesOrderDetail&gt;(_context);
                return _salesOrderDetails;
            }
        }

        public IRepository&lt;SalesOrderHeader&gt; SalesOrderHeaders
        {
            get
            {
                if (_salesOrderHeaders == null) _salesOrderHeaders = new SqlRepository&lt;SalesOrderHeader&gt;(_context);
                return _salesOrderHeaders;
            }
        }

        public IRepository&lt;BuildVersion&gt; BuildVersions
        {
            get
            {
                if (_buildVersions == null) _buildVersions = new SqlRepository&lt;BuildVersion&gt;(_context);
                return _buildVersions;
            }
        }

        public void Commit()
        {
            _context.SaveChanges();
        }

        SqlRepository&lt;ErrorLog&gt; _errorLogs = null;
        SqlRepository&lt;Address&gt; _addresses = null;
        SqlRepository&lt;Customer&gt; _customers = null;
        SqlRepository&lt;CustomerAddress&gt; _customerAddresses = null;
        SqlRepository&lt;Product&gt; _products = null;
        SqlRepository&lt;ProductCategory&gt; _productCategories = null;
        SqlRepository&lt;ProductDescription&gt; _productDescriptions = null;
        SqlRepository&lt;ProductModel&gt; _productModels = null;
        SqlRepository&lt;ProductModelProductDescription&gt; _productModelProductDescriptions = null;
        SqlRepository&lt;SalesOrderDetail&gt; _salesOrderDetails = null;
        SqlRepository&lt;SalesOrderHeader&gt; _salesOrderHeaders = null;
        SqlRepository&lt;BuildVersion&gt; _buildVersions = null;

        readonly DbContext _context;
    }
}
</code></pre><h2>And Now Let&#x27;s Start Integration Testing!</h2><p>Let&#x27;s create a new Unit Test project called &quot;AdventureWorks.Repositories.IntegrationTests&quot;. (And just to be clear: this is <!-- -->*<!-- -->not<!-- -->*<!-- --> a unit test project - it is an <strong><em>integration</em></strong> test project.) We&#x27;ll add a reference back to our <code>AdventureWorks.Repositories</code> project for the repositories and one back to <code>AdventureWorks.EntityFramework</code> for our domain models. And finally you&#x27;ll need a reference to Entity Framework in your IntegrationTest project as well as well.</p><p>We&#x27;ll copy across the <code>app.config</code> from <code>AdventureWorks.EntityFramework</code> to <code>AdventureWorks.Repositories.IntegrationTests</code> as it contains the database connection details. It&#x27;ll look something like this:</p><pre><code class="language-xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;
&lt;configuration&gt;
    &lt;configSections&gt;
        &lt;section name=&quot;entityFramework&quot; type=&quot;System.Data.Entity.Internal.ConfigFile.EntityFrameworkSection, EntityFramework, Version=6.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089&quot; requirePermission=&quot;false&quot; /&gt;
        &lt;!-- For more information on Entity Framework configuration, visit http://go.microsoft.com/fwlink/?LinkID=237468 --&gt;
    &lt;/configSections&gt;
    &lt;connectionStrings&gt;
        &lt;add name=&quot;AdventureWorksLT2008R2Entities&quot;
             connectionString=&quot;metadata=res://*/AdventureWorks.csdl|res://*/AdventureWorks.ssdl|res://*/AdventureWorks.msl;provider=System.Data.SqlClient;provider connection string=&amp;quot;data source=.;initial catalog=AdventureWorksLT2008R2;integrated security=True;MultipleActiveResultSets=True;App=EntityFramework&amp;quot;&quot;
             providerName=&quot;System.Data.EntityClient&quot; /&gt;
    &lt;/connectionStrings&gt;
    &lt;entityFramework&gt;
        &lt;defaultConnectionFactory type=&quot;System.Data.Entity.Infrastructure.SqlConnectionFactory, EntityFramework&quot; /&gt;
        &lt;providers&gt;
            &lt;provider invariantName=&quot;System.Data.SqlClient&quot; type=&quot;System.Data.Entity.SqlServer.SqlProviderServices, EntityFramework.SqlServer&quot; /&gt;
        &lt;/providers&gt;
    &lt;/entityFramework&gt;
&lt;/configuration&gt;
</code></pre><p>Now we&#x27;re ready for a test. We&#x27;ll add ourselves a class called <code>BuildVersionTests</code>:</p><pre><code class="language-cs">using System;
using System.Linq;
using System.Linq.Expressions;
using Microsoft.VisualStudio.TestTools.UnitTesting;

namespace AdventureWorks.Repositories.IntegrationTests
{
    [TestClass]
    public class BuildVersionTests
    {
        [TestMethod]
        public void BuildVersions_should_return_the_correct_version_information()
        {
            // Arrange
            var uow = new SqlUnitOfWork();

            // Act
            var buildVersions = uow.BuildVersions.FindAll().ToList();

            // Assert
            Assert.AreEqual(1, buildVersions.Count);
            Assert.AreEqual(&quot;10.00.80404.00&quot;, buildVersions[0].Database_Version);
            Assert.AreEqual(new DateTime(2008, 4, 4), buildVersions[0].ModifiedDate);
            Assert.AreEqual(1, buildVersions[0].SystemInformationID);
            Assert.AreEqual(new DateTime(2008, 4, 4), buildVersions[0].VersionDate);
        }
    }
}
</code></pre><p>This is as simple as it gets - our test creates a new unit of work and queries the <code>BuildVersions</code> table to see what we can see. All it&#x27;s really doing is demonstrating that we can now hit our database through our repositories. As a side note, we could have the exact same test operating directly on the <code>DbContext</code> like this:</p><pre><code class="language-cs">[TestMethod]
        public void DbContext_BuildVersions_should_return_the_correct_version_information()
        {
            // Arrange
            var dbContext = new AdventureWorks.EntityFramework.AdventureWorksLT2008R2Entities();

            // Act
            var buildVersions = dbContext.BuildVersions.ToList();

            // Assert
            Assert.AreEqual(1, buildVersions.Count);
            Assert.AreEqual(&quot;10.00.80404.00&quot;, buildVersions[0].Database_Version);
            Assert.AreEqual(new DateTime(2008, 4, 4), buildVersions[0].ModifiedDate);
            Assert.AreEqual(1, buildVersions[0].SystemInformationID);
            Assert.AreEqual(new DateTime(2008, 4, 4), buildVersions[0].VersionDate);
        }
</code></pre><p>For the most part we won&#x27;t be doing this but I wanted to be clear that full power of Entity Framework is available to you as you&#x27;re putting together your integration tests.</p><h2>Database Snapshotting Time</h2><p>Up until this point we&#x27;ve essentially been laying our infrastructure and doing our plumbing. We now have a database, domain models and data access courtesy of Entity Framework, a testable repository layer and finally an integration test pack. What we want now is to get our database snapshot / backup and restore mechanism set up and integrated into the test pack.</p><p>Let&#x27;s add references to the <code>System.Data</code> and <code>System.Configuration</code> assemblies to our integration testing project and then add a new class called <code>DatabaseSnapshot</code>:</p><pre><code class="language-cs">using System.Configuration;
using System.Data;
using System.Data.SqlClient;

namespace AdventureWorks.Repositories.IntegrationTests
{
    public static class DatabaseSnapshot
    {
        private const string SpCreateSnapShotName = &quot;SnapshotBackup_Create&quot;;
        private const string SpCreateSnapShot =
@&quot;CREATE PROCEDURE [dbo].[&quot; + SpCreateSnapShotName + @&quot;]
    @databaseName        varchar(512),
    @databaseLogicalName varchar(512),
    @snapshotBackupPath  varchar(512),
    @snapshotBackupName  varchar(512)
AS
BEGIN
    SET NOCOUNT ON;

    DECLARE @sql varchar(500)
    SELECT @sql = &#x27;CREATE DATABASE &#x27; + @snapshotBackupName +
                  &#x27; ON (NAME=[&#x27; + @databaseLogicalName +
                  &#x27;], FILENAME=&#x27;&#x27;&#x27; + @snapshotBackupPath + @snapshotBackupName +
                  &#x27;&#x27;&#x27;) AS SNAPSHOT OF [&#x27; + @databaseName + &#x27;]&#x27;
    EXEC(@sql)
END&quot;;

        private const string SpRestoreSnapShotName = &quot;SnapshotBackup_Restore&quot;;
        private const string SpRestoreSnapShot =
@&quot;CREATE PROCEDURE [dbo].[&quot; + SpRestoreSnapShotName + @&quot;]
    @databaseName varchar(512),
    @snapshotBackupName varchar(512)
AS
BEGIN
    SET NOCOUNT ON;

    DECLARE @sql varchar(500)
    SET @sql  = &#x27;ALTER DATABASE [&#x27; + @databaseName + &#x27;] SET SINGLE_USER WITH ROLLBACK IMMEDIATE&#x27;
    EXEC (@sql)

    RESTORE DATABASE @databaseName
    FROM DATABASE_SNAPSHOT = @snapshotBackupName

    SET @sql = &#x27;ALTER DATABASE [&#x27; + @databaseName + &#x27;] SET MULTI_USER&#x27;
    EXEC (@sql)
END&quot;;

        private const string SpDeleteSnapShotName = &quot;SnapshotBackup_Delete&quot;;
        private const string SpDeleteSnapShot =
@&quot;CREATE PROCEDURE [dbo].[&quot; + SpDeleteSnapShotName + @&quot;]
    @snapshotBackupName varchar(512)
AS
BEGIN
    SET NOCOUNT ON;

    DECLARE @sql varchar(500)

    SELECT @sql = &#x27;DROP DATABASE &#x27; + @snapshotBackupName
    EXEC(@sql)
END&quot;;

        private static string _masterDbConnectionString;
        private static string _dbName;
        private static ConnectionStringSettings _dbConnectionStringSettings;

        private static ConnectionStringSettings DbConnectionStringSettings
        {
            get
            {
                if (_dbConnectionStringSettings == null)
                    _dbConnectionStringSettings = ConfigurationManager.ConnectionStrings[&quot;SnapshotBackup&quot;];

                return _dbConnectionStringSettings;
            }
        }

        /// &lt;summary&gt;
        /// Stored procedures should be executed against master database
        /// &lt;/summary&gt;
        private static string MasterDbConnectionString
        {
            get
            {
                if (string.IsNullOrEmpty(_masterDbConnectionString))
                {
                    var sqlConnection = new SqlConnection(DbConnectionStringSettings.ConnectionString);
                    _masterDbConnectionString = DbConnectionStringSettings.ConnectionString.Replace(sqlConnection.Database, &quot;master&quot;);
                }
                return _masterDbConnectionString;
            }
        }

        private static string DbName
        {
            get
            {
                if (string.IsNullOrEmpty(_dbName))
                    _dbName = new SqlConnection(DbConnectionStringSettings.ConnectionString).Database.TrimStart(&#x27;[&#x27;).TrimEnd(&#x27;]&#x27;);

                return _dbName;
            }
        }

        public static void SetupStoredProcedures()
        {
            using (var conn = new SqlConnection(MasterDbConnectionString))
            {
                conn.Open();

                // Drop the existing stored procedures
                SqlCommand cmd;
                const string dropProcSql = &quot;IF EXISTS (SELECT * FROM sys.objects WHERE object_id = OBJECT_ID(N&#x27;[dbo].[{0}]&#x27;) AND type in (N&#x27;P&#x27;, N&#x27;PC&#x27;)) DROP PROCEDURE [dbo].[{0}]&quot;;
                foreach (var spName in new[] { SpCreateSnapShotName, SpDeleteSnapShotName, SpRestoreSnapShotName })
                {
                    cmd = new SqlCommand(string.Format(dropProcSql, spName), conn);
                    cmd.ExecuteNonQuery();
                }

                // Create the stored procedures anew
                foreach (var createProcSql in new[] { SpCreateSnapShot, SpDeleteSnapShot, SpRestoreSnapShot })
                {
                    cmd = new SqlCommand(createProcSql, conn);
                    cmd.ExecuteNonQuery();
                }

                conn.Close();
            }
        }

        public static void CreateSnapShot()
        {
            var databaseName = new SqlParameter { ParameterName = &quot;@databaseName&quot;, SqlValue = SqlDbType.VarChar, Value = DbName };
            var databaseLogicalName = new SqlParameter { ParameterName = &quot;@databaseLogicalName&quot;, SqlValue = SqlDbType.VarChar, Value = ConfigurationManager.AppSettings[&quot;DatabaseLogicalName&quot;] };
            var snapshotBackupPath = new SqlParameter { ParameterName = &quot;@snapshotBackupPath&quot;, SqlValue = SqlDbType.VarChar, Value = ConfigurationManager.AppSettings[&quot;SnapshotBackupPath&quot;] };
            var snapshotBackupName = new SqlParameter { ParameterName = &quot;@snapshotBackupName&quot;, SqlValue = SqlDbType.VarChar, Value = ConfigurationManager.AppSettings[&quot;SnapshotBackupName&quot;] };

            ExecuteStoredProcAgainstMaster(SpCreateSnapShotName, new[] { databaseName, databaseLogicalName, snapshotBackupPath, snapshotBackupName });
        }

        public static void DeleteSnapShot()
        {
            var snapshotBackupName = new SqlParameter { ParameterName = &quot;@snapshotBackupName&quot;, SqlValue = SqlDbType.VarChar, Value = ConfigurationManager.AppSettings[&quot;SnapshotBackupName&quot;] };

            ExecuteStoredProcAgainstMaster(SpDeleteSnapShotName, new[] { snapshotBackupName });
        }

        public static void RestoreSnapShot()
        {
            var databaseName = new SqlParameter { ParameterName = &quot;@databaseName&quot;, SqlValue = SqlDbType.VarChar, Value = DbName };
            var snapshotBackupName = new SqlParameter { ParameterName = &quot;@snapshotBackupName&quot;, SqlValue = SqlDbType.VarChar, Value = ConfigurationManager.AppSettings[&quot;SnapshotBackupName&quot;] };

            ExecuteStoredProcAgainstMaster(SpRestoreSnapShotName, new[] { databaseName, snapshotBackupName });
        }

        private static void ExecuteStoredProcAgainstMaster(string storedProc, SqlParameter[] parameters)
        {
            using (var conn = new SqlConnection(MasterDbConnectionString))
            {
                conn.Open();
                var cmd = new SqlCommand(storedProc, conn) { CommandType = CommandType.StoredProcedure };
                cmd.Parameters.AddRange(parameters);
                cmd.ExecuteNonQuery();
                conn.Close();
            }
        }
    }
}
</code></pre><p>The <code>DatabaseSnapshot</code> class exposes 4 methods:</p><dl><dt>SetupStoredProcedures</dt><dd>This method creates 3 stored procedures on the master database: <code>SnapshotBackup_Create</code>, <code>SnapshotBackup_Restore</code> and <code>SnapshotBackup_Delete</code>. These procs do pretty much what their names suggest and the other 3 methods call these stored procedures when creating, restoring and deleting snapshot backups respectively. You can see the (fairly minimal) SQL for these stored procs at the top of the<code>DatabaseSnapshot</code> class.</dd><dt>CreateSnapShot</dt><dd>This method creates a snapshot backup of the database at this point in time.</dd><dt>RestoreSnapShot</dt><dd>This method restores the database back to state it was in when the snapshot backup was created.</dd><dt>DeleteSnapShot</dt><dd>This method attempts to delete the existing snapshot backup.</dd></dl><p>In order that we can use the <code>DatabaseSnapshot</code> class we need to add the following entries to our <code>app.config</code>:</p><pre><code class="language-xml">&lt;configuration&gt;

    &lt;connectionStrings&gt;

        &lt;add name=&quot;SnapshotBackup&quot;
             connectionString=&quot;data source=.;initial catalog=AdventureWorksLT2008R2;Trusted_Connection=true;Connection Timeout=200&quot; /&gt;

    &lt;/connectionStrings&gt;

    &lt;appSettings&gt;
        &lt;add key=&quot;DatabaseLogicalName&quot; value=&quot;AdventureWorksLT2008_Data&quot; /&gt;
        &lt;add key=&quot;SnapshotBackupPath&quot; value=&quot;C:\DbSnapshots\&quot; /&gt;
        &lt;add key=&quot;SnapshotBackupName&quot; value=&quot;AdventureWorksLT2008R2_Snapshot&quot; /&gt;
    &lt;/appSettings&gt;
&lt;/configuration&gt;
</code></pre><p>These settings allow have the following purposes:</p><dl><dt>SnapshotBackup</dt><dd>A connection string that allows <code>DatabaseSnapshot</code> to connect to the database.</dd><dt>DatabaseLogicalName</dt><dd>The logical name of the database you want to backup. (This can be found on the Files tab of the Database Properties in SSMS)</dd><dt>SnapshotBackupPath</dt><dd>The location where the snapshot backup is to be stored. You need to make sure that this exists on your machine.</dd><dt>SnapshotBackupName</dt><dd>The name of the snapshot backup that will be created.</dd></dl><p>Now to make use of <code>DatabaseSnapshot</code>. Let&#x27;s add a new class called <code>SetUpTearDown</code>:</p><pre><code class="language-cs">using Microsoft.VisualStudio.TestTools.UnitTesting;

namespace AdventureWorks.Repositories.IntegrationTests
{
    [TestClass]
    public static class SetUpTearDown
    {
        [AssemblyInitialize]
        public static void TestRunInitialize(TestContext context)
        {
            try
            {
                // Try to delete the snapshot in case it was left over from aborted test runs
                DatabaseSnapshot.DeleteSnapShot();
            }
            catch { /* this should fail with snapshot does not exist */ }

            DatabaseSnapshot.SetupStoredProcedures();
            DatabaseSnapshot.CreateSnapShot();
        }


        [AssemblyCleanup]
        public static void TestRunCleanup()
        {
            DatabaseSnapshot.DeleteSnapShot();
        }
    }
}
</code></pre><p>At the start of the test run this will create a snapshot in case one doesn&#x27;t exist already. And at the end of the test run it will be a good citizen and delete the snapshot. We&#x27;ll also add an extra method to our <code>BuildVersionTests</code> class:</p><pre><code class="language-cs">namespace AdventureWorks.Repositories.IntegrationTests
{
    [TestClass]
    public class BuildVersionTests
    {
        // ...

        [TestCleanup]
        public void TestCleanup()
        {
            DatabaseSnapshot.RestoreSnapShot();
        }
    }
}
</code></pre><p>This will ensure that after each test runs the database will be restored back to the snapshot created in <code>SetUpTearDown</code>. Now if you re-run your tests, in between each test the restore back to the snapshot is taking place.</p><h2>Prove it!</h2><p>Of course the tests we have in place at present don&#x27;t actually change the data at all. So I could be lying. I&#x27;m not. Let&#x27;s prove it by adding one more class called <code>CustomerTests</code>:</p><pre><code class="language-cs">using System;
using System.Linq;
using System.Linq.Expressions;
using Microsoft.VisualStudio.TestTools.UnitTesting;
using AdventureWorks.EntityFramework;

namespace AdventureWorks.Repositories.IntegrationTests
{
    [TestClass]
    public class CustomerTests
    {
        [TestMethod]
        public void Should_change_a_customers_first_and_last_name()
        {
            // Arrange
            var uow = new SqlUnitOfWork();

            // Act
            var customer = uow.Customers.FindWhere(x =&gt; x.FirstName == &quot;Jay&quot; &amp;&amp; x.LastName == &quot;Adams&quot;).First();
            var customerId = customer.CustomerID;
            customer.FirstName = &quot;John&quot;;
            customer.LastName = &quot;Reilly&quot;;
            uow.Commit();

            // Assert
            Assert.IsNotNull(uow.Customers.FindWhere(x =&gt; x.FirstName == &quot;John&quot; &amp;&amp; x.LastName == &quot;Reilly&quot; &amp;&amp; x.CustomerID == customerId).SingleOrDefault());
        }

        [TestCleanup]
        public void TestCleanup()
        {
            DatabaseSnapshot.RestoreSnapShot();
        }
    }
}
</code></pre><p>The above test checks that you can look up an existing customer, Mr Jay Adams, and change his name to my name - to John Reilly. If I execute the test above and there was no restore in place then subsequently when I came to exercise this test it should start to fail as it no longer has a Mr Jay Adams to lookup. But with this restore mechanism in place I can execute this test repeatedly without worrying.</p><h2>Rounding off</h2><p>And that&#x27;s us finished - we now have a database snapshot restore mechanism in place. With this we can develop integration tests that thoroughly change the data in our database secure in the knowledge that once the test is complete our database will be restored back to it&#x27;s initial state.</p><p>Obviously there are other alternative approaches for integration testing available to that which I&#x27;ve laid out in this post. But I can imagine that this approach is very useful for applying to legacy applications that you might inherit and need to continue supporting. Also, this approach should fit in well with a continuous integration setup. It would be pretty straightforward to have database that existed purely for testing purposes against which all the integration tests could be set to run at the point of each check in.</p><p>Thanks to Marc Talary, Sandeep Deo and Tishul Vadher who all contributed to <code>DatabaseSnapshot</code>. Credit is also due to Google due to the hundreds of articles the team ended up reading on snapshot backups.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Upgrading to TypeScript 0.9.5 - A Personal Memoir]]></title>
            <link>https://blog.johnnyreilly.com/2014/01/09/upgrading-to-typescript-095-personal</link>
            <guid>Upgrading to TypeScript 0.9.5 - A Personal Memoir</guid>
            <pubDate>Thu, 09 Jan 2014 00:00:00 GMT</pubDate>
            <description><![CDATA[I recently made the step to upgrade from TypeScript 0.9.1.1 to 0.9.5. To my surprise this process was rather painful and certainly not an unalloyed pleasure. Since I'm now on the other side, so to speak, I thought I'd share my experience and cast back a rope bridge to those about to journey over the abyss.]]></description>
            <content:encoded><![CDATA[<p>I recently made the step to upgrade from TypeScript 0.9.1.1 to 0.9.5. To my surprise this process was rather painful and certainly not an unalloyed pleasure. Since I&#x27;m now on the other side, so to speak, I thought I&#x27;d share my experience and cast back a rope bridge to those about to journey over the abyss.</p><h2>TL;DR</h2><p>TypeScript 0.9.5 is worth making the jump to. However, if you are using Visual Studio (as I would guess many are) then you should be aware of a number of problems with the TypeScript Visual Studio tooling for TS 0.9.5. These problems can be worked around if you follow the instructions in this post.</p><h2>Upgrading the Plugin</h2><p>At home I upgraded the moment TS 0.9.5 was released. This allowed me to help with migrating the <a href="https://github.com/borisyankov/DefinitelyTyped">Definitely Typed typings</a> over from 0.9.1.1. And allowed me to give TS 0.9.5 a little test drive. However, I deliberately held off performing the upgrade at work until I knew that all the Definitely Typed typings had been upgraded. This was completed <a href="https://github.com/borisyankov/DefinitelyTyped/pull/1385">by the end of 2013</a>. So in the new year it seemed a good time to make the move.</p><p>If, like me, you are using TypeScript inside Visual Studio then you&#x27;d imagine it&#x27;s as simple as closing down VS, uninstalling TypeScript 0.9.1.1 from Programs and Features and then installing the <a href="http://www.typescriptlang.org/#Download">new plugin</a>. And it is if you are running IE 10 or IE 11 on your Windows machine. If you are running a lower IE version then there is a problem.</p><p>Regrettably, the TypeScript 0.9.5 plugin installer has a dependency on IE 10. Fortunately TypeScript itself has no dependency on IE 10 at all (and why would it?). This dependency appears to have been a mistake. I <a href="https://typescript.codeplex.com/workitem/1975">raised it as an issue</a> and the TS team have said that this will be resolved in the next major release.</p><p>Happily there is a workaround if you&#x27;re running IE 9 or lower which has been noted in the <a href="https://blogs.msdn.com/b/typescript/archive/2013/12/05/announcing-typescript-0-9-5.aspx">comments underneath the TS 0.9.5 release blog post</a>. All you do is set the <code>HKEY_LOCAL_MACHINE\SOFTWARE\Wow6432Node\Microsoft\Internet Explorer\svcVersion</code> registry key value to <code>10.0.9200.16384</code> for the duration of the install.</p><p>First hurdle jumped, the upgrade continues simple enough. Then the fun starts...</p><h2>Declaration Merging is dead... Sort of</h2><p>Having upgraded my plugin I opened up the project I&#x27;m working on in Visual Studio. I used NuGet to upgrade all the Definitely Typed packages to the latest (TS 0.9.5) versions. Then I tried, and failed, to compile. It was the the most obscure error I&#x27;ve seen in a while:</p><pre><code class="language-ts">VSTSC : tsc.js(37574, 25) Microsoft JScript runtime error : Unable to get value of the property &#x27;wrapsSomeTypeParameter&#x27;: object is null or undefined
</code></pre><p>As you can see there was no indication where in my code the problem was being caused. Fortunately someone had already suffered this particular problem and logged an issue <a href="https://typescript.codeplex.com/workitem/1995">here</a>. Digging through the comments I found a common theme; everyone experiencing the problem was using the <a href="https://github.com/borisyankov/DefinitelyTyped/blob/master/q/Q.d.ts">Q typings</a>. So what&#x27;s up with that?</p><p>Strangely, if you directly referenced the Q typings everything was okay - which is how the Definitely Typed tests came to pass in the first place. But if you wanted to make use of these typings with implicit referencing (in Visual Studio since TS 0.9.1, all TypeScript files in a project are considered to be referencing each other) - well it doesn&#x27;t work.</p><p>I decided to take a look at the Q typings at this point to see what was so upsetting about them. The one thing that was obvious was that these typings make use of <a href="https://blogs.msdn.com/b/typescript/archive/2013/06/18/announcing-typescript-0-9.aspx">Declaration Merging</a>. And this made them slightly different to most of the other typing libraries that I was using. So I decided to refactor the Q typings to use the more interface driven approach the other typing libraries used in the hope that might resolve the issue.</p><p>Roughly speaking I went from:</p><pre><code class="language-ts">declare function Q&lt;T&gt;(promise: Q.IPromise&lt;T&gt;): Q.Promise&lt;T&gt;;
declare function Q&lt;T&gt;(promise: JQueryPromise&lt;T&gt;): Q.Promise&lt;T&gt;;
declare function Q&lt;T&gt;(value: T): Q.Promise&lt;T&gt;;

declare module Q {
  //… functions etc in here
}

declare module &#x27;q&#x27; {
  export = Q;
}
</code></pre><p>To:</p><pre><code class="language-ts">interface QIPromise&lt;T&gt; {
    //… functions etc in here
}

interface QDeferred&lt;T&gt; {
    //… functions etc in here
}

interface QPromise&lt;T&gt; {
    //… functions etc in here
}

interface QPromiseState&lt;T&gt; {
    //… functions etc in here
}

interface QStatic {

    &lt;t&gt;(promise: QIPromise&lt;T&gt;): QPromise&lt;T&gt;;
    &lt;t&gt;(promise: JQueryPromise&lt;T&gt;): QPromise&lt;T&gt;;
    &lt;t&gt;(value: T): QPromise&lt;T&gt;;

    //… other functions etc continue here
}

declare module &quot;q&quot; {
    export = Q;
}
declare var Q: QStatic;
&lt;/t&gt;&lt;/t&gt;&lt;/t&gt;
</code></pre><p>And that fixed the obscure &#x27;wrapsSomeTypeParameter&#x27; error. The full source code of these amended typings can be found as a GitHub Repo <a href="https://github.com/johnnyreilly/Q-TS-0.9.5-WorkAround">here</a> in case you want to use it yourself. (I did originally consider adding this to Definitely Typed but opted not to in the end - <a href="https://github.com/borisyankov/DefinitelyTyped/pull/1497">see discussion on GitHub</a>.)</p><h2>The Promised Land</h2><p>You&#x27;re there. You&#x27;ve upgraded to the new plugin and the new typings. All is compiling as it should and the language service is working as well. Was it worth it? I think yes, for the following reasons:</p><ol><li>TS 0.9.5 compiles faster, and hogs less memory.</li><li>When we compiled with TS 0.9.5 we found there were a couple of bugs in our codebase which the tightened up compiler was now detecting. Essentially where we&#x27;d assumed types were flowing through to functions there were a couple of occasions with TS 0.9.1.1 where they weren&#x27;t. Where we&#x27;d assumed we had a type of <code>T</code> available in a function whereas it was actually a type of <code>any</code>. I was really surprised that this was the case since we were already making use of <code>noImplicitAny</code> compiler flag in our project. So where a type had changed and a retired property was being referenced TS 0.9.5 picked up an error that TS 0.9.1.1 had not. Good catch!</li><li>And finally (and I know these are really minor), the compiled JS is a little different now. Firstly, the compiled JS features all of TypeScript comments in the positions that you might hope for. Previously it seemed that about 75% came along for the ride and ended up in some strange locations sometimes. Secondly, enums are treated differently during compilation now - where it makes sense the actual backing value of an enum is used rather than going through the JavaScript construct. So it&#x27;s a bit like a <code>const</code> I guess - presumably this allows JavaScript engines to optimise a little more.</li></ol><p>I hope I haven&#x27;t put you off with this post. I think TypeScript 0.9.5 is well worth making the leap for - and hopefully by reading this you&#x27;ll have saved yourself from a few of the rough edges.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[NuGet and WebMatrix: How to install a specific version of a package]]></title>
            <link>https://blog.johnnyreilly.com/2013/12/13/nuget-and-webmatrix-how-to-install</link>
            <guid>NuGet and WebMatrix: How to install a specific version of a package</guid>
            <pubDate>Fri, 13 Dec 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[I've recently been experimenting with WebMatrix. If you haven't heard of it, WebMatrix is Microsoft's "free, lightweight, cloud-connected web development tool". All marketing aside, it's pretty cool. You can whip up a site in next to no time, it has source control, publishing abilities, intellisense. Much good stuff. And one thing it has, that I genuinely hadn't expected is NuGet. Brilliant!]]></description>
            <content:encoded><![CDATA[<p>I&#x27;ve recently been experimenting with WebMatrix. If you haven&#x27;t heard of it, WebMatrix is Microsoft&#x27;s <em><a href="http://www.microsoft.com/web/webmatrix/">&quot;free, lightweight, cloud-connected web development tool&quot;</a></em>. All marketing aside, it&#x27;s pretty cool. You can whip up a site in next to no time, it has source control, publishing abilities, intellisense. Much good stuff. And one thing it has, that I genuinely hadn&#x27;t expected is <a href="https://www.nuget.org/">NuGet</a>. Brilliant!</p><p>But like any free product there are disadvantages. As a long time Visual Studio user I&#x27;ve become very used to the power of the NuGet command line. I&#x27;ve been spoiled. You don&#x27;t have this in WebMatrix. You have a nice UI that looks like this:</p><p><img src="https://2.bp.blogspot.com/-MLAAVw9-O_A/UqstzAa1-8I/AAAAAAAAAfU/gtg8kPjsP7M/s400/NuGetWebMatrix.png"/></p><p>Looks great right? However, if you want to install a specific version of a NuGet package... well let&#x27;s see what happens...</p><p>As you&#x27;re probably aware jQuery currently exists in 2 branches; the 1.10.x branch which supports IE 6-8 and the 2.0.x branch which doesn&#x27;t. However there is only 1 jQuery inside NuGet. Let&#x27;s click on install and see if we can select a specific version:</p><p><img src="https://4.bp.blogspot.com/-Phqw0WYN0BM/UqswJPr7X1I/AAAAAAAAAfg/4lpkwUG5p5w/s400/NuGetWebMatrixjQuery.png"/></p><p>Hmmm.... As you can see it&#x27;s 2.0.3 or bust. We can&#x27;t select a specific version; we&#x27;re forced to go with the latest and greatest which is a problem if you need to support IE 6-8. So the obvious strategy if you&#x27;re in this particular camp is to forego NuGet entirely. Go old school. And we could. But let&#x27;s say we want to keep using NuGet, mindful that a little while down the road we&#x27;ll be ready to do that upgrade. Can it be done? Let&#x27;s find out.</p><h2>NuGet, by hook or by crook</h2><p>I&#x27;ve created a new site in WebMatrix using the Empty Site template. Looks like this:</p><p><img src="https://4.bp.blogspot.com/-yDf_KCHWImA/Uqs8Csn8UWI/AAAAAAAAAfs/cmnj5ddqDCk/s400/EmptySite.png"/></p><p>Lovely.</p><p>Now to get me some jQuery 1.10.2 goodness. To the console Batman! We&#x27;ve already got the NuGet command line installed (if you haven&#x27;t you could get it from <a href="http://nuget.org/nuget.exe">here</a>) and so we follow these steps:</p><ul><li>At the <code>C:\</code> prompt we enter <code>nuget install jQuery -Version 1.10.2</code> and down comes jQuery 1.10.2.</li><li>We move <code>C:\jQuery.1.10.2</code> to <code>C:\Users\me\Documents\My Web Sites\Empty Site\App_Data\packages\jQuery.1.10.2</code>.</li><li>Then we delete the <code>C:\Users\me\Documents\My Web Sites\Empty Site\App_Data\packages\jQuery.1.10.2\Tools</code> subfolder.</li><li>We move <code>C:\Users\me\Documents\My Web Sites\Empty Site\App_Data\packages\jQuery.1.10.2\Content\Scripts</code> to <code>C:\Users\me\Documents\My Web Sites\Empty Site\Scripts</code>.</li><li>And finally we delete the <code>C:\Users\me\Documents\My Web Sites\Empty Site\App_Data\packages\jQuery.1.10.2\Content</code> folder.</li></ul><p>We hit refresh back in WebMatrix and now we get this:</p><p><img src="https://4.bp.blogspot.com/-EAfCq2zjNl4/UqtAvAW35PI/AAAAAAAAAf4/u97kbdXWO84/s400/EmptySiteWithjQuery.png"/></p><p>If we go to NuGet and select updates you&#x27;ll see that jQuery is now considered &quot;installed&quot; and an update is available. So, in short, our plan worked - yay!</p><p><img src="https://3.bp.blogspot.com/-3-pJuMZVJPo/UqtBiuBNdyI/AAAAAAAAAgE/GA_4difKXdQ/s320/NuGetWebMatrixjQueryUpgrade.png"/></p><h2>Now for bonus points</h2><p>Just to prove that you can upgrade using the WebMatrix tooling following our manual install let&#x27;s do it. Click &quot;Update&quot;, then &quot;Yes&quot; and finally &quot;I Accept&quot; to the EULA. You&#x27;ll now see we&#x27;re now on jQuery 2.0.3:</p><p><img src="https://4.bp.blogspot.com/-6tLukWlzBfg/UqtE_Ni4FaI/AAAAAAAAAgM/AUxszwXhlGo/s400/NuGetWebMatrixjQueryUpgraded.png"/></p><h2>Rounding off</h2><p>In my example I&#x27;m only looking at a simple JavaScript library. But the same principal should be able to be applied to any NuGet package as far as I&#x27;m aware. Hope that helps!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Simple fading in and out using CSS transitions and classes]]></title>
            <link>https://blog.johnnyreilly.com/2013/12/04/simple-fading-in-and-out-using-css-transitions</link>
            <guid>Simple fading in and out using CSS transitions and classes</guid>
            <pubDate>Wed, 04 Dec 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[Caveat emptor folks... Let me start off by putting my hands up and saying I am no expert on CSS. And furthermore let me say that this blog post is essentially the distillation of a heady session of googling on the topic of CSS transitions. The credit for the technique detailed here belongs to many others, I'm just documenting it for my own benefit (and for anyone who stumbles upon this).]]></description>
            <content:encoded><![CDATA[<p>Caveat emptor folks... Let me start off by putting my hands up and saying I am no expert on CSS. And furthermore let me say that this blog post is essentially the distillation of a heady session of googling on the topic of CSS transitions. The credit for the technique detailed here belongs to many others, I&#x27;m just documenting it for my own benefit (and for anyone who stumbles upon this).</p><h2>What do we want to do?</h2><p>Most web developers have likely reached at some point for jQuery&#x27;s <a href="http://api.jquery.com/fadeIn/"><code>fadeIn</code></a> and <a href="http://api.jquery.com/fadeOut/"><code>fadeOut</code></a> awesomeness. What could be cooler than fading in or out your UI, right?</p><p>Behind the scenes of <code>fadeIn</code> and <code>fadeOut</code> JavaScript is doing an awful lot of work to create that animation. And in our modern world we simply don&#x27;t need to do that work anymore; it&#x27;s gone native and is covered by <a href="https://developer.mozilla.org/en-US/docs/Web/Guide/CSS/Using_CSS_transitions">CSS transitions</a>.</p><p>Added to the &quot;<a href="http://en.wikipedia.org/wiki/George_Mallory">because it&#x27;s there</a>&quot; reason for using CSS transitions to do fading there is a more important reason; let me quote <a href="http://www.html5rocks.com/en/tutorials/speed/html5/#toc-css3-transitions">HTML5 rocks</a>:</p><blockquote><p>&quot;<em>CSS Transitions make style animation trivial for everyone, but they also are a smart performance feature. Because a CSS transition is managed by the browser, the fidelity of its animation can be greatly improved, and in many cases hardware accelerated. Currently WebKit (Chrome, Safari, iOS) have hardware accelerated CSS transforms, but it&#x27;s coming quickly to other browsers and platforms.</em>&quot;</p></blockquote><p>Added to this, if you have mobile users then the usage of native functionality (as opposed to doing it manually in JavaScript) actually saves battery life.</p><h2>I&#x27;m sold - let&#x27;s do it!</h2><p>This is the CSS we&#x27;ll need:</p><pre><code class="language-css">.fader {
  -moz-transition: opacity 0.7s linear;
  -o-transition: opacity 0.7s linear;
  -webkit-transition: opacity 0.7s linear;
  transition: opacity 0.7s linear;
}

.fader.fadedOut {
  opacity: 0;
}
</code></pre><p>Note we have 2 CSS classes:</p><ul><li><code>fader</code> <!-- -->-<!-- --> if this class is applied to an element then when the opacity of that element is changed it will be an animated change. The duration of the transition and the timing function used are customisable - in this case it takes 0.7 seconds and is linear.</li><li><code>fadedOut</code> <!-- -->-<!-- --> when used in conjunction with <code>fader</code> this class creates a fading in or fading out effect as it is removed or applied respectively. (This relies upon the default value of opacity being 1.)</li></ul><p>Let&#x27;s see it in action:</p><iframe width="100%" height="200" src="https://jsfiddle.net/johnny_reilly/86amq/embedded/result,js,html,css" allowfullscreen="" frameBorder="0"></iframe><p>It goes without saying that one day in the not too distant future (I hope) we&#x27;ll be able to leave behind the horrible world of vendor prefixes. Then we&#x27;ll be down to just the single <code>transition</code> statement. One day...</p><h2>Now, a warning...</h2><p>Unfortunately the technique detailed above differs from <a href="http://api.jquery.com/fadeIn/"><code>fadeIn</code></a> and <a href="http://api.jquery.com/fadeOut/"><code>fadeOut</code></a> in one important way. When the <code>fadeOut</code> animation completes it sets removes the element from the flow of the DOM using <code>display: none</code>. However, display is not a property that can be animated and so you can&#x27;t include this in your CSS transition. If removing the element from the flow of the DOM is something you need then you&#x27;ll need to bear this in mind. If anyone has any suggestions for an nice way to approach this I&#x27;d love to hear from you.</p><h2>A halfway there solution to the <code>display: none</code></h2><p>Andrew Davey tweeted me the suggestion below:</p><blockquote><p><a href="https://twitter.com/johnny_reilly">@johnny_reilly</a> Yep, transitions are sweet. You could use the transitionend event to remove the element from the DOM <a href="http://t.co/Q1oWy3g8Lp">http://t.co/Q1oWy3g8Lp</a></p><p>— Andrew Davey (@andrewdavey) <a href="https://twitter.com/andrewdavey/statuses/408545283606212608">December 5, 2013</a></p></blockquote><script src="//platform.twitter.com/widgets.js" charSet="utf-8"></script><p>So I thought I&#x27;d give it a go. However, whilst we&#x27;ve a <code>transitionend</code> event to play with we don&#x27;t have a corresponding <code>transitionstart</code> or <code>transitionbegin</code>. So I tried this:</p><pre><code class="language-js">$(&#x27;#showHideButton&#x27;).click(function () {
  var $alertDiv = $(&#x27;#alertDiv&#x27;);
  if ($alertDiv.hasClass(&#x27;fadedOut&#x27;)) {
    $alertDiv.removeClass(&#x27;fadedOut&#x27;).css(&#x27;display&#x27;, &#x27;&#x27;);
  } else {
    $(&#x27;#alertDiv&#x27;).addClass(&#x27;fadedOut&#x27;);
  }
});

$(document).on(
  &#x27;webkitTransitionEnd transitionend oTransitionEnd&#x27;,
  &#x27;.fader&#x27;,
  function (evnt) {
    var $faded = $(evnt.target);
    if ($faded.hasClass(&#x27;fadedOut&#x27;)) {
      $faded.css(&#x27;display&#x27;, &#x27;none&#x27;);
    }
  }
);
</code></pre><p>Essentially, on the <code>transitionend</code> event <code>display: none</code> is applied to the element in question. Groovy. In the absence of a <code>transitionstart</code> or <code>transitionbegin</code>, when removing the <code>fadeOut</code> class I&#x27;m first manually clearing out the <code>display: none</code>. Whilst this works in terms of adding it back into the flow of the DOM it takes away all the <code>fadeIn</code> gorgeousness. So it&#x27;s not quite the fully featured solution you might hope for. But it&#x27;s a start.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Rolling your own confirm mechanism using Promises and jQuery UI]]></title>
            <link>https://blog.johnnyreilly.com/2013/11/26/rolling-your-own-confirm-mechanism</link>
            <guid>Rolling your own confirm mechanism using Promises and jQuery UI</guid>
            <pubDate>Tue, 26 Nov 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[It is said that a picture speaks a thousand words. So here's two:]]></description>
            <content:encoded><![CDATA[<p>It is said that a picture speaks a thousand words. So here&#x27;s two:</p><p><img src="https://4.bp.blogspot.com/-zZvqgKiP9CI/UpN7YtkFbnI/AAAAAAAAAe4/OUpA5uVpCl4/s400/Ugly.png"/></p><p><img src="https://4.bp.blogspot.com/-VVzJ7B0Uhys/UpN7vnX7diI/AAAAAAAAAe8/i3hlMT1ECB8/s400/Pretty.png"/></p><p>That&#x27;s right, we&#x27;re here to talk about the <a href="https://developer.mozilla.org/en-US/docs/Web/API/Window.confirm">confirm</a> dialog. Or, more specifically, how we can make our own confirm dialog.</p><p>JavaScript in the browser has had the <code>window.confirm</code> method for the longest time. This method takes a string as an argument and displays it in the form of a dialog, giving the user the option to click on either an &quot;OK&quot; or a &quot;Cancel&quot; button. If the user clicks &quot;OK&quot; the method returns <code>true</code>, if the user clicks &quot;Cancel&quot; the method returns <code>false</code>.</p><p><code>window.confirm</code> is wonderful in one way - it has a simple API which is easy to grok. But regardless of the browser, <code>window.confirm</code> is always as ugly as sin. Look at the first picture in this blog post; hideous. Or, put more dispassionately, it&#x27;s not terribly configurable; want to change the button text? You can&#x27;t. Want to change the styling of the dialog? You can&#x27;t. You get the picture.</p><h2>Making confirm 2.0</h2><p><a href="http://jqueryui.com/dialog/#modal-confirmation">jQuery UI&#x27;s dialog</a> has been around for a long time. I&#x27;ve been using it for a long time. But, if you look at the API, you&#x27;ll see it works in a very different way to <code>window.confirm</code> <!-- -->-<!-- --> basically it&#x27;s all about the callbacks. My intention was to create a mechanism which allowed me to prompt the user with jQuery UI&#x27;s tried and tested dialog, but to expose it in a way that embraced the simplicity of the <code>window.confirm</code> API.</p><p>How to do this? Promises! To quote <a href="http://martinfowler.com/bliki/JavascriptPromise.html">Martin Fowler</a> (makes you look smart when you do that):</p><blockquote><p><em>&quot;In Javascript, promises are objects which represent the pending result of an asynchronous operation. You can use these to schedule further activity after the asynchronous operation has completed by supplying a callback.&quot;</em></p></blockquote><p>When we show our dialog we are in asynchronous land; waiting for the user to click &quot;OK&quot; or &quot;Cancel&quot;. When they do, we need to act on their response. So if our custom confirm dialog returns a promise of a boolean (<code>true</code> when the users click &quot;OK&quot;, <code>false</code> otherwise) then that should be exactly what we need. I&#x27;m going to use <a href="https://github.com/kriskowal/q">Q</a> for promises. (Nothing particularly special about Q - it&#x27;s one of many <a href="https://github.com/promises-aplus/promises-spec/blob/master/implementations.md">Promises / A+</a> compliant implementations available.)</p><p>Here&#x27;s my custom confirm dialog:</p><pre><code class="language-js">/**
 * Show a &quot;confirm&quot; dialog to the user (using jQuery UI&#x27;s dialog)
 *
 * @param {string} message The message to display to the user
 * @param {string} okButtonText OPTIONAL - The OK button text, defaults to &quot;Yes&quot;
 * @param {string} cancelButtonText OPTIONAL - The Cancel button text, defaults to &quot;No&quot;
 * @param {string} title OPTIONAL - The title of the dialog box, defaults to &quot;Confirm...&quot;
 * @returns {Q.Promise&lt;boolean&gt;} A promise of a boolean value
 */
function confirmDialog(message, okButtonText, cancelButtonText, title) {
  okButtonText = okButtonText || &#x27;Yes&#x27;;
  cancelButtonText = cancelButtonText || &#x27;No&#x27;;
  title = title || &#x27;Confirm...&#x27;;

  var deferred = Q.defer();
  $(&#x27;&lt;div title=&quot;&#x27; + title + &#x27;&quot;&gt;&#x27; + message + &#x27;&lt;/div&gt;&#x27;).dialog({
    modal: true,
    buttons: [
      {
        // The OK button
        text: okButtonText,
        click: function () {
          // Resolve the promise as true indicating the user clicked &quot;OK&quot;
          deferred.resolve(true);
          $(this).dialog(&#x27;close&#x27;);
        },
      },
      {
        // The Cancel button
        text: cancelButtonText,
        click: function () {
          $(this).dialog(&#x27;close&#x27;);
        },
      },
    ],
    close: function (event, ui) {
      // Destroy the jQuery UI dialog and remove it from the DOM
      $(this).dialog(&#x27;destroy&#x27;).remove();

      // If the promise has not yet been resolved (eg the user clicked the close icon)
      // then resolve the promise as false indicating the user did *not* click &quot;OK&quot;
      if (deferred.promise.isPending()) {
        deferred.resolve(false);
      }
    },
  });

  return deferred.promise;
}
</code></pre><p>What&#x27;s happening here? Well first of all, if <code>okButtonText</code>, <code>cancelButtonText</code> or <code>title</code> have false-y values then they are initialised to defaults. Next, we create a deferred object with Q. Then we create our modal dialog using jQuery UI. There&#x27;s a few things worth noting about this:</p><ul><li>We&#x27;re not dependent on the dialog markup being in our HTML from the off. We create a brand new element which gets added to the DOM when the dialog is created. (I draw attention to this as the jQuery UI dialog documentation doesn&#x27;t mention that you can use this approach - and frankly I prefer it.)</li><li>The &quot;OK&quot; and &quot;Cancel&quot; buttons are initialised with the string values stored in <code>okButtonText</code> and <code>cancelButtonText</code>. So by default, &quot;Yes&quot; and &quot;No&quot;.</li><li>If the user clicks the &quot;OK&quot; button then the promise is resolved with a value of <code>true</code>.</li><li>If the dialog closes and the promise has not been resolved then the promise is resolved with a value of <code>false</code>. This covers people clicking on the &quot;Cancel&quot; button as well as closing the dialog through other means.</li></ul><p>Finally we return the promise from our deferred object.</p><h2>Going from <code>window.confirm</code> to <code>confirmDialog</code></h2><p>It&#x27;s very simple to move from using <code>window.confirm</code> to <code>confirmDialog</code>. Take this example:</p><pre><code class="language-js">if (window.confirm(&#x27;Are you sure?&#x27;)) {
  // Do something
}
</code></pre><p>Becomes:</p><pre><code class="language-js">confirmDialog(&#x27;Are you sure?&#x27;).then(function (confirmed) {
  if (confirmed) {
    // Do something
  }
});
</code></pre><p>There&#x27;s no more to it than that.</p><h2>And finally a demo...</h2><p>With the JSFiddle below you can create your own custom dialogs and see the result of clicking on either the &quot;OK&quot; or &quot;Cancel&quot; buttons.</p><iframe width="100%" height="500" src="https://jsfiddle.net/johnny_reilly/ARWL5/embedded/result,js,html,css" allowfullscreen="" frameBorder="0"></iframe>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[TypeScript: Don't forget Build Action for Implicit Referencing...]]></title>
            <link>https://blog.johnnyreilly.com/2013/11/04/typescript-dont-forget-build-action-for-implicit-referencing</link>
            <guid>TypeScript: Don't forget Build Action for Implicit Referencing...</guid>
            <pubDate>Mon, 04 Nov 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[As part of the known breaking changes between 0.9 and 0.9.1 there was this subtle but significant switch:]]></description>
            <content:encoded><![CDATA[<p>As part of the <a href="https://typescript.codeplex.com/wikipage?title=Known%20breaking%20changes%20between%200.8%20and%200.9&amp;referringTitle=Documentation">known breaking changes between 0.9 and 0.9.1</a> there was this subtle but significant switch:</p><blockquote><p>In Visual Studio, all TypeScript files in a project are considered to be referencing each other</p><p><em>Description:</em> Previously, all TypeScript files in a project had to reference each other explicitly. With 0.9.1, they now implicitly reference all other TypeScript files in the project. For existing projects that fit multiple projects into a single projects, these will now have to be separate projects.</p><p><em>Reason:</em> This greatly simplifies using TypeScript in the project context.</p></blockquote><p>Having been <a href="https://typescript.codeplex.com/workitem/1471">initially resistant</a> to this change I recently decided to give it a try. That is to say I started pulling out the <code>/// &amp;lt;reference</code>&#x27;s from my TypeScript files. However, to my surprise, pulling out these references stopped my TypeScript from compiling and killed my Intellisense. After wrestling with this for a couple of hours I finally <a href="https://typescript.codeplex.com/workitem/1855">filed an issue on the TypeScript CodePlex site</a>. (Because clearly the problem was with TypeScript and not how I was using it, right?)</p><h2>Wrong!</h2><p>When I looked through my typing files (<!-- -->*<!-- -->.d.ts) I found that, pretty much without exception, all had a Build Action of &quot;Content&quot; and not &quot;TypeScriptCompile&quot;. I went through the project and switched the files over to being &quot;TypeScriptCompile&quot;. This resolved the issue and I was then able to pull out the remaining <code>/// &amp;lt;reference</code> comments from the codebase (though I did have to restart Visual Studio to get the Intellisense working).</p><p>Most, if not all, of the typing files had been pulled in from NuGet and are part of the <a href="https://github.com/borisyankov/DefinitelyTyped">DefinitelyTyped</a> project on GitHub. Unfortunately, at present, when TypeScript NuGet packages are added they are added without the &quot;TypeScriptCompile&quot; Build Action. I was going to post an issue there and ask if it&#x27;s possible for NuGet packages to pull in typings files as &quot;TypeScriptCompile&quot; from the off - fortunately a chap called Natan Vivo <a href="https://github.com/borisyankov/DefinitelyTyped/issues/1138">already has</a>.</p><p>So until this issue is resolved it&#x27;s probably a good idea to check that your TypeScript files are set to the correct Build Action in your project. And every time you upgrade your TypeScript NuGet packages double check that you still have the correct Build Action afterwards (and to get Intellisense working in VS 2012 at least you&#x27;ll need to close and re-open the solution as well).</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Getting TypeScript Compile-on-Save and Continuous Integration to play nice]]></title>
            <link>https://blog.johnnyreilly.com/2013/10/30/getting-typescript-compile-on-save-and-continous-integration-to-play-nice</link>
            <guid>Getting TypeScript Compile-on-Save and Continuous Integration to play nice</guid>
            <pubDate>Wed, 30 Oct 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[Well sort of... Perhaps this post should more accurately called "How to get CI to ignore your TypeScript whilst Visual Studio still compiles it..."]]></description>
            <content:encoded><![CDATA[<p>Well sort of... Perhaps this post should more accurately called &quot;How to get CI to ignore your TypeScript whilst Visual Studio still compiles it...&quot;</p><h2>Once there was Web Essentials</h2><p>When I first started using TypeScript, I was using it in combination with Web Essentials. Those were happy days. I saved my TS file and Web Essentials would kick off TypeScript compilation. Ah bliss. But the good times couldn&#x27;t last forever and sure enough when version 3.0 of Web Essentials shipped it <a href="http://madskristensen.net/post/Web-Essentials-2013-Where-is-the-TypeScript-support">pulled support for TypeScript</a>.</p><p>This made me, <a href="https://typescript.codeplex.com/workitem/1616">and others</a>, very sad. Essentially we were given the choice between sticking with an old version of Web Essentials (2.9 - the last release before 3.0) and keeping our Compile-on-Save <!-- -->*<strong>or</strong>*<!-- --> keeping with the latest version of Web Essentials and losing it. And since I understood that newer versions of TypeScript had differences in the compiler flags which slightly broke compatibility with WE 2.9 the latter choice seemed the most sensible...</p><h2>But there is still Compile on Save hope!</h2><p>The information was that we need not lose our Compile on Save. We just need to follow the instructions <a href="https://typescript.codeplex.com/wikipage?title=Compile-on-Save">here</a>. Or to quote them:</p><blockquote><p>Then additionally add (or replace if you had an older PreBuild action for TypeScript) the following at the end of your project file to include TypeScript compilation in your project.</p><p>...</p><p>For C#-style projects (.csproj):</p><pre><code class="language-xml">&lt;PropertyGroup Condition=&quot;&#x27;$(Configuration)&#x27; == &#x27;Debug&#x27;&quot;&gt;
    &lt;TypeScriptTarget&gt;ES5&lt;/TypeScriptTarget&gt;
    &lt;TypeScriptIncludeComments&gt;true&lt;/TypeScriptIncludeComments&gt;
    &lt;TypeScriptSourceMap&gt;true&lt;/TypeScriptSourceMap&gt;
  &lt;/PropertyGroup&gt;
  &lt;PropertyGroup Condition=&quot;&#x27;$(Configuration)&#x27; == &#x27;Release&#x27;&quot;&gt;
    &lt;TypeScriptTarget&gt;ES5&lt;/TypeScriptTarget&gt;
    &lt;TypeScriptIncludeComments&gt;false&lt;/TypeScriptIncludeComments&gt;
    &lt;TypeScriptSourceMap&gt;false&lt;/TypeScriptSourceMap&gt;
  &lt;/PropertyGroup&gt;
  &lt;Import Project=&quot;$(MSBuildExtensionsPath32)\Microsoft\VisualStudio\v$(VisualStudioVersion)\TypeScript\Microsoft.TypeScript.targets&quot; /&gt;
</code></pre></blockquote><p>I followed these instructions (well I had to tweak the <code>Import Project</code> location) and I was in business again. But I when I came to check my code into TFS I came unstuck. The automated build kicked off and then, in short order, kicked me:</p><blockquote><pre><code>C:\Builds\1\MyApp\MyApp Continuous Integration\src\MyApp\MyApp.csproj (1520): The imported project &quot;C:\Program Files (x86)\MSBuild\Microsoft\VisualStudio\v11.0\TypeScript\Microsoft.TypeScript.targets&quot; was not found. Confirm that the path in the &lt;import&gt; declaration is correct, and that the file exists on disk.
C:\Builds\1\MyApp\MyApp Continuous Integration\src\MyApp\MyApp.csproj (1520): The imported project &quot;C:\Program Files (x86)\MSBuild\Microsoft\VisualStudio\v11.0\TypeScript\Microsoft.TypeScript.targets&quot; was not found. Confirm that the path in the &lt;import&gt; declaration is correct, and that the file exists on disk.
&lt;/import&gt;&lt;/import&gt;
</code></pre></blockquote><p>That&#x27;s right, TypeScript wasn&#x27;t installed on the build server. And since TypeScript was now part of the build process my builds were now failing. Ouch.</p><h2>So what now?</h2><p>I did a little digging and found <a href="https://typescript.codeplex.com/workitem/1518">this issue report on the TypeScript CodePlex site</a>. To quote the issue, it seemed there were 2 possible solutions to get continuous integration and typescript playing nice:</p><ol><li>Install TypeScript on the build server</li><li>Copy the required files for Microsoft.TypeScript.targets to a different source-controlled folder and change the path references in the csproj file to this folder.</li></ol><p>#<!-- -->1 wasn&#x27;t an option for us - we couldn&#x27;t install on the build server. And covering both #1 and #2, I wasn&#x27;t particularly inclined to kick off builds on the build server since I was wary of <a href="https://typescript.codeplex.com/workitem/1432">reported problems with memory leaks</a> etc with the TS compiler. I may feel differently later when TS is no longer in Alpha and has stabilised but it didn&#x27;t seem like the right time.</p><h2>A solution</h2><p>So, to sum up, what I wanted was to be able to compile TypeScript in Visual Studio on my machine, and indeed in VS on the machine of anyone else working on the project. But I <!-- -->*<strong>didn&#x27;t</strong>*<!-- --> want TypeScript compilation to be part of the build process on the server.</p><p>The solution in the end was pretty simple - I replaced the <code>.csproj</code> changes with the code below:</p><pre><code class="language-xml">&lt;PropertyGroup Condition=&quot;&#x27;$(Configuration)&#x27; == &#x27;Debug&#x27;&quot;&gt;
    &lt;TypeScriptTarget&gt;ES5&lt;/TypeScriptTarget&gt;
    &lt;TypeScriptRemoveComments&gt;false&lt;/TypeScriptRemoveComments&gt;
    &lt;TypeScriptSourceMap&gt;false&lt;/TypeScriptSourceMap&gt;
    &lt;TypeScriptModuleKind&gt;AMD&lt;/TypeScriptModuleKind&gt;
    &lt;TypeScriptNoImplicitAny&gt;true&lt;/TypeScriptNoImplicitAny&gt;
  &lt;/PropertyGroup&gt;
  &lt;PropertyGroup Condition=&quot;&#x27;$(Configuration)&#x27; == &#x27;Release&#x27;&quot;&gt;
    &lt;TypeScriptTarget&gt;ES5&lt;/TypeScriptTarget&gt;
    &lt;TypeScriptRemoveComments&gt;false&lt;/TypeScriptRemoveComments&gt;
    &lt;TypeScriptSourceMap&gt;false&lt;/TypeScriptSourceMap&gt;
    &lt;TypeScriptModuleKind&gt;AMD&lt;/TypeScriptModuleKind&gt;
    &lt;TypeScriptNoImplicitAny&gt;true&lt;/TypeScriptNoImplicitAny&gt;
  &lt;/PropertyGroup&gt;
  &lt;Import Project=&quot;$(VSToolsPath)\TypeScript\Microsoft.TypeScript.targets&quot; Condition=&quot;Exists(&#x27;$(VSToolsPath)\TypeScript\Microsoft.TypeScript.targets&#x27;)&quot; /&gt;
</code></pre><p>What this does is enable TypeScript compilation <!-- -->*<strong>only</strong>*<!-- --> if TypeScript is installed. So when I&#x27;m busy developing with Visual Studio on my machine with the plugin installed I can compile TypeScript. But when I check in the TypeScript compilation is <!-- -->*<strong>not</strong>*<!-- --> performed on the build server. This is because TypeScript is not installed on the build server and we are only compiling if it is installed. (Just to completely labour the point.)</p><h2>Final thoughts</h2><p>I do consider this an interim solution. As I mentioned earlier, when TypeScript has stabilised I think I&#x27;d like TS compilation to be part of the build process. Like with any other code I think compiling on check-in to catch bugs early is an excellent idea. But I think I&#x27;ll wait until there&#x27;s some clearer guidance on the topic from the TypeScript team before I take this step.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Migrating from jquery.validate.unobtrusive.js to jQuery.Validation.Unobtrusive.Native]]></title>
            <link>https://blog.johnnyreilly.com/2013/10/04/migrating-from-jquery.validate.unobtrusive.js-to-jQuery.Validation.Unobtrusive.Native</link>
            <guid>Migrating from jquery.validate.unobtrusive.js to jQuery.Validation.Unobtrusive.Native</guid>
            <pubDate>Fri, 04 Oct 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[So, you're looking at jQuery.Validation.Unobtrusive.Native. You're thinking to yourself "Yeah, I'd really like to use the native unobtrusive support in jQuery Validation. But I've already got this app which is using jquery.validate.unobtrusive.js \- actually how easy is switching over?" Well I'm here to tell you that it's pretty straightforward - here's a walkthrough of how it might be done.]]></description>
            <content:encoded><![CDATA[<p>So, you&#x27;re looking at <a href="https://github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native">jQuery.Validation.Unobtrusive.Native</a>. You&#x27;re thinking to yourself &quot;Yeah, I&#x27;d really like to use the native unobtrusive support in jQuery Validation. But I&#x27;ve already got this app which is using <a href="https://www.nuget.org/packages/jQuery.Validation.Unobtrusive/">jquery.validate.unobtrusive.js</a> <!-- -->-<!-- --> actually how easy is switching over?&quot; Well I&#x27;m here to tell you that it&#x27;s pretty straightforward - here&#x27;s a walkthrough of how it might be done.</p><h2>I need something to migrate</h2><p>So let&#x27;s File &gt; New Project ourselves a new MVC 4 application using the Internet Application template. I&#x27;ve picked this template as I know it ships with account registration / login screens in place which make use of jquery.validate.unobtrusive.js. To demo this just run the project, click the &quot;Log in&quot; link and then click the &quot;Log in&quot; button - you should see something like this:</p><p><img src="https://1.bp.blogspot.com/-7JxyO2uaVow/Uk5373yDbaI/AAAAAAAAAdk/gD8h47ObQcg/s400/BeforeLoginScreen.png"/></p><p>What you&#x27;ve just witnessed is jquery.validate.unobtrusive.js doing its thing. Both the <code>UserName</code> and <code>Password</code> properties on the <code>LoginModel</code> are decorated with the <code>Required</code> data annotation which, in the above scenario, causes the validation to be triggered on the client thanks to MVC rendering data attributes in the HTML which jquery.validate.unobtrusive.js picks up on. The question is, how can we take the log in screen above and migrate it across to to using jQuery.Validation.Unobtrusive.Native?</p><h2>Hit me up NuGet!</h2><p>Time to dive into NuGet and install jQuery.Validation.Unobtrusive.Native. We&#x27;ll install the MVC 4 version using this command:</p><div class="nuget-badge"><p><code>PM&gt; Install-Package jQuery.Validation.Unobtrusive.Native.MVC4</code></p></div><p>What has this done to my project? Well 2 things</p><ol><li>It&#x27;s upgraded jQuery Validation (<a href="http://jqueryvalidation.org/">jquery.validate.js</a>) from v1.10.0 (the version that is currently part of the MVC 4 template) to v1.11.1 (the latest and greatest jQuery Validation as of the time of writing)</li><li>It&#x27;s added a reference to the jQuery.Validation.Unobtrusive.Native.MVC4 assembly, like so:</li></ol><p><img src="https://3.bp.blogspot.com/-V-21V1Ypo3E/Uk583DTbegI/AAAAAAAAAd0/O0nv7w6kmew/s400/NewReference.png"/></p><p>In case you were wondering, doing this hasn&#x27;t broken the existing jquery.validate.unobtrusive.js - if you head back to the Log in screen you&#x27;ll still see the same behaviour as before.</p><h2>Migrating...</h2><p>We need to switch our TextBox and Password helpers over to using jQuery.Validation.Unobtrusive.Native, which we achieve by simply passing a second argument of <code>true</code> to <code>useNativeUnobtrusiveAttributes</code>. So we go from this:</p><pre><code class="language-cs">// ...
@Html.TextBoxFor(m =&gt; m.UserName)
// ...
@Html.PasswordFor(m =&gt; m.Password)
// ...
</code></pre><p>To this:</p><pre><code class="language-cs">// ...
@Html.TextBoxFor(m =&gt; m.UserName, true)
// ...
@Html.PasswordFor(m =&gt; m.Password, true)
// ...
</code></pre><p>With these minor tweaks in place the natively supported jQuery Validation data attributes will be rendered into the textbox / password elements instead of the jquery.validate.unobtrusive.js ones.</p><p>Next lets do the JavaScript. If you take a look at the bottom of the <code>Login.cshtml</code> view you&#x27;ll see this:</p><pre><code class="language-cs">@section Scripts {
    @Scripts.Render(&quot;~/bundles/jqueryval&quot;)
}
</code></pre><p>Which renders the following scripts:</p><pre><code class="language-html">&lt;script src=&quot;/Scripts/jquery.unobtrusive-ajax.js&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;/Scripts/jquery.validate.js&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;/Scripts/jquery.validate.unobtrusive.js&quot;&gt;&lt;/script&gt;
</code></pre><p>In our brave new world we&#x27;re only going to need jquery.validate.js - so let&#x27;s create ourselves a new bundle in <code>BundleConfig.cs</code> which only contains that single file:</p><pre><code class="language-cs">bundles.Add(new ScriptBundle(&quot;~/bundles/jqueryvalnative&quot;)
    .Include(&quot;~/Scripts/jquery.validate.js&quot;));
</code></pre><p>To finish off our migrated screen we need to do 2 things. First we need to switch over the <code>Login.cshtml</code> view to only render the jquery.validate.js script (in the form of our new bundle). Secondly, the other thing that jquery.validate.unobtrusive.js did was to trigger validation for the current form. So we need to do that ourselves now. So our finished Scripts section looks like this:</p><pre><code class="language-html">@section Scripts { @Scripts.Render(&quot;~/bundles/jqueryvalnative&quot;)
&lt;script&gt;
  $(&#x27;form&#x27;).validate();
&lt;/script&gt;
}
</code></pre><p>Which renders the following script:</p><pre><code class="language-html">&lt;script src=&quot;/Scripts/jquery.validate.js&quot;&gt;&lt;/script&gt;
&lt;script&gt;
  $(&#x27;form&#x27;).validate();
&lt;/script&gt;
</code></pre><p>And, pretty much, that&#x27;s it. If you run the app now and go to the Log in screen and try to log in without credentials you&#x27;ll get this:</p><p><img src="https://2.bp.blogspot.com/-nD3-3jW1_Yo/Uk7G5sTpsGI/AAAAAAAAAeE/NDNf4jqhJSk/s400/AfterLoginScreen.png"/></p><p>Which is functionally exactly the same as previously. The eagle eyed will notice some styling differences but that&#x27;s all it comes down to really; style. And if you were so inclined you could easily style this up as you liked using CSS and the options you can pass to jQuery Validation (in fact a quick rummage through jquery.validate.unobtrusive.js should give you everything you need).</p><h2>Rounding off</h2><p>Before I sign off I&#x27;d like to illustrate how little we&#x27;ve had to change the code to start using jQuery.Validation.Unobtrusive.Native. Just take a look at this code comparison:</p><p><img src="https://2.bp.blogspot.com/-vnA84f1JXHw/Uk7HoDPGqMI/AAAAAAAAAeM/qZVlRak92_o/s400/WhatsTheDifference.png"/></p><p>As you see, it takes very little effort to migrate from one approach to the other. And it&#x27;s <!-- -->*<strong>your</strong>*<!-- --> choice. If you want to have one screen that uses jQuery.Validation.Unobtrusive.Native and one screen that uses jquery.validation.unobtrusive.js then you can! Including jQuery.Validation.Unobtrusive.Native in your project gives you the <strong>option</strong> to use it. It doesn&#x27;t force you to, you can do so as you need to and when you want to. It&#x27;s down to you.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using Bootstrap Tooltips to display jQuery Validation error messages]]></title>
            <link>https://blog.johnnyreilly.com/2013/08/17/using-bootstrap-tooltips-to-display</link>
            <guid>Using Bootstrap Tooltips to display jQuery Validation error messages</guid>
            <pubDate>Sat, 17 Aug 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[I love jQuery Validation. I was recently putting together a screen which had a lot of different bits of validation going on. And the default jQuery Validation approach of displaying the validation messages next to the element being validated wasn't working for me. That is to say, because of the amount of elements on the form, the appearance of validation messages was really making a mess of the presentation. So what to do?]]></description>
            <content:encoded><![CDATA[<p>I love jQuery Validation. I was recently putting together a screen which had a lot of different bits of validation going on. And the default jQuery Validation approach of displaying the validation messages next to the element being validated wasn&#x27;t working for me. That is to say, because of the amount of elements on the form, the appearance of validation messages was really making a mess of the presentation. So what to do?</p><h2>Tooltips to the rescue!</h2><p>I was chatting to <a href="https://plus.google.com/u/0/116859810359377785616/posts">Marc Talary</a> about this and he had the bright idea of using tooltips to display the error messages. Tooltips would allow the existing presentation of the form to remain as is whilst still displaying the messages to the users. Brilliant idea!</p><p>After a certain amount of fiddling I came up with a fairly solid mechanism for getting jQuery Validation to display error messages as tooltips which I&#x27;ll share here. It&#x27;s worth saying that for the application that Marc and I were working on we already had <a href="http://jqueryui.com/">jQuery UI</a> in place and so we decided to use the <a href="http://jqueryui.com/tooltip/">jQuery UI tooltip</a>. This example will use the <a href="http://getbootstrap.com/javascript/#tooltips">Bootstrap tooltip</a> instead. As much as anything else this demonstrates that you could swap out the tooltip mechanism here with any of your choosing.</p><iframe src="https://htmlpreview.github.io/?https://gist.github.com/johnnyreilly/5867188/raw/2543a12fbd5c0aaad1da6793b7a7437492be3baf/DemoTooltip.html" width="100%" height="350"></iframe><p>Beautiful isn&#x27;t it? Now look at the source:</p><script src="https://gist.github.com/johnnyreilly/5867188.js?file=DemoTooltip.html"></script><p>All the magic is in the JavaScript, specifically the <code>showErrors</code> function that&#x27;s passed as an option to jQuery Validation. Enjoy!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Announcing jQuery Validation Unobtrusive Native...]]></title>
            <link>https://blog.johnnyreilly.com/2013/08/08/announcing-jquery-validation</link>
            <guid>Announcing jQuery Validation Unobtrusive Native...</guid>
            <pubDate>Thu, 08 Aug 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[I've been busy working on an open source project called jQuery Validation Unobtrusive Native. To see it in action take a look here.]]></description>
            <content:encoded><![CDATA[<p>I&#x27;ve been busy working on an open source project called <strong><a href="http://github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native">jQuery Validation Unobtrusive Native</a></strong>. <a href="http://johnnyreilly.github.io/jQuery.Validation.Unobtrusive.Native/">To see it in action take a look here</a>.</p><h2>A Little Background</h2><p>I noticed a little while ago that jQuery Validation was now providing native support for validation driven by HTML 5 data attributes. As you may be aware, Microsoft shipped <a href="http://bradwilson.typepad.com/blog/2010/10/mvc3-unobtrusive-validation.html">jquery.validate.unobtrusive.js</a> back with MVC 3. (<a href="http://icanmakethiswork.blogspot.com/2012/08/jquery-unobtrusive-validation.html">I have written about it before.</a>) It provided a way to apply data model validations to the client side using a combination of jQuery Validation and HTML 5 data attributes.</p><p>The principal of this was and is fantastic. But since that time the jQuery Validation project has implemented its own support for driving validation unobtrusively (shipping with <a href="http://jquery.bassistance.de/validate/changelog.txt">jQuery Validation 1.11.0</a>). I&#x27;ve been looking at a way to directly use the native support instead of jquery.validate.unobtrusive.js.</p><h2>So... What is jQuery Validation Unobtrusive Native?</h2><p>jQuery Validation Unobtrusive Native is a collection of ASP.Net MVC HTML helper extensions. These make use of jQuery Validation&#x27;s native support for validation driven by HTML 5 data attributes. The advantages of the native support over jquery.validate.unobtrusive.js are:</p><ul><li>Dynamically created form elements are parsed automatically. jquery.validate.unobtrusive.js does not support this whilst jQuery Validation does. <a href="http://johnnyreilly.github.io/jQuery.Validation.Unobtrusive.Native/AdvancedDemo/Knockout.html">Take a look at a demo using Knockout.</a></li><li>jquery.validate.unobtrusive.js restricts how you use jQuery Validation. If you want to use showErrors or something similar then you may find that you need to go native (or at least you may find that significantly easier than working with the jquery.validate.unobtrusive.js defaults)...</li><li>Send less code to your browser, make your browser to do less work and even get a (marginal) performance benefit .</li></ul><p>This project intends to be a bridge between MVC&#x27;s inbuilt support for driving validation from data attributes and jQuery Validation&#x27;s native support for the same. This is achieved by hooking into the MVC data attribute creation mechanism and using it to generate the data attributes natively supported by jQuery Validation.</p><h2>Future Plans</h2><p>So far the basic set of the HtmlHelpers and their associated unobtrusive mappings have been implemented. If any have been missed then let me know. As time goes by I intend to:</p><ul><li>fill in any missing gaps there may be</li><li>maintain MVC 3, 4 (and when the time comes 5+) versions of this on Nuget</li><li>not all data annotations generate client data attributes - if it makes sense I may look to implement some of these where it seems sensible. (eg the <a href="http://msdn.microsoft.com/en-us/library/system.componentmodel.dataannotations.minlengthattribute.aspx">MinLengthAttribute</a> annotation could be mapped to <a href="http://jqueryvalidation.org/minlength-method/">minlength</a> validation...)</li><li>get the unit test coverage to a good level and finally (and perhaps most importantly)</li><li>create some really useful <a href="http://johnnyreilly.github.io/jQuery.Validation.Unobtrusive.Native/Demo.html">demos and documentation</a>.</li></ul><p>Help is appreciated so feel free to pitch in! You can find the project on GitHub <a href="http://github.com/johnnyreilly/jQuery.Validation.Unobtrusive.Native">here</a>...</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How I'm Using Cassette part 3:Cassette and TypeScript Integration]]></title>
            <link>https://blog.johnnyreilly.com/2013/07/06/how-im-using-cassette-part-3-typescript</link>
            <guid>How I'm Using Cassette part 3:Cassette and TypeScript Integration</guid>
            <pubDate>Sat, 06 Jul 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[The modern web is JavaScript. There's no two ways about it. HTML 5 has new CSS, new HTML but the most important aspect of it from an application development point of view is JavaScript. It's the engine. Without it HTML 5 wouldn't be the exciting application platform that it is. Half the posts on Hacker News would vanish.]]></description>
            <content:encoded><![CDATA[<p>The modern web is JavaScript. There&#x27;s no two ways about it. HTML 5 has new CSS, new HTML but the most important aspect of it from an application development point of view is JavaScript. It&#x27;s the engine. Without it HTML 5 wouldn&#x27;t be the exciting application platform that it is. Half the posts on <a href="https://news.ycombinator.com/">Hacker News</a> would vanish.</p><p>It&#x27;s easy to break a JavaScript application. One false keypress and you can mysteriously turn a fully functioning app into toast. And not know why. There&#x27;s tools you can use to help yourself - <a href="http://icanmakethiswork.blogspot.co.uk/2012/04/jshint-customising-your-hurt-feelings.html">JSHint / JSLint</a> but whilst these make error detection a little easier it remains very easy to shoot yourself in the foot with JavaScript. Because of this I&#x27;ve come to really rather love <a href="http://www.typescriptlang.org/">TypeScript</a>. If you didn&#x27;t already know, TypeScript can be summed up as JavaScript with optional static typing. It&#x27;s a <strong><em>superset</em></strong> of JavaScript - JavaScript with go-faster stripes. When run through the compiler TypeScript is <a href="https://en.wikipedia.org/wiki/Source-to-source_compiler">transpiled</a> into JavaScript. And importantly, if you have bugs in your code, the compiler should catch them at this point and let you know.</p><p>Now very few of us are working on greenfield applications. Most of us have existing applications to maintain and support. Happily, TypeScript fits very well with this purely because TypeScript is a superset of JavaScript. That is to say: all JavaScript is valid TypeScript in the same way that all CSS is valid <a href="http://lesscss.org/">LESS</a>. This means that you can take an existing <code>.js</code> file, rename it to have a <code>.ts</code> suffix, run the TypeScript compiler over it and out will pop your JavaScript file just as it was before. You&#x27;re then free to enrich your TypeScript file with the relevant type annotations at your own pace. Increasing the robustness of your codebase is a choice left to you.</p><p>The project I am working on has recently started to incorporate TypeScript. It&#x27;s an ASP.Net MVC 4 application which makes use of <a href="http://knockoutjs.com/">Knockout</a>. The reason we started to incorporate TypeScript is because certain parts of the app, particularly the Knockout parts, were becoming more complex. This complexity wasn&#x27;t really an issue when we were writing the relevant JavaScript. However, when it came to refactoring and handing files from one team member to another we realised it was very easy to introduce bugs into the codebase, particularly around the JavaScript. Hence TypeScript.</p><h2>Cassette and TypeScript</h2><p>Enough of the pre-amble. The project was making use of Cassette for serving up its CSS and JavaScript. Because Cassette rocks. One of the reasons we use it is that we&#x27;re making extensive use of <a href="http://icanmakethiswork.blogspot.co.uk/2013/06/how-im-using-cassette-part-2.html">Cassette&#x27;s ability to serve scripts in dependency order</a>. So if we were to move to using TypeScript it was important that TypeScript and Cassette would play well together.</p><p>I&#x27;m happy to report that Cassettes and TypeScript do work well together, but there are a few things that you need to get up and running. Or, to be a little clearer, if you want to make use of Cassette&#x27;s in-file Asset Referencing then you&#x27;ll need to follow these steps. If you don&#x27;t need Asset Referencing then you&#x27;ll be fine using Cassette with TypeScript generated JavaScript as is <!-- -->*<strong>provided</strong>*<!-- --> you ensure the TypeScript compiler is not preserving comments in the generated JavaScript.</p><h2>The Fly in the Ointment: Asset References</h2><p>TypeScript is designed to allow you to break up your application into modules. However, the referencing mechanism which allows you to reference one TypeScript file / module from another is exactly the same as the existing Visual Studio XML reference comments mechanism that was originally introduced to drive JavaScript Intellisense in Visual Studio. To quote the <a href="http://www.typescriptlang.org/Content/TypeScript%20Language%20Specification.pdf">TypeScript spec</a>:</p><ul><li><em>A comment of the form /// <reference path="…"></reference> adds a dependency on the source file specified in the path argument. The path is resolved relative to the directory of the containing source file.</em></li><li><em>An external import declaration that specifies a relative external module name (section 11.2.1) resolves the name relative to the directory of the containing source file. If a source file with the resulting path and file extension ‘.ts’ exists, that file is added as a dependency. Otherwise, if a source file with the resulting path and file extension ‘.d.ts’ exists, that file is added as a dependency.</em></li></ul><p>The problem is that <a href="http://getcassette.net/documentation/v1/AssetReferences">Cassette <!-- -->*<strong>also</strong>*<!-- --> supports Visual Studio XML reference comments to drive Asset References</a>. The upshot of this is, that Cassette will parse the <code>/// &amp;lt;reference path=&quot;*.ts&quot;/&amp;gt;</code>s and will attempt to serve up the TypeScript files in the browser... Calamity!</p><h2>Pulling the Fly from the Ointment</h2><p>Again I&#x27;m going to take the demo from last time (<a href="https://github.com/johnnyreilly/CassetteDemo/tree/References">the References branch of my CassetteDemo project</a>) and build on top of it. First of all, we need to update the Cassette package. This is because to get Cassette working with TypeScript you need to be running at least Cassette 2.1. So let&#x27;s let NuGet do it&#x27;s thing:</p><p><code>Update-Package Cassette.Aspnet</code></p><p>And whilst we&#x27;re at it let&#x27;s grab the jQuery TypeScript typings - we&#x27;ll need them later:</p><p><code>Install-Package jquery.TypeScript.DefinitelyTyped</code></p><p>Now we need to add a couple of classes to the project. First of all this:</p><script src="https://gist.github.com/johnnyreilly/5934706.js?file=ParseJavaScriptNotTypeScriptReferences.cs"></script><p>Which subclasses <code>ParseJavaScriptReferences</code> and ensures TypeScript files are excluded when JavaScript references are being parsed. And to make sure that Cassette makes use of <code>ParseJavaScriptNotTypeScriptReferences</code> in place of <code>ParseJavaScriptReferences</code> we need this:</p><script src="https://gist.github.com/johnnyreilly/5934706.js?file=InsertIntoPipelineParseJavaScriptNotTypeScriptReferences.cs"></script><p>Now we&#x27;re in a position to use TypeScript with Cassette. To demonstrate this let&#x27;s take the <code>Index.js</code> and rename it to <code>Index.ts</code>. And now it&#x27;s TypeScript. However before it can compile it needs to know what jQuery is - so we drag in the jQuery typings from <a href="http://github.com/borisyankov/DefinitelyTyped">Definitely Typed</a>. And now it can compile from this:</p><script src="https://gist.github.com/johnnyreilly/5934706.js?file=Index.ts"></script><p>To this: (Please note that I get the TypeScript compiler to preserve my comments in order that I can continue to use Cassettes Asset Referencing)</p><script src="https://gist.github.com/johnnyreilly/5934706.js?file=Index.js"></script><p>As you can see the output JavaScript has both the TypeScript and the Cassette references in place. However thanks to <code>ParseJavaScriptNotTypeScriptReferences</code> those TypeScript references will be ignored by Cassette.</p><p>So that&#x27;s it - we&#x27;re home free. Before I finish off I&#x27;d like to say thanks to Cassette&#x27;s <a href="http://twitter.com/andrewdavey">Andrew Davey</a> who <a href="https://groups.google.com/forum/?fromgroups=#!topic/cassette/SM3Rxh48D7Q">set me on the right path</a> when trying to work out how to do this. A thousand thank yous Andrew!</p><p>And finally, again as last time you can see what I&#x27;ve done in this post by just looking at the repository on <a href="https://github.com/johnnyreilly/CassetteDemo/tree/TypeScript">GitHub</a>. The changes I made are on the TypeScript branch of that particular repository.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[jQuery Validate - Native Unobtrusive Validation Support!]]></title>
            <link>https://blog.johnnyreilly.com/2013/06/26/jquery-validate-native-unobtrusive-validation</link>
            <guid>jQuery Validate - Native Unobtrusive Validation Support!</guid>
            <pubDate>Wed, 26 Jun 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[Did you know that jQuery Validate natively supports the use of HTML 5 data attributes to drive validation unobtrusively? Neither did I - I haven't seen any documentation for it. However, I was reading the jQuery Validate test suite and that's what I spotted being used in some of the tests.]]></description>
            <content:encoded><![CDATA[<p>Did you know that jQuery Validate natively supports the use of <a href="http://ejohn.org/blog/html-5-data-attributes/">HTML 5 data attributes</a> to drive validation unobtrusively? Neither did I - I haven&#x27;t seen any documentation for it. However, I was reading the <a href="https://github.com/jzaefferer/jquery-validation/blob/master/test/index.html">jQuery Validate test suite</a> and that&#x27;s what I spotted being used in some of the tests.</p><p>I was quite keen to give it a try as I&#x27;ve found the Microsoft produced <a href="http://nuget.org/packages/jQuery.Validation.Unobtrusive/">unobtrusive extensions</a> both fantastic and frustrating in nearly equal measure. Fantastic because they work and they&#x27;re <a href="http://icanmakethiswork.blogspot.co.uk/2012/08/jquery-unobtrusive-validation.html">integrated nicely with MVC</a>. Frustrating, because they don&#x27;t allow you do all the things that jQuery Validate in the raw does.</p><p>So when I realised that there was native alternative available I was delighted. Enough with the fine words - what we want is a demo:</p><iframe src="https://htmlpreview.github.io/?http://gist.github.com/johnnyreilly/5867188/raw/272b1b42f4773fe6df843550b3e3d457013522a8/Demo.html" width="100%" height="575"></iframe><p>Not particularly exciting? Not noticably different to any other jQuery Validate demo you&#x27;ve ever seen? Fair enough. Now look at the source:</p><script src="https://gist.github.com/johnnyreilly/5867188.js?file=Demo.html"></script><p>Do you see what I see? Data attributes (both <code>data-rule-*</code> and <code>data-msg-*</code>s) being used to drive the validation unobtrusively! And if you look at the JavaScript files referenced you will see <!-- -->*<strong>no sign</strong>*<!-- --> of <code>jquery.validate.unobtrusive.js</code> <!-- -->-<!-- --> this is all raw jQuery Validate. Nothing else.</p><h2>Why is this useful?</h2><p>First of all, I&#x27;m of the opinion that it makes intuitive sense to have the validation information relevant to various DOM elements stored directly with those DOM elements. There will be occasions where you may not want to use this approach but, in the main, I think it&#x27;s very sensible. It saves you bouncing back and forth between your HTML and your JavaScript and it means when you read the HTML you know there and then what validation applies to your form.</p><p>I think this particularly applies when it comes to adding elements to the DOM dynamically. If I use data attributes to drive my validation and I dynamically add elements then jQuery Validate will parse the validation rules for me. I won&#x27;t have to subsequently apply validation to those new elements once they&#x27;ve been added to the DOM. 1 step instead of 2. It makes for simpler code and that&#x27;s always a win.</p><h2>Wrapping up</h2><p>For myself I&#x27;m in the early stages of experimenting with this but I thought it might be good to get something out there to show how this works. If anyone knows of any official documentation for this please do let me know - I&#x27;d love to have a read of it. Maybe it&#x27;s been out there all along and it&#x27;s just my Googling powers are inadequate.</p><h2>Update 09/08/2012</h2><p>If you&#x27;re using ASP.Net MVC 3+ and this post has been of interest to you then you might want to take a look at <a href="http://icanmakethiswork.blogspot.co.uk/2013/08/announcing-jquery-validation.html">this</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How I'm Using Cassette part 2:Get Cassette to Serve Scripts in Dependency Order]]></title>
            <link>https://blog.johnnyreilly.com/2013/06/06/how-im-using-cassette-part-2</link>
            <guid>How I'm Using Cassette part 2:Get Cassette to Serve Scripts in Dependency Order</guid>
            <pubDate>Thu, 06 Jun 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[Last time I wrote about Cassette I was talking about how to generally get up and running. How to use Cassette within an ASP.Net MVC project. What I want to write about now is (in my eyes) the most useful feature of Cassette by a country mile. This is Cassettes ability to ensure scripts are served in dependency order.]]></description>
            <content:encoded><![CDATA[<p><a href="http://icanmakethiswork.blogspot.co.uk/2013/05/how-im-using-cassette.html">Last time</a> I wrote about Cassette I was talking about how to generally get up and running. How to use Cassette within an ASP.Net MVC project. What I want to write about now is (in my eyes) the most useful feature of Cassette by a country mile. This is Cassettes ability to ensure scripts are served in dependency order.</p><h2>Why does this matter?</h2><p>You might well ask. If we go back 10 years or so then really this wasn&#x27;t a problem. No-one was doing a great deal with JavaScript. And if they did anything it tended to be code snippets in amongst the HTML; nothing adventurous. But unless you&#x27;ve had your head in the sand for the last 3 years then you will have clearly noticed that JavaScript is in rude health and being used for all kinds of things you&#x27;d never have imagined. In fact some would have it that it&#x27;s the <a href="http://www.hanselman.com/blog/JavaScriptisAssemblyLanguagefortheWebPart2MadnessorjustInsanity.aspx">assembly language of the web</a>.</p><p>For my part, I&#x27;ve been doing more and more with JavaScript. And as I do more and more with it I seek to modularise my code; (<a href="http://en.wikipedia.org/wiki/Separation_of_concerns">like any good developer would</a>) breaking it up into discrete areas of functionality. I aim to only serve up the JavaScript that I need on a given page. And that would be all well and good but for one of the languages shortcomings. Modules. JavaScript doesn&#x27;t yet have a good module loading story to tell. (Apparently one&#x27;s coming in <a href="http://wiki.ecmascript.org/doku.php?id=harmony:modules">EcmaScript 6</a>). (I don&#x27;t want to get diverted into this topic as it&#x27;s a big area. But if you&#x27;re interested then you can read up a little on different approaches being used <a href="http://requirejs.org/docs/whyamd.html#today">here</a>. The ongoing contest between RequireJS and CommonJS frankly makes me want to keep my distance for now.)</p><h2>It Depends</h2><p>Back to my point, JavaScripts native handling of script dependencies is non-existent. It&#x27;s real &quot;here be dragons&quot; territory. If you serve up, for example, Slave.js that depends on things set up in Master.js before you&#x27;ve actually served up Master.js, well it&#x27;s not a delightful debugging experience. The errors tend be obscure and it&#x27;s not always obvious what the correct ordering should be.</p><p>Naturally this creates something of a headache around my own JavaScript modules. A certain amount of jiggery-pokery is required to ensure that scripts are served in the correct order so that they run as expected. And as your application becomes more complicated / modular, the number of problems around this area increase exponentially. It&#x27;s <strong>really</strong> tedious. I don&#x27;t want to be thinking about managing that as I&#x27;m developing - I want to be focused on solving the problem at hand.</p><p>In short, what I want to do is reference a script file somewhere in my server-side pipeline. I could be in a view, a layout, a controller, a partial view, a HTML helper... - I just want to know that that script is going to turn up at the client in the right place in the HTML so it works. Always. And I don&#x27;t want to have to think about it any further than that.</p><h2>Enter Cassette, riding a white horse</h2><p>And this is where Cassette takes the pain away. To quote the documentation:</p><blockquote><p>&quot;<em>Some assets must be included in a page before others. For example, your code may use jQuery, so the jQuery script must be included first. Cassette will sort all assets based on references they declare.</em>&quot;</p></blockquote><p>Just the ticket!</p><h2>Declaring References Server-Side</h2><p>What does this look like in reality? Let&#x27;s build on what I did last time to demonstrate how I make use of Asset References to ensure my scripts turn up in the order I require.</p><p>In my <code>_Layout.cshtml</code> file I&#x27;m going to remove the following reference from the head of the file:</p><p><code>Bundles.Reference(&quot;~/bundles/core&quot;);</code></p><p>I&#x27;m pulling this out of my layout page because it&#x27;s presence means that <strong>every</strong> page MVC serves up is also serving up jQuery and jQuery UI (which is what <code>~/bundles/core</code> is). If a page doesn&#x27;t actually make use of jQuery and / or jQuery UI then there&#x27;s no point in doing this.</p><p>&quot;<em>But wait!</em>&quot;, I hear you cry, &quot;<em>Haven&#x27;t you just caused a bug with your reckless action? I distinctly recall that the <code>Login.cshtml</code> page has the following code in place:</em>&quot;</p><p><code>Bundles.Reference(&quot;~/bundles/validate&quot;);</code></p><p>&quot;<em>And now with your foolhardy, nay, reckless attitude to the <code>~/bundles/core</code> bundle you&#x27;ve broken your Login screen. How can jQuery Validation be expected to work if there&#x27;s no jQuery there to extend?</em>&quot;</p><p>Well, I understand your concerns but really you needn&#x27;t worry - Cassette&#x27;s got my back. Look closely at the code below:</p><script src="https://gist.github.com/johnnyreilly/5693071.js?file=ReferencesValidateDependesOnCoreCassetteConfiguration.cs"></script><p>See it? The <code>~/bundles/validate</code> bundle declares a reference to the <code>~/bundles/core</code> bundle. The upshot of this is, that if you tell Cassette to reference <code>~/bundles/validate</code> it will ensure that before it renders that bundle it first renders any bundles that bundle depends on (in this case the <code>~/bundles/core</code> bundle).</p><p>This is a very simple demonstration of the feature but I can&#x27;t underplay just how useful I find this.</p><h2>Declaring References in your JavaScript itself</h2><p>And the good news doesn&#x27;t stop there. Let&#x27;s say you <strong>don&#x27;t</strong> want to maintain your references in a separate file. You&#x27;d rather declare references inside your JavaScript files themselves. Well - you can. Cassette caters for this through the usage of <a href="http://getcassette.net/documentation/v1/AssetReferences">Asset References</a>.</p><p>Let&#x27;s demo this. First of all add the following file at this location in the project: <code>~/Scripts/Views/Home/Index.js</code></p><script src="https://gist.github.com/johnnyreilly/5693071.js?file=Index.js"></script><p>The eagle-eyed amongst you will have noticed</p><ol><li>I&#x27;m mirroring the MVC folder structure inside the Scripts directory. (There&#x27;s nothing special about that by the way - it&#x27;s just a file structure I&#x27;ve come to find useful. It&#x27;s very easy to find the script associated with a View if the scripts share the same organisational approach as the Views.).</li><li>The purpose of the script is very simple, it fades out the main body of the screen, re-writes the HTML in that tag and then fades back in. It&#x27;s purpose is just to do something that is obvious to the user - so they can see the evidence of JavaScript executing.</li><li>Lastly and most importantly, do you notice that <code>// @reference ~/bundles/core</code> is the first line of the file? This is our script reference. It&#x27;s this that Cassette will be reading to pick up references.</li></ol><p>To make sure Cassette is picking up our brand new file let&#x27;s take a look at <code>CassetteConfiguration.cs</code> and uncomment the line of code below:</p><p><code>bundles.AddPerIndividualFile&lt;scriptbundle&gt;(&quot;~/Scripts/Views&quot;);&lt;/scriptbundle&gt;</code></p><p>With this in place Cassette will render out a bundle for each script in the Views subdirectory. Let&#x27;s see if it works. Add the following reference to our new JavaScript file in <code>~/Views/Home/Index.cshtml</code>:</p><p><code>Bundles.Reference(&quot;~/Scripts/Views/Home/Index.js&quot;);</code></p><p>If you browse to the home page of the application this is what you should now see:</p><p><img src="https://2.bp.blogspot.com/-tGZTEhhkGz8/Ua7xlgl3n5I/AAAAAAAAAcs/miNZsysrJeY/s320/Index.js.png"/></p><p>What this means is, <code>Index.js</code> was served up by Cassette. And more importantly before <code>Index.js</code> was served the referenced <code>~/bundles/core</code> was served too.</p><h2>Avoiding the Gotcha</h2><p>There is a gotcha which I&#x27;ve discovered whilst using Cassette&#x27;s Asset References. Strictly speaking it&#x27;s a Visual Studio gotcha rather than a Cassette gotcha. It concerns Cassette&#x27;s support for Visual Studio XML style reference comments. In the example above I could have written this:</p><p><code>/// &amp;lt;reference path=&quot;~/bundles/core&quot; /&amp;gt;</code></p><p>Instead of this:</p><p><code>// @reference ~/bundles/core</code></p><p>It would fulfil exactly the same purpose and would work identically. But there&#x27;s a problem. Using Visual Studio XML style reference comments to refer to Cassette bundles appears to trash the Visual Studios JavaScript Intellisense. You&#x27;ll lose the Intellisense that&#x27;s driven by <code>~/Scripts/_references.js</code> in VS 2012. So if you value your Intellisense (and I do) my advice is to stick to using the standard Cassette references style instead.</p><h2>Go Forth and Reference</h2><p>There is also support in Cassette for CSS referencing (as well as other types of referencing relating to LESS and even CoffeeScript). I haven&#x27;t made use of CSS referencing myself as, in stark contrast to my JS, my CSS is generally one bundle of styles which I&#x27;m happy to be rendered on each page. But it&#x27;s nice to know the option is there if I wanted it.</p><p>Finally, as last time you can see what I&#x27;ve done in this post by just looking at the repository on <a href="https://github.com/johnnyreilly/CassetteDemo/tree/References">GitHub</a>. The changes I made are on the References branch of that particular repository.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How I'm Using Cassette part 1:Getting Up and Running]]></title>
            <link>https://blog.johnnyreilly.com/2013/05/04/how-im-using-cassette</link>
            <guid>How I'm Using Cassette part 1:Getting Up and Running</guid>
            <pubDate>Sat, 04 May 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[Backing into the light]]></description>
            <content:encoded><![CDATA[<h2>Backing into the light</h2><p>For a while now, I&#x27;ve been seeking a bulletproof way to handle the following scenarios... all at the same time in the context of an ASP.Net MVC application:</p><ol><li>How to serve full-fat JavaScript in debug mode and minified in release mode</li><li>When debugging, ensure that the full-fat JS being served is definitely the latest version; and <!-- -->*<strong>not</strong>*<!-- --> from the cache. (The time I&#x27;ve wasted due to <a href="http://en.wikipedia.org/wiki/List_of_HTTP_status_codes#304">304&#x27;s</a>...)</li><li>How to add Javascript assets that need to be served up from any point in an ASP.Net MVC application (including views, layouts, partial views... even controllers if so desired) whilst preventing duplicate scripts from being served.</li><li>How to ensure that Javascript files are served up last to any web page to ensure a speedy feel to users (don&#x27;t want JS blocking rendering).</li><li>And last but certainly not least the need to load Javascript files in dependency order. If <code>myView.js</code> depends on jQuery then clearly <code>jQuery-latest.js</code> needs to be served before <code>myView.js</code>.</li></ol><p>Now the best, most comprehensive and solid looking solution to this problem has for some time seemed to me to be <a href="http://aboutcode.net/">Andrew Davey&#x27;s</a><a href="http://getcassette.net/">Cassette</a>. This addresses all my issues in one way or another, as well as bringing in a raft of other features (support for Coffeescript etc).</p><p>However, up until now I&#x27;ve slightly shied away from using Cassette as I was under the impression it had a large number of dependencies. That doesn&#x27;t appear to be the case at all. I also had some vague notion that I could quite simply build my own solution to these problems making use of Microsoft&#x27;s <a href="http://nuget.org/packages/Microsoft.AspNet.Web.Optimization/1.0.0">Web Optimization</a> which nicely handles my #1 problem above. However, looking again at the documentation Cassette was promising to handle scenarios #1 - #5 without breaking sweat. How could I ignore that? I figured I should do the sensible thing and take another look at it. And, lo and behold, when I started evaluating it again it seemed to be just what I needed.</p><p>With the minumum of fuss I was able to get an ASP.Net MVC 4 solution up and running, integrated with Cassette, which dealt with all my scenarios very nicely indeed. I thought it might be good to write this up over a short series of posts and share what my finished code looks like. If you follow the steps I go through below it&#x27;ll get you started using Cassette. Or you could skip to the end of this post and look at the repo on GitHub. Here we go...</p><h2>Adding Cassette to a raw MVC 4 project</h2><p>Fire up Visual Studio and create a new MVC 4 project (I used the internet template to have some content in place).</p><p>Go to the Package Manager Console and key in &quot;<code>Install-Package Cassette.Aspnet</code>&quot;. Cassette will install itself.</p><p>Now you&#x27;ve got Cassette in place you may as well pull out usage of Web Optimization as you&#x27;re not going to need it any more.Be ruthless, delete App_Start/BundleConfig.cs and delete the line of code that references it in Global.asax.cs. If you take the time to run the app now you&#x27;ll see you&#x27;ve miraculously lost your CSS and your JavaScript. The code referencing it is still in place but there&#x27;s nothing for it to serve up. Don&#x27;t worry about that - we&#x27;re going to come back and Cassette-ify things later on...</p><p>You&#x27;ll also notice you now have a CassetteConfiguration.cs file in your project. Open it. Replace the contents with this (I&#x27;ve just commented out the default code and implemented my own CSS and Script bundles based on what is available in the default template of an MVC 4 app):</p><script src="https://gist.github.com/johnnyreilly/5393608.js?file=CassetteConfiguration.cs"></script><p>In the script above I&#x27;ve created 4 bundles, 1 stylesheet bundle and 3 JavaScript bundles - each of these is roughly equivalent to Web Optimization bundles that are part of the MVC 4 template:</p><dl><dt>~/bundles/css</dt><dd>Our site CSS - this includes both our own CSS and the jQuery UI CSS as well. This is the rough equivalent of the Web Optimization bundles <em>~/Content/css</em> and <em>~/Content/themes/base/css</em> brought together.</dd><dt>~/bundles/head</dt><dd>What scripts we want served in the head tag - Modernizr basically. Do note the setting of the <em>PageLocation</em> property - the purpose of this will become apparent later. This is the direct equivalent of the Web Optimization bundle: <em>~/bundles/modernizr</em>.</dd><dt>~/bundles/core</dt><dd>The scripts we want served on every page. For this example project I&#x27;ve picked jQuery and jQuery UI. This is the rough equivalent of the Web Optimization bundles <em>~/bundles/jquery</em> and <em>~/bundles/jqueryui</em> brought together.</dd><dt>~/bundles/validate</dt><dd>The validation scripts (that are dependent on the core scripts). This is the rough equivalent of the Web Optimization bundle: <em>~/bundles/jqueryval</em>.</dd></dl><p>At this point we&#x27;ve set up Cassette in our project - although we&#x27;re not making use of it yet. If you want to double check that everything is working properly then you can fire up your project and browse to &quot;Cassette.axd&quot; in the root. You should see something a bit like this:</p><p><img src="https://1.bp.blogspot.com/-xM9iU6HjB7o/UWzjAN5EieI/AAAAAAAAAaA/EAXTTnD6vdY/s320/CassetteScripts.png"/></p><h2>How Web Optimization and Cassette Differ</h2><p>If you&#x27;re more familiar with the workings of Web Optimization than Cassette then it&#x27;s probably worth taking a moment to appreciate an important distinction between the slightly different ways each works.</p><p><strong>Web Optimization</strong></p><ol><li>Create bundles as desired.</li><li>Serve up bundles and / or straight JavaScript files as you like within your MVC views / partial views / layouts.</li></ol><p><strong>Cassette</strong></p><ol><li>Create bundles for <!-- -->*<strong>all</strong>*<!-- --> JavaScript files you wish to serve up. You may wish to create some bundles which consist of a number of a number of JavaScript files pushed together. But for each individual file you wish to serve you also need to create an individual bundle. (Failure to do so may mean you fall prey to the &quot;<em>Cannot find an asset bundle containing the path &quot;<!-- -->~<!-- -->/Scripts/somePath.js&quot;.</em>&quot;)</li><li>Reference bundles and / or individual JavaScript files in their individual bundles as you like within your MVC views / partial views / layouts / controllers / HTML helpers... the list goes on!</li><li>Render the referenced scripts to the page (typically just before the closing <code>body</code> tag)</li></ol><h2>Making use of our Bundles</h2><p>Now we&#x27;ve created our bundles let&#x27;s get the project serving up CSS and JavaScript using Cassette. First the layout file. Take the <code>_Layout.cshtml</code> file from this:</p><script src="https://gist.github.com/johnnyreilly/5393608.js?file=_LayoutBefore.cshtml"></script><p>To this:</p><script src="https://gist.github.com/johnnyreilly/5393608.js?file=_LayoutAfter.cshtml"></script><p>And now let&#x27;s take one of the views, <code>Login.cshtml</code> and take it from this:</p><script src="https://gist.github.com/johnnyreilly/5393608.js?file=LoginBefore.cshtml"></script><p>To this:</p><script src="https://gist.github.com/johnnyreilly/5393608.js?file=LoginAfter.cshtml"></script><p>So now you should be up and running with Cassette. If you want the code behind this then take I&#x27;ve put it on GitHub <a href="https://github.com/johnnyreilly/CassetteDemo">here</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A navigation animation (for your users delectation)]]></title>
            <link>https://blog.johnnyreilly.com/2013/04/26/a-navigation-animation-for-your-users</link>
            <guid>A navigation animation (for your users delectation)</guid>
            <pubDate>Fri, 26 Apr 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[The Vexation]]></description>
            <content:encoded><![CDATA[<h2>The Vexation</h2><p>The current application I&#x27;m working on lives within an <code>iframe</code>. A side effect of that is that my users no longer get the visual feedback that they&#x27;re used to as they navigate around the site. By &quot;visual feedback&quot; what I mean are the little visual tics that are displayed in the browser when you&#x27;re in the process of navigating from one screen to the next. Basically, these:</p><p><img src="https://1.bp.blogspot.com/-x_8VXvW0w1M/UXpam8TtPQI/AAAAAAAAAbI/uVFFkL8uBAc/s320/TheSpinCycleOfProgress.png"/></p><p>When an application is nested in an <code>iframe</code> it seems that these visual tics aren&#x27;t propogated up to the top frame of the browser as the user navigates around. Clicking on links results in a short lag whilst nothing appears to be happening and then, BANG!, a new page is rendered. This is not a great user experience. There&#x27;s nothing to indicate that the link has been clicked on and the browser is doing something. Well, not in Internet Explorer at least - Chrome (my browser of choice) appears to do just that. But that&#x27;s really by the by, the people using my app will be using the corporate browser, IE; so I need to think about them.</p><p>Now I&#x27;m fully aware that this is more in the region of nice-to-have rather than absolute necessity. That said, my experience is that when users think an application isn&#x27;t responding fast enough their action point is usually &quot;click it again, and maybe once more for luck&quot;. To prevent this from happening, I wanted to give the users back some kind of steer when they were in the process of navigation, <code>iframe</code> or no <code>iframe</code>.</p><h2>The Agreeable Resolution</h2><p>To that end, I&#x27;ve come up with something that I feel does the job, and does it well. I&#x27;ve taken a CSS animation courtesy of the good folk at <a href="http://cssload.net/">CSS Load</a> and embedded it in the layout of my application. This animation is hidden from view until the user navigates to another page. At that point, the CSS animation appears in the header of the screen and remains in place until the new screen is rendered. This is what it looks like:</p><p><img src="https://2.bp.blogspot.com/-RaGwl1llrDM/UX4pEiUfGWI/AAAAAAAAAbo/jSZC0skiLfQ/s320/NavigationAnimationAtWork.png"/></p><h2>How&#x27;s that work then guv?</h2><p>You&#x27;re no doubt dazzled by the glory of it all. How was it accomplished? Well, it was actually a great deal easier than you might think. First of all we have the html:</p><script src="https://gist.github.com/johnnyreilly/5466370.js?file=navigationAnimation.html"></script><p>Apart from the outer <code>div</code> tag (#navigationAnimation) all of this is the HTML taken from <a href="http://cssload.net/">CSS Load</a>. If you wanted to use a different navigation animation you could easily replace the inner HTML with something else instead. Next up is the CSS, again courtesy of CSS Load (and it&#x27;s this that turns this simple HTML into sumptuous animated goodness):</p><script src="https://gist.github.com/johnnyreilly/5466370.js?file=navigationAnimation.css"></script><p>And finally we have the JavaScript which is responsible for showing animation when the user starts navigating:</p><script src="https://gist.github.com/johnnyreilly/5466370.js?file=navigationAnimation.js"></script><p>It&#x27;s helped along with a little jQuery here but this could easily be accomplished with vanilla JS if you fancied. The approach works by hooking into the <a href="https://developer.mozilla.org/en-US/docs/DOM/Mozilla_event_reference/beforeunload">beforeunload</a> event that fires when &quot;<em>the window, the document and its resources are about to be unloaded</em>&quot;. There&#x27;s a little bit more to the functionality in the JavaScript abover which I go into in the PPS below. Essentially that covers backwards compatibility with earlier versions of IE.</p><p>I&#x27;ve coded this up in a manner that lends itself to re-use. I can imagine that you might also want to make use of the navigation animation if, for example, you had an expensive AJAX operation on a page and you didn&#x27;t want the users to despair. So the navigation animation could become a kind of a generic &quot;I am doing something&quot; animation instead - I leave it to your disgression.</p><h2>Oh, and a final PS</h2><p>I had initially planned to use an old school animated GIF instead of a CSS animation. The thing that stopped me taking this course of action is that, to quote an <a href="http://stackoverflow.com/a/780617/761388">answer on Stack Overflow</a> &quot;<em>IE assumes that the clicking of a link heralds a new navigation where the current page contents will be replaced. As part of the process for perparing for that it halts the code that animates the GIFs.</em>&quot;. So I needed animation that stayed animated. And lo, there were CSS animations...</p><h2>Better make that a PPS - catering for IE 9 and earlier</h2><p>I spoke a touch too soon when I expounded on how CSS animations were going to get me out of a hole. Unfortunately, and to my lasting regret, they aren&#x27;t supported in IE 9. And yes, at least for now that is what the users have. To get round this I&#x27;ve delved a little bit further and discovered a frankly hacky way to make animated gifs stay animated after beforeunload has fired. It works by rendering an animated gif to the screen when beforeunload is fired. Why this works I couldn&#x27;t say - but if you&#x27;re interested to research more then take a look at <a href="http://stackoverflow.com/a/1904931/761388">this answer on Stack Overflow</a>. In my case I&#x27;ve found an animated gif on <a href="http://www.ajaxload.info/">AjaxLoad</a> which looks pretty similar to the CSS animation:</p><p><img src="https://4.bp.blogspot.com/-_9OgkLfflAg/UYEXn7dgByI/AAAAAAAAAb8/3Q33pAs6WeM/s320/navigationAnimation.gif" alt="null"/>This is now saved away as <code>navigationAnimation.gif</code> in the application. The JavaScript uses Modernizr to detect if CSS animations are in play. If they&#x27;re not then the animated gif is rendered to the screen in place of the CSS animation HTML. Ugly, but it seems to work well; I think this will work on IE 6 - 9. The CSS animations will work on IE 10+.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[IE 10 Install Torches JavaScript Debugging in Visual Studio 2012 Through Auto Update (Probably)]]></title>
            <link>https://blog.johnnyreilly.com/2013/04/17/ie-10-install-torches-javascript</link>
            <guid>IE 10 Install Torches JavaScript Debugging in Visual Studio 2012 Through Auto Update (Probably)</guid>
            <pubDate>Wed, 17 Apr 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[OK the title of this post is a little verbose. I've just wasted a morning of my life trying to discover what happened to my ability to debug JavaScript in Visual Studio 2012. If you don't want to experience the same pain then read on...]]></description>
            <content:encoded><![CDATA[<p>OK the title of this post is a little verbose. I&#x27;ve just wasted a morning of my life trying to discover what happened to my ability to debug JavaScript in Visual Studio 2012. If you don&#x27;t want to experience the same pain then read on...</p><h2>The Symptoms</h2><ol><li>I&#x27;m not hitting my JavaScript breakpoints when I hit F5 in Visual Studio.</li><li><a href="http://msdn.microsoft.com/en-us/library/bb385621.aspx">Script Documents</a> is missing from the Solution Explorer when I&#x27;m debugging in Visual Studio.</li></ol><h2>The Cure</h2><p>In the end, after a great deal of frustration, I happened upon <a href="http://stackoverflow.com/a/15908391/761388">this answer</a> on Stack Overflow. It set me in the right direction.</p><p>In my &quot;Browse With...&quot; drop down in Visual Studio I was <!-- -->*<strong>not</strong>*<!-- --> seeing this:</p><p><img src="https://3.bp.blogspot.com/-iRgBxdxUrkU/UW6csypEd3I/AAAAAAAAAaY/rNK6N79GT6k/s320/BrowseWith.png"/></p><p>I was seeing exactly the same as this list but with <strong>TWO</strong> instances of Internet Explorer in the list instead of one. Odd, I know.</p><p>I fixed this up by selecting Google Chrome as my target instead of IE, running it and then setting it back to IE. And interestingly, when I went to set it back to IE there was only one instance of Internet Explorer in the list again.</p><h2>The Probable Cause</h2><p>My machine was auto updated from IE 9 to IE 10 just the other day. I <!-- -->*<strong>think</strong>*<!-- --> my JavaScript debugging issue appeared at the same time. This would explain to me why I had two instances of &quot;Internet Explorer&quot; in my list. Not certain but I&#x27;d say the evidence is fairly compelling.</p><p>Painful Microsoft. Painful</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Making IE 10's clear field (X) button and jQuery UI autocomplete play nice]]></title>
            <link>https://blog.johnnyreilly.com/2013/04/09/making-ie-10s-clear-field-x-button-and</link>
            <guid>Making IE 10's clear field (X) button and jQuery UI autocomplete play nice</guid>
            <pubDate>Tue, 09 Apr 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[This morning when I logged on I was surprised to discover IE 10 had been installed onto my machine. I hadn't taken any action to trigger this myself and so I’m assuming that this was part of the general Windows Update mechanism. I know Microsoft had planned to push IE 10 out through this mechanism.]]></description>
            <content:encoded><![CDATA[<p>This morning when I logged on I was surprised to discover IE 10 had been installed onto my machine. I hadn&#x27;t taken any action to trigger this myself and so I’m assuming that this was part of the general Windows Update mechanism. I know <a href="http://technet.microsoft.com/en-us/ie/jj898508.aspx">Microsoft had planned to push IE 10 out through this mechanism</a>.</p><p>I was a little surprised that my work desktop had been upgraded without any notice. And I was initially rather concerned given that most of my users have IE 9 and now I didn&#x27;t have a test harness on my development machine any more. (I&#x27;ve generally found that having the majority users browser on your own machine is a good idea.) However, I wasn&#x27;t too concerned as I didn’t think it would makes much of a difference to my development experience. I say that because IE10, as far as I understand, is basically IE 9 + more advanced CSS 3 and extra HTML 5 features. The rendering of my existing content developed for the IE 9 target should look pixel for pixel identical in IE 10. That’s the theory anyway.</p><p>However, I have found one exception to this rule already. IE 10 provides clear field buttons in text boxes that look like this:</p><p><img src="https://3.bp.blogspot.com/-HRxuGdc6PhE/UWQMq15gKHI/AAAAAAAAAZg/XmFfFZBMHYk/s320/ClearField.png"/></p><p>Unhappily I found these were clashing with our jQuery UI auto complete loading gif – looking like this:</p><p><img src="https://3.bp.blogspot.com/-RPXGJkm1Lyk/UWQNCcFlmpI/AAAAAAAAAZo/pGP3IubeBiQ/s320/ClearFieldMeetAutoComplete.png"/></p><p>I know; ugly isn&#x27;t it? Happily I was able to resolve this with a CSS <strike>hack</strike></p><p>fix which looks like this:</p><script src="https://gist.github.com/johnnyreilly/5345373.js?file=ie10jQueryUI.css"></script><p>And now the jQuery UI autocomplete looks like we expect during the loading phase:</p><p><img src="https://3.bp.blogspot.com/-mzBwe7BGwjk/UWQNdEKiucI/AAAAAAAAAZw/OmUFi6zMY7g/s320/AutoComplete.png"/></p><p>But happily when the autocomplete is not in the loading phase we still have access to the IE 10 clear field button. This works because the CSS selector above only applies to the <em>ui-autocomplete-loading</em> class (which is only applied to the textbox when the loading is taking place). So we still get to use this:</p><p><img src="https://3.bp.blogspot.com/-HRxuGdc6PhE/UWQMq15gKHI/AAAAAAAAAZg/XmFfFZBMHYk/s320/ClearField.png"/></p><p>Which is nice.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Death to compatibility mode]]></title>
            <link>https://blog.johnnyreilly.com/2013/04/01/death-to-compatibility-mode</link>
            <guid>Death to compatibility mode</guid>
            <pubDate>Mon, 01 Apr 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[For just over 10 years my bread and butter has been the development and maintenance of line of business apps. More particularly, web apps built on the Microsoft stack of love (© Scott Hanselman). These sort of apps are typically accessed via the company intranet and since "bring your own device" is still a relatively new innovation these apps are invariably built for everyones favourite browser: Internet Explorer. As we all know, enterprises are generally not that speedy when it comes to upgrades. So we're basically talking IE 9 at best, but more often than not, IE 8.]]></description>
            <content:encoded><![CDATA[<p>For just over 10 years my bread and butter has been the development and maintenance of line of business apps. More particularly, web apps built on the Microsoft stack of love (<a href="https://channel9.msdn.com/Events/MIX/MIX11/FRM02">© Scott Hanselman</a>). These sort of apps are typically accessed via the company intranet and since &quot;bring your own device&quot; is still a relatively new innovation these apps are invariably built for everyones favourite browser: Internet Explorer. As we all know, enterprises are generally not that speedy when it comes to upgrades. So we&#x27;re basically talking IE 9 at best, but more often than not, IE 8.</p><p>Now, unlike many people, I don&#x27;t regard IE as a work of evil. I spent a fair number of years working for an organization which had IE 6 as the only installed browser on company desktops. (In fact, this was still the case as late as 2012!) Now, because JavaScript is so marvellously flexible I was still able to do a great deal with the help of a number of <a href="http://paulirish.com/2011/the-history-of-the-html5-shiv/">shivs / shims</a>.</p><p>But rendering and CSS - well that&#x27;s another matter. Because here we&#x27;re at the mercy of &quot;compatibility mode&quot;. Perhaps a quick history lesson is in order. What is this &quot;compatibility mode&quot; of which you speak?</p><h2>A Brief History</h2><p>Well it all started when Microsoft released IE 8. To quote them:</p><blockquote><p><em>A fundamental problem discussed during each and every Internet Explorer release is balancing new features and functionality with site compatibility for the existing Web. On the one hand, new features and functionality push the Web forward. On the other hand, the Web is a large expanse; requiring every legacy page to support the &quot;latest and greatest&quot; browser version immediately at product launch just isn&#x27;t feasible. Internet Explorer 8 addresses this challenge by introducing compatibility modes which gives a way to introduce new features and stricter compliance to standards while enabling it to be backward compliant.</em> <!-- -->-<!-- --> excerpted from <a href="https://blogs.msdn.com/b/askie/archive/2009/03/23/understanding-compatibility-modes-in-internet-explorer-8.aspx">understanding compatibility modes in Internet Explorer 8</a>.</p></blockquote><h2>There&#x27;s the rub</h2><p>Sounds fair enough? Of course it does. Microsoft have generally bent over backwards to facilitate backwards compatibility. Quite right too - good business sense and all that. However, one of the choices made around backwards compatibility I&#x27;ve come to regard as somewhat irksome. Later down in the article you&#x27;ll find this doozy: (emphasis mine)</p><blockquote><p><em>&quot;<strong>for Intranet pages, 7 (IE 7 Standards) rendering mode is used by default</strong> and can be changed.&quot;</em></p></blockquote><p>For whatever reason, this decision was not particularly well promoted. As a result, a fair number of devs I&#x27;ve encountered have little or no knowledge of compatibility mode. Certainly it came as a surprise to me. Here was I, developing away on my desktop. I&#x27;d fire up the app hosted on my machine and test on my local install of IE 8. All would look new and shiny (well non-anchor tags would have <code>:hover</code> support). Happy and content, I&#x27;d push to our test system and browse to it. Wait, what&#x27;s happened? Where&#x27;s the new style rendering? What&#x27;s up with my CSS? This is a bug right?</p><p>Obviously I know now it&#x27;s not a bug it&#x27;s a &quot;feature&quot;. And I have learned how to get round the intranet default of compatibility mode through cunning deployment of meta tags and custom http headers. Recently compatibility mode has come to bite me for the second time (in this case I was building for IE 9 and was left wondering where all my rounded corners had vanished to when I deployed...).</p><p>For my own sanity I thought it might be good to document the various ways that exist to solve this particular problem. Just to clarify terms, &quot;solve&quot; in this context means &quot;force IE to render in the most standards compliant / like other browsers fashion it can muster&quot;. You can use compatibility mode to do more than just that and if you&#x27;re interested in more about this then I recommend <a href="http://stackoverflow.com/a/6771584/761388">this Stack Overflow answer</a>.</p><h2>Solution 1: Custom HTTP Header through web.config</h2><p>If you&#x27;re running IIS7 or greater then, for my money, this is the simplest and most pain free solution. All you need do is include the following snippet in your web config file:</p><script src="https://gist.github.com/johnnyreilly/5283462.js?file=web.config"></script><p>This will make IIS serve up the above custom response HTTP header with each page.</p><h2>Solution 2: Custom HTTP Header the hard way</h2><p>Maybe you&#x27;re running II6 and so you making a change to the web.config won&#x27;t make a difference. That&#x27;s fine, you can still get the same behaviour by going to the HTTP headers tab in IIS (see below) and adding the <code>X-UA-Compatible: IE=edge</code> header by hand.</p><p><img src="https://4.bp.blogspot.com/-78CYavaCiUk/UVlGNv87U_I/AAAAAAAAAZQ/qtchMc14JsY/s320/CustomHeadersIIS6.gif"/></p><p>Or, if you don&#x27;t have access to IIS (don&#x27;t laugh - it happens) you can fall back to doing this in code like this:</p><script src="https://gist.github.com/johnnyreilly/5283462.js?file=servingUpTheHardWay.cs"></script><p>Obviously there&#x27;s a whole raft of ways you could get this in, using <code>Application_BeginRequest</code> in <code>Global.asax.cs</code> would probably as good an approach as any.</p><h2>Solution 3: Meta Tags are go!</h2><p>The final approach uses meta tags. And, in my experience it is the most quirky approach - it doesn&#x27;t always seem to work. First up, what do we do? Well, in each page served we include the following meta tag like this:</p><script src="https://gist.github.com/johnnyreilly/5283462.js?file=any.html"></script><p>Having crawled over the WWW equivalent of broken glass I now know why this <!-- -->*<strong>sometimes</strong>*<!-- --> doesn&#x27;t work. (And credit where it&#x27;s due the answer came from <a href="http://stackoverflow.com/a/3960197/761388">here</a>.) It&#x27;s all down to the positioning of the meta tag:</p><blockquote><p><em>The X-UA-compatible header is not case sensitive; however, it must appear in the Web page&#x27;s header (the HEAD section) before all other elements, except for the title element and other meta elements.</em> <!-- -->-<!-- --> excerpted from <a href="http://msdn.microsoft.com/en-gb/library/jj676915(v=vs.85).aspx">specifying legacy document modes</a></p></blockquote><p>That&#x27;s right, get your meta tag in the wrong place and things won&#x27;t work. And you won&#x27;t know why. Lovely. But get it right and it&#x27;s all gravy. This remains the most unsatisfactory approach in my book though.</p><h2>And for bonus points: <code>IFRAME</code>s!</h2><p>Before I finish off I thought it worth sharing a little known feature of <code>IFRAME</code>s. If page is running in compatibility mode and it contains an <code>IFRAME</code> then the page loaded in that <code>IFRAME</code> will <strong>also run in compatibility mode</strong>. No ifs, no buts.</p><p>In the case that I encountered this behaviour, the application was being hosted in an <code>IFRAME</code> inside Sharepoint. Because of the way our Sharepoint was configured it ended up that the only real game in town for us was the meta tags approach - which happily worked once we&#x27;d correctly placed our meta tag.</p><p>Again, it&#x27;s lamentable that this behaviour isn&#x27;t better documented - hopefully the act of writing this here will mean that it becomes a little better known. There&#x27;s probably a good reason for this behaviour, though I&#x27;m frankly, I don&#x27;t know what it is. If anyone does, I&#x27;d be interested.</p><h2>That&#x27;s it</h2><p>Armed with the above I hope you have less compatibility mode pain than I have. The following blog entry is worth a read by the way:</p><p><a href="https://blogs.msdn.com/b/ie/archive/2009/02/16/just-the-facts-recap-of-compatibility-view.aspx">https://blogs.msdn.com/b/ie/archive/2009/02/16/just-the-facts-recap-of-compatibility-view.aspx</a></p><p>Finally, I have an open question about compatibility mode. I <em>think</em> (but I don&#x27;t know) that even in compatibility mode IE runs using the same JavaScript engine. However I suspect it has a different DOM to play with. If anyone knows a little more about this and wants to let me know that&#x27;d be fantastic.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[DecimalModelBinder for nullable Decimals]]></title>
            <link>https://blog.johnnyreilly.com/2013/03/11/decimalmodelbinder-for-nullable-decimals</link>
            <guid>DecimalModelBinder for nullable Decimals</guid>
            <pubDate>Mon, 11 Mar 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[My memory appears to be a sieve. Twice in the last year I've forgotten that MVCs ModelBinding doesn't handle regionalised numbers terribly well. Each time I've thought "hmmmm.... best Google that" and lo and behold come upon this post on the issue by the fantastic Phil Haack:]]></description>
            <content:encoded><![CDATA[<p>My memory appears to be a sieve. Twice in the last year I&#x27;ve forgotten that MVCs ModelBinding doesn&#x27;t handle regionalised numbers terribly well. Each time I&#x27;ve thought &quot;hmmmm.... best Google that&quot; and lo and behold come upon this post on the issue by the fantastic Phil Haack:</p><p><a href="http://haacked.com/archive/2011/03/19/fixing-binding-to-decimals.aspx">http://haacked.com/archive/2011/03/19/fixing-binding-to-decimals.aspx </a></p><p>This post has got me 90% of the way there, the last 10% being me tweaking it so the model binder can handle nullable decimals as well.</p><p>In the expectation I that I may forget this again I thought I&#x27;d note down my tweaks now and hopefully save myself sometime when I&#x27;m next looking at this next...</p><script src="https://gist.github.com/johnnyreilly/5135647.js?file=DecimalModelBinder.cs"></script><h2>And now a question...</h2><p>Why hasn&#x27;t MVC got an out-of-the-box model binder that does this anyway? In Phil Haack&#x27;s original post it looks like they were considering putting this into MVC itself at some point:</p><p>&quot;<em>... In that case, the DefaultModelBinder chokes on the value. This is unfortunate because jQuery Validate allows that value just fine. I’ll talk to the rest of my team about whether we should fix this in the next version of ASP.NET MVC, but for now it’s good to know there’s a workaround...</em>&quot;</p><p>If anyone knows the reason this never made it into core I&#x27;d love to know. Maybe there&#x27;s a good reason?</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Unit testing ModelState]]></title>
            <link>https://blog.johnnyreilly.com/2013/03/03/unit-testing-modelstate</link>
            <guid>Unit testing ModelState</guid>
            <pubDate>Sun, 03 Mar 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[- Me: "It can't be done"]]></description>
            <content:encoded><![CDATA[<ul><li>Me: &quot;It can&#x27;t be done&quot;</li><li>Him: &quot;Yes it can&quot;</li><li>Me: &quot;No it can&#x27;t&quot;</li><li>Him: &quot;Yes it can, I&#x27;ve just done it&quot;</li><li>Me: &quot;Ooooh! Show me ...&quot;</li></ul><p>The above conversation (or one much like it) took place between my colleague Marc Talary and myself a couple of weeks ago. It was one of those faintly embarrassing situations where you state your case with absolute certainty only to subsequently discover that you were <!-- -->*<strong>completely</strong>*<!-- --> wrong. Ah arrogance, thy name is Reilly...</p><p>The disputed situation in this case was ModelState validation in ASP.Net MVC. How can you unit test a models validation driven by <code>DataAnnotations</code>? If at all. Well it can be done, and here&#x27;s how.</p><h2>Simple scenario</h2><p>Let&#x27;s start with a simple model:</p><script src="https://gist.github.com/johnnyreilly/5069901.js?file=CarModel.cs"></script><p>And let&#x27;s have a controller which makes use of that model:</p><script src="https://gist.github.com/johnnyreilly/5069901.js?file=CarController.cs"></script><p>When I was first looking at unit testing this I was slightly baffled by the behaviour I witnessed. I took an invalid model (where the properties set on the model were violating the model&#x27;s validation <code>DataAnnotations</code>):</p><script src="https://gist.github.com/johnnyreilly/5069901.js?file=NaomiCampbell.cs"></script><p>I passed the invalid model to the <code>Edit</code> controller action inside a unit test. My expectation was that the <code>ModelState.IsValid</code> code path would <!-- -->*<strong>not</strong>*<!-- --> be followed as this was <!-- -->*<strong>not</strong>*<!-- --> a valid model. So <code>ModelState.IsValid</code> should evaluate to <code>false</code>, right? Wrong!</p><p>Contrary to my expectation the validity of <code>ModelState</code> is not evaluated on the fly inside the controller. Rather it is determined during the model binding that takes place <!-- -->*<strong>before</strong>*<!-- --> the actual controller action method is called. And that completely explains why during my unit test with an invalid model we find we&#x27;re following the <code>ModelState.IsValid</code> code path.</p><h2>Back to the dispute</h2><p>As this blog post started off I was slightly missing Marc&#x27;s point. I thought he was saying we should be testing the <code>ModelState.IsValid == false</code> code path. And given that <code>ModelState</code> is determined before we reach the controller my view was that the only way to achieve this was through making use of <code>ModelState.AddModelError</code> in our unit test (you can read a good explanation of that <a href="http://stackoverflow.com/a/3816143/761388">here</a>). And indeed we were already testing for this; we were surfacing errors via a <code>JsonResult</code> and so had a test in place to ensure that <code>ModelState</code> errors were transformed in the manner we would expect.</p><p>However, Marc&#x27;s point was actually that we should have unit tests that enforced our design. That is to say, if we&#x27;d decided a certain property on a model was mandatory we should have a test that checked that this was indeed the case. If someone came along later and removed the <code>Required</code> data annotation then we wanted that test to fail.</p><p>It&#x27;s worth saying, we didn&#x27;t want a unit test to ensure that ASP.Net MVC worked as expected. Rather, where we had used DataAnnotations against our models to drive validation, we wanted to ensure the validation didn&#x27;t disappear further down the track. Just to be clear: we wanted to test our code, not Microsoft&#x27;s.</p><h2>Now I get to learn something</h2><p>When I grasped Marc&#x27;s point I thought that the the only way to write these tests would be to make use of reflection. And whilst we could certainly do that I wasn&#x27;t entirely happy with that as a solution. To my mind it was kind of testing &quot;at one remove&quot;, if you see what I mean. What I really wanted was to see that MVC was surfacing validations in the manner I might have hoped. And you can!</p><p>.... Drum roll... Ladies and gents may I present Marc&#x27;s <code>ModelStateTestController</code>:</p><script src="https://gist.github.com/johnnyreilly/5069901.js?file=ModelStateTestController.cs"></script><p>This class is, as you can see, incredibly simple. It is a controller, it inherits from <code>System.Web.Mvc.Controller</code> and establishes a mock context in the constructor using MOQ. This controller exposes a single method: <code>TestTryValidateModel</code>. This method internally determines the controller&#x27;s <code>ModelState</code> given the supplied object by calling off to Mvc&#x27;s (protected) <code>TryValidateModel</code> method (<code>TryValidateModel</code> evaluates <code>ModelState</code>).</p><p>This simple class allows us to test the validations on a model in a simple fashion that stays close to the way our models will actually be used in the wild. It&#x27;s pragmatic and it&#x27;s useful.</p><h2>An example</h2><p>Let me wrap up with an example unit test. The test below makes use of the <code>ModelStateTestController</code> to check the application of the DataAnnotations on our model:</p><script src="https://gist.github.com/johnnyreilly/5069901.js?file=ModelStateUnitTests.cs"></script><h2>Wrapping up</h2><p>In a way I think it&#x27;s a shame that <code>TryValidateModel</code> is a protected method. If it weren&#x27;t it would be simplicity to write a unit test which tested the ModelState directly in context of the action method. It would be possible to get round this by establishing a base controller class which all our controllers would inherit from which implemented the <code>TestTryValidateModel</code> method from above. On the other hand maybe it&#x27;s good to have clarity of the difference between testing model validations and testing controller actions. Something to ponder...</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Unit testing MVC controllers / Mocking UrlHelper]]></title>
            <link>https://blog.johnnyreilly.com/2013/02/18/unit-testing-mvc-controllers-mocking</link>
            <guid>Unit testing MVC controllers / Mocking UrlHelper</guid>
            <pubDate>Mon, 18 Feb 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[I have put a name to my pain...]]></description>
            <content:encoded><![CDATA[<h2>I have put a name to my pain...</h2><p>And it is unit testing ASP.Net MVC controllers.</p><p>Well perhaps that&#x27;s unfair. I have no problem unit testing MVC controllers.... <strong>until</strong> it comes to making use of the &quot;innards&quot; of MVC. Let me be more specific. This week I had a controller action that I needed to test. It looked a little like this:</p><script src="https://gist.github.com/johnnyreilly/4959924.js?file=DemoController.cs"></script><p>Looks fine right? It&#x27;s an action that takes a simple object as an argument. That&#x27;s ok. It returns a JsonResult. No worries. The JsonResult consists of an anonymous class. De nada. The anonymous class has one property that is driven by the controllers <code>UrlHelper</code>. Yeah that shouldn&#x27;t be an issue... <strong>Hold your horses sunshine - you&#x27;re going nowhere!</strong></p><h2>Getting disillusioned</h2><p>Yup, the minute you start pumping in asserts around that <code>UrlHelper</code> driven property you&#x27;re going to be mighty disappointed. What, you didn&#x27;t expect the result to be <code>null</code>? Damn shame.</p><p>Despite <a href="http://msdn.microsoft.com/en-us/magazine/dd942838.aspx">articles</a> on MSDN about how the intention is for MVC to be deliberately testable the sad fact of the matter is that there is a yawning hole around the testing support for controllers in ASP.Net MVC. Whenever you try to test something that makes use of controller &quot;gubbins&quot; you have <strong>serious</strong> problems. And unfortunately I didn&#x27;t find anyone out there who could offer the whole solution.</p><p>After what I can best describe as a day of pain I found a way to scratch my particular itch. I found a way to write unit tests for controllers that made use of UrlHelper. As a bonus I managed to include the unit testing of Routes and Areas (well kind of) too.</p><h2>MvcMockControllers updated</h2><p>This solution is heavily based on the work of Scott Hanselman who <a href="http://www.hanselman.com/blog/ASPNETMVCSessionAtMix08TDDAndMvcMockHelpers.aspx">wrote and blogged about <code>MvcMockHelpers</code></a> back in 2008. Essentially I&#x27;ve taken this and tweaked it so I could achieve my ends. My version of <code>MvcMockHelpers</code> looks a little like this:</p><script src="https://gist.github.com/johnnyreilly/4959924.js?file=MvcMockHelpers.cs"></script><h2>What I want to test</h2><p>I want to be able to unit test the controller <code>Edit</code> method I mentioned earlier. This method calls the <code>Action</code> method on the controllers <code>Url</code> member (which is, in turn, a <code>UrlHelper</code>) to generate a URL for passing pack to the client. The URL generated should fit with the routing mechanism I have set up. In this case the route we expect a URL for was mapped by the following area registration:</p><script src="https://gist.github.com/johnnyreilly/4959924.js?file=DemoAreaRegistration.cs"></script><h2>Enough of the waffle - show me a unit test</h2><p>Now to the meat; here&#x27;s a unit test which demonstrates how this is used:</p><script src="https://gist.github.com/johnnyreilly/4959924.js?file=UnitTestingAnAreaUsingUrlHelper.cs"></script><p>Let&#x27;s go through this unit test and breakdown what&#x27;s happening:</p><ol><li>Arrange</li><li>Act</li><li>Assert</li></ol><p>The most interesting thing you&#x27;ll note is the controller&#x27;s UrlHelper is now generating a URL as we might have hoped. The URL is generated making use of our routing, yay! Finally we&#x27;re also managing to unit test a route registered by our area.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using Expressions with Constructors]]></title>
            <link>https://blog.johnnyreilly.com/2013/02/13/using-expressions-with-constructors</link>
            <guid>Using Expressions with Constructors</guid>
            <pubDate>Wed, 13 Feb 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[Every now and then you think "x should be easy" - and it isn't. I had one of those situations this morning. Something I thought would take 5 minutes had me still pondering 30 minutes later. I finally cracked it (with the help of a colleague - thanks Marc!) and I wanted to note down what I did since I'm sure to forget this.]]></description>
            <content:encoded><![CDATA[<p>Every now and then you think &quot;x should be easy&quot; - and it isn&#x27;t. I had one of those situations this morning. Something I thought would take 5 minutes had me still pondering 30 minutes later. I finally cracked it (with the help of a colleague - thanks Marc!) and I wanted to note down what I did since I&#x27;m sure to forget this.</p><h2>So what&#x27;s the problem?</h2><p>In our project we had a very simple validation class. It looked a bit like this:</p><script src="https://gist.github.com/johnnyreilly/4944545.js?file=FieldValidationBefore.cs"></script><p>I wanted to take this class and extend it to have a constructor which allowed me to specify a Type and subsequently an Expression of that Type that allowed me to specify a property. 10 points if you read the last sentence and understood it without reading it a second time.</p><p>Code is a better illustration; take a look below. I wanted to go from #1 to #2:</p><script src="https://gist.github.com/johnnyreilly/4944545.js?file=HowItIsUsed.cs"></script><p>&quot;Why?&quot; I hear you ask. Well we had a swathe of statements in the code which test each property for a problem and would create a <code>FieldValidation</code> with the very same property name if one was found. There&#x27;s no real problem with that but I&#x27;m a man that likes to refactor. Property names change and I didn&#x27;t want to have to remember to manually go through each <code>FieldValidation</code> keeping these in line. If I was using the actual property name to drive the creation of my <code>FieldValidations</code> then that problem disappears. And I like that.</p><h2>So what&#x27;s the solution?</h2><p>Well it&#x27;s this:</p><script src="https://gist.github.com/johnnyreilly/4944545.js?file=FieldValidationAfter.cs"></script><p>As you can see we have taken the original FieldValidation class and added in a generic constructor which instead of taking <code>string fieldName</code> as a first argument it takes <code>Expression&amp;lt;Func&amp;lt;T, object&amp;gt;&amp;gt; expression</code>. LINQ&#x27;s Expression magic is used to determine the supplied property name which is smashing. If you were wondering, the first <code>MemberExpression</code> code is used for <em>reference</em> types. The <code>UnaryExpression</code> wrapping a <code>MemberExpression</code> code is used for <em>value</em> types. A good explanation of this can be found <a href="http://stackoverflow.com/a/12975480/761388">here</a>.</p><p>My colleague directed me to <a href="http://stackoverflow.com/a/2916344">this crucial StackOverflow answer</a> which provided some much needed direction when I was thrashing. And that&#x27;s it; we&#x27;re done, home free.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Twitter.Bootstrap.MVC4 meet Bootstrap Datepicker *and* get your Internationalization on...]]></title>
            <link>https://blog.johnnyreilly.com/2013/01/14/twitterbootstrapmvc4-meet-bootstrap_14</link>
            <guid>Twitter.Bootstrap.MVC4 meet Bootstrap Datepicker *and* get your Internationalization on...</guid>
            <pubDate>Mon, 14 Jan 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[Last time I wrote about marrying up Twitter.Bootstrap.MVC4 and Bootstrap Datepicker. It came together quite nicely but when I took a more in depth look at what I'd done I discovered a problem. The brief work on regionalisation / internationalisation / localisation / globalisation / whatever it's called this week... wasn't really working. We had problems with the validation.]]></description>
            <content:encoded><![CDATA[<p><a href="http://icanmakethiswork.blogspot.co.uk/2013/01/twitterbootstrapmvc4-meet-bootstrap.html">Last time</a> I wrote about marrying up Twitter.Bootstrap.MVC4 and Bootstrap Datepicker. It came together quite nicely but when I took a more in depth look at what I&#x27;d done I discovered a problem. The brief work on regionalisation / internationalisation / localisation / globalisation / whatever it&#x27;s called this week... wasn&#x27;t really working. We had problems with the validation.</p><p>I also discovered that Stefan Petre&#x27;s Bootstrap Datepicker appears to have been abandoned. Andrew Rowls has taken it on and created a GitHub repository for it <a href="https://github.com/eternicode/bootstrap-datepicker">here</a>. Besides bug fixes he&#x27;s also introduced the ability for the Bootstrap Datepicker to customised for different cultures.</p><p>Since these 2 subjects are linked I tackled them together and thought it might be worth writing up here. You can find the conclusion of my work in a GitHub repository I created <a href="https://github.com/johnnyreilly/BootstrapMvcSample">here</a>.</p><h2>Going global down in Acapulco</h2><p>First step in internationalising any ASP.Net web app is adding the following to the <code>web.config</code>:</p><script src="https://gist.github.com/4528994.js?file=web.config"></script><p>Then I pulled <a href="https://github.com/jquery/globalize">Globalize</a> and the <a href="https://github.com/eternicode/bootstrap-datepicker">Andrew Rowls fork of Bootstrap Datepicker</a> into the project (replacing Stefan&#x27;s original assets). As well as this I pulled in the <code>jQuery.validate.globalize.js</code> extension <a href="http://icanmakethiswork.blogspot.co.uk/2012/09/globalize-and-jquery-validate.html">I wrote about here</a>. (This replaces some of the default jQuery Validate functionality for culture-specific functionality based on Globalize.) This extension depends on a meta tag that is generated using the following file (which also handles the serving up of the relevant JavaScript culture bundles, more of which shortly):</p><script src="https://gist.github.com/4528994.js?file=GlobalizationHelpers.cs"></script><h2>Culture-specific script bundles</h2><p>With all of my dependancies in place I was now ready to press on. Since both Globalize and the new Bootstrap Datepicker come with their own culture-specific JavaScript files it seemed a good idea to make use of ASP.Nets new bundling functionality. This I did here:</p><script src="https://gist.github.com/4528994.js?file=BootstrapBundleConfig.cs"></script><p>The code above creates a script bundle for each culture when the application starts up. This script bundle contains the culture-specific Globalize and Bootstrap Datepicker JavaScript files. If further culture-specific components were added to the application it would make sense to include these here as well.</p><p><code>_BootstrapLayout.basic.cshtml</code> has been amended to make use of the new bundles and also to include a meta tag that will used to drive regionalisation:</p><script src="https://gist.github.com/4528994.js?file=_BootstrapLayout.basic.cshtml"></script><p>To illustrate how this works, a German user running a machine with the de-DE culture would be served up the following 2 files:</p><ul><li><code>globalize.culture.de-DE.js</code></li><li><code>bootstrap-datepicker.de.js</code></li></ul><h2>Where have we got to?</h2><p>With all this done we have now fixed the validation issues we were experiencing previously. This was done by including the Globalize library, the accept-language meta tag and the jQuery Validate Globalize extensions.</p><p>Besides this we&#x27;ve laid the groundwork for introducing internationalised datepickers by introducing Andrew Rowls fork of the Bootstrap Datepicker. That&#x27;s what we&#x27;ll do next...</p><h2>International Bootstrap Datepicker</h2><p>The final steps of switching over to using a culture-specific date picker are achieved by making a change to the Scripts section in the <code>Create.cshtml</code> file. The existing (and very simple) section should be replaced with this:</p><script src="https://gist.github.com/4528994.js?file=Create.cshtml"></script><p>The script above takes the region from the accept-language meta tag and attempts to look up an associated &quot;language&quot; for the Bootstrap Datepicker. If it finds one it uses it, if not then the default language of &quot;en&quot; / English will be used.</p><h2>Summary</h2><p>In this post we:</p><ol><li>fixed the validation issues we&#x27;d introduced by marrying up Twitter.Bootstrap.MVC4 and the Bootstrap Datepicker</li><li>switched over to using the Andrew Rowls fork of Bootstrap Datepicker and made use of the internationalisation functionality it exposes.</li></ol>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Twitter.Bootstrap.MVC4 meet Bootstrap Datepicker]]></title>
            <link>https://blog.johnnyreilly.com/2013/01/09/twitterbootstrapmvc4-meet-bootstrap</link>
            <guid>Twitter.Bootstrap.MVC4 meet Bootstrap Datepicker</guid>
            <pubDate>Wed, 09 Jan 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[Update 14/01/2013]]></description>
            <content:encoded><![CDATA[<h2>Update 14/01/2013</h2><p>Since I wrote this I&#x27;ve taken things on a little further - to read about that go <a href="http://icanmakethiswork.blogspot.co.uk/2013/01/twitterbootstrapmvc4-meet-bootstrap_14.html">here</a>.</p><h2>Getting Responsive</h2><p>It&#x27;s the new year, it&#x27;s time for new things. Long on my list of &quot;things to do&quot; was getting up to speed with <a href="http://en.wikipedia.org/wiki/Responsive_web_design">Responsive web design</a>. No doubt like everyone else I&#x27;ve been hearing more and more about this over the last year (by the way there was a <a href="http://mashable.com/2012/12/11/responsive-web-design/">good article on Mashable</a> about this last month). RWD (in case you don&#x27;t already know) is pretty much about having web interfaces that format their presentation based on the device they&#x27;re running to provide a good user experience. (I kind of think of it as a <a href="http://en.wikipedia.org/wiki/Write_once,_run_anywhere">write once, run anywhere</a> approach - though hopefully without the negative connotations...)</p><p>Rather than diving straight in myself I&#x27;d heard at a user group that it might be worth taking <a href="http://twitter.github.com/bootstrap/">Twitter Bootstrap</a> as a baseline. I&#x27;m a <strike>lazy</strike></p><p>busy fellow so this sounded ideal.</p><h2>I like ASP.Net MVC...</h2><p>... and this flavoured my investigations. I quickly stumbled on an <a href="http://lostechies.com/erichexter/2012/11/20/twitter-bootstrap-mvc4-the-template-nuget-package-for-asp-net-mvc4-projects/">article written by Eric Hexter</a>. Eric had brought together Twitter Bootstrap and ASP.Net MVC 4 in a <a href="http://nuget.org/packages/twitter.bootstrap.mvc4">NuGet package</a>. Excellent work chap!</p><p>To get up and running with Eric&#x27;s work was a straightforward proposition. I...</p><ol><li>Created new MVC 4 application in Visual Studio called “BootstrapMvcSample” using the “Empty” Project Template.</li><li>Executed the following commands at the NuGet Package Manager Console: - <code>Install-Package twitter.bootstrap.mvc4</code><ul><li><code>Install-Package twitter.bootstrap.mvc4.sample</code></li></ul></li></ol><p>Check out the responsive goodness I had when I ran it:</p><p><img src="https://4.bp.blogspot.com/-2ytMlGLGRpo/UO1mqfi7yQI/AAAAAAAAAYs/RRuVGbr8nAg/s400/TwitterBootstrapFullSize.png"/></p><p><img src="https://3.bp.blogspot.com/-780OCEuXoLw/UO1maRJ-CZI/AAAAAAAAAYg/chBHgYMAIJk/s400/TwitterBootstrapTitchyTiny.png"/></p><p>This is just 1 page, with <code>@media</code> queries doing the heavy lifting.</p><h2>Bootstrap Datepicker</h2><p>The eagle-eyed amongst you will have noticed that the edit screen above features a date field. I&#x27;ve long been a fan of datepickers to allow users to enter a date in an application in an intuitive fashion. Until native browser datepickers become the norm we&#x27;ll be relying on some kind of component. Up until now my datepicker of choice has been the <a href="http://jqueryui.com/datepicker/">jQuery UI one</a>. Based on a quick Google it seemed that jQuery UI and Twitter Bootstrap were not necessarily natural bedfellows. (Though <a href="http://addyosmani.github.com/jquery-ui-bootstrap/">Addy Osmani&#x27;s jQuery UI Bootstrap</a> shows some promise...)</p><p>Since I feared ending up down a blind alley I found myself casting around for a Twitter Bootstrap datepicker. I quickly happened upon <a href="http://www.eyecon.ro/bootstrap-datepicker/">Stefan Petre&#x27;s Bootstrap Datepicker</a> which looked just the ticket.</p><h2>Shake hands and play nice...</h2><p>Incorporating the Bootstrap Datepicker into Twitter.Bootstrap.MVC4 was actually a pretty straightforward affair. I added the following datepicker assets to the ASP.Net MVC project as follows:</p><ul><li><code>bootstrap-datepicker.js</code> was added to <code>~\Scripts</code>.</li><li><code>datepicker.css</code> was added to <code>~\Content</code>. I renamed this file to <code>bootstrap-datepicker.css</code> to stay in line with the other css files.</li></ul><p>Once this was done I amended the <code>BootstrapBundleConfig.cs</code> bundles to include these assets. Once this was done the bundle file looked like this:</p><script src="https://gist.github.com/4529746.js?file=BootstrapBundleConfig.cs"></script><p>I then created this folder:<code>~\Views\Shared\EditorTemplates</code>. To this folder I added the following <code>Date.cshtml</code> Partial to hold the datepicker EditorTemplate: (Having this in place meant that properties with the <code>[DataType(DataType.Date)]</code> attribute would automatically use this EditorTemplate when rendering an editor - I understand <code>[UIHint]</code> attributes can be used to the same end.)</p><script src="https://gist.github.com/4529746.js?file=Date.cshtml"></script><p>And finally I amended the <code>Create.cshtml</code> View (which perhaps more accurately might be called the Edit View?) to include a bit of JavaScript at the bottom to initialise any datepickers on the screen.</p><script src="https://gist.github.com/4529746.js?file=Create.cshtml"></script><p>Et voilà - it works!</p><p><img src="https://4.bp.blogspot.com/-_SfaYN2dfuk/UO2JmrqO_gI/AAAAAAAAAZA/Y904hmwcqaI/s400/TwitterBootstrapDatepicker.png"/></p><p>My thanks to <a href="https://twitter.com/ehexter">Eric Hexter</a> and Stefan Petre for doing all the hard work!</p><h2>Still to do</h2><p>I haven&#x27;t really tested how this all fits together (if at all) with browsers running a non-English culture. There may still be a little tinkering require to get that working...</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[HTML to PDF using a WCF Service]]></title>
            <link>https://blog.johnnyreilly.com/2013/01/03/html-to-pdf-using-wcf-service</link>
            <guid>HTML to PDF using a WCF Service</guid>
            <pubDate>Thu, 03 Jan 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[TL; DR - "Talk is cheap. Show me the code."]]></description>
            <content:encoded><![CDATA[<h2>TL; DR - &quot;Talk is cheap. Show me the code.&quot;</h2><p>Some time ago I wrote a <a href="http://icanmakethiswork.blogspot.com/2012/04/making-pdfs-from-html-in-c-using.html">post which demonstrated how you could make PDFs from HTML</a> using C# and <a href="http://code.google.com/p/wkhtmltopdf/">wkhtmltopdf</a>. To my lasting surprise this has been the most popular post I&#x27;ve written. I recently put together an ASP.NET WCF service which exposed this functionality which I thought might be worth sharing. The code can be found on GitHub <a href="https://github.com/johnnyreilly/PdfMakerWcfService">here</a>.</p><h2>A little more detail</h2><p>I should say up front that I&#x27;m still a little ambivalent about how sensible an idea this is. Behind the scenes this WCF service is remotely firing up wkhtmltopdf using <code>System.Diagnostics.Process</code>. I feel a little wary about recommending this as a solution for a variety of not particularly defined reasons. However, I have to say I&#x27;ve found this pretty stable and reliable. Bottom line it seems to work and work consistently. But I though I should include a caveat emptor; there is probably a better approach than this available. Anyway...</p><p>There isn&#x27;t actually a great deal to say about this WCF service. It should (hopefully) just do what it says on the tin. Putting it together didn&#x27;t involve a great deal of work; essentially it takes the code from the initial blog post and just wraps it in a WCF service called <code>PdfMaker</code>. The service exposes 2 methods:</p><ol><li><code>GetPdf</code> <!-- -->-<!-- --> given a supplied URL this method creates a PDF and then returns it as a Stream to the client</li><li><code>GetPdfUrl</code> <!-- -->-<!-- --> given a supplied URL this method creates a PDF and then returns the location of it to the client</li></ol><p>Both of these methods also set a Location header in the response indicating the location of the created PDF.</p><h2>That which binds us</h2><p>The service uses <code>webHttpBinding</code>. This is commonly employed when people want to expose a RESTful WCF service. The reason I&#x27;ve used this binding is I wanted a simple &quot;in&quot; when calling the service. I wanted to be able to call the service via AJAX as well as directly by browsing to the service and supplying a URL-encoded URL like this:</p><p><code>http://localhost:59002/PdfMaker.svc/GetPdf?url=http%3A%2F%2Fnews.ycombinator.com/</code>You may wonder why I&#x27;m using <a href="http://news.ycombinator.com">http://news.ycombinator.com</a> for the example above. I chose this as Hacker News is a very simple site; very few resources and a small page size. This means the service has less work to do when creating the PDF; it&#x27;s a quick demo.</p><p>I should say that this service is arguably <!-- -->*<!-- -->*<!-- -->not<!-- -->*<!-- -->*<!-- --> completely RESTful as each GET operation behind the scenes attempts to create a new PDF (arguably a side-effect). These should probably be POST operations as they create a new resource each time they&#x27;re hit. However, if they were I wouldn&#x27;t be able to just enter a URL into a browser for testing and that&#x27;s really useful. So tough, I shake my fist at the devotees of pure REST on this occasion. (If I should be attacked in the street shortly after this blog is posted then the police should be advised this is good line of inquiry...)</p><h2>Good behaviour</h2><p>It&#x27;s worth noting that <code>automaticFormatSelectionEnabled</code> set to true on the behaviour so that content negotiation is enabled. Obviously for the <code>GetPdf</code> action this is rather meaningless as it&#x27;s a stream that&#x27;s passed back. However, for the <code>GetPdfUrl</code> action the returned string can either be JSON or XML. The Fiddler screenshots below demonstrate this in action:</p><p><img src="https://4.bp.blogspot.com/-CX7w0jI0jTE/UOVaDP5Ae-I/AAAAAAAAAXk/H7zhyYYjPGA/s400/GetPdfUrl%2B-%2BJSON.png"/></p><p><img src="https://4.bp.blogspot.com/-78GBDqI596I/UOVaTchTbBI/AAAAAAAAAXw/rz2Dg4g8BRs/s400/GetPdfUrl%2B-%2BXML.png"/></p><h2>Test Harness</h2><p>As a final touch I added in a test harness in the form of <code>Demo.aspx</code>. If you browse to it you&#x27;ll see a screen a little like this:</p><p><img src="https://2.bp.blogspot.com/-zoyt7ufl9FQ/UOVmD0VPh0I/AAAAAAAAAYE/DnmZmbx-Mxc/s400/PdfMakerDemo.png"/></p><p>It&#x27;s fairly self-explanatory as you can see. And here&#x27;s an example of the output generated when pointing at Hacker News:</p><iframe src="https://docs.google.com/file/d/0B87K8-qxOZGFMGNCUWRneUFsVFU/preview" width="500" height="500"></iframe><p>And that&#x27;s it. If there was a need this service could be easily extended to leverage the <a href="http://madalgo.au.dk/~jakobt/wkhtmltoxdoc/wkhtmltopdf-0.9.9-doc.html">various options</a> that wkhtmltopdf makes available. Hope people find it useful.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Getting up to speed with Bloomberg's Open API...]]></title>
            <link>https://blog.johnnyreilly.com/2012/11/13/a-nicer-net-api-for-bloombergs-open-api</link>
            <guid>Getting up to speed with Bloomberg's Open API...</guid>
            <pubDate>Tue, 13 Nov 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[A good portion of any devs life is usually spent playing with APIs. If you need to integrate some other system into the system you're working on (and it's rare to come upon a situation where this doesn't happen at some point) then it's API time.]]></description>
            <content:encoded><![CDATA[<p>A good portion of any devs life is usually spent playing with APIs. If you need to integrate some other system into the system you&#x27;re working on (and it&#x27;s rare to come upon a situation where this doesn&#x27;t happen at some point) then it&#x27;s API time.</p><p>Some APIs are well documented and nice to use. Some aren&#x27;t. I recently spent a goodly period of time investigating <a href="http://www.openbloomberg.com/open-api/">Bloomberg&#x27;s Open API</a> and it was a slightly painful experience. So much so that I thought it best to write up my own experiences and maybe I can save others time and a bit of pain.</p><p>Also, as I investigated the Bloomberg Open API I found myself coming up with my own little mini-C#-API. (It&#x27;s generally a sure sign you&#x27;ve found an API you don&#x27;t love if you end up writing your own wrapper.) This mini API did the heavy lifting for me and just handed back nicely structured data to deal with. I have included this wrapper here as well.</p><h2>Research</h2><p>The initial plan was to, through code, extract Libor and Euribor rates from Bloomberg. I had access to a Bloomberg terminal and I had access to the internet - what could stop me? After digging around for a little while I found some useful resources that could be accessed from the Bloomberg terminal:</p><ol><li>Typing “<code>WAPI&amp;lt;GO&amp;gt;</code>” into Bloomberg lead me to the Bloomberg API documentation.</li><li>Typing “<code>DOCS 2055451&amp;lt;GO&amp;gt;</code>” into Bloomberg (I know - it&#x27;s a bit cryptic) provided me with samples of how to use the Bloomberg API in VBA</li></ol><p><img src="https://4.bp.blogspot.com/-mZxP0-jXRIo/UKJ8y8Gs5AI/AAAAAAAAAW0/qNyIN9hGBiQ/s400/bloombergwapidocumentation.gif"/></p><p>To go with this I found some useful documentation of the Bloomberg Open API <a href="http://www.openbloomberg.com/files/2012/10/blpapi-developers-guide.pdf">here</a> and I found the .NET Bloomberg Open API itself <a href="http://www.openbloomberg.com/open-api/">here</a>.</p><h2>Hello World?</h2><p>The first goal when getting up to speed with an API is getting it to do something. Anything. Just stick a fork into it and see if it croaks. Sticking a fork into Open API was achieved by taking the 30-odd example apps included in the Bloomberg Open API and running each in turn on the Bloomberg box until I had my &quot;he&#x27;s alive!!&quot; moment. (I did find it surprising that not all of the examples worked - I don&#x27;t know if there&#x27;s a good reason for this...)</p><p>However, when I tried to write my own C# console application to interrogate the Open API it wasn&#x27;t as plain sailing as I&#x27;d hoped. I&#x27;d write something that looked correct, compiled successfully and deploy it onto the Bloomberg terminal only to have it die a sad death whenever I tried to fire it off.</p><p>I generally find the fastest way to get up and running with an API is to debug it. To make calls to the API and then examine, field by field and method by method, what is actually there. This wasn&#x27;t really an option with my console app though. I was using a shared Bloomberg terminal with very limited access. No Visual Studio on the box and no remote debugging enabled.</p><p>It was then that I had something of a eureka moment. I realised that the code in the VBA samples I&#x27;d downloaded from Bloomberg looked quite similar to the C# code samples that shipped with Open API. Hmmmm.... Shortly after this I found myself sat at the Bloomberg machine debugging the Bloomberg API using the VBA IDE in Excel. (For the record, these debugging tools are aren&#x27;t too bad at all - they&#x27;re nowhere near as slick as their VS counterparts but they do the job.) This was my <a href="http://en.wikipedia.org/wiki/Rosetta_Stone">Rosetta Stone</a> <!-- -->-<!-- --> I could take what I&#x27;d learned from the VBA samples and translate that into equivalent C# / .NET code (bearing in mind what I&#x27;d learned from debugging in Excel and in fact sometimes bringing along the VBA comments themselves if they provided some useful insight).</p><h2>He&#x27;s the Bloomberg, I&#x27;m the Wrapper</h2><p>So I&#x27;m off and romping... I have something that works. Hallelujah! Now that that hurdle had been crossed I found myself examining the actual Bloomberg API code itself. It functioned just fine but it did a couple of things that I wasn&#x27;t too keen on:</p><ol><li>The Bloomberg API came with custom data types. I didn&#x27;t want to use these unless it was absolutely necessary - I just wanted to stick to the standard .NET types. This way if I needed to hand data onto another application I wouldn&#x27;t be making each of these applications dependant on the Bloomberg Open API.</li><li>To get the data out of the Bloomberg API there was an awful lot of boilerplate. Code which handled the possibilities of very large responses that might be split into several packages. Code which walked the element tree returned from Bloomberg parsing out the data. It wasn&#x27;t a beacon of simplicity.</li></ol><p>I wanted an API that I could simply invoke with security codes and required fields. And in return I wanted to be passed nicely structured data. As I&#x27;ve already mentioned a desire to not introduce unnecessary dependencies I thought it might well suit to make use of nested Dictionaries. I came up with a simple C# Console project / application which had a reference to the Bloomberg Open API. It contained the following class; essentially my wrapper for Open API operations: (please note this is deliberately a very &quot;bare-bones&quot; implementation)</p><script src="https://gist.github.com/4065815.js?file=BloombergApi.cs"></script><p>The project also contained this class which demonstrates how I made use of my wrapper:</p><script src="https://gist.github.com/4065815.js?file=NicerBloombergApiDemo.cs"></script><p>And here&#x27;s what the output looked like:</p><p><img src="https://1.bp.blogspot.com/-1ghUYqbl0AE/UKJ_3vsuKqI/AAAAAAAAAXI/pPKR5dup48U/s400/Bloomberg.png"/></p><p>This covered my bases. It was simple, it was easy to consume and it didn&#x27;t require any custom types. My mini-API is only really catering for my own needs (unsurprisingly). However, there&#x27;s lots more to the Bloomberg Open API and I may end up taking this further in the future if I encounter use cases that my current API doesn&#x27;t cover.</p><h2>Update (07/12/2012)</h2><p>Finally, a PS. I found in the <a href="http://www.openbloomberg.com/faq/">Open API FAQs</a> that <em>&quot;Testing any of that functionality currently requires a valid Bloomberg Desktop API (DAPI), Server API (SAPI) or Managed B-Pipe subscription. Bloomberg is planning on releasing a stand-alone simulator which will not require a subscription.&quot;</em> There isn&#x27;t any word yet on this stand-alone simulator. I emailed Bloomberg at <a href="mailto:open-tech@bloomberg.net">open-tech@bloomberg.net</a> to ask about this. They kindly replied that <em>&quot;Unfortunately it is not yet available. We understand that this makes testing API applications somewhat impractical, so we&#x27;re continuing to work on this tool.&quot;</em> Fingers crossed for something we can test soon!</p><h2>Note to self (because I keep forgetting)</h2><p>If you&#x27;re looking to investigate what data is available about a security in Bloomberg it&#x27;s worth typing “<code>FLDS&amp;lt;GO&amp;gt;</code>” into Bloomberg. This is the Bloomberg Fields Finder. Likewise, if you&#x27;re trying to find a security you could try typing “<code>SECF&amp;lt;GO&amp;gt;</code>” into Bloomberg as this is the Security Finder.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[XSD/XML Schema Generator + Xsd.exe:Taking the pain out of manual XML]]></title>
            <link>https://blog.johnnyreilly.com/2012/11/02/xsdxml-schema-generator-xsdexe-taking</link>
            <guid>XSD/XML Schema Generator + Xsd.exe:Taking the pain out of manual XML</guid>
            <pubDate>Fri, 02 Nov 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[Is it 2003 again?!?]]></description>
            <content:encoded><![CDATA[<h2>Is it 2003 again?!?</h2><p>I&#x27;ve just discovered Xsd.exe. It&#x27;s not new. Or shiny. And in fact it&#x27;s been around since .NET 1.1. Truth be told, I&#x27;ve been aware of it for years but up until now I&#x27;ve not had need of it. But now now I&#x27;ve investigated it a bit I&#x27;ve found that it, combined with the XSD/XML Schema Generator can make for a nice tool to add to the utility belt.</p><p>Granted XML has long since stopped being sexy. But if you need it, as I did recently, then this is for you.</p><h2>To the XML Batman!</h2><p>Now XML is nothing new to me (or I imagine anyone who&#x27;s been developing within the last 10 years). But most of the time when I use XML I&#x27;m barely aware that it&#x27;s going on - by and large it&#x27;s XML doing the heavy lifting underneath my web services. But the glory of this situation is, I never have to think about it. It just works. All I have to deal with are nice strongly typed objects which makes writing robust code a doddle.</p><p>I recently came upon a situation where I was working with XML in the raw; that is to say strings. I was going to be supplied with strings of XML which would represent various objects. It would be my job to take the supplied XML, extract out the data I needed and proceed accordingly.</p><h2>We Don&#x27;t Need No Validation...</h2><p>I lied!</p><p>In order to write something reliable I needed to be able to validate that the supplied XML was as I expected. So, <a href="http://en.wikipedia.org/wiki/XML_Schema_(W3C)">XSD</a> time. If you&#x27;re familiar with XML then you&#x27;re probably equally familar with XSD which, to quote Wikipedia <em>&quot;can be used to express a set of rules to which an XML document must conform in order to be considered &#x27;valid&#x27;&quot;</em>.</p><p>Now I&#x27;ve written my fair share of XSDs over the years and I&#x27;ve generally found it a slightly tedious exercise. So I was delighted to discover an online tool to simplify the task. It&#x27;s called the <a href="http://www.freeformatter.com/xsd-generator.html">XSD/XML Schema Generator</a>. What this marvellous tool does is allow you to enter an example of your XML which it then uses to reverse engineer an XSD.</p><p>Here&#x27;s an example. I plugged in this:</p><script src="https://gist.github.com/4000326.js?file=contact.xml"></script><p>And pulled out this:</p><script src="https://gist.github.com/4000326.js?file=contact.xsd"></script><p>Fantastic! It doesn&#x27;t matter if the tool gets something slightly wrong; you can tweak the generated XSD to your hearts content. This is great because it does the hard work for you, allowing you to step back, mop your brow and then heartily approve the results. This tool is a labour saving device. Put simply, it&#x27;s a dishwasher.</p><h2>Tools of the Trade</h2><p>How to get to the actual data? I was initially planning to break out the <a href="http://msdn.microsoft.com/en-us/library/system.xml.linq.xdocument(v=vs.100).aspx"><code>XDocument</code></a>, plug in my XSD and use the <code>Validate</code> method. Which would do the job just dandy.</p><p>However I resisted. As much as I like LINQ to XML I turned to use <a href="http://msdn.microsoft.com/en-us/library/x6c1kb0s(v=vs.100).aspx">Xsd.exe</a> instead. As I&#x27;ve mentioned, this tool is as old as the hills. But there&#x27;s gold in them thar hills, listen: <em>&quot;The XML Schema Definition (Xsd.exe) tool generates XML schema or common language runtime classes from XDR, XML, and XSD files, or from classes in a runtime assembly.&quot;</em></p><p>Excited? Thought not. But what this means is we can hurl our XSD at this tool and it will toss back a nicely formatted C# class for me to use. Good stuff! So how&#x27;s it done? Well MSDN is roughly as informative as it ever is (which is to say, not terribly) but fortunately there&#x27;s not a great deal to it. You fire up the Visual Studio Command Prompt (and I advise doing this in Administrator mode to escape permissions pain). Then you enter a command to generate your class. Here&#x27;s an example using the Contact.xsd file we generated earlier:</p><p><code>xsd.exe &quot;C:\\Contact.xsd&quot; /classes /out:&quot;C:\\&quot; /namespace:&quot;MyNameSpace&quot;</code>Generation looks like this:</p><p><img src="https://1.bp.blogspot.com/-TR-eaxshZo8/UJPclxs8JjI/AAAAAAAAAWg/TNKZuyi-8NU/s400/XsdInAction.png"/></p><p>And you&#x27;re left with the lovely Contact.cs class:</p><script src="https://gist.github.com/4000326.js?file=Contact.cs"></script><h2>Justify Your Actions</h2><p>But why is this good stuff? Indeed why is this more interesting than the newer, and hence obviously cooler, LINQ to XML? Well for my money it&#x27;s the following reasons that are important:</p><ol><li>Intellisense - I have always loved this. Call me lazy but I think intellisense frees up the mind to think about what problem you&#x27;re actually trying to solve. Xsd.exe&#x27;s generated classes give me that; I don&#x27;t need to hold the whole data structure in my head as I code.</li><li>Terse code - I&#x27;m passionate about less code. I think that a noble aim in software development is to write as little code as possible in order to achieve your aims. I say this as generally I have found that writing a minimal amount of code expresses the intention of the code in a far clearer fashion. In service of that aim Xsd.exe&#x27;s generated classes allow me to write less code than would be required with LINQ to XML.</li><li>To quote Scott Hanselman &quot;<a href="http://www.hanselman.com/blog/NuGetPackageOfTheWeek6DynamicMalleableEnjoyableExpandoObjectsWithClay.aspx">successful compilation is just the first unit test</a>&quot;. That it is but it&#x27;s a doozy. If I&#x27;m making changes to the code and I&#x27;ve been using LINQ to XML I&#x27;m not going to see the benefits of strong typing that I would with Xsd.exe&#x27;s generated classes. I like learning if I&#x27;ve broken the build sooner rather than later; strong typing gives me that safety net.</li></ol><h2>Serialization / Deserialization Helper</h2><p>As you read this you&#x27;re no doubt thinking &quot;but wait he&#x27;s shown us how to create XSDs from XML and classes from XSDs but how do we take XML and turn it into objects? And how do we turn those objects back into XML?&quot;</p><p>See how I read your mind just there? It&#x27;s a gift. Well, I&#x27;ve written a little static helper class for the very purpose:</p><script src="https://gist.github.com/4000326.js?file=XmlConverter.cs"></script><p>And here&#x27;s an example of how to use it:</p><script src="https://gist.github.com/4000326.js?file=XmlConverterUsage.cs"></script><p>I was tempted to name my methods in tribute to Crockford&#x27;s JSON (namely <code>ToXML</code> becoming <code>stringify</code> and <code>ToObject</code> becoming <code>parse</code>). Maybe later.</p><p>And that&#x27;s us done. Whilst it&#x27;s no doubt unfashionable I think that this is a very useful approach indeed and I commend it to the interweb!</p><h2>Update - using Xsd.exe to generate XSD from XML</h2><p>I was chatting to a friend about this blog post and he mentioned that you can actually use Xsd.exe to generate XSD files from XML as well. He&#x27;s quite right - this feature does exist. To go back to our example from earlier we can execute the following command:</p><p><code>xsd.exe &quot;C:\\Contact.xml&quot; /out:&quot;C:\\&quot; </code>And this will generate the following file:</p><script src="https://gist.github.com/4000326.js?file=Generated by XSD contact.xsd"></script><p>However, the XSD generated above is very much a &quot;Microsoft XSD&quot;; it&#x27;s an XSD which features MS properties and so on. It&#x27;s fine but I think that generally I prefer my XSDs to be as vanilla as possible. To that end I&#x27;m likely to stick to using the XSD/XML Schema Generator as it doesn&#x27;t appear to be possible to get Xsd.exe to generate &quot;vanilla XSD&quot;.</p><p>Thanks to Ajay for bringing it to my attention though.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[MVC 3 meet Dictionary]]></title>
            <link>https://blog.johnnyreilly.com/2012/10/22/mvc-3-meet-dictionary</link>
            <guid>MVC 3 meet Dictionary</guid>
            <pubDate>Mon, 22 Oct 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[Documenting a JsonValueProviderFactory Gotcha]]></description>
            <content:encoded><![CDATA[<h2>Documenting a JsonValueProviderFactory Gotcha</h2><p>About a year ago I was involved in the migration of an ASP.NET WebForms application over to MVC 3. We&#x27;d been doing a lot of AJAX-y / Single Page Application-y things in the project and had come to the conclusion that MVC might be a slightly better fit since we intended to continue down this path.</p><p>During the migration we encountered a bug in MVC 3 concerning Dictionary deserialization. This bug has subsequently tripped me up a few more times as I failed to remember the nature of the problem correctly. So I&#x27;ve written the issue up here as an aide to my own lamentable memory.</p><p>Before I begin I should say that the problem <!-- -->*<u>has been resolved in MVC 4</u></p><p>*<!-- -->. However given that I imagine many MVC 3 projects will not upgrade instantly there&#x27;s probably some value in documenting the issue (and how to work around it). By the way, you can see my initial plea for assistance in <a href="http://stackoverflow.com/q/6881440/761388">this StackOverflow question</a>.</p><h2>The Problem</h2><p>The problem is that deserialization of Dictionary objects does not behave in the expected and desired fashion. When you fire off a dictionary it arrives at your endpoint as the enormously unhelpful <code>null</code>. To see this for yourself you can try using this JavaScript:</p><script src="https://gist.github.com/3931778.js?file=PostDictionaryTest.js"></script><p>With this C#:</p><script src="https://gist.github.com/3931778.js?file=HomeController.cs"></script><p>You get a null <code>null</code> dictionary.</p><p><img src="https://3.bp.blogspot.com/-Lsz_lrqsLF8/UIVcfCzfGrI/AAAAAAAAAVM/gkq0qsVZTMw/s400/MyDictionaryIsNull.png"/></p><p>After a long time googling around on the topic I eventually discovered, much to my surprise, that I was actually tripping over a bug in MVC 3. It was filed by <a href="http://stackoverflow.com/users/29407/darin-dimitrov">Darin Dimitrov</a> of Stack Overflow fame and I found details about it filed as an official bug <a href="http://connect.microsoft.com/VisualStudio/feedback/details/636647/make-jsonvalueproviderfactory-work-with-dictionary-types-in-asp-net-mvc">here</a>. To quote Darin:</p><p>&quot;<em>The System.Web.Mvc.JsonValueProviderFactory introduced in ASP.NET MVC 3 enables action methods to send and receive JSON-formatted text and to model-bind the JSON text to parameters of action methods. Unfortunately it doesn&#x27;t work with dictionaries</em>&quot;</p><h2>The Workaround</h2><p>My colleague found a workaround for the issue <a href="http://stackoverflow.com/a/5397743/761388">here</a>. There are 2 parts to this:</p><ol><li>Dictionaries in JavaScript are simple JavaScript Object Literals. In order to workaround this issue it is necessary to <code>JSON.stringify</code> our Dictionary / JOL before sending it to the endpoint. This is done so a string can be picked up at the endpoint.</li><li>The signature of your action is switched over from a Dictionary reference to a string reference. Deserialization is then manually performed back from the string to a Dictionary within the Action itself.</li></ol><p>I&#x27;ve adapted my example from earlier to demonstrate this; first the JavaScript:</p><script src="https://gist.github.com/3931778.js?file=PostDictionaryTestWorkaround.js"></script><p>Then the C#:</p><script src="https://gist.github.com/3931778.js?file=HomeControllerWorkaround.cs"></script><p>And now we&#x27;re able to get a dictionary:</p><p><img src="https://1.bp.blogspot.com/-7_sHRAsZjbY/UIVnwqH7tRI/AAAAAAAAAVg/jkYd3aHKPF4/s400/MyDictionaryIsNotNull.png"/></p><h2>Summary and a PS</h2><p>So that&#x27;s it; a little unglamourous but this works. I&#x27;m slightly surprised that that wasn&#x27;t picked up before MVC 3 was released but at least it&#x27;s been fixed for MVC 4. I look forward to this blog post being irrelevant and out of date ☺.</p><p>For what it&#x27;s worth in my example above we&#x27;re using the trusty old <code>System.Web.Script.Serialization.JavaScriptSerializer</code> to perform deserialization. My preference is actually to use <a href="http://james.newtonking.com/projects/json-net.aspx">JSON.Nets</a> implementation but for the sake of simplicity I went with .NETs internal one here. To be honest, either is fine to my knowledge.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using Web Optimization with MVC 3]]></title>
            <link>https://blog.johnnyreilly.com/2012/10/05/using-web-optimization-with-mvc-3</link>
            <guid>Using Web Optimization with MVC 3</guid>
            <pubDate>Fri, 05 Oct 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[A while ago I wrote about optimally serving up JavaScript in web applications. I mentioned that Microsoft had come up with a NuGet package called Microsoft ASP.NET Web Optimization which could help with that by minifying and bundling CSS and JavaScript. At the time I was wondering if I would be able to to use this package with pre-existing MVC 3 projects (given that the package had been released together with MVC 4). Happily it turns out you can. But it's not quite as straightforward as I might have liked so I've documented how to get going with this here...]]></description>
            <content:encoded><![CDATA[<p>A while ago I <a href="http://icanmakethiswork.blogspot.com/2012/06/how-im-structuring-my-javascript-in-web.html#WebOptimization">wrote</a> about optimally serving up JavaScript in web applications. I mentioned that Microsoft had come up with a NuGet package called <a href="http://nuget.org/packages/Microsoft.AspNet.Web.Optimization">Microsoft ASP.NET Web Optimization</a> which could help with that by minifying and bundling CSS and JavaScript. At the time I was wondering if I would be able to to use this package with pre-existing MVC 3 projects (given that the package had been released together with MVC 4). Happily it turns out you can. But it&#x27;s not quite as straightforward as I might have liked so I&#x27;ve documented how to get going with this here...</p><h2>Getting the Basics in Place</h2><p>To keep it simple I&#x27;m going to go through taking a &quot;vanilla&quot; MVC 3 app and enhancing it to work with Web Optimization. To start, follow these basic steps:</p><ol><li>Open Visual Studio (bet you didn&#x27;t see that coming!)</li><li>Create a new MVC 3 application (I called mine &quot;WebOptimizationWithMvc3&quot; to demonstrate my imaginative flair). It doesn&#x27;t really matter which sort of MVC 3 project you create - I chose an Intranet application but really that&#x27;s by the by.</li><li>Update pre-existing NuGet packages</li><li>At the NuGet console type: &quot;<code>Install-Package Microsoft.AspNet.Web.Optimization</code>&quot;</li></ol><p>Whilst the NuGet package adds the necessary references to your MVC 3 project it doesn&#x27;t add the corresponding namespaces to the web.configs. To fix this manually add the following child XML element to the <code>&amp;lt;namespaces&amp;gt;</code> element in your root and Views web.config files:</p><p><code>&amp;lt;add namespace=&quot;System.Web.Optimization&quot; /&amp;gt;</code></p><p>This gives you access to <code>Scripts</code> and <code>Styles</code> in your views without needing the fully qualified namespace. For reasons best known to Microsoft I had to close down and restart Visual Studio before intellisense started working. You may need to do likewise.</p><p>Next up we want to get some JavaScript / CSS bundles in place. To do this, create a folder in the root of your project called &quot;App_Start&quot;. There&#x27;s nothing magical about this to my knowledge; this is just a convention that&#x27;s been adopted to store all the bits of startup in one place and avoid clutterage. (I think this grew out of Nuget; see <a href="http://blog.davidebbo.com/2011/02/appstart-folder-convention-for-nuget.html">David Ebbo talking about this here</a>.) Inside your new folder you should add a new class called <code>BundleConfig.cs</code> which looks like this:</p><script src="https://gist.github.com/3839486.js?file=BundleConfig.cs"></script><p>The above is what you get when you create a new MVC 4 project (as it includes Web Optimization out of the box). All it does is create some JavaScript and CSS bundles relating to jQuery, jQuery UI, jQuery Validate, Modernizr and the standard site CSS. Nothing radical here but this example should give you an idea of how bundling can be configured and used. To make use of <code>BundleConfig.cs</code> you should modify your <code>Global.asax.cs</code> so it looks like this:</p><script src="https://gist.github.com/3839486.js?file=Global.asax.cs"></script><p>Once you&#x27;ve done this you&#x27;re ready to start using Web Optimization in your MVC 3 application.</p><h2>Switching over <!-- -->_<!-- -->Layout.cshtml to use Web Optimization</h2><p>With a &quot;vanilla&quot; MVC 3 app the only use of CSS and JavaScript files is found in <code>_Layout.cshtml</code>. To switch over to using Web Optimization you should replace the existing <code>_Layout.cshtml</code> with this: (you&#x27;ll see that the few differences that there are between the 2 are solely around the replacement of link / script tags with references to <code>Scripts</code> and <code>Styles</code> instead)</p><script src="https://gist.github.com/3839486.js?file=_Layout.cshtml"></script><p>Do note that in the above <code>Scripts.Render</code> call we&#x27;re rendering out 3 bundles; jQuery, jQuery UI and jQuery Validate. We&#x27;re not using any of these in <code>_Layout.cshtml</code> but rendering these (and their associated link tags) gives us a chance to demonstrate that everything is working as expected.</p><p>In your root web.config file make sure that the following tag is in place: <code>&amp;lt;compilation debug=&quot;&lt;b&gt;true&lt;/b&gt;&quot; targetFramework=&quot;4.0&quot;&amp;gt;</code>. Then run, the generated HTML should look something like this:</p><script src="https://gist.github.com/3839486.js?file=debug  true"></script><p>This demonstrates that when the application has debug set to true you see the full scripts / links being rendered out as you would hope (to make your debugging less painful).</p><p>Now go back to your root <code>web.config</code> file and chance the debug tag to false: <code>&amp;lt;compilation debug=&quot;&lt;b&gt;false&lt;/b&gt;&quot; targetFramework=&quot;4.0&quot;&amp;gt;</code>. This time when you run, the generated HTML should look something like this:</p><script src="https://gist.github.com/3839486.js?file=debug  false"></script><p>This time you can see that in non-debug mode (ie how it would run in Production) minified bundles of scripts and css files are being served up instead of the raw files. And that&#x27;s it; done.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Unit Testing and Entity Framework: The Filth and the Fury]]></title>
            <link>https://blog.johnnyreilly.com/2012/10/03/unit-testing-and-entity-framework-filth</link>
            <guid>Unit Testing and Entity Framework: The Filth and the Fury</guid>
            <pubDate>Wed, 03 Oct 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[Just recently I've noticed that there appears to be something of a controversy around Unit Testing and Entity Framework. I first came across it as I was Googling around for useful posts on using MOQ in conjunction with EF. I've started to notice the topic more and more and as I have mixed feelings on the subject (that is to say I don't have a settled opinion) I thought I'd write about this and see if I came to any kind of conclusion...]]></description>
            <content:encoded><![CDATA[<p>Just recently I&#x27;ve noticed that there appears to be something of a controversy around Unit Testing and Entity Framework. I first came across it as I was Googling around for useful posts on using MOQ in conjunction with EF. I&#x27;ve started to notice the topic more and more and as I have mixed feelings on the subject (that is to say I don&#x27;t have a settled opinion) I thought I&#x27;d write about this and see if I came to any kind of conclusion...</p><h2>The Setup</h2><p>It started as I was working on a new project. We were using ASP.NET MVC 3 and Entity Framework with DbContext as our persistence layer. Rather than crowbarring the tests in afterwards the intention was to write tests to support the ongoing development. Not quite test driven development but certainly <a href="http://blog.troyd.net/Test+Supported+Development+TSD+Is+NOT+Test+Driven+Development+TDD.aspx">test supported development</a>. (Let&#x27;s not get into the internecine conflict as to whether this is black belt testable code or not - it isn&#x27;t but he who pays the piper etc.) Oh and we were planning to use MOQ as our mocking library.</p><p>It was the first time I&#x27;d used DbContext rather than ObjectContext and so I thought I&#x27;d do a little research on how people were using DbContext with regards to testability. I had expected to find that there was some kind of consensus and an advised way forwards. I didn&#x27;t get that at all. Instead I found a number of conflicting opinions.</p><h2>Using the Repository / Unit of Work Patterns</h2><p>One thread of advice that came out was that people advised using the Repository / Unit of Work patterns as wrappers when it came to making testable code. This is kind of interesting in itself as to the best of my understanding ObjectSet / ObjectContext and DbSet / DbContext are both in themselves implementations of the Repository / Unit of Work patterns. So the advice was to build a Repository / Unit of Work pattern to wrap an existing Repository / Unit of Work pattern.</p><p>Not as mad as it sounds. The reason for the extra abstraction is that ObjectContext / DbContext in the raw are not MOQ-able.</p><h2>Or maybe I&#x27;m wrong, maybe you can MOQ DbContext?</h2><p>No you can&#x27;t. Well that&#x27;s not true. You can and it&#x27;s documented <a href="http://romiller.com/2012/02/14/testing-with-a-fake-dbcontext/">here</a> but there&#x27;s a &quot;but&quot;. You need to be using Entity Frameworks Code First approach; actually coding up your DbContext yourself. Before I&#x27;d got on board the project had already begun and we were already some way down the road of using the Database First approach. So this didn&#x27;t seem to be a go-er really.</p><p>The best article I found on testability and Entity Framework was <a href="http://msdn.microsoft.com/en-us/library/ff714955.aspx">this one</a> by <a href="http://odetocode.com/">K. Scott Allen</a> which essentially detailed how you could implement the Repository / Unit of Work patterns on top of ObjectSet / ObjectContext. In the end I adapted this to do the same thing sat on top of DbSet / DbContext instead.</p><p>With this in place I had me my testable code. I was quite happy with this as it seemed quite intelligible. My new approach looked similar to the existing DbSet / DbContext code and so there wasn&#x27;t a great deal of re-writing to do. Sorted, right?</p><h2>Here come the nagging doubts...</h2><p>I did wonder, given that I found a number of articles about applying the Repository / Unit of Work patterns on top of ObjectSet / ObjectContext that there didn&#x27;t seem to be many examples to do the same for DbSet / DbContext. (I did find a few examples of this but none that felt satisfactory to me for a variety of reasons.) This puzzled me.</p><p>I also started to notice that a 1 man war was being waged against the approach I was using by <a href="http://www.ladislavmrnka.com/about/">Ladislav Mrnka</a>. Here are a couple of examples of his crusade:</p><ul><li><a href="http://stackoverflow.com/a/6904479/761388">An answer on StackOverflow</a> (there&#x27;s quite a few similar answers around on StackOverflow saying similar)</li><li><a href="http://romiller.com/2012/02/14/testing-with-a-fake-dbcontext/#div-comment-1620">A comment on Rowan Millers post about fake DbContexts</a></li></ul><p>Ladislav is quite strongly of the opinion that wrapping DbSet / DbContext (and I presume ObjectSet / ObjectContext too) in a further Repository / Unit of Work is an antipattern. To quote him: <em>&quot;The reason why I don’t like it is leaky abstraction in Linq-to-entities queries ... In your test you have Linq-to-Objects which is superset of Linq-to-entities and only subset of queries written in L2O is translatable to L2E&quot;</em>. It&#x27;s worth looking at <a href="http://www.youtube.com/watch?v=gNeSZYke-_Q">Jon Skeets explanation of &quot;leaky abstractions&quot;</a> which he did for TekPub.</p><p>As much as I didn&#x27;t want to admit it - I have come to the conclusion Ladislav probably has a point for a number of reasons:</p><h3>1<!-- -->.<!-- --> Just because it compiles and passes unit tests don&#x27;t imagine that means it works...</h3><p>Unfortunately, a LINQ query that looks right, compiles and has passing unit tests written for it doesn&#x27;t necessarily work. You can take a query that fails when executed against Entity Framework and come up with test data that will pass that unit test. As Ladislav rightly points out: <code>LINQ-to-Objects != LINQ-to-Entities</code>.</p><p>So in this case unit tests of this sort don&#x27;t provide you with any security. What you need are <!-- -->*<!-- -->*<u>integration</u></p><p>*<!-- -->*<!-- --> tests. Tests that run against an instance of the database and demonstrate that LINQ will actually translate queries / operations into valid SQL.</p><h3>2<!-- -->.<!-- --> Complex queries</h3><p>You can write some pretty complex LINQ queries if you want. This is made particularly easy if you&#x27;re using <a href="https://blogs.msdn.com/b/ericlippert/archive/2009/12/07/query-transformations-are-syntactic.aspx">comprehension syntax</a>. Whilst these queries may be simple to write it can be uphill work to generate test data to satisfy this. So much so that at times it can feel you&#x27;ve made a rod for your own back using this approach.</p><h3>3<!-- -->.<!-- --> Lazy Loading</h3><p>By default Entity Framework employs lazy loading. This a useful approach which reduces the amount of data that is transported. Sometimes this approach forces you to specify up front if you require a particular entity through use of <code>Include</code> statements. This again doesn&#x27;t lend itself to testing particularly well.</p><h2>Where does this leave us?</h2><p>Having considered all of the above for a while and tried out various different approaches I think I&#x27;m coming to the conclusion that Ladislav is probably right. Implementing the Repository / Unit of Work patterns on top of ObjectSet / ObjectContext or DbSet / DbContext doesn&#x27;t seem a worthwhile effort in the end.</p><p>So what&#x27;s a better idea? I think that in the name of simplicity you might as well have a simple class which wraps all of your Entity Framework code. This class could implement an interface and hence be straightforwardly MOQ-able (or alternatively all methods could be virtual and you could forego the interface). Along with this you should have integration tests in place which test the execution of the actual Entity Framework code against a test database.</p><p>Now I should say this approach is not necessarily my final opinion. It seems sensible and practical. I think it is likely to simplify the tests that are written around a project. It will certainly be more reliable than just having unit tests in place.</p><p>In terms of the project I&#x27;m working on at the moment we&#x27;re kind of doing this in a halfway house sense. That is to say, we&#x27;re still using our Repository / Unit of Work wrappers for DbSet / DbContext but where things move away from simple operations we&#x27;re adding extra methods to our Unit of Work class or Repository classes which wrap this functionality and then testing it using our integration tests.</p><p>I&#x27;m open to the possibility that my opinion may be modified further. And I&#x27;d be very interested to know what other people think on the subject.</p><h2>Update</h2><p>It turns out that I&#x27;m not alone in thinking about this issue and indeed others have expressed this rather better than me - take a look at Jimmy Bogard&#x27;s post for an example: <a href="http://lostechies.com/jimmybogard/2012/09/20/limiting-your-abstractions/">http://lostechies.com/jimmybogard/2012/09/20/limiting-your-abstractions/</a>.</p><h2>Update 2</h2><p>I&#x27;ve also recently watched the following Pluralsight course by Julie Lerman: <a href="http://pluralsight.com/training/Courses/TableOfContents/efarchitecture#efarchitecture-m3-archrepo">http://pluralsight.com/training/Courses/TableOfContents/efarchitecture#efarchitecture-m3-archrepo</a>. In this course Julie talks about different implementations of the Repository and Unit of Work patterns in conjunction with Entity Framework. Julie is in favour of using this approach but in this module she elaborates on different &quot;flavours&quot; of these patterns that you might want to use for different reasons (bounded contexts / reference contexts etc). She makes a compelling case and helpfully she is open enough to say that this a point of contention in the community. At the end of watching this I think I felt happy that our &quot;halfway house&quot; approach seems to fit and seems to work. More than anything else Julie made clear that there isn&#x27;t one definitively &quot;true&quot; approach. Rather many different but similar approaches for achieving the same goal. Good stuff Julie!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Giving OData to CRM 4.0]]></title>
            <link>https://blog.johnnyreilly.com/2012/09/24/giving-odata-to-crm-40</link>
            <guid>Giving OData to CRM 4.0</guid>
            <pubDate>Mon, 24 Sep 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[Just recently I was tasked with seeing if we could provide a way to access our Dynamics CRM instance via OData. My initial investigations made it seem like there was nothing for me to do; CRM 2011 provides OData support out of the box. Small problem. We were running CRM 4.0.]]></description>
            <content:encoded><![CDATA[<p>Just recently I was tasked with seeing if we could provide a way to access our Dynamics CRM instance via OData. My initial investigations made it seem like there was nothing for me to do; <a href="http://msdn.microsoft.com/en-us/library/gg309461.aspx">CRM 2011 provides OData support out of the box</a>. Small problem. We were running CRM 4.0.</p><p>It could well have ended there apart from the fact that Microsoft makes it astonishingly easy to to create your own OData service using WCF Data Services. Because it&#x27;s so straightforward I was able to get an OData solution for CRM 4.0 up and running with very little heavy lifting at all. Want to know how it&#x27;s done?</p><h2>LINQ to CRM</h2><p>To start with you&#x27;re going to need the <a href="http://www.microsoft.com/en-us/download/details.aspx?id=38">CRM SDK 4.0</a>. This contains a &quot;vanilla&quot; LINQ to CRM client which is used in each of the example applications that can be found in <code>microsoft.xrm\samples</code>. We want this client (or something very like it) to use as the basis for our OData service.</p><p>In order to get a LINQ to CRM provider that caters for your own customised CRM instance you need to use the <code>crmsvcutil</code> utility from the CRM SDK (found in the <code>microsoft.xrm\tools\</code> directory). Detailed instructions on how to use this can be found in this Word document: <code>microsoft.xrm\advanced_developer_extensions_-_developers_guide.docx</code>. Extra information around the topic can be found using these links:</p><ul><li><a href="http://msdn.microsoft.com/en-us/library/ff681559">MSDN docs on xRM</a></li><li><a href="http://msdn.microsoft.com/en-us/library/ff681573">MSDN examples of LINQ queries</a></li><li><a href="http://www.dynamicscrmtrickbag.com/">CRM blog site</a></li><li><a href="http://community.adxstudio.com/products/adxstudio-portals/developers-guide/archive/linq-to-crm-22/">Another site listing examples of LINQ to CRM</a></li></ul><p>You should end up with custom generated data context classes which look not dissimilar to similar classes that you may already have in place for Entity Framework etc. With your <code>Xrm.DataContext</code> in hand (a subclass of <code>Microsoft.Xrm.Client.Data.Services.CrmDataContext</code>) you&#x27;ll be ready to move forwards.</p><h2>Make me an OData Service</h2><p>As I said, Microsoft makes it fantastically easy to get an OData service up and running. <a href="http://msdn.microsoft.com/en-US/library/dd728275">In this example</a> an entity context model is created from the Northwind database and then exposed as an OData service. To create my CRM OData service I followed a similar process. But rather than creating an entity context model using a database I plugged in the <code>Xrm.DataContext</code> instance of CRM that we created a moment ago. These are the steps I followed to make my service:</p><ol><li><p>Create a new ASP.NET Web Application called &quot;CrmOData&quot; (in case it&#x27;s relevant I was using Visual Studio 2010 to do this).</p></li><li><p>Remove all ASPXs / JavaScript / CSS files etc leaving you with an essentially empty project.</p></li><li><p>Add references to the following DLLs that come with the SDK: - microsoft.crm.sdk.dll</p><ul><li>microsoft.crm.sdktypeproxy.dll</li><li>microsoft.crm.sdktypeproxy.xmlserializers.dll</li><li>microsoft.xrm.client.dll</li><li>microsoft.xrm.portal.dll</li><li>microsoft.xrm.portal.files.dll</li></ul></li><li><p>Add the <code>&amp;lt;microsoft.xrm.client&amp;gt;</code> config section to your web.config (not forgetting the associated Xrm connection string)</p></li><li><p>Add this new file below to the root of the project:</p></li></ol><script src="https://gist.github.com/3765280.js?file=Crm.svc.cs"></script><p>And that&#x27;s it - done. When you run this web application you will find an OData service exposed at http://localhost:12345/Crm.svc. You could have it even simpler if you wanted - you could pull out the logging that&#x27;s in place and leave only the <code>InitializeService</code> there. That&#x27;s all you need. (The <code>GetEntityById</code> method is a helper method of my own for identifying the GUIDs of CRM.)</p><p>You may have noticed that I have made use of caching for my OData service following the steps I found <a href="https://blogs.msdn.com/b/peter_qian/archive/2010/11/17/using-asp-net-output-caching-with-wcf-data-services.aspx">here</a>. Again you may or may not want to use this.</p><h2>Now, a warning...</h2><p>Okay - not so much a warning as a limitation. Whilst most aspects of the OData service work as you would hope there is no support for the $select operator. I had a frustrating time trying to discover why and then came upon this explanation:</p><p><em>&quot;$select statements are not supported. This problem is being discussed here <a href="http://social.msdn.microsoft.com/Forums/en/adodotnetdataservices/thread/366086ee-dcef-496a-ad15-f461788ae678">http://social.msdn.microsoft.com/Forums/en/adodotnetdataservices/thread/366086ee-dcef-496a-ad15-f461788ae678</a> and is caused by the fact that CrmDataContext implements the IExpandProvider interface which in turn causes the DataService to lose support for $select projections&quot;</em></p><p>You can also see <a href="http://social.microsoft.com/Forums/en/crmdevelopment/thread/31daedb4-3d75-483a-8d7f-269af3375d74">here</a> for the original post discussing this.</p><h2>Finishing off</h2><p>In the example I set out here I used the version of WCF Data Services that shipped with Visual Studio 2010. WCF Data Services now ships separately from the .NET Framework and you can <a href="http://nuget.org/packages?q=wcf+data+services">pick up the latest and greatest from Nuget</a>. I understand that you could easily switch over to using the latest versions but since I didn&#x27;t see any feature that I needed on this occasion I haven&#x27;t.</p><p>I hope you find this useful.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Globalize and jQuery Validation]]></title>
            <link>https://blog.johnnyreilly.com/2012/09/06/globalize-and-jquery-validate</link>
            <guid>Globalize and jQuery Validation</guid>
            <pubDate>Thu, 06 Sep 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[Update 05/10/2015]]></description>
            <content:encoded><![CDATA[<h2>Update 05/10/2015</h2><p>If you&#x27;re after a version of this that works with Globalize 1.x then take a look <a href="https://blog.johnnyreilly.com/2015/10/jquery-validation-globalize-hits-10.html">here</a>.</p><h2>Update 27/08/2013</h2><p>To make it easier for people to use the approach detailed in this post I have created a repository for <code>jquery.validate.globalize.js</code> on GitHub <a href="https://github.com/johnnyreilly/jquery-validation-globalize">here</a>.</p><p>This is also available as a nuget package <a href="https://www.nuget.org/packages/jQuery.Validation.Globalize/">here</a>.</p><p>To see a good demo take a look <a href="http://jqueryvalidationunobtrusivenative.azurewebsites.net/AdvancedDemo/Globalize">here</a>.</p><h2>Background</h2><p><a href="http://icanmakethiswork.blogspot.co.uk/2012/05/globalizejs-number-and-date.html">I&#x27;ve written before about a great little library called Globalize</a> which makes locale specific number / date formatting simple within JavaScript. And I&#x27;ve just stumbled upon an <a href="http://www.hanselman.com/blog/GlobalizationInternationalizationAndLocalizationInASPNETMVC3JavaScriptAndJQueryPart1.aspx">old post written by Scott Hanselman about the business of Globalisation / Internationalisation / Localisation within ASP.NET</a>. It&#x27;s a great post and I recommend reading it (I&#x27;m using many of the approaches he discusses).</p><h2>jQuery Global is dead... Long live Globalize!</h2><p>However, there&#x27;s one tweak I would make to Scotts suggestions and that&#x27;s to use Globalize in place of the jQuery Global plugin. The jQuery Global plugin has now effectively been reborn as Globalize (with no dependancy on jQuery). As far as I can tell jQuery Global is now disappearing from the web - certainly the link in Scotts post is dead now at least. I&#x27;ve <del>ripped off</del> been inspired by the &quot;Globalized jQuery Unobtrusive Validation&quot; section of Scotts article and made <code>jquery.validate.globalize.js</code>.</p><p>And for what it&#x27;s worth <code>jquery.validate.globalize.js</code> applies equally to standard jQuery Validation as well as to jQuery Unobtrusive Validation. I say that as the above JavaScript is effectively a monkey patch to the number / date / range / min / max methods of jQuery.validate.js which forces these methods to use Globalize&#x27;s parsing support instead.</p><p>Here&#x27;s the JavaScript:</p><script src="https://gist.github.com/3651751.js?file=jquery.validate.globalize.js"></script><p>The above script does 2 things. Firstly it monkey patches jquery.validate.js to make use of Globalize.js number and date parsing in place of the defaults. Secondly it initialises Globalize to relevant current culture driven by the <code>html lang</code> property. So if the html tag looked like this:</p><pre><code class="language-html">&lt;html lang=&quot;de-DE&quot;&gt;
  ...
&lt;/html&gt;
</code></pre><p>Then Globalize would be initialised with the &quot;de-DE&quot; culture assuming that culture was available and had been served up to the client. (By the way, the Globalize initialisation logic has only been placed in the code above to demonstrate that Globalize needs to be initialised to the culture. It&#x27;s more likely that this initialisation step would sit elsewhere in a &quot;proper&quot; app.)</p><h2>Wait, where&#x27;s <code>html lang</code> getting set?</h2><p>In Scott&#x27;s article he created a <code>MetaAcceptLanguage</code> helper to generate a META tag like this: <code>&amp;lt;meta name=&quot;accept-language&quot; content=&quot;en-GB&quot; /&amp;gt;</code> which he used to drive Globalizes specified culture.</p><p>Rather than generating a meta tag I&#x27;ve chosen to use the <code>lang</code> attribute of the <code>html</code> tag to specify the culture. I&#x27;ve chosen to do this as it&#x27;s more in line with the <a href="http://www.w3.org/TR/i18n-html-tech-lang/#ri20030510.102829377">W3C spec</a>. But it should be noted this is just a different way of achieving exactly the same end.</p><p>So how&#x27;s it getting set? Well, it&#x27;s no great shakes; in my <code>_Layout.cshtml</code> file my html tag looks like this:</p><pre><code class="language-html">&lt;html lang=&quot;@System.Globalization.CultureInfo.CurrentUICulture.Name&quot;&gt;&lt;/html&gt;
</code></pre><p>And in my <code>web.config</code> I have following setting set:</p><pre><code class="language-xml">&lt;configuration&gt;
  &lt;system.web&gt;
    &lt;globalization culture=&quot;auto&quot; uiCulture=&quot;auto&quot; /&gt;
    &lt;!--- Other stuff.... --&gt;
  &lt;/system.web&gt;
&lt;/configuration&gt;
</code></pre><p>With both of these set this means I get <code>&amp;lt;html lang=&quot;de-DE&quot;&amp;gt;</code> or <code>&amp;lt;html lang=&quot;en-GB&quot;&amp;gt;</code> etc. depending on a users culture.</p><h2>Serving up the right Globalize culture files</h2><p>In order that I send the correct Globalize culture to the client I&#x27;ve come up with this static class which provides the user with the relevant culture URL (falling back to the en-GB culture if it can&#x27;t find one based your culture):</p><script src="https://gist.github.com/3651751.js?file=GlobalizeUrls.cs"></script><h2>Putting it all together</h2><p>To make use of all of this together you&#x27;ll need to have the <code>html lang</code> attribute set as described earlier and some scripts output in your layout page like this:</p><pre><code class="language-html">&lt;script src=&quot;@Url.Content(&quot;~/Scripts/jquery.js&quot;)&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;@Url.Content(GlobalizeUrls.Globalize)&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;@Url.Content(GlobalizeUrls.GlobalizeCulture)&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;@Url.Content(&quot;~/Scripts/jquery.validate.js&quot;)&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;@Url.Content(&quot;~/scripts/jquery.validate.globalize.js&quot;)&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;

@* Only serve the following script if you need it: *@
&lt;script src=&quot;@Url.Content(&quot;~/scripts/jquery.validate.unobtrusive.js&quot;)&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
</code></pre><p>Which will render something like this:</p><pre><code class="language-html">&lt;script src=&quot;/Scripts/jquery.js&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;/Scripts/globalize.js&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;script
  src=&quot;/scripts/globalize/globalize.culture.en-GB.js&quot;
  type=&quot;text/javascript&quot;
&gt;&lt;/script&gt;
&lt;script src=&quot;/Scripts/jquery.validate.js&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;script
  src=&quot;/Scripts/jquery.validate.globalize.js&quot;
  type=&quot;text/javascript&quot;
&gt;&lt;/script&gt;
&lt;script
  src=&quot;/Scripts/jquery.validate.unobtrusive.js&quot;
  type=&quot;text/javascript&quot;
&gt;&lt;/script&gt;
</code></pre><p>This will load up jQuery, Globalize, your Globalize culture, jQuery Validate, jQuery Validates unobtrusive extensions (which you don&#x27;t need if you&#x27;re not using them) and the jQuery Validate Globalize script which will set up culture aware validation.</p><p>Finally and just to re-iterate, it&#x27;s highly worthwhile to give <a href="http://www.hanselman.com/blog/GlobalizationInternationalizationAndLocalizationInASPNETMVC3JavaScriptAndJQueryPart1.aspx">Scott Hanselman&#x27;s original article a look</a>. Most all the ideas in here were taken wholesale from him!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How to attribute encode a PartialView in MVC (Razor)]]></title>
            <link>https://blog.johnnyreilly.com/2012/08/24/how-to-attribute-encode-partialview-in</link>
            <guid>How to attribute encode a PartialView in MVC (Razor)</guid>
            <pubDate>Fri, 24 Aug 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[This post is plagiarism. But I'm plagiarising myself so I don't feel too bad.]]></description>
            <content:encoded><![CDATA[<p>This post is plagiarism. But I&#x27;m plagiarising myself so I don&#x27;t feel too bad.</p><p>I posted a <a href="http://stackoverflow.com/q/12093005/761388">question</a> on StackOverflow recently asking if there was a simple way to attribute encode a PartialView in Razor / ASP.NET MVC. I ended up answering my own question and since I thought it was a useful solution it might be worth sharing.</p><h2>The Question</h2><p>In the project I was working on I was using PartialViews to store the HTML that would be rendered in a tooltip in my ASP.NET MVC application. (In case you&#x27;re curious I was using the <a href="http://jquerytools.org/demos/tooltip/index.html">jQuery Tools library for my tooltip</a> effect.)</p><p>I had thought that Razor, clever beast that it is, would automatically attribute encode anything sat between quotes in my HTML. Unfortunately this doesn&#x27;t appear to be the case. In the short term I was able to workaround this by using single quotation marks to encapsulate my PartialViews HTML. See below for an example:</p><pre><code class="language-xml">&lt;div class=&quot;tooltip&quot;
     title=&#x27;@Html.Partial(&quot;_MyTooltipInAPartial&quot;)&#x27;&gt;
    Some content
&lt;/div&gt;
</code></pre><p>Now this worked just fine but I was aware that if any PartialView needed to use single quotation marks I would have a problem. Let&#x27;s say for a moment that <code>_MyTooltipInAPartial.cshtml</code> contained this:</p><pre><code class="language-xml">&lt;span style=&quot;color:green&quot;&gt;fjkdsjf&#x27;lksdjdlks&lt;/span&gt;
</code></pre><p>Well when I used my handy little single quote workaround, the following would result:</p><pre><code class="language-xml">&lt;div class=&quot;tooltip&quot;
     title=&#x27;&lt;span style=&quot;color:green&quot;&gt;fjkdsjf&#x27;lksdjdlks&lt;/span&gt;&#x27;&gt;
    Some content
&lt;/div&gt;
</code></pre><p>Which although it doesn&#x27;t show up so well in the code sample above is definite <em>&quot;does not compute, does not compute, does not compute <!-- -->*<!-- -->LOUD EXPLOSION<!-- -->*<!-- -->&quot;</em> territory.</p><h2>The Answer</h2><p>This took me back to my original intent which was to encapsulate the HTML in double quotes like this:</p><pre><code class="language-xml">&lt;div class=&quot;tooltip&quot;
     title=&quot;@Html.Partial(&quot;_MyTooltipInAPartial&quot;)&quot;&gt;
    Some content
&lt;/div&gt;
</code></pre><p>Though with the example discussed above we clearly had a problem whether we used single or double quotes. What to do?</p><p>Well the answer wasn&#x27;t too complicated. After a little pondering I ended up scratching my own itch by writing an HTML helper method called <code>PartialAttributeEncoded</code> which made use of <code>HttpUtility.HtmlAttributeEncode</code> to HTML attribute encode a PartialView.</p><p>Here&#x27;s the code:</p><script src="https://gist.github.com/3449462.js?file=PartialExtensions.cs"></script><p>Using the above helper is simplicity itself:</p><pre><code class="language-xml">&lt;div class=&quot;tooltip&quot;
     title=&quot;@Html.PartialAttributeEncoded(&quot;_MyTooltipInAPartial&quot;)&quot;&gt;
    Some content
&lt;/div&gt;
</code></pre><p>And, given the example I&#x27;ve been going through, it would provide you with this output:</p><pre><code class="language-xml">&lt;div class=&quot;tooltip&quot;
     title=&quot;&amp;lt;span style=&amp;quot;color:green&amp;quot;&gt;fjkdsjf&amp;#39;lksdjdlks&lt;/span&gt;&quot;&gt;
    Some content
&lt;/div&gt;
</code></pre><p>Now the HTML in the title attribute above might be an unreadable mess - but it&#x27;s the unreadable mess you need. That&#x27;s what the HTML we&#x27;ve been discussing looks like when it&#x27;s been encoded.</p><h2>Final thoughts</h2><p>I was surprised that Razor didn&#x27;t handle this out of the box. I wonder if this is something that will come along with a later version? It&#x27;s worth saying that I experienced this issue when working on an MVC 3 application. It&#x27;s possible that this issue may actually have been solved with MVC 4 already; I haven&#x27;t had chance to check yet though.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ClosedXML - the real SDK for Excel]]></title>
            <link>https://blog.johnnyreilly.com/2012/08/16/closedxml-real-sdk-for-excel</link>
            <guid>ClosedXML - the real SDK for Excel</guid>
            <pubDate>Thu, 16 Aug 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[Simplicity appeals to me. It always has. Something that is simple is straightforward to comprehend and is consequently easy to use. It's clarity.]]></description>
            <content:encoded><![CDATA[<p>Simplicity appeals to me. It always has. Something that is simple is straightforward to comprehend and is consequently easy to use. It&#x27;s clarity.</p><h2>Open XML</h2><p>So imagine my joy when I first encountered <a href="http://msdn.microsoft.com/en-us/office/bb265236.aspx">Open XML</a>. In Microsofts own words:</p><p>ECMA Office Open XML (&quot;Open XML&quot;) is an international, open standard for word-processing documents, presentations, and spreadsheets that can be freely implemented by multiple applications on multiple platforms.</p><p>What does that actually mean? Well, from my perspective in the work I was doing I needed to be able to programmatically interact with Excel documents from C#. I needed to be able to create spreadsheets, to use existing template spreadsheets which I could populate dynamically in code. I needed to do Excel. And according to Microsoft, the Open XML SDK was how I did this.</p><p>What can I say about it? Open XML works. The API functions. You can use this to achieve your aims; and I did (initially). However, there&#x27;s a but and it&#x27;s this: it became quickly apparent just how hard Open XML makes you work to achieve relatively simple goals. Things that ought to be, in my head, a doddle require reams and reams of obscure code. Sadly, I feel that Open XML is probably the most frustrating API that I have yet encountered (and I&#x27;ve coded against the old school Lotus Notes API).</p><h2>Closed XML - Open XML&#x27;s DbContext</h2><p>As I&#x27;ve intimated I found Open XML to be enormously frustrating. I&#x27;d regularly find myself thinking I&#x27;d achieved my goal. I may have written War and Peace code-wise but it compiled, it looked right - the end was in sight. More fool me. I&#x27;d run, sit back watch my Excel doc get created / updated / whatever. Then I&#x27;d open it and be presented with some obscure error about a corrupt file. Not great.</p><p>As I was Googling around looking for answers to my problem that I discovered an open source project on CodePlex called <a href="http://closedxml.codeplex.com/">Closed XML</a>. I wasn&#x27;t alone in frustrations with Open XML - there were many of us sharing the same opinion. And some fantastic person had stepped into the breach to save us! In ClosedXMLs own words:</p><p>ClosedXML makes it easier for developers to create Excel 2007/2010 files. It provides a nice object oriented way to manipulate the files (similar to VBA) without dealing with the hassles of XML Documents. It can be used by any .NET language like C# and Visual Basic (VB).</p><p>Hallelujah!!!</p><p>The way it works (as far as I understand) is that ClosedXML sits on top of Open XML and exposes a really straightforward API for you to interact with. I haven&#x27;t looked into the guts of it but my guess is that it internally uses Open XML to achieve this (as to use ClosedXML you must reference DocumentFormat.OpenXml.dll).</p><p>I&#x27;ve found myself thinking of ClosedXML&#x27;s relationship to Open XML in the same way as I think about Entity Frameworks DbContexts relationship to ObjectContext. They do the same thing but the former in both cases offers a better API. They makes achieving the same goals <!-- -->*<strong>much</strong>*<!-- --> easier. (Although in fairness to the EF team I should say that ObjectContext was not particularly problematic to use; just DbContext made life even easier.)</p><h2>Support - This is how it should be done!</h2><p>Shortly after I started using ClosedXML I was asked if we could use it to perform a certain task. I tested. We couldn&#x27;t.</p><p>When I discovered this <a href="http://closedxml.codeplex.com/workitem/8174">I raised a ticket</a> against the project asking if the functionality was likely to be added at any point. I honestly didn&#x27;t expect to hear back any time soon and was mentally working out ways to get round the issue for now.</p><p>To my surprise within <em>5 hours</em><a href="http://www.codeplex.com/site/users/view/MDeLeon">MDeLeon</a> the developer behind ClosedXML had released a patch to the source code! By any stretch of the imagination that is fast! As it happened there were a few bugs that needed ironing out and over the course of the next 3 working days MDeLeon performed a number of fixes and left me quickly in the position of having a version of ClosedXML which allowed me to achieve my goal.</p><p>So this blog post exists in part to point anyone who is battling Open XML to ClosedXML. It&#x27;s brilliant, well documented and I&#x27;d advise anyone to use it. You won&#x27;t be disappointed. And in part I wanted to say thanks and well done to MDeLeon who quite made my week! Thank you!</p><p><a href="http://closedxml.codeplex.com/">http://closedxml.codeplex.com/</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[jQuery Unobtrusive Validation (+ associated gotchas)]]></title>
            <link>https://blog.johnnyreilly.com/2012/08/06/jquery-unobtrusive-validation</link>
            <guid>jQuery Unobtrusive Validation (+ associated gotchas)</guid>
            <pubDate>Mon, 06 Aug 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[I was recently working on a project which had client side validation manually set up which essentially duplicated the same logic on the server. Like many things this had started out small and grown and grown until it became arduos and tedious to maintain.]]></description>
            <content:encoded><![CDATA[<p>I was recently working on a project which had client side validation manually set up which essentially duplicated the same logic on the server. Like many things this had started out small and grown and grown until it became arduos and tedious to maintain.</p><p>Time to break out the unobtrusive jQuery validation.</p><p>If you’re not aware of this, as part of MVC 3 Microsoft leveraged the pre-existing <a href="http://bassistance.de/jquery-plugins/jquery-plugin-validation/">jQuery Validate library</a> and introduced an “unobtrusive” extension to this which allows the library to be driven by HTML 5 data attributes. I have mentioned this lovely extension before but I haven&#x27;t been using it for the last 6 months or so. And coming back to it I realised that I had forgotten a few of the details / quirks.</p><p>First up, &quot;where do these HTML 5 data attributes come from?&quot; I hear you cry. Why from the <a href="http://msdn.microsoft.com/en-us/library/system.componentmodel.dataannotations.validationattribute.aspx">Validation attributes that live in System.ComponentModel.DataAnnotations</a>.</p><p>Let me illustrate. This decoration:</p><pre><code class="language-cs">[Required(),
   Range(0.01, Double.MaxValue, ErrorMessage = &quot;A positive value is required for Price&quot;),
   Display(Name = &quot;My Price&quot;)]
  public double Price { get; set; }
</code></pre><p>specifies that the Price field on the model is required, that it requires a positive numeric value and that it’s official name is “My Price”. As a result of this decoration, when you use syntax like this in your view:</p><pre><code class="language-xml">@Html.LabelFor(x =&gt; x.Price)
  @Html.TextBoxFor(x =&gt; x.Price, new { id = &quot;itsMyPrice&quot;, type = &quot;number&quot; })
</code></pre><p>You end up with this HTML:</p><pre><code class="language-xml">&lt;label for=&quot;Price&quot;&gt;My Price&lt;/label&gt;
  &lt;input data-val=&quot;true&quot; data-val-number=&quot;The field My Price must be a number.&quot; data-val-range=&quot;A positive value is required for My Price&quot; data-val-range-max=&quot;1.79769313486232E+308&quot; data-val-range-min=&quot;0.01&quot; data-val-required=&quot;The My Price field is required.&quot; id=&quot;itsMyPrice&quot; name=&quot;Price&quot; type=&quot;number&quot; value=&quot;&quot;&gt;
</code></pre><p>As you can see MVC has done the hard work of translating these data annotations into HTML 5 data attributes so you don’t have to. With this in place you can apply your validation in 1 place (the model) and 1 place only. This reduces the code you need to write exponentially. It also reduces duplication and therefore reduces the likelihood of mistakes.</p><p>To validate a form it’s as simple as this:</p><pre><code class="language-js">$(&#x27;form&#x27;).validate();
</code></pre><p>Or if you wanted to validate a single element:</p><pre><code class="language-js">$(&#x27;form&#x27;).validate().element(&#x27;elementSelector&#x27;);
</code></pre><p>Or if you wanted to prevent default form submission until validation was passed:</p><pre><code class="language-js">$(&#x27;form&#x27;).submit(function (event) {
  var isValid = $(this).validate().valid();

  return isValid; //True will allow submission, false will not
});
</code></pre><p>See what I mean? Simple!</p><p>If you want to read up on this further I recommend these links:</p><ul><li><a href="http://bassistance.de/jquery-plugins/jquery-plugin-validation/">The home of jQuery Validate</a> <!-- -->-<!-- --> by the way it seems to be important to work with the latest version (1.9 at time of writing). I found some strange AJAX issues when using 1.7...</li><li><a href="http://bradwilson.typepad.com/blog/2010/10/mvc3-unobtrusive-validation.html">Brad Wilson&#x27;s walkthrough of unobtrusive client validation</a></li><li><a href="http://www.devtrends.co.uk/blog/the-complete-guide-to-validation-in-asp.net-mvc-3-part-2">An example of how to implement your own custom validation both server side <!-- -->*<!-- -->and<!-- -->*<!-- --> client side</a></li><li><a href="http://xhalent.wordpress.com/2011/01/24/applying-unobtrusive-validation-to-dynamic-content/">How to apply unobtrusive jQuery validation to dynamic content</a> <!-- -->-<!-- --> handy if you&#x27;re creating HTML on the client which you want to be validated.</li><li>And finally, a workaround for <a href="http://aspnet.codeplex.com/workitem/7629">a bug in MVC 3</a> which means that data attributes aren’t emitted when using DropDownListFor for nested objects: <a href="http://forums.asp.net/t/1649193.aspx/1/10">http://forums.asp.net/t/1649193.aspx/1/10</a>. In fact because I&#x27;ve only seen this on a forum I&#x27;ve copied and the pasted the code there to below because I feared it being lost: <strong>Update: It turns out the self-same issue exists for TextAreaFor as well. Details of this and a workaround can be found <a href="http://aspnet.codeplex.com/workitem/8576">here</a>... </strong></li></ul><pre><code class="language-cs">/// &lt;summary&gt;
    /// MVC HtmlHelper extension methods - html element extensions
    /// These are drop down list extensions that work round a bug in MVC 3: http://aspnet.codeplex.com/workitem/7629
    /// These workarounds were taken from here: http://forums.asp.net/t/1649193.aspx/1/10
    /// &lt;/summary&gt;
    public static class DropDownListExtensions
    {
        [SuppressMessage(&quot;Microsoft.Design&quot;, &quot;CA1006:DoNotNestGenericTypesInMemberSignatures&quot;, Justification = &quot;This is an appropriate nesting of generic types&quot;)]
        public static MvcHtmlString SelectListFor&lt;TModel, TProperty&gt;(this HtmlHelper&lt;TModel&gt; htmlHelper, Expression&lt;Func&lt;TModel, TProperty&gt;&gt; expression, IEnumerable&lt;SelectListItem&gt; selectList)
        {
            return SelectListFor(htmlHelper, expression, selectList, null /* optionLabel */, null /* htmlAttributes */);
        }


        [SuppressMessage(&quot;Microsoft.Design&quot;, &quot;CA1006:DoNotNestGenericTypesInMemberSignatures&quot;, Justification = &quot;This is an appropriate nesting of generic types&quot;)]
        public static MvcHtmlString SelectListFor&lt;TModel, TProperty&gt;(this HtmlHelper&lt;TModel&gt; htmlHelper, Expression&lt;Func&lt;TModel, TProperty&gt;&gt; expression, IEnumerable&lt;SelectListItem&gt; selectList, object htmlAttributes)
        {
            return SelectListFor(htmlHelper, expression, selectList, null /* optionLabel */, new RouteValueDictionary(htmlAttributes));
        }


        [SuppressMessage(&quot;Microsoft.Design&quot;, &quot;CA1006:DoNotNestGenericTypesInMemberSignatures&quot;, Justification = &quot;This is an appropriate nesting of generic types&quot;)]
        public static MvcHtmlString SelectListFor&lt;TModel, TProperty&gt;(this HtmlHelper&lt;TModel&gt; htmlHelper, Expression&lt;Func&lt;TModel, TProperty&gt;&gt; expression, IEnumerable&lt;SelectListItem&gt; selectList, IDictionary&lt;string, object&gt; htmlAttributes)
        {
            return SelectListFor(htmlHelper, expression, selectList, null /* optionLabel */, htmlAttributes);
        }


        [SuppressMessage(&quot;Microsoft.Design&quot;, &quot;CA1006:DoNotNestGenericTypesInMemberSignatures&quot;, Justification = &quot;This is an appropriate nesting of generic types&quot;)]
        public static MvcHtmlString SelectListFor&lt;TModel, TProperty&gt;(this HtmlHelper&lt;TModel&gt; htmlHelper, Expression&lt;Func&lt;TModel, TProperty&gt;&gt; expression, IEnumerable&lt;SelectListItem&gt; selectList, string optionLabel)
        {
            return SelectListFor(htmlHelper, expression, selectList, optionLabel, null /* htmlAttributes */);
        }


        [SuppressMessage(&quot;Microsoft.Design&quot;, &quot;CA1006:DoNotNestGenericTypesInMemberSignatures&quot;, Justification = &quot;This is an appropriate nesting of generic types&quot;)]
        public static MvcHtmlString SelectListFor&lt;TModel, TProperty&gt;(this HtmlHelper&lt;TModel&gt; htmlHelper, Expression&lt;Func&lt;TModel, TProperty&gt;&gt; expression, IEnumerable&lt;SelectListItem&gt; selectList, string optionLabel, object htmlAttributes)
        {
            return SelectListFor(htmlHelper, expression, selectList, optionLabel, new RouteValueDictionary(htmlAttributes));
        }


        [SuppressMessage(&quot;Microsoft.Design&quot;, &quot;CA1011:ConsiderPassingBaseTypesAsParameters&quot;, Justification = &quot;Users cannot use anonymous methods with the LambdaExpression type&quot;)]
        [SuppressMessage(&quot;Microsoft.Design&quot;, &quot;CA1006:DoNotNestGenericTypesInMemberSignatures&quot;, Justification = &quot;This is an appropriate nesting of generic types&quot;)]
        public static MvcHtmlString SelectListFor&lt;TModel, TProperty&gt;(this HtmlHelper&lt;TModel&gt; htmlHelper, Expression&lt;Func&lt;TModel, TProperty&gt;&gt; expression, IEnumerable&lt;SelectListItem&gt; selectList, string optionLabel, IDictionary&lt;string, object&gt; htmlAttributes)
        {
            if (expression == null)
            {
                throw new ArgumentNullException(&quot;expression&quot;);
            }


            ModelMetadata metadata = ModelMetadata.FromLambdaExpression(expression, htmlHelper.ViewData);


            IDictionary&lt;string, object&gt; validationAttributes = htmlHelper
                .GetUnobtrusiveValidationAttributes(ExpressionHelper.GetExpressionText(expression), metadata);


            if (htmlAttributes == null)
                htmlAttributes = validationAttributes;
            else
                htmlAttributes = htmlAttributes.Concat(validationAttributes).ToDictionary(k =&gt; k.Key, v =&gt; v.Value);


            return SelectExtensions.DropDownListFor(htmlHelper, expression, selectList, optionLabel, htmlAttributes);
        }
    }
</code></pre>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Rendering Partial View to a String]]></title>
            <link>https://blog.johnnyreilly.com/2012/07/16/rendering-partial-view-to-string</link>
            <guid>Rendering Partial View to a String</guid>
            <pubDate>Mon, 16 Jul 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[Well done that man!]]></description>
            <content:encoded><![CDATA[<h2>Well done that man!</h2><p>Every now and then I&#x27;m thinking to myself &quot;<em>wouldn&#x27;t it be nice if you could do x...</em>&quot; And then I discover that someone else has thought the self same thoughts and better yet they have the answer! I had this situation recently and discovered the wonderful Kevin Craft had been there, done that and made the T-shirt. Here&#x27;s his blog: <a href="http://craftycodeblog.com/2010/05/15/asp-net-mvc-render-partial-view-to-string/">http://craftycodeblog.com/2010/05/15/asp-net-mvc-render-partial-view-to-string/</a> I wanted to talk about how this simple post provided me with an elegant solution to something I&#x27;ve found niggling and unsatisfactory for a while now... ## How it helped</p><p>Just last week I was thinking about <code>Partial Views</code>. Some background. I&#x27;m working on an ASP.NET MVC 3 project which provides users with a nice web interface to manage the workflow surrounding certain types of financial asset. The user is presented with a web page which shows a kind of grid to the user. As the user hovers over a row they are presented with a context menu which allows them to perform certain workflow actions. If they perform an action then that row will need to be updated to reflect this. Back in the day this would have been achieved by doing a full postback to the server. At the server the action would be taken, the persistent storage updated and then the whole page would be served up to the user again with the relevant row of HTML updated but everything else staying as is. Now there&#x27;s nothing wrong with this approach as such. I mean it works just fine. But in my case since I knew that it was only that single row of HTML that was going to be updated and so I was loath to re-render the whole page. It seemed a waste to get so much data back from the server when only a marginal amount was due to change. And also I didn&#x27;t want the user to experience the screen refresh flash. Looks ugly. Now in the past when I&#x27;ve had a solution to this problem which from a UI perspective is good but from a development perspective slightly unsatisfactory. I would have my page call a controller method (via <code>jQuery.ajax</code>) to perform the action. This controller would return a <code>JsonResult</code> indicating success or failure and any data necessary to update the screen. Then in the <code>success</code> function I would manually update the HTML on the screen using the data provided. Now this solution works but there&#x27;s a problem. <a href="http://en.wikipedia.org/wiki/Rolf_Harris">Can you tell what it is yet?</a> It&#x27;s not very DRY. I&#x27;m repeating myself. When the page is initially rendered I have a <code>View</code> which renders (in this example) all the relevant HTML for the screen <!-- -->*<!-- -->including<!-- -->*<!-- --> the HTML for my rows of data. And likewise I have my JavaScript method for updating the screen too. So with this solution I have duplicated my GUI logic. If I update 1, I need to update the other. It&#x27;s not a massive hardship but it is, as I say, unsatisfactory. I was recently thinking that it would be nice if I could refactor my row HTML into a <code>Partial View</code> which I could then use in 2 places: 1. In my standard <code>View</code> as I iterated through each element for display and 2. Nested inside a <code>JsonResult</code>...</p><p>The wonderful thing about approach 2 is that it allows me to massively simplify my <code>success</code> to this:</p><pre><code class="language-js">$(&#x27;myRowSelector&#x27;).empty().html(data.RowHTML); //Where RowHTML is the property that
//contains my stringified PartialView
</code></pre><p>and if I later make changes to the <code>Partial View</code> these changes will not require me to make any changes to my JavaScript at all. Brilliant! And entirely satisfactory. On the grounds that someone else might have had the same idea I did a little googling around. Sure enough I discovered <a href="http://craftycodeblog.com/2010/05/15/asp-net-mvc-render-partial-view-to-string/">Kevin Craft&#x27;s post</a> which was just the ticket. It does exactly what I&#x27;d hoped. Besides being a nice and DRY solution this approach has a number of other advantages as well: - Given it&#x27;s a <code>Partial View</code> the Visual Studio IDE provides a nice experience when coding it up with regards to intellisense / highlighting etc. Not something available when you&#x27;re hand coding up a string which contains the HTML you&#x27;d like passed back...</p><ul><li>A wonderful debug experience. You can debug the rendering of a <code>Partial View</code> being rendered to a string in the same way as if the ASP.NET MVC framework was serving it up. I could have lived without this but it&#x27;s fantastic to have it available.</li><li>It&#x27;s possible to nest <!-- -->*<strong>multiple</strong>*<!-- --> <code>Partial Views</code> within your <code>JsonResult</code>. THIS IS WONDERFUL!!! This means that if several parts of your screen need to be updated (perhaps the row and a status panel as well) then as long as both are refactored into a <code>Partial View</code> you can generate them on the fly and pass them back.</li></ul><p>Excellent stuff!</p><pre><code></code></pre>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Optimally Serving Up JavaScript]]></title>
            <link>https://blog.johnnyreilly.com/2012/07/01/how-im-structuring-my-javascript-in-web</link>
            <guid>Optimally Serving Up JavaScript</guid>
            <pubDate>Sun, 01 Jul 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[I have occasionally done some server-side JavaScript with Rhino and Node.js but this is the exception rather than the rule. Like most folk at the moment, almost all the JavaScript I write is in a web context.]]></description>
            <content:encoded><![CDATA[<p>I have occasionally done some server-side JavaScript with Rhino and Node.js but this is the exception rather than the rule. Like most folk at the moment, almost all the JavaScript I write is in a web context.</p><p>Over time I&#x27;ve come to adopt a roughly standard approach to how I structure my JavaScript; both the JavaScript itself and how it is placed / rendered in the an HTML document. I wanted to write about the approach I&#x27;m using. Partly just to document the approach but also because I often find writing about something crystalises my feelings on the subject in one way or another. I think that most of what I&#x27;m doing is sensible and rational but maybe as I write about this I&#x27;ll come to some firmer conclusions about my direction of travel.</p><h2>What are you up to?</h2><p>Before I get started it&#x27;s probably worth mentioning the sort of web development I&#x27;m generally called to do (as this has obviously influenced my decisions).</p><p>Most of my work tends to be on web applications used internally within a company. That is to say, web applications accessible on a Company intranet. Consequently, the user base for my applications tends to be smaller than the Amazons and Googles of this world. It almost invariably sits on the ASP.NET stack in some way. Either classic WebForms or MVC.</p><h2>&quot;Render first. JS second.&quot;</h2><p>I took 2 things away from <a href="http://www.stevesouders.com/blog/2010/09/30/render-first-js-second/">Steve Souder&#x27;s article</a>:</p><ol><li>Async script loading is better than synchronous script loading</li><li>Get your screen rendered and <!-- -->*<strong>then</strong>*<!-- --> execute your JavaScript</li></ol><p>I&#x27;m not doing any async script loading as yet; although I am thinking of giving it a try at some point. In terms of choosing a loader I&#x27;ll probably give RequireJS first crack of the whip (purely as it looks like most people are tending it&#x27;s direction and that can&#x27;t be without reason).</p><p>However - it seems that the concept of async script loading is kind of conflict with one of the other tenets of web wisdom: script bundling. Script bundling, if you&#x27;re not already aware, is the idea that you should combine all your scripts into a single file and then just serve that. This prevents multiple HTTP requests as each script loads in. Async script loading is obviously okay with multiple HTTP requests, presumably because of the asynchronous non-blocking pattern of loading. So. 2 different ideas. And there&#x27;s further movement on this front right now as <a href="http://www.hanselman.com/blog/VisualStudio2012RCIsReleasedTheBigWebRollup.aspx">Microsoft are baking in script bundling to .NET 4.5</a>.</p><p>Rather than divide myself between these 2 horses I have at the moment tried to follow the &quot;JS second&quot; part of this advice in my own (perhaps slightly old fashioned) way...</p><h2>I want to serve you...</h2><p>I have been making sure that scripts are the last thing served to the screen by using a customised version of <a href="http://frugalcoder.us/post/2009/06/29/Handling-Scripts-in-ASPNet-MVC.aspx">Michael J. Ryan&#x27;s HtmlHelper</a>. This lovely helper allows you to add script references as required from a number of different sources (layout page, view, partial view etc - even the controller if you so desired). It&#x27;s simple to control the ordering of scripts by allowing you to set a priority for each script which determines the render order.</p><p>Then as a final step before rendering the <code>&amp;lt;/body&amp;gt;</code> tag the scripts can be rendered in one block. By this point the web page is rendered visually and a marginal amount of blocking is, in my view, acceptable.</p><p>If anyone is curious - the class below is my own version of Michael&#x27;s helper. My contribution is the go faster stripes relating to the caching suffix and the ability to specify dependancies using script references rather than using numeric priority mechanism):</p><script src="https://gist.github.com/3019159.js?file=ScriptExtensions.cs"></script><h2>Minification - I want to serve you less...</h2><p>Another tweak I made to the script helper meant that when compiling either the debug or production (minified) versions of common JS files will be included if available. This means in a production environment the users get minified JS files so faster loading. And in a development environment we get the full JS files which make debugging more straightforward.</p><p>What I haven&#x27;t started doing is minifying my own JS files as yet. I know I&#x27;m being somewhat inconsistent here by sometimes serving minified files and sometimes not. I&#x27;m not proud. Part of my rationale for this that since most of my users use my apps on a daily basis they will for the most part be using cached JS files. Obviously there&#x27;ll be slightly slower load times the first time they go to a page but nothing that significant I hope.</p><p>I have thought of starting to do my own minification as a build step but have held off for now. Again this is something being baked into .NET 4.5; another reason why I have held off doing this a different way for now.</p><p>Update</p><p>It now looks like this Microsofts optimisations have become <a href="http://nuget.org/packages/Microsoft.AspNet.Web.Optimization">this Nuget package</a>. It&#x27;s early days (well it was released on 15th August 2012 and I&#x27;m writing this on the 16th) but I think this looks not to be tied to MVC 4 or .NET 4.5 in which case I could use it in my current MVC 3 projects. I hope so...</p><p>By the way there&#x27;s a <a href="http://www.pluralsight.com/training/Courses/TableOfContents/mvc4#mvc4-m3-optimization">nice rundown of how to use this by K. Scott Allen of Pluralsight</a>. It&#x27;s fantastic. Recommended.</p><p>Update 2</p><p>Having done a little asking around I now understand that this <!-- -->*<strong>can</strong>*<!-- --> be used with MVC 3 / .NET 4.0. Excellent!</p><p>One rather nice alternative script serving mechanism I&#x27;ve seen (but not yet used) is Andrew Davey&#x27;s <a href="http://getcassette.net">Cassette</a> which I mean to take for a test drive soon. This looks fantastic (and is available as a <a href="http://nuget.org/packages/Cassette">Nuget package</a> <!-- -->-<!-- --> 10 points!).</p><h2>CDNs (they want to serve you)</h2><p>I&#x27;ve never professionally made use of CDNs at all. There are <a href="http://encosia.com/3-reasons-why-you-should-let-google-host-jquery-for-you/">clearly good reasons why you should</a> but most of those good reasons relate most to public facing web apps.</p><p>As I&#x27;ve said, the applications I tend to work on sit behind firewalls and it&#x27;s not always guaranteed what my users can see from the grand old world of web beyond. (Indeed what they see can change on hour by hour basis sometimes...) Combined with that, because my apps are only accessible by a select few I don&#x27;t face the pressure to reduce load on the server that public web apps can face.</p><p>So while CDN&#x27;s are clearly a good thing. I don&#x27;t use them at present. And that&#x27;s unlikely to change in the short term.</p><h2>TL:DR</h2><ol><li>I don&#x27;t use CDNs - they&#x27;re clearly useful but they don&#x27;t suit my particular needs</li><li>I serve each JavaScript file individually just before the body tag. I don&#x27;t bundle.</li><li>I don&#x27;t minify my own scripts (though clearly it wouldn&#x27;t be hard) but I do serve the minified versions of 3rd party libraries (eg jQuery) in a Production environment.</li><li>I don&#x27;t use async script loaders at present. I may in future; we shall see.</li></ol><p>I expect some of the above may change (well, possibly not point #1) but this general approach is working well for me at present.</p><p>I haven&#x27;t touched at all on how I&#x27;m structuring my JavaScript code itself. Perhaps next time.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Reasons to be Cheerful (why now is a good time to be a dev)]]></title>
            <link>https://blog.johnnyreilly.com/2012/06/04/reasons-to-be-cheerful-why-now-is-good</link>
            <guid>Reasons to be Cheerful (why now is a good time to be a dev)</guid>
            <pubDate>Mon, 04 Jun 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[I've been a working as a developer in some way, shape or form for just over 10 years now. And it occurred to me the other day that I can't think of a better time to be a software developer than right now]]></description>
            <content:encoded><![CDATA[<p>I&#x27;ve been a working as a developer in some way, shape or form for just over 10 years now. And it occurred to me the other day that I can&#x27;t think of a better time to be a software developer than <u>right now</u></p><p>. This year was better than last year. Last year was better than the year before. This is a happily recurring theme. So why? Well I guess there are a whole host of reasons; this is my effort to identify just some of them... ## Google and the World Wide Web (other search providers are available)</p><p>When I first started out as a humble Delphi developer back in 1999 learning was not the straightforward proposition it is today. If you want to know how to do something these days a good place to start is firing up your browser and putting your question into Google. If I was to ask the question <em>&quot;how do I use AJAX&quot;</em> of a search engine 10 years ago and now I would see very different things. <img src="../static/blog/2012-06-04-reasons-to-be-cheerful-why-now-is-good/AJAX%2Bbleach.jpg"/></p><p>On the left the past, on the right the present. Do try not to let the presence of W3Schools in the search results detract... And also best ignore that the term AJAX wasn&#x27;t coined until 2006... What I&#x27;m getting at is that finding out information these days is can be done really quickly. Excellent search engines are now the norm. Back when I started out this was not the case and you were essentially reliant on what had been written down in books and the kindliness of more experienced developers. Google (and others like them) have done us a great service. They&#x27;ve made it easier to learn. ## Blogs / Screencasts / Training websites</p><p>Something else that has made it easier to learn is the rise and rise of blogs, screencasts and training websites. Over the last 5 years the internet has been filling up with people writing blogs talking about tools, techniques and approaches they are using. When you&#x27;re searching for advice on how to do something you can pretty much guarantee these days that some good soul will have written about it already. The most generous devs out there have gone a step further producing screencasts demonstrating them coding and sharing it with the world <!-- -->*<strong>for free</strong>*<!-- -->. See an example from the ever awesome Rebecca Murphey below:</p><iframe src="https://player.vimeo.com/video/20457625" width="500" height="281" frameBorder="0" mozallowfullscreen=""></iframe><p>Similarly, there are now a number of commercially available screencasts which make it really easy to ramp up and learn. There&#x27;s <a href="http://tekpub.com/">TekPub</a>, there&#x27;s <a href="http://www.pluralsight-training.net">Pluralsight</a> (who have massively improved my commute with their mobile app by the way). All of these help tug away the curtain away from the software development Wizard of Oz. All this is a very wonderful thing indeed! ## Podcasts</p><p>If you&#x27;re a Boogie Down Productions fan then you may be aware of the concept of <a href="http://en.wikipedia.org/wiki/Edutainment_(album)">Edutainment</a>. That is to say, the bridge that can exist between entertainment and education. This is what I&#x27;ve found podcasts to be. I listen to a lot. <a href="http://www.hanselminutes.com/">Hanselminutes</a>. <a href="http://herdingcode.com/">Herding Code</a>. <a href="http://javascriptjabber.com/">JavaScript Jabber</a>. <a href="http://javascriptshow.com/">The JavaScript Show</a>. <a href="http://jesseliberty.com/podcast/">Yet Another Podcast</a>. There&#x27;s more. There&#x27;s something wonderful about about listening to other developers who are passionate about what they are doing. Interested in their work. Enthusiastic about their projects. It&#x27;s infectious. It makes you want to grab a keyboard and start trying things out. I can&#x27;t imagine I&#x27;m the only dev that feels this way. And of course I couldn&#x27;t fail to mention my favourite podcast: <a href="http://www.thisdeveloperslife.com/">This Developer&#x27;s Life</a>. Put together by Scott Hanselman and Rob Conery (I love these guys by the way), and inspired by <a href="http://www.thisamericanlife.org/">This American Life</a>, this show tells some of the stories experienced by developers. It gives an insight into what it&#x27;s like to be a developer. This podcast is more entertaining than educational but it&#x27;s absolutely <!-- -->*<strong>fantastic</strong>*<!-- -->. ## JavaScript (and HTML and CSS too)</p><p>All of the above have eased the learning path of developers and made it easier to keep in touch with the latest and greatest happenings in the dev world. Along with this there has, in my opinion, also been something of a unifying of purpose in the developer community of late. I attribute this to JavaScript, HTML and CSS. Back when I started out it seemed much more the case that developers were split into different tribes. There was the Delphi tribe, the Visual Basic tribe, the C++ tribe, the Java tribe (very much the &quot;hip young gunslingers&quot; tribe back then - I guess these days it&#x27;d be the Node.JS guys) as well as many others. And each tribe more or less seemed to keep themselves to themselves. This wasn&#x27;t malicious that I could tell; that just seemed to be the way it was. But shortly after I started out the idea of the web application took off in a major way. I was involved in this coming from the position of being an early adopter of ASP.NET (which I used, and loved, since it was first in beta). Many other web application technologies were available; JSP, PHP, Perl and the like. But what they all had in common was this: they all pumped out HTML and CSS to the user. Suddenly all these developers from subtly different backgrounds were all targeting the same technology for their GUI. This unifying effect has been <!-- -->*<strong>massively</strong>*<!-- --> reinforced by JavaScript. Whereas HTML is a markup language, JavaScript is a programming language. And more by accident than grand design JavaScript has kind of become the <a href="http://www.hanselman.com/blog/JavaScriptIsAssemblyLanguageForTheWebPart2MadnessOrJustInsanity.aspx">VM of the web</a>. Given the rise and rise of the rich web client (driven onwards and upwards by the popularity of AJAX, Backbone.JS etc) this has meant that devs of all creeds and colours have been forced to pitch a tent on the same patch of dirt. Pretty much all of the tribes now have an embassy in JavaScript land. So there are all these devs out there who are used to working with different server-side technologies from each other. But when it comes to the client, we are all sharing the common language of JavaScript. To a certain extent we&#x27;re all creating data services that just pump out JSON to the client. Through forums like <a href="http://stackoverflow.com/">StackOverflow</a> devs of all the tribes are helping each other with web client &quot;stuff&quot;. They&#x27;re all interacting in ways that they probably wouldn&#x27;t otherwise if the web client environment was as diverse as the server-side environment... ## The Browser Wars Begin Again</p><p>Didn&#x27;t things seem a little dull around 2003/2004? IE 6 had come out 3 years previously and had vanquished all comers. Microsoft was really the only game in town browser-wise. Things had stopped changing; it seemed like browsers were &quot;done&quot;. You know, good enough and there was no need to take things any further. Then came Firefox. This lone browser appeared as an alternative to might of IE. I must admit the thing that first attracted me to Firefox was the fact it had tabs. I mean technically I knew Firefox was more secure than IE but honestly it was the tabs that attracted me in the first place. (This may offer some insight as to why so many people still smoke...) And somehow Firefox managed to jolt Microsoft out of it&#x27;s inertia on the web. Microsoft started caring about IE again. (Not enough until quite recently in my book but you&#x27;ve got to start somewhere.) I&#x27;m a firm believer that change for it&#x27;s own sake can often be a good thing. Change makes you think about why you do what you do and wonder if there might be better approaches that could be used instead. And these changes kind of feed into... ## ...HTML 5!</p><p>That&#x27;s right HTML 5 which is all about change. It&#x27;s taking HTML as we know and love it and bolting on new stuff. New elements (canvas), new styling (CSS 3), new JavaScript APIs, faster JavaScript engines, support for JavaScript 5. The list goes on... And all this new stuff is exciting, whizzy, fun to play with. That which wasn&#x27;t possible yesterday is possible now. Playing with new toys is half the fun of being a dev. There&#x27;s a lot of new toys about right now. ## The Feeling of Possibilites</p><p>This is what it comes down to I think. It&#x27;s so easy to learn these days and there&#x27;s so much to learn about. Right now lots of things are happening above and beyond what I&#x27;ve mentioned above. Open source has come of age and gone mainstream. Github is with us. Google are making contentious forays into new languages with Dart and Native Client. Microsoft aren&#x27;t remotely evil empire-like these days; they&#x27;ve made .NET like a Swiss army knife. You can even run Node.js on IIS these days! Signal-R, Websockets, Coffeescript, JS.Next, Backbone.JS, Entity Framework, LINQ, the mobile web, ASP.NET MVC, Razor, Knockout.JS, the cloud, Windows Azure... So much is happening right now. People are making things. It&#x27;s a very interesting time to be a dev. There are many reasons to be cheerful.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Dad Didn't Buy Any Games]]></title>
            <link>https://blog.johnnyreilly.com/2012/05/30/dad-didnt-buy-any-games</link>
            <guid>Dad Didn't Buy Any Games</guid>
            <pubDate>Wed, 30 May 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[Inspired by Hanselmans post on how he got started in programming I thought I'd shared my own tale about how it all began... I grew up the 80's just outside London. For those of you of a different vintage let me paint a picture. These were the days when "Personal Computers", as they were then styled, were taking the world by storm. Every house would be equipped with either a ZX Spectrum, a Commodore 64 or an Amstrad CPC. These were 8 bit computers which were generally plugged into the family television and spent a good portion of their time loading games like Target]]></description>
            <content:encoded><![CDATA[<p>Inspired by <a href="http://www.hanselman.com/blog/SheLetMeTakeTheComputerHomeHowDidYouGetStartedInComputersAndProgramming.aspx">Hanselmans post on how he got started in programming</a> I thought I&#x27;d shared my own tale about how it all began... I grew up the 80&#x27;s just outside London. For those of you of a different vintage let me paint a picture. These were the days when &quot;Personal Computers&quot;, as they were then styled, were taking the world by storm. Every house would be equipped with either a ZX Spectrum, a Commodore 64 or an Amstrad CPC. These were 8 bit computers which were generally plugged into the family television and spent a good portion of their time loading games like <a href="http://en.wikipedia.org/wiki/Target:_Renegade">Target: Renegade</a> from an audio cassette. But not in our house; we didn&#x27;t have a computer. I remember mournfully pedalling home from friends houses on a number of occasions, glum as I compared my lot with theirs. Whereas my friends would be spending their evenings gleefully battering their keyboards as they thrashed the life out of various end-of-level bosses I was reduced to <!-- -->*<strong>wasting</strong>*<!-- --> my time reading. That&#x27;s right Enid Blyton - you were second best in my head. Then one happy day (and it may have been a Christmas present although I&#x27;m not certain) our family became the proud possessors of an <a href="http://en.wikipedia.org/wiki/Amstrad_CPC">Amstrad CPC 6128</a>: <img src="../static/blog/2012-05-30-dad-didnt-buy-any-games/CPC6128.jpg"/></p><p>Glory be! I was going to play so many games! I would have such larks! My evenings would be filled with pixelated keyboard related destruction! Hallelujah!! But I was wrong. I had reckoned without my father. For reasons that I&#x27;ve never really got to the bottom of Dad had invested in the computer but not in the games. Whilst I was firmly of the opinion that these 2 went together like Lennon and McCartney he was having none of it. &quot;You can write your own son&quot; he intoned and handed over a manual which contained listings for games: <img src="../static/blog/2012-05-30-dad-didnt-buy-any-games/6a0120a85dcdae970b0120a86ddeee970b.png"/></p><p>And that&#x27;s where it first began really. I would spend my evenings typing the Locomotive Basic listings for computer games into the family computer. Each time I started I would be filled with great hopes for what might result. Each time I tended to be rewarded with something that looked a bit like this: <img src="../static/blog/2012-05-30-dad-didnt-buy-any-games/images.jpg"/></p><p>I&#x27;m not sure that it&#x27;s possible to learn to program by osmosis but if it is I&#x27;m definitely a viable test case. I didn&#x27;t become an expert Locomotive Basic programmer (was there ever such a thing?) but I did undoubtedly begin my understanding of software.... Thanks Dad!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Globalize.js - number and date localisation made easy]]></title>
            <link>https://blog.johnnyreilly.com/2012/05/07/globalizejs-number-and-date</link>
            <guid>Globalize.js - number and date localisation made easy</guid>
            <pubDate>Mon, 07 May 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[I wanted to write about a JavaScript library which seems to have had very little attention so far. And that surprises me as it's]]></description>
            <content:encoded><![CDATA[<p>I wanted to write about a JavaScript library which seems to have had very little attention so far. And that surprises me as it&#x27;s</p><ol><li>Brilliant!</li><li>Solves a common problem that faces many app developers who work in the wonderful world of web; myself included</li></ol><p>The library is called Globalize.js and can be found on <a href="https://github.com/jquery/globalize">GitHub here</a>. Globalize.js is a simple JavaScript library that allows you to format and parse numbers and dates in culture specific fashion.</p><h2>Why does this matter?</h2><p>Because different countries and cultures do dates and numbers in different ways. Christmas Day this year in England will be <code>25/12/2012</code> (dd/MM/yyyy). But for American eyes this should be <code>12/25/2012</code> (M/d/yyyy). And for German <code>25.12.2012</code> (dd.MM.yyyy). Likewise, if I was to express numerically the value of &quot;one thousand exactly - to 2 decimal places&quot;, as a UK citizen I would do it like so: <code>1,000.00</code>. But if I was French I&#x27;d express it like this: <code>1.000,00</code>. You see my point?</p><h2>Why does this matter to me?</h2><p>For a number of years I&#x27;ve been working on applications that are used globally, from London to Frankfurt to Shanghai to New York to Singapore and many other locations besides. The requirement has always been to serve up localised dates and numbers so users experience of the system is more natural. Since our applications are all ASP.NET we&#x27;ve never really had a problem server-side. Microsoft have blessed us with all the goodness of <a href="http://msdn.microsoft.com/en-us/library/system.globalization.aspx">System.Globalization</a> which covers hundreds of different cultures and localisations. It makes it frankly easy:</p><pre><code class="language-cs">using System.Globalization;

//Produces: &quot;06.05.2012&quot;
new DateTime(2012,5,6).ToString(&quot;d&quot;, new CultureInfo(&quot;de-DE&quot;));

//Produces: &quot;45,56&quot;
45.56M.ToString(&quot;n&quot;, new CultureInfo(&quot;fr-FR&quot;));
</code></pre><p>The problem has always been client-side. If you need to localise dates and numbers on the client what do you do?</p><h2>JavaScript Date / Number Localisation - the Status Quo</h2><p>Well to be frank - it&#x27;s a bit rubbish really. What&#x27;s on offer natively at present basically amounts to this:</p><ul><li><a href="https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/Date/toLocaleDateString">Date.toLocaleDateString</a></li><li><a href="https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/Number/ToLocaleString">Number.ToLocaleString</a></li></ul><p>This is better than nothing - but not by much. There&#x27;s no real control or flexibility here. If you don&#x27;t like the native localisation format or you want something slightly different then tough. This is all you&#x27;ve got to play with.</p><p>For the longest time this didn&#x27;t matter too much. Up until relatively recently the world of web was far more about the thin client and the fat server. It would be quite standard to have all HTML generated on the server. And, as we&#x27;ve seen .NET (and many other back end enviroments as well) give you all the flexiblility you might desire given this approach.</p><p><a href="http://www.youtube.com/watch?v=k2sYIIjS-cQ">But the times they are a-changing</a>. And given the ongoing explosion of HTML 5 the rich client is very definitely with us. So we need tools.</p><h2>Microsoft doing <!-- -->*<!-- -->good things<!-- -->*</h2><p>Hands up who remembers when Microsoft first shipped it&#x27;s <a href="http://msdn.microsoft.com/en-us/magazine/cc163300.aspx">ASP.NET AJAX</a> library back in 2007?</p><p>Well a small part of this was the extensions ASP.NET AJAX added to JavaScripts native Date and Number objects.... These extensions allowed the localisation of Dates and Numbers to the current UI culture and the subsequent string parsing of these back into Dates / Numbers. These extensions pretty much gave JavaScript the functionality that the server already had in <code>System.Globalization</code>. (not quite like-for-like but near enough the mark)</p><p>I&#x27;m not aware of a great fuss ever being made about this - a fact I find surprising since one would imagine this is a common need. There&#x27;s good documentation about this on MSDN - here&#x27;s some useful links:</p><ul><li><a href="http://msdn.microsoft.com/en-us/library/bb386572.aspx">Ajax Script Globalization and Localization</a></li><li><a href="http://msdn.microsoft.com/en-us/library/bb386581.aspx">Walkthrough: Globalizing a Date by Using Client Script</a></li><li><a href="http://msdn.microsoft.com/en-us/library/bb397506.aspx">JavaScript Base Type Extensions</a></li><li><a href="http://msdn.microsoft.com/en-us/library/bb397521.aspx">Date.parseLocale</a></li><li><a href="http://msdn.microsoft.com/en-us/library/bb383816.aspx">Date.localeFormat</a></li><li><a href="http://msdn.microsoft.com/en-us/library/bb310813.aspx">Number.localeFormat</a></li><li><a href="http://msdn.microsoft.com/en-us/library/bb310985.aspx">Number.parseLocale</a></li></ul><p>When our team became aware of this we started to make use of it in our web applications. I imagine we weren&#x27;t alone...</p><h2>Microsoft doing <!-- -->*<!-- -->even better things<!-- -->*<!-- --> (Scott Gu to the rescue!)</h2><p>I started to think about this again when MVC reared it&#x27;s lovely head.</p><p>Like many, I found I preferred the separation of concerns / testability etc that MVC allowed. As such, our team was planning to, over time, migrate our ASP.NET WebForms applications over to MVC. However, before we could even begin to do this we had a problem. Our JavaScript localisation was dependant on the ScriptManager. The <a href="http://msdn.microsoft.com/en-us/library/system.web.ui.scriptmanager.aspx">ScriptManager</a> is very much a WebForms construct.</p><p>What to do? To the users it wouldn&#x27;t be acceptable to remove the localisation functionality from the web apps. The architecture of an application is, to a certain extent, meaningless from the users perspective - they&#x27;re only interested in what directly impacts them. That makes sense, even if it was a problem for us.</p><p>Fortunately the Great Gu had it in hand. Lo and behold the <a href="http://forum.jquery.com/topic/proposal-for-a-globalization-plugin-jquery-glob-js">this post</a> appeared on the jQuery forum and the following post appeared on Guthrie&#x27;s blog:</p><p><a href="http://weblogs.asp.net/scottgu/archive/2010/06/10/jquery-globalization-plugin-from-microsoft.aspx">http://weblogs.asp.net/scottgu/archive/2010/06/10/jquery-globalization-plugin-from-microsoft.aspx</a></p><p>Yes that&#x27;s right. Microsoft were giving back to the jQuery community by contributing a jQuery globalisation plug-in. They&#x27;d basically taken the work done with ASP.NET AJAX Date / Number extensions, jQuery-plug-in-ified it and put it out there. Fantastic!</p><p>Using this we could localise / globalise dates and numbers whether we were working in WebForms or in MVC. Or anything else for that matter. If we were suddenly seized with a desire to re-write our apps in PHP we&#x27;d <!-- -->*<strong>still</strong>*<!-- --> be able to use Globalize.js on the client to handle our regionalisation of dates and numbers.</p><h2>History takes a funny course...</h2><p>Now for my part I would have expected that this announcement to be followed in short order by dancing in the streets and widespread adoption. Surprisingly, not so. All went quiet on the globalisation front for some time and then out of the blue the following comment appeared on the jQuery forum by <a href="http://rdworth.org/blog/">Richard D. Worth</a> (he of jQuery UI fame):</p><p><a href="http://blog.jquery.com/2011/04/16/official-plugins-a-change-in-the-roadmap/#comment-527484">http://blog.jquery.com/2011/04/16/official-plugins-a-change-in-the-roadmap/#comment-527484</a></p><p>The long and short of which was:</p><ul><li>The jQuery UI team were now taking care of (the re-named) Globalize.js library as the grid control they were developing had a need for some of Globalize.js&#x27;s goodness. Consequently a home for Globalize.js appeared on the jQuery UI website: <a href="http://wiki.jqueryui.com/Globalize">http://wiki.jqueryui.com/Globalize</a></li><li>The source of Globalize.js moved to this location on GitHub: <a href="https://github.com/jquery/globalize/">https://github.com/jquery/globalize/</a></li><li>Perhaps most significantly, the jQuery globalisation plug-in as developed by Microsoft had now been made a standalone JavaScript library. This was clearly brilliant news for Node.js developers as they would now be able to take advantage of this and perform localisation / globalisation server-side - they wouldn&#x27;t need to have jQuery along for the ride. Also, this would be presumably be good news for users of other client side JavaScript libraries like Dojo / YUI etc.</li></ul><p>Globalize.js clearly has a rosy future in front of it. Using the new Globalize.js library was still simplicity itself. Here&#x27;s some examples of localising dates / numbers using the German culture:</p><pre><code class="language-js">&lt;script
  src=&quot;/Scripts/Globalize/globalize.js&quot;
  type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;script
  src=&quot;/Scripts/Globalize/cultures/globalize.culture.de-DE.js&quot;
  type=&quot;text/javascript&quot;&gt;&lt;/script&gt;

Globalize.culture(&quot;de-DE&quot;);

//&quot;2012-05-06&quot; - ISO 8601 format
Globalize.format(new Date(2012,4,6), &quot;yyyy-MM-dd&quot;);

//&quot;06.05.2012&quot; - standard German short date format of dd.MM.yyyy
Globalize.format(new Date(2012,4,6), Globalize.culture().calendar.patterns.d);

//&quot;4.576,3&quot; - a number rendered to 1 decimal place
Globalize.format(4576.34, &quot;n1&quot;);
</code></pre><h2>Stick a fork in it - it&#x27;s done</h2><p>The entry for Globalize.js on the jQuery UI site reads as follows:</p><blockquote><p><em>&quot;version: 0.1.0a1 (not a jQuery UI version number, as this is a standalone utility) status: in development (part of Grid project)&quot;</em></p></blockquote><p>I held back from making use of the library for some time, deterred by the &quot;in development&quot; status. However, I had a bit of dialog with one of the jQuery UI team (I forget exactly who) who advised that the API was unlikely to change further and that the codebase was actually pretty stable. Our team did some testing of Globalize.js and found this very much to be case. Everything worked just as we expected and hoped. We&#x27;re now using Globalize.js in a production environment with no problems reported; it&#x27;s been doing a grand job.</p><p>In my opinion, Number / Date localisation on the client is ready for primetime right now - it works! Unfortunately, because Globalize.js has been officially linked in with the jQuery UI grid project it seems unlikely that this will officially ship until the grid does. Looking at the jQuery UI <a href="http://wiki.jqueryui.com/Roadmap">roadmap</a> the grid is currently slated to release with jQuery UI 2.1. There isn&#x27;t yet a release date for jQuery UI 1.9 and so it could be a long time before the grid actually sees the light of day.</p><p>I&#x27;m hoping that the jQuery UI team will be persuaded to &quot;officially&quot; release Globalize.js long before the grid actually ships. Obviously people can use Globalize.js as is right now (as we are) but it seems a shame that many others will be missing out on using this excellent functionality, deterred by the &quot;in development&quot; status. Either way, <a href="http://www.youtube.com/watch?v=qEMytPF8YuY">the campaign to release Globalise.js officially starts here!</a></p><h2>The Future?</h2><p>There are plans to bake globalisation right into JavaScript natively with EcmaScript 5.1. There&#x27;s a good post on the topic <a href="http://generatedcontent.org/post/59403168016/esintlapi">here</a>. And here&#x27;s a couple of historical links worth reading too:</p><p><a href="http://norbertlindenberg.com/2012/02/ecmascript-internationalization-api/">http://norbertlindenberg.com/2012/02/ecmascript-internationalization-api/</a><a href="http://wiki.ecmascript.org/doku.php?id=globalization:specification_drafts">http://wiki.ecmascript.org/doku.php?id=globalization:specification_drafts</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Beg, Steal or Borrow a Decent JavaScript DateTime Converter]]></title>
            <link>https://blog.johnnyreilly.com/2012/04/28/beg-steal-or-borrow-decent-javascript</link>
            <guid>Beg, Steal or Borrow a Decent JavaScript DateTime Converter</guid>
            <pubDate>Sat, 28 Apr 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[I've so named this blog post because it shamelessly borrows from the fine work of others 1. http 2. http://n8v.enteuxis.org/2010/12/parsing-iso-8601-dates-in-javascript/]]></description>
            <content:encoded><![CDATA[<p>I&#x27;ve so named this blog post because it shamelessly borrows from the fine work of others: Sebastian Markbåge and Nathan Vonnahme. Sebastian wrote a blog post documenting a good solution to the ASP.NET JavaScriptSerializer DateTime problem at the tail end of last year. However, his solution didn&#x27;t get me 100% of the way there when I tried to use it because of a need to support IE 8 which lead me to use Nathan Vonnahme&#x27;s ISO 8601 JavaScript Date parser. I thought it was worth documenting this, hence this post, but just so I&#x27;m clear; the hard work here was done by Sebastian Markbåge and Nathan Vonnahme and not me. Consider me just a curator in this case. The original blog posts that I am drawing upon can be found here: 1. <a href="http://blog.calyptus.eu/seb/2011/12/custom-datetime-json-serialization/">http://blog.calyptus.eu/seb/2011/12/custom-datetime-json-serialization/</a> and here: 2. <a href="http://n8v.enteuxis.org/2010/12/parsing-iso-8601-dates-in-javascript/">http://n8v.enteuxis.org/2010/12/parsing-iso-8601-dates-in-javascript/</a></p><h2>DateTime, JSON, JavaScript Dates....</h2><p>Like many, I&#x27;ve long been frustrated with the quirky DateTime serialisation employed by the <code>System.Web.Script.Serialization.JavaScriptSerializer</code> class. When serialising DateTimes so they can be JSON.parsed on the client, this serialiser uses the following approach: (from MSDN) <a href="http://msdn.microsoft.com/en-us/library/system.web.script.serialization.javascriptserializer.aspx"><em>Date object, represented in JSON as &quot;\/Date(number of ticks)\/&quot;. The number of ticks is a positive or negative long value that indicates the number of ticks (milliseconds) that have elapsed since midnight 01 January, 1970 UTC.&quot;</em></a> Now this is not particularly helpful in my opinion because it&#x27;s not human readable (at least not this human; perhaps <a href="http://stackoverflow.com/users/22656/jon-skeet">Jon Skeet</a>...) When consuming your data from web services / PageMethods using <a href="http://api.jquery.com/jQuery.ajax/">jQuery.ajax</a> you are landed with the extra task of having to convert what were DateTimes on the server from Microsofts string Date format (eg <code>&quot;\/Date(1293840000000)\/&quot;</code>) into actual JavaScript Dates. It&#x27;s also unhelpful because it&#x27;s divergent from the approach to DateTime / Date serialisation used by a native JSON serialisers: <img src="../static/blog/2012-04-28-beg-steal-or-borrow-decent-javascript/FireBug-Dates.png"/></p><p>Just as an aside it&#x27;s worth emphasising that one of the limitations of JSON is that the JSON.parsing of a JSON.stringified date will <!-- -->*<strong>not</strong>*<!-- --> return you to a JavaScript Date but rather an ISO 8601 date string which will need to be subsequently converted into a Date. Not JSON&#x27;s fault - essentially down to the absence of a Date literal within JavaScript. ## Making JavaScriptSerializer behave more JSON&#x27;y</p><p>Anyway, I didn&#x27;t think there was anything I could really do about this in an ASP.NET classic / WebForms world because, to my knowledge, it is not possible to swap out the serialiser that is used. JavaScriptSerializer is the only game in town. (Though I am optimistic about the future; given the announcement that I first picked up on Rick Strahl&#x27;s blog that <a href="http://www.west-wind.com/weblog/posts/2012/Mar/09/Using-an-alternate-JSON-Serializer-in-ASPNET-Web-API">Json.NET was going to be adopted as the default JSON serializer for ASP.NET Web API</a>; what with Json.NET having out-of-the-box <a href="http://james.newtonking.com/archive/2009/02/20/good-date-times-with-json-net.aspx">ISO 8601 support</a>. I digress...) Because it can make debugging a much more straightforward process I place a lot of value on being able to read the network traffic that web apps generate. It&#x27;s much easier to drop into Fiddler / FireBug / Chrome dev tools etc and watch what&#x27;s happening there and then instead of having to manually process the data separately first so that you can understand it. I think this is nicely aligned with the <a href="http://en.wikipedia.org/wiki/KISS_principle">KISS principle</a>. For that reason I&#x27;ve been generally converting DateTimes to ISO 8601 strings on the server before returning them to the client. A bit of extra overhead but generally worth it for the gains in clarity in my opinion. So I was surprised and delighted when I happened upon <a href="http://blog.calyptus.eu/seb/2011/12/custom-datetime-json-serialization/">Sebastian Markbåge&#x27;s blog post</a> which provided a DateTime JavaScriptConverter that could be plugged into the JavaScriptSerializer. You can see the code below (or on Sebastian&#x27;s original post with a good explanation of how it works): <script src="https://gist.github.com/2489976.js?file=DateTimeJavaScriptConverter.cs"></script></p><p>Using this converter meant that a DateTime that previously would have been serialised as <code>&quot;\/Date(1293840000000)\/&quot;</code> would now be serialised as <code>&quot;2011-01-01T00:00:00.0000000Z&quot;</code> instead. This is entirely agreeable because 1. it&#x27;s entirely clear what a <code>&quot;2011-01-01T00:00:00.0000000Z&quot;</code> style date represents and 2. this is more in line with native browser JSON implementations and <code>&amp;lt;statingTheObvious&amp;gt;</code>consistency is a good thing.<code>&amp;lt;/statingTheObvious&amp;gt;</code></p><h2>Getting your web services to use the ISO 8601 DateTime Converter</h2><p>Sebastian alluded in his post to a <code>web.config</code> setting that could be used to get web services / pagemethods etc. implementing his custom DateTime serialiser. This is it: <script src="https://gist.github.com/2489976.js?file=web.config"></script></p><p>With this in place your web services / page methods will happily be able to serialise / deserialise ISO style date strings to your hearts content. ## What no ISO 8601 date string Date constructor?</p><p>As I mentioned earlier, Sebastian&#x27;s solution didn&#x27;t get me 100% of the way there. There was still a fly in the ointment in the form of IE 8. Unfortunately IE 8 doesn&#x27;t have JavaScript <a href="https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/Date/parse">Date constructor that takes ISO 8601 date strings</a>. This lead me to using Nathan Vonnahme&#x27;s ISO 8601 JavaScript Date parser, the code of which is below (or see his original post <a href="http://n8v.enteuxis.org/2010/12/parsing-iso-8601-dates-in-javascript/">here</a>): <script src="https://gist.github.com/2489976.js?file=parseISO8601Date.js"></script></p><p>With this in place I could parse ISO 8601 Dates just like anyone else. Great stuff. <code>parseISO8601Date(&quot;2011-01-01T00:00:00.0000000Z&quot;)</code> would give me a JavaScript Date of <code>Sat Jan 1 00:00:00 UTC 2011</code>. Obviously in the fullness of time the parseISO8601Date solution should no longer be necessary because <a href="http://es5.github.com/#x15.9.3.2">EcmaScript 5 specifies an ISO 8601 date string constructor</a>. However, in the interim Nathan&#x27;s solution is a lifesaver. Thanks again both to Sebastian Markbåge and Nathan Vonnahme who have both generously allowed me use their work as the basis for this post. ## PS And it would have worked if it wasn&#x27;t for that pesky IE 9...</p><p>Subsequent to writing this post I thought I&#x27;d check that IE 9 had implemented a JavaScript Date constructor that would process an ISO 8601 date string like this: <code>new Date(&quot;2011-01-01T00:00:00.0000000Z&quot;)</code>. It hasn&#x27;t. Take a look: <img src="../static/blog/2012-04-28-beg-steal-or-borrow-decent-javascript/IE9%2B%2528shakes%2Bfist%2529.png"/></p><p>This is slightly galling as the above code works dandy in Firefox and Chrome. As you can see from the screenshot you can get the JavaScript IE 9 Date constructor to play nice by trimming off the final 4 &quot;0&quot;&#x27;s from the string. Frustrating. Obviously we can still use Nathan&#x27;s solution but it&#x27;s a shame that we can&#x27;t use the native support. Based on what I&#x27;ve read <a href="http://msdn.microsoft.com/en-us/library/az4se3k1.aspx#Roundtrip">here</a> I think it would be possible to amend Sebastians serializer to fall in line with IE 9&#x27;s pendantry by changing this:</p><pre><code class="language-cs">return new CustomString(((DateTime)obj).ToUniversalTime()
  .ToString(&lt;b&gt;&quot;O&quot;&lt;/b&gt;)
);
</code></pre><p>To this:</p><pre><code class="language-cs">return new CustomString(((DateTime)obj).ToUniversalTime()
  .ToString(&lt;b&gt;&quot;yyyy&#x27;-&#x27;MM&#x27;-&#x27;dd&#x27;T&#x27;HH&#x27;:&#x27;mm&#x27;:&#x27;ss&#x27;.&#x27;fffzzz&quot;&lt;/b&gt;)
);
</code></pre><p>I&#x27;ve held off from doing this myself as I rather like Sebastian&#x27;s idea of being able to use Microsoft&#x27;s Round-trip (&quot;O&quot;, &quot;o&quot;) Format Specifier. And it seems perverse that we should have to move away from using Microsoft&#x27;s Round-trip Format Specifier purely because of (Microsoft&#x27;s) IE! But it&#x27;s a possibility to consider and so I put it out there. I would hope that MS will improve their JavaScript Date constructor with IE 10. A missed opportunity if they don&#x27;t I think. ## PPS Just when you thought is over... IE 9 was right!</p><p>Sebastian got in contact after I first published this post and generously pointed out that, contrary to my expectation, IE 9 technically had the correct implementation. According to the <a href="http://es5.github.com/#x15.9.1.15">EMCAScript standard</a> the Date constructor should not allow more than millisecond precision. In this case, Chrome and Firefox are being less strict - not more correct. On reflection this does rather make sense as the result of a <code>JSON.stringify(new Date())</code> never results in an ISO date string to the 10 millionths of a second detail. Sebastian has himself stopped using Microsoft&#x27;s Round-trip (&quot;O&quot;, &quot;o&quot;) Format Specifier in favour of this format string: ```cs
return new CustomString(((DateTime)obj).ToUniversalTime()</p><p>.ToString(<b>&quot;yyyy-MM-ddTHH:mm:ss.fffZ&quot;</b>)</p><p>);</p><pre><code>
 This results in date strings that comply perfectly with the ECMAScript spec. I suspect I&#x27;ll switch to using this also now. Though I&#x27;ll probably leave the first part of the post intact as I think the background remains interesting. Thanks again Sebastian!
</code></pre>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[JSHint - Customising your hurt feelings]]></title>
            <link>https://blog.johnnyreilly.com/2012/04/23/jshint-customising-your-hurt-feelings</link>
            <guid>JSHint - Customising your hurt feelings</guid>
            <pubDate>Mon, 23 Apr 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[As I've started making greater use of JavaScript to give a richer GUI experience the amount of JS in my ASP.NET apps has unsurprisingly ballooned. If I'm honest, I hadn't given much consideration to the code quality of my JavaScript in the past. However, if I was going to make increasing use of it (and given the way the web is going at the moment I'd say that's a given) I didn't think this was tenable position to maintain. A friend of mine works for Coverity which is a company that provides tools for analysing code quality. I understand, from conversations with him, that their tools provide static analysis for compiled languages such as C++ / C# / Java etc. I was looking for something similar for JavaScript. Like many, I have read and loved Douglas Crockford's "JavaScript]]></description>
            <content:encoded><![CDATA[<p>As I&#x27;ve started making greater use of JavaScript to give a richer GUI experience the amount of JS in my ASP.NET apps has unsurprisingly ballooned. If I&#x27;m honest, I hadn&#x27;t given much consideration to the code quality of my JavaScript in the past. However, if I was going to make increasing use of it (and given the way the web is going at the moment I&#x27;d say that&#x27;s a given) I didn&#x27;t think this was tenable position to maintain. A friend of mine works for <a href="http://www.coverity.com/">Coverity</a> which is a company that provides tools for analysing code quality. I understand, from conversations with him, that their tools provide static analysis for compiled languages such as C++ / C# / Java etc. I was looking for something similar for JavaScript. Like many, I have read and loved <a href="http://www.amazon.com/JavaScript-Good-Parts-Douglas-Crockford/dp/0596517742">Douglas Crockford&#x27;s &quot;JavaScript: The Good Parts&quot;</a>; it is by some margin the most useful and interesting software related book I have read.So I was aware that Crockford had come up with his own JavaScript code quality tool called <a href="http://www.jslint.com/">JSLint</a>. JSLint is quite striking when you first encounter it: <img src="../static/blog/2012-04-23-jshint-customising-your-hurt-feelings/JSLint.png"/></p><p>It&#x27;s the &quot;Warning! JSLint will hurt your feelings.&quot; that grabs you. And it&#x27;s not wrong. I&#x27;ve copied and pasted code that I&#x27;ve written into JSLint and then gasped at the reams of errors JSLint would produce. I subsequently tried JSLint-ing various well known JS libraries (jQuery etc) and saw that JSLint considered they were thoroughly problematic as well. This made me feel slightly better. It was when I started examining some of the &quot;errors&quot; JSLint reported that I took exception. Yes, I took exception to exceptions! (I&#x27;m <!-- -->*<strong>very</strong>*<!-- --> pleased with that!) Here&#x27;s a few of the errors generated by JSLint when inspecting <a href="http://ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.js">jquery-1.7.2.js</a>: - <code>Problem at line 16 character 10: Expected exactly one space between &#x27;function&#x27; and &#x27;(&#x27;.</code></p><ul><li><code>Problem at line 25 character 1: Expected &#x27;var&#x27; at column 13, not column 1.</code></li><li><code>Problem at line 31 character 5: Unexpected dangling &#x27;_&#x27; in &#x27;_jQuery&#x27;.</code></li></ul><p>JSLint is, much like it&#x27;s creator, quite opinionated. Which is no bad thing. Many of Crockfords opinions are clearly worth their salt. It&#x27;s just I didn&#x27;t want all of them enforced upon me. As you can see above most of these &quot;problems&quot; are essentially complaints about a different style rather than bugs or potential issues. Now there are options in JSLint that you can turn on and off which looked quite promising. But before I got to investigating them I heard about <a href="http://www.jshint.com">JSHint</a>, brainchild of Anton Kovalyov and Paul Irish. In their own words: <em>JSHint is a fork of JSLint, the tool written and maintained by Douglas Crockford. The project originally started as an effort to make a more configurable version of JSLint—one that doesn&#x27;t enforce one particular coding style on its users—but then transformed into a separate static analysis tool with its own goals and ideals.</em> This sounded right up my alley! So I thought I&#x27;d repeat my jQuery test. Here&#x27;s a sample of what JSHint threw back at me, with its default settings in place: - <code>Line 230: return num == null ? Expected &#x27;===&#x27; and instead saw &#x27;==&#x27;. </code></p><ul><li><code>Line 352: if ( (options = arguments[ i ]) != null ) { Expected &#x27;!==&#x27; and instead saw &#x27;!=&#x27;. </code></li><li><code>Line 354: for ( name in options ) { The body of a for in should be wrapped in an if statement to filter unwanted properties from the prototype. </code></li></ul><p>These were much more the sort of &quot;issues&quot; I was interested in. Plus it seemed there was plenty of scope to tweak my options. Excellent. This was good. The icing on my cake would have been a plug-in for Visual Studio which would allow me to evaluate my JS files from within my IDE. Happily the world seems to be full of developers doing good turns for one another. I discovered an extension for VS called <a href="http://jslint4vs2010.codeplex.com/">JSLint for Visual Studio 2010</a>: <img src="../static/blog/2012-04-23-jshint-customising-your-hurt-feelings/Extensions.png"/></p><p>This was an extension that provided either JSLint <!-- -->*<strong>or</strong>*<!-- --> JSHint evaluation as you preferred from within Visual Studio. Fantastic! With this extension in play you could add JavaScript static code analysis to your compilation process and so learn of all the issues in your code at the same time, whether they lay in C# or JS or <!-- -->[insert language here]<!-- -->. You could control how JS problems were reported; as warnings, errors etc. You could straightforwardly exclude files from evaluation (essential if you&#x27;re reliant on a number of 3rd party JS libraries which you are not responsible for maintaining). You could cater for predefined variables; allow for jQuery or DOJO. You could simply evaluate a single file in your solution by right clicking it and hitting the &quot;JS Lint&quot; option in the context menu. And it was simplicity itself to activate and deactivate the JSHint / JSLint extension as required. For a more exhaustive round up of the options available I advise taking a look here: <a href="http://jslint4vs2010.codeplex.com/">http://jslint4vs2010.codeplex.com</a>. I would heartily recommend using JSHint if you&#x27;re looking to improve your JS code quality. I&#x27;m grateful to Crockford for making JSHint possible by first writing JSLint. For my part though I think JSHint is the more pragmatic and useful tool and likely to be the one I stick with. For interest (and frankly sheer entertainment value at the crotchetiness of Crockford) it&#x27;s definitely worth having a read up on how JSHint came to pass: - <a href="http://anton.kovalyov.net/2011/02/20/why-i-forked-jslint-to-jshint/">http://anton.kovalyov.net/2011/02/20/why-i-forked-jslint-to-jshint/</a></p><ul><li><a href="http://badassjs.com/post/3364925033/jshint-an-community-driven-fork-of-jslint">http://badassjs.com/post/3364925033/jshint-an-community-driven-fork-of-jslint</a></li></ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A Simple Technique for Initialising Properties with Internal Setters for Unit Testing]]></title>
            <link>https://blog.johnnyreilly.com/2012/04/16/simple-technique-for-initialising</link>
            <guid>A Simple Technique for Initialising Properties with Internal Setters for Unit Testing</guid>
            <pubDate>Mon, 16 Apr 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[I was recently working with my colleagues on refactoring a legacy application. We didn't have an immense amount of time available for this but the plan was to try and improve what was there as much as possible. In its initial state the application had no unit tests in place at all and so the plan was to refactor the code base in such a way as to make testing it a realistic proposition. To that end the domain layer was being heavily adjusted and the GUI was being migrated from WebForms to MVC 3. The intention was to build up a pretty solid collection of unit tests. However, as we were working on this we realised we had a problem with properties on our models with internal setters...]]></description>
            <content:encoded><![CDATA[<p>I was recently working with my colleagues on refactoring a legacy application. We didn&#x27;t have an immense amount of time available for this but the plan was to try and improve what was there as much as possible. In its initial state the application had no unit tests in place at all and so the plan was to refactor the code base in such a way as to make testing it a realistic proposition. To that end the <a href="http://en.wikipedia.org/wiki/Domain_layer">domain layer</a> was being heavily adjusted and the GUI was being migrated from WebForms to MVC 3. The intention was to build up a pretty solid collection of unit tests. However, as we were working on this we realised we had a problem with properties on our models with <a href="http://msdn.microsoft.com/en-us/library/7c5ka91b(v=vs.80).aspx"><code>internal</code></a> setters...</p><h2>Background</h2><p>The entities of the project in question used an approach which would store pertinent bits of <a href="http://en.wikipedia.org/wiki/Database_normalization">normalised</a> data for read-only purposes in related entities. I&#x27;ve re-read that sentence and realise it&#x27;s as clear as mud. Here is an example to clarify:</p><pre><code class="language-cs">public class Person
{
  public int Id { get; set; }
  public string FirstName { get; set; }
  public string LastName { get; set; }
  public string Address { get; set; }
  public DateTime DateOfBirth { get; set; }
  /* Other fascinating properties... */
}

public class Order
{
  public int Id { get; set; }
  public string ProductOrdered { get; set; }
  public string OrderedById { get; set; }
  public string OrderedByFirstName { get; internal set; }
  public string OrderedByLastName { get; internal set; }
}
</code></pre><p>In the example above you have 2 types of entity: <code>Person</code> and <code>Order</code>. The <code>Order</code> entity makes use of the the <code>Id</code>, <code>FirstName</code> and <code>LastName</code> properties of the <code>Person</code> entity in the properties <code>OrderedById</code>, <code>OrderedByFirstName</code> and <code>OrderedByLastName</code>. For persistence (ie saving to the database) purposes the only necessary <code>Person</code> property is <code>OrderedById</code> identity. <code>OrderedByFirstName</code> and <code>OrderedByLastName</code> are just &quot;nice to haves&quot; - essentially present to make implementing the GUI more straightforward.</p><p>To express this behaviour / intention in the object model the setters for <code>OrderedByFirstName</code> and <code>OrderedByLastName</code> are marked as <code>internal</code>. The implication of this is that properties like this can only be initialised within the current assembly - or any explicitly associated &quot;friend&quot; assemblies. In practice this meant that internally set properties were only populated when an object was read in from the database. It wasn&#x27;t possible to set these properties in other assemblies which meant less code was written (<u>a good thing</u></p><p>) - after all, why set a property when you don&#x27;t need to?</p><p>Background explanation over. It may still be a little unclear but I hope you get the gist.</p><h2>What&#x27;s our problem?</h2><p>I was writing unit tests for the controllers in our main web application and was having problems with my arrangements. I was mocking the database calls in my controllers much in the manner that you might expect:</p><pre><code class="language-ts">// Arrange
  var orderDb = new Mock&lt;IOrderDb&gt;();
  orderDb
    .Setup(x =&gt; x.GetOrder(It.IsAny&lt;int&gt;()))
    .Returns(new Order{
      Id = 123,
      ProductOrdered = &quot;Packet of coffee&quot;,
      OrderedById = 987456,
      OrderedByFirstName = &quot;John&quot;,
      OrderedByLastName = &quot;Reilly&quot;
    });
}
</code></pre><p>All looks fine doesn&#x27;t it? It&#x27;s not. Because <code>OrderedByFirstName</code> and <code>OrderedByLastName</code> have internal setters we are <u>unable</u></p><p>to initialise them from within the context of our test project. So what to do?</p><p>We toyed with 3 approaches and since each has merits I thought it worth going through each of them:</p><ol><li><p>To the MOQumentation Batman!: <a href="http://code.google.com/p/moq/wiki/QuickStart">http://code.google.com/p/moq/wiki/QuickStart</a>! Looking at the MOQ documentation it states the following:</p><p><em>Mocking internal types of another project: add the following assembly attributes (typically to the AssemblyInfo.cs) to the project containing the internal types:</em></p><pre><code class="language-cs">// This assembly is the default dynamic assembly generated Castle DynamicProxy,
// used by Moq. Paste in a single line.
[assembly:InternalsVisibleTo(&quot;DynamicProxyGenAssembly2,PublicKey=0024000004800000940000000602000000240000525341310004000001000100c547cac37abd99c8db225ef2f6c8a3602f3b3606cc9891605d02baa56104f4cfc0734aa39b93bf7852f7d9266654753cc297e7d2edfe0bac1cdcf9f717241550e0a7b191195b7667bb4f64bcb8e2121380fd1d9d46ad2d92d2d15605093924cceaf74c4861eff62abf69b9291ed0a340e113be11e6a7d3113e92484cf7045cc7&quot;)]
[assembly: InternalsVisibleTo(&quot;The.NameSpace.Of.Your.Unit.Test&quot;)] //I&#x27;d hope it was shorter than that...
</code></pre><p>This looked to be exactly what we needed and in most situations it would make sense to go with this. Unfortunately for us there was a gotcha. Certain core shared parts of our application platform were <a href="http://en.wikipedia.org/wiki/Global_Assembly_Cache">GAC</a>&#x27;d. A requirement for GAC-ing an assembly is that it is <a href="http://msdn.microsoft.com/en-us/library/xc31ft41.aspx">signed</a>.</p><p>The upshot of this was that if we wanted to use the <code>InternalsVisibleTo</code> approach then we would need to sign our web application test project. We weren&#x27;t particularly averse to that and initially did so without much thought. It was then we remembered that every assembly referenced by a signed assembly must also be signed as well. We didn&#x27;t really want to sign our main web application purely for testing purposes. We could and if there weren&#x27;t viable alternatives we well might have. But it just seemed like the wrong reason to be taking that decision. Like using a sledgehammer to crack a nut.</p></li><li><p>The next approach we took was using mock objects. Instead of using our objects straight we would mock them as below:</p><pre><code class="language-cs">//Create mock and set internal properties
      var orderMock = new Mock&lt;Order&gt;();
      orderMock.SetupGet(x =&gt; x.OrderedByFirstName).Returns(&quot;John&quot;);
      orderMock.SetupGet(x =&gt; x.OrderedByLastName).Returns(&quot;Reilly&quot;);

      //Set up standard properties
      orderMock.SetupAllProperties();
      var orderStub = orderMock.Object;
      orderStub.Id = 123;
      orderStub.ProductOrdered = &quot;Packet of coffee&quot;;
      orderStub.OrderedById = 987456;
</code></pre><p>Now this approach worked fine but had a couple of snags:</p><ul><li><p>As you can see it&#x27;s pretty verbose and much less clear to read than it was previously.</p></li><li><p>It required that we add the <code>virtual</code> keyword to all our internally set properties like so:</p><pre><code class="language-cs">public class Order
{
  // ....
  public virtual string OrderedByFirstName { get; internal set; }
  public virtual string OrderedByLastName { get; internal set; }
  // ...
}
</code></pre></li><li><p>Our standard constructor already initialised the value of our internally set properties. So adding <code>virtual</code> to the internally set properties generated <a href="http://www.jetbrains.com/resharper/">ReSharper</a> warnings aplenty about virtual properties being initialised in the constructor. Fair enough.</p></li></ul><p>Because of the snags it still felt like we were in nutcracking territory...</p></li><li><p>... and this took us to the approach that we ended up adopting: a special mocking constructor for each class we wanted to test, for example:</p><pre><code class="language-cs">/// &lt;summary&gt;
/// Mocking constructor used to initialise internal properties
/// &lt;/summary&gt;
public Order(string orderedByFirstName = null, string orderedByLastName = null)
: this()
{
OrderedByFirstName = orderedByFirstName;
OrderedByLastName = orderedByLastName;
}

</code></pre><p>Thanks to the ever lovely <a href="http://msdn.microsoft.com/en-us/library/dd264739.aspx">Named and Optional Arguments</a> feature of C# combined with <a href="http://msdn.microsoft.com/en-us/library/bb397680.aspx">Object Initializers</a> it meant it was possible to write quite expressive, succinct code using this approach; for example:</p><pre><code class="language-cs">var order = new Order(
        orderedByFirstName: &quot;John&quot;,
        orderedByLastName: &quot;Reilly&quot;
      )
      {
        Id = 123,
        ProductOrdered = &quot;Packet of coffee&quot;,
        OrderedById = 987456
      };
</code></pre><p>Here we&#x27;re calling the mocking constructor to set the internally set properties and subsequently initialising the other properties using the object initialiser mechanism.</p><p>Implementing these custom constructors wasn&#x27;t a massive piece of work and so we ended up settling on this technique for initialising internal properties.</p></li></ol>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Making PDFs from HTML in C# using WKHTMLtoPDF]]></title>
            <link>https://blog.johnnyreilly.com/2012/04/05/making-pdfs-from-html-in-c-using</link>
            <guid>Making PDFs from HTML in C# using WKHTMLtoPDF</guid>
            <pubDate>Thu, 05 Apr 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[Update 03/01/2013]]></description>
            <content:encoded><![CDATA[<h2>Update 03/01/2013</h2><p>I&#x27;ve written a subsequent post which builds on the work of this original post. The new post exposes this functionality via a WCF service and can be found <a href="http://icanmakethiswork.blogspot.co.uk/2013/01/html-to-pdf-using-wcf-service.html">here</a>.</p><h2>Making PDFs from HTML</h2><p>I wanted to talk about an approach I&#x27;ve discovered for making PDFs directly from HTML. I realise that in these wild and crazy days of <a href="http://mozilla.github.com/pdf.js/">PDF.js</a> and the like that techniques like this must seem very old hat. That said, this technique works and more importantly it solves a problem I was faced with but without forcing the users to move the &quot;newest hottest version of X&quot;. Much as many of would love to solve problems this way, alas many corporations move slower than that and in the meantime we still have to deliver - we still have to meet requirements. Rather than just say &quot;I did this&quot; I thought I&#x27;d record how I got to this point in the first place. I don&#x27;t know about you but I find the reasoning behind why different technical decisions get made quite an interesting topic...</p><p>For some time I&#x27;ve been developing / supporting an application which is used in an intranet environment where the company mandated browser is still IE 6. It was a requirement that there be &quot;print&quot; functionality in this application. As is well known (even by Microsoft themselves) the print functionality in IE 6 was never fantastic. But the requirement for usable printouts remained.</p><p>The developers working on the system before me decided to leverage Crystal Reports (remember that?). Essentially there was a reporting component to the application at the time which created custom reports using Crystal and rendered them to the user in the form of PDFs (which have been eminently printable for as long as I care to remember). One of the developers working on the system realised that it would be perfectly possible to create some &quot;reports&quot; within Crystal which were really &quot;print to PDF&quot; screens for the app.</p><p>It worked well and this solution stayed in place for a very long time. However, some years down the line the Crystal Reports was discarded as the reporting mechanism for the app. But we were unable to decommission Crystal entirely because we still needed it for printing.</p><p>I&#x27;d never really liked the Crystal solution for a number of reasons:</p><ol><li>We needed custom stored procs to drive the Crystal print screens which were near duplicates of the main app procs. This duplication of effort never felt right.</li><li>We had to switch IDEs whenever we were maintaining our print screens. And the Crystal IDE is not a joy to use.</li><li>Perhaps most importantly, for certain users we needed to hide bits of information from the print. The version of Crystal we were using did not make the dynamic customisation of our print screens a straightforward proposition. (In its defence we weren&#x27;t really using it for what it was designed for.) As a result the developers before me had ended up creating various versions of each print screen revealing different levels of information. As you can imagine, this meant that the effort involved in making changes to the print screens had increased exponentially</li></ol><p>It occurred to me that it would be good if we could find some way of generating our own PDF reports without using Crystal that would be a step forward. It was shortly after this that I happened upon <a href="http://code.google.com/p/wkhtmltopdf/">WKHTMLtoPDF</a>. This is an open source project which describes itself as a <em>&quot;Simple shell utility to convert html to pdf using the webkit rendering engine, and qt.&quot;</em> I tested it out on various websites and it worked. It wasn&#x27;t by any stretch of the imagination a perfect HTML to PDF tool but the quality it produced greatly outstripped the presentation currently in place via Crystal.</p><p>This was just the ticket. Using WKHTMLtoPDF I could have simple web pages in the application which could be piped into WKHTMLtoPDF to make a PDF as needed. It could be dynamic - because ASP.NET is dynamic. We wouldn&#x27;t need to write and maintain custom stored procs anymore. And happily we would no longer need to use Crystal.</p><p>Before we could rid ourselves of Crystal though, I needed a way that I could generate these PDFs on the fly within the website. For this I ended up writing a simple wrapper class for WKHTMLtoPDF which could be used to invoke it on the fly. In fact a good portion of this was derived from various contributions on <a href="http://stackoverflow.com/q/1331926">a post on StackOverflow</a>. It ended up looking like this:</p><script src="https://gist.github.com/2341776.js?file=PdfGenerator.cs"></script><p>With this wrapper I could pass in URLs and extract out PDFs. Here&#x27;s a couple of examples of me doing just that:</p><pre><code class="language-cs">//Create PDF from a single URL
    var pdfUrl = PdfGenerator.HtmlToPdf(pdfOutputLocation: &quot;~/PDFs/&quot;,
        outputFilenamePrefix: &quot;GeneratedPDF&quot;,
        urls: new string[] { &quot;http://news.bbc.co.uk&quot; });

    //Create PDF from multiple URLs
    var pdfUrl = PdfGenerator.HtmlToPdf(pdfOutputLocation: &quot;~/PDFs/&quot;,
        outputFilenamePrefix: &quot;GeneratedPDF&quot;,
        urls: new string[] { &quot;http://www.google.co.uk&quot;, &quot;http://news.bbc.co.uk&quot; });
</code></pre><p>As you can see from the second example above it&#x27;s possible to pipe a number of URLs into the wrapper all to be rendered to a single PDF. Most of the time this was surplus to our requirements but it&#x27;s good to know it&#x27;s possible. Take a look at the BBC website PDF generated by the first example:</p><iframe src="https://docs.google.com/file/d/0B87K8-qxOZGFYktEWGtXRXJSSS1ZWFR4emFfMmVxZw/preview" width="500" height="500"></iframe><p>Pretty good, no? As you can see it&#x27;s not perfect from looking at the titles (bit squashed) but I deliberately picked a more complicated page to show what WKHTMLtoPDF was capable of. The print screens I had in mind to build would be significantly simpler than this.</p><p>Once this was in place I was able to scrap the Crystal solution. It was replaced with a couple of &quot;print to PDF&quot; ASPXs in the main web app which would be customised when rendering to hide the relevant bits of data from the user. These ASPXs would be piped into the HtmlToPdf method as needed and then the user would be redirected to that PDF. If for some reason the PDF failed to render the users would see the straight &quot;print to PDF&quot; ASPX - just not as a PDF if you see what I mean. I should say that it was pretty rare for a PDF to not render but this was my failsafe.</p><p>This new solution had a number of upsides from our perspective:</p><ul><li>Development maintenance time (and consequently cost for our customers) for print screens was significantly reduced. This was due to the print screens being part of the main web app. This meant they shared styling etc with all the other web screens and the dynamic nature of ASP.NET made customising a screen on the fly simplicity itself.</li><li>We were now able to regionalise our print screens for the users in the same way as we did with our main web app. This just wasn&#x27;t realistic with the Crystal solution because of the amount of work involved.</li><li>I guess this is kind of a <a href="http://en.wikipedia.org/wiki/Don%27t_repeat_yourself">DRY</a> solution :-)</li></ul><p>You can easily make use of the above approach yourself. All you need do is download and install <a href="http://code.google.com/p/wkhtmltopdf/">WKHTMLtoPDF</a> on your machine. I advise using version 0.9.9 as the later release candidates appear slightly buggy at present.</p><p>Couple of gotchas:</p><ol><li>Make sure that you pass the correct installation path to the HtmlToPdf method if you installed it anywhere other than the default location. You&#x27;ll see that the class assumes the default if it wasn&#x27;t passed</li><li>Ensure that Read and Execute rights are granted to the wkhtmltopdf folder for the relevant process</li><li>Ensure that Write rights are granted for the location you want to create your PDFs for the relevant process</li></ol><p>In our situation we are are invoking this directly in our web application on demand. I have no idea how this would scale - perhaps not well. This is not really an issue for us as our user base is fairly small and this functionality isn&#x27;t called excessively. I think if this was used much more than it is I&#x27;d be tempted to hive off this functionality into a separate app. But this works just dandy for now.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[WCF - moving from Config to Code, a simple WCF service harness (plus implementing your own Authorization)]]></title>
            <link>https://blog.johnnyreilly.com/2012/03/22/wcf-moving-from-config-to-code-simple</link>
            <guid>WCF - moving from Config to Code, a simple WCF service harness (plus implementing your own Authorization)</guid>
            <pubDate>Thu, 22 Mar 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[Last time I wrote about WCF I was getting up and running with WCF Transport Windows authentication using NetTcpBinding in an Intranet environment. I ended up with a WCF service hosted in a Windows Service which did pretty much what the previous post name implies.]]></description>
            <content:encoded><![CDATA[<p>Last time I wrote about WCF I was getting up and running with <a href="http://icanmakethiswork.blogspot.com/2012/02/wcf-transport-windows-authentication.html">WCF Transport Windows authentication using NetTcpBinding in an Intranet environment</a>. I ended up with a WCF service hosted in a Windows Service which did pretty much what the previous post name implies.</p><p>Since writing that I&#x27;ve taken things on a bit further and I thought it worth recording my approach whilst it&#x27;s still fresh in my mind. There&#x27;s 3 things I want to go over:</p><ol><li>I&#x27;ve moved away from the standard config driven WCF approach to a more &quot;code-first&quot; style</li><li>I&#x27;ve established a basic Windows Service hosted WCF service / client harness which is useful if you&#x27;re trying to get up and running with a WCF service quickly</li><li>I&#x27;ve locked down the WCF authorization to a single Windows account through the use of my own <a href="http://msdn.microsoft.com/en-us/library/ms731774.aspx">ServiceAuthorizationManager</a></li></ol><h2>Moving from Config to Code</h2><p>So, originally I was doing what all the cool kids are doing and driving the configuration of my WCF service and all its clients through config files. And why not? I&#x27;m in good company.</p><p>Here&#x27;s why not: it gets <!-- -->*<strong>very</strong>*<!-- --> verbose <!-- -->*<strong>very</strong>*<!-- --> quickly....</p><p>Okay - that&#x27;s not the end of the world. My problem was that I had <!-- -->~<!-- -->10 Windows Services and 3 Web applications that needed to call into my WCF Service. I didn&#x27;t want to have to separately tweak 15 or so configs each time I wanted to make one standard change to WCF configuration settings. I wanted everything in one place.</p><p>Now there&#x27;s newer (and probably hipper) ways of achieving this. <a href="http://stackoverflow.com/a/2814286">Here&#x27;s one possibility I happened upon on StackOverflow that looks perfectly fine.</a></p><p>Well I didn&#x27;t use a hip new approach - no I went Old School with my old friend the <a href="http://msdn.microsoft.com/en-us/library/ms228154.aspx">appSettings file attribute</a>. Remember that? It&#x27;s just a simple way to have all your common appSettings configuration settings in a single file which can be linked to from as many other apps as you like. It&#x27;s wonderful and I&#x27;ve been using it for a long time now. Unfortunately it&#x27;s pretty basic in that it&#x27;s only the appSettings section that can be shared out; no <code>&amp;lt;system.serviceModel&amp;gt;</code> or similar.</p><p>But that wasn&#x27;t really a problem from my perspective. I realised that there were actually very few things that needed to be configurable for my WCF service. Really I wanted a basic WCF harness that could be initialised in code which implicitly set all the basic configuration with settings that worked (ie it was set up with defaults like maximum message size which were sufficiently sized). On top of that I would allow myself to configure just those things that I needed to through the use of my own custom WCF config settings in the shared appSettings.config file.</p><p>Once done I massively reduced the size of my configs from frankly gazillions of entries to just these appSettings.config entries which were shared across each of my WCF service clients and by my Windows Service harness:</p><pre><code class="language-xml">&lt;appSettings&gt;
  &lt;add key=&quot;WcfBaseAddressForClient&quot; value=&quot;net.tcp://localhost:9700/&quot;/&gt;
  &lt;add key=&quot;WcfWindowsSecurityApplied&quot; value=&quot;true&quot; /&gt;
  &lt;add key=&quot;WcfCredentialsUserName&quot; value=&quot;myUserName&quot; /&gt;
  &lt;add key=&quot;WcfCredentialsPassword&quot; value=&quot;myPassword&quot; /&gt;
  &lt;add key=&quot;WcfCredentialsDomain&quot; value=&quot;myDomain&quot; /&gt;
  &lt;/appSettings&gt;
</code></pre><p>And these config settings used only by my Windows Service harness:</p><pre><code class="language-xml">&lt;appSettings file=&quot;../Shared/AppSettings.config&quot;&gt;
    &lt;add key=&quot;WcfBaseAddressForService&quot; value=&quot;net.tcp://localhost:9700/&quot;/&gt;
  &lt;/appSettings&gt;
</code></pre><h2>Show me your harness</h2><p>I ended up with a quite a nice basic &quot;vanilla&quot; framework that allowed me to quickly set up Windows Service hosted WCF services. The framework also provided me with a simple way to consume these WCF services with a minimum of code an configuration. No muss. No fuss. :-) So pleased with it was I that I thought I&#x27;d go through it here much in the manner of a chef baking a cake...</p><p>To start with I created myself a Windows Service in Visual Studio which I grandly called &quot;WcfWindowsService&quot;. The main service class looked like this:</p><pre><code class="language-cs">public class WcfWindowsService: ServiceBase
  {
    public static string WindowsServiceName = &quot;WCF Windows Service&quot;;
    public static string WindowsServiceDescription = &quot;Windows service that hosts a WCF service.&quot;;

    private static readonly log4net.ILog _logger = log4net.LogManager.GetLogger(System.Reflection.MethodBase.GetCurrentMethod().DeclaringType);

    public List&lt;ServiceHost&gt; _serviceHosts = null;

    public WcfWindowsService()
    {
      ServiceName = WindowsServiceName;
    }

    public static void Main()
    {
      ServiceBase.Run(new WcfWindowsService());
    }

    /// &lt;summary&gt;
    /// The Windows Service is starting
    /// &lt;/summary&gt;
    /// &lt;param name=&quot;args&quot;&gt;&lt;/param&gt;
    protected override void OnStart(string[] args)
    {
      try
      {
        CloseAndClearServiceHosts();

        //Make log4net startup
        XmlConfigurator.Configure();
        _logger.Warn(&quot;WCF Windows Service starting...&quot;);
        _logger.Info(&quot;Global.WcfWindowsSecurityApplied = &quot; + Global.WcfWindowsSecurityApplied.ToString().ToLower());

        if (Global.WcfWindowsSecurityApplied)
        {
          _logger.Info(&quot;Global.WcfOnlyAuthorizedForWcfCredentials = &quot; + Global.WcfOnlyAuthorizedForWcfCredentials.ToString().ToLower());

          if (Global.WcfOnlyAuthorizedForWcfCredentials)
          {
            _logger.Info(&quot;Global.WcfCredentialsDomain = &quot; + Global.WcfCredentialsDomain);
            _logger.Info(&quot;Global.WcfCredentialsUserName = &quot; + Global.WcfCredentialsUserName);
          }
        }

        //Create binding
        var wcfBinding = WcfHelper.CreateBinding(Global.WcfWindowsSecurityApplied);

        // Create a servicehost and endpoints for each service and open each
        _serviceHosts = new List&lt;ServiceHost&gt;();
        _serviceHosts.Add(WcfServiceFactory&lt;IHello&gt;.CreateAndOpenServiceHost(typeof(HelloService), wcfBinding));
        _serviceHosts.Add(WcfServiceFactory&lt;IGoodbye&gt;.CreateAndOpenServiceHost(typeof(GoodbyeService), wcfBinding));

        _logger.Warn(&quot;WCF Windows Service started.&quot;);
      }
      catch (Exception exc)
      {
        _logger.Error(&quot;Problem starting up&quot;, exc);

        throw exc;
      }
    }

    /// &lt;summary&gt;
    /// The Windows Service is stopping
    /// &lt;/summary&gt;
    protected override void OnStop()
    {
      CloseAndClearServiceHosts();

      _logger.Warn(&quot;WCF Windows Service stopped&quot;);
    }

    /// &lt;summary&gt;
    /// Close and clear service hosts in list and clear it down
    /// &lt;/summary&gt;
    private void CloseAndClearServiceHosts()
    {
      if (_serviceHosts != null)
      {
        foreach (var serviceHost in _serviceHosts)
        {
          CloseAndClearServiceHost(serviceHost);
        }

        _serviceHosts.Clear();
      }
    }

    /// &lt;summary&gt;
    /// Close and clear the passed service host
    /// &lt;/summary&gt;
    /// &lt;param name=&quot;serviceHost&quot;&gt;&lt;/param&gt;
    private void CloseAndClearServiceHost(ServiceHost serviceHost)
    {
      if (serviceHost != null)
      {
        _logger.Info(string.Join(&quot;, &quot;, serviceHost.BaseAddresses) + &quot; is closing...&quot;);

        serviceHost.Close();

        _logger.Info(string.Join(&quot;, &quot;, serviceHost.BaseAddresses) + &quot; is closed&quot;);
      }
    }
  }
</code></pre><p>As you&#x27;ve no doubt noticed this makes use of <a href="http://logging.apache.org/log4net/">Log4Net</a> for logging purposes (I&#x27;ll assume you&#x27;re aware of it). My Windows Service implements such fantastic WCF services as HelloService and GoodbyeService. Each revolutionary in their own little way. To give you a taste of the joie de vivre that these services exemplify take a look at this:</p><pre><code class="language-cs">// Implement the IHello service contract in a service class.
  public class HelloService : WcfServiceAuthorizationManager, IHello
  {
    // Implement the IHello methods.
    public string GreetMe(string thePersonToGreet)
    {
      return &quot;well hello there &quot; + thePersonToGreet;
    }
  }
</code></pre><p>Exciting! WcfWindowsService also references another class called &quot;Global&quot; which is a helper class - to be honest not much more than a wrapper for my config settings. It looks like this:</p><pre><code class="language-cs">static public class Global
  {
    #region Properties

    // eg &quot;net.tcp://localhost:9700/&quot;
    public static string WcfBaseAddressForService { get { return ConfigurationManager.AppSettings[&quot;WcfBaseAddressForService&quot;]; } }

    // eg true
    public static bool WcfWindowsSecurityApplied { get { return bool.Parse(ConfigurationManager.AppSettings[&quot;WcfWindowsSecurityApplied&quot;]); } }

    // eg true
    public static bool WcfOnlyAuthorizedForWcfCredentials { get { return bool.Parse(ConfigurationManager.AppSettings[&quot;WcfOnlyAuthorizedForWcfCredentials&quot;]); } }

    // eg &quot;myDomain&quot;
    public static string WcfCredentialsDomain { get { return ConfigurationManager.AppSettings[&quot;WcfCredentialsDomain&quot;]; } }

    // eg &quot;myUserName&quot;
    public static string WcfCredentialsUserName { get { return ConfigurationManager.AppSettings[&quot;WcfCredentialsUserName&quot;]; } }

    // eg &quot;myPassword&quot; - this should *never* be stored unencrypted and is only ever used by clients that are not already running with the approved Windows credentials
    public static string WcfCredentialsPassword { get { return ConfigurationManager.AppSettings[&quot;WcfCredentialsPassword&quot;]; } }

    #endregion
  }
</code></pre><p>WcfWindowsService creates and hosts a HelloService and a GoodbyeService when it starts up. It does this using my handy WcfServiceFactory:</p><pre><code class="language-cs">public class WcfServiceFactory&lt;TInterface&gt;
  {
    private static readonly log4net.ILog _logger = log4net.LogManager.GetLogger(System.Reflection.MethodBase.GetCurrentMethod().DeclaringType);

    public static ServiceHost CreateAndOpenServiceHost(Type serviceType, NetTcpBinding wcfBinding)
    {
      var serviceHost = new ServiceHost(serviceType, new Uri(Global.WcfBaseAddressForService + ServiceHelper&lt;TInterface&gt;.GetServiceName()));
      serviceHost.AddServiceEndpoint(typeof(TInterface), wcfBinding, &quot;&quot;);
      serviceHost.Authorization.ServiceAuthorizationManager = new WcfServiceAuthorizationManager(); // This allows us to control authorisation within WcfServiceAuthorizationManager
      serviceHost.Open();

      _logger.Info(string.Join(&quot;, &quot;, serviceHost.BaseAddresses) + &quot; is now listening.&quot;);

      return serviceHost;
    }
  }
</code></pre><p>To do this it also uses my equally handy WcfHelper class:</p><pre><code class="language-cs">static public class WcfHelper
  {
    /// &lt;summary&gt;
    /// Create a NetTcpBinding
    /// &lt;/summary&gt;
    /// &lt;param name=&quot;useWindowsSecurity&quot;&gt;&lt;/param&gt;
    /// &lt;returns&gt;&lt;/returns&gt;
    public static NetTcpBinding CreateBinding(bool useWindowsSecurity)
    {
      var wcfBinding = new NetTcpBinding();
      if (useWindowsSecurity)
      {
        wcfBinding.Security.Mode = SecurityMode.Transport;
        wcfBinding.Security.Transport.ClientCredentialType = TcpClientCredentialType.Windows;
      }
      else
        wcfBinding.Security.Mode = SecurityMode.None;

      wcfBinding.MaxBufferSize = int.MaxValue;
      wcfBinding.MaxReceivedMessageSize = int.MaxValue;
      wcfBinding.ReaderQuotas.MaxArrayLength = int.MaxValue;
      wcfBinding.ReaderQuotas.MaxDepth = int.MaxValue;
      wcfBinding.ReaderQuotas.MaxStringContentLength = int.MaxValue;
      wcfBinding.ReaderQuotas.MaxBytesPerRead = int.MaxValue;

      return wcfBinding;
    }
  }

  /// &lt;summary&gt;
  /// Create a WCF Client for use anywhere (be it Windows Service or ASP.Net web application)
  /// nb Credential fields are optional and only likely to be needed by web applications
  /// &lt;/summary&gt;
  /// &lt;typeparam name=&quot;TInterface&quot;&gt;&lt;/typeparam&gt;
  public class WcfClientFactory&lt;TInterface&gt;
  {
    public static TInterface CreateChannel(bool useWindowsSecurity, string wcfBaseAddress, string wcfCredentialsUserName = null, string wcfCredentialsPassword = null, string wcfCredentialsDomain = null)
    {
      //Create NetTcpBinding using universally
      var wcfBinding = WcfHelper.CreateBinding(useWindowsSecurity);

      //Get Service name from examining the ServiceNameAttribute decorating the interface
      var serviceName = ServiceHelper&lt;TInterface&gt;.GetServiceName();

      //Create the factory for creating your channel
      var factory = new ChannelFactory&lt;TInterface&gt;(
        wcfBinding,
        new EndpointAddress(wcfBaseAddress + serviceName)
        );

      //if credentials have been supplied then use them
      if (!string.IsNullOrEmpty(wcfCredentialsUserName))
      {
        factory.Credentials.Windows.ClientCredential = new System.Net.NetworkCredential(wcfCredentialsUserName, wcfCredentialsPassword, wcfCredentialsDomain);
      }

      //Create the channel
      var channel = factory.CreateChannel();

      return channel;
    }
  }
</code></pre><p>Now the above WcfHelper class and it&#x27;s comrade-in-arms the WcfClientFactory don&#x27;t live in the WcfWindowsService project with the other classes. No. They live in a separate project called the WcfWindowsServiceContracts project with their old mucker the ServiceHelper:</p><pre><code class="language-cs">public class ServiceHelper&lt;T&gt;
  {
    public static string GetServiceName()
    {
      var customAttributes = typeof(T).GetCustomAttributes(false);
      if (customAttributes.Length &gt; 0)
      {
        foreach (var customAttribute in customAttributes)
        {
          if (customAttribute is ServiceNameAttribute)
          {
            return ((ServiceNameAttribute)customAttribute).ServiceName;
          }
        }
      }

      throw new ArgumentException(&quot;Interface is missing ServiceNameAttribute&quot;);
    }
  }

  [AttributeUsage(AttributeTargets.Interface, AllowMultiple = false)]
  public class ServiceNameAttribute : System.Attribute
  {
    public ServiceNameAttribute(string serviceName)
    {
      this.ServiceName = serviceName;
    }

    public string ServiceName { get; set; }
  }
</code></pre><p>Now can you guess what the WcfWindowsServiceContracts project might contain? Yes; contracts for your services (oh the excitement)! What might one of these contracts look like I hear you ask... Well, like this:</p><pre><code class="language-cs">[ServiceContract()]
  [ServiceName(&quot;HelloService&quot;)]
  public interface IHello
  {
    [OperationContract]
    string GreetMe(string thePersonToGreet);
  }
</code></pre><p>The WcfWindowsServiceContracts project is included in <!-- -->*<strong>any</strong>*<!-- --> WCF client solution that wants to call your WCF services. It is also included in the WCF service solution. It facilitates the calling of services. What you&#x27;re no doubt wondering is how this might be achieved. Well here&#x27;s how, it uses our old friend the <code>WcfClientFactory</code>:</p><pre><code class="language-cs">var helloClient = WcfClientFactory&lt;IHello&gt;
    .CreateChannel(
      useWindowsSecurity:     Global.WcfWindowsSecurityApplied,  // eg true
      wcfBaseAddress:         Global.WcfBaseAddressForClient,    // eg &quot;net.tcp://localhost:9700/&quot;
      wcfCredentialsUserName: Global.WcfCredentialsUserName,     // eg &quot;myUserName&quot; - Optional parameter - only passed by web applications that need to impersonate the valid user
      wcfCredentialsPassword: Global.WcfCredentialsPassword,     // eg &quot;myPassword&quot; - Optional parameter - only passed by web applications that need to impersonate the valid user
      wcfCredentialsDomain:   Global.WcfCredentialsDomain        // eg &quot;myDomain&quot; - Optional parameter - only passed by web applications that need to impersonate the valid user
    );
  var greeting = helloClient.GreetMe(&quot;John&quot;); //&quot;well hello there John&quot;
</code></pre><p>See? Simple as simple. The eagle eyed amongst you will have noticed that client example above is using &quot;<code>Global</code>&quot; which is essentially a copy of the <code>Global</code> class mentioned above that is part of the WcfWindowsService project.</p><h2>Locking down Authorization to a single Windows account</h2><p>I can tell you think i&#x27;ve forgotten something. &quot;Tell me about this locking down to the single Windows account / what is this mysterious <code>WcfServiceAuthorizationManager</code> class that all your WCF services inherit from? Don&#x27;t you fob me off now.... etc&quot;</p><p>Well ensuring that only a single Windows account is authorised (yes dammit the original English spelling) to access our WCF services is achieved by implementing our own <code>ServiceAuthorizationManager</code> class. This implementation is used for authorisation by your <code>ServiceHost</code> and the logic sits in the overridden <code>CheckAccessCore</code> method. All of our WCF service classes will inherit from our <code>ServiceAuthorizationManager</code> class and so trigger the <code>CheckAccessCore</code> authorisation each time they are called.</p><p>As you can see from the code below, depending on our configuration, we lock down access to all our WCF services to a specific Windows account. This is far from the only approach that you might want to take to authorisation; it&#x27;s simply the one that we&#x27;ve been using. However the power of being able to implement your own authorisation in the <code>CheckAccessCore</code> method allows you the flexibility to do pretty much anything you want:</p><pre><code class="language-cs">public class WcfServiceAuthorizationManager : ServiceAuthorizationManager
  {
    protected static readonly log4net.ILog _logger = log4net.LogManager.GetLogger(System.Reflection.MethodBase.GetCurrentMethod().DeclaringType);

    protected override bool CheckAccessCore(OperationContext operationContext)
    {
      if (Global.WcfWindowsSecurityApplied)
      {
        if ((operationContext.ServiceSecurityContext.IsAnonymous) ||
          (operationContext.ServiceSecurityContext.PrimaryIdentity == null))
        {
          _logger.Error(&quot;WcfWindowsSecurityApplied = true but no credentials have been supplied&quot;);
          return false;
        }

        if (Global.WcfOnlyAuthorizedForWcfCredentials)
        {
          if (operationContext.ServiceSecurityContext.PrimaryIdentity.Name.ToLower() == Global.WcfCredentialsDomain.ToLower() + &quot;\\&quot; + Global.WcfCredentialsUserName.ToLower())
          {
            _logger.Debug(&quot;WcfOnlyAuthorizedForWcfCredentials = true and the valid user (&quot; + operationContext.ServiceSecurityContext.PrimaryIdentity.Name + &quot;) has been supplied and access allowed&quot;);
            return true;
          }
          else
          {
            _logger.Error(&quot;WcfOnlyAuthorizedForWcfCredentials = true and an invalid user (&quot; + operationContext.ServiceSecurityContext.PrimaryIdentity.Name + &quot;) has been supplied and access denied&quot;);
            return false;
          }
        }
        else
        {
          _logger.Debug(&quot;WcfOnlyAuthorizedForWcfCredentials = false, credentials were supplied (&quot; + operationContext.ServiceSecurityContext.PrimaryIdentity.Name + &quot;) so access allowed&quot;);
          return true;
        }
      }
      else
      {
        _logger.Info(&quot;WcfWindowsSecurityApplied = false so we are allowing unfettered access&quot;);
        return true;
      }
    }
  }
</code></pre><p>Phewwww... I know this has ended up as a bit of a brain dump but hopefully people will find it useful. At some point I&#x27;ll try to put up the above solution on GitHub so people can grab it easily for themselves.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using the PubSub / Observer pattern to emulate constructor chaining without cluttering up global scope]]></title>
            <link>https://blog.johnnyreilly.com/2012/03/17/using-pubsub-observer-pattern-to</link>
            <guid>Using the PubSub / Observer pattern to emulate constructor chaining without cluttering up global scope</guid>
            <pubDate>Sat, 17 Mar 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[Yes the title of this post is \*painfully\* verbose. Sorry about that. Couple of questions for you: - Have you ever liked the way you can have base classes in C# which can then be inherited and subclassed in a different file / class]]></description>
            <content:encoded><![CDATA[<p>Yes the title of this post is <!-- -->*<strong>painfully</strong>*<!-- --> verbose. Sorry about that. Couple of questions for you: - Have you ever liked the way you can have base classes in C# which can then be inherited and subclassed <u>in a different file / class</u></p><p>?</p><ul><li>Have you ever thought; gosh it&#x27;d be nice to do something like that in JavaScript...</li><li>Have you then looked at JavaScripts prototypical inheritance and thought &quot;right.... I&#x27;m sure it&#x27;s possible but this going to end up like <a href="http://en.wikipedia.org/wiki/War_and_Peace">War and Peace</a>&quot;</li><li>Have you then subsequently thought &quot;and hold on a minute... even if I did implement this using the prototype and split things between different files / modules wouldn&#x27;t I have to pollute the global scope to achieve that? And wouldn&#x27;t that mean that my code was exposed to the vagaries of any other scripts on the page? Hmmm...&quot;</li><li><a href="http://www.thrillingdetective.com/eyes/oxford.html">Men! Are you skinny? Do bullies kick sand in your face?</a> (Just wanted to see if you were still paying attention...)</li></ul><h2>The Problem</h2><p>Well, the above thoughts occurred to me just recently. I had a situation where I was working on an MVC project and needed to build up quite large objects within JavaScript representing various models. The models in question were already implemented on the server side using classes and made extensive use of inheritance because many of the properties were shared between the various models. That is to say we would have models which were implemented through the use of a class inheriting a base class which in turn inherits a further base class. With me? Good. Perhaps I can make it a little clearer with an example. Here are my 3 classes. First BaseReilly.cs:</p><pre><code class="language-cs">public class BaseReilly
{
    public string LastName { get; set; }

        public BaseReilly()
        {
            LastName = &quot;Reilly&quot;;
        }
    }
</code></pre><p>Next BoyReilly.cs (which inherits from BaseReilly):</p><pre><code class="language-cs">public class BoyReilly : BaseReilly
{
    public string Sex { get; set; }

    public BoyReilly()
        : base()
    {
        Sex = &quot;It is a manchild&quot;;
    }
}
</code></pre><p>And finally JohnReilly.cs (which inherits from BoyReilly which in turn inherits from BaseReilly):</p><pre><code class="language-cs">public class JohnReilly : BoyReilly
{
    public string FirstName { get; set; }

    public JohnReilly()
        : base()
    {
        FirstName = &quot;John&quot;;
    }
}
</code></pre><p>Using the above I can create myself my very own &quot;JohnReilly&quot; like so:</p><pre><code class="language-cs">var johnReilly = new JohnReilly();
</code></pre><p>And it will look like this: <img src="../static/blog/2012-03-17-using-pubsub-observer-pattern-to/C%2523%2Bversion%2Bof%2BJohnReilly.png"/></p><p>I was looking to implement something similar on the client and within JavaScript. I was keen to ensure <a href="http://en.wikipedia.org/wiki/Code_reuse">code reuse</a>. And my inclination to keep things simple made me wary of making use of the <a href="http://bonsaiden.github.com/JavaScript-Garden/#object.prototype">prototype</a>. It is undoubtedly powerful but I don&#x27;t think even the mighty <a href="http://javascript.crockford.com/prototypal.html">Crockford</a> would consider it &quot;simple&quot;. Also I had the reservation of exposing my object to the global scope. So what to do? I had an idea.... ## The Big Idea</p><p>For a while I&#x27;ve been making use explicit use of the <a href="http://en.wikipedia.org/wiki/Observer_pattern">Observer pattern</a> in my JavaScript, better known by most as the publish/subscribe (or &quot;PubSub&quot;) pattern. There&#x27;s a million JavaScript libraries that facilitate this and after some experimentation I finally settled on <a href="https://github.com/phiggins42/bloody-jquery-plugins/blob/master/pubsub.js">higgins</a> implementation as it&#x27;s simple and I saw a <a href="http://jsperf.com/pubsubjs-vs-jquery-custom-events/11">JSPerf</a> which demonstrated it as either the fastest or second fastest in class. Up until now my main use for it had been to facilitate loosely coupled GUI interactions. If I wanted one component on the screen to influence anothers behaviour I simply needed to get the first component to publish out the relevant events and the second to subscribe to these self-same events. One of the handy things about publishing out events this way is that with them you can also include data. This data can be useful when driving the response in the subscribers. However, it occurred to me that it would be equally possible to pass an object when publishing an event. <!-- -->*<!-- -->*<u>And the subscribers could enrich that object with data as they saw fit.</u></p><p>*<!-- -->*<!-- --> Now this struck me as a pretty useful approach. It&#x27;s not rock solid secure as it&#x27;s always possible that someone could subscribe to your events and get access to your object as you published out. However, that&#x27;s pretty unlikely to happen accidentally; certainly far less likely than someone else&#x27;s global object clashing with your global object. ## What might this look like in practice?</p><p>So this is what it ended up looking like when I turned my 3 classes into JavaScript files / modules. First BaseReilly.js:</p><pre><code class="language-js">$(function () {
  $.subscribe(&#x27;PubSub.Inheritance.Emulation&#x27;, function (obj) {
    obj.LastName = &#x27;Reilly&#x27;;
  });
});
</code></pre><p>Next BoyReilly.js:</p><pre><code class="language-js">$(function () {
  $.subscribe(&#x27;PubSub.Inheritance.Emulation&#x27;, function (obj) {
    obj.Sex = &#x27;It is a manchild&#x27;;
  });
});
</code></pre><p>And finally JohnReilly.js:</p><pre><code class="language-js">$(function () {
  $.subscribe(&#x27;PubSub.Inheritance.Emulation&#x27;, function (obj) {
    obj.FirstName = &#x27;John&#x27;;
  });
});
</code></pre><p>If the above scripts have been included in a page I can create myself my very own &quot;JohnReilly&quot; in JavaScript like so:</p><pre><code class="language-js">var oJohnReilly = {}; //Empty object

$.publish(&#x27;PubSub.Inheritance.Emulation&#x27;, [oJohnReilly]); //Empty object &quot;published&quot; so it can be enriched by subscribers

console.log(JSON.stringify(oJohnReilly)); //Show me this thing you call &quot;JohnReilly&quot;
</code></pre><p>And it will look like this: <img src="../static/blog/2012-03-17-using-pubsub-observer-pattern-to/JavaScript%2Bversion%2Bof%2BJohnReilly.png"/></p><p>And it works. Obviously the example I&#x27;ve given above it somewhat naive - in reality my object properties are driven by GUI components rather than hard-coded. But I hope this illustrates the point. This technique allows you to simply share functionality between different JavaScript files and so keep your codebase tight. I certainly wouldn&#x27;t recommend it for all circumstances but when you&#x27;re doing something as simple as building up an object to be used to pass data around (as I am) then it works very well indeed. ## A Final Thought on Script Ordering</p><p>A final thing that maybe worth mentioning is script ordering. The order in which functions are called is driven by the order in which subscriptions are made. In my example I was registering the scripts in this order:</p><pre><code class="language-html">&lt;script src=&quot;/Scripts/PubSubInheritanceDemo/BaseReilly.js&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;/Scripts/PubSubInheritanceDemo/BoyReilly.js&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;/Scripts/PubSubInheritanceDemo/JohnReilly.js&quot;&lt;&gt;/script&gt;
</code></pre><p>So when my event was published out the functions in the above JS files would be called in this order: 1. BaseReilly.js 2. BoyReilly.js 3. JohnReilly.js</p><p>If you were so inclined you could use this to emulate inheritance in behaviour. Eg you could set a property in <code>BaseReilly.js</code> which was subsequently overridden in <code>JohnReilly.js</code> or <code>BoyReilly.js</code> if you so desired. I&#x27;m not doing that myself but it occurred as a possibility. ## PS</p><p>If you&#x27;re interested in learning more about JavaScript stabs at inheritance you could do far worse than look at Bob Inces in depth StackOverflow <a href="http://stackoverflow.com/a/1598077/761388">answer</a>.</p><pre><code></code></pre>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Striving for (JavaScript) Convention]]></title>
            <link>https://blog.johnnyreilly.com/2012/03/12/striving-for-javascript-convention</link>
            <guid>Striving for (JavaScript) Convention</guid>
            <pubDate>Mon, 12 Mar 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[Update]]></description>
            <content:encoded><![CDATA[<h2>Update</h2><p>The speed of change makes fools of us all. Since I originally wrote this post all of 3 weeks ago Visual Studio 11 beta has been released and the issues I was seeking to solve have pretty much been resolved by the new innovations found therein. It&#x27;s nicely detailed in <a href="http://www.twitter.com/carlbergenhem">@carlbergenhem</a>&#x27;s blog post: <a href="https://blogs.telerik.com/blogs/posts/12-03-26/my-top-5-visual-studio-11-designer-improvements-for-asp-net-4-5-development.aspx">My Top 5 Visual Studio 11 Designer Improvements for ASP.NET 4.5 Development</a>. I&#x27;ve left the post in place below but much of what I said (particularly with regard to Hungarian Notation) I&#x27;ve now moved away from. That was originally my intention anyway so that&#x27;s no bad thing. The one HN artefact that I&#x27;ve held onto is using &quot;$&quot; as a prefix for jQuery objects. I think that still makes sense. I would have written my first line of JavaScript in probably 2000. It probably looked something like this: <code>alert(&#x27;hello world&#x27;)</code>. I know. Classy. As I&#x27;ve mentioned before it was around 2010 before I took JavaScript in any way seriously. Certainly it was then when I started to actively learn the language. Because up until this point I&#x27;d been studiously avoiding writing any JavaScript at all I&#x27;d never really given thought to forms and conventions. When I wrote any JavaScript I just used the same style and approaches as I used in my main development language (of C#). By and large I have been following the .net naming conventions which are ably explained by Pete Brown <a href="http://10rem.net/articles/net-naming-conventions-and-programming-standards---best-practices">here</a>. Over time I have started to move away from this approach. Without a deliberate intention to do so I have found myself adopting a different style for my JavaScript code as compared with anything else I write. I wouldn&#x27;t go so far as to say I&#x27;m completely happy with the style I&#x27;m currently using. But I find it more helpful than not and thought it might be worth talking about. It was really 2 things that started me down the road of &quot;rolling my own&quot; convention: dynamic typing and the lack of safety nets. Let&#x27;s take each in turn.... ### 1<!-- -->.<!-- --> Dynamic typing</p><p>Having grown up (in a development sense) using compiled and strongly-typed languages I was used to the IDE making it pretty clear what was what through friendly tooltips and the like: <img src="../static/blog/2012-03-12-striving-for-javascript-convention/IDE.png"/></p><p>JavaScript is loosely / dynamically typed (<a href="http://stackoverflow.com/questions/9154388/does-untyped-also-mean-dynamically-typed-in-the-academic-cs-world">occasionally called &quot;untyped&quot; but let&#x27;s not go there</a>). This means that the IDE can&#x27;t easily determine what&#x27;s what. So no tooltips for you sunshine. ### 2<!-- -->.<!-- --> The lack of safety nets / running with scissors</p><p>Now I&#x27;ve come to love it but what I realised pretty quickly when getting into JavaScript was this: you are running with scissors. If you&#x27;re not careful and you don&#x27;t take precautions it can bloody quickly. If I&#x27;m writing C# I have a lot of safety nets. Not the least of which is &quot;does it compile&quot;? If I declare an integer and then subsequently try to assign a string value to it <u>it won&#x27;t let me</u></p><p>. But JavaScript is forgiving. Some would say too forgiving. Let&#x27;s do something mad:</p><pre><code class="language-js">var iAmANumber = 77;

console.log(iAmANumber); //Logs a number

iAmANumber = &quot;It&#x27;s a string&quot;;

console.log(iAmANumber); //Logs a string

iAmANumber = {
  description: &#x27;I am an object&#x27;,
};

console.log(iAmANumber); //Logs an object

iAmANumber = function (myVariable) {
  console.log(myVariable);
};

console.log(iAmANumber); //Logs a function
iAmANumber(&#x27;I am not a number, I am a free man!&#x27;); //Calls a function which performs a log
</code></pre><p>Now if I were to attempt something similar in C# fuggedaboudit but JavaScript; no I&#x27;m romping home free: <img src="../static/blog/2012-03-12-striving-for-javascript-convention/Mad%2BStuff.png"/></p><p>Now I&#x27;m not saying that you should ever do the above, and thinking about it I can&#x27;t think of a situation where you&#x27;d want to (suggestions on a postcard). But the point is it&#x27;s possible. And because it&#x27;s possible to do this deliberately, it&#x27;s doubly possible to do this accidentally. My point is this: it&#x27;s easy to make bugs in JavaScript. ## What <del>Katy</del> Johnny Did Next</p><p>I&#x27;d started making more and more extensive use of JavaScript. I was beginning to move in the direction of using the <a href="http://en.wikipedia.org/wiki/Single-page_application">single-page application</a> approach (<em><sideNote>although more in the sense of giving application style complexity to individual pages rather than ensuring that entire applications ended up in a single page</sideNote></em>). This meant that whereas in the past I&#x27;d had the occasional 2 lines of JavaScript I now had a multitude of functions which were all interacting in response to user input. All these functions would contain a number of different variables. As well as this I was making use of jQuery for both Ajax purposes and to smooth out the DOM inconsistencies between various browsers. This only added to the mix as variables in one of my functions could be any one of the following: - a number</p><ul><li>a string</li><li>a boolean</li><li>a date</li><li>an object</li><li>an array</li><li>a function</li><li>a jQuery object - not strictly a distinct JavaScript type obviously but treated pretty much as one in the sense that it has a particular functions / properties etc associated with it</li></ul><p>As I started doing this sort of work I made no changes to my coding style. Wherever possible I did <!-- -->*<strong>exactly</strong>*<!-- --> what I would have been doing in C# in JavaScript. And it worked fine. Until.... Okay there is no &quot;until&quot; as such, it did work fine. But what I found was that I would do a piece of work, check it into source control, get users to test it, release the work into Production and promptly move onto the next thing. However, a little way down the line there would be a request to add a new feature or perhaps a bug was reported and I&#x27;d find myself back looking at the code. And, as is often the case, despite the comments I would realise that it wasn&#x27;t particularly clear why something worked in the way it did. (Happily it&#x27;s not just me that has this experience, paranoia has lead me to ask many a fellow developer and they have confessed to similar) When it came to bug hunting in particular I found myself cursing the lack of friendly tooltips and the like. Each time I wanted to look at a variable I&#x27;d find myself tracking back through the function, looking for the initial use of the variable to determine the type. Then I&#x27;d be tracking forward through the function for each subsequent use to ensure that it conformed. Distressingly, I would find examples of where it looked like I&#x27;d forgotten the type of the variable towards the end of a function (for which I can only, regrettably, blame myself). Most commonly I would have a situation like this:</p><pre><code class="language-js">var tableCell = $(&#x27;#ItIsMostDefinitelyATableCell&#x27;); //I jest ;-)

/* ...THERE WOULD BE SOME CODE DOING SOMETHING HERE... */

tableCell.className = &#x27;makeMeProminent&#x27;; //Oh dear - not good.
</code></pre><p>You see what happened above? I forgot I had a jQuery object and instead treated it like it was a standard DOM element. Oh dear. ## Spinning my own safety net; Hungarian style</p><p>After I&#x27;d experienced a few of the situations described above I decided that steps needed to be taken to minimise the risk of this. In this case, I decided that &quot;steps&quot; meant <a href="http://en.wikipedia.org/wiki/Hungarian_notation">Hungarian notation</a>. I know. I bet you&#x27;re wincing right now. For those of you that don&#x27;t remember HN was pretty much the standard way of coding at one point (although at the point that I started coding professionally it had already started to decline). It was adopted in simpler times long before the modern IDE&#x27;s that tell you what each variable is became the norm. Back when you couldn&#x27;t be sure of the types you were dealing with. In short, kind of like my situation with JavaScript right now. There&#x27;s not much to it. By and large HN simply means having a lowercase prefix of 1-3 characters on all your variables indicating type. It doesn&#x27;t solve all your problems. It doesn&#x27;t guarantee to stop bugs. But because each instance of the variables use implicitly indicates it&#x27;s type it makes bugs more glaringly obvious. This means when writing code I&#x27;m less likely to misuse a variable (eg <code>iNum = &quot;JIKJ&quot;</code>) because part of my brain would be bellowing: &quot;that just looks wrong... pay better attention lad!&quot;. Likewise, if I&#x27;m scanning through some JavaScript and searching for a bug then this can make it more obvious. Here&#x27;s some examples of different types of variables declared using the style I have adopted:</p><pre><code class="language-js">var iInteger = 4;
var dDecimal = 10.5;
var sString = &#x27;I am a string&#x27;;
var bBoolean = true;
var dteDate = new Date();
var oObject = {
  description: &#x27;I am an object&#x27;,
};
var aArray = [34, 77];
var fnFunction = function () {
  //Do something
};
var $jQueryObject = $(&#x27;#ItIsMostDefinitelyATableCell&#x27;);
</code></pre><p>Some of you have read this and thought &quot;hold on a minute... JavaScript doesn&#x27;t have integers / decimals etc&quot;. You&#x27;re quite right. My style is not specifically stating the type of a variable. More it is seeking to provide a guide on how a variable should be used. JavaScript does not have integers. But oftentimes I&#x27;ll be using a number variable which i will only ever want to treat as an integer. And so I&#x27;ll name it accordingly. ## Spinning a better safety net; DOJO style</p><p>I would be the first to say that alternative approaches are available. And here&#x27;s one I recently happened upon that I rather like the look of: look 2/3rds down at the parameters section of <a href="http://dojotoolkit.org/community/styleGuide">the DOJO styleguide</a> Essentially they advise specifying parameter types through the use of prefixed comments. See the examples below:</p><pre><code class="language-js">function(/*String*/ foo, /*int*/ bar)...
</code></pre><p>or</p><pre><code class="language-js">function(/_String?_/ foo, /_int_/ bar, /_String[]?_/ baz)...
</code></pre><p>I really rather like this approach and I&#x27;m thinking about starting to adopt it. It&#x27;s not possible in Hungarian Notation to be so clear about the purpose of a variable. At least not without starting to adopt all kinds of kooky conventions that take in all the possible permutations of variable types. And if you did that you&#x27;d really be defeating yourself anyway as it would simply reduce the clarity of your code and make bugs more likely. ## Spinning a better safety net; unit tests</p><p>Despite being quite used to writing unit tests for all my server-side code I have not yet fully embraced unit testing on the client. Partly I&#x27;ve been holding back because of the variety of JavaScript testing frameworks available. I wasn&#x27;t sure which to start with. But given that it is so easy to introduce bugs into JavaScript I have come to the conclusion that it&#x27;s better to have some tests in place rather than none. Time to embrace the new. ## Conclusion</p><p>I&#x27;ve found using Hungarian Notation useful whilst working in JavaScript. Not everyone will feel the same and I think that&#x27;s fair enough; within reason I think it&#x27;s generally a good idea to go with what you find useful. However, I am giving genuine consideration to moving to the DOJO style and moving back to my more standard camel-cased variable names instead of Hungarian Notation. Particularly since I strive to keep my functions short with the view that ideally each should 1 thing well. Keep it simple etc... And so in a perfect world the situation of forgetting a variables purpose shouldn&#x27;t really arise... I think once I&#x27;ve got up and running with JavaScript unit tests I may make that move. Hungarian Notation may have proved to be just a stop-gap measure until better techniques were employed...</p><pre><code></code></pre>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[jQuery Unobtrusive Remote Validation]]></title>
            <link>https://blog.johnnyreilly.com/2012/03/03/jquery-unobtrusive-remote-validation</link>
            <guid>jQuery Unobtrusive Remote Validation</guid>
            <pubDate>Sat, 03 Mar 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[Just recently I have been particularly needing to make use of remote / server-side validation in my ASP.NET MVC application and found that the unobtrusive way of using this seemed to be rather inadequately documented (of course it's possible that it's well documented and I just didn't find the resources). Anyway I've rambled on much longer than I intended to in this post so here's the TL;DR:]]></description>
            <content:encoded><![CDATA[<p>Just recently I have been particularly needing to make use of remote / server-side validation in my ASP.NET MVC application and found that the unobtrusive way of using this seemed to be rather inadequately documented (of course it&#x27;s possible that it&#x27;s well documented and I just didn&#x27;t find the resources). Anyway I&#x27;ve rambled on much longer than I intended to in this post so here&#x27;s the TL;DR:</p><ul><li>You <!-- -->*<strong>can</strong>*<!-- --> use remote validation driven by unobtrusive data attributes</li><li>Using remote validation you can supply <!-- -->*<strong>multiple</strong>*<!-- --> parameters to be evaluated</li><li>It is possible to block validation and force it to be re-evaluted - although using a slightly hacky method which I document here. For what it&#x27;s worth I acknowledge up front that this is <!-- -->*<strong>not</strong>*<!-- --> an ideal solution but it does seem to work. I really hope there is a better solution out there and if anyone knows about it then please get in contact and let me know.</li></ul><p>Off we go... So, jQuery unobtrusive validation; clearly the new cool right?</p><p>I&#x27;d never been particularly happy with the validation that I had traditionally been using with ASP.NET classic. It worked... but it always seemed a little... clunky? I realise that&#x27;s not the most well expressed concern. For basic scenarios it seemed fine, but I have recollections of going through some pain as soon as I stepped outside of the basic form validation. Certainly when it came to validating custom controls that we had developed it never seemed entirely straightforward to get validation to play nice.</p><p>Based on this I was keen to try something new and the opportunity presented itself when we started integrating MVC into our classic WebForms app. (By the way if you didn&#x27;t know that MVC and ASP.NET could live together in perfect harmony, well, they can! And a good explanation on how to achieve it is offered by Colin Farr <a href="http://www.britishdeveloper.co.uk/2011/05/convert-web-forms-mvc3-how-to.html">here</a>.)</p><p>Jörn Zaefferer came out with the <a href="http://bassistance.de/jquery-plugins/jquery-plugin-validation/">jQuery validation plug-in</a> way back in 2006. And mighty fine it is too. Microsoft (gor&#x27; bless &#x27;em) really brought something new to the jQuery validation party when they came out with their unobtrusive javascript validation library along with MVC 3. What this library does, in short, is allows for jQuery validation to be driven by <code>data-val-*</code> attributes alone as long as the <a href="http://ajax.aspnetcdn.com/ajax/jquery.validate/1.9/jquery.validate.js">jquery.validate.js</a> and <a href="http://ajax.aspnetcdn.com/ajax/mvc/3.0/jquery.validate.unobtrusive.js">jquery.validate.unobtrusive.js</a> libraries are included in the screen (I have assumed you are already including jQuery). I know; powerful stuff!</p><p>A good explanation of unobtrusive validation is given by Brad Wilson <a href="http://bradwilson.typepad.com/blog/2010/10/mvc3-unobtrusive-validation.html">here</a>.</p><p>Anyway, to my point: what about remote validation? That is to say, what about validation which needs to go back to the server to perform the necessary tests? Well I struggled to find decent examples of how to use this. Those that I did find seemed to universally be php examples; not so useful for an ASP.NET user. Also, when I did root out an ASP.NET example there seemed to be a fundamental flaw. Namely, if remote validation hadn&#x27;t been triggered and completed successfully then the submit could fire anyway. This seems to be down to the asynchronous nature of the test; ie because it is <!-- -->*<strong>not</strong>*<!-- --> synchronous there is no &quot;block&quot; to the submit. And out of the box with unobtrusive validation there seems no way to make this synchronous. I could of course wire this up manually and simply side-step the restrictions of unobtrusive validation but that wasn&#x27;t what I wanted.</p><p>*<!-- -->*<!-- -->*<!-- -->Your mission John, should you decide to accept it, is this: <u>block the submit until remote validation has completed successfully</u></p><p>. As always, should you or any of your I.M. Force be caught or killed, the Secretary will disavow any knowledge of your actions.<!-- -->*<!-- -->*<!-- -->*</p><p>So that&#x27;s what I wanted to do. Make it act like it&#x27;s synchronous even though it&#x27;s asynchronous. Bit horrible but I had a deadline to meet and so this is my pragmatic solution. There may be better alternatives but this worked for me.</p><p>First of all the HTML:</p><pre><code class="language-html">&lt;form
  action=&quot;/Dummy/ValidationDemo.mvc/SaveUser&quot;
  id=&quot;ValidationForm&quot;
  method=&quot;post&quot;
&gt;
  First name:
  &lt;input
    data-val=&quot;true&quot;
    data-val-required=&quot;First Name required&quot;
    id=&quot;FirstName&quot;
    name=&quot;FirstName&quot;
    type=&quot;text&quot;
    value=&quot;&quot;
  /&gt;

  Last name:
  &lt;input
    data-val=&quot;true&quot;
    data-val-required=&quot;Last Name required&quot;
    id=&quot;LastName&quot;
    name=&quot;LastName&quot;
    type=&quot;text&quot;
    value=&quot;&quot;
  /&gt;

  User name:
  &lt;input
    id=&quot;UserName&quot;
    name=&quot;UserName&quot;
    type=&quot;text&quot;
    value=&quot;&quot;
    data-val=&quot;true&quot;
    data-val-required=&quot;You must enter a user name before we can validate it remotely&quot;
    data-val-remote=&quot;&amp;amp;#39;UserNameInput&amp;amp;#39; is invalid.&quot;
    data-val-remote-additionalfields=&quot;*.FirstName,*.LastName&quot;
    data-val-remote-url=&quot;/Dummy/ValidationDemo/IsUserNameValid&quot;
  /&gt;

  &lt;input
    id=&quot;SaveMyDataButton&quot;
    name=&quot;SaveMyDataButton&quot;
    type=&quot;button&quot;
    value=&quot;Click to Save&quot;
  /&gt;
&lt;/form&gt;
</code></pre><p>I should mention that on my actual page (a cshtml partial view) the HTML for the inputs is generated by the use of the <a href="http://msdn.microsoft.com/en-us/library/system.web.mvc.html.inputextensions.textboxfor.aspx">InputExtensions.TextBoxFor</a> method which is lovely. It takes your model and using the validation attributes that decorate your models properties it generates the relevant jQuery unobtrusive validation data attributes so you don&#x27;t have to do it manually.</p><p>But for the purposes of seeing what&#x27;s &quot;under the bonnet&quot; I thought it would be more useful to post the raw HTML so it&#x27;s entirely clear what is being used. Also there doesn&#x27;t appear to be a good way (that I&#x27;ve yet seen) for automatically generating Remote validation data attributes in the way that I&#x27;ve found works. So I&#x27;m manually specifying the <code>data-val-remote-*</code> attributes using the htmlAttributes parameter of the TextBoxFor (<a href="http://stackoverflow.com/questions/4844001/html5-data-with-asp-net-mvc-textboxfor-html-attributes">using &quot;<!-- -->_<!-- -->&quot; to replace &quot;-&quot;</a> obviously).</p><p>Next the JavaScript that performs the validation:</p><pre><code class="language-js">$(document).ready(function () {
  var intervalId = null,
    //
    // DECLARE FUNCTION EXPRESSIONS
    //

    //======================================================
    // function that triggers update when remote validation
    // completes successfully
    //======================================================
    pendingValidationComplete = function () {
      var i, errorList, errorListForUsers;
      var $ValidationForm = $(&#x27;#ValidationForm&#x27;);
      if ($ValidationForm.data(&#x27;validator&#x27;).pendingRequest === 0) {
        clearInterval(intervalId);

        //Force validation to present to user
        //(this will *not* retrigger remote validation)
        if ($ValidationForm.valid()) {
          alert(&#x27;Validation has succeeded - you can now submit&#x27;);
        } else {
          //Validation failed!
          errorList = $ValidationForm.data(&#x27;validator&#x27;).errorList;
          errorListForUsers = [];
          for (i = 0; i &lt; errorList.length; i++) {
            errorListForUsers.push(errorList[i].message);
          }

          alert(errorListForUsers.join(&#x27;\r\n&#x27;));
        }
      }
    },
    //======================================================
    // Trigger validation
    //======================================================
    triggerValidation = function (evt) {
      //Removed cached values where remote is concerned
      // so remote validation is retriggered
      $(&#x27;#UserName&#x27;).removeData(&#x27;previousValue&#x27;);

      //Trigger validation
      $(&#x27;#ValidationForm&#x27;).valid();

      //Setup interval which will evaluate validation
      //(this approach because of remote validation)
      intervalId = setInterval(pendingValidationComplete, 50);
    };

  //
  //ASSIGN EVENT HANDLERS
  //
  $(&#x27;#SaveMyDataButton&#x27;).click(triggerValidation);
});
</code></pre><p>And finally the Controller:</p><pre><code class="language-cs">public JsonResult IsUserNameValid(string UserName,
                                  string FirstName,
                                  string LastName)
{
  var userNameIsUnique = IsUserNameUnique(UserName);
  if (userNameIsUnique)
    return Json(true, JsonRequestBehavior.AllowGet);
  else
    return Json(string.Format(
                  &quot;{0} is already taken I&#x27;m afraid {1} {2}&quot;,
                  UserName, FirstName, LastName),
                JsonRequestBehavior.AllowGet);
}

private bool IsUserNameUnique(string potentialUserName)
{
  return false;
}
</code></pre><p>So what happens here exactly? Well it&#x27;s like this:</p><ol><li>The user enters their first name, last name and desired user name and hits the &quot;Click to Save&quot; button.</li><li>This forces validation by first removing any cached validation values stored in <code>previousValue</code> data attribute and then triggering the <code>valid</code> method. Disclaimer: I KNOW THIS IS A LITTLE HACKY. I would have expected there would be some way in the API to manually re-force validation. Unless I&#x27;ve missed something there doesn&#x27;t appear to be. (<a href="http://stackoverflow.com/a/3797712/761388">And the good citizens of Stack Overflow would seem to concur.</a>) I would guess that the underlying assumption is that if nothing has changed on the client then that&#x27;s all that matters. Clearly that&#x27;s invalid for our remote example given that a username could be &quot;claimed&quot; at any time; eg in between people first entering their username (when validation should have fired automatically) and actually submitting the form. Anyway - this approach seems to get us round the problem.</li><li>When validation takes place the IsUserNameValid action / method on our controller will be called. It&#x27;s important to note that I have set up a method that takes 3 inputs; UserName, which is supplied by default as the UserName input is the one which is decorated with remote validation attributes as well as the 2 extra inputs of FirstName and LastName. In the example I&#x27;ve given I don&#x27;t actually need these extra attributes. I&#x27;m doing this because I know that I have situations in remote validation where I <!-- -->*<strong>need</strong>*<!-- --> to supply multiple inputs and so essentially I did it here as a proof of concept. The addition of these 2 extra inputs was achieved through the use of the <code>data-val-remote-additionalfields</code> attribute. When searching for documentation about this I found absolutely <u>none</u></li></ol><p>. I assume there is some out there - if anyone knows then I&#x27;d very pleased to learn about it. I only learned about it in the end by finding an example of someone using this out in the great wide world and understanding how to use it based on their example. To understand how the <code>data-val-remote-additionalfields</code> attribute works you can look at jquery.validate.unobtrusive.js. If you&#x27;re just looking to get up and running then I found that the following works: <code>data-val-remote-additionalfields=&quot;*.FirstName,*.LastName&quot;</code> You will notice that: - Each parameter is supplied in the format <em>*<!-- -->.<!-- -->[InputName]</em> and inputs are delimited by &quot;,&quot;&#x27;s - Name is a <u>required</u></p><p>attribute for an input if you wish it to be evaluated with unobtrusive validation. (Completely obvious statement I realise; I&#x27;m writing that sentence more for my benefit than yours) - Finally, our validation always fails. That&#x27;s deliberate - I just wanted to be clear on the approach used to get remote unobtrusive validation with extra parameters up and running. 4. Using <code>setInterval</code> we intend to trigger the <code>pendingValidationComplete</code> function to check if remote validation has completed every 50ms - again I try to avoid setInterval wherever possible but this seems to be the most sensible solution in this case. 5. When the remote request finally completes (ie when <code>pendingRequest</code> has a value of 0) then we can safely proceed on the basis of our validation results. In the example above I&#x27;m simply alerting to the screen based on my results; this is <!-- -->*<strong>not</strong>*<!-- --> advised for any finished work; I&#x27;m just using this mechanism here to demonstrate the principle.</p><p>Validation in action:</p><p><img src="../static/blog/2012-03-03-jquery-unobtrusive-remote-validation/validation-screenshot2.png"/></p><p>Well I&#x27;ve gone on for far too long but I am happy to have an approach that does what I need. It does feel like a slightly hacky solution and I expect that there is a better approach for this that I&#x27;m not aware of. As much as anything else I&#x27;ve written this post in the hope that someone who knows this better approach will set me straight. In summary, this works. But if you&#x27;re aware of a better solution then please do get in contact - I&#x27;d love to know!</p><p><strong>PS:</strong>Just in case you&#x27;re in the process of initially getting up and running with unobtrusive validation I&#x27;ve listed below a couple of general helpful bits of config etc:</p><p>The following setting is essential for Application_Start in Global.asax.cs:</p><pre><code class="language-cs">DataAnnotationsModelValidatorProvider.AddImplicitRequiredAttributeForValueTypes = false;
</code></pre><p>The following settings should be used in your Web.Config:</p><pre><code class="language-xml">&lt;appSettings&gt;
  &lt;add key=&quot;ClientValidationEnabled&quot; value=&quot;true&quot; /&gt;
  &lt;add key=&quot;UnobtrusiveJavaScriptEnabled&quot; value=&quot;true &quot;/&gt;
&lt;/appSettings&gt;
</code></pre><p>My example used the following scripts:</p><pre><code class="language-html">&lt;script src=&quot;Scripts/jquery-1.7.1.js&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;Scripts/jquery.validate.js&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;Scripts/jquery.validate.unobtrusive.js&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;Scripts/ValidationDemo.js&quot;&gt;&lt;/script&gt;
</code></pre>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Joy of JSON]]></title>
            <link>https://blog.johnnyreilly.com/2012/02/23/joy-of-json</link>
            <guid>The Joy of JSON</guid>
            <pubDate>Thu, 23 Feb 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[So back to JSON. For those of you that don't know JSON stands for JavaScript Object Notation and is lightweight text based data interchange format. Rather than quote other people verbatim you can find thorough explanations of JSON here: - Introducing JSON]]></description>
            <content:encoded><![CDATA[<p>So back to JSON. For those of you that don&#x27;t know JSON stands for JavaScript Object Notation and is lightweight text based data interchange format. Rather than quote other people verbatim you can find thorough explanations of JSON here: - <a href="http://www.json.org/">Introducing JSON</a></p><ul><li><a href="http://www.json.org/js.html">JSON in Javascript</a></li></ul><p>As mentioned in my previous <a href="http://icanmakethiswork.blogspot.com/2012/02/potted-history-of-using-ajax-on.html">post on Ajax</a> I came upon JSON quite by accident and was actually using it for some time without having any idea. But let&#x27;s pull back a bit. Let&#x27;s start with the JavaScript Object Literal. Some years ago I came upon this article by Christan Heilmann about the JavaScript Object Literal which had been published all the way back in 2006: <a href="http://christianheilmann.com/2006/02/16/show-love-to-the-object-literal/">Show love to the JavaScript Object Literal</a> Now when I read this it was a revelation to me. I hadn&#x27;t really used JavaScript objects a great deal at this point (yes I am one of those people that started using JavaScript without actually learning the thing) and when I had used them is was through the <code>var obj = new Object()</code> pattern (as that&#x27;s the only approach I knew). So it was wonderful to discover that instead of the needlessly verbose:</p><pre><code class="language-js">var myCar = new Object();
myCar.wheels = 4;
myCar.colour = &#x27;blue&#x27;;
</code></pre><p>I could simply use the much more concise object literal syntax to declare an object instead:</p><pre><code class="language-js">var myCar = { wheels: 4, colour: &#x27;blue&#x27; };
</code></pre><p>Lovely. Henceforth I adopted this approach in my code as I&#x27;m generally a believer that brevity is best. It was sometime later that I happened upon JSON (when I started looking into <a href="http://icanmakethiswork.blogspot.com/2012/01/jqgrid-its-just-far-better-grid.html">jqGrid</a>). Basically I was looking to pass complex data structures backward and forward to the server and, as far as I knew, there was no way to achieve this simply in JavaScript. I was expecting that I would have to manually serialise and deserialise (yes dammit I will use the English spellings!) objects when ever I wanted to do this sort of thing. However, I was reading the the fantastic Dave Ward&#x27;s <a href="http://encosia.com/">Encosia</a> blog which on this occasion was talking about the <a href="http://encosia.com/why-aspnet-ajax-updatepanels-are-dangerous/">troubles of UpdatePanels</a> (a subject close to my heart by the way) and more interestingly the use of PageMethods in ASP.NET. This is what he said that made me prick up my ears: <em>&quot;Page methods allow ASP.NET AJAX pages to directly execute a page’s static methods, using JSON (JavaScript Object Notation). JSON is basically a minimalistic version of SOAP, which is perfectly suited for light weight communication between client and server.&quot;</em> JSON is a lightweight SOAP eh? I&#x27;ve used SOAP. I wonder if I could use this.... To my complete surprise, and may I say delight, I discovered that a wonderful fellow called Douglas Crockford, he of <a href="http://www.amazon.co.uk/JavaScript-Good-Parts-Douglas-Crockford/dp/0596517742">JavaScript, The Good Parts</a> fame had quietly come up with JSON some time ago. JSON, from my perspective, turned out to be a simple way to turn an object into a string and then from a string back into an object. So simple that it consists of 2 methods on a JSON object: - JSON.stringify(myObject) - take an object and make me a JSON string. (and by the way isn&#x27;t &quot;stringify&quot; just the loveliest method name ever?)</p><ul><li>JSON.parse(myJSONString) - take a JSON string and make me an object</li></ul><p>Let me illustrate the above method names using the myCar example from earlier:</p><pre><code class="language-js">var myCar = { wheels: 4, colour: &#x27;blue&#x27; };
// myCar is an object

var myCarJSON = JSON.stringify(myCar);
//myCarJSON will look like this: &#x27;{&quot;wheels&quot;:4,&quot;colour&quot;:&quot;blue&quot;}&#x27;

var anotherCarMadeFromMyJSON = JSON.parse(myCarJSON);
//anotherCarMadeFromMyJSON will be a brand new &quot;car&quot; object
</code></pre><p>I&#x27;ve also demonstrated this using the Chrome Console: <img src="../static/blog/2012-02-23-joy-of-json/Using%2BJSON.png"/></p><p>Crockford initially invented/discovered JSON himself and wrote a little helper library which provided a JSON object to be used by all and sundry. This can be found here: <a href="https://github.com/douglascrockford/JSON-js">JSON on GitHub</a> Because JSON was so clearly wonderful, glorious and useful it ended up becoming a part of the EcmaScript 5 spec (in fact it&#x27;s worth reading the brilliant <a href="http://ejohn.org/blog/ecmascript-5-strict-mode-json-and-more/">John Resig&#x27;s blog post</a> on this). This has lead to JSON being offered <a href="http://en.wikipedia.org/wiki/JSON#Native_encoding_and_decoding_in_browsers">natively in browsers</a> for quite some time. However, for those of us (and I am one alas) still supporting IE 6 and the like we still have Crockfords JSON2.js to fall back on.</p><pre><code></code></pre>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[WCF Transport Windows authentication using NetTcpBinding in an Intranet environment]]></title>
            <link>https://blog.johnnyreilly.com/2012/02/15/wcf-transport-windows-authentication</link>
            <guid>WCF Transport Windows authentication using NetTcpBinding in an Intranet environment</guid>
            <pubDate>Wed, 15 Feb 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[Update]]></description>
            <content:encoded><![CDATA[<h2>Update</h2><p>Since I wrote this initial post I&#x27;ve taken thinks on a bit further. Take a look at this post to see what I mean: <a href="http://icanmakethiswork.blogspot.com/2012/03/wcf-moving-from-config-to-code-simple.html">http://icanmakethiswork.blogspot.com/2012/03/wcf-moving-from-config-to-code-simple.html</a> I know I said I&#x27;d write about JSON this time. I will get to that but not this time. This time WCF authentication quirks. I&#x27;ve been working on a project that uses .NET Remoting to have a single central point to which web applications and Windows services can call into. This is used in an intranet environment and all the websites and Windows services were hosted on the same single server along with our .NET Remoting Windows service. (They could quite easily have been on different servers but there was no need in this case.) It was decided to &quot;embrace the new&quot; by migrating this .NET Remoting project over to WCF. The plan wasn&#x27;t to do anything revolutionary, just to move from one approach to the other as easily as possible. I found the following useful article on MSDN: <a href="http://msdn.microsoft.com/en-us/library/aa730857%28v=vs.80%29.aspx">http://msdn.microsoft.com/en-us/library/aa730857%28v=vs.80%29.aspx</a> This particular article was helpful and following the steps enclosed I was quickly up and running with a basic WCF service hosted in a Windows service. It was at this point I started thinking about security. The existing .NET Remoting approach had no security in place. This wasn&#x27;t ideal but also probably wasn&#x27;t the worry you might think. It was hosted in an intranet environment and hence not so exposed to the rigours of the Wild Wild Web. However, since I was looking at WCF I thought it would be a good opportunity to get some basic security in place. This generally pleases auditors. I opted to use <a href="http://msdn.microsoft.com/en-us/library/ms733089.aspx">Windows Transport authentication</a> as this seemed pretty appropriate for an intranet environment. The idea being that we&#x27;d authenticate with Windows for an account in our domain. After headbutting Windows for some time I managed to get a successful client call going from the website running on my development machine to the (separate) development server that was hosting our WCF Window service using Transport Windows authentication. However, when deploying the website to the development server I discovered we would experience the following error when the website attempted to call the WCF service (on the same server).</p><pre><code>Event Type: Failure Audit
Event Source: Security
Event Category: Logon/Logoff
Event ID: 537
Date: 15/02/2012
Time: 16:32:04
User: NT AUTHORITY\SYSTEM
Computer: MINE999
Description:
Logon Failure:
Reason: An error occurred during logon
Logon Type: 3
Logon Process: ^
Authentication Package: NTLM
Status code: 0xC000006D
</code></pre><p>Not terribly helpful. At the end of the day it seemed we were suffering from a security &quot;feature&quot; introduced by Microsoft to prevent services calling services on the same box with a fully qualified name. An explanation of this can be found here: <a href="http://developers.de/blogs/damir_dobric/archive/2009/08/28/authentication-problems-by-using-of-ntlm.aspx">http://developers.de/blogs/damir_dobric/archive/2009/08/28/authentication-problems-by-using-of-ntlm.aspx</a> Using method 1 in the enclosed link I initially worked round this by amending the registry and rebooting the server: <a href="http://support.microsoft.com/kb/887993">http://support.microsoft.com/kb/887993</a> This was not a fantastic solution. Fortunately I subsequently found a better one but since the resources on the web are <!-- -->*<strong>ATROCIOUS</strong>*<!-- --> on this point I thought I should take the time to note down the full explanation since otherwise it&#x27;ll be lost in the mists of time. Here we go: The equivalent security to the previous .NET Remoting solution in WCF was to use this config setting on client and service:</p><pre><code class="language-xml">&lt;security mode=&quot;None&quot; /&gt;
</code></pre><p>As I&#x27;ve said, this is an intranet environment and so having this &quot;none&quot; security setting in place is made less worrying by the fact that the network itself is secured. But obviously this is not ideal and unlikely to be audit compliant. To use Windows security you need this netTcpBinding config setting on client and service:</p><pre><code class="language-xml">&lt;security mode=&quot;Transport&quot;&gt;
&lt;transport clientCredentialType=&quot;Windows&quot; /&gt;
&lt;/security&gt;
</code></pre><p>To call the service with this setting in place you will need to be an authenticated Windows user. (Or at the very least impersonating one - but you knew that.) <strong>NOW FOR THE MOST IMPORTANT BIT.....</strong> The endpoint addresses <!-- -->*<strong>must</strong>*<!-- --> be &quot;localhost&quot; for <em>both</em> client and service when both are deployed to the same server. If this is not the case then you will suffer from the aforementioned security &quot;feature&quot; which will provide you with unhelpful &quot;the server has rejected the client credentials&quot; messages and <!-- -->*<strong>nothing</strong>*<!-- --> else. <strong>OK FINISHED - MOVE ALONG NOW... NOTHING MORE TO SEE HERE</strong> With WCF Windows Transport authentication in place you can interrogate the calling user id within the service methods by simply evaluating ServiceSecurityContext.Current.PrimaryIdentity.Name (which will be something like &quot;myDomain\myUserName&quot;). So we you wanted to, we could have a simple step which evaluated if the calling user is on the &quot;approved&quot; / &quot;authorised&quot; list. I&#x27;m sure this could be made more sophisticated by using groups etc I guess - though I haven&#x27;t investigated it further as yet. In fact, I suspect Microsoft may have something even more sophisticated still available for use which I&#x27;m unaware of - if anyone knows a simple explanation of this then please do let me know! In closing, I do think Microsoft could work on providing more helpful error messages than &quot;the server has rejected the client credentials&quot;. Going by what I read as I researched this error many people seem to have struggled much as I did before eventually bailing out and ended up chancing it by turning security off in their applications. Clearly it is not desirable to have people so confused by errors that they give up and settle for a less secure solution.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A Potted History of using Ajax (on the Microsoft Stack of Love)]]></title>
            <link>https://blog.johnnyreilly.com/2012/02/05/potted-history-of-using-ajax-on</link>
            <guid>A Potted History of using Ajax (on the Microsoft Stack of Love)</guid>
            <pubDate>Sun, 05 Feb 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[This post originally started out as an explanation of JSON. However as I wrote this quickly got abandoned in favour of writing about how I came to use JSON in the first place - which was through the use of Ajax. Having written a goodly amount I've now decided to move the actual JSON stuff into another post since I think Ajax is probably worth thinking about by itself rather than as an aside. So let me start at the beginning and explain how I came to use Ajax in the first place (this may take some time so please bear with me). In late 2004 I first started working on a project which I was to remain involved with (on and off) for a very long time indeed. The project was part financial reporting system and part sales incentivisation tool; it was used internally in the investment bank in which I was working. The project had been in existence for a number of years and had a web front end which at that point would been built in a combination of HTML, JavaScript, classic ASP and with a Visual Basic 6.0 back end. One of the reasons I had been brought on to the project was to help ".Net-ify" the thing and migrate it to ASP.NET and C#. I digress. The interesting thing about this app was that there were actually some quite advanced things being done with it (despite the classic ASP / VB). The users could enter trades into the system which represented actual trades that had been entered into a trading system elsewhere in the organisation. These trades would be assigned a reporting value which would be based on their various attributes. (Stay with me people this will get more interesting I \*promise\*.) The calculation of the reporting value was quite an in depth process and needed to be performed server-side. However, the users had decreed that it wasn't acceptable to do a full postback to the server to perform this calculation; they wanted it done "on-the-fly". Now if you asked me at the time I'd have said "can't be done". Fortunately the other people working on the project then weren't nearly so defeatist. Instead they went away and found Microsoft's webservice.htc library. For those of you that don't know this was a JavaScript library that Microsoft came up with to enable the access of Web Services on the client. Given that it was designed to work with IE 5 I suspect it was created between 1999-2001 (but I'm not certain about that). Now it came as a revelation to me but this was a JavaScript library that talked to our web services through the medium of XML. In short it was my first encounter with anything remotely Ajax\-y. It was exciting! However, the possibilities of what we could do didn't actually become apparent to me for some years. It's worth saying that the way we were using webservice.htc was exceedingly simplistic and rather than investigating further I took the limited ways we were using it as indications of the limitations of Ajax and / or webservice.htc. So for a long time I thought the following: - The only way to pass multiple arguments to a web service was to package up arguments into a single string with delimiters which you could split and unpackage as your first step on the server.]]></description>
            <content:encoded><![CDATA[<p>This post originally started out as an explanation of JSON. However as I wrote this quickly got abandoned in favour of writing about how I came to use JSON in the first place - which was through the use of Ajax. Having written a goodly amount I&#x27;ve now decided to move the actual JSON stuff into another post since I think Ajax is probably worth thinking about by itself rather than as an aside. So let me start at the beginning and explain how I came to use Ajax in the first place (this may take some time so please bear with me). In late 2004 I first started working on a project which I was to remain involved with (on and off) for a very long time indeed. The project was part financial reporting system and part sales incentivisation tool; it was used internally in the investment bank in which I was working. The project had been in existence for a number of years and had a web front end which at that point would been built in a combination of HTML, JavaScript, classic ASP and with a Visual Basic 6.0 back end. One of the reasons I had been brought on to the project was to help &quot;.Net-ify&quot; the thing and migrate it to ASP.NET and C#. I digress. The interesting thing about this app was that there were actually some quite advanced things being done with it (despite the classic ASP / VB). The users could enter trades into the system which represented actual trades that had been entered into a trading system elsewhere in the organisation. These trades would be assigned a reporting value which would be based on their various attributes. (Stay with me people this will get more interesting I <!-- -->*<strong>promise</strong>*<!-- -->.) The calculation of the reporting value was quite an in depth process and needed to be performed server-side. However, the users had decreed that it wasn&#x27;t acceptable to do a full postback to the server to perform this calculation; they wanted it done &quot;on-the-fly&quot;. Now if you asked me at the time I&#x27;d have said &quot;can&#x27;t be done&quot;. Fortunately the other people working on the project then weren&#x27;t nearly so defeatist. Instead they went away and found Microsoft&#x27;s <a href="http://msdn.microsoft.com/en-us/library/ie/ms531033%28v=vs.85%29.aspx">webservice.htc</a> library. For those of you that don&#x27;t know this was a JavaScript library that Microsoft came up with to enable the access of Web Services on the client. Given that it was designed to work with IE 5 I suspect it was created between 1999-2001 (but I&#x27;m not certain about that). Now it came as a revelation to me but this was a JavaScript library that talked to our web services through the medium of XML. In short it was my first encounter with anything remotely <a href="http://en.wikipedia.org/wiki/Ajax_(programming)">Ajax</a>-<!-- -->y. It was exciting! However, the possibilities of what we could do didn&#x27;t actually become apparent to me for some years. It&#x27;s worth saying that the way we were using webservice.htc was exceedingly simplistic and rather than investigating further I took the limited ways we were using it as indications of the limitations of Ajax and / or webservice.htc. So for a long time I thought the following: - The only way to pass multiple arguments to a web service was to package up arguments into a single string with delimiters which you could <a href="http://en.wikipedia.org/wiki/Comparison_of_programming_languages_(string_functions)#split">split</a> and unpackage as your first step on the server.</p><ul><li>The only valid return type was a single string. And so if you wanted to return a number of numeric values (as we did) the only way to do this was to package up return values into a very long string with delimiters in and (you guessed it!) <a href="http://en.wikipedia.org/wiki/Comparison_of_programming_languages_(string_functions)#split">split</a> and unpackage as your first step on the client.</li><li>The only thing that you could (or would want to) send back and forth between client and server was XML</li></ul><p>So to recap, I&#x27;m now aware that it&#x27;s possible for JavaScript to interact with the server through the use of web services. It&#x27;s possible, but ugly, not that quick and requires an awful lot of manual serialization / deserialization operations. It&#x27;s clearly powerful but not much fun at all. And that&#x27;s where I left it for a number of years. Let&#x27;s fade to black... It&#x27;s now 2007 and Microsoft have released ASP.NET Ajax, the details of which are well explained in this <a href="http://msdn.microsoft.com/en-us/magazine/cc163499.aspx">article</a> (which I have only recently discovered). Now I&#x27;m always interested in &quot;the new&quot; and so I was naturally interested in this. Just to be completely upfront about this I should confess that when I first discovered ASP.NET Ajax I didn&#x27;t clock the power of it at all. Initially I just switched over from using webservice.htc to ASP.NET Ajax. This alone gave us a <!-- -->*<strong>massive</strong>*<!-- --> performance improvement (I know it was massive since we actually received a &quot;well done&quot; email from our users which is testament to the difference it was making to their experience of the system). But we were still performing our manual serialisation / deserialisation of values on the client and the server. ie. Using Ajax was now much faster but still not too much fun. Let&#x27;s jump forward in time again to around 2010 to the point in time when I was discovering jQuery and that JavaScript wasn&#x27;t actually evil. It&#x27;s not unusual for me to play around with &quot;what if&quot; scenarios in my code, just to see what might might be possible. Sometimes I discover things. So it was with JSON. We had a web service in the system that allowed us to look up a counterparty (ie a bank account) with an identifier. Once we looked it up we packaged up the counterparty details (eg name, location etc) into a big long string with delimiters and sent it back to client. One day I decided to change the return type on the web service from a string to the actual counterparty class. So we went from something like this:</p><pre><code class="language-cs">[WebService(Namespace = &quot;http://tempuri.org/&quot;)]
[WebServiceBinding(ConformsTo = WsiProfiles.BasicProfile1_1)]
[ScriptService]
public class CounterpartyWebService : System.Web.Services.WebService
{
  [WebMethod]
  public string GetCounterparty(string parameters)
  {
    string[] aParameters = parameters.Split(&quot;|&quot;);
    int counterpartyId = int.Parse(aParameters[0]);
    bool includeLocation = (aParameters[1] == &quot;1&quot;);
    Counterparty counterparty = \_counterpartyDb
    .GetCounterparty(counterpartyId);

        string returnValue = counterparty.Id +
                          &quot;|&quot; + counterparty.Name +
                          (includeLocation
                            ? &quot;|&quot; + counterparty.Location
                            : &quot;&quot;);

        return returnValue;
  }
}
</code></pre><p>To something like this:</p><pre><code class="language-cs">[WebMethod]
public Counterparty GetCounterparty(string parameters)
{
  string[] aParameters = parameters.Split(&quot;|&quot;);
  int counterpartyId = int.Parse(aParameters[0]);
  bool includeLocation = (aParameters[1] == &quot;1&quot;);
  Counterparty counterparty = _counterpartyDb
    .GetCounterparty(counterpartyId);

  return counterparty;
}
</code></pre><p>I genuinely expected that this was just going to break. It didn&#x27;t. Suddenly on the client I&#x27;m sat there with a full blown object that looks just like the object I had on the server. <strong>WHAT BLACK MAGIC COULD THIS BE??????????</strong> Certain that I&#x27;d discovered witchcraft I decided to try something else. What would happen if I changed the signature on the method so it received individual parameters and passed my individual parameters to the web service instead of packaging them up into a string? I tried this:</p><pre><code class="language-cs">[WebMethod]
public Counterparty GetCounterparty(int counterpartyId, bool includeLocation)
{
  Counterparty counterparty = \_counterpartyDb
  .GetCounterparty(counterpartyId);

  return counterparty;
}
</code></pre><p>And it worked! <strong><a href="http://www.youtube.com/watch?v=N_dWpCy8rdc&amp;feature=related">IT WORKED!!!!!!!!!!!!!!!!!!!!!</a></strong> (And yes I know I wasn&#x27;t actually using the includeLocation parameter - but the point was it was being passed to the server and I could have used it if I&#x27;d wanted to.) I couldn&#x27;t believe it. For <strong>years</strong> I&#x27;d been using Ajax and without <strong>any</strong> idea of the power available to me. The ignorance! The stupidity of the man! To my complete surprise it turned out that: - Ajax could be quick! ASP.NET Ajax was lightening fast when compared to webservice.htc</p><ul><li>You could send multiple arguments to a web service without all that packaging nonsense</li><li>You could return complex objects without the need for packaging it all up yourself.</li></ul><p>Essentially the source of all this goodness was the magic of JSON. I wouldn&#x27;t really come to comprehend this until I moved away from using the ASP.NET Ajax client libraries in favour of using the <a href="http://api.jquery.com/jQuery.ajax/">jQuery.ajax</a> functionality. (Yes, having mostly rattled on about using webservice.htc and ASP.NET Ajax I should clarify that I have now forsaken both for jQuery as I find it more powerful and more configurable - but it&#x27;s the journey that counts I guess!) It&#x27;s abysmal that I didn&#x27;t discover the power of Ajax sooner but the difference this discovery made to me was immense. Approaches that I would have dismissed or shied away from previously because of the amount of &quot;plumbing&quot; involved now became easy. This massively contributed to my <a href="http://www.hanselman.com/blog/HanselminutesPodcast260NETAPIDesignThatOptimizesForProgrammerJoyWithJonathanCarter.aspx">programmer joy</a>! Next time I promise I&#x27;ll aim to actually get onto JSON.</p><pre><code></code></pre>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[JavaScript - getting to know the beast...]]></title>
            <link>https://blog.johnnyreilly.com/2012/01/30/javascript-getting-to-know-beast</link>
            <guid>JavaScript - getting to know the beast...</guid>
            <pubDate>Mon, 30 Jan 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[So it's 2010 and I've started using jQuery. jQuery is a JavaScript library. This means that I'm writing JavaScript... Gulp! I should say that at this point in time I \*hated\* JavaScript (I have mentioned this previously). But what I know now is that I barely understood the language at all. All the JavaScript I knew was the result of copying and pasting after I'd hit "view source". I don't feel too bad about this - not because my ignorance was laudable but because I certainly wasn't alone in this. It seems that up until recently hardly anyone knew anything about JavaScript. It puzzles me now that I thought this was okay. I suppose like many people I didn't think JavaScript was capable of much and hence felt time spent researching it would be wasted. Just to illustrate where I was then, here is 2009 John's idea of some pretty "advanced" JavaScript:]]></description>
            <content:encoded><![CDATA[<p>So it&#x27;s 2010 and I&#x27;ve started using jQuery. jQuery is a JavaScript library. This means that I&#x27;m writing JavaScript... Gulp! I should say that at this point in time I <!-- -->*<strong>hated</strong>*<!-- --> JavaScript (I have mentioned this previously). But what I know now is that I barely understood the language at all. All the JavaScript I knew was the result of copying and pasting after I&#x27;d hit &quot;view source&quot;. I don&#x27;t feel too bad about this - not because my ignorance was laudable but because I certainly wasn&#x27;t alone in this. It seems that up until recently hardly anyone knew anything about JavaScript. It puzzles me now that I thought this was okay. I suppose like many people I didn&#x27;t think JavaScript was capable of much and hence felt time spent researching it would be wasted. Just to illustrate where I was then, here is 2009 John&#x27;s idea of some pretty &quot;advanced&quot; JavaScript:</p><pre><code class="language-html">function GiveMeASum(iNum1, iNum2) { var dteDate = new Date(); var iTotal = iNum1
+ iNum2; return &quot;This is your total: &quot; + iTotal + &quot;, at this time: &quot; +
dteDate.toString(); }

&lt;input type=&quot;text&quot; id=&quot;Number1&quot; value=&quot;4&quot; /&gt;
&lt;input type=&quot;text&quot; id=&quot;Number2&quot; value=&quot;6&quot; /&gt;
&lt;input
  type=&quot;button&quot;
  value=&quot;Click Me To Add&quot;
  onclick=&quot;alert(GiveMeASum(parseInt(document.getElementById(Number1).value, 10), parseInt(document.getElementById(Number2).value, 10)))&quot;
/&gt;
</code></pre><p>I know - I&#x27;m not to proud of it... Certainly if it was a horse you&#x27;d shoot it. Basically, at that point I knew the following: - JavaScript had functions (but I knew only one way to use them - see above)</p><ul><li>It had some concept of numbers (but I had no idea of the type of numbers I was dealing with; integer / float / decimal / who knows?)</li><li>It had some concept of strings</li><li>It had a date object</li></ul><p>This was about the limit of my knowledge. If I was right, and that&#x27;s all there was to JavaScript then my evaluation of it as utter rubbish would have been accurate. I was wrong. SOOOOOOOOOOOO WRONG! I first realised how wrong I was when I opened up the jQuery source to have a read. Put simply I had <!-- -->*<strong>no</strong>*<!-- --> idea what I was looking at. For a while I wondered if I was actually looking at JavaScript; the code was so different to what I was expecting that for a goodly period I considered jQuery to be some kind of strange black magic; written in a language I did not understand. I was half right. jQuery wasn&#x27;t black magic. But it was written in a language I didn&#x27;t understand; namely JavaScript. :-( Here beginneth the lessons... I started casting around looking for information about JavaScript. Before very long I discovered one <a href="http://www.elijahmanor.com/">Elijah Manor</a> who had helpfully done a number of talks and blog posts directed at C# developers (which I was) about JavaScript. My man! - <a href="http://enterprisejquery.com/2010/10/how-good-c-habits-can-encourage-bad-javascript-habits-part-1/">How good C# habits can encourage bad JavaScript habits part 1</a></p><ul><li><a href="http://enterprisejquery.com/2010/10/how-good-c-habits-can-encourage-bad-javascript-habits-part-2/">How good C# habits can encourage bad JavaScript habits part 2</a></li><li><a href="http://enterprisejquery.com/2010/10/how-good-c-habits-can-encourage-bad-javascript-habits-part-3/">How good C# habits can encourage bad JavaScript habits part 3</a></li><li><a href="https://blogs.msdn.com/b/ukmsdn/archive/2011/06/10/javascript-for-the-c-developer.aspx">Video of Elijah Manor talking through the above material</a></li></ul><p>For me this was all massively helpful. In my development life so far I had only ever dealt with strongly typed, compiled &quot;classical&quot; languages. I had little to no experience of functional, dynamic and loosely typed languages (essentially what JavaScript is). Elijahs work opened up my eyes to some of the massive differences that exist. He also pointed me in the direction of the (never boring) Doug Crockford, author of the best programming book I have ever purchased: <a href="http://www.amazon.co.uk/JavaScript-Good-Parts-Douglas-Crockford/dp/0596517742">JavaScript: The Good Parts</a>. Who could not like a book about JavaScript which starts each chapter with a quote from Shakespeare and still comes in at only a 100 pages? It&#x27;s also worth watching the man in person as he&#x27;s a thoroughly engaging presence. There&#x27;s loads of videos of him out there but this one is pretty good: <a href="http://www.youtube.com/watch?v=v2ifWcnQs6M">Douglas Crockford: The JavaScript Programming Language</a>. I don&#x27;t want to waste your time by attempting to rehash what these guys have done already. I think it&#x27;s always best to go to the source so I&#x27;d advise you to check them out for yourselves. That said it&#x27;s probably worth summarising some of the main points I took away from them (you can find better explanations of all of these through looking at their posts): 1. JavaScript has objects but has no classes. Instead it has (what I still consider to be) the weirdest type of inheritance going: prototypical inheritance. 2. JavaScript has the simplest and loveliest way of creating a new object out there; the &quot;JavaScript Object Literal&quot;. Using this we can simply <code>var myCar = { wheels: 4, colour: &quot;blue&quot; }</code> and ladies and gents we have ourselves a car! (object) 3. In JavaScript functions are <a href="http://en.wikipedia.org/wiki/First-class_function">first class objects</a>. This means functions can be assigned to variables (as easily as you&#x27;d assign a string to a variable) and crucially you can pass them as parameters to a function and pass them back as a return type. Herein lies power! 4. JavaScript has 6 possible values (false, null, undefined, empty strings, 0 and NaN) which it evaluates as false. These are known as the &quot;false-y&quot; values. It&#x27;s a bit weird but on the plus side this can lead to some nicely terse code. 5. To perform comparisons in JavaScript you should avoid == and != and instead use === and !==. Before I discovered this I had been using == and != and then regularly puzzling over some truly odd behaviour. Small though it may sound, this may be the most important discovery of the lot as it was this that lead to me actually <!-- -->*<strong>trusting</strong>*<!-- --> the language. Prior to this I vaguely thought I was picking up on some kind of bug in the JavaScript language which I plain didn&#x27;t understand. (After all, in any sane universe should this really evaluate to true?: <code>0 == &quot;&quot;</code>) 6. Finally JavaScript has function scope rather than block scope. Interestingly it &quot;hoists&quot; variable declaration to the top of each function which can lead to some very surprising behaviour if you don&#x27;t realise what is happening.</p><p>I now realise that JavaScript is a fantastic language because of it&#x27;s flexibility. It is also a deeply flawed language; in part due to it&#x27;s unreasonably forgiving nature (you haven&#x27;t finished your line with a semi-colon; that&#x27;s okay - I can see you meant to so I&#x27;ll stick one in / you haven&#x27;t declared your variable; not a problem I won&#x27;t tell you but I&#x27;ll create a new variable stick it in global scope and off we go etc). It is without question the easiest language with which to create a proper dogs breakfast. To get the best out of JavaScript we need to understand the quirks of the language and we need good patterns. If you&#x27;re interested in getting to grips with it I really advise you to check out the Elijah and Dougs work - it really helped me.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What on earth is jQuery?  And why should I care?]]></title>
            <link>https://blog.johnnyreilly.com/2012/01/24/what-on-earth-is-jquery-and-why-should</link>
            <guid>What on earth is jQuery?  And why should I care?</guid>
            <pubDate>Tue, 24 Jan 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[What on earth is jQuery? What's a jQuery plugin?]]></description>
            <content:encoded><![CDATA[<p>What on earth is jQuery? What&#x27;s a jQuery plugin?</p><p>These were the questions I was asking myself shortly after discovering that jqGrid was a &quot;jQuery plugin&quot;. I&#x27;d been vaguely aware of the phrase &quot;jQuery&quot; being increasingly mentioned on various techical websites since about 2009. But for some reason I&#x27;d felt no urge to find out what it was. I seem to remember that I read the name &quot;jQuery&quot; and jumped to the perfectly logical (in my head) conclusion that this must be a Java SQL engine of some sort. (After all &quot;j&quot; as a prefix to anything so far had generally been Java and &quot;Query&quot; just rang of databases to me.) Clearly I was wrong - life&#x27;s full of surprises.</p><p>I soon discovered that, contrary to expectations, jQuery had nothing to do with Java <!-- -->*<strong>and</strong>*<!-- --> nothing to do with databases either. It was in fact a JavaScript library written by the amazing <a href="http://ejohn.org/about/">John Resig</a>. At the time I had no love for JavaScript. I now realise I knew nearly nothing about it but my feeling was that JavaScript was awful - evil even. However, given JavaScripts ubiquity in the world of web it seemed to be a necessary evil.</p><p>I took a look at the <a href="http://jquery.com/">jQuery website</a> and after reading round a bit I noticed that it could be used for <a href="http://en.wikipedia.org/wiki/Ajax_%28programming%29">Ajax</a> operations. This lead to me reaching the (incorrect) conclusion that jQuery was basically an alternative to the <a href="http://en.wikipedia.org/wiki/ASP.NET_AJAX#Microsoft_Ajax_Library">Microsoft Ajax library</a> which we were already using to call various Web Services. But I remained frankly suspicious of jQuery. What was the point of this library? Why did it exist?</p><p>I read the the <a href="http://weblogs.asp.net/scottgu/archive/2008/09/28/jquery-and-microsoft.aspx">blog</a> by Scott Gu announcing Microsoft was going to start shipping jQuery with Visual Studio. The Great Gu trusted it. Therefore, I figured, it must be okay... Right?</p><p>The thing was, I was quite happy with the Microsoft Ajax library. I was familiar with it. It worked. Why switch? I saw the various operations Scott Gu was doing to divs on the screen using jQuery. I didn&#x27;t want to do anything like that at all. As I said; I had no love for JavaScript - I viewed it as C#&#x27;s simple-minded idiot cousin. My unofficial motto when doing web stuff was &quot;wherever possible, do it on the server&quot;.</p><p>I think I would have ignored jQuery entirely but for the fact of jqGrid. If I wanted to use jqGrid I had to use jQuery as well. In the end I decided I&#x27;d allow it house room just for the sake of jqGrid and I&#x27;d just ignore it apart from that. And that&#x27;s how it was for a while.</p><p>Then I had an epiphany. Okay - that&#x27;s overplaying it. What actually happened was I realised that something we were doing elsewhere could be done faster and easier with jQuery. It&#x27;s something so ridiculously feeble that I feel vaguely embarrassed sharing it. Anyway.</p><p>So, you know the css hover behaviour is only implemented for anchor tags in IE6? No? Well read this <a href="http://stackoverflow.com/questions/36605/ie-6-css-hover-non-anchor-tag">Stack Overflow</a> entry - it&#x27;ll clarify. Well, the app that I was working on was an internal web application only used by people with the corporate installation of IE 6 on their desktops. And it was &quot;terribly important&quot; that buttons had hover behaviour. For reasons that now escape me we were doing this by manually adding inline onmouseover / onmouseout event handlers to each input button on the screen in turn in every page in the <a href="http://msdn.microsoft.com/en-us/library/ms178472.aspx">Page_Load</a> event server side. I think we were aware it wasn&#x27;t fantastic to have to wire up each button in turn. But it worked and as with so many development situations we had other pressures, other requirements to fulfil and other fish to fry - so we left it at that.</p><p>And then it occurred to me... What about using the <a href="http://api.jquery.com/class-selector/">jQuery class selector</a> in conjunction with the <a href="http://api.jquery.com/hover/">jQuery hover event</a>? I could have one method that I called on a page which would wire up all of my hover behaviours in one fell swoop. I wouldn&#x27;t need to do input-by-input wireups anymore! Hallelujah! This is what I did:</p><p>The buttons I would like to style:</p><pre><code class="language-html">&lt;input type=&quot;button&quot; value=&quot;I am a button&quot; class=&quot;itIsAButton&quot; /&gt;
&lt;input type=&quot;button&quot; value=&quot;So am I&quot; class=&quot;itIsAButton&quot; /&gt;
&lt;input type=&quot;button&quot; value=&quot;Me too&quot; class=&quot;itIsAButton&quot; /&gt;
</code></pre><p>My CSS (filter, by the way, is just linear gradients in IE 6-9):</p><pre><code class="language-css">.itIsAButton {
  filter: progid:DXImageTransform.Microsoft.Gradient (GradientType=0,StartColorStr=&#x27;#ededed&#x27;,EndColorStr=&#x27;#cdcdcd&#x27;);
}

.itIsAButton:hover, .itIsAButton_hover /* &quot;_hover&quot; is for IE6 */ {
  filter: progid:DXImageTransform.Microsoft.Gradient (GradientType=0,StartColorStr=&#x27;#f6f6f6&#x27;,EndColorStr=&#x27;#efefef&#x27;);
}
</code></pre><p>My jQuery:</p><pre><code class="language-js">$(document).ready(function () {
  //Add hover behaviour on picker buttons for IE6
  if ($.browser.msie &amp;&amp; parseInt($.browser.version, 10) &lt; 7) {
    var fnButtonHover = function (handlerInOut) {
      var $btn = $(this);
      var sOriginalClass = $btn.prop(&#x27;class&#x27;);

      if (handlerInOut.type === &#x27;mouseenter&#x27;) {
        //If not already hovering class then apply it
        if (sOriginalClass.indexOf(&#x27;_hover&#x27;) === -1) {
          $btn.prop(&#x27;class&#x27;, sOriginalClass + &#x27;_hover&#x27;);
        }
      } else if (handlerInOut.type === &#x27;mouseleave&#x27;) {
        //If not already non-hovering class then apply it
        if (sOriginalClass.indexOf(&#x27;_hover&#x27;) !== -1) {
          $btn.prop(&#x27;class&#x27;, sOriginalClass.split(&#x27;_&#x27;)[0]);
        }
      }
    };

    $(&#x27;.itIsAButton&#x27;).hover(fnButtonHover);
  }
});
</code></pre><p>And it worked. I didn&#x27;t really understand this much about this jQuery &quot;thing&quot; at that point but I could now see that it clearly had at least one use. I&#x27;ve come to appreciate that jQuery is one of the best pieces of software I&#x27;ve ever encountered. Over time I may go further into some of the good stuff of jQuery. It is, quite simply, brilliant.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[jqGrid - it's just a far better grid]]></title>
            <link>https://blog.johnnyreilly.com/2012/01/14/jqgrid-its-just-far-better-grid</link>
            <guid>jqGrid - it's just a far better grid</guid>
            <pubDate>Sat, 14 Jan 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[The year was 2010 (not really that long ago I know) and the project that I was working on was sorely in need of a new grid component. It was an ASP.NET WebForms project and for some time we'd been using what was essentially a glorified datagrid which had a few extra features implemented to allow us to change column order / columns displayed / copy contents to clipboard etc. Our grid worked perfectly fine - it gave us the functionality we needed. However, it looked pretty terrible, and had some "quirky" approaches in place for supporting IE and Firefox side by side. Also, at the time we were attempting to make our app seem new and exciting again for the users. The surprising truth is that users seem to be more impressed with a visual revamp than with new or amended functionality. So I was looking for something which would make them sit up and say "oooh - isn't it pretty!". Unfortunately the nature of the organisation I was working for was not one that lended itself to paying for components. They were occasionally willing to do that but the hoops that would have to be jumped through first, the forms that would need to be signed in triplicate by people that had nearly nothing to do with the project made that an unattractive prospect. So I began my search initially looking at the various open source offerings that were around. As a minimum I was looking for something that would do what our home-grown component did already (change column order / columns displayed / copy contents to clipboard etc) but hopefully in a "nicer" way. Also, I had long been unhappy with the fact that to get our current grid to render results we did a \*full postback\* to the server and re-rendered the whole page. Pointless! Why should you need to do all this each time when you only wanted to refresh the data? Instead I was thinking about using an Ajax approach; a grid that could just get the data that it needed and render it to the client. This seemed to me a vastly "cleaner" solution - why update a whole screen when you only want to update a small part of it? Why not save yourself the trouble of having to ensure that all other screen controls are persisted just as you'd like them after the postback? I also thought it was probably something that would scale better as it would massively reduce the amount of data moving backwards and forwards between client and server. No need for a full page life cycle on the server each time the grid refreshes. Just simple data travelling down the pipes of web. With the above criteria in mind I set out on my Google quest for a grid. Quite soon I found that there was a component out there which seemed to do all that I wanted and far more besides. It was called jqGrid:]]></description>
            <content:encoded><![CDATA[<p>The year was 2010 (not really that long ago I know) and the project that I was working on was sorely in need of a new grid component. It was an <a href="http://www.asp.net/web-forms">ASP.NET WebForms</a> project and for some time we&#x27;d been using what was essentially a glorified <a href="http://msdn.microsoft.com/en-us/library/system.web.ui.webcontrols.datagrid.aspx">datagrid</a> which had a few extra features implemented to allow us to change column order / columns displayed / copy contents to clipboard etc. Our grid worked perfectly fine - it gave us the functionality we needed. However, it looked pretty terrible, and had some &quot;quirky&quot; approaches in place for supporting IE and Firefox side by side. Also, at the time we were attempting to make our app seem new and exciting again for the users. The surprising truth is that users seem to be more impressed with a visual revamp than with new or amended functionality. So I was looking for something which would make them sit up and say &quot;oooh - isn&#x27;t it pretty!&quot;. Unfortunately the nature of the organisation I was working for was not one that lended itself to paying for components. They were occasionally willing to do that but the hoops that would have to be jumped through first, the forms that would need to be signed in triplicate by people that had nearly nothing to do with the project made that an unattractive prospect. So I began my search initially looking at the various open source offerings that were around. As a minimum I was looking for something that would do what our home-grown component did already (change column order / columns displayed / copy contents to clipboard etc) but hopefully in a &quot;nicer&quot; way. Also, I had long been unhappy with the fact that to get our current grid to render results we did a <!-- -->*<strong>full postback</strong>*<!-- --> to the server and re-rendered the whole page. Pointless! Why should you need to do all this each time when you only wanted to refresh the data? Instead I was thinking about using an <a href="http://en.wikipedia.org/wiki/Ajax_%28programming%29">Ajax</a> approach; a grid that could just get the data that it needed and render it to the client. This seemed to me a vastly &quot;cleaner&quot; solution - why update a whole screen when you only want to update a small part of it? Why not save yourself the trouble of having to ensure that all other screen controls are persisted just as you&#x27;d like them after the postback? I also thought it was probably something that would scale better as it would massively reduce the amount of data moving backwards and forwards between client and server. No need for a full page life cycle on the server each time the grid refreshes. Just simple data travelling down the pipes of web. With the above criteria in mind I set out on my Google quest for a grid. Quite soon I found that there was a component out there which seemed to do all that I wanted and far more besides. It was called <a href="http://www.trirand.com/blog/">jqGrid</a>: <img src="../static/blog/2012-01-14-jqgrid-its-just-far-better-grid/jqgrid%2Bin%2Ball%2Bits%2Bglory.png"/></p><p>Oooh look at the goodness! It had both column re-ordering and column choosing built in!: This was a <!-- -->*<strong>very promising sign</strong>*<!-- -->! Now it&#x27;s time for me to demonstrate my ignorance. According to the website this grid component was a &quot;jQuery plugin&quot;. At the time I read this I had no idea what jQuery was at all - let alone what a plugin for it was. Anyway, I don&#x27;t want to get diverted so let&#x27;s just say that reading this lead to me getting an urgent education about some of the client side aspects of the modern web that I had been previously unaware of. I digress. This component did exactly what I wanted in terms of just sending data down the pipe. jqGrid worked with a whole number of possible data sources; XML, Array but the most exciting for me was obviously <a href="http://www.json.org/">JSON</a>. Take a look a the grid rendered below and the JSON that powered it (all from a simple <a href="http://www.trirand.com/blog/jqgrid/server.php?q=2&amp;_search=false&amp;nd=1326531357333&amp;rows=10&amp;page=1&amp;sidx=id&amp;sord=desc">GET</a> request): <img src="../static/blog/2012-01-14-jqgrid-its-just-far-better-grid/Check%2Bout%2Bthe%2BJSON.png"/></p><p>As you can see from the above screenshot, the grid has populated itself using the results of a web request. The only information that has gone to the server are the relevant criteria to drive the search results. The only information that has come back from the server is the data needed to drive the grid. Simple. Beautiful. I loved it and I wanted to use it. So I did! I had to take a few steps that most people thinking about using a grid component probably wont need to. First of all I had to write an ASP.Net WebForms wrapper for jqGrid which could be implemented in a similar way to our current custom datagrid. This was because, until the users were convinced that the new grid was better than the old both had to co-exist in the project and the user would have the option to switch between the two. This WebForms wrapper plugged into our old school XML column definition files and translated them into JSON for the grid. It also took <a href="http://msdn.microsoft.com/en-us/library/system.data.dataset.aspx">datasets</a> (which drove our old grid) and translated them into jqGrid-friendly JSON. I wanted to power the jqGrid using WebMethods on ASPX&#x27;s. After a little digging I found <a href="http://encosia.com/using-jquery-to-directly-call-aspnet-ajax-page-methods/">Dave Ward of Encosia&#x27;s post</a> which made it very simple (and in line with this I switched over from <a href="http://en.wikipedia.org/wiki/GET_%28HTTP%29#Request_methods">GET</a> requests to <a href="http://en.wikipedia.org/wiki/POST_%28HTTP%29">POSTs</a>). Finally I wrote some custom javascript which added a button to jqGrid which, if clicked, would copy the contents of the jqGrid to the clipboard (this was the only bit of functionality that didn&#x27;t appear to be implemented out of the box with jqGrid). I think I&#x27;m going to leave it there for now but I just wanted to say that I think jqGrid is a fantastic component and it&#x27;s certainly made my life better! It&#x27;s: - well supported, there is lots on <a href="http://stackoverflow.com/questions/tagged/jqgrid">StackOverflow</a> and the like about it</p><ul><li>there are regular <a href="http://www.trirand.com/blog/">releases / upgrades</a></li><li>there are good online <a href="http://trirand.com/blog/jqgrid/jqgrid.html">demonstrations</a> and <a href="http://www.trirand.com/jqgridwiki/doku.php">documentation</a></li></ul><p>I think Tony Tomov (the man behind jqGrid) has come up with something truly brilliant. It&#x27;s worth saying that the equally brilliant jQueryUI team are in the process of writing an official <a href="http://wiki.jqueryui.com/w/page/34246941/Grid">jQuery UI grid component</a> which uses jqGrid as one of its inspirations. However, this is still a long way from even a &quot;zero feature&quot; release. In the meantime jqGrid is continuing to go from strength to strength and as such I heartily recommend it. Finally, you can take a look at jqGrid&#x27;s source on <a href="https://github.com/tonytomov/jqGrid">GitHub</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Standing on the Shoulders of Giants...]]></title>
            <link>https://blog.johnnyreilly.com/2012/01/07/standing-on-shoulders-of-giants</link>
            <guid>Standing on the Shoulders of Giants...</guid>
            <pubDate>Sat, 07 Jan 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[It started with Scott Hanselman. I had no particular plans to start a blog at all. However, I was reading Scott Hanselman's turn of the year post and I was struck with an idea.]]></description>
            <content:encoded><![CDATA[<p>It started with Scott Hanselman. I had no particular plans to start a blog at all. However, I was reading Scott Hanselman&#x27;s turn of the year <a href="http://www.hanselman.com/blog/YourBlogIsTheEngineOfCommunity.aspx">post</a> and I was struck with an idea.</p><p>First, let me give a little background about myself. I&#x27;m a software developer. I&#x27;ve been in the industry for coming up to 15 years. I started out professionally writing call centre software. I moved on to code in a variety of different industries from straight IT to marketing and, for the last 7 years, finance.</p><p>Though I initially started out writing in Delphi I fast found myself moving toward the Microsoft &quot;stack of love&quot;. I should say that this move was not because I instinctively liked Microsofts stuff (in fact in the beginning I actively disliked it - moving from Delphi 3.0 to Visual Studio 5 left me finding Microsoft&#x27;s offering very much wanting). Rather it was pragmatic. I needed a job and at the time VB was a far more transferable skill than Delphi. What with the all encompassing <a href="http://en.wikipedia.org/wiki/Dot-com_bubble">dot-com bubble</a> of the late 90&#x27;s I soon found myself working in the webtastic world of classic ASP (weep) and VB server components (remember them?).</p><p>Though things can improve - and in my opinion they really did when Microsoft coughed up the first furball of ASP.NET Beta in (I think) 2001. I grabbed on with both hands. Since that point I&#x27;ve been earning my bread pretty much, though not exclusively, in the ASP.NET universe.</p><p>The one thing that might not be clear from the above curriculum vitae is this: <strong>I AM A COMPLETE AMATEUR.</strong> I mean this in both senses of the word:</p><ol><li>I have no formal training to speak of - I didn&#x27;t do a computer sciences degree. In fact my first real coding experience was writing a program in <a href="http://en.wikipedia.org/wiki/Locomotive_BASIC">Locomotive Basic</a> for my father on our humble Amstrad CPC.</li><li>That said, I love it. I find writing code an intellectually, emotionally, creatively satisfying act. And whilst I undoubtedly have less of the theoretical knowledge which most professional developers seem to have, I probably counter-balance that with a hunger to keep learning and keep trying new things. And since software never sits still that&#x27;s probably just as well. Keep watching the horizon - there will be something coming over it! And it&#x27;s worth saying, I have an instinct for developing which serves me pretty well. I&#x27;m good at coming up with elegant and pragmatic solutions. Put simply: I&#x27;m good at making code work.</li></ol><p>So back to the point. In my daily work life, like any other developer, I am repeatedly called on to turn someones requirement into a reality. Very rarely do I achieve this on my own. Like most of us I&#x27;m a dwarf standing on the shoulders of giants. There&#x27;s a lot of people out there who come up with useful tools / components / plug-ins that make it possible for me to deliver much more than I would given my own abilities.</p><p>So that&#x27;s what I want to do: I want to talk about the tools, components and techniques that I have found useful in the everyday working life of a developer. It&#x27;s likely to be quite a &quot;webby&quot; blog as I probably find that the most interesting area of development at the moment.</p><p>I don&#x27;t know how often I will write but my plan is that when I do, each time I&#x27;ll talk about something I&#x27;ve found useful - why I found it useful, what problems it solved, what issues it still presented me with and so on. This is probably not going to be a &quot;techie techie&quot; blog. Rather a blog that deals with the situations that can confront a developer and how I&#x27;ve responded to them. I hope you find it interesting. And if you don&#x27;t; please keep it to yourself :-)</p>]]></content:encoded>
        </item>
    </channel>
</rss>